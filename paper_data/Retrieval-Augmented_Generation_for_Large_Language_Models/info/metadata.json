{
    "de549c1592a62c129b8d49c8c0137aa6859b103f.pdf": {
        "title": "Internet-Augmented Dialogue Generation",
        "authors": [
            "M. Komeili",
            "Kurt Shuster",
            "J. Weston"
        ],
        "published_date": "2021",
        "abstract": "The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b).",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/de549c1592a62c129b8d49c8c0137aa6859b103f.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "38b0803b59e4973f09018ce942164b02be4b8bc9.pdf": {
        "title": "MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text",
        "authors": [
            "Wenhu Chen",
            "Hexiang Hu",
            "Xi Chen",
            "Pat Verga",
            "William W. Cohen"
        ],
        "published_date": "2022",
        "abstract": "While language Models store a massive amount of world knowledge implicitly in their parameters, even very large models often fail to encode information about rare entities and events, while incurring huge computational costs. Recently, retrieval-augmented models, such as REALM, RAG, and RETRO, have incorporated world knowledge into language generation by leveraging an external non-parametric index and have demonstrated impressive performance with constrained model sizes. However, these methods are restricted to retrieving only textual knowledge, neglecting the ubiquitous amount of knowledge in other modalities like images \u2013 much of which contains information not covered by any text. To address this limitation, we propose the first Multimodal Retrieval-Augmented Transformer (MuRAG), which accesses an external non-parametric multimodal memory to augment language generation. MuRAG is pre-trained with a mixture of large-scale image-text and text-only corpora using a joint contrastive and generative loss. We perform experiments on two different datasets that require retrieving and reasoning over both images and text to answer a given query: WebQA, and MultimodalQA. Our results show that MuRAG achieves state-of-the-art accuracy, outperforming existing models by 10-20% absolute on both datasets and under both distractor and full-wiki settings.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/38b0803b59e4973f09018ce942164b02be4b8bc9.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "ab8ee8d06ed581c876da3a2e7a5fdb1cbec21a45.pdf": {
        "title": "KAT: A Knowledge Augmented Transformer for Vision-and-Language",
        "authors": [
            "Liangke Gui",
            "Borui Wang",
            "Qiuyuan Huang",
            "A. Hauptmann",
            "Yonatan Bisk",
            "Jianfeng Gao"
        ],
        "published_date": "2021",
        "abstract": "The primary focus of recent work with large-scale transformers has been on optimizing the amount of information packed into the model\u2019s parameters. In this work, we ask a complementary question: Can multimodal transformers leverage explicit knowledge in their reasoning? Existing, primarily unimodal, methods have explored approaches under the paradigm of knowledge retrieval followed by answer prediction, but leave open questions about the quality and relevance of the retrieved knowledge used, and how the reasoning processes over implicit and explicit knowledge should be integrated. To address these challenges, we propose a - Knowledge Augmented Transformer (KAT) - which achieves a strong state-of-the-art result (+6% absolute) on the open-domain multimodal task of OK-VQA. Our approach integrates implicit and explicit knowledge in an encoder-decoder architecture, while still jointly reasoning over both knowledge sources during answer generation. Additionally, explicit knowledge integration improves interpretability of model predictions in our analysis.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/ab8ee8d06ed581c876da3a2e7a5fdb1cbec21a45.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "ed99a2572fb5f4240aa6068e3bf274832e831306.pdf": {
        "title": "Recitation-Augmented Language Models",
        "authors": [
            "Zhiqing Sun",
            "Xuezhi Wang",
            "Yi Tay",
            "Yiming Yang",
            "Denny Zhou"
        ],
        "published_date": "2022",
        "abstract": "We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs' own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of \\method~on four pre-trained models (PaLM, UL2, OPT, and Codex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at\"https://github.com/Edward-Sun/RECITE\".",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/ed99a2572fb5f4240aa6068e3bf274832e831306.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "4a21aa3c75c4f29f9b340a73ff24f20834a7b686.pdf": {
        "title": "Retrieval-Augmented Transformer for Image Captioning",
        "authors": [
            "Sara Sarto",
            "Marcella Cornia",
            "L. Baraldi",
            "R. Cucchiara"
        ],
        "published_date": "2022",
        "abstract": "Image captioning models aim at connecting Vision and Language by providing natural language descriptions of input images. In the past few years, the task has been tackled by learning parametric models and proposing visual feature extraction advancements or by modeling better multi-modal connections. In this paper, we investigate the development of an image captioning approach with a kNN memory, with which knowledge can be retrieved from an external corpus to aid the generation process. Our architecture combines a knowledge retriever based on visual similarities, a differentiable encoder, and a kNN-augmented attention layer to predict tokens based on the past context and on text retrieved from the external memory. Experimental results, conducted on the COCO dataset, demonstrate that employing an explicit external memory can aid the generation process and increase caption quality. Our work opens up new avenues for improving image captioning models at larger scale.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/4a21aa3c75c4f29f9b340a73ff24f20834a7b686.pdf",
        "venue": "International Conference on Content-Based Multimedia Indexing",
        "citationCount": 0,
        "score": 0
    },
    "ef76276f9ef929496f03282fa85ae1bbcdc69767.pdf": {
        "title": "Robust Retrieval Augmented Generation for Zero-shot Slot Filling",
        "authors": [
            "Michael R. Glass",
            "Gaetano Rossiello",
            "Md. Faisal Mahbub Chowdhury",
            "A. Gliozzo"
        ],
        "published_date": "2021",
        "abstract": "Automatically inducing high quality knowledge graphs from a given collection of documents still remains a challenging problem in AI. One way to make headway for this problem is through advancements in a related task known as slot filling. In this task, given an entity query in form of [Entity, Slot, ?], a system is asked to \u2018fill\u2019 the slot by generating or extracting the missing value exploiting evidence extracted from relevant passage(s) in the given document collection. The recent works in the field try to solve this task in an end-to-end fashion using retrieval-based language models. In this paper, we present a novel approach to zero-shot slot filling that extends dense passage retrieval with hard negatives and robust training procedures for retrieval augmented generation models. Our model reports large improvements on both T-REx and zsRE slot filling datasets, improving both passage retrieval and slot value generation, and ranking at the top-1 position in the KILT leaderboard. Moreover, we demonstrate the robustness of our system showing its domain adaptation capability on a new variant of the TACRED dataset for slot filling, through a combination of zero/few-shot learning. We release the source code and pre-trained models.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/ef76276f9ef929496f03282fa85ae1bbcdc69767.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5.pdf": {
        "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
        "authors": [
            "Yunfan Gao",
            "Yun Xiong",
            "Xinyu Gao",
            "Kangxiang Jia",
            "Jinliu Pan",
            "Yuxi Bi",
            "Yi Dai",
            "Jiawei Sun",
            "Qianyu Guo",
            "Meng Wang",
            "Haofen Wang"
        ],
        "published_date": "2023",
        "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "eb9c4a07a336e8deefe7b399c550d3af0241238e.pdf": {
        "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
        "authors": [
            "Wenqi Fan",
            "Yujuan Ding",
            "Liang-bo Ning",
            "Shijie Wang",
            "Hengyun Li",
            "Dawei Yin",
            "Tat-Seng Chua",
            "Qing Li"
        ],
        "published_date": "2024",
        "abstract": "As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at: https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/eb9c4a07a336e8deefe7b399c550d3af0241238e.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 0,
        "score": 0
    },
    "b798cf6af813638fab09a8af6ad0f3df6c241485.pdf": {
        "title": "Benchmarking Retrieval-Augmented Generation for Medicine",
        "authors": [
            "Guangzhi Xiong",
            "Qiao Jin",
            "Zhiyong Lu",
            "Aidong Zhang"
        ],
        "published_date": "2024",
        "abstract": "While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs by up to 18% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the\"lost-in-the-middle\"effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/b798cf6af813638fab09a8af6ad0f3df6c241485.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "28e2ecb4183ebc0eec504b12dddc677f8aef8745.pdf": {
        "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
        "authors": [
            "Jiawei Chen",
            "Hongyu Lin",
            "Xianpei Han",
            "Le Sun"
        ],
        "published_date": "2023",
        "abstract": "Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/28e2ecb4183ebc0eec504b12dddc677f8aef8745.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0
    },
    "9ab45aa875b56335303398e84a59a3756cd9d530.pdf": {
        "title": "Graph Retrieval-Augmented Generation: A Survey",
        "authors": [
            "Boci Peng",
            "Yun Zhu",
            "Yongchao Liu",
            "Xiaohe Bo",
            "Haizhou Shi",
            "Chuntao Hong",
            "Yan Zhang",
            "Siliang Tang"
        ],
        "published_date": "2024",
        "abstract": "Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable success in addressing the challenges of Large Language Models (LLMs) without necessitating retraining. By referencing an external knowledge base, RAG refines LLM outputs, effectively mitigating issues such as ``hallucination'', lack of domain-specific knowledge, and outdated information. However, the complex structure of relationships among different entities in databases presents challenges for RAG systems. In response, GraphRAG leverages structural information across entities to enable more precise and comprehensive retrieval, capturing relational knowledge and facilitating more accurate, context-aware responses. Given the novelty and potential of GraphRAG, a systematic review of current technologies is imperative. This paper provides the first comprehensive overview of GraphRAG methodologies. We formalize the GraphRAG workflow, encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced Generation. We then outline the core technologies and training methods at each stage. Additionally, we examine downstream tasks, application domains, evaluation methodologies, and industrial use cases of GraphRAG. Finally, we explore future research directions to inspire further inquiries and advance progress in the field. In order to track recent progress in this field, we set up a repository at \\url{https://github.com/pengboci/GraphRAG-Survey}.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/9ab45aa875b56335303398e84a59a3756cd9d530.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "4e71624e90960cb003e311a0fe3b8be4c2863239.pdf": {
        "title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries",
        "authors": [
            "Yixuan Tang",
            "Yi Yang"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) augments large language models (LLM) by retrieving relevant knowledge, showing promising potential in mitigating LLM hallucinations and enhancing response quality, thereby facilitating the great adoption of LLMs in practice. However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utilizing an English news article dataset as the underlying RAG knowledge base. We demonstrate the benchmarking utility of MultiHop-RAG in two experiments. The first experiment compares different embedding models for retrieving evidence for multi-hop queries. In the second experiment, we examine the capabilities of various state-of-the-art LLMs, including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop queries given the evidence. Both experiments reveal that existing RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable resource for the community in developing effective RAG systems, thereby facilitating greater adoption of LLMs in practice. The MultiHop-RAG and implemented RAG system is publicly available at https://github.com/yixuantt/MultiHop-RAG/.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/4e71624e90960cb003e311a0fe3b8be4c2863239.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "a41d4a3b005c8ec4f821e6ee96672d930ca9596c.pdf": {
        "title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
        "authors": [
            "Xiaoxin He",
            "Yijun Tian",
            "Yifei Sun",
            "N. Chawla",
            "T. Laurent",
            "Yann LeCun",
            "Xavier Bresson",
            "Bryan Hooi"
        ],
        "published_date": "2024",
        "abstract": "Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.~\\footnote{Our codes and datasets are available at: \\url{https://github.com/XiaoxinHe/G-Retriever}}",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/a41d4a3b005c8ec4f821e6ee96672d930ca9596c.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "b708e0f49d8e9708bc649debd9a9372748fffa3d.pdf": {
        "title": "Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering",
        "authors": [
            "Zhentao Xu",
            "Mark Jerome Cruz",
            "Matthew Guevara",
            "Tie Wang",
            "Manasi Deshpande",
            "Xiaofeng Wang",
            "Zheng Li"
        ],
        "published_date": "2024",
        "abstract": "In customer service technical support, swiftly and accurately retrieving relevant past issues is critical for efficiently resolving customer inquiries. The conventional retrieval methods in retrieval-augmented generation (RAG) for large language models (LLMs) treat a large corpus of past issue tracking tickets as plain text, ignoring the crucial intra-issue structure and inter-issue relations, which limits performance. We introduce a novel customer service question-answering method that amalgamates RAG with a knowledge graph (KG). Our method constructs a KG from historical issues for use in retrieval, retaining the intra-issue structure and inter-issue relations. During the question-answering phase, our method parses consumer queries and retrieves related sub-graphs from the KG to generate answers. This integration of a KG not only improves retrieval accuracy by preserving customer service structure information but also enhances answering quality by mitigating the effects of text segmentation. Empirical assessments on our benchmark datasets, utilizing key retrieval (MRR, Recall@K, NDCG@K) and text generation (BLEU, ROUGE, METEOR) metrics, reveal that our method outperforms the baseline by 77.6% in MRR and by 0.32 in BLEU. Our method has been deployed within LinkedIn's customer service team for approximately six months and has reduced the median per-issue resolution time by 28.6%.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/b708e0f49d8e9708bc649debd9a9372748fffa3d.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 0,
        "score": 0
    },
    "5bbc2b5aa6c63c6a2cfccf095d6020b063ad47ac.pdf": {
        "title": "Corrective Retrieval Augmented Generation",
        "authors": [
            "Shi-Qi Yan",
            "Jia-Chen Gu",
            "Yun Zhu",
            "Zhen-Hua Ling"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) inevitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return sub-optimal documents, large-scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose-then-recompose algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them. CRAG is plug-and-play and can be seamlessly coupled with various RAG-based approaches. Experiments on four datasets covering short- and long-form generation tasks show that CRAG can significantly improve the performance of RAG-based approaches.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/5bbc2b5aa6c63c6a2cfccf095d6020b063ad47ac.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "80478de9c7a81561e2f3dac9b8b1ef3df389ff2d.pdf": {
        "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs",
        "authors": [
            "Yue Yu",
            "Wei Ping",
            "Zihan Liu",
            "Boxin Wang",
            "Jiaxuan You",
            "Chao Zhang",
            "M. Shoeybi",
            "Bryan Catanzaro"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, we compare our model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, our Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/80478de9c7a81561e2f3dac9b8b1ef3df389ff2d.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "ea89b058ce619ed16d4de633126b02a8179457c8.pdf": {
        "title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)",
        "authors": [
            "Shenglai Zeng",
            "Jiankun Zhang",
            "Pengfei He",
            "Yue Xing",
            "Yiding Liu",
            "Han Xu",
            "Jie Ren",
            "Shuaiqiang Wang",
            "Dawei Yin",
            "Yi Chang",
            "Jiliang Tang"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) is a powerful technique to facilitate language model with proprietary and private data, where data privacy is a pivotal concern. Whereas extensive research has demonstrated the privacy risks of large language models (LLMs), the RAG technique could potentially reshape the inherent behaviors of LLM generation, posing new privacy issues that are currently under-explored. In this work, we conduct extensive empirical studies with novel attack methods, which demonstrate the vulnerability of RAG systems on leaking the private retrieval database. Despite the new risk brought by RAG on the retrieval data, we further reveal that RAG can mitigate the leakage of the LLMs' training data. Overall, we provide new insights in this paper for privacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG systems builders. Our code is available at https://github.com/phycholosogy/RAG-privacy.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/ea89b058ce619ed16d4de633126b02a8179457c8.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "e90435e1ae06fab4efa272f5f46ed74ca0a8cde0.pdf": {
        "title": "Evaluating Retrieval Quality in Retrieval-Augmented Generation",
        "authors": [
            "Alireza Salemi",
            "Hamed Zamani"
        ],
        "published_date": "2024",
        "abstract": "Evaluating retrieval-augmented generation (RAG) presents challenges, particularly for retrieval models within these systems. Traditional end-to-end evaluation methods are computationally expensive. Furthermore, evaluation of the retrieval model's performance based on query-document relevance labels shows a small correlation with the RAG system's downstream performance. We propose a novel evaluation approach, eRAG, where each document in the retrieval list is individually utilized by the large language model within the RAG system. The output generated for each document is then evaluated based on the downstream task ground truth labels. In this manner, the downstream performance for each document serves as its relevance label. We employ various downstream task metrics to obtain document-level annotations and aggregate them using set-based or ranking metrics. Extensive experiments on a wide range of datasets demonstrate that eRAG achieves a higher correlation with downstream RAG performance compared to baseline methods, with improvements in Kendall's tau correlation ranging from 0.168 to 0.494. Additionally, eRAG offers significant computational advantages, improving runtime and consuming up to 50 times less GPU memory than end-to-end evaluation.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/e90435e1ae06fab4efa272f5f46ed74ca0a8cde0.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 0,
        "score": 0
    },
    "746b96ee17e329f1085a047116c05e12eaa3925a.pdf": {
        "title": "RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation",
        "authors": [
            "Chi-Min Chan",
            "Chunpu Xu",
            "Ruibin Yuan",
            "Hongyin Luo",
            "Wei Xue",
            "Yi-Ting Guo",
            "Jie Fu"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, and disambiguation. Our experimental results indicate that our method, when applied to a 7B Llama2 model, surpasses the previous state-of-the-art (SOTA) by an average of 1.9\\% across three single-hop QA datasets, and also demonstrates enhanced performance in handling complex, multi-hop QA datasets. Our code is available at https://github.com/chanchimin/RQ-RAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/746b96ee17e329f1085a047116c05e12eaa3925a.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "965a0969b460f9246158d88fb28e21c5d80d0a8b.pdf": {
        "title": "Optimization of hepatological clinical guidelines interpretation by large language models: a retrieval augmented generation-based framework",
        "authors": [
            "Simone Kresevic",
            "M. Giuffr\u00e9",
            "M. Aj\u010devi\u0107",
            "A. Accardo",
            "L. Croc\u00e8",
            "D. Shung"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) can potentially transform healthcare, particularly in providing the right information to the right provider at the right time in the hospital workflow. This study investigates the integration of LLMs into healthcare, specifically focusing on improving clinical decision support systems (CDSSs) through accurate interpretation of medical guidelines for chronic Hepatitis C Virus infection management. Utilizing OpenAI\u2019s GPT-4 Turbo model, we developed a customized LLM framework that incorporates retrieval augmented generation (RAG) and prompt engineering. Our framework involved guideline conversion into the best-structured format that can be efficiently processed by LLMs to provide the most accurate output. An ablation study was conducted to evaluate the impact of different formatting and learning strategies on the LLM\u2019s answer generation accuracy. The baseline GPT-4 Turbo model\u2019s performance was compared against five experimental setups with increasing levels of complexity: inclusion of in-context guidelines, guideline reformatting, and implementation of few-shot learning. Our primary outcome was the qualitative assessment of accuracy based on expert review, while secondary outcomes included the quantitative measurement of similarity of LLM-generated responses to expert-provided answers using text-similarity scores. The results showed a significant improvement in accuracy from 43 to 99% (p\u2009<\u20090.001), when guidelines were provided as context in a coherent corpus of text and non-text sources were converted into text. In addition, few-shot learning did not seem to improve overall accuracy. The study highlights that structured guideline reformatting and advanced prompt engineering (data quality vs. data quantity) can enhance the efficacy of LLM integrations to CDSSs for guideline delivery.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/965a0969b460f9246158d88fb28e21c5d80d0a8b.pdf",
        "venue": "npj Digit. Medicine",
        "citationCount": 0,
        "score": 0
    },
    "336605fc899aab6c5b375d1129bf656d246b9013.pdf": {
        "title": "GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning",
        "authors": [
            "Costas Mavromatis",
            "George Karypis"
        ],
        "published_date": "2024",
        "abstract": "Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form of triplets (head, relation, tail), which collectively form a graph. Question Answering over KGs (KGQA) is the task of answering natural questions grounding the reasoning to the information provided by the KG. Large Language Models (LLMs) are the state-of-the-art models for QA tasks due to their remarkable ability to understand natural language. On the other hand, Graph Neural Networks (GNNs) have been widely used for KGQA as they can handle the complex graph information stored in the KG. In this work, we introduce GNN-RAG, a novel method for combining language understanding abilities of LLMs with the reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style. First, a GNN reasons over a dense KG subgraph to retrieve answer candidates for a given question. Second, the shortest paths in the KG that connect question entities and answer candidates are extracted to represent KG reasoning paths. The extracted paths are verbalized and given as input for LLM reasoning with RAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to extract useful graph information, while the LLM leverages its natural language processing ability for ultimate KGQA. Furthermore, we develop a retrieval augmentation (RA) technique to further boost KGQA performance with GNN-RAG. Experimental results show that GNN-RAG achieves state-of-the-art performance in two widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching GPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop and multi-entity questions outperforming competing approaches by 8.9--15.5% points at answer F1.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/336605fc899aab6c5b375d1129bf656d246b9013.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "daebec92963ab8dea492f0c209bdf57e87bcaa07.pdf": {
        "title": "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research",
        "authors": [
            "Jiajie Jin",
            "Yutao Zhu",
            "Xinyu Yang",
            "Chenghao Zhang",
            "Zhicheng Dou"
        ],
        "published_date": "2024",
        "abstract": "With the advent of large language models (LLMs) and multimodal large language models (MLLMs), the potential of retrieval-augmented generation (RAG) has attracted considerable research attention. However, the absence of a standardized framework for implementation, coupled with the inherently complex RAG process, makes it challenging and time-consuming for researchers to compare and evaluate these approaches in a consistent environment. In response to this challenge, we develop FlashRAG, an efficient and modular open-source toolkit designed to assist researchers in reproducing and comparing existing RAG methods and developing their own algorithms within a unified framework. Our toolkit has implemented 16 advanced RAG methods and gathered and organized 38 benchmark datasets. It has various features, including a customizable modular framework, a rich collection of pre-implemented RAG works, comprehensive datasets, efficient auxiliary pre-processing scripts, and extensive and standard evaluation metrics. Our toolkit and resources are available at https://github.com/RUC-NLPIR/FlashRAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/daebec92963ab8dea492f0c209bdf57e87bcaa07.pdf",
        "venue": "The Web Conference",
        "citationCount": 0,
        "score": 0
    },
    "9af8bccf3e42996cbb198a6ceccafa2a084689f6.pdf": {
        "title": "HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction",
        "authors": [
            "Bhaskarjit Sarmah",
            "Dhagash Mehta",
            "Benika Hall",
            "Rohan Rao",
            "Sunil Patel",
            "Stefano Pasquali"
        ],
        "published_date": "2024",
        "abstract": "Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q&A format, and hence provide a natural set of pairs of ground-truth Q&As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/9af8bccf3e42996cbb198a6ceccafa2a084689f6.pdf",
        "venue": "International Conference on AI in Finance",
        "citationCount": 0,
        "score": 0
    },
    "2986b2b06173e065c94bae49c7a9a3718dad486c.pdf": {
        "title": "Reducing hallucination in structured outputs via Retrieval-Augmented Generation",
        "authors": [
            "Patrice B'echard",
            "Orlando Marquez Ayala"
        ],
        "published_date": "2024",
        "abstract": "A common and fundamental limitation of Generative AI (GenAI) is its propensity to hallucinate. While large language models (LLM) have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging Retrieval Augmented Generation (RAG) to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucinations in the output and improves the generalization of our LLM in out-of-domain settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/2986b2b06173e065c94bae49c7a9a3718dad486c.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "9a946c503b6e799b3d57375b6edfaf4e24febcea.pdf": {
        "title": "Searching for Best Practices in Retrieval-Augmented Generation",
        "authors": [
            "Xiaohua Wang",
            "Zhenghua Wang",
            "Xuan Gao",
            "Feiran Zhang",
            "Yixin Wu",
            "Zhibo Xu",
            "Tianyuan Shi",
            "Zhengyuan Wang",
            "Shizheng Li",
            "Qi Qian",
            "Ruicheng Yin",
            "Changze Lv",
            "Xiaoqing Zheng",
            "Xuanjing Huang"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a \u201cretrieval as generation\u201d strategy.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/9a946c503b6e799b3d57375b6edfaf4e24febcea.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "4308208fac24626e0c927ee728038aadc4e87266.pdf": {
        "title": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models",
        "authors": [
            "Bernal Jimenez Gutierrez",
            "Yiheng Shu",
            "Yu Gu",
            "Michihiro Yasunaga",
            "Yu Su"
        ],
        "published_date": "2024",
        "abstract": "In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting. Despite the impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training. In this work, we introduce HippoRAG, a novel retrieval framework inspired by the hippocampal indexing theory of human long-term memory to enable deeper and more efficient knowledge integration over new experiences. HippoRAG synergistically orchestrates LLMs, knowledge graphs, and the Personalized PageRank algorithm to mimic the different roles of neocortex and hippocampus in human memory. We compare HippoRAG with existing RAG methods on multi-hop question answering and show that our method outperforms the state-of-the-art methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves comparable or better performance than iterative retrieval like IRCoT while being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into IRCoT brings further substantial gains. Finally, we show that our method can tackle new types of scenarios that are out of reach of existing methods. Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/4308208fac24626e0c927ee728038aadc4e87266.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "1ea143c34b9bc359780f79ba4d68dee68bcc1129.pdf": {
        "title": "LightRAG: Simple and Fast Retrieval-Augmented Generation",
        "authors": [
            "Zirui Guo",
            "Lianghao Xia",
            "Yanhua Yu",
            "Tu Ao",
            "Chao Huang"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs. However, existing RAG systems have significant limitations, including reliance on flat data representations and inadequate contextual awareness, which can lead to fragmented answers that fail to capture complex inter-dependencies. To address these challenges, we propose LightRAG, which incorporates graph structures into text indexing and retrieval processes. This innovative framework employs a dual-level retrieval system that enhances comprehensive information retrieval from both low-level and high-level knowledge discovery. Additionally, the integration of graph structures with vector representations facilitates efficient retrieval of related entities and their relationships, significantly improving response times while maintaining contextual relevance. This capability is further enhanced by an incremental update algorithm that ensures the timely integration of new data, allowing the system to remain effective and responsive in rapidly changing data environments. Extensive experimental validation demonstrates considerable improvements in retrieval accuracy and efficiency compared to existing approaches. We have made our LightRAG open-source and available at the link: https://github.com/HKUDS/LightRAG",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/1ea143c34b9bc359780f79ba4d68dee68bcc1129.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "339d2a56f0e5176b691c358a86891e2923045c8c.pdf": {
        "title": "Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely",
        "authors": [
            "Siyun Zhao",
            "Yuqing Yang",
            "Zilong Wang",
            "Zhiyuan He",
            "Luna K. Qiu",
            "Lili Qiu"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) augmented with external data have demonstrated remarkable capabilities in completing real-world tasks. Techniques for integrating external data into LLMs, such as Retrieval-Augmented Generation (RAG) and fine-tuning, are gaining increasing attention and widespread application. Nonetheless, the effective deployment of data-augmented LLMs across various specialized fields presents substantial challenges. These challenges encompass a wide range of issues, from retrieving relevant data and accurately interpreting user intent to fully harnessing the reasoning capabilities of LLMs for complex tasks. We believe that there is no one-size-fits-all solution for data-augmented LLM applications. In practice, underperformance often arises from a failure to correctly identify the core focus of a task or because the task inherently requires a blend of multiple capabilities that must be disentangled for better resolution. In this survey, we propose a RAG task categorization method, classifying user queries into four levels based on the type of external data required and primary focus of the task: explicit fact queries, implicit fact queries, interpretable rationale queries, and hidden rationale queries. We define these levels of queries, provide relevant datasets, and summarize the key challenges and most effective techniques for addressing these challenges. Finally, we discuss three main forms of integrating external data into LLMs: context, small model, and fine-tuning, highlighting their respective strengths, limitations, and the types of problems they are suited to solve. This work aims to help readers thoroughly understand and decompose the data requirements and key bottlenecks in building LLM applications, offering solutions to the different challenges and serving as a guide to systematically developing such applications.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/339d2a56f0e5176b691c358a86891e2923045c8c.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "94034fd2ed4b6cf41113abb7adc9ae469313c958.pdf": {
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
            "Yizheng Huang",
            "Jimmy X. Huang"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/94034fd2ed4b6cf41113abb7adc9ae469313c958.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "d4a5c2ab2b459426869e1a3ab1550897b005303e.pdf": {
        "title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
        "authors": [
            "Shangyu Wu",
            "Ying Xiong",
            "Yufei Cui",
            "Haolun Wu",
            "Can Chen",
            "Ye Yuan",
            "Lianming Huang",
            "Xue Liu",
            "Tei-Wei Kuo",
            "Nan Guan",
            "C. Xue"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have demonstrated great success in various fields, benefiting from their huge amount of parameters that store knowledge. However, LLMs still suffer from several key issues, such as hallucination problems, knowledge update issues, and lacking domain-specific expertise. The appearance of retrieval-augmented generation (RAG), which leverages an external knowledge database to augment LLMs, makes up those drawbacks of LLMs. This paper reviews all significant techniques of RAG, especially in the retriever and the retrieval fusions. Besides, tutorial codes are provided for implementing the representative techniques in RAG. This paper further discusses the RAG update, including RAG with/without knowledge update. Then, we introduce RAG evaluation and benchmarking, as well as the application of RAG in representative NLP tasks and industrial scenarios. Finally, this paper discusses RAG's future directions and challenges for promoting this field's development.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/d4a5c2ab2b459426869e1a3ab1550897b005303e.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "e2050c0aa8aa27235c0708b5a5ff741dfd11e2a9.pdf": {
        "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
        "authors": [
            "Yuanjie Lyu",
            "Zhiyu Li",
            "Simin Niu",
            "Feiyu Xiong",
            "Bo Tang",
            "Wenjin Wang",
            "Hao Wu",
            "Huan Liu",
            "Tong Xu",
            "Enhong Chen"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate \u201challucinated\u201d content. However, evaluating RAG systems is a challenge. Most benchmarks focus primarily on question-answering applications, neglecting other potential scenarios where RAG could be beneficial. Accordingly, in the experiments, these benchmarks often assess only the LLM components of the RAG pipeline or the retriever in knowledge-intensive scenarios, overlooking the impact of external knowledge base construction and the retrieval component on the entire RAG pipeline in non-knowledge-intensive scenarios. To address these issues, this article constructs a large-scale and more comprehensive benchmark and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we refer to the CRUD actions that describe interactions between users and knowledge bases and also categorize the range of RAG applications into four distinct types\u2014create, read, update, and delete (CRUD). \u201cCreate\u201d refers to scenarios requiring the generation of original, varied content. \u201cRead\u201d involves responding to intricate questions in knowledge-intensive situations. \u201cUpdate\u201d focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. \u201cDelete\u201d pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed different datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, context length, knowledge base construction, and LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios. The source code is available at GitHub: https://github.com/IAAR-Shanghai/CRUD_RAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/e2050c0aa8aa27235c0708b5a5ff741dfd11e2a9.pdf",
        "venue": "ACM Trans. Inf. Syst.",
        "citationCount": 0,
        "score": 0
    },
    "a2a4ddbed34916cfa345e957cf060da99685e37b.pdf": {
        "title": "Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning",
        "authors": [
            "Gelei Deng",
            "Yi Liu",
            "Kailong Wang",
            "Yuekang Li",
            "Tianwei Zhang",
            "Yang Liu"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models~(LLMs) have gained immense popularity and are being increasingly applied in various domains. Consequently, ensuring the security of these models is of paramount importance. Jailbreak attacks, which manipulate LLMs to generate malicious content, are recognized as a significant vulnerability. While existing research has predominantly focused on direct jailbreak attacks on LLMs, there has been limited exploration of indirect methods. The integration of various plugins into LLMs, notably Retrieval Augmented Generation~(RAG), which enables LLMs to incorporate external knowledge bases into their response generation such as GPTs, introduces new avenues for indirect jailbreak attacks. To fill this gap, we investigate indirect jailbreak attacks on LLMs, particularly GPTs, introducing a novel attack vector named Retrieval Augmented Generation Poisoning. This method, Pandora, exploits the synergy between LLMs and RAG through prompt manipulation to generate unexpected responses. Pandora uses maliciously crafted content to influence the RAG process, effectively initiating jailbreak attacks. Our preliminary tests show that Pandora successfully conducts jailbreak attacks in four different scenarios, achieving higher success rates than direct attacks, with 64.3\\% for GPT-3.5 and 34.8\\% for GPT-4.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/a2a4ddbed34916cfa345e957cf060da99685e37b.pdf",
        "venue": "Proceedings 2024 Workshop on AI Systems with Confidential COmputing",
        "citationCount": 0,
        "score": 0
    },
    "9111d6632e3ad648e65c57c52fd945641ccbdac2.pdf": {
        "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
        "authors": [
            "Heydar Soudani",
            "E. Kanoulas",
            "Faegheh Hasibi"
        ],
        "published_date": "2024",
        "abstract": "Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LMs in handling low-frequency entities on question answering tasks. We conduct extensive experiments on twelve LMs of varying size and type and different FT methods, data augmentation, and retrieval models. Our findings indicate that while FT boosts the performance across entities of varying popularity, RAG surpasses FT by a large margin particularly for least popular factual knowledge. Additionally, the success of both RAG and FT approaches is amplified by improving retrieval and data augmentation techniques. Fine tuning, while beneficial for small LMs, requires extensive resources. To address this issue, we propose the new Stimulus RAG approach that surpasses the effectiveness of fine tuning based approaches, thereby eliminating the need for the costly data augmentation and fine tuning step for enriching LMs with less popular factual knowledge.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/9111d6632e3ad648e65c57c52fd945641ccbdac2.pdf",
        "venue": "SIGIR-AP",
        "citationCount": 0,
        "score": 0
    },
    "46ff7e02fd4ff5fdfb9f85bc7071725b8089061f.pdf": {
        "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
        "authors": [
            "Satyapriya Krishna",
            "Kalpesh Krishna",
            "Anhad Mohananey",
            "Steven Schwarcz",
            "Adam Stambler",
            "Shyam Upadhyay",
            "Manaal Faruqui"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have demonstrated significant performance improvements across various cognitive tasks. An emerging application is using LLMs to enhance retrieval-augmented generation (RAG) capabilities. These systems require LLMs to understand user queries, retrieve relevant information, and synthesize coherent and accurate responses. Given the increasing real-world deployment of such systems, comprehensive evaluation becomes crucial. To this end, we propose FRAMES (Factuality, Retrieval, And reasoning MEasurement Set), a high-quality evaluation dataset designed to test LLMs' ability to provide factual responses, assess retrieval capabilities, and evaluate the reasoning required to generate final answers. While previous work has provided datasets and benchmarks to evaluate these abilities in isolation, FRAMES offers a unified framework that provides a clearer picture of LLM performance in end-to-end RAG scenarios. Our dataset comprises challenging multi-hop questions that require the integration of information from multiple sources. We present baseline results demonstrating that even state-of-the-art LLMs struggle with this task, achieving 0.40 accuracy with no retrieval. The accuracy is significantly improved with our proposed multi-step retrieval pipeline, achieving an accuracy of 0.66 (>50% improvement). We hope our work will help bridge evaluation gaps and assist in developing more robust and capable RAG systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/46ff7e02fd4ff5fdfb9f85bc7071725b8089061f.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "273c145ea080f277839b89628c255017fc0e1e7c.pdf": {
        "title": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey",
        "authors": [
            "Yujia Zhou",
            "Yan Liu",
            "Xiaoxi Li",
            "Jiajie Jin",
            "Hongjin Qian",
            "Zheng Liu",
            "Chaozhuo Li",
            "Zhicheng Dou",
            "Tsung-Yi Ho",
            "Philip S. Yu"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal paradigm in the development of Large Language Models (LLMs). While much of the current research in this field focuses on performance optimization, particularly in terms of accuracy and efficiency, the trustworthiness of RAG systems remains an area still under exploration. From a positive perspective, RAG systems are promising to enhance LLMs by providing them with useful and up-to-date knowledge from vast external databases, thereby mitigating the long-standing problem of hallucination. While from a negative perspective, RAG systems are at the risk of generating undesirable contents if the retrieved information is either inappropriate or poorly utilized. To address these concerns, we propose a unified framework that assesses the trustworthiness of RAG systems across six key dimensions: factuality, robustness, fairness, transparency, accountability, and privacy. Within this framework, we thoroughly review the existing literature on each dimension. Additionally, we create the evaluation benchmark regarding the six dimensions and conduct comprehensive evaluations for a variety of proprietary and open-source models. Finally, we identify the potential challenges for future research based on our investigation results. Through this work, we aim to lay a structured foundation for future investigations and provide practical insights for enhancing the trustworthiness of RAG systems in real-world applications.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/273c145ea080f277839b89628c255017fc0e1e7c.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "3daedc1e0a9db8c4dda7e06724b0b556f64c0752.pdf": {
        "title": "LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the Legal Domain",
        "authors": [
            "Nicholas Pipitone",
            "Ghita Houir Alami"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) systems are showing promising potential, and are becoming increasingly relevant in AI-powered legal applications. Existing benchmarks, such as LegalBench, assess the generative capabilities of Large Language Models (LLMs) in the legal domain, but there is a critical gap in evaluating the retrieval component of RAG systems. To address this, we introduce LegalBench-RAG, the first benchmark specifically designed to evaluate the retrieval step of RAG pipelines within the legal space. LegalBench-RAG emphasizes precise retrieval by focusing on extracting minimal, highly relevant text segments from legal documents. These highly relevant snippets are preferred over retrieving document IDs, or large sequences of imprecise chunks, both of which can exceed context window limitations. Long context windows cost more to process, induce higher latency, and lead LLMs to forget or hallucinate information. Additionally, precise results allow LLMs to generate citations for the end user. The LegalBench-RAG benchmark is constructed by retracing the context used in LegalBench queries back to their original locations within the legal corpus, resulting in a dataset of 6,858 query-answer pairs over a corpus of over 79M characters, entirely human-annotated by legal experts. We also introduce LegalBench-RAG-mini, a lightweight version for rapid iteration and experimentation. By providing a dedicated benchmark for legal retrieval, LegalBench-RAG serves as a critical tool for companies and researchers focused on enhancing the accuracy and performance of RAG systems in the legal domain. The LegalBench-RAG dataset is publicly available at https://github.com/zeroentropy-cc/legalbenchrag.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/3daedc1e0a9db8c4dda7e06724b0b556f64c0752.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "7326329c09c11aac423ef4910222a16952bb01dc.pdf": {
        "title": "RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation",
        "authors": [
            "Chao Jin",
            "Zili Zhang",
            "Xuanlin Jiang",
            "Fangyue Liu",
            "Xin Liu",
            "Xuanzhe Liu",
            "Xin Jin"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) has demonstrated substantial advancements in various natural language processing tasks by integrating the strengths of large language models (LLMs) and external knowledge databases. However, the retrieval step introduces long sequence generation and extra data dependency, resulting in long end-to-end latency.\n \n Our analysis benchmarks current RAG systems and reveals that, while the retrieval step poses performance challenges, it also offers optimization opportunities through its retrieval pattern and streaming search behavior. We propose RAGCache, a latency-optimized serving system tailored for RAG. RAGCache leverages the retrieval pattern to organize and cache the intermediate states of retrieved knowledge in a\n knowledge tree\n across the GPU and host memory hierarchy, reducing LLM generation time. RAGCache employs\n dynamic speculative pipelining\n to exploit the streaming search behavior, overlapping retrieval with LLM generation to minimize end-to-end latency. We implement RAGCache based on vLLM and Faiss, and evaluate it on both open-source and production datasets. Experimental results demonstrate that RAGCache reduces the time to first token (TTFT) by up to 4 \u00d7 and improves the throughput by up to 2.1 \u00d7 compared to vLLM integrated with Faiss.\n",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/7326329c09c11aac423ef4910222a16952bb01dc.pdf",
        "venue": "ACM Transactions on Computer Systems",
        "citationCount": 0,
        "score": 0
    },
    "160924af0791331ec8fa5a3d526ea125355f3b8b.pdf": {
        "title": "Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting",
        "authors": [
            "Zilong Wang",
            "Zifeng Wang",
            "Long T. Le",
            "Huaixiu Steven Zheng",
            "Swaroop Mishra",
            "Vincent Perot",
            "Yuwei Zhang",
            "Anush Mattapalli",
            "Ankur Taly",
            "Jingbo Shang",
            "Chen-Yu Lee",
            "Tomas Pfister"
        ],
        "published_date": "2024",
        "abstract": "Retrieval augmented generation (RAG) combines the generative abilities of large language models (LLMs) with external knowledge sources to provide more accurate and up-to-date responses. Recent RAG advancements focus on improving retrieval outcomes through iterative LLM refinement or self-critique capabilities acquired through additional instruction tuning of LLMs. In this work, we introduce Speculative RAG - a framework that leverages a larger generalist LM to efficiently verify multiple RAG drafts produced in parallel by a smaller, distilled specialist LM. Each draft is generated from a distinct subset of retrieved documents, offering diverse perspectives on the evidence while reducing input token counts per draft. This approach enhances comprehension of each subset and mitigates potential position bias over long context. Our method accelerates RAG by delegating drafting to the smaller specialist LM, with the larger generalist LM performing a single verification pass over the drafts. Extensive experiments demonstrate that Speculative RAG achieves state-of-the-art performance with reduced latency on TriviaQA, MuSiQue, PopQA, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy by up to 12.97% while reducing latency by 50.83% compared to conventional RAG systems on PubHealth.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/160924af0791331ec8fa5a3d526ea125355f3b8b.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "2bb9a87bdfc8a35bc1813e5a88180f43615785a8.pdf": {
        "title": "Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up Questions",
        "authors": [
            "Guangzhi Xiong",
            "Qiao Jin",
            "Xiao Wang",
            "Minjia Zhang",
            "Zhiyong Lu",
            "Aidong Zhang"
        ],
        "published_date": "2024",
        "abstract": "The emergent abilities of large language models (LLMs) have demonstrated great potential in solving medical questions. They can possess considerable medical knowledge, but may still hallucinate and are inflexible in the knowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed to enhance the medical question-answering capabilities of LLMs with external knowledge bases, it may still fail in complex cases where multiple rounds of information-seeking are required. To address such an issue, we propose iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up queries based on previous information-seeking attempts. In each iteration of i-MedRAG, the follow-up queries will be answered by a vanilla RAG system and they will be further used to guide the query generation in the next iteration. Our experiments show the improved performance of various LLMs brought by i-MedRAG compared with vanilla RAG on complex questions from clinical vignettes in the United States Medical Licensing Examination (USMLE), as well as various knowledge tests in the Massive Multitask Language Understanding (MMLU) dataset. Notably, our zero-shot i-MedRAG outperforms all existing prompt engineering and fine-tuning methods on GPT-3.5, achieving an accuracy of 69.68% on the MedQA dataset. In addition, we characterize the scaling properties of i-MedRAG with different iterations of follow-up queries and different numbers of queries per iteration. Our case studies show that i-MedRAG can flexibly ask follow-up queries to form reasoning chains, providing an in-depth analysis of medical questions. To the best of our knowledge, this is the first-of-its-kind study on incorporating follow-up queries into medical RAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/2bb9a87bdfc8a35bc1813e5a88180f43615785a8.pdf",
        "venue": "Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing",
        "citationCount": 0,
        "score": 0
    },
    "addd475c96056491539b790c1b264d0855c80fb7.pdf": {
        "title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training",
        "authors": [
            "Feiteng Fang",
            "Yuelin Bai",
            "Shiwen Ni",
            "Min Yang",
            "Xiaojun Chen",
            "Ruifeng Xu"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/addd475c96056491539b790c1b264d0855c80fb7.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "a82a1be7639301ea47ebcb346f6119065b68b3d0.pdf": {
        "title": "GRAG: Graph Retrieval-Augmented Generation",
        "authors": [
            "Yuntong Hu",
            "Zhihan Lei",
            "Zhengwu Zhang",
            "Bo Pan",
            "Chen Ling",
            "Liang Zhao"
        ],
        "published_date": "2024",
        "abstract": "Naive Retrieval-Augmented Generation (RAG) focuses on individual documents during retrieval and, as a result, falls short in handling networked documents which are very popular in many applications such as citation graphs, social media, and knowledge graphs. To overcome this limitation, we introduce Graph Retrieval-Augmented Generation (GRAG), which tackles the fundamental challenges in retrieving textual subgraphs and integrating the joint textual and topological information into Large Language Models (LLMs) to enhance its generation. To enable efficient textual subgraph retrieval, we propose a novel divide-and-conquer strategy that retrieves the optimal subgraph structure in linear time. To achieve graph context-aware generation, incorporate textual graphs into LLMs through two complementary views-the text view and the graph view-enabling LLMs to more effectively comprehend and utilize the graph context. Extensive experiments on graph reasoning benchmarks demonstrate that in scenarios requiring multi-hop reasoning on textual graphs, our GRAG approach significantly outperforms current state-of-the-art RAG methods. Our datasets as well as codes of GRAG are available at https://github.com/HuieL/GRAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/a82a1be7639301ea47ebcb346f6119065b68b3d0.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "1027d120189fd6cd9aec1af273cd9a5baaf645d7.pdf": {
        "title": "BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models",
        "authors": [
            "Jiaqi Xue",
            "Meng Zheng",
            "Yebowen Hu",
            "Fei Liu",
            "Xun Chen",
            "Qian Lou"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) are constrained by outdated information and a tendency to generate incorrect data, commonly referred to as\"hallucinations.\"Retrieval-Augmented Generation (RAG) addresses these limitations by combining the strengths of retrieval-based methods and generative models. This approach involves retrieving relevant information from a large, up-to-date dataset and using it to enhance the generation process, leading to more accurate and contextually appropriate responses. Despite its benefits, RAG introduces a new attack surface for LLMs, particularly because RAG databases are often sourced from public data, such as the web. In this paper, we propose \\TrojRAG{} to identify the vulnerabilities and attacks on retrieval parts (RAG database) and their indirect attacks on generative parts (LLMs). Specifically, we identify that poisoning several customized content passages could achieve a retrieval backdoor, where the retrieval works well for clean queries but always returns customized poisoned adversarial queries. Triggers and poisoned passages can be highly customized to implement various attacks. For example, a trigger could be a semantic group like\"The Republican Party, Donald Trump, etc.\"Adversarial passages can be tailored to different contents, not only linked to the triggers but also used to indirectly attack generative LLMs without modifying them. These attacks can include denial-of-service attacks on RAG and semantic steering attacks on LLM generations conditioned by the triggers. Our experiments demonstrate that by just poisoning 10 adversarial passages can induce 98.2\\% success rate to retrieve the adversarial passages. Then, these passages can increase the reject ratio of RAG-based GPT-4 from 0.01\\% to 74.6\\% or increase the rate of negative responses from 0.22\\% to 72\\% for targeted queries.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/1027d120189fd6cd9aec1af273cd9a5baaf645d7.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "848772a50cee68e88988ded7522e280d1c490598.pdf": {
        "title": "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
        "authors": [
            "Minbyul Jeong",
            "Jiwoong Sohn",
            "Mujeen Sung",
            "Jaewoo Kang"
        ],
        "published_date": "2024",
        "abstract": "Abstract Summary Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens. Our work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions. Using three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less. Similarly, Self-BioRAG outperforms RAG by 8% Rouge-1 score in generating more proficient answers on two long-form question-answering benchmarks on average. Overall, we analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does. We release our data and code for training our framework components and model weights (7B and 13B) to enhance capabilities in biomedical and clinical domains. Availability and implementation Self-BioRAG is available at https://github.com/dmis-lab/self-biorag.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/848772a50cee68e88988ded7522e280d1c490598.pdf",
        "venue": "Bioinform.",
        "citationCount": 0,
        "score": 0
    },
    "4dc4644508a7868ad14f6c2c06a34056c6c333f7.pdf": {
        "title": "KRAGEN: a knowledge graph-enhanced RAG framework for biomedical problem solving using large language models",
        "authors": [
            "Nicholas Matsumoto",
            "Jay Moran",
            "Hyunjun Choi",
            "Miguel E. Hernandez",
            "Mythreye Venkatesan",
            "Paul Wang",
            "Jason H. Moore"
        ],
        "published_date": "2024",
        "abstract": "Abstract Motivation Answering and solving complex problems using a large language model (LLM) given a certain domain such as biomedicine is a challenging task that requires both factual consistency and logic, and LLMs often suffer from some major limitations, such as hallucinating false or irrelevant information, or being influenced by noisy data. These issues can compromise the trustworthiness, accuracy, and compliance of LLM-generated text and insights. Results Knowledge Retrieval Augmented Generation ENgine (KRAGEN) is a new tool that combines knowledge graphs, Retrieval Augmented Generation (RAG), and advanced prompting techniques to solve complex problems with natural language. KRAGEN converts knowledge graphs into a vector database and uses RAG to retrieve relevant facts from it. KRAGEN uses advanced prompting techniques: namely graph-of-thoughts (GoT), to dynamically break down a complex problem into smaller subproblems, and proceeds to solve each subproblem by using the relevant knowledge through the RAG framework, which limits the hallucinations, and finally, consolidates the subproblems and provides a solution. KRAGEN\u2019s graph visualization allows the user to interact with and evaluate the quality of the solution\u2019s GoT structure and logic. Availability and implementation KRAGEN is deployed by running its custom Docker containers. KRAGEN is available as open-source from GitHub at: https://github.com/EpistasisLab/KRAGEN.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/4dc4644508a7868ad14f6c2c06a34056c6c333f7.pdf",
        "venue": "Bioinformatics",
        "citationCount": 0,
        "score": 0
    },
    "1b0aba023d7aa5fb9853f9e942efb5c243dc1201.pdf": {
        "title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems",
        "authors": [
            "Robert Friel",
            "Masha Belyi",
            "Atindriyo Sanyal"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) has become a standard architectural pattern for incorporating domain-specific knowledge into user-facing chat applications powered by Large Language Models (LLMs). RAG systems are characterized by (1) a document retriever that queries a domain-specific corpus for context information relevant to an input query, and (2) an LLM that generates a response based on the provided query and context. However, comprehensive evaluation of RAG systems remains a challenge due to the lack of unified evaluation criteria and annotated datasets. In response, we introduce RAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k examples. It covers five unique industry-specific domains and various RAG task types. RAGBench examples are sourced from industry corpora such as user manuals, making it particularly relevant for industry applications. Further, we formalize the TRACe evaluation framework: a set of explainable and actionable RAG evaluation metrics applicable across all RAG domains. We release the labeled dataset at https://huggingface.co/datasets/rungalileo/ragbench. RAGBench explainable labels facilitate holistic evaluation of RAG systems, enabling actionable feedback for continuous improvement of production applications. Thorough extensive benchmarking, we find that LLM-based RAG evaluation methods struggle to compete with a finetuned RoBERTa model on the RAG evaluation task. We identify areas where existing approaches fall short and propose the adoption of RAGBench with TRACe towards advancing the state of RAG evaluation systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/1b0aba023d7aa5fb9853f9e942efb5c243dc1201.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "125a9c020316341bde65ea374f19caf346cfecfa.pdf": {
        "title": "Graph Retrieval-Augmented Generation for Large Language Models: A Survey",
        "authors": [
            "T. Procko",
            "Omar Ochoa"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) demonstrate general knowledge, but they suffer when specifically needed knowledge is not present in their training set. Two approaches to ameliorating this, without retraining, are 1) prompt engineering and 2) Retrieval-Augmented Generation (RAG). RAG is a form of prompt engineering, insofar as relevant lexical snippets retrieved from RAG corpora are vectorized and aggregated with prompts. However, RAG documents are often noisy, i.e., while relevant to a given prompt, they can contain much other information that obfuscates the desired snippet. If the purpose of pretraining a LLM on massive and general corpora is to engender a generally applicable model, RAG is not: it is a means of LLM optimization, and as such, RAG document selection must be precise, not general. For expert tasks, it is imperative that a RAG corpus be as noise-free as possible, in much the same way a good prompt should be free of irrelevant text. Knowledge Graphs (KGs) provide a concise means of representing domain knowledge free of noisy information. This paper surveys work incorporating KGs with LLM RAG, intending to equip scientists with a better understanding of this novel research area for future work.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/125a9c020316341bde65ea374f19caf346cfecfa.pdf",
        "venue": "2024 Conference on AI, Science, Engineering, and Technology (AIxSET)",
        "citationCount": 0,
        "score": 0
    },
    "810b3f4475f22f6ca0f1bded3b8523f3cdebee8d.pdf": {
        "title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems",
        "authors": [
            "Hongru Wang",
            "Wenyu Huang",
            "Yang Deng",
            "Rui Wang",
            "Zezhong Wang",
            "Yufei Wang",
            "Fei Mi",
            "Jeff Z. Pan",
            "Kam-Fai Wong"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their behavior to diverse task requirements. Meanwhile, evaluation tokens gauge the relevance score between the dialogue context and the retrieved evidence. In addition, we carefully design a self-refinement mechanism to iteratively refine the generated response considering 1) the consistency scores between the generated response and retrieved evidence; and 2) the relevance scores. Experiments on two personalized datasets (DuLeMon and KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge source selection and response generation task with itself as a retriever in a unified manner. Extensive analyses and discussions are provided for shedding some new perspectives for personalized dialogue systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/810b3f4475f22f6ca0f1bded3b8523f3cdebee8d.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "908d45b0d2b88ba72ee501c368eb618d29d61ce0.pdf": {
        "title": "A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models",
        "authors": [
            "Qinggang Zhang",
            "Shengyuan Chen",
            "Yuan-Qi Bei",
            "Zheng Yuan",
            "Huachi Zhou",
            "Zijin Hong",
            "Junnan Dong",
            "Hao Chen",
            "Yi Chang",
            "Xiao Huang"
        ],
        "published_date": "2025",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks, yet their application to specialized domains remains challenging due to the need for deep expertise. Retrieval-Augmented generation (RAG) has emerged as a promising solution to customize LLMs for professional fields by seamlessly integrating external knowledge bases, enabling real-time access to domain-specific expertise during inference. Despite its potential, traditional RAG systems, based on flat text retrieval, face three critical challenges: (i) complex query understanding in professional contexts, (ii) difficulties in knowledge integration across distributed sources, and (iii) system efficiency bottlenecks at scale. This survey presents a systematic analysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new paradigm that revolutionizes domain-specific LLM applications. GraphRAG addresses traditional RAG limitations through three key innovations: (i) graph-structured knowledge representation that explicitly captures entity relationships and domain hierarchies, (ii) efficient graph-based retrieval techniques that enable context-preserving knowledge retrieval with multihop reasoning ability, and (iii) structure-aware knowledge integration algorithms that leverage retrieved knowledge for accurate and logical coherent generation of LLMs. In this survey, we systematically analyze the technical foundations of GraphRAG and examine current implementations across various professional domains, identifying key technical challenges and promising research directions. All the related resources of GraphRAG, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphRAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/908d45b0d2b88ba72ee501c368eb618d29d61ce0.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "29528d8cb030a65f62a35b1237f1f5483077ad0a.pdf": {
        "title": "Inference Scaling for Long-Context Retrieval Augmented Generation",
        "authors": [
            "Zhenrui Yue",
            "Honglei Zhuang",
            "Aijun Bai",
            "Kai Hui",
            "R. Jagerman",
            "Hansi Zeng",
            "Zhen Qin",
            "Dong Wang",
            "Xuanhui Wang",
            "Michael Bendersky"
        ],
        "published_date": "2024",
        "abstract": "The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/29528d8cb030a65f62a35b1237f1f5483077ad0a.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "858cbd99d5a3d2658254d055cd26e06f81050927.pdf": {
        "title": "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design",
        "authors": [
            "Wenqi Jiang",
            "Shuai Zhang",
            "Boran Han",
            "Jie Wang",
            "Bernie Wang",
            "Tim Kraska"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) can enhance the generation quality of large language models (LLMs) by incorporating external token databases. However, retrievals from large databases can constitute a substantial portion of the overall generation time, particularly when retrievals are periodically performed to align the retrieved content with the latest states of generation. In this paper, we introduce PipeRAG, a novel algorithm-system co-design approach to reduce generation latency and enhance generation quality. PipeRAG integrates (1) pipeline parallelism to enable concurrent retrieval and generation processes, (2) flexible retrieval intervals to maximize the efficiency of pipeline parallelism, and (3) a performance model to automatically balance retrieval quality and latency based on the generation states and underlying hardware. Our evaluation shows that, by combining the three aforementioned methods, PipeRAG achieves up to 2.6$\\times$ speedup in end-to-end generation latency while improving generation quality. These promising results showcase the effectiveness of co-designing algorithms with underlying systems, paving the way for the adoption of PipeRAG in future RAG systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/858cbd99d5a3d2658254d055cd26e06f81050927.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "bbf77bd463768a5322a63ffc19322d5c764493e0.pdf": {
        "title": "Development of a liver disease\u2013specific large language model chat interface using retrieval-augmented generation",
        "authors": [
            "J. Ge",
            "Steve Sun",
            "Joseph Owens",
            "Victor Galvez",
            "Oksana Gologorskaya",
            "Jennifer C Lai",
            "Mark J. Pletcher",
            "Ki Lai"
        ],
        "published_date": "2024",
        "abstract": "Background and Aims: Large language models (LLMs) have significant capabilities in clinical information processing tasks. Commercially available LLMs, however, are not optimized for clinical uses and are prone to generating hallucinatory information. Retrieval-augmented generation (RAG) is an enterprise architecture that allows the embedding of customized data into LLMs. This approach \u201cspecializes\u201d the LLMs and is thought to reduce hallucinations. Approach and Results We developed \u201cLiVersa,\u201d a liver disease\u2013specific LLM, by using our institution\u2019s protected health information-complaint text embedding and LLM platform, \u201cVersa.\u201d We conducted RAG on 30 publicly available American Association for the Study of Liver Diseases guidance documents to be incorporated into LiVersa. We evaluated LiVersa\u2019s performance by conducting 2 rounds of testing. First, we compared LiVersa\u2019s outputs versus those of trainees from a previously published knowledge assessment. LiVersa answered all 10 questions correctly. Second, we asked 15 hepatologists to evaluate the outputs of 10 hepatology topic questions generated by LiVersa, OpenAI\u2019s ChatGPT 4, and Meta\u2019s Large Language Model Meta AI 2. LiVersa\u2019s outputs were more accurate but were rated less comprehensive and safe compared to those of ChatGPT 4. Results: We evaluated LiVersa\u2019s performance by conducting 2 rounds of testing. First, we compared LiVersa\u2019s outputs versus those of trainees from a previously published knowledge assessment. LiVersa answered all 10 questions correctly. Second, we asked 15 hepatologists to evaluate the outputs of 10 hepatology topic questions generated by LiVersa, OpenAI\u2019s ChatGPT 4, and Meta\u2019s Large Language Model Meta AI 2. LiVersa\u2019s outputs were more accurate but were rated less comprehensive and safe compared to those of ChatGPT 4. Conclusions: In this demonstration, we built disease-specific and protected health information-compliant LLMs using RAG. While LiVersa demonstrated higher accuracy in answering questions related to hepatology, there were some deficiencies due to limitations set by the number of documents used for RAG. LiVersa will likely require further refinement before potential live deployment. The LiVersa prototype, however, is a proof of concept for utilizing RAG to customize LLMs for clinical use cases.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/bbf77bd463768a5322a63ffc19322d5c764493e0.pdf",
        "venue": "Hepatology",
        "citationCount": 0,
        "score": 0
    },
    "a681b1085c088c51347cdb9358dd344081d29c99.pdf": {
        "title": "Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation",
        "authors": [
            "Shengjie Ma",
            "Chengjin Xu",
            "Xuhui Jiang",
            "Muzhi Li",
            "Huaren Qu",
            "Cehao Yang",
            "Jiaxin Mao",
            "Jian Guo"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) has improved large language models (LLMs) by using knowledge retrieval to overcome knowledge deficiencies. However, current RAG methods often fall short of ensuring the depth and completeness of retrieved information, which is necessary for complex reasoning tasks. In this work, we introduce Think-on-Graph 2.0 (ToG-2), a hybrid RAG framework that iteratively retrieves information from both unstructured and structured knowledge sources in a tight-coupling manner. Specifically, ToG-2 leverages knowledge graphs (KGs) to link documents via entities, facilitating deep and knowledge-guided context retrieval. Simultaneously, it utilizes documents as entity contexts to achieve precise and efficient graph retrieval. ToG-2 alternates between graph retrieval and context retrieval to search for in-depth clues relevant to the question, enabling LLMs to generate answers. We conduct a series of well-designed experiments to highlight the following advantages of ToG-2: 1) ToG-2 tightly couples the processes of context retrieval and graph retrieval, deepening context retrieval via the KG while enabling reliable graph retrieval based on contexts; 2) it achieves deep and faithful reasoning in LLMs through an iterative knowledge retrieval process of collaboration between contexts and the KG; and 3) ToG-2 is training-free and plug-and-play compatible with various LLMs. Extensive experiments demonstrate that ToG-2 achieves overall state-of-the-art (SOTA) performance on 6 out of 7 knowledge-intensive datasets with GPT-3.5, and can elevate the performance of smaller models (e.g., LLAMA-2-13B) to the level of GPT-3.5's direct reasoning. The source code is available on https://github.com/IDEA-FinAI/ToG-2.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/a681b1085c088c51347cdb9358dd344081d29c99.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "aa32cce28aad1d04fea026860c3e2d4a218d9a57.pdf": {
        "title": "Telco-RAG: Navigating the Challenges of Retrieval Augmented Language Models for Telecommunications",
        "authors": [
            "Andrei-Laurentiu Bornea",
            "Fadhel Ayed",
            "Antonio De Domenico",
            "Nicola Piovesan",
            "Ali Maatouk"
        ],
        "published_date": "2024",
        "abstract": "The application of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems in the telecommunication domain presents unique challenges, primarily due to the complex nature of telecom standard documents and the rapid evolution of the field. The paper introduces Telco-RAG, 1 an open-source RAG framework designed to handle the specific needs of telecommunications standards, particularly 3rd Generation Partnership Project (3GPP) documents. Telco-RAG addresses the critical challenges of implementing a RAG pipeline on highly technical content, paving the way for applying LLMs in telecommunications and offering guidelines for RAG implementation in other technical domains.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/aa32cce28aad1d04fea026860c3e2d4a218d9a57.pdf",
        "venue": "Global Communications Conference",
        "citationCount": 0,
        "score": 0
    },
    "f091acf9dc2cc486c253dcead3a74ba916c64eb7.pdf": {
        "title": "IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning Inner Monologues",
        "authors": [
            "Diji Yang",
            "Jinmeng Rao",
            "Kezhen Chen",
            "Xiaoyuan Guo",
            "Yawen Zhang",
            "Jie Yang",
            "Yi Zhang"
        ],
        "published_date": "2024",
        "abstract": "Although the Retrieval-Augmented Generation (RAG) paradigms can use external knowledge to enhance and ground the outputs of Large Language Models (LLMs) to mitigate generative hallucinations and static knowledge base problems, they still suffer from limited flexibility in adopting Information Retrieval (IR) systems with varying capabilities, constrained interpretability during the multi-round retrieval process, and a lack of end-to-end optimization. To address these challenges, we propose a novel LLM-centric approach, IM-RAG, that integrates IR systems with LLMs to support multi-round RAG through learning Inner Monologues (IM, i.e., the human inner voice that narrates one's thoughts). During the IM process, the LLM serves as the core reasoning model (i.e., Reasoner ) to either propose queries to collect more information via the Retriever or to provide a final answer based on the conversational context. We also introduce a Refiner that improves the outputs from the Retriever, effectively bridging the gap between the Reasoner and IR modules with varying capabilities and fostering multi-round communications. The entire IM process is optimized via Reinforcement Learning (RL) where a Progress Tracker is incorporated to provide mid-step rewards, and the answer prediction is further separately optimized via Supervised Fine-Tuning (SFT). We conduct extensive experiments with the HotPotQA dataset, a popular benchmark for retrieval-based, multi-step question-answering. The results show that our approach achieves state-of-the-art (SOTA) performance while providing high flexibility in integrating IR modules as well as strong interpretability exhibited in the learned inner monologue.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/f091acf9dc2cc486c253dcead3a74ba916c64eb7.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 0,
        "score": 0
    },
    "63a1617af179ee8b5b096b3038913a19166168d4.pdf": {
        "title": "Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models",
        "authors": [
            "Shayekh Bin Islam",
            "Md Asib Rahman",
            "K. S. M. T. Hossain",
            "Enamul Hoque",
            "Shafiq R. Joty",
            "Md. Rizwan Parvez"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we introduce a novel framework, Open-RAG, designed to enhance reasoning capabilities in RAG with open-source LLMs. Our framework transforms an arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE) model capable of handling complex reasoning tasks, including both single- and multi-hop queries. Open-RAG uniquely trains the model to navigate challenging distractors that appear relevant but are misleading. As a result, Open-RAG leverages latent learning, dynamically selecting relevant experts and integrating external knowledge effectively for more accurate and contextually relevant responses. In addition, we propose a hybrid adaptive retrieval method to determine retrieval necessity and balance the trade-off between performance gain and inference speed. Experimental results show that the Llama2-7B-based Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT, Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source our code and models at https://openragmoe.github.io/",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/63a1617af179ee8b5b096b3038913a19166168d4.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "83939671534dc3d374c9bc4e3e03b5ec2c7ba301.pdf": {
        "title": "Improving large language model applications in biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and clinical development guidelines",
        "authors": [
            "Siru Liu",
            "Allison B. McCoy",
            "Adam Wright"
        ],
        "published_date": "2025",
        "abstract": "Abstract Objective The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large language models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness. Materials and Methods We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, PsycINFO) using terms related to \u201cretrieval augmented generation\u201d and \u201clarge language model,\u201d for articles published in 2023 and 2024. We selected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using odds ratio as the effect size. Results Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of 1.19-1.53, indicating a statistically significant effect (P\u2009=\u2009.001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as well as evaluation methods. Discussion Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Applications with RAG in Clinical Settings to inform clinical applications using RAG. Conclusion Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research should focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of knowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/83939671534dc3d374c9bc4e3e03b5ec2c7ba301.pdf",
        "venue": "J. Am. Medical Informatics Assoc.",
        "citationCount": 0,
        "score": 0
    },
    "7423e5c903fb2befaf471cae64e2530f7c1d0404.pdf": {
        "title": "Development and Testing of Retrieval Augmented Generation in Large Language Models - A Case Study Report",
        "authors": [
            "Yuhe Ke",
            "Liyuan Jin",
            "Kabilan Elangovan",
            "H. Abdullah",
            "Nan Liu",
            "Alex Tiong Heng Sia",
            "Chai Rick Soh",
            "Joshua Yi Min Tung",
            "J. Ong",
            "D. Ting"
        ],
        "published_date": "2024",
        "abstract": "Purpose: Large Language Models (LLMs) hold significant promise for medical applications. Retrieval Augmented Generation (RAG) emerges as a promising approach for customizing domain knowledge in LLMs. This case study presents the development and evaluation of an LLM-RAG pipeline tailored for healthcare, focusing specifically on preoperative medicine. Methods: We developed an LLM-RAG model using 35 preoperative guidelines and tested it against human-generated responses, with a total of 1260 responses evaluated. The RAG process involved converting clinical documents into text using Python-based frameworks like LangChain and Llamaindex, and processing these texts into chunks for embedding and retrieval. Vector storage techniques and selected embedding models to optimize data retrieval, using Pinecone for vector storage with a dimensionality of 1536 and cosine similarity for loss metrics. Human-generated answers, provided by junior doctors, were used as a comparison. Results: The LLM-RAG model generated answers within an average of 15-20 seconds, significantly faster than the 10 minutes typically required by humans. Among the basic LLMs, GPT4.0 exhibited the best accuracy of 80.1%. This accuracy was further increased to 91.4% when the model was enhanced with RAG. Compared to the human-generated instructions, which had an accuracy of 86.3%, the performance of the GPT4.0 RAG model demonstrated non-inferiority (p=0.610). Conclusions: In this case study, we demonstrated a LLM-RAG model for healthcare implementation. The pipeline shows the advantages of grounded knowledge, upgradability, and scalability as important aspects of healthcare LLM deployment.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/7423e5c903fb2befaf471cae64e2530f7c1d0404.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "27f8af480211586d07ff5ee6441ff6724ce85f4e.pdf": {
        "title": "PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers",
        "authors": [
            "Myeonghwa Lee",
            "Seonho An",
            "Min-Soo Kim"
        ],
        "published_date": "2024",
        "abstract": "In this paper, we conduct a study to utilize LLMs as a solution for decision making that requires complex data analysis. We define **Decision QA** as the task of answering the best decision, d_{best}, for a decision-making question Q, business rules R and a database D. Since there is no benchmark that can examine Decision QA, we propose Decision QA benchmark, **DQA**. It has two scenarios, Locating and Building, constructed from two video games (Europa Universalis IV and Victoria 3) that have almost the same goal as Decision QA. To address Decision QA effectively, we also propose a new RAG technique called the *iterative plan-then-retrieval augmented generation* (**PlanRAG**). Our PlanRAG-based LM generates the plan for decision making as the first step, and the retriever generates the queries for data analysis as the second step. The proposed method outperforms the state-of-the-art iterative RAG method by 15.8% in the Locating scenario and by 7.4% in the Building scenario, respectively. We release our code and benchmark at https://github.com/myeon9h/PlanRAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/27f8af480211586d07ff5ee6441ff6724ce85f4e.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "16b459de55727171aff6ea674535bea499e58261.pdf": {
        "title": "Simple is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation",
        "authors": [
            "Mufei Li",
            "Siqi Miao",
            "Pan Li"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) demonstrate strong reasoning abilities but face limitations such as hallucinations and outdated knowledge. Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by grounding LLM outputs in structured external knowledge from KGs. However, current KG-based RAG frameworks still struggle to optimize the trade-off between retrieval effectiveness and efficiency in identifying a suitable amount of relevant graph information for the LLM to digest. We introduce SubgraphRAG, extending the KG-based RAG framework that retrieves subgraphs and leverages LLMs for reasoning and answer prediction. Our approach innovatively integrates a lightweight multilayer perceptron with a parallel triple-scoring mechanism for efficient and flexible subgraph retrieval while encoding directional structural distances to enhance retrieval effectiveness. The size of retrieved subgraphs can be flexibly adjusted to match the query's need and the downstream LLM's capabilities. This design strikes a balance between model complexity and reasoning power, enabling scalable and generalizable retrieval processes. Notably, based on our retrieved subgraphs, smaller LLMs like Llama3.1-8B-Instruct deliver competitive results with explainable reasoning, while larger models like GPT-4o achieve state-of-the-art accuracy compared with previous baselines -- all without fine-tuning. Extensive evaluations on the WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency, accuracy, and reliability by reducing hallucinations and improving response grounding.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/16b459de55727171aff6ea674535bea499e58261.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "8c85026fe0d5c96fb275b81d483f8910db6ada03.pdf": {
        "title": "Retrieval augmented generation for 10 large language models and its generalizability in assessing medical fitness",
        "authors": [
            "Yuhe Ke",
            "Liyuan Jin",
            "Kabilan Elangovan",
            "H. Abdullah",
            "Nan Liu",
            "Alex Tiong Heng Sia",
            "Chai Rick Soh",
            "Joshua Yi Min Tung",
            "J. Ong",
            "C. Kuo",
            "Shao-Chun Wu",
            "V. Kovacheva",
            "D. Ting"
        ],
        "published_date": "2025",
        "abstract": "Large Language Models (LLMs) hold promise for medical applications but often lack domain-specific expertise. Retrieval Augmented Generation (RAG) enables customization by integrating specialized knowledge. This study assessed the accuracy, consistency, and safety of LLM-RAG models in determining surgical fitness and delivering preoperative instructions using 35 local and 23 international guidelines. Ten LLMs (e.g., GPT3.5, GPT4, GPT4o, Gemini, Llama2, and Llama3, Claude) were tested across 14 clinical scenarios. A total of 3234 responses were generated and compared to 448 human-generated answers. The GPT4 LLM-RAG model with international guidelines generated answers within 20\u2009s and achieved the highest accuracy, which was significantly better than human-generated responses (96.4% vs. 86.6%, p\u2009=\u20090.016). Additionally, the model exhibited an absence of hallucinations and produced more consistent output than humans. This study underscores the potential of GPT-4-based LLM-RAG models to deliver highly accurate, efficient, and consistent preoperative assessments.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/8c85026fe0d5c96fb275b81d483f8910db6ada03.pdf",
        "venue": "npj Digit. Medicine",
        "citationCount": 0,
        "score": 0
    },
    "8d0df3168870fd17b36ecd5575e406feb5a5a1b5.pdf": {
        "title": "M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions",
        "authors": [
            "Zheng Wang",
            "Shu Xian Teo",
            "Jieer Ouyang",
            "Yongjun Xu",
            "Wei Shi"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant memories from an external database. However, existing RAG methods typically organize all memories in a whole database, potentially limiting focus on crucial memories and introducing noise. In this paper, we introduce a multiple partition paradigm for RAG (called M-RAG), where each database partition serves as a basic unit for RAG execution. Based on this paradigm, we propose a novel framework that leverages LLMs with Multi-Agent Reinforcement Learning to optimize different language generation tasks explicitly. Through comprehensive experiments conducted on seven datasets, spanning three language generation tasks and involving three distinct language model architectures, we confirm that M-RAG consistently outperforms various baseline methods, achieving improvements of 11%, 8%, and 12% for text summarization, machine translation, and dialogue generation, respectively.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/8d0df3168870fd17b36ecd5575e406feb5a5a1b5.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "522c47365931e0ad722fbdac463ae415c97c65e4.pdf": {
        "title": "Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition",
        "authors": [
            "Demiao Lin"
        ],
        "published_date": "2024",
        "abstract": "With the rapid development of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) has become a predominant method in the field of professional knowledge-based question answering. Presently, major foundation model companies have opened up Embedding and Chat API interfaces, and frameworks like LangChain have already integrated the RAG process. It appears that the key models and steps in RAG have been resolved, leading to the question: are professional knowledge QA systems now approaching perfection? This article discovers that current primary methods depend on the premise of accessing high-quality text corpora. However, since professional documents are mainly stored in PDFs, the low accuracy of PDF parsing significantly impacts the effectiveness of professional knowledge-based QA. We conducted an empirical RAG experiment across hundreds of questions from the corresponding real-world professional documents. The results show that, ChatDOC, a RAG system equipped with a panoptic and pinpoint PDF parser, retrieves more accurate and complete segments, and thus better answers. Empirical experiments show that ChatDOC is superior to baseline on nearly 47% of questions, ties for 38% of cases, and falls short on only 15% of cases. It shows that we may revolutionize RAG with enhanced PDF structure recognition.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/522c47365931e0ad722fbdac463ae415c97c65e4.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "55c3095681acc82780508b0e484dba0c30cf1caa.pdf": {
        "title": "Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation",
        "authors": [
            "Gauthier Guinet",
            "Behrooz Omidvar-Tehrani",
            "Anoop Deoras",
            "Laurent Callot"
        ],
        "published_date": "2024",
        "abstract": "We propose a new method to measure the task-specific accuracy of Retrieval-Augmented Large Language Models (RAG). Evaluation is performed by scoring the RAG on an automatically-generated synthetic exam composed of multiple choice questions based on the corpus of documents associated with the task. Our method is an automated, cost-efficient, interpretable, and robust strategy to select the optimal components for a RAG system. We leverage Item Response Theory (IRT) to estimate the quality of an exam and its informativeness on task-specific accuracy. IRT also provides a natural way to iteratively improve the exam by eliminating the exam questions that are not sufficiently informative about a model's ability. We demonstrate our approach on four new open-ended Question-Answering tasks based on Arxiv abstracts, StackExchange questions, AWS DevOps troubleshooting guides, and SEC filings. In addition, our experiments reveal more general insights into factors impacting RAG performance like size, retrieval mechanism, prompting and fine-tuning. Most notably, our findings show that choosing the right retrieval algorithms often leads to bigger performance gains than simply using a larger language model.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/55c3095681acc82780508b0e484dba0c30cf1caa.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 0,
        "score": 0
    },
    "eaf2f0ed4281699f52f3c03ee4a2ee411fa0aa6e.pdf": {
        "title": "Biomedical knowledge graph-optimized prompt generation for large language models",
        "authors": [
            "Karthik Soman",
            "Peter W Rose",
            "John H Morris",
            "Rabia E Akbas",
            "Brett Smith",
            "Braian Peetoom",
            "Catalina Villouta-Reyes",
            "G. Cerono",
            "Yongmei Shi",
            "Angela Rizk-Jackson",
            "Sharat Israni",
            "Charlotte A. Nelson",
            "Sui Huang",
            "Sergio Baranzini"
        ],
        "published_date": "2023",
        "abstract": "Abstract Motivation Large language models (LLMs) are being adopted at an unprecedented rate, yet still face challenges in knowledge-intensive domains such as biomedicine. Solutions such as pretraining and domain-specific fine-tuning add substantial computational overhead, requiring further domain-expertise. Here, we introduce a token-optimized and robust Knowledge Graph-based Retrieval Augmented Generation (KG-RAG) framework by leveraging a massive biomedical KG (SPOKE) with LLMs such as Llama-2-13b, GPT-3.5-Turbo, and GPT-4, to generate meaningful biomedical text rooted in established knowledge. Results Compared to the existing RAG technique for Knowledge Graphs, the proposed method utilizes minimal graph schema for context extraction and uses embedding methods for context pruning. This optimization in context extraction results in more than 50% reduction in token consumption without compromising the accuracy, making a cost-effective and robust RAG implementation on proprietary LLMs. KG-RAG consistently enhanced the performance of LLMs across diverse biomedical prompts by generating responses rooted in established knowledge, accompanied by accurate provenance and statistical evidence (if available) to substantiate the claims. Further benchmarking on human curated datasets, such as biomedical true/false and multiple-choice questions (MCQ), showed a remarkable 71% boost in the performance of the Llama-2 model on the challenging MCQ dataset, demonstrating the framework\u2019s capacity to empower open-source models with fewer parameters for domain-specific questions. Furthermore, KG-RAG enhanced the performance of proprietary GPT models, such as GPT-3.5 and GPT-4. In summary, the proposed framework combines explicit and implicit knowledge of KG and LLM in a token optimized fashion, thus enhancing the adaptability of general-purpose LLMs to tackle domain-specific questions in a cost-effective fashion. Availability and implementation SPOKE KG can be accessed at https://spoke.rbvi.ucsf.edu/neighborhood.html. It can also be accessed using REST-API (https://spoke.rbvi.ucsf.edu/swagger/). KG-RAG code is made available at https://github.com/BaranziniLab/KG_RAG. Biomedical benchmark datasets used in this study are made available to the research community in the same GitHub repository.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/eaf2f0ed4281699f52f3c03ee4a2ee411fa0aa6e.pdf",
        "venue": "Bioinformatics",
        "citationCount": 0,
        "score": 0
    },
    "6bdb704aa7f99a3d9899532c547616767bbf8302.pdf": {
        "title": "MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented Generation via Knowledge-enhanced Reranking and Noise-injected Training",
        "authors": [
            "Zhanpeng Chen",
            "Chengjin Xu",
            "Yiyan Qi",
            "Jian Guo"
        ],
        "published_date": "2024",
        "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in processing and generating content across multiple data modalities. However, a significant drawback of MLLMs is their reliance on static training data, leading to outdated information and limited contextual awareness. This static nature hampers their ability to provide accurate and up-to-date responses, particularly in dynamic or rapidly evolving contexts. Though integrating Multimodal Retrieval-augmented Generation (Multimodal RAG) offers a promising solution, the system would inevitably encounter the multi-granularity noisy correspondence (MNC) problem, which hinders accurate retrieval and generation. In this work, we propose RagVL, a novel framework with knowledge-enhanced reranking and noise-injected training, to address these limitations. We instruction-tune the MLLM with a simple yet effective instruction template to induce its ranking ability and serve it as a reranker to precisely filter the top-k retrieved images. For generation, we inject visual noise during training at the data and token levels to enhance the generator's robustness. Extensive experiments on the subsets of two datasets that require retrieving and reasoning over images to answer a given query verify the effectiveness of our method. Code and models are available at https://github.com/IDEA-FinAI/RagVL.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/6bdb704aa7f99a3d9899532c547616767bbf8302.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "03182415b7e769a387ae16c4a61c1df908304e7e.pdf": {
        "title": "Retrieval Augmented Generation Enabled Generative Pre-Trained Transformer 4 (GPT-4) Performance for Clinical Trial Screening",
        "authors": [
            "Ozan Unlu",
            "Jiyeon Shin",
            "Charlotte J. Mailly",
            "Michael Oates",
            "Michela R. Tucci",
            "Matthew Varugheese",
            "K. Wagholikar",
            "Fei Wang",
            "Benjamin M. Scirica",
            "A. Blood",
            "Samuel J. Aronson"
        ],
        "published_date": "2024",
        "abstract": "Background: Subject screening is a key aspect of all clinical trials; however, traditionally, it is a labor-intensive and error-prone task, demanding significant time and resources. With the advent of large language models (LLMs) and related technologies, a paradigm shift in natural language processing capabilities offers a promising avenue for increasing both quality and efficiency of screening efforts. This study aimed to test the Retrieval-Augmented Generation (RAG) process enabled Generative Pretrained Transformer Version 4 (GPT-4) to accurately identify and report on inclusion and exclusion criteria for a clinical trial. Methods: The (Co-Operative Program for Implementation of Optimal Therapy in Heart Failure) COPILOT-HF trial aims to recruit patients with symptomatic heart failure. As part of the screening process, a list of potentially eligible patients is created through an electronic health record (EHR) query. Currently, structured data in the EHR can only be used to determine 5 out of 6 inclusion and 5 out of 17 exclusion criteria. Trained, but non-licensed, study staff complete manual chart review to determine patient eligibility and record their assessment of the inclusion and exclusion criteria. We obtained the structured assessments completed by the study staff and clinical notes for the past two years and developed a workflow of clinical note-based question answering system powered by RAG architecture and GPT-4 that we named RECTIFIER (RAG-Enabled Clinical Trial Infrastructure for Inclusion Exclusion Review). We used notes from 100 patients as a development dataset, 282 patients as a validation dataset, and 1894 patients as a test set. An expert clinician completed a blinded review of patients' charts to answer the eligibility questions and determine the \"gold standard\" answers. We calculated the sensitivity, specificity, accuracy, and Matthews correlation coefficient (MCC) for each question and screening method. We also performed bootstrapping to calculate the confidence intervals for each statistic. Results: Both RECTIFIER and study staff answers closely aligned with the expert clinician answers across criteria with accuracy ranging between 97.9% and 100% (MCC 0.837 and 1) for RECTIFIER and 91.7% and 100% (MCC 0.644 and 1) for study staff. RECTIFIER performed better than study staff to determine the inclusion criteria of \"symptomatic heart failure\" with an accuracy of 97.9% vs 91.7% and an MCC of 0.924 vs 0.721, respectively. Overall, the sensitivity and specificity of determining eligibility for the RECTIFIER was 92.3% (CI) and 93.9% (CI), and study staff was 90.1% (CI) and 83.6% (CI), respectively. Conclusion: GPT-4 based solutions have the potential to improve efficiency and reduce costs in clinical trial screening. When incorporating new tools such as RECTIFIER, it is important to consider the potential hazards of automating the screening process and set up appropriate mitigation strategies such as final clinician review before patient engagement.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/03182415b7e769a387ae16c4a61c1df908304e7e.pdf",
        "venue": "medRxiv",
        "citationCount": 0,
        "score": 0
    },
    "5b3c1a291cc717fa80218ead429e7507e967ec01.pdf": {
        "title": "Development of a Liver Disease-Specific Large Language Model Chat Interface using Retrieval Augmented Generation",
        "authors": [
            "J. Ge",
            "Steve Sun",
            "Joseph Owens",
            "Victor Galvez",
            "Oksana Gologorskaya",
            "Jennifer C. Lai",
            "Mark J. Pletcher",
            "Ki Lai"
        ],
        "published_date": "2023",
        "abstract": "Background: Large language models (LLMs) have significant capabilities in clinical information processing tasks. Commercially available LLMs, however, are not optimized for clinical uses and are prone to generating incorrect or hallucinatory information. Retrieval-augmented generation (RAG) is an enterprise architecture that allows embedding of customized data into LLMs. This approach \"specializes\" the LLMs and is thought to reduce hallucinations. Methods: We developed \"LiVersa,\" a liver disease-specific LLM, by using our institution's protected health information (PHI)-complaint text embedding and LLM platform, \"Versa.\" We conducted RAG on 30 publicly available American Association for the Study of Liver Diseases (AASLD) guidelines and guidance documents to be incorporated into LiVersa. We evaluated LiVersa's performance by comparing its responses versus those of trainees from a previously published knowledge assessment study regarding hepatitis B (HBV) treatment and hepatocellular carcinoma (HCC) surveillance. Results: LiVersa answered all 10 questions correctly when forced to provide a \"yes\" or \"no\" answer. Full detailed responses with justifications and rationales, however, were not completely correct for three of the questions. Discussions: In this study, we demonstrated the ability to build disease-specific and PHI-compliant LLMs using RAG. While our LLM, LiVersa, demonstrated more specificity in answering questions related to clinical hepatology - there were some knowledge deficiencies due to limitations set by the number and types of documents used for RAG. The LiVersa prototype, however, is a proof of concept for utilizing RAG to customize LLMs for clinical uses and a potential strategy to realize personalized medicine in the future.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/5b3c1a291cc717fa80218ead429e7507e967ec01.pdf",
        "venue": "medRxiv",
        "citationCount": 0,
        "score": 0
    },
    "20a8a84db1ebf5a8b75525671f5baf431a32f1a3.pdf": {
        "title": "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation",
        "authors": [
            "David Rau",
            "Herv'e D'ejean",
            "Nadezhda Chirkova",
            "Thibault Formal",
            "Shuai Wang",
            "Vassilina Nikoulina",
            "S. Clinchant"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation allows to enhance Large Language Models with external knowledge. In response to the recent popularity of generative LLMs, many RAG approaches have been proposed, which involve an intricate number of different configurations such as evaluation datasets, collections, metrics, retrievers, and LLMs. Inconsistent benchmarking poses a major challenge in comparing approaches and understanding the impact of each component in the pipeline. In this work, we study best practices that lay the groundwork for a systematic evaluation of RAG and present BERGEN, an end-to-end library for reproducible research standardizing RAG experiments. In an extensive study focusing on QA, we benchmark different state-of-the-art retrievers, rerankers, and LLMs. Additionally, we analyze existing RAG metrics and datasets. Our open-source library BERGEN is available under \\url{https://github.com/naver/bergen}.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/20a8a84db1ebf5a8b75525671f5baf431a32f1a3.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "09022e0e75fa472e9a1f6b743223c016a5ec35e2.pdf": {
        "title": "Systematic Analysis of Retrieval-Augmented Generation-Based LLMs for Medical Chatbot Applications",
        "authors": [
            "Arunabh Bora",
            "H. Cuay\u00e1huitl"
        ],
        "published_date": "2024",
        "abstract": "Artificial Intelligence (AI) has the potential to revolutionise the medical and healthcare sectors. AI and related technologies could significantly address some supply-and-demand challenges in the healthcare system, such as medical AI assistants, chatbots and robots. This paper focuses on tailoring LLMs to medical data utilising a Retrieval-Augmented Generation (RAG) database to evaluate their performance in a computationally resource-constrained environment. Existing studies primarily focus on fine-tuning LLMs on medical data, but this paper combines RAG and fine-tuned models and compares them against base models using RAG or only fine-tuning. Open-source LLMs (Flan-T5-Large, LLaMA-2-7B, and Mistral-7B) are fine-tuned using the medical datasets Meadow-MedQA and MedMCQA. Experiments are reported for response generation and multiple-choice question answering. The latter uses two distinct methodologies: Type A, as standard question answering via direct choice selection; and Type B, as language generation and probability confidence score generation of choices available. Results in the medical domain revealed that Fine-tuning and RAG are crucial for improved performance, and that methodology Type A outperforms Type B.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/09022e0e75fa472e9a1f6b743223c016a5ec35e2.pdf",
        "venue": "Machine Learning and Knowledge Extraction",
        "citationCount": 0,
        "score": 0
    },
    "680824bef5d6f98d669c49246363f0894a678e3b.pdf": {
        "title": "Ragnar\u00f6k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track",
        "authors": [
            "Ronak Pradeep",
            "Nandan Thakur",
            "Sahel Sharifymoghaddam",
            "Eric Zhang",
            "Ryan Nguyen",
            "Daniel Campos",
            "Nick Craswell",
            "Jimmy Lin"
        ],
        "published_date": "2024",
        "abstract": "Did you try out the new Bing Search? Or maybe you fiddled around with Google AI~Overviews? These might sound familiar because the modern-day search stack has recently evolved to include retrieval-augmented generation (RAG) systems. They allow searching and incorporating real-time data into large language models (LLMs) to provide a well-informed, attributed, concise summary in contrast to the traditional search paradigm that relies on displaying a ranked list of documents. Therefore, given these recent advancements, it is crucial to have an arena to build, test, visualize, and systematically evaluate RAG-based search systems. With this in mind, we propose the TREC 2024 RAG Track to foster innovation in evaluating RAG systems. In our work, we lay out the steps we've made towards making this track a reality -- we describe the details of our reusable framework, Ragnar\\\"ok, explain the curation of the new MS MARCO V2.1 collection choice, release the development topics for the track, and standardize the I/O definitions which assist the end user. Next, using Ragnar\\\"ok, we identify and provide key industrial baselines such as OpenAI's GPT-4o or Cohere's Command R+. Further, we introduce a web-based user interface for an interactive arena allowing benchmarking pairwise RAG systems by crowdsourcing. We open-source our Ragnar\\\"ok framework and baselines to achieve a unified standard for future RAG systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/680824bef5d6f98d669c49246363f0894a678e3b.pdf",
        "venue": "European Conference on Information Retrieval",
        "citationCount": 0,
        "score": 0
    },
    "9a3d1d1a1c00feb6c7cbe0e488eff57c606463c9.pdf": {
        "title": "Retrieval-augmented generation in multilingual settings",
        "authors": [
            "Nadezhda Chirkova",
            "David Rau",
            "Herv'e D'ejean",
            "Thibault Formal",
            "S. Clinchant",
            "Vassilina Nikoulina"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) has recently emerged as a promising solution for incorporating up-to-date or domain-specific knowledge into large language models (LLMs) and improving LLM factuality, but is predominantly studied in English-only settings. In this work, we consider RAG in the multilingual setting (mRAG), i.e. with user queries and the datastore in 13 languages, and investigate which components and with which adjustments are needed to build a well-performing mRAG pipeline, that can be used as a strong baseline in future works. Our findings highlight that despite the availability of high-quality off-the-shelf multilingual retrievers and generators, task-specific prompt engineering is needed to enable generation in user languages. Moreover, current evaluation metrics need adjustments for multilingual setting, to account for variations in spelling named entities. The main limitations to be addressed in future works include frequent code-switching in non-Latin alphabet languages, occasional fluency errors, wrong reading of the provided documents, or irrelevant retrieval. We release the code for the resulting mRAG baseline pipeline at https://github.com/naver/bergen, Documentation: https://github.com/naver/bergen/blob/main/documentations/multilingual.md.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/9a3d1d1a1c00feb6c7cbe0e488eff57c606463c9.pdf",
        "venue": "KNOWLLM",
        "citationCount": 0,
        "score": 0
    },
    "64ee29d6ddb2c2167a201783ddd4d0a9b744f352.pdf": {
        "title": "Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation",
        "authors": [
            "Guanting Dong",
            "Yutao Zhu",
            "Chenghao Zhang",
            "Zechen Wang",
            "Zhicheng Dou",
            "Ji-Rong Wen"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) has effectively mitigated the hallucination problem of large language models (LLMs). However, the difficulty of aligning the retriever with the LLMs' diverse knowledge preferences inevitably poses a challenge in developing a reliable RAG system. To address this issue, we propose DPA-RAG, a universal framework designed to align diverse knowledge preferences within RAG systems. Specifically, we initially introduce a preference knowledge construction pipeline and incorporate five novel query augmentation strategies to alleviate preference data scarcity. Based on preference data, DPA-RAG accomplishes both external and internal preference alignment: 1) It jointly integrates pairwise, pointwise, and contrastive preference alignment abilities into the reranker, achieving external preference alignment among RAG components. 2) It further introduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT), enabling LLMs to implicitly capture knowledge aligned with their reasoning preferences, achieving LLMs' internal alignment. Experimental results across four knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all baselines and seamlessly integrates both black-box and open-sourced LLM readers. Further qualitative analysis and discussions provide empirical guidance for achieving reliable RAG systems. Our code and example dataset are available at https://github.com/dongguanting/DPA-RAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/64ee29d6ddb2c2167a201783ddd4d0a9b744f352.pdf",
        "venue": "The Web Conference",
        "citationCount": 0,
        "score": 0
    },
    "650f7db2b37069614c0fb04ea77f099bb5d4efa5.pdf": {
        "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed KV Caches for Chunked Text",
        "authors": [
            "Songshuo Lu",
            "Hua Wang",
            "Yutian Rong",
            "Zhi Chen",
            "Yaohua Tang"
        ],
        "published_date": "2024",
        "abstract": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process numerous retrieved document chunks for prefill which requires a large volume of computation, therefore leading to significant latency in time-to-first-token (TTFT). To reduce the computation overhead as well as TTFT, we introduce TurboRAG, a novel RAG system that redesigns the inference paradigm of the current RAG system by first pre-computing and storing the key-value (KV) caches of documents offline, and then directly retrieving the saved KV cache for prefill. Hence, online computation of KV caches is eliminated during inference. In addition, we provide a number of insights into the mask matrix and positional embedding mechanisms, plus fine-tune a pretrained language model to maintain model accuracy of TurboRAG. Our approach is applicable to most existing large language models and their applications without any requirement in modification of models and inference systems. Experimental results across a suite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x compared to the conventional RAG systems (on an average of 8.6x), but reserving comparable performance to the standard RAG systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/650f7db2b37069614c0fb04ea77f099bb5d4efa5.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "30394de373b65aeda84e2bd100e8efe38f4d1a8c.pdf": {
        "title": "Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection",
        "authors": [
            "Yun Zhu",
            "Jia-Chen Gu",
            "Caitlin Sikora",
            "Ho Ko",
            "Yinxiao Liu",
            "Chu-Cheng Lin",
            "Lei Shu",
            "Liangchen Luo",
            "Lei Meng",
            "Bang Liu",
            "Jindong Chen"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) augmented with retrieval exhibit robust performance and extensive versatility by incorporating external contexts. However, the input length grows linearly in the number of retrieved documents, causing a dramatic increase in latency. In this paper, we propose a novel paradigm named Sparse RAG, which seeks to cut computation costs through sparsity. Specifically, Sparse RAG encodes retrieved documents in parallel, which eliminates latency introduced by long-range attention of retrieved documents. Then, LLMs selectively decode the output by only attending to highly relevant caches auto-regressively, which are chosen via prompting LLMs with special control tokens. It is notable that Sparse RAG combines the assessment of each individual document and the generation of the response into a single process. The designed sparse mechanism in a RAG system can facilitate the reduction of the number of documents loaded during decoding for accelerating the inference of the RAG system. Additionally, filtering out undesirable contexts enhances the model's focus on relevant context, inherently improving its generation quality. Evaluation results of two datasets show that Sparse RAG can strike an optimal balance between generation quality and computational efficiency, demonstrating its generalizability across both short- and long-form generation tasks.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/30394de373b65aeda84e2bd100e8efe38f4d1a8c.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "1d1beece295703c0cb3e545edaa12a4336b407bc.pdf": {
        "title": "Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models",
        "authors": [
            "Tian Yu",
            "Shaolei Zhang",
            "Yang Feng"
        ],
        "published_date": "2024",
        "abstract": "Iterative retrieval refers to the process in which the model continuously queries the retriever during generation to enhance the relevance of the retrieved knowledge, thereby improving the performance of Retrieval-Augmented Generation (RAG). Existing work typically employs few-shot prompting or manually constructed rules to implement iterative retrieval. This introduces additional inference overhead and overlooks the remarkable reasoning capabilities of Large Language Models (LLMs). In this paper, we introduce Auto-RAG, an autonomous iterative retrieval model centered on the LLM's powerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues with the retriever, systematically planning retrievals and refining queries to acquire valuable knowledge. This process continues until sufficient external information is gathered, at which point the results are presented to the user. To this end, we develop a method for autonomously synthesizing reasoning-based decision-making instructions in iterative retrieval and fine-tuned the latest open-source LLMs. The experimental results indicate that Auto-RAG is capable of autonomous iterative interaction with the retriever, effectively leveraging the remarkable reasoning and decision-making abilities of LLMs, which lead to outstanding performance across six benchmarks. Further analysis reveals that Auto-RAG can autonomously adjust the number of iterations based on the difficulty of the questions and the utility of the retrieved knowledge, without requiring any human intervention. Moreover, Auto-RAG expresses the iterative retrieval process in natural language, enhancing interpretability while providing users with a more intuitive experience\\footnote{Code is available at \\url{https://github.com/ictnlp/Auto-RAG}.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/1d1beece295703c0cb3e545edaa12a4336b407bc.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "5879575701b9b65b5cc56c00d9eebbfa219e0428.pdf": {
        "title": "Retrieval augmented generation for large language models in healthcare: A systematic review",
        "authors": [
            "L. M. Amugongo",
            "Pietro Mascheroni",
            "Steven Brooks",
            "Stefan Doering",
            "Jan Seidel"
        ],
        "published_date": "2025",
        "abstract": "Large Language Models (LLMs) have demonstrated promising capabilities to solve complex tasks in critical sectors such as healthcare. However, LLMs are limited by their training data which is often outdated, the tendency to generate inaccurate (\u201challucinated\u201d) content and a lack of transparency in the content they generate. To address these limitations, retrieval augmented generation (RAG) grounds the responses of LLMs by exposing them to external knowledge sources. However, in the healthcare domain there is currently a lack of systematic understanding of which datasets, RAG methodologies and evaluation frameworks are available. This review aims to bridge this gap by assessing RAG-based approaches employed by LLMs in healthcare, focusing on the different steps of retrieval, augmentation and generation. Additionally, we identify the limitations, strengths and gaps in the existing literature. Our synthesis shows that 78.9% of studies used English datasets and 21.1% of the datasets are in Chinese. We find that a range of techniques are employed RAG-based LLMs in healthcare, including Naive RAG, Advanced RAG, and Modular RAG. Surprisingly, proprietary models such as GPT-3.5/4 are the most used for RAG applications in healthcare. We find that there is a lack of standardised evaluation frameworks for RAG-based applications. In addition, the majority of the studies do not assess or address ethical considerations related to RAG in healthcare. It is important to account for ethical challenges that are inherent when AI systems are implemented in the clinical setting. Lastly, we highlight the need for further research and development to ensure responsible and effective adoption of RAG in the medical domain.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/5879575701b9b65b5cc56c00d9eebbfa219e0428.pdf",
        "venue": "PLOS Digital Health",
        "citationCount": 0,
        "score": 0
    },
    "61f8baae9aecadc8bed1ce263c32bd108b476ebd.pdf": {
        "title": "UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis",
        "authors": [
            "Yulong Hui",
            "Yao Lu",
            "Huanchen Zhang"
        ],
        "published_date": "2024",
        "abstract": "The use of Retrieval-Augmented Generation (RAG) has improved Large Language Models (LLMs) in collaborating with external data, yet significant challenges exist in real-world scenarios. In areas such as academic literature and finance question answering, data are often found in raw text and tables in HTML or PDF formats, which can be lengthy and highly unstructured. In this paper, we introduce a benchmark suite, namely Unstructured Document Analysis (UDA), that involves 2,965 real-world documents and 29,590 expert-annotated Q&A pairs. We revisit popular LLM- and RAG-based solutions for document analysis and evaluate the design choices and answer qualities across multiple document domains and diverse query types. Our evaluation yields interesting findings and highlights the importance of data parsing and retrieval. We hope our benchmark can shed light and better serve real-world document analysis applications. The benchmark suite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/61f8baae9aecadc8bed1ce263c32bd108b476ebd.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0
    },
    "1bce5a6d8c037a90e9cd49d38fe6446652389674.pdf": {
        "title": "Towards a Search Engine for Machines: Unified Ranking for Multiple Retrieval-Augmented Large Language Models",
        "authors": [
            "Alireza Salemi",
            "Hamed Zamani"
        ],
        "published_date": "2024",
        "abstract": "This paper introduces uRAG-a framework with a unified retrieval engine that serves multiple downstream retrieval-augmented generation (RAG) systems. Each RAG system consumes the retrieval results for a unique purpose, such as open-domain question answering, fact verification, entity linking, and relation extraction. We introduce a generic training guideline that standardizes the communication between the search engine and the downstream RAG systems that engage in optimizing the retrieval model. This lays the groundwork for us to build a large-scale experimentation ecosystem consisting of 18 RAG systems that engage in training and 18 unknown RAG systems that use the uRAG as the new users of the search engine. Using this experimentation ecosystem, we answer a number of fundamental research questions that improve our understanding of promises and challenges in developing search engines for machines.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/1bce5a6d8c037a90e9cd49d38fe6446652389674.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 0,
        "score": 0
    },
    "a76209fea4627974b5e12d8b4942268eb17bc7df.pdf": {
        "title": "Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation",
        "authors": [
            "Shicheng Xu",
            "Liang Pang",
            "Mo Yu",
            "Fandong Meng",
            "Huawei Shen",
            "Xueqi Cheng",
            "Jie Zhou"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ``Information Refiner'', which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named InFO-RAG that optimizes LLMs for RAG in an unsupervised manner. InFO-RAG is low-cost and general across various tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue, and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an average of 9.39\\% relative points. InFO-RAG also shows advantages in in-context learning and robustness of RAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/a76209fea4627974b5e12d8b4942268eb17bc7df.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "9b7854829ae4d4653a56ba04880aff848d70fc42.pdf": {
        "title": "Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models",
        "authors": [
            "Zhibo Hu",
            "Chen Wang",
            "Yanfeng Shu",
            "Helen Paik",
            "Liming Zhu"
        ],
        "published_date": "2024",
        "abstract": "The robustness of large language models (LLMs) becomes increasingly important as their use rapidly grows in a wide range of domains. Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of text generation from LLMs. However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied. In this work, we find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers. We systematically evaluate the effect of such prefixes on RAG by introducing a novel optimization technique called Gradient Guided Prompt Perturbation (GGPP). GGPP achieves a high success rate in steering outputs of RAG-based LLMs to targeted wrong answers. It can also cope with instructions in the prompts requesting to ignore irrelevant context. We also exploit LLMs' neuron activation difference between prompts with and without GGPP perturbations to give a method that improves the robustness of RAG-based LLMs through a highly effective detector trained on neuron activation triggered by GGPP generated prompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of our methods.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/9b7854829ae4d4653a56ba04880aff848d70fc42.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 0,
        "score": 0
    },
    "b03cfca7672e60cbe7999853e9c6f833ab20e012.pdf": {
        "title": "Rationale-Guided Retrieval Augmented Generation for Medical Question Answering",
        "authors": [
            "Jiwoong Sohn",
            "Yein Park",
            "Chanwoong Yoon",
            "Sihyeon Park",
            "Hyeon Hwang",
            "Mujeen Sung",
            "Hyunjae Kim",
            "Jaewoo Kang"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLM) hold significant potential for applications in biomedicine, but they struggle with hallucinations and outdated knowledge. While retrieval-augmented generation (RAG) is generally employed to address these issues, it also has its own set of challenges: (1) LLMs are vulnerable to irrelevant or incorrect context, (2) medical queries are often not well-targeted for helpful information, and (3) retrievers are prone to bias toward the specific source corpus they were trained on. In this study, we present RAG$^2$ (RAtionale-Guided RAG), a new framework for enhancing the reliability of RAG in biomedical contexts. RAG$^2$ incorporates three key innovations: a small filtering model trained on perplexity-based labels of rationales, which selectively augments informative snippets of documents while filtering out distractors; LLM-generated rationales as queries to improve the utility of retrieved snippets; a structure designed to retrieve snippets evenly from a comprehensive set of four biomedical corpora, effectively mitigating retriever bias. Our experiments demonstrate that RAG$^2$ improves the state-of-the-art LLMs of varying sizes, with improvements of up to 6.1\\%, and it outperforms the previous best medical RAG model by up to 5.6\\% across three medical question-answering benchmarks. Our code is available at https://github.com/dmis-lab/RAG2.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/b03cfca7672e60cbe7999853e9c6f833ab20e012.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "74fdf8053638eb12e5cce073ab4fb476fdb9f9cb.pdf": {
        "title": "Automating Systematic Literature Reviews with Retrieval-Augmented Generation: A Comprehensive Overview",
        "authors": [
            "Binglan Han",
            "Teo Su\u0161njak",
            "A. Mathrani"
        ],
        "published_date": "2024",
        "abstract": "This study examines Retrieval-Augmented Generation (RAG) in large language models (LLMs) and their significant application for undertaking systematic literature reviews (SLRs). RAG-based LLMs can potentially automate tasks like data extraction, summarization, and trend identification. However, while LLMs are exceptionally proficient in generating human-like text and interpreting complex linguistic nuances, their dependence on static, pre-trained knowledge can result in inaccuracies and hallucinations. RAG mitigates these limitations by integrating LLMs\u2019 generative capabilities with the precision of real-time information retrieval. We review in detail the three key processes of the RAG framework\u2014retrieval, augmentation, and generation. We then discuss applications of RAG-based LLMs to SLR automation and highlight future research topics, including integration of domain-specific LLMs, multimodal data processing and generation, and utilization of multiple retrieval sources. We propose a framework of RAG-based LLMs for automating SRLs, which covers four stages of SLR process: literature search, literature screening, data extraction, and information synthesis. Future research aims to optimize the interaction between LLM selection, training strategies, RAG techniques, and prompt engineering to implement the proposed framework, with particular emphasis on the retrieval of information from individual scientific papers and the integration of these data to produce outputs addressing various aspects such as current status, existing gaps, and emerging trends.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/74fdf8053638eb12e5cce073ab4fb476fdb9f9cb.pdf",
        "venue": "Applied Sciences",
        "citationCount": 0,
        "score": 0
    },
    "9b302002c4b764f61fa7a3d14270470f625945cf.pdf": {
        "title": "RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards",
        "authors": [
            "Xinze Li",
            "Senkun Mei",
            "Zhenghao Liu",
            "Yukun Yan",
            "Shuo Wang",
            "Shi Yu",
            "Zheni Zeng",
            "Hao Chen",
            "Ge Yu",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Chenyan Xiong"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) has proven its effectiveness in mitigating hallucinations in Large Language Models (LLMs) by retrieving knowledge from external resources. To adapt LLMs for the RAG systems, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system. In this paper, we propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG systems by aligning data preferences between different RAG modules. DDR works by collecting the rewards to optimize each agent in the RAG system with the rollout method, which prompts agents to sample some potential responses as perturbations, evaluates the impact of these perturbations on the whole RAG system, and subsequently optimizes the agent to produce outputs that improve the performance of the RAG system. Our experiments on various knowledge-intensive tasks demonstrate that DDR significantly outperforms the SFT method, particularly for LLMs with smaller-scale parameters that depend more on the retrieved knowledge. Additionally, DDR exhibits a stronger capability to align the data preference between RAG modules. The DDR method makes the generation module more effective in extracting key information from documents and mitigating conflicts between parametric memory and external knowledge. All codes are available at https://github.com/OpenMatch/RAG-DDR.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/9b302002c4b764f61fa7a3d14270470f625945cf.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0
    },
    "fe2dd4ac8bb14c622a890482384c9e1136479bf6.pdf": {
        "title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models",
        "authors": [
            "Fei Wang",
            "Xingchen Wan",
            "Ruoxi Sun",
            "Jiefeng Chen",
            "Sercan \u00d6. Arik"
        ],
        "published_date": "2024",
        "abstract": "Retrieval augmented generation (RAG), while effectively integrating external knowledge to address the inherent limitations of large language models (LLMs), can be hindered by imperfect retrieval that contain irrelevant, misleading, or even malicious information. Previous studies have rarely connected the behavior of RAG through joint analysis, particularly regarding error propagation coming from imperfect retrieval and potential conflicts between LLMs' internal knowledge and external sources. Through comprehensive and controlled analyses under realistic conditions, we find that imperfect retrieval augmentation is inevitable, common, and harmful. We identify the knowledge conflicts between LLM-internal and external knowledge from retrieval as a bottleneck to overcome imperfect retrieval in the post-retrieval stage of RAG. To address this, we propose Astute RAG, a novel RAG approach designed to be resilient to imperfect retrieval augmentation. It adaptively elicits essential information from LLMs' internal knowledge, iteratively consolidates internal and external knowledge with source-awareness, and finalizes the answer according to information reliability. Our experiments with Gemini and Claude demonstrate the superior performance of Astute RAG compared to previous robustness-enhanced RAG approaches. Specifically, Astute RAG is the only RAG method that achieves performance comparable to or even surpassing conventional use of LLMs under the worst-case scenario. Further analysis reveals the effectiveness of Astute RAG in resolving knowledge conflicts, thereby improving the trustworthiness of RAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/fe2dd4ac8bb14c622a890482384c9e1136479bf6.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "b71141dbd0e4827156c696e05ba1fa068ad43ee1.pdf": {
        "title": "GastroBot: a Chinese gastrointestinal disease chatbot based on the retrieval-augmented generation",
        "authors": [
            "Qingqing Zhou",
            "Can Liu",
            "Yuchen Duan",
            "Kaijie Sun",
            "Yu Li",
            "Hongxing Kan",
            "Zongyun Gu",
            "Jianhua Shu",
            "Jili Hu"
        ],
        "published_date": "2024",
        "abstract": "Introduction Large Language Models (LLMs) play a crucial role in clinical information processing, showcasing robust generalization across diverse language tasks. However, existing LLMs, despite their significance, lack optimization for clinical applications, presenting challenges in terms of illusions and interpretability. The Retrieval-Augmented Generation (RAG) model addresses these issues by providing sources for answer generation, thereby reducing errors. This study explores the application of RAG technology in clinical gastroenterology to enhance knowledge generation on gastrointestinal diseases. Methods We fine-tuned the embedding model using a corpus consisting of 25 guidelines on gastrointestinal diseases. The fine-tuned model exhibited an 18% improvement in hit rate compared to its base model, gte-base-zh. Moreover, it outperformed OpenAI\u2019s Embedding model by 20%. Employing the RAG framework with the llama-index, we developed a Chinese gastroenterology chatbot named \u201cGastroBot,\u201d which significantly improves answer accuracy and contextual relevance, minimizing errors and the risk of disseminating misleading information. Results When evaluating GastroBot using the RAGAS framework, we observed a context recall rate of 95%. The faithfulness to the source, stands at 93.73%. The relevance of answers exhibits a strong correlation, reaching 92.28%. These findings highlight the effectiveness of GastroBot in providing accurate and contextually relevant information about gastrointestinal diseases. During manual assessment of GastroBot, in comparison with other models, our GastroBot model delivers a substantial amount of valuable knowledge while ensuring the completeness and consistency of the results. Discussion Research findings suggest that incorporating the RAG method into clinical gastroenterology can enhance the accuracy and reliability of large language models. Serving as a practical implementation of this method, GastroBot has demonstrated significant enhancements in contextual comprehension and response quality. Continued exploration and refinement of the model are poised to drive forward clinical information processing and decision support in the gastroenterology field.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/b71141dbd0e4827156c696e05ba1fa068ad43ee1.pdf",
        "venue": "Frontiers in Medicine",
        "citationCount": 0,
        "score": 0
    },
    "d9ff626e7c2fad53e80061d34e4d0d0dbbaab1dd.pdf": {
        "title": "AutoRAG: Automated Framework for optimization of Retrieval Augmented Generation Pipeline",
        "authors": [
            "Dongkyu Kim",
            "Byoungwook Kim",
            "Donggeon Han",
            "Matouvs Eibich"
        ],
        "published_date": "2024",
        "abstract": "Using LLMs (Large Language Models) in conjunction with external documents has made RAG (Retrieval-Augmented Generation) an essential technology. Numerous techniques and modules for RAG are being researched, but their performance can vary across different datasets. Finding RAG modules that perform well on specific datasets is challenging. In this paper, we propose the AutoRAG framework, which automatically identifies suitable RAG modules for a given dataset. AutoRAG explores and approximates the optimal combination of RAG modules for the dataset. Additionally, we share the results of optimizing a dataset using AutoRAG. All experimental results and data are publicly available and can be accessed through our GitHub repository https://github.com/Marker-Inc-Korea/AutoRAG_ARAGOG_Paper .",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/d9ff626e7c2fad53e80061d34e4d0d0dbbaab1dd.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "9d9268b0191891511b09362759ba6a754c28fd9e.pdf": {
        "title": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains",
        "authors": [
            "Ran Xu",
            "Hui Liu",
            "Sreyashi Nag",
            "Zhenwei Dai",
            "Yaochen Xie",
            "Xianfeng Tang",
            "Chen Luo",
            "Yang Li",
            "Joyce C. Ho",
            "Carl Yang",
            "Qi He"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) enhances the question-answering (QA) abilities of large language models (LLMs) by integrating external knowledge. However, adapting general-purpose RAG systems to specialized fields such as science and medicine poses unique challenges due to distribution shifts and limited access to domain-specific data. To tackle this, we propose SimRAG, a self-training approach that equips the LLM with joint capabilities of question answering and question generation for domain adaptation. Our method first fine-tunes the LLM on instruction-following, question-answering, and search-related data. Then, it prompts the same LLM to generate diverse domain-relevant questions from unlabeled corpora, with an additional filtering strategy to retain high-quality synthetic examples. By leveraging these self-generated synthetic examples, the LLM can improve their performance on domain-specific RAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three domains, demonstrate that SimRAG outperforms baselines by 1.2\\%--8.6\\%.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/9d9268b0191891511b09362759ba6a754c28fd9e.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "5aabaf59808091eca1c6cba123ac2003017f4011.pdf": {
        "title": "Can Small Language Models With Retrieval-Augmented Generation Replace Large Language Models When Learning Computer Science?",
        "authors": [
            "Suqing Liu",
            "Zezhu Yu",
            "Feiran Huang",
            "Yousef Bulbulia",
            "Andi Bergen",
            "Michael Liut"
        ],
        "published_date": "2024",
        "abstract": "Leveraging Large Language Models (LLMs) for personalized learning and support is becoming a promising tool in computing education. AI Assistants can help students with programming, problem-solving, converse with them to clarify course content, explain error messages to help with debugging, and much more. However, using cloud-based LLMs poses risks around data security, privacy, but also control of the overarching system. To address these concerns, we created a locally-stored Small Language Model (SLM) that leverages different Retrieval-Augmented Generation (RAG) methods to support computing students' learning. We compare one SLM (neural-chat-7b-v3 - fine-tuned version of Mistral-7B-v0.1) against two popular LLMs (gpt-3.5-turbo and gpt-4-32k) to see the viability for computing educators to use in their course(s). We use conversations from a CS1 course (N = 1,260), providing students with an AI Assistant (using gpt-3.5-turbo) to help them learn content and support problem-solving while completing their Python programming assignment. In total, we had 269 students use the AI Assistant, with a total of 1,988 questions asked. Using this real conversational data, we re-ran student questions using our novel SLM (neural-chat-7b-v3 testing nine different RAG methods) and gpt-4-32k, then compared those results against the original gpt-3.5-turbo responses. Our findings indicate that using an SLM with RAG can perform similarly, if not better, than LLMs. This shows that it is possible for computing educators to use SLMs (with RAG) in their course(s) as a tool for scalable learning, supporting content understanding and problem-solving needs, while employing their own policies on data privacy and security.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/5aabaf59808091eca1c6cba123ac2003017f4011.pdf",
        "venue": "Annual Conference on Innovation and Technology in Computer Science Education",
        "citationCount": 0,
        "score": 0
    },
    "d1c163e6f6e4b26e94ea8f639cd441326a68afd3.pdf": {
        "title": "DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation",
        "authors": [
            "Shuting Wang",
            "Jiongnan Liu",
            "Jiehan Cheng",
            "Yuqi Fu",
            "Peidong Guo",
            "Kun Fang",
            "Yutao Zhu",
            "Zhicheng Dou"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) offers a promising solution to address various limitations of Large Language Models (LLMs), such as hallucination and difficulties in keeping up with real-time updates. This approach is particularly critical in expert and domain-specific applications where LLMs struggle to cover expert knowledge. Therefore, evaluating RAG models in such scenarios is crucial, yet current studies often rely on general knowledge sources like Wikipedia to assess the models' abilities in solving common-sense problems. In this paper, we evaluated LLMs by RAG settings in a domain-specific context, college enrollment. We identified six required abilities for RAG models, including the ability in conversational RAG, analyzing structural information, faithfulness to external knowledge, denoising, solving time-sensitive problems, and understanding multi-document interactions. Each ability has an associated dataset with shared corpora to evaluate the RAG models' performance. We evaluated popular LLMs such as Llama, Baichuan, ChatGLM, and GPT models. Experimental results indicate that existing closed-book LLMs struggle with domain-specific questions, highlighting the need for RAG models to solve expert problems. Moreover, there is room for RAG models to improve their abilities in comprehending conversational history, analyzing structural information, denoising, processing multi-document interactions, and faithfulness in expert knowledge. We expect future studies could solve these problems better.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/d1c163e6f6e4b26e94ea8f639cd441326a68afd3.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "0f8546b7634af1a70ae8649d8538d32bf6cc4660.pdf": {
        "title": "Hybrid Retrieval-Augmented Generation Approach for LLMs Query Response Enhancement",
        "authors": [
            "Pouria Omrani",
            "Alireza Hosseini",
            "Kiana Hooshanfar",
            "Zahra Ebrahimian",
            "Ramin Toosi",
            "Mohammad Ali Akhaee"
        ],
        "published_date": "2024",
        "abstract": "In the domain of Natural Language Processing (NLP), the integration of Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) represents a significant advancement towards enhancing the depth and relevance of model-generated responses. This paper introduces a novel hybrid RAG framework that synergizes the Sentence-Window and Parent-Child methodologies with an innovative re-ranking mechanism, aimed at optimizing the query response capabilities of LLMs. By leveraging external knowledge sources more effectively, the proposed method enriches LLM outputs with greater accuracy, relevance, and information fidelity. We subject our hybrid model to rigorous evaluation against benchmark datasets and metrics, demonstrating its superior performance over existing state-of-the-art RAG techniques. The results highlight our method\u2019s enhanced ability to generate responses that are not only contextually appropriate but also demonstrate a high degree of faithfulness to the source material, thereby setting a new standard for query response enhancement in LLMs. Our study underscores the potential of hybrid RAG models in refining the interaction between LLMs and external knowledge, paving the way for future research in the field of NLP.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/0f8546b7634af1a70ae8649d8538d32bf6cc4660.pdf",
        "venue": "2024 10th International Conference on Web Research (ICWR)",
        "citationCount": 0,
        "score": 0
    },
    "d039d31ae5768c53ee567e51bd6fadf5d62d58db.pdf": {
        "title": "Application of NotebookLM, a Large Language Model with Retrieval-Augmented Generation, for Lung Cancer Staging",
        "authors": [
            "Ryota Tozuka",
            "Hisashi Johno",
            "Akitomo Amakawa",
            "Junichi Sato",
            "Mizuki Muto",
            "Shoichiro Seki",
            "Atsushi Komaba",
            "Hiroshi Onishi"
        ],
        "published_date": "2024",
        "abstract": "PURPOSE\nIn radiology, large language models (LLMs), including ChatGPT, have recently gained attention, and their utility is being rapidly evaluated. However, concerns have emerged regarding their reliability in clinical applications due to limitations such as hallucinations and insufficient referencing. To address these issues, we focus on the latest technology, retrieval-augmented generation (RAG), which enables LLMs to reference reliable external knowledge (REK). Specifically, this study examines the utility and reliability of a recently released RAG-equipped LLM (RAG-LLM), NotebookLM, for staging lung cancer.\n\n\nMATERIALS AND METHODS\nWe summarized the current lung cancer staging guideline in Japan and provided this as REK to NotebookLM. We then tasked NotebookLM with staging 100 fictional lung cancer cases based on CT findings and evaluated its accuracy. For comparison, we performed the same task using a gold-standard LLM, GPT-4 Omni (GPT-4o), both with and without the REK. For GPT-4o, the REK was provided directly within the prompt rather than through RAG.\n\n\nRESULTS\nNotebookLM achieved 86% diagnostic accuracy in the lung cancer staging experiment, outperforming GPT-4o, which recorded 39% accuracy with the REK and 25% without it. Moreover, NotebookLM demonstrated 95% accuracy in searching reference locations within the REK.\n\n\nCONCLUSION\nNotebookLM, a RAG-LLM, successfully performed lung cancer staging by utilizing the REK, demonstrating superior performance compared to GPT-4o (without RAG). Additionally, it provided highly accurate reference locations within the REK, allowing radiologists to efficiently evaluate the reliability of NotebookLM's responses and detect possible hallucinations. Overall, this study highlights the potential of NotebookLM, a RAG-LLM, in image diagnosis.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/d039d31ae5768c53ee567e51bd6fadf5d62d58db.pdf",
        "venue": "Japanese Journal of Radiology",
        "citationCount": 0,
        "score": 0
    },
    "1b6c568f3b8c10f4f887d2f0487c7c0ad46093e4.pdf": {
        "title": "Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems",
        "authors": [
            "Robert Lakatos",
            "P. Pollner",
            "Andr\u00e1s Hajdu",
            "Tam\u00e1s Jo\u00f3"
        ],
        "published_date": "2024",
        "abstract": "Generative large language models (LLMs) have revolutionized the development of knowledge-based systems, enabling new possibilities in applications like ChatGPT, Bing, and Gemini. Two key strategies for domain adaptation in these systems are Domain-Specific Fine-Tuning (DFT) and Retrieval-Augmented Generation (RAG). In this study, we evaluate the performance of RAG and DFT on several LLM architectures, including GPT-J-6B, OPT-6.7B, LLaMA, and LLaMA-2. We use the ROUGE, BLEU, and METEOR scores to evaluate the performance of the models. We also measure the performance of the models with our own designed cosine similarity-based Coverage Score (CS). Our results, based on experiments across multiple datasets, show that RAG-based systems consistently outperform those fine-tuned with DFT. Specifically, RAG models outperform DFT by an average of 17% in ROUGE, 13% in BLEU, and 36% in CS. At the same time, DFT achieves only a modest advantage in METEOR, suggesting slightly better creative capabilities. We also highlight the challenges of integrating RAG with DFT, as such integration can lead to performance degradation. Furthermore, we propose a simplified RAG-based architecture that maximizes efficiency and reduces hallucination, underscoring the advantages of RAG in building reliable, domain-adapted knowledge systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/1b6c568f3b8c10f4f887d2f0487c7c0ad46093e4.pdf",
        "venue": "Machine Learning and Knowledge Extraction",
        "citationCount": 0,
        "score": 0
    },
    "89729cdfe0f71ad7a04c73e9167c2b266ee0ee8c.pdf": {
        "title": "Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models",
        "authors": [
            "Zhuo Chen",
            "Jiawei Liu",
            "Haotan Liu",
            "Qikai Cheng",
            "Fan Zhang",
            "Wei Lu",
            "Xiaozhong Liu"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) is applied to solve hallucination problems and real-time constraints of large language models, but it also induces vulnerabilities against retrieval corruption attacks. Existing research mainly explores the unreliability of RAG in white-box and closed-domain QA tasks. In this paper, we aim to reveal the vulnerabilities of Retrieval-Enhanced Generative (RAG) models when faced with black-box attacks for opinion manipulation. We explore the impact of such attacks on user cognition and decision-making, providing new insight to enhance the reliability and security of RAG models. We manipulate the ranking results of the retrieval model in RAG with instruction and use these results as data to train a surrogate model. By employing adversarial retrieval attack methods to the surrogate model, black-box transfer attacks on RAG are further realized. Experiments conducted on opinion datasets across multiple topics show that the proposed attack strategy can significantly alter the opinion polarity of the content generated by RAG. This demonstrates the model's vulnerability and, more importantly, reveals the potential negative impact on user cognition and decision-making, making it easier to mislead users into accepting incorrect or biased information.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/89729cdfe0f71ad7a04c73e9167c2b266ee0ee8c.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "821e7c70e6637f07ab94a843c0de273f8618763b.pdf": {
        "title": "PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents",
        "authors": [
            "Saber Zerhoudi",
            "Michael Granitzer"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) struggle with generating reliable outputs due to outdated knowledge and hallucinations. Retrieval-Augmented Generation (RAG) models address this by enhancing LLMs with external knowledge, but often fail to personalize the retrieval process. This paper introduces PersonaRAG, a novel framework incorporating user-centric agents to adapt retrieval and generation based on real-time user data and interactions. Evaluated across various question answering datasets, PersonaRAG demonstrates superiority over baseline models, providing tailored answers to user needs. The results suggest promising directions for user-adapted information retrieval systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/821e7c70e6637f07ab94a843c0de273f8618763b.pdf",
        "venue": "IR-RAG@SIGIR",
        "citationCount": 0,
        "score": 0
    },
    "a1f3aac8462a709a7c73484699f513a92f443927.pdf": {
        "title": "Transforming Healthcare Education: Harnessing Large Language Models for Frontline Health Worker Capacity Building using Retrieval-Augmented Generation",
        "authors": [
            "Yasmina Al Ghadban",
            "Yvonne Lu",
            "Uday Adavi",
            "Ankita Sharma",
            "Sridevi Gara",
            "Neelanjana Das",
            "Bhaskar Kumar",
            "Renu John",
            "Praveen Devarsetty",
            "Jane E. Hirst"
        ],
        "published_date": "2023",
        "abstract": "In recent years, large language models (LLMs) have emerged as a transformative force in several domains, including medical education and healthcare. This paper presents a case study on the practical application of using retrieval-augmented generation (RAG) based models for enhancing healthcare education in low- and middle-income countries. The model described in this paper, SMARThealth GPT, stems from the necessity for accessible and locally relevant medical information to aid community health workers in delivering high-quality maternal care. We describe the development process of the complete RAG pipeline, including the creation of a knowledge base of Indian pregnancy-related guidelines, knowledge embedding retrieval, parameter selection and optimization, and answer generation. This case study highlights the potential of LLMs in building frontline healthcare worker capacity and enhancing guideline-based health education; and offers insights for similar applications in resource-limited settings. It serves as a reference for machine learning scientists, educators, healthcare professionals, and policymakers aiming to harness the power of LLMs for substantial educational improvement.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/a1f3aac8462a709a7c73484699f513a92f443927.pdf",
        "venue": "medRxiv",
        "citationCount": 0,
        "score": 0
    },
    "0406e1397b57448cfadba25222d1d8664c45c53a.pdf": {
        "title": "SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model",
        "authors": [
            "Xun Liang",
            "Simin Niu",
            "Zhiyu Li",
            "Sensen Zhang",
            "Hanyu Wang",
            "Feiyu Xiong",
            "Jason Zhaoxin Fan",
            "Bo Tang",
            "Shichao Song",
            "Mengwei Wang",
            "Jiawei Yang"
        ],
        "published_date": "2025",
        "abstract": "The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledge-intensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulnerability of LLMs because attackers can perform attack tasks by manipulating knowledge. In this paper, we introduce a benchmark named SafeRAG designed to evaluate the RAG security. First, we classify attack tasks into silver noise, inter-context conflict, soft ad, and white Denial-of-Service. Next, we construct RAG security evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We then utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter. Experiments conducted on 14 representative RAG components demonstrate that RAG exhibits significant vulnerability to all attack tasks and even the most apparent attack task can easily bypass existing retrievers, filters, or advanced LLMs, resulting in the degradation of RAG service quality. Code is available at: https://github.com/IAAR-Shanghai/SafeRAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/0406e1397b57448cfadba25222d1d8664c45c53a.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "1dbc0e65604e34d87e6ffda47f35b6fc792af10b.pdf": {
        "title": "Accelerating Retrieval-Augmented Generation",
        "authors": [
            "Derrick Quinn",
            "Mohammad Nouri",
            "Neel Patel",
            "John Salihu",
            "Alireza Salemi",
            "Sukhan Lee",
            "Hamed Zamani",
            "Mohammad Alian"
        ],
        "published_date": "2024",
        "abstract": "An evolving solution to address hallucination and enhance accuracy in large language models (LLMs) is Retrieval-Augmented Generation (RAG), which involves augmenting LLMs with information retrieved from an external knowledge source, such as the web. This paper profiles several RAG execution pipelines and demystifies the complex interplay between their retrieval and generation phases. We demonstrate that while exact retrieval schemes are expensive, they can reduce inference time compared to approximate retrieval variants because an exact retrieval model can send a smaller but more accurate list of documents to the generative model while maintaining the same end-to-end accuracy. This observation motivates the acceleration of the exact nearest neighbor search for RAG. In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL device that implements a scale-out near-memory acceleration architecture with a novel cache-coherent interface between the host CPU and near-memory accelerators. IKS offers 13.4--27.9\u00d7 faster exact nearest neighbor search over a 512GB vector database compared with executing the search on Intel Sapphire Rapids CPUs. This higher search performance translates to 1.7--26.3\u00d7 lower end-to-end inference time for representative RAG applications. IKS is inherently a memory expander; its internal DRAM can be disaggregated and used for other applications running on the server to prevent DRAM -- which is the most expensive component in today's servers -- from being stranded.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/1dbc0e65604e34d87e6ffda47f35b6fc792af10b.pdf",
        "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems",
        "citationCount": 0,
        "score": 0
    },
    "b565394952065c37345fb75fb66e84709b6402a3.pdf": {
        "title": "Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge Graphs, and Tensor Factorization",
        "authors": [
            "Ryan Barron",
            "Ves Grantcharov",
            "Selma Wanna",
            "M. Eren",
            "Manish Bhattarai",
            "N. Solovyev",
            "George Tompkins",
            "Charles Nicholas",
            "Kim \u00d8. Rasmussen",
            "Cynthia Matuszek",
            "Boian Alexandrov"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) are pre-trained on large-scale corpora and excel in numerous general natural language processing (NLP) tasks, such as question answering (QA). Despite their advanced language capabilities, when it comes to domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations, knowledge cut-offs, and lack of knowledge attributions. Additionally, fine tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and time consuming process. The retrieval-augmented generation (RAG) process has recently emerged as a method capable of optimization of LLM responses, by referencing them to a predetermined ontology. It was shown that using a Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into account relevant sub-graphs that preserve the information in a structured manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM framework, that integrates RAG with KG and a vector store (VS) that store factual domain specific information. Importantly, to avoid hallucinations in the KG, we build these highly domain-specific KGs and VSs without the use of LLMs, but via NLP, data mining, and nonnegative tensor factorization with automatic model selection. Pairing our RAG with a domain-specific: (i) KG (containing structured information), and (ii) VS (containing unstructured information) enables the development of domain-specific chat-bots that attribute the source of information, mitigate hallucinations, lessen the need for fine-tuning, and excel in highly domain-specific question answering tasks. We pair SMART-SLIC with chain-of-thought prompting agents. The framework is designed to be generalizable to adapt to any specific or specialized domain. In this paper, we demonstrate the question answering capabilities of our framework on a corpus of scientific publications on malware analysis and anomaly detection.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/b565394952065c37345fb75fb66e84709b6402a3.pdf",
        "venue": "International Conference on Machine Learning and Applications",
        "citationCount": 0,
        "score": 0
    },
    "f716a18b462826004899010dfc30947f9c01ef90.pdf": {
        "title": "RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models",
        "authors": [
            "Shiyue Zhang",
            "Mark Dredze",
            "AI Bloomberg",
            "M. Zitnik",
            "Meng Jiang",
            "Mohit Bansal",
            "James Zou",
            "Jian Pei",
            "Jian Liu",
            "Jianfeng Gao",
            "Jiawei Han",
            "Jieyu Zhao",
            "Jiliang Tang",
            "Jindong Wang",
            "Joaquin Vanschoren",
            "John C. Mitchell",
            "Kai Shu",
            "Kaidi Xu",
            "Kai-Wei Chang",
            "Lifang He",
            "Lifu Huang",
            "Michael Backes",
            "Aaron Hurst",
            "Adam Lerer",
            "Adam P. Goucher",
            "Adam Perelman",
            "Aditya Ramesh",
            "Aidan Clark",
            "AJ Os-trow",
            "Akila Welihinda",
            "Alan Hayes",
            "Alec Radford",
            "Alon Jacovi",
            "Andrew Wang",
            "Chris Alberti",
            "Connie Tao",
            "Jon Lipovetz",
            "Kate Olszewska",
            "Lukas Haas",
            "Michelle Liu",
            "Nate Keating",
            "Adam Bloniarz",
            "Carl Saroufim",
            "Corey Fry",
            "Dror Marcus",
            "Doron Kukliansky",
            "Gau-rav Singh Tomar",
            "James Swirhun",
            "J. Xing",
            "Lily Wang",
            "Madhu Gurumurthy",
            "Michael Aaron",
            "Moran Ambar",
            "Rachana Fellinger",
            "Rui Wang",
            "Zizhao Zhang",
            "S. Goldshtein",
            "Dipanjan Das. 2025",
            "Neel Jain",
            "Avi Schwarzschild",
            "Yuxin Wen",
            "Gowthami Somepalli",
            "John Kirchenbauer",
            "Ping-yeh Chiang",
            "Micah Goldblum",
            "Aniruddha Saha",
            "Jonas Geiping",
            "Tom Goldstein. 2023",
            "Jiaming Ji",
            "Mickel Liu",
            "Josef Dai",
            "Xuehai Pan",
            "Chi Zhang",
            "Juntao Dai",
            "Tianyi Qiu",
            "Bo Chen",
            "Borong Zhang",
            "Hantao Lou",
            "Kaile Wang",
            "Ya Duan"
        ],
        "published_date": "2025",
        "abstract": "Efforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming. However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on standard LLMs, which means we know little about how RAG use cases change a model's safety profile. We conduct a detailed comparative analysis of RAG and non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe and change their safety profile. We explore the causes of this change and find that even combinations of safe models with safe documents can cause unsafe generations. In addition, we evaluate some existing red teaming methods for RAG settings and show that they are less effective than when used for non-RAG settings. Our work highlights the need for safety research and red-teaming methods specifically tailored for RAG LLMs.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/f716a18b462826004899010dfc30947f9c01ef90.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "43d35e1c866bbcfa3c573d69df217c35fcec27b2.pdf": {
        "title": "Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi Layered Thoughts",
        "authors": [
            "Chunjing Gan",
            "Dan Yang",
            "Binbin Hu",
            "Hanxiao Zhang",
            "Siyuan Li",
            "Ziqi Liu",
            "Yue Shen",
            "Lin Ju",
            "Zhiqiang Zhang",
            "Jinjie Gu",
            "Lei Liang",
            "Jun Zhou"
        ],
        "published_date": "2024",
        "abstract": "In recent years, large language models (LLMs) have made remarkable achievements in various domains. However, the untimeliness and cost of knowledge updates coupled with hallucination issues of LLMs have curtailed their applications in knowledge intensive tasks, where retrieval augmented generation (RAG) can be of help. Nevertheless, existing retrieval augmented models typically use similarity as a bridge between queries and documents and follow a retrieve then read procedure. In this work, we argue that similarity is not always the panacea and totally relying on similarity would sometimes degrade the performance of retrieval augmented generation. To this end, we propose MetRag, a Multi layEred Thoughts enhanced Retrieval Augmented Generation framework. To begin with, beyond existing similarity oriented thought, we embrace a small scale utility model that draws supervision from an LLM for utility oriented thought and further come up with a smarter model by comprehensively combining the similarity and utility oriented thoughts. Furthermore, given the fact that the retrieved document set tends to be huge and using them in isolation makes it difficult to capture the commonalities and characteristics among them, we propose to make an LLM as a task adaptive summarizer to endow retrieval augmented generation with compactness-oriented thought. Finally, with multi layered thoughts from the precedent stages, an LLM is called for knowledge augmented generation. Extensive experiments on knowledge-intensive tasks have demonstrated the superiority of MetRag.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/43d35e1c866bbcfa3c573d69df217c35fcec27b2.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "3fb916e2d701680cca142167496aeca7b9ec3dd3.pdf": {
        "title": "RichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation",
        "authors": [
            "Shuting Wang",
            "Xin Xu",
            "Mang Wang",
            "Weipeng Chen",
            "Yutao Zhu",
            "Zhicheng Dou"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) effectively addresses issues of static knowledge and hallucination in large language models. Existing studies mostly focus on question scenarios with clear user intents and concise answers. However, it is prevalent that users issue broad, open-ended queries with diverse sub-intents, for which they desire rich and long-form answers covering multiple relevant aspects. To tackle this important yet underexplored problem, we propose a novel RAG framework, namely RichRAG. It includes a sub-aspect explorer to identify potential sub-aspects of input questions, a multi-faceted retriever to build a candidate pool of diverse external documents related to these sub-aspects, and a generative list-wise ranker, which is a key module to provide the top-k most valuable documents for the final generator. These ranked documents sufficiently cover various query aspects and are aware of the generator's preferences, hence incentivizing it to produce rich and comprehensive responses for users. The training of our ranker involves a supervised fine-tuning stage to ensure the basic coverage of documents, and a reinforcement learning stage to align downstream LLM's preferences to the ranking of documents. Experimental results on two publicly available datasets prove that our framework effectively and efficiently provides comprehensive and satisfying responses to users.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/3fb916e2d701680cca142167496aeca7b9ec3dd3.pdf",
        "venue": "International Conference on Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "99b364eaf55295f53f6afaf6fce7c61dcd567eb9.pdf": {
        "title": "Generating Is Believing: Membership Inference Attacks against Retrieval-Augmented Generation",
        "authors": [
            "Yuying Li",
            "Gaoyang Liu",
            "Chen Wang",
            "Yang Yang"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that mitigates issues such as hallucinations and knowledge staleness in Large Language Models (LLMs) by retrieving relevant knowledge from an external database to assist in content generation. Existing research has demonstrated potential privacy risks associated with the LLMs of RAG. However, the privacy risks posed by the integration of an external database, which often contains sensitive data such as medical records or personal identities, have remained largely unexplored. In this paper, we aim to bridge this gap by focusing on membership privacy of RAG's external database, with the aim of determining whether a given sample is part of the RAG's database. Our basic idea is that if a sample is in the external database, it will exhibit a high degree of semantic similarity to the text generated by the RAG system. We present S$^2$MIA, a \\underline{M}embership \\underline{I}nference \\underline{A}ttack that utilizes the \\underline{S}emantic \\underline{S}imilarity between a given sample and the content generated by the RAG system. With our proposed S$^2$MIA, we demonstrate the potential to breach the membership privacy of the RAG database. Extensive experiment results demonstrate that S$^2$MIA can achieve a strong inference performance compared with five existing MIAs, and is able to escape from the protection of three representative defenses.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/99b364eaf55295f53f6afaf6fce7c61dcd567eb9.pdf",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "citationCount": 0,
        "score": 0
    },
    "918fb17504fe62438e40c3340669ea53c202be04.pdf": {
        "title": "DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented Generation for Question-Answering",
        "authors": [
            "Zijian Hei",
            "Weiling Liu",
            "Wenjie Ou",
            "Juyi Qiao",
            "Junming Jiao",
            "Guowen Song",
            "Ting Tian",
            "Yi Lin"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) has recently demonstrated the performance of Large Language Models (LLMs) in the knowledge-intensive tasks such as Question-Answering (QA). RAG expands the query context by incorporating external knowledge bases to enhance the response accuracy. However, it would be inefficient to access LLMs multiple times for each query and unreliable to retrieve all the relevant documents by a single query. We have found that even though there is low relevance between some critical documents and query, it is possible to retrieve the remaining documents by combining parts of the documents with the query. To mine the relevance, a two-stage retrieval framework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is proposed to improve document retrieval recall and the accuracy of answers while maintaining efficiency. Additionally, a compact classifier is applied to two different selection strategies to determine the contribution of the retrieved documents to answering the query and retrieve the relatively relevant documents. Meanwhile, DR-RAG call the LLMs only once, which significantly improves the efficiency of the experiment. The experimental results on multi-hop QA datasets show that DR-RAG can significantly improve the accuracy of the answers and achieve new progress in QA systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/918fb17504fe62438e40c3340669ea53c202be04.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0
    },
    "6ea7b51bc1c2865d6af95d93df18687a8de16c7a.pdf": {
        "title": "Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs",
        "authors": [
            "Zheng Wang",
            "Zhongyang Li",
            "Zeren Jiang",
            "Dandan Tu",
            "Wei Shi"
        ],
        "published_date": "2024",
        "abstract": "In the age of mobile internet, user data, often referred to as memories, is continuously generated on personal devices. Effectively managing and utilizing this data to deliver services to users is a compelling research topic. In this paper, we introduce a novel task of crafting personalized agents powered by large language models (LLMs), which utilize a user\u2019s smartphone memories to enhance downstream applications with advanced LLM capabilities. To achieve this goal, we introduce EMG-RAG, a solution that combines Retrieval-Augmented Generation (RAG) techniques with an Editable Memory Graph (EMG). This approach is further optimized using Reinforcement Learning to address three distinct challenges: data collection, editability, and selectability. Extensive experiments on a real-world dataset validate the effectiveness of EMG-RAG, achieving an improvement of approximately 10% over the best existing approach. Additionally, the personalized agents have been transferred into a real smartphone AI assistant, which leads to enhanced usability.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/6ea7b51bc1c2865d6af95d93df18687a8de16c7a.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0
    },
    "66a3bc8d77c5e1883ab293c8398872e982b5fbe2.pdf": {
        "title": "Adaptive Retrieval-Augmented Generation for Conversational Systems",
        "authors": [
            "Xi Wang",
            "Procheta Sen",
            "Ruizhe Li",
            "Emine Yilmaz"
        ],
        "published_date": "2024",
        "abstract": "Despite the success of integrating large language models into the development of conversational systems, many studies have shown the effectiveness of retrieving and augmenting external knowledge for informative responses. Hence, many existing studies commonly assume the always need for Retrieval Augmented Generation (RAG) in a conversational system without explicit control. This raises a research question about such a necessity. In this study, we propose to investigate the need for each turn of system response to be augmented with external knowledge. In particular, by leveraging human judgements on the binary choice of adaptive augmentation, we develop RAGate, a gating model, which models conversation context and relevant inputs to predict if a conversational system requires RAG for improved responses. We conduct extensive experiments on devising and applying RAGate to conversational models and well-rounded analyses of different conversational scenarios. Our experimental results and analysis indicate the effective application of RAGate in RAG-based conversational systems in identifying system responses for appropriate RAG with high-quality responses and a high generation confidence. This study also identifies the correlation between the generation's confidence level and the relevance of the augmented knowledge.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/66a3bc8d77c5e1883ab293c8398872e982b5fbe2.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "3e59c8f47b211bf82a1b0fa886fa399ec25044c7.pdf": {
        "title": "Emergency Patient Triage Improvement through a Retrieval-Augmented Generation Enhanced Large-Scale Language Model",
        "authors": [
            "Megumi Yazaki",
            "S. Maki",
            "T. Furuya",
            "Ken Inoue",
            "Ko Nagai",
            "Yuki Nagashima",
            "Juntaro Maruyama",
            "Yasunori Toki",
            "Kyota Kitagawa",
            "Shuhei Iwata",
            "Takaki Kitamura",
            "Sho Gushiken",
            "Yuji Noguchi",
            "M. Inoue",
            "Yasuhiro Shiga",
            "K. Inage",
            "S. Orita",
            "Taka-aki Nakada",
            "S. Ohtori"
        ],
        "published_date": "2024",
        "abstract": "Abstract Objectives Emergency medical triage is crucial for prioritizing patient care in emergency situations, yet its effectiveness can vary significantly based on the experience and training of the personnel involved. This study aims to evaluate the efficacy of integrating Retrieval Augmented Generation (RAG) with Large Language Models (LLMs), specifically OpenAI's GPT models, to standardize triage procedures and reduce variability in emergency care. Methods We created 100 simulated triage scenarios based on modified cases from the Japanese National Examination for Emergency Medical Technicians. These scenarios were processed by the RAG-enhanced LLMs, and the models were given patient vital signs, symptoms, and observations from emergency medical services (EMS) teams as inputs. The primary outcome was the accuracy of triage classifications, which was used to compare the performance of the RAG-enhanced LLMs with that of emergency medical technicians and emergency physicians. Secondary outcomes included the rates of under-triage and over-triage. Results The Generative Pre-trained Transformer 3.5 (GPT-3.5) with RAG model achieved a correct triage rate of 70%, significantly outperforming Emergency Medical Technicians (EMTs) with 35% and 38% correct rates, and emergency physicians with 50% and 47% correct rates (p\u2009<\u20090.05). Additionally, this model demonstrated a substantial reduction in under-triage rates to 8%, compared with 33% for GPT-3.5 without RAG, and 39% for GPT-4 without RAG. Conclusions The integration of RAG with LLMs shows promise in improving the accuracy and consistency of medical assessments in emergency settings. Further validation in diverse medical settings with broader datasets is necessary to confirm the effectiveness and adaptability of these technologies in live environments.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/3e59c8f47b211bf82a1b0fa886fa399ec25044c7.pdf",
        "venue": "Prehospital Emergency Care",
        "citationCount": 0,
        "score": 0
    },
    "42d1dfab4a35583cac1e522a652800f0093285ff.pdf": {
        "title": "Enhancing Large Language Model Reliability: Minimizing Hallucinations with Dual Retrieval-Augmented Generation Based on the Latest Diabetes Guidelines",
        "authors": [
            "Jaedong Lee",
            "H. Cha",
            "Y. Hwangbo",
            "W. Cheon"
        ],
        "published_date": "2024",
        "abstract": "Background/Objectives: Large language models (LLMs) show promise in healthcare but face challenges with hallucinations, particularly in rapidly evolving fields like diabetes management. Traditional LLM updating methods are resource-intensive, necessitating new approaches for delivering reliable, current medical information. This study aimed to develop and evaluate a novel retrieval system to enhance LLM reliability in diabetes management across different languages and guidelines. Methods: We developed a dual retrieval-augmented generation (RAG) system integrating both Korean Diabetes Association and American Diabetes Association 2023 guidelines. The system employed dense retrieval with 11 embedding models (including OpenAI, Upstage, and multilingual models) and sparse retrieval using BM25 algorithm with language-specific tokenizers. Performance was evaluated across different top-k values, leading to optimized ensemble retrievers for each guideline. Results: For dense retrievers, Upstage\u2019s Solar Embedding-1-large and OpenAI\u2019s text-embedding-3-large showed superior performance for Korean and English guidelines, respectively. Multilingual models outperformed language-specific models in both cases. For sparse retrievers, the ko_kiwi tokenizer demonstrated superior performance for Korean text, while both ko_kiwi and porter_stemmer showed comparable effectiveness for English text. The ensemble retrievers, combining optimal dense and sparse configurations, demonstrated enhanced coverage while maintaining precision. Conclusions: This study presents an effective dual RAG system that enhances LLM reliability in diabetes management across different languages. The successful implementation with both Korean and American guidelines demonstrates the system\u2019s cross-regional capability, laying a foundation for more trustworthy AI-assisted healthcare applications.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/42d1dfab4a35583cac1e522a652800f0093285ff.pdf",
        "venue": "Journal of Personalized Medicine",
        "citationCount": 0,
        "score": 0
    },
    "4440c1d44c449f4b4ea9273dd667b5c036e6b8c6.pdf": {
        "title": "Towards Omni-RAG: Comprehensive Retrieval-Augmented Generation for Large Language Models in Medical Applications",
        "authors": [
            "Zhe Chen",
            "Yusheng Liao",
            "Shuyang Jiang",
            "Pingjie Wang",
            "Yiqiu Guo",
            "Yanfeng Wang",
            "Yu Wang"
        ],
        "published_date": "2025",
        "abstract": "Large language models hold promise for addressing medical challenges, such as medical diagnosis reasoning, research knowledge acquisition, clinical decision-making, and consumer health inquiry support. However, they often generate hallucinations due to limited medical knowledge. Incorporating external knowledge is therefore critical, which necessitates multi-source knowledge acquisition. We address this challenge by framing it as a source planning problem, which is to formulate context-appropriate queries tailored to the attributes of diverse sources. Existing approaches either overlook source planning or fail to achieve it effectively due to misalignment between the model's expectation of the sources and their actual content. To bridge this gap, we present MedOmniKB, a repository comprising multigenre and multi-structured medical knowledge sources. Leveraging these sources, we propose the Source Planning Optimisation method, which enhances multi-source utilisation. Our approach involves enabling an expert model to explore and evaluate potential plans while training a smaller model to learn source alignment. Experimental results demonstrate that our method substantially improves multi-source planning performance, enabling the optimised small model to achieve state-of-the-art results in leveraging diverse medical knowledge sources.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/4440c1d44c449f4b4ea9273dd667b5c036e6b8c6.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "4da5c68bea931480d6abb288639cf412f7719e5f.pdf": {
        "title": "Dual retrieving and ranking medical large language model with retrieval augmented generation",
        "authors": [
            "Qimin Yang",
            "Huan Zuo",
            "Runqi Su",
            "Hanyinghong Su",
            "Tangyi Zeng",
            "Huimei Zhou",
            "Rongsheng Wang",
            "Jiexin Chen",
            "Yijun Lin",
            "Zhiyi Chen",
            "Tao Tan"
        ],
        "published_date": "2025",
        "abstract": "Recent advancements in large language models (LLMs) have significantly enhanced text generation across various sectors; however, their medical application faces critical challenges regarding both accuracy and real-time responsiveness. To address these dual challenges, we propose a novel two-step retrieval and ranking retrieval-augmented generation (RAG) framework that synergistically combines embedding search with Elasticsearch technology. Built upon a dynamically updated medical knowledge base incorporating expert-reviewed documents from leading healthcare institutions, our hybrid architecture employs ColBERTv2 for context-aware result ranking while maintaining computational efficiency. Experimental results show a 10% improvement in accuracy for complex medical queries compared to standalone LLM and single-search RAG variants, while acknowledging that latency challenges remain in emergency situations requiring sub-second responses in an experimental setting, which can be achieved in real-time using more powerful hardware in real-world deployments. This work establishes a new paradigm for reliable medical AI assistants that successfully balances accuracy and practical deployment considerations.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/4da5c68bea931480d6abb288639cf412f7719e5f.pdf",
        "venue": "Scientific Reports",
        "citationCount": 0,
        "score": 0
    },
    "9b52afc58ea4326642970e75b8b10d6a97090900.pdf": {
        "title": "Evaluation of the integration of retrieval-augmented generation in large language model for breast cancer nursing care responses",
        "authors": [
            "Ruiyu Xu",
            "Ying Hong",
            "Feifei Zhang",
            "Hongmei Xu"
        ],
        "published_date": "2024",
        "abstract": "Breast cancer is one of the most common malignant tumors in women worldwide. Although large language models (LLMs) can provide breast cancer nursing care consultation, inherent hallucinations can lead to inaccurate responses. Retrieval-augmented generation (RAG) technology can improve LLM performance, offering a new approach for clinical applications. In the present study, we evaluated the performance of a LLM in breast cancer nursing care using RAG technology. In the control group (GPT-4), questions were answered directly using the GPT-4 model, whereas the experimental group (RAG-GPT) used the GPT-4 model combined with RAG. A knowledge base for breast cancer nursing was created for the RAG-GPT group, and 15 of 200 real-world clinical care questions were answered randomly. The primary endpoint was overall satisfaction, and the secondary endpoints were accuracy and empathy. RAG-GPT included a curated knowledge base related to breast cancer nursing care, including textbooks, guidelines, and traditional Chinese therapy. The RAG-GPT group showed significantly higher overall satisfaction than that of the GPT-4 group (8.4\u2009\u00b1\u20090.84 vs. 5.4\u2009\u00b1\u20091.27, p\u2009<\u20090.01) as well as an improved accuracy of responses (8.6\u2009\u00b1\u20090.69 vs. 5.6\u2009\u00b1\u20090.96, p\u2009<\u20090.01). However, there was no inter-group difference in empathy (8.4\u2009\u00b1\u20090.85 vs. 7.8\u2009\u00b1\u20091.22, p\u2009>\u20090.05). Overall, this study revealed that RAG technology could improve LLM performance significantly, likely because of the increased accuracy of the answers without diminishing empathy. These findings provide a theoretical basis for applying RAG technology to LLMs in clinical nursing practice and education. Supplementary Information The online version contains supplementary material available at 10.1038/s41598-024-81052-3.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/9b52afc58ea4326642970e75b8b10d6a97090900.pdf",
        "venue": "Scientific Reports",
        "citationCount": 0,
        "score": 0
    },
    "503c5cb6bfb451e38c12e5c1ba41ccf844e79fa8.pdf": {
        "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
        "authors": [
            "Yutao Zhu",
            "Zhaoheng Huang",
            "Zhicheng Dou",
            "Ji-Rong Wen"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capabilities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across 12 question-answering tasks demonstrate the superiority of our approach.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/503c5cb6bfb451e38c12e5c1ba41ccf844e79fa8.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0
    },
    "6fff65981e79981e47ab219fd12ccc824d47d6ce.pdf": {
        "title": "Adaptive Control of Retrieval-Augmented Generation for Large Language Models Through Reflective Tags",
        "authors": [
            "Chengyuan Yao",
            "Satoshi Fujita"
        ],
        "published_date": "2024",
        "abstract": "While retrieval-augmented generation (RAG) enhances large language models (LLMs), it also introduces challenges that can impact accuracy and performance. In practice, RAG can obscure the intrinsic strengths of LLMs. Firstly, LLMs may become too reliant on external retrieval, underutilizing their own knowledge and reasoning, which can diminish responsiveness. Secondly, RAG may introduce irrelevant or low-quality data, adding noise that disrupts generation, especially with complex tasks. This paper proposes an RAG framework that uses reflective tags to manage retrieval, evaluating documents in parallel and applying the chain-of-thought (CoT) technique for step-by-step generation. The model selects the highest quality content for final output. The key contributions are as follows: (1) reducing hallucinations by focusing on high-scoring documents; (2) improving real-time performance through efficient retrieval; and (3) mitigating negative effects by filtering out irrelevant information using parallel generation and reflective tagging. These innovations aim to optimize RAG for more reliable, high-quality results.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/6fff65981e79981e47ab219fd12ccc824d47d6ce.pdf",
        "venue": "Electronics",
        "citationCount": 0,
        "score": 0
    },
    "2795358f23f1485f71693245576d1fd57f3134b2.pdf": {
        "title": "Advancing TTP Analysis: Harnessing the Power of Large Language Models with Retrieval Augmented Generation",
        "authors": [
            "Reza Fayyazi",
            "Rozhina Taghdimi",
            "S. Yang"
        ],
        "published_date": "2023",
        "abstract": "Tactics, Techniques, and Procedures (TTPs) outline the methods attackers use to exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT&CK framework can be challenging for cybersecurity practitioners due to presumed expertise and complex dependencies. Meanwhile, advancements with Large Language Models (LLMs) have led to recent surge in studies exploring its uses in cybersecurity operations. It is, however, unclear how LLMs can be used in an efficient and proper way to provide accurate responses for critical domains such as cybersecurity. This leads us to investigate how to better use two types of LLMs: small-scale encoder-only (e.g., RoBERTa) and large-scale decoder-only (e.g., GPT-3.5) LLMs to comprehend and summarize TTPs with the intended purposes (i.e., tactics) of a cyberattack procedure. This work studies and compares the uses of Supervised Fine-Tuning (SFT) of encoder-only LLMs vs. Retrieval Augmented Generation (RAG) for decoder-only LLMs (without fine-tuning). Both SFT and RAG techniques presumably enhance the LLMs with relevant contexts for each cyberattack procedure. Our studies show decoder-only LLMs with RAG achieves better performance than encoder-only models with SFT, particularly when directly relevant context is extracted by RAG. The decoder-only results could suffer low \u2018Precision\u2019 while achieving high \u2018Recall\u2019, indicating the hallucinations typically occur during decoding phase. Our findings further highlight a counter-intuitive observation that more generic prompts tend to yield better predictions of cyberattack tactics than those that are more specifically tailored. 1",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/2795358f23f1485f71693245576d1fd57f3134b2.pdf",
        "venue": "2024 Annual Computer Security Applications Conference Workshops (ACSAC Workshops)",
        "citationCount": 0,
        "score": 0
    },
    "ed16f6feda941164e6370638ebd98b81c13d2c4b.pdf": {
        "title": "RAG-Ex: A Generic Framework for Explaining Retrieval Augmented Generation",
        "authors": [
            "Viju Sudhi",
            "Sinchana Ramakanth Bhat",
            "Max Rudat",
            "Roman Teucher"
        ],
        "published_date": "2024",
        "abstract": "Owing to their size and complexity, large language models (LLMs) hardly explain why they generate a response. This effectively reduces the trust and confidence of end users in LLM-based applications, including Retrieval Augmented Generation (RAG) for Question Answering (QA) tasks. In this work, we introduce RAG-Ex, a model- and language-agnostic explanation framework that presents approximate explanations to the users revealing why the LLMs possibly generated a piece of text as a response, given the user input. Our framework is compatible with both open-source and proprietary LLMs. We report the significance scores of the approximated explanations from our generic explainer in both English and German QA tasks and also study their correlation with the downstream performance of LLMs. In the extensive user studies, our explainer yields an F1-score of 76.9% against the end user annotations and attains almost on-par performance with model-intrinsic approaches.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/ed16f6feda941164e6370638ebd98b81c13d2c4b.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 0,
        "score": 0
    },
    "3ed128b6f0e08294fd9cb413eac96b4319481c67.pdf": {
        "title": "Knowledge Graph Retrieval-Augmented Generation for LLM-based Recommendation",
        "authors": [
            "Shijie Wang",
            "Wenqi Fan",
            "Yue Feng",
            "Xinyu Ma",
            "Shuaiqiang Wang",
            "Dawei Yin"
        ],
        "published_date": "2025",
        "abstract": "Recommender systems have become increasingly vital in our daily lives, helping to alleviate the problem of information overload across various user-oriented online services. The emergence of Large Language Models (LLMs) has yielded remarkable achievements, demonstrating their potential for the development of next-generation recommender systems. Despite these advancements, LLM-based recommender systems face inherent limitations stemming from their LLM backbones, particularly issues of hallucinations and the lack of up-to-date and domain-specific knowledge. Recently, Retrieval-Augmented Generation (RAG) has garnered significant attention for addressing these limitations by leveraging external knowledge sources to enhance the understanding and generation of LLMs. However, vanilla RAG methods often introduce noise and neglect structural relationships in knowledge, limiting their effectiveness in LLM-based recommendations. To address these limitations, we propose to retrieve high-quality and up-to-date structure information from the knowledge graph (KG) to augment recommendations. Specifically, our approach develops a retrieval-augmented framework, termed K-RagRec, that facilitates the recommendation generation process by incorporating structure information from the external KG. Extensive experiments have been conducted to demonstrate the effectiveness of our proposed method.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/3ed128b6f0e08294fd9cb413eac96b4319481c67.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "403bd2292154cf84bfaebe440ebd642b623839f1.pdf": {
        "title": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization",
        "authors": [
            "Yang Jiao",
            "Xiaodong Wang",
            "Kai Yang"
        ],
        "published_date": "2025",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of applications, e.g., medical question-answering, mathematical sciences, and code generation. However, they also exhibit inherent limitations, such as outdated knowledge and susceptibility to hallucinations. Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to address these issues, but it also introduces new vulnerabilities. Recent efforts have focused on the security of RAG-based LLMs, yet existing attack methods face three critical challenges: (1) their effectiveness declines sharply when only a limited number of poisoned texts can be injected into the knowledge database, (2) they lack sufficient stealth, as the attacks are often detectable by anomaly detection systems, which compromises their effectiveness, and (3) they rely on heuristic approaches to generate poisoned texts, lacking formal optimization frameworks and theoretic guarantees, which limits their effectiveness and applicability. To address these issues, we propose coordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack that introduces a small number of poisoned texts into the knowledge database while embedding a backdoor trigger within the prompt. When activated, the trigger causes the LLM to generate pre-designed responses to targeted queries, while maintaining normal behavior in other contexts. This ensures both high effectiveness and stealth. We formulate the attack generation process as a bilevel optimization problem leveraging a principled optimization framework to develop optimal poisoned texts and triggers. Extensive experiments across diverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving a high attack success rate even with a limited number of poisoned texts and significantly improved stealth compared to existing methods.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/403bd2292154cf84bfaebe440ebd642b623839f1.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 0,
        "score": 0
    },
    "68e34d51ee49b562269dd7e6a325ec6ddaa9aa8d.pdf": {
        "title": "Detecting emergencies in patient portal messages using large language models and knowledge graph-based retrieval-augmented generation",
        "authors": [
            "Siru Liu",
            "A. Wright",
            "Allison B. McCoy",
            "Sean S. Huang",
            "Bryan D. Steitz",
            "Adam Wright"
        ],
        "published_date": "2025",
        "abstract": "Abstract Objectives This study aims to develop and evaluate an approach using large language models (LLMs) and a knowledge graph to triage patient messages that need emergency care. The goal is to notify patients when their messages indicate an emergency, guiding them to seek immediate help rather than using the patient portal, to improve patient safety. Materials and Methods We selected 1020 messages sent to Vanderbilt University Medical Center providers between January 1, 2022 and March 7, 2023. We developed four models to triage these messages for emergencies: (1) Prompt-Only: the patient message was input with a prompt directly into the LLM; (2) Na\u00efve Retrieval Augmented Generation (RAG): provided retrieved information as context to the LLM; (3) RAG from Knowledge Graph with Local Search: a knowledge graph was used to retrieve locally relevant information based on semantic similarities; (4) RAG from Knowledge Graph with Global Search: a knowledge graph was used to retrieve globally relevant information through hierarchical community detection. The knowledge base was a triage book covering 225 protocols. Results The RAG from Knowledge Graph model with global search outperformed other models, achieving an accuracy of 0.99, a sensitivity of 0.98, and a specificity of 0.99. It demonstrated significant improvements in triaging emergency messages compared to LLM without RAG and na\u00efve RAG. Discussion The traditional LLM without any retrieval mechanism underperformed compared to models with RAG, which aligns with the expected benefits of augmenting LLMs with domain-specific knowledge sources. Our results suggest that providing external knowledge, especially in a structured manner and in community summaries, can improve LLM performance in triaging patient portal messages. Conclusion LLMs can effectively assist in triaging emergency patient messages after integrating with a knowledge graph about a nurse triage book. Future research should focus on expanding the knowledge graph and deploying the system to evaluate its impact on patient outcomes.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/68e34d51ee49b562269dd7e6a325ec6ddaa9aa8d.pdf",
        "venue": "J. Am. Medical Informatics Assoc.",
        "citationCount": 0,
        "score": 0
    },
    "ce3f2260a73e602516c6aa51678bc5384cafadce.pdf": {
        "title": "Judge as A Judge: Improving the Evaluation of Retrieval-Augmented Generation through the Judge-Consistency of Large Language Models",
        "authors": [
            "Shuliang Liu",
            "Xinze Li",
            "Zhenghao Liu",
            "Yukun Yan",
            "Cheng Yang",
            "Zheni Zeng",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Ge Yu"
        ],
        "published_date": "2025",
        "abstract": "Retrieval-Augmented Generation (RAG) has proven its effectiveness in alleviating hallucinations for Large Language Models (LLMs). However, existing automated evaluation metrics cannot fairly evaluate the outputs generated by RAG models during training and evaluation. LLM-based judgment models provide the potential to produce high-quality judgments, but they are highly sensitive to evaluation prompts, leading to inconsistencies when judging the output of RAG models. This paper introduces the Judge-Consistency (ConsJudge) method, which aims to enhance LLMs to generate more accurate evaluations for RAG models. Specifically, ConsJudge prompts LLMs to generate different judgments based on various combinations of judgment dimensions, utilize the judge-consistency to evaluate these judgments and select the accepted and rejected judgments for DPO training. Our experiments show that ConsJudge can effectively provide more accurate judgments for optimizing RAG models across various RAG models and datasets. Further analysis reveals that judgments generated by ConsJudge have a high agreement with the superior LLM. All codes are available at https://github.com/OpenBMB/ConsJudge.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/ce3f2260a73e602516c6aa51678bc5384cafadce.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0
    },
    "938908bc82d5c37b1df2c76d4fbdbdf2075e50de.pdf": {
        "title": "Advancing Question-Answering in Ophthalmology with Retrieval Augmented Generations (RAG): Benchmarking Open-source and Proprietary Large Language Models",
        "authors": [
            "Quang Nguyen",
            "Duy-Anh Nguyen",
            "Khang Dang",
            "Siyin Liu",
            "Khai Nguyen",
            "Sophia Y. Wang",
            "W. Woof",
            "Peter Thomas",
            "Praveen J Patel",
            "Konstantinos Balaskas",
            "Johan H Thygesen",
            "Honghan Wu",
            "N. Pontikos"
        ],
        "published_date": "2024",
        "abstract": "Purpose To evaluate the application of Retrieval-Augmented Generation (RAG), a technique that combines information retrieval with text generation, to benchmark the performance of open-source and proprietary generative large language models (LLMs) in medical question-answering tasks within the ophthalmology domain. Methods Our dataset comprised 260 multiple-choice questions sourced from two question-answer banks designed to assess ophthalmic knowledge: the American Academy of Ophthalmology's Basic and Clinical Science Course (BCSC) Self-Assessment program and OphthoQuestions. Our RAG pipeline involved initial retrieval of documents in the BCSC companion textbook using ChromaDB, followed by reranking with Cohere to refine the context provided to the LLMs. We benchmarked four models, including GPT-4 and three open-source models (Llama-3-70B, Gemma-2-27B, and Mixtral-8x7B, all under 4-bit quantization), under three settings: zero-shot, zero-shot with Chain-of-Thought and RAG. Model performance was evaluated using accuracy on the two datasets. Quantization was applied to improve the efficiency of the open-source models. Effects of quantization level was also measured. Results Using RAG, GPT-4-turbo' s accuracy increased from 80.38% to 91.92% on BCSC and from 77.69% to 88.65 % on OphthoQuestions. Importantly, the RAG pipeline greatly enhanced overall performance of Llama-3 from 57.50% to 81.35% (23.85% increase), Gemma-2 62.12% to 79.23% (17.11% increase), and Mixtral-8x7B 52.89% to 75% (22.11% increase). Zero-shot-CoT had overall no significant improvement on the models' performance. Quantization using 4 bit was shown to be as effective as using 8 bits while requiring half the resources.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/938908bc82d5c37b1df2c76d4fbdbdf2075e50de.pdf",
        "venue": "medRxiv",
        "citationCount": 0,
        "score": 0
    },
    "622947f6f70520ffd8579b5ed9bae681096b1b67.pdf": {
        "title": "SelfRewardRAG: Enhancing Medical Reasoning with Retrieval-Augmented Generation and Self-Evaluation in Large Language Models",
        "authors": [
            "Zakaria Hammane",
            "Fatima-Ezzahraa Ben-Bouazza",
            "A. Fennan"
        ],
        "published_date": "2024",
        "abstract": "In this study, we present a pioneering approach known as Retrieval Augmented Generation (RAG), which integrates Large Language Models (LLMs) with dynamic data retrieval to surmount the challenge of knowledge obsolescence, a matter of particular significance in the healthcare domain. This innovative system leverages real-time access to up-to-date clinical records, thereby enabling the generation of precise and informed responses, a notable leap over the conventional limitations faced by LLMs due to their reliance on static datasets. Our methodology embodies the seamless integration of RAG with LLMs to adeptly retrieve pertinent medical information from continuously updated repositories, such as PubMed, and to synthesize this information into accurate responses for medical queries. This advancement marks a considerable enhancement in the application of AI within medical decision-making processes, ensuring that the information provided remains both current and relevant. The effectiveness of our approach is validated through a series of experiments, which demonstrate a significant improvement in the accuracy and timeliness of the AI-generated responses, thereby underscoring its transformative potential for medical AI applications. Furthermore, the foundational principles underlying our system indicate its broader applicability in various other fields confronted with the challenges of rapidly changing knowledge bases. Through this work, we not only address the critical need for real-time information integration in healthcare AI but also establish a paradigm for future AI systems, promoting the incorporation of continuous learning and updating mechanisms to enhance their efficacy and relevance.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/622947f6f70520ffd8579b5ed9bae681096b1b67.pdf",
        "venue": "International Symposium on Computer Vision",
        "citationCount": 0,
        "score": 0
    }
}