{
    "659bf9ce7175e1ec266ff54359e2bd76e0b7ff31.pdf": {
        "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
        "authors": [
            "Patrick Lewis",
            "Ethan Perez",
            "Aleksandara Piktus",
            "F. Petroni",
            "Vladimir Karpukhin",
            "Naman Goyal",
            "Heinrich Kuttler",
            "M. Lewis",
            "Wen-tau Yih",
            "Tim Rockt\u00e4schel",
            "Sebastian Riedel",
            "Douwe Kiela"
        ],
        "published_date": "2020",
        "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/659bf9ce7175e1ec266ff54359e2bd76e0b7ff31.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 8053,
        "score": 1610.6000000000001
    },
    "de549c1592a62c129b8d49c8c0137aa6859b103f.pdf": {
        "title": "Internet-Augmented Dialogue Generation",
        "authors": [
            "M. Komeili",
            "Kurt Shuster",
            "J. Weston"
        ],
        "published_date": "2021",
        "abstract": "The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b).",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/de549c1592a62c129b8d49c8c0137aa6859b103f.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 301,
        "score": 75.25
    },
    "38b0803b59e4973f09018ce942164b02be4b8bc9.pdf": {
        "title": "MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text",
        "authors": [
            "Wenhu Chen",
            "Hexiang Hu",
            "Xi Chen",
            "Pat Verga",
            "William W. Cohen"
        ],
        "published_date": "2022",
        "abstract": "While language Models store a massive amount of world knowledge implicitly in their parameters, even very large models often fail to encode information about rare entities and events, while incurring huge computational costs. Recently, retrieval-augmented models, such as REALM, RAG, and RETRO, have incorporated world knowledge into language generation by leveraging an external non-parametric index and have demonstrated impressive performance with constrained model sizes. However, these methods are restricted to retrieving only textual knowledge, neglecting the ubiquitous amount of knowledge in other modalities like images \u2013 much of which contains information not covered by any text. To address this limitation, we propose the first Multimodal Retrieval-Augmented Transformer (MuRAG), which accesses an external non-parametric multimodal memory to augment language generation. MuRAG is pre-trained with a mixture of large-scale image-text and text-only corpora using a joint contrastive and generative loss. We perform experiments on two different datasets that require retrieving and reasoning over both images and text to answer a given query: WebQA, and MultimodalQA. Our results show that MuRAG achieves state-of-the-art accuracy, outperforming existing models by 10-20% absolute on both datasets and under both distractor and full-wiki settings.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/38b0803b59e4973f09018ce942164b02be4b8bc9.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 191,
        "score": 63.666666666666664
    },
    "1a1e99514d8d175459f7c61cfd0c394b46e63359.pdf": {
        "title": "Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training",
        "authors": [
            "Oshin Agarwal",
            "Heming Ge",
            "Siamak Shakeri",
            "Rami Al-Rfou"
        ],
        "published_date": "2021",
        "abstract": "Prior work on Data-To-Text Generation, the task of converting knowledge graph (KG) triples into natural text, focused on domain-specific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata KG, and discuss the unique challenges associated with a broad, open-domain, large-scale verbalization. We further show that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora. In contrast to the many architectures that have been developed to integrate these two sources, our approach converts the KG into natural text, allowing it to be seamlessly integrated into existing language models. It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model. We evaluate this approach by augmenting the retrieval corpus in a retrieval language model and showing significant improvements on the knowledge intensive tasks of open domain QA and the LAMA knowledge probe.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/1a1e99514d8d175459f7c61cfd0c394b46e63359.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 183,
        "score": 45.75
    },
    "ab8ee8d06ed581c876da3a2e7a5fdb1cbec21a45.pdf": {
        "title": "KAT: A Knowledge Augmented Transformer for Vision-and-Language",
        "authors": [
            "Liangke Gui",
            "Borui Wang",
            "Qiuyuan Huang",
            "A. Hauptmann",
            "Yonatan Bisk",
            "Jianfeng Gao"
        ],
        "published_date": "2021",
        "abstract": "The primary focus of recent work with large-scale transformers has been on optimizing the amount of information packed into the model\u2019s parameters. In this work, we ask a complementary question: Can multimodal transformers leverage explicit knowledge in their reasoning? Existing, primarily unimodal, methods have explored approaches under the paradigm of knowledge retrieval followed by answer prediction, but leave open questions about the quality and relevance of the retrieved knowledge used, and how the reasoning processes over implicit and explicit knowledge should be integrated. To address these challenges, we propose a - Knowledge Augmented Transformer (KAT) - which achieves a strong state-of-the-art result (+6% absolute) on the open-domain multimodal task of OK-VQA. Our approach integrates implicit and explicit knowledge in an encoder-decoder architecture, while still jointly reasoning over both knowledge sources during answer generation. Additionally, explicit knowledge integration improves interpretability of model predictions in our analysis.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/ab8ee8d06ed581c876da3a2e7a5fdb1cbec21a45.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 175,
        "score": 43.75
    },
    "4335230068228b26dda364f2c579c8041fc70cdb.pdf": {
        "title": "Evaluating base and retrieval augmented LLMs with document or online support for evidence based neurology",
        "authors": [
            "L. Masanneck",
            "Sven G. Meuth",
            "M. Pawlitzki"
        ],
        "published_date": "2014",
        "abstract": "Effectively managing evidence-based information is increasingly challenging. This study tested large language models (LLMs), including document- and online-enabled retrieval-augmented generation (RAG) systems, using 13 recent neurology guidelines across 130 questions. Results showed substantial variability. RAG improved accuracy compared to base models but still produced potentially harmful answers. RAG-based systems performed worse on case-based than knowledge-based questions. Further refinement and improved regulation is needed for safe clinical integration of RAG-enhanced LLMs.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/4335230068228b26dda364f2c579c8041fc70cdb.pdf",
        "venue": "The Lancet",
        "citationCount": 310,
        "score": 28.181818181818183
    },
    "ed99a2572fb5f4240aa6068e3bf274832e831306.pdf": {
        "title": "Recitation-Augmented Language Models",
        "authors": [
            "Zhiqing Sun",
            "Xuezhi Wang",
            "Yi Tay",
            "Yiming Yang",
            "Denny Zhou"
        ],
        "published_date": "2022",
        "abstract": "We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs' own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of \\method~on four pre-trained models (PaLM, UL2, OPT, and Codex) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at\"https://github.com/Edward-Sun/RECITE\".",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/ed99a2572fb5f4240aa6068e3bf274832e831306.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 70,
        "score": 23.333333333333332
    },
    "4a21aa3c75c4f29f9b340a73ff24f20834a7b686.pdf": {
        "title": "Retrieval-Augmented Transformer for Image Captioning",
        "authors": [
            "Sara Sarto",
            "Marcella Cornia",
            "L. Baraldi",
            "R. Cucchiara"
        ],
        "published_date": "2022",
        "abstract": "Image captioning models aim at connecting Vision and Language by providing natural language descriptions of input images. In the past few years, the task has been tackled by learning parametric models and proposing visual feature extraction advancements or by modeling better multi-modal connections. In this paper, we investigate the development of an image captioning approach with a kNN memory, with which knowledge can be retrieved from an external corpus to aid the generation process. Our architecture combines a knowledge retriever based on visual similarities, a differentiable encoder, and a kNN-augmented attention layer to predict tokens based on the past context and on text retrieved from the external memory. Experimental results, conducted on the COCO dataset, demonstrate that employing an explicit external memory can aid the generation process and increase caption quality. Our work opens up new avenues for improving image captioning models at larger scale.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/4a21aa3c75c4f29f9b340a73ff24f20834a7b686.pdf",
        "venue": "International Conference on Content-Based Multimedia Indexing",
        "citationCount": 62,
        "score": 20.666666666666664
    },
    "d80241e05947581719bf2839e1621875890a12b0.pdf": {
        "title": "RACE: Retrieval-augmented Commit Message Generation",
        "authors": [
            "Ensheng Shi",
            "Yanlin Wang",
            "Wei Tao",
            "Lun Du",
            "Hongyu Zhang",
            "Shi Han",
            "Dongmei Zhang",
            "Hongbin Sun"
        ],
        "published_date": "2022",
        "abstract": "Commit messages are important for software development and maintenance. Many neural network-based approaches have been proposed and shown promising results on automatic commit message generation. However, the generated commit messages could be repetitive or redundant. In this paper, we propose RACE, a new retrieval-augmented neural commit message generation method, which treats the retrieved similar commit as an exemplar and leverages it to generate an accurate commit message. As the retrieved commit message may not always accurately describe the content/intent of the current code diff, we also propose an exemplar guider, which learns the semantic similarity between the retrieved and current code diff and then guides the generation of commit message based on the similarity. We conduct extensive experiments on a large public dataset with five programming languages. Experimental results show that RACE can outperform all baselines. Furthermore, RACE can boost the performance of existing Seq2Seq models in commit message generation.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/d80241e05947581719bf2839e1621875890a12b0.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 51,
        "score": 17.0
    },
    "9038f40c43e7d62d8f1dc4819093083090911f7a.pdf": {
        "title": "Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning",
        "authors": [
            "Jishnu Ray Chowdhury",
            "Yong Zhuang",
            "Shuyi Wang"
        ],
        "published_date": "2022",
        "abstract": "Paraphrase generation is a fundamental and long-standing task in natural language processing. In this paper, we concentrate on two contributions to the task: (1) we propose Retrieval Augmented Prompt Tuning (RAPT) as a parameter-efficient method to adapt large pre-trained language models for paraphrase generation; (2) we propose Novelty Conditioned RAPT (NC-RAPT) as a simple model-agnostic method of using specialized prompt tokens for controlled paraphrase generation with varying levels of lexical novelty. By conducting extensive experiments on four datasets, we demonstrate the effectiveness of the proposed approaches for retaining the semantic content of the original text while inducing lexical novelty in the generation.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/9038f40c43e7d62d8f1dc4819093083090911f7a.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 40,
        "score": 13.333333333333332
    },
    "003c1005c719d8f5691fd963f4ed8a0b2d50f4f7.pdf": {
        "title": "Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters",
        "authors": [
            "Yan Xu",
            "Etsuko Ishii",
            "Zihan Liu",
            "Genta Indra Winata",
            "Dan Su",
            "Andrea Madotto",
            "Pascale Fung"
        ],
        "published_date": "2021",
        "abstract": "To diversify and enrich generated dialogue responses, knowledge-grounded dialogue has been investigated in recent years. The existing methods tackle the knowledge grounding challenge by retrieving the relevant sentences over a large corpus and augmenting the dialogues with explicit extra information. Despite their success, however, the existing works have drawbacks on the inference efficiency. This paper proposes KnowExpert, an end-to-end framework to bypass the explicit retrieval process and inject knowledge into the pre-trained language models with lightweight adapters and adapt to the knowledge-grounded dialogue task. To the best of our knowledge, this is the first attempt to tackle this challenge without retrieval in this task under an open-domain chit-chat scenario. The experimental results show that KnowExpert performs comparably with some retrieval-based baselines while being time-efficient in inference, demonstrating the effectiveness of our proposed method.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/003c1005c719d8f5691fd963f4ed8a0b2d50f4f7.pdf",
        "venue": "Workshop on Document-grounded Dialogue and Conversational Question Answering",
        "citationCount": 46,
        "score": 11.5
    },
    "d15d96517370c9ed0658d176b979bcf92d1373ea.pdf": {
        "title": "Reason first, then respond: Modular Generation for Knowledge-infused Dialogue",
        "authors": [
            "Leonard Adolphs",
            "Kurt Shuster",
            "Jack Urbanek",
            "Arthur Szlam",
            "J. Weston"
        ],
        "published_date": "2021",
        "abstract": "Large language models can produce fluent dialogue but often hallucinate factual inaccuracies. While retrieval-augmented models help alleviate this issue, they still face a difficult challenge of both reasoning to provide correct knowledge and generating conversation simultaneously. In this work, we propose a modular model, Knowledge to Response (K2R), for incorporating knowledge into conversational agents, which breaks down this problem into two easier steps. K2R first generates a knowledge sequence, given a dialogue context, as an intermediate step. After this\"reasoning step\", the model then attends to its own generated knowledge sequence, as well as the dialogue context, to produce a final response. In detailed experiments, we find that such a model hallucinates less in knowledge-grounded dialogue tasks, and has advantages in terms of interpretability and modularity. In particular, it can be used to fuse QA and dialogue systems together to enable dialogue agents to give knowledgeable answers, or QA models to give conversational responses in a zero-shot setting.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/d15d96517370c9ed0658d176b979bcf92d1373ea.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 41,
        "score": 10.25
    },
    "4989c08930e42d322b3bfed167d7ea434a698f2c.pdf": {
        "title": "CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation",
        "authors": [
            "Tanay Dixit",
            "Bhargavi Paranjape",
            "Hannaneh Hajishirzi",
            "Luke Zettlemoyer"
        ],
        "published_date": "2022",
        "abstract": "Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbed inputs during training -- helps reduce model reliance on spurious correlations and improves generalization to out-of-distribution (OOD) data. Prior work on generating counterfactuals only considered restricted classes of perturbations, limiting their effectiveness. We present COunterfactual Generation via Retrieval and Editing (CORE), a retrieval-augmented generation framework for creating diverse counterfactual perturbations for CDA. For each training example, CORE first performs a dense retrieval over a task-related unlabeled text corpus using a learned bi-encoder and extracts relevant counterfactual excerpts. CORE then incorporates these into prompts to a large language model with few-shot learning capabilities, for counterfactual editing. Conditioning language model edits on naturally occurring data results in diverse perturbations. Experiments on natural language inference and sentiment analysis benchmarks show that CORE counterfactuals are more effective at improving generalization to OOD data compared to other DA approaches. We also show that the CORE retrieval framework can be used to encourage diversity in manually authored perturbations",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/4989c08930e42d322b3bfed167d7ea434a698f2c.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 28,
        "score": 9.333333333333332
    },
    "ef76276f9ef929496f03282fa85ae1bbcdc69767.pdf": {
        "title": "Robust Retrieval Augmented Generation for Zero-shot Slot Filling",
        "authors": [
            "Michael R. Glass",
            "Gaetano Rossiello",
            "Md. Faisal Mahbub Chowdhury",
            "A. Gliozzo"
        ],
        "published_date": "2021",
        "abstract": "Automatically inducing high quality knowledge graphs from a given collection of documents still remains a challenging problem in AI. One way to make headway for this problem is through advancements in a related task known as slot filling. In this task, given an entity query in form of [Entity, Slot, ?], a system is asked to \u2018fill\u2019 the slot by generating or extracting the missing value exploiting evidence extracted from relevant passage(s) in the given document collection. The recent works in the field try to solve this task in an end-to-end fashion using retrieval-based language models. In this paper, we present a novel approach to zero-shot slot filling that extends dense passage retrieval with hard negatives and robust training procedures for retrieval augmented generation models. Our model reports large improvements on both T-REx and zsRE slot filling datasets, improving both passage retrieval and slot value generation, and ranking at the top-1 position in the KILT leaderboard. Moreover, we demonstrate the robustness of our system showing its domain adaptation capability on a new variant of the TACRED dataset for slot filling, through a combination of zero/few-shot learning. We release the source code and pre-trained models.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/ef76276f9ef929496f03282fa85ae1bbcdc69767.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 33,
        "score": 8.25
    },
    "b360427d0991143013da6a208ccf28bcc8028fab.pdf": {
        "title": "Large Scale Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training",
        "authors": [
            "Oshin Agarwal",
            "Heming Ge",
            "Siamak Shakeri",
            "Rami Al-Rfou"
        ],
        "published_date": "2020",
        "abstract": "Generating natural sentences from Knowledge Graph (KG) triples, known as Data-To-Text Generation, is a task with many datasets for which numerous complex systems have been developed. However, no prior work has attempted to perform this generation at scale by converting an entire KG into natural text. In this paper, we verbalize the entire Wikidata KG, and create a KG-Text aligned corpus in the training process. We discuss the challenges in verbalizing an entire KG versus verbalizing smaller datasets. We further show that verbalizing an entire KG can be used to integrate structured and natural language data. In contrast to the many architectures that have been developed to integrate the structural differences between these two sources, our approach converts the KG into the same format as natural text allowing it to be seamlessly plugged into existing natural language systems. We evaluate this approach by augmenting the retrieval corpus in REALM and showing improvements, both on the LAMA knowledge probe and open domain QA.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/b360427d0991143013da6a208ccf28bcc8028fab.pdf",
        "venue": "arXiv.org",
        "citationCount": 37,
        "score": 7.4
    },
    "e2907d8a966ba44022c76ddc17d9d0a1ce5b9205.pdf": {
        "title": "End-to-End Table Question Answering via Retrieval-Augmented Generation",
        "authors": [
            "Feifei Pan",
            "Mustafa Canim",
            "Michael R. Glass",
            "A. Gliozzo",
            "J. Hendler"
        ],
        "published_date": "2022",
        "abstract": "Most existing end-to-end Table Question Answering (Table QA) models consist of a two-stage framework with a retriever to select relevant table candidates from a corpus and a reader to locate the correct answers from table candidates. Even though the accuracy of the reader models is significantly improved with the recent transformer-based approaches, the overall performance of such frameworks still suffers from the poor accuracy of using traditional information retrieval techniques as retrievers. To alleviate this problem, we introduce T-RAG, an end-to-end Table QA model, where a non-parametric dense vector index is fine-tuned jointly with BART, a parametric sequence-to-sequence model to generate answer tokens. Given any natural language question, T-RAG utilizes a unified pipeline to automatically search through a table corpus to directly locate the correct answer from the table cells. We apply T-RAG to recent open-domain Table QA benchmarks and demonstrate that the fine-tuned T-RAG model is able to achieve state-of-the-art performance in both the end-to-end Table QA and the table retrieval tasks.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/e2907d8a966ba44022c76ddc17d9d0a1ce5b9205.pdf",
        "venue": "arXiv.org",
        "citationCount": 17,
        "score": 5.666666666666666
    },
    "83c151d8e6f5967ac031b021b30a1a10e890c2eb.pdf": {
        "title": "A Large-Scale Comparative Evaluation of IR-Based Tools for Bug Localization",
        "authors": [
            "Shayan A. Akbar",
            "A. Kak"
        ],
        "published_date": "2020",
        "abstract": "This paper reports on a large-scale comparative evaluation of IR-based tools for automatic bug localization. We have divided the tools in our evaluation into the following three generations: (1) The firstgeneration tools, now over a decade old, that are based purely on the Bag-of-Words (BoW) modeling of software libraries. (2) The somewhat more recent second-generation tools that augment BoW-based modeling with two additional pieces of information: historical data, such as change history, and structured information such as class names, method names, etc. And, finally, (3) The third-generation tools that are currently the focus of much research and that also exploit proximity, order, and semantic relationships between the terms. It is important to realize that the original authors of all these three generations of tools have mostly tested them on relatively small-sized datasets that typically consisted no more than a few thousand bug reports. Additionally, those evaluations only involved Java code libraries. The goal of the present paper is to present a comprehensive large-scale evaluation of all three generations of bug-localization tools with code libraries in multiple languages. Our study involves over 20,000 bug reports drawn from a diverse collection of Java, C/C++, and Python projects. Our results show that the third-generation tools are significantly superior to the older tools. We also show that the word embeddings generated using code files written in one language are effective for retrieval from code libraries in other languages.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/83c151d8e6f5967ac031b021b30a1a10e890c2eb.pdf",
        "venue": "IEEE Working Conference on Mining Software Repositories",
        "citationCount": 28,
        "score": 5.6000000000000005
    },
    "6058ce3819d72c3e429bea58d78d80c719cb4bdb.pdf": {
        "title": "Data Augmentation for Biomedical Factoid Question Answering",
        "authors": [
            "Dimitris Pappas",
            "Prodromos Malakasiotis",
            "Ion Androutsopoulos"
        ],
        "published_date": "2022",
        "abstract": "We study the effect of seven data augmentation (DA) methods in factoid question answering, focusing on the biomedical domain, where obtaining training instances is particularly difficult. We experiment with data from the BIOASQ challenge, which we augment with training instances obtained from an artificial biomedical machine reading comprehension dataset, or via back-translation, information retrieval, word substitution based on WORD2VEC embeddings, or masked language modeling, question generation, or extending the given passage with additional context. We show that DA can lead to very significant performance gains, even when using large pre-trained Transformers, contributing to a broader discussion of if/when DA benefits large pre-trained models. One of the simplest DA methods, WORD2VEC-based word substitution, performed best and is recommended. We release our artificial training instances and code.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/6058ce3819d72c3e429bea58d78d80c719cb4bdb.pdf",
        "venue": "Workshop on Biomedical Natural Language Processing",
        "citationCount": 13,
        "score": 4.333333333333333
    },
    "ca89781d7915eac3089a7b47a065943ce722109f.pdf": {
        "title": "Retrieval-Augmented Controllable Review Generation",
        "authors": [
            "Jihyeok Kim",
            "Seungtaek Choi",
            "Reinald Kim Amplayo",
            "Seung-won Hwang"
        ],
        "published_date": "2020",
        "abstract": "In this paper, we study review generation given a set of attribute identifiers which are user ID, product ID and rating. This is a difficult subtask of natural language generation since models are limited to the given identifiers, without any specific descriptive information regarding the inputs, when generating the text. The capacity of these models is thus confined and dependent to how well the models can capture vector representations of attributes. We thus propose to additionally leverage references, which are selected from a large pool of texts labeled with one of the attributes, as textual information that enriches inductive biases of given attributes. With these references, we can now pose the problem as an instance of text-to-text generation, which makes the task easier since texts that are syntactically, semantically similar with the output text are provided as input. Using this framework, we address issues such as selecting references from a large candidate set without textual context and improving the model complexity for generation. Our experiments show that our models improve over previous approaches on both automatic and human evaluation metrics.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/ca89781d7915eac3089a7b47a065943ce722109f.pdf",
        "venue": "International Conference on Computational Linguistics",
        "citationCount": 21,
        "score": 4.2
    },
    "46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5.pdf": {
        "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
        "authors": [
            "Yunfan Gao",
            "Yun Xiong",
            "Xinyu Gao",
            "Kangxiang Jia",
            "Jinliu Pan",
            "Yuxi Bi",
            "Yi Dai",
            "Jiawei Sun",
            "Qianyu Guo",
            "Meng Wang",
            "Haofen Wang"
        ],
        "published_date": "2023",
        "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5.pdf",
        "venue": "arXiv.org",
        "citationCount": 2345,
        "score": 1172.5
    },
    "eb9c4a07a336e8deefe7b399c550d3af0241238e.pdf": {
        "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
        "authors": [
            "Wenqi Fan",
            "Yujuan Ding",
            "Liang-bo Ning",
            "Shijie Wang",
            "Hengyun Li",
            "Dawei Yin",
            "Tat-Seng Chua",
            "Qing Li"
        ],
        "published_date": "2024",
        "abstract": "As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at: https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/eb9c4a07a336e8deefe7b399c550d3af0241238e.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 397,
        "score": 397.0
    },
    "b798cf6af813638fab09a8af6ad0f3df6c241485.pdf": {
        "title": "Benchmarking Retrieval-Augmented Generation for Medicine",
        "authors": [
            "Guangzhi Xiong",
            "Qiao Jin",
            "Zhiyong Lu",
            "Aidong Zhang"
        ],
        "published_date": "2024",
        "abstract": "While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs by up to 18% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the\"lost-in-the-middle\"effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/b798cf6af813638fab09a8af6ad0f3df6c241485.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 266,
        "score": 266.0
    },
    "28e2ecb4183ebc0eec504b12dddc677f8aef8745.pdf": {
        "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
        "authors": [
            "Jiawei Chen",
            "Hongyu Lin",
            "Xianpei Han",
            "Le Sun"
        ],
        "published_date": "2023",
        "abstract": "Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/28e2ecb4183ebc0eec504b12dddc677f8aef8745.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 380,
        "score": 190.0
    },
    "9ab45aa875b56335303398e84a59a3756cd9d530.pdf": {
        "title": "Graph Retrieval-Augmented Generation: A Survey",
        "authors": [
            "Boci Peng",
            "Yun Zhu",
            "Yongchao Liu",
            "Xiaohe Bo",
            "Haizhou Shi",
            "Chuntao Hong",
            "Yan Zhang",
            "Siliang Tang"
        ],
        "published_date": "2024",
        "abstract": "Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable success in addressing the challenges of Large Language Models (LLMs) without necessitating retraining. By referencing an external knowledge base, RAG refines LLM outputs, effectively mitigating issues such as ``hallucination'', lack of domain-specific knowledge, and outdated information. However, the complex structure of relationships among different entities in databases presents challenges for RAG systems. In response, GraphRAG leverages structural information across entities to enable more precise and comprehensive retrieval, capturing relational knowledge and facilitating more accurate, context-aware responses. Given the novelty and potential of GraphRAG, a systematic review of current technologies is imperative. This paper provides the first comprehensive overview of GraphRAG methodologies. We formalize the GraphRAG workflow, encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced Generation. We then outline the core technologies and training methods at each stage. Additionally, we examine downstream tasks, application domains, evaluation methodologies, and industrial use cases of GraphRAG. Finally, we explore future research directions to inspire further inquiries and advance progress in the field. In order to track recent progress in this field, we set up a repository at \\url{https://github.com/pengboci/GraphRAG-Survey}.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/9ab45aa875b56335303398e84a59a3756cd9d530.pdf",
        "venue": "arXiv.org",
        "citationCount": 171,
        "score": 171.0
    },
    "4e71624e90960cb003e311a0fe3b8be4c2863239.pdf": {
        "title": "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries",
        "authors": [
            "Yixuan Tang",
            "Yi Yang"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) augments large language models (LLM) by retrieving relevant knowledge, showing promising potential in mitigating LLM hallucinations and enhancing response quality, thereby facilitating the great adoption of LLMs in practice. However, we find that existing RAG systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utilizing an English news article dataset as the underlying RAG knowledge base. We demonstrate the benchmarking utility of MultiHop-RAG in two experiments. The first experiment compares different embedding models for retrieving evidence for multi-hop queries. In the second experiment, we examine the capabilities of various state-of-the-art LLMs, including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop queries given the evidence. Both experiments reveal that existing RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable resource for the community in developing effective RAG systems, thereby facilitating greater adoption of LLMs in practice. The MultiHop-RAG and implemented RAG system is publicly available at https://github.com/yixuantt/MultiHop-RAG/.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/4e71624e90960cb003e311a0fe3b8be4c2863239.pdf",
        "venue": "arXiv.org",
        "citationCount": 142,
        "score": 142.0
    },
    "a41d4a3b005c8ec4f821e6ee96672d930ca9596c.pdf": {
        "title": "G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
        "authors": [
            "Xiaoxin He",
            "Yijun Tian",
            "Yifei Sun",
            "N. Chawla",
            "T. Laurent",
            "Yann LeCun",
            "Xavier Bresson",
            "Bryan Hooi"
        ],
        "published_date": "2024",
        "abstract": "Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, G-Retriever performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.~\\footnote{Our codes and datasets are available at: \\url{https://github.com/XiaoxinHe/G-Retriever}}",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/a41d4a3b005c8ec4f821e6ee96672d930ca9596c.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 131,
        "score": 131.0
    },
    "b708e0f49d8e9708bc649debd9a9372748fffa3d.pdf": {
        "title": "Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering",
        "authors": [
            "Zhentao Xu",
            "Mark Jerome Cruz",
            "Matthew Guevara",
            "Tie Wang",
            "Manasi Deshpande",
            "Xiaofeng Wang",
            "Zheng Li"
        ],
        "published_date": "2024",
        "abstract": "In customer service technical support, swiftly and accurately retrieving relevant past issues is critical for efficiently resolving customer inquiries. The conventional retrieval methods in retrieval-augmented generation (RAG) for large language models (LLMs) treat a large corpus of past issue tracking tickets as plain text, ignoring the crucial intra-issue structure and inter-issue relations, which limits performance. We introduce a novel customer service question-answering method that amalgamates RAG with a knowledge graph (KG). Our method constructs a KG from historical issues for use in retrieval, retaining the intra-issue structure and inter-issue relations. During the question-answering phase, our method parses consumer queries and retrieves related sub-graphs from the KG to generate answers. This integration of a KG not only improves retrieval accuracy by preserving customer service structure information but also enhances answering quality by mitigating the effects of text segmentation. Empirical assessments on our benchmark datasets, utilizing key retrieval (MRR, Recall@K, NDCG@K) and text generation (BLEU, ROUGE, METEOR) metrics, reveal that our method outperforms the baseline by 77.6% in MRR and by 0.32 in BLEU. Our method has been deployed within LinkedIn's customer service team for approximately six months and has reduced the median per-issue resolution time by 28.6%.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/b708e0f49d8e9708bc649debd9a9372748fffa3d.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 116,
        "score": 116.0
    },
    "5bbc2b5aa6c63c6a2cfccf095d6020b063ad47ac.pdf": {
        "title": "Corrective Retrieval Augmented Generation",
        "authors": [
            "Shi-Qi Yan",
            "Jia-Chen Gu",
            "Yun Zhu",
            "Zhen-Hua Ling"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) inevitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return sub-optimal documents, large-scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose-then-recompose algorithm is designed for retrieved documents to selectively focus on key information and filter out irrelevant information in them. CRAG is plug-and-play and can be seamlessly coupled with various RAG-based approaches. Experiments on four datasets covering short- and long-form generation tasks show that CRAG can significantly improve the performance of RAG-based approaches.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/5bbc2b5aa6c63c6a2cfccf095d6020b063ad47ac.pdf",
        "venue": "arXiv.org",
        "citationCount": 112,
        "score": 112.0
    },
    "80478de9c7a81561e2f3dac9b8b1ef3df389ff2d.pdf": {
        "title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs",
        "authors": [
            "Yue Yu",
            "Wei Ping",
            "Zihan Liu",
            "Boxin Wang",
            "Jiaxuan You",
            "Chao Zhang",
            "M. Shoeybi",
            "Bryan Catanzaro"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) typically utilize the top-k contexts from a retriever in retrieval-augmented generation (RAG). In this work, we propose a novel instruction fine-tuning framework RankRAG, which instruction-tunes a single LLM for the dual purpose of context ranking and answer generation in RAG. In particular, the instruction-tuned LLMs work surprisingly well by adding a small fraction of ranking data into the training blend, and outperform existing expert ranking models, including the same LLM exclusively fine-tuned on a large amount of ranking data. For generation, we compare our model with many strong baselines, including GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced model with the state-of-the-art performance on RAG benchmarks. Specifically, our Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4 models on nine knowledge-intensive benchmarks. In addition, it also performs comparably to GPT-4 on five RAG benchmarks in the biomedical domain without instruction fine-tuning on biomedical data, demonstrating its superb capability for generalization to new domains.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/80478de9c7a81561e2f3dac9b8b1ef3df389ff2d.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 109,
        "score": 109.0
    },
    "ea89b058ce619ed16d4de633126b02a8179457c8.pdf": {
        "title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)",
        "authors": [
            "Shenglai Zeng",
            "Jiankun Zhang",
            "Pengfei He",
            "Yue Xing",
            "Yiding Liu",
            "Han Xu",
            "Jie Ren",
            "Shuaiqiang Wang",
            "Dawei Yin",
            "Yi Chang",
            "Jiliang Tang"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) is a powerful technique to facilitate language model with proprietary and private data, where data privacy is a pivotal concern. Whereas extensive research has demonstrated the privacy risks of large language models (LLMs), the RAG technique could potentially reshape the inherent behaviors of LLM generation, posing new privacy issues that are currently under-explored. In this work, we conduct extensive empirical studies with novel attack methods, which demonstrate the vulnerability of RAG systems on leaking the private retrieval database. Despite the new risk brought by RAG on the retrieval data, we further reveal that RAG can mitigate the leakage of the LLMs' training data. Overall, we provide new insights in this paper for privacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG systems builders. Our code is available at https://github.com/phycholosogy/RAG-privacy.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/ea89b058ce619ed16d4de633126b02a8179457c8.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 108,
        "score": 108.0
    },
    "e90435e1ae06fab4efa272f5f46ed74ca0a8cde0.pdf": {
        "title": "Evaluating Retrieval Quality in Retrieval-Augmented Generation",
        "authors": [
            "Alireza Salemi",
            "Hamed Zamani"
        ],
        "published_date": "2024",
        "abstract": "Evaluating retrieval-augmented generation (RAG) presents challenges, particularly for retrieval models within these systems. Traditional end-to-end evaluation methods are computationally expensive. Furthermore, evaluation of the retrieval model's performance based on query-document relevance labels shows a small correlation with the RAG system's downstream performance. We propose a novel evaluation approach, eRAG, where each document in the retrieval list is individually utilized by the large language model within the RAG system. The output generated for each document is then evaluated based on the downstream task ground truth labels. In this manner, the downstream performance for each document serves as its relevance label. We employ various downstream task metrics to obtain document-level annotations and aggregate them using set-based or ranking metrics. Extensive experiments on a wide range of datasets demonstrate that eRAG achieves a higher correlation with downstream RAG performance compared to baseline methods, with improvements in Kendall's tau correlation ranging from 0.168 to 0.494. Additionally, eRAG offers significant computational advantages, improving runtime and consuming up to 50 times less GPU memory than end-to-end evaluation.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/e90435e1ae06fab4efa272f5f46ed74ca0a8cde0.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 105,
        "score": 105.0
    },
    "746b96ee17e329f1085a047116c05e12eaa3925a.pdf": {
        "title": "RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation",
        "authors": [
            "Chi-Min Chan",
            "Chunpu Xu",
            "Ruibin Yuan",
            "Hongyin Luo",
            "Wei Xue",
            "Yi-Ting Guo",
            "Jie Fu"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) exhibit remarkable capabilities but are prone to generating inaccurate or hallucinatory responses. This limitation stems from their reliance on vast pretraining datasets, making them susceptible to errors in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation (RAG) addresses this by incorporating external, relevant documents into the response generation process, thus leveraging non-parametric knowledge alongside LLMs' in-context learning abilities. However, existing RAG implementations primarily focus on initial input for context retrieval, overlooking the nuances of ambiguous or complex queries that necessitate further clarification or decomposition for accurate responses. To this end, we propose learning to Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper, endeavoring to enhance the model by equipping it with capabilities for explicit rewriting, decomposition, and disambiguation. Our experimental results indicate that our method, when applied to a 7B Llama2 model, surpasses the previous state-of-the-art (SOTA) by an average of 1.9\\% across three single-hop QA datasets, and also demonstrates enhanced performance in handling complex, multi-hop QA datasets. Our code is available at https://github.com/chanchimin/RQ-RAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/746b96ee17e329f1085a047116c05e12eaa3925a.pdf",
        "venue": "arXiv.org",
        "citationCount": 105,
        "score": 105.0
    },
    "965a0969b460f9246158d88fb28e21c5d80d0a8b.pdf": {
        "title": "Optimization of hepatological clinical guidelines interpretation by large language models: a retrieval augmented generation-based framework",
        "authors": [
            "Simone Kresevic",
            "M. Giuffr\u00e9",
            "M. Aj\u010devi\u0107",
            "A. Accardo",
            "L. Croc\u00e8",
            "D. Shung"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) can potentially transform healthcare, particularly in providing the right information to the right provider at the right time in the hospital workflow. This study investigates the integration of LLMs into healthcare, specifically focusing on improving clinical decision support systems (CDSSs) through accurate interpretation of medical guidelines for chronic Hepatitis C Virus infection management. Utilizing OpenAI\u2019s GPT-4 Turbo model, we developed a customized LLM framework that incorporates retrieval augmented generation (RAG) and prompt engineering. Our framework involved guideline conversion into the best-structured format that can be efficiently processed by LLMs to provide the most accurate output. An ablation study was conducted to evaluate the impact of different formatting and learning strategies on the LLM\u2019s answer generation accuracy. The baseline GPT-4 Turbo model\u2019s performance was compared against five experimental setups with increasing levels of complexity: inclusion of in-context guidelines, guideline reformatting, and implementation of few-shot learning. Our primary outcome was the qualitative assessment of accuracy based on expert review, while secondary outcomes included the quantitative measurement of similarity of LLM-generated responses to expert-provided answers using text-similarity scores. The results showed a significant improvement in accuracy from 43 to 99% (p\u2009<\u20090.001), when guidelines were provided as context in a coherent corpus of text and non-text sources were converted into text. In addition, few-shot learning did not seem to improve overall accuracy. The study highlights that structured guideline reformatting and advanced prompt engineering (data quality vs. data quantity) can enhance the efficacy of LLM integrations to CDSSs for guideline delivery.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/965a0969b460f9246158d88fb28e21c5d80d0a8b.pdf",
        "venue": "npj Digit. Medicine",
        "citationCount": 101,
        "score": 101.0
    },
    "336605fc899aab6c5b375d1129bf656d246b9013.pdf": {
        "title": "GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning",
        "authors": [
            "Costas Mavromatis",
            "George Karypis"
        ],
        "published_date": "2024",
        "abstract": "Knowledge Graphs (KGs) represent human-crafted factual knowledge in the form of triplets (head, relation, tail), which collectively form a graph. Question Answering over KGs (KGQA) is the task of answering natural questions grounding the reasoning to the information provided by the KG. Large Language Models (LLMs) are the state-of-the-art models for QA tasks due to their remarkable ability to understand natural language. On the other hand, Graph Neural Networks (GNNs) have been widely used for KGQA as they can handle the complex graph information stored in the KG. In this work, we introduce GNN-RAG, a novel method for combining language understanding abilities of LLMs with the reasoning abilities of GNNs in a retrieval-augmented generation (RAG) style. First, a GNN reasons over a dense KG subgraph to retrieve answer candidates for a given question. Second, the shortest paths in the KG that connect question entities and answer candidates are extracted to represent KG reasoning paths. The extracted paths are verbalized and given as input for LLM reasoning with RAG. In our GNN-RAG framework, the GNN acts as a dense subgraph reasoner to extract useful graph information, while the LLM leverages its natural language processing ability for ultimate KGQA. Furthermore, we develop a retrieval augmentation (RA) technique to further boost KGQA performance with GNN-RAG. Experimental results show that GNN-RAG achieves state-of-the-art performance in two widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching GPT-4 performance with a 7B tuned LLM. In addition, GNN-RAG excels on multi-hop and multi-entity questions outperforming competing approaches by 8.9--15.5% points at answer F1.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/336605fc899aab6c5b375d1129bf656d246b9013.pdf",
        "venue": "arXiv.org",
        "citationCount": 98,
        "score": 98.0
    },
    "daebec92963ab8dea492f0c209bdf57e87bcaa07.pdf": {
        "title": "FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research",
        "authors": [
            "Jiajie Jin",
            "Yutao Zhu",
            "Xinyu Yang",
            "Chenghao Zhang",
            "Zhicheng Dou"
        ],
        "published_date": "2024",
        "abstract": "With the advent of large language models (LLMs) and multimodal large language models (MLLMs), the potential of retrieval-augmented generation (RAG) has attracted considerable research attention. However, the absence of a standardized framework for implementation, coupled with the inherently complex RAG process, makes it challenging and time-consuming for researchers to compare and evaluate these approaches in a consistent environment. In response to this challenge, we develop FlashRAG, an efficient and modular open-source toolkit designed to assist researchers in reproducing and comparing existing RAG methods and developing their own algorithms within a unified framework. Our toolkit has implemented 16 advanced RAG methods and gathered and organized 38 benchmark datasets. It has various features, including a customizable modular framework, a rich collection of pre-implemented RAG works, comprehensive datasets, efficient auxiliary pre-processing scripts, and extensive and standard evaluation metrics. Our toolkit and resources are available at https://github.com/RUC-NLPIR/FlashRAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/daebec92963ab8dea492f0c209bdf57e87bcaa07.pdf",
        "venue": "The Web Conference",
        "citationCount": 93,
        "score": 93.0
    },
    "9af8bccf3e42996cbb198a6ceccafa2a084689f6.pdf": {
        "title": "HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction",
        "authors": [
            "Bhaskarjit Sarmah",
            "Dhagash Mehta",
            "Benika Hall",
            "Rohan Rao",
            "Sunil Patel",
            "Stefano Pasquali"
        ],
        "published_date": "2024",
        "abstract": "Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q&A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q&A format, and hence provide a natural set of pairs of ground-truth Q&As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/9af8bccf3e42996cbb198a6ceccafa2a084689f6.pdf",
        "venue": "International Conference on AI in Finance",
        "citationCount": 88,
        "score": 88.0
    },
    "2986b2b06173e065c94bae49c7a9a3718dad486c.pdf": {
        "title": "Reducing hallucination in structured outputs via Retrieval-Augmented Generation",
        "authors": [
            "Patrice B'echard",
            "Orlando Marquez Ayala"
        ],
        "published_date": "2024",
        "abstract": "A common and fundamental limitation of Generative AI (GenAI) is its propensity to hallucinate. While large language models (LLM) have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging Retrieval Augmented Generation (RAG) to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucinations in the output and improves the generalization of our LLM in out-of-domain settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/2986b2b06173e065c94bae49c7a9a3718dad486c.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 88,
        "score": 88.0
    },
    "9a946c503b6e799b3d57375b6edfaf4e24febcea.pdf": {
        "title": "Searching for Best Practices in Retrieval-Augmented Generation",
        "authors": [
            "Xiaohua Wang",
            "Zhenghua Wang",
            "Xuan Gao",
            "Feiran Zhang",
            "Yixin Wu",
            "Zhibo Xu",
            "Tianyuan Shi",
            "Zhengyuan Wang",
            "Shizheng Li",
            "Qi Qian",
            "Ruicheng Yin",
            "Changze Lv",
            "Xiaoqing Zheng",
            "Xuanjing Huang"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a \u201cretrieval as generation\u201d strategy.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/9a946c503b6e799b3d57375b6edfaf4e24febcea.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 77,
        "score": 77.0
    },
    "5c204b2421d05b83d3c96a6c515cc03143073935.pdf": {
        "title": "PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models",
        "authors": [
            "Wei Zou",
            "Runpeng Geng",
            "Binghui Wang",
            "Jinyuan Jia"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/5c204b2421d05b83d3c96a6c515cc03143073935.pdf",
        "venue": "arXiv.org",
        "citationCount": 76,
        "score": 76.0
    },
    "4308208fac24626e0c927ee728038aadc4e87266.pdf": {
        "title": "HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models",
        "authors": [
            "Bernal Jimenez Gutierrez",
            "Yiheng Shu",
            "Yu Gu",
            "Michihiro Yasunaga",
            "Yu Su"
        ],
        "published_date": "2024",
        "abstract": "In order to thrive in hostile and ever-changing natural environments, mammalian brains evolved to store large amounts of knowledge about the world and continually integrate new information while avoiding catastrophic forgetting. Despite the impressive accomplishments, large language models (LLMs), even with retrieval-augmented generation (RAG), still struggle to efficiently and effectively integrate a large amount of new experiences after pre-training. In this work, we introduce HippoRAG, a novel retrieval framework inspired by the hippocampal indexing theory of human long-term memory to enable deeper and more efficient knowledge integration over new experiences. HippoRAG synergistically orchestrates LLMs, knowledge graphs, and the Personalized PageRank algorithm to mimic the different roles of neocortex and hippocampus in human memory. We compare HippoRAG with existing RAG methods on multi-hop question answering and show that our method outperforms the state-of-the-art methods remarkably, by up to 20%. Single-step retrieval with HippoRAG achieves comparable or better performance than iterative retrieval like IRCoT while being 10-30 times cheaper and 6-13 times faster, and integrating HippoRAG into IRCoT brings further substantial gains. Finally, we show that our method can tackle new types of scenarios that are out of reach of existing methods. Code and data are available at https://github.com/OSU-NLP-Group/HippoRAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/4308208fac24626e0c927ee728038aadc4e87266.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 75,
        "score": 75.0
    },
    "d9052dd87959e6076baf35e8f7ee87d568a32b58.pdf": {
        "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents",
        "authors": [
            "Shi Yu",
            "Chaoyue Tang",
            "Bokai Xu",
            "Junbo Cui",
            "Junhao Ran",
            "Yukun Yan",
            "Zhenghao Liu",
            "Shuo Wang",
            "Xu Han",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) is an effective technique that enables large language models (LLMs) to utilize external knowledge sources for generation. However, current RAG systems are solely based on text, rendering it impossible to utilize vision information like layout and images that play crucial roles in real-world multi-modality documents. In this paper, we introduce VisRAG, which tackles this issue by establishing a vision-language model (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the document to obtain text, the document is directly embedded using a VLM as an image and then retrieved to enhance the generation of a VLM. Compared to traditional text-based RAG, VisRAG maximizes the retention and utilization of the data information in the original documents, eliminating the information loss introduced during the parsing process. We collect both open-source and synthetic data to train the retriever in VisRAG and explore a variety of generation methods. Experiments demonstrate that VisRAG outperforms traditional RAG in both the retrieval and generation stages, achieving a 20--40% end-to-end performance gain over traditional text-based RAG pipeline. Further analysis reveals that VisRAG is efficient in utilizing training data and demonstrates strong generalization capability, positioning it as a promising solution for RAG on multi-modality documents. Our code and data are available at https://github.com/openbmb/visrag.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/d9052dd87959e6076baf35e8f7ee87d568a32b58.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 69,
        "score": 69.0
    },
    "1ea143c34b9bc359780f79ba4d68dee68bcc1129.pdf": {
        "title": "LightRAG: Simple and Fast Retrieval-Augmented Generation",
        "authors": [
            "Zirui Guo",
            "Lianghao Xia",
            "Yanhua Yu",
            "Tu Ao",
            "Chao Huang"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs. However, existing RAG systems have significant limitations, including reliance on flat data representations and inadequate contextual awareness, which can lead to fragmented answers that fail to capture complex inter-dependencies. To address these challenges, we propose LightRAG, which incorporates graph structures into text indexing and retrieval processes. This innovative framework employs a dual-level retrieval system that enhances comprehensive information retrieval from both low-level and high-level knowledge discovery. Additionally, the integration of graph structures with vector representations facilitates efficient retrieval of related entities and their relationships, significantly improving response times while maintaining contextual relevance. This capability is further enhanced by an incremental update algorithm that ensures the timely integration of new data, allowing the system to remain effective and responsive in rapidly changing data environments. Extensive experimental validation demonstrates considerable improvements in retrieval accuracy and efficiency compared to existing approaches. We have made our LightRAG open-source and available at the link: https://github.com/HKUDS/LightRAG",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/1ea143c34b9bc359780f79ba4d68dee68bcc1129.pdf",
        "venue": "arXiv.org",
        "citationCount": 67,
        "score": 67.0
    },
    "ccb5afb760a73f5507e31995397f80960db7842d.pdf": {
        "title": "Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach",
        "authors": [
            "Zhuowan Li",
            "Cheng Li",
            "Mingyang Zhang",
            "Qiaozhu Mei",
            "Michael Bendersky"
        ],
        "published_date": "2024",
        "abstract": "Retrieval Augmented Generation (RAG) has been a powerful tool for Large Language Models (LLMs) to efficiently process overly lengthy contexts. However, recent LLMs like Gemini-1.5 and GPT-4 show exceptional capabilities to understand long contexts directly. We conduct a comprehensive comparison between RAG and long-context (LC) LLMs, aiming to leverage the strengths of both. We benchmark RAG and LC across various public datasets using three latest LLMs. Results reveal that when resourced sufficiently, LC consistently outperforms RAG in terms of average performance. However, RAG\u2019s significantly lower cost remains a distinct advantage. Based on this observation, we propose Self-Route, a simple yet effective method that routes queries to RAG or LC based on model self-reflection. Self-Route significantly reduces the computation cost while maintaining a comparable performance to LC. Our findings provide a guideline for long-context applications of LLMs using RAG and LC.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/ccb5afb760a73f5507e31995397f80960db7842d.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 67,
        "score": 67.0
    },
    "339d2a56f0e5176b691c358a86891e2923045c8c.pdf": {
        "title": "Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely",
        "authors": [
            "Siyun Zhao",
            "Yuqing Yang",
            "Zilong Wang",
            "Zhiyuan He",
            "Luna K. Qiu",
            "Lili Qiu"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) augmented with external data have demonstrated remarkable capabilities in completing real-world tasks. Techniques for integrating external data into LLMs, such as Retrieval-Augmented Generation (RAG) and fine-tuning, are gaining increasing attention and widespread application. Nonetheless, the effective deployment of data-augmented LLMs across various specialized fields presents substantial challenges. These challenges encompass a wide range of issues, from retrieving relevant data and accurately interpreting user intent to fully harnessing the reasoning capabilities of LLMs for complex tasks. We believe that there is no one-size-fits-all solution for data-augmented LLM applications. In practice, underperformance often arises from a failure to correctly identify the core focus of a task or because the task inherently requires a blend of multiple capabilities that must be disentangled for better resolution. In this survey, we propose a RAG task categorization method, classifying user queries into four levels based on the type of external data required and primary focus of the task: explicit fact queries, implicit fact queries, interpretable rationale queries, and hidden rationale queries. We define these levels of queries, provide relevant datasets, and summarize the key challenges and most effective techniques for addressing these challenges. Finally, we discuss three main forms of integrating external data into LLMs: context, small model, and fine-tuning, highlighting their respective strengths, limitations, and the types of problems they are suited to solve. This work aims to help readers thoroughly understand and decompose the data requirements and key bottlenecks in building LLM applications, offering solutions to the different challenges and serving as a guide to systematically developing such applications.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/339d2a56f0e5176b691c358a86891e2923045c8c.pdf",
        "venue": "arXiv.org",
        "citationCount": 65,
        "score": 65.0
    },
    "94034fd2ed4b6cf41113abb7adc9ae469313c958.pdf": {
        "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
        "authors": [
            "Yizheng Huang",
            "Jimmy X. Huang"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/94034fd2ed4b6cf41113abb7adc9ae469313c958.pdf",
        "venue": "arXiv.org",
        "citationCount": 61,
        "score": 61.0
    },
    "b39aba9b515723745c994aa0fbd80a566c268282.pdf": {
        "title": "FinBen: A Holistic Financial Benchmark for Large Language Models",
        "authors": [
            "Qianqian Xie",
            "Weiguang Han",
            "Zhengyu Chen",
            "Ruoyu Xiang",
            "Xiao Zhang",
            "Yueru He",
            "Mengxi Xiao",
            "Dong Li",
            "Yongfu Dai",
            "Duanyu Feng",
            "Yijing Xu",
            "Haoqiang Kang",
            "Zi-Zhou Kuang",
            "Chenhan Yuan",
            "Kailai Yang",
            "Zheheng Luo",
            "Tianlin Zhang",
            "Zhiwei Liu",
            "Guojun Xiong",
            "Zhiyang Deng",
            "Yuechen Jiang",
            "Zhiyuan Yao",
            "Haohang Li",
            "Yangyang Yu",
            "Gang Hu",
            "Jiajia Huang",
            "Xiao-Yang Liu",
            "Alejandro Lopez-Lira",
            "Benyou Wang",
            "Yanzhao Lai",
            "Hao Wang",
            "Min Peng",
            "Sophia Ananiadou",
            "Jimin Huang"
        ],
        "published_date": "2024",
        "abstract": "LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of comprehensive evaluation benchmarks, the rapid development of LLMs, and the complexity of financial tasks. In this paper, we introduce FinBen, the first extensive open-source evaluation benchmark, including 36 datasets spanning 24 financial tasks, covering seven critical aspects: information extraction (IE), textual analysis, question answering (QA), text generation, risk management, forecasting, and decision-making. FinBen offers several key innovations: a broader range of tasks and datasets, the first evaluation of stock trading, novel agent and Retrieval-Augmented Generation (RAG) evaluation, and three novel open-source evaluation datasets for text summarization, question answering, and stock trading. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals several key findings: While LLMs excel in IE and textual analysis, they struggle with advanced reasoning and complex tasks like text generation and forecasting. GPT-4 excels in IE and stock trading, while Gemini is better at text generation and forecasting. Instruction-tuned LLMs improve textual analysis but offer limited benefits for complex tasks such as QA. FinBen has been used to host the first financial LLMs shared task at the FinNLP-AgentScen workshop during IJCAI-2024, attracting 12 teams. Their novel solutions outperformed GPT-4, showcasing FinBen's potential to drive innovation in financial LLMs. All datasets, results, and codes are released for the research community: https://github.com/The-FinAI/PIXIU.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/b39aba9b515723745c994aa0fbd80a566c268282.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 61,
        "score": 61.0
    },
    "d4a5c2ab2b459426869e1a3ab1550897b005303e.pdf": {
        "title": "Retrieval-Augmented Generation for Natural Language Processing: A Survey",
        "authors": [
            "Shangyu Wu",
            "Ying Xiong",
            "Yufei Cui",
            "Haolun Wu",
            "Can Chen",
            "Ye Yuan",
            "Lianming Huang",
            "Xue Liu",
            "Tei-Wei Kuo",
            "Nan Guan",
            "C. Xue"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have demonstrated great success in various fields, benefiting from their huge amount of parameters that store knowledge. However, LLMs still suffer from several key issues, such as hallucination problems, knowledge update issues, and lacking domain-specific expertise. The appearance of retrieval-augmented generation (RAG), which leverages an external knowledge database to augment LLMs, makes up those drawbacks of LLMs. This paper reviews all significant techniques of RAG, especially in the retriever and the retrieval fusions. Besides, tutorial codes are provided for implementing the representative techniques in RAG. This paper further discusses the RAG update, including RAG with/without knowledge update. Then, we introduce RAG evaluation and benchmarking, as well as the application of RAG in representative NLP tasks and industrial scenarios. Finally, this paper discusses RAG's future directions and challenges for promoting this field's development.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/d4a5c2ab2b459426869e1a3ab1550897b005303e.pdf",
        "venue": "arXiv.org",
        "citationCount": 60,
        "score": 60.0
    },
    "e2050c0aa8aa27235c0708b5a5ff741dfd11e2a9.pdf": {
        "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models",
        "authors": [
            "Yuanjie Lyu",
            "Zhiyu Li",
            "Simin Niu",
            "Feiyu Xiong",
            "Bo Tang",
            "Wenjin Wang",
            "Hao Wu",
            "Huan Liu",
            "Tong Xu",
            "Enhong Chen"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate \u201challucinated\u201d content. However, evaluating RAG systems is a challenge. Most benchmarks focus primarily on question-answering applications, neglecting other potential scenarios where RAG could be beneficial. Accordingly, in the experiments, these benchmarks often assess only the LLM components of the RAG pipeline or the retriever in knowledge-intensive scenarios, overlooking the impact of external knowledge base construction and the retrieval component on the entire RAG pipeline in non-knowledge-intensive scenarios. To address these issues, this article constructs a large-scale and more comprehensive benchmark and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we refer to the CRUD actions that describe interactions between users and knowledge bases and also categorize the range of RAG applications into four distinct types\u2014create, read, update, and delete (CRUD). \u201cCreate\u201d refers to scenarios requiring the generation of original, varied content. \u201cRead\u201d involves responding to intricate questions in knowledge-intensive situations. \u201cUpdate\u201d focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. \u201cDelete\u201d pertains to the task of summarizing extensive texts into more concise forms. For each of these CRUD categories, we have developed different datasets to evaluate the performance of RAG systems. We also analyze the effects of various components of the RAG system, such as the retriever, context length, knowledge base construction, and LLM. Finally, we provide useful insights for optimizing the RAG technology for different scenarios. The source code is available at GitHub: https://github.com/IAAR-Shanghai/CRUD_RAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/e2050c0aa8aa27235c0708b5a5ff741dfd11e2a9.pdf",
        "venue": "ACM Trans. Inf. Syst.",
        "citationCount": 59,
        "score": 59.0
    },
    "a2a4ddbed34916cfa345e957cf060da99685e37b.pdf": {
        "title": "Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning",
        "authors": [
            "Gelei Deng",
            "Yi Liu",
            "Kailong Wang",
            "Yuekang Li",
            "Tianwei Zhang",
            "Yang Liu"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models~(LLMs) have gained immense popularity and are being increasingly applied in various domains. Consequently, ensuring the security of these models is of paramount importance. Jailbreak attacks, which manipulate LLMs to generate malicious content, are recognized as a significant vulnerability. While existing research has predominantly focused on direct jailbreak attacks on LLMs, there has been limited exploration of indirect methods. The integration of various plugins into LLMs, notably Retrieval Augmented Generation~(RAG), which enables LLMs to incorporate external knowledge bases into their response generation such as GPTs, introduces new avenues for indirect jailbreak attacks. To fill this gap, we investigate indirect jailbreak attacks on LLMs, particularly GPTs, introducing a novel attack vector named Retrieval Augmented Generation Poisoning. This method, Pandora, exploits the synergy between LLMs and RAG through prompt manipulation to generate unexpected responses. Pandora uses maliciously crafted content to influence the RAG process, effectively initiating jailbreak attacks. Our preliminary tests show that Pandora successfully conducts jailbreak attacks in four different scenarios, achieving higher success rates than direct attacks, with 64.3\\% for GPT-3.5 and 34.8\\% for GPT-4.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/a2a4ddbed34916cfa345e957cf060da99685e37b.pdf",
        "venue": "Proceedings 2024 Workshop on AI Systems with Confidential COmputing",
        "citationCount": 59,
        "score": 59.0
    },
    "9111d6632e3ad648e65c57c52fd945641ccbdac2.pdf": {
        "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
        "authors": [
            "Heydar Soudani",
            "E. Kanoulas",
            "Faegheh Hasibi"
        ],
        "published_date": "2024",
        "abstract": "Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LMs in handling low-frequency entities on question answering tasks. We conduct extensive experiments on twelve LMs of varying size and type and different FT methods, data augmentation, and retrieval models. Our findings indicate that while FT boosts the performance across entities of varying popularity, RAG surpasses FT by a large margin particularly for least popular factual knowledge. Additionally, the success of both RAG and FT approaches is amplified by improving retrieval and data augmentation techniques. Fine tuning, while beneficial for small LMs, requires extensive resources. To address this issue, we propose the new Stimulus RAG approach that surpasses the effectiveness of fine tuning based approaches, thereby eliminating the need for the costly data augmentation and fine tuning step for enriching LMs with less popular factual knowledge.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/9111d6632e3ad648e65c57c52fd945641ccbdac2.pdf",
        "venue": "SIGIR-AP",
        "citationCount": 58,
        "score": 58.0
    },
    "46ff7e02fd4ff5fdfb9f85bc7071725b8089061f.pdf": {
        "title": "Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented Generation",
        "authors": [
            "Satyapriya Krishna",
            "Kalpesh Krishna",
            "Anhad Mohananey",
            "Steven Schwarcz",
            "Adam Stambler",
            "Shyam Upadhyay",
            "Manaal Faruqui"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have demonstrated significant performance improvements across various cognitive tasks. An emerging application is using LLMs to enhance retrieval-augmented generation (RAG) capabilities. These systems require LLMs to understand user queries, retrieve relevant information, and synthesize coherent and accurate responses. Given the increasing real-world deployment of such systems, comprehensive evaluation becomes crucial. To this end, we propose FRAMES (Factuality, Retrieval, And reasoning MEasurement Set), a high-quality evaluation dataset designed to test LLMs' ability to provide factual responses, assess retrieval capabilities, and evaluate the reasoning required to generate final answers. While previous work has provided datasets and benchmarks to evaluate these abilities in isolation, FRAMES offers a unified framework that provides a clearer picture of LLM performance in end-to-end RAG scenarios. Our dataset comprises challenging multi-hop questions that require the integration of information from multiple sources. We present baseline results demonstrating that even state-of-the-art LLMs struggle with this task, achieving 0.40 accuracy with no retrieval. The accuracy is significantly improved with our proposed multi-step retrieval pipeline, achieving an accuracy of 0.66 (>50% improvement). We hope our work will help bridge evaluation gaps and assist in developing more robust and capable RAG systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/46ff7e02fd4ff5fdfb9f85bc7071725b8089061f.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 58,
        "score": 58.0
    },
    "273c145ea080f277839b89628c255017fc0e1e7c.pdf": {
        "title": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey",
        "authors": [
            "Yujia Zhou",
            "Yan Liu",
            "Xiaoxi Li",
            "Jiajie Jin",
            "Hongjin Qian",
            "Zheng Liu",
            "Chaozhuo Li",
            "Zhicheng Dou",
            "Tsung-Yi Ho",
            "Philip S. Yu"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal paradigm in the development of Large Language Models (LLMs). While much of the current research in this field focuses on performance optimization, particularly in terms of accuracy and efficiency, the trustworthiness of RAG systems remains an area still under exploration. From a positive perspective, RAG systems are promising to enhance LLMs by providing them with useful and up-to-date knowledge from vast external databases, thereby mitigating the long-standing problem of hallucination. While from a negative perspective, RAG systems are at the risk of generating undesirable contents if the retrieved information is either inappropriate or poorly utilized. To address these concerns, we propose a unified framework that assesses the trustworthiness of RAG systems across six key dimensions: factuality, robustness, fairness, transparency, accountability, and privacy. Within this framework, we thoroughly review the existing literature on each dimension. Additionally, we create the evaluation benchmark regarding the six dimensions and conduct comprehensive evaluations for a variety of proprietary and open-source models. Finally, we identify the potential challenges for future research based on our investigation results. Through this work, we aim to lay a structured foundation for future investigations and provide practical insights for enhancing the trustworthiness of RAG systems in real-world applications.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/273c145ea080f277839b89628c255017fc0e1e7c.pdf",
        "venue": "arXiv.org",
        "citationCount": 58,
        "score": 58.0
    },
    "3daedc1e0a9db8c4dda7e06724b0b556f64c0752.pdf": {
        "title": "LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the Legal Domain",
        "authors": [
            "Nicholas Pipitone",
            "Ghita Houir Alami"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) systems are showing promising potential, and are becoming increasingly relevant in AI-powered legal applications. Existing benchmarks, such as LegalBench, assess the generative capabilities of Large Language Models (LLMs) in the legal domain, but there is a critical gap in evaluating the retrieval component of RAG systems. To address this, we introduce LegalBench-RAG, the first benchmark specifically designed to evaluate the retrieval step of RAG pipelines within the legal space. LegalBench-RAG emphasizes precise retrieval by focusing on extracting minimal, highly relevant text segments from legal documents. These highly relevant snippets are preferred over retrieving document IDs, or large sequences of imprecise chunks, both of which can exceed context window limitations. Long context windows cost more to process, induce higher latency, and lead LLMs to forget or hallucinate information. Additionally, precise results allow LLMs to generate citations for the end user. The LegalBench-RAG benchmark is constructed by retracing the context used in LegalBench queries back to their original locations within the legal corpus, resulting in a dataset of 6,858 query-answer pairs over a corpus of over 79M characters, entirely human-annotated by legal experts. We also introduce LegalBench-RAG-mini, a lightweight version for rapid iteration and experimentation. By providing a dedicated benchmark for legal retrieval, LegalBench-RAG serves as a critical tool for companies and researchers focused on enhancing the accuracy and performance of RAG systems in the legal domain. The LegalBench-RAG dataset is publicly available at https://github.com/zeroentropy-cc/legalbenchrag.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/3daedc1e0a9db8c4dda7e06724b0b556f64c0752.pdf",
        "venue": "arXiv.org",
        "citationCount": 58,
        "score": 58.0
    },
    "7326329c09c11aac423ef4910222a16952bb01dc.pdf": {
        "title": "RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation",
        "authors": [
            "Chao Jin",
            "Zili Zhang",
            "Xuanlin Jiang",
            "Fangyue Liu",
            "Xin Liu",
            "Xuanzhe Liu",
            "Xin Jin"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) has demonstrated substantial advancements in various natural language processing tasks by integrating the strengths of large language models (LLMs) and external knowledge databases. However, the retrieval step introduces long sequence generation and extra data dependency, resulting in long end-to-end latency.\n \n Our analysis benchmarks current RAG systems and reveals that, while the retrieval step poses performance challenges, it also offers optimization opportunities through its retrieval pattern and streaming search behavior. We propose RAGCache, a latency-optimized serving system tailored for RAG. RAGCache leverages the retrieval pattern to organize and cache the intermediate states of retrieved knowledge in a\n knowledge tree\n across the GPU and host memory hierarchy, reducing LLM generation time. RAGCache employs\n dynamic speculative pipelining\n to exploit the streaming search behavior, overlapping retrieval with LLM generation to minimize end-to-end latency. We implement RAGCache based on vLLM and Faiss, and evaluate it on both open-source and production datasets. Experimental results demonstrate that RAGCache reduces the time to first token (TTFT) by up to 4 \u00d7 and improves the throughput by up to 2.1 \u00d7 compared to vLLM integrated with Faiss.\n",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/7326329c09c11aac423ef4910222a16952bb01dc.pdf",
        "venue": "ACM Transactions on Computer Systems",
        "citationCount": 57,
        "score": 57.0
    },
    "160924af0791331ec8fa5a3d526ea125355f3b8b.pdf": {
        "title": "Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting",
        "authors": [
            "Zilong Wang",
            "Zifeng Wang",
            "Long T. Le",
            "Huaixiu Steven Zheng",
            "Swaroop Mishra",
            "Vincent Perot",
            "Yuwei Zhang",
            "Anush Mattapalli",
            "Ankur Taly",
            "Jingbo Shang",
            "Chen-Yu Lee",
            "Tomas Pfister"
        ],
        "published_date": "2024",
        "abstract": "Retrieval augmented generation (RAG) combines the generative abilities of large language models (LLMs) with external knowledge sources to provide more accurate and up-to-date responses. Recent RAG advancements focus on improving retrieval outcomes through iterative LLM refinement or self-critique capabilities acquired through additional instruction tuning of LLMs. In this work, we introduce Speculative RAG - a framework that leverages a larger generalist LM to efficiently verify multiple RAG drafts produced in parallel by a smaller, distilled specialist LM. Each draft is generated from a distinct subset of retrieved documents, offering diverse perspectives on the evidence while reducing input token counts per draft. This approach enhances comprehension of each subset and mitigates potential position bias over long context. Our method accelerates RAG by delegating drafting to the smaller specialist LM, with the larger generalist LM performing a single verification pass over the drafts. Extensive experiments demonstrate that Speculative RAG achieves state-of-the-art performance with reduced latency on TriviaQA, MuSiQue, PopQA, PubHealth, and ARC-Challenge benchmarks. It notably enhances accuracy by up to 12.97% while reducing latency by 50.83% compared to conventional RAG systems on PubHealth.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/160924af0791331ec8fa5a3d526ea125355f3b8b.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 57,
        "score": 57.0
    },
    "22467a50298439854d44a40100bf03c6ce6fa001.pdf": {
        "title": "CyberMetric: A Benchmark Dataset based on Retrieval-Augmented Generation for Evaluating LLMs in Cybersecurity Knowledge",
        "authors": [
            "Norbert Tihanyi",
            "M. Ferrag",
            "Ridhi Jain",
            "Tam\u00e1s Bisztray",
            "M. Debbah"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) are increasingly used across various domains, from software development to cyber threat intelligence. Understanding all the different cybersecurity fields, including topics such as cryptography, reverse engineering, and risk assessment, poses a challenge even for human experts. The research community needs a diverse, accurate, and up-to-date dataset to test the general knowledge of LLMs in cybersecurity. To address this gap, we present CyberMetric-80, CyberMetric-500, CyberMetric-2000, and CyberMetric-10000, which are multiple-choice Q&A benchmark datasets comprising 80, 500, 2000, and 10,000 questions, respectively. By utilizing GPT-3.5 and Retrieval-Augmented Generation (RAG), we collected documents, including NIST standards, research papers, publicly accessible books, RFCs, and other publications in the cybersecurity domain, to generate questions, each with four possible answers. The results underwent several rounds of error checking and refinement. Human experts invested over 200 hours validating the questions and solutions to ensure their accuracy and relevance and to filter out any questions unrelated to cybersecurity. We have evaluated and compared 25 state-of-the-art LLM models on the CyberMetric datasets. In addition to our primary goal of evaluating LLMs, we involved 30 human participants to solve CyberMetric-80 in a closed-book scenario. The results can serve as a reference for comparing the general cybersecurity knowledge of humans and LLMs. The findings revealed that GPT-4o, GPT-4-turbo, Mixtral-8x7B-Instruct, Falcon-180B-Chat, and GEMINI-pro 1.0 were the best-performing LLMs. Additionally, the top LLMs were more accurate than humans on CyberMetric-80, although highly experienced human experts still outperformed small models such as Llama-3-8B, Phi-2 or Gemma-7b. The CyberMetric dataset is publicly available for the research community and can be downloaded from the projects' website: https://github.com/CyberMetric.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/22467a50298439854d44a40100bf03c6ce6fa001.pdf",
        "venue": "Computer Science Symposium in Russia",
        "citationCount": 55,
        "score": 55.0
    },
    "f4e06256ab07727ff4e0465deea83fcf45012354.pdf": {
        "title": "PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models",
        "authors": [
            "Wei Zou",
            "Runpeng Geng",
            "Binghui Wang",
            "Jinyuan Jia"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate these limitations. The key idea of RAG is to ground the answer generation of an LLM on external knowledge retrieved from a knowledge database. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. We find that the knowledge database in a RAG system introduces a new and practical attack surface. Based on this attack surface, we propose PoisonedRAG, the first knowledge corruption attack to RAG, where an attacker could inject a few malicious texts into the knowledge database of a RAG system to induce an LLM to generate an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge corruption attacks as an optimization problem, whose solution is a set of malicious texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on a RAG system, we propose two solutions to solve the optimization problem, respectively. Our results show PoisonedRAG could achieve a 90% attack success rate when injecting five malicious texts for each target question into a knowledge database with millions of texts. We also evaluate several defenses and our results show they are insufficient to defend against PoisonedRAG, highlighting the need for new defenses.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/f4e06256ab07727ff4e0465deea83fcf45012354.pdf",
        "venue": "",
        "citationCount": 53,
        "score": 53.0
    },
    "2bb9a87bdfc8a35bc1813e5a88180f43615785a8.pdf": {
        "title": "Improving Retrieval-Augmented Generation in Medicine with Iterative Follow-up Questions",
        "authors": [
            "Guangzhi Xiong",
            "Qiao Jin",
            "Xiao Wang",
            "Minjia Zhang",
            "Zhiyong Lu",
            "Aidong Zhang"
        ],
        "published_date": "2024",
        "abstract": "The emergent abilities of large language models (LLMs) have demonstrated great potential in solving medical questions. They can possess considerable medical knowledge, but may still hallucinate and are inflexible in the knowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed to enhance the medical question-answering capabilities of LLMs with external knowledge bases, it may still fail in complex cases where multiple rounds of information-seeking are required. To address such an issue, we propose iterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up queries based on previous information-seeking attempts. In each iteration of i-MedRAG, the follow-up queries will be answered by a vanilla RAG system and they will be further used to guide the query generation in the next iteration. Our experiments show the improved performance of various LLMs brought by i-MedRAG compared with vanilla RAG on complex questions from clinical vignettes in the United States Medical Licensing Examination (USMLE), as well as various knowledge tests in the Massive Multitask Language Understanding (MMLU) dataset. Notably, our zero-shot i-MedRAG outperforms all existing prompt engineering and fine-tuning methods on GPT-3.5, achieving an accuracy of 69.68% on the MedQA dataset. In addition, we characterize the scaling properties of i-MedRAG with different iterations of follow-up queries and different numbers of queries per iteration. Our case studies show that i-MedRAG can flexibly ask follow-up queries to form reasoning chains, providing an in-depth analysis of medical questions. To the best of our knowledge, this is the first-of-its-kind study on incorporating follow-up queries into medical RAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/2bb9a87bdfc8a35bc1813e5a88180f43615785a8.pdf",
        "venue": "Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing",
        "citationCount": 51,
        "score": 51.0
    },
    "addd475c96056491539b790c1b264d0855c80fb7.pdf": {
        "title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training",
        "authors": [
            "Feiteng Fang",
            "Yuelin Bai",
            "Shiwen Ni",
            "Min Yang",
            "Xiaojun Chen",
            "Ruifeng Xu"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/addd475c96056491539b790c1b264d0855c80fb7.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 50,
        "score": 50.0
    },
    "a82a1be7639301ea47ebcb346f6119065b68b3d0.pdf": {
        "title": "GRAG: Graph Retrieval-Augmented Generation",
        "authors": [
            "Yuntong Hu",
            "Zhihan Lei",
            "Zhengwu Zhang",
            "Bo Pan",
            "Chen Ling",
            "Liang Zhao"
        ],
        "published_date": "2024",
        "abstract": "Naive Retrieval-Augmented Generation (RAG) focuses on individual documents during retrieval and, as a result, falls short in handling networked documents which are very popular in many applications such as citation graphs, social media, and knowledge graphs. To overcome this limitation, we introduce Graph Retrieval-Augmented Generation (GRAG), which tackles the fundamental challenges in retrieving textual subgraphs and integrating the joint textual and topological information into Large Language Models (LLMs) to enhance its generation. To enable efficient textual subgraph retrieval, we propose a novel divide-and-conquer strategy that retrieves the optimal subgraph structure in linear time. To achieve graph context-aware generation, incorporate textual graphs into LLMs through two complementary views-the text view and the graph view-enabling LLMs to more effectively comprehend and utilize the graph context. Extensive experiments on graph reasoning benchmarks demonstrate that in scenarios requiring multi-hop reasoning on textual graphs, our GRAG approach significantly outperforms current state-of-the-art RAG methods. Our datasets as well as codes of GRAG are available at https://github.com/HuieL/GRAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/a82a1be7639301ea47ebcb346f6119065b68b3d0.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 47,
        "score": 47.0
    },
    "1027d120189fd6cd9aec1af273cd9a5baaf645d7.pdf": {
        "title": "BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models",
        "authors": [
            "Jiaqi Xue",
            "Meng Zheng",
            "Yebowen Hu",
            "Fei Liu",
            "Xun Chen",
            "Qian Lou"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) are constrained by outdated information and a tendency to generate incorrect data, commonly referred to as\"hallucinations.\"Retrieval-Augmented Generation (RAG) addresses these limitations by combining the strengths of retrieval-based methods and generative models. This approach involves retrieving relevant information from a large, up-to-date dataset and using it to enhance the generation process, leading to more accurate and contextually appropriate responses. Despite its benefits, RAG introduces a new attack surface for LLMs, particularly because RAG databases are often sourced from public data, such as the web. In this paper, we propose \\TrojRAG{} to identify the vulnerabilities and attacks on retrieval parts (RAG database) and their indirect attacks on generative parts (LLMs). Specifically, we identify that poisoning several customized content passages could achieve a retrieval backdoor, where the retrieval works well for clean queries but always returns customized poisoned adversarial queries. Triggers and poisoned passages can be highly customized to implement various attacks. For example, a trigger could be a semantic group like\"The Republican Party, Donald Trump, etc.\"Adversarial passages can be tailored to different contents, not only linked to the triggers but also used to indirectly attack generative LLMs without modifying them. These attacks can include denial-of-service attacks on RAG and semantic steering attacks on LLM generations conditioned by the triggers. Our experiments demonstrate that by just poisoning 10 adversarial passages can induce 98.2\\% success rate to retrieve the adversarial passages. Then, these passages can increase the reject ratio of RAG-based GPT-4 from 0.01\\% to 74.6\\% or increase the rate of negative responses from 0.22\\% to 72\\% for targeted queries.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/1027d120189fd6cd9aec1af273cd9a5baaf645d7.pdf",
        "venue": "arXiv.org",
        "citationCount": 46,
        "score": 46.0
    },
    "848772a50cee68e88988ded7522e280d1c490598.pdf": {
        "title": "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models",
        "authors": [
            "Minbyul Jeong",
            "Jiwoong Sohn",
            "Mujeen Sung",
            "Jaewoo Kang"
        ],
        "published_date": "2024",
        "abstract": "Abstract Summary Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens. Our work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions. Using three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less. Similarly, Self-BioRAG outperforms RAG by 8% Rouge-1 score in generating more proficient answers on two long-form question-answering benchmarks on average. Overall, we analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does. We release our data and code for training our framework components and model weights (7B and 13B) to enhance capabilities in biomedical and clinical domains. Availability and implementation Self-BioRAG is available at https://github.com/dmis-lab/self-biorag.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/848772a50cee68e88988ded7522e280d1c490598.pdf",
        "venue": "Bioinform.",
        "citationCount": 45,
        "score": 45.0
    },
    "4dc4644508a7868ad14f6c2c06a34056c6c333f7.pdf": {
        "title": "KRAGEN: a knowledge graph-enhanced RAG framework for biomedical problem solving using large language models",
        "authors": [
            "Nicholas Matsumoto",
            "Jay Moran",
            "Hyunjun Choi",
            "Miguel E. Hernandez",
            "Mythreye Venkatesan",
            "Paul Wang",
            "Jason H. Moore"
        ],
        "published_date": "2024",
        "abstract": "Abstract Motivation Answering and solving complex problems using a large language model (LLM) given a certain domain such as biomedicine is a challenging task that requires both factual consistency and logic, and LLMs often suffer from some major limitations, such as hallucinating false or irrelevant information, or being influenced by noisy data. These issues can compromise the trustworthiness, accuracy, and compliance of LLM-generated text and insights. Results Knowledge Retrieval Augmented Generation ENgine (KRAGEN) is a new tool that combines knowledge graphs, Retrieval Augmented Generation (RAG), and advanced prompting techniques to solve complex problems with natural language. KRAGEN converts knowledge graphs into a vector database and uses RAG to retrieve relevant facts from it. KRAGEN uses advanced prompting techniques: namely graph-of-thoughts (GoT), to dynamically break down a complex problem into smaller subproblems, and proceeds to solve each subproblem by using the relevant knowledge through the RAG framework, which limits the hallucinations, and finally, consolidates the subproblems and provides a solution. KRAGEN\u2019s graph visualization allows the user to interact with and evaluate the quality of the solution\u2019s GoT structure and logic. Availability and implementation KRAGEN is deployed by running its custom Docker containers. KRAGEN is available as open-source from GitHub at: https://github.com/EpistasisLab/KRAGEN.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/4dc4644508a7868ad14f6c2c06a34056c6c333f7.pdf",
        "venue": "Bioinformatics",
        "citationCount": 44,
        "score": 44.0
    },
    "1b0aba023d7aa5fb9853f9e942efb5c243dc1201.pdf": {
        "title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems",
        "authors": [
            "Robert Friel",
            "Masha Belyi",
            "Atindriyo Sanyal"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) has become a standard architectural pattern for incorporating domain-specific knowledge into user-facing chat applications powered by Large Language Models (LLMs). RAG systems are characterized by (1) a document retriever that queries a domain-specific corpus for context information relevant to an input query, and (2) an LLM that generates a response based on the provided query and context. However, comprehensive evaluation of RAG systems remains a challenge due to the lack of unified evaluation criteria and annotated datasets. In response, we introduce RAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k examples. It covers five unique industry-specific domains and various RAG task types. RAGBench examples are sourced from industry corpora such as user manuals, making it particularly relevant for industry applications. Further, we formalize the TRACe evaluation framework: a set of explainable and actionable RAG evaluation metrics applicable across all RAG domains. We release the labeled dataset at https://huggingface.co/datasets/rungalileo/ragbench. RAGBench explainable labels facilitate holistic evaluation of RAG systems, enabling actionable feedback for continuous improvement of production applications. Thorough extensive benchmarking, we find that LLM-based RAG evaluation methods struggle to compete with a finetuned RoBERTa model on the RAG evaluation task. We identify areas where existing approaches fall short and propose the adoption of RAGBench with TRACe towards advancing the state of RAG evaluation systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/1b0aba023d7aa5fb9853f9e942efb5c243dc1201.pdf",
        "venue": "arXiv.org",
        "citationCount": 43,
        "score": 43.0
    },
    "125a9c020316341bde65ea374f19caf346cfecfa.pdf": {
        "title": "Graph Retrieval-Augmented Generation for Large Language Models: A Survey",
        "authors": [
            "T. Procko",
            "Omar Ochoa"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) demonstrate general knowledge, but they suffer when specifically needed knowledge is not present in their training set. Two approaches to ameliorating this, without retraining, are 1) prompt engineering and 2) Retrieval-Augmented Generation (RAG). RAG is a form of prompt engineering, insofar as relevant lexical snippets retrieved from RAG corpora are vectorized and aggregated with prompts. However, RAG documents are often noisy, i.e., while relevant to a given prompt, they can contain much other information that obfuscates the desired snippet. If the purpose of pretraining a LLM on massive and general corpora is to engender a generally applicable model, RAG is not: it is a means of LLM optimization, and as such, RAG document selection must be precise, not general. For expert tasks, it is imperative that a RAG corpus be as noise-free as possible, in much the same way a good prompt should be free of irrelevant text. Knowledge Graphs (KGs) provide a concise means of representing domain knowledge free of noisy information. This paper surveys work incorporating KGs with LLM RAG, intending to equip scientists with a better understanding of this novel research area for future work.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/125a9c020316341bde65ea374f19caf346cfecfa.pdf",
        "venue": "2024 Conference on AI, Science, Engineering, and Technology (AIxSET)",
        "citationCount": 41,
        "score": 41.0
    },
    "810b3f4475f22f6ca0f1bded3b8523f3cdebee8d.pdf": {
        "title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems",
        "authors": [
            "Hongru Wang",
            "Wenyu Huang",
            "Yang Deng",
            "Rui Wang",
            "Zezhong Wang",
            "Yufei Wang",
            "Fei Mi",
            "Jeff Z. Pan",
            "Kam-Fai Wong"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their behavior to diverse task requirements. Meanwhile, evaluation tokens gauge the relevance score between the dialogue context and the retrieved evidence. In addition, we carefully design a self-refinement mechanism to iteratively refine the generated response considering 1) the consistency scores between the generated response and retrieved evidence; and 2) the relevance scores. Experiments on two personalized datasets (DuLeMon and KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge source selection and response generation task with itself as a retriever in a unified manner. Extensive analyses and discussions are provided for shedding some new perspectives for personalized dialogue systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/810b3f4475f22f6ca0f1bded3b8523f3cdebee8d.pdf",
        "venue": "arXiv.org",
        "citationCount": 41,
        "score": 41.0
    },
    "908d45b0d2b88ba72ee501c368eb618d29d61ce0.pdf": {
        "title": "A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models",
        "authors": [
            "Qinggang Zhang",
            "Shengyuan Chen",
            "Yuan-Qi Bei",
            "Zheng Yuan",
            "Huachi Zhou",
            "Zijin Hong",
            "Junnan Dong",
            "Hao Chen",
            "Yi Chang",
            "Xiao Huang"
        ],
        "published_date": "2025",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks, yet their application to specialized domains remains challenging due to the need for deep expertise. Retrieval-Augmented generation (RAG) has emerged as a promising solution to customize LLMs for professional fields by seamlessly integrating external knowledge bases, enabling real-time access to domain-specific expertise during inference. Despite its potential, traditional RAG systems, based on flat text retrieval, face three critical challenges: (i) complex query understanding in professional contexts, (ii) difficulties in knowledge integration across distributed sources, and (iii) system efficiency bottlenecks at scale. This survey presents a systematic analysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new paradigm that revolutionizes domain-specific LLM applications. GraphRAG addresses traditional RAG limitations through three key innovations: (i) graph-structured knowledge representation that explicitly captures entity relationships and domain hierarchies, (ii) efficient graph-based retrieval techniques that enable context-preserving knowledge retrieval with multihop reasoning ability, and (iii) structure-aware knowledge integration algorithms that leverage retrieved knowledge for accurate and logical coherent generation of LLMs. In this survey, we systematically analyze the technical foundations of GraphRAG and examine current implementations across various professional domains, identifying key technical challenges and promising research directions. All the related resources of GraphRAG, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphRAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/908d45b0d2b88ba72ee501c368eb618d29d61ce0.pdf",
        "venue": "arXiv.org",
        "citationCount": 40,
        "score": 40.0
    },
    "d8d0d704446ffaf09b9722360ed76341934ec3a3.pdf": {
        "title": "TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models",
        "authors": [
            "Pengzhou Cheng",
            "Yidong Ding",
            "Tianjie Ju",
            "Zongru Wu",
            "Wei Du",
            "Ping Yi",
            "Zhuosheng Zhang",
            "Gongshen Liu"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have raised concerns about potential security threats despite performing significantly in Natural Language Processing (NLP). Backdoor attacks initially verified that LLM is doing substantial harm at all stages, but the cost and robustness have been criticized. Attacking LLMs is inherently risky in security review, while prohibitively expensive. Besides, the continuous iteration of LLMs will degrade the robustness of backdoors. In this paper, we propose TrojanRAG, which employs a joint backdoor attack in the Retrieval-Augmented Generation, thereby manipulating LLMs in universal attack scenarios. Specifically, the adversary constructs elaborate target contexts and trigger sets. Multiple pairs of backdoor shortcuts are orthogonally optimized by contrastive learning, thus constraining the triggering conditions to a parameter subspace to improve the matching. To improve the recall of the RAG for the target contexts, we introduce a knowledge graph to construct structured data to achieve hard matching at a fine-grained level. Moreover, we normalize the backdoor scenarios in LLMs to analyze the real harm caused by backdoors from both attackers' and users' perspectives and further verify whether the context is a favorable tool for jailbreaking models. Extensive experimental results on truthfulness, language understanding, and harmfulness show that TrojanRAG exhibits versatility threats while maintaining retrieval capabilities on normal queries.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/d8d0d704446ffaf09b9722360ed76341934ec3a3.pdf",
        "venue": "arXiv.org",
        "citationCount": 39,
        "score": 39.0
    },
    "29528d8cb030a65f62a35b1237f1f5483077ad0a.pdf": {
        "title": "Inference Scaling for Long-Context Retrieval Augmented Generation",
        "authors": [
            "Zhenrui Yue",
            "Honglei Zhuang",
            "Aijun Bai",
            "Kai Hui",
            "R. Jagerman",
            "Hansi Zeng",
            "Zhen Qin",
            "Dong Wang",
            "Xuanhui Wang",
            "Michael Bendersky"
        ],
        "published_date": "2024",
        "abstract": "The scaling of inference computation has unlocked the potential of long-context large language models (LLMs) across diverse settings. For knowledge-intensive tasks, the increased compute is often allocated to incorporate more external knowledge. However, without effectively utilizing such knowledge, solely expanding context does not always enhance performance. In this work, we investigate inference scaling for retrieval augmented generation (RAG), exploring the combination of multiple strategies beyond simply increasing the quantity of knowledge, including in-context learning and iterative prompting. These strategies provide additional flexibility to scale test-time computation (e.g., by increasing retrieved documents or generation steps), thereby enhancing LLMs' ability to effectively acquire and utilize contextual information. We address two key questions: (1) How does RAG performance benefit from the scaling of inference computation when optimally configured? (2) Can we predict the optimal test-time compute allocation for a given budget by modeling the relationship between RAG performance and inference parameters? Our observations reveal that increasing inference computation leads to nearly linear gains in RAG performance when optimally allocated, a relationship we describe as the inference scaling laws for RAG. Building on this, we further develop the computation allocation model to estimate RAG performance across different inference configurations. The model predicts optimal inference parameters under various computation constraints, which align closely with the experimental results. By applying these optimal configurations, we demonstrate that scaling inference compute on long-context LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/29528d8cb030a65f62a35b1237f1f5483077ad0a.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 38,
        "score": 38.0
    },
    "858cbd99d5a3d2658254d055cd26e06f81050927.pdf": {
        "title": "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design",
        "authors": [
            "Wenqi Jiang",
            "Shuai Zhang",
            "Boran Han",
            "Jie Wang",
            "Bernie Wang",
            "Tim Kraska"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) can enhance the generation quality of large language models (LLMs) by incorporating external token databases. However, retrievals from large databases can constitute a substantial portion of the overall generation time, particularly when retrievals are periodically performed to align the retrieved content with the latest states of generation. In this paper, we introduce PipeRAG, a novel algorithm-system co-design approach to reduce generation latency and enhance generation quality. PipeRAG integrates (1) pipeline parallelism to enable concurrent retrieval and generation processes, (2) flexible retrieval intervals to maximize the efficiency of pipeline parallelism, and (3) a performance model to automatically balance retrieval quality and latency based on the generation states and underlying hardware. Our evaluation shows that, by combining the three aforementioned methods, PipeRAG achieves up to 2.6$\\times$ speedup in end-to-end generation latency while improving generation quality. These promising results showcase the effectiveness of co-designing algorithms with underlying systems, paving the way for the adoption of PipeRAG in future RAG systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/858cbd99d5a3d2658254d055cd26e06f81050927.pdf",
        "venue": "arXiv.org",
        "citationCount": 38,
        "score": 38.0
    },
    "bbf77bd463768a5322a63ffc19322d5c764493e0.pdf": {
        "title": "Development of a liver disease\u2013specific large language model chat interface using retrieval-augmented generation",
        "authors": [
            "J. Ge",
            "Steve Sun",
            "Joseph Owens",
            "Victor Galvez",
            "Oksana Gologorskaya",
            "Jennifer C Lai",
            "Mark J. Pletcher",
            "Ki Lai"
        ],
        "published_date": "2024",
        "abstract": "Background and Aims: Large language models (LLMs) have significant capabilities in clinical information processing tasks. Commercially available LLMs, however, are not optimized for clinical uses and are prone to generating hallucinatory information. Retrieval-augmented generation (RAG) is an enterprise architecture that allows the embedding of customized data into LLMs. This approach \u201cspecializes\u201d the LLMs and is thought to reduce hallucinations. Approach and Results We developed \u201cLiVersa,\u201d a liver disease\u2013specific LLM, by using our institution\u2019s protected health information-complaint text embedding and LLM platform, \u201cVersa.\u201d We conducted RAG on 30 publicly available American Association for the Study of Liver Diseases guidance documents to be incorporated into LiVersa. We evaluated LiVersa\u2019s performance by conducting 2 rounds of testing. First, we compared LiVersa\u2019s outputs versus those of trainees from a previously published knowledge assessment. LiVersa answered all 10 questions correctly. Second, we asked 15 hepatologists to evaluate the outputs of 10 hepatology topic questions generated by LiVersa, OpenAI\u2019s ChatGPT 4, and Meta\u2019s Large Language Model Meta AI 2. LiVersa\u2019s outputs were more accurate but were rated less comprehensive and safe compared to those of ChatGPT 4. Results: We evaluated LiVersa\u2019s performance by conducting 2 rounds of testing. First, we compared LiVersa\u2019s outputs versus those of trainees from a previously published knowledge assessment. LiVersa answered all 10 questions correctly. Second, we asked 15 hepatologists to evaluate the outputs of 10 hepatology topic questions generated by LiVersa, OpenAI\u2019s ChatGPT 4, and Meta\u2019s Large Language Model Meta AI 2. LiVersa\u2019s outputs were more accurate but were rated less comprehensive and safe compared to those of ChatGPT 4. Conclusions: In this demonstration, we built disease-specific and protected health information-compliant LLMs using RAG. While LiVersa demonstrated higher accuracy in answering questions related to hepatology, there were some deficiencies due to limitations set by the number of documents used for RAG. LiVersa will likely require further refinement before potential live deployment. The LiVersa prototype, however, is a proof of concept for utilizing RAG to customize LLMs for clinical use cases.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/bbf77bd463768a5322a63ffc19322d5c764493e0.pdf",
        "venue": "Hepatology",
        "citationCount": 37,
        "score": 37.0
    },
    "0576e9ab604ba4bf20cd5947f3c4a2c609ac2705.pdf": {
        "title": "A Survey on RAG Meets LLMs: Towards Retrieval-Augmented Large Language Models",
        "authors": [
            "Yujuan Ding",
            "Wenqi Fan",
            "Liang-bo Ning",
            "Shijie Wang",
            "Hengyun Li",
            "Dawei Yin",
            "Tat-Seng Chua",
            "Qing Li"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/0576e9ab604ba4bf20cd5947f3c4a2c609ac2705.pdf",
        "venue": "arXiv.org",
        "citationCount": 36,
        "score": 36.0
    },
    "f3658afcd181e4078e1e96ff86eac224fd92faab.pdf": {
        "title": "ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability",
        "authors": [
            "ZhongXiang Sun",
            "Xiaoxue Zang",
            "Kai Zheng",
            "Yang Song",
            "Jun Xu",
            "Xiao Zhang",
            "Weijie Yu",
            "Han Li"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) models are designed to incorporate external knowledge, reducing hallucinations caused by insufficient parametric (internal) knowledge. However, even with accurate and relevant retrieved content, RAG models can still produce hallucinations by generating outputs that conflict with the retrieved information. Detecting such hallucinations requires disentangling how Large Language Models (LLMs) utilize external and parametric knowledge. Current detection methods often focus on one of these mechanisms or without decoupling their intertwined effects, making accurate detection difficult. In this paper, we investigate the internal mechanisms behind hallucinations in RAG scenarios. We discover hallucinations occur when the Knowledge FFNs in LLMs overemphasize parametric knowledge in the residual stream, while Copying Heads fail to effectively retain or integrate external knowledge from retrieved content. Based on these findings, we propose ReDeEP, a novel method that detects hallucinations by decoupling LLM's utilization of external context and parametric knowledge. Our experiments show that ReDeEP significantly improves RAG hallucination detection accuracy. Additionally, we introduce AARF, which mitigates hallucinations by modulating the contributions of Knowledge FFNs and Copying Heads.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/f3658afcd181e4078e1e96ff86eac224fd92faab.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 34,
        "score": 34.0
    },
    "a681b1085c088c51347cdb9358dd344081d29c99.pdf": {
        "title": "Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation",
        "authors": [
            "Shengjie Ma",
            "Chengjin Xu",
            "Xuhui Jiang",
            "Muzhi Li",
            "Huaren Qu",
            "Cehao Yang",
            "Jiaxin Mao",
            "Jian Guo"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) has improved large language models (LLMs) by using knowledge retrieval to overcome knowledge deficiencies. However, current RAG methods often fall short of ensuring the depth and completeness of retrieved information, which is necessary for complex reasoning tasks. In this work, we introduce Think-on-Graph 2.0 (ToG-2), a hybrid RAG framework that iteratively retrieves information from both unstructured and structured knowledge sources in a tight-coupling manner. Specifically, ToG-2 leverages knowledge graphs (KGs) to link documents via entities, facilitating deep and knowledge-guided context retrieval. Simultaneously, it utilizes documents as entity contexts to achieve precise and efficient graph retrieval. ToG-2 alternates between graph retrieval and context retrieval to search for in-depth clues relevant to the question, enabling LLMs to generate answers. We conduct a series of well-designed experiments to highlight the following advantages of ToG-2: 1) ToG-2 tightly couples the processes of context retrieval and graph retrieval, deepening context retrieval via the KG while enabling reliable graph retrieval based on contexts; 2) it achieves deep and faithful reasoning in LLMs through an iterative knowledge retrieval process of collaboration between contexts and the KG; and 3) ToG-2 is training-free and plug-and-play compatible with various LLMs. Extensive experiments demonstrate that ToG-2 achieves overall state-of-the-art (SOTA) performance on 6 out of 7 knowledge-intensive datasets with GPT-3.5, and can elevate the performance of smaller models (e.g., LLAMA-2-13B) to the level of GPT-3.5's direct reasoning. The source code is available on https://github.com/IDEA-FinAI/ToG-2.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/a681b1085c088c51347cdb9358dd344081d29c99.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 32,
        "score": 32.0
    },
    "aa32cce28aad1d04fea026860c3e2d4a218d9a57.pdf": {
        "title": "Telco-RAG: Navigating the Challenges of Retrieval Augmented Language Models for Telecommunications",
        "authors": [
            "Andrei-Laurentiu Bornea",
            "Fadhel Ayed",
            "Antonio De Domenico",
            "Nicola Piovesan",
            "Ali Maatouk"
        ],
        "published_date": "2024",
        "abstract": "The application of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems in the telecommunication domain presents unique challenges, primarily due to the complex nature of telecom standard documents and the rapid evolution of the field. The paper introduces Telco-RAG, 1 an open-source RAG framework designed to handle the specific needs of telecommunications standards, particularly 3rd Generation Partnership Project (3GPP) documents. Telco-RAG addresses the critical challenges of implementing a RAG pipeline on highly technical content, paving the way for applying LLMs in telecommunications and offering guidelines for RAG implementation in other technical domains.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/aa32cce28aad1d04fea026860c3e2d4a218d9a57.pdf",
        "venue": "Global Communications Conference",
        "citationCount": 32,
        "score": 32.0
    },
    "f091acf9dc2cc486c253dcead3a74ba916c64eb7.pdf": {
        "title": "IM-RAG: Multi-Round Retrieval-Augmented Generation Through Learning Inner Monologues",
        "authors": [
            "Diji Yang",
            "Jinmeng Rao",
            "Kezhen Chen",
            "Xiaoyuan Guo",
            "Yawen Zhang",
            "Jie Yang",
            "Yi Zhang"
        ],
        "published_date": "2024",
        "abstract": "Although the Retrieval-Augmented Generation (RAG) paradigms can use external knowledge to enhance and ground the outputs of Large Language Models (LLMs) to mitigate generative hallucinations and static knowledge base problems, they still suffer from limited flexibility in adopting Information Retrieval (IR) systems with varying capabilities, constrained interpretability during the multi-round retrieval process, and a lack of end-to-end optimization. To address these challenges, we propose a novel LLM-centric approach, IM-RAG, that integrates IR systems with LLMs to support multi-round RAG through learning Inner Monologues (IM, i.e., the human inner voice that narrates one's thoughts). During the IM process, the LLM serves as the core reasoning model (i.e., Reasoner ) to either propose queries to collect more information via the Retriever or to provide a final answer based on the conversational context. We also introduce a Refiner that improves the outputs from the Retriever, effectively bridging the gap between the Reasoner and IR modules with varying capabilities and fostering multi-round communications. The entire IM process is optimized via Reinforcement Learning (RL) where a Progress Tracker is incorporated to provide mid-step rewards, and the answer prediction is further separately optimized via Supervised Fine-Tuning (SFT). We conduct extensive experiments with the HotPotQA dataset, a popular benchmark for retrieval-based, multi-step question-answering. The results show that our approach achieves state-of-the-art (SOTA) performance while providing high flexibility in integrating IR modules as well as strong interpretability exhibited in the learned inner monologue.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/f091acf9dc2cc486c253dcead3a74ba916c64eb7.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 32,
        "score": 32.0
    },
    "1cc6cc4960f7df59e7813d9a8e11098d0a0d0720.pdf": {
        "title": "DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models",
        "authors": [
            "Weihang Su",
            "Yichen Tang",
            "Qingyao Ai",
            "Zhijing Wu",
            "Yiqun Liu"
        ],
        "published_date": "2024",
        "abstract": "Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs). There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve). However, current dynamic RAG methods fall short in both aspects. Firstly, the strategies for deciding when to retrieve often rely on static rules. Moreover, the strategies for deciding what to retrieve typically limit themselves to the LLM's most recent sentence or the last few tokens, while the LLM's real-time information needs may span across the entire context. To overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic Retrieval Augmented Generation based on the real-time Information Needs of LLMs. Our framework is specifically designed to make decisions on when and what to retrieve based on the LLM's real-time information needs during the text generation process. We evaluate DRAGIN along with existing methods comprehensively over 4 knowledge-intensive generation datasets. Experimental results show that DRAGIN achieves superior performance on all tasks, demonstrating the effectiveness of our method. We have open-sourced all the code, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/1cc6cc4960f7df59e7813d9a8e11098d0a0d0720.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 31,
        "score": 31.0
    },
    "63a1617af179ee8b5b096b3038913a19166168d4.pdf": {
        "title": "Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models",
        "authors": [
            "Shayekh Bin Islam",
            "Md Asib Rahman",
            "K. S. M. T. Hossain",
            "Enamul Hoque",
            "Shafiq R. Joty",
            "Md. Rizwan Parvez"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) has been shown to enhance the factual accuracy of Large Language Models (LLMs), but existing methods often suffer from limited reasoning capabilities in effectively using the retrieved evidence, particularly when using open-source LLMs. To mitigate this gap, we introduce a novel framework, Open-RAG, designed to enhance reasoning capabilities in RAG with open-source LLMs. Our framework transforms an arbitrary dense LLM into a parameter-efficient sparse mixture of experts (MoE) model capable of handling complex reasoning tasks, including both single- and multi-hop queries. Open-RAG uniquely trains the model to navigate challenging distractors that appear relevant but are misleading. As a result, Open-RAG leverages latent learning, dynamically selecting relevant experts and integrating external knowledge effectively for more accurate and contextually relevant responses. In addition, we propose a hybrid adaptive retrieval method to determine retrieval necessity and balance the trade-off between performance gain and inference speed. Experimental results show that the Llama2-7B-based Open-RAG outperforms state-of-the-art LLMs and RAG models such as ChatGPT, Self-RAG, and Command R+ in various knowledge-intensive tasks. We open-source our code and models at https://openragmoe.github.io/",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/63a1617af179ee8b5b096b3038913a19166168d4.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 31,
        "score": 31.0
    },
    "83939671534dc3d374c9bc4e3e03b5ec2c7ba301.pdf": {
        "title": "Improving large language model applications in biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and clinical development guidelines",
        "authors": [
            "Siru Liu",
            "Allison B. McCoy",
            "Adam Wright"
        ],
        "published_date": "2025",
        "abstract": "Abstract Objective The objectives of this study are to synthesize findings from recent research of retrieval-augmented generation (RAG) and large language models (LLMs) in biomedicine and provide clinical development guidelines to improve effectiveness. Materials and Methods We conducted a systematic literature review and a meta-analysis. The report was created in adherence to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses 2020 analysis. Searches were performed in 3 databases (PubMed, Embase, PsycINFO) using terms related to \u201cretrieval augmented generation\u201d and \u201clarge language model,\u201d for articles published in 2023 and 2024. We selected studies that compared baseline LLM performance with RAG performance. We developed a random-effect meta-analysis model, using odds ratio as the effect size. Results Among 335 studies, 20 were included in this literature review. The pooled effect size was 1.35, with a 95% confidence interval of 1.19-1.53, indicating a statistically significant effect (P\u2009=\u2009.001). We reported clinical tasks, baseline LLMs, retrieval sources and strategies, as well as evaluation methods. Discussion Building on our literature review, we developed Guidelines for Unified Implementation and Development of Enhanced LLM Applications with RAG in Clinical Settings to inform clinical applications using RAG. Conclusion Overall, RAG implementation showed a 1.35 odds ratio increase in performance compared to baseline LLMs. Future research should focus on (1) system-level enhancement: the combination of RAG and agent, (2) knowledge-level enhancement: deep integration of knowledge into LLM, and (3) integration-level enhancement: integrating RAG systems within electronic health records.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/83939671534dc3d374c9bc4e3e03b5ec2c7ba301.pdf",
        "venue": "J. Am. Medical Informatics Assoc.",
        "citationCount": 31,
        "score": 31.0
    },
    "7423e5c903fb2befaf471cae64e2530f7c1d0404.pdf": {
        "title": "Development and Testing of Retrieval Augmented Generation in Large Language Models - A Case Study Report",
        "authors": [
            "Yuhe Ke",
            "Liyuan Jin",
            "Kabilan Elangovan",
            "H. Abdullah",
            "Nan Liu",
            "Alex Tiong Heng Sia",
            "Chai Rick Soh",
            "Joshua Yi Min Tung",
            "J. Ong",
            "D. Ting"
        ],
        "published_date": "2024",
        "abstract": "Purpose: Large Language Models (LLMs) hold significant promise for medical applications. Retrieval Augmented Generation (RAG) emerges as a promising approach for customizing domain knowledge in LLMs. This case study presents the development and evaluation of an LLM-RAG pipeline tailored for healthcare, focusing specifically on preoperative medicine. Methods: We developed an LLM-RAG model using 35 preoperative guidelines and tested it against human-generated responses, with a total of 1260 responses evaluated. The RAG process involved converting clinical documents into text using Python-based frameworks like LangChain and Llamaindex, and processing these texts into chunks for embedding and retrieval. Vector storage techniques and selected embedding models to optimize data retrieval, using Pinecone for vector storage with a dimensionality of 1536 and cosine similarity for loss metrics. Human-generated answers, provided by junior doctors, were used as a comparison. Results: The LLM-RAG model generated answers within an average of 15-20 seconds, significantly faster than the 10 minutes typically required by humans. Among the basic LLMs, GPT4.0 exhibited the best accuracy of 80.1%. This accuracy was further increased to 91.4% when the model was enhanced with RAG. Compared to the human-generated instructions, which had an accuracy of 86.3%, the performance of the GPT4.0 RAG model demonstrated non-inferiority (p=0.610). Conclusions: In this case study, we demonstrated a LLM-RAG model for healthcare implementation. The pipeline shows the advantages of grounded knowledge, upgradability, and scalability as important aspects of healthcare LLM deployment.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/7423e5c903fb2befaf471cae64e2530f7c1d0404.pdf",
        "venue": "arXiv.org",
        "citationCount": 30,
        "score": 30.0
    },
    "0c42f77546551ae8b103057771a3b1b25cdd35bd.pdf": {
        "title": "Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey",
        "authors": [
            "Bo Ni",
            "Zheyuan Liu",
            "Leyao Wang",
            "Yongjia Lei",
            "Yuying Zhao",
            "Xueqi Cheng",
            "Qingkai Zeng",
            "Luna Dong",
            "Yinglong Xia",
            "K. Kenthapadi",
            "Ryan A. Rossi",
            "Franck Dernoncourt",
            "Md. Mehrab Tanjim",
            "Nesreen K. Ahmed",
            "Xiaorui Liu",
            "Wenqi Fan",
            "Erik Blasch",
            "Yu Wang",
            "Meng Jiang",
            "Tyler Derr"
        ],
        "published_date": "2025",
        "abstract": "Retrieval-Augmented Generation (RAG) is an advanced technique designed to address the challenges of Artificial Intelligence-Generated Content (AIGC). By integrating context retrieval into content generation, RAG provides reliable and up-to-date external knowledge, reduces hallucinations, and ensures relevant context across a wide range of tasks. However, despite RAG's success and potential, recent studies have shown that the RAG paradigm also introduces new risks, including robustness issues, privacy concerns, adversarial attacks, and accountability issues. Addressing these risks is critical for future applications of RAG systems, as they directly impact their trustworthiness. Although various methods have been developed to improve the trustworthiness of RAG methods, there is a lack of a unified perspective and framework for research in this topic. Thus, in this paper, we aim to address this gap by providing a comprehensive roadmap for developing trustworthy RAG systems. We place our discussion around five key perspectives: reliability, privacy, safety, fairness, explainability, and accountability. For each perspective, we present a general framework and taxonomy, offering a structured approach to understanding the current challenges, evaluating existing solutions, and identifying promising future research directions. To encourage broader adoption and innovation, we also highlight the downstream applications where trustworthy RAG systems have a significant impact.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/0c42f77546551ae8b103057771a3b1b25cdd35bd.pdf",
        "venue": "arXiv.org",
        "citationCount": 29,
        "score": 29.0
    },
    "27f8af480211586d07ff5ee6441ff6724ce85f4e.pdf": {
        "title": "PlanRAG: A Plan-then-Retrieval Augmented Generation for Generative Large Language Models as Decision Makers",
        "authors": [
            "Myeonghwa Lee",
            "Seonho An",
            "Min-Soo Kim"
        ],
        "published_date": "2024",
        "abstract": "In this paper, we conduct a study to utilize LLMs as a solution for decision making that requires complex data analysis. We define **Decision QA** as the task of answering the best decision, d_{best}, for a decision-making question Q, business rules R and a database D. Since there is no benchmark that can examine Decision QA, we propose Decision QA benchmark, **DQA**. It has two scenarios, Locating and Building, constructed from two video games (Europa Universalis IV and Victoria 3) that have almost the same goal as Decision QA. To address Decision QA effectively, we also propose a new RAG technique called the *iterative plan-then-retrieval augmented generation* (**PlanRAG**). Our PlanRAG-based LM generates the plan for decision making as the first step, and the retriever generates the queries for data analysis as the second step. The proposed method outperforms the state-of-the-art iterative RAG method by 15.8% in the Locating scenario and by 7.4% in the Building scenario, respectively. We release our code and benchmark at https://github.com/myeon9h/PlanRAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/27f8af480211586d07ff5ee6441ff6724ce85f4e.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 29,
        "score": 29.0
    },
    "16b459de55727171aff6ea674535bea499e58261.pdf": {
        "title": "Simple is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation",
        "authors": [
            "Mufei Li",
            "Siqi Miao",
            "Pan Li"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) demonstrate strong reasoning abilities but face limitations such as hallucinations and outdated knowledge. Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by grounding LLM outputs in structured external knowledge from KGs. However, current KG-based RAG frameworks still struggle to optimize the trade-off between retrieval effectiveness and efficiency in identifying a suitable amount of relevant graph information for the LLM to digest. We introduce SubgraphRAG, extending the KG-based RAG framework that retrieves subgraphs and leverages LLMs for reasoning and answer prediction. Our approach innovatively integrates a lightweight multilayer perceptron with a parallel triple-scoring mechanism for efficient and flexible subgraph retrieval while encoding directional structural distances to enhance retrieval effectiveness. The size of retrieved subgraphs can be flexibly adjusted to match the query's need and the downstream LLM's capabilities. This design strikes a balance between model complexity and reasoning power, enabling scalable and generalizable retrieval processes. Notably, based on our retrieved subgraphs, smaller LLMs like Llama3.1-8B-Instruct deliver competitive results with explainable reasoning, while larger models like GPT-4o achieve state-of-the-art accuracy compared with previous baselines -- all without fine-tuning. Extensive evaluations on the WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency, accuracy, and reliability by reducing hallucinations and improving response grounding.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/16b459de55727171aff6ea674535bea499e58261.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 28,
        "score": 28.0
    },
    "8c85026fe0d5c96fb275b81d483f8910db6ada03.pdf": {
        "title": "Retrieval augmented generation for 10 large language models and its generalizability in assessing medical fitness",
        "authors": [
            "Yuhe Ke",
            "Liyuan Jin",
            "Kabilan Elangovan",
            "H. Abdullah",
            "Nan Liu",
            "Alex Tiong Heng Sia",
            "Chai Rick Soh",
            "Joshua Yi Min Tung",
            "J. Ong",
            "C. Kuo",
            "Shao-Chun Wu",
            "V. Kovacheva",
            "D. Ting"
        ],
        "published_date": "2025",
        "abstract": "Large Language Models (LLMs) hold promise for medical applications but often lack domain-specific expertise. Retrieval Augmented Generation (RAG) enables customization by integrating specialized knowledge. This study assessed the accuracy, consistency, and safety of LLM-RAG models in determining surgical fitness and delivering preoperative instructions using 35 local and 23 international guidelines. Ten LLMs (e.g., GPT3.5, GPT4, GPT4o, Gemini, Llama2, and Llama3, Claude) were tested across 14 clinical scenarios. A total of 3234 responses were generated and compared to 448 human-generated answers. The GPT4 LLM-RAG model with international guidelines generated answers within 20\u2009s and achieved the highest accuracy, which was significantly better than human-generated responses (96.4% vs. 86.6%, p\u2009=\u20090.016). Additionally, the model exhibited an absence of hallucinations and produced more consistent output than humans. This study underscores the potential of GPT-4-based LLM-RAG models to deliver highly accurate, efficient, and consistent preoperative assessments.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/8c85026fe0d5c96fb275b81d483f8910db6ada03.pdf",
        "venue": "npj Digit. Medicine",
        "citationCount": 26,
        "score": 26.0
    },
    "8d0df3168870fd17b36ecd5575e406feb5a5a1b5.pdf": {
        "title": "M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions",
        "authors": [
            "Zheng Wang",
            "Shu Xian Teo",
            "Jieer Ouyang",
            "Yongjun Xu",
            "Wei Shi"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant memories from an external database. However, existing RAG methods typically organize all memories in a whole database, potentially limiting focus on crucial memories and introducing noise. In this paper, we introduce a multiple partition paradigm for RAG (called M-RAG), where each database partition serves as a basic unit for RAG execution. Based on this paradigm, we propose a novel framework that leverages LLMs with Multi-Agent Reinforcement Learning to optimize different language generation tasks explicitly. Through comprehensive experiments conducted on seven datasets, spanning three language generation tasks and involving three distinct language model architectures, we confirm that M-RAG consistently outperforms various baseline methods, achieving improvements of 11%, 8%, and 12% for text summarization, machine translation, and dialogue generation, respectively.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/8d0df3168870fd17b36ecd5575e406feb5a5a1b5.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 25,
        "score": 25.0
    },
    "32c260ddd7e2d0b05c58f2e67d244b8036699db4.pdf": {
        "title": "C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models",
        "authors": [
            "Mintong Kang",
            "Nezihe Merve Gurel",
            "Ning Yu",
            "D. Song",
            "Bo Li"
        ],
        "published_date": "2024",
        "abstract": "Despite the impressive capabilities of large language models (LLMs) across diverse applications, they still suffer from trustworthiness issues, such as hallucinations and misalignments. Retrieval-augmented language models (RAG) have been proposed to enhance the credibility of generations by grounding external knowledge, but the theoretical understandings of their generation risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed lead to low generation risks, 2) how to provide provable guarantees on the generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions enable RAG models to reduce generation risks. We propose C-RAG, the first framework to certify generation risks for RAG models. Specifically, we provide conformal risk analysis for RAG models and certify an upper confidence bound of generation risks, which we refer to as conformal generation risk. We also provide theoretical guarantees on conformal generation risks for general bounded risk functions under test distribution shifts. We prove that RAG achieves a lower conformal generation risk than that of a single LLM when the quality of the retrieval model and transformer is non-trivial. Our intensive empirical results demonstrate the soundness and tightness of our conformal generation risk guarantees across four widely-used NLP datasets on four state-of-the-art retrieval models.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/32c260ddd7e2d0b05c58f2e67d244b8036699db4.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 25,
        "score": 25.0
    },
    "522c47365931e0ad722fbdac463ae415c97c65e4.pdf": {
        "title": "Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition",
        "authors": [
            "Demiao Lin"
        ],
        "published_date": "2024",
        "abstract": "With the rapid development of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) has become a predominant method in the field of professional knowledge-based question answering. Presently, major foundation model companies have opened up Embedding and Chat API interfaces, and frameworks like LangChain have already integrated the RAG process. It appears that the key models and steps in RAG have been resolved, leading to the question: are professional knowledge QA systems now approaching perfection? This article discovers that current primary methods depend on the premise of accessing high-quality text corpora. However, since professional documents are mainly stored in PDFs, the low accuracy of PDF parsing significantly impacts the effectiveness of professional knowledge-based QA. We conducted an empirical RAG experiment across hundreds of questions from the corresponding real-world professional documents. The results show that, ChatDOC, a RAG system equipped with a panoptic and pinpoint PDF parser, retrieves more accurate and complete segments, and thus better answers. Empirical experiments show that ChatDOC is superior to baseline on nearly 47% of questions, ties for 38% of cases, and falls short on only 15% of cases. It shows that we may revolutionize RAG with enhanced PDF structure recognition.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/522c47365931e0ad722fbdac463ae415c97c65e4.pdf",
        "venue": "arXiv.org",
        "citationCount": 25,
        "score": 25.0
    },
    "55c3095681acc82780508b0e484dba0c30cf1caa.pdf": {
        "title": "Automated Evaluation of Retrieval-Augmented Language Models with Task-Specific Exam Generation",
        "authors": [
            "Gauthier Guinet",
            "Behrooz Omidvar-Tehrani",
            "Anoop Deoras",
            "Laurent Callot"
        ],
        "published_date": "2024",
        "abstract": "We propose a new method to measure the task-specific accuracy of Retrieval-Augmented Large Language Models (RAG). Evaluation is performed by scoring the RAG on an automatically-generated synthetic exam composed of multiple choice questions based on the corpus of documents associated with the task. Our method is an automated, cost-efficient, interpretable, and robust strategy to select the optimal components for a RAG system. We leverage Item Response Theory (IRT) to estimate the quality of an exam and its informativeness on task-specific accuracy. IRT also provides a natural way to iteratively improve the exam by eliminating the exam questions that are not sufficiently informative about a model's ability. We demonstrate our approach on four new open-ended Question-Answering tasks based on Arxiv abstracts, StackExchange questions, AWS DevOps troubleshooting guides, and SEC filings. In addition, our experiments reveal more general insights into factors impacting RAG performance like size, retrieval mechanism, prompting and fine-tuning. Most notably, our findings show that choosing the right retrieval algorithms often leads to bigger performance gains than simply using a larger language model.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/55c3095681acc82780508b0e484dba0c30cf1caa.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 24,
        "score": 24.0
    },
    "0d2103620d4e72688e2aef6258345418fe6a3a3b.pdf": {
        "title": "Web Application for Retrieval-Augmented Generation: Implementation and Testing",
        "authors": [
            "I. Radeva",
            "I. Popchev",
            "L. Doukovska",
            "Miroslava Dimitrova"
        ],
        "published_date": "2024",
        "abstract": "The purpose of this paper is to explore the implementation of retrieval-augmented generation (RAG) technology with open-source large language models (LLMs). A dedicated web-based application, PaSSER, was developed, integrating RAG with Mistral:7b, Llama2:7b, and Orca2:7b models. Various software instruments were used in the application\u2019s development. PaSSER employs a set of evaluation metrics, including METEOR, ROUGE, BLEU, perplexity, cosine similarity, Pearson correlation, and F1 score, to assess LLMs\u2019 performance, particularly within the smart agriculture domain. The paper presents the results and analyses of two tests. One test assessed the performance of LLMs across different hardware configurations, while the other determined which model delivered the most accurate and contextually relevant responses within RAG. The paper discusses the integration of blockchain with LLMs to manage and store assessment results within a blockchain environment. The tests revealed that GPUs are essential for fast text generation, even for 7b models. Orca2:7b on Mac M1 was the fastest, and Mistral:7b had superior performance on the 446 question\u2013answer dataset. The discussion is on technical and hardware considerations affecting LLMs\u2019 performance. The conclusion outlines future developments in leveraging other LLMs, fine-tuning approaches, and further integration with blockchain and IPFS.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/0d2103620d4e72688e2aef6258345418fe6a3a3b.pdf",
        "venue": "Electronics",
        "citationCount": 24,
        "score": 24.0
    },
    "eaf2f0ed4281699f52f3c03ee4a2ee411fa0aa6e.pdf": {
        "title": "Biomedical knowledge graph-optimized prompt generation for large language models",
        "authors": [
            "Karthik Soman",
            "Peter W Rose",
            "John H Morris",
            "Rabia E Akbas",
            "Brett Smith",
            "Braian Peetoom",
            "Catalina Villouta-Reyes",
            "G. Cerono",
            "Yongmei Shi",
            "Angela Rizk-Jackson",
            "Sharat Israni",
            "Charlotte A. Nelson",
            "Sui Huang",
            "Sergio Baranzini"
        ],
        "published_date": "2023",
        "abstract": "Abstract Motivation Large language models (LLMs) are being adopted at an unprecedented rate, yet still face challenges in knowledge-intensive domains such as biomedicine. Solutions such as pretraining and domain-specific fine-tuning add substantial computational overhead, requiring further domain-expertise. Here, we introduce a token-optimized and robust Knowledge Graph-based Retrieval Augmented Generation (KG-RAG) framework by leveraging a massive biomedical KG (SPOKE) with LLMs such as Llama-2-13b, GPT-3.5-Turbo, and GPT-4, to generate meaningful biomedical text rooted in established knowledge. Results Compared to the existing RAG technique for Knowledge Graphs, the proposed method utilizes minimal graph schema for context extraction and uses embedding methods for context pruning. This optimization in context extraction results in more than 50% reduction in token consumption without compromising the accuracy, making a cost-effective and robust RAG implementation on proprietary LLMs. KG-RAG consistently enhanced the performance of LLMs across diverse biomedical prompts by generating responses rooted in established knowledge, accompanied by accurate provenance and statistical evidence (if available) to substantiate the claims. Further benchmarking on human curated datasets, such as biomedical true/false and multiple-choice questions (MCQ), showed a remarkable 71% boost in the performance of the Llama-2 model on the challenging MCQ dataset, demonstrating the framework\u2019s capacity to empower open-source models with fewer parameters for domain-specific questions. Furthermore, KG-RAG enhanced the performance of proprietary GPT models, such as GPT-3.5 and GPT-4. In summary, the proposed framework combines explicit and implicit knowledge of KG and LLM in a token optimized fashion, thus enhancing the adaptability of general-purpose LLMs to tackle domain-specific questions in a cost-effective fashion. Availability and implementation SPOKE KG can be accessed at https://spoke.rbvi.ucsf.edu/neighborhood.html. It can also be accessed using REST-API (https://spoke.rbvi.ucsf.edu/swagger/). KG-RAG code is made available at https://github.com/BaranziniLab/KG_RAG. Biomedical benchmark datasets used in this study are made available to the research community in the same GitHub repository.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/eaf2f0ed4281699f52f3c03ee4a2ee411fa0aa6e.pdf",
        "venue": "Bioinformatics",
        "citationCount": 46,
        "score": 23.0
    },
    "6bdb704aa7f99a3d9899532c547616767bbf8302.pdf": {
        "title": "MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented Generation via Knowledge-enhanced Reranking and Noise-injected Training",
        "authors": [
            "Zhanpeng Chen",
            "Chengjin Xu",
            "Yiyan Qi",
            "Jian Guo"
        ],
        "published_date": "2024",
        "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in processing and generating content across multiple data modalities. However, a significant drawback of MLLMs is their reliance on static training data, leading to outdated information and limited contextual awareness. This static nature hampers their ability to provide accurate and up-to-date responses, particularly in dynamic or rapidly evolving contexts. Though integrating Multimodal Retrieval-augmented Generation (Multimodal RAG) offers a promising solution, the system would inevitably encounter the multi-granularity noisy correspondence (MNC) problem, which hinders accurate retrieval and generation. In this work, we propose RagVL, a novel framework with knowledge-enhanced reranking and noise-injected training, to address these limitations. We instruction-tune the MLLM with a simple yet effective instruction template to induce its ranking ability and serve it as a reranker to precisely filter the top-k retrieved images. For generation, we inject visual noise during training at the data and token levels to enhance the generator's robustness. Extensive experiments on the subsets of two datasets that require retrieving and reasoning over images to answer a given query verify the effectiveness of our method. Code and models are available at https://github.com/IDEA-FinAI/RagVL.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/6bdb704aa7f99a3d9899532c547616767bbf8302.pdf",
        "venue": "arXiv.org",
        "citationCount": 23,
        "score": 23.0
    },
    "03182415b7e769a387ae16c4a61c1df908304e7e.pdf": {
        "title": "Retrieval Augmented Generation Enabled Generative Pre-Trained Transformer 4 (GPT-4) Performance for Clinical Trial Screening",
        "authors": [
            "Ozan Unlu",
            "Jiyeon Shin",
            "Charlotte J. Mailly",
            "Michael Oates",
            "Michela R. Tucci",
            "Matthew Varugheese",
            "K. Wagholikar",
            "Fei Wang",
            "Benjamin M. Scirica",
            "A. Blood",
            "Samuel J. Aronson"
        ],
        "published_date": "2024",
        "abstract": "Background: Subject screening is a key aspect of all clinical trials; however, traditionally, it is a labor-intensive and error-prone task, demanding significant time and resources. With the advent of large language models (LLMs) and related technologies, a paradigm shift in natural language processing capabilities offers a promising avenue for increasing both quality and efficiency of screening efforts. This study aimed to test the Retrieval-Augmented Generation (RAG) process enabled Generative Pretrained Transformer Version 4 (GPT-4) to accurately identify and report on inclusion and exclusion criteria for a clinical trial. Methods: The (Co-Operative Program for Implementation of Optimal Therapy in Heart Failure) COPILOT-HF trial aims to recruit patients with symptomatic heart failure. As part of the screening process, a list of potentially eligible patients is created through an electronic health record (EHR) query. Currently, structured data in the EHR can only be used to determine 5 out of 6 inclusion and 5 out of 17 exclusion criteria. Trained, but non-licensed, study staff complete manual chart review to determine patient eligibility and record their assessment of the inclusion and exclusion criteria. We obtained the structured assessments completed by the study staff and clinical notes for the past two years and developed a workflow of clinical note-based question answering system powered by RAG architecture and GPT-4 that we named RECTIFIER (RAG-Enabled Clinical Trial Infrastructure for Inclusion Exclusion Review). We used notes from 100 patients as a development dataset, 282 patients as a validation dataset, and 1894 patients as a test set. An expert clinician completed a blinded review of patients' charts to answer the eligibility questions and determine the \"gold standard\" answers. We calculated the sensitivity, specificity, accuracy, and Matthews correlation coefficient (MCC) for each question and screening method. We also performed bootstrapping to calculate the confidence intervals for each statistic. Results: Both RECTIFIER and study staff answers closely aligned with the expert clinician answers across criteria with accuracy ranging between 97.9% and 100% (MCC 0.837 and 1) for RECTIFIER and 91.7% and 100% (MCC 0.644 and 1) for study staff. RECTIFIER performed better than study staff to determine the inclusion criteria of \"symptomatic heart failure\" with an accuracy of 97.9% vs 91.7% and an MCC of 0.924 vs 0.721, respectively. Overall, the sensitivity and specificity of determining eligibility for the RECTIFIER was 92.3% (CI) and 93.9% (CI), and study staff was 90.1% (CI) and 83.6% (CI), respectively. Conclusion: GPT-4 based solutions have the potential to improve efficiency and reduce costs in clinical trial screening. When incorporating new tools such as RECTIFIER, it is important to consider the potential hazards of automating the screening process and set up appropriate mitigation strategies such as final clinician review before patient engagement.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/03182415b7e769a387ae16c4a61c1df908304e7e.pdf",
        "venue": "medRxiv",
        "citationCount": 23,
        "score": 23.0
    },
    "5b3c1a291cc717fa80218ead429e7507e967ec01.pdf": {
        "title": "Development of a Liver Disease-Specific Large Language Model Chat Interface using Retrieval Augmented Generation",
        "authors": [
            "J. Ge",
            "Steve Sun",
            "Joseph Owens",
            "Victor Galvez",
            "Oksana Gologorskaya",
            "Jennifer C. Lai",
            "Mark J. Pletcher",
            "Ki Lai"
        ],
        "published_date": "2023",
        "abstract": "Background: Large language models (LLMs) have significant capabilities in clinical information processing tasks. Commercially available LLMs, however, are not optimized for clinical uses and are prone to generating incorrect or hallucinatory information. Retrieval-augmented generation (RAG) is an enterprise architecture that allows embedding of customized data into LLMs. This approach \"specializes\" the LLMs and is thought to reduce hallucinations. Methods: We developed \"LiVersa,\" a liver disease-specific LLM, by using our institution's protected health information (PHI)-complaint text embedding and LLM platform, \"Versa.\" We conducted RAG on 30 publicly available American Association for the Study of Liver Diseases (AASLD) guidelines and guidance documents to be incorporated into LiVersa. We evaluated LiVersa's performance by comparing its responses versus those of trainees from a previously published knowledge assessment study regarding hepatitis B (HBV) treatment and hepatocellular carcinoma (HCC) surveillance. Results: LiVersa answered all 10 questions correctly when forced to provide a \"yes\" or \"no\" answer. Full detailed responses with justifications and rationales, however, were not completely correct for three of the questions. Discussions: In this study, we demonstrated the ability to build disease-specific and PHI-compliant LLMs using RAG. While our LLM, LiVersa, demonstrated more specificity in answering questions related to clinical hepatology - there were some knowledge deficiencies due to limitations set by the number and types of documents used for RAG. The LiVersa prototype, however, is a proof of concept for utilizing RAG to customize LLMs for clinical uses and a potential strategy to realize personalized medicine in the future.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/5b3c1a291cc717fa80218ead429e7507e967ec01.pdf",
        "venue": "medRxiv",
        "citationCount": 44,
        "score": 22.0
    },
    "20a8a84db1ebf5a8b75525671f5baf431a32f1a3.pdf": {
        "title": "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation",
        "authors": [
            "David Rau",
            "Herv'e D'ejean",
            "Nadezhda Chirkova",
            "Thibault Formal",
            "Shuai Wang",
            "Vassilina Nikoulina",
            "S. Clinchant"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation allows to enhance Large Language Models with external knowledge. In response to the recent popularity of generative LLMs, many RAG approaches have been proposed, which involve an intricate number of different configurations such as evaluation datasets, collections, metrics, retrievers, and LLMs. Inconsistent benchmarking poses a major challenge in comparing approaches and understanding the impact of each component in the pipeline. In this work, we study best practices that lay the groundwork for a systematic evaluation of RAG and present BERGEN, an end-to-end library for reproducible research standardizing RAG experiments. In an extensive study focusing on QA, we benchmark different state-of-the-art retrievers, rerankers, and LLMs. Additionally, we analyze existing RAG metrics and datasets. Our open-source library BERGEN is available under \\url{https://github.com/naver/bergen}.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/20a8a84db1ebf5a8b75525671f5baf431a32f1a3.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 22,
        "score": 22.0
    },
    "09022e0e75fa472e9a1f6b743223c016a5ec35e2.pdf": {
        "title": "Systematic Analysis of Retrieval-Augmented Generation-Based LLMs for Medical Chatbot Applications",
        "authors": [
            "Arunabh Bora",
            "H. Cuay\u00e1huitl"
        ],
        "published_date": "2024",
        "abstract": "Artificial Intelligence (AI) has the potential to revolutionise the medical and healthcare sectors. AI and related technologies could significantly address some supply-and-demand challenges in the healthcare system, such as medical AI assistants, chatbots and robots. This paper focuses on tailoring LLMs to medical data utilising a Retrieval-Augmented Generation (RAG) database to evaluate their performance in a computationally resource-constrained environment. Existing studies primarily focus on fine-tuning LLMs on medical data, but this paper combines RAG and fine-tuned models and compares them against base models using RAG or only fine-tuning. Open-source LLMs (Flan-T5-Large, LLaMA-2-7B, and Mistral-7B) are fine-tuned using the medical datasets Meadow-MedQA and MedMCQA. Experiments are reported for response generation and multiple-choice question answering. The latter uses two distinct methodologies: Type A, as standard question answering via direct choice selection; and Type B, as language generation and probability confidence score generation of choices available. Results in the medical domain revealed that Fine-tuning and RAG are crucial for improved performance, and that methodology Type A outperforms Type B.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/09022e0e75fa472e9a1f6b743223c016a5ec35e2.pdf",
        "venue": "Machine Learning and Knowledge Extraction",
        "citationCount": 22,
        "score": 22.0
    },
    "680824bef5d6f98d669c49246363f0894a678e3b.pdf": {
        "title": "Ragnar\u00f6k: A Reusable RAG Framework and Baselines for TREC 2024 Retrieval-Augmented Generation Track",
        "authors": [
            "Ronak Pradeep",
            "Nandan Thakur",
            "Sahel Sharifymoghaddam",
            "Eric Zhang",
            "Ryan Nguyen",
            "Daniel Campos",
            "Nick Craswell",
            "Jimmy Lin"
        ],
        "published_date": "2024",
        "abstract": "Did you try out the new Bing Search? Or maybe you fiddled around with Google AI~Overviews? These might sound familiar because the modern-day search stack has recently evolved to include retrieval-augmented generation (RAG) systems. They allow searching and incorporating real-time data into large language models (LLMs) to provide a well-informed, attributed, concise summary in contrast to the traditional search paradigm that relies on displaying a ranked list of documents. Therefore, given these recent advancements, it is crucial to have an arena to build, test, visualize, and systematically evaluate RAG-based search systems. With this in mind, we propose the TREC 2024 RAG Track to foster innovation in evaluating RAG systems. In our work, we lay out the steps we've made towards making this track a reality -- we describe the details of our reusable framework, Ragnar\\\"ok, explain the curation of the new MS MARCO V2.1 collection choice, release the development topics for the track, and standardize the I/O definitions which assist the end user. Next, using Ragnar\\\"ok, we identify and provide key industrial baselines such as OpenAI's GPT-4o or Cohere's Command R+. Further, we introduce a web-based user interface for an interactive arena allowing benchmarking pairwise RAG systems by crowdsourcing. We open-source our Ragnar\\\"ok framework and baselines to achieve a unified standard for future RAG systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/680824bef5d6f98d669c49246363f0894a678e3b.pdf",
        "venue": "European Conference on Information Retrieval",
        "citationCount": 22,
        "score": 22.0
    },
    "3e2cbbf5f677f372cbb21969fe268a9b8a1ad64a.pdf": {
        "title": "LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering",
        "authors": [
            "Qingfei Zhao",
            "Ruobing Wang",
            "Yukuo Cen",
            "Daren Zha",
            "Shicheng Tan",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "published_date": "2024",
        "abstract": "Long-Context Question Answering (LCQA), a challenging task, aims to reason over long-context documents to yield accurate answers to questions. Existing long-context Large Language Models (LLMs) for LCQA often struggle with the \u201clost in the middle\u201d issue. Retrieval-Augmented Generation (RAG) mitigates this issue by providing external factual evidence. However, its chunking strategy disrupts the global long-context information, and its low-quality retrieval in long contexts hinders LLMs from identifying effective factual details due to substantial noise. To this end, we propose LongRAG, a general, dual-perspective, and robust LLM-based RAG system paradigm for LCQA to enhance RAG\u2019s understanding of complex long-context knowledge (i.e., global information and factual details). We design LongRAG as a plug-and-play paradigm, facilitating adaptation to various domains and LLMs. Extensive experiments on three multi-hop datasets demonstrate that LongRAG significantly outperforms long-context LLMs (up by 6.94%), advanced RAG (up by 6.16%), and Vanilla RAG (up by 17.25%). Furthermore, we conduct quantitative ablation studies and multi-dimensional analyses, highlighting the effectiveness of the system\u2019s components and fine-tuning strategies.Data and code are available at [https://github.com/QingFei1/LongRAG](https://github.com/QingFei1/LongRAG).",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/3e2cbbf5f677f372cbb21969fe268a9b8a1ad64a.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 21,
        "score": 21.0
    },
    "9a3d1d1a1c00feb6c7cbe0e488eff57c606463c9.pdf": {
        "title": "Retrieval-augmented generation in multilingual settings",
        "authors": [
            "Nadezhda Chirkova",
            "David Rau",
            "Herv'e D'ejean",
            "Thibault Formal",
            "S. Clinchant",
            "Vassilina Nikoulina"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) has recently emerged as a promising solution for incorporating up-to-date or domain-specific knowledge into large language models (LLMs) and improving LLM factuality, but is predominantly studied in English-only settings. In this work, we consider RAG in the multilingual setting (mRAG), i.e. with user queries and the datastore in 13 languages, and investigate which components and with which adjustments are needed to build a well-performing mRAG pipeline, that can be used as a strong baseline in future works. Our findings highlight that despite the availability of high-quality off-the-shelf multilingual retrievers and generators, task-specific prompt engineering is needed to enable generation in user languages. Moreover, current evaluation metrics need adjustments for multilingual setting, to account for variations in spelling named entities. The main limitations to be addressed in future works include frequent code-switching in non-Latin alphabet languages, occasional fluency errors, wrong reading of the provided documents, or irrelevant retrieval. We release the code for the resulting mRAG baseline pipeline at https://github.com/naver/bergen, Documentation: https://github.com/naver/bergen/blob/main/documentations/multilingual.md.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/9a3d1d1a1c00feb6c7cbe0e488eff57c606463c9.pdf",
        "venue": "KNOWLLM",
        "citationCount": 21,
        "score": 21.0
    },
    "64ee29d6ddb2c2167a201783ddd4d0a9b744f352.pdf": {
        "title": "Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation",
        "authors": [
            "Guanting Dong",
            "Yutao Zhu",
            "Chenghao Zhang",
            "Zechen Wang",
            "Zhicheng Dou",
            "Ji-Rong Wen"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) has effectively mitigated the hallucination problem of large language models (LLMs). However, the difficulty of aligning the retriever with the LLMs' diverse knowledge preferences inevitably poses a challenge in developing a reliable RAG system. To address this issue, we propose DPA-RAG, a universal framework designed to align diverse knowledge preferences within RAG systems. Specifically, we initially introduce a preference knowledge construction pipeline and incorporate five novel query augmentation strategies to alleviate preference data scarcity. Based on preference data, DPA-RAG accomplishes both external and internal preference alignment: 1) It jointly integrates pairwise, pointwise, and contrastive preference alignment abilities into the reranker, achieving external preference alignment among RAG components. 2) It further introduces a pre-aligned stage before vanilla Supervised Fine-tuning (SFT), enabling LLMs to implicitly capture knowledge aligned with their reasoning preferences, achieving LLMs' internal alignment. Experimental results across four knowledge-intensive QA datasets demonstrate that DPA-RAG outperforms all baselines and seamlessly integrates both black-box and open-sourced LLM readers. Further qualitative analysis and discussions provide empirical guidance for achieving reliable RAG systems. Our code and example dataset are available at https://github.com/dongguanting/DPA-RAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/64ee29d6ddb2c2167a201783ddd4d0a9b744f352.pdf",
        "venue": "The Web Conference",
        "citationCount": 21,
        "score": 21.0
    },
    "650f7db2b37069614c0fb04ea77f099bb5d4efa5.pdf": {
        "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed KV Caches for Chunked Text",
        "authors": [
            "Songshuo Lu",
            "Hua Wang",
            "Yutian Rong",
            "Zhi Chen",
            "Yaohua Tang"
        ],
        "published_date": "2024",
        "abstract": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process numerous retrieved document chunks for prefill which requires a large volume of computation, therefore leading to significant latency in time-to-first-token (TTFT). To reduce the computation overhead as well as TTFT, we introduce TurboRAG, a novel RAG system that redesigns the inference paradigm of the current RAG system by first pre-computing and storing the key-value (KV) caches of documents offline, and then directly retrieving the saved KV cache for prefill. Hence, online computation of KV caches is eliminated during inference. In addition, we provide a number of insights into the mask matrix and positional embedding mechanisms, plus fine-tune a pretrained language model to maintain model accuracy of TurboRAG. Our approach is applicable to most existing large language models and their applications without any requirement in modification of models and inference systems. Experimental results across a suite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x compared to the conventional RAG systems (on an average of 8.6x), but reserving comparable performance to the standard RAG systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/650f7db2b37069614c0fb04ea77f099bb5d4efa5.pdf",
        "venue": "arXiv.org",
        "citationCount": 21,
        "score": 21.0
    },
    "30394de373b65aeda84e2bd100e8efe38f4d1a8c.pdf": {
        "title": "Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection",
        "authors": [
            "Yun Zhu",
            "Jia-Chen Gu",
            "Caitlin Sikora",
            "Ho Ko",
            "Yinxiao Liu",
            "Chu-Cheng Lin",
            "Lei Shu",
            "Liangchen Luo",
            "Lei Meng",
            "Bang Liu",
            "Jindong Chen"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) augmented with retrieval exhibit robust performance and extensive versatility by incorporating external contexts. However, the input length grows linearly in the number of retrieved documents, causing a dramatic increase in latency. In this paper, we propose a novel paradigm named Sparse RAG, which seeks to cut computation costs through sparsity. Specifically, Sparse RAG encodes retrieved documents in parallel, which eliminates latency introduced by long-range attention of retrieved documents. Then, LLMs selectively decode the output by only attending to highly relevant caches auto-regressively, which are chosen via prompting LLMs with special control tokens. It is notable that Sparse RAG combines the assessment of each individual document and the generation of the response into a single process. The designed sparse mechanism in a RAG system can facilitate the reduction of the number of documents loaded during decoding for accelerating the inference of the RAG system. Additionally, filtering out undesirable contexts enhances the model's focus on relevant context, inherently improving its generation quality. Evaluation results of two datasets show that Sparse RAG can strike an optimal balance between generation quality and computational efficiency, demonstrating its generalizability across both short- and long-form generation tasks.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/30394de373b65aeda84e2bd100e8efe38f4d1a8c.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 21,
        "score": 21.0
    },
    "1d1beece295703c0cb3e545edaa12a4336b407bc.pdf": {
        "title": "Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models",
        "authors": [
            "Tian Yu",
            "Shaolei Zhang",
            "Yang Feng"
        ],
        "published_date": "2024",
        "abstract": "Iterative retrieval refers to the process in which the model continuously queries the retriever during generation to enhance the relevance of the retrieved knowledge, thereby improving the performance of Retrieval-Augmented Generation (RAG). Existing work typically employs few-shot prompting or manually constructed rules to implement iterative retrieval. This introduces additional inference overhead and overlooks the remarkable reasoning capabilities of Large Language Models (LLMs). In this paper, we introduce Auto-RAG, an autonomous iterative retrieval model centered on the LLM's powerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues with the retriever, systematically planning retrievals and refining queries to acquire valuable knowledge. This process continues until sufficient external information is gathered, at which point the results are presented to the user. To this end, we develop a method for autonomously synthesizing reasoning-based decision-making instructions in iterative retrieval and fine-tuned the latest open-source LLMs. The experimental results indicate that Auto-RAG is capable of autonomous iterative interaction with the retriever, effectively leveraging the remarkable reasoning and decision-making abilities of LLMs, which lead to outstanding performance across six benchmarks. Further analysis reveals that Auto-RAG can autonomously adjust the number of iterations based on the difficulty of the questions and the utility of the retrieved knowledge, without requiring any human intervention. Moreover, Auto-RAG expresses the iterative retrieval process in natural language, enhancing interpretability while providing users with a more intuitive experience\\footnote{Code is available at \\url{https://github.com/ictnlp/Auto-RAG}.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/1d1beece295703c0cb3e545edaa12a4336b407bc.pdf",
        "venue": "arXiv.org",
        "citationCount": 20,
        "score": 20.0
    },
    "5879575701b9b65b5cc56c00d9eebbfa219e0428.pdf": {
        "title": "Retrieval augmented generation for large language models in healthcare: A systematic review",
        "authors": [
            "L. M. Amugongo",
            "Pietro Mascheroni",
            "Steven Brooks",
            "Stefan Doering",
            "Jan Seidel"
        ],
        "published_date": "2025",
        "abstract": "Large Language Models (LLMs) have demonstrated promising capabilities to solve complex tasks in critical sectors such as healthcare. However, LLMs are limited by their training data which is often outdated, the tendency to generate inaccurate (\u201challucinated\u201d) content and a lack of transparency in the content they generate. To address these limitations, retrieval augmented generation (RAG) grounds the responses of LLMs by exposing them to external knowledge sources. However, in the healthcare domain there is currently a lack of systematic understanding of which datasets, RAG methodologies and evaluation frameworks are available. This review aims to bridge this gap by assessing RAG-based approaches employed by LLMs in healthcare, focusing on the different steps of retrieval, augmentation and generation. Additionally, we identify the limitations, strengths and gaps in the existing literature. Our synthesis shows that 78.9% of studies used English datasets and 21.1% of the datasets are in Chinese. We find that a range of techniques are employed RAG-based LLMs in healthcare, including Naive RAG, Advanced RAG, and Modular RAG. Surprisingly, proprietary models such as GPT-3.5/4 are the most used for RAG applications in healthcare. We find that there is a lack of standardised evaluation frameworks for RAG-based applications. In addition, the majority of the studies do not assess or address ethical considerations related to RAG in healthcare. It is important to account for ethical challenges that are inherent when AI systems are implemented in the clinical setting. Lastly, we highlight the need for further research and development to ensure responsible and effective adoption of RAG in the medical domain.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/5879575701b9b65b5cc56c00d9eebbfa219e0428.pdf",
        "venue": "PLOS Digital Health",
        "citationCount": 20,
        "score": 20.0
    },
    "61f8baae9aecadc8bed1ce263c32bd108b476ebd.pdf": {
        "title": "UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-world Document Analysis",
        "authors": [
            "Yulong Hui",
            "Yao Lu",
            "Huanchen Zhang"
        ],
        "published_date": "2024",
        "abstract": "The use of Retrieval-Augmented Generation (RAG) has improved Large Language Models (LLMs) in collaborating with external data, yet significant challenges exist in real-world scenarios. In areas such as academic literature and finance question answering, data are often found in raw text and tables in HTML or PDF formats, which can be lengthy and highly unstructured. In this paper, we introduce a benchmark suite, namely Unstructured Document Analysis (UDA), that involves 2,965 real-world documents and 29,590 expert-annotated Q&A pairs. We revisit popular LLM- and RAG-based solutions for document analysis and evaluate the design choices and answer qualities across multiple document domains and diverse query types. Our evaluation yields interesting findings and highlights the importance of data parsing and retrieval. We hope our benchmark can shed light and better serve real-world document analysis applications. The benchmark suite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/61f8baae9aecadc8bed1ce263c32bd108b476ebd.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 20,
        "score": 20.0
    },
    "f89ed27318cb930ae884af0c62be37f0355571b5.pdf": {
        "title": "RAGAR, Your Falsehood Radar: RAG-Augmented Reasoning for Political Fact-Checking using Multimodal Large Language Models",
        "authors": [
            "M. A. Khaliq",
            "P. Chang",
            "M. Ma",
            "B. Pflugfelder",
            "F. Mileti'c"
        ],
        "published_date": "2024",
        "abstract": "The escalating challenge of misinformation, particularly in political discourse, requires advanced fact-checking solutions; this is even clearer in the more complex scenario of multimodal claims. We tackle this issue using a multimodal large language model in conjunction with retrieval-augmented generation (RAG), and introduce two novel reasoning techniques: Chain of RAG (CoRAG) and Tree of RAG (ToRAG). They fact-check multimodal claims by extracting both textual and image content, retrieving external information, and reasoning subsequent questions to be answered based on prior evidence. We achieve a weighted F1-score of 0.85, surpassing a baseline reasoning technique by 0.14 points. Human evaluation confirms that the vast majority of our generated fact-check explanations contain all information from gold standard data.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/f89ed27318cb930ae884af0c62be37f0355571b5.pdf",
        "venue": "FEVER",
        "citationCount": 20,
        "score": 20.0
    },
    "1bce5a6d8c037a90e9cd49d38fe6446652389674.pdf": {
        "title": "Towards a Search Engine for Machines: Unified Ranking for Multiple Retrieval-Augmented Large Language Models",
        "authors": [
            "Alireza Salemi",
            "Hamed Zamani"
        ],
        "published_date": "2024",
        "abstract": "This paper introduces uRAG-a framework with a unified retrieval engine that serves multiple downstream retrieval-augmented generation (RAG) systems. Each RAG system consumes the retrieval results for a unique purpose, such as open-domain question answering, fact verification, entity linking, and relation extraction. We introduce a generic training guideline that standardizes the communication between the search engine and the downstream RAG systems that engage in optimizing the retrieval model. This lays the groundwork for us to build a large-scale experimentation ecosystem consisting of 18 RAG systems that engage in training and 18 unknown RAG systems that use the uRAG as the new users of the search engine. Using this experimentation ecosystem, we answer a number of fundamental research questions that improve our understanding of promises and challenges in developing search engines for machines.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/1bce5a6d8c037a90e9cd49d38fe6446652389674.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 19,
        "score": 19.0
    },
    "a76209fea4627974b5e12d8b4942268eb17bc7df.pdf": {
        "title": "Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation",
        "authors": [
            "Shicheng Xu",
            "Liang Pang",
            "Mo Yu",
            "Fandong Meng",
            "Huawei Shen",
            "Xueqi Cheng",
            "Jie Zhou"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignoring it or being misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as ``Information Refiner'', which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named InFO-RAG that optimizes LLMs for RAG in an unsupervised manner. InFO-RAG is low-cost and general across various tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue, and Code Generation show that InFO-RAG improves the performance of LLaMA2 by an average of 9.39\\% relative points. InFO-RAG also shows advantages in in-context learning and robustness of RAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/a76209fea4627974b5e12d8b4942268eb17bc7df.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 18,
        "score": 18.0
    },
    "9b7854829ae4d4653a56ba04880aff848d70fc42.pdf": {
        "title": "Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models",
        "authors": [
            "Zhibo Hu",
            "Chen Wang",
            "Yanfeng Shu",
            "Helen Paik",
            "Liming Zhu"
        ],
        "published_date": "2024",
        "abstract": "The robustness of large language models (LLMs) becomes increasingly important as their use rapidly grows in a wide range of domains. Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of text generation from LLMs. However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied. In this work, we find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers. We systematically evaluate the effect of such prefixes on RAG by introducing a novel optimization technique called Gradient Guided Prompt Perturbation (GGPP). GGPP achieves a high success rate in steering outputs of RAG-based LLMs to targeted wrong answers. It can also cope with instructions in the prompts requesting to ignore irrelevant context. We also exploit LLMs' neuron activation difference between prompts with and without GGPP perturbations to give a method that improves the robustness of RAG-based LLMs through a highly effective detector trained on neuron activation triggered by GGPP generated prompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of our methods.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/9b7854829ae4d4653a56ba04880aff848d70fc42.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 18,
        "score": 18.0
    },
    "b03cfca7672e60cbe7999853e9c6f833ab20e012.pdf": {
        "title": "Rationale-Guided Retrieval Augmented Generation for Medical Question Answering",
        "authors": [
            "Jiwoong Sohn",
            "Yein Park",
            "Chanwoong Yoon",
            "Sihyeon Park",
            "Hyeon Hwang",
            "Mujeen Sung",
            "Hyunjae Kim",
            "Jaewoo Kang"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLM) hold significant potential for applications in biomedicine, but they struggle with hallucinations and outdated knowledge. While retrieval-augmented generation (RAG) is generally employed to address these issues, it also has its own set of challenges: (1) LLMs are vulnerable to irrelevant or incorrect context, (2) medical queries are often not well-targeted for helpful information, and (3) retrievers are prone to bias toward the specific source corpus they were trained on. In this study, we present RAG$^2$ (RAtionale-Guided RAG), a new framework for enhancing the reliability of RAG in biomedical contexts. RAG$^2$ incorporates three key innovations: a small filtering model trained on perplexity-based labels of rationales, which selectively augments informative snippets of documents while filtering out distractors; LLM-generated rationales as queries to improve the utility of retrieved snippets; a structure designed to retrieve snippets evenly from a comprehensive set of four biomedical corpora, effectively mitigating retriever bias. Our experiments demonstrate that RAG$^2$ improves the state-of-the-art LLMs of varying sizes, with improvements of up to 6.1\\%, and it outperforms the previous best medical RAG model by up to 5.6\\% across three medical question-answering benchmarks. Our code is available at https://github.com/dmis-lab/RAG2.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/b03cfca7672e60cbe7999853e9c6f833ab20e012.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 18,
        "score": 18.0
    },
    "edb2cc0f2d7ae50717b708292a543b319bae026e.pdf": {
        "title": "LONG\u00b2RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation with Key Point Recall",
        "authors": [
            "Zehan Qi",
            "Rongwu Xu",
            "Zhijiang Guo",
            "Cunxiang Wang",
            "Hao Zhang",
            "Wei Xu"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) is a promising approach to address the limitations of fixed knowledge in large language models (LLMs). However, current benchmarks for evaluating RAG systems suffer from two key deficiencies: (1) they fail to adequately measure LLMs' capability in handling long-context retrieval due to a lack of datasets that reflect the characteristics of retrieved documents, and (2) they lack a comprehensive evaluation method for assessing LLMs' ability to generate long-form responses that effectively exploits retrieved information. To address these shortcomings, we introduce the Long$^2$RAG benchmark and the Key Point Recall (KPR) metric. Long$^2$RAG comprises 280 questions spanning 10 domains and across 8 question categories, each associated with 5 retrieved documents with an average length of 2,444 words. KPR evaluates the extent to which LLMs incorporate key points extracted from the retrieved documents into their generated responses, providing a more nuanced assessment of their ability to exploit retrieved information.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/edb2cc0f2d7ae50717b708292a543b319bae026e.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 18,
        "score": 18.0
    },
    "74fdf8053638eb12e5cce073ab4fb476fdb9f9cb.pdf": {
        "title": "Automating Systematic Literature Reviews with Retrieval-Augmented Generation: A Comprehensive Overview",
        "authors": [
            "Binglan Han",
            "Teo Su\u0161njak",
            "A. Mathrani"
        ],
        "published_date": "2024",
        "abstract": "This study examines Retrieval-Augmented Generation (RAG) in large language models (LLMs) and their significant application for undertaking systematic literature reviews (SLRs). RAG-based LLMs can potentially automate tasks like data extraction, summarization, and trend identification. However, while LLMs are exceptionally proficient in generating human-like text and interpreting complex linguistic nuances, their dependence on static, pre-trained knowledge can result in inaccuracies and hallucinations. RAG mitigates these limitations by integrating LLMs\u2019 generative capabilities with the precision of real-time information retrieval. We review in detail the three key processes of the RAG framework\u2014retrieval, augmentation, and generation. We then discuss applications of RAG-based LLMs to SLR automation and highlight future research topics, including integration of domain-specific LLMs, multimodal data processing and generation, and utilization of multiple retrieval sources. We propose a framework of RAG-based LLMs for automating SRLs, which covers four stages of SLR process: literature search, literature screening, data extraction, and information synthesis. Future research aims to optimize the interaction between LLM selection, training strategies, RAG techniques, and prompt engineering to implement the proposed framework, with particular emphasis on the retrieval of information from individual scientific papers and the integration of these data to produce outputs addressing various aspects such as current status, existing gaps, and emerging trends.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/74fdf8053638eb12e5cce073ab4fb476fdb9f9cb.pdf",
        "venue": "Applied Sciences",
        "citationCount": 18,
        "score": 18.0
    },
    "036155ed8ec0b922e62741444b1dc4a011390116.pdf": {
        "title": "Optimizing LLM Based Retrieval Augmented Generation Pipelines in the Financial Domain",
        "authors": [
            "Yiyun Zhao",
            "Prateek Singh",
            "Hanoz Bhathena",
            "Bernardo Ramos",
            "Aviral Joshi",
            "Swaroop Gadiyaram",
            "Saket Sharma"
        ],
        "published_date": "2024",
        "abstract": "Retrieval Augmented Generation (RAG) is a prominent approach in real-word applications for grounding large language model (LLM) generations in up-to-date and domain-specific knowledge. However, there is a lack of systematic investigations of the impact of each component (retrieval pipeline, prompts, generation models) on the generation quality of a RAG pipeline in real world scenarios. In this study, we benchmark 6 LLMs in 15 retrieval scenarios, exploring 9 prompts over 2 real world financial domain datasets. We thoroughly discuss the impact of each component in RAG pipeline on answer generation quality and formulate specific recommendations for the design of RAG systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/036155ed8ec0b922e62741444b1dc4a011390116.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 18,
        "score": 18.0
    },
    "9b302002c4b764f61fa7a3d14270470f625945cf.pdf": {
        "title": "RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards",
        "authors": [
            "Xinze Li",
            "Senkun Mei",
            "Zhenghao Liu",
            "Yukun Yan",
            "Shuo Wang",
            "Shi Yu",
            "Zheni Zeng",
            "Hao Chen",
            "Ge Yu",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Chenyan Xiong"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) has proven its effectiveness in mitigating hallucinations in Large Language Models (LLMs) by retrieving knowledge from external resources. To adapt LLMs for the RAG systems, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system. In this paper, we propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG systems by aligning data preferences between different RAG modules. DDR works by collecting the rewards to optimize each agent in the RAG system with the rollout method, which prompts agents to sample some potential responses as perturbations, evaluates the impact of these perturbations on the whole RAG system, and subsequently optimizes the agent to produce outputs that improve the performance of the RAG system. Our experiments on various knowledge-intensive tasks demonstrate that DDR significantly outperforms the SFT method, particularly for LLMs with smaller-scale parameters that depend more on the retrieved knowledge. Additionally, DDR exhibits a stronger capability to align the data preference between RAG modules. The DDR method makes the generation module more effective in extracting key information from documents and mitigating conflicts between parametric memory and external knowledge. All codes are available at https://github.com/OpenMatch/RAG-DDR.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/9b302002c4b764f61fa7a3d14270470f625945cf.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 18,
        "score": 18.0
    },
    "fe2dd4ac8bb14c622a890482384c9e1136479bf6.pdf": {
        "title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models",
        "authors": [
            "Fei Wang",
            "Xingchen Wan",
            "Ruoxi Sun",
            "Jiefeng Chen",
            "Sercan \u00d6. Arik"
        ],
        "published_date": "2024",
        "abstract": "Retrieval augmented generation (RAG), while effectively integrating external knowledge to address the inherent limitations of large language models (LLMs), can be hindered by imperfect retrieval that contain irrelevant, misleading, or even malicious information. Previous studies have rarely connected the behavior of RAG through joint analysis, particularly regarding error propagation coming from imperfect retrieval and potential conflicts between LLMs' internal knowledge and external sources. Through comprehensive and controlled analyses under realistic conditions, we find that imperfect retrieval augmentation is inevitable, common, and harmful. We identify the knowledge conflicts between LLM-internal and external knowledge from retrieval as a bottleneck to overcome imperfect retrieval in the post-retrieval stage of RAG. To address this, we propose Astute RAG, a novel RAG approach designed to be resilient to imperfect retrieval augmentation. It adaptively elicits essential information from LLMs' internal knowledge, iteratively consolidates internal and external knowledge with source-awareness, and finalizes the answer according to information reliability. Our experiments with Gemini and Claude demonstrate the superior performance of Astute RAG compared to previous robustness-enhanced RAG approaches. Specifically, Astute RAG is the only RAG method that achieves performance comparable to or even surpassing conventional use of LLMs under the worst-case scenario. Further analysis reveals the effectiveness of Astute RAG in resolving knowledge conflicts, thereby improving the trustworthiness of RAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/fe2dd4ac8bb14c622a890482384c9e1136479bf6.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 17,
        "score": 17.0
    },
    "79dcc5a39e79bd52119867f9644c3f1dbf38d2c5.pdf": {
        "title": "FACTS About Building Retrieval Augmented Generation-based Chatbots",
        "authors": [
            "Rama Akkiraju",
            "Anbang Xu",
            "Deepak Bora",
            "Tan Yu",
            "Lu An",
            "Vishal Seth",
            "Aaditya Shukla",
            "Pritam Gundecha",
            "Hridhay Mehta",
            "Ashwin Jha",
            "Prithvi Raj",
            "Abhinav Balasubramanian",
            "Murali Maram",
            "Guru Muthusamy",
            "Shivakesh Reddy Annepally",
            "Sidney Knowles",
            "Min Du",
            "Nick Burnett",
            "Sean Javiya",
            "Ashok Marannan",
            "Mamta Kumari",
            "Surbhi Jha",
            "Ethan Dereszenski",
            "Anupam Chakraborty",
            "Subhash Ranjan",
            "Amina Terfai",
            "Anoop Surya",
            "Tracey Mercer",
            "Vinodh Kumar Thanigachalam",
            "Tamar Bar",
            "Sanjana Krishnan",
            "Samy Kilaru",
            "Jasmine Jaksic",
            "Nave Algarici",
            "Jacob Liberman",
            "Joey Conway",
            "Sonu Nayyar",
            "Justin Boitano"
        ],
        "published_date": "2024",
        "abstract": "Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.\"",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/79dcc5a39e79bd52119867f9644c3f1dbf38d2c5.pdf",
        "venue": "arXiv.org",
        "citationCount": 17,
        "score": 17.0
    },
    "b71141dbd0e4827156c696e05ba1fa068ad43ee1.pdf": {
        "title": "GastroBot: a Chinese gastrointestinal disease chatbot based on the retrieval-augmented generation",
        "authors": [
            "Qingqing Zhou",
            "Can Liu",
            "Yuchen Duan",
            "Kaijie Sun",
            "Yu Li",
            "Hongxing Kan",
            "Zongyun Gu",
            "Jianhua Shu",
            "Jili Hu"
        ],
        "published_date": "2024",
        "abstract": "Introduction Large Language Models (LLMs) play a crucial role in clinical information processing, showcasing robust generalization across diverse language tasks. However, existing LLMs, despite their significance, lack optimization for clinical applications, presenting challenges in terms of illusions and interpretability. The Retrieval-Augmented Generation (RAG) model addresses these issues by providing sources for answer generation, thereby reducing errors. This study explores the application of RAG technology in clinical gastroenterology to enhance knowledge generation on gastrointestinal diseases. Methods We fine-tuned the embedding model using a corpus consisting of 25 guidelines on gastrointestinal diseases. The fine-tuned model exhibited an 18% improvement in hit rate compared to its base model, gte-base-zh. Moreover, it outperformed OpenAI\u2019s Embedding model by 20%. Employing the RAG framework with the llama-index, we developed a Chinese gastroenterology chatbot named \u201cGastroBot,\u201d which significantly improves answer accuracy and contextual relevance, minimizing errors and the risk of disseminating misleading information. Results When evaluating GastroBot using the RAGAS framework, we observed a context recall rate of 95%. The faithfulness to the source, stands at 93.73%. The relevance of answers exhibits a strong correlation, reaching 92.28%. These findings highlight the effectiveness of GastroBot in providing accurate and contextually relevant information about gastrointestinal diseases. During manual assessment of GastroBot, in comparison with other models, our GastroBot model delivers a substantial amount of valuable knowledge while ensuring the completeness and consistency of the results. Discussion Research findings suggest that incorporating the RAG method into clinical gastroenterology can enhance the accuracy and reliability of large language models. Serving as a practical implementation of this method, GastroBot has demonstrated significant enhancements in contextual comprehension and response quality. Continued exploration and refinement of the model are poised to drive forward clinical information processing and decision support in the gastroenterology field.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/b71141dbd0e4827156c696e05ba1fa068ad43ee1.pdf",
        "venue": "Frontiers in Medicine",
        "citationCount": 17,
        "score": 17.0
    },
    "d9ff626e7c2fad53e80061d34e4d0d0dbbaab1dd.pdf": {
        "title": "AutoRAG: Automated Framework for optimization of Retrieval Augmented Generation Pipeline",
        "authors": [
            "Dongkyu Kim",
            "Byoungwook Kim",
            "Donggeon Han",
            "Matouvs Eibich"
        ],
        "published_date": "2024",
        "abstract": "Using LLMs (Large Language Models) in conjunction with external documents has made RAG (Retrieval-Augmented Generation) an essential technology. Numerous techniques and modules for RAG are being researched, but their performance can vary across different datasets. Finding RAG modules that perform well on specific datasets is challenging. In this paper, we propose the AutoRAG framework, which automatically identifies suitable RAG modules for a given dataset. AutoRAG explores and approximates the optimal combination of RAG modules for the dataset. Additionally, we share the results of optimizing a dataset using AutoRAG. All experimental results and data are publicly available and can be accessed through our GitHub repository https://github.com/Marker-Inc-Korea/AutoRAG_ARAGOG_Paper .",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/d9ff626e7c2fad53e80061d34e4d0d0dbbaab1dd.pdf",
        "venue": "arXiv.org",
        "citationCount": 17,
        "score": 17.0
    },
    "eea1b651b029f30f483ee35a7a42cbbbf79bcad6.pdf": {
        "title": "TelecomRAG: Taming Telecom Standards with Retrieval Augmented Generation and LLMs",
        "authors": [
            "G. M. Yilma",
            "J. Ayala-Romero",
            "A. Garcia-Saavedra",
            "Xavier P\u00e9rez Costa"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have immense potential to transform the telecommunications industry. They could help professionals understand complex standards, generate code, and accelerate development. However, traditional LLMs struggle with the precision and source verification essential for telecom work. To address this, specialized LLM-based solutions tailored to telecommunication standards are needed. This Editorial Note showcases how Retrieval-Augmented Generation (RAG) can offer a way to create precise, factual answers. In particular, we show how to build a Telecommunication Standards Assistant that provides accurate, detailed, and verifiable responses. We show a usage example of this framework using 3GPP Release 16 and Release 18 specification documents. We believe that the application of RAG can bring significant value to the telecommunications field.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/eea1b651b029f30f483ee35a7a42cbbbf79bcad6.pdf",
        "venue": "Computer communication review",
        "citationCount": 17,
        "score": 17.0
    },
    "ccaa1a9b70a14c6725af1b0328f9468947ab7d32.pdf": {
        "title": "GenAI-powered Multi-Agent Paradigm for Smart Urban Mobility: Opportunities and Challenges for Integrating Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) with Intelligent Transportation Systems",
        "authors": [
            "Haowen Xu",
            "Jinghui Yuan",
            "Anye Zhou",
            "Guanhao Xu",
            "Wan Li",
            "Xuegang Ban",
            "Xinyue Ye"
        ],
        "published_date": "2024",
        "abstract": "Leveraging recent advances in generative AI, multi-agent systems are increasingly being developed to enhance the functionality and efficiency of smart city applications. This paper explores the transformative potential of large language models (LLMs) and emerging Retrieval-Augmented Generation (RAG) technologies in Intelligent Transportation Systems (ITS), paving the way for innovative solutions to address critical challenges in urban mobility. We begin by providing a comprehensive overview of the current state-of-the-art in mobility data, ITS, and Connected Vehicles (CV) applications. Building on this review, we discuss the rationale behind RAG and examine the opportunities for integrating these Generative AI (GenAI) technologies into the smart mobility sector. We propose a conceptual framework aimed at developing multi-agent systems capable of intelligently and conversationally delivering smart mobility services to urban commuters, transportation operators, and decision-makers. Our approach seeks to foster an autonomous and intelligent approach that (a) promotes science-based advisory to reduce traffic congestion, accidents, and carbon emissions at multiple scales, (b) facilitates public education and engagement in participatory mobility management, and (c) automates specialized transportation management tasks and the development of critical ITS platforms, such as data analytics and interpretation, knowledge representation, and traffic simulations. By integrating LLM and RAG, our approach seeks to overcome the limitations of traditional rule-based multi-agent systems, which rely on fixed knowledge bases and limited reasoning capabilities. This integration paves the way for a more scalable, intuitive, and automated multi-agent paradigm, driving advancements in ITS and urban mobility.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/ccaa1a9b70a14c6725af1b0328f9468947ab7d32.pdf",
        "venue": "arXiv.org",
        "citationCount": 16,
        "score": 16.0
    },
    "9d9268b0191891511b09362759ba6a754c28fd9e.pdf": {
        "title": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains",
        "authors": [
            "Ran Xu",
            "Hui Liu",
            "Sreyashi Nag",
            "Zhenwei Dai",
            "Yaochen Xie",
            "Xianfeng Tang",
            "Chen Luo",
            "Yang Li",
            "Joyce C. Ho",
            "Carl Yang",
            "Qi He"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) enhances the question-answering (QA) abilities of large language models (LLMs) by integrating external knowledge. However, adapting general-purpose RAG systems to specialized fields such as science and medicine poses unique challenges due to distribution shifts and limited access to domain-specific data. To tackle this, we propose SimRAG, a self-training approach that equips the LLM with joint capabilities of question answering and question generation for domain adaptation. Our method first fine-tunes the LLM on instruction-following, question-answering, and search-related data. Then, it prompts the same LLM to generate diverse domain-relevant questions from unlabeled corpora, with an additional filtering strategy to retain high-quality synthetic examples. By leveraging these self-generated synthetic examples, the LLM can improve their performance on domain-specific RAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three domains, demonstrate that SimRAG outperforms baselines by 1.2\\%--8.6\\%.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/9d9268b0191891511b09362759ba6a754c28fd9e.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 16,
        "score": 16.0
    },
    "5aabaf59808091eca1c6cba123ac2003017f4011.pdf": {
        "title": "Can Small Language Models With Retrieval-Augmented Generation Replace Large Language Models When Learning Computer Science?",
        "authors": [
            "Suqing Liu",
            "Zezhu Yu",
            "Feiran Huang",
            "Yousef Bulbulia",
            "Andi Bergen",
            "Michael Liut"
        ],
        "published_date": "2024",
        "abstract": "Leveraging Large Language Models (LLMs) for personalized learning and support is becoming a promising tool in computing education. AI Assistants can help students with programming, problem-solving, converse with them to clarify course content, explain error messages to help with debugging, and much more. However, using cloud-based LLMs poses risks around data security, privacy, but also control of the overarching system. To address these concerns, we created a locally-stored Small Language Model (SLM) that leverages different Retrieval-Augmented Generation (RAG) methods to support computing students' learning. We compare one SLM (neural-chat-7b-v3 - fine-tuned version of Mistral-7B-v0.1) against two popular LLMs (gpt-3.5-turbo and gpt-4-32k) to see the viability for computing educators to use in their course(s). We use conversations from a CS1 course (N = 1,260), providing students with an AI Assistant (using gpt-3.5-turbo) to help them learn content and support problem-solving while completing their Python programming assignment. In total, we had 269 students use the AI Assistant, with a total of 1,988 questions asked. Using this real conversational data, we re-ran student questions using our novel SLM (neural-chat-7b-v3 testing nine different RAG methods) and gpt-4-32k, then compared those results against the original gpt-3.5-turbo responses. Our findings indicate that using an SLM with RAG can perform similarly, if not better, than LLMs. This shows that it is possible for computing educators to use SLMs (with RAG) in their course(s) as a tool for scalable learning, supporting content understanding and problem-solving needs, while employing their own policies on data privacy and security.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/5aabaf59808091eca1c6cba123ac2003017f4011.pdf",
        "venue": "Annual Conference on Innovation and Technology in Computer Science Education",
        "citationCount": 16,
        "score": 16.0
    },
    "425fb2f829d06d3a7b4f5936b4ee9dce71bb823f.pdf": {
        "title": "Federated Recommendation via Hybrid Retrieval Augmented Generation",
        "authors": [
            "Huimin Zeng",
            "Zhenrui Yue",
            "Qian Jiang",
            "Dong Wang"
        ],
        "published_date": "2024",
        "abstract": "Federated Recommendation (FR) emerges as a novel paradigm that enables privacy-preserving recommendations. However, traditional FR systems usually represent users/items with discrete identities (IDs), suffering from performance degradation due to data sparsity and heterogeneity in FR. On the other hand, Large Language Models (LLMs) as recommenders have proven effective across various recommendation scenarios. Yet, LLM-based recommenders encounter challenges such as incomplete recommendation and potential hallucination, compromising their performance in real-world scenarios. To this end, we propose GPT-FedRec, a federated recommendation framework leveraging ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism. GPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval process, mining ID-based user patterns and text-based item features. Next, in the second stage, the results returned by hybrid retrieval are converted into text prompts and fed into GPT for re-ranking. Under GPT-FedRec, the privacy of both local training data and global test data is well protected, as there is no data exchange across any clients or the global server. For test users, GPT-FedRec executes inference only on the global server: given the historical data of a test user, GPT-FedRec performs hybrid retrieval and GPT-based re-ranking, without exposing test data to any other clients. Our proposed hybrid retrieval mechanism and LLM-based re-ranking aim to extract generalized features from data and exploit pretrained knowledge within LLM, overcoming data sparsity and heterogeneity in FR. Finally, the RAG nature of GPT-FedRec also prevents LLM hallucination, improving the recommendation performance for real-world users. Experimental results on diverse benchmark datasets demonstrate the superior performance of GPT-FedRec against state-of-the-art baseline methods. Our code is available at https://github.com/huiminzeng/GPT-FedRec.git.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/425fb2f829d06d3a7b4f5936b4ee9dce71bb823f.pdf",
        "venue": "BigData Congress [Services Society]",
        "citationCount": 16,
        "score": 16.0
    },
    "ff173ea5f3c63127a2c8fb4cbb98dc6ab4f3696c.pdf": {
        "title": "Enhancing Code Translation in Language Models with Few-Shot Learning via Retrieval-Augmented Generation",
        "authors": [
            "Manish Bhattarai",
            "Javier E. Santos",
            "Shawn Jones",
            "Ayan Biswas",
            "Boian Alexandrov",
            "Dan O\u2019Malley"
        ],
        "published_date": "2024",
        "abstract": "The advent of large language models (LLMs) has revolutionized the field of code translation, enabling automated translation between programming languages. Despite these advancements, the accuracy and reliability of these models often falter in complex translation tasks due to a lack of contextual understanding. This paper introduces a novel approach to enhance code translation through Few-Shot Learning augmented with retrieval-based techniques. By leveraging a repository of existing code translations, we dynamically retrieve the most relevant examples to guide the model in translating new code segments. Our method, based on Retrieval-Augmented Generation (RAG), significantly improves translation quality by providing contextual examples that the model can learn from in real-time. We chose RAG over traditional fine-tuning methods due to its ability to leverage existing codebases or a locally stored corpus of code, allowing it to dynamically adapt to diverse translation tasks without the need for extensive retraining. Extensive experiments on diverse datasets, using open LLM models such as Starcoder, Llama3-70B Instruct, CodeLlama-34B Instruct, Granite-34B Code Instruct, and Mixtral-8\u00d722B, and commercial LLM models such as GPT-3.5 turbo, and GPT-4o demonstrate the superiority of our approach over traditional zero-shot, particularly in translating between Fortran and C++. We also explored different numbers of shots (examples provided to the model during inference) \u2013 specifically 1, 2, and 3 shots \u2013 and various embedding models for RAG, including Nomic-Embed, Starencoder, and CodeBERT, to evaluate the robustness and effectiveness of our approach.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/ff173ea5f3c63127a2c8fb4cbb98dc6ab4f3696c.pdf",
        "venue": "IEEE Conference on High Performance Extreme Computing",
        "citationCount": 15,
        "score": 15.0
    },
    "d1c163e6f6e4b26e94ea8f639cd441326a68afd3.pdf": {
        "title": "DomainRAG: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation",
        "authors": [
            "Shuting Wang",
            "Jiongnan Liu",
            "Jiehan Cheng",
            "Yuqi Fu",
            "Peidong Guo",
            "Kun Fang",
            "Yutao Zhu",
            "Zhicheng Dou"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) offers a promising solution to address various limitations of Large Language Models (LLMs), such as hallucination and difficulties in keeping up with real-time updates. This approach is particularly critical in expert and domain-specific applications where LLMs struggle to cover expert knowledge. Therefore, evaluating RAG models in such scenarios is crucial, yet current studies often rely on general knowledge sources like Wikipedia to assess the models' abilities in solving common-sense problems. In this paper, we evaluated LLMs by RAG settings in a domain-specific context, college enrollment. We identified six required abilities for RAG models, including the ability in conversational RAG, analyzing structural information, faithfulness to external knowledge, denoising, solving time-sensitive problems, and understanding multi-document interactions. Each ability has an associated dataset with shared corpora to evaluate the RAG models' performance. We evaluated popular LLMs such as Llama, Baichuan, ChatGLM, and GPT models. Experimental results indicate that existing closed-book LLMs struggle with domain-specific questions, highlighting the need for RAG models to solve expert problems. Moreover, there is room for RAG models to improve their abilities in comprehending conversational history, analyzing structural information, denoising, processing multi-document interactions, and faithfulness in expert knowledge. We expect future studies could solve these problems better.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/d1c163e6f6e4b26e94ea8f639cd441326a68afd3.pdf",
        "venue": "arXiv.org",
        "citationCount": 15,
        "score": 15.0
    },
    "0f8546b7634af1a70ae8649d8538d32bf6cc4660.pdf": {
        "title": "Hybrid Retrieval-Augmented Generation Approach for LLMs Query Response Enhancement",
        "authors": [
            "Pouria Omrani",
            "Alireza Hosseini",
            "Kiana Hooshanfar",
            "Zahra Ebrahimian",
            "Ramin Toosi",
            "Mohammad Ali Akhaee"
        ],
        "published_date": "2024",
        "abstract": "In the domain of Natural Language Processing (NLP), the integration of Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) represents a significant advancement towards enhancing the depth and relevance of model-generated responses. This paper introduces a novel hybrid RAG framework that synergizes the Sentence-Window and Parent-Child methodologies with an innovative re-ranking mechanism, aimed at optimizing the query response capabilities of LLMs. By leveraging external knowledge sources more effectively, the proposed method enriches LLM outputs with greater accuracy, relevance, and information fidelity. We subject our hybrid model to rigorous evaluation against benchmark datasets and metrics, demonstrating its superior performance over existing state-of-the-art RAG techniques. The results highlight our method\u2019s enhanced ability to generate responses that are not only contextually appropriate but also demonstrate a high degree of faithfulness to the source material, thereby setting a new standard for query response enhancement in LLMs. Our study underscores the potential of hybrid RAG models in refining the interaction between LLMs and external knowledge, paving the way for future research in the field of NLP.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/0f8546b7634af1a70ae8649d8538d32bf6cc4660.pdf",
        "venue": "2024 10th International Conference on Web Research (ICWR)",
        "citationCount": 15,
        "score": 15.0
    },
    "d039d31ae5768c53ee567e51bd6fadf5d62d58db.pdf": {
        "title": "Application of NotebookLM, a Large Language Model with Retrieval-Augmented Generation, for Lung Cancer Staging",
        "authors": [
            "Ryota Tozuka",
            "Hisashi Johno",
            "Akitomo Amakawa",
            "Junichi Sato",
            "Mizuki Muto",
            "Shoichiro Seki",
            "Atsushi Komaba",
            "Hiroshi Onishi"
        ],
        "published_date": "2024",
        "abstract": "PURPOSE\nIn radiology, large language models (LLMs), including ChatGPT, have recently gained attention, and their utility is being rapidly evaluated. However, concerns have emerged regarding their reliability in clinical applications due to limitations such as hallucinations and insufficient referencing. To address these issues, we focus on the latest technology, retrieval-augmented generation (RAG), which enables LLMs to reference reliable external knowledge (REK). Specifically, this study examines the utility and reliability of a recently released RAG-equipped LLM (RAG-LLM), NotebookLM, for staging lung cancer.\n\n\nMATERIALS AND METHODS\nWe summarized the current lung cancer staging guideline in Japan and provided this as REK to NotebookLM. We then tasked NotebookLM with staging 100 fictional lung cancer cases based on CT findings and evaluated its accuracy. For comparison, we performed the same task using a gold-standard LLM, GPT-4 Omni (GPT-4o), both with and without the REK. For GPT-4o, the REK was provided directly within the prompt rather than through RAG.\n\n\nRESULTS\nNotebookLM achieved 86% diagnostic accuracy in the lung cancer staging experiment, outperforming GPT-4o, which recorded 39% accuracy with the REK and 25% without it. Moreover, NotebookLM demonstrated 95% accuracy in searching reference locations within the REK.\n\n\nCONCLUSION\nNotebookLM, a RAG-LLM, successfully performed lung cancer staging by utilizing the REK, demonstrating superior performance compared to GPT-4o (without RAG). Additionally, it provided highly accurate reference locations within the REK, allowing radiologists to efficiently evaluate the reliability of NotebookLM's responses and detect possible hallucinations. Overall, this study highlights the potential of NotebookLM, a RAG-LLM, in image diagnosis.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/d039d31ae5768c53ee567e51bd6fadf5d62d58db.pdf",
        "venue": "Japanese Journal of Radiology",
        "citationCount": 14,
        "score": 14.0
    },
    "0554739e9e1df5ef2750099f8ece9243e9c5a654.pdf": {
        "title": "VISA: Retrieval Augmented Generation with Visual Source Attribution",
        "authors": [
            "Xueguang Ma",
            "Shengyao Zhuang",
            "B. Koopman",
            "G. Zuccon",
            "Wenhu Chen",
            "Jimmy Lin"
        ],
        "published_date": "2024",
        "abstract": "Generation with source attribution is important for enhancing the verifiability of retrieval-augmented generation (RAG) systems. However, existing approaches in RAG primarily link generated content to document-level references, making it challenging for users to locate evidence among multiple content-rich retrieved documents. To address this challenge, we propose Retrieval-Augmented Generation with Visual Source Attribution (VISA), a novel approach that combines answer generation with visual source attribution. Leveraging large vision-language models (VLMs), VISA identifies the evidence and highlights the exact regions that support the generated answers with bounding boxes in the retrieved document screenshots. To evaluate its effectiveness, we curated two datasets: Wiki-VISA, based on crawled Wikipedia webpage screenshots, and Paper-VISA, derived from PubLayNet and tailored to the medical domain. Experimental results demonstrate the effectiveness of VISA for visual source attribution on documents' original look, as well as highlighting the challenges for improvement. Code, data, and model checkpoints will be released.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/0554739e9e1df5ef2750099f8ece9243e9c5a654.pdf",
        "venue": "arXiv.org",
        "citationCount": 14,
        "score": 14.0
    },
    "108a5a16e4e32a3f7cb4d2668e4c6bbee3feb692.pdf": {
        "title": "TimeRAG: BOOSTING LLM Time Series Forecasting via Retrieval-Augmented Generation",
        "authors": [
            "Si-Nan Yang",
            "Dong Wang",
            "Haoqi Zheng",
            "Ruochun Jin"
        ],
        "published_date": "2024",
        "abstract": "Although the rise of large language models (LLMs) has introduced new opportunities for time series forecasting, existing LLM-based solutions require excessive training and exhibit limited transferability. In view of these challenges, we propose TimeRAG, a framework that incorporates Retrieval-Augmented Generation (RAG) into time series forecasting LLMs, which constructs a time series knowledge base from historical sequences, retrieves reference sequences from the knowledge base that exhibit similar patterns to the query sequence measured by Dynamic Time Warping (DTW), and combines these reference sequences and the prediction query as a textual prompt to the time series forecasting LLM. Experiments on datasets from various domains show that the integration of RAG improved the prediction accuracy of the original model by 2.97% on average.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/108a5a16e4e32a3f7cb4d2668e4c6bbee3feb692.pdf",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "citationCount": 14,
        "score": 14.0
    },
    "1b6c568f3b8c10f4f887d2f0487c7c0ad46093e4.pdf": {
        "title": "Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems",
        "authors": [
            "Robert Lakatos",
            "P. Pollner",
            "Andr\u00e1s Hajdu",
            "Tam\u00e1s Jo\u00f3"
        ],
        "published_date": "2024",
        "abstract": "Generative large language models (LLMs) have revolutionized the development of knowledge-based systems, enabling new possibilities in applications like ChatGPT, Bing, and Gemini. Two key strategies for domain adaptation in these systems are Domain-Specific Fine-Tuning (DFT) and Retrieval-Augmented Generation (RAG). In this study, we evaluate the performance of RAG and DFT on several LLM architectures, including GPT-J-6B, OPT-6.7B, LLaMA, and LLaMA-2. We use the ROUGE, BLEU, and METEOR scores to evaluate the performance of the models. We also measure the performance of the models with our own designed cosine similarity-based Coverage Score (CS). Our results, based on experiments across multiple datasets, show that RAG-based systems consistently outperform those fine-tuned with DFT. Specifically, RAG models outperform DFT by an average of 17% in ROUGE, 13% in BLEU, and 36% in CS. At the same time, DFT achieves only a modest advantage in METEOR, suggesting slightly better creative capabilities. We also highlight the challenges of integrating RAG with DFT, as such integration can lead to performance degradation. Furthermore, we propose a simplified RAG-based architecture that maximizes efficiency and reduces hallucination, underscoring the advantages of RAG in building reliable, domain-adapted knowledge systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/1b6c568f3b8c10f4f887d2f0487c7c0ad46093e4.pdf",
        "venue": "Machine Learning and Knowledge Extraction",
        "citationCount": 14,
        "score": 14.0
    },
    "89729cdfe0f71ad7a04c73e9167c2b266ee0ee8c.pdf": {
        "title": "Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models",
        "authors": [
            "Zhuo Chen",
            "Jiawei Liu",
            "Haotan Liu",
            "Qikai Cheng",
            "Fan Zhang",
            "Wei Lu",
            "Xiaozhong Liu"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) is applied to solve hallucination problems and real-time constraints of large language models, but it also induces vulnerabilities against retrieval corruption attacks. Existing research mainly explores the unreliability of RAG in white-box and closed-domain QA tasks. In this paper, we aim to reveal the vulnerabilities of Retrieval-Enhanced Generative (RAG) models when faced with black-box attacks for opinion manipulation. We explore the impact of such attacks on user cognition and decision-making, providing new insight to enhance the reliability and security of RAG models. We manipulate the ranking results of the retrieval model in RAG with instruction and use these results as data to train a surrogate model. By employing adversarial retrieval attack methods to the surrogate model, black-box transfer attacks on RAG are further realized. Experiments conducted on opinion datasets across multiple topics show that the proposed attack strategy can significantly alter the opinion polarity of the content generated by RAG. This demonstrates the model's vulnerability and, more importantly, reveals the potential negative impact on user cognition and decision-making, making it easier to mislead users into accepting incorrect or biased information.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/89729cdfe0f71ad7a04c73e9167c2b266ee0ee8c.pdf",
        "venue": "arXiv.org",
        "citationCount": 13,
        "score": 13.0
    },
    "821e7c70e6637f07ab94a843c0de273f8618763b.pdf": {
        "title": "PersonaRAG: Enhancing Retrieval-Augmented Generation Systems with User-Centric Agents",
        "authors": [
            "Saber Zerhoudi",
            "Michael Granitzer"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) struggle with generating reliable outputs due to outdated knowledge and hallucinations. Retrieval-Augmented Generation (RAG) models address this by enhancing LLMs with external knowledge, but often fail to personalize the retrieval process. This paper introduces PersonaRAG, a novel framework incorporating user-centric agents to adapt retrieval and generation based on real-time user data and interactions. Evaluated across various question answering datasets, PersonaRAG demonstrates superiority over baseline models, providing tailored answers to user needs. The results suggest promising directions for user-adapted information retrieval systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/821e7c70e6637f07ab94a843c0de273f8618763b.pdf",
        "venue": "IR-RAG@SIGIR",
        "citationCount": 13,
        "score": 13.0
    },
    "554bf1ba2e93599309e56d914509ec26f239301c.pdf": {
        "title": "Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World Spatial Reasoning Questions",
        "authors": [
            "Dazhou Yu",
            "Riyang Bao",
            "Gengchen Mai",
            "Liang Zhao"
        ],
        "published_date": "2025",
        "abstract": "",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/554bf1ba2e93599309e56d914509ec26f239301c.pdf",
        "venue": "arXiv.org",
        "citationCount": 13,
        "score": 13.0
    },
    "a1f3aac8462a709a7c73484699f513a92f443927.pdf": {
        "title": "Transforming Healthcare Education: Harnessing Large Language Models for Frontline Health Worker Capacity Building using Retrieval-Augmented Generation",
        "authors": [
            "Yasmina Al Ghadban",
            "Yvonne Lu",
            "Uday Adavi",
            "Ankita Sharma",
            "Sridevi Gara",
            "Neelanjana Das",
            "Bhaskar Kumar",
            "Renu John",
            "Praveen Devarsetty",
            "Jane E. Hirst"
        ],
        "published_date": "2023",
        "abstract": "In recent years, large language models (LLMs) have emerged as a transformative force in several domains, including medical education and healthcare. This paper presents a case study on the practical application of using retrieval-augmented generation (RAG) based models for enhancing healthcare education in low- and middle-income countries. The model described in this paper, SMARThealth GPT, stems from the necessity for accessible and locally relevant medical information to aid community health workers in delivering high-quality maternal care. We describe the development process of the complete RAG pipeline, including the creation of a knowledge base of Indian pregnancy-related guidelines, knowledge embedding retrieval, parameter selection and optimization, and answer generation. This case study highlights the potential of LLMs in building frontline healthcare worker capacity and enhancing guideline-based health education; and offers insights for similar applications in resource-limited settings. It serves as a reference for machine learning scientists, educators, healthcare professionals, and policymakers aiming to harness the power of LLMs for substantial educational improvement.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/a1f3aac8462a709a7c73484699f513a92f443927.pdf",
        "venue": "medRxiv",
        "citationCount": 24,
        "score": 12.0
    },
    "0406e1397b57448cfadba25222d1d8664c45c53a.pdf": {
        "title": "SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model",
        "authors": [
            "Xun Liang",
            "Simin Niu",
            "Zhiyu Li",
            "Sensen Zhang",
            "Hanyu Wang",
            "Feiyu Xiong",
            "Jason Zhaoxin Fan",
            "Bo Tang",
            "Shichao Song",
            "Mengwei Wang",
            "Jiawei Yang"
        ],
        "published_date": "2025",
        "abstract": "The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledge-intensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulnerability of LLMs because attackers can perform attack tasks by manipulating knowledge. In this paper, we introduce a benchmark named SafeRAG designed to evaluate the RAG security. First, we classify attack tasks into silver noise, inter-context conflict, soft ad, and white Denial-of-Service. Next, we construct RAG security evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We then utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter. Experiments conducted on 14 representative RAG components demonstrate that RAG exhibits significant vulnerability to all attack tasks and even the most apparent attack task can easily bypass existing retrievers, filters, or advanced LLMs, resulting in the degradation of RAG service quality. Code is available at: https://github.com/IAAR-Shanghai/SafeRAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/0406e1397b57448cfadba25222d1d8664c45c53a.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 12,
        "score": 12.0
    },
    "1dbc0e65604e34d87e6ffda47f35b6fc792af10b.pdf": {
        "title": "Accelerating Retrieval-Augmented Generation",
        "authors": [
            "Derrick Quinn",
            "Mohammad Nouri",
            "Neel Patel",
            "John Salihu",
            "Alireza Salemi",
            "Sukhan Lee",
            "Hamed Zamani",
            "Mohammad Alian"
        ],
        "published_date": "2024",
        "abstract": "An evolving solution to address hallucination and enhance accuracy in large language models (LLMs) is Retrieval-Augmented Generation (RAG), which involves augmenting LLMs with information retrieved from an external knowledge source, such as the web. This paper profiles several RAG execution pipelines and demystifies the complex interplay between their retrieval and generation phases. We demonstrate that while exact retrieval schemes are expensive, they can reduce inference time compared to approximate retrieval variants because an exact retrieval model can send a smaller but more accurate list of documents to the generative model while maintaining the same end-to-end accuracy. This observation motivates the acceleration of the exact nearest neighbor search for RAG. In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL device that implements a scale-out near-memory acceleration architecture with a novel cache-coherent interface between the host CPU and near-memory accelerators. IKS offers 13.4--27.9\u00d7 faster exact nearest neighbor search over a 512GB vector database compared with executing the search on Intel Sapphire Rapids CPUs. This higher search performance translates to 1.7--26.3\u00d7 lower end-to-end inference time for representative RAG applications. IKS is inherently a memory expander; its internal DRAM can be disaggregated and used for other applications running on the server to prevent DRAM -- which is the most expensive component in today's servers -- from being stranded.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/1dbc0e65604e34d87e6ffda47f35b6fc792af10b.pdf",
        "venue": "International Conference on Architectural Support for Programming Languages and Operating Systems",
        "citationCount": 12,
        "score": 12.0
    },
    "d92a423e09804595c8a2e241f890f5a24d326bb5.pdf": {
        "title": "Prompt-based Code Completion via Multi-Retrieval Augmented Generation",
        "authors": [
            "Hanzhuo Tan",
            "Qi Luo",
            "Lingixao Jiang",
            "Zizheng Zhan",
            "Jing Li",
            "Haotian Zhang",
            "Yuqun Zhang"
        ],
        "published_date": "2024",
        "abstract": "Automated code completion, aiming at generating subsequent tokens from unfinished code, has significantly benefited from recent progress in pre-trained Large Language Models (LLMs). However, these models often suffer from coherence issues and hallucinations when dealing with complex code logic or extrapolating beyond their training data. Existing Retrieval Augmented Generation (RAG) techniques partially address these issues by retrieving relevant code with a separate encoding model where the retrieved snippet serves as contextual reference for code completion. However, their retrieval scope is subject to a singular perspective defined by the encoding model, which largely overlooks the complexity and diversity inherent in code semantics. To address this limitation, we propose ProCC, a code completion framework leveraging prompt engineering and the contextual multi-armed bandits algorithm to flexibly incorporate and adapt to multiple perspectives of code. ProCC first employs a prompt-based multi-retriever system which crafts prompt templates to elicit LLM knowledge to understand code semantics with multiple retrieval perspectives. Then, it adopts the adaptive retrieval selection algorithm to incorporate code similarity into the decision-making process to determine the most suitable retrieval perspective for the LLM to complete the code. Experimental results demonstrate that ProCC outperforms a widely-studied code completion technique RepoCoder by 7.92% on the public benchmark CCEval, 3.19% in HumanEval-Infilling, 2.80% on our collected open-source benchmark suite, and 4.48% on the private-domain benchmark suite collected from Kuaishou Technology in terms of Exact Match. ProCC also allows augmenting fine-tuned techniques in a plug-and-play manner, yielding an averaged 6.5% improvement over the fine-tuned model.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/d92a423e09804595c8a2e241f890f5a24d326bb5.pdf",
        "venue": "ACM Transactions on Software Engineering and Methodology",
        "citationCount": 12,
        "score": 12.0
    },
    "945c6f89758074e1e7830e9086f7e58780e9e0c4.pdf": {
        "title": "Ad Auctions for LLMs via Retrieval Augmented Generation",
        "authors": [
            "Mohammadtaghi Hajiaghayi",
            "S'ebastien Lahaie",
            "Keivan Rezaei",
            "Suho Shin"
        ],
        "published_date": "2024",
        "abstract": "In the field of computational advertising, the integration of ads into the outputs of large language models (LLMs) presents an opportunity to support these services without compromising content integrity. This paper introduces novel auction mechanisms for ad allocation and pricing within the textual outputs of LLMs, leveraging retrieval-augmented generation (RAG). We propose a segment auction where an ad is probabilistically retrieved for each discourse segment (paragraph, section, or entire output) according to its bid and relevance, following the RAG framework, and priced according to competing bids. We show that our auction maximizes logarithmic social welfare, a new notion of welfare that balances allocation efficiency and fairness, and we characterize the associated incentive-compatible pricing rule. These results are extended to multi-ad allocation per segment. An empirical evaluation validates the feasibility and effectiveness of our approach over several ad auction scenarios, and exhibits inherent tradeoffs in metrics as we allow the LLM more flexibility to allocate ads.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/945c6f89758074e1e7830e9086f7e58780e9e0c4.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 12,
        "score": 12.0
    },
    "81f24ac736735a4e1b0cb860aa1ffd34af469b6d.pdf": {
        "title": "SDG target detection in environmental reports using Retrieval-augmented Generation with LLMs",
        "authors": [
            "Dar\u00edo Garigliotti"
        ],
        "published_date": "2024",
        "abstract": "With the consolidation of Large Language Models (LLM) as a dominant component in approaches for multiple linguistic tasks, the interest in these technologies has greatly increased within a variety of areas and domains. A particular scenario of information needs where to exploit these approaches is climate-aware NLP. Paradigmatically, the vast manual labour of inspecting long, heterogeneous documents to find environment-relevant expressions and claims suits well within a recently established Retrieval-augmented Generation (RAG) framework. In this paper, we tackle two dual problems within environment analysis dealing with the common goal of detecting a Sustainable Developmental Goal (SDG) target being addressed in a textual passage of an environmental assessment report.We develop relevant test collections, and propose and evaluate a series of methods within the general RAG pipeline, in order to assess the current capabilities of LLMs for the tasks of SDG target evidence identification and SDG target detection.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/81f24ac736735a4e1b0cb860aa1ffd34af469b6d.pdf",
        "venue": "CLIMATENLP",
        "citationCount": 12,
        "score": 12.0
    },
    "b565394952065c37345fb75fb66e84709b6402a3.pdf": {
        "title": "Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge Graphs, and Tensor Factorization",
        "authors": [
            "Ryan Barron",
            "Ves Grantcharov",
            "Selma Wanna",
            "M. Eren",
            "Manish Bhattarai",
            "N. Solovyev",
            "George Tompkins",
            "Charles Nicholas",
            "Kim \u00d8. Rasmussen",
            "Cynthia Matuszek",
            "Boian Alexandrov"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) are pre-trained on large-scale corpora and excel in numerous general natural language processing (NLP) tasks, such as question answering (QA). Despite their advanced language capabilities, when it comes to domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations, knowledge cut-offs, and lack of knowledge attributions. Additionally, fine tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and time consuming process. The retrieval-augmented generation (RAG) process has recently emerged as a method capable of optimization of LLM responses, by referencing them to a predetermined ontology. It was shown that using a Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into account relevant sub-graphs that preserve the information in a structured manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM framework, that integrates RAG with KG and a vector store (VS) that store factual domain specific information. Importantly, to avoid hallucinations in the KG, we build these highly domain-specific KGs and VSs without the use of LLMs, but via NLP, data mining, and nonnegative tensor factorization with automatic model selection. Pairing our RAG with a domain-specific: (i) KG (containing structured information), and (ii) VS (containing unstructured information) enables the development of domain-specific chat-bots that attribute the source of information, mitigate hallucinations, lessen the need for fine-tuning, and excel in highly domain-specific question answering tasks. We pair SMART-SLIC with chain-of-thought prompting agents. The framework is designed to be generalizable to adapt to any specific or specialized domain. In this paper, we demonstrate the question answering capabilities of our framework on a corpus of scientific publications on malware analysis and anomaly detection.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/b565394952065c37345fb75fb66e84709b6402a3.pdf",
        "venue": "International Conference on Machine Learning and Applications",
        "citationCount": 12,
        "score": 12.0
    },
    "f716a18b462826004899010dfc30947f9c01ef90.pdf": {
        "title": "RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models",
        "authors": [
            "Shiyue Zhang",
            "Mark Dredze",
            "AI Bloomberg",
            "M. Zitnik",
            "Meng Jiang",
            "Mohit Bansal",
            "James Zou",
            "Jian Pei",
            "Jian Liu",
            "Jianfeng Gao",
            "Jiawei Han",
            "Jieyu Zhao",
            "Jiliang Tang",
            "Jindong Wang",
            "Joaquin Vanschoren",
            "John C. Mitchell",
            "Kai Shu",
            "Kaidi Xu",
            "Kai-Wei Chang",
            "Lifang He",
            "Lifu Huang",
            "Michael Backes",
            "Aaron Hurst",
            "Adam Lerer",
            "Adam P. Goucher",
            "Adam Perelman",
            "Aditya Ramesh",
            "Aidan Clark",
            "AJ Os-trow",
            "Akila Welihinda",
            "Alan Hayes",
            "Alec Radford",
            "Alon Jacovi",
            "Andrew Wang",
            "Chris Alberti",
            "Connie Tao",
            "Jon Lipovetz",
            "Kate Olszewska",
            "Lukas Haas",
            "Michelle Liu",
            "Nate Keating",
            "Adam Bloniarz",
            "Carl Saroufim",
            "Corey Fry",
            "Dror Marcus",
            "Doron Kukliansky",
            "Gau-rav Singh Tomar",
            "James Swirhun",
            "J. Xing",
            "Lily Wang",
            "Madhu Gurumurthy",
            "Michael Aaron",
            "Moran Ambar",
            "Rachana Fellinger",
            "Rui Wang",
            "Zizhao Zhang",
            "S. Goldshtein",
            "Dipanjan Das. 2025",
            "Neel Jain",
            "Avi Schwarzschild",
            "Yuxin Wen",
            "Gowthami Somepalli",
            "John Kirchenbauer",
            "Ping-yeh Chiang",
            "Micah Goldblum",
            "Aniruddha Saha",
            "Jonas Geiping",
            "Tom Goldstein. 2023",
            "Jiaming Ji",
            "Mickel Liu",
            "Josef Dai",
            "Xuehai Pan",
            "Chi Zhang",
            "Juntao Dai",
            "Tianyi Qiu",
            "Bo Chen",
            "Borong Zhang",
            "Hantao Lou",
            "Kaile Wang",
            "Ya Duan"
        ],
        "published_date": "2025",
        "abstract": "Efforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming. However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on standard LLMs, which means we know little about how RAG use cases change a model's safety profile. We conduct a detailed comparative analysis of RAG and non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe and change their safety profile. We explore the causes of this change and find that even combinations of safe models with safe documents can cause unsafe generations. In addition, we evaluate some existing red teaming methods for RAG settings and show that they are less effective than when used for non-RAG settings. Our work highlights the need for safety research and red-teaming methods specifically tailored for RAG LLMs.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/f716a18b462826004899010dfc30947f9c01ef90.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 11,
        "score": 11.0
    },
    "43d35e1c866bbcfa3c573d69df217c35fcec27b2.pdf": {
        "title": "Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi Layered Thoughts",
        "authors": [
            "Chunjing Gan",
            "Dan Yang",
            "Binbin Hu",
            "Hanxiao Zhang",
            "Siyuan Li",
            "Ziqi Liu",
            "Yue Shen",
            "Lin Ju",
            "Zhiqiang Zhang",
            "Jinjie Gu",
            "Lei Liang",
            "Jun Zhou"
        ],
        "published_date": "2024",
        "abstract": "In recent years, large language models (LLMs) have made remarkable achievements in various domains. However, the untimeliness and cost of knowledge updates coupled with hallucination issues of LLMs have curtailed their applications in knowledge intensive tasks, where retrieval augmented generation (RAG) can be of help. Nevertheless, existing retrieval augmented models typically use similarity as a bridge between queries and documents and follow a retrieve then read procedure. In this work, we argue that similarity is not always the panacea and totally relying on similarity would sometimes degrade the performance of retrieval augmented generation. To this end, we propose MetRag, a Multi layEred Thoughts enhanced Retrieval Augmented Generation framework. To begin with, beyond existing similarity oriented thought, we embrace a small scale utility model that draws supervision from an LLM for utility oriented thought and further come up with a smarter model by comprehensively combining the similarity and utility oriented thoughts. Furthermore, given the fact that the retrieved document set tends to be huge and using them in isolation makes it difficult to capture the commonalities and characteristics among them, we propose to make an LLM as a task adaptive summarizer to endow retrieval augmented generation with compactness-oriented thought. Finally, with multi layered thoughts from the precedent stages, an LLM is called for knowledge augmented generation. Extensive experiments on knowledge-intensive tasks have demonstrated the superiority of MetRag.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/43d35e1c866bbcfa3c573d69df217c35fcec27b2.pdf",
        "venue": "arXiv.org",
        "citationCount": 11,
        "score": 11.0
    },
    "3fb916e2d701680cca142167496aeca7b9ec3dd3.pdf": {
        "title": "RichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation",
        "authors": [
            "Shuting Wang",
            "Xin Xu",
            "Mang Wang",
            "Weipeng Chen",
            "Yutao Zhu",
            "Zhicheng Dou"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) effectively addresses issues of static knowledge and hallucination in large language models. Existing studies mostly focus on question scenarios with clear user intents and concise answers. However, it is prevalent that users issue broad, open-ended queries with diverse sub-intents, for which they desire rich and long-form answers covering multiple relevant aspects. To tackle this important yet underexplored problem, we propose a novel RAG framework, namely RichRAG. It includes a sub-aspect explorer to identify potential sub-aspects of input questions, a multi-faceted retriever to build a candidate pool of diverse external documents related to these sub-aspects, and a generative list-wise ranker, which is a key module to provide the top-k most valuable documents for the final generator. These ranked documents sufficiently cover various query aspects and are aware of the generator's preferences, hence incentivizing it to produce rich and comprehensive responses for users. The training of our ranker involves a supervised fine-tuning stage to ensure the basic coverage of documents, and a reinforcement learning stage to align downstream LLM's preferences to the ranking of documents. Experimental results on two publicly available datasets prove that our framework effectively and efficiently provides comprehensive and satisfying responses to users.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/3fb916e2d701680cca142167496aeca7b9ec3dd3.pdf",
        "venue": "International Conference on Computational Linguistics",
        "citationCount": 11,
        "score": 11.0
    },
    "99b364eaf55295f53f6afaf6fce7c61dcd567eb9.pdf": {
        "title": "Generating Is Believing: Membership Inference Attacks against Retrieval-Augmented Generation",
        "authors": [
            "Yuying Li",
            "Gaoyang Liu",
            "Chen Wang",
            "Yang Yang"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) is a state-of-the-art technique that mitigates issues such as hallucinations and knowledge staleness in Large Language Models (LLMs) by retrieving relevant knowledge from an external database to assist in content generation. Existing research has demonstrated potential privacy risks associated with the LLMs of RAG. However, the privacy risks posed by the integration of an external database, which often contains sensitive data such as medical records or personal identities, have remained largely unexplored. In this paper, we aim to bridge this gap by focusing on membership privacy of RAG's external database, with the aim of determining whether a given sample is part of the RAG's database. Our basic idea is that if a sample is in the external database, it will exhibit a high degree of semantic similarity to the text generated by the RAG system. We present S$^2$MIA, a \\underline{M}embership \\underline{I}nference \\underline{A}ttack that utilizes the \\underline{S}emantic \\underline{S}imilarity between a given sample and the content generated by the RAG system. With our proposed S$^2$MIA, we demonstrate the potential to breach the membership privacy of the RAG database. Extensive experiment results demonstrate that S$^2$MIA can achieve a strong inference performance compared with five existing MIAs, and is able to escape from the protection of three representative defenses.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/99b364eaf55295f53f6afaf6fce7c61dcd567eb9.pdf",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "citationCount": 11,
        "score": 11.0
    },
    "758881985475e137439da465fadf968aead68c4c.pdf": {
        "title": "AutoRAG-HP: Automatic Online Hyper-Parameter Tuning for Retrieval-Augmented Generation",
        "authors": [
            "Jia Fu",
            "Xiaoting Qin",
            "Fangkai Yang",
            "Lu Wang",
            "Jue Zhang",
            "Qingwei Lin",
            "Yubo Chen",
            "Dongmei Zhang",
            "S. Rajmohan",
            "Qi Zhang"
        ],
        "published_date": "2024",
        "abstract": "Recent advancements in Large Language Models have transformed ML/AI development, necessitating a reevaluation of AutoML principles for the Retrieval-Augmented Generation (RAG) systems. To address the challenges of hyper-parameter optimization and online adaptation in RAG, we propose the AutoRAG-HP framework, which formulates the hyper-parameter tuning as an online multi-armed bandit (MAB) problem and introduces a novel two-level Hierarchical MAB (Hier-MAB) method for efficient exploration of large search spaces. We conduct extensive experiments on tuning hyper-parameters, such as top-k retrieved documents, prompt compression ratio, and embedding methods, using the ALCE-ASQA and Natural Questions datasets. Our evaluation from jointly optimization all three hyper-parameters demonstrate that MAB-based online learning methods can achieve Recall@5 $\\approx 0.8$ for scenarios with prominent gradients in search space, using only $\\sim20\\%$ of the LLM API calls required by the Grid Search approach. Additionally, the proposed Hier-MAB approach outperforms other baselines in more challenging optimization scenarios. The code will be made available at https://aka.ms/autorag.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/758881985475e137439da465fadf968aead68c4c.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 11,
        "score": 11.0
    },
    "cb1bfd79445b7c4838cf59b6a93c898a8270f42e.pdf": {
        "title": "CtrlA: Adaptive Retrieval-Augmented Generation via Probe-Guided Control",
        "authors": [
            "Huanshuo Liu",
            "Hao Zhang",
            "Zhijiang Guo",
            "Kuicai Dong",
            "Xiangyang Li",
            "Yi Lee",
            "Cong Zhang",
            "Yong Liu"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/cb1bfd79445b7c4838cf59b6a93c898a8270f42e.pdf",
        "venue": "arXiv.org",
        "citationCount": 11,
        "score": 11.0
    },
    "df2d5b7c24ce8ab7ee7efc4d7d713922d0c12abc.pdf": {
        "title": "Retrieval-Augmented Generation Approach: Document Question Answering using Large Language Model",
        "authors": [
            "Kurnia Muludi",
            "Kaira Milani Fitria",
            "Joko Triloka",
            "Sutedi"
        ],
        "published_date": "2024",
        "abstract": "\u2014This study introduces the Retrieval Augmented Generation (RAG) method to improve Question-Answering (QA) systems by addressing document processing in Natural Language Processing problems. It represents the latest breakthrough in applying RAG to document question and answer applications, overcoming previous QA system obstacles. RAG combines search techniques in vector store and text generation mechanism developed by Large Language Models, offering a time-efficient alternative to manual reading limitations. The research evaluates RAG's that use Generative Pre-trained Transformer 3.5 or GPT-3.5-turbo from the ChatGPT model and its impact on document data processing, comparing it with other applications. This research also provides datasets to test the capabilities of the QA document system. The proposed dataset and Stanford Question Answering Dataset (SQuAD) are used for performance testing. The study contributes theoretically by advancing methodologies and knowledge representation, supporting benchmarking in research communities. Results highlight RAG's superiority: achieving a precision of 0.74 in Recall-Oriented Understudy for Gisting Evaluation (ROUGE) testing, outperforming others at 0.5; obtaining an F1 score of 0.88 in BERTScore, surpassing other QA apps at 0.81; attaining a precision of 0.28 in Bilingual Evaluation Understudy (BLEU) testing, surpassing others with a precision of 0.09; and scoring 0.33 in Jaccard Similarity, outshining others at 0.04. These findings underscore RAG's efficiency and competitiveness, promising a positive impact on various industrial sectors through advanced Artificial Intelligence (AI) technology.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/df2d5b7c24ce8ab7ee7efc4d7d713922d0c12abc.pdf",
        "venue": "International Journal of Advanced Computer Science and Applications",
        "citationCount": 10,
        "score": 10.0
    },
    "f1e604441841b486f8bc257933d99e32190a06b3.pdf": {
        "title": "AlzheimerRAG: Multimodal Retrieval Augmented Generation for PubMed articles",
        "authors": [
            "A. Lahiri",
            "Q. Hu"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/f1e604441841b486f8bc257933d99e32190a06b3.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 10.0
    },
    "918fb17504fe62438e40c3340669ea53c202be04.pdf": {
        "title": "DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented Generation for Question-Answering",
        "authors": [
            "Zijian Hei",
            "Weiling Liu",
            "Wenjie Ou",
            "Juyi Qiao",
            "Junming Jiao",
            "Guowen Song",
            "Ting Tian",
            "Yi Lin"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) has recently demonstrated the performance of Large Language Models (LLMs) in the knowledge-intensive tasks such as Question-Answering (QA). RAG expands the query context by incorporating external knowledge bases to enhance the response accuracy. However, it would be inefficient to access LLMs multiple times for each query and unreliable to retrieve all the relevant documents by a single query. We have found that even though there is low relevance between some critical documents and query, it is possible to retrieve the remaining documents by combining parts of the documents with the query. To mine the relevance, a two-stage retrieval framework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is proposed to improve document retrieval recall and the accuracy of answers while maintaining efficiency. Additionally, a compact classifier is applied to two different selection strategies to determine the contribution of the retrieved documents to answering the query and retrieve the relatively relevant documents. Meanwhile, DR-RAG call the LLMs only once, which significantly improves the efficiency of the experiment. The experimental results on multi-hop QA datasets show that DR-RAG can significantly improve the accuracy of the answers and achieve new progress in QA systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/918fb17504fe62438e40c3340669ea53c202be04.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 10.0
    },
    "d083e6eded99f1345f461766a843fae9d0fee3c4.pdf": {
        "title": "HijackRAG: Hijacking Attacks against Retrieval-Augmented Large Language Models",
        "authors": [
            "Yucheng Zhang",
            "Qinfeng Li",
            "Tianyu Du",
            "Xuhong Zhang",
            "Xinkui Zhao",
            "Zhengwen Feng",
            "Jianwei Yin"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge, making them adaptable and cost-effective for various applications. However, the growing reliance on these systems also introduces potential security risks. In this work, we reveal a novel vulnerability, the retrieval prompt hijack attack (HijackRAG), which enables attackers to manipulate the retrieval mechanisms of RAG systems by injecting malicious texts into the knowledge database. When the RAG system encounters target questions, it generates the attacker's pre-determined answers instead of the correct ones, undermining the integrity and trustworthiness of the system. We formalize HijackRAG as an optimization problem and propose both black-box and white-box attack strategies tailored to different levels of the attacker's knowledge. Extensive experiments on multiple benchmark datasets show that HijackRAG consistently achieves high attack success rates, outperforming existing baseline attacks. Furthermore, we demonstrate that the attack is transferable across different retriever models, underscoring the widespread risk it poses to RAG systems. Lastly, our exploration of various defense mechanisms reveals that they are insufficient to counter HijackRAG, emphasizing the urgent need for more robust security measures to protect RAG systems in real-world deployments.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/d083e6eded99f1345f461766a843fae9d0fee3c4.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 10.0
    },
    "90193735c3a84cf608409007df1bf409fd6635c6.pdf": {
        "title": "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation",
        "authors": [
            "Jirui Qi",
            "Gabriele Sarti",
            "R. Fern'andez",
            "Arianna Bisazza"
        ],
        "published_date": "2024",
        "abstract": "Ensuring the verifiability of model answers is a fundamental challenge for retrieval-augmented generation (RAG) in the question answering (QA) domain. Recently, self-citation prompting was proposed to make large language models (LLMs) generate citations to supporting documents along with their answers. However, self-citing LLMs often struggle to match the required format, refer to non-existent sources, and fail to faithfully reflect LLMs\u2019 context usage throughout the generation. In this work, we present MIRAGE \u2013 Model Internals-based RAG Explanations \u2013 a plug-and-play approach using model internals for faithful answer attribution in RAG applications. MIRAGE detects context-sensitive answer tokens and pairs them with retrieved documents contributing to their prediction via saliency methods. We evaluate our proposed approach on a multilingual extractive QA dataset, finding high agreement with human answer attribution. On open-ended QA, MIRAGE achieves citation quality and efficiency comparable to self-citation while also allowing for a finer-grained control of attribution parameters. Our qualitative evaluation highlights the faithfulness of MIRAGE\u2019s attributions and underscores the promising application of model internals for RAG answer attribution. Code and data released at https://github.com/Betswish/MIRAGE.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/90193735c3a84cf608409007df1bf409fd6635c6.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 10,
        "score": 10.0
    },
    "f7ed6f02ba64866d3f7b7bac3bf65da36cef1336.pdf": {
        "title": "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation",
        "authors": [
            "Bolei He",
            "Nuo Chen",
            "Xinran He",
            "Lingyong Yan",
            "Zhenkai Wei",
            "Jinchang Luo",
            "Zhen-Hua Ling"
        ],
        "published_date": "2024",
        "abstract": "Recent Retrieval Augmented Generation (RAG) aims to enhance Large Language Models (LLMs) by incorporating extensive knowledge retrieved from external sources. However, such approach encounters some challenges: Firstly, the original queries may not be suitable for precise retrieval, resulting in erroneous contextual knowledge; Secondly, the language model can easily generate inconsistent answer with external references due to their knowledge boundary limitation. To address these issues, we propose the chain-of-verification (CoV-RAG) to enhance the external retrieval correctness and internal generation consistency. Specifically, we integrate the verification module into the RAG, engaging in scoring, judgment, and rewriting. To correct external retrieval errors, CoV-RAG retrieves new knowledge using a revised query. To correct internal generation errors, we unify QA and verification tasks with a Chain-of-Thought (CoT) reasoning during training. Our comprehensive experiments across various LLMs demonstrate the effectiveness and adaptability compared with other strong baselines. Especially, our CoV-RAG can significantly surpass the state-of-the-art baselines using different LLM backbones.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/f7ed6f02ba64866d3f7b7bac3bf65da36cef1336.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 10,
        "score": 10.0
    },
    "444aa31192c87f996bb01fa856cb765a19cd5323.pdf": {
        "title": "Robust Implementation of Retrieval-Augmented Generation on Edge-Based Computing-in-Memory Architectures",
        "authors": [
            "Ruiyang Qin",
            "Zheyu Yan",
            "Dewen Zeng",
            "Zhenge Jia",
            "Dancheng Liu",
            "Jianbo Liu",
            "Zhi Zheng",
            "N. Cao",
            "Kai Ni",
            "Jinjun Xiong",
            "Yiyu Shi"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) deployed on edge devices learn through fine-tuning and updating a certain portion of their parameters. Although such learning methods can be optimized to reduce resource utilization, the overall required resources remain a heavy burden on edge devices. Instead, Retrieval-Augmented Generation (RAG), a resource-efficient LLM learning method, can improve the quality of the LLM-generated content without updating model parameters. However, the RAG-based LLM may involve repetitive searches on the profile data in every user-LLM interaction. This search can lead to significant latency along with the accumulation of user data. Conventional efforts to decrease latency result in restricting the size of saved user data, thus reducing the scalability of RAG as user data continuously grows. It remains an open question: how to free RAG from the constraints of latency and scalability on edge devices? In this paper, we propose a novel framework to accelerate RAG via Computing-in-Memory (CiM) architectures. It accelerates matrix multiplications by performing in-situ computation inside the memory while avoiding the expensive data transfer between the computing unit and memory. Our framework, Ro bust CiM-backed R AG (RoCR), utilizing a novel contrastive learning-based training method and noise-aware training, can enable RAG to efficiently search profile data with CiM. To the best of our knowledge, this is the first work utilizing CiM to accelerate RAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/444aa31192c87f996bb01fa856cb765a19cd5323.pdf",
        "venue": "International Conference on Computer Aided Design",
        "citationCount": 10,
        "score": 10.0
    },
    "6ea7b51bc1c2865d6af95d93df18687a8de16c7a.pdf": {
        "title": "Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs",
        "authors": [
            "Zheng Wang",
            "Zhongyang Li",
            "Zeren Jiang",
            "Dandan Tu",
            "Wei Shi"
        ],
        "published_date": "2024",
        "abstract": "In the age of mobile internet, user data, often referred to as memories, is continuously generated on personal devices. Effectively managing and utilizing this data to deliver services to users is a compelling research topic. In this paper, we introduce a novel task of crafting personalized agents powered by large language models (LLMs), which utilize a user\u2019s smartphone memories to enhance downstream applications with advanced LLM capabilities. To achieve this goal, we introduce EMG-RAG, a solution that combines Retrieval-Augmented Generation (RAG) techniques with an Editable Memory Graph (EMG). This approach is further optimized using Reinforcement Learning to address three distinct challenges: data collection, editability, and selectability. Extensive experiments on a real-world dataset validate the effectiveness of EMG-RAG, achieving an improvement of approximately 10% over the best existing approach. Additionally, the personalized agents have been transferred into a real smartphone AI assistant, which leads to enhanced usability.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/6ea7b51bc1c2865d6af95d93df18687a8de16c7a.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 10,
        "score": 10.0
    },
    "9c45b4af25e192733d42a8d384e41002786d0d32.pdf": {
        "title": "Superposition Prompting: Improving and Accelerating Retrieval-Augmented Generation",
        "authors": [
            "Thomas Merth",
            "Qichen Fu",
            "Mohammad Rastegari",
            "Mahyar Najibi"
        ],
        "published_date": "2024",
        "abstract": "Despite the successes of large language models (LLMs), they exhibit significant drawbacks, particularly when processing long contexts. Their inference cost scales quadratically with respect to sequence length, making it expensive for deployment in some real-world text processing applications, such as retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the\"distraction phenomenon\", where irrelevant context in the prompt degrades output quality. To address these drawbacks, we propose a novel RAG prompting methodology, *superposition prompting*, which can be directly applied to pre-trained transformer-based LLMs *without the need for fine-tuning*. At a high level, superposition prompting allows the LLM to process input documents in parallel *prompt paths*, discarding paths once they are deemed irrelevant. We demonstrate the capability of our method to simultaneously enhance time efficiency across a variety of question-answering benchmarks using multiple pre-trained LLMs. Furthermore, our technique significantly improves accuracy when the retrieved context is large relative the context the model was trained on. For example, our approach facilitates a 93x reduction in compute time while *improving* accuracy by 43% on the NaturalQuestions-Open dataset with the MPT-7B instruction-tuned model over naive RAG.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/9c45b4af25e192733d42a8d384e41002786d0d32.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 10,
        "score": 10.0
    },
    "d0f9e5fbbbdacbb6deabfc260cd495b1f8457e28.pdf": {
        "title": "Optimizing Retrieval-Augmented Generation with Elasticsearch for Enhanced Question-Answering Systems",
        "authors": [
            "Jiajing Chen",
            "Runyuan Bao",
            "Hongye Zheng",
            "Zhen Qi",
            "Jianjun Wei",
            "Jiacheng Hu"
        ],
        "published_date": "2024",
        "abstract": "This study aims to improve the accuracy and quality of large-scale language models (LLMs) in answering questions by integrating Elasticsearch into the Retrieval Augmented Generation (RAG) framework. The experiment uses the Stanford Question Answering Dataset (SQuAD) version 2.0 as the test dataset and compares the performance of different retrieval methods, including traditional methods based on keyword matching or semantic similarity calculation, BM25-RAG and TF-IDF-RAG, and the newly proposed ES-RAG scheme. The results show that ES-RAG not only has obvious advantages in retrieval efficiency but also performs well in key indicators such as accuracy, which is 0.51 percentage points higher than TF-IDFRAG. In addition, Elasticsearch's powerful search capabilities and rich configuration options enable the entire questionanswering system to better handle complex queries and provide more flexible and efficient responses based on the diverse needs of users. Future research directions can further explore how to optimize the interaction mechanism between Elasticsearch and LLM, such as introducing higher-level semantic understanding and context-awareness capabilities, to achieve a more intelligent and humanized question-answering experience.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/d0f9e5fbbbdacbb6deabfc260cd495b1f8457e28.pdf",
        "venue": "2024 5th International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",
        "citationCount": 10,
        "score": 10.0
    },
    "7047d94171efc72f868339302d966b51122fe6a1.pdf": {
        "title": "Dubo-SQL: Diverse Retrieval-Augmented Generation and Fine Tuning for Text-to-SQL",
        "authors": [
            "Dayton G. Thorpe",
            "Andrew Duberstein",
            "Ian A. Kinsey"
        ],
        "published_date": "2024",
        "abstract": "The current state-of-the-art (SOTA) for automated text-to-SQL still falls well short of expert human performance as measured by execution accuracy (EX) on the BIRD-SQL benchmark. The most accurate methods are also slow and expensive. To advance the SOTA for text-to-SQL while reducing cost and improving speed, we explore the combination of low-cost fine tuning, novel methods for diverse retrieval-augmented generation (RAG) and new input and output formats that help large language models (LLMs) achieve higher EX. We introduce two new methods, Dubo-SQL v1 and v2. Dubo-SQL v1 sets a new record for EX on the holdout test set of BIRD-SQL. Dubo-SQL v2 achieves even higher performance on the BIRD-SQL dev set. Dubo-SQL v1 relies on LLMs from OpenAI, but uses the low-cost GPT-3.5 Turbo while exceeding the performance of the next-best model using OpenAI, which instead uses the more expensive GPT-4. Dubo-SQL v1 exceeds the performance of the next-best model using GPT-3.5 by over 20%. Dubo-SQL v2 uses GPT-4 Turbo and RAG in place of fine tuning to push EX higher.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/7047d94171efc72f868339302d966b51122fe6a1.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 10.0
    },
    "e7863d8f292062078475aa1dfbdc182b67c2fcfb.pdf": {
        "title": "Advancing Cyber Incident Timeline Analysis Through Retrieval-Augmented Generation and Large Language Models",
        "authors": [
            "Fatma Yasmine Loumachi",
            "M. C. Ghanem",
            "M. Ferrag"
        ],
        "published_date": "2024",
        "abstract": "Cyber timeline analysis or forensic timeline analysis is critical in digital forensics and incident response (DFIR) investigations. It involves examining artefacts and events\u2014particularly their timestamps and associated metadata\u2014to detect anomalies, establish correlations, and reconstruct a detailed sequence of the incident. Traditional approaches rely on processing structured artefacts, such as logs and filesystem metadata, using multiple specialised tools for evidence identification, feature extraction, and timeline reconstruction. This paper introduces an innovative framework, GenDFIR, a context-specific approach powered via large language model (LLM) capabilities. Specifically, it proposes the use of Llama 3.1 8B in zero-shot, selected for its ability to understand cyber threat nuances, integrated with a retrieval-augmented generation (RAG) agent. Our approach comprises two main stages: (1) Data preprocessing and structuring: incident events, represented as textual data, are transformed into a well-structured document, forming a comprehensive knowledge base of the incident. (2) Context retrieval and semantic enrichment: a RAG agent retrieves relevant incident events from the knowledge base based on user prompts. The LLM processes the pertinent retrieved context, enabling a detailed interpretation and semantic enhancement. The proposed framework was tested on synthetic cyber incident events in a controlled environment, with results assessed using DFIR-tailored, context-specific metrics designed to evaluate the framework\u2019s performance, reliability, and robustness, supported by human evaluation to validate the accuracy and reliability of the outcomes. Our findings demonstrate the practical power of LLMs in advancing the automation of cyber-incident timeline analysis, a subfield within DFIR. This research also highlights the potential of generative AI, particularly LLMs, and opens new possibilities for advanced threat detection and incident reconstruction.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/e7863d8f292062078475aa1dfbdc182b67c2fcfb.pdf",
        "venue": "De Computis",
        "citationCount": 9,
        "score": 9.0
    },
    "fc409c663357758248eea787afd1c7809f30c6f3.pdf": {
        "title": "P-RAG: Progressive Retrieval Augmented Generation For Planning on Embodied Everyday Task",
        "authors": [
            "Weiye Xu",
            "Min Wang",
            "Wen-gang Zhou",
            "Houqiang Li"
        ],
        "published_date": "2024",
        "abstract": "Embodied Everyday Task is a popular task in the embodied AI community, requiring agents to make a sequence of actions based on natural language instructions and visual observations. Traditional learning-based approaches face two challenges. Firstly, natural language instructions often lack explicit task planning. Secondly, extensive training is required to equip models with knowledge of the task environment. Previous works based on Large Language Model (LLM) either suffer from poor performance due to the lack of task-specific knowledge or rely on ground truth as few-shot samples. To address the above limitations, we propose a novel approach called Progressive Retrieval Augmented Generation (P-RAG), which not only effectively leverages the powerful language processing capabilities of LLMs but also progressively accumulates task-specific knowledge without ground-truth. Compared to the conventional RAG methods, which retrieve relevant information from the database in a one-shot manner to assist generation, P-RAG introduces an iterative approach to progressively update the database. In each iteration, P-RAG retrieves the latest database and obtains historical information from the previous interaction as experiential references for the current interaction. Moreover, we also introduce a more granular retrieval scheme that not only retrieves similar tasks but also incorporates retrieval of similar situations to provide more valuable reference experiences. Extensive experiments reveal that P-RAG achieves competitive results without utilizing ground truth and can even further improve performance through self-iterations.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/fc409c663357758248eea787afd1c7809f30c6f3.pdf",
        "venue": "ACM Multimedia",
        "citationCount": 9,
        "score": 9.0
    },
    "6a16c62fbbc1d08bb8db14715af565252a0c099e.pdf": {
        "title": "Advancing TTP Analysis: Harnessing the Power of Encoder-Only and Decoder-Only Language Models with Retrieval Augmented Generation",
        "authors": [
            "Reza Fayyazi",
            "Rozhina Taghdimi",
            "S. Yang"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/6a16c62fbbc1d08bb8db14715af565252a0c099e.pdf",
        "venue": "arXiv.org",
        "citationCount": 9,
        "score": 9.0
    },
    "66a3bc8d77c5e1883ab293c8398872e982b5fbe2.pdf": {
        "title": "Adaptive Retrieval-Augmented Generation for Conversational Systems",
        "authors": [
            "Xi Wang",
            "Procheta Sen",
            "Ruizhe Li",
            "Emine Yilmaz"
        ],
        "published_date": "2024",
        "abstract": "Despite the success of integrating large language models into the development of conversational systems, many studies have shown the effectiveness of retrieving and augmenting external knowledge for informative responses. Hence, many existing studies commonly assume the always need for Retrieval Augmented Generation (RAG) in a conversational system without explicit control. This raises a research question about such a necessity. In this study, we propose to investigate the need for each turn of system response to be augmented with external knowledge. In particular, by leveraging human judgements on the binary choice of adaptive augmentation, we develop RAGate, a gating model, which models conversation context and relevant inputs to predict if a conversational system requires RAG for improved responses. We conduct extensive experiments on devising and applying RAGate to conversational models and well-rounded analyses of different conversational scenarios. Our experimental results and analysis indicate the effective application of RAGate in RAG-based conversational systems in identifying system responses for appropriate RAG with high-quality responses and a high generation confidence. This study also identifies the correlation between the generation's confidence level and the relevance of the augmented knowledge.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/66a3bc8d77c5e1883ab293c8398872e982b5fbe2.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 9,
        "score": 9.0
    },
    "809fd2803368801913840712eefba23737d7e64c.pdf": {
        "title": "RAD-Bench: Evaluating Large Language Models Capabilities in Retrieval Augmented Dialogues",
        "authors": [
            "Tzu-Lin Kuo",
            "Fengting Liao",
            "Mu-Wei Hsieh",
            "Fu-Chieh Chang",
            "Po-Chun Hsu",
            "Da-shan Shiu"
        ],
        "published_date": "2024",
        "abstract": "In real-world applications with Large Language Models (LLMs), external retrieval mechanisms - such as Search-Augmented Generation (SAG), tool utilization, and Retrieval-Augmented Generation (RAG) - are often employed to enhance the quality of augmented generations in dialogues. These approaches often come with multi-turn dialogue, where each interaction is enriched by relevant information retrieved from external sources. Existing benchmarks either assess LLMs' chat abilities in multi-turn dialogues or their use of retrieval for augmented responses in single-turn settings. However, there is a gap in evaluating LLMs' ability to leverage retrieval for more precise responses across multiple turns. To address this limitation, we introduce RAD-Bench (Retrieval Augmented Dialogue), a benchmark designed to evaluate LLMs' capabilities in multi-turn dialogues following retrievals, essential for their deployment in context-rich applications. RAD-Bench evaluates two key abilities of LLMs: Retrieval Synthesis and Retrieval Reasoning. These are measured using discriminative questions and retrieved contexts, and corresponding reference answers, assessing how effectively LLMs integrate and reason with context to maintain and enhance conversation quality over multiple turns. Our evaluation results on commonly used LLMs reveal that model performance deteriorates as additional layers of conditions or constraints are applied across conversation turns, even when accurate retrieved contexts are provided. The data and code are available at https://github.com/mtkresearch/RAD-Bench",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/809fd2803368801913840712eefba23737d7e64c.pdf",
        "venue": "arXiv.org",
        "citationCount": 9,
        "score": 9.0
    },
    "3e59c8f47b211bf82a1b0fa886fa399ec25044c7.pdf": {
        "title": "Emergency Patient Triage Improvement through a Retrieval-Augmented Generation Enhanced Large-Scale Language Model",
        "authors": [
            "Megumi Yazaki",
            "S. Maki",
            "T. Furuya",
            "Ken Inoue",
            "Ko Nagai",
            "Yuki Nagashima",
            "Juntaro Maruyama",
            "Yasunori Toki",
            "Kyota Kitagawa",
            "Shuhei Iwata",
            "Takaki Kitamura",
            "Sho Gushiken",
            "Yuji Noguchi",
            "M. Inoue",
            "Yasuhiro Shiga",
            "K. Inage",
            "S. Orita",
            "Taka-aki Nakada",
            "S. Ohtori"
        ],
        "published_date": "2024",
        "abstract": "Abstract Objectives Emergency medical triage is crucial for prioritizing patient care in emergency situations, yet its effectiveness can vary significantly based on the experience and training of the personnel involved. This study aims to evaluate the efficacy of integrating Retrieval Augmented Generation (RAG) with Large Language Models (LLMs), specifically OpenAI's GPT models, to standardize triage procedures and reduce variability in emergency care. Methods We created 100 simulated triage scenarios based on modified cases from the Japanese National Examination for Emergency Medical Technicians. These scenarios were processed by the RAG-enhanced LLMs, and the models were given patient vital signs, symptoms, and observations from emergency medical services (EMS) teams as inputs. The primary outcome was the accuracy of triage classifications, which was used to compare the performance of the RAG-enhanced LLMs with that of emergency medical technicians and emergency physicians. Secondary outcomes included the rates of under-triage and over-triage. Results The Generative Pre-trained Transformer 3.5 (GPT-3.5) with RAG model achieved a correct triage rate of 70%, significantly outperforming Emergency Medical Technicians (EMTs) with 35% and 38% correct rates, and emergency physicians with 50% and 47% correct rates (p\u2009<\u20090.05). Additionally, this model demonstrated a substantial reduction in under-triage rates to 8%, compared with 33% for GPT-3.5 without RAG, and 39% for GPT-4 without RAG. Conclusions The integration of RAG with LLMs shows promise in improving the accuracy and consistency of medical assessments in emergency settings. Further validation in diverse medical settings with broader datasets is necessary to confirm the effectiveness and adaptability of these technologies in live environments.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/3e59c8f47b211bf82a1b0fa886fa399ec25044c7.pdf",
        "venue": "Prehospital Emergency Care",
        "citationCount": 9,
        "score": 9.0
    },
    "4eec7ad1aef177297d0c2019434a84e48fb1f4f7.pdf": {
        "title": "Backdoored Retrievers for Prompt Injection Attacks on Retrieval Augmented Generation of Large Language Models",
        "authors": [
            "Cody Clop",
            "Yannick Teglia"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in generating coherent text but remain limited by the static nature of their training data. Retrieval Augmented Generation (RAG) addresses this issue by combining LLMs with up-to-date information retrieval, but also expand the attack surface of the system. This paper investigates prompt injection attacks on RAG, focusing on malicious objectives beyond misinformation, such as inserting harmful links, promoting unauthorized services, and initiating denial-of-service behaviors. We build upon existing corpus poisoning techniques and propose a novel backdoor attack aimed at the fine-tuning process of the dense retriever component. Our experiments reveal that corpus poisoning can achieve significant attack success rates through the injection of a small number of compromised documents into the retriever corpus. In contrast, backdoor attacks demonstrate even higher success rates but necessitate a more complex setup, as the victim must fine-tune the retriever using the attacker poisoned dataset.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/4eec7ad1aef177297d0c2019434a84e48fb1f4f7.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 8.0
    },
    "42d1dfab4a35583cac1e522a652800f0093285ff.pdf": {
        "title": "Enhancing Large Language Model Reliability: Minimizing Hallucinations with Dual Retrieval-Augmented Generation Based on the Latest Diabetes Guidelines",
        "authors": [
            "Jaedong Lee",
            "H. Cha",
            "Y. Hwangbo",
            "W. Cheon"
        ],
        "published_date": "2024",
        "abstract": "Background/Objectives: Large language models (LLMs) show promise in healthcare but face challenges with hallucinations, particularly in rapidly evolving fields like diabetes management. Traditional LLM updating methods are resource-intensive, necessitating new approaches for delivering reliable, current medical information. This study aimed to develop and evaluate a novel retrieval system to enhance LLM reliability in diabetes management across different languages and guidelines. Methods: We developed a dual retrieval-augmented generation (RAG) system integrating both Korean Diabetes Association and American Diabetes Association 2023 guidelines. The system employed dense retrieval with 11 embedding models (including OpenAI, Upstage, and multilingual models) and sparse retrieval using BM25 algorithm with language-specific tokenizers. Performance was evaluated across different top-k values, leading to optimized ensemble retrievers for each guideline. Results: For dense retrievers, Upstage\u2019s Solar Embedding-1-large and OpenAI\u2019s text-embedding-3-large showed superior performance for Korean and English guidelines, respectively. Multilingual models outperformed language-specific models in both cases. For sparse retrievers, the ko_kiwi tokenizer demonstrated superior performance for Korean text, while both ko_kiwi and porter_stemmer showed comparable effectiveness for English text. The ensemble retrievers, combining optimal dense and sparse configurations, demonstrated enhanced coverage while maintaining precision. Conclusions: This study presents an effective dual RAG system that enhances LLM reliability in diabetes management across different languages. The successful implementation with both Korean and American guidelines demonstrates the system\u2019s cross-regional capability, laying a foundation for more trustworthy AI-assisted healthcare applications.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/42d1dfab4a35583cac1e522a652800f0093285ff.pdf",
        "venue": "Journal of Personalized Medicine",
        "citationCount": 8,
        "score": 8.0
    },
    "4440c1d44c449f4b4ea9273dd667b5c036e6b8c6.pdf": {
        "title": "Towards Omni-RAG: Comprehensive Retrieval-Augmented Generation for Large Language Models in Medical Applications",
        "authors": [
            "Zhe Chen",
            "Yusheng Liao",
            "Shuyang Jiang",
            "Pingjie Wang",
            "Yiqiu Guo",
            "Yanfeng Wang",
            "Yu Wang"
        ],
        "published_date": "2025",
        "abstract": "Large language models hold promise for addressing medical challenges, such as medical diagnosis reasoning, research knowledge acquisition, clinical decision-making, and consumer health inquiry support. However, they often generate hallucinations due to limited medical knowledge. Incorporating external knowledge is therefore critical, which necessitates multi-source knowledge acquisition. We address this challenge by framing it as a source planning problem, which is to formulate context-appropriate queries tailored to the attributes of diverse sources. Existing approaches either overlook source planning or fail to achieve it effectively due to misalignment between the model's expectation of the sources and their actual content. To bridge this gap, we present MedOmniKB, a repository comprising multigenre and multi-structured medical knowledge sources. Leveraging these sources, we propose the Source Planning Optimisation method, which enhances multi-source utilisation. Our approach involves enabling an expert model to explore and evaluate potential plans while training a smaller model to learn source alignment. Experimental results demonstrate that our method substantially improves multi-source planning performance, enabling the optimised small model to achieve state-of-the-art results in leveraging diverse medical knowledge sources.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/4440c1d44c449f4b4ea9273dd667b5c036e6b8c6.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 7,
        "score": 7.0
    },
    "d9676825ff6e102c2bb7c19677612987e0923739.pdf": {
        "title": "Improving Automated Deep Phenotyping Through Large Language Models Using Retrieval Augmented Generation",
        "authors": [
            "Brandon T Garcia",
            "Lauren Westerfield",
            "Priya Yelemali",
            "Nikhita Gogate",
            "E. A. Rivera-Munoz",
            "Haowei Du",
            "Moez Dawood",
            "Angad Jolly",
            "James R. Lupski",
            "J. Posey"
        ],
        "published_date": "2024",
        "abstract": "Abstract Background: Diagnosing rare genetic disorders relies on precise phenotypic and genotypic analysis, with the Human Phenotype Ontology (HPO) providing a standardized language for capturing clinical phenotypes. Traditional HPO tools, such as Doc2HPO and ClinPhen, employ concept recognition to automate phenotype extraction but struggle with incomplete phenotype assignment, often requiring intensive manual review. While large language models (LLMs) hold promise for more context-driven phenotype extraction, they are prone to errors and hallucinations, making them less reliable without further refinement. We present RAG-HPO, a Python-based tool that leverages Retrieval-Augmented Generation (RAG) to elevate LLM accuracy in HPO term assignment, bypassing the limitations of baseline models while avoiding the time and resource intensive process of fine-tuning. RAG-HPO integrates a dynamic vector database, allowing real-time retrieval and contextual matching. Methods: The high-dimensional vector database utilized by RAG-HPO includes >54,000 phenotypic phrases mapped to HPO IDs, derived from the HPO database and supplemented with additional validated phrases. The RAG-HPO workflow uses an LLM to first extract phenotypic phrases that are then matched via semantic similarity to entries within a vector database before providing best term matches back to the LLM as context for final HPO term assignment. A benchmarking dataset of 120 published case reports with 1,792 manually-assigned HPO terms was developed, and the performance of RAG-HPO measured against existing published tools Doc2HPO, ClinPhen, and FastHPOCR. Results: In evaluations, RAG-HPO, powered by Llama-3 70B and applied to a set of 120 case reports, achieved a mean precision of 0.84, recall of 0.78, and an F1 score of 0.80-significantly surpassing conventional tools (p<0.00001). False positive HPO term identification occurred for 15.8% (256/1,624) of terms, of which only 2.7% (7/256) represented hallucinations, and 33.6% (86/256) unrelated terms; the remainder of false positives (63.7%, 163/256) were relative terms of the target term. Conclusions: RAG-HPO is a user-friendly, adaptable tool designed for secure evaluation of clinical text and outperforms standard HPO-matching tools in precision, recall, and F1. Its enhanced precision and recall represent a substantial advancement in phenotypic analysis, accelerating the identification of genetic mechanisms underlying rare diseases and driving progress in genetic research and clinical genomics.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/d9676825ff6e102c2bb7c19677612987e0923739.pdf",
        "venue": "medRxiv",
        "citationCount": 7,
        "score": 7.0
    },
    "4da5c68bea931480d6abb288639cf412f7719e5f.pdf": {
        "title": "Dual retrieving and ranking medical large language model with retrieval augmented generation",
        "authors": [
            "Qimin Yang",
            "Huan Zuo",
            "Runqi Su",
            "Hanyinghong Su",
            "Tangyi Zeng",
            "Huimei Zhou",
            "Rongsheng Wang",
            "Jiexin Chen",
            "Yijun Lin",
            "Zhiyi Chen",
            "Tao Tan"
        ],
        "published_date": "2025",
        "abstract": "Recent advancements in large language models (LLMs) have significantly enhanced text generation across various sectors; however, their medical application faces critical challenges regarding both accuracy and real-time responsiveness. To address these dual challenges, we propose a novel two-step retrieval and ranking retrieval-augmented generation (RAG) framework that synergistically combines embedding search with Elasticsearch technology. Built upon a dynamically updated medical knowledge base incorporating expert-reviewed documents from leading healthcare institutions, our hybrid architecture employs ColBERTv2 for context-aware result ranking while maintaining computational efficiency. Experimental results show a 10% improvement in accuracy for complex medical queries compared to standalone LLM and single-search RAG variants, while acknowledging that latency challenges remain in emergency situations requiring sub-second responses in an experimental setting, which can be achieved in real-time using more powerful hardware in real-world deployments. This work establishes a new paradigm for reliable medical AI assistants that successfully balances accuracy and practical deployment considerations.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/4da5c68bea931480d6abb288639cf412f7719e5f.pdf",
        "venue": "Scientific Reports",
        "citationCount": 7,
        "score": 7.0
    },
    "800b396437db5844b5d5ddd08e46b15b8910a49d.pdf": {
        "title": "How to Build an AI Tutor that Can Adapt to Any Course and Provide Accurate Answers Using Large Language Model and Retrieval-Augmented Generation",
        "authors": [
            "Chenxi Dong"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/800b396437db5844b5d5ddd08e46b15b8910a49d.pdf",
        "venue": "arXiv.org",
        "citationCount": 14,
        "score": 7.0
    },
    "095decd5488d0890c3860e6f8344dafe187d7eb6.pdf": {
        "title": "Time-Sensitve Retrieval-Augmented Generation for Question Answering",
        "authors": [
            "Feifan Wu",
            "Lingyuan Liu",
            "Wentao He",
            "Ziqi Liu",
            "Zhiqiang Zhang",
            "Haofen Wang",
            "Meng Wang"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by accessing external data sources, offering a promising way to improve accuracy and reliability. Despite its potential, conventional retrievers encounter bias and flaws with time-sensitive queries. In this paper, a benchmark query dataset is constructed to retrieve documents containing time-evolving facts, and the results show that current embedding-based similarity-matching methods struggle to handle queries with explicit temporal constraints. Therefore, we propose a novel approach that integrates supervised contrastive learning with tailored negative sample pairs for temporal constraints to train the retriever of an RAG system, along with query-side fine-tuning and routing techniques. Experimental results show that our approach significantly enhances the retriever performance of time-sensitive queries while ensuring the effectiveness of general queries. We will make the code and dataset publicly available at https://github.com/suzhou-22/TS-Retriever.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/095decd5488d0890c3860e6f8344dafe187d7eb6.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 7,
        "score": 7.0
    },
    "1bab539dd0318fe446fe50574253bdf4600b112a.pdf": {
        "title": "On the Role of Long-tail Knowledge in Retrieval Augmented Large Language Models",
        "authors": [
            "Dongyang Li",
            "Junbing Yan",
            "Taolin Zhang",
            "Chengyu Wang",
            "Xiaofeng He",
            "Longtao Huang",
            "Hui Xue",
            "Junyuan Huang"
        ],
        "published_date": "2024",
        "abstract": "Retrieval augmented generation (RAG) exhibits outstanding performance in promoting the knowledge capabilities of large language models (LLMs) with retrieved documents related to user queries. However, RAG only focuses on improving the response quality of LLMs via enhancing queries indiscriminately with retrieved information, paying little attention to what type of knowledge LLMs really need to answer original queries more accurately. In this paper, we suggest that long-tail knowledge is crucial for RAG as LLMs have already remembered common world knowledge during large-scale pre-training. Based on our observation, we propose a simple but effective long-tail knowledge detection method for LLMs. Specifically, the novel Generative Expected Calibration Error (GECE) metric is derived to measure the ``long-tailness'' of knowledge based on both statistics and semantics. Hence, we retrieve relevant documents and infuse them into the model for patching knowledge loopholes only when the input query relates to long-tail knowledge. Experiments show that, compared to existing RAG pipelines, our method achieves over 4x speedup in average inference time and consistent performance improvement in downstream tasks.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/1bab539dd0318fe446fe50574253bdf4600b112a.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 7,
        "score": 7.0
    },
    "dbf5054b6aa6ef75887174d0ea1f075974743765.pdf": {
        "title": "OG-RAG: Ontology-Grounded Retrieval-Augmented Generation For Large Language Models",
        "authors": [
            "Kartik Sharma",
            "Peeyush Kumar",
            "Yunqing Li"
        ],
        "published_date": "2024",
        "abstract": "This paper presents OG-RAG, an Ontology-Grounded Retrieval Augmented Generation method designed to enhance LLM-generated responses by anchoring retrieval processes in domain-specific ontologies. While LLMs are widely used for tasks like question answering and search, they struggle to adapt to specialized knowledge, such as industrial workflows or knowledge work, without expensive fine-tuning or sub-optimal retrieval methods. Existing retrieval-augmented models, such as RAG, offer improvements but fail to account for structured domain knowledge, leading to suboptimal context generation. Ontologies, which conceptually organize domain knowledge by defining entities and their interrelationships, offer a structured representation to address this gap. OG-RAG constructs a hypergraph representation of domain documents, where each hyperedge encapsulates clusters of factual knowledge grounded using domain-specific ontology. An optimization algorithm then retrieves the minimal set of hyperedges that constructs a precise, conceptually grounded context for the LLM. This method enables efficient retrieval while preserving the complex relationships between entities. OG-RAG applies to domains where fact-based reasoning is essential, particularly in tasks that require workflows or decision-making steps to follow predefined rules and procedures. These include industrial workflows in healthcare, legal, and agricultural sectors, as well as knowledge-driven tasks such as news journalism, investigative research, consulting and more. Our evaluations demonstrate that OG-RAG increases the recall of accurate facts by 55% and improves response correctness by 40% across four different LLMs. Additionally, OG-RAG enables 30% faster attribution of responses to context and boosts fact-based reasoning accuracy by 27% compared to baseline methods.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/dbf5054b6aa6ef75887174d0ea1f075974743765.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 6.0
    },
    "5d2a03d679e0cc540d839a6e82d9000a9cab90c9.pdf": {
        "title": "War of Words: Harnessing the Potential of Large Language Models and Retrieval Augmented Generation to Classify, Counter and Diffuse Hate Speech",
        "authors": [
            "R. Leekha",
            "Olga Simek",
            "Charlie Dagli"
        ],
        "published_date": "2024",
        "abstract": "This paper explores the emergence of divergent narratives in the wake of the Russian-Ukraine war, which began on February 24, 2022, and the innovative application of AI language models, specifically RetrievalAugmented Generation (RAG) and instruction-based large language models (LLMs), in countering hateful speech on social media. We design a pipeline to automatically discover and then respond to hateful content trending on social media platforms. Monitoring via traditional topic/narrative modeling often focuses on lowlevel content, which is difficult to interpret. In addition, workflows for prioritization and response generation are often highly manual. We utilize several large language models (LLMs) throughout our pipeline to detect and summarize topics, to determine whether tweets contain hate speech and to generate counter narratives. We test our approach on Ukraine Bio-Lab Tweet Corpus of 500k Tweets and evaluate the counter-narrative generation performance across several dimensions: relevance, grammaticality, factuality, and diversity. Our approach outperforms existing state of the art algorithms for hate speech detection and promising counter-narrative generation performance scores across our metrics reflect effectiveness of our pipeline in addressing hateful social media posts",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/5d2a03d679e0cc540d839a6e82d9000a9cab90c9.pdf",
        "venue": "The Florida AI Research Society",
        "citationCount": 6,
        "score": 6.0
    },
    "9b52afc58ea4326642970e75b8b10d6a97090900.pdf": {
        "title": "Evaluation of the integration of retrieval-augmented generation in large language model for breast cancer nursing care responses",
        "authors": [
            "Ruiyu Xu",
            "Ying Hong",
            "Feifei Zhang",
            "Hongmei Xu"
        ],
        "published_date": "2024",
        "abstract": "Breast cancer is one of the most common malignant tumors in women worldwide. Although large language models (LLMs) can provide breast cancer nursing care consultation, inherent hallucinations can lead to inaccurate responses. Retrieval-augmented generation (RAG) technology can improve LLM performance, offering a new approach for clinical applications. In the present study, we evaluated the performance of a LLM in breast cancer nursing care using RAG technology. In the control group (GPT-4), questions were answered directly using the GPT-4 model, whereas the experimental group (RAG-GPT) used the GPT-4 model combined with RAG. A knowledge base for breast cancer nursing was created for the RAG-GPT group, and 15 of 200 real-world clinical care questions were answered randomly. The primary endpoint was overall satisfaction, and the secondary endpoints were accuracy and empathy. RAG-GPT included a curated knowledge base related to breast cancer nursing care, including textbooks, guidelines, and traditional Chinese therapy. The RAG-GPT group showed significantly higher overall satisfaction than that of the GPT-4 group (8.4\u2009\u00b1\u20090.84 vs. 5.4\u2009\u00b1\u20091.27, p\u2009<\u20090.01) as well as an improved accuracy of responses (8.6\u2009\u00b1\u20090.69 vs. 5.6\u2009\u00b1\u20090.96, p\u2009<\u20090.01). However, there was no inter-group difference in empathy (8.4\u2009\u00b1\u20090.85 vs. 7.8\u2009\u00b1\u20091.22, p\u2009>\u20090.05). Overall, this study revealed that RAG technology could improve LLM performance significantly, likely because of the increased accuracy of the answers without diminishing empathy. These findings provide a theoretical basis for applying RAG technology to LLMs in clinical nursing practice and education. Supplementary Information The online version contains supplementary material available at 10.1038/s41598-024-81052-3.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/9b52afc58ea4326642970e75b8b10d6a97090900.pdf",
        "venue": "Scientific Reports",
        "citationCount": 6,
        "score": 6.0
    },
    "d21d706637f1c7f0c87fad3955a11e0166f653a2.pdf": {
        "title": "Answering real-world clinical questions using large language model, retrieval-augmented generation, and agentic systems",
        "authors": [
            "Y. Low",
            "Michael L. Jackson",
            "Rebecca J. Hyde",
            "Robert E. Brown",
            "Neil M. Sanghavi",
            "Julian D Baldwin",
            "C. W. Pike",
            "Jananee Muralidharan",
            "Gavin Hui",
            "Natasha Alexander",
            "Hadeel Hassan",
            "Rahul Nene",
            "Morgan Pike",
            "Courtney J. Pokrzywa",
            "Shivam Vedak",
            "A. Yan",
            "Dong-han Yao",
            "A. Zipursky",
            "Christina Dinh",
            "Philip Ballentine",
            "D. Derieg",
            "Vladimir Polony",
            "Rehan N. Chawdry",
            "Jordan Davies",
            "Brigham B Hyde",
            "Nigam H. Shah",
            "S. Gombar"
        ],
        "published_date": "2025",
        "abstract": "Objective The practice of evidence-based medicine can be challenging when relevant data are lacking or difficult to contextualize for a specific patient. Large language models (LLMs) could potentially address both challenges by summarizing published literature or generating new studies using real-world data. Materials and Methods We submitted 50 clinical questions to five LLM-based systems: OpenEvidence, which uses an LLM for retrieval-augmented generation (RAG); ChatRWD, which uses an LLM as an interface to a data extraction and analysis pipeline; and three general-purpose LLMs (ChatGPT-4, Claude 3 Opus, Gemini 1.5 Pro). Nine independent physicians evaluated the answers for relevance, quality of supporting evidence, and actionability (i.e., sufficient to justify or change clinical practice). Results General-purpose LLMs rarely produced relevant, evidence-based answers (2\u201310% of questions). In contrast, RAG-based and agentic LLM systems, respectively, produced relevant, evidence-based answers for 24% (OpenEvidence) to 58% (ChatRWD) of questions. OpenEvidence produced actionable results for 48% of questions with existing evidence, compared to 37% for ChatRWD and <5% for the general-purpose LLMs. ChatRWD provided actionable results for 52% of questions that lacked existing literature compared to <10% for other LLMs. Discussion Special-purpose LLM systems greatly outperformed general-purpose LLMs in producing answers to clinical questions. Retrieval-augmented generation-based LLM (OpenEvidence) performed well when existing data were available, while only the agentic ChatRWD was able to provide actionable answers when preexisting studies were lacking. Conclusion Synergistic systems combining RAG-based evidence summarization and agentic generation of novel evidence could improve the availability of pertinent evidence for patient care.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/d21d706637f1c7f0c87fad3955a11e0166f653a2.pdf",
        "venue": "Digital Health",
        "citationCount": 6,
        "score": 6.0
    },
    "d511435015370a5ec91689cbd6d1ddb1dc9da0b4.pdf": {
        "title": "KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large Language Models",
        "authors": [
            "Weijie Chen",
            "Ting Bai",
            "Jinbo Su",
            "Jian Luan",
            "Wei Liu",
            "Chuan Shi"
        ],
        "published_date": "2024",
        "abstract": "Large language models with retrieval-augmented generation encounter a pivotal challenge in intricate retrieval tasks, e.g., multi-hop question answering, which requires the model to navigate across multiple documents and generate comprehensive responses based on fragmented information. To tackle this challenge, we introduce a novel Knowledge Graph-based RAG framework with a hierarchical knowledge retriever, termed KG-Retriever. The retrieval indexing in KG-Retriever is constructed on a hierarchical index graph that consists of a knowledge graph layer and a collaborative document layer. The associative nature of graph structures is fully utilized to strengthen intra-document and inter-document connectivity, thereby fundamentally alleviating the information fragmentation problem and meanwhile improving the retrieval efficiency in cross-document retrieval of LLMs. With the coarse-grained collaborative information from neighboring documents and concise information from the knowledge graph, KG-Retriever achieves marked improvements on five public QA datasets, showing the effectiveness and efficiency of our proposed RAG framework.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/d511435015370a5ec91689cbd6d1ddb1dc9da0b4.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 6.0
    },
    "503c5cb6bfb451e38c12e5c1ba41ccf844e79fa8.pdf": {
        "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
        "authors": [
            "Yutao Zhu",
            "Zhaoheng Huang",
            "Zhicheng Dou",
            "Ji-Rong Wen"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capabilities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across 12 question-answering tasks demonstrate the superiority of our approach.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/503c5cb6bfb451e38c12e5c1ba41ccf844e79fa8.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 6,
        "score": 6.0
    },
    "641a39330b533dde61e0c66487c53a811ae43755.pdf": {
        "title": "Contextual Compression in Retrieval-Augmented Generation for Large Language Models: A Survey",
        "authors": [
            "Sourav Verma"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) showcase remarkable abilities, yet they struggle with limitations such as hallucinations, outdated knowledge, opacity, and inexplicable reasoning. To address these challenges, Retrieval-Augmented Generation (RAG) has proven to be a viable solution, leveraging external databases to improve the consistency and coherence of generated content, especially valuable for complex, knowledge-rich tasks, and facilitates continuous improvement by leveraging domain-specific insights. By combining the intrinsic knowledge of LLMs with the vast, dynamic repositories of external databases, RAG achieves a synergistic effect. However, RAG is not without its limitations, including a limited context window, irrelevant information, and the high processing overhead for extensive contextual data. In this comprehensive work, we explore the evolution of Contextual Compression paradigms, providing an in-depth examination of the field. Finally, we outline the current challenges and suggest potential research and development directions, paving the way for future advancements in this area.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/641a39330b533dde61e0c66487c53a811ae43755.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 5.0
    },
    "6fff65981e79981e47ab219fd12ccc824d47d6ce.pdf": {
        "title": "Adaptive Control of Retrieval-Augmented Generation for Large Language Models Through Reflective Tags",
        "authors": [
            "Chengyuan Yao",
            "Satoshi Fujita"
        ],
        "published_date": "2024",
        "abstract": "While retrieval-augmented generation (RAG) enhances large language models (LLMs), it also introduces challenges that can impact accuracy and performance. In practice, RAG can obscure the intrinsic strengths of LLMs. Firstly, LLMs may become too reliant on external retrieval, underutilizing their own knowledge and reasoning, which can diminish responsiveness. Secondly, RAG may introduce irrelevant or low-quality data, adding noise that disrupts generation, especially with complex tasks. This paper proposes an RAG framework that uses reflective tags to manage retrieval, evaluating documents in parallel and applying the chain-of-thought (CoT) technique for step-by-step generation. The model selects the highest quality content for final output. The key contributions are as follows: (1) reducing hallucinations by focusing on high-scoring documents; (2) improving real-time performance through efficient retrieval; and (3) mitigating negative effects by filtering out irrelevant information using parallel generation and reflective tagging. These innovations aim to optimize RAG for more reliable, high-quality results.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/6fff65981e79981e47ab219fd12ccc824d47d6ce.pdf",
        "venue": "Electronics",
        "citationCount": 5,
        "score": 5.0
    },
    "3729e52e94382ae42894a355b2f97a2c6cc38b5c.pdf": {
        "title": "Enhancing Environmental Control in Broiler Production: Retrieval-Augmented Generation for Improved Decision-Making with Large Language Models",
        "authors": [
            "Marcus Vinicius Leite",
            "J. Abe",
            "Marcos Leandro Hoffmann Souza",
            "I. de Alencar N\u00e4\u00e4s"
        ],
        "published_date": "2025",
        "abstract": "The growing global demand for animal protein, particularly chicken meat, challenges poultry farming to adapt production systems through the adoption of digital technologies. Among the promising advances in artificial intelligence (AI), large language models (LLMs) hold potential to enhance decision-making in broiler production by supporting environmental control through the interpretation of climatic data, the generation of reports to optimize conditions, guidance on ventilation adjustments, recommendations for thermal management, assistance in air quality monitoring, and the translation of simulation results into actionable suggestions to improve bird welfare. For this purpose, the key limitations of LLMs in terms of transparency, accuracy, precision, and relevance must be effectively addressed. This study investigates the impact of retrieval-augmented generation (RAG) on improving LLM precision and relevance for environmental control in broiler production. Experiments with the OpenAI GPT-4o model and semantic similarity analysis were used to evaluate response quality with and without RAG. The results confirmed the approach\u2019s effectiveness while identifying areas for improvement. A paired t-test revealed significantly higher similarity scores with RAG, demonstrating its impact on response quality. This study contributes to the field by advancing RAG-enhanced LLMs for environmental control, addressing market demands by demonstrating how AI improves decision-making for productivity and animal welfare, and benefits society by providing small-scale producers with cost-effective and accessible solutions for actionable insights.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/3729e52e94382ae42894a355b2f97a2c6cc38b5c.pdf",
        "venue": "AgriEngineering",
        "citationCount": 5,
        "score": 5.0
    },
    "0da66fdf7e5095fc4c74b376fb404b37dad97380.pdf": {
        "title": "Developing a Retrieval Augmented Generation (RAG) Chatbot App Using Adaptive Large Language Models (LLM) and LangChain Framework",
        "authors": [
            "Cara Burgan",
            "Josiah Kowalski",
            "Weidong Liao"
        ],
        "published_date": "2024",
        "abstract": "CARA BURGAN, Dept of Computer Sciences, Mathematics, and Engineering, Shepherd University, Shepherdstown, WV, 25443, JOSIAH KOWALSKI, Dept of Computer Sciences, Mathematics, and Engineering, Shepherd University, Shepherdstown, WV, 25443, and WEIDONG LIAO (Faculty Advisor), Dept of Computer Sciences, Mathematics, and Engineering, Shepherd University, Shepherdstown, WV, 25443.\u00a0 Developing a Retrieval Augmented Generation (RAG) Chatbot App Using Adaptive Large Language Models (LLM) and LangChain Framework\u00a0 \n\u00a0RamChat is an AI chatbot designed to assist Shepherd University students in navigating the student handbook. Developed in Python, it utilizes both API-based and local Large Language Models (LLMs) for natural language processing (NLP), alongside a vector store system. Our aim is to create a high-quality chatbot app tailored for student use. \nWe began by researching existing chatbot platforms and created a vector store with embeddings from OpenAI's text-embedding-3-small model, trained on the Shepherd University handbook. Testing each LLM helped assess answer types and accuracy. \nDevelopment involved debugging and optimizing RamChat's code, including replacing OpenAI's davinci-002 model with gemma, a local LLM based on Google's Gemini model. Ollama framework aids in automatic LLM selection based on user prompts. \nOur conference presentation will detail RamChat's development, methodology, challenges, and insights. RamChat represents an innovative application of AI to enhance the Shepherd University student experience.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/0da66fdf7e5095fc4c74b376fb404b37dad97380.pdf",
        "venue": "Proceedings of the West Virginia Academy of Science",
        "citationCount": 5,
        "score": 5.0
    },
    "2d4689e8cddff9a99673f4e63512cf9a06534e88.pdf": {
        "title": "Reducing Hallucinations of Medical Multimodal Large Language Models with Visual Retrieval-Augmented Generation",
        "authors": [
            "Yun-Wei Chu",
            "Kai Zhang",
            "Christopher Malon",
            "Martin Renqiang Min"
        ],
        "published_date": "2025",
        "abstract": "Multimodal Large Language Models (MLLMs) have shown impressive performance in vision and text tasks. However, hallucination remains a major challenge, especially in fields like healthcare where details are critical. In this work, we show how MLLMs may be enhanced to support Visual RAG (V-RAG), a retrieval-augmented generation framework that incorporates both text and visual data from retrieved images. On the MIMIC-CXR chest X-ray report generation and Multicare medical image caption generation datasets, we show that Visual RAG improves the accuracy of entity probing, which asks whether a medical entities is grounded by an image. We show that the improvements extend both to frequent and rare entities, the latter of which may have less positive training data. Downstream, we apply V-RAG with entity probing to correct hallucinations and generate more clinically accurate X-ray reports, obtaining a higher RadGraph-F1 score.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/2d4689e8cddff9a99673f4e63512cf9a06534e88.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 5.0
    },
    "ce34488023b7111c99751808e268e56eed03c2c1.pdf": {
        "title": "Relation Extraction with Fine-Tuned Large Language Models in Retrieval Augmented Generation Frameworks",
        "authors": [
            "Sefika Efeoglu",
            "Adrian Paschke"
        ],
        "published_date": "2024",
        "abstract": "Information Extraction (IE) is crucial for converting unstructured data into structured formats like Knowledge Graphs (KGs). A key task within IE is Relation Extraction (RE), which identifies relationships between entities in text. Various RE methods exist, including supervised, unsupervised, weakly supervised, and rule-based approaches. Recent studies leveraging pre-trained language models (PLMs) have shown significant success in this area. In the current era dominated by Large Language Models (LLMs), fine-tuning these models can overcome limitations associated with zero-shot LLM prompting-based RE methods, especially regarding domain adaptation challenges and identifying implicit relations between entities in sentences. These implicit relations, which cannot be easily extracted from a sentence's dependency tree, require logical inference for accurate identification. This work explores the performance of fine-tuned LLMs and their integration into the Retrieval Augmented-based (RAG) RE approach to address the challenges of identifying implicit relations at the sentence level, particularly when LLMs act as generators within the RAG framework. Empirical evaluations on the TACRED, TACRED-Revisited (TACREV), Re-TACRED, and SemEVAL datasets show significant performance improvements with fine-tuned LLMs, including Llama2-7B, Mistral-7B, and T5 (Large). Notably, our approach achieves substantial gains on SemEVAL, where implicit relations are common, surpassing previous results on this dataset. Additionally, our method outperforms previous works on TACRED, TACREV, and Re-TACRED, demonstrating exceptional performance across diverse evaluation scenarios.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/ce34488023b7111c99751808e268e56eed03c2c1.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 5.0
    },
    "5b42c71b9de4322f152ae5987c9bb3ff093beceb.pdf": {
        "title": "Retrieval Augmented Generation Integrated Large Language Models in Smart Contract Vulnerability Detection",
        "authors": [
            "Jeffy Yu"
        ],
        "published_date": "2024",
        "abstract": "The rapid growth of Decentralized Finance (DeFi) has been accompanied by substantial financial losses due to smart contract vulnerabilities, underscoring the critical need for effective security auditing. With attacks becoming more frequent, the necessity and demand for auditing services has escalated. This especially creates a financial burden for independent developers and small businesses, who often have limited available funding for these services. Our study builds upon existing frameworks by integrating Retrieval-Augmented Generation (RAG) with large language models (LLMs), specifically employing GPT-4-1106 for its 128k token context window. We construct a vector store of 830 known vulnerable contracts, leveraging Pinecone for vector storage, OpenAI's text-embedding-ada-002 for embeddings, and LangChain to construct the RAG-LLM pipeline. Prompts were designed to provide a binary answer for vulnerability detection. We first test 52 smart contracts 40 times each against a provided vulnerability type, verifying the replicability and consistency of the RAG-LLM. Encouraging results were observed, with a 62.7% success rate in guided detection of vulnerabilities. Second, we challenge the model under a\"blind\"audit setup, without the vulnerability type provided in the prompt, wherein 219 contracts undergo 40 tests each. This setup evaluates the general vulnerability detection capabilities without hinted context assistance. Under these conditions, a 60.71% success rate was observed. While the results are promising, we still emphasize the need for human auditing at this time. We provide this study as a proof of concept for a cost-effective smart contract auditing process, moving towards democratic access to security.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/5b42c71b9de4322f152ae5987c9bb3ff093beceb.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 5.0
    },
    "7b466b52cb3810e4eda47a2d345ca32b839ce4df.pdf": {
        "title": "Optimizing Microservice Deployment in Edge Computing with Large Language Models: Integrating Retrieval Augmented Generation and Chain of Thought Techniques",
        "authors": [
            "Kan Feng",
            "Lijun Luo",
            "Yongjun Xia",
            "Bin Luo",
            "Xingfeng He",
            "Kaihong Li",
            "Zhiyong Zha",
            "Bo Xu",
            "Kai Peng"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in autogenerating code based on natural language instructions provided by humans. We observed that in the microservice models of edge computing, the problem of deployment latency optimization can be transformed into an NP-hard mathematical optimization problem. However, in the real world, deployment strategies at the edge often require immediate updates, while human-engineered code tends to be lagging. To bridge this gap, we innovatively integrated LLMs into the decision-making process for microservice deployment. Initially, we constructed a private Retrieval Augmented Generation (RAG) database containing prior knowledge. Subsequently, we employed meticulously designed step-by-step inductive instructions and used the chain of thought (CoT) technique to enable the LLM to learn, reason, reflect, and regenerate. We decomposed the microservice deployment latency optimization problem into a collection of granular sub-problems (described in natural language), progressively providing instructions to the fine-tuned LLM to generate corresponding code blocks. The generated code blocks underwent integration and consistency assessment. Additionally, we prompted the LLM to generate code without the use of the RAG database for comparative analysis. We executed the aforementioned code and comparison algorithm under identical operational environments and simulation parameters, conducting rigorous result analysis. Our fine-tuned model significantly reduced latencies by 22.8% in handling surges in request flows, 37.8% in managing complex microservice types, and 39.5% in processing increased network nodes compared to traditional algorithms. Moreover, our approach demonstrated marked improvements in latency performance over LLMs not utilizing RAG technology and reinforcement learning algorithms reported in other literature. The use of LLMs also highlights the concept of symmetry, as the symmetrical structure of input-output relationships in microservice deployment models aligns with the LLM\u2019s inherent ability to process and generate balanced and optimized code. Symmetry in this context allows for more efficient resource allocation and reduces redundant operations, further enhancing the model\u2019s effectiveness. We believe that LLMs hold substantial potential in optimizing microservice deployment models.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/7b466b52cb3810e4eda47a2d345ca32b839ce4df.pdf",
        "venue": "Symmetry",
        "citationCount": 5,
        "score": 5.0
    },
    "beb3389ded23688da387f5ed027a52da06b54e17.pdf": {
        "title": "A Retrieval-Augmented Generation Based Large Language Model Benchmarked On a Novel Dataset",
        "authors": [
            "Kieran Pichai"
        ],
        "published_date": "2023",
        "abstract": "The evolution of natural language processing has seen marked advancements, particularly with the advent of models like BERT, Transformers, and GPT variants, with recent additions like GPT and Bard. This paper investigates the Retrieval-Augmented Generation (RAG) framework, providing insights into its modular design and the impact of its constituent modules on performance. Leveraging a unique dataset from Amazon Rainforest natives and biologists, our research demonstrates the significance of preserving indigenous cultures and biodiversity. The experiment employs a customizable RAG methodology, allowing for the interchangeability of various components, such as the base language model and similarity score tools. Findings indicate that while GPT performs slightly better when given context, Palm exhibits superior performance without context. The results also suggest that models tend to perform optimally when paired with similarity scores from their native platforms. Conclusively, our approach showcases the potential of a modular RAG design in optimizing language models, presenting it as a more advantageous strategy compared to traditional fine-tuning of large language models.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/beb3389ded23688da387f5ed027a52da06b54e17.pdf",
        "venue": "Journal of student-scientists' research",
        "citationCount": 8,
        "score": 4.0
    },
    "2795358f23f1485f71693245576d1fd57f3134b2.pdf": {
        "title": "Advancing TTP Analysis: Harnessing the Power of Large Language Models with Retrieval Augmented Generation",
        "authors": [
            "Reza Fayyazi",
            "Rozhina Taghdimi",
            "S. Yang"
        ],
        "published_date": "2023",
        "abstract": "Tactics, Techniques, and Procedures (TTPs) outline the methods attackers use to exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT&CK framework can be challenging for cybersecurity practitioners due to presumed expertise and complex dependencies. Meanwhile, advancements with Large Language Models (LLMs) have led to recent surge in studies exploring its uses in cybersecurity operations. It is, however, unclear how LLMs can be used in an efficient and proper way to provide accurate responses for critical domains such as cybersecurity. This leads us to investigate how to better use two types of LLMs: small-scale encoder-only (e.g., RoBERTa) and large-scale decoder-only (e.g., GPT-3.5) LLMs to comprehend and summarize TTPs with the intended purposes (i.e., tactics) of a cyberattack procedure. This work studies and compares the uses of Supervised Fine-Tuning (SFT) of encoder-only LLMs vs. Retrieval Augmented Generation (RAG) for decoder-only LLMs (without fine-tuning). Both SFT and RAG techniques presumably enhance the LLMs with relevant contexts for each cyberattack procedure. Our studies show decoder-only LLMs with RAG achieves better performance than encoder-only models with SFT, particularly when directly relevant context is extracted by RAG. The decoder-only results could suffer low \u2018Precision\u2019 while achieving high \u2018Recall\u2019, indicating the hallucinations typically occur during decoding phase. Our findings further highlight a counter-intuitive observation that more generic prompts tend to yield better predictions of cyberattack tactics than those that are more specifically tailored. 1",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/2795358f23f1485f71693245576d1fd57f3134b2.pdf",
        "venue": "2024 Annual Computer Security Applications Conference Workshops (ACSAC Workshops)",
        "citationCount": 6,
        "score": 3.0
    },
    "ed16f6feda941164e6370638ebd98b81c13d2c4b.pdf": {
        "title": "RAG-Ex: A Generic Framework for Explaining Retrieval Augmented Generation",
        "authors": [
            "Viju Sudhi",
            "Sinchana Ramakanth Bhat",
            "Max Rudat",
            "Roman Teucher"
        ],
        "published_date": "2024",
        "abstract": "Owing to their size and complexity, large language models (LLMs) hardly explain why they generate a response. This effectively reduces the trust and confidence of end users in LLM-based applications, including Retrieval Augmented Generation (RAG) for Question Answering (QA) tasks. In this work, we introduce RAG-Ex, a model- and language-agnostic explanation framework that presents approximate explanations to the users revealing why the LLMs possibly generated a piece of text as a response, given the user input. Our framework is compatible with both open-source and proprietary LLMs. We report the significance scores of the approximated explanations from our generic explainer in both English and German QA tasks and also study their correlation with the downstream performance of LLMs. In the extensive user studies, our explainer yields an F1-score of 76.9% against the end user annotations and attains almost on-par performance with model-intrinsic approaches.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/ed16f6feda941164e6370638ebd98b81c13d2c4b.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 18,
        "score": 18.0
    },
    "3ed128b6f0e08294fd9cb413eac96b4319481c67.pdf": {
        "title": "Knowledge Graph Retrieval-Augmented Generation for LLM-based Recommendation",
        "authors": [
            "Shijie Wang",
            "Wenqi Fan",
            "Yue Feng",
            "Xinyu Ma",
            "Shuaiqiang Wang",
            "Dawei Yin"
        ],
        "published_date": "2025",
        "abstract": "Recommender systems have become increasingly vital in our daily lives, helping to alleviate the problem of information overload across various user-oriented online services. The emergence of Large Language Models (LLMs) has yielded remarkable achievements, demonstrating their potential for the development of next-generation recommender systems. Despite these advancements, LLM-based recommender systems face inherent limitations stemming from their LLM backbones, particularly issues of hallucinations and the lack of up-to-date and domain-specific knowledge. Recently, Retrieval-Augmented Generation (RAG) has garnered significant attention for addressing these limitations by leveraging external knowledge sources to enhance the understanding and generation of LLMs. However, vanilla RAG methods often introduce noise and neglect structural relationships in knowledge, limiting their effectiveness in LLM-based recommendations. To address these limitations, we propose to retrieve high-quality and up-to-date structure information from the knowledge graph (KG) to augment recommendations. Specifically, our approach develops a retrieval-augmented framework, termed K-RagRec, that facilitates the recommendation generation process by incorporating structure information from the external KG. Extensive experiments have been conducted to demonstrate the effectiveness of our proposed method.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/3ed128b6f0e08294fd9cb413eac96b4319481c67.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 17,
        "score": 17.0
    },
    "f36a63f5d8dd9f97c16c8c21dc6d80aa1631c07d.pdf": {
        "title": "Visual-RAG: Benchmarking Text-to-Image Retrieval Augmented Generation for Visual Knowledge Intensive Queries",
        "authors": [
            "Yin Wu",
            "Quanyu Long",
            "Jing Li",
            "Jianfei Yu",
            "Wenya Wang"
        ],
        "published_date": "2025",
        "abstract": "Retrieval-augmented generation (RAG) is a paradigm that augments large language models (LLMs) with external knowledge to tackle knowledge-intensive question answering. While several benchmarks evaluate Multimodal LLMs (MLLMs) under Multimodal RAG settings, they predominantly retrieve from textual corpora and do not explicitly assess how models exploit visual evidence during generation. Consequently, there still lacks benchmark that isolates and measures the contribution of retrieved images in RAG. We introduce Visual-RAG, a question-answering benchmark that targets visually grounded, knowledge-intensive questions. Unlike prior work, Visual-RAG requires text-to-image retrieval and the integration of retrieved clue images to extract visual evidence for answer generation. With Visual-RAG, we evaluate 5 open-source and 3 proprietary MLLMs, showcasing that images provide strong evidence in augmented generation. However, even state-of-the-art models struggle to efficiently extract and utilize visual knowledge. Our results highlight the need for improved visual retrieval, grounding, and attribution in multimodal RAG systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/f36a63f5d8dd9f97c16c8c21dc6d80aa1631c07d.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 8.0
    },
    "272d0cfef44320feb482c8013c51efcb9c6f9448.pdf": {
        "title": "CaseGPT: a case reasoning framework based on language models and retrieval-augmented generation",
        "authors": [
            "Rui Yang"
        ],
        "published_date": "2024",
        "abstract": "This paper presents CaseGPT, an innovative approach that combines Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) technology to enhance case-based reasoning in the healthcare and legal sectors. The system addresses the challenges of traditional database queries by enabling fuzzy searches based on imprecise descriptions, thereby improving data searchability and usability. CaseGPT not only retrieves relevant case data but also generates insightful suggestions and recommendations based on patterns discerned from existing case data. This functionality proves especially valuable for tasks such as medical diagnostics, legal precedent research, and case strategy formulation. The paper includes an in-depth discussion of the system's methodology, its performance in both medical and legal domains, and its potential for future applications. Our experiments demonstrate that CaseGPT significantly outperforms traditional keyword-based and simple LLM-based systems in terms of precision, recall, and efficiency.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/272d0cfef44320feb482c8013c51efcb9c6f9448.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 5.0
    },
    "e30b976d4c7d9d023dcef102a360d7305bc6a32b.pdf": {
        "title": "Tool Calling: Enhancing Medication Consultation via Retrieval-Augmented Large Language Models",
        "authors": [
            "Zhongzhen Huang",
            "Kui Xue",
            "Yongqi Fan",
            "Linjie Mu",
            "Ruoyu Liu",
            "Tong Ruan",
            "Shaoting Zhang",
            "Xiaofan Zhang"
        ],
        "published_date": "2024",
        "abstract": "Large-scale language models (LLMs) have achieved remarkable success across various language tasks but suffer from hallucinations and temporal misalignment. To mitigate these shortcomings, Retrieval-augmented generation (RAG) has been utilized to provide external knowledge to facilitate the answer generation. However, applying such models to the medical domain faces several challenges due to the lack of domain-specific knowledge and the intricacy of real-world scenarios. In this study, we explore LLMs with RAG framework for knowledge-intensive tasks in the medical field. To evaluate the capabilities of LLMs, we introduce MedicineQA, a multi-round dialogue benchmark that simulates the real-world medication consultation scenario and requires LLMs to answer with retrieved evidence from the medicine database. MedicineQA contains 300 multi-round question-answering pairs, each embedded within a detailed dialogue history, highlighting the challenge posed by this knowledge-intensive task to current LLMs. We further propose a new \\textit{Distill-Retrieve-Read} framework instead of the previous \\textit{Retrieve-then-Read}. Specifically, the distillation and retrieval process utilizes a tool calling mechanism to formulate search queries that emulate the keyword-based inquiries used by search engines. With experimental results, we show that our framework brings notable performance improvements and surpasses the previous counterparts in the evidence retrieval process in terms of evidence retrieval accuracy. This advancement sheds light on applying RAG to the medical domain.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/e30b976d4c7d9d023dcef102a360d7305bc6a32b.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 5.0
    },
    "a3e55c874fa220fc42eb2ee43acfe24c7a309ffe.pdf": {
        "title": "RAG-WM: An Efficient Black-Box Watermarking Approach for Retrieval-Augmented Generation of Large Language Models",
        "authors": [
            "Peizhuo Lv",
            "Mengjie Sun",
            "Hao Wang",
            "Xiaofeng Wang",
            "Shengzhi Zhang",
            "Yuxuan Chen",
            "Kai Chen",
            "Limin Sun"
        ],
        "published_date": "2025",
        "abstract": "In recent years, tremendous success has been witnessed in Retrieval-Augmented Generation (RAG), widely used to enhance Large Language Models (LLMs) in domain-specific, knowledge-intensive, and privacy-sensitive tasks. However, attackers may steal those valuable RAGs and deploy or commercialize them, making it essential to detect Intellectual Property (IP) infringement. Most existing ownership protection solutions, such as watermarks, are designed for relational databases and texts. They cannot be directly applied to RAGs because relational database watermarks require white-box access to detect IP infringement, which is unrealistic for the knowledge base in RAGs. Meanwhile, post-processing by the adversary's deployed LLMs typically destructs text watermark information. To address those problems, we propose a novel black-box\"knowledge watermark\"approach, named RAG-WM, to detect IP infringement of RAGs. RAG-WM uses a multi-LLM interaction framework, comprising a Watermark Generator, Shadow LLM&RAG, and Watermark Discriminator, to create watermark texts based on watermark entity-relationship tuples and inject them into the target RAG. We evaluate RAG-WM across three domain-specific and two privacy-sensitive tasks on four benchmark LLMs. Experimental results show that RAG-WM effectively detects the stolen RAGs in various deployed LLMs. Furthermore, RAG-WM is robust against paraphrasing, unrelated content removal, knowledge insertion, and knowledge expansion attacks. Lastly, RAG-WM can also evade watermark detection approaches, highlighting its promising application in detecting IP infringement of RAG systems.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/a3e55c874fa220fc42eb2ee43acfe24c7a309ffe.pdf",
        "venue": "arXiv.org",
        "citationCount": 4,
        "score": 4.0
    },
    "403bd2292154cf84bfaebe440ebd642b623839f1.pdf": {
        "title": "PR-Attack: Coordinated Prompt-RAG Attacks on Retrieval-Augmented Generation in Large Language Models via Bilevel Optimization",
        "authors": [
            "Yang Jiao",
            "Xiaodong Wang",
            "Kai Yang"
        ],
        "published_date": "2025",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of applications, e.g., medical question-answering, mathematical sciences, and code generation. However, they also exhibit inherent limitations, such as outdated knowledge and susceptibility to hallucinations. Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to address these issues, but it also introduces new vulnerabilities. Recent efforts have focused on the security of RAG-based LLMs, yet existing attack methods face three critical challenges: (1) their effectiveness declines sharply when only a limited number of poisoned texts can be injected into the knowledge database, (2) they lack sufficient stealth, as the attacks are often detectable by anomaly detection systems, which compromises their effectiveness, and (3) they rely on heuristic approaches to generate poisoned texts, lacking formal optimization frameworks and theoretic guarantees, which limits their effectiveness and applicability. To address these issues, we propose coordinated Prompt-RAG attack (PR-attack), a novel optimization-driven attack that introduces a small number of poisoned texts into the knowledge database while embedding a backdoor trigger within the prompt. When activated, the trigger causes the LLM to generate pre-designed responses to targeted queries, while maintaining normal behavior in other contexts. This ensures both high effectiveness and stealth. We formulate the attack generation process as a bilevel optimization problem leveraging a principled optimization framework to develop optimal poisoned texts and triggers. Extensive experiments across diverse LLMs and datasets demonstrate the effectiveness of PR-Attack, achieving a high attack success rate even with a limited number of poisoned texts and significantly improved stealth compared to existing methods.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/403bd2292154cf84bfaebe440ebd642b623839f1.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 4,
        "score": 4.0
    },
    "3eeb6829db131c59558bff33f05aa26891245680.pdf": {
        "title": "Carbon Footprint Accounting Driven by Large Language Models and Retrieval-augmented Generation",
        "authors": [
            "Haijin Wang",
            "Mianrong Zhang",
            "Zheng Chen",
            "Nan Shang",
            "Shangheng Yao",
            "Fushuan Wen",
            "Junhua Zhao"
        ],
        "published_date": "2024",
        "abstract": "Carbon footprint accounting is crucial for quantifying greenhouse gas emissions and achieving carbon neutrality.The dynamic nature of processes, accounting rules, carbon-related policies, and energy supply structures necessitates real-time updates of CFA. Traditional life cycle assessment methods rely heavily on human expertise, making near-real-time updates challenging. This paper introduces a novel approach integrating large language models (LLMs) with retrieval-augmented generation technology to enhance the real-time, professional, and economical aspects of carbon footprint information retrieval and analysis. By leveraging LLMs' logical and language understanding abilities and RAG's efficient retrieval capabilities, the proposed method LLMs-RAG-CFA can retrieve more relevant professional information to assist LLMs, enhancing the model's generative abilities. This method offers broad professional coverage, efficient real-time carbon footprint information acquisition and accounting, and cost-effective automation without frequent LLMs' parameter updates. Experimental results across five industries(primary aluminum, lithium battery, photovoltaic, new energy vehicles, and transformers)demonstrate that the LLMs-RAG-CFA method outperforms traditional methods and other LLMs, achieving higher information retrieval rates and significantly lower information deviations and carbon footprint accounting deviations. The economically viable design utilizes RAG technology to balance real-time updates with cost-effectiveness, providing an efficient, reliable, and cost-saving solution for real-time carbon emission management, thereby enhancing environmental sustainability practices.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/3eeb6829db131c59558bff33f05aa26891245680.pdf",
        "venue": "arXiv.org",
        "citationCount": 4,
        "score": 4.0
    },
    "c0032972c9775967dc3c123521c147f6ec05c885.pdf": {
        "title": "A Comparative Analysis of Large Language Models with Retrieval-Augmented Generation based Question Answering System",
        "authors": [
            "Hetul Niteshbhai Patel",
            "Azara Surti",
            "Parth Goel",
            "Bankim Patel"
        ],
        "published_date": "2024",
        "abstract": "In recent studies, Large Language Models (LLMs) have shown remarkable effectiveness in a wide range of natural language processing tasks. However, their knowledge is limited to the data they were trained on, which may not cover context-aware information. This limitation reduces their capability to provide precise and specific information apart from other parameters, especially in specialized fields. Retrieval Augmented Generation (RAG) Systems utilizes to overcome this limitation by enhancing the capabilities of LLMs through the retrieval of relevant information from external sources during the generation process. This research work presents the comparative analysis of the performance of three popular LLMs\u2019 \u2013 GPT-3.5-turbo from OpenAI, Gemini-Pro from Google, LLama3 by Meta when integrated into RAG system for question answering application. The study contrasts the efficiency of these LLM in generating relevant response with the aid of retrieved information in Q&A task. Along with these three LLMs other embedding models such as Text-embedding-ada-002-v2, embedding-001 and nomic-embed-text embedding model are used. Seven evaluation matrices are used from the RAGAS evaluation framework on self-created dataset to assess the performance of the RAG QA systems. The result showed that the gpt-3.5-turbo Large Language model achieved highest performance outperforming Gemini Pro by 5.66% and Llama3 by 8.40%.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/c0032972c9775967dc3c123521c147f6ec05c885.pdf",
        "venue": "2024 8th International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)",
        "citationCount": 4,
        "score": 4.0
    },
    "44cae1463d64f62f89e089455d25a84a154a7793.pdf": {
        "title": "Information retrieval from textual data: Harnessing large language models, retrieval augmented generation and prompt engineering",
        "authors": [
            "Asen Hikov",
            "Laura Murphy"
        ],
        "published_date": "2024",
        "abstract": "This paper describes how recent advancements in the field of Generative AI (GenAI), and more specifically large language models (LLMs), are incorporated into a practical application that solves the widespread and relevant business problem of information retrieval from textual data in PDF format: searching through legal texts, financial reports, research articles and so on. Marketing research, for example, often requires reading through hundreds of pages of financial reports to extract key information for research on competitors, partners, markets and prospective clients. It is a manual, error-prone and time-consuming task for marketers, where until recently there was little scope for automation, optimisation and scaling. The application we have developed combines LLMs with a retrieval augmented generation (RAG) architecture and prompt engineering to make this process more efficient. We have developed a chatbot that allows the user to upload multiple PDF documents and obtain a summary of predefined key areas as well as to ask specific questions and get answers from the combined documents\u2019 content. The application\u2019s architecture begins with the creation of an index for each of the PDF files. This index includes embedding the textual content and constructing a vector store. A query engine, employing a small-to-big retrieval method, is then used to accurately respond to a set of predefined questions for each PDF to create the summary. The prompt has been designed in a manner that minimises the risk of hallucination which is common in this type of model. The user interacts with the model via a chatbot feature. It utilises similar small-to-big retrieval techniques over the indices for straightforward queries, and a more complex sub-questions engine for in-depth analysis, providing a comprehensive and interactive tool for document analysis. We have estimated that the implementation of this tool would reduce the time spent on manual research tasks by around 60 per cent, based on the discussions we have had with potential users.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/44cae1463d64f62f89e089455d25a84a154a7793.pdf",
        "venue": "Journal of AI, Robotics &amp; Workplace Automation",
        "citationCount": 4,
        "score": 4.0
    },
    "bff1069b6dd141b3e89c94fcb97e0f75980dbf84.pdf": {
        "title": "RadioRAG: Factual Large Language Models for Enhanced Diagnostics in Radiology Using Dynamic Retrieval Augmented Generation",
        "authors": [
            "Soroosh Tayebi",
            "A. Aripoli",
            "P. Iglar",
            "E. Friedman",
            "C. Ayeni",
            "L. Misbach",
            "M. Quintana",
            "P. Slanetz",
            "L. Shah",
            "T. Kuritza",
            "S. Benjamin",
            "R. Ganesh",
            "C. Walker",
            "S. Gerrie",
            "M. Aquino",
            "Pleuropulmonary Blastoma",
            "J. Daniel",
            "S. A. Al-Katib",
            "N. Raval",
            "R. Jha",
            "P. Bergquist",
            "N. Jain",
            "Multifocal Micronodular",
            "Pneumocyte Hyperplasia",
            "B. Guthridge",
            "B. Fink",
            "B. Tallman",
            "R. Jarman",
            "Epiploic Appendagitis",
            "Q. Li",
            "J. Wang",
            "D. Gao",
            "A. Kumar",
            "D. Gewolb",
            "L. Chiu",
            "J. Yoon",
            "G. Rahmani",
            "T. Schermann",
            "R. Potenza",
            "T. DenOtter",
            "B. Franz",
            "P. Patel",
            "C. Scher",
            "R. Iyer",
            "M. Kumaravel",
            "N. Vu",
            "R. Woods",
            "S. Carter",
            "F. Flaherty",
            "L. Verst",
            "D. Constantino",
            "M. Chalian",
            "S. Goddard",
            "A. Annamalai",
            "C. Chamberlin",
            "B. Triche",
            "Renal Arteriovenous",
            "E. Berger",
            "M. MacDonald",
            "F. Lo",
            "S. Robert",
            "G. Brahm",
            "A. Canan",
            "N. Cabrera",
            "V. Krishnan",
            "S. Jaganathan",
            "K. Schmitz",
            "M. Renno",
            "Y. Park",
            "O. Kalinkin",
            "K. Banks",
            "C. Qian",
            "N. Parikh",
            "J. Oh",
            "J. Amor\u00f3sa",
            "A. Bamashmos",
            "K. Elfatairy",
            "R. Hegde",
            "O. Awan",
            "J. Benjamin",
            "A. Shah",
            "O. Shah",
            "T. Shera",
            "S. Shabir",
            "Z. Timmerman",
            "B. S. M. Carrillo",
            "J. Eichhorn",
            "N. Phelan",
            "J. Gilstrap",
            "Polyostotic Paget",
            "D. Mehta",
            "S. Shinde",
            "I. Buren",
            "A. Fung",
            "K. Nutter",
            "J. Chaudry",
            "D. Kennedy",
            "R. Morris",
            "R. Savjani",
            "Y. Yang",
            "A. Kishan",
            "N. Zakhari",
            "Multifocal Glioblastoma",
            "P. Gonzalez",
            "J. Diaz",
            "G. Schiappacasse",
            "P. Rios",
            "R. McCallum",
            "N. Mallak",
            "A. Camacho",
            "T. Dittmer",
            "W. Rieter",
            "S. Elojeimy",
            "O. Schoeck",
            "H. Tran",
            "Sister Mary Joseph Nodule",
            "F. Siddiqui",
            "A. Chauhan",
            "A. Khurana",
            "M. Yang",
            "R. Shrestha",
            "J. Zhang",
            "K. Addae-Mensah",
            "M. Tafoya",
            "W. Crawford",
            "L. Raspante",
            "T. Foureaux",
            "E. Ayub",
            "C. Silva",
            "J. Madsen",
            "C. Cooley",
            "C. Dumas",
            "D. Bittles",
            "A. Gunn",
            "J. Liu",
            "V. Gorolay",
            "J. Shen",
            "R. Lim",
            "J. Tse",
            "O. Yusufzai",
            "Thoracic Aortic",
            "A. Ritchey",
            "J. Kucera",
            "Takayasu Arteritis",
            "R. Davidyan",
            "S. Singh",
            "S. Srinivas",
            "T. Retson",
            "Cerebral Arteriovenous",
            "D. Albenda",
            "M. Queiroz",
            "R. Zeitoun",
            "R. Merard",
            "Dermatofibrosarcoma Protuberans",
            "Z. LeBaron",
            "J. Rabang",
            "D. Gridley",
            "Gallbladder Carcinoma",
            "R. Vyas",
            "A. Singh",
            "H. Diep",
            "D. Poletto",
            "G. Rauch",
            "A. Griffith",
            "A. Velasco",
            "C. Manjarrez",
            "M. Matos",
            "T. Albataineh",
            "T. Rizvi",
            "A. Vyas",
            "M. Horrow",
            "J. Ganeles",
            "J. Gubernick",
            "A. Rand",
            "D. Akselrod"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/bff1069b6dd141b3e89c94fcb97e0f75980dbf84.pdf",
        "venue": "arXiv.org",
        "citationCount": 4,
        "score": 4.0
    },
    "cc7eaa2b30f9bd3cc1a90a5543fc9848906610dc.pdf": {
        "title": "Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models",
        "authors": [
            "Nguyen Quang Duc",
            "Le Hai Son",
            "Nguyen Duc Nhan",
            "Nguyen Dich Nhat Minh",
            "Le Thanh Huong",
            "D. V. Sang"
        ],
        "published_date": "2024",
        "abstract": "This paper presents our contributions towards advancing the state of Vietnamese language understanding and generation through the development and dissemination of open datasets and pre-trained models for Vietnamese Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs).",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/cc7eaa2b30f9bd3cc1a90a5543fc9848906610dc.pdf",
        "venue": "arXiv.org",
        "citationCount": 4,
        "score": 4.0
    },
    "311b157c7b327c5db156f1fc514ed075847c3c3d.pdf": {
        "title": "Integrating Ontologies and Large Language Models to Implement Retrieval Augmented Generation",
        "authors": [
            "Michael DeBellis",
            "Nivedita Dutta",
            "Jacob Gino",
            "Aadarsh Balaji"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have captured the imagination of the public and the technical community. As powerful as they are they have problems that prohibit their use for highly skilled users. These issues are hallucinations, bias, black-box reasoning, and lack of domain depth. One of the most popular architectures to alleviate these problems is retrieval augmented generation (RAG). In a RAG architecture, the LLM is utilized to generate vectors and to parse and generate natural language. The knowledge base for a RAG architecture is typically a set of documents focused on a particular type of vertical (question answering) or horizontal (domain) set of use cases as opposed to the general knowledge base of an LLM. Typically, the corpus for the RAG knowledge base is stored in a relational database. This project investigates the use of an ontology and knowledge graph to form a domain-specific knowledge base for RAG in order to leverage LLMs for specific domains without the four problems that typically make them inappropriate for mission and life critical domains. The domain is support of dental clinicians in India who face specific problems that can be significantly improved by better, timely, and easily accessible access to the latest knowledge on dental material products. We demonstrate that using an ontology and knowledge graph to implement RAG has several benefits such as rapid agile development and retrieval by reformulation browsing.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/311b157c7b327c5db156f1fc514ed075847c3c3d.pdf",
        "venue": "Appl. Ontology",
        "citationCount": 4,
        "score": 4.0
    },
    "209eda779b29843c4c6c432c2e608ff430435757.pdf": {
        "title": "Explainable Biomedical Hypothesis Generation via Retrieval Augmented Generation enabled Large Language Models",
        "authors": [
            "A. Pelletier",
            "Joseph Ramirez",
            "Irsyad Adam",
            "Simha Sankar",
            "Yu Yan",
            "Ding Wang",
            "Dylan Steinecke",
            "Wei Wang",
            "Peipei Ping"
        ],
        "published_date": "2024",
        "abstract": "The vast amount of biomedical information available today presents a significant challenge for investigators seeking to digest, process, and understand these findings effectively. Large Language Models (LLMs) have emerged as powerful tools to navigate this complex and challenging data landscape. However, LLMs may lead to hallucinatory responses, making Retrieval Augmented Generation (RAG) crucial for achieving accurate information. In this protocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease Distinction), a comprehensive workflow designed to support investigators with knowledge integration and hypothesis generation, identifying validated paths forward. Relevant biomedical information from publications and knowledge bases are reviewed, integrated, and extracted via text-mining association analysis and explainable graph prediction models on disease nodes, forecasting potential links among drugs and diseases. These analyses, along with biomedical texts, are integrated into a framework that facilitates user-directed mechanism elucidation as well as hypothesis exploration through RAG-enabled LLMs. A clinical use-case demonstrates RUGGED's ability to evaluate and recommend therapeutics for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy (DCM), analyzing prescribed drugs for molecular interactions and unexplored uses. The platform minimizes LLM hallucinations, offers actionable insights, and improves the investigation of novel therapeutics.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/209eda779b29843c4c6c432c2e608ff430435757.pdf",
        "venue": "arXiv.org",
        "citationCount": 4,
        "score": 4.0
    },
    "e15a11e33bd115612a7713f4af4329b6a9bb2a1d.pdf": {
        "title": "GeneRAG: Enhancing Large Language Models with Gene-Related Task by Retrieval-Augmented Generation",
        "authors": [
            "Xinyi Lin",
            "Gelei Deng",
            "Yuekang Li",
            "Jingquan Ge",
            "Joshua Wing Kei Ho",
            "Yi Liu"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) like GPT-4 have revolutionized natural language processing and are used in gene analysis, but their gene knowledge is incomplete. Fine-tuning LLMs with external data is costly and resource-intensive. Retrieval-Augmented Generation (RAG) integrates relevant external information dynamically. We introduce GeneRAG, a frame-work that enhances LLMs\u2019 gene-related capabilities using RAG and the Maximal Marginal Relevance (MMR) algorithm. Evaluations with datasets from the National Center for Biotechnology Information (NCBI) show that GeneRAG outperforms GPT-3.5 and GPT-4, with a 39% improvement in answering gene questions, a 43% performance increase in cell type annotation, and a 0.25 decrease in error rates for gene interaction prediction. These results highlight GeneRAG\u2019s potential to bridge a critical gap in LLM capabilities for more effective applications in genetics.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/e15a11e33bd115612a7713f4af4329b6a9bb2a1d.pdf",
        "venue": "bioRxiv",
        "citationCount": 4,
        "score": 4.0
    },
    "102df7aa35ea82358223f43522406f3c98e44147.pdf": {
        "title": "Enhancing Large Language Models with Retrieval-augmented Generation: A Radiology-specific Approach.",
        "authors": [
            "Dane A Weinert",
            "A. Rauschecker"
        ],
        "published_date": "2025",
        "abstract": "\"Just Accepted\" papers have undergone full peer review and have been accepted for publication in Radiology: Artificial Intelligence. This article will undergo copyediting, layout, and proof review before it is published in its final version. Please note that during production of the final copyedited article, errors may be discovered which could affect the content. Retrieval-augmented generation (RAG) is a strategy to improve performance of large language models (LLMs) by providing the LLM with an updated corpus of knowledge that can be used for answer generation in real-time. RAG may improve LLM performance and clinical applicability in radiology by providing citable, up-to-date information without requiring model fine-tuning. In this retrospective study, a radiology-specific RAG was developed using a vector database of 3,689 RadioGraphics articles published from January 1999 to December 2023. Performance of 5 LLMs with and without RAG on a 192-question radiology examination was compared. RAG significantly improved examination scores for GPT-4 (81.2% versus 75.5%, P = .04) and Command R+ (70.3% versus 62.0%, P = .02), but not for Claude Opus, Mixtral, or Gemini 1.5 Pro. RAG-System performed significantly better than pure LLMs on a 24-question subset directly sourced from RadioGraphics (85% versus 76%, P = .03). The RAG-System retrieved 21/24 (87.5%, P < .001) relevant RadioGraphics references cited in the examination's answer explanations and successfully cited them in 18/21 (85.7%, P < .001) outputs. The results suggest that RAG is a promising approach to enhance LLM capabilities for radiology knowledge tasks, providing transparent, domain-specific information retrieval. \u00a9RSNA, 2025.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/102df7aa35ea82358223f43522406f3c98e44147.pdf",
        "venue": "Radiology: Artificial Intelligence",
        "citationCount": 4,
        "score": 4.0
    },
    "68e34d51ee49b562269dd7e6a325ec6ddaa9aa8d.pdf": {
        "title": "Detecting emergencies in patient portal messages using large language models and knowledge graph-based retrieval-augmented generation",
        "authors": [
            "Siru Liu",
            "A. Wright",
            "Allison B. McCoy",
            "Sean S. Huang",
            "Bryan D. Steitz",
            "Adam Wright"
        ],
        "published_date": "2025",
        "abstract": "Abstract Objectives This study aims to develop and evaluate an approach using large language models (LLMs) and a knowledge graph to triage patient messages that need emergency care. The goal is to notify patients when their messages indicate an emergency, guiding them to seek immediate help rather than using the patient portal, to improve patient safety. Materials and Methods We selected 1020 messages sent to Vanderbilt University Medical Center providers between January 1, 2022 and March 7, 2023. We developed four models to triage these messages for emergencies: (1) Prompt-Only: the patient message was input with a prompt directly into the LLM; (2) Na\u00efve Retrieval Augmented Generation (RAG): provided retrieved information as context to the LLM; (3) RAG from Knowledge Graph with Local Search: a knowledge graph was used to retrieve locally relevant information based on semantic similarities; (4) RAG from Knowledge Graph with Global Search: a knowledge graph was used to retrieve globally relevant information through hierarchical community detection. The knowledge base was a triage book covering 225 protocols. Results The RAG from Knowledge Graph model with global search outperformed other models, achieving an accuracy of 0.99, a sensitivity of 0.98, and a specificity of 0.99. It demonstrated significant improvements in triaging emergency messages compared to LLM without RAG and na\u00efve RAG. Discussion The traditional LLM without any retrieval mechanism underperformed compared to models with RAG, which aligns with the expected benefits of augmenting LLMs with domain-specific knowledge sources. Our results suggest that providing external knowledge, especially in a structured manner and in community summaries, can improve LLM performance in triaging patient portal messages. Conclusion LLMs can effectively assist in triaging emergency patient messages after integrating with a knowledge graph about a nurse triage book. Future research should focus on expanding the knowledge graph and deploying the system to evaluate its impact on patient outcomes.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/68e34d51ee49b562269dd7e6a325ec6ddaa9aa8d.pdf",
        "venue": "J. Am. Medical Informatics Assoc.",
        "citationCount": 4,
        "score": 4.0
    },
    "ce3f2260a73e602516c6aa51678bc5384cafadce.pdf": {
        "title": "Judge as A Judge: Improving the Evaluation of Retrieval-Augmented Generation through the Judge-Consistency of Large Language Models",
        "authors": [
            "Shuliang Liu",
            "Xinze Li",
            "Zhenghao Liu",
            "Yukun Yan",
            "Cheng Yang",
            "Zheni Zeng",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Ge Yu"
        ],
        "published_date": "2025",
        "abstract": "Retrieval-Augmented Generation (RAG) has proven its effectiveness in alleviating hallucinations for Large Language Models (LLMs). However, existing automated evaluation metrics cannot fairly evaluate the outputs generated by RAG models during training and evaluation. LLM-based judgment models provide the potential to produce high-quality judgments, but they are highly sensitive to evaluation prompts, leading to inconsistencies when judging the output of RAG models. This paper introduces the Judge-Consistency (ConsJudge) method, which aims to enhance LLMs to generate more accurate evaluations for RAG models. Specifically, ConsJudge prompts LLMs to generate different judgments based on various combinations of judgment dimensions, utilize the judge-consistency to evaluate these judgments and select the accepted and rejected judgments for DPO training. Our experiments show that ConsJudge can effectively provide more accurate judgments for optimizing RAG models across various RAG models and datasets. Further analysis reveals that judgments generated by ConsJudge have a high agreement with the superior LLM. All codes are available at https://github.com/OpenBMB/ConsJudge.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/ce3f2260a73e602516c6aa51678bc5384cafadce.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 4,
        "score": 4.0
    },
    "938908bc82d5c37b1df2c76d4fbdbdf2075e50de.pdf": {
        "title": "Advancing Question-Answering in Ophthalmology with Retrieval Augmented Generations (RAG): Benchmarking Open-source and Proprietary Large Language Models",
        "authors": [
            "Quang Nguyen",
            "Duy-Anh Nguyen",
            "Khang Dang",
            "Siyin Liu",
            "Khai Nguyen",
            "Sophia Y. Wang",
            "W. Woof",
            "Peter Thomas",
            "Praveen J Patel",
            "Konstantinos Balaskas",
            "Johan H Thygesen",
            "Honghan Wu",
            "N. Pontikos"
        ],
        "published_date": "2024",
        "abstract": "Purpose To evaluate the application of Retrieval-Augmented Generation (RAG), a technique that combines information retrieval with text generation, to benchmark the performance of open-source and proprietary generative large language models (LLMs) in medical question-answering tasks within the ophthalmology domain. Methods Our dataset comprised 260 multiple-choice questions sourced from two question-answer banks designed to assess ophthalmic knowledge: the American Academy of Ophthalmology's Basic and Clinical Science Course (BCSC) Self-Assessment program and OphthoQuestions. Our RAG pipeline involved initial retrieval of documents in the BCSC companion textbook using ChromaDB, followed by reranking with Cohere to refine the context provided to the LLMs. We benchmarked four models, including GPT-4 and three open-source models (Llama-3-70B, Gemma-2-27B, and Mixtral-8x7B, all under 4-bit quantization), under three settings: zero-shot, zero-shot with Chain-of-Thought and RAG. Model performance was evaluated using accuracy on the two datasets. Quantization was applied to improve the efficiency of the open-source models. Effects of quantization level was also measured. Results Using RAG, GPT-4-turbo' s accuracy increased from 80.38% to 91.92% on BCSC and from 77.69% to 88.65 % on OphthoQuestions. Importantly, the RAG pipeline greatly enhanced overall performance of Llama-3 from 57.50% to 81.35% (23.85% increase), Gemma-2 62.12% to 79.23% (17.11% increase), and Mixtral-8x7B 52.89% to 75% (22.11% increase). Zero-shot-CoT had overall no significant improvement on the models' performance. Quantization using 4 bit was shown to be as effective as using 8 bits while requiring half the resources.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/938908bc82d5c37b1df2c76d4fbdbdf2075e50de.pdf",
        "venue": "medRxiv",
        "citationCount": 4,
        "score": 4.0
    },
    "f8d3281e21acd6691b4123b68693b86c6393f199.pdf": {
        "title": "RAG Meets Detox: Enhancing Text Detoxification Using Open Large Language Models with Retrieval Augmented Generation",
        "authors": [
            "Erik Rehulka",
            "Marek Suppa"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/f8d3281e21acd6691b4123b68693b86c6393f199.pdf",
        "venue": "Conference and Labs of the Evaluation Forum",
        "citationCount": 3,
        "score": 3.0
    },
    "a8d6215afbcfd64a84aee0df764c3619f67fa1be.pdf": {
        "title": "Layered Query Retrieval: An Adaptive Framework for Retrieval-Augmented Generation in Complex Question Answering for Large Language Models",
        "authors": [
            "Jie Huang",
            "Mo Wang",
            "Yunpeng Cui",
            "Juan Liu",
            "Li Chen",
            "Ting Wang",
            "Huan Li",
            "Jinming Wu"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) addresses the problem of knowledge cutoff and overcomes the inherent limitations of pre-trained language models by retrieving relevant information in real time. However, challenges related to efficiency and accuracy persist in current RAG strategies. A key issue is how to select appropriate methods for user queries of varying complexity dynamically. This study introduces a novel adaptive retrieval-augmented generation framework termed Layered Query Retrieval (LQR). The LQR framework focuses on query complexity classification, retrieval strategies, and relevance analysis, utilizing a custom-built training dataset to train smaller models that aid the large language model (LLM) in efficiently retrieving relevant information. A central technique in LQR is a semantic rule-based approach to distinguish between different levels of multi-hop queries. The process begins by parsing the user\u2019s query for keywords, followed by a keyword-based document retrieval. Subsequently, we employ a natural language inference (NLI) model to assess whether the retrieved document is relevant to the query. We validated our approach on multiple single-hop and multi-hop datasets, demonstrating significant improvements in both accuracy and efficiency compared to existing single-step, multi-step, and adaptive methods. Our method exhibits high accuracy and efficiency, particularly on the HotpotQA dataset, where it outperforms the Adaptive-RAG method by improving accuracy by 9.4% and the F1 score by 16.14%. The proposed approach carefully balances retrieval efficiency with the accuracy of the LLM\u2019s responses.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/a8d6215afbcfd64a84aee0df764c3619f67fa1be.pdf",
        "venue": "Applied Sciences",
        "citationCount": 3,
        "score": 3.0
    },
    "622947f6f70520ffd8579b5ed9bae681096b1b67.pdf": {
        "title": "SelfRewardRAG: Enhancing Medical Reasoning with Retrieval-Augmented Generation and Self-Evaluation in Large Language Models",
        "authors": [
            "Zakaria Hammane",
            "Fatima-Ezzahraa Ben-Bouazza",
            "A. Fennan"
        ],
        "published_date": "2024",
        "abstract": "In this study, we present a pioneering approach known as Retrieval Augmented Generation (RAG), which integrates Large Language Models (LLMs) with dynamic data retrieval to surmount the challenge of knowledge obsolescence, a matter of particular significance in the healthcare domain. This innovative system leverages real-time access to up-to-date clinical records, thereby enabling the generation of precise and informed responses, a notable leap over the conventional limitations faced by LLMs due to their reliance on static datasets. Our methodology embodies the seamless integration of RAG with LLMs to adeptly retrieve pertinent medical information from continuously updated repositories, such as PubMed, and to synthesize this information into accurate responses for medical queries. This advancement marks a considerable enhancement in the application of AI within medical decision-making processes, ensuring that the information provided remains both current and relevant. The effectiveness of our approach is validated through a series of experiments, which demonstrate a significant improvement in the accuracy and timeliness of the AI-generated responses, thereby underscoring its transformative potential for medical AI applications. Furthermore, the foundational principles underlying our system indicate its broader applicability in various other fields confronted with the challenges of rapidly changing knowledge bases. Through this work, we not only address the critical need for real-time information integration in healthcare AI but also establish a paradigm for future AI systems, promoting the incorporation of continuous learning and updating mechanisms to enhance their efficacy and relevance.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/622947f6f70520ffd8579b5ed9bae681096b1b67.pdf",
        "venue": "International Symposium on Computer Vision",
        "citationCount": 3,
        "score": 3.0
    },
    "2d362392fb0e13baf77e8ee35b5543e08207e97e.pdf": {
        "title": "Causal Reasoning in Large Language Models using Causal Graph Retrieval Augmented Generation",
        "authors": [
            "Chamod Samarajeewa",
            "Daswin De Silva",
            "Evgeny Osipov",
            "D. Alahakoon",
            "Milos Manic"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) are leading the Generative Artificial Intelligence transformation in natural language understanding. Beyond language understanding, LLMs have demonstrated capabilities in reasoning tasks, including commonsense, logical, and mathematical reasoning. However, their proficiency in causal understanding has been limited due to the complex nature of causal reasoning. Several recent studies have discussed the role of external causal models for improved causal understanding. Building on the success of Retrieval-Augmented Generation (RAG) for factual reasoning in LLMs, this paper introduces a novel approach that utilizes Causal Graphs as external sources for establishing causal relationships between complex vectors. This method is empirically evaluated using two benchmark datasets across the metrics of Context Relevance, Answer Relevance, and Grounding, in its ability to retrieve relevant context with causal alignment. The retrieval effectiveness is further compared with traditional RAG methods that are based on semantic proximity.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/2d362392fb0e13baf77e8ee35b5543e08207e97e.pdf",
        "venue": "International Conference on Human System Interaction",
        "citationCount": 3,
        "score": 3.0
    },
    "edfedf2af9a49b8f7b3d86e9af3b3097c9625201.pdf": {
        "title": "Improving Dietary Supplement Information Retrieval: Development of a Retrieval-Augmented Generation System With Large Language Models",
        "authors": [
            "Yu Hou",
            "J. R. Bishop",
            "Hongfang Liu",
            "Rui Zhang"
        ],
        "published_date": "2024",
        "abstract": "Background Dietary supplements (DSs) are widely used to improve health and nutrition, but challenges related to misinformation, safety, and efficacy persist due to less stringent regulations compared with pharmaceuticals. Accurate and reliable DS information is critical for both consumers and health care providers to make informed decisions. Objective This study aimed to enhance DS-related question answering by integrating an advanced retrieval-augmented generation (RAG) system with the integrated Dietary Supplement Knowledgebase 2.0 (iDISK2.0), a dietary supplement knowledge base, to improve accuracy and reliability. Methods We developed iDISK2.0 by integrating updated data from authoritative sources, including the Natural Medicines Comprehensive Database, the Memorial Sloan Kettering Cancer Center database, Dietary Supplement Label Database, and Licensed Natural Health Products Database, and applied advanced data cleaning and standardization techniques to reduce noise. The RAG system combined the retrieval power of a biomedical knowledge graph with the generative capabilities of large language models (LLMs) to address limitations of stand-alone LLMs, such as hallucination. The system retrieves contextually relevant subgraphs from iDISK2.0 based on user queries, enabling accurate and evidence-based responses through a user-friendly interface. We evaluated the system using true-or-false and multiple-choice questions derived from the Memorial Sloan Kettering Cancer Center database and compared its performance with stand-alone LLMs. Results iDISK2.0 integrates 174,317 entities across 7 categories, including 8091 dietary supplement ingredients; 163,806 dietary supplement products; 786 diseases; and 625 drugs, along with 6 types of relationships. The RAG system achieved an accuracy of 99% (990/1000) for true-or-false questions on DS effectiveness and 95% (948/100) for multiple-choice questions on DS-drug interactions, substantially outperforming stand-alone LLMs like GPT-4o (OpenAI), which scored 62% (618/1000) and 52% (517/1000) on these respective tasks. The user interface enabled efficient interaction, supporting free-form text input and providing accurate responses. Integration strategies minimized data noise, ensuring access to up-to-date, DS-related information. Conclusions By integrating a robust knowledge graph with RAG and LLM technologies, iDISK2.0 addresses the critical limitations of stand-alone LLMs in DS information retrieval. This study highlights the importance of combining structured data with advanced artificial intelligence methods to improve accuracy and reduce misinformation in health care applications. Future work includes extending the framework to broader biomedical domains and improving evaluation with real-world, open-ended queries.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/edfedf2af9a49b8f7b3d86e9af3b3097c9625201.pdf",
        "venue": "Journal of Medical Internet Research",
        "citationCount": 3,
        "score": 3.0
    },
    "719a34511a4a0ad428405eae75061d9fd459370f.pdf": {
        "title": "TaxTajweez: A Large Language Model-based Chatbot for Income Tax Information In Pakistan Using Retrieval Augmented Generation (RAG)",
        "authors": [
            "Mohammad Affan Habib",
            "Shehryar Amin",
            "Muhammad Oqba",
            "Sameer Jaipal",
            "Muhammad Junaid Khan",
            "Abdul Samad"
        ],
        "published_date": "2024",
        "abstract": "The advent of Large Language Models (LLMs) has heralded a transformative era in natural language processing across diverse fields, igniting considerable interest in domain-specific applications. However, while proprietary models have made significant strides in sectors such as medicine, education, and law through tailored data accumulations, similar advancements have yet to emerge in the Pakistani taxation domain, hindering its digital transformation. \n\u00a0In this paper, we introduce TaxTajweez, a specialized Retrieval Augmented Generation (RAG) system powered by the OpenAI GPT-3.5-turbo LLM, designed specifically for income taxation. Complemented by a meticulously curated dataset tailored to the intricacies of income taxation, TaxTajweez leverages the RAG pipeline to mitigate model hallucinations, enhancing the reliability of generated responses. Through a blend of qualitative and quantitative evaluation methodologies, we rigorously assess the accuracy and usability of TaxTajweez, establishing its efficacy as an income tax advisory tool.",
        "file_path": "paper_data/Retrieval-Augmented_Generation_for_Large_Language_Models/info/719a34511a4a0ad428405eae75061d9fd459370f.pdf",
        "venue": "The Florida AI Research Society",
        "citationCount": 3,
        "score": 3.0
    }
}