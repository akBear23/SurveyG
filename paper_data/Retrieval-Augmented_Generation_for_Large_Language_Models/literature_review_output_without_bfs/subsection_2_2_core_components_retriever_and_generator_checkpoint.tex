\subsection{Core Components: Retriever and Generator}
The efficacy of Retrieval-Augmented Generation (RAG) systems hinges on the synergistic operation of two fundamental components: a retriever and a generator. These essential building blocks enable Large Language Models (LLMs) to transcend the limitations of their parametric knowledge by dynamically accessing and synthesizing information from vast, external knowledge bases. This architecture, exemplified by pioneering works, laid the crucial groundwork for subsequent innovations in the field by enhancing factual accuracy and mitigating hallucinations.

The first core component is the \textit{retriever}, whose primary function is to efficiently fetch relevant documents or passages from a large corpus in response to an input query. Prior to the advent of dense retrieval, traditional information retrieval systems predominantly relied on \textit{sparse retrieval} methods, such as BM25 or TF-IDF. These methods operate on lexical matching, identifying documents that share keywords with the query. While effective for exact-match queries, sparse retrievers often struggle with semantic understanding, failing to retrieve passages that are conceptually relevant but lexically dissimilar. To overcome this limitation, early RAG systems prominently adopted \textit{dense retrievers}, with the Dense Passage Retriever (DPR) \cite{lewis2020pwr} serving as a seminal example. DPR employs a dual-encoder (or bi-encoder) architecture, independently mapping input queries and passages into a shared, dense vector space using neural networks (e.g., BERT-based encoders).

The training of dense retrievers like DPR is typically achieved through contrastive learning. The model is optimized to embed semantically similar query-passage pairs (positive examples) close together in the vector space, while simultaneously pushing dissimilar pairs (negative examples) far apart. A common strategy involves using in-batch negatives, where other passages within the same training batch serve as negative examples for a given query. More advanced techniques, such as hard negative mining, further refine this process by selecting particularly challenging negative examples that are semantically close but factually irrelevant. For instance, \cite{glass2021qte} introduced Dense Negative Sampling (DNS), which mines hard negatives from the learned dense vector index itself, rather than relying solely on lexical methods like BM25. This iterative process of identifying and learning from hard negatives is crucial for creating a highly discriminative embedding space, significantly boosting retrieval performance by ensuring that only the most relevant passages are retrieved. During inference, the retriever encodes the input query into its vector representation, which is then used to perform an efficient maximum inner product search (MIPS) against a pre-indexed database of passage embeddings (often facilitated by libraries like FAISS), enabling rapid identification of the top-k relevant textual contexts.

Following the retrieval of relevant passages, the \textit{generator} component assumes the responsibility of synthesizing information from the retrieved context and the input query to produce a coherent, factual, and attributable response. Early RAG systems commonly leveraged pre-trained sequence-to-sequence (seq2seq) models, such as BART or T5, for this generation task \cite{lewis2020pwr}. The retrieved passages are typically concatenated with the original input query and fed into the generator. The generator then utilizes its sophisticated attention mechanisms to condition the output sequence on both the query and the provided context, allowing it to integrate the external knowledge effectively. The seminal work by \cite{lewis2020pwr} demonstrated how both the retriever and the generator could be fine-tuned end-to-end, enabling the generator to learn to effectively leverage the retrieved context, discerning between relevant and irrelevant information to formulate its response. This joint training fostered a tight coupling between the retrieval and generation stages, optimizing them for downstream task performance.

This tight integration of a neural retriever and a sequence-to-sequence generator represented a significant architectural innovation, enabling language models to overcome the inherent knowledge cutoff of their training data and substantially improve performance on knowledge-intensive NLP tasks, as demonstrated by models like RAG \cite{lewis2020pwr}, REALM \cite{REALM}, and Atlas \cite{Atlas}. However, this foundational paradigm also presented several challenges that motivated subsequent research. The end-to-end training process was computationally expensive and complex, requiring careful balancing of the optimization objectives for two distinct neural modules. Furthermore, the performance of the generator was highly dependent on the quality of the initial retrieval; poor retrieval could lead to irrelevant or even misleading contexts, resulting in hallucinated or inaccurate generations. The memory requirements for indexing vast corpora and for effective hard negative mining during contrastive learning (as noted by \cite{glass2021qte} regarding FAISS indexing) also posed practical scalability challenges. These limitations, particularly the implicit reliance on high-quality retrieval and the complexities of joint optimization, became focal points for subsequent advancements in retrieval strategies, multi-stage processing, and system-level optimizations.