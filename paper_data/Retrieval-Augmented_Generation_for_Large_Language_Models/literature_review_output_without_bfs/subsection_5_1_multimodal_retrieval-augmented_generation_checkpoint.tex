\subsection*{Multimodal Retrieval-Augmented Generation}

The inherent limitations of text-only Retrieval-Augmented Generation (RAG) systems, which struggle with queries requiring non-textual knowledge, have spurred a critical advancement towards integrating diverse modalities such as images, video, and audio into the knowledge retrieval and generation process. This expansion addresses the need for Large Language Models (LLMs) to access a broader spectrum of human knowledge, leading to more comprehensive and contextually rich responses, particularly for visually-grounded queries where visual evidence is paramount.

A pioneering effort in this domain is \textit{MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text} by \cite{chen2022j8c}. MuRAG introduced a Multimodal Retrieval-Augmented Transformer, enabling LLMs to effectively leverage both visual and textual information. Its core methodology involves a unified multimodal encoder, combining pre-trained T5 and ViT models, to process queries and memory candidates across image and text modalities. A retriever stage, typically using Maximum Inner Product Search (MIPS), fetches Top-K multimodal items, which are then fed to a reader stage for text generation. A key innovation was its joint pre-training objective, combining a contrastive loss for effective retrieval and a generative loss for leveraging multimodal knowledge, alongside an efficient two-stage fine-tuning pipeline. While MuRAG significantly advanced the field by integrating diverse knowledge modalities and demonstrating state-of-the-art performance on multimodal QA datasets like WebQA, it also highlighted the inherent complexity and computational cost of managing massive external multimodal memories, posing scalability challenges for truly open-domain applications.

Building upon such foundational architectures, subsequent research has explored alternative strategies for multimodal document processing and retrieval. For instance, \cite{yu2024arx} introduced \textit{VisRAG}, a novel vision-language model (VLM)-based RAG pipeline that directly embeds entire multi-modality documents (including layout and images) as images, rather than first parsing them into text. This approach aims to maximize the retention and utilization of visual and layout information, eliminating the data loss often introduced during the parsing process in traditional text-based RAG. VisRAG's direct visual embedding offers a distinct architectural paradigm compared to MuRAG's separate text and image encoders, demonstrating superior performance by preserving the holistic context of multi-modality documents.

A significant challenge in multimodal RAG (MM-RAG) is the "multi-granularity noisy correspondence (MNC) problem," where retrieved multimodal evidence may contain irrelevant or conflicting information, hindering accurate generation. To address this, \cite{chen20245d2} proposed \textit{RagVL}, which leverages Multimodal Large Language Models (MLLMs) as powerful rerankers. RagVL instruction-tunes an MLLM to precisely filter the top-k retrieved images, thereby improving the relevance and quality of the multimodal context fed to the generator. Furthermore, to enhance the generator's robustness against noisy retrieval, RagVL injects visual noise during training at both data and token levels. This approach refines MuRAG's paradigm by introducing a more sophisticated mechanism for filtering and utilizing multimodal evidence, acknowledging that raw retrieval often benefits from an additional verification or ranking step. The importance of such robust retrieval and reranking strategies is underscored by broader RAG optimization efforts, as discussed by \cite{wang20248gm}, which systematically explores best practices across RAG components, including the integration of multimodal retrieval techniques for visual inputs and "retrieval as generation" for multimodal content.

Beyond improving retrieval quality, MM-RAG is crucial for addressing critical issues like hallucination and enhancing trustworthiness, particularly in high-stakes domains. MLLMs, despite their capabilities, are prone to generating factually incorrect information. \cite{chu2025wz5} demonstrated how Visual RAG (V-RAG) can significantly reduce hallucinations in medical MLLMs, specifically for tasks like chest X-ray report generation. Their work showed that V-RAG improves the accuracy of entity probing, ensuring that medical entities are properly grounded by retrieved images, thereby leading to more clinically accurate reports. This highlights MM-RAG's potential to provide verifiable grounding for sensitive applications. Complementing this, \cite{ma20245jl} introduced \textit{VISA: Retrieval-Augmented Generation with Visual Source Attribution}, which focuses on enhancing the verifiability of MM-RAG systems. VISA leverages large vision-language models to not only generate answers but also to identify and highlight the exact regions (with bounding boxes) in retrieved document screenshots that support the generated content. This visual attribution is critical for users to locate evidence, especially in complex, content-rich documents like Wikipedia pages or scientific papers, thereby significantly boosting the trustworthiness and explainability of MM-RAG outputs.

The capabilities of MM-RAG extend to complex reasoning and specialized applications. \cite{khaliq2024ne2} introduced \textit{RAGAR: Your Falsehood Radar}, applying multimodal RAG to the challenging task of political fact-checking for multimodal claims. RAGAR employs novel reasoning techniques like Chain of RAG (CoRAG) and Tree of RAG (ToRAG), which iteratively extract textual and image content, retrieve external information, and reason through subsequent questions based on prior evidence. This demonstrates how MM-RAG can move beyond simple question answering to support sophisticated, multi-step reasoning processes grounded in diverse evidence.

Despite these advancements, the field faces ongoing challenges. The scalability to even larger and more diverse multimodal knowledge bases, potentially including video and audio, remains a complex computational and architectural problem. Current research predominantly focuses on image-text pairs, with video and audio modalities often mentioned as future work rather than being deeply integrated. Benchmarking efforts, such as \textit{Visual-RAG} by \cite{wu2025eum}, specifically aim to isolate and measure the contribution of retrieved images in RAG for visually grounded queries. Their findings indicate that even state-of-the-art MLLMs struggle to efficiently extract and utilize visual knowledge, highlighting the need for improved visual retrieval, grounding, and attribution. Furthermore, handling complex inter-modal reasoning, ensuring dynamic knowledge updates, and developing robust evaluation metrics for truly open-domain multimodal understanding are critical areas for future research. The development of more advanced multimodal fusion techniques, adaptive retrieval strategies that can dynamically select modalities based on query context, and sophisticated mechanisms for handling ambiguity across modalities will be essential for realizing the full potential of multimodal RAG.