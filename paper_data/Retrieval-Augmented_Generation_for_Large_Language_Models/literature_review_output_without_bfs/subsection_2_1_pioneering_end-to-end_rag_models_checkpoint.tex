\subsection*{Pioneering End-to-End RAG Models}

Prior to the advent of end-to-end Retrieval-Augmented Generation (RAG), systems addressing knowledge-intensive tasks typically relied on multi-stage "retrieve-then-read" pipelines, exemplified by models like DrQA \cite{chen2017reading}. In these architectures, a retriever first fetched documents, and a separate reader model then processed these retrieved documents to generate an answer. Critically, the retriever and reader were trained independently, often leading to a suboptimal coupling where the retriever was not explicitly optimized for the downstream generation task. This paradigm shifted significantly with the introduction of foundational RAG models, which pioneered the concept of jointly training a retriever and a generator in an end-to-end fashion. This marked a profound departure from purely parametric Large Language Models (LLMs) by integrating a non-parametric memory component, enabling models to learn to augment their generation capabilities with external, up-to-date knowledge directly.

A seminal contribution was the Retrieval-Augmented Generation (RAG) framework introduced by \cite{lewis2020pwr}. This work proposed combining a pre-trained sequence-to-sequence generator, such as BART, with a dense passage retriever (DPR) that queries a large non-parametric knowledge base like Wikipedia. The core innovation lay in its end-to-end fine-tuning approach, where the entire model was optimized to maximize the likelihood of the target sequence. This was achieved by marginalizing over the latent retrieved documents: for each target token, the model considers all possible retrieved passages, computes the probability of generating the token given each passage, and sums these probabilities, effectively allowing gradients to flow back to the retriever. The RAG framework further explored two distinct architectural variants: RAG-Sequence, which retrieves a single document for the entire generated sequence, and RAG-Token, which retrieves a new document at each token generation step. This nuanced design space highlighted the flexibility in integrating external knowledge, leading to more factual, specific, and diverse language generation.

Concurrently, the Retrieval-Augmented Language Model (REALM) proposed by \cite{guu2020realm} explored a similar concept, focusing on pre-training a masked language model to retrieve and incorporate knowledge. REALM integrated a differentiable retriever directly into the pre-training objective, enabling the model to learn to fetch relevant passages to complete masked tokens. The differentiability of the retriever was a key technical achievement, allowing gradients from the masked language model loss to propagate back to the retriever's parameters. This was typically achieved by treating document selection as a latent variable and employing an approximate gradient estimator (e.g., using a Gumbel-Softmax approximation or a straight-through estimator) to enable end-to-end optimization. Both RAG and REALM showcased substantial improvements on knowledge-intensive NLP tasks, effectively mitigating the hallucination problem inherent in purely parametric models by grounding responses in verifiable external knowledge.

While these initial models established the core RAG paradigm, they also highlighted certain challenges. The computational expense and complexity associated with end-to-end training, particularly for very large LLMs or massive knowledge bases, could be prohibitive. Furthermore, their performance was inherently tied to the quality of the retrieved documents, lacking explicit mechanisms to evaluate or filter out irrelevant or noisy passages during the generation process itself. Building upon these foundations, \cite{izacard2022atlas} extended the RAG framework to few-shot learning scenarios with Atlas. This model unified retrieval and generation within a single T5-based architecture, demonstrating how end-to-end trained RAG models could significantly boost performance in data-scarce settings by effectively leveraging external knowledge. Atlas showcased the versatility and immediate impact of the initial RAG framework, proving its applicability beyond standard knowledge-intensive tasks and into more challenging few-shot learning environments.

In conclusion, these pioneering works laid the groundwork for modern RAG systems by demonstrating the profound benefits of tightly integrating retrieval with generation. They established the core methodologies for learning to retrieve relevant passages and condition language generation on them, effectively overcoming the knowledge cutoff of parametric LLMs and providing a mechanism for factual grounding. However, the computational demands of end-to-end training and the implicit assumption of high-quality retrieval remained significant challenges. These limitations spurred subsequent research into more efficient training strategies, robust retrieval mechanisms, and adaptive RAG systems. The effectiveness of these integrated models was critically dependent on the two main architectural pillars: the retriever's ability to find relevant information and the generator's capacity to synthesize it. The next section will therefore deconstruct these systems to examine the core components in greater detail, laying the groundwork for understanding subsequent innovations in retrieval and generation strategies.