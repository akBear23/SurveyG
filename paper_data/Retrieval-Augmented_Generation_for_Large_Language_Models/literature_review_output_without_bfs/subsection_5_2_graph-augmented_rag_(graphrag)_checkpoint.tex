\subsection{Graph-Augmented RAG (GraphRAG)}
While Retrieval-Augmented Generation (RAG) significantly enhances Large Language Models (LLMs) by grounding responses in external knowledge, traditional RAG systems primarily operate on unstructured text, often struggling with complex relationships and explicit facts inherent in many real-world domains \cite{gao20238ea, huang2024a59}. This limitation can lead to persistent hallucinations and difficulties in handling intricate, multi-hop queries \cite{chen2023nzb, tang2024i5r}. Graph-Augmented RAG (GraphRAG) emerges as a powerful paradigm to address these challenges by integrating structured knowledge, particularly Knowledge Graphs (KGs) and textual graphs, into the retrieval process to provide LLMs with verifiable facts and relational context \cite{peng2024mp3, zhang2025gnc}.

Early explorations into GraphRAG focused on leveraging graph structures to improve retrieval and generation for specific tasks. \cite{he20248lp} introduced G-Retriever, a pioneering framework for textual graph understanding and question answering. It formulates subgraph retrieval as a Prize-Collecting Steiner Tree (PCST) problem, enabling the extraction of relevant subgraphs that capture neighborhood information, thereby mitigating hallucination and improving scalability for general textual graphs. Building on this, \cite{xu202412d} demonstrated the practical benefits of integrating RAG with KGs for customer service question answering. Their method constructs a dual-level KG that preserves both intra-issue structure and inter-issue relations, utilizing LLM-driven query parsing and Cypher queries for precise subgraph extraction, leading to substantial improvements in retrieval efficacy and answer quality.

Further advancements have focused on more sophisticated graph-aware retrieval algorithms and integration strategies. \cite{hu2024eyw} proposed GRAG for networked documents, employing a divide-and-conquer strategy for efficient textual subgraph retrieval and a graph soft pruning mechanism to filter irrelevant information. GRAG integrates graph context into LLMs through both hierarchical text descriptions (hard prompts) and GNN-based soft prompts, where message passing is guided by learned relevance factors. Addressing the challenge of multi-hop reasoning in KGQA, \cite{mavromatis2024ml9} introduced GNN-RAG, which repurposes Graph Neural Networks (GNNs) as "dense subgraph reasoners" to identify answer candidates and extract precise reasoning paths from KGs. This approach significantly outperforms LLM-based retrievers on complex multi-hop questions, achieving state-of-the-art results on benchmarks like WebQuestionsSP and Complex WebQuestions. In contrast, \cite{li2024hb4} argued for simplicity with SubgraphRAG, utilizing a lightweight Multilayer Perceptron (MLP) combined with Directional Distance Encoding (DDE) for efficient and flexible multi-hop subgraph retrieval. By framing subgraph retrieval as a triple factorization problem, SubgraphRAG allows for adjustable subgraph sizes and achieves competitive performance with smaller LLMs without fine-tuning, demonstrating that effective structural feature encoding can be highly efficient.

Recognizing the complementary strengths of vector-based and graph-based retrieval, hybrid GraphRAG approaches have emerged. \cite{sarmah20245f3} proposed HybridRAG, explicitly integrating VectorRAG and GraphRAG components. Their system employs a two-tiered LLM chain for robust KG construction from unstructured text, combining vector similarity search with graph traversal to overcome the limitations of each method individually, particularly in complex financial document analysis. Taking this integration a step further, \cite{ma2024pwd} introduced Think-on-Graph 2.0 (ToG-2), a hybrid RAG framework that performs iterative, "tight-coupling" knowledge exploration between unstructured documents and KGs. ToG-2 leverages LLMs for relation and entity pruning, guiding a continuous search for in-depth clues, which results in deeper and more faithful LLM reasoning across various knowledge-intensive tasks.

The field of GraphRAG is still evolving, with comprehensive surveys like \cite{peng2024mp3}, \cite{procko202417i}, and \cite{zhang2025gnc} formalizing its workflow and categorizing methodologies. \cite{zhang2025gnc} notably classifies GraphRAG into knowledge-based, index-based, and hybrid categories, highlighting the diverse ways graphs can be leveraged. Despite significant progress in mitigating hallucination and enhancing reasoning, challenges persist in the complexity of KG construction, ensuring the quality and completeness of graph knowledge, and developing highly efficient large-scale graph traversal and reasoning algorithms for real-time applications. Future research will likely focus on more automated and dynamic KG construction, adaptive graph-aware retrieval mechanisms, and seamless integration strategies to further ground LLMs in verifiable, structured knowledge.