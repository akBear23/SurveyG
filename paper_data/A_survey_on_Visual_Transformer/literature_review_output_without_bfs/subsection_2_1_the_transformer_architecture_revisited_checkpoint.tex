\subsection*{The Transformer Architecture Revisited}

The Transformer architecture, initially introduced by Vaswani et al. \cite{vaswani2017attention} for sequence-to-sequence tasks in natural language processing (NLP), marked a profound paradigm shift by entirely eschewing recurrent and convolutional layers in favor of a purely attention-based mechanism. This design choice enabled unprecedented parallelism and the capture of long-range dependencies, fundamentally altering how sequential data is processed. Grasping these foundational elements is crucial for understanding how Transformers operate on sequential data and how these principles were subsequently adapted for visual inputs, forming the bedrock of Vision Transformers (ViTs).

At its core, the original Transformer operates on an encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations, while the decoder generates an output sequence one symbol at a time, attending to the encoded representations. For many vision applications, particularly image classification, the architecture primarily leverages the encoder stack. The fundamental building block within both the encoder and decoder is the multi-head self-attention (MHSA) mechanism. This mechanism enables the model to dynamically weigh the importance of different input elements by computing attention scores. It achieves this by projecting input representations into three distinct learned linear transformations: Query ($\mathbf{Q}$), Key ($\mathbf{K}$), and Value ($\mathbf{V}$). The attention score for each query is calculated against all keys, determining how much focus to place on each value, thereby capturing global contextual information and long-range dependencies within the sequence. Mathematically, the scaled dot-product attention is computed as $\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{QK}^T}{\sqrt{d_k}})\mathbf{V}$, where $d_k$ is the dimension of the keys. The "multi-head" aspect allows the model to jointly attend to information from different representation subspaces at different positions, enriching the learned relationships and providing a more comprehensive understanding of the input.

Following the self-attention layer, a position-wise feed-forward network (FFN) processes each position independently. This FFN typically consists of two linear transformations with a ReLU activation in between, applying a simple two-layer neural network to the attention outputs. To facilitate the training of deep networks and prevent issues like vanishing gradients, both the self-attention and FFN sub-layers are augmented with residual connections \cite{he2016deep}, which add the input of the sub-layer to its output, and layer normalization \cite{ba2016layer}, which normalizes the activations across the feature dimension for each sample. Furthermore, as the self-attention mechanism itself is permutation-invariant (i.e., it treats the input as a set rather than a sequence), positional encodings are added to the input embeddings. These encodings inject crucial information about the relative or absolute position of tokens in the sequence, allowing the model to understand sequence order.

The conceptual leap to Vision Transformers involved adapting these foundational principles to visual inputs. The seminal approach, as introduced by the original Vision Transformer (ViT) \cite{dosovitskiy2020image}, involves segmenting an input image into a sequence of fixed-size, non-overlapping patches. Each patch is then linearly embedded into a higher-dimensional space to create visual tokens, analogous to words in NLP. These visual tokens, along with positional encodings, are then fed into a standard Transformer encoder. This direct adaptation demonstrated the power of global self-attention for image classification, achieving competitive performance on large-scale datasets.

However, this direct application of the NLP-centric Transformer to images also exposed inherent challenges and limitations stemming from the architecture's original design choices. Firstly, the quadratic computational complexity of global self-attention with respect to the sequence length (number of patches) becomes a significant bottleneck for high-resolution images, making it computationally expensive and memory-intensive. Secondly, unlike Convolutional Neural Networks (CNNs) which possess strong inductive biases for local feature extraction, translation equivariance, and hierarchical processing, the pure Transformer architecture inherently lacks these spatial priors. This deficiency means that ViTs often require massive pre-training datasets to learn robust visual representations from scratch, a limitation not typically shared by CNNs. The reliance on explicit positional encodings, rather than inherent spatial understanding, further underscores this difference. These challenges, inherent in the direct adaptation of the foundational Transformer to the visual domain, became the primary drivers for subsequent research, motivating a wave of innovations aimed at enhancing efficiency, introducing multi-scale processing, and integrating convolutional inductive biases, which are explored in detail in later sections of this review.