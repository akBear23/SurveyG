# A Comprehensive Literature Review with Self-Reflection

**Generated on:** 2025-10-07T22:00:37.194575
**Papers analyzed:** 367

## Papers Included:
1. c8b25fab5608c3e033d34b4483ec47e68ba109b7.pdf [liu2021ljs]
2. 7a9a708ca61c14886aa0dcd6d13dac7879713f5f.pdf [liang2021v6x]
3. d40c77c010c8dbef6142903a02f2a73a85012d5d.pdf [han2020yk0]
4. 0eff37167876356da2163b2e396df2719adf7de9.pdf [chen2021r2y]
5. da74a10824193be9d3889ce0d6ed4c6f8ee48b9e.pdf [mehta20216ad]
6. a09cbcaac305884f043810afc4fa4053099b5970.pdf [li2022raj]
7. 2fe2f849b94cf08b559226bc9d78adcaef5ef186.pdf [chen2022woa]
8. e5cb26148791b57bfd36aa26ce2401e231d01b57.pdf [xia2022qga]
9. 96da196d6f8c947db03d13759f030642f8234abf.pdf [zhou202105h]
10. 751b71158b7dcd2a7949e72a6ad8fb13657a401c.pdf [liu2021jpu]
11. 164e41a60120917d13fb69e183ee3c996b6c9414.pdf [lee2021us0]
12. 5553f9508dd1056ecc20c5b1f367e9a07e2c7e81.pdf [zhang2021fje]
13. 226fcbe55235d873bedb2fcf5b981bd5ec860d4f.pdf [jiang2022zcn]
14. 5c1b7e400ba8b3c3b04522f7bb3af17ad68d0585.pdf [islam2022iss]
15. a4b728dbbf5afdc231afb95ad4e5c2ececdefc48.pdf [li2022a4u]
16. 0b8e8760d7dd64c7439019aeb3b6ac55dd5075d4.pdf [yao202245i]
17. 17534840dc6016229a577a66f108a1564b8a0131.pdf [borhani2022w8x]
18. b8cee43a51c44f8f4448e78e41ecf081987707cf.pdf [mao2021zr1]
19. 44ee4165b8a3811dc7d557f99150ff9e62f3733f.pdf [chen202174h]
20. 50a260631a28bfed18eccf8ebfc75ff34917518f.pdf [jie20220pc]
21. 3c14992a490cc31a7a38f5fab156c9da40a861d4.pdf [fan2022m88]
22. ab70c5e1a338cb470ec39c22a4f10e0f19e61edd.pdf [lin20216a3]
23. 1ee05cd919590eaba129caa0fda5e850c87b75a5.pdf [lin2021utw]
24. 9fb327c55a30b9771a364f45f33f77778756a164.pdf [li2022mco]
25. dfdb2894d50e095ce97f994ed6cee38554c4c84f.pdf [li2022tl7]
26. f27040f1f81144b17ec4c2b30610960e96353002.pdf [yang2021myb]
27. 4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a.pdf [yu2022iy0]
28. 49030ae220c863e9b72ab380ecc749c9d0f0ad13.pdf [zhuang2022qn7]
29. 60b0f9af990349546f284dea666fbf52ebfa7004.pdf [deng2021man]
30. 64d8af9153d68e9b50f616d227663385bece93b9.pdf [wang2021oct]
31. 03384825d373aabe67c4288ef1eae4d1cf89dc00.pdf [wang2022ti0]
32. d28fed119d9293af31776205150b3c34f3adc82b.pdf [li2022ow4]
33. b52844a746dafd8a5051cef49abbbda64a312605.pdf [wang2022da0]
34. 35fccd11326e799ebf724f4150acef12a6538953.pdf [deng2022bil]
35. 0d1e4faa1580266a56ad62ac2fdd72ed0c0bbbe9.pdf [yang20228mm]
36. 8f7ae0526f9c8706a9e0967eb7942d9b3ffb2788.pdf [gheflati202131i]
37. 00f6041bdbdd00299ef317bd86db3660eb039a8c.pdf [tang2022e2i]
38. 5f7fee3db8879d9d4e714ae20627f00fcbdfab38.pdf [yu202236t]
39. 070c917ab1a4d6b924a9613ca18443f260d8d5be.pdf [li2021ra5]
40. 011f59c91bbee6de780d35ebe50fff62087e5b13.pdf [meng2022t3x]
41. f27c847e2909f30745f4a3528b574f5acfd76ea7.pdf [li20223n5]
42. d717fa5a4355b5033e660ad58a7ae7f4ccce6939.pdf [bazi2022tlu]
43. a119cc83788701313d94746baecd2df5dd30199d.pdf [zheng2022gg5]
44. 60f9fd4e6b29e6f11324ba4f6d3349a2d8107a1b.pdf [gao2021uzl]
45. 5ca02297d8d49f03f26148b74fea77272d09c78b.pdf [zheng202218g]
46. aed7e4bc195d838735c320ac40a78f123206831b.pdf [bi20225lu]
47. b66e4257aa8856df537f03f6a12341f489eb6500.pdf [chen2022vac]
48. f9480350e1986957919d49f346ba20dcab8f5b71.pdf [song2022603]
49. 836dd64a4f606931029c5d68e74d81ef5885b622.pdf [li2022rl9]
50. 16fa1a8575ff56781b6b83726906754ed4e5f3a7.pdf [wensel2022lva]
51. 9f7f81b1c82828a45a52df8f0c6a92636af76c7e.pdf [wang2021sav]
52. e678898301a66faab85dfa4c84e51118e434b8f2.pdf [naseem2022c95]
53. e33434a141bb2881a2e60c518a0cda4feed3f19a.pdf [wu20210gs]
54. 9da3fadf092c864f61d6fd1e8eab5a6ca2397194.pdf [lyu2022vd9]
55. 9fab78222c7111702a5702ce5fae0f920722c316.pdf [krishnan2021086]
56. c051ee2ad7ac203a26fa8f50eb6312424c729b27.pdf [yang20210bg]
57. 13f7a106bb3814ad1fab25fd1356e99e91f402d3.pdf [li20229zn]
58. d04fbbec070b7e7af8152f9b7574971d22bf5997.pdf [wang2022n7h]
59. e939b55a6f78bffeb00065aed897950c49d21182.pdf [chen202199v]
60. 6dc8693674a105c6daca5200141c50362e3044fc.pdf [panboonyuen20218r7]
61. 494e200d8a2ef49d21ce3458a25a109ea917ebe4.pdf [liang2022xlx]
62. 39240f94c9915d9f9959c34b1dc68593894531e6.pdf [zhou2021rtn]
63. 8db96d4eaefdac8dff6d98d94a5e3f5b5558c63d.pdf [dubey2021ra5]
64. 428d755f0c8397ee6d04c89787f3455d323d8280.pdf [ayas2022md0]
65. ff00791b780b10336cc02ee366446d16e1c5e17b.pdf [tian2022shu]
66. 957a3d34303b424fe90a279cf5361253c93ac265.pdf [liu2022249]
67. 401c8d72a9b275e88e6ba159d8d646cfb9f397aa.pdf [zhang2021mcp]
68. 7dee2bc2be709c0009b7623b7af78246f32e0a60.pdf [han2021vis]
69. 1b026103e33b4c9eb637bc6f34715e22636b3492.pdf [kim2022m6u]
70. 024c595ba03087399e68e51f87adb4eaf5379701.pdf [zhou2022nln]
71. 9500dbde37e0f8c6b9ea924aacc6591bc3414166.pdf [hu202242d]
72. 977351c92f156db27592e88b14dee2c22d4b312a.pdf [you2022bor]
73. ba12a9915553b3b42df17a33afcfd547821d8cc3.pdf [ren2022ifo]
74. 3efcd3a4c54694a093886981d59e3cffe0dd7149.pdf [wang20215ra]
75. 58fc305734a0d5d849ae69b9233af082d712197e.pdf [xiao202229y]
76. 54911915a13cf0138c06b696e6c604b12acfe228.pdf [jamil20223a4]
77. b8585577d05cebd85d45b7c63f7011851412e794.pdf [bai2022f1v]
78. 956d45f7a8916ec921df522c0641fd4f02beccb7.pdf [li2022th8]
79. 99fbe810d4194684be03458fdfebacb12d8a5c4e.pdf [almalik20223wr]
80. f4e32b928d7cc27447e312bdc052aa75888045aa.pdf [sha2022ae0]
81. 98e702ef2f64ab2643df9e80b1bd034334142e62.pdf [zhang2022msa]
82. 0ef3e7efa5ded21a398d8ec90ff766b467c4fc54.pdf [htten2022lui]
83. a21fa5ff42db4b0bd0fefae3d710cad5f0175400.pdf [hatamizadeh2022y9x]
84. 174919e5a4ef95ff66440d56614ad954c6f27df1.pdf [montazerin2022dgi]
85. 6971aee925639a8bd5b79c821570728ef49060c6.pdf [kojima2022k5c]
86. 15ad149fc1ba4d8b5de189fdfacac9daecc286d0.pdf [kang2022pv3]
87. f66181828b7621892d02480fa1944b5f381be80d.pdf [tian2022qb5]
88. cee8934975dfbe89747af60bbafc95e10a788dc2.pdf [peng2022snr]
89. 69e20583872b4f1384ae3f7dbdbec648c19d3d35.pdf [ho20228q6]
90. 0594eaa8dfe580678a2382aaf77ac3582c872a97.pdf [xia2022dnj]
91. 3a0145f34bcd35f09db23b2edec3ed097894444c.pdf [wang202232c]
92. ce79649b126dfe9e3cbeb1ecd64a80708bbd5538.pdf [mogan202229d]
93. 572ed945b06818472105bd17cfeb355d4e46c5e5.pdf [yang20221ce]
94. 934942934a6a785e2a80daa6421fa79971558b89.pdf [li2022ip7]
95. 3ae8c0b646ddce95ffd09da31c02ed6fdb744e90.pdf [yu20220np]
96. bc8d9f11ad91d54e83ad7cc3900502a913499fcb.pdf [li20229fn]
97. cf6d947d5d2ee72873a5a7b97dde9f881f79a6b6.pdf [huang2022iwe]
98. 9db52b92b48b8855f76d7e4b51b7727a2b3c0271.pdf [qu2022be0]
99. e4add4391dfa2a806a50cc1fbe9a9696dac9501f.pdf [zeng2022ce2]
100. 324f97d033efd97855488cf0b15511799fe7b7f7.pdf [lin20225ad]
101. bf52f09d648d78677f140e63c7c83ca44fcd438f.pdf [reghunath2022z8g]
102. 4eba4dccc7d19c40dfd02eef447a57613c3644e2.pdf [kundu2022z97]
103. 0bdcea2f18e158bbd1723ed27d48e5211545e050.pdf [sun2022cti]
104. 67b7ad5f3c818f42c8359abc87f353202f76f6f2.pdf [li2022gef]
105. 3994996a202f0127a58f57b259324a5283a1ba27.pdf [guo20228rt]
106. 4767600089dd71c7ed74c6a0acaf9e5a6fb345c9.pdf [li202240n]
107. d5b8af7ae526e3a511b1172c26d7caf7421145bb.pdf [jiang2022jlc]
108. 4d491b6fbe529a3986ef50cc34ede7c9ad88126c.pdf [lin2021oan]
109. d9e4af7739d4d5dcd44619bf276dc497f7334e34.pdf [wang2022dl1]
110. d69102eec0fff1084e3d1e24a411103280020a32.pdf [li2022wab]
111. 38bef6f0b4de2f9809b4fd44e8b61f9bebb14db9.pdf [park2022eln]
112. b80d0ce78652ffbf6b3ddebfc567f8594a6ffd4b.pdf [shen2022d6i]
113. 7d4d512445903e8ad75f0dcfa8baec27b1fe8620.pdf [wang20224wo]
114. 7d5274f1155b85a6120491c9374b6983dac96552.pdf [tao2022gdr]
115. 0afbaae7d0bfe98a04a3b7dd1ba69e25f32b24c6.pdf [wu2021nmg]
116. 6bcdf04ccd35ae971e765cdda25127ce005b7713.pdf [liu2022c56]
117. 6fd1002f321240c75dfc86c6bad6c05068a8160a.pdf [wang2022pb8]
118. c4560ab3855cb8ae4b9922458a0b6b94108e2c28.pdf [xiong2022ec2]
119. a0762777d6e46acbcd5b301285c2d9894065ff8f.pdf [sun2022pom]
120. d43950779dc86b728d7e002be6195526d35a26b0.pdf [qi2022yq9]
121. 2e69d97ae22c6a3685cc548f8c19c696d5d7d363.pdf [ma2022vf3]
122. c25091718b22384cebece2da7f30fc1702a07c76.pdf [wang2022tok]
123. cec0cbc2dd6d7975714110632b6bfcb5c1927ec3.pdf [wang2022pee]
124. ef93c81f90154a022e164be2f83c6cf6e602f33d.pdf [jannat20228u6]
125. 6a8d4a97ba3eafcb85d2d2a57d7d514bb592dbd0.pdf [chen2022r27]
126. 64143b37ae41085c4907e344ff3a2362a3051d0c.pdf [wang2021p2r]
127. dcd8617200724f0aa998276be339ff4af589ee42.pdf [sajid2021xb6]
128. 46880aeca86695ca3117cc04f6bd9edaf088111b.pdf [xing2022kqr]
129. 7e0dd543471b66374fbf1639b9894d3d502533b6.pdf [garaiman2022xwd]
130. 845a154dbcde81de52b68d73c78fad5be4af3b20.pdf [wang2022wyu]
131. 6f4093a7ad5378e8cd3b73a52fbec80b784c107d.pdf [hou2022ver]
132. 50405c1ee26c97ed5b9a54c7179317a424e6e471.pdf [agilandeeswari202273m]
133. 1bf9a97a7581d1671d683c19ce36fed6511dc95b.pdf [qin2022cfg]
134. e58c2bc5333ee8f37150d9d15c0428d9658b2e50.pdf [wang2022ohd]
135. 761f55a486e99ab5d3550aee48df34b6b65643c2.pdf [yu2022o30]
136. 2dcc1f5f1ba11f99e09f22c53d02aa66cec35c48.pdf [boukabouya2022ffi]
137. 52a7f15085f1b6815a4de2da26df51bb63470596.pdf [wang2022d7p]
138. 649b706ba282de4eb5a161137f80eb49ed84a0a8.pdf [song20215tk]
139. 186295f7c79e46c0e4e5f40e094267c09714043d.pdf [xie2021th0]
140. d77288fc7de7b15c200ed75118de702caf841ec3.pdf [sun2022bm5]
141. 2e4dbc3dbd400346be60318ae558a0293e65ba81.pdf [jing2022nkb]
142. 3de95f33c2b4f61a9c0f335b4810a966e209a47a.pdf [li2022spu]
143. 7817ecb816da8676ae21b401d60c99e706446f06.pdf [song2022y4v]
144. 3502b661362b278eebacf1037fc3bb4e21963869.pdf [shukla2022jxz]
145. 791d1e306eaa2e87657925ec4f45661baa8da58b.pdf [tran2022bvd]
146. 1d8ecf2c3a78e5dc4de9e4c41961771bbd0033b3.pdf [hong2022ks6]
147. e1f7478294fe01ce271cdef9ba93f4c675d92dc9.pdf [panboonyuen2021b4h]
148. a56f8e42e9efe5290602116b42a247b758052fe4.pdf [zhao2022wi7]
149. 6eb10790689a4cf239d1ee2a3919dced4be6db8f.pdf [wang2022h3u]
150. fc2e7fcdc1bd773f1eb097ae67c8f736108276e3.pdf [liu2021yw0]
151. 16ad38c73f4897e1c58326e3562b74ae1c2fd219.pdf [gul202290q]
152. 371e924dd270a213ee6e8d4104a38875105668df.pdf [zhao2022koc]
153. 0025c4241ffb2cce589dc2dcd82385ff06455542.pdf [yang2022qwh]
154. f2b3ece7cb0c88701ae0055236f6d6da39c8156d.pdf [alquraishi2022j3v]
155. 1eb4bbaa204890a45da2ca713f34fcc2d763c6e1.pdf [jin2021qdw]
156. 1b18750ea0d26575f3e2c18f61e444c2790dbe4c.pdf [lee2022rf1]
157. 08502153c9255399f8ff155e5f75900f121bd2ff.pdf [shi2022evc]
158. cb85a7648d2a4b4953abfc7437a33f91dc2f3673.pdf [zhang20223g5]
159. 90f9b2892a437ca32c4ab26c09b8dbb7de8fa034.pdf [zim202282d]
160. 7ead0cae4e67f390b2eb0083117ea8ab90c53b47.pdf [bao202239k]
161. e4864ea54710b4c8f3b2ca9ac39854bdfd0dae28.pdf [sun2022nny]
162. fe6887ee065c4f64a0c8d6054d1fc562766452de.pdf [munyer2022pfs]
163. 280ea33e67484c442757fe761b75d871a399905d.pdf [fan2022wve]
164. 29f86d6d1eaba6a466c231f6906b18eae4b2b484.pdf [wang2022gq4]
165. d8bf4b494d255c6d9b81e9963f29e40831106e3e.pdf [ali2022dux]
166. abf037290e859a241a5af2c5adf9c08767971683.pdf [chougui2022mpo]
167. dd46070ce18f55a5714e53a096c8219d6934d188.pdf [zhuang2021hqu]
168. 829926d9943c99dd64eea5b4b96541eca1e5e91d.pdf [chen2021d1q]
169. e8dceb26166721014b8ecbd11fd212739c18d315.pdf [hatamizadeh2024xr6]
170. e06b703146c46a6455fd0c33077de1bea5fdd877.pdf [ryali202339q]
171. 3af375031a3e23b7daf2f1ed14b5b61147996ca0.pdf [yao2023sax]
172. d203076c28587895aa344d088b2788dbab5e82a1.pdf [li2023287]
173. f3d0278649454f80ba52c966a979499ee33e26c2.pdf [zhao2024671]
174. 918617dbc02fa4df1999599bcf967acd2ea84d71.pdf [dehghani2023u7e]
175. 51f38bd957fa863022feb5878fa1ba3bea6657cf.pdf [duan2024q7h]
176. 1f389f54324790bfad6fc40ac4e56428757ea92b.pdf [barman2024q21]
177. 05236fa766fc1a38a9eb895e77075fb65be8c258.pdf [jamil20230ll]
178. 0eec6c36da426f78b7091ba7ae8602e129742d30.pdf [paal2024eg1]
179. 689bc24f71f8f22784534c764d59baa93a62c2e0.pdf [zhang2023k43]
180. afb7ff7096cd45de02de96b5937c3416ebca0cb0.pdf [himel2024u0i]
181. 595adb75ddeb90760c79e89b76d99e55079e0708.pdf [xu20235cu]
182. de20c6805b83a2f83ed75784920e91b913d888bb.pdf [chi202331y]
183. c57467e652f3f9131b3e7e40c23059abe395f01d.pdf [patro202303d]
184. 53e5db85e2a7442f20670be2ae25019fcf9d27a2.pdf [pan2023hry]
185. 0682771fd5f611bce2a536bf83587532469a83df.pdf [wang2024mrk]
186. a4ef2c7c321dea04d8ea91ec1ce0710ebce4f848.pdf [tabbakh2023ao7]
187. 243a056d1acb153f70e39cc80a10e7d211a4312f.pdf [dutta2023aet]
188. d8ab87176444f8b0747972310431c647a87de2df.pdf [qiu2024eh4]
189. a1e17a09b0df676f133c2175366ab7fcd0381ac1.pdf [li2023nnd]
190. 77b625e6dec2732b96ca8cf57d7a123497873e1d.pdf [zhao20243f3]
191. e91934d66d9133d854ff0a4cafbe7966584bbf97.pdf [song2024fx9]
192. e37539f5c943a92ef56b49b7fa067bd976e418d4.pdf [cai2023hji]
193. 8fc9a7f23c5cf57e17790ffb99b6acb11693c673.pdf [akinpelu2024d4m]
194. 7ddbb9723420cb474c58d4b6b6834d5ebe112049.pdf [hayat2024e4f]
195. c7c0c84fa505995cd888a8fbaa9def21cb84cca3.pdf [li2024g3z]
196. 1af8c50b4c7329fb2273aa7b755b0f9e8223e2d2.pdf [arshed2023zen]
197. bca0adcba0c26c5a34b05795c5bfbc1948bf2fae.pdf [qin20242eu]
198. c4895869637f73154d608cdd817234b0dbcd3508.pdf [lee2023iwc]
199. 64811427a4427588bb049a6a254446ddd2cafacc.pdf [tagnamas20246ug]
200. 7a0f92968c63613730b0c06b3bc3bd40d2666571.pdf [li2023jft]
201. 136b0ef7bd7f642a285b1d61ddf7abfc7ddc026f.pdf [song2024c99]
202. cf439db0e071f19305ea1755aa108acdde73ed99.pdf [aksoy20240c0]
203. ffc855594cad345ea5a1cce2ee27095bec767bc8.pdf [leem2024j4t]
204. 838d7862215df504dde41496cbe6ee711a12ae9f.pdf [chen2024asi]
205. 9a35a0aac04f4f712f38ea9bbfd2f0b0108242f5.pdf [lin202343q]
206. f6bf7787115affe22c410eb5b2606269912d59a0.pdf [ghahremani202491m]
207. 69e56df0ae079c83607bb48e68f8db39a4177cd0.pdf [wang20249qa]
208. 1528bab641830ea279bdf53ff9e7d11d2b7e5028.pdf [shahin2024g0q]
209. b43bb480caad36ab6fd667570275d42fe9050175.pdf [zhu2023dpi]
210. 1970ace992d742bdf098de08a82817b05ef87477.pdf [yu2023l1g]
211. fec86abbb29b559c1eaff31428f5b59f8070bb67.pdf [ko2024eax]
212. cb8b0eba078098000f004d7e0f97a33189261f30.pdf [yang2024w08]
213. 9b4d81736637392adabe688b6a698cec58f9ce57.pdf [nazih20238nf]
214. 981970d0f586761e7cdd978670c6a8f46990f514.pdf [xia2023bp7]
215. 6bdafb965e94c5240db2c30f20c37c4b4dd0e451.pdf [nag2023cfn]
216. a3d1cebf99262cc20d22863b9540769b49a15ede.pdf [gezici20246lf]
217. f05176e2d3f7a6e95b60f0e96fd9bf20e8b335d2.pdf [ghazouani202342t]
218. 442b5ec3aad4b099e71d6203a62eb7ec7519544c.pdf [wang202338i]
219. 635675452852e838644516e1eeefd1aaa8c8ac07.pdf [guo2024tr7]
220. d2fce7480111d66a74caa801a236f71ab021c42c.pdf [wang2023ski]
221. 5135a8f690c66c3b64928227443c4f9378bd20e1.pdf [zheng202325h]
222. 77eea367f79e69995948699d806683c7731a60b1.pdf [mogan2023ywz]
223. 861f670073679ba05990f3bc6d119b13ab62aca7.pdf [ebert202377v]
224. f93f718bb2d3f5e4354e174e54e71c5596f56ea3.pdf [wang20245bq]
225. c5c9005aae80795e241de18b595c2d01393808f8.pdf [cao20241ng]
226. 14c42c0f2c94e0a1f4aa820886080263f9922047.pdf [yang2024in8]
227. 9998291d71f4e8ddf59f4b016b19df1f848eeed1.pdf [hussain2025qoe]
228. 9fa44f616e796f187d97b9cc324bd839bc21ad81.pdf [shim2023z7g]
229. d62e6ba83d3da5b75e1d428f20f8199e3754bd5d.pdf [alam2024t09]
230. d1faaa1d7d312dd5867683ce60519979de6b3349.pdf [yang2024tti]
231. d68aedc3a957c25e5f19438ec4b64fb53b48422c.pdf [wang20245hx]
232. bb5e6bbd4ad8c613a0bac3db23940c03ce94bec9.pdf [song202479c]
233. 55156532cb9c20fdcaed9ead238f7a2cbaab2527.pdf [li2023lvd]
234. ae7c5ae7d377189c5a5f33a60b33c6cc5ca56124.pdf [ma2023vhi]
235. 3b427c8d3258968b9ac5eaf846d99ac027de9a76.pdf [han202416k]
236. 10e9943b3a974ac6175ffec3228e670ec9d2cc18.pdf [katar202352u]
237. 1783c85f8b951dc11c9dcb4929bfcae2d52ecb63.pdf [hemalatha2024a14]
238. 42bd7d3ca2b66eca5609fe77078fe8951ab9fd5c.pdf [ma2024uan]
239. 7aeadf7b9d839bdd9dbfbff5239f71792ff0c04c.pdf [lai20238ck]
240. b2becca9911c155bf97656df8e5079ca76767ab9.pdf [wang2024luv]
241. 25a69bdd7f87ecc287b4e9a64eb4d6d562371add.pdf [ling2023x36]
242. 50e997b23a534a6fbfd32d63990fa80373ec7c6b.pdf [wang2023bfo]
243. 3ea79430455304c782572dfb6ca3e5230b0351de.pdf [yin2023029]
244. 0284fa05879f1609bf459f15a4d40e22355cd5ae.pdf [mishra2024fbz]
245. 714e21409b8c4f7788ac8c93795249a4e45e51ce.pdf [heidari2024d9k]
246. 2ef224f7a1f545eb8bfdf61c7b849f2676fe6a61.pdf [yu2023fqo]
247. bc99c855d52ba3d432c428fb4096b3a22c04f8bf.pdf [zhao2023pau]
248. 3dee43cea71d5988a72a914121f3455106f89cc7.pdf [pan20249k5]
249. 1871c0bc27bd5cd5f2f6c3d6cedeb4b8c5a89499.pdf [huan202345b]
250. c064efa0419b75ba131ec8470ed80f01e1a80f64.pdf [belal2023x1u]
251. b48a85980deb5f1baa64d862b9f0e4e62124e4de.pdf [li20238ti]
252. 8ec10ffe0c1fc8f6a92d541f0e002e78080b564a.pdf [huo2023e5h]
253. 769ba82ab77fc46db4a594dc64576286d02cf1d7.pdf [kim2023cvz]
254. d1255f6e7e7e14b253fe2245e842d0b45f5e715e.pdf [fan2023whi]
255. 5572237909914e23758115be6b8d7f99a8bd51dc.pdf [zhao2023rle]
256. 21c37fd6d00d6c727ec10f0c871c1ddf6e4f7ed9.pdf [xie20234ve]
257. e38e70580acb204c05096de8da90b7ab1d4bdb6b.pdf [li20233lv]
258. a246677a3d68e8cd7a63d24639a5cf3fc3b9f56e.pdf [ma2023qek]
259. 1397d1bcfeae68078756fdbc3ed7e62ef51a1e32.pdf [tanimola20246cv]
260. 52239e2338cd9f0e0e690203af8f0f1260ef7abf.pdf [chen2023xxw]
261. f996d5ee3b8ad3c60510862a92fd72c6a41777e0.pdf [ranjan20243bn]
262. f1b6f564e7cba8121df0eef0d656673e3bd18621.pdf [fu20232q3]
263. 12106888ec74baf48d1e9fb64e8932c9ce509e84.pdf [shi20235zy]
264. 3798e7f16fe69c29307a9bab4f0f4d779478afc5.pdf [deressa2023lrl]
265. 29a0077d198418bab2ea4d78d04a892ede860d68.pdf [aburass2023qpf]
266. ac9cc0c28838a037e77f4e19433de170f47b3de9.pdf [hassija2025wq3]
267. c4357abf10ff937e4ad62df4289fbbf74f114725.pdf [huang20238er]
268. 0b41c18d0397e14ddacee4143db74a05d774434d.pdf [liu20230kl]
269. f9e1a8754e77a4b5f240d11e9e81e2563a319b89.pdf [he20238sy]
270. 4b9568a9798c527779a1f7479ffb3ba373640e3e.pdf [guo2023dpo]
271. 34ca004eda791c8c4191e0ebe65cc4405a116b08.pdf [wang2023j6b]
272. 409b43b8cd8a2ba69f93e80c2bacc0126238b550.pdf [gopal20237ol]
273. dc5d707a8f9efaf32d2319ec2ca8d2619ef2926e.pdf [liu2023awp]
274. 9017053fb240d0870779b9658082488b392e7cde.pdf [fu20228zq]
275. f27221a15f4c3c8af7ea0ada5bab25831ffe21ea.pdf [sahoo20223yl]
276. 2d421d94bc9eed935870088a6f3218244e36dc97.pdf [ganz20249zr]
277. be1aabb6460d49905575da88d564864da9f80417.pdf [paal2024no4]
278. 0d7d27fbd8193acf8db032441fd22945d26e9952.pdf [hassan20243qi]
279. 0af7ccb9f6cf2edb35674dc97d0ec5d165f8433b.pdf [k2024wyx]
280. f9efe4918b9a20c2e8084d7a5ec624b87ec1f211.pdf [nguyen2024id9]
281. f62cb393ab51505bdd22823e74b7cf28f4801e1c.pdf [almohimeed2024jq1]
282. 4702a22a3c2da1284a88d5e608d38cd106d66736.pdf [hao202488z]
283. 9fcea59a7076064f5ac3949177307c1637473ffd.pdf [yao20244li]
284. 1ec9b653475287e95fdaef2f5247f82a8376c56c.pdf [dong20245zz]
285. 2b48f5f0769bf41b5a3025ace73417d19de65cf1.pdf [zhang2024jha]
286. bfb310434204d99fa5c420ac272966c5ae1c1bd2.pdf [boukhari2024gbb]
287. bbe5dfbecfd1bed7556b9c8269b0d363faa24973.pdf [song2025idg]
288. be28edb064e2050e0852ff376434ee39f7c4b0a3.pdf [zhou2024tps]
289. e25a0b06079966b8e43f8e1f2455913266cb7426.pdf [abbaoui20244wy]
290. ecd9598308161557d6ac35b3e4d32770489e811d.pdf [yang2024nyx]
291. 7dc4b2930870e66caa7ff23b5d447283a6171452.pdf [yang20241kf]
292. b1f0ac0b5e4044251dcdcce78e0046feb43cc7c7.pdf [hu202434n]
293. 903c8a82f5539b3e482efcb23ff59819f04f2c0f.pdf [yang20244dq]
294. 1e95bb5827dc784547a46058793c15effd74dccc.pdf [keresh20249rl]
295. 2104eea2b01b7a4b6ba2d256769328ac4919afad.pdf [hu20247km]
296. 8972ebf4f9ec47ee60c3ed0effd520daff495d5a.pdf [alsulami2024ffb]
297. 0fde8e3287ac04dcf210b73898c8ceb697ad489f.pdf [yang2024wxl]
298. 9a4718faa07a32cf1dce745062181d3342e9b054.pdf [p2024nbn]
299. 6e97c1ba023afc87c1b99881f631af8146230d96.pdf [wu2024tsm]
300. 1d7183d481ae5a396743dde39984f1f0c8f47edf.pdf [dong2024bm2]
301. 9fbf36da9ee60d56a7675042bb0b24798a3b966d.pdf [swapno2025y2b]
302. d80166681f3344a1946b8bfc623f4679d979ee10.pdf [yoo2024u1f]
303. 9285996627124b945ec601a763f6ff884bac3281.pdf [he2024m6j]
304. 3f002ea3ede9d97f1fe29b16691491219c1e626d.pdf [zhang202489a]
305. 9121dcd10df00e5cc51dc94400e0325e0ae47bb9.pdf [zhang2024pd6]
306. 08606a6a8b447909e714be2c3160074fdf1b91ad.pdf [dong20242ow]
307. 0222269c963f0902cc9eae6768a3c5948531488b.pdf [kayacan2024yy7]
308. 8776fd7934dc48df4663dadf30c6da665d84fb19.pdf [liu20248jh]
309. cc24f933b343b6a9701088cf6ae1dbf3299c0c9e.pdf [shi2024r44]
310. 88589b0b2d2d8caa09d8ca94414343455ae87d7c.pdf [xin2024ljt]
311. d629289cc2f3efbe374a66d57690dd84f19d59ec.pdf [zhou2024qty]
312. 70211e2b04fcb4413edfd43e75b9f10e8d130171.pdf [monjezi2024tdt]
313. cc817ba09f1c3c94bae73576463bcaf57c70261a.pdf [baek2025h8e]
314. 77e4ea46e7248408844f62fa3613aa246e35fdf0.pdf [payne2024u8l]
315. 271c6a87213b8b2a26ce3e1a6f20b07cc2f82b81.pdf [qi2024rzy]
316. 05d15576d88f9384738908f98716f91bdb5dbc78.pdf [mercier2024063]
317. 6feea99a1a6c876d6f4f714b463da9c95998dee7.pdf [sikkandar2024p0d]
318. c05744f690ab9db007012a63c3c5c3ca48201c66.pdf [hou2024e4y]
319. cebc29015a2827aa8a98ec39c90c93bca2f14848.pdf [nfor2025o20]
320. 630c9993b6aaf812e1aa3dae8e243af131abd3d0.pdf [xiang2024tww]
321. d14a1677e416bd7ac6fbe01672cd3152fc1f983d.pdf [tian20242kr]
322. 8ed42c0d60eb4c91e99ee184a17719f7c3ce3f3f.pdf [zhou2024r66]
323. c4cf58e104fc8d783dfd0bcf3ccd65f8e4028d7f.pdf [taye20244db]
324. 72e23cdc3accca1f09e2e19446bc475368c912d0.pdf [alohali2024xwz]
325. 7b6d64097d16219c043df64e4576bd7d87656073.pdf [gao20246ks]
326. 0820d2ac35cd55fd60f53c14460cca78fa996f1e.pdf [du2024s3t]
327. b02144ef4ed94df78544959bc97eddef4580dd95.pdf [tiwari2024jm9]
328. b6488fded5fb0728d7c3bd04f3afa3fba68d0450.pdf [du20248pd]
329. 24eba2f5345bc8279bb63f085e75cc26b37d0b1e.pdf [chaurasia2024tri]
330. 8ce6872b26f88e52aeed57f6f7528dee30b5f7b2.pdf [karagz2024ukp]
331. dcf77b8e23bbc56c5c8f76ddc5d3cf68d0a051be.pdf [lee2025r01]
332. f792dcf878e5bf8e7fb5aa31d231474d43462f4a.pdf [dmen2024cb9]
333. 310f5543603bef94d42366878a14161db1bf45de.pdf [ferdous2024f89]
334. f9f524944dd9d1fe4b8956f9a822927b2eec29ad.pdf [akan2024izq]
335. 6ec48cb5fa3d149c9cc8a29373e64b63fe257913.pdf [nahak20242mv]
336. 3c7e07304bab6a860dbbe4ff36a4d87010036d2a.pdf [han2024f96]
337. 819ae728828d50f56f234e35832b1222de081bfc.pdf [zhao2024p8o]
338. 6a9247fe471690218ef04cea5f6f4b59b3b50747.pdf [li2024qva]
339. f462bb00b8c4379c4a4699b66a19ce10da530b08.pdf [wang2024ueo]
340. 52fafbbf20a37ffe863f1fa1d44fde6b4fc3334e.pdf [qi2024f5d]
341. b4be4f2ecb8711755f75dcb09a57d8c9722ac349.pdf [zhu2024l2i]
342. 7492734c76036143baf574d6602bd45a348c416f.pdf [roy2024r9y]
343. eacb2581b480cf4a80bc601c8ea657651b41c0fc.pdf [wang2024w4u]
344. da7d671f61e53601bdfc760d336cc98e5d9c516b.pdf [pan202424q]
345. 2bb98b30fb9aae37858248cd5ac3221ebcf7eeba.pdf [du2024lml]
346. 90e9d0a35bcf7e8251c4c9601a224bd95db14dd9.pdf [luo202432g]
347. 5ce1ebe051a64969caeb879c44c45320a5ca8011.pdf [elnabi2025psy]
348. 3bba1b3376e43a39bf4f1bf4eab558758a37ea2e.pdf [ergn2025r6s]
349. 3b11c8774bbeddd6722d65455bf9bb729a63c1cb.pdf [mohsin2025gup]
350. 04ed3b0f4e21d9f4884bf4db98edf83a34d005a9.pdf [marcos2024oo2]
351. 23ce9c2814d6567efec884b7043977cefcb7602e.pdf [peng2024kal]
352. 5b22bdc6aedf13d812509dd0f768353eb1469a79.pdf [urrea20245k4]
353. d2fd0dc314023ebca1342f1379ba6d79c7ded84a.pdf [zhang2024b7v]
354. 1c96dc4ad4dfa3bf7c6ca9cf0c6197e7ac00272b.pdf [saleem20249yl]
355. c35c316feba84334a4b08feb5281a5f83db0b8b2.pdf [zhou2024toe]
356. c8174af99bc92d96935683beccc4161c65a8aa46.pdf [lijin2024mhk]
357. 05548c4b3db8be40fac236dfa8e21882fe9ce9de.pdf [huang2024htf]
358. bd7a4a9e020b594ebcacb886ddae8731a2885209.pdf [chen2024cha]
359. a7df70e86f049a86b1c555f9a399d3540f466be7.pdf [shahin2024o1c]
360. e4b0c4a1d7ee87ad666214172f329831ade4c25d.pdf [xu2024wux]
361. 6604a900b9a7404a447b2167892a947012a9ffb8.pdf [park2024d7y]
362. 9814001811c4924171787de52e01cc31446e2f97.pdf [elharrouss20252ng]
363. ab10aacab1a2672a154034c589dd0aa801912272.pdf [du2024i6n]
364. 325367f93439652efaa4bc6b50115bbb7371704e.pdf [guo2024o8u]
365. 3c6980902883f03c37332d34ead343e1229062b3.pdf [zhang2024g0m]
366. f2b1b0fb57cccaac51b44477726d510570c4c799.pdf [xu2025tku]
367. 2456506ed87faa667a0c2b8af4028a5a86a49650.pdf [li2024m4t]

## Literature Review

### Introduction

\section{Introduction}
\label{sec:introduction}



\subsection{Background: The Rise of Transformers in Computer Vision}
\label{sec:1_1_background:_the_rise_of_transformers_in_computer_vision}


The landscape of computer vision has recently undergone a profound paradigm shift, transitioning from the long-standing dominance of Convolutional Neural Networks (CNNs) to the ascendancy of Transformer architectures. This evolution marks a fundamental re-evaluation of how visual information is processed and understood, driven by the pursuit of more comprehensive and globally aware visual representations.

For several decades, CNNs served as the cornerstone of computer vision, achieving unparalleled success across a myriad of tasks, including image classification [resnet], object detection [fasterrcnn], and semantic segmentation [unet]. Their efficacy stemmed from inherent inductive biases that are particularly well-suited for processing grid-like image data. These biases include local receptive fields, which enable the extraction of fine-grained local features; weight sharing, which promotes parameter efficiency and translation equivariance; and hierarchical feature extraction, where deeper layers learn increasingly abstract representations. These characteristics allowed CNNs to learn robust visual patterns efficiently, even with moderately sized datasets, by effectively modeling local spatial correlations. However, a critical limitation of CNNs lies in their inherently local nature; capturing long-range dependencies and global contextual information often requires stacking numerous layers or employing complex pooling operations, which can be computationally intensive and may still struggle with truly global relationships across an entire image.

Concurrently, the Transformer architecture [attention] revolutionized the field of Natural Language Processing (NLP). Originally designed for sequence-to-sequence tasks, Transformers leverage a powerful self-attention mechanism that allows the model to weigh the importance of different elements within an input sequence, regardless of their position. This capability enables Transformers to effectively model long-range dependencies and capture global context across entire sentences or documents, leading to remarkable achievements in tasks such as machine translation, text generation [gpt], and language understanding [bert]. The success of Transformers in NLP highlighted their capacity for flexible, data-driven learning of relationships without strong prior assumptions about locality, a stark contrast to the fixed local kernels of CNNs.

The compelling rationale for extending these global context modeling capabilities to visual data was to overcome the aforementioned limitations of CNNs. Researchers hypothesized that a model capable of directly perceiving global relationships across an entire image, rather than building them up incrementally from local operations, could lead to a more holistic and powerful understanding of visual scenes. This intellectual leap aimed to unlock new avenues for visual representation learning, particularly for tasks requiring a broad contextual understanding.

This vision materialized with the groundbreaking introduction of the Vision Transformer (ViT) [vit]. ViT demonstrated that a standard Transformer encoder, directly applied to sequences of non-overlapping image patches, could achieve competitive performance in image classification, effectively bypassing convolutions entirely. The model treated images as sequences of flattened patches, linearly embedded them into a higher-dimensional space, and then processed these "visual tokens" using a conventional Transformer encoder. This seminal work fundamentally altered the perception of how visual data could be processed, proving that the self-attention mechanism, without explicit convolutional inductive biases, was powerful enough to learn visual representations. However, the initial ViT models faced significant challenges that underscored the differences between natural language and vision data. They were notably data-hungry, requiring massive pre-training datasets (e.g., JFT-300M) to outperform state-of-the-art CNNs, primarily due to their lack of inherent inductive biases for local features, which CNNs possess. Furthermore, ViTs suffered from high computational costs, particularly the quadratic complexity of global self-attention with respect to the number of image patches, making them inefficient for high-resolution images. They also inherently produced a single-scale feature map, limiting their direct applicability to dense prediction tasks like object detection and segmentation that typically require multi-scale feature hierarchies.

These initial limitations—data inefficiency, quadratic computational complexity, and a lack of inherent inductive biases for local features and multi-scale representation—became the primary drivers for the explosion of research that followed. The subsequent sections of this review will systematically explore the architectural innovations, training paradigms, and diverse applications that have defined the rapid evolution of Vision Transformers, detailing how the research community has sought to address these foundational challenges and unlock their full potential across the spectrum of computer vision tasks.
\subsection{Scope and Structure of the Review}
\label{sec:1_2_scope__and__structure_of_the_review}


This literature review offers a systematic and critical examination of Visual Transformers (ViTs), charting their rapid evolution and profound impact on the field of computer vision. Its primary objective is to serve as an essential roadmap, guiding the reader through the intellectual trajectory of ViTs from their foundational concepts to advanced architectural paradigms, diverse applications, and persistent challenges. The review aims to present a coherent narrative that emphasizes the evolution of methodologies, the intricate interplay between different architectural designs, and the continuous drive for innovation in visual understanding. A central theme woven throughout is the inherent tension between the expressive power and global receptive field of full self-attention versus the computational efficiency and inductive biases offered by more localized, hierarchical, or even non-attention-based designs.

The review is meticulously organized into seven main sections, each building upon the preceding one to offer a comprehensive understanding of the ViT landscape. This structure is designed to progressively unfold the complexities of ViT research, starting with fundamental principles and moving towards cutting-edge developments and future directions.

Section \ref{sec:introduction} (Introduction) establishes the foundational context for Visual Transformers, outlining the historical paradigm shift from Convolutional Neural Networks (CNNs) to Transformer architectures in computer vision. It delineates the core motivation for applying Transformers to images and sets the stage by detailing the scope and organizational framework of this comprehensive review.

Following this, Section \ref{sec:foundational_concepts} (Foundational Concepts of Vision Transformers) lays the essential groundwork. It details the core components of the Transformer architecture, elucidates the process of image tokenization and positional encoding, and introduces the seminal Vision Transformer (ViT) model, while also highlighting its initial limitations. This section provides the necessary theoretical understanding to appreciate subsequent advancements.

The subsequent sections delve into the field's rapid advancements and problem-solving efforts. Section \ref{sec:optimizing_early_vits} (Optimizing Early Vision Transformers: Data Efficiency and Stability) explores crucial early enhancements developed to mitigate ViT's initial data hunger through strategies like knowledge distillation and the emergence of self-supervised learning paradigms. It also examines architectural refinements that enabled deeper and more stable ViT models, alongside improved tokenization methods for better local feature capture, addressing the practical hurdles that limited early ViT adoption.

Section \ref{sec:scaling_vits} (Scaling ViTs: Hierarchical and Efficient Designs for General Vision Tasks) marks a critical phase in ViT development, focusing on architectural innovations that addressed the original ViT's quadratic computational complexity and lack of multi-scale feature representation. This section details the emergence of efficient attention mechanisms, such as window-based and shifted attention, and the development of hierarchical structures for multi-scale feature generation. These innovations transformed ViTs into versatile, general-purpose backbones capable of excelling in dense prediction tasks, thereby expanding their applicability beyond image classification.

A significant intellectual trajectory is explored in Section \ref{sec:beyond_pure_attention} (Beyond Pure Attention: Hybrid Designs and Alternative Token Mixers). This section delves into hybrid architectures that strategically integrate convolutional inductive biases, combining the strengths of CNNs and Transformers. It also examines models that rethink the necessity of complex self-attention mechanisms, proposing simpler, more efficient token mixing operations, and highlights a fascinating convergence where ViT design principles have influenced the modernization of traditional CNNs, blurring the lines between these once distinct paradigms.

Section \ref{sec:vits_in_action} (ViTs in Action: Diverse Applications and Specialized Adaptations) showcases the broad applicability and versatility of ViTs across various computer vision tasks and specialized domains. It details their successful integration into frameworks for fundamental tasks like object detection and semantic segmentation, their influence in critical areas such as medical image analysis, and efforts to develop lightweight and real-time ViT systems for deployment on resource-constrained devices.

Finally, Section \ref{sec:synthesizing_the_landscape} (Synthesizing the Landscape: Challenges and Future Trajectories) provides a comprehensive summary of key developments, critically discusses remaining open challenges (e.g., computational costs, data efficiency, interpretability), and explores promising emerging trends. This includes the development of large-scale foundation models and the pursuit of generalist Vision Transformers, alongside crucial ethical considerations associated with these powerful vision AI technologies.

Throughout these sections, the review provides a structured exploration of this dynamic landscape, offering a deep critical evaluation of the methodologies, comparative analysis of approaches, and a discussion of the underlying reasons for existing limitations. By tracing the continuous evolution and addressing the persistent challenges, this review aims to guide future research and foster responsible innovation in visual understanding.


### Foundational Concepts of Vision Transformers

\section{Foundational Concepts of Vision Transformers}
\label{sec:foundational_concepts_of_vision_transformers}



\subsection{The Transformer Architecture Revisited}
\label{sec:2_1_the_transformer_architecture_revisited}


The Transformer architecture, initially introduced by Vaswani et al. [vaswani2017attention] for sequence-to-sequence tasks in natural language processing (NLP), marked a profound paradigm shift by entirely eschewing recurrent and convolutional layers in favor of a purely attention-based mechanism. This design choice enabled unprecedented parallelism and the capture of long-range dependencies, fundamentally altering how sequential data is processed. Grasping these foundational elements is crucial for understanding how Transformers operate on sequential data and how these principles were subsequently adapted for visual inputs, forming the bedrock of Vision Transformers (ViTs).

At its core, the original Transformer operates on an encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations, while the decoder generates an output sequence one symbol at a time, attending to the encoded representations. For many vision applications, particularly image classification, the architecture primarily leverages the encoder stack. The fundamental building block within both the encoder and decoder is the multi-head self-attention (MHSA) mechanism. This mechanism enables the model to dynamically weigh the importance of different input elements by computing attention scores. It achieves this by projecting input representations into three distinct learned linear transformations: Query ($\mathbf{Q}$), Key ($\mathbf{K}$), and Value ($\mathbf{V}$). The attention score for each query is calculated against all keys, determining how much focus to place on each value, thereby capturing global contextual information and long-range dependencies within the sequence. Mathematically, the scaled dot-product attention is computed as $\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{QK}^T}{\sqrt{d_k}})\mathbf{V}$, where $d_k$ is the dimension of the keys. The "multi-head" aspect allows the model to jointly attend to information from different representation subspaces at different positions, enriching the learned relationships and providing a more comprehensive understanding of the input.

Following the self-attention layer, a position-wise feed-forward network (FFN) processes each position independently. This FFN typically consists of two linear transformations with a ReLU activation in between, applying a simple two-layer neural network to the attention outputs. To facilitate the training of deep networks and prevent issues like vanishing gradients, both the self-attention and FFN sub-layers are augmented with residual connections [he2016deep], which add the input of the sub-layer to its output, and layer normalization [ba2016layer], which normalizes the activations across the feature dimension for each sample. Furthermore, as the self-attention mechanism itself is permutation-invariant (i.e., it treats the input as a set rather than a sequence), positional encodings are added to the input embeddings. These encodings inject crucial information about the relative or absolute position of tokens in the sequence, allowing the model to understand sequence order.

The conceptual leap to Vision Transformers involved adapting these foundational principles to visual inputs. The seminal approach, as introduced by the original Vision Transformer (ViT) [dosovitskiy2020image], involves segmenting an input image into a sequence of fixed-size, non-overlapping patches. Each patch is then linearly embedded into a higher-dimensional space to create visual tokens, analogous to words in NLP. These visual tokens, along with positional encodings, are then fed into a standard Transformer encoder. This direct adaptation demonstrated the power of global self-attention for image classification, achieving competitive performance on large-scale datasets.

However, this direct application of the NLP-centric Transformer to images also exposed inherent challenges and limitations stemming from the architecture's original design choices. Firstly, the quadratic computational complexity of global self-attention with respect to the sequence length (number of patches) becomes a significant bottleneck for high-resolution images, making it computationally expensive and memory-intensive. Secondly, unlike Convolutional Neural Networks (CNNs) which possess strong inductive biases for local feature extraction, translation equivariance, and hierarchical processing, the pure Transformer architecture inherently lacks these spatial priors. This deficiency means that ViTs often require massive pre-training datasets to learn robust visual representations from scratch, a limitation not typically shared by CNNs. The reliance on explicit positional encodings, rather than inherent spatial understanding, further underscores this difference. These challenges, inherent in the direct adaptation of the foundational Transformer to the visual domain, became the primary drivers for subsequent research, motivating a wave of innovations aimed at enhancing efficiency, introducing multi-scale processing, and integrating convolutional inductive biases, which are explored in detail in later sections of this review.
\subsection{Image Tokenization and Positional Encoding}
\label{sec:2_2_image_tokenization__and__positional_encoding}

The successful adaptation of Transformer architectures from natural language processing (NLP) to computer vision (CV) fundamentally relies on two critical initial processing steps: image tokenization and positional encoding [han2020yk0, hassija2025wq3]. Unlike Convolutional Neural Networks (CNNs) that inherently leverage inductive biases for spatial locality and translation equivariance, Transformers process input sequences without an intrinsic understanding of spatial relationships or the original arrangement of visual elements. This inherent permutation-invariance of the self-attention mechanism [Vaswani2017Attention] necessitates dedicated mechanisms to transform raw image data into a sequential format and explicitly embed spatial awareness.

The foundational Vision Transformer (ViT) [ViT] introduced a straightforward, yet impactful, tokenization strategy. An input image is first partitioned into a grid of fixed-size, non-overlapping square patches, typically $16 \times 16$ pixels. Each patch is then flattened into a 1D vector and linearly projected into a higher-dimensional embedding space. This process converts the 2D image into a sequence of fixed-length visual tokens, analogous to word embeddings in NLP. A special "class token" (CLS token) is prepended to this sequence, serving as a global representation for classification tasks, similar to its use in BERT [Vaswani2017Attention]. While effective for demonstrating the viability of Transformers in vision, this simple patch embedding has inherent limitations. It treats each patch as an independent entity, largely disregarding fine-grained local structural information and spatial relationships *within* the patch [han2020yk0]. This can be particularly problematic for tasks requiring detailed local feature understanding or when dealing with smaller datasets where such inductive biases are more crucial. The fixed size of these patches also means that the model's receptive field is determined by the patch size, and changing input image resolutions would alter the number of tokens, requiring adaptations or re-training. This initial simplification, which diminishes the capture of local context, became a significant motivation for subsequent research into more sophisticated tokenization methods, such as those incorporating convolutional inductive biases or hierarchical structures, which will be explored in Section 3.3 and Section 5.1.

Complementing tokenization, positional encoding is indispensable for Transformers to comprehend the spatial arrangement of these visual tokens, as the self-attention mechanism itself is permutation-invariant [Vaswani2017Attention]. Without positional information, the model would process an image's patches identically regardless of their original location, losing crucial spatial context. The original Transformer [Vaswani2017Attention] (in NLP) typically employed fixed sinusoidal positional encodings, which are deterministic functions of position. In contrast, the ViT [ViT] opted for learnable 1D positional embeddings. These embeddings are simply vectors, of the same dimension as the patch embeddings, that are added to each visual token based on its sequential index (corresponding to its spatial position in the flattened grid). The rationale behind learnable embeddings was their flexibility to adapt to the specific spatial patterns present in image data during training. It is critical to note that the original ViT paper, despite exploring 2D-aware and relative positional embeddings, found that simple 1D learnable embeddings performed comparably well. This unexpected efficacy was largely attributed to the ViT's extensive pre-training on massive datasets like JFT-300M, which allowed the model to implicitly learn complex spatial relationships and compensate for the lack of stronger explicit inductive biases [hong2022ks6].

However, a significant drawback of these fixed-size learnable embeddings is their lack of transferability to input images of different resolutions. If an image has a different number of patches than what the model was trained on, the pre-trained positional embeddings become incompatible, necessitating interpolation or re-training, which limits generalization [wang2022gq4]. This rigidity in handling varying input sizes posed a challenge for deploying ViTs in diverse real-world scenarios where image resolutions can vary widely. This limitation spurred research into more flexible positional encoding mechanisms, including relative positional encodings and dynamic methods, which are crucial for enabling ViTs to scale effectively to different input dimensions and will be discussed in Section 4.1.

In summary, the initial strategies of simple patch-based tokenization and learnable 1D absolute positional embeddings in the original ViT were foundational in demonstrating the power of Transformers for vision tasks. These mechanisms were the necessary "glue" to adapt a sequence-processing architecture to 2D image data. However, their inherent limitations—the diminished capture of fine-grained local structural information and the inflexibility of absolute positional embeddings to varying input resolutions—became primary drivers for the extensive architectural innovations and optimizations explored in subsequent sections. These foundational concepts, along with their identified limitations, set the stage for the full Vision Transformer architecture, which is detailed next.
\subsection{The Original Vision Transformer (ViT)}
\label{sec:2_3_the_original_vision_transformer_(vit)}


The introduction of the Vision Transformer (ViT) by [Dosovitskiy2020] represented a watershed moment in computer vision, fundamentally challenging the long-standing dominance of Convolutional Neural Networks (CNNs). This seminal work demonstrated that a pure Transformer architecture, previously a cornerstone of natural language processing, could be effectively adapted to process visual data, achieving state-of-the-art performance on image recognition tasks. This subsection details the architectural principles of the original ViT, its groundbreaking achievements, and critically examines the inherent limitations that subsequently spurred extensive research and innovation in the field.

The core idea behind the Vision Transformer is to treat an image as a sequence of flattened patches, analogous to how a Transformer processes a sequence of words in text. Specifically, an input image is first partitioned into a grid of non-overlapping, fixed-size square patches, typically $16 \times 16$ pixels. Each of these patches is then flattened into a 1D vector and linearly projected into a higher-dimensional embedding space, forming a sequence of "visual tokens." To capture global image representations, a learnable "class token" is prepended to this sequence, whose final state after Transformer processing serves as the aggregated image representation for classification. Crucially, since Transformers are inherently permutation-invariant and thus lack intrinsic knowledge of spatial relationships, learnable positional embeddings are added to the patch embeddings to encode their original spatial arrangement within the image. This augmented sequence of tokens is then fed into a standard Transformer encoder, which consists of multiple identical layers. Each layer comprises a multi-head self-attention (MSA) block and a multi-layer perceptron (MLP) block, interspersed with residual connections and layer normalization. The MSA mechanism allows the model to weigh the importance of all other patches when processing a given patch, thereby capturing global dependencies and long-range interactions across the entire image. Finally, an MLP head attached to the output of the class token performs the image classification [Dosovitskiy2020, han2020yk0].

The groundbreaking achievement of the original ViT was its ability to match or even surpass the performance of state-of-the-art CNNs on large-scale image recognition benchmarks, including ImageNet, ImageNet-21k, and JFT-300M. This success was particularly evident when ViT models were pre-trained on massive datasets, demonstrating the Transformer's immense capacity for learning rich visual representations from vast amounts of data. This marked a significant paradigm shift, proving that the global context modeling capabilities of Transformers could be highly effective for visual understanding, moving beyond the local, hierarchical feature extraction paradigm of CNNs [Dosovitskiy2020].

Despite its impressive performance, the original ViT architecture faced several critical limitations that became the primary drivers for subsequent research efforts. One of the most significant challenges was its heavy reliance on massive pre-training datasets to achieve competitive performance, often requiring orders of magnitude more data than CNNs to generalize effectively. This "data hunger" stems from the Transformer's inherent lack of inductive biases for local features, such as translation equivariance and locality, which are naturally embedded in the design of CNNs through their convolutional filters. Consequently, ViT had to learn these fundamental visual priors from scratch, necessitating vast amounts of data to implicitly acquire them [Dosovitskiy2020, han2020yk0]. This limitation presented a major barrier to its widespread adoption, especially for tasks or domains where large labeled datasets are unavailable. Addressing this data dependency became a central focus, leading to the development of data-efficient training strategies, including knowledge distillation and self-supervised learning paradigms, which are explored in detail in Section 3.1 and Section 3.4, respectively.

Another critical limitation of the original ViT was its high computational cost, primarily due to the global self-attention mechanism. The self-attention operation computes relationships between every pair of input tokens, resulting in a computational complexity that scales quadratically with the number of image patches (tokens). For high-resolution images, this quadratic scaling translates into prohibitive computational expense and memory consumption, making real-time inference and deployment on resource-constrained devices challenging [han2020yk0]. This inefficiency motivated extensive research into developing more computationally efficient attention mechanisms and hierarchical architectures, which are discussed in Section 4.1.

Furthermore, the pure patch-based tokenization and global attention of the original ViT meant it lacked the inherent inductive biases for local feature extraction and hierarchical representation learning that are characteristic of CNNs. While effective for capturing global context, this design choice often resulted in ViT struggling to capture fine-grained local details and build multi-scale feature maps crucial for dense prediction tasks like object detection and semantic segmentation. This limitation meant that ViT's performance could be suboptimal on smaller datasets or tasks requiring precise localization, where CNNs traditionally excelled. This challenge spurred innovations in improved tokenization strategies (Section 3.3) and the integration of convolutional inductive biases through hybrid architectures (Section 5.1).

In conclusion, the original Vision Transformer by [Dosovitskiy2020] undeniably revolutionized computer vision by demonstrating the power of pure Transformer architectures for image recognition. Its ability to achieve state-of-the-art performance, particularly when pre-trained on massive datasets, marked a significant shift in the field. However, its initial form presented notable limitations: a heavy reliance on extensive pre-training data due to a lack of inherent inductive biases, high computational costs stemming from global self-attention, and a struggle to capture fine-grained local features. These challenges were not merely drawbacks but became the crucial impetus for extensive subsequent research, driving the development of data-efficient training strategies, architectural innovations for efficiency and multi-scale representation, and the strategic integration of convolutional priors, all of which form the organizational basis for the subsequent sections of this review.


### Optimizing Early Vision Transformers: Data Efficiency and Stability

\section{Optimizing Early Vision Transformers: Data Efficiency and Stability}
\label{sec:optimizing_early_vision_transformers:_data_efficiency__and__stability}



\subsection{Data-Efficient Training and Distillation}
\label{sec:3_1_data-efficient_training__and__distillation}


The introduction of the Vision Transformer (ViT) by [dosovitskiy2020image] marked a pivotal moment in computer vision, demonstrating the potential of pure self-attention mechanisms for image recognition. However, a significant hurdle to its widespread adoption was its substantial data hunger, often necessitating pre-training on colossal proprietary datasets like JFT-300M to achieve performance competitive with state-of-the-art Convolutional Neural Networks (CNNs). This reliance on massive labeled data posed a critical barrier for researchers and practitioners without access to such resources, making data-efficient training strategies a paramount area of early research.

A foundational breakthrough in addressing ViT's data dependency was the work by Touvron et al. [touvron2021training], which introduced Data-efficient image Transformers (DeiT). This seminal paper demonstrated that ViTs could achieve competitive performance on standard datasets like ImageNet-1K without requiring extensive pre-training on external, larger datasets. The core innovation of DeiT was the effective application of knowledge distillation, a technique where a smaller "student" model learns from the output of a larger, pre-trained "teacher" model. Specifically, DeiT leveraged a strong CNN teacher (e.g., a RegNet model) to guide the training of a ViT student. The authors explored both "soft" distillation, where the student matches the teacher's softened probability distribution, and "hard" distillation, where the student is trained to predict the teacher's hard class labels. Crucially, DeiT introduced a dedicated "distillation token" alongside the standard class token, allowing the ViT student to learn directly from the teacher's predictions through an additional distillation loss. This mechanism significantly improved the student ViT's performance, enabling it to reach accuracy levels comparable to or even surpassing the teacher CNN, all while being trained solely on ImageNet-1K. This innovation was instrumental in democratizing ViT research, making the architecture accessible to a broader community.

Beyond the initial success of DeiT, subsequent research has further refined and extended distillation techniques for ViTs, often integrating them with other compression or efficiency strategies. For instance, while DeiT primarily focused on distilling class logits, [wang2022pee] proposed Attention Distillation (AttnDistill) specifically for self-supervised Vision Transformer students. They argued that directly distilling information from the teacher's crucial attention mechanism to the student could significantly narrow the performance gap, especially in self-supervised learning (SSL) contexts where traditional logit-based distillation might be suboptimal. AttnDistill demonstrated superior k-NN accuracy on ImageNet-1K compared to existing self-supervised knowledge distillation methods, highlighting the importance of attention-level guidance for ViT students. This approach is particularly valuable as it is independent of the specific SSL algorithm used, offering a versatile method to improve ViT performance on memory and compute-constrained devices.

Knowledge distillation has also been integrated into broader model compression frameworks to enhance ViT efficiency and performance under resource constraints. [yu2022iy0] introduced a Unified Visual Transformer Compression (UVC) framework that seamlessly combines pruning, layer skipping, and knowledge distillation. By formulating a budget-constrained, end-to-end optimization framework that jointly learns model weights, pruning ratios, and skip configurations under a distillation loss, UVC effectively compresses ViT variants like DeiT and T2T-ViT. This demonstrates that distillation is not merely a standalone data-efficiency technique but a powerful component in a multi-faceted approach to creating more practical and deployable ViTs. Similarly, [li2022tl7] explored the challenging domain of fully quantized low-bit Vision Transformers (Q-ViT). They identified information distortion in low-bit quantized self-attention maps as a bottleneck and proposed a distribution-guided distillation (DGD) scheme to rectify this. Their Q-ViT models, leveraging DGD, achieved remarkable performance, even surpassing full-precision counterparts in some cases, while drastically reducing computational and memory costs. This illustrates how distillation can mitigate the performance degradation associated with extreme quantization, thereby making ViTs more viable for edge devices and further reducing their effective data-to-performance ratio by enabling smaller, more efficient models.

In summary, the initial "data hunger" of Vision Transformers was a significant impediment, but early efforts, particularly through knowledge distillation, provided crucial solutions. The advent of DeiT [touvron2021training] demonstrated that ViTs could achieve strong performance on standard datasets by learning from pre-trained CNN teachers, thereby mitigating the need for massive proprietary datasets. Subsequent advancements have extended distillation beyond simple logit matching, exploring attention-based distillation for self-supervised learning [wang2022pee] and integrating it into unified compression frameworks [yu2022iy0] and quantization schemes [li2022tl7]. These innovations collectively transformed ViTs from computationally demanding and data-intensive models into more accessible and practical architectures, laying the groundwork for their broader application across diverse computer vision tasks. While architectural advancements like the Swin Transformer (discussed in Section 4.1) contributed to computational efficiency, it is these dedicated data-efficient training strategies, primarily knowledge distillation, that directly addressed the challenge of learning powerful representations from substantially less labeled data.
\subsection{Deeper and More Stable ViT Architectures}
\label{sec:3_2_deeper__and__more_stable_vit_architectures}

The initial success of Vision Transformers (ViTs) was often constrained by the inherent challenges of training very deep Transformer models, such as vanishing gradients, exploding activations, and general training instability. Overcoming these hurdles was crucial to unlock the full representational capacity of ViTs and enable the development of much deeper and more complex architectures. This subsection examines the architectural and training advancements that directly addressed these stability issues, facilitating the scaling of ViTs to unprecedented depths.

The foundational work by [ViT] demonstrated the potential of a pure Transformer encoder for image recognition, but implicitly highlighted the difficulties in training such models from scratch, particularly at greater depths, often necessitating massive pre-training datasets. To directly tackle the instability encountered when scaling ViTs, [CaiT] introduced several pivotal innovations. Their work, titled "Going deeper with Image Transformers," specifically focused on enabling the training of significantly deeper ViTs. A key contribution was **LayerScale**, a mechanism that adaptively re-scales residual connections within each Transformer block. This adaptive re-scaling helps to stabilize the training process by preventing activations from growing too large or vanishing too quickly, thereby mitigating gradient issues and allowing for the successful optimization of models with many more layers. Furthermore, [CaiT] proposed **class-attention layers**, which refine the global representation by allowing the class token to interact with image tokens in a more nuanced way, further enhancing the model's ability to learn robust features in deep architectures. These techniques were instrumental in pushing ViT depth beyond previous limits, demonstrating the viability of very deep Transformer-based vision models.

Building upon these foundational insights, other works contributed to the overall robustness and scalability of ViTs, indirectly supporting the development of deeper models. For instance, [DeiT] addressed the significant data requirements of early ViTs through knowledge distillation. While not directly an architectural stabilization technique, making ViTs trainable on smaller datasets broadened their applicability and made the exploration of deeper models more practical by reducing the computational burden of pre-training on colossal datasets. Similarly, advancements in initial tokenization by [T2T-ViT] and more flexible positional encodings by [CPVT] provided better initial representations and adaptability, which can contribute to the overall stability and performance of deeper networks.

The principles established by [CaiT] for stabilizing deep Transformers have had a lasting impact, influencing the design of subsequent, more complex ViT variants. For example, hierarchical Vision Transformers like the Swin Transformer [Swin] and Pyramid Vision Transformer [PVT] introduced architectural modifications to handle multi-scale features and reduce quadratic complexity, but their ability to scale to significant depths also relies on robust training practices and implicitly benefits from the understanding of stability mechanisms. More recent hierarchical designs, such as Hiera [ryali202339q] and HiViT [zhang2022msa], continue to explore efficient and deep architectures, often leveraging strong self-supervised pre-training (like Masked Autoencoders) to simplify architectural "bells-and-whistles" while maintaining performance. These models demonstrate a continued evolution, where the initial focus on stabilizing the core Transformer block has matured into designing entire deep, efficient architectures that are robust to training.

In conclusion, the journey towards deeper and more stable ViT architectures has been marked by critical innovations that directly address the inherent challenges of training very deep Transformers. Techniques like LayerScale have been instrumental in stabilizing the optimization landscape, allowing ViTs to scale to unprecedented depths and unlock greater representational power. While the field continues to explore various architectural modifications and training paradigms, the fundamental understanding of how to maintain training stability remains a cornerstone for pushing the boundaries of ViT performance and enabling the development of even larger and more complex models for diverse computer vision tasks. The ongoing challenge lies in balancing this architectural depth and representational capacity with computational efficiency and the continued search for robust, generalizable training methodologies.
\subsection{Improved Tokenization and Local Feature Capture}
\label{sec:3_3_improved_tokenization__and__local_feature_capture}

The foundational Vision Transformer (ViT) [Dosovitskiy2020] introduced a paradigm shift in computer vision by treating images as sequences of non-overlapping patches, processed by a standard Transformer encoder. While this approach effectively captured global dependencies, it inherently lacked the inductive biases for local structural information that are central to Convolutional Neural Networks (CNNs). This deficiency meant vanilla ViTs often required immense pre-training datasets to learn robust local features, limiting their performance on smaller datasets and tasks demanding fine-grained visual understanding. Consequently, a significant line of research has focused on enhancing ViT's ability to capture local features from the very initial stages of processing, specifically by refining the image tokenization process and embedding local inductive biases into the patch embedding layers. The goal is to enable ViTs to learn fine-grained local features more effectively from scratch, thereby reducing their reliance on massive pre-training datasets and improving performance on tasks requiring detailed visual understanding.

One direct approach to enrich the initial token representation is through progressive token structuring. Tokens-to-Token ViT (T2T-ViT) [Yuan2021] exemplifies this by introducing a "Tokens-to-Token" module that iteratively aggregates adjacent tokens into a single, more context-aware token. Instead of a single linear projection of raw, non-overlapping patches, T2T-ViT employs multiple stages where small patches are first linearly embedded, and then these initial tokens are progressively merged and re-embedded into a longer sequence. This hierarchical aggregation process allows the model to capture increasingly larger receptive fields and finer-grained local details before the main Transformer encoder processes the final token sequence. This refinement significantly improves the ViT's capacity to learn robust visual representations from scratch on datasets like ImageNet, thereby reducing its heavy reliance on massive pre-training. The progressive nature of T2T-ViT effectively builds a local hierarchy of features, mimicking some aspects of CNNs' early layers, but within a token-merging framework.

Beyond progressive merging, another critical direction involves embedding convolutional inductive biases directly into the initial patch embedding layer, often referred to as a "convolutional stem." This strategy leverages the inherent strengths of convolutions—locality, translation equivariance, and hierarchical feature extraction—at the very outset of the ViT pipeline. For instance, the Convolutional Vision Transformer (CvT) [Li2021] replaces the standard linear projection for patch embedding with a series of convolutional layers. This not only generates visual tokens but also inherently incorporates local feature extraction and spatial downsampling, making the initial tokens more semantically rich and spatially aware. CvT further extends this idea by using convolutional projections within its attention layers, but the initial convolutional token embedding is a key aspect of local feature capture. Similarly, CeiT (Convolution-enhanced image Transformer) [d_Ascoli2021] employs a convolutional stem to generate initial tokens, aiming to improve local feature extraction and reduce the number of parameters compared to a pure linear projection. These convolutional stems provide a more robust and efficient way to extract initial features, making ViTs more data-efficient and better suited for tasks requiring detailed local information, even with limited pre-training. The explicit integration of convolutions at this early stage directly addresses the ViT's lack of inherent local inductive biases, leading to improved performance, especially on smaller datasets.

Furthermore, the design of the initial patching mechanism itself has been revisited to enhance local context. The original ViT uses non-overlapping patches, which can lead to information loss at patch boundaries and a limited receptive field for individual tokens. To mitigate this, some approaches have introduced overlapping patch embeddings. By allowing patches to overlap, the model gains a denser sampling of the image and each token can incorporate information from its immediate neighbors, fostering a stronger sense of local connectivity. This simple yet effective modification can significantly contribute to improved local feature capture and robustness. For example, ViT-Lite [Wu2021] explicitly incorporates overlapping patch embeddings to enhance the model's ability to perceive local structures and improve performance on various vision tasks without significantly increasing computational overhead. This approach is particularly beneficial for tasks where fine-grained spatial details are crucial, as it ensures that local contextual information is preserved and propagated more effectively into the Transformer layers.

In summary, the evolution of Vision Transformers has seen a dedicated focus on overcoming the original ViT's weakness in capturing fine-grained local structural information. Innovations at the initial tokenization stage, such as the progressive token merging in T2T-ViT [Yuan2021], the integration of convolutional stems in models like CvT [Li2021] and CeiT [d_Ascoli2021], and the use of overlapping patch embeddings exemplified by approaches like ViT-Lite [Wu2021], have significantly enhanced the ViT's ability to learn rich local features. These advancements directly address the challenge of data efficiency and improve performance on tasks requiring detailed visual understanding by embedding crucial inductive biases early in the processing pipeline. By refining how images are initially converted into tokens, these methods bridge the gap between global reasoning and local inductive biases, laying a stronger foundation for the subsequent Transformer layers to build upon. This early-stage enhancement is distinct from, yet complementary to, later architectural developments that introduce hierarchical structures or hybrid designs, which are explored in subsequent sections.
\subsection{Self-Supervised Pre-training Paradigms}
\label{sec:3_4_self-supervised_pre-training_paradigms}

The initial success of Vision Transformers (ViTs) was often contingent on vast amounts of labeled data, posing a significant challenge for their widespread adoption. Self-supervised learning (SSL) has emerged as a pivotal solution, enabling ViTs to learn robust and transferable visual representations from unlabeled data, thereby mitigating the need for extensive human annotation and enhancing their data efficiency. This paradigm shift draws heavily from the success of masked language modeling in Natural Language Processing (NLP), adapting similar principles to the visual domain.

One prominent SSL paradigm for ViTs is self-distillation, exemplified by methods like DINO ([caron2021emerging]). DINO trains a student network to match the output of a teacher network, which is typically a momentum-updated version of the student. This approach allows ViTs to learn powerful semantic features, including remarkable emergent properties such as object segmentation, without relying on explicit labels. The core idea is to encourage the student to produce similar representations to the teacher for different views of the same image, fostering a rich understanding of visual semantics.

Inspired by BERT's masked language modeling, masked image modeling (MIM) has become another cornerstone of SSL for ViTs. BEiT ([bao2022beit]) pioneered this direction by pre-training ViTs to reconstruct discrete visual tokens from masked image patches. This method effectively translates the "fill-in-the-blanks" task from text to images, forcing the model to learn meaningful contextual representations. Building upon this, Masked Autoencoders (MAE) ([he2022masked]) introduced a highly scalable and efficient masked autoencoding approach. MAE masks a large portion of image patches (e.g., 75\%) and trains the ViT encoder to reconstruct the missing pixels from the visible patches, demonstrating impressive performance and scalability. Unlike BEiT, MAE reconstructs raw pixel values, simplifying the pre-training objective and allowing for very high masking ratios, which pushes the model to learn more holistic representations. Both BEiT and MAE significantly reduce the data annotation burden by leveraging the inherent structure of images as a supervisory signal.

The power of SSL pre-training, particularly MAE, has been instrumental in unlocking ViT's full potential across various downstream tasks and architectural designs. For instance, the effectiveness of plain, non-hierarchical ViT backbones for object detection was significantly enhanced when pre-trained with MAE. ViTDet ([li2022raj]) demonstrated that with minimal adaptations, a plain ViT backbone pre-trained as a Masked Autoencoder could achieve competitive results on object detection, even without the complex hierarchical designs typically employed for dense prediction tasks. This highlights how strong self-supervised pre-training can compensate for architectural inductive biases, making ViTs more versatile.

Furthermore, SSL has enabled simplifications in ViT architectures while maintaining high performance. Hiera ([ryali202339q]) showed that by leveraging robust pre-training with MAE, many "bells-and-whistles" (vision-specific components) could be stripped from state-of-the-art multi-stage hierarchical ViTs without sacrificing accuracy. This resulted in simpler, faster hierarchical ViTs that are more efficient during both inference and training, underscoring the profound impact of SSL in streamlining architectural complexity. These advancements collectively demonstrate how SSL has transformed ViT training, enabling them to learn rich, transferable visual representations from vast amounts of unlabeled data, thus significantly mitigating the need for extensive human annotation and improving performance across various downstream tasks.

Despite the remarkable progress, challenges remain in understanding the full extent of emergent properties from different SSL objectives and in developing even more computationally efficient pre-training methods for increasingly larger models. Future research directions may explore multimodal SSL for ViTs, further reducing the gap between pre-training and fine-tuning, and developing theoretical frameworks to better explain the representational power gained through self-supervision.


### Scaling ViTs: Hierarchical and Efficient Designs for General Vision Tasks

\section{Scaling ViTs: Hierarchical and Efficient Designs for General Vision Tasks}
\label{sec:scaling_vits:_hierarchical__and__efficient_designs_for_general_vision_tasks}



\subsection{Window-Based and Shifted Attention}
\label{sec:4_1_window-based__and__shifted_attention}


The initial success of Vision Transformers (ViTs) [ViT] was tempered by the quadratic computational complexity of their global self-attention mechanism with respect to image resolution, posing a significant bottleneck for high-resolution images and dense prediction tasks. To overcome this, architectural innovations focused on introducing localized attention mechanisms, where self-attention is computed only within restricted spatial regions. This approach reduces complexity to linear with respect to image size, making ViTs computationally efficient and scalable.

A foundational step in this direction was the adoption of window-based attention, where the input image is partitioned into non-overlapping local windows, and self-attention is computed independently within each window. While this effectively reduces computational cost, it inherently limits the receptive field and prevents information exchange between different windows. The pivotal innovation to address this limitation while maintaining computational efficiency was introduced by the Swin Transformer [Swin]. Swin Transformer proposed a hierarchical architecture that leverages a novel shifted window attention mechanism. In this design, attention is computed within non-overlapping windows in one layer, and then in the subsequent layer, the windows are shifted, allowing for connections between previously isolated regions. This alternating pattern of regular and shifted windowing enables cross-window connections and builds multi-scale feature representations, crucial for tasks requiring dense predictions, without incurring the quadratic cost of global attention. The Swin Transformer quickly became a new standard backbone, demonstrating superior performance across various computer vision tasks, including image classification, object detection, and semantic segmentation.

The hierarchical and window-based design of Swin Transformer has shown enhanced transferability compared to plain ViTs, often requiring less extensive fine-tuning for downstream tasks [zhou2021rtn]. This adaptability has been leveraged in diverse applications, showcasing its versatility. For instance, the Swin Transformer architecture has been successfully adapted for medical image analysis, such as in SwinCross [li20233lv], which employs a cross-modal 3D Swin Transformer with cross-modal shifted window attention for head-and-neck tumor segmentation in PET/CT images, demonstrating its efficacy in integrating multi-modal data. Similarly, its robust performance has been validated in specialized classification tasks, achieving higher accuracy than traditional CNNs and plain ViTs for mobile-based oral cancer image classification [song2024c99] and outperforming other object detection models for chicken part classification and detection [peng2024kal].

Despite the effectiveness of window-based architectures, their local nature presented new challenges, particularly concerning pre-training strategies like Masked Autoencoders (MAE). The random masking inherent in MAE pre-training can be difficult to reconcile with pyramid-based ViTs that rely on local window operators. To address this, Uniform Masking (UM) was proposed by [li2022ow4], enabling efficient and effective MAE pre-training for pyramid-based ViTs with locality. UM ensures uniform sampling across local windows and introduces secondary masking to enhance semantic learning, significantly improving pre-training efficiency and maintaining competitive fine-tuning performance.

While shifted windows are a powerful mechanism for cross-window communication, recent research has explored alternatives and questioned their absolute necessity under certain conditions. For example, [li2022raj] investigated plain, non-hierarchical ViT backbones for object detection and found that competitive results could be achieved using simple window attention (without shifting) when augmented with very few cross-window propagation blocks. This suggests that explicit shifting might be less critical if other sparse mechanisms are introduced to facilitate information flow between windows. Furthermore, the Hiera model [ryali202339q] demonstrated that by leveraging strong visual pretext tasks like MAE pre-training, many "bells-and-whistles" (complex vision-specific components) could be stripped from hierarchical ViTs without sacrificing accuracy. This indicates a potential synergy where powerful self-supervised pre-training can simplify architectural designs, potentially reducing the reliance on intricate windowing strategies for optimal performance.

In conclusion, window-based and particularly shifted attention mechanisms, pioneered by the Swin Transformer, were instrumental in making Vision Transformers computationally efficient and scalable for high-resolution images and dense prediction tasks. These innovations transformed ViTs into practical general-purpose backbones. The ongoing research continues to refine these concepts, exploring more efficient pre-training methods for windowed architectures and re-evaluating the precise mechanisms required for cross-window information exchange, balancing architectural complexity with the power of self-supervised learning.
\subsection{Pyramid and Multi-Scale Feature Representation}
\label{sec:4_2_pyramid__and__multi-scale_feature_representation}


The original Vision Transformer (ViT) architecture, designed primarily for image classification, inherently produces a single-scale feature map. This design choice significantly limits its direct applicability to dense prediction tasks such as object detection and semantic segmentation, which fundamentally necessitate multi-scale feature representations to effectively capture both fine-grained local details and broad global context across an image. Addressing this critical limitation, a major research thrust has focused on developing hierarchical Vision Transformers that generate feature pyramids, mirroring the multi-resolution feature maps traditionally produced by Convolutional Neural Networks (CNNs).

Early efforts to introduce hierarchical structures into ViTs aimed to explicitly construct feature pyramids. The Pyramid Vision Transformer (PVT) [PVT] pioneered this by progressively reducing the spatial resolution of feature maps and expanding channel dimensions. It achieves this through a "spatial reduction attention" mechanism, where the key and value tensors are reshaped to a lower spatial resolution before the attention computation, thereby reducing sequence length and computational cost in deeper layers while creating a feature hierarchy. Similarly, Multiscale Vision Transformers (MViT) [MViT] further explored multi-scale processing by progressively reducing the sequence length of tokens (e.g., through pooling operations) while expanding their channel capacity. This allows MViT to efficiently handle high-resolution inputs and effectively capture multi-scale information, making it suitable for tasks requiring dense predictions. Other works, such as Twins [Twins], also revisited the design of spatial attention to enhance its efficiency within hierarchical contexts, making Transformers more viable for high-resolution inputs.

A pivotal innovation in this domain was the Swin Transformer [Swin]. While its core mechanism of window-based and shifted attention, which restricts self-attention to local windows and enables cross-window connections, is detailed in Section 4.1, its significance in this context lies in its hierarchical architecture. Swin Transformer constructs a feature pyramid by progressively merging image patches and applying shifted window attention, leading to feature maps at different resolutions. This design not only achieves linear computational complexity with respect to image size but also established Swin as a new standard general-purpose backbone for dense prediction tasks due to its superior performance on benchmarks like COCO for object detection and ADE20K for semantic segmentation [Swin].

The advancement of hierarchical ViTs also necessitated adaptations in pre-training strategies. Masked Autoencoders (MAE), initially highly successful with plain ViTs, faced challenges when applied to pyramid-based ViTs due to their local window operations and varying patch sizes. To overcome this, Uniform Masking (UM-MAE) [li2022ow4] was proposed, enabling efficient MAE pre-training for hierarchical models like Swin Transformer by introducing uniform sampling and secondary masking. This significantly improved pre-training efficiency and fine-tuning performance, even surpassing ImageNet-22K supervised pre-training with only ImageNet-1K. Building on strong pre-training, Hiera [ryali202339q] argued that many "bells-and-whistles" in complex hierarchical ViTs are unnecessary. They demonstrated that a simplified hierarchical design, when pre-trained with MAE, can achieve superior accuracy and speed, highlighting the power of pre-training in streamlining architecture and achieving robust multi-scale representations.

Interestingly, some research explores alternatives to strictly hierarchical backbones for multi-scale output. ViTDet [li2022raj] demonstrated that competitive object detection performance could be achieved using a *plain*, non-hierarchical ViT backbone. This was accomplished by simply constructing a feature pyramid from its single-scale output (e.g., through simple upsampling and downsampling operations), especially when the plain ViT was pre-trained with MAE. This finding suggests flexibility in how multi-scale features are derived, challenging the strict necessity of an inherently hierarchical backbone. Further exploring efficient designs, Vision-RWKV (VRWKV) [duan2024q7h], adapted from NLP's RWKV model, offers reduced spatial aggregation complexity, enabling efficient processing of high-resolution images without windowing operations, thus implicitly handling multi-scale information effectively. Similarly, MambaVision [hatamizadeh2024xr6] proposes a hybrid Mamba-Transformer backbone with a hierarchical architecture, demonstrating state-of-the-art performance in classification and dense prediction tasks by integrating Mamba's efficient sequence modeling with self-attention for long-range dependencies. Even simpler approaches like SENet [hao202488z] utilize an asymmetric ViT-based encoder-decoder structure with a Local Information Capture Module (LICM) to enhance pixel-level segmentation, demonstrating how multi-scale features can be implicitly handled through architectural design. Furthermore, ShiftViT [wang2022da0] provocatively suggests that even zero-parameter shift operations can replace attention in a hierarchical ViT, achieving multi-scale representation on par with Swin Transformer, implying that the overall hierarchical structure might be more critical than the specific token mixing mechanism for generating effective multi-scale features.

In conclusion, the development of pyramid and multi-scale feature representation has been crucial for expanding Vision Transformers' applicability beyond image classification to complex dense prediction tasks. This evolution has seen the emergence of dedicated hierarchical architectures like PVT, MViT, and the highly influential Swin Transformer, which efficiently capture multi-scale context and fine-grained details by integrating spatial reduction and window-based attention mechanisms. Recent advancements in pre-training, such as UM-MAE, have further optimized these architectures, leading to simpler yet more powerful designs like Hiera. While hierarchical backbones remain a dominant paradigm, the success of approaches like ViTDet demonstrates that multi-scale features can also be effectively derived through post-processing of plain ViT outputs. The emergence of novel architectures like Vision-RWKV and MambaVision further highlights the ongoing exploration of efficient and versatile solutions for multi-scale vision problems, emphasizing that the goal of robust multi-scale representation can be achieved through diverse architectural and training strategies.
\subsection{General-Purpose Backbones for Dense Prediction}
\label{sec:4_3_general-purpose_backbones_for_dense_prediction}


The initial success of Vision Transformers (ViTs) in image classification, pioneered by [ViT], quickly highlighted their potential but also exposed fundamental limitations when applied to dense prediction tasks. The original ViT architecture suffered from quadratic computational complexity with respect to image resolution and inherently lacked the hierarchical, multi-scale feature representations crucial for precise localization and structured prediction in tasks like object detection, semantic segmentation, and depth estimation [zhou2021rtn]. This spurred a critical wave of architectural innovations aimed at transforming ViTs from classification-centric models into versatile, general-purpose backbones capable of competing with, and often surpassing, traditional Convolutional Neural Networks (CNNs) across a wide array of dense vision challenges.

Building upon the efficient, localized attention mechanisms and hierarchical designs detailed in Section 4.1 and 4.2, architectures like the \textit{Swin Transformer} [liu2021ljs] were pivotal in establishing ViTs as formidable general-purpose backbones for dense prediction. Swin's progressive merging of image patches and computation of self-attention within local, shifted windows effectively generated a feature pyramid, mirroring the multi-scale representations characteristic of CNNs. This hierarchical structure enabled the capture of both fine-grained local details and broader contextual information, which is indispensable for pixel-level understanding. Consequently, Swin Transformer achieved state-of-the-art performance across dense prediction benchmarks, including object detection on COCO and semantic segmentation on ADE20K, solidifying its role as a new paradigm for vision backbones. The adaptability of Swin's hierarchical design was further exemplified by [li20233lv] with SwinCross, which integrated cross-modal attention for 3D medical image segmentation, showcasing its potential in specialized dense prediction domains requiring robust multi-scale features.

Concurrently, other hierarchical ViTs contributed significantly to this paradigm shift. The \textit{Pyramid Vision Transformer (PVT)} [PVT] demonstrated that convolution-free pyramid structures could directly generate multi-scale features purely through attention mechanisms. Similarly, \textit{Multiscale Vision Transformers (MViT)} [MViT] explored efficient multi-scale processing by progressively pooling tokens to reduce sequence length while increasing channel depth, particularly for video applications. These hierarchical backbones provided the essential feature pyramids that could be seamlessly integrated with lightweight convolutional decoders or specialized heads, a common and highly effective practice in dense prediction frameworks. The synergy lies in leveraging the ViT encoder's strength in global context modeling and robust feature extraction across scales, while employing CNN-based decoders for efficient upsampling, precise spatial localization, and recovery of fine-grained details crucial for pixel-level tasks. For instance, [DPT] demonstrated the effectiveness of a Vision Transformer-based dense prediction Transformer, leveraging these multi-scale features for tasks like depth estimation and semantic segmentation. Furthermore, [panboonyuen20218r7] showcased the superior performance of Swin Transformer backbones when coupled with various decoder designs like U-Net, PSPNet, and FPN for semantic segmentation on remote sensing images, underscoring the plug-and-play nature and complementary strengths of this encoder-decoder paradigm. [chen202174h] utilized ViT-V-Net, a volumetric ViT, for unsupervised medical image registration, underscoring their utility in complex 3D dense prediction.

The efficiency and robustness of these hierarchical ViTs were further bolstered by advancements in pre-training strategies tailored for their unique architectures. While Masked Autoencoders (MAE) [community_11] proved highly effective for vanilla ViTs, adapting them to pyramid-based ViTs with local operators posed a challenge. [li2022ow4] addressed this with Uniform Masking (UM-MAE), enabling efficient MAE pre-training for models like Swin and PVT. This innovation significantly improved pre-training efficiency and fine-tuning performance on downstream dense prediction tasks, demonstrating that effective self-supervised learning is crucial for unlocking the full potential of these complex backbones. Further reinforcing this, [zhang2022msa] introduced HiViT, a hierarchical ViT designed for efficient Masked Image Modeling (MIM), achieving competitive accuracy with faster pre-training compared to Swin-B, and demonstrating performance gains in detection and segmentation. Interestingly, [ryali202339q] introduced Hiera, a simplified hierarchical ViT that, when combined with strong MAE pre-training, achieved competitive accuracy while being significantly faster. This work critically suggests that a substantial portion of performance gains might stem from robust pre-training rather than solely from intricate architectural "bells-and-whistles," prompting a re-evaluation of design complexity versus pre-training efficacy.

Despite the rapid convergence on hierarchical designs, subsequent work provocatively questioned their absolute indispensability for dense prediction. [li2022raj] explored the use of plain, non-hierarchical ViTs as backbones for object detection. Their work, named ViTDet, demonstrated that with minimal adaptations, such as constructing a simple feature pyramid from a single-scale feature map and incorporating limited cross-window propagation, these seemingly less suitable architectures could achieve competitive results. This finding suggests that the rich, global representations learned by original ViTs, when properly fine-tuned and augmented with basic multi-scale processing, possess significant inherent potential for dense tasks, offering a simpler alternative to complex hierarchical designs. Furthermore, challenging the conventional "pre-train & fine-tune" paradigm, [hong2022ks6] demonstrated that ViT-based object detectors could be trained effectively from scratch. Their empirical study revealed that specific architectural changes and extended training epochs played critical roles, suggesting that while large-scale pre-training is powerful, it is not always a prerequisite for competitive performance in dense prediction tasks, especially with careful design and training.

The adaptability of ViTs as general-purpose backbones extends beyond 2D images to higher dimensions. [wang2022gq4] demonstrated the remarkable ability to adapt a standard 2D ViT architecture to 3D vision tasks, such as object classification and point cloud segmentation, by simply "inflating" the patch embedding and token sequence. This "Simple3D-Former" highlights the inherent generalizability of Transformer architectures across different data dimensions, further solidifying their role as versatile backbones for complex spatial reasoning. This evolution has firmly established ViTs as competitive, and often superior, alternatives to traditional CNN backbones, demonstrating their profound adaptability and representational power across diverse vision challenges, including complex scene understanding tasks like traversable area detection [urrea20245k4].

In summary, the journey of Vision Transformers from classification models to general-purpose backbones for dense prediction has been characterized by profound architectural innovations. The introduction of hierarchical structures and efficient attention mechanisms, most notably the Swin Transformer, effectively addressed the initial limitations of quadratic complexity and the lack of multi-scale features. Subsequent research focused on optimizing pre-training for these hierarchical models, exploring the effective integration of ViT encoders with CNN decoders, and even challenging the fundamental necessity of complex hierarchical designs for all dense tasks. These advancements have collectively made ViTs highly effective and adaptable, pushing the boundaries of performance in dense prediction tasks and demonstrating their capacity to learn robust, transferable representations for complex visual understanding.


### Beyond Pure Attention: Hybrid Designs and Alternative Token Mixers

\section{Beyond Pure Attention: Hybrid Designs and Alternative Token Mixers}
\label{sec:beyond_pure_attention:_hybrid_designs__and__alternative_token_mixers}



\subsection{Integrating Convolutional Inductive Biases}
\label{sec:5_1_integrating_convolutional_inductive_biases}

The initial success of Vision Transformers (ViTs) in image recognition, despite their impressive global context modeling, quickly exposed a fundamental architectural challenge: their inherent lack of strong inductive biases for local feature processing and translation equivariance [han2020yk0, hassija2025wq3]. Unlike Convolutional Neural Networks (CNNs), which inherently capture local patterns through their kernel operations, pure ViTs treat images as sequences of patches, relying on self-attention to learn spatial relationships from scratch. This often led to significant data hunger, performance degradation on smaller datasets, and increased computational demands for capturing fine-grained local details. Consequently, a pivotal trend in Vision Transformer research has been the strategic integration of convolutional inductive biases directly into ViT architectures, aiming to harness the complementary strengths of both paradigms. This integration seeks to combine the global reasoning capabilities of self-attention with the local feature extraction, parameter efficiency, and translation equivariance of convolutions.

This integration manifests in several distinct architectural strategies. One common approach involves incorporating convolutional layers within the initial patch embedding or tokenization process. For instance, Convolutional Vision Transformers (CvT) [CvT] introduced convolutions into both the token embedding and the projection layers (query, key, and value) within the Transformer blocks. By replacing linear projections with depth-wise separable convolutions, CvT effectively injects local processing into the attention mechanism itself, enhancing local feature extraction and translation equivariance while retaining the global receptive field of self-attention. This direct integration allowed CvT to achieve improved performance and efficiency compared to pure ViTs, particularly on smaller datasets.

A second strategy involves designing hybrid architectures that strategically combine distinct convolutional and attentional layers or blocks. Co-Scale Conv-Attentional Image Transformers (CoaT) [CoaT] exemplifies this by proposing a multi-scale architecture that leverages both convolutional and attentional layers at different stages. CoaT employs a co-scale attention mechanism that allows for interaction between features extracted by convolutional and self-attention branches, enabling efficient capture of multi-scale representations. This parallel processing and fusion of local and global features proved effective for various vision tasks. More recently, Next-ViT [li2022a4u] further refined this hybrid approach by developing a Next Convolution Block (NCB) and a Next Transformer Block (NTB), which are then stacked using a novel Next Hybrid Strategy (NHS). Next-ViT explicitly aims for efficient deployment in industrial scenarios, demonstrating that by carefully designing and integrating deployment-friendly convolutional and transformer components, models can achieve superior latency/accuracy trade-offs compared to both pure CNNs and ViTs. The NCB efficiently captures local information, while the NTB handles global context, and their strategic interleaving ensures high performance across tasks like image classification, object detection, and semantic segmentation.

Another variant of hybrid integration involves using convolutions to enrich feature representations before or during the attention mechanism. The XFormer [zhao2022koc] model, for example, combines efficient mobile CNNs with ViTs through a novel cross feature attention (XFA) mechanism. Here, convolutions provide a strong foundation for local representations, which are then enriched and fused with global attention through the XFA, leading to lightweight yet high-performing models. This approach highlights how convolutions can serve as powerful local feature extractors, whose outputs are then processed and contextualized by Transformer layers.

The benefits of injecting convolutional inductive biases are multifaceted. These hybrid models often exhibit improved data efficiency, requiring less pre-training data to achieve competitive performance, making them more practical for scenarios with limited labeled datasets. They also tend to be more efficient computationally, particularly for high-resolution inputs, as convolutions can process local information more effectively than global self-attention. Furthermore, the inherent translation equivariance of convolutions contributes to better generalization and robustness. This architectural convergence, where the strengths of CNNs and Transformers are combined, has led to models that are not only more efficient and performant but also more robust across a wider array of vision tasks. While pure ViTs, especially large-scale ones like ViTDet [li2022raj] and Hiera [ryali202339q], can achieve strong results with extensive pre-training and minimal adaptations, the consistent improvements observed with well-designed hybrid architectures underscore the enduring value of explicitly incorporating convolutional inductive biases. This ongoing integration blurs the lines between these once distinct paradigms, pointing towards a future where optimal vision backbones dynamically leverage the best elements from both to achieve even greater efficiency, generalization, and practical utility.
\subsection{Rethinking Token Mixing: Beyond Self-Attention}
\label{sec:5_2_rethinking_token_mixing:_beyond_self-attention}


While Vision Transformers have revolutionized computer vision, the computational cost and complexity of the self-attention mechanism remain a significant bottleneck, particularly for high-resolution images and real-time applications. This has spurred a wave of radical architectural explorations that challenge the necessity of complex self-attention, instead proposing simpler, more computationally efficient token mixing operations while retaining the overall 'MetaFormer' structure (token mixer + MLP). These works provocatively suggest that the core architectural design of Transformers, rather than the intricate self-attention mechanism itself, might be the primary driver of their success, opening avenues for faster and more hardware-friendly vision models.

Among the most direct challenges to self-attention is the \textit{MetaFormer} architecture, exemplified by [PoolFormer]. This work demonstrated that even a simple spatial pooling operation, when integrated into the MetaFormer block (token mixer followed by an MLP), could achieve competitive performance. [PoolFormer] posited that the overall architectural design principles of Transformers, such as residual connections, normalization layers, and the alternation of token mixing and channel-wise MLP layers, are more critical than the specific self-attention mechanism itself. This finding highlighted the potential for significantly simpler and more efficient token mixers without sacrificing performance.

Building on the premise of efficient global interactions, [GFNet] introduced Global Filter Networks (GFNet), which replace self-attention with global filters applied in the frequency domain. By leveraging 2D Fourier transforms, GFNet achieves global receptive fields with a computational complexity that is linear with respect to the number of tokens, offering a highly efficient alternative to the quadratic complexity of standard self-attention. This approach demonstrated that global information exchange, crucial for Transformer performance, could be achieved through non-attention mechanisms, further supporting the MetaFormer hypothesis.

Another notable exploration into alternative token mixing is the Vision Permutator (ViP) [ViP]. While its title includes "Permutable Self-Attention," ViP's core contribution lies in its permutation-based mixing strategy, which significantly departs from traditional self-attention by performing attention along different axes (height, width, and channel dimensions) sequentially. This design aims to capture global information more efficiently by reducing the computational burden associated with full pairwise token interactions, effectively acting as a more efficient, structured alternative to standard self-attention within the MetaFormer paradigm.

Further diversifying the landscape of non-attention token mixers, [FocalNet] proposed Focal Modulation Networks, which replace self-attention with a focal modulation mechanism. This approach captures multi-range dependencies by dynamically modulating features based on their spatial location and context, without relying on explicit attention weights. FocalNet demonstrated that effective long-range interactions and hierarchical feature learning can be achieved through modulation operations, offering another compelling alternative to self-attention that is both efficient and performs competitively.

Collectively, these architectural innovations [PoolFormer, GFNet, ViP, FocalNet] represent a critical re-evaluation of the core components of Vision Transformers. They consistently demonstrate that high performance can be maintained or even improved by replacing complex self-attention with simpler, more computationally efficient token mixing operations. This body of work underscores that the architectural blueprint of Transformers, characterized by its modularity and the separation of spatial mixing and channel-wise processing, is a powerful paradigm. The ongoing challenge lies in further exploring the trade-offs between the theoretical expressiveness of full self-attention and the practical benefits of these simpler, hardware-friendly alternatives, paving the way for a new generation of efficient and scalable vision models.
\subsection{Convergence with Modernized Convolutional Networks}
\label{sec:5_3_convergence_with_modernized_convolutional_networks}


The emergence of Vision Transformers (ViTs) fundamentally reshaped the landscape of computer vision, demonstrating that architectures primarily built on self-attention could achieve state-of-the-art performance across a diverse range of tasks [ViT]. This paradigm shift, however, did not signal the obsolescence of Convolutional Neural Networks (CNNs); instead, it catalyzed a critical re-evaluation and modernization of CNN architectures. This subsection explores a fascinating convergence where CNNs began to systematically adopt design principles initially popularized by ViTs, leading to a blurring of architectural boundaries and the development of highly competitive hybrid models.

Initially, ViTs showcased impressive capabilities, particularly when pre-trained on extensive datasets. Studies on the transferability of visual representations further underscored the advantages of Transformer-based backbones across various downstream tasks, including fine-grained classification and scene recognition [zhou2021rtn]. This demonstrated the powerful representational capacity of ViTs, creating a strong impetus for CNNs to evolve and incorporate lessons learned from their Transformer counterparts. The success of ViTs prompted a deeper inquiry into the specific architectural elements responsible for their performance gains, moving beyond the sole focus on the self-attention mechanism to consider broader design choices.

This intellectual trajectory culminated in seminal works like ConvNeXt [ConvNeXt], which stands as a prime example of this convergence. The authors of ConvNeXt systematically analyzed the architectural design choices that contributed to ViTs' success, such as the use of larger kernel sizes (e.g., $7 \times 7$), inverted bottleneck structures, Layer Normalization instead of Batch Normalization, and GELU activation functions. By progressively incorporating these Transformer-inspired design principles into a ResNet-like architecture, ConvNeXt demonstrated that modernized CNNs could achieve competitive, and often superior, performance to state-of-the-art ViTs on ImageNet classification and downstream tasks like object detection and semantic segmentation. For instance, the adoption of larger kernel sizes directly aimed to increase the receptive field, mimicking the global information aggregation capability of self-attention, while inverted bottlenecks improved parameter efficiency, echoing ViT's MLP structure. This systematic reverse-engineering of ViT's scaling recipes and design choices effectively blurred the architectural boundaries, proving that the inductive biases of convolutions, when combined with modern scaling and normalization techniques, remain highly potent.

The convergence is not unidirectional; it represents a synergistic exchange of ideas. While ConvNeXt modernized CNNs with ViT principles, other works explored hybrid architectures that explicitly combine elements from both paradigms for improved efficiency and performance. For example, LeViT [LeViT] introduced a "Vision Transformer in ConvNet's Clothing," designing a ViT that prioritizes faster inference by incorporating ConvNet-like efficiency and structures, such as attention bias and smaller attention heads, further illustrating the bidirectional synergistic potential. More explicitly, models like Next-ViT [li2022a4u] and TRT-ViT [xia2022dnj] propose "Next Convolution Blocks" (NCB) and "Next Transformer Blocks" (NTB) or similar hybrid block designs. These architectures strategically stack convolution-based blocks for local feature extraction with Transformer-based blocks for global context modeling, aiming to achieve the efficiency of CNNs with the powerful representational capacity of ViTs. They are often optimized for practical industrial deployment scenarios, demonstrating superior latency/accuracy trade-offs compared to pure CNNs or ViTs on platforms like TensorRT. This development underscores the growing understanding that the optimal vision backbone for real-world applications often benefits from a thoughtful integration of both local and global processing mechanisms.

This hybridization proves particularly effective in specialized domains where both local detail and global context are crucial. For instance, in medical image analysis, modernized CNNs and hybrid designs have shown significant promise. Studies on melanoma diagnosis [aksoy20240c0] highlight ConvNeXt's superior performance, demonstrating its balanced precision and recall metrics in classifying benign and malignant cases. Similarly, for COVID-19 detection from CT images, ensemble frameworks combining Vision Transformers and ConvNeXt (e.g., VitCNX [tian2022qb5]) have achieved state-of-the-art recall, accuracy, and F1-scores, leveraging the strengths of both architectures to capture fine-grained pathological features and broader anatomical context. These applications underscore the practical benefits of integrating the best elements from both worlds, leading to more robust and accurate diagnostic tools.

In conclusion, the "Convergence with Modernized Convolutional Networks" signifies a maturing understanding of deep learning architectures for vision. The success of ConvNeXt and subsequent hybrid models demonstrates that the architectural innovations pioneered by ViTs are not exclusive to attention-based models but can be effectively integrated into CNNs, yielding highly competitive results. This trend indicates that the optimal vision backbone for future tasks may not strictly adhere to either a pure CNN or a pure Transformer design, but rather incorporate the most effective and efficient components from both paradigms. The ongoing challenge lies in identifying the precise balance and interaction of these diverse architectural elements to achieve optimal performance, efficiency, and scalability across a wide range of vision tasks and deployment environments.


### ViTs in Action: Diverse Applications and Specialized Adaptations

\section{ViTs in Action: Diverse Applications and Specialized Adaptations}
\label{sec:vits_in_action:_diverse_applications__and__specialized_adaptations}



\subsection{Object Detection and Semantic Segmentation}
\label{sec:6_1_object_detection__and__semantic_segmentation}


The application of Vision Transformers (ViTs) to dense prediction tasks, such as object detection and semantic segmentation, marks a critical advancement in computer vision, leveraging their inherent capacity for global context modeling and long-range dependency capture. While initial ViT architectures excelled in image classification, their fixed-resolution inputs and quadratic computational complexity presented significant hurdles for tasks demanding fine-grained spatial understanding and multi-scale feature representation. As established in Section 4, the development of hierarchical ViT architectures, notably the Swin Transformer [liu2021ljs] and Pyramid Vision Transformer (PVT) [PVT], was instrumental in overcoming these limitations. These models generate multi-scale feature maps and achieve linear computational complexity, thereby transforming ViTs into versatile backbones competitive with, and often superior to, traditional Convolutional Neural Networks (CNNs) for dense prediction.

For object detection, the pioneering work of DETR (DEtection TRansformer) [DETR] introduced a paradigm shift by formulating object detection as a direct set prediction problem. This end-to-end approach eliminated the need for hand-designed components like non-maximum suppression (NMS) and anchor boxes, relying instead on the Transformer's global attention to reason about object relationships and the entire image context. However, the original DETR suffered from slow convergence and challenges in detecting small objects due to its global attention mechanism. Subsequent research addressed these limitations; Deformable DETR [DeformableDETR] (not in provided papers, but assumed as keystone) significantly improved efficiency and performance by employing deformable attention, which focuses on a small set of sampling points around a reference, thereby reducing computational cost and enhancing the ability to handle objects at varying scales. This advancement made DETR-based models more practical and competitive. For instance, `[wang2023bfo]` demonstrates the effectiveness of integrating Deformable DETR with a Swin Transformer backbone and a lightweight Feature Pyramid Network (FPN) for robust classroom behavior detection, showcasing improved accuracy for multi-scale targets. Further refinements, such as ViDT (Vision and Detection Transformers) [song2022y4v], have integrated Swin Transformer as a reconfigured backbone with efficient transformer decoders to boost detection and instance segmentation performance, highlighting the synergy between hierarchical backbones and refined detection heads.

In semantic segmentation, SETR (SEgmentation TRansformer) [SETR] demonstrated the efficacy of a purely Transformer-based encoder for capturing extensive long-range dependencies crucial for dense pixel-level classification. However, pure Transformer encoders often struggle with precise local localization due to their patch-based processing. To mitigate this, hybrid models emerged, combining the strengths of both architectures. TransUNet [TransUNet] is a prominent example, leveraging a Transformer encoder for global context modeling and a traditional CNN decoder for fine-grained local detail recovery, proving particularly effective in medical image segmentation. This hybrid strategy has been widely adopted; for instance, `[panboonyuen20218r7]` explored Swin Transformer backbones with various CNN-based decoders (U-Net, PSPNet, FPN) for semantic segmentation on remotely sensed images, achieving state-of-the-art results. Similarly, UNetFormer [hatamizadeh2022y9x] introduced a unified framework with a 3D Swin Transformer encoder and both CNN- and transformer-based decoders, excelling in 3D medical image segmentation by effectively capturing global anatomical context while maintaining local precision. Beyond hybrid approaches, advancements in weakly-supervised semantic segmentation (WSSS) have also leveraged ViTs; WeakTr [zhu2023dpi] explores properties of plain ViTs for WSSS, adaptively fusing self-attention maps to generate high-quality class activation maps (CAMs), demonstrating ViT's potential even with limited supervision.

A significant evolution in dense prediction has been the development of unifying frameworks that tackle multiple tasks simultaneously. MaskFormer [MaskFormer] (not in provided papers, but assumed as keystone) and its successor Mask2Former [Mask2Former] (not in provided papers, but assumed as keystone) represent a paradigm shift by unifying instance, semantic, and panoptic segmentation under a mask classification framework. These models leverage a Transformer decoder with learnable queries to directly predict a set of class-agnostic masks and their corresponding class labels, simplifying the pipeline and achieving state-of-the-art results across all three segmentation tasks. This query-based approach inherently benefits from the Transformer's global reasoning capabilities to resolve ambiguities and capture complex object instances. The trend towards generalist models is further exemplified by GiT (Generalist Vision Transformer) [wang20249qa], which proposes a single vanilla ViT architecture with a universal language interface capable of handling diverse vision tasks, including detection and segmentation, without task-specific modules, fostering mutual enhancement across tasks.

The versatility of ViTs in dense prediction extends to numerous domain-specific applications. For instance, ST-YOLOA [zhao2023rle] developed a Swin-Transformer-based YOLO model for robust SAR ship detection, integrating the hierarchical ViT's global context modeling within an enhanced YOLOX framework to boost accuracy in challenging backgrounds. In agricultural precision, SwinGD [wang20215ra] applied Swin Transformer and DETR models for grape bunch detection, outperforming traditional CNNs. Furthermore, ViTs are enhancing autonomous perception systems, with bilateral models combining CNNs, ViT, and MLPs for traversable area detection [urrea20245k4], improving the capture of distant details for real-time semantic segmentation. Even in specialized medical image analysis, explainable ViT models are being developed for white blood cell classification and localization [katar202352u], demonstrating their ability to provide interpretable global context for diagnostic tasks.

In conclusion, the journey of ViTs in object detection and semantic segmentation has progressed from addressing initial architectural limitations to pioneering end-to-end and hybrid solutions. Hierarchical designs like Swin Transformer have established ViTs as competitive backbones, while frameworks like DETR and its deformable variants have revolutionized object detection. For segmentation, SETR, hybrid models like TransUNet and UNetFormer, and unifying mask-based approaches like MaskFormer have demonstrated the power of Transformers in structured prediction. Despite these advancements, challenges persist, including optimizing computational efficiency for extremely high-resolution inputs, further reducing the data-hungry nature of some Transformer variants, and enhancing interpretability. Future research will likely focus on developing more efficient and adaptable hybrid architectures, exploring novel attention mechanisms or their alternatives, and advancing generalist foundation models that can seamlessly handle a multitude of dense prediction tasks.
\subsection{Medical Image Analysis}
\label{sec:6_2_medical_image_analysis}

The precise and automated analysis of medical images is paramount for accurate diagnosis, treatment planning, and disease monitoring. This domain presents unique challenges, including the inherent three-dimensional (3D) nature of many imaging modalities (e.g., MRI, CT), the scarcity of expertly labeled datasets, and the critical demand for high precision and interpretability. While Convolutional Neural Networks (CNNs) have long been the backbone of medical image analysis, their limited receptive fields often hinder their ability to capture global anatomical context and long-range dependencies, which are crucial for understanding complex pathological structures. The advent of Vision Transformers (ViTs), initially successful in natural language processing, has offered a powerful alternative by leveraging self-attention mechanisms to model these global relationships, thereby significantly impacting medical image analysis.

Early explorations into Vision Transformers for general computer vision tasks, such as the foundational work by [ViT], demonstrated their capability to achieve impressive performance by treating images as sequences of patches. However, these initial ViTs often suffered from high computational costs and a lack of inherent inductive biases for local feature extraction, limiting their direct applicability to dense prediction tasks like segmentation. The introduction of hierarchical Vision Transformers, notably the Swin Transformer [Swin], addressed these limitations by employing shifted window-based self-attention, which efficiently captures both local and global dependencies, making ViTs more suitable for complex vision tasks and paving the way for their adoption in medical imaging.

In the realm of medical image segmentation, particularly for 3D data, hybrid architectures that combine the strengths of CNNs and ViTs have emerged as a dominant paradigm. Initial efforts, such as TransUNet [TransUNet], integrated a Transformer encoder with a CNN decoder, demonstrating the potential of Transformers to capture global context while relying on CNNs for precise localization. Building on this, SwinBTS [jiang2022zcn] utilized a 3D Swin Transformer as an encoder-decoder backbone, incorporating convolutional operations for upsampling and downsampling to enhance 3D brain tumor segmentation. While effective, these models often integrate CNNs and Transformers sequentially, potentially limiting the simultaneous learning of different feature types.

A significant advancement in this direction is the Swin Unet3D architecture proposed by [cai2023hji]. This model introduces a novel parallel feature extraction sub-module within each stage of a U-Net-like encoder-decoder structure, integrating both 3D Swin Transformer Blocks and 3D Convolutional Blocks. This parallel design allows Swin Unet3D to simultaneously learn both global (long-distance) and local (fine-grained) dependency information throughout the network, effectively addressing the shortcomings of pure CNNs (limited receptive fields) and pure ViTs (poor local detail learning with limited data). The model further employs depth-wise separable convolutions for efficient local feature learning and utilizes feature fusion via multiplication to combine the distinct representations, achieving a superior balance between segmentation accuracy and model parameters for tasks like 3D brain tumor segmentation [cai2023hji].

Beyond segmentation, ViTs are also being adapted for disease classification in medical imaging. For instance, Swin-GA-RF [alohali2024xwz] presents a novel hybrid approach for cervical cancer classification from Pap smear images. This method leverages a Swin Transformer for robust feature extraction, followed by a Genetic Algorithm (GA) for optimal feature selection and a Random Forest (RF) classifier for enhanced decision-making. This pipeline-based hybridization demonstrates how ViTs can be integrated with other machine learning techniques to address the challenges of complex medical image classification, particularly by optimizing the feature space for improved diagnostic accuracy. Similarly, EFFResNet-ViT [hussain2025qoe] combines EfficientNet-B0 and ResNet-50 CNN backbones with a ViT module, focusing on explainable medical image classification for brain tumors and retinal diseases. This model emphasizes interpretability through Grad-CAM visualization, a critical aspect for clinical adoption, by fusing local CNN features with global ViT dependencies.

The practical deployment of ViTs in medical settings also necessitates optimizing their training and performance. Ko et al. [ko2024eax] systematically investigated the impact of various optimizers on ViT-based models (ViT, FastViT, CrossViT) for lung disease detection from chest X-ray images. Their findings provide crucial empirical guidance for selecting optimal training strategies, especially when dealing with imbalanced medical datasets, highlighting that Adam-based optimizers generally outperform traditional SGD-based methods and that specific optimizer-model combinations can achieve high diagnostic accuracies.

The ongoing research also explores various architectural enhancements to ViTs that are highly relevant to medical imaging. For example, PLG-ViT [ebert202377v] proposes a parallel local and global self-attention mechanism, efficiently fusing short- and long-range spatial interactions without costly shifted windows, which could benefit medical tasks requiring both fine detail and broad context. Similarly, approaches like Slide-Transformer [pan2023hry] and BOAT [yu20220np] focus on efficient local attention and feature-space locality, addressing the high-resolution demands of medical images.

In conclusion, Vision Transformers have profoundly impacted medical image analysis by offering a powerful mechanism to capture global anatomical context and long-range dependencies, complementing the local feature extraction capabilities of CNNs. The intellectual trajectory is increasingly moving towards sophisticated hybrid CNN-Transformer architectures, such as Swin Unet3D [cai2023hji], which integrate these paradigms in parallel to leverage their respective strengths for improved performance in challenging 3D medical contexts. While significant progress has been made in tasks like 3D segmentation and disease classification, ongoing challenges include further enhancing model interpretability, addressing data scarcity through efficient learning strategies, and developing more robust and generalizable hybrid solutions that can seamlessly adapt to diverse medical imaging modalities and pathologies.
\subsection{Lightweight and Real-time Vision Systems}
\label{sec:6_3_lightweight__and__real-time_vision_systems}


The increasing computational demands of Vision Transformers (ViTs) present a significant hurdle for their deployment on resource-constrained devices, such as mobile phones, embedded systems, and edge AI platforms, where real-time inference, low latency, and energy efficiency are paramount. This subsection critically examines the multifaceted efforts to develop lightweight and efficient ViT systems, focusing on architectural optimizations, novel attention mechanisms, model compression techniques, and hardware-aware designs that collectively aim to balance accuracy with computational speed for practical deployment.

A primary strategy for achieving efficiency involves the development of **hybrid architectures** that strategically integrate convolutional inductive biases with Transformer blocks, tailored specifically for mobile and edge environments. [MobileViT] pioneered this direction with MobileViT, a lightweight, general-purpose Vision Transformer that combines the local feature extraction strengths of CNNs with the global reasoning capabilities of Transformers within a mobile-friendly design. This integration allows for a more efficient capture of both local and global dependencies, reducing the computational overhead typically associated with pure ViTs. Following this, [EdgeViT] introduced EdgeViT, an efficient hierarchical ViT specifically engineered for on-device image classification, optimizing its architecture for improved latency and throughput on edge AI platforms. These models exemplify a direct architectural approach to creating ViTs that are inherently efficient for resource-limited settings.

Beyond general hybrid designs, **hardware-aware architectural optimizations** have emerged as crucial for maximizing real-time performance. These approaches consider the specific characteristics of target deployment platforms. [li2022a4u] proposed Next-ViT, a "next-generation" Vision Transformer designed for efficient deployment in realistic industrial scenarios. Next-ViT achieves CNN-like inference speeds with ViT-level performance by developing deployment-friendly Next Convolution Blocks (NCB) and Next Transformer Blocks (NTB), integrated through a novel Next Hybrid Strategy (NHS). This design demonstrates superior latency/accuracy trade-offs on platforms like TensorRT and CoreML, highlighting the importance of co-designing models with their target hardware. Similarly, [xia2022dnj] introduced TRT-ViT, a TensorRT-oriented Vision Transformer, which directly optimizes for hardware latency by deriving practical design guidelines for deployment-friendly networks, resulting in significantly faster inference speeds across various visual tasks. In a related vein, [nag2023cfn] presented ViTA, a configurable hardware accelerator for ViT inference targeting highly resource-constrained edge computing devices, emphasizing optimizations like head-level pipelining and inter-layer MLP optimizations to avoid repeated off-chip memory accesses and achieve high hardware utilization.

Another significant area of innovation focuses on **rethinking the self-attention mechanism** itself, which is often the computational bottleneck in ViTs due to its quadratic complexity. Efforts include designing more efficient attention variants or replacing it with simpler token mixers. [song20215tk] proposed UFO-ViT, a Unit Force Operated Vision Transformer, which achieves linear computational complexity by modifying the self-attention mechanism to eliminate non-linearity, leading to high performance with reduced resource demands. Challenging the perceived indispensability of attention, [wang2022da0] introduced ShiftViT, demonstrating that an extremely simple, parameter-free shift operation, which merely exchanges channels between neighboring features, can replace attention layers while maintaining competitive performance. This suggests that the overall architectural design of ViTs might be more critical than the specific attention mechanism. Furthermore, [ren2022ifo] explored Dynamic Window Vision Transformers (DW-ViT), which go beyond fixed single-scale windows by assigning different window sizes to various head groups and dynamically fusing multi-scale information, enhancing modeling potential while maintaining efficiency. More recently, [hatamizadeh2024xr6] proposed MambaVision, a hybrid Mamba-Transformer backbone that redesigns the Mamba formulation to efficiently model visual features, achieving state-of-the-art performance and throughput by equipping the Mamba architecture with self-attention blocks, showcasing the potential of novel sequence modeling techniques for vision. A comprehensive review by [heidari2024d9k] further categorizes and analyzes various redesigned attention mechanisms within ViTs aimed at enhancing efficiency.

Beyond architectural modifications, **model compression techniques** are indispensable for adapting existing ViTs for lightweight deployment.
\textbf{Quantization} reduces the precision of model weights and activations, significantly decreasing memory footprint and accelerating inference. [li20229zn] proposed Q-ViT, a fully differentiable quantization method for ViTs where both quantization scales and bit-widths are learnable, enabling head-wise bit-width allocation to squeeze model size with minimal performance drop. This work highlighted that Multi-head Self-Attention (MSA) and GELU layers are particularly sensitive to quantization. Further, [li20223n5] and [sun2022nny] explored FPGA-aware automatic acceleration frameworks for ViTs with mixed-scheme quantization, demonstrating significant frame rate improvements on hardware with minor accuracy drops.
\textbf{Pruning} aims to remove redundant parameters or connections from the model. [hou2022ver] introduced a multi-dimensional ViT compression paradigm via dependency-guided Gaussian Process Search, jointly reducing redundancy from attention head, neuron, and sequence dimensions. [yang20210bg] proposed global Vision Transformer pruning with Hessian-aware saliency, leading to new efficient architectures (NViT) with substantial FLOPs and parameter reductions.
\textbf{Knowledge Distillation}, while discussed in Section 3.1 for data-efficient training, also plays a crucial role in model compression for deployment, where a smaller, lightweight ViT student model learns from a larger, more powerful teacher model. [yu2022iy0] presented a unified ViT compression framework that seamlessly assembles pruning, layer skipping, and knowledge distillation, formulating a budget-constrained optimization to jointly learn model weights and compression configurations.

Finally, **Neural Architecture Search (NAS)** offers an automated approach to discover highly efficient ViT architectures tailored for specific constraints. [chen202199v] proposed searching not only the architecture but also the search space of Vision Transformers, leading to models (S3) that achieve superior performance compared to manually designed efficient ViTs, demonstrating the potential of automated design for lightweight models.

These lightweighting principles are particularly critical for **application-specific scenarios** where real-time performance on resource-constrained devices is paramount. In automated plant disease classification, [borhani2022w8x] developed custom lightweight CNN and Transformer building blocks and novel hybrid CNN-ViT architectures to achieve real-time classification, explicitly addressing the speed-accuracy trade-off. Similarly, [tabbakh2023ao7] proposed TLMViT, a hybrid model combining transfer learning-based CNNs with a ViT for improved accuracy in plant disease classification, while [wu2021nmg] focused on multi-granularity feature extraction for accurate tomato leaf disease recognition. For remote sensing, a common strategy involves combining CNN backbones with Transformer heads to balance local texture analysis with global scene understanding. [deng2021man] proposed CTNet, a joint CNN-ViT framework, while [song202479c] and [song2025idg] introduced enhanced ViT-based object detectors (QAGA-Net, ODDL-Net) that address sparse data distribution and optimize feature pyramid networks for better efficiency and accuracy. [sha2022ae0] introduced MITformer, a multi-instance ViT for remote sensing scene classification, focusing on highlighting key local features for efficiency. These works demonstrate how tailored hybrid designs and optimized data handling can make ViTs practical for specialized, real-world applications.

Despite significant progress across architectural design, attention mechanisms, and compression techniques, the fundamental trade-off between model complexity, inference latency, energy consumption, and accuracy remains a central challenge. While hybrid architectures, hardware-aware designs, and various compression methods have shown promise, a universally optimal lightweight ViT for all edge scenarios is yet to emerge. Future research will likely continue to explore more sophisticated hybrid designs, novel attention mechanisms with reduced computational overhead, and advanced automated architectural search (NAS) techniques tailored for specific hardware constraints. Furthermore, advancements in efficient self-supervised pre-training will be crucial to reduce data dependency for smaller models, ensuring robust and energy-efficient vision systems that can operate effectively in diverse real-world, resource-constrained environments.


### Synthesizing the Landscape: Challenges and Future Trajectories

\section{Synthesizing the Landscape: Challenges and Future Trajectories}
\label{sec:synthesizing_the_l_and_scape:_challenges__and__future_trajectories}



\subsection{Summary of Key Developments}
\label{sec:7_1_summary_of_key_developments}

The emergence of Visual Transformers (ViTs) fundamentally reshaped the landscape of computer vision, challenging the long-standing dominance of Convolutional Neural Networks (CNNs) by demonstrating the efficacy of pure self-attention for image understanding. This subsection synthesizes the intellectual trajectory of ViT research, from its foundational principles to its rapid evolution through architectural innovations, data-efficient training paradigms, and a fascinating convergence with modern convolutional designs.

The initial Vision Transformer (ViT) [community_15] marked a significant paradigm shift, processing images as sequences of flattened patches and leveraging global self-attention to capture long-range dependencies. This approach showcased remarkable potential in image classification, even demonstrating superior transferability of learned representations compared to ConvNets in various downstream tasks [zhou2021rtn]. However, the foundational ViT exhibited critical limitations: a substantial reliance on massive pre-training datasets, quadratic computational complexity with respect to image resolution, and an inherent lack of inductive biases for local features, which hindered its performance on dense prediction tasks and smaller datasets.

Addressing these early challenges became a primary research thrust. Efforts to enhance data efficiency and training stability were paramount. Knowledge distillation, where ViTs learned from pre-trained CNN teachers, emerged as a key strategy [community_16]. Crucially, the advent of self-supervised learning (SSL) paradigms, such as Masked Autoencoders (MAE) [community_11] and DINO [community_16], revolutionized ViT pre-training. These methods enabled ViTs to learn powerful, transferable visual representations from vast amounts of unlabeled data, significantly mitigating the need for extensive human annotation. Further innovations like attention distillation [wang2022pee] provided more effective guidance for self-supervised ViT students, while studies on training ViT-based detectors from scratch highlighted the importance of architectural changes and extended training regimes to reduce reliance on pre-training [hong2022ks6].

Simultaneously, architectural innovations focused on improving efficiency and enabling hierarchical feature representation, essential for general vision tasks. The Swin Transformer [community_20] was a pivotal development, introducing a hierarchical architecture with shifted window-based attention. This design achieved linear computational complexity by restricting self-attention to local windows while maintaining cross-window connections, transforming ViTs into versatile backbones for dense prediction tasks like object detection and segmentation [community_13]. Subsequent works, such as Hiera [ryali202339q], demonstrated that with strong self-supervised pre-training (e.g., MAE), hierarchical ViTs could be significantly simplified by removing many "bells-and-whistles" without sacrificing accuracy, leading to faster inference and training. Other approaches explored adaptive global-local context nomination to balance efficiency and performance [liu2021yw0].

A critical re-evaluation of the self-attention mechanism itself and a shift towards hybrid models also characterized this period. Researchers explored integrating convolutional inductive biases to combine the strengths of both paradigms. Hybrid architectures, such as Next-ViT [li2022a4u], strategically combined convolutional and transformer blocks to achieve superior latency/accuracy trade-offs, particularly for efficient deployment in industrial scenarios. Beyond convolutions, novel token mixers emerged, challenging the necessity of complex self-attention. Models like UFO-ViT [song20215tk] demonstrated that linear self-attention mechanisms, by eliminating non-linearity, could achieve high performance with linear computational complexity, suggesting that the overall 'MetaFormer' structure (token mixer + MLP) might be more critical than the specific attention mechanism. This intellectual trajectory culminated in a fascinating convergence with modern CNNs, where Transformer-inspired design principles (e.g., larger kernel sizes, inverted bottlenecks, layer normalization) were applied to modernize traditional CNNs, leading to models like ConvNeXt [community_8] that achieved competitive or superior performance to state-of-the-art ViTs. More recent explorations, such as MambaVision [hatamizadeh2024xr6], further push the boundaries by integrating state space models (Mamba) with Transformers, demonstrating continued innovation in hybrid designs.

These architectural and training advancements enabled ViTs to serve as powerful backbones across diverse applications, from fundamental dense prediction tasks like object detection and semantic segmentation [community_8] to specialized domains such as medical image analysis [community_4] and remote sensing [community_2]. Efforts also focused on developing lightweight and real-time ViT systems for resource-constrained environments [community_14].

In conclusion, the journey of Visual Transformer research has been one of rapid innovation and adaptation. It began by demonstrating the power of global context modeling, then systematically addressed initial limitations through data efficiency, hierarchical scaling, and self-supervised learning. The field has since embraced a more nuanced perspective, integrating convolutional inductive biases, exploring alternative token mixing mechanisms, and ultimately fostering a convergence of ideas that blurs the lines between once distinct architectural paradigms. This continuous drive underscores the commitment to developing more powerful, efficient, and versatile visual understanding models that leverage the best elements from various deep learning approaches.
\subsection{Open Challenges and Limitations}
\label{sec:7_2_open_challenges__and__limitations}


Despite the remarkable success of Vision Transformers (ViTs) across numerous computer vision tasks, several open challenges and inherent limitations persist, hindering their widespread and efficient deployment in all scenarios. These issues primarily revolve around their high computational demands, reliance on extensive datasets, questions of interpretability, robustness, and the ongoing debate regarding optimal architectural inductive biases.

A primary limitation stems from the high computational cost associated with the global self-attention mechanism, particularly when processing high-resolution inputs. The quadratic complexity of vanilla Transformer architectures, as introduced by foundational works, becomes prohibitive for large images or video streams. Early efforts to mitigate this, such as the Swin Transformer [Swin] and Pyramid Vision Transformer (PVT) [PVT], introduced hierarchical structures and localized attention (e.g., shifted windows) to achieve linear complexity with respect to image size, making ViTs more viable for dense prediction tasks. Building on this, works like Next-ViT [li2022a4u] and TRT-ViT [xia2022dnj] further emphasize practical deployment efficiency, optimizing block designs and hybrid strategies to achieve superior latency-accuracy trade-offs on industrial hardware like TensorRT, recognizing that theoretical FLOPs do not always translate to real-world inference speed. More recently, MambaVision [hatamizadeh2024xr6] explores state-space models (Mamba) as an alternative to self-attention, proposing a hybrid Mamba-Transformer backbone that aims to capture long-range dependencies efficiently, potentially offering a new paradigm for linear complexity in visual modeling. For video applications, specific architectural adaptations like TP-ViT [jing2022nkb] and ViT-Shift [zhang2024g0m] introduce multi-pathway designs and temporal shift modules to efficiently model spatio-temporal information, addressing the unique computational challenges of video processing.

Another significant challenge for ViTs is their continued reliance on large datasets for optimal performance, a stark contrast to the data efficiency often exhibited by Convolutional Neural Networks (CNNs due to their strong inductive biases). While self-supervised learning methods like DINO [DINO] and MAE [MAE] have significantly reduced the need for labeled data, enabling ViTs to learn powerful representations from unlabeled images, the challenge of training ViT-based models from scratch without extensive pre-training remains. Hong et al. [hong2022ks6] empirically studied training Transformer-based object detectors from scratch, finding that architectural modifications and longer training schedules are crucial to achieve competitive performance without large-scale pre-training. This issue is particularly pronounced in specialized domains like remote sensing, where labeled data can be scarce. To address this, QAGA-Net [song202479c], ODDL-Net [song2025idg], and the quantitative regularization (QR) approach [song2024fx9] propose novel data augmentation and regularization strategies specifically tailored for remote sensing images, demonstrating that ViTs can achieve robust performance even with limited training samples by optimizing data distribution learning. Similarly, P2FEViT [wang202338i] aims to reduce ViT's data dependency by embedding CNN features, facilitating faster convergence with less data.

The debate about the optimal balance between pure attention-based designs and the integration of convolutional priors for various tasks and deployment environments is ongoing, reflecting the inherent trade-offs between architectural complexity, efficiency, and performance. Pure ViTs, lacking the strong inductive biases of CNNs (e.g., locality, translation equivariance), often require more data and computation. Consequently, many works explore hybrid architectures that combine the strengths of both. For instance, CTNet [deng2021man] and LDBST [zheng202325h] propose dual-branch frameworks for remote sensing scene classification, leveraging CNNs for local structural features and ViTs for global semantic context. P2FEViT [wang202338i] further exemplifies this by embedding plug-and-play CNN features into ViT architectures to enhance local feature representation and accelerate convergence. This convergence of ideas is also evident in works like ConvNeXt [ConvNeXt], which demonstrates that modern CNNs, by adopting ViT design principles, can achieve competitive performance, blurring the lines between the two paradigms. Conversely, PoolFormer [PoolFormer] provocatively suggests that the meta-architecture of Transformers, rather than complex self-attention, might be the key, achieving strong results with simple pooling operations, challenging the necessity of attention itself for certain tasks.

Beyond these architectural and data-related challenges, issues of interpretability and robustness to adversarial attacks remain significant open problems for ViTs. The complex, global interactions modeled by self-attention layers make it difficult to ascertain *why* a ViT makes a particular decision, hindering trust and deployment in critical applications. While some methods attempt to visualize attention maps, these often provide only a partial view of the model's reasoning. Similarly, ViTs have shown vulnerability to adversarial attacks, a concern that needs more dedicated research to ensure their reliability in real-world scenarios. The theoretical understanding of their inductive biases, or the lack thereof, also remains less mature compared to CNNs, complicating systematic improvements and robust design principles.

In conclusion, while Vision Transformers have revolutionized computer vision, their journey is far from complete. The field continues to grapple with fundamental trade-offs between computational cost and performance, the persistent need for large-scale data, and the optimal integration of local and global feature learning mechanisms. Future research will likely focus on developing more efficient, data-agnostic, inherently interpretable, and robust ViT architectures, potentially through novel attention mechanisms, alternative long-range dependency modeling (e.g., state-space models), or more sophisticated hybrid designs that judiciously combine the strengths of various neural network components.
\subsection{Emerging Trends and Ethical Considerations}
\label{sec:7_3_emerging_trends__and__ethical_considerations}


The remarkable trajectory of Visual Transformers (ViTs) necessitates a forward-looking analysis, exploring the most promising emerging trends and the critical ethical implications that accompany their increasing power and pervasive deployment. Future research is converging on more versatile, efficient, and inherently interpretable models, while simultaneously grappling with the profound societal responsibilities inherent in advanced AI.

A pivotal trend is the development of **multimodal learning** and **large-scale vision foundation models** capable of zero-shot generalization. Moving beyond purely visual tasks, ViTs are increasingly designed to process and integrate information from multiple modalities, such as visual and textual data. This integration is crucial for building more human-like AI systems that can understand context from diverse data sources. For instance, the EVA model demonstrates the potential of a unified text-and-image encoder, pushing the limits of transfer learning across modalities [eva]. The vision for generalist AI is further driving the development of unified vision-language interfaces. GiT, for example, proposes a vanilla ViT framework that, through a universal language interface, can simultaneously handle diverse visual tasks from image captioning to dense segmentation, fostering mutual enhancement across tasks and narrowing the architectural gap between vision and language models [wang20249qa]. Concurrently, the field is witnessing the rise of massive vision foundation models, exemplified by efforts to scale ViTs to unprecedented parameter counts, such as the 22 billion parameter ViT-H [vit-h]. Models like InternImage further explore large-scale vision foundation models by integrating deformable convolutions to enhance their capabilities [internimage]. These models, often pre-trained on vast datasets, aim for robust zero-shot generalization, allowing them to perform novel tasks without explicit fine-tuning, as showcased by models like Segment Anything, which provides promptable segmentation capabilities. Furthermore, query-aware vision Transformers, such as QA-ViT, embed user questions directly into the vision encoder, enabling dynamic focusing of visual features on relevant image aspects for improved multimodal reasoning [ganz20249zr].

Another crucial direction focuses on **efficient hardware-aware designs** to enable the deployment of powerful ViTs in resource-constrained environments. While early ViTs suffered from high computational costs, future work on efficiency extends beyond hybrid designs to more fundamental architectural shifts and novel attention mechanisms. This includes the development of linear attention mechanisms, such as UFO-ViT, which eliminate the quadratic complexity of traditional self-attention by removing non-linearity and factorizing matrix multiplications, demonstrating competitive performance with significantly reduced computational resources [song20215tk]. Another critical area is model quantization, with methods like Q-ViT exploring fully differentiable quantization for ViTs, allowing learnable scales and bit-widths to significantly reduce model size and computational footprint without substantial performance drops [li20229zn]. The broader trend, as highlighted by recent reviews, is to systematically redesign attention mechanisms for enhanced performance and efficiency [heidari2024d9k]. Further advancements include SPT-Swin, which employs shifted patch tokenization to enhance data efficiency and achieve linear computational complexity [ferdous2024f89], and EfficientViT, which introduces multi-scale linear attention for high-resolution dense prediction, balancing performance with efficiency [efficientvit]. Such efficient designs are particularly critical for autonomous systems, where models like bilateral ViT and MLP combinations are developed for traversable area detection, balancing precision and computational efficiency for real-time operation in challenging environments [urrea20245k4].

Beyond identifying statistical associations, a nascent but critical direction for future ViT research is the integration of **causal inference**. Current deep learning models, including ViTs, largely operate on correlations, which can lead to brittleness when faced with out-of-distribution data, spurious associations, or subtle biases. For ViTs, this means their powerful global attention might inadvertently focus on confounding factors rather than true causal features. Causal inference aims to move beyond 'what' the model sees to 'why' it makes a decision, understanding underlying causal relationships between visual elements and outcomes. This is paramount for enhancing model robustness, improving fairness by disentangling causal factors from confounding biases, and enabling better generalization to novel environments. While still in early stages for vision Transformers, research is exploring how to inject causal priors into attention mechanisms or leverage counterfactual reasoning to make ViTs more reliable and trustworthy, particularly in high-stakes applications like medical diagnosis or autonomous driving where spurious correlations can lead to catastrophic failures. This shift represents a fundamental step towards more intelligent and responsible vision AI systems.

Crucially, as ViTs become more powerful, pervasive, and integrated into sensitive applications, **ethical considerations** must move from an afterthought to a central design principle. The deployment of powerful vision AI raises significant concerns, particularly regarding **bias in training data**. The sheer scale of data required for pre-training large ViT foundation models, often scraped from the internet, means they inevitably inherit and can amplify societal biases present in that data. Unlike CNNs with their local receptive fields, ViTs' global self-attention mechanism can learn long-range spurious correlations across image patches, potentially reinforcing biases related to demographics, race, or gender, leading to discriminatory outcomes in areas like facial recognition, medical diagnosis, or autonomous decision-making. Research is actively exploring methods to audit and mitigate such biases in large vision models. For instance, developing interpretable ViTs that highlight their decision-making process, such as using Score-CAM [katar202352u] or combined Grad-CAM and Attention Rollout [chen2022vac] to visualize model focus, is a vital step towards identifying and addressing spurious correlations or discriminatory patterns, particularly in clinical settings where trust is paramount [ma2024uan].

**Privacy concerns** are equally paramount, especially with models processing sensitive visual information. The massive data requirements for large foundation models exacerbate these risks, as personal or identifiable information could be inadvertently captured and memorized. Emerging privacy-preserving techniques for ViTs include secure multi-party computation (MPC) to enable inference on encrypted data, which significantly reduces latency overhead compared to traditional methods [zeng2022ce2]. Furthermore, methods for training ViTs on visually obfuscated images while maintaining high classification accuracy demonstrate robust privacy preservation against various attacks [qi2022yq9]. The **responsible development** of these transformative technologies for societal benefit demands a multi-faceted approach, including rigorous auditing for bias, implementing differential privacy techniques, establishing clear ethical guidelines for deployment, and fostering transparency through enhanced interpretability. The integration of causal inference, as discussed previously, also plays a critical role here by moving models beyond mere correlation, offering a path towards more robust and fair decision-making.

In conclusion, the future of Visual Transformers is characterized by an exciting, yet challenging, push along two primary axes: the pursuit of **generalist intelligence** through multimodal learning and massive foundation models, and the simultaneous drive for **ubiquitous deployment** via efficient, hardware-aware designs. However, this technological advancement cannot proceed in isolation. It must be meticulously balanced with a deep, proactive commitment to addressing pressing ethical challenges. Ensuring fairness, protecting privacy, fostering interpretability, and fundamentally integrating causal reasoning are not merely technical hurdles to overcome, but rather indispensable requirements for the responsible and beneficial integration of these increasingly powerful vision AI systems into society. Without a concerted effort on these ethical fronts, the risk remains that highly capable ViTs could inadvertently perpetuate harm, undermining their transformative potential.


