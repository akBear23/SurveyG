\subsection{Medical Image Analysis}
The precise and automated analysis of medical images is paramount for accurate diagnosis, treatment planning, and disease monitoring. This domain presents unique challenges, including the inherent three-dimensional (3D) nature of many imaging modalities (e.g., MRI, CT), the scarcity of expertly labeled datasets, and the critical demand for high precision and interpretability. While Convolutional Neural Networks (CNNs) have long been the backbone of medical image analysis, their limited receptive fields often hinder their ability to capture global anatomical context and long-range dependencies, which are crucial for understanding complex pathological structures. The advent of Vision Transformers (ViTs), initially successful in natural language processing, has offered a powerful alternative by leveraging self-attention mechanisms to model these global relationships, thereby significantly impacting medical image analysis.

Early explorations into Vision Transformers for general computer vision tasks, such as the foundational work by \cite{ViT}, demonstrated their capability to achieve impressive performance by treating images as sequences of patches. However, these initial ViTs often suffered from high computational costs and a lack of inherent inductive biases for local feature extraction, limiting their direct applicability to dense prediction tasks like segmentation. The introduction of hierarchical Vision Transformers, notably the Swin Transformer \cite{Swin}, addressed these limitations by employing shifted window-based self-attention, which efficiently captures both local and global dependencies, making ViTs more suitable for complex vision tasks and paving the way for their adoption in medical imaging.

In the realm of medical image segmentation, particularly for 3D data, hybrid architectures that combine the strengths of CNNs and ViTs have emerged as a dominant paradigm. Initial efforts, such as TransUNet \cite{TransUNet}, integrated a Transformer encoder with a CNN decoder, demonstrating the potential of Transformers to capture global context while relying on CNNs for precise localization. Building on this, SwinBTS \cite{jiang2022zcn} utilized a 3D Swin Transformer as an encoder-decoder backbone, incorporating convolutional operations for upsampling and downsampling to enhance 3D brain tumor segmentation. While effective, these models often integrate CNNs and Transformers sequentially, potentially limiting the simultaneous learning of different feature types.

A significant advancement in this direction is the Swin Unet3D architecture proposed by \cite{cai2023hji}. This model introduces a novel parallel feature extraction sub-module within each stage of a U-Net-like encoder-decoder structure, integrating both 3D Swin Transformer Blocks and 3D Convolutional Blocks. This parallel design allows Swin Unet3D to simultaneously learn both global (long-distance) and local (fine-grained) dependency information throughout the network, effectively addressing the shortcomings of pure CNNs (limited receptive fields) and pure ViTs (poor local detail learning with limited data). The model further employs depth-wise separable convolutions for efficient local feature learning and utilizes feature fusion via multiplication to combine the distinct representations, achieving a superior balance between segmentation accuracy and model parameters for tasks like 3D brain tumor segmentation \cite{cai2023hji}.

Beyond segmentation, ViTs are also being adapted for disease classification in medical imaging. For instance, Swin-GA-RF \cite{alohali2024xwz} presents a novel hybrid approach for cervical cancer classification from Pap smear images. This method leverages a Swin Transformer for robust feature extraction, followed by a Genetic Algorithm (GA) for optimal feature selection and a Random Forest (RF) classifier for enhanced decision-making. This pipeline-based hybridization demonstrates how ViTs can be integrated with other machine learning techniques to address the challenges of complex medical image classification, particularly by optimizing the feature space for improved diagnostic accuracy. Similarly, EFFResNet-ViT \cite{hussain2025qoe} combines EfficientNet-B0 and ResNet-50 CNN backbones with a ViT module, focusing on explainable medical image classification for brain tumors and retinal diseases. This model emphasizes interpretability through Grad-CAM visualization, a critical aspect for clinical adoption, by fusing local CNN features with global ViT dependencies.

The practical deployment of ViTs in medical settings also necessitates optimizing their training and performance. Ko et al. \cite{ko2024eax} systematically investigated the impact of various optimizers on ViT-based models (ViT, FastViT, CrossViT) for lung disease detection from chest X-ray images. Their findings provide crucial empirical guidance for selecting optimal training strategies, especially when dealing with imbalanced medical datasets, highlighting that Adam-based optimizers generally outperform traditional SGD-based methods and that specific optimizer-model combinations can achieve high diagnostic accuracies.

The ongoing research also explores various architectural enhancements to ViTs that are highly relevant to medical imaging. For example, PLG-ViT \cite{ebert202377v} proposes a parallel local and global self-attention mechanism, efficiently fusing short- and long-range spatial interactions without costly shifted windows, which could benefit medical tasks requiring both fine detail and broad context. Similarly, approaches like Slide-Transformer \cite{pan2023hry} and BOAT \cite{yu20220np} focus on efficient local attention and feature-space locality, addressing the high-resolution demands of medical images.

In conclusion, Vision Transformers have profoundly impacted medical image analysis by offering a powerful mechanism to capture global anatomical context and long-range dependencies, complementing the local feature extraction capabilities of CNNs. The intellectual trajectory is increasingly moving towards sophisticated hybrid CNN-Transformer architectures, such as Swin Unet3D \cite{cai2023hji}, which integrate these paradigms in parallel to leverage their respective strengths for improved performance in challenging 3D medical contexts. While significant progress has been made in tasks like 3D segmentation and disease classification, ongoing challenges include further enhancing model interpretability, addressing data scarcity through efficient learning strategies, and developing more robust and generalizable hybrid solutions that can seamlessly adapt to diverse medical imaging modalities and pathologies.