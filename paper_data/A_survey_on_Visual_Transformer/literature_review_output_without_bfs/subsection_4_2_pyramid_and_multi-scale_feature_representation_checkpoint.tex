\subsection{Pyramid and Multi-Scale Feature Representation}

The original Vision Transformer (ViT) architecture, designed primarily for image classification, inherently produces a single-scale feature map. This design choice significantly limits its direct applicability to dense prediction tasks such as object detection and semantic segmentation, which fundamentally necessitate multi-scale feature representations to effectively capture both fine-grained local details and broad global context across an image. Addressing this critical limitation, a major research thrust has focused on developing hierarchical Vision Transformers that generate feature pyramids, mirroring the multi-resolution feature maps traditionally produced by Convolutional Neural Networks (CNNs).

Early efforts to introduce hierarchical structures into ViTs aimed to explicitly construct feature pyramids. The Pyramid Vision Transformer (PVT) \cite{PVT} pioneered this by progressively reducing the spatial resolution of feature maps and expanding channel dimensions. It achieves this through a "spatial reduction attention" mechanism, where the key and value tensors are reshaped to a lower spatial resolution before the attention computation, thereby reducing sequence length and computational cost in deeper layers while creating a feature hierarchy. Similarly, Multiscale Vision Transformers (MViT) \cite{MViT} further explored multi-scale processing by progressively reducing the sequence length of tokens (e.g., through pooling operations) while expanding their channel capacity. This allows MViT to efficiently handle high-resolution inputs and effectively capture multi-scale information, making it suitable for tasks requiring dense predictions. Other works, such as Twins \cite{Twins}, also revisited the design of spatial attention to enhance its efficiency within hierarchical contexts, making Transformers more viable for high-resolution inputs.

A pivotal innovation in this domain was the Swin Transformer \cite{Swin}. While its core mechanism of window-based and shifted attention, which restricts self-attention to local windows and enables cross-window connections, is detailed in Section 4.1, its significance in this context lies in its hierarchical architecture. Swin Transformer constructs a feature pyramid by progressively merging image patches and applying shifted window attention, leading to feature maps at different resolutions. This design not only achieves linear computational complexity with respect to image size but also established Swin as a new standard general-purpose backbone for dense prediction tasks due to its superior performance on benchmarks like COCO for object detection and ADE20K for semantic segmentation \cite{Swin}.

The advancement of hierarchical ViTs also necessitated adaptations in pre-training strategies. Masked Autoencoders (MAE), initially highly successful with plain ViTs, faced challenges when applied to pyramid-based ViTs due to their local window operations and varying patch sizes. To overcome this, Uniform Masking (UM-MAE) \cite{li2022ow4} was proposed, enabling efficient MAE pre-training for hierarchical models like Swin Transformer by introducing uniform sampling and secondary masking. This significantly improved pre-training efficiency and fine-tuning performance, even surpassing ImageNet-22K supervised pre-training with only ImageNet-1K. Building on strong pre-training, Hiera \cite{ryali202339q} argued that many "bells-and-whistles" in complex hierarchical ViTs are unnecessary. They demonstrated that a simplified hierarchical design, when pre-trained with MAE, can achieve superior accuracy and speed, highlighting the power of pre-training in streamlining architecture and achieving robust multi-scale representations.

Interestingly, some research explores alternatives to strictly hierarchical backbones for multi-scale output. ViTDet \cite{li2022raj} demonstrated that competitive object detection performance could be achieved using a *plain*, non-hierarchical ViT backbone. This was accomplished by simply constructing a feature pyramid from its single-scale output (e.g., through simple upsampling and downsampling operations), especially when the plain ViT was pre-trained with MAE. This finding suggests flexibility in how multi-scale features are derived, challenging the strict necessity of an inherently hierarchical backbone. Further exploring efficient designs, Vision-RWKV (VRWKV) \cite{duan2024q7h}, adapted from NLP's RWKV model, offers reduced spatial aggregation complexity, enabling efficient processing of high-resolution images without windowing operations, thus implicitly handling multi-scale information effectively. Similarly, MambaVision \cite{hatamizadeh2024xr6} proposes a hybrid Mamba-Transformer backbone with a hierarchical architecture, demonstrating state-of-the-art performance in classification and dense prediction tasks by integrating Mamba's efficient sequence modeling with self-attention for long-range dependencies. Even simpler approaches like SENet \cite{hao202488z} utilize an asymmetric ViT-based encoder-decoder structure with a Local Information Capture Module (LICM) to enhance pixel-level segmentation, demonstrating how multi-scale features can be implicitly handled through architectural design. Furthermore, ShiftViT \cite{wang2022da0} provocatively suggests that even zero-parameter shift operations can replace attention in a hierarchical ViT, achieving multi-scale representation on par with Swin Transformer, implying that the overall hierarchical structure might be more critical than the specific token mixing mechanism for generating effective multi-scale features.

In conclusion, the development of pyramid and multi-scale feature representation has been crucial for expanding Vision Transformers' applicability beyond image classification to complex dense prediction tasks. This evolution has seen the emergence of dedicated hierarchical architectures like PVT, MViT, and the highly influential Swin Transformer, which efficiently capture multi-scale context and fine-grained details by integrating spatial reduction and window-based attention mechanisms. Recent advancements in pre-training, such as UM-MAE, have further optimized these architectures, leading to simpler yet more powerful designs like Hiera. While hierarchical backbones remain a dominant paradigm, the success of approaches like ViTDet demonstrates that multi-scale features can also be effectively derived through post-processing of plain ViT outputs. The emergence of novel architectures like Vision-RWKV and MambaVision further highlights the ongoing exploration of efficient and versatile solutions for multi-scale vision problems, emphasizing that the goal of robust multi-scale representation can be achieved through diverse architectural and training strategies.