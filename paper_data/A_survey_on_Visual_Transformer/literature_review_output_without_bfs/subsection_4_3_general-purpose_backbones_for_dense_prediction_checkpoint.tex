\subsection{General-Purpose Backbones for Dense Prediction}

The initial success of Vision Transformers (ViTs) in image classification, pioneered by \cite{ViT}, quickly highlighted their potential but also exposed fundamental limitations when applied to dense prediction tasks. The original ViT architecture suffered from quadratic computational complexity with respect to image resolution and inherently lacked the hierarchical, multi-scale feature representations crucial for precise localization and structured prediction in tasks like object detection, semantic segmentation, and depth estimation \cite{zhou2021rtn}. This spurred a critical wave of architectural innovations aimed at transforming ViTs from classification-centric models into versatile, general-purpose backbones capable of competing with, and often surpassing, traditional Convolutional Neural Networks (CNNs) across a wide array of dense vision challenges.

Building upon the efficient, localized attention mechanisms and hierarchical designs detailed in Section 4.1 and 4.2, architectures like the \textit{Swin Transformer} \cite{liu2021ljs} were pivotal in establishing ViTs as formidable general-purpose backbones for dense prediction. Swin's progressive merging of image patches and computation of self-attention within local, shifted windows effectively generated a feature pyramid, mirroring the multi-scale representations characteristic of CNNs. This hierarchical structure enabled the capture of both fine-grained local details and broader contextual information, which is indispensable for pixel-level understanding. Consequently, Swin Transformer achieved state-of-the-art performance across dense prediction benchmarks, including object detection on COCO and semantic segmentation on ADE20K, solidifying its role as a new paradigm for vision backbones. The adaptability of Swin's hierarchical design was further exemplified by \cite{li20233lv} with SwinCross, which integrated cross-modal attention for 3D medical image segmentation, showcasing its potential in specialized dense prediction domains requiring robust multi-scale features.

Concurrently, other hierarchical ViTs contributed significantly to this paradigm shift. The \textit{Pyramid Vision Transformer (PVT)} \cite{PVT} demonstrated that convolution-free pyramid structures could directly generate multi-scale features purely through attention mechanisms. Similarly, \textit{Multiscale Vision Transformers (MViT)} \cite{MViT} explored efficient multi-scale processing by progressively pooling tokens to reduce sequence length while increasing channel depth, particularly for video applications. These hierarchical backbones provided the essential feature pyramids that could be seamlessly integrated with lightweight convolutional decoders or specialized heads, a common and highly effective practice in dense prediction frameworks. The synergy lies in leveraging the ViT encoder's strength in global context modeling and robust feature extraction across scales, while employing CNN-based decoders for efficient upsampling, precise spatial localization, and recovery of fine-grained details crucial for pixel-level tasks. For instance, \cite{DPT} demonstrated the effectiveness of a Vision Transformer-based dense prediction Transformer, leveraging these multi-scale features for tasks like depth estimation and semantic segmentation. Furthermore, \cite{panboonyuen20218r7} showcased the superior performance of Swin Transformer backbones when coupled with various decoder designs like U-Net, PSPNet, and FPN for semantic segmentation on remote sensing images, underscoring the plug-and-play nature and complementary strengths of this encoder-decoder paradigm. \cite{chen202174h} utilized ViT-V-Net, a volumetric ViT, for unsupervised medical image registration, underscoring their utility in complex 3D dense prediction.

The efficiency and robustness of these hierarchical ViTs were further bolstered by advancements in pre-training strategies tailored for their unique architectures. While Masked Autoencoders (MAE) \cite{community_11} proved highly effective for vanilla ViTs, adapting them to pyramid-based ViTs with local operators posed a challenge. \cite{li2022ow4} addressed this with Uniform Masking (UM-MAE), enabling efficient MAE pre-training for models like Swin and PVT. This innovation significantly improved pre-training efficiency and fine-tuning performance on downstream dense prediction tasks, demonstrating that effective self-supervised learning is crucial for unlocking the full potential of these complex backbones. Further reinforcing this, \cite{zhang2022msa} introduced HiViT, a hierarchical ViT designed for efficient Masked Image Modeling (MIM), achieving competitive accuracy with faster pre-training compared to Swin-B, and demonstrating performance gains in detection and segmentation. Interestingly, \cite{ryali202339q} introduced Hiera, a simplified hierarchical ViT that, when combined with strong MAE pre-training, achieved competitive accuracy while being significantly faster. This work critically suggests that a substantial portion of performance gains might stem from robust pre-training rather than solely from intricate architectural "bells-and-whistles," prompting a re-evaluation of design complexity versus pre-training efficacy.

Despite the rapid convergence on hierarchical designs, subsequent work provocatively questioned their absolute indispensability for dense prediction. \cite{li2022raj} explored the use of plain, non-hierarchical ViTs as backbones for object detection. Their work, named ViTDet, demonstrated that with minimal adaptations, such as constructing a simple feature pyramid from a single-scale feature map and incorporating limited cross-window propagation, these seemingly less suitable architectures could achieve competitive results. This finding suggests that the rich, global representations learned by original ViTs, when properly fine-tuned and augmented with basic multi-scale processing, possess significant inherent potential for dense tasks, offering a simpler alternative to complex hierarchical designs. Furthermore, challenging the conventional "pre-train & fine-tune" paradigm, \cite{hong2022ks6} demonstrated that ViT-based object detectors could be trained effectively from scratch. Their empirical study revealed that specific architectural changes and extended training epochs played critical roles, suggesting that while large-scale pre-training is powerful, it is not always a prerequisite for competitive performance in dense prediction tasks, especially with careful design and training.

The adaptability of ViTs as general-purpose backbones extends beyond 2D images to higher dimensions. \cite{wang2022gq4} demonstrated the remarkable ability to adapt a standard 2D ViT architecture to 3D vision tasks, such as object classification and point cloud segmentation, by simply "inflating" the patch embedding and token sequence. This "Simple3D-Former" highlights the inherent generalizability of Transformer architectures across different data dimensions, further solidifying their role as versatile backbones for complex spatial reasoning. This evolution has firmly established ViTs as competitive, and often superior, alternatives to traditional CNN backbones, demonstrating their profound adaptability and representational power across diverse vision challenges, including complex scene understanding tasks like traversable area detection \cite{urrea20245k4}.

In summary, the journey of Vision Transformers from classification models to general-purpose backbones for dense prediction has been characterized by profound architectural innovations. The introduction of hierarchical structures and efficient attention mechanisms, most notably the Swin Transformer, effectively addressed the initial limitations of quadratic complexity and the lack of multi-scale features. Subsequent research focused on optimizing pre-training for these hierarchical models, exploring the effective integration of ViT encoders with CNN decoders, and even challenging the fundamental necessity of complex hierarchical designs for all dense tasks. These advancements have collectively made ViTs highly effective and adaptable, pushing the boundaries of performance in dense prediction tasks and demonstrating their capacity to learn robust, transferable representations for complex visual understanding.