\subsection*{Background: The Rise of Transformers in Computer Vision}

The landscape of computer vision has recently undergone a profound paradigm shift, transitioning from the long-standing dominance of Convolutional Neural Networks (CNNs) to the ascendancy of Transformer architectures. This evolution marks a fundamental re-evaluation of how visual information is processed and understood, driven by the pursuit of more comprehensive and globally aware visual representations.

For several decades, CNNs served as the cornerstone of computer vision, achieving unparalleled success across a myriad of tasks, including image classification \cite{resnet}, object detection \cite{fasterrcnn}, and semantic segmentation \cite{unet}. Their efficacy stemmed from inherent inductive biases that are particularly well-suited for processing grid-like image data. These biases include local receptive fields, which enable the extraction of fine-grained local features; weight sharing, which promotes parameter efficiency and translation equivariance; and hierarchical feature extraction, where deeper layers learn increasingly abstract representations. These characteristics allowed CNNs to learn robust visual patterns efficiently, even with moderately sized datasets, by effectively modeling local spatial correlations. However, a critical limitation of CNNs lies in their inherently local nature; capturing long-range dependencies and global contextual information often requires stacking numerous layers or employing complex pooling operations, which can be computationally intensive and may still struggle with truly global relationships across an entire image.

Concurrently, the Transformer architecture \cite{attention} revolutionized the field of Natural Language Processing (NLP). Originally designed for sequence-to-sequence tasks, Transformers leverage a powerful self-attention mechanism that allows the model to weigh the importance of different elements within an input sequence, regardless of their position. This capability enables Transformers to effectively model long-range dependencies and capture global context across entire sentences or documents, leading to remarkable achievements in tasks such as machine translation, text generation \cite{gpt}, and language understanding \cite{bert}. The success of Transformers in NLP highlighted their capacity for flexible, data-driven learning of relationships without strong prior assumptions about locality, a stark contrast to the fixed local kernels of CNNs.

The compelling rationale for extending these global context modeling capabilities to visual data was to overcome the aforementioned limitations of CNNs. Researchers hypothesized that a model capable of directly perceiving global relationships across an entire image, rather than building them up incrementally from local operations, could lead to a more holistic and powerful understanding of visual scenes. This intellectual leap aimed to unlock new avenues for visual representation learning, particularly for tasks requiring a broad contextual understanding.

This vision materialized with the groundbreaking introduction of the Vision Transformer (ViT) \cite{vit}. ViT demonstrated that a standard Transformer encoder, directly applied to sequences of non-overlapping image patches, could achieve competitive performance in image classification, effectively bypassing convolutions entirely. The model treated images as sequences of flattened patches, linearly embedded them into a higher-dimensional space, and then processed these "visual tokens" using a conventional Transformer encoder. This seminal work fundamentally altered the perception of how visual data could be processed, proving that the self-attention mechanism, without explicit convolutional inductive biases, was powerful enough to learn visual representations. However, the initial ViT models faced significant challenges that underscored the differences between natural language and vision data. They were notably data-hungry, requiring massive pre-training datasets (e.g., JFT-300M) to outperform state-of-the-art CNNs, primarily due to their lack of inherent inductive biases for local features, which CNNs possess. Furthermore, ViTs suffered from high computational costs, particularly the quadratic complexity of global self-attention with respect to the number of image patches, making them inefficient for high-resolution images. They also inherently produced a single-scale feature map, limiting their direct applicability to dense prediction tasks like object detection and segmentation that typically require multi-scale feature hierarchies.

These initial limitations—data inefficiency, quadratic computational complexity, and a lack of inherent inductive biases for local features and multi-scale representation—became the primary drivers for the explosion of research that followed. The subsequent sections of this review will systematically explore the architectural innovations, training paradigms, and diverse applications that have defined the rapid evolution of Vision Transformers, detailing how the research community has sought to address these foundational challenges and unlock their full potential across the spectrum of computer vision tasks.