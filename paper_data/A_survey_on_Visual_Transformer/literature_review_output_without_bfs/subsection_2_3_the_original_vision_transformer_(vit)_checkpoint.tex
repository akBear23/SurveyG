\subsection{The Original Vision Transformer (ViT)}

The introduction of the Vision Transformer (ViT) by \cite{Dosovitskiy2020} represented a watershed moment in computer vision, fundamentally challenging the long-standing dominance of Convolutional Neural Networks (CNNs). This seminal work demonstrated that a pure Transformer architecture, previously a cornerstone of natural language processing, could be effectively adapted to process visual data, achieving state-of-the-art performance on image recognition tasks. This subsection details the architectural principles of the original ViT, its groundbreaking achievements, and critically examines the inherent limitations that subsequently spurred extensive research and innovation in the field.

The core idea behind the Vision Transformer is to treat an image as a sequence of flattened patches, analogous to how a Transformer processes a sequence of words in text. Specifically, an input image is first partitioned into a grid of non-overlapping, fixed-size square patches, typically $16 \times 16$ pixels. Each of these patches is then flattened into a 1D vector and linearly projected into a higher-dimensional embedding space, forming a sequence of "visual tokens." To capture global image representations, a learnable "class token" is prepended to this sequence, whose final state after Transformer processing serves as the aggregated image representation for classification. Crucially, since Transformers are inherently permutation-invariant and thus lack intrinsic knowledge of spatial relationships, learnable positional embeddings are added to the patch embeddings to encode their original spatial arrangement within the image. This augmented sequence of tokens is then fed into a standard Transformer encoder, which consists of multiple identical layers. Each layer comprises a multi-head self-attention (MSA) block and a multi-layer perceptron (MLP) block, interspersed with residual connections and layer normalization. The MSA mechanism allows the model to weigh the importance of all other patches when processing a given patch, thereby capturing global dependencies and long-range interactions across the entire image. Finally, an MLP head attached to the output of the class token performs the image classification \cite{Dosovitskiy2020, han2020yk0}.

The groundbreaking achievement of the original ViT was its ability to match or even surpass the performance of state-of-the-art CNNs on large-scale image recognition benchmarks, including ImageNet, ImageNet-21k, and JFT-300M. This success was particularly evident when ViT models were pre-trained on massive datasets, demonstrating the Transformer's immense capacity for learning rich visual representations from vast amounts of data. This marked a significant paradigm shift, proving that the global context modeling capabilities of Transformers could be highly effective for visual understanding, moving beyond the local, hierarchical feature extraction paradigm of CNNs \cite{Dosovitskiy2020}.

Despite its impressive performance, the original ViT architecture faced several critical limitations that became the primary drivers for subsequent research efforts. One of the most significant challenges was its heavy reliance on massive pre-training datasets to achieve competitive performance, often requiring orders of magnitude more data than CNNs to generalize effectively. This "data hunger" stems from the Transformer's inherent lack of inductive biases for local features, such as translation equivariance and locality, which are naturally embedded in the design of CNNs through their convolutional filters. Consequently, ViT had to learn these fundamental visual priors from scratch, necessitating vast amounts of data to implicitly acquire them \cite{Dosovitskiy2020, han2020yk0}. This limitation presented a major barrier to its widespread adoption, especially for tasks or domains where large labeled datasets are unavailable. Addressing this data dependency became a central focus, leading to the development of data-efficient training strategies, including knowledge distillation and self-supervised learning paradigms, which are explored in detail in Section 3.1 and Section 3.4, respectively.

Another critical limitation of the original ViT was its high computational cost, primarily due to the global self-attention mechanism. The self-attention operation computes relationships between every pair of input tokens, resulting in a computational complexity that scales quadratically with the number of image patches (tokens). For high-resolution images, this quadratic scaling translates into prohibitive computational expense and memory consumption, making real-time inference and deployment on resource-constrained devices challenging \cite{han2020yk0}. This inefficiency motivated extensive research into developing more computationally efficient attention mechanisms and hierarchical architectures, which are discussed in Section 4.1.

Furthermore, the pure patch-based tokenization and global attention of the original ViT meant it lacked the inherent inductive biases for local feature extraction and hierarchical representation learning that are characteristic of CNNs. While effective for capturing global context, this design choice often resulted in ViT struggling to capture fine-grained local details and build multi-scale feature maps crucial for dense prediction tasks like object detection and semantic segmentation. This limitation meant that ViT's performance could be suboptimal on smaller datasets or tasks requiring precise localization, where CNNs traditionally excelled. This challenge spurred innovations in improved tokenization strategies (Section 3.3) and the integration of convolutional inductive biases through hybrid architectures (Section 5.1).

In conclusion, the original Vision Transformer by \cite{Dosovitskiy2020} undeniably revolutionized computer vision by demonstrating the power of pure Transformer architectures for image recognition. Its ability to achieve state-of-the-art performance, particularly when pre-trained on massive datasets, marked a significant shift in the field. However, its initial form presented notable limitations: a heavy reliance on extensive pre-training data due to a lack of inherent inductive biases, high computational costs stemming from global self-attention, and a struggle to capture fine-grained local features. These challenges were not merely drawbacks but became the crucial impetus for extensive subsequent research, driving the development of data-efficient training strategies, architectural innovations for efficiency and multi-scale representation, and the strategic integration of convolutional priors, all of which form the organizational basis for the subsequent sections of this review.