\subsection{Lightweight and Real-time Vision Systems}

The increasing computational demands of Vision Transformers (ViTs) present a significant hurdle for their deployment on resource-constrained devices, such as mobile phones, embedded systems, and edge AI platforms, where real-time inference, low latency, and energy efficiency are paramount. This subsection critically examines the multifaceted efforts to develop lightweight and efficient ViT systems, focusing on architectural optimizations, novel attention mechanisms, model compression techniques, and hardware-aware designs that collectively aim to balance accuracy with computational speed for practical deployment.

A primary strategy for achieving efficiency involves the development of **hybrid architectures** that strategically integrate convolutional inductive biases with Transformer blocks, tailored specifically for mobile and edge environments. \cite{MobileViT} pioneered this direction with MobileViT, a lightweight, general-purpose Vision Transformer that combines the local feature extraction strengths of CNNs with the global reasoning capabilities of Transformers within a mobile-friendly design. This integration allows for a more efficient capture of both local and global dependencies, reducing the computational overhead typically associated with pure ViTs. Following this, \cite{EdgeViT} introduced EdgeViT, an efficient hierarchical ViT specifically engineered for on-device image classification, optimizing its architecture for improved latency and throughput on edge AI platforms. These models exemplify a direct architectural approach to creating ViTs that are inherently efficient for resource-limited settings.

Beyond general hybrid designs, **hardware-aware architectural optimizations** have emerged as crucial for maximizing real-time performance. These approaches consider the specific characteristics of target deployment platforms. \cite{li2022a4u} proposed Next-ViT, a "next-generation" Vision Transformer designed for efficient deployment in realistic industrial scenarios. Next-ViT achieves CNN-like inference speeds with ViT-level performance by developing deployment-friendly Next Convolution Blocks (NCB) and Next Transformer Blocks (NTB), integrated through a novel Next Hybrid Strategy (NHS). This design demonstrates superior latency/accuracy trade-offs on platforms like TensorRT and CoreML, highlighting the importance of co-designing models with their target hardware. Similarly, \cite{xia2022dnj} introduced TRT-ViT, a TensorRT-oriented Vision Transformer, which directly optimizes for hardware latency by deriving practical design guidelines for deployment-friendly networks, resulting in significantly faster inference speeds across various visual tasks. In a related vein, \cite{nag2023cfn} presented ViTA, a configurable hardware accelerator for ViT inference targeting highly resource-constrained edge computing devices, emphasizing optimizations like head-level pipelining and inter-layer MLP optimizations to avoid repeated off-chip memory accesses and achieve high hardware utilization.

Another significant area of innovation focuses on **rethinking the self-attention mechanism** itself, which is often the computational bottleneck in ViTs due to its quadratic complexity. Efforts include designing more efficient attention variants or replacing it with simpler token mixers. \cite{song20215tk} proposed UFO-ViT, a Unit Force Operated Vision Transformer, which achieves linear computational complexity by modifying the self-attention mechanism to eliminate non-linearity, leading to high performance with reduced resource demands. Challenging the perceived indispensability of attention, \cite{wang2022da0} introduced ShiftViT, demonstrating that an extremely simple, parameter-free shift operation, which merely exchanges channels between neighboring features, can replace attention layers while maintaining competitive performance. This suggests that the overall architectural design of ViTs might be more critical than the specific attention mechanism. Furthermore, \cite{ren2022ifo} explored Dynamic Window Vision Transformers (DW-ViT), which go beyond fixed single-scale windows by assigning different window sizes to various head groups and dynamically fusing multi-scale information, enhancing modeling potential while maintaining efficiency. More recently, \cite{hatamizadeh2024xr6} proposed MambaVision, a hybrid Mamba-Transformer backbone that redesigns the Mamba formulation to efficiently model visual features, achieving state-of-the-art performance and throughput by equipping the Mamba architecture with self-attention blocks, showcasing the potential of novel sequence modeling techniques for vision. A comprehensive review by \cite{heidari2024d9k} further categorizes and analyzes various redesigned attention mechanisms within ViTs aimed at enhancing efficiency.

Beyond architectural modifications, **model compression techniques** are indispensable for adapting existing ViTs for lightweight deployment.
\textbf{Quantization} reduces the precision of model weights and activations, significantly decreasing memory footprint and accelerating inference. \cite{li20229zn} proposed Q-ViT, a fully differentiable quantization method for ViTs where both quantization scales and bit-widths are learnable, enabling head-wise bit-width allocation to squeeze model size with minimal performance drop. This work highlighted that Multi-head Self-Attention (MSA) and GELU layers are particularly sensitive to quantization. Further, \cite{li20223n5} and \cite{sun2022nny} explored FPGA-aware automatic acceleration frameworks for ViTs with mixed-scheme quantization, demonstrating significant frame rate improvements on hardware with minor accuracy drops.
\textbf{Pruning} aims to remove redundant parameters or connections from the model. \cite{hou2022ver} introduced a multi-dimensional ViT compression paradigm via dependency-guided Gaussian Process Search, jointly reducing redundancy from attention head, neuron, and sequence dimensions. \cite{yang20210bg} proposed global Vision Transformer pruning with Hessian-aware saliency, leading to new efficient architectures (NViT) with substantial FLOPs and parameter reductions.
\textbf{Knowledge Distillation}, while discussed in Section 3.1 for data-efficient training, also plays a crucial role in model compression for deployment, where a smaller, lightweight ViT student model learns from a larger, more powerful teacher model. \cite{yu2022iy0} presented a unified ViT compression framework that seamlessly assembles pruning, layer skipping, and knowledge distillation, formulating a budget-constrained optimization to jointly learn model weights and compression configurations.

Finally, **Neural Architecture Search (NAS)** offers an automated approach to discover highly efficient ViT architectures tailored for specific constraints. \cite{chen202199v} proposed searching not only the architecture but also the search space of Vision Transformers, leading to models (S3) that achieve superior performance compared to manually designed efficient ViTs, demonstrating the potential of automated design for lightweight models.

These lightweighting principles are particularly critical for **application-specific scenarios** where real-time performance on resource-constrained devices is paramount. In automated plant disease classification, \cite{borhani2022w8x} developed custom lightweight CNN and Transformer building blocks and novel hybrid CNN-ViT architectures to achieve real-time classification, explicitly addressing the speed-accuracy trade-off. Similarly, \cite{tabbakh2023ao7} proposed TLMViT, a hybrid model combining transfer learning-based CNNs with a ViT for improved accuracy in plant disease classification, while \cite{wu2021nmg} focused on multi-granularity feature extraction for accurate tomato leaf disease recognition. For remote sensing, a common strategy involves combining CNN backbones with Transformer heads to balance local texture analysis with global scene understanding. \cite{deng2021man} proposed CTNet, a joint CNN-ViT framework, while \cite{song202479c} and \cite{song2025idg} introduced enhanced ViT-based object detectors (QAGA-Net, ODDL-Net) that address sparse data distribution and optimize feature pyramid networks for better efficiency and accuracy. \cite{sha2022ae0} introduced MITformer, a multi-instance ViT for remote sensing scene classification, focusing on highlighting key local features for efficiency. These works demonstrate how tailored hybrid designs and optimized data handling can make ViTs practical for specialized, real-world applications.

Despite significant progress across architectural design, attention mechanisms, and compression techniques, the fundamental trade-off between model complexity, inference latency, energy consumption, and accuracy remains a central challenge. While hybrid architectures, hardware-aware designs, and various compression methods have shown promise, a universally optimal lightweight ViT for all edge scenarios is yet to emerge. Future research will likely continue to explore more sophisticated hybrid designs, novel attention mechanisms with reduced computational overhead, and advanced automated architectural search (NAS) techniques tailored for specific hardware constraints. Furthermore, advancements in efficient self-supervised pre-training will be crucial to reduce data dependency for smaller models, ensuring robust and energy-efficient vision systems that can operate effectively in diverse real-world, resource-constrained environments.