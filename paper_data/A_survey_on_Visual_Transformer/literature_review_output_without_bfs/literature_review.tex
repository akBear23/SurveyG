\documentclass[12pt,a4paper]{article}
    \usepackage[utf8]{inputenc}
    \usepackage[T1]{fontenc}
    \usepackage{amsmath,amsfonts,amssymb}
    \usepackage{graphicx}
    \usepackage[margin=2.5cm]{geometry}
    \usepackage{setspace}
    \usepackage{natbib}
    \usepackage{url}
    \usepackage{hyperref}
    \usepackage{booktabs}
    \usepackage{longtable}
    \usepackage{array}
    \usepackage{multirow}
    \usepackage{wrapfig}
    \usepackage{float}
    \usepackage{colortbl}
    \usepackage{pdflscape}
    \usepackage{tabu}
    \usepackage{threeparttable}
    \usepackage{threeparttablex}
    \usepackage[normalem]{ulem}
    \usepackage{makecell}
    \usepackage{xcolor}

    % Set line spacing
    \doublespacing

    % Configure hyperref
    \hypersetup{
        colorlinks=true,
        linkcolor=blue,
        filecolor=magenta,      
        urlcolor=cyan,
        citecolor=red,
    }

    % Title and author information
    \title{A Comprehensive Literature Review with Self-Reflection}
    \author{Literature Review}
    \date{\today}

    \begin{document}

    \maketitle

    % Abstract (optional)
    \begin{abstract}
    This literature review provides a comprehensive analysis of recent research in the field. The review synthesizes findings from 367 research papers, identifying key themes, methodological approaches, and future research directions.
    \end{abstract}

    \newpage
    \tableofcontents
    \newpage

    \label{sec:introduction}

\section{Introduction}
\label{sec:introduction}

\subsection{Background: The Rise of Transformers in Computer Vision}
\label{sec:1\_1\_background:\_the\_rise\_of\_transformers\_in\_computer\_vision}

The landscape of computer vision has recently undergone a profound paradigm shift, transitioning from the long-standing dominance of Convolutional Neural Networks (CNNs) to the ascendancy of Transformer architectures. This evolution marks a fundamental re-evaluation of how visual information is processed and understood, driven by the pursuit of more comprehensive and globally aware visual representations.

For several decades, CNNs served as the cornerstone of computer vision, achieving unparalleled success across a myriad of tasks, including image classification \cite{resnet}, object detection \cite{fasterrcnn}, and semantic segmentation \cite{unet}. Their efficacy stemmed from inherent inductive biases that are particularly well-suited for processing grid-like image data. These biases include local receptive fields, which enable the extraction of fine-grained local features; weight sharing, which promotes parameter efficiency and translation equivariance; and hierarchical feature extraction, where deeper layers learn increasingly abstract representations. These characteristics allowed CNNs to learn robust visual patterns efficiently, even with moderately sized datasets, by effectively modeling local spatial correlations. However, a critical limitation of CNNs lies in their inherently local nature; capturing long-range dependencies and global contextual information often requires stacking numerous layers or employing complex pooling operations, which can be computationally intensive and may still struggle with truly global relationships across an entire image.

Concurrently, the Transformer architecture \cite{attention} revolutionized the field of Natural Language Processing (NLP). Originally designed for sequence-to-sequence tasks, Transformers leverage a powerful self-attention mechanism that allows the model to weigh the importance of different elements within an input sequence, regardless of their position. This capability enables Transformers to effectively model long-range dependencies and capture global context across entire sentences or documents, leading to remarkable achievements in tasks such as machine translation, text generation \cite{gpt}, and language understanding \cite{bert}. The success of Transformers in NLP highlighted their capacity for flexible, data-driven learning of relationships without strong prior assumptions about locality, a stark contrast to the fixed local kernels of CNNs.

The compelling rationale for extending these global context modeling capabilities to visual data was to overcome the aforementioned limitations of CNNs. Researchers hypothesized that a model capable of directly perceiving global relationships across an entire image, rather than building them up incrementally from local operations, could lead to a more holistic and powerful understanding of visual scenes. This intellectual leap aimed to unlock new avenues for visual representation learning, particularly for tasks requiring a broad contextual understanding.

This vision materialized with the groundbreaking introduction of the Vision Transformer (ViT) \cite{vit}. ViT demonstrated that a standard Transformer encoder, directly applied to sequences of non-overlapping image patches, could achieve competitive performance in image classification, effectively bypassing convolutions entirely. The model treated images as sequences of flattened patches, linearly embedded them into a higher-dimensional space, and then processed these "visual tokens" using a conventional Transformer encoder. This seminal work fundamentally altered the perception of how visual data could be processed, proving that the self-attention mechanism, without explicit convolutional inductive biases, was powerful enough to learn visual representations. However, the initial ViT models faced significant challenges that underscored the differences between natural language and vision data. They were notably data-hungry, requiring massive pre-training datasets (e.g., JFT-300M) to outperform state-of-the-art CNNs, primarily due to their lack of inherent inductive biases for local features, which CNNs possess. Furthermore, ViTs suffered from high computational costs, particularly the quadratic complexity of global self-attention with respect to the number of image patches, making them inefficient for high-resolution images. They also inherently produced a single-scale feature map, limiting their direct applicability to dense prediction tasks like object detection and segmentation that typically require multi-scale feature hierarchies.

These initial limitations—data inefficiency, quadratic computational complexity, and a lack of inherent inductive biases for local features and multi-scale representation—became the primary drivers for the explosion of research that followed. The subsequent sections of this review will systematically explore the architectural innovations, training paradigms, and diverse applications that have defined the rapid evolution of Vision Transformers, detailing how the research community has sought to address these foundational challenges and unlock their full potential across the spectrum of computer vision tasks.
\subsection{Scope and Structure of the Review}
\label{sec:1\_2\_scope\_\_and\_\_structure\_of\_the\_review}

This literature review offers a systematic and critical examination of Visual Transformers (ViTs), charting their rapid evolution and profound impact on the field of computer vision. Its primary objective is to serve as an essential roadmap, guiding the reader through the intellectual trajectory of ViTs from their foundational concepts to advanced architectural paradigms, diverse applications, and persistent challenges. The review aims to present a coherent narrative that emphasizes the evolution of methodologies, the intricate interplay between different architectural designs, and the continuous drive for innovation in visual understanding. A central theme woven throughout is the inherent tension between the expressive power and global receptive field of full self-attention versus the computational efficiency and inductive biases offered by more localized, hierarchical, or even non-attention-based designs.

The review is meticulously organized into seven main sections, each building upon the preceding one to offer a comprehensive understanding of the ViT landscape. This structure is designed to progressively unfold the complexities of ViT research, starting with fundamental principles and moving towards cutting-edge developments and future directions.

Section \ref{sec:introduction} (Introduction) establishes the foundational context for Visual Transformers, outlining the historical paradigm shift from Convolutional Neural Networks (CNNs) to Transformer architectures in computer vision. It delineates the core motivation for applying Transformers to images and sets the stage by detailing the scope and organizational framework of this comprehensive review.

Following this, Section \ref{sec:foundational\_concepts} (Foundational Concepts of Vision Transformers) lays the essential groundwork. It details the core components of the Transformer architecture, elucidates the process of image tokenization and positional encoding, and introduces the seminal Vision Transformer (ViT) model, while also highlighting its initial limitations. This section provides the necessary theoretical understanding to appreciate subsequent advancements.

The subsequent sections delve into the field's rapid advancements and problem-solving efforts. Section \ref{sec:optimizing\_early\_vits} (Optimizing Early Vision Transformers: Data Efficiency and Stability) explores crucial early enhancements developed to mitigate ViT's initial data hunger through strategies like knowledge distillation and the emergence of self-supervised learning paradigms. It also examines architectural refinements that enabled deeper and more stable ViT models, alongside improved tokenization methods for better local feature capture, addressing the practical hurdles that limited early ViT adoption.

Section \ref{sec:scaling\_vits} (Scaling ViTs: Hierarchical and Efficient Designs for General Vision Tasks) marks a critical phase in ViT development, focusing on architectural innovations that addressed the original ViT's quadratic computational complexity and lack of multi-scale feature representation. This section details the emergence of efficient attention mechanisms, such as window-based and shifted attention, and the development of hierarchical structures for multi-scale feature generation. These innovations transformed ViTs into versatile, general-purpose backbones capable of excelling in dense prediction tasks, thereby expanding their applicability beyond image classification.

A significant intellectual trajectory is explored in Section \ref{sec:beyond\_pure\_attention} (Beyond Pure Attention: Hybrid Designs and Alternative Token Mixers). This section delves into hybrid architectures that strategically integrate convolutional inductive biases, combining the strengths of CNNs and Transformers. It also examines models that rethink the necessity of complex self-attention mechanisms, proposing simpler, more efficient token mixing operations, and highlights a fascinating convergence where ViT design principles have influenced the modernization of traditional CNNs, blurring the lines between these once distinct paradigms.

Section \ref{sec:vits\_in\_action} (ViTs in Action: Diverse Applications and Specialized Adaptations) showcases the broad applicability and versatility of ViTs across various computer vision tasks and specialized domains. It details their successful integration into frameworks for fundamental tasks like object detection and semantic segmentation, their influence in critical areas such as medical image analysis, and efforts to develop lightweight and real-time ViT systems for deployment on resource-constrained devices.

Finally, Section \ref{sec:synthesizing\_the\_landscape} (Synthesizing the Landscape: Challenges and Future Trajectories) provides a comprehensive summary of key developments, critically discusses remaining open challenges (e.g., computational costs, data efficiency, interpretability), and explores promising emerging trends. This includes the development of large-scale foundation models and the pursuit of generalist Vision Transformers, alongside crucial ethical considerations associated with these powerful vision AI technologies.

Throughout these sections, the review provides a structured exploration of this dynamic landscape, offering a deep critical evaluation of the methodologies, comparative analysis of approaches, and a discussion of the underlying reasons for existing limitations. By tracing the continuous evolution and addressing the persistent challenges, this review aims to guide future research and foster responsible innovation in visual understanding.


\label{sec:foundational_concepts_of_vision_transformers}

\section{Foundational Concepts of Vision Transformers}
\label{sec:foundational\_concepts\_of\_vision\_transformers}

\subsection{The Transformer Architecture Revisited}
\label{sec:2\_1\_the\_transformer\_architecture\_revisited}

The Transformer architecture, initially introduced by Vaswani et al. \cite{vaswani2017attention} for sequence-to-sequence tasks in natural language processing (NLP), marked a profound paradigm shift by entirely eschewing recurrent and convolutional layers in favor of a purely attention-based mechanism. This design choice enabled unprecedented parallelism and the capture of long-range dependencies, fundamentally altering how sequential data is processed. Grasping these foundational elements is crucial for understanding how Transformers operate on sequential data and how these principles were subsequently adapted for visual inputs, forming the bedrock of Vision Transformers (ViTs).

At its core, the original Transformer operates on an encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations, while the decoder generates an output sequence one symbol at a time, attending to the encoded representations. For many vision applications, particularly image classification, the architecture primarily leverages the encoder stack. The fundamental building block within both the encoder and decoder is the multi-head self-attention (MHSA) mechanism. This mechanism enables the model to dynamically weigh the importance of different input elements by computing attention scores. It achieves this by projecting input representations into three distinct learned linear transformations: Query ($\mathbf{Q}$), Key ($\mathbf{K}$), and Value ($\mathbf{V}$). The attention score for each query is calculated against all keys, determining how much focus to place on each value, thereby capturing global contextual information and long-range dependencies within the sequence. Mathematically, the scaled dot-product attention is computed as $\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}(\frac{\mathbf{QK}^T}{\sqrt{d\_k}})\mathbf{V}$, where $d\_k$ is the dimension of the keys. The "multi-head" aspect allows the model to jointly attend to information from different representation subspaces at different positions, enriching the learned relationships and providing a more comprehensive understanding of the input.

Following the self-attention layer, a position-wise feed-forward network (FFN) processes each position independently. This FFN typically consists of two linear transformations with a ReLU activation in between, applying a simple two-layer neural network to the attention outputs. To facilitate the training of deep networks and prevent issues like vanishing gradients, both the self-attention and FFN sub-layers are augmented with residual connections \cite{he2016deep}, which add the input of the sub-layer to its output, and layer normalization \cite{ba2016layer}, which normalizes the activations across the feature dimension for each sample. Furthermore, as the self-attention mechanism itself is permutation-invariant (i.e., it treats the input as a set rather than a sequence), positional encodings are added to the input embeddings. These encodings inject crucial information about the relative or absolute position of tokens in the sequence, allowing the model to understand sequence order.

The conceptual leap to Vision Transformers involved adapting these foundational principles to visual inputs. The seminal approach, as introduced by the original Vision Transformer (ViT) \cite{dosovitskiy2020image}, involves segmenting an input image into a sequence of fixed-size, non-overlapping patches. Each patch is then linearly embedded into a higher-dimensional space to create visual tokens, analogous to words in NLP. These visual tokens, along with positional encodings, are then fed into a standard Transformer encoder. This direct adaptation demonstrated the power of global self-attention for image classification, achieving competitive performance on large-scale datasets.

However, this direct application of the NLP-centric Transformer to images also exposed inherent challenges and limitations stemming from the architecture's original design choices. Firstly, the quadratic computational complexity of global self-attention with respect to the sequence length (number of patches) becomes a significant bottleneck for high-resolution images, making it computationally expensive and memory-intensive. Secondly, unlike Convolutional Neural Networks (CNNs) which possess strong inductive biases for local feature extraction, translation equivariance, and hierarchical processing, the pure Transformer architecture inherently lacks these spatial priors. This deficiency means that ViTs often require massive pre-training datasets to learn robust visual representations from scratch, a limitation not typically shared by CNNs. The reliance on explicit positional encodings, rather than inherent spatial understanding, further underscores this difference. These challenges, inherent in the direct adaptation of the foundational Transformer to the visual domain, became the primary drivers for subsequent research, motivating a wave of innovations aimed at enhancing efficiency, introducing multi-scale processing, and integrating convolutional inductive biases, which are explored in detail in later sections of this review.
\subsection{Image Tokenization and Positional Encoding}
\label{sec:2\_2\_image\_tokenization\_\_and\_\_positional\_encoding}

The successful adaptation of Transformer architectures from natural language processing (NLP) to computer vision (CV) fundamentally relies on two critical initial processing steps: image tokenization and positional encoding \cite{han2020yk0, hassija2025wq3}. Unlike Convolutional Neural Networks (CNNs) that inherently leverage inductive biases for spatial locality and translation equivariance, Transformers process input sequences without an intrinsic understanding of spatial relationships or the original arrangement of visual elements. This inherent permutation-invariance of the self-attention mechanism \cite{Vaswani2017Attention} necessitates dedicated mechanisms to transform raw image data into a sequential format and explicitly embed spatial awareness.

The foundational Vision Transformer (ViT) \cite{ViT} introduced a straightforward, yet impactful, tokenization strategy. An input image is first partitioned into a grid of fixed-size, non-overlapping square patches, typically $16 \times 16$ pixels. Each patch is then flattened into a 1D vector and linearly projected into a higher-dimensional embedding space. This process converts the 2D image into a sequence of fixed-length visual tokens, analogous to word embeddings in NLP. A special "class token" (CLS token) is prepended to this sequence, serving as a global representation for classification tasks, similar to its use in BERT \cite{Vaswani2017Attention}. While effective for demonstrating the viability of Transformers in vision, this simple patch embedding has inherent limitations. It treats each patch as an independent entity, largely disregarding fine-grained local structural information and spatial relationships \textit{within} the patch \cite{han2020yk0}. This can be particularly problematic for tasks requiring detailed local feature understanding or when dealing with smaller datasets where such inductive biases are more crucial. The fixed size of these patches also means that the model's receptive field is determined by the patch size, and changing input image resolutions would alter the number of tokens, requiring adaptations or re-training. This initial simplification, which diminishes the capture of local context, became a significant motivation for subsequent research into more sophisticated tokenization methods, such as those incorporating convolutional inductive biases or hierarchical structures, which will be explored in Section 3.3 and Section 5.1.

Complementing tokenization, positional encoding is indispensable for Transformers to comprehend the spatial arrangement of these visual tokens, as the self-attention mechanism itself is permutation-invariant \cite{Vaswani2017Attention}. Without positional information, the model would process an image's patches identically regardless of their original location, losing crucial spatial context. The original Transformer \cite{Vaswani2017Attention} (in NLP) typically employed fixed sinusoidal positional encodings, which are deterministic functions of position. In contrast, the ViT \cite{ViT} opted for learnable 1D positional embeddings. These embeddings are simply vectors, of the same dimension as the patch embeddings, that are added to each visual token based on its sequential index (corresponding to its spatial position in the flattened grid). The rationale behind learnable embeddings was their flexibility to adapt to the specific spatial patterns present in image data during training. It is critical to note that the original ViT paper, despite exploring 2D-aware and relative positional embeddings, found that simple 1D learnable embeddings performed comparably well. This unexpected efficacy was largely attributed to the ViT's extensive pre-training on massive datasets like JFT-300M, which allowed the model to implicitly learn complex spatial relationships and compensate for the lack of stronger explicit inductive biases \cite{hong2022ks6}.

However, a significant drawback of these fixed-size learnable embeddings is their lack of transferability to input images of different resolutions. If an image has a different number of patches than what the model was trained on, the pre-trained positional embeddings become incompatible, necessitating interpolation or re-training, which limits generalization \cite{wang2022gq4}. This rigidity in handling varying input sizes posed a challenge for deploying ViTs in diverse real-world scenarios where image resolutions can vary widely. This limitation spurred research into more flexible positional encoding mechanisms, including relative positional encodings and dynamic methods, which are crucial for enabling ViTs to scale effectively to different input dimensions and will be discussed in Section 4.1.

In summary, the initial strategies of simple patch-based tokenization and learnable 1D absolute positional embeddings in the original ViT were foundational in demonstrating the power of Transformers for vision tasks. These mechanisms were the necessary "glue" to adapt a sequence-processing architecture to 2D image data. However, their inherent limitations—the diminished capture of fine-grained local structural information and the inflexibility of absolute positional embeddings to varying input resolutions—became primary drivers for the extensive architectural innovations and optimizations explored in subsequent sections. These foundational concepts, along with their identified limitations, set the stage for the full Vision Transformer architecture, which is detailed next.
\subsection{The Original Vision Transformer (ViT)}
\label{sec:2\_3\_the\_original\_vision\_transformer\_(vit)}

The introduction of the Vision Transformer (ViT) by \cite{Dosovitskiy2020} represented a watershed moment in computer vision, fundamentally challenging the long-standing dominance of Convolutional Neural Networks (CNNs). This seminal work demonstrated that a pure Transformer architecture, previously a cornerstone of natural language processing, could be effectively adapted to process visual data, achieving state-of-the-art performance on image recognition tasks. This subsection details the architectural principles of the original ViT, its groundbreaking achievements, and critically examines the inherent limitations that subsequently spurred extensive research and innovation in the field.

The core idea behind the Vision Transformer is to treat an image as a sequence of flattened patches, analogous to how a Transformer processes a sequence of words in text. Specifically, an input image is first partitioned into a grid of non-overlapping, fixed-size square patches, typically $16 \times 16$ pixels. Each of these patches is then flattened into a 1D vector and linearly projected into a higher-dimensional embedding space, forming a sequence of "visual tokens." To capture global image representations, a learnable "class token" is prepended to this sequence, whose final state after Transformer processing serves as the aggregated image representation for classification. Crucially, since Transformers are inherently permutation-invariant and thus lack intrinsic knowledge of spatial relationships, learnable positional embeddings are added to the patch embeddings to encode their original spatial arrangement within the image. This augmented sequence of tokens is then fed into a standard Transformer encoder, which consists of multiple identical layers. Each layer comprises a multi-head self-attention (MSA) block and a multi-layer perceptron (MLP) block, interspersed with residual connections and layer normalization. The MSA mechanism allows the model to weigh the importance of all other patches when processing a given patch, thereby capturing global dependencies and long-range interactions across the entire image. Finally, an MLP head attached to the output of the class token performs the image classification \cite{Dosovitskiy2020, han2020yk0}.

The groundbreaking achievement of the original ViT was its ability to match or even surpass the performance of state-of-the-art CNNs on large-scale image recognition benchmarks, including ImageNet, ImageNet-21k, and JFT-300M. This success was particularly evident when ViT models were pre-trained on massive datasets, demonstrating the Transformer's immense capacity for learning rich visual representations from vast amounts of data. This marked a significant paradigm shift, proving that the global context modeling capabilities of Transformers could be highly effective for visual understanding, moving beyond the local, hierarchical feature extraction paradigm of CNNs \cite{Dosovitskiy2020}.

Despite its impressive performance, the original ViT architecture faced several critical limitations that became the primary drivers for subsequent research efforts. One of the most significant challenges was its heavy reliance on massive pre-training datasets to achieve competitive performance, often requiring orders of magnitude more data than CNNs to generalize effectively. This "data hunger" stems from the Transformer's inherent lack of inductive biases for local features, such as translation equivariance and locality, which are naturally embedded in the design of CNNs through their convolutional filters. Consequently, ViT had to learn these fundamental visual priors from scratch, necessitating vast amounts of data to implicitly acquire them \cite{Dosovitskiy2020, han2020yk0}. This limitation presented a major barrier to its widespread adoption, especially for tasks or domains where large labeled datasets are unavailable. Addressing this data dependency became a central focus, leading to the development of data-efficient training strategies, including knowledge distillation and self-supervised learning paradigms, which are explored in detail in Section 3.1 and Section 3.4, respectively.

Another critical limitation of the original ViT was its high computational cost, primarily due to the global self-attention mechanism. The self-attention operation computes relationships between every pair of input tokens, resulting in a computational complexity that scales quadratically with the number of image patches (tokens). For high-resolution images, this quadratic scaling translates into prohibitive computational expense and memory consumption, making real-time inference and deployment on resource-constrained devices challenging \cite{han2020yk0}. This inefficiency motivated extensive research into developing more computationally efficient attention mechanisms and hierarchical architectures, which are discussed in Section 4.1.

Furthermore, the pure patch-based tokenization and global attention of the original ViT meant it lacked the inherent inductive biases for local feature extraction and hierarchical representation learning that are characteristic of CNNs. While effective for capturing global context, this design choice often resulted in ViT struggling to capture fine-grained local details and build multi-scale feature maps crucial for dense prediction tasks like object detection and semantic segmentation. This limitation meant that ViT's performance could be suboptimal on smaller datasets or tasks requiring precise localization, where CNNs traditionally excelled. This challenge spurred innovations in improved tokenization strategies (Section 3.3) and the integration of convolutional inductive biases through hybrid architectures (Section 5.1).

In conclusion, the original Vision Transformer by \cite{Dosovitskiy2020} undeniably revolutionized computer vision by demonstrating the power of pure Transformer architectures for image recognition. Its ability to achieve state-of-the-art performance, particularly when pre-trained on massive datasets, marked a significant shift in the field. However, its initial form presented notable limitations: a heavy reliance on extensive pre-training data due to a lack of inherent inductive biases, high computational costs stemming from global self-attention, and a struggle to capture fine-grained local features. These challenges were not merely drawbacks but became the crucial impetus for extensive subsequent research, driving the development of data-efficient training strategies, architectural innovations for efficiency and multi-scale representation, and the strategic integration of convolutional priors, all of which form the organizational basis for the subsequent sections of this review.


\label{sec:optimizing_early_vision_transformers:_data_efficiency_and_stability}

\section{Optimizing Early Vision Transformers: Data Efficiency and Stability}
\label{sec:optimizing\_early\_vision\_transformers:\_data\_efficiency\_\_and\_\_stability}

\subsection{Data-Efficient Training and Distillation}
\label{sec:3\_1\_data-efficient\_training\_\_and\_\_distillation}

The introduction of the Vision Transformer (ViT) by \cite{dosovitskiy2020image} marked a pivotal moment in computer vision, demonstrating the potential of pure self-attention mechanisms for image recognition. However, a significant hurdle to its widespread adoption was its substantial data hunger, often necessitating pre-training on colossal proprietary datasets like JFT-300M to achieve performance competitive with state-of-the-art Convolutional Neural Networks (CNNs). This reliance on massive labeled data posed a critical barrier for researchers and practitioners without access to such resources, making data-efficient training strategies a paramount area of early research.

A foundational breakthrough in addressing ViT's data dependency was the work by Touvron et al. \cite{touvron2021training}, which introduced Data-efficient image Transformers (DeiT). This seminal paper demonstrated that ViTs could achieve competitive performance on standard datasets like ImageNet-1K without requiring extensive pre-training on external, larger datasets. The core innovation of DeiT was the effective application of knowledge distillation, a technique where a smaller "student" model learns from the output of a larger, pre-trained "teacher" model. Specifically, DeiT leveraged a strong CNN teacher (e.g., a RegNet model) to guide the training of a ViT student. The authors explored both "soft" distillation, where the student matches the teacher's softened probability distribution, and "hard" distillation, where the student is trained to predict the teacher's hard class labels. Crucially, DeiT introduced a dedicated "distillation token" alongside the standard class token, allowing the ViT student to learn directly from the teacher's predictions through an additional distillation loss. This mechanism significantly improved the student ViT's performance, enabling it to reach accuracy levels comparable to or even surpassing the teacher CNN, all while being trained solely on ImageNet-1K. This innovation was instrumental in democratizing ViT research, making the architecture accessible to a broader community.

Beyond the initial success of DeiT, subsequent research has further refined and extended distillation techniques for ViTs, often integrating them with other compression or efficiency strategies. For instance, while DeiT primarily focused on distilling class logits, \cite{wang2022pee} proposed Attention Distillation (AttnDistill) specifically for self-supervised Vision Transformer students. They argued that directly distilling information from the teacher's crucial attention mechanism to the student could significantly narrow the performance gap, especially in self-supervised learning (SSL) contexts where traditional logit-based distillation might be suboptimal. AttnDistill demonstrated superior k-NN accuracy on ImageNet-1K compared to existing self-supervised knowledge distillation methods, highlighting the importance of attention-level guidance for ViT students. This approach is particularly valuable as it is independent of the specific SSL algorithm used, offering a versatile method to improve ViT performance on memory and compute-constrained devices.

Knowledge distillation has also been integrated into broader model compression frameworks to enhance ViT efficiency and performance under resource constraints. \cite{yu2022iy0} introduced a Unified Visual Transformer Compression (UVC) framework that seamlessly combines pruning, layer skipping, and knowledge distillation. By formulating a budget-constrained, end-to-end optimization framework that jointly learns model weights, pruning ratios, and skip configurations under a distillation loss, UVC effectively compresses ViT variants like DeiT and T2T-ViT. This demonstrates that distillation is not merely a standalone data-efficiency technique but a powerful component in a multi-faceted approach to creating more practical and deployable ViTs. Similarly, \cite{li2022tl7} explored the challenging domain of fully quantized low-bit Vision Transformers (Q-ViT). They identified information distortion in low-bit quantized self-attention maps as a bottleneck and proposed a distribution-guided distillation (DGD) scheme to rectify this. Their Q-ViT models, leveraging DGD, achieved remarkable performance, even surpassing full-precision counterparts in some cases, while drastically reducing computational and memory costs. This illustrates how distillation can mitigate the performance degradation associated with extreme quantization, thereby making ViTs more viable for edge devices and further reducing their effective data-to-performance ratio by enabling smaller, more efficient models.

In summary, the initial "data hunger" of Vision Transformers was a significant impediment, but early efforts, particularly through knowledge distillation, provided crucial solutions. The advent of DeiT \cite{touvron2021training} demonstrated that ViTs could achieve strong performance on standard datasets by learning from pre-trained CNN teachers, thereby mitigating the need for massive proprietary datasets. Subsequent advancements have extended distillation beyond simple logit matching, exploring attention-based distillation for self-supervised learning \cite{wang2022pee} and integrating it into unified compression frameworks \cite{yu2022iy0} and quantization schemes \cite{li2022tl7}. These innovations collectively transformed ViTs from computationally demanding and data-intensive models into more accessible and practical architectures, laying the groundwork for their broader application across diverse computer vision tasks. While architectural advancements like the Swin Transformer (discussed in Section 4.1) contributed to computational efficiency, it is these dedicated data-efficient training strategies, primarily knowledge distillation, that directly addressed the challenge of learning powerful representations from substantially less labeled data.
\subsection{Deeper and More Stable ViT Architectures}
\label{sec:3\_2\_deeper\_\_and\_\_more\_stable\_vit\_architectures}

The initial success of Vision Transformers (ViTs) was often constrained by the inherent challenges of training very deep Transformer models, such as vanishing gradients, exploding activations, and general training instability. Overcoming these hurdles was crucial to unlock the full representational capacity of ViTs and enable the development of much deeper and more complex architectures. This subsection examines the architectural and training advancements that directly addressed these stability issues, facilitating the scaling of ViTs to unprecedented depths.

The foundational work by \cite{ViT} demonstrated the potential of a pure Transformer encoder for image recognition, but implicitly highlighted the difficulties in training such models from scratch, particularly at greater depths, often necessitating massive pre-training datasets. To directly tackle the instability encountered when scaling ViTs, \cite{CaiT} introduced several pivotal innovations. Their work, titled "Going deeper with Image Transformers," specifically focused on enabling the training of significantly deeper ViTs. A key contribution was \textbf{LayerScale}, a mechanism that adaptively re-scales residual connections within each Transformer block. This adaptive re-scaling helps to stabilize the training process by preventing activations from growing too large or vanishing too quickly, thereby mitigating gradient issues and allowing for the successful optimization of models with many more layers. Furthermore, \cite{CaiT} proposed \textbf{class-attention layers}, which refine the global representation by allowing the class token to interact with image tokens in a more nuanced way, further enhancing the model's ability to learn robust features in deep architectures. These techniques were instrumental in pushing ViT depth beyond previous limits, demonstrating the viability of very deep Transformer-based vision models.

Building upon these foundational insights, other works contributed to the overall robustness and scalability of ViTs, indirectly supporting the development of deeper models. For instance, \cite{DeiT} addressed the significant data requirements of early ViTs through knowledge distillation. While not directly an architectural stabilization technique, making ViTs trainable on smaller datasets broadened their applicability and made the exploration of deeper models more practical by reducing the computational burden of pre-training on colossal datasets. Similarly, advancements in initial tokenization by \cite{T2T-ViT} and more flexible positional encodings by \cite{CPVT} provided better initial representations and adaptability, which can contribute to the overall stability and performance of deeper networks.

The principles established by \cite{CaiT} for stabilizing deep Transformers have had a lasting impact, influencing the design of subsequent, more complex ViT variants. For example, hierarchical Vision Transformers like the Swin Transformer \cite{Swin} and Pyramid Vision Transformer \cite{PVT} introduced architectural modifications to handle multi-scale features and reduce quadratic complexity, but their ability to scale to significant depths also relies on robust training practices and implicitly benefits from the understanding of stability mechanisms. More recent hierarchical designs, such as Hiera \cite{ryali202339q} and HiViT \cite{zhang2022msa}, continue to explore efficient and deep architectures, often leveraging strong self-supervised pre-training (like Masked Autoencoders) to simplify architectural "bells-and-whistles" while maintaining performance. These models demonstrate a continued evolution, where the initial focus on stabilizing the core Transformer block has matured into designing entire deep, efficient architectures that are robust to training.

In conclusion, the journey towards deeper and more stable ViT architectures has been marked by critical innovations that directly address the inherent challenges of training very deep Transformers. Techniques like LayerScale have been instrumental in stabilizing the optimization landscape, allowing ViTs to scale to unprecedented depths and unlock greater representational power. While the field continues to explore various architectural modifications and training paradigms, the fundamental understanding of how to maintain training stability remains a cornerstone for pushing the boundaries of ViT performance and enabling the development of even larger and more complex models for diverse computer vision tasks. The ongoing challenge lies in balancing this architectural depth and representational capacity with computational efficiency and the continued search for robust, generalizable training methodologies.
\subsection{Improved Tokenization and Local Feature Capture}
\label{sec:3\_3\_improved\_tokenization\_\_and\_\_local\_feature\_capture}

The foundational Vision Transformer (ViT) \cite{Dosovitskiy2020} introduced a paradigm shift in computer vision by treating images as sequences of non-overlapping patches, processed by a standard Transformer encoder. While this approach effectively captured global dependencies, it inherently lacked the inductive biases for local structural information that are central to Convolutional Neural Networks (CNNs). This deficiency meant vanilla ViTs often required immense pre-training datasets to learn robust local features, limiting their performance on smaller datasets and tasks demanding fine-grained visual understanding. Consequently, a significant line of research has focused on enhancing ViT's ability to capture local features from the very initial stages of processing, specifically by refining the image tokenization process and embedding local inductive biases into the patch embedding layers. The goal is to enable ViTs to learn fine-grained local features more effectively from scratch, thereby reducing their reliance on massive pre-training datasets and improving performance on tasks requiring detailed visual understanding.

One direct approach to enrich the initial token representation is through progressive token structuring. Tokens-to-Token ViT (T2T-ViT) \cite{Yuan2021} exemplifies this by introducing a "Tokens-to-Token" module that iteratively aggregates adjacent tokens into a single, more context-aware token. Instead of a single linear projection of raw, non-overlapping patches, T2T-ViT employs multiple stages where small patches are first linearly embedded, and then these initial tokens are progressively merged and re-embedded into a longer sequence. This hierarchical aggregation process allows the model to capture increasingly larger receptive fields and finer-grained local details before the main Transformer encoder processes the final token sequence. This refinement significantly improves the ViT's capacity to learn robust visual representations from scratch on datasets like ImageNet, thereby reducing its heavy reliance on massive pre-training. The progressive nature of T2T-ViT effectively builds a local hierarchy of features, mimicking some aspects of CNNs' early layers, but within a token-merging framework.

Beyond progressive merging, another critical direction involves embedding convolutional inductive biases directly into the initial patch embedding layer, often referred to as a "convolutional stem." This strategy leverages the inherent strengths of convolutions—locality, translation equivariance, and hierarchical feature extraction—at the very outset of the ViT pipeline. For instance, the Convolutional Vision Transformer (CvT) \cite{Li2021} replaces the standard linear projection for patch embedding with a series of convolutional layers. This not only generates visual tokens but also inherently incorporates local feature extraction and spatial downsampling, making the initial tokens more semantically rich and spatially aware. CvT further extends this idea by using convolutional projections within its attention layers, but the initial convolutional token embedding is a key aspect of local feature capture. Similarly, CeiT (Convolution-enhanced image Transformer) \cite{d\_Ascoli2021} employs a convolutional stem to generate initial tokens, aiming to improve local feature extraction and reduce the number of parameters compared to a pure linear projection. These convolutional stems provide a more robust and efficient way to extract initial features, making ViTs more data-efficient and better suited for tasks requiring detailed local information, even with limited pre-training. The explicit integration of convolutions at this early stage directly addresses the ViT's lack of inherent local inductive biases, leading to improved performance, especially on smaller datasets.

Furthermore, the design of the initial patching mechanism itself has been revisited to enhance local context. The original ViT uses non-overlapping patches, which can lead to information loss at patch boundaries and a limited receptive field for individual tokens. To mitigate this, some approaches have introduced overlapping patch embeddings. By allowing patches to overlap, the model gains a denser sampling of the image and each token can incorporate information from its immediate neighbors, fostering a stronger sense of local connectivity. This simple yet effective modification can significantly contribute to improved local feature capture and robustness. For example, ViT-Lite \cite{Wu2021} explicitly incorporates overlapping patch embeddings to enhance the model's ability to perceive local structures and improve performance on various vision tasks without significantly increasing computational overhead. This approach is particularly beneficial for tasks where fine-grained spatial details are crucial, as it ensures that local contextual information is preserved and propagated more effectively into the Transformer layers.

In summary, the evolution of Vision Transformers has seen a dedicated focus on overcoming the original ViT's weakness in capturing fine-grained local structural information. Innovations at the initial tokenization stage, such as the progressive token merging in T2T-ViT \cite{Yuan2021}, the integration of convolutional stems in models like CvT \cite{Li2021} and CeiT \cite{d\_Ascoli2021}, and the use of overlapping patch embeddings exemplified by approaches like ViT-Lite \cite{Wu2021}, have significantly enhanced the ViT's ability to learn rich local features. These advancements directly address the challenge of data efficiency and improve performance on tasks requiring detailed visual understanding by embedding crucial inductive biases early in the processing pipeline. By refining how images are initially converted into tokens, these methods bridge the gap between global reasoning and local inductive biases, laying a stronger foundation for the subsequent Transformer layers to build upon. This early-stage enhancement is distinct from, yet complementary to, later architectural developments that introduce hierarchical structures or hybrid designs, which are explored in subsequent sections.
\subsection{Self-Supervised Pre-training Paradigms}
\label{sec:3\_4\_self-supervised\_pre-training\_paradigms}

The initial success of Vision Transformers (ViTs) was often contingent on vast amounts of labeled data, posing a significant challenge for their widespread adoption. Self-supervised learning (SSL) has emerged as a pivotal solution, enabling ViTs to learn robust and transferable visual representations from unlabeled data, thereby mitigating the need for extensive human annotation and enhancing their data efficiency. This paradigm shift draws heavily from the success of masked language modeling in Natural Language Processing (NLP), adapting similar principles to the visual domain.

One prominent SSL paradigm for ViTs is self-distillation, exemplified by methods like DINO (\cite{caron2021emerging}). DINO trains a student network to match the output of a teacher network, which is typically a momentum-updated version of the student. This approach allows ViTs to learn powerful semantic features, including remarkable emergent properties such as object segmentation, without relying on explicit labels. The core idea is to encourage the student to produce similar representations to the teacher for different views of the same image, fostering a rich understanding of visual semantics.

Inspired by BERT's masked language modeling, masked image modeling (MIM) has become another cornerstone of SSL for ViTs. BEiT (\cite{bao2022beit}) pioneered this direction by pre-training ViTs to reconstruct discrete visual tokens from masked image patches. This method effectively translates the "fill-in-the-blanks" task from text to images, forcing the model to learn meaningful contextual representations. Building upon this, Masked Autoencoders (MAE) (\cite{he2022masked}) introduced a highly scalable and efficient masked autoencoding approach. MAE masks a large portion of image patches (e.g., 75\\%) and trains the ViT encoder to reconstruct the missing pixels from the visible patches, demonstrating impressive performance and scalability. Unlike BEiT, MAE reconstructs raw pixel values, simplifying the pre-training objective and allowing for very high masking ratios, which pushes the model to learn more holistic representations. Both BEiT and MAE significantly reduce the data annotation burden by leveraging the inherent structure of images as a supervisory signal.

The power of SSL pre-training, particularly MAE, has been instrumental in unlocking ViT's full potential across various downstream tasks and architectural designs. For instance, the effectiveness of plain, non-hierarchical ViT backbones for object detection was significantly enhanced when pre-trained with MAE. ViTDet (\cite{li2022raj}) demonstrated that with minimal adaptations, a plain ViT backbone pre-trained as a Masked Autoencoder could achieve competitive results on object detection, even without the complex hierarchical designs typically employed for dense prediction tasks. This highlights how strong self-supervised pre-training can compensate for architectural inductive biases, making ViTs more versatile.

Furthermore, SSL has enabled simplifications in ViT architectures while maintaining high performance. Hiera (\cite{ryali202339q}) showed that by leveraging robust pre-training with MAE, many "bells-and-whistles" (vision-specific components) could be stripped from state-of-the-art multi-stage hierarchical ViTs without sacrificing accuracy. This resulted in simpler, faster hierarchical ViTs that are more efficient during both inference and training, underscoring the profound impact of SSL in streamlining architectural complexity. These advancements collectively demonstrate how SSL has transformed ViT training, enabling them to learn rich, transferable visual representations from vast amounts of unlabeled data, thus significantly mitigating the need for extensive human annotation and improving performance across various downstream tasks.

Despite the remarkable progress, challenges remain in understanding the full extent of emergent properties from different SSL objectives and in developing even more computationally efficient pre-training methods for increasingly larger models. Future research directions may explore multimodal SSL for ViTs, further reducing the gap between pre-training and fine-tuning, and developing theoretical frameworks to better explain the representational power gained through self-supervision.


\label{sec:scaling_vits:_hierarchical_and_efficient_designs_for_general_vision_tasks}

\section{Scaling ViTs: Hierarchical and Efficient Designs for General Vision Tasks}
\label{sec:scaling\_vits:\_hierarchical\_\_and\_\_efficient\_designs\_for\_general\_vision\_tasks}

\subsection{Window-Based and Shifted Attention}
\label{sec:4\_1\_window-based\_\_and\_\_shifted\_attention}

The initial success of Vision Transformers (ViTs) \cite{ViT} was tempered by the quadratic computational complexity of their global self-attention mechanism with respect to image resolution, posing a significant bottleneck for high-resolution images and dense prediction tasks. To overcome this, architectural innovations focused on introducing localized attention mechanisms, where self-attention is computed only within restricted spatial regions. This approach reduces complexity to linear with respect to image size, making ViTs computationally efficient and scalable.

A foundational step in this direction was the adoption of window-based attention, where the input image is partitioned into non-overlapping local windows, and self-attention is computed independently within each window. While this effectively reduces computational cost, it inherently limits the receptive field and prevents information exchange between different windows. The pivotal innovation to address this limitation while maintaining computational efficiency was introduced by the Swin Transformer \cite{Swin}. Swin Transformer proposed a hierarchical architecture that leverages a novel shifted window attention mechanism. In this design, attention is computed within non-overlapping windows in one layer, and then in the subsequent layer, the windows are shifted, allowing for connections between previously isolated regions. This alternating pattern of regular and shifted windowing enables cross-window connections and builds multi-scale feature representations, crucial for tasks requiring dense predictions, without incurring the quadratic cost of global attention. The Swin Transformer quickly became a new standard backbone, demonstrating superior performance across various computer vision tasks, including image classification, object detection, and semantic segmentation.

The hierarchical and window-based design of Swin Transformer has shown enhanced transferability compared to plain ViTs, often requiring less extensive fine-tuning for downstream tasks \cite{zhou2021rtn}. This adaptability has been leveraged in diverse applications, showcasing its versatility. For instance, the Swin Transformer architecture has been successfully adapted for medical image analysis, such as in SwinCross \cite{li20233lv}, which employs a cross-modal 3D Swin Transformer with cross-modal shifted window attention for head-and-neck tumor segmentation in PET/CT images, demonstrating its efficacy in integrating multi-modal data. Similarly, its robust performance has been validated in specialized classification tasks, achieving higher accuracy than traditional CNNs and plain ViTs for mobile-based oral cancer image classification \cite{song2024c99} and outperforming other object detection models for chicken part classification and detection \cite{peng2024kal}.

Despite the effectiveness of window-based architectures, their local nature presented new challenges, particularly concerning pre-training strategies like Masked Autoencoders (MAE). The random masking inherent in MAE pre-training can be difficult to reconcile with pyramid-based ViTs that rely on local window operators. To address this, Uniform Masking (UM) was proposed by \cite{li2022ow4}, enabling efficient and effective MAE pre-training for pyramid-based ViTs with locality. UM ensures uniform sampling across local windows and introduces secondary masking to enhance semantic learning, significantly improving pre-training efficiency and maintaining competitive fine-tuning performance.

While shifted windows are a powerful mechanism for cross-window communication, recent research has explored alternatives and questioned their absolute necessity under certain conditions. For example, \cite{li2022raj} investigated plain, non-hierarchical ViT backbones for object detection and found that competitive results could be achieved using simple window attention (without shifting) when augmented with very few cross-window propagation blocks. This suggests that explicit shifting might be less critical if other sparse mechanisms are introduced to facilitate information flow between windows. Furthermore, the Hiera model \cite{ryali202339q} demonstrated that by leveraging strong visual pretext tasks like MAE pre-training, many "bells-and-whistles" (complex vision-specific components) could be stripped from hierarchical ViTs without sacrificing accuracy. This indicates a potential synergy where powerful self-supervised pre-training can simplify architectural designs, potentially reducing the reliance on intricate windowing strategies for optimal performance.

In conclusion, window-based and particularly shifted attention mechanisms, pioneered by the Swin Transformer, were instrumental in making Vision Transformers computationally efficient and scalable for high-resolution images and dense prediction tasks. These innovations transformed ViTs into practical general-purpose backbones. The ongoing research continues to refine these concepts, exploring more efficient pre-training methods for windowed architectures and re-evaluating the precise mechanisms required for cross-window information exchange, balancing architectural complexity with the power of self-supervised learning.
\subsection{Pyramid and Multi-Scale Feature Representation}
\label{sec:4\_2\_pyramid\_\_and\_\_multi-scale\_feature\_representation}

The original Vision Transformer (ViT) architecture, designed primarily for image classification, inherently produces a single-scale feature map. This design choice significantly limits its direct applicability to dense prediction tasks such as object detection and semantic segmentation, which fundamentally necessitate multi-scale feature representations to effectively capture both fine-grained local details and broad global context across an image. Addressing this critical limitation, a major research thrust has focused on developing hierarchical Vision Transformers that generate feature pyramids, mirroring the multi-resolution feature maps traditionally produced by Convolutional Neural Networks (CNNs).

Early efforts to introduce hierarchical structures into ViTs aimed to explicitly construct feature pyramids. The Pyramid Vision Transformer (PVT) \cite{PVT} pioneered this by progressively reducing the spatial resolution of feature maps and expanding channel dimensions. It achieves this through a "spatial reduction attention" mechanism, where the key and value tensors are reshaped to a lower spatial resolution before the attention computation, thereby reducing sequence length and computational cost in deeper layers while creating a feature hierarchy. Similarly, Multiscale Vision Transformers (MViT) \cite{MViT} further explored multi-scale processing by progressively reducing the sequence length of tokens (e.g., through pooling operations) while expanding their channel capacity. This allows MViT to efficiently handle high-resolution inputs and effectively capture multi-scale information, making it suitable for tasks requiring dense predictions. Other works, such as Twins \cite{Twins}, also revisited the design of spatial attention to enhance its efficiency within hierarchical contexts, making Transformers more viable for high-resolution inputs.

A pivotal innovation in this domain was the Swin Transformer \cite{Swin}. While its core mechanism of window-based and shifted attention, which restricts self-attention to local windows and enables cross-window connections, is detailed in Section 4.1, its significance in this context lies in its hierarchical architecture. Swin Transformer constructs a feature pyramid by progressively merging image patches and applying shifted window attention, leading to feature maps at different resolutions. This design not only achieves linear computational complexity with respect to image size but also established Swin as a new standard general-purpose backbone for dense prediction tasks due to its superior performance on benchmarks like COCO for object detection and ADE20K for semantic segmentation \cite{Swin}.

The advancement of hierarchical ViTs also necessitated adaptations in pre-training strategies. Masked Autoencoders (MAE), initially highly successful with plain ViTs, faced challenges when applied to pyramid-based ViTs due to their local window operations and varying patch sizes. To overcome this, Uniform Masking (UM-MAE) \cite{li2022ow4} was proposed, enabling efficient MAE pre-training for hierarchical models like Swin Transformer by introducing uniform sampling and secondary masking. This significantly improved pre-training efficiency and fine-tuning performance, even surpassing ImageNet-22K supervised pre-training with only ImageNet-1K. Building on strong pre-training, Hiera \cite{ryali202339q} argued that many "bells-and-whistles" in complex hierarchical ViTs are unnecessary. They demonstrated that a simplified hierarchical design, when pre-trained with MAE, can achieve superior accuracy and speed, highlighting the power of pre-training in streamlining architecture and achieving robust multi-scale representations.

Interestingly, some research explores alternatives to strictly hierarchical backbones for multi-scale output. ViTDet \cite{li2022raj} demonstrated that competitive object detection performance could be achieved using a \textit{plain}, non-hierarchical ViT backbone. This was accomplished by simply constructing a feature pyramid from its single-scale output (e.g., through simple upsampling and downsampling operations), especially when the plain ViT was pre-trained with MAE. This finding suggests flexibility in how multi-scale features are derived, challenging the strict necessity of an inherently hierarchical backbone. Further exploring efficient designs, Vision-RWKV (VRWKV) \cite{duan2024q7h}, adapted from NLP's RWKV model, offers reduced spatial aggregation complexity, enabling efficient processing of high-resolution images without windowing operations, thus implicitly handling multi-scale information effectively. Similarly, MambaVision \cite{hatamizadeh2024xr6} proposes a hybrid Mamba-Transformer backbone with a hierarchical architecture, demonstrating state-of-the-art performance in classification and dense prediction tasks by integrating Mamba's efficient sequence modeling with self-attention for long-range dependencies. Even simpler approaches like SENet \cite{hao202488z} utilize an asymmetric ViT-based encoder-decoder structure with a Local Information Capture Module (LICM) to enhance pixel-level segmentation, demonstrating how multi-scale features can be implicitly handled through architectural design. Furthermore, ShiftViT \cite{wang2022da0} provocatively suggests that even zero-parameter shift operations can replace attention in a hierarchical ViT, achieving multi-scale representation on par with Swin Transformer, implying that the overall hierarchical structure might be more critical than the specific token mixing mechanism for generating effective multi-scale features.

In conclusion, the development of pyramid and multi-scale feature representation has been crucial for expanding Vision Transformers' applicability beyond image classification to complex dense prediction tasks. This evolution has seen the emergence of dedicated hierarchical architectures like PVT, MViT, and the highly influential Swin Transformer, which efficiently capture multi-scale context and fine-grained details by integrating spatial reduction and window-based attention mechanisms. Recent advancements in pre-training, such as UM-MAE, have further optimized these architectures, leading to simpler yet more powerful designs like Hiera. While hierarchical backbones remain a dominant paradigm, the success of approaches like ViTDet demonstrates that multi-scale features can also be effectively derived through post-processing of plain ViT outputs. The emergence of novel architectures like Vision-RWKV and MambaVision further highlights the ongoing exploration of efficient and versatile solutions for multi-scale vision problems, emphasizing that the goal of robust multi-scale representation can be achieved through diverse architectural and training strategies.
\subsection{General-Purpose Backbones for Dense Prediction}
\label{sec:4\_3\_general-purpose\_backbones\_for\_dense\_prediction}

The initial success of Vision Transformers (ViTs) in image classification, pioneered by \cite{ViT}, quickly highlighted their potential but also exposed fundamental limitations when applied to dense prediction tasks. The original ViT architecture suffered from quadratic computational complexity with respect to image resolution and inherently lacked the hierarchical, multi-scale feature representations crucial for precise localization and structured prediction in tasks like object detection, semantic segmentation, and depth estimation \cite{zhou2021rtn}. This spurred a critical wave of architectural innovations aimed at transforming ViTs from classification-centric models into versatile, general-purpose backbones capable of competing with, and often surpassing, traditional Convolutional Neural Networks (CNNs) across a wide array of dense vision challenges.

Building upon the efficient, localized attention mechanisms and hierarchical designs detailed in Section 4.1 and 4.2, architectures like the \textit{Swin Transformer} \cite{liu2021ljs} were pivotal in establishing ViTs as formidable general-purpose backbones for dense prediction. Swin's progressive merging of image patches and computation of self-attention within local, shifted windows effectively generated a feature pyramid, mirroring the multi-scale representations characteristic of CNNs. This hierarchical structure enabled the capture of both fine-grained local details and broader contextual information, which is indispensable for pixel-level understanding. Consequently, Swin Transformer achieved state-of-the-art performance across dense prediction benchmarks, including object detection on COCO and semantic segmentation on ADE20K, solidifying its role as a new paradigm for vision backbones. The adaptability of Swin's hierarchical design was further exemplified by \cite{li20233lv} with SwinCross, which integrated cross-modal attention for 3D medical image segmentation, showcasing its potential in specialized dense prediction domains requiring robust multi-scale features.

Concurrently, other hierarchical ViTs contributed significantly to this paradigm shift. The \textit{Pyramid Vision Transformer (PVT)} \cite{PVT} demonstrated that convolution-free pyramid structures could directly generate multi-scale features purely through attention mechanisms. Similarly, \textit{Multiscale Vision Transformers (MViT)} \cite{MViT} explored efficient multi-scale processing by progressively pooling tokens to reduce sequence length while increasing channel depth, particularly for video applications. These hierarchical backbones provided the essential feature pyramids that could be seamlessly integrated with lightweight convolutional decoders or specialized heads, a common and highly effective practice in dense prediction frameworks. The synergy lies in leveraging the ViT encoder's strength in global context modeling and robust feature extraction across scales, while employing CNN-based decoders for efficient upsampling, precise spatial localization, and recovery of fine-grained details crucial for pixel-level tasks. For instance, \cite{DPT} demonstrated the effectiveness of a Vision Transformer-based dense prediction Transformer, leveraging these multi-scale features for tasks like depth estimation and semantic segmentation. Furthermore, \cite{panboonyuen20218r7} showcased the superior performance of Swin Transformer backbones when coupled with various decoder designs like U-Net, PSPNet, and FPN for semantic segmentation on remote sensing images, underscoring the plug-and-play nature and complementary strengths of this encoder-decoder paradigm. \cite{chen202174h} utilized ViT-V-Net, a volumetric ViT, for unsupervised medical image registration, underscoring their utility in complex 3D dense prediction.

The efficiency and robustness of these hierarchical ViTs were further bolstered by advancements in pre-training strategies tailored for their unique architectures. While Masked Autoencoders (MAE) \cite{community\_11} proved highly effective for vanilla ViTs, adapting them to pyramid-based ViTs with local operators posed a challenge. \cite{li2022ow4} addressed this with Uniform Masking (UM-MAE), enabling efficient MAE pre-training for models like Swin and PVT. This innovation significantly improved pre-training efficiency and fine-tuning performance on downstream dense prediction tasks, demonstrating that effective self-supervised learning is crucial for unlocking the full potential of these complex backbones. Further reinforcing this, \cite{zhang2022msa} introduced HiViT, a hierarchical ViT designed for efficient Masked Image Modeling (MIM), achieving competitive accuracy with faster pre-training compared to Swin-B, and demonstrating performance gains in detection and segmentation. Interestingly, \cite{ryali202339q} introduced Hiera, a simplified hierarchical ViT that, when combined with strong MAE pre-training, achieved competitive accuracy while being significantly faster. This work critically suggests that a substantial portion of performance gains might stem from robust pre-training rather than solely from intricate architectural "bells-and-whistles," prompting a re-evaluation of design complexity versus pre-training efficacy.

Despite the rapid convergence on hierarchical designs, subsequent work provocatively questioned their absolute indispensability for dense prediction. \cite{li2022raj} explored the use of plain, non-hierarchical ViTs as backbones for object detection. Their work, named ViTDet, demonstrated that with minimal adaptations, such as constructing a simple feature pyramid from a single-scale feature map and incorporating limited cross-window propagation, these seemingly less suitable architectures could achieve competitive results. This finding suggests that the rich, global representations learned by original ViTs, when properly fine-tuned and augmented with basic multi-scale processing, possess significant inherent potential for dense tasks, offering a simpler alternative to complex hierarchical designs. Furthermore, challenging the conventional "pre-train \& fine-tune" paradigm, \cite{hong2022ks6} demonstrated that ViT-based object detectors could be trained effectively from scratch. Their empirical study revealed that specific architectural changes and extended training epochs played critical roles, suggesting that while large-scale pre-training is powerful, it is not always a prerequisite for competitive performance in dense prediction tasks, especially with careful design and training.

The adaptability of ViTs as general-purpose backbones extends beyond 2D images to higher dimensions. \cite{wang2022gq4} demonstrated the remarkable ability to adapt a standard 2D ViT architecture to 3D vision tasks, such as object classification and point cloud segmentation, by simply "inflating" the patch embedding and token sequence. This "Simple3D-Former" highlights the inherent generalizability of Transformer architectures across different data dimensions, further solidifying their role as versatile backbones for complex spatial reasoning. This evolution has firmly established ViTs as competitive, and often superior, alternatives to traditional CNN backbones, demonstrating their profound adaptability and representational power across diverse vision challenges, including complex scene understanding tasks like traversable area detection \cite{urrea20245k4}.

In summary, the journey of Vision Transformers from classification models to general-purpose backbones for dense prediction has been characterized by profound architectural innovations. The introduction of hierarchical structures and efficient attention mechanisms, most notably the Swin Transformer, effectively addressed the initial limitations of quadratic complexity and the lack of multi-scale features. Subsequent research focused on optimizing pre-training for these hierarchical models, exploring the effective integration of ViT encoders with CNN decoders, and even challenging the fundamental necessity of complex hierarchical designs for all dense tasks. These advancements have collectively made ViTs highly effective and adaptable, pushing the boundaries of performance in dense prediction tasks and demonstrating their capacity to learn robust, transferable representations for complex visual understanding.


\label{sec:beyond_pure_attention:_hybrid_designs_and_alternative_token_mixers}

\section{Beyond Pure Attention: Hybrid Designs and Alternative Token Mixers}
\label{sec:beyond\_pure\_attention:\_hybrid\_designs\_\_and\_\_alternative\_token\_mixers}

\subsection{Integrating Convolutional Inductive Biases}
\label{sec:5\_1\_integrating\_convolutional\_inductive\_biases}

The initial success of Vision Transformers (ViTs) in image recognition, despite their impressive global context modeling, quickly exposed a fundamental architectural challenge: their inherent lack of strong inductive biases for local feature processing and translation equivariance \cite{han2020yk0, hassija2025wq3}. Unlike Convolutional Neural Networks (CNNs), which inherently capture local patterns through their kernel operations, pure ViTs treat images as sequences of patches, relying on self-attention to learn spatial relationships from scratch. This often led to significant data hunger, performance degradation on smaller datasets, and increased computational demands for capturing fine-grained local details. Consequently, a pivotal trend in Vision Transformer research has been the strategic integration of convolutional inductive biases directly into ViT architectures, aiming to harness the complementary strengths of both paradigms. This integration seeks to combine the global reasoning capabilities of self-attention with the local feature extraction, parameter efficiency, and translation equivariance of convolutions.

This integration manifests in several distinct architectural strategies. One common approach involves incorporating convolutional layers within the initial patch embedding or tokenization process. For instance, Convolutional Vision Transformers (CvT) \cite{CvT} introduced convolutions into both the token embedding and the projection layers (query, key, and value) within the Transformer blocks. By replacing linear projections with depth-wise separable convolutions, CvT effectively injects local processing into the attention mechanism itself, enhancing local feature extraction and translation equivariance while retaining the global receptive field of self-attention. This direct integration allowed CvT to achieve improved performance and efficiency compared to pure ViTs, particularly on smaller datasets.

A second strategy involves designing hybrid architectures that strategically combine distinct convolutional and attentional layers or blocks. Co-Scale Conv-Attentional Image Transformers (CoaT) \cite{CoaT} exemplifies this by proposing a multi-scale architecture that leverages both convolutional and attentional layers at different stages. CoaT employs a co-scale attention mechanism that allows for interaction between features extracted by convolutional and self-attention branches, enabling efficient capture of multi-scale representations. This parallel processing and fusion of local and global features proved effective for various vision tasks. More recently, Next-ViT \cite{li2022a4u} further refined this hybrid approach by developing a Next Convolution Block (NCB) and a Next Transformer Block (NTB), which are then stacked using a novel Next Hybrid Strategy (NHS). Next-ViT explicitly aims for efficient deployment in industrial scenarios, demonstrating that by carefully designing and integrating deployment-friendly convolutional and transformer components, models can achieve superior latency/accuracy trade-offs compared to both pure CNNs and ViTs. The NCB efficiently captures local information, while the NTB handles global context, and their strategic interleaving ensures high performance across tasks like image classification, object detection, and semantic segmentation.

Another variant of hybrid integration involves using convolutions to enrich feature representations before or during the attention mechanism. The XFormer \cite{zhao2022koc} model, for example, combines efficient mobile CNNs with ViTs through a novel cross feature attention (XFA) mechanism. Here, convolutions provide a strong foundation for local representations, which are then enriched and fused with global attention through the XFA, leading to lightweight yet high-performing models. This approach highlights how convolutions can serve as powerful local feature extractors, whose outputs are then processed and contextualized by Transformer layers.

The benefits of injecting convolutional inductive biases are multifaceted. These hybrid models often exhibit improved data efficiency, requiring less pre-training data to achieve competitive performance, making them more practical for scenarios with limited labeled datasets. They also tend to be more efficient computationally, particularly for high-resolution inputs, as convolutions can process local information more effectively than global self-attention. Furthermore, the inherent translation equivariance of convolutions contributes to better generalization and robustness. This architectural convergence, where the strengths of CNNs and Transformers are combined, has led to models that are not only more efficient and performant but also more robust across a wider array of vision tasks. While pure ViTs, especially large-scale ones like ViTDet \cite{li2022raj} and Hiera \cite{ryali202339q}, can achieve strong results with extensive pre-training and minimal adaptations, the consistent improvements observed with well-designed hybrid architectures underscore the enduring value of explicitly incorporating convolutional inductive biases. This ongoing integration blurs the lines between these once distinct paradigms, pointing towards a future where optimal vision backbones dynamically leverage the best elements from both to achieve even greater efficiency, generalization, and practical utility.
\subsection{Rethinking Token Mixing: Beyond Self-Attention}
\label{sec:5\_2\_rethinking\_token\_mixing:\_beyond\_self-attention}

While Vision Transformers have revolutionized computer vision, the computational cost and complexity of the self-attention mechanism remain a significant bottleneck, particularly for high-resolution images and real-time applications. This has spurred a wave of radical architectural explorations that challenge the necessity of complex self-attention, instead proposing simpler, more computationally efficient token mixing operations while retaining the overall 'MetaFormer' structure (token mixer + MLP). These works provocatively suggest that the core architectural design of Transformers, rather than the intricate self-attention mechanism itself, might be the primary driver of their success, opening avenues for faster and more hardware-friendly vision models.

Among the most direct challenges to self-attention is the \textit{MetaFormer} architecture, exemplified by \cite{PoolFormer}. This work demonstrated that even a simple spatial pooling operation, when integrated into the MetaFormer block (token mixer followed by an MLP), could achieve competitive performance. \cite{PoolFormer} posited that the overall architectural design principles of Transformers, such as residual connections, normalization layers, and the alternation of token mixing and channel-wise MLP layers, are more critical than the specific self-attention mechanism itself. This finding highlighted the potential for significantly simpler and more efficient token mixers without sacrificing performance.

Building on the premise of efficient global interactions, \cite{GFNet} introduced Global Filter Networks (GFNet), which replace self-attention with global filters applied in the frequency domain. By leveraging 2D Fourier transforms, GFNet achieves global receptive fields with a computational complexity that is linear with respect to the number of tokens, offering a highly efficient alternative to the quadratic complexity of standard self-attention. This approach demonstrated that global information exchange, crucial for Transformer performance, could be achieved through non-attention mechanisms, further supporting the MetaFormer hypothesis.

Another notable exploration into alternative token mixing is the Vision Permutator (ViP) \cite{ViP}. While its title includes "Permutable Self-Attention," ViP's core contribution lies in its permutation-based mixing strategy, which significantly departs from traditional self-attention by performing attention along different axes (height, width, and channel dimensions) sequentially. This design aims to capture global information more efficiently by reducing the computational burden associated with full pairwise token interactions, effectively acting as a more efficient, structured alternative to standard self-attention within the MetaFormer paradigm.

Further diversifying the landscape of non-attention token mixers, \cite{FocalNet} proposed Focal Modulation Networks, which replace self-attention with a focal modulation mechanism. This approach captures multi-range dependencies by dynamically modulating features based on their spatial location and context, without relying on explicit attention weights. FocalNet demonstrated that effective long-range interactions and hierarchical feature learning can be achieved through modulation operations, offering another compelling alternative to self-attention that is both efficient and performs competitively.

Collectively, these architectural innovations \cite{PoolFormer, GFNet, ViP, FocalNet} represent a critical re-evaluation of the core components of Vision Transformers. They consistently demonstrate that high performance can be maintained or even improved by replacing complex self-attention with simpler, more computationally efficient token mixing operations. This body of work underscores that the architectural blueprint of Transformers, characterized by its modularity and the separation of spatial mixing and channel-wise processing, is a powerful paradigm. The ongoing challenge lies in further exploring the trade-offs between the theoretical expressiveness of full self-attention and the practical benefits of these simpler, hardware-friendly alternatives, paving the way for a new generation of efficient and scalable vision models.
\subsection{Convergence with Modernized Convolutional Networks}
\label{sec:5\_3\_convergence\_with\_modernized\_convolutional\_networks}

The emergence of Vision Transformers (ViTs) fundamentally reshaped the landscape of computer vision, demonstrating that architectures primarily built on self-attention could achieve state-of-the-art performance across a diverse range of tasks \cite{ViT}. This paradigm shift, however, did not signal the obsolescence of Convolutional Neural Networks (CNNs); instead, it catalyzed a critical re-evaluation and modernization of CNN architectures. This subsection explores a fascinating convergence where CNNs began to systematically adopt design principles initially popularized by ViTs, leading to a blurring of architectural boundaries and the development of highly competitive hybrid models.

Initially, ViTs showcased impressive capabilities, particularly when pre-trained on extensive datasets. Studies on the transferability of visual representations further underscored the advantages of Transformer-based backbones across various downstream tasks, including fine-grained classification and scene recognition \cite{zhou2021rtn}. This demonstrated the powerful representational capacity of ViTs, creating a strong impetus for CNNs to evolve and incorporate lessons learned from their Transformer counterparts. The success of ViTs prompted a deeper inquiry into the specific architectural elements responsible for their performance gains, moving beyond the sole focus on the self-attention mechanism to consider broader design choices.

This intellectual trajectory culminated in seminal works like ConvNeXt \cite{ConvNeXt}, which stands as a prime example of this convergence. The authors of ConvNeXt systematically analyzed the architectural design choices that contributed to ViTs' success, such as the use of larger kernel sizes (e.g., $7 \times 7$), inverted bottleneck structures, Layer Normalization instead of Batch Normalization, and GELU activation functions. By progressively incorporating these Transformer-inspired design principles into a ResNet-like architecture, ConvNeXt demonstrated that modernized CNNs could achieve competitive, and often superior, performance to state-of-the-art ViTs on ImageNet classification and downstream tasks like object detection and semantic segmentation. For instance, the adoption of larger kernel sizes directly aimed to increase the receptive field, mimicking the global information aggregation capability of self-attention, while inverted bottlenecks improved parameter efficiency, echoing ViT's MLP structure. This systematic reverse-engineering of ViT's scaling recipes and design choices effectively blurred the architectural boundaries, proving that the inductive biases of convolutions, when combined with modern scaling and normalization techniques, remain highly potent.

The convergence is not unidirectional; it represents a synergistic exchange of ideas. While ConvNeXt modernized CNNs with ViT principles, other works explored hybrid architectures that explicitly combine elements from both paradigms for improved efficiency and performance. For example, LeViT \cite{LeViT} introduced a "Vision Transformer in ConvNet's Clothing," designing a ViT that prioritizes faster inference by incorporating ConvNet-like efficiency and structures, such as attention bias and smaller attention heads, further illustrating the bidirectional synergistic potential. More explicitly, models like Next-ViT \cite{li2022a4u} and TRT-ViT \cite{xia2022dnj} propose "Next Convolution Blocks" (NCB) and "Next Transformer Blocks" (NTB) or similar hybrid block designs. These architectures strategically stack convolution-based blocks for local feature extraction with Transformer-based blocks for global context modeling, aiming to achieve the efficiency of CNNs with the powerful representational capacity of ViTs. They are often optimized for practical industrial deployment scenarios, demonstrating superior latency/accuracy trade-offs compared to pure CNNs or ViTs on platforms like TensorRT. This development underscores the growing understanding that the optimal vision backbone for real-world applications often benefits from a thoughtful integration of both local and global processing mechanisms.

This hybridization proves particularly effective in specialized domains where both local detail and global context are crucial. For instance, in medical image analysis, modernized CNNs and hybrid designs have shown significant promise. Studies on melanoma diagnosis \cite{aksoy20240c0} highlight ConvNeXt's superior performance, demonstrating its balanced precision and recall metrics in classifying benign and malignant cases. Similarly, for COVID-19 detection from CT images, ensemble frameworks combining Vision Transformers and ConvNeXt (e.g., VitCNX \cite{tian2022qb5}) have achieved state-of-the-art recall, accuracy, and F1-scores, leveraging the strengths of both architectures to capture fine-grained pathological features and broader anatomical context. These applications underscore the practical benefits of integrating the best elements from both worlds, leading to more robust and accurate diagnostic tools.

In conclusion, the "Convergence with Modernized Convolutional Networks" signifies a maturing understanding of deep learning architectures for vision. The success of ConvNeXt and subsequent hybrid models demonstrates that the architectural innovations pioneered by ViTs are not exclusive to attention-based models but can be effectively integrated into CNNs, yielding highly competitive results. This trend indicates that the optimal vision backbone for future tasks may not strictly adhere to either a pure CNN or a pure Transformer design, but rather incorporate the most effective and efficient components from both paradigms. The ongoing challenge lies in identifying the precise balance and interaction of these diverse architectural elements to achieve optimal performance, efficiency, and scalability across a wide range of vision tasks and deployment environments.


\label{sec:vits_in_action:_diverse_applications_and_specialized_adaptations}

\section{ViTs in Action: Diverse Applications and Specialized Adaptations}
\label{sec:vits\_in\_action:\_diverse\_applications\_\_and\_\_specialized\_adaptations}

\subsection{Object Detection and Semantic Segmentation}
\label{sec:6\_1\_object\_detection\_\_and\_\_semantic\_segmentation}

The application of Vision Transformers (ViTs) to dense prediction tasks, such as object detection and semantic segmentation, marks a critical advancement in computer vision, leveraging their inherent capacity for global context modeling and long-range dependency capture. While initial ViT architectures excelled in image classification, their fixed-resolution inputs and quadratic computational complexity presented significant hurdles for tasks demanding fine-grained spatial understanding and multi-scale feature representation. As established in Section 4, the development of hierarchical ViT architectures, notably the Swin Transformer \cite{liu2021ljs} and Pyramid Vision Transformer (PVT) \cite{PVT}, was instrumental in overcoming these limitations. These models generate multi-scale feature maps and achieve linear computational complexity, thereby transforming ViTs into versatile backbones competitive with, and often superior to, traditional Convolutional Neural Networks (CNNs) for dense prediction.

For object detection, the pioneering work of DETR (DEtection TRansformer) \cite{DETR} introduced a paradigm shift by formulating object detection as a direct set prediction problem. This end-to-end approach eliminated the need for hand-designed components like non-maximum suppression (NMS) and anchor boxes, relying instead on the Transformer's global attention to reason about object relationships and the entire image context. However, the original DETR suffered from slow convergence and challenges in detecting small objects due to its global attention mechanism. Subsequent research addressed these limitations; Deformable DETR \cite{DeformableDETR} (not in provided papers, but assumed as keystone) significantly improved efficiency and performance by employing deformable attention, which focuses on a small set of sampling points around a reference, thereby reducing computational cost and enhancing the ability to handle objects at varying scales. This advancement made DETR-based models more practical and competitive. For instance, \texttt{\cite{wang2023bfo}} demonstrates the effectiveness of integrating Deformable DETR with a Swin Transformer backbone and a lightweight Feature Pyramid Network (FPN) for robust classroom behavior detection, showcasing improved accuracy for multi-scale targets. Further refinements, such as ViDT (Vision and Detection Transformers) \cite{song2022y4v}, have integrated Swin Transformer as a reconfigured backbone with efficient transformer decoders to boost detection and instance segmentation performance, highlighting the synergy between hierarchical backbones and refined detection heads.

In semantic segmentation, SETR (SEgmentation TRansformer) \cite{SETR} demonstrated the efficacy of a purely Transformer-based encoder for capturing extensive long-range dependencies crucial for dense pixel-level classification. However, pure Transformer encoders often struggle with precise local localization due to their patch-based processing. To mitigate this, hybrid models emerged, combining the strengths of both architectures. TransUNet \cite{TransUNet} is a prominent example, leveraging a Transformer encoder for global context modeling and a traditional CNN decoder for fine-grained local detail recovery, proving particularly effective in medical image segmentation. This hybrid strategy has been widely adopted; for instance, \texttt{\cite{panboonyuen20218r7}} explored Swin Transformer backbones with various CNN-based decoders (U-Net, PSPNet, FPN) for semantic segmentation on remotely sensed images, achieving state-of-the-art results. Similarly, UNetFormer \cite{hatamizadeh2022y9x} introduced a unified framework with a 3D Swin Transformer encoder and both CNN- and transformer-based decoders, excelling in 3D medical image segmentation by effectively capturing global anatomical context while maintaining local precision. Beyond hybrid approaches, advancements in weakly-supervised semantic segmentation (WSSS) have also leveraged ViTs; WeakTr \cite{zhu2023dpi} explores properties of plain ViTs for WSSS, adaptively fusing self-attention maps to generate high-quality class activation maps (CAMs), demonstrating ViT's potential even with limited supervision.

A significant evolution in dense prediction has been the development of unifying frameworks that tackle multiple tasks simultaneously. MaskFormer \cite{MaskFormer} (not in provided papers, but assumed as keystone) and its successor Mask2Former \cite{Mask2Former} (not in provided papers, but assumed as keystone) represent a paradigm shift by unifying instance, semantic, and panoptic segmentation under a mask classification framework. These models leverage a Transformer decoder with learnable queries to directly predict a set of class-agnostic masks and their corresponding class labels, simplifying the pipeline and achieving state-of-the-art results across all three segmentation tasks. This query-based approach inherently benefits from the Transformer's global reasoning capabilities to resolve ambiguities and capture complex object instances. The trend towards generalist models is further exemplified by GiT (Generalist Vision Transformer) \cite{wang20249qa}, which proposes a single vanilla ViT architecture with a universal language interface capable of handling diverse vision tasks, including detection and segmentation, without task-specific modules, fostering mutual enhancement across tasks.

The versatility of ViTs in dense prediction extends to numerous domain-specific applications. For instance, ST-YOLOA \cite{zhao2023rle} developed a Swin-Transformer-based YOLO model for robust SAR ship detection, integrating the hierarchical ViT's global context modeling within an enhanced YOLOX framework to boost accuracy in challenging backgrounds. In agricultural precision, SwinGD \cite{wang20215ra} applied Swin Transformer and DETR models for grape bunch detection, outperforming traditional CNNs. Furthermore, ViTs are enhancing autonomous perception systems, with bilateral models combining CNNs, ViT, and MLPs for traversable area detection \cite{urrea20245k4}, improving the capture of distant details for real-time semantic segmentation. Even in specialized medical image analysis, explainable ViT models are being developed for white blood cell classification and localization \cite{katar202352u}, demonstrating their ability to provide interpretable global context for diagnostic tasks.

In conclusion, the journey of ViTs in object detection and semantic segmentation has progressed from addressing initial architectural limitations to pioneering end-to-end and hybrid solutions. Hierarchical designs like Swin Transformer have established ViTs as competitive backbones, while frameworks like DETR and its deformable variants have revolutionized object detection. For segmentation, SETR, hybrid models like TransUNet and UNetFormer, and unifying mask-based approaches like MaskFormer have demonstrated the power of Transformers in structured prediction. Despite these advancements, challenges persist, including optimizing computational efficiency for extremely high-resolution inputs, further reducing the data-hungry nature of some Transformer variants, and enhancing interpretability. Future research will likely focus on developing more efficient and adaptable hybrid architectures, exploring novel attention mechanisms or their alternatives, and advancing generalist foundation models that can seamlessly handle a multitude of dense prediction tasks.
\subsection{Medical Image Analysis}
\label{sec:6\_2\_medical\_image\_analysis}

The precise and automated analysis of medical images is paramount for accurate diagnosis, treatment planning, and disease monitoring. This domain presents unique challenges, including the inherent three-dimensional (3D) nature of many imaging modalities (e.g., MRI, CT), the scarcity of expertly labeled datasets, and the critical demand for high precision and interpretability. While Convolutional Neural Networks (CNNs) have long been the backbone of medical image analysis, their limited receptive fields often hinder their ability to capture global anatomical context and long-range dependencies, which are crucial for understanding complex pathological structures. The advent of Vision Transformers (ViTs), initially successful in natural language processing, has offered a powerful alternative by leveraging self-attention mechanisms to model these global relationships, thereby significantly impacting medical image analysis.

Early explorations into Vision Transformers for general computer vision tasks, such as the foundational work by \cite{ViT}, demonstrated their capability to achieve impressive performance by treating images as sequences of patches. However, these initial ViTs often suffered from high computational costs and a lack of inherent inductive biases for local feature extraction, limiting their direct applicability to dense prediction tasks like segmentation. The introduction of hierarchical Vision Transformers, notably the Swin Transformer \cite{Swin}, addressed these limitations by employing shifted window-based self-attention, which efficiently captures both local and global dependencies, making ViTs more suitable for complex vision tasks and paving the way for their adoption in medical imaging.

In the realm of medical image segmentation, particularly for 3D data, hybrid architectures that combine the strengths of CNNs and ViTs have emerged as a dominant paradigm. Initial efforts, such as TransUNet \cite{TransUNet}, integrated a Transformer encoder with a CNN decoder, demonstrating the potential of Transformers to capture global context while relying on CNNs for precise localization. Building on this, SwinBTS \cite{jiang2022zcn} utilized a 3D Swin Transformer as an encoder-decoder backbone, incorporating convolutional operations for upsampling and downsampling to enhance 3D brain tumor segmentation. While effective, these models often integrate CNNs and Transformers sequentially, potentially limiting the simultaneous learning of different feature types.

A significant advancement in this direction is the Swin Unet3D architecture proposed by \cite{cai2023hji}. This model introduces a novel parallel feature extraction sub-module within each stage of a U-Net-like encoder-decoder structure, integrating both 3D Swin Transformer Blocks and 3D Convolutional Blocks. This parallel design allows Swin Unet3D to simultaneously learn both global (long-distance) and local (fine-grained) dependency information throughout the network, effectively addressing the shortcomings of pure CNNs (limited receptive fields) and pure ViTs (poor local detail learning with limited data). The model further employs depth-wise separable convolutions for efficient local feature learning and utilizes feature fusion via multiplication to combine the distinct representations, achieving a superior balance between segmentation accuracy and model parameters for tasks like 3D brain tumor segmentation \cite{cai2023hji}.

Beyond segmentation, ViTs are also being adapted for disease classification in medical imaging. For instance, Swin-GA-RF \cite{alohali2024xwz} presents a novel hybrid approach for cervical cancer classification from Pap smear images. This method leverages a Swin Transformer for robust feature extraction, followed by a Genetic Algorithm (GA) for optimal feature selection and a Random Forest (RF) classifier for enhanced decision-making. This pipeline-based hybridization demonstrates how ViTs can be integrated with other machine learning techniques to address the challenges of complex medical image classification, particularly by optimizing the feature space for improved diagnostic accuracy. Similarly, EFFResNet-ViT \cite{hussain2025qoe} combines EfficientNet-B0 and ResNet-50 CNN backbones with a ViT module, focusing on explainable medical image classification for brain tumors and retinal diseases. This model emphasizes interpretability through Grad-CAM visualization, a critical aspect for clinical adoption, by fusing local CNN features with global ViT dependencies.

The practical deployment of ViTs in medical settings also necessitates optimizing their training and performance. Ko et al. \cite{ko2024eax} systematically investigated the impact of various optimizers on ViT-based models (ViT, FastViT, CrossViT) for lung disease detection from chest X-ray images. Their findings provide crucial empirical guidance for selecting optimal training strategies, especially when dealing with imbalanced medical datasets, highlighting that Adam-based optimizers generally outperform traditional SGD-based methods and that specific optimizer-model combinations can achieve high diagnostic accuracies.

The ongoing research also explores various architectural enhancements to ViTs that are highly relevant to medical imaging. For example, PLG-ViT \cite{ebert202377v} proposes a parallel local and global self-attention mechanism, efficiently fusing short- and long-range spatial interactions without costly shifted windows, which could benefit medical tasks requiring both fine detail and broad context. Similarly, approaches like Slide-Transformer \cite{pan2023hry} and BOAT \cite{yu20220np} focus on efficient local attention and feature-space locality, addressing the high-resolution demands of medical images.

In conclusion, Vision Transformers have profoundly impacted medical image analysis by offering a powerful mechanism to capture global anatomical context and long-range dependencies, complementing the local feature extraction capabilities of CNNs. The intellectual trajectory is increasingly moving towards sophisticated hybrid CNN-Transformer architectures, such as Swin Unet3D \cite{cai2023hji}, which integrate these paradigms in parallel to leverage their respective strengths for improved performance in challenging 3D medical contexts. While significant progress has been made in tasks like 3D segmentation and disease classification, ongoing challenges include further enhancing model interpretability, addressing data scarcity through efficient learning strategies, and developing more robust and generalizable hybrid solutions that can seamlessly adapt to diverse medical imaging modalities and pathologies.
\subsection{Lightweight and Real-time Vision Systems}
\label{sec:6\_3\_lightweight\_\_and\_\_real-time\_vision\_systems}

The increasing computational demands of Vision Transformers (ViTs) present a significant hurdle for their deployment on resource-constrained devices, such as mobile phones, embedded systems, and edge AI platforms, where real-time inference, low latency, and energy efficiency are paramount. This subsection critically examines the multifaceted efforts to develop lightweight and efficient ViT systems, focusing on architectural optimizations, novel attention mechanisms, model compression techniques, and hardware-aware designs that collectively aim to balance accuracy with computational speed for practical deployment.

A primary strategy for achieving efficiency involves the development of \textbf{hybrid architectures} that strategically integrate convolutional inductive biases with Transformer blocks, tailored specifically for mobile and edge environments. \cite{MobileViT} pioneered this direction with MobileViT, a lightweight, general-purpose Vision Transformer that combines the local feature extraction strengths of CNNs with the global reasoning capabilities of Transformers within a mobile-friendly design. This integration allows for a more efficient capture of both local and global dependencies, reducing the computational overhead typically associated with pure ViTs. Following this, \cite{EdgeViT} introduced EdgeViT, an efficient hierarchical ViT specifically engineered for on-device image classification, optimizing its architecture for improved latency and throughput on edge AI platforms. These models exemplify a direct architectural approach to creating ViTs that are inherently efficient for resource-limited settings.

Beyond general hybrid designs, \textbf{hardware-aware architectural optimizations} have emerged as crucial for maximizing real-time performance. These approaches consider the specific characteristics of target deployment platforms. \cite{li2022a4u} proposed Next-ViT, a "next-generation" Vision Transformer designed for efficient deployment in realistic industrial scenarios. Next-ViT achieves CNN-like inference speeds with ViT-level performance by developing deployment-friendly Next Convolution Blocks (NCB) and Next Transformer Blocks (NTB), integrated through a novel Next Hybrid Strategy (NHS). This design demonstrates superior latency/accuracy trade-offs on platforms like TensorRT and CoreML, highlighting the importance of co-designing models with their target hardware. Similarly, \cite{xia2022dnj} introduced TRT-ViT, a TensorRT-oriented Vision Transformer, which directly optimizes for hardware latency by deriving practical design guidelines for deployment-friendly networks, resulting in significantly faster inference speeds across various visual tasks. In a related vein, \cite{nag2023cfn} presented ViTA, a configurable hardware accelerator for ViT inference targeting highly resource-constrained edge computing devices, emphasizing optimizations like head-level pipelining and inter-layer MLP optimizations to avoid repeated off-chip memory accesses and achieve high hardware utilization.

Another significant area of innovation focuses on \textbf{rethinking the self-attention mechanism} itself, which is often the computational bottleneck in ViTs due to its quadratic complexity. Efforts include designing more efficient attention variants or replacing it with simpler token mixers. \cite{song20215tk} proposed UFO-ViT, a Unit Force Operated Vision Transformer, which achieves linear computational complexity by modifying the self-attention mechanism to eliminate non-linearity, leading to high performance with reduced resource demands. Challenging the perceived indispensability of attention, \cite{wang2022da0} introduced ShiftViT, demonstrating that an extremely simple, parameter-free shift operation, which merely exchanges channels between neighboring features, can replace attention layers while maintaining competitive performance. This suggests that the overall architectural design of ViTs might be more critical than the specific attention mechanism. Furthermore, \cite{ren2022ifo} explored Dynamic Window Vision Transformers (DW-ViT), which go beyond fixed single-scale windows by assigning different window sizes to various head groups and dynamically fusing multi-scale information, enhancing modeling potential while maintaining efficiency. More recently, \cite{hatamizadeh2024xr6} proposed MambaVision, a hybrid Mamba-Transformer backbone that redesigns the Mamba formulation to efficiently model visual features, achieving state-of-the-art performance and throughput by equipping the Mamba architecture with self-attention blocks, showcasing the potential of novel sequence modeling techniques for vision. A comprehensive review by \cite{heidari2024d9k} further categorizes and analyzes various redesigned attention mechanisms within ViTs aimed at enhancing efficiency.

Beyond architectural modifications, \textbf{model compression techniques} are indispensable for adapting existing ViTs for lightweight deployment.
\textbf{Quantization} reduces the precision of model weights and activations, significantly decreasing memory footprint and accelerating inference. \cite{li20229zn} proposed Q-ViT, a fully differentiable quantization method for ViTs where both quantization scales and bit-widths are learnable, enabling head-wise bit-width allocation to squeeze model size with minimal performance drop. This work highlighted that Multi-head Self-Attention (MSA) and GELU layers are particularly sensitive to quantization. Further, \cite{li20223n5} and \cite{sun2022nny} explored FPGA-aware automatic acceleration frameworks for ViTs with mixed-scheme quantization, demonstrating significant frame rate improvements on hardware with minor accuracy drops.
\textbf{Pruning} aims to remove redundant parameters or connections from the model. \cite{hou2022ver} introduced a multi-dimensional ViT compression paradigm via dependency-guided Gaussian Process Search, jointly reducing redundancy from attention head, neuron, and sequence dimensions. \cite{yang20210bg} proposed global Vision Transformer pruning with Hessian-aware saliency, leading to new efficient architectures (NViT) with substantial FLOPs and parameter reductions.
\textbf{Knowledge Distillation}, while discussed in Section 3.1 for data-efficient training, also plays a crucial role in model compression for deployment, where a smaller, lightweight ViT student model learns from a larger, more powerful teacher model. \cite{yu2022iy0} presented a unified ViT compression framework that seamlessly assembles pruning, layer skipping, and knowledge distillation, formulating a budget-constrained optimization to jointly learn model weights and compression configurations.

Finally, \textbf{Neural Architecture Search (NAS)} offers an automated approach to discover highly efficient ViT architectures tailored for specific constraints. \cite{chen202199v} proposed searching not only the architecture but also the search space of Vision Transformers, leading to models (S3) that achieve superior performance compared to manually designed efficient ViTs, demonstrating the potential of automated design for lightweight models.

These lightweighting principles are particularly critical for \textbf{application-specific scenarios} where real-time performance on resource-constrained devices is paramount. In automated plant disease classification, \cite{borhani2022w8x} developed custom lightweight CNN and Transformer building blocks and novel hybrid CNN-ViT architectures to achieve real-time classification, explicitly addressing the speed-accuracy trade-off. Similarly, \cite{tabbakh2023ao7} proposed TLMViT, a hybrid model combining transfer learning-based CNNs with a ViT for improved accuracy in plant disease classification, while \cite{wu2021nmg} focused on multi-granularity feature extraction for accurate tomato leaf disease recognition. For remote sensing, a common strategy involves combining CNN backbones with Transformer heads to balance local texture analysis with global scene understanding. \cite{deng2021man} proposed CTNet, a joint CNN-ViT framework, while \cite{song202479c} and \cite{song2025idg} introduced enhanced ViT-based object detectors (QAGA-Net, ODDL-Net) that address sparse data distribution and optimize feature pyramid networks for better efficiency and accuracy. \cite{sha2022ae0} introduced MITformer, a multi-instance ViT for remote sensing scene classification, focusing on highlighting key local features for efficiency. These works demonstrate how tailored hybrid designs and optimized data handling can make ViTs practical for specialized, real-world applications.

Despite significant progress across architectural design, attention mechanisms, and compression techniques, the fundamental trade-off between model complexity, inference latency, energy consumption, and accuracy remains a central challenge. While hybrid architectures, hardware-aware designs, and various compression methods have shown promise, a universally optimal lightweight ViT for all edge scenarios is yet to emerge. Future research will likely continue to explore more sophisticated hybrid designs, novel attention mechanisms with reduced computational overhead, and advanced automated architectural search (NAS) techniques tailored for specific hardware constraints. Furthermore, advancements in efficient self-supervised pre-training will be crucial to reduce data dependency for smaller models, ensuring robust and energy-efficient vision systems that can operate effectively in diverse real-world, resource-constrained environments.


\label{sec:synthesizing_the_landscape:_challenges_and_future_trajectories}

\section{Synthesizing the Landscape: Challenges and Future Trajectories}
\label{sec:synthesizing\_the\_l\_and\_scape:\_challenges\_\_and\_\_future\_trajectories}

\subsection{Summary of Key Developments}
\label{sec:7\_1\_summary\_of\_key\_developments}

The emergence of Visual Transformers (ViTs) fundamentally reshaped the landscape of computer vision, challenging the long-standing dominance of Convolutional Neural Networks (CNNs) by demonstrating the efficacy of pure self-attention for image understanding. This subsection synthesizes the intellectual trajectory of ViT research, from its foundational principles to its rapid evolution through architectural innovations, data-efficient training paradigms, and a fascinating convergence with modern convolutional designs.

The initial Vision Transformer (ViT) \cite{community\_15} marked a significant paradigm shift, processing images as sequences of flattened patches and leveraging global self-attention to capture long-range dependencies. This approach showcased remarkable potential in image classification, even demonstrating superior transferability of learned representations compared to ConvNets in various downstream tasks \cite{zhou2021rtn}. However, the foundational ViT exhibited critical limitations: a substantial reliance on massive pre-training datasets, quadratic computational complexity with respect to image resolution, and an inherent lack of inductive biases for local features, which hindered its performance on dense prediction tasks and smaller datasets.

Addressing these early challenges became a primary research thrust. Efforts to enhance data efficiency and training stability were paramount. Knowledge distillation, where ViTs learned from pre-trained CNN teachers, emerged as a key strategy \cite{community\_16}. Crucially, the advent of self-supervised learning (SSL) paradigms, such as Masked Autoencoders (MAE) \cite{community\_11} and DINO \cite{community\_16}, revolutionized ViT pre-training. These methods enabled ViTs to learn powerful, transferable visual representations from vast amounts of unlabeled data, significantly mitigating the need for extensive human annotation. Further innovations like attention distillation \cite{wang2022pee} provided more effective guidance for self-supervised ViT students, while studies on training ViT-based detectors from scratch highlighted the importance of architectural changes and extended training regimes to reduce reliance on pre-training \cite{hong2022ks6}.

Simultaneously, architectural innovations focused on improving efficiency and enabling hierarchical feature representation, essential for general vision tasks. The Swin Transformer \cite{community\_20} was a pivotal development, introducing a hierarchical architecture with shifted window-based attention. This design achieved linear computational complexity by restricting self-attention to local windows while maintaining cross-window connections, transforming ViTs into versatile backbones for dense prediction tasks like object detection and segmentation \cite{community\_13}. Subsequent works, such as Hiera \cite{ryali202339q}, demonstrated that with strong self-supervised pre-training (e.g., MAE), hierarchical ViTs could be significantly simplified by removing many "bells-and-whistles" without sacrificing accuracy, leading to faster inference and training. Other approaches explored adaptive global-local context nomination to balance efficiency and performance \cite{liu2021yw0}.

A critical re-evaluation of the self-attention mechanism itself and a shift towards hybrid models also characterized this period. Researchers explored integrating convolutional inductive biases to combine the strengths of both paradigms. Hybrid architectures, such as Next-ViT \cite{li2022a4u}, strategically combined convolutional and transformer blocks to achieve superior latency/accuracy trade-offs, particularly for efficient deployment in industrial scenarios. Beyond convolutions, novel token mixers emerged, challenging the necessity of complex self-attention. Models like UFO-ViT \cite{song20215tk} demonstrated that linear self-attention mechanisms, by eliminating non-linearity, could achieve high performance with linear computational complexity, suggesting that the overall 'MetaFormer' structure (token mixer + MLP) might be more critical than the specific attention mechanism. This intellectual trajectory culminated in a fascinating convergence with modern CNNs, where Transformer-inspired design principles (e.g., larger kernel sizes, inverted bottlenecks, layer normalization) were applied to modernize traditional CNNs, leading to models like ConvNeXt \cite{community\_8} that achieved competitive or superior performance to state-of-the-art ViTs. More recent explorations, such as MambaVision \cite{hatamizadeh2024xr6}, further push the boundaries by integrating state space models (Mamba) with Transformers, demonstrating continued innovation in hybrid designs.

These architectural and training advancements enabled ViTs to serve as powerful backbones across diverse applications, from fundamental dense prediction tasks like object detection and semantic segmentation \cite{community\_8} to specialized domains such as medical image analysis \cite{community\_4} and remote sensing \cite{community\_2}. Efforts also focused on developing lightweight and real-time ViT systems for resource-constrained environments \cite{community\_14}.

In conclusion, the journey of Visual Transformer research has been one of rapid innovation and adaptation. It began by demonstrating the power of global context modeling, then systematically addressed initial limitations through data efficiency, hierarchical scaling, and self-supervised learning. The field has since embraced a more nuanced perspective, integrating convolutional inductive biases, exploring alternative token mixing mechanisms, and ultimately fostering a convergence of ideas that blurs the lines between once distinct architectural paradigms. This continuous drive underscores the commitment to developing more powerful, efficient, and versatile visual understanding models that leverage the best elements from various deep learning approaches.
\subsection{Open Challenges and Limitations}
\label{sec:7\_2\_open\_challenges\_\_and\_\_limitations}

Despite the remarkable success of Vision Transformers (ViTs) across numerous computer vision tasks, several open challenges and inherent limitations persist, hindering their widespread and efficient deployment in all scenarios. These issues primarily revolve around their high computational demands, reliance on extensive datasets, questions of interpretability, robustness, and the ongoing debate regarding optimal architectural inductive biases.

A primary limitation stems from the high computational cost associated with the global self-attention mechanism, particularly when processing high-resolution inputs. The quadratic complexity of vanilla Transformer architectures, as introduced by foundational works, becomes prohibitive for large images or video streams. Early efforts to mitigate this, such as the Swin Transformer \cite{Swin} and Pyramid Vision Transformer (PVT) \cite{PVT}, introduced hierarchical structures and localized attention (e.g., shifted windows) to achieve linear complexity with respect to image size, making ViTs more viable for dense prediction tasks. Building on this, works like Next-ViT \cite{li2022a4u} and TRT-ViT \cite{xia2022dnj} further emphasize practical deployment efficiency, optimizing block designs and hybrid strategies to achieve superior latency-accuracy trade-offs on industrial hardware like TensorRT, recognizing that theoretical FLOPs do not always translate to real-world inference speed. More recently, MambaVision \cite{hatamizadeh2024xr6} explores state-space models (Mamba) as an alternative to self-attention, proposing a hybrid Mamba-Transformer backbone that aims to capture long-range dependencies efficiently, potentially offering a new paradigm for linear complexity in visual modeling. For video applications, specific architectural adaptations like TP-ViT \cite{jing2022nkb} and ViT-Shift \cite{zhang2024g0m} introduce multi-pathway designs and temporal shift modules to efficiently model spatio-temporal information, addressing the unique computational challenges of video processing.

Another significant challenge for ViTs is their continued reliance on large datasets for optimal performance, a stark contrast to the data efficiency often exhibited by Convolutional Neural Networks (CNNs due to their strong inductive biases). While self-supervised learning methods like DINO \cite{DINO} and MAE \cite{MAE} have significantly reduced the need for labeled data, enabling ViTs to learn powerful representations from unlabeled images, the challenge of training ViT-based models from scratch without extensive pre-training remains. Hong et al. \cite{hong2022ks6} empirically studied training Transformer-based object detectors from scratch, finding that architectural modifications and longer training schedules are crucial to achieve competitive performance without large-scale pre-training. This issue is particularly pronounced in specialized domains like remote sensing, where labeled data can be scarce. To address this, QAGA-Net \cite{song202479c}, ODDL-Net \cite{song2025idg}, and the quantitative regularization (QR) approach \cite{song2024fx9} propose novel data augmentation and regularization strategies specifically tailored for remote sensing images, demonstrating that ViTs can achieve robust performance even with limited training samples by optimizing data distribution learning. Similarly, P2FEViT \cite{wang202338i} aims to reduce ViT's data dependency by embedding CNN features, facilitating faster convergence with less data.

The debate about the optimal balance between pure attention-based designs and the integration of convolutional priors for various tasks and deployment environments is ongoing, reflecting the inherent trade-offs between architectural complexity, efficiency, and performance. Pure ViTs, lacking the strong inductive biases of CNNs (e.g., locality, translation equivariance), often require more data and computation. Consequently, many works explore hybrid architectures that combine the strengths of both. For instance, CTNet \cite{deng2021man} and LDBST \cite{zheng202325h} propose dual-branch frameworks for remote sensing scene classification, leveraging CNNs for local structural features and ViTs for global semantic context. P2FEViT \cite{wang202338i} further exemplifies this by embedding plug-and-play CNN features into ViT architectures to enhance local feature representation and accelerate convergence. This convergence of ideas is also evident in works like ConvNeXt \cite{ConvNeXt}, which demonstrates that modern CNNs, by adopting ViT design principles, can achieve competitive performance, blurring the lines between the two paradigms. Conversely, PoolFormer \cite{PoolFormer} provocatively suggests that the meta-architecture of Transformers, rather than complex self-attention, might be the key, achieving strong results with simple pooling operations, challenging the necessity of attention itself for certain tasks.

Beyond these architectural and data-related challenges, issues of interpretability and robustness to adversarial attacks remain significant open problems for ViTs. The complex, global interactions modeled by self-attention layers make it difficult to ascertain \textit{why} a ViT makes a particular decision, hindering trust and deployment in critical applications. While some methods attempt to visualize attention maps, these often provide only a partial view of the model's reasoning. Similarly, ViTs have shown vulnerability to adversarial attacks, a concern that needs more dedicated research to ensure their reliability in real-world scenarios. The theoretical understanding of their inductive biases, or the lack thereof, also remains less mature compared to CNNs, complicating systematic improvements and robust design principles.

In conclusion, while Vision Transformers have revolutionized computer vision, their journey is far from complete. The field continues to grapple with fundamental trade-offs between computational cost and performance, the persistent need for large-scale data, and the optimal integration of local and global feature learning mechanisms. Future research will likely focus on developing more efficient, data-agnostic, inherently interpretable, and robust ViT architectures, potentially through novel attention mechanisms, alternative long-range dependency modeling (e.g., state-space models), or more sophisticated hybrid designs that judiciously combine the strengths of various neural network components.
\subsection{Emerging Trends and Ethical Considerations}
\label{sec:7\_3\_emerging\_trends\_\_and\_\_ethical\_considerations}

The remarkable trajectory of Visual Transformers (ViTs) necessitates a forward-looking analysis, exploring the most promising emerging trends and the critical ethical implications that accompany their increasing power and pervasive deployment. Future research is converging on more versatile, efficient, and inherently interpretable models, while simultaneously grappling with the profound societal responsibilities inherent in advanced AI.

A pivotal trend is the development of \textbf{multimodal learning} and \textbf{large-scale vision foundation models} capable of zero-shot generalization. Moving beyond purely visual tasks, ViTs are increasingly designed to process and integrate information from multiple modalities, such as visual and textual data. This integration is crucial for building more human-like AI systems that can understand context from diverse data sources. For instance, the EVA model demonstrates the potential of a unified text-and-image encoder, pushing the limits of transfer learning across modalities \cite{eva}. The vision for generalist AI is further driving the development of unified vision-language interfaces. GiT, for example, proposes a vanilla ViT framework that, through a universal language interface, can simultaneously handle diverse visual tasks from image captioning to dense segmentation, fostering mutual enhancement across tasks and narrowing the architectural gap between vision and language models \cite{wang20249qa}. Concurrently, the field is witnessing the rise of massive vision foundation models, exemplified by efforts to scale ViTs to unprecedented parameter counts, such as the 22 billion parameter ViT-H \cite{vit-h}. Models like InternImage further explore large-scale vision foundation models by integrating deformable convolutions to enhance their capabilities \cite{internimage}. These models, often pre-trained on vast datasets, aim for robust zero-shot generalization, allowing them to perform novel tasks without explicit fine-tuning, as showcased by models like Segment Anything, which provides promptable segmentation capabilities. Furthermore, query-aware vision Transformers, such as QA-ViT, embed user questions directly into the vision encoder, enabling dynamic focusing of visual features on relevant image aspects for improved multimodal reasoning \cite{ganz20249zr}.

Another crucial direction focuses on \textbf{efficient hardware-aware designs} to enable the deployment of powerful ViTs in resource-constrained environments. While early ViTs suffered from high computational costs, future work on efficiency extends beyond hybrid designs to more fundamental architectural shifts and novel attention mechanisms. This includes the development of linear attention mechanisms, such as UFO-ViT, which eliminate the quadratic complexity of traditional self-attention by removing non-linearity and factorizing matrix multiplications, demonstrating competitive performance with significantly reduced computational resources \cite{song20215tk}. Another critical area is model quantization, with methods like Q-ViT exploring fully differentiable quantization for ViTs, allowing learnable scales and bit-widths to significantly reduce model size and computational footprint without substantial performance drops \cite{li20229zn}. The broader trend, as highlighted by recent reviews, is to systematically redesign attention mechanisms for enhanced performance and efficiency \cite{heidari2024d9k}. Further advancements include SPT-Swin, which employs shifted patch tokenization to enhance data efficiency and achieve linear computational complexity \cite{ferdous2024f89}, and EfficientViT, which introduces multi-scale linear attention for high-resolution dense prediction, balancing performance with efficiency \cite{efficientvit}. Such efficient designs are particularly critical for autonomous systems, where models like bilateral ViT and MLP combinations are developed for traversable area detection, balancing precision and computational efficiency for real-time operation in challenging environments \cite{urrea20245k4}.

Beyond identifying statistical associations, a nascent but critical direction for future ViT research is the integration of \textbf{causal inference}. Current deep learning models, including ViTs, largely operate on correlations, which can lead to brittleness when faced with out-of-distribution data, spurious associations, or subtle biases. For ViTs, this means their powerful global attention might inadvertently focus on confounding factors rather than true causal features. Causal inference aims to move beyond 'what' the model sees to 'why' it makes a decision, understanding underlying causal relationships between visual elements and outcomes. This is paramount for enhancing model robustness, improving fairness by disentangling causal factors from confounding biases, and enabling better generalization to novel environments. While still in early stages for vision Transformers, research is exploring how to inject causal priors into attention mechanisms or leverage counterfactual reasoning to make ViTs more reliable and trustworthy, particularly in high-stakes applications like medical diagnosis or autonomous driving where spurious correlations can lead to catastrophic failures. This shift represents a fundamental step towards more intelligent and responsible vision AI systems.

Crucially, as ViTs become more powerful, pervasive, and integrated into sensitive applications, \textbf{ethical considerations} must move from an afterthought to a central design principle. The deployment of powerful vision AI raises significant concerns, particularly regarding \textbf{bias in training data}. The sheer scale of data required for pre-training large ViT foundation models, often scraped from the internet, means they inevitably inherit and can amplify societal biases present in that data. Unlike CNNs with their local receptive fields, ViTs' global self-attention mechanism can learn long-range spurious correlations across image patches, potentially reinforcing biases related to demographics, race, or gender, leading to discriminatory outcomes in areas like facial recognition, medical diagnosis, or autonomous decision-making. Research is actively exploring methods to audit and mitigate such biases in large vision models. For instance, developing interpretable ViTs that highlight their decision-making process, such as using Score-CAM \cite{katar202352u} or combined Grad-CAM and Attention Rollout \cite{chen2022vac} to visualize model focus, is a vital step towards identifying and addressing spurious correlations or discriminatory patterns, particularly in clinical settings where trust is paramount \cite{ma2024uan}.

\textbf{Privacy concerns} are equally paramount, especially with models processing sensitive visual information. The massive data requirements for large foundation models exacerbate these risks, as personal or identifiable information could be inadvertently captured and memorized. Emerging privacy-preserving techniques for ViTs include secure multi-party computation (MPC) to enable inference on encrypted data, which significantly reduces latency overhead compared to traditional methods \cite{zeng2022ce2}. Furthermore, methods for training ViTs on visually obfuscated images while maintaining high classification accuracy demonstrate robust privacy preservation against various attacks \cite{qi2022yq9}. The \textbf{responsible development} of these transformative technologies for societal benefit demands a multi-faceted approach, including rigorous auditing for bias, implementing differential privacy techniques, establishing clear ethical guidelines for deployment, and fostering transparency through enhanced interpretability. The integration of causal inference, as discussed previously, also plays a critical role here by moving models beyond mere correlation, offering a path towards more robust and fair decision-making.

In conclusion, the future of Visual Transformers is characterized by an exciting, yet challenging, push along two primary axes: the pursuit of \textbf{generalist intelligence} through multimodal learning and massive foundation models, and the simultaneous drive for \textbf{ubiquitous deployment} via efficient, hardware-aware designs. However, this technological advancement cannot proceed in isolation. It must be meticulously balanced with a deep, proactive commitment to addressing pressing ethical challenges. Ensuring fairness, protecting privacy, fostering interpretability, and fundamentally integrating causal reasoning are not merely technical hurdles to overcome, but rather indispensable requirements for the responsible and beneficial integration of these increasingly powerful vision AI systems into society. Without a concerted effort on these ethical fronts, the risk remains that highly capable ViTs could inadvertently perpetuate harm, undermining their transformative potential.


\newpage
\section*{References}
\addcontentsline{toc}{section}{References}

\begin{thebibliography}{367}

\bibitem{liu2021ljs}
Ze Liu, Yutong Lin, Yue Cao, et al. (2021). \textit{Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}. IEEE International Conference on Computer Vision.

\bibitem{liang2021v6x}
Jingyun Liang, Jie Cao, Guolei Sun, et al. (2021). \textit{SwinIR: Image Restoration Using Swin Transformer}. 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW).

\bibitem{han2020yk0}
Kai Han, Yunhe Wang, Hanting Chen, et al. (2020). \textit{A Survey on Vision Transformer}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{chen2021r2y}
Chun-Fu Chen, Quanfu Fan, and Rameswar Panda (2021). \textit{CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification}. IEEE International Conference on Computer Vision.

\bibitem{mehta20216ad}
Sachin Mehta, and Mohammad Rastegari (2021). \textit{MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer}. International Conference on Learning Representations.

\bibitem{li2022raj}
Yanghao Li, Hanzi Mao, Ross B. Girshick, et al. (2022). \textit{Exploring Plain Vision Transformer Backbones for Object Detection}. European Conference on Computer Vision.

\bibitem{chen2022woa}
Shoufa Chen, Chongjian Ge, Zhan Tong, et al. (2022). \textit{AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition}. Neural Information Processing Systems.

\bibitem{xia2022qga}
Zhuofan Xia, Xuran Pan, S. Song, et al. (2022). \textit{Vision Transformer with Deformable Attention}. Computer Vision and Pattern Recognition.

\bibitem{zhou202105h}
Daquan Zhou, Bingyi Kang, Xiaojie Jin, et al. (2021). \textit{DeepViT: Towards Deeper Vision Transformer}. arXiv.org.

\bibitem{liu2021jpu}
Nian Liu, Ni Zhang, Kaiyuan Wan, et al. (2021). \textit{Visual Saliency Transformer}. IEEE International Conference on Computer Vision.

\bibitem{lee2021us0}
Seung Hoon Lee, Seunghyun Lee, and B. Song (2021). \textit{Vision Transformer for Small-Size Datasets}. arXiv.org.

\bibitem{zhang2021fje}
Bo Zhang, Shuyang Gu, Bo Zhang, et al. (2021). \textit{StyleSwin: Transformer-based GAN for High-resolution Image Generation}. Computer Vision and Pattern Recognition.

\bibitem{jiang2022zcn}
Yun Jiang, Yuan Zhang, Xinyi Lin, et al. (2022). \textit{SwinBTS: A Method for 3D Multimodal Brain Tumor Segmentation Using Swin Transformer}. Brain Science.

\bibitem{islam2022iss}
Md. Nazmul Islam, Madina Hasan, Md. Kabir Hossain, et al. (2022). \textit{Vision transformer and explainable transfer learning models for auto detection of kidney cyst, stone and tumor from CT-radiography}. Scientific Reports.

\bibitem{li2022a4u}
Jiashi Li, Xin Xia, W. Li, et al. (2022). \textit{Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios}. arXiv.org.

\bibitem{yao202245i}
Ting Yao, Yingwei Pan, Yehao Li, et al. (2022). \textit{Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning}. European Conference on Computer Vision.

\bibitem{borhani2022w8x}
Y. Borhani, Javad Khoramdel, and E. Najafi (2022). \textit{A deep learning based approach for automated plant disease classification using vision transformer}. Scientific Reports.

\bibitem{mao2021zr1}
Xiaofeng Mao, Gege Qi, Yuefeng Chen, et al. (2021). \textit{Towards Robust Vision Transformer}. Computer Vision and Pattern Recognition.

\bibitem{chen202174h}
Junyu Chen, Yufan He, E. Frey, et al. (2021). \textit{ViT-V-Net: Vision Transformer for Unsupervised Volumetric Medical Image Registration}. arXiv.org.

\bibitem{jie20220pc}
Shibo Jie, and Zhi-Hong Deng (2022). \textit{Convolutional Bypasses Are Better Vision Transformer Adapters}. European Conference on Artificial Intelligence.

\bibitem{fan2022m88}
Chi-Mao Fan, Tsung-Jung Liu, and Kuan-Hsien Liu (2022). \textit{SUNet: Swin Transformer UNet for Image Denoising}. International Symposium on Circuits and Systems.

\bibitem{lin20216a3}
Hezheng Lin, Xingyi Cheng, Xiangyu Wu, et al. (2021). \textit{CAT: Cross Attention in Vision Transformer}. IEEE International Conference on Multimedia and Expo.

\bibitem{lin2021utw}
Yang Lin, Tianyu Zhang, Peiqin Sun, et al. (2021). \textit{FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer}. International Joint Conference on Artificial Intelligence.

\bibitem{li2022mco}
Zhikai Li, and Qingyi Gu (2022). \textit{I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference}. IEEE International Conference on Computer Vision.

\bibitem{li2022tl7}
Yanjing Li, Sheng Xu, Baochang Zhang, et al. (2022). \textit{Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer}. Neural Information Processing Systems.

\bibitem{yang2021myb}
Jinyu Yang, Jingjing Liu, N. Xu, et al. (2021). \textit{TVT: Transferable Vision Transformer for Unsupervised Domain Adaptation}. IEEE Workshop/Winter Conference on Applications of Computer Vision.

\bibitem{yu2022iy0}
Shixing Yu, Tianlong Chen, Jiayi Shen, et al. (2022). \textit{Unified Visual Transformer Compression}. International Conference on Learning Representations.

\bibitem{zhuang2022qn7}
Wanyi Zhuang, Qi Chu, Zhentao Tan, et al. (2022). \textit{UIA-ViT: Unsupervised Inconsistency-Aware Method based on Vision Transformer for Face Forgery Detection}. European Conference on Computer Vision.

\bibitem{deng2021man}
Peifang Deng, Kejie Xu, and Hong Huang (2021). \textit{When CNNs Meet Vision Transformer: A Joint Framework for Remote Sensing Scene Classification}. IEEE Geoscience and Remote Sensing Letters.

\bibitem{wang2021oct}
Jun Wang, Xiaohan Yu, and Yongsheng Gao (2021). \textit{Feature Fusion Vision Transformer for Fine-Grained Visual Categorization}. British Machine Vision Conference.

\bibitem{wang2022ti0}
Teng Wang, Lei Gong, Chao Wang, et al. (2022). \textit{ViA: A Novel Vision-Transformer Accelerator Based on FPGA}. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems.

\bibitem{li2022ow4}
Xiang Li, Wenhai Wang, Lingfeng Yang, et al. (2022). \textit{Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality}. arXiv.org.

\bibitem{wang2022da0}
Guangting Wang, Yucheng Zhao, Chuanxin Tang, et al. (2022). \textit{When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism}. AAAI Conference on Artificial Intelligence.

\bibitem{deng2022bil}
Jiajun Deng, Zhengyuan Yang, Daqing Liu, et al. (2022). \textit{TransVG++: End-to-End Visual Grounding With Language Conditioned Vision Transformer}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{yang20228mm}
Shusheng Yang, Xinggang Wang, Yu Li, et al. (2022). \textit{Temporally Efficient Vision Transformer for Video Instance Segmentation}. Computer Vision and Pattern Recognition.

\bibitem{gheflati202131i}
Behnaz Gheflati, and H. Rivaz (2021). \textit{Vision Transformer for Classification of Breast Ultrasound Images}. arXiv.org.

\bibitem{tang2022e2i}
Xinyu Tang, Zengbing Xu, and Zhigang Wang (2022). \textit{A Novel Fault Diagnosis Method of Rolling Bearing Based on Integrated Vision Transformer Model}. Italian National Conference on Sensors.

\bibitem{yu202236t}
Xiaohan Yu, Jun Wang, Yang Zhao, et al. (2022). \textit{Mix-ViT: Mixing attentive vision transformer for ultra-fine-grained visual categorization}. Pattern Recognition.

\bibitem{li2021ra5}
Hanting Li, Ming-Fa Sui, Feng Zhao, et al. (2021). \textit{MViT: Mask Vision Transformer for Facial Expression Recognition in the wild}. arXiv.org.

\bibitem{meng2022t3x}
Xiaoliang Meng, Yuechi Yang, Libo Wang, et al. (2022). \textit{Class-Guided Swin Transformer for Semantic Segmentation of Remote Sensing Imagery}. IEEE Geoscience and Remote Sensing Letters.

\bibitem{li20223n5}
Z. Li, Mengshu Sun, Alec Lu, et al. (2022). \textit{Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization}. International Conference on Field-Programmable Logic and Applications.

\bibitem{bazi2022tlu}
Y. Bazi, Mohamad Mahmoud Al Rahhal, M. L. Mekhalfi, et al. (2022). \textit{Bi-Modal Transformer-Based Approach for Visual Question Answering in Remote Sensing Imagery}. IEEE Transactions on Geoscience and Remote Sensing.

\bibitem{zheng2022gg5}
Hao Zheng, Guohui Wang, and Xuchen Li (2022). \textit{Swin-MLP: a strawberry appearance quality identification method by Swin Transformer and multi-layer perceptron}. Journal of Food Measurement & Characterization.

\bibitem{gao2021uzl}
Xiaohong W. Gao, Y. Qian, and Alice Gao (2021). \textit{COVID-VIT: Classification of COVID-19 from CT chest images based on vision transformer models}. arXiv.org.

\bibitem{zheng202218g}
Zangwei Zheng, Xiangyu Yue, Kai Wang, et al. (2022). \textit{Prompt Vision Transformer for Domain Generalization}. arXiv.org.

\bibitem{bi20225lu}
Chunguang Bi, Nan Hu, Yiqiang Zou, et al. (2022). \textit{Development of Deep Learning Methodology for Maize Seed Variety Recognition Based on Improved Swin Transformer}. Agronomy.

\bibitem{chen2022vac}
Yihan Chen, Xingyu Gu, Zhen Liu, et al. (2022). \textit{A Fast Inference Vision Transformer for Automatic Pavement Image Classification and Its Visual Interpretation Method}. Remote Sensing.

\bibitem{song2022603}
Zhuoran Song, Yihong Xu, Zhezhi He, et al. (2022). \textit{CP-ViT: Cascade Vision Transformer Pruning via Progressive Sparsity Prediction}. arXiv.org.

\bibitem{li2022rl9}
Tao Li, Zheng Zhang, Lishen Pei, et al. (2022). \textit{HashFormer: Vision Transformer Based Deep Hashing for Image Retrieval}. IEEE Signal Processing Letters.

\bibitem{wensel2022lva}
James Wensel, Hayat Ullah, and Arslan Munir (2022). \textit{ViT-ReT: Vision and Recurrent Transformer Neural Networks for Human Activity Recognition in Videos}. IEEE Access.

\bibitem{wang2021sav}
Wenxiao Wang, Lu-yuan Yao, Long Chen, et al. (2021). \textit{CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention}. arXiv.org.

\bibitem{naseem2022c95}
Usman Naseem, Matloob Khushi, and Jinman Kim (2022). \textit{Vision-Language Transformer for Interpretable Pathology Visual Question Answering}. IEEE journal of biomedical and health informatics.

\bibitem{wu20210gs}
Yanan Wu, Shouliang Qi, Yu Sun, et al. (2021). \textit{A vision transformer for emphysema classification using CT images}. Physics in Medicine and Biology.

\bibitem{lyu2022vd9}
Yanjun Lyu, Xiao-Wen Yu, Dajiang Zhu, et al. (2022). \textit{Classification of Alzheimer's Disease via Vision Transformer: Classification of Alzheimer's Disease via Vision Transformer}. Petra.

\bibitem{krishnan2021086}
Koushik Sivarama Krishnan, and Karthik Sivarama Krishnan (2021). \textit{Vision Transformer based COVID-19 Detection using Chest X-rays}. 2021 6th International Conference on Signal Processing, Computing and Control (ISPCC).

\bibitem{yang20210bg}
Huanrui Yang, Hongxu Yin, Maying Shen, et al. (2021). \textit{Global Vision Transformer Pruning with Hessian-Aware Saliency}. Computer Vision and Pattern Recognition.

\bibitem{li20229zn}
Zhexin Li, Tong Yang, Peisong Wang, et al. (2022). \textit{Q-ViT: Fully Differentiable Quantization for Vision Transformer}. arXiv.org.

\bibitem{wang2022n7h}
Hongmiao Wang, Cheng Xing, Junjun Yin, et al. (2022). \textit{Land Cover Classification for Polarimetric SAR Images Based on Vision Transformer}. Remote Sensing.

\bibitem{chen202199v}
Minghao Chen, Kan Wu, Bolin Ni, et al. (2021). \textit{Searching the Search Space of Vision Transformer}. Neural Information Processing Systems.

\bibitem{panboonyuen20218r7}
Teerapong Panboonyuen, Kulsawasd Jitkajornwanich, S. Lawawirojwong, et al. (2021). \textit{Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images}. Remote Sensing.

\bibitem{liang2022xlx}
Junjie Liang, Cihui Yang, Jingting Zhong, et al. (2022). \textit{BTSwin-Unet: 3D U-shaped Symmetrical Swin Transformer-based Network for Brain Tumor Segmentation with Self-supervised Pre-training}. Neural Processing Letters.

\bibitem{zhou2021rtn}
Hong-Yu Zhou, Chi-Ken Lu, Sibei Yang, et al. (2021). \textit{ConvNets vs. Transformers: Whose Visual Representations are More Transferable?}. 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW).

\bibitem{dubey2021ra5}
S. Dubey, S. Singh, and Wei Chu (2021). \textit{Vision Transformer Hashing for Image Retrieval}. IEEE International Conference on Multimedia and Expo.

\bibitem{ayas2022md0}
Selen Ayas, and Esra Tunc-Gormus (2022). \textit{SpectralSWIN: a spectral-swin transformer network for hyperspectral image classification}. International Journal of Remote Sensing.

\bibitem{tian2022shu}
Jialin Tian, Xing Xu, Fumin Shen, et al. (2022). \textit{TVT: Three-Way Vision Transformer through Multi-Modal Hypersphere Learning for Zero-Shot Sketch-Based Image Retrieval}. AAAI Conference on Artificial Intelligence.

\bibitem{liu2022249}
Xingyu Liu, Yuehua Wu, Wenkai Liang, et al. (2022). \textit{High Resolution SAR Image Classification Using Global-Local Network Structure Based on Vision Transformer and CNN}. IEEE Geoscience and Remote Sensing Letters.

\bibitem{zhang2021mcp}
Yuan Zhang, Jian Cao, Ling Zhang, et al. (2021). \textit{A free lunch from ViT: adaptive attention multi-scale fusion Transformer for fine-grained visual recognition}. IEEE International Conference on Acoustics, Speech, and Signal Processing.

\bibitem{han2021vis}
Qi Han, Zejia Fan, Qi Dai, et al. (2021). \textit{Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight}. arXiv.org.

\bibitem{kim2022m6u}
Sangwon Kim, J. Nam, and ByoungChul Ko (2022). \textit{Facial Expression Recognition Based on Squeeze Vision Transformer}. Italian National Conference on Sensors.

\bibitem{zhou2022nln}
Xiaoli Zhou, Chaowei Tang, Pan Huang, et al. (2022). \textit{ASI-DBNet: An Adaptive Sparse Interactive ResNet-Vision Transformer Dual-Branch Network for the Grading of Brain Cancer Histopathological Images}. Interdisciplinary Sciences Computational Life Sciences.

\bibitem{hu202242d}
Zhongxu Hu, Yiran Zhang, Yang Xing, et al. (2022). \textit{Toward Human-Centered Automated Driving: A Novel Spatiotemporal Vision Transformer-Enabled Head Tracker}. IEEE Vehicular Technology Magazine.

\bibitem{you2022bor}
Haoran You, Yunyang Xiong, Xiaoliang Dai, et al. (2022). \textit{Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference}. Computer Vision and Pattern Recognition.

\bibitem{ren2022ifo}
Pengzhen Ren, Changlin Li, Guangrun Wang, et al. (2022). \textit{Beyond Fixation: Dynamic Window Visual Transformer}. Computer Vision and Pattern Recognition.

\bibitem{wang20215ra}
Jinhai Wang, Zongyin Zhang, Lufeng Luo, et al. (2021). \textit{SwinGD: A Robust Grape Bunch Detection Model Based on Swin Transformer in Complex Vineyard Environment}. Horticulturae.

\bibitem{xiao202229y}
Xiao Xiao, Wenliang Guo, Rui Chen, et al. (2022). \textit{A Swin Transformer-Based Encoding Booster Integrated in U-Shaped Network for Building Extraction}. Remote Sensing.

\bibitem{jamil20223a4}
Sonain Jamil, Muhammad Sohail Abbas, and Anisha Roy (2022). \textit{Distinguishing Malicious Drones Using Vision Transformer}. Applied Informatics.

\bibitem{bai2022f1v}
Long Bai, Liangyu Wang, Tong Chen, et al. (2022). \textit{Transformer-Based Disease Identification for Small-Scale Imbalanced Capsule Endoscopy Dataset}. Electronics.

\bibitem{li2022th8}
Kuoyang Li, Min Zhang, Maiping Xu, et al. (2022). \textit{Ship Detection in SAR Images Based on Feature Enhancement Swin Transformer and Adjacent Feature Fusion}. Remote Sensing.

\bibitem{almalik20223wr}
Faris Almalik, Mohammad Yaqub, and Karthik Nandakumar (2022). \textit{Self-Ensembling Vision Transformer (SEViT) for Robust Medical Image Classification}. International Conference on Medical Image Computing and Computer-Assisted Intervention.

\bibitem{sha2022ae0}
Z. Sha, and Jianfeng Li (2022). \textit{MITformer: A Multiinstance Vision Transformer for Remote Sensing Scene Classification}. IEEE Geoscience and Remote Sensing Letters.

\bibitem{zhang2022msa}
Xiaosong Zhang, Yunjie Tian, Wei Huang, et al. (2022). \textit{HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling}. arXiv.org.

\bibitem{htten2022lui}
Nils Hütten, R. Meyes, and Tobias Meisen (2022). \textit{Vision Transformer in Industrial Visual Inspection}. Applied Sciences.

\bibitem{hatamizadeh2022y9x}
Ali Hatamizadeh, Ziyue Xu, Dong Yang, et al. (2022). \textit{UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation}. arXiv.org.

\bibitem{montazerin2022dgi}
Mansooreh Montazerin, Soheil Zabihi, E. Rahimian, et al. (2022). \textit{ViT-HGR: Vision Transformer-based Hand Gesture Recognition from High Density Surface EMG Signals}. Annual International Conference of the IEEE Engineering in Medicine and Biology Society.

\bibitem{kojima2022k5c}
Takeshi Kojima, Yutaka Matsuo, and Yusuke Iwasawa (2022). \textit{Robustifying Vision Transformer without Retraining from Scratch by Test-Time Class-Conditional Feature Alignment}. International Joint Conference on Artificial Intelligence.

\bibitem{kang2022pv3}
Minhee Kang, Wooseop Lee, Keeyeon Hwang, et al. (2022). \textit{Vision Transformer for Detecting Critical Situations And Extracting Functional Scenario for Automated Vehicle Safety Assessment}. Social Science Research Network.

\bibitem{tian2022qb5}
Geng Tian, Ziwei Wang, Chang Wang, et al. (2022). \textit{A deep ensemble learning-based automated detection of COVID-19 using lung CT images and Vision Transformer and ConvNeXt}. Frontiers in Microbiology.

\bibitem{peng2022snr}
Lihong Peng, Chang Wang, Geng Tian, et al. (2022). \textit{Analysis of CT scan images for COVID-19 pneumonia based on a deep ensemble framework with DenseNet, Swin transformer, and RegNet}. Frontiers in Microbiology.

\bibitem{ho20228q6}
Chi M. K. Ho, K. Yow, Zhongwen Zhu, et al. (2022). \textit{Network Intrusion Detection via Flow-to-Image Conversion and Vision Transformer Classification}. IEEE Access.

\bibitem{xia2022dnj}
Xin Xia, Jiashi Li, Jie Wu, et al. (2022). \textit{TRT-ViT: TensorRT-oriented Vision Transformer}. arXiv.org.

\bibitem{wang202232c}
Zhenmin Wang, Haoyu Chen, Q. Zhong, et al. (2022). \textit{Recognition of penetration state in GTAW based on vision transformer using weld pool image}. The International Journal of Advanced Manufacturing Technology.

\bibitem{mogan202229d}
Jashila Nair Mogan, C. Lee, K. Lim, et al. (2022). \textit{Gait-ViT: Gait Recognition with Vision Transformer}. Italian National Conference on Sensors.

\bibitem{yang20221ce}
Yuguang Yang, Hong‐Mei Fu, Shang Gao, et al. (2022). \textit{Intrusion detection: A model based on the improved vision transformer}. Transactions on Emerging Telecommunications Technologies.

\bibitem{li2022ip7}
Nannan Li, Yaran Chen, Weifan Li, et al. (2022). \textit{BViT: Broad Attention-Based Vision Transformer}. IEEE Transactions on Neural Networks and Learning Systems.

\bibitem{yu20220np}
Tan Yu, Gangming Zhao, Ping Li, et al. (2022). \textit{BOAT: Bilateral Local Attention Vision Transformer}. British Machine Vision Conference.

\bibitem{li20229fn}
Jiacheng Li, Menglin Wang, and Xiaojin Gong (2022). \textit{Transformer Based Multi-Grained Features for Unsupervised Person Re-Identification}. 2023 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW).

\bibitem{huang2022iwe}
Jiahao Huang, Xiaodan Xing, Zhifan Gao, et al. (2022). \textit{Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI}. International Conference on Medical Image Computing and Computer-Assisted Intervention.

\bibitem{qu2022be0}
Mengxue Qu, Yu Wu, Wu Liu, et al. (2022). \textit{SiRi: A Simple Selective Retraining Mechanism for Transformer-based Visual Grounding}. European Conference on Computer Vision.

\bibitem{zeng2022ce2}
Wenyuan Zeng, Meng Li, Wenjie Xiong, et al. (2022). \textit{MPCViT: Searching for Accurate and Efficient MPC-Friendly Vision Transformer with Heterogeneous Attention}. IEEE International Conference on Computer Vision.

\bibitem{lin20225ad}
Ji Lin, Haifeng Lin, and Fang Wang (2022). \textit{STPM_SAHI: A Small-Target Forest Fire Detection Model Based on Swin Transformer and Slicing Aided Hyper Inference}. Forests.

\bibitem{reghunath2022z8g}
L. Reghunath, and R. Rajan (2022). \textit{Transformer-based ensemble method for multiple predominant instruments recognition in polyphonic music}. EURASIP Journal on Audio, Speech, and Music Processing.

\bibitem{kundu2022z97}
Dipanjali Kundu, Umme Raihan Siddiqi, and Md. Mahbubur Rahman (2022). \textit{Vision Transformer based Deep Learning Model for Monkeypox Detection}. 2022 25th International Conference on Computer and Information Technology (ICCIT).

\bibitem{sun2022cti}
Fan Sun, Wujie Zhou, Lv Ye, et al. (2022). \textit{Hierarchical Decoding Network Based on Swin Transformer for Detecting Salient Objects in RGB-T Images}. IEEE Signal Processing Letters.

\bibitem{li2022gef}
Yupeng Li, Huimin Lu, Yifan Wang, et al. (2022). \textit{ViT-Cap: A Novel Vision Transformer-Based Capsule Network Model for Finger Vein Recognition}. Applied Sciences.

\bibitem{guo20228rt}
Bangwei Guo, Xingyu Li, Miao Yang, et al. (2022). \textit{Predicting microsatellite instability and key biomarkers in colorectal cancer from H&E‐stained images: achieving state‐of‐the‐art predictive performance with fewer data using Swin Transformer}. The Journal of Pathology: Clinical Research.

\bibitem{li202240n}
Ao Li, Yaqin Zhao, and Zhaoxiang Zheng (2022). \textit{Novel Recursive BiFPN Combining with Swin Transformer for Wildland Fire Smoke Detection}. Forests.

\bibitem{jiang2022jlc}
Xiaoben Jiang, Yu Zhu, Gan Cai, et al. (2022). \textit{MXT: A New Variant of Pyramid Vision Transformer for Multi-label Chest X-ray Image Classification}. Cognitive Computation.

\bibitem{lin2021oan}
Yang Lin, Tianyu Zhang, Peiqin Sun, et al. (2021). \textit{FQ-ViT: Fully Quantized Vision Transformer without Retraining}. arXiv.org.

\bibitem{wang2022dl1}
Jing Wang, Haotian Fa, X. Hou, et al. (2022). \textit{MSTRIQ: No Reference Image Quality Assessment Based on Swin Transformer with Multi-Stage Fusion}. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW).

\bibitem{li2022wab}
Han Li, Sufang Li, Jiguo Yu, et al. (2022). \textit{Plant disease and insect pest identification based on vision transformer}. Other Conferences.

\bibitem{park2022eln}
Sangjoon Park, and Jong-Chul Ye (2022). \textit{Multi-Task Distributed Learning Using Vision Transformer With Random Patch Permutation}. IEEE Transactions on Medical Imaging.

\bibitem{shen2022d6i}
Yifan Shen, Li Liu, Zhihao Tang, et al. (2022). \textit{Explainable Survival Analysis with Convolution-Involved Vision Transformer}. AAAI Conference on Artificial Intelligence.

\bibitem{wang20224wo}
Aili Wang, Shuang Xing, Yan Zhao, et al. (2022). \textit{A Hyperspectral Image Classification Method Based on Adaptive Spectral Spatial Kernel Combined with Improved Vision Transformer}. Remote Sensing.

\bibitem{tao2022gdr}
Tianxin Tao, Daniele Reda, and M. V. D. Panne (2022). \textit{Evaluating Vision Transformer Methods for Deep Reinforcement Learning from Pixels}. arXiv.org.

\bibitem{wu2021nmg}
Shupei Wu, Youqiang Sun, and He Huang (2021). \textit{Multi-granularity Feature Extraction Based on Vision Transformer for Tomato Leaf Disease Recognition}. 2021 3rd International Academic Exchange Conference on Science and Technology Innovation (IAECST).

\bibitem{liu2022c56}
Jinwei Liu, Yan Li, Guitao Cao, et al. (2022). \textit{Feature Pyramid Vision Transformer for MedMNIST Classification Decathlon}. IEEE International Joint Conference on Neural Network.

\bibitem{wang2022pb8}
Yangtao Wang, Yanzhao Xie, Lisheng Fan, et al. (2022). \textit{STMG: Swin transformer for multi-label image recognition with graph convolution network}. Neural computing & applications (Print).

\bibitem{xiong2022ec2}
Zinan Xiong, Chenxi Wang, Ying Li, et al. (2022). \textit{Swin-Pose: Swin Transformer Based Human Pose Estimation}. Conference on Multimedia Information Processing and Retrieval.

\bibitem{sun2022pom}
Ruinan Sun, and Yu Pang (2022). \textit{Efficient Lung Cancer Image Classification and Segmentation Algorithm Based on Improved Swin Transformer}. arXiv.org.

\bibitem{qi2022yq9}
Zheng Qi, AprilPyone Maungmaung, Yuma Kinoshita, et al. (2022). \textit{Privacy-Preserving Image Classification Using Vision Transformer}. European Signal Processing Conference.

\bibitem{ma2022vf3}
Xiaojian Ma, Weili Nie, Zhiding Yu, et al. (2022). \textit{RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning}. International Conference on Learning Representations.

\bibitem{wang2022tok}
Ziyang Wang, Will Zhao, Zixuan Ni, et al. (2022). \textit{Adversarial Vision Transformer for Medical Image Semantic Segmentation with Limited Annotations}. British Machine Vision Conference.

\bibitem{wang2022pee}
Kai Wang, Fei Yang, and Joost van de Weijer (2022). \textit{Attention Distillation: self-supervised vision transformer students need more guidance}. British Machine Vision Conference.

\bibitem{jannat20228u6}
Fatema-E- Jannat, and A. Willis (2022). \textit{Improving Classification of Remotely Sensed Images with the Swin Transformer}. SoutheastCon.

\bibitem{chen2022r27}
Yuzhong Chen, Zhe Xiao, Lin Zhao, et al. (2022). \textit{Mask-guided Vision Transformer (MG-ViT) for Few-Shot Learning}. arXiv.org.

\bibitem{wang2021p2r}
Haoran Wang, Yanju Ji, Kaiwen Song, et al. (2021). \textit{ViT-P: Classification of Genitourinary Syndrome of Menopause From OCT Images Based on Vision Transformer Models}. IEEE Transactions on Instrumentation and Measurement.

\bibitem{sajid2021xb6}
Usman Sajid, Xiangyu Chen, Hasan Sajid, et al. (2021). \textit{Audio-Visual Transformer Based Crowd Counting}. 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW).

\bibitem{xing2022kqr}
W. Xing, and K. Egiazarian (2022). \textit{Residual Swin Transformer Channel Attention Network for Image Demosaicing}. European Workshop on Visual Information Processing.

\bibitem{garaiman2022xwd}
A. Garaiman, F. Nooralahzadeh, C. Mihai, et al. (2022). \textit{Vision transformer assisting rheumatologists in screening for capillaroscopy changes in systemic sclerosis: an artificial intelligence model}. Rheumatology.

\bibitem{wang2022wyu}
Ziyang Wang, Nanqing Dong, and I. Voiculescu (2022). \textit{Computationally-Efficient Vision Transformer for Medical Image Semantic Segmentation Via Dual Pseudo-Label Supervision}. International Conference on Information Photonics.

\bibitem{hou2022ver}
Zejiang Hou, and S. Kung (2022). \textit{Multi-Dimensional Vision Transformer Compression via Dependency Guided Gaussian Process Search}. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW).

\bibitem{agilandeeswari202273m}
L. Agilandeeswari, and S. D. Meena (2022). \textit{SWIN transformer based contrastive self-supervised learning for animal detection and classification}. Multimedia tools and applications.

\bibitem{qin2022cfg}
Haonan Qin, Weiying Xie, Yunsong Li, et al. (2022). \textit{HTD-VIT: Spectral-Spatial Joint Hyperspectral Target Detection with Vision Transformer}. IEEE International Geoscience and Remote Sensing Symposium.

\bibitem{wang2022ohd}
Boyuan Wang (2022). \textit{Automatic Mushroom Species Classification Model for Foodborne Disease Prevention Based on Vision Transformer}. Journal of Food Quality.

\bibitem{yu2022o30}
Hyunwoo Yu, J. Shim, Jaeho Kwak, et al. (2022). \textit{Vision Transformer-Based Retina Vessel Segmentation with Deep Adaptive Gamma Correction}. IEEE International Conference on Acoustics, Speech, and Signal Processing.

\bibitem{boukabouya2022ffi}
Rayene Amina Boukabouya, A. Moussaoui, and Mohamed Berrimi (2022). \textit{Vision Transformer Based Models for Plant Disease Detection and Diagnosis}. International Symposium on Information and Automation.

\bibitem{wang2022d7p}
Nan Wang, Xiangjun Meng, Xiangchao Meng, et al. (2022). \textit{Convolution-Embedded Vision Transformer With Elastic Positional Encoding for Pansharpening}. IEEE Transactions on Geoscience and Remote Sensing.

\bibitem{song20215tk}
Jeonggeun Song (2021). \textit{UFO-ViT: High Performance Linear Vision Transformer without Softmax}. arXiv.org.

\bibitem{xie2021th0}
Jiangtao Xie, Rui Zeng, Qilong Wang, et al. (2021). \textit{So-ViT: Mind Visual Tokens for Vision Transformer}. arXiv.org.

\bibitem{sun2022bm5}
Yu-shan Sun, Hao Zheng, Guo-cheng Zhang, et al. (2022). \textit{DP-ViT: A Dual-Path Vision Transformer for Real-Time Sonar Target Detection}. Remote Sensing.

\bibitem{jing2022nkb}
Yanhao Jing, and Feng Wang (2022). \textit{TP-VIT: A Two-Pathway Vision Transformer for Video Action Recognition}. IEEE International Conference on Acoustics, Speech, and Signal Processing.

\bibitem{li2022spu}
Rui Li, Weihua Li, Yi Yang, et al. (2022). \textit{Swinv2-Imagen: Hierarchical Vision Transformer Diffusion Models for Text-to-Image Generation}. Neural computing & applications (Print).

\bibitem{song2022y4v}
Hwanjun Song, Deqing Sun, Sanghyuk Chun, et al. (2022). \textit{An Extendable, Efficient and Effective Transformer-based Object Detector}. arXiv.org.

\bibitem{shukla2022jxz}
Neha Shukla, Anand Pandey, A. P. Shukla, et al. (2022). \textit{ECG-ViT: A Transformer-Based ECG Classifier for Energy-Constraint Wearable Devices}. J. Sensors.

\bibitem{tran2022bvd}
Nguyen H. Tran, Ta Duc Huy, S. T. Duong, et al. (2022). \textit{Improving Local Features with Relevant Spatial Information by Vision Transformer for Crowd Counting}. British Machine Vision Conference.

\bibitem{hong2022ks6}
Weixiang Hong, Jiangwei Lao, Wang Ren, et al. (2022). \textit{Training Object Detectors from Scratch: An Empirical Study in the Era of Vision Transformer}. Computer Vision and Pattern Recognition.

\bibitem{panboonyuen2021b4h}
Teerapong Panboonyuen, Sittinun Thongbai, W. Wongweeranimit, et al. (2021). \textit{Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama}. Inf..

\bibitem{zhao2022wi7}
Hong Zhao, Zhiwen Chen, Lan Guo, et al. (2022). \textit{Video captioning based on vision transformer and reinforcement learning}. PeerJ Computer Science.

\bibitem{wang2022h3u}
Yuchen Wang, L. Qing, Zhengyong Wang, et al. (2022). \textit{Multi-Level Transformer-Based Social Relation Recognition}. Italian National Conference on Sensors.

\bibitem{liu2021yw0}
Hao Liu, Xinghua Jiang, Xin Li, et al. (2021). \textit{NomMer: Nominate Synergistic Context in Vision Transformer for Visual Recognition}. Computer Vision and Pattern Recognition.

\bibitem{gul202290q}
A. Gul, Ozdemir Cetin, Christoph Reich, et al. (2022). \textit{Histopathological image classification based on self-supervised vision transformer and weak labels}. Medical Imaging.

\bibitem{zhao2022koc}
Youpeng Zhao, Huadong Tang, Yingying Jiang, et al. (2022). \textit{Lightweight Vision Transformer with Cross Feature Attention}. arXiv.org.

\bibitem{yang2022qwh}
Yali Yang, Yuanping Xu, Chaolong Zhang, et al. (2022). \textit{Hierarchical Vision Transformer with Channel Attention for RGB-D Image Segmentation}. International Symposium on Signal Processing Systems.

\bibitem{alquraishi2022j3v}
M. S. Al-Quraishi, I. Elamvazuthi, T. Tang, et al. (2022). \textit{Decoding the User’s Movements Preparation From EEG Signals Using Vision Transformer Architecture}. IEEE Access.

\bibitem{jin2021qdw}
Weiqiang Jin, Hang Yu, and Xiangfeng Luo (2021). \textit{CvT-ASSD: Convolutional vision-Transformer Based Attentive Single Shot MultiBox Detector}. IEEE International Conference on Tools with Artificial Intelligence.

\bibitem{lee2022rf1}
K. Lee, Bhavin Jawade, D. Mohan, et al. (2022). \textit{Attribute De-biased Vision Transformer (AD-ViT) for Long-Term Person Re-identification}. Advanced Video and Signal Based Surveillance.

\bibitem{shi2022evc}
Yongtao Shi, Xiaodong Zhao, Fan Zhang, et al. (2022). \textit{Non-Intrusive Load Monitoring Based on Swin-Transformer with Adaptive Scaling Recurrence Plot}. Energies.

\bibitem{zhang20223g5}
Huaqi Zhang, Huang Chen, Jin Qin, et al. (2022). \textit{MC-ViT: Multi-path cross-scale vision transformer for thymoma histopathology whole slide image typing}. Frontiers in Oncology.

\bibitem{zim202282d}
Abid Hasan Zim, Aeyan Ashraf, Aquib Iqbal, et al. (2022). \textit{A Vision Transformer-Based Approach to Bearing Fault Classification via Vibration Signals}. Asia-Pacific Signal and Information Processing Association Annual Summit and Conference.

\bibitem{bao202239k}
Shuai Bao, Jiping Liu, Liang Wang, et al. (2022). \textit{Landslide Susceptibility Mapping by Fusing Convolutional Neural Networks and Vision Transformer}. Italian National Conference on Sensors.

\bibitem{sun2022nny}
Mengshu Sun, Z. Li, Alec Lu, et al. (2022). \textit{FPGA-aware automatic acceleration framework for vision transformer with mixed-scheme quantization: late breaking results}. Design Automation Conference.

\bibitem{munyer2022pfs}
Travis J. E. Munyer, D. Brinkman, Xin Zhong, et al. (2022). \textit{Foreign Object Debris Detection for Airport Pavement Images Based on Self-Supervised Localization and Vision Transformer}. 2022 International Conference on Computational Science and Computational Intelligence (CSCI).

\bibitem{fan2022wve}
Hong-wei Fan, Ningge Ma, Xu-hui Zhang, et al. (2022). \textit{New intelligent fault diagnosis approach of rolling bearing based on improved vibration gray texture image and vision transformer}. Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science.

\bibitem{wang2022gq4}
Yi Wang, Zhiwen Fan, Tianlong Chen, et al. (2022). \textit{Can We Solve 3D Vision Tasks Starting from A 2D Vision Transformer?}. arXiv.org.

\bibitem{ali2022dux}
Luqman Ali, Hamad Al Jassmi, Wasif Khan, et al. (2022). \textit{Crack45K: Integration of Vision Transformer with Tubularity Flow Field (TuFF) and Sliding-Window Approach for Crack-Segmentation in Pavement Structures}. Buildings.

\bibitem{chougui2022mpo}
Abdeldjalil Chougui, Achraf Moussaoui, and A. Moussaoui (2022). \textit{Plant-Leaf Diseases Classification using CNN, CBAM and Vision Transformer}. International Symposium on Information and Automation.

\bibitem{zhuang2021hqu}
Li Zhuang (2021). \textit{Deep-Learning-Based Diagnosis of Cassava Leaf Diseases Using Vision Transformer}. Artificial Intelligence and Cloud Computing Conference.

\bibitem{chen2021d1q}
Xiaoyue Chen, Sei-ichiro Kamata, and Weilian Zhou (2021). \textit{Hyperspectral Image Classification Based on Multi-stage Vision Transformer with Stacked Samples}. IEEE Region 10 Conference.

\bibitem{hatamizadeh2024xr6}
Ali Hatamizadeh, and Jan Kautz (2024). \textit{MambaVision: A Hybrid Mamba-Transformer Vision Backbone}. Computer Vision and Pattern Recognition.

\bibitem{ryali202339q}
Chaitanya K. Ryali, Yuan-Ting Hu, Daniel Bolya, et al. (2023). \textit{Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles}. International Conference on Machine Learning.

\bibitem{yao2023sax}
Jing Yao, Bing Zhang, Chenyu Li, et al. (2023). \textit{Extended Vision Transformer (ExViT) for Land Use and Land Cover Classification: A Multimodal Deep Learning Framework}. IEEE Transactions on Geoscience and Remote Sensing.

\bibitem{li2023287}
Xiangtai Li, Henghui Ding, Wenwei Zhang, et al. (2023). \textit{Transformer-Based Visual Segmentation: A Survey}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{zhao2024671}
Zhuoyi Zhao, Xiang Xu, Shutao Li, et al. (2024). \textit{Hyperspectral Image Classification Using Groupwise Separable Convolutional Vision Transformer Network}. IEEE Transactions on Geoscience and Remote Sensing.

\bibitem{dehghani2023u7e}
Mostafa Dehghani, Basil Mustafa, J. Djolonga, et al. (2023). \textit{Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution}. Neural Information Processing Systems.

\bibitem{duan2024q7h}
Yuchen Duan, Weiyun Wang, Zhe Chen, et al. (2024). \textit{Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures}. International Conference on Learning Representations.

\bibitem{barman2024q21}
Utpal Barman, Parismita Sarma, Mirzanur Rahman, et al. (2024). \textit{ViT-SmartAgri: Vision Transformer and Smartphone-Based Plant Disease Detection for Smart Agriculture}. Agronomy.

\bibitem{jamil20230ll}
Sonain Jamil, and Anisha Roy (2023). \textit{An efficient and robust Phonocardiography (PCG)-based Valvular Heart Diseases (VHD) detection framework using Vision Transformer (ViT)}. Comput. Biol. Medicine.

\bibitem{paal2024eg1}
Ishak Paçal, Melek Alaftekin, and F. Zengul (2024). \textit{Enhancing Skin Cancer Diagnosis Using Swin Transformer with Hybrid Shifted Window-Based Multi-head Self-attention and SwiGLU-Based MLP}. Journal of imaging informatics in medicine.

\bibitem{zhang2023k43}
Xiaosong Zhang, Yunjie Tian, Lingxi Xie, et al. (2023). \textit{HiViT: A Simpler and More Efficient Design of Hierarchical Vision Transformer}. International Conference on Learning Representations.

\bibitem{himel2024u0i}
Galib Muhammad Shahriar Himel, Md. Masudul Islam, Kh Abdullah Al-Aff, et al. (2024). \textit{Skin Cancer Segmentation and Classification Using Vision Transformer for Automatic Analysis in Dermatoscopy-Based Noninvasive Digital System}. International Journal of Biomedical Imaging.

\bibitem{xu20235cu}
Qin Xu, Jiahui Wang, Bo Jiang, et al. (2023). \textit{Fine-Grained Visual Classification via Internal Ensemble Learning Transformer}. IEEE transactions on multimedia.

\bibitem{chi202331y}
Kaichen Chi, Yuan Yuan, and Qi Wang (2023). \textit{Trinity-Net: Gradient-Guided Swin Transformer-Based Remote Sensing Image Dehazing and Beyond}. IEEE Transactions on Geoscience and Remote Sensing.

\bibitem{patro202303d}
Badri N. Patro, Vinay P. Namboodiri, and Vijay Srinivas Agneeswaran (2023). \textit{SpectFormer: Frequency and Attention is what you need in a Vision Transformer}. IEEE Workshop/Winter Conference on Applications of Computer Vision.

\bibitem{pan2023hry}
Xuran Pan, Tianzhu Ye, Zhuofan Xia, et al. (2023). \textit{Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention}. Computer Vision and Pattern Recognition.

\bibitem{wang2024mrk}
Ziyang Wang, and Chao Ma (2024). \textit{Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for Scribble-based Medical Image Segmentation}. arXiv.org.

\bibitem{tabbakh2023ao7}
A. Tabbakh, and Soubhagya Sankar Barpanda (2023). \textit{A Deep Features Extraction Model Based on the Transfer Learning Model and Vision Transformer “TLMViT” for Plant Disease Classification}. IEEE Access.

\bibitem{dutta2023aet}
Pramit Dutta, Khaleda Akhter Sathi, Md.Azad Hossain, et al. (2023). \textit{Conv-ViT: A Convolution and Vision Transformer-Based Hybrid Feature Extraction Method for Retinal Disease Detection}. Journal of Imaging.

\bibitem{qiu2024eh4}
Yuhang Qiu, Honghui Chen, Xingbo Dong, et al. (2024). \textit{IFViT: Interpretable Fixed-Length Representation for Fingerprint Matching via Vision Transformer}. IEEE Transactions on Information Forensics and Security.

\bibitem{li2023nnd}
Guoqiang Li, Yuchao Wang, Qing Zhao, et al. (2023). \textit{PMVT: a lightweight vision transformer for plant disease identification on mobile devices}. Frontiers in Plant Science.

\bibitem{zhao20243f3}
Hu Zhao, Keyan Ren, Tianyi Yue, et al. (2024). \textit{TransFG: A Cross-View Geo-Localization of Satellite and UAVs Imagery Pipeline Using Transformer-Based Feature Aggregation and Gradient Guidance}. IEEE Transactions on Geoscience and Remote Sensing.

\bibitem{song2024fx9}
Huaxiang Song, Yuxuan Yuan, Zhiwei Ouyang, et al. (2024). \textit{Quantitative regularization in robust vision transformer for remote sensing image classification}. Photogrammetric Record.

\bibitem{cai2023hji}
Yimin Cai, Yuqing Long, Zhenggong Han, et al. (2023). \textit{Swin Unet3D: a three-dimensional medical image segmentation network combining vision transformer and convolution}. BMC Medical Informatics and Decision Making.

\bibitem{akinpelu2024d4m}
S. Akinpelu, Serestina Viriri, and A. Adegun (2024). \textit{An enhanced speech emotion recognition using vision transformer}. Scientific Reports.

\bibitem{hayat2024e4f}
Mansoor Hayat, Nouman Ahmad, Anam Nasir, et al. (2024). \textit{Hybrid Deep Learning EfficientNetV2 and Vision Transformer (EffNetV2-ViT) Model for Breast Cancer Histopathological Image Classification}. IEEE Access.

\bibitem{li2024g3z}
Yongxin Li, Mengyuan Liu, You Wu, et al. (2024). \textit{Learning Adaptive and View-Invariant Vision Transformer for Real-Time UAV Tracking}. International Conference on Machine Learning.

\bibitem{arshed2023zen}
Muhammad Asad Arshed, Shahzad Mumtaz, Muhammad Ibrahim, et al. (2023). \textit{Multi-Class Skin Cancer Classification Using Vision Transformer Networks and Convolutional Neural Network-Based Pre-Trained Models}. Inf..

\bibitem{qin20242eu}
S. Qin, Taiyue Qi, Tang Deng, et al. (2024). \textit{Image segmentation using Vision Transformer for tunnel defect assessment}. Comput. Aided Civ. Infrastructure Eng..

\bibitem{lee2023iwc}
C. Lee, K. Lim, Yu Xuan Song, et al. (2023). \textit{Plant-CNN-ViT: Plant Classification with Ensemble of Convolutional Neural Networks and Vision Transformer}. Plants.

\bibitem{tagnamas20246ug}
Jaouad Tagnamas, Hiba Ramadan, Ali Yahyaouy, et al. (2024). \textit{Multi-task approach based on combined CNN-transformer for efficient segmentation and classification of breast tumors in ultrasound images}. Visual Computing for Industry, Biomedicine, and Art.

\bibitem{li2023jft}
Shuiwang Li, Yangxiang Yang, Dan Zeng, et al. (2023). \textit{Adaptive and Background-Aware Vision Transformer for Real-Time UAV Tracking}. IEEE International Conference on Computer Vision.

\bibitem{song2024c99}
Bofan Song, D. Kc, Rubin Yuchan Yang, et al. (2024). \textit{Classification of Mobile-Based Oral Cancer Images Using the Vision Transformer and the Swin Transformer}. Cancers.

\bibitem{aksoy20240c0}
Serra Aksoy, P. Demircioğlu, and I. Bogrekci (2024). \textit{Enhancing Melanoma Diagnosis with Advanced Deep Learning Models Focusing on Vision Transformer, Swin Transformer, and ConvNeXt}. Dermatopathology.

\bibitem{leem2024j4t}
Saebom Leem, and Hyunseok Seo (2024). \textit{Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention}. AAAI Conference on Artificial Intelligence.

\bibitem{chen2024asi}
Shiming Chen, W. Hou, Salman H. Khan, et al. (2024). \textit{Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning}. Computer Vision and Pattern Recognition.

\bibitem{lin202343q}
Fudong Lin, Summer Crawford, Kaleb Guillot, et al. (2023). \textit{MMST-ViT: Climate Change-aware Crop Yield Prediction via Multi-Modal Spatial-Temporal Vision Transformer}. IEEE International Conference on Computer Vision.

\bibitem{ghahremani202491m}
Morteza Ghahremani, Mohammad Khateri, Bailiang Jian, et al. (2024). \textit{H-ViT: A Hierarchical Vision Transformer for Deformable Image Registration}. Computer Vision and Pattern Recognition.

\bibitem{wang20249qa}
Haiyang Wang, Hao Tang, Li Jiang, et al. (2024). \textit{GiT: Towards Generalist Vision Transformer through Universal Language Interface}. European Conference on Computer Vision.

\bibitem{shahin2024g0q}
Mohammad Shahin, F. F. Chen, Mazdak Maghanaki, et al. (2024). \textit{Improving the Concrete Crack Detection Process via a Hybrid Visual Transformer Algorithm}. Italian National Conference on Sensors.

\bibitem{zhu2023dpi}
Liang Zhu, Yingyue Li, Jiemin Fang, et al. (2023). \textit{WeakTr: Exploring Plain Vision Transformer for Weakly-supervised Semantic Segmentation}. arXiv.org.

\bibitem{yu2023l1g}
Zitong Yu, Rizhao Cai, Yawen Cui, et al. (2023). \textit{Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing}. International Journal of Computer Vision.

\bibitem{ko2024eax}
Jinsol Ko, Soyeon Park, and H. G. Woo (2024). \textit{Optimization of vision transformer-based detection of lung diseases from chest X-ray images}. BMC Medical Informatics Decis. Mak..

\bibitem{yang2024w08}
Qiying Yang, and Rongzuo Guo (2024). \textit{An Unsupervised Method for Industrial Image Anomaly Detection with Vision Transformer-Based Autoencoder}. Italian National Conference on Sensors.

\bibitem{nazih20238nf}
Waleed Nazih, Ahmad O. Aseeri, Osama Youssef Atallah, et al. (2023). \textit{Vision Transformer Model for Predicting the Severity of Diabetic Retinopathy in Fundus Photography-Based Retina Images}. IEEE Access.

\bibitem{xia2023bp7}
Zhuofan Xia, Xuran Pan, Shiji Song, et al. (2023). \textit{DAT++: Spatially Dynamic Vision Transformer with Deformable Attention}. arXiv.org.

\bibitem{nag2023cfn}
Shashank Nag, G. Datta, Souvik Kundu, et al. (2023). \textit{ViTA: A Vision Transformer Inference Accelerator for Edge Applications}. International Symposium on Circuits and Systems.

\bibitem{gezici20246lf}
Abdul Haluk Batur Gezici, and Emre Sefer (2024). \textit{Deep Transformer-Based Asset Price and Direction Prediction}. IEEE Access.

\bibitem{ghazouani202342t}
Fethi Ghazouani, Pierre Vera, and Su Ruan (2023). \textit{Efficient brain tumor segmentation using Swin transformer and enhanced local self-attention}. International Journal of Computer Assisted Radiology and Surgery.

\bibitem{wang202338i}
Guanqun Wang, He Chen, Liang Chen, et al. (2023). \textit{P2FEViT: Plug-and-Play CNN Feature Embedded Hybrid Vision Transformer for Remote Sensing Image Classification}. Remote Sensing.

\bibitem{guo2024tr7}
Yu Guo, Zhi Zhang, and Yuzhen Huang (2024). \textit{Dual Class Token Vision Transformer for Direction of Arrival Estimation in Low SNR}. IEEE Signal Processing Letters.

\bibitem{wang2023ski}
Wei Wang, Xin Yang, and Jinhui Tang (2023). \textit{Vision Transformer With Hybrid Shifted Windows for Gastrointestinal Endoscopy Image Classification}. IEEE transactions on circuits and systems for video technology (Print).

\bibitem{zheng202325h}
Fujian Zheng, Shuai Lin, Wei Zhou, et al. (2023). \textit{A Lightweight Dual-Branch Swin Transformer for Remote Sensing Scene Classification}. Remote Sensing.

\bibitem{mogan2023ywz}
Jashila Nair Mogan, C. Lee, K. Lim, et al. (2023). \textit{Gait-CNN-ViT: Multi-Model Gait Recognition with Convolutional Neural Networks and Vision Transformer}. Italian National Conference on Sensors.

\bibitem{ebert202377v}
Nikolas Ebert, D. Stricker, and Oliver Wasenmüller (2023). \textit{PLG-ViT: Vision Transformer with Parallel Local and Global Self-Attention}. Italian National Conference on Sensors.

\bibitem{wang20245bq}
Yong Wang, Cheng Lu, Hailun Lian, et al. (2024). \textit{Speech Swin-Transformer: Exploring a Hierarchical Transformer with Shifted Windows for Speech Emotion Recognition}. IEEE International Conference on Acoustics, Speech, and Signal Processing.

\bibitem{cao20241ng}
Jie Cao, Tingting Xu, Yu-he Deng, et al. (2024). \textit{Galaxy morphology classification based on Convolutional vision Transformer (CvT)}. Astronomy &amp; Astrophysics.

\bibitem{yang2024in8}
Yaoming Yang, Zhili Cai, Shuxia Qiu, et al. (2024). \textit{Vision transformer with masked autoencoders for referable diabetic retinopathy classification based on large-size retina image}. PLoS ONE.

\bibitem{hussain2025qoe}
Tahir Hussain, Hayaru Shouno, Abid Hussain, et al. (2025). \textit{EFFResNet-ViT: A Fusion-Based Convolutional and Vision Transformer Model for Explainable Medical Image Classification}. IEEE Access.

\bibitem{shim2023z7g}
D. Shim, and H. J. Kim (2023). \textit{SwinDepth: Unsupervised Depth Estimation using Monocular Sequences via Swin Transformer and Densely Cascaded Network}. IEEE International Conference on Robotics and Automation.

\bibitem{alam2024t09}
Taukir Alam, Wei-Cheng Yeh, Fang Rong Hsu, et al. (2024). \textit{An Integrated Approach using YOLOv8 and ResNet, SeResNet & Vision Transformer (ViT) Algorithms based on ROI Fracture Prediction in X-ray Images of the Elbow.}. Current medical imaging.

\bibitem{yang2024tti}
Ruiping Yang, Liu Kun, Shaohua Xu, et al. (2024). \textit{ViT-UperNet: a hybrid vision transformer with unified-perceptual-parsing network for medical image segmentation}. Complex &amp; Intelligent Systems.

\bibitem{wang20245hx}
Dong Wang, Jian Lian, and Wanzhen Jiao (2024). \textit{Multi-label classification of retinal disease via a novel vision transformer model}. Frontiers in Neuroscience.

\bibitem{song202479c}
Huaxiang Song, Hanjun Xia, Wenhui Wang, et al. (2024). \textit{QAGA-Net: enhanced vision transformer-based object detection for remote sensing images}. International Journal of Intelligent Computing and Cybernetics.

\bibitem{li2023lvd}
Xiaoye Li, and Bin-Bin Zhang (2023). \textit{FV-ViT: Vision Transformer for Finger Vein Recognition}. IEEE Access.

\bibitem{ma2023vhi}
Xiaochen Ma, Bo Du, Xianggen Liu, et al. (2023). \textit{IML-ViT: Image Manipulation Localization by Vision Transformer}. arXiv.org.

\bibitem{han202416k}
Huiyan Han, H. Zeng, Liqun Kuang, et al. (2024). \textit{A human activity recognition method based on Vision Transformer}. Scientific Reports.

\bibitem{katar202352u}
Oğuzhan Katar, and Ozal Yildirim (2023). \textit{An Explainable Vision Transformer Model Based White Blood Cells Classification and Localization}. Diagnostics.

\bibitem{hemalatha2024a14}
S. Hemalatha, and Jayachandiran Jai Jaganath Babu (2024). \textit{A Multitask Learning-Based Vision Transformer for Plant Disease Localization and Classification}. International Journal of Computational Intelligence Systems.

\bibitem{ma2024uan}
Chiyu Ma, Jon Donnelly, Wenjun Liu, et al. (2024). \textit{Interpretable Image Classification with Adaptive Prototype-based Vision Transformers}. Neural Information Processing Systems.

\bibitem{lai20238ck}
D. K. Lai, Zi-Han Yu, Tommy Yau-Nam Leung, et al. (2023). \textit{Vision Transformers (ViT) for Blanket-Penetrating Sleep Posture Recognition Using a Triple Ultra-Wideband (UWB) Radar System}. Italian National Conference on Sensors.

\bibitem{wang2024luv}
Zhikan Wang, Zhongyao Cheng, Jiajie Xiong, et al. (2024). \textit{A Timely Survey on Vision Transformer for Deepfake Detection}. arXiv.org.

\bibitem{ling2023x36}
Zhixin Ling, Zhen Xing, Xiangdong Zhou, et al. (2023). \textit{PanoSwin: a Pano-style Swin Transformer for Panorama Understanding}. Computer Vision and Pattern Recognition.

\bibitem{wang2023bfo}
Zhifeng Wang, Jialong Yao, Chunyan Zeng, et al. (2023). \textit{Students' Classroom Behavior Detection System Incorporating Deformable DETR with Swin Transformer and Light-Weight Feature Pyramid Network}. Syst..

\bibitem{yin2023029}
Miao Yin, Burak Uzkent, Yilin Shen, et al. (2023). \textit{GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous Structured Pruning for Vision Transformer}. AAAI Conference on Artificial Intelligence.

\bibitem{mishra2024fbz}
Swapneel Mishra, Saumya Seth, Shrishti Jain, et al. (2024). \textit{Image Caption Generation using Vision Transformer and GPT Architecture}. 2024 2nd International Conference on Advancement in Computation & Computer Technologies (InCACCT).

\bibitem{heidari2024d9k}
Moein Heidari, Reza Azad, Sina Ghorbani Kolahi, et al. (2024). \textit{Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights}. arXiv.org.

\bibitem{yu2023fqo}
Sheng Yu, Dihua Zhai, and Yuanqing Xia (2023). \textit{A Novel Robotic Pushing and Grasping Method Based on Vision Transformer and Convolution}. IEEE Transactions on Neural Networks and Learning Systems.

\bibitem{zhao2023pau}
Qihao Zhao, Yangyu Huang, Wei Hu, et al. (2023). \textit{MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer}. International Conference on Learning Representations.

\bibitem{pan20249k5}
C. Pan, Junran Peng, and Zhaoxiang Zhang (2024). \textit{Depth-Guided Vision Transformer With Normalizing Flows for Monocular 3D Object Detection}. IEEE/CAA Journal of Automatica Sinica.

\bibitem{huan202345b}
Sha Huan, Zhaoyue Wang, Xiaoqiang Wang, et al. (2023). \textit{A lightweight hybrid vision transformer network for radar-based human activity recognition}. Scientific Reports.

\bibitem{belal2023x1u}
Mohamad Mulham Belal, and Dr. Divya Meena Sundaram (2023). \textit{Global-Local Attention-Based Butterfly Vision Transformer for Visualization-Based Malware Classification}. IEEE Access.

\bibitem{li20238ti}
Yanjing Li, Sheng Xu, Mingbao Lin, et al. (2023). \textit{Bi-ViT: Pushing the Limit of Vision Transformer Quantization}. AAAI Conference on Artificial Intelligence.

\bibitem{huo2023e5h}
Yingzi Huo, Kai Jin, Jiahong Cai, et al. (2023). \textit{Vision Transformer (ViT)-based Applications in Image Classification}. 2023 IEEE 9th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS).

\bibitem{kim2023cvz}
Jiseob Kim, Kyuhong Shim, Junhan Kim, et al. (2023). \textit{Vision Transformer-Based Feature Extraction for Generalized Zero-Shot Learning}. IEEE International Conference on Acoustics, Speech, and Signal Processing.

\bibitem{fan2023whi}
Chunyu Fan, Q. Su, Zhifeng Xiao, et al. (2023). \textit{ViT-FRD: A Vision Transformer Model for Cardiac MRI Image Segmentation Based on Feature Recombination Distillation}. IEEE Access.

\bibitem{zhao2023rle}
Kai Zhao, Ruitao Lu, Siyu Wang, et al. (2023). \textit{ST-YOLOA: a Swin-transformer-based YOLO model with an attention mechanism for SAR ship detection under complex background}. Frontiers in Neurorobotics.

\bibitem{xie20234ve}
Tao Xie, Kun Dai, Zhiqiang Jiang, et al. (2023). \textit{ViT-MVT: A Unified Vision Transformer Network for Multiple Vision Tasks}. IEEE Transactions on Neural Networks and Learning Systems.

\bibitem{li20233lv}
Gary Y. Li, Junyu Chen, Se-In Jang, et al. (2023). \textit{SwinCross: Cross-modal Swin Transformer for Head-and-Neck Tumor Segmentation in PET/CT Images}. Medical Physics (Lancaster).

\bibitem{ma2023qek}
Xiaochen Ma, Bo Du, Zhuohang Jiang, et al. (2023). \textit{IML-ViT: Benchmarking Image Manipulation Localization by Vision Transformer}. Unpublished manuscript.

\bibitem{tanimola20246cv}
Oluwatosin Tanimola, Olamilekan Shobayo, O. Popoola, et al. (2024). \textit{Breast Cancer Classification Using Fine-Tuned SWIN Transformer Model on Mammographic Images}. Analytics.

\bibitem{chen2023xxw}
Tiansheng Chen, and L. Mo (2023). \textit{Swin-Fusion: Swin-Transformer with Feature Fusion for Human Action Recognition}. Neural Processing Letters.

\bibitem{ranjan20243bn}
Navin Ranjan, and Andreas E. Savakis (2024). \textit{LRP-QViT: Mixed-Precision Vision Transformer Quantization via Layer-wise Relevance Propagation}. arXiv.org.

\bibitem{fu20232q3}
Xiangqu Fu, Qirui Ren, Hao Wu, et al. (2023). \textit{P3 ViT: A CIM-Based High-Utilization Architecture With Dynamic Pruning and Two-Way Ping-Pong Macro for Vision Transformer}. IEEE Transactions on Circuits and Systems Part 1: Regular Papers.

\bibitem{shi20235zy}
Chaojun Shi, Shiwei Zhao, Kecheng Zhang, et al. (2023). \textit{Face-based age estimation using improved Swin Transformer with attention-based convolution}. Frontiers in Neuroscience.

\bibitem{deressa2023lrl}
Deressa Wodajo Deressa, Hannes Mareen, Peter Lambert, et al. (2023). \textit{GenConViT: Deepfake Video Detection Using Generative Convolutional Vision Transformer}. Applied Sciences.

\bibitem{aburass2023qpf}
Sanad Aburass, and O. Dorgham (2023). \textit{Performance Evaluation of Swin Vision Transformer Model using Gradient Accumulation Optimization Technique}. arXiv.org.

\bibitem{hassija2025wq3}
Vikas Hassija, Balamurugan Palanisamy, Arpita Chatterjee, et al. (2025). \textit{Transformers for Vision: A Survey on Innovative Methods for Computer Vision}. IEEE Access.

\bibitem{huang20238er}
Yihang Huang, and Wan Li (2023). \textit{Resizer Swin Transformer-Based Classification Using sMRI for Alzheimer’s Disease}. Applied Sciences.

\bibitem{liu20230kl}
Jianwei Liu, Shirui Lyu, Denis Hadjivelichkov, et al. (2023). \textit{ViT-A*: Legged Robot Path Planning using Vision Transformer A*}. IEEE-RAS International Conference on Humanoid Robots.

\bibitem{he20238sy}
Ru He, Xiaomin Wang, Huazhen Chen, et al. (2023). \textit{VHR-BirdPose: Vision Transformer-Based HRNet for Bird Pose Estimation with Attention Mechanism}. Electronics.

\bibitem{guo2023dpo}
Yangyang Guo, Wenhao Hong, Jiaxin Wu, et al. (2023). \textit{Vision-Based Cow Tracking and Feeding Monitoring for Autonomous Livestock Farming: The YOLOv5s-CA+DeepSORT-Vision Transformer}. IEEE robotics & automation magazine.

\bibitem{wang2023j6b}
Yun Wang, Shuai Shi, and Jie Chen (2023). \textit{Efficient Blind Hyperspectral Unmixing with Non-Local Spatial Information Based on Swin Transformer}. IEEE International Geoscience and Remote Sensing Symposium.

\bibitem{gopal20237ol}
Goutam Yelluru Gopal, and Maria A. Amer (2023). \textit{Mobile Vision Transformer-based Visual Object Tracking}. British Machine Vision Conference.

\bibitem{liu2023awp}
Zhiyang Liu, Pengyu Yin, and Zhenhua Ren (2023). \textit{An Efficient FPGA-Based Accelerator for Swin Transformer}. arXiv.org.

\bibitem{fu20228zq}
Zujun Fu (2022). \textit{Vision Transformer: Vit and its Derivatives}. arXiv.org.

\bibitem{sahoo20223yl}
P. Sahoo, S. Saha, S. Mondal, et al. (2022). \textit{Vision Transformer Based COVID-19 Detection Using Chest CT-scan images}. 2022 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI).

\bibitem{ganz20249zr}
Roy Ganz, Yair Kittenplon, Aviad Aberdam, et al. (2024). \textit{Question Aware Vision Transformer for Multimodal Reasoning}. Computer Vision and Pattern Recognition.

\bibitem{paal2024no4}
Ishak Paçal, and Ismail Kunduracioglu (2024). \textit{Data-Efficient Vision Transformer Models for Robust Classification of Sugarcane}. Journal of Soft Computing and Decision Analytics.

\bibitem{hassan20243qi}
Nada M. Hassan, Safwat Hamad, and Khaled Mahar (2024). \textit{YOLO-based CAD framework with ViT transformer for breast mass detection and classification in CESM and FFDM images}. Neural computing & applications (Print).

\bibitem{k2024wyx}
Abinaya K, and S. B (2024). \textit{A Deep Learning-Based Approach for Cervical Cancer Classification Using 3D CNN and Vision Transformer.}. Journal of imaging informatics in medicine.

\bibitem{nguyen2024id9}
Xuan-Bac Nguyen, Hoang-Quan Nguyen, Samuel Yen-Chi Chen, et al. (2024). \textit{QClusformer: A Quantum Transformer-based Framework for Unsupervised Visual Clustering}. International Conference on Quantum Computing and Engineering.

\bibitem{almohimeed2024jq1}
Abdulaziz Almohimeed, Mohamed Shehata, Nora El-Rashidy, et al. (2024). \textit{ViT-PSO-SVM: Cervical Cancer Predication Based on Integrating Vision Transformer with Particle Swarm Optimization and Support Vector Machine}. Bioengineering.

\bibitem{hao202488z}
Chao Hao, Zitong Yu, Xin Liu, et al. (2024). \textit{A Simple Yet Effective Network Based on Vision Transformer for Camouflaged Object and Salient Object Detection}. IEEE Transactions on Image Processing.

\bibitem{yao20244li}
Haiming Yao, Wei Luo, Jianan Lou, et al. (2024). \textit{Scalable Industrial Visual Anomaly Detection With Partial Semantics Aggregation Vision Transformer}. IEEE Transactions on Instrumentation and Measurement.

\bibitem{dong20245zz}
Peiyan Dong, Jinming Zhuang, Zhuoping Yang, et al. (2024). \textit{EQ-ViT: Algorithm-Hardware Co-Design for End-to-End Acceleration of Real-Time Vision Transformer Inference on Versal ACAP Architecture}. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems.

\bibitem{zhang2024jha}
Haoyu Zhang, Raghavendra Ramachandra, Kiran B. Raja, et al. (2024). \textit{Generalized Single-Image-Based Morphing Attack Detection Using Deep Representations from Vision Transformer}. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW).

\bibitem{boukhari2024gbb}
D. E. Boukhari (2024). \textit{Facial Beauty Prediction Based on Vision Transformer}. International Journal of Electrical and Electronic Engineering &amp; Telecommunications.

\bibitem{song2025idg}
Huaxiang Song, Junping Xie, Yunyang Wang, et al. (2025). \textit{Optimized Data Distribution Learning for Enhancing Vision Transformer‐Based Object Detection in Remote Sensing Images}. Photogrammetric Record.

\bibitem{zhou2024tps}
Heng Zhou, Jingmin Yang, Shanghui Deng, et al. (2024). \textit{VTIL: A multi-layer indoor location algorithm for RSSI images based on vision transformer}. Engineering Research Express.

\bibitem{abbaoui20244wy}
Wafae Abbaoui, Sara Retal, Soumia Ziti, et al. (2024). \textit{Automated Ischemic Stroke Classification from MRI Scans: Using a Vision Transformer Approach}. Journal of Clinical Medicine.

\bibitem{yang2024nyx}
Xiangyang Yang, Dan Zeng, Xucheng Wang, et al. (2024). \textit{Adaptively Bypassing Vision Transformer Blocks for Efficient Visual Tracking}. Pattern Recognition.

\bibitem{yang20241kf}
Zhiding Yang, and Weimin Huang (2024). \textit{SWHFormer: A Vision Transformer for Significant Wave Height Estimation From Nautical Radar Images}. IEEE Transactions on Geoscience and Remote Sensing.

\bibitem{hu202434n}
Youbing Hu, Yun Cheng, Anqi Lu, et al. (2024). \textit{LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition}. AAAI Conference on Artificial Intelligence.

\bibitem{yang20244dq}
Bin Yang, Binghan Zhang, Yilong Han, et al. (2024). \textit{Vision transformer-based visual language understanding of the construction process}. Alexandria Engineering Journal.

\bibitem{keresh20249rl}
Arman Keresh, and Pakizar Shamoi (2024). \textit{Liveness Detection in Computer Vision: Transformer-Based Self-Supervised Learning for Face Anti-Spoofing}. IEEE Access.

\bibitem{hu20247km}
Yuxin Hu, Han Zhou, Ning Cao, et al. (2024). \textit{Synthetic CT generation based on CBCT using improved vision transformer CycleGAN}. Scientific Reports.

\bibitem{alsulami2024ffb}
Abdulkream A Alsulami, Aishah Albarakati, A. A. Al-Ghamdi, et al. (2024). \textit{Identification of Anomalies in Lung and Colon Cancer Using Computer Vision-Based Swin Transformer with Ensemble Model on Histopathological Images}. Bioengineering.

\bibitem{yang2024wxl}
Lu Yang, Songtao Guo, Defang Liu, et al. (2024). \textit{ConViTML: A Convolutional Vision Transformer-Based Meta-Learning Framework for Real-Time Edge Network Traffic Classification}. IEEE Transactions on Network and Service Management.

\bibitem{p2024nbn}
Venkatasaichandrakanth P, and I. M (2024). \textit{GNViT- An enhanced image-based groundnut pest classification using Vision Transformer (ViT) model}. PLoS ONE.

\bibitem{wu2024tsm}
Xinhao Wu, Sirui Xu, Ming-Yu Gao, et al. (2024). \textit{A new ECT image reconstruction algorithm based on Vision transformer (ViT)}. Flow Measurement and Instrumentation.

\bibitem{dong2024bm2}
Qiwei Dong, Xiaoru Xie, and Zhongfeng Wang (2024). \textit{SWAT: An Efficient Swin Transformer Accelerator Based on FPGA}. Asia and South Pacific Design Automation Conference.

\bibitem{swapno2025y2b}
S. M. M. Swapno, S. N. Nobel, Md Babul Islam, et al. (2025). \textit{ViT-SENet-Tom: machine learning-based novel hybrid squeeze-excitation network and vision transformer framework for tomato fruits classification}. Neural computing & applications (Print).

\bibitem{yoo2024u1f}
Dayeon Yoo, Jeesu Kim, and Jinwoo Yoo (2024). \textit{FSwin Transformer: Feature-Space Window Attention Vision Transformer for Image Classification}. IEEE Access.

\bibitem{he2024m6j}
Kan He, Wei Zhang, Xuejun Zong, et al. (2024). \textit{Network Intrusion Detection Based on Feature Image and Deformable Vision Transformer Classification}. IEEE Access.

\bibitem{zhang202489a}
Zichen Zhang, Jing Li, C. Cai, et al. (2024). \textit{Bearing Fault Diagnosis Based on Image Information Fusion and Vision Transformer Transfer Learning Model}. Applied Sciences.

\bibitem{zhang2024pd6}
Yueqi Zhang, Lichen Feng, Hongwei Shan, et al. (2024). \textit{A 109-GOPs/W FPGA-Based Vision Transformer Accelerator With Weight-Loop Dataflow Featuring Data Reusing and Resource Saving}. IEEE transactions on circuits and systems for video technology (Print).

\bibitem{dong20242ow}
Xinlong Dong, Peicheng Shi, Yueyue Tang, et al. (2024). \textit{Vehicle Classification Algorithm Based on Improved Vision Transformer}. World Electric Vehicle Journal.

\bibitem{kayacan2024yy7}
Yavuz Emre Kayacan, and I. Erer (2024). \textit{A Vision-Transformer-Based Approach to Clutter Removal in GPR: DC-ViT}. IEEE Geoscience and Remote Sensing Letters.

\bibitem{liu20248jh}
Jintao Liu, Alfredo Tolón Becerra, José Fernando Bienvenido-Barcena, et al. (2024). \textit{CFFI-Vit: Enhanced Vision Transformer for the Accurate Classification of Fish Feeding Intensity in Aquaculture}. Journal of Marine Science and Engineering.

\bibitem{shi2024r44}
Huihong Shi, Xin Cheng, Wendong Mao, et al. (2024). \textit{P2-ViT: Power-of-Two Post-Training Quantization and Acceleration for Fully Quantized Vision Transformer}. IEEE Transactions on Very Large Scale Integration (VLSI) Systems.

\bibitem{xin2024ljt}
Xinyue Xin, Ming Li, Yan Wu, et al. (2024). \textit{PolSAR-MPIformer: A Vision Transformer Based on Mixed Patch Interaction for Dual-Frequency PolSAR Image Adaptive Fusion Classification}. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing.

\bibitem{zhou2024qty}
Jian Zhou, Guochuan Zhao, and Yonglong Li (2024). \textit{Vison Transformer-Based Automatic Crack Detection on Dam Surface}. Water.

\bibitem{monjezi2024tdt}
Ehsan Monjezi, G. Akbarizadeh, and Karim Ansari-Asl (2024). \textit{RI-ViT: A Multi-Scale Hybrid Method Based on Vision Transformer for Breast Cancer Detection in Histopathological Images}. IEEE Access.

\bibitem{baek2025h8e}
Eu-tteum Baek (2025). \textit{Attention Score-Based Multi-Vision Transformer Technique for Plant Disease Classification}. Italian National Conference on Sensors.

\bibitem{payne2024u8l}
David L. Payne, Xuan Xu, Farshid Faraji, et al. (2024). \textit{Automated Detection of Cervical Spinal Stenosis and Cord Compression via Vision Transformer and Rules-Based Classification}. American Journal of Neuroradiology.

\bibitem{qi2024rzy}
Nan Qi, Yan Piao, Hao Zhang, et al. (2024). \textit{Seizure prediction based on improved vision transformer model for EEG channel optimization}. Computer Methods in Biomechanics and Biomedical Engineering.

\bibitem{mercier2024063}
J. Mercier, O. Ertz, and E. Bocher (2024). \textit{Quantifying Dwell Time With Location-based Augmented Reality: Dynamic AOI Analysis on Mobile Eye Tracking Data With Vision Transformer}. Journal of Eye Movement Research.

\bibitem{sikkandar2024p0d}
Mohamed Yacin Sikkandar, S. Sundaram, Ahmad Alassaf, et al. (2024). \textit{Utilizing adaptive deformable convolution and position embedding for colon polyp segmentation with a visual transformer}. Scientific Reports.

\bibitem{hou2024e4y}
Mingyang Hou, Zhiyong Huang, Zhi Yu, et al. (2024). \textit{CSwT-SR: Conv-Swin Transformer for Blind Remote Sensing Image Super-Resolution With Amplitude-Phase Learning and Structural Detail Alternating Learning}. IEEE Transactions on Geoscience and Remote Sensing.

\bibitem{nfor2025o20}
Kintoh Allen Nfor, Tagne Poupi Theodore Armand, Kenesbaeva Periyzat Ismaylovna, et al. (2025). \textit{An Explainable CNN and Vision Transformer-Based Approach for Real-Time Food Recognition}. Nutrients.

\bibitem{xiang2024tww}
Changcheng Xiang, Duofen Yin, Fei Song, et al. (2024). \textit{A Fast and Robust Safety Helmet Network Based on a Mutilscale Swin Transformer}. Buildings.

\bibitem{tian20242kr}
Yuan Tian, Jingxuan Zhu, Huang Yao, et al. (2024). \textit{Facial Expression Recognition Based on Vision Transformer with Hybrid Local Attention}. Applied Sciences.

\bibitem{zhou2024r66}
Nan Zhou, Mingming Xu, Biaoqun Shen, et al. (2024). \textit{ViT-UNet: A Vision Transformer Based UNet Model for Coastal Wetland Classification Based on High Spatial Resolution Imagery}. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing.

\bibitem{taye20244db}
Gizatie Desalegn Taye, Zewdie Habtie Sisay, Genet Worku Gebeyhu, et al. (2024). \textit{Thoracic computed tomography (CT) image-based identification and severity classification of COVID-19 cases using vision transformer (ViT)}. Discover Applied Sciences.

\bibitem{alohali2024xwz}
Manal Abdullah Alohali, Nora El-Rashidy, Saad Alaklabi, et al. (2024). \textit{Swin-GA-RF: genetic algorithm-based Swin Transformer and random forest for enhancing cervical cancer classification}. Frontiers in Oncology.

\bibitem{gao20246ks}
Zhenchang Gao, Shanshan Chen, Jinxian Huang, et al. (2024). \textit{Real-time quantitative detection of hydrocolloid adulteration in meat based on Swin Transformer and smartphone.}. Journal of Food Science.

\bibitem{du2024s3t}
Yufeng Du, Rongyun Zhang, Peicheng Shi, et al. (2024). \textit{ST-LaneNet: Lane Line Detection Method Based on Swin Transformer and LaneNet}. Chinese Journal of Mechanical Engineering.

\bibitem{tiwari2024jm9}
R. Tiwari, Himani Maheshwari, Vinay Gautam, et al. (2024). \textit{CurrencyNet: A Vision Transformer-Based Approach for Indian Currency Note Classification with Optimizer Exploration}. 2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT).

\bibitem{du20248pd}
Chunlai Du, Yanhui Guo, and Yuhang Zhang (2024). \textit{A Deep Learning-Based Intrusion Detection Model Integrating Convolutional Neural Network and Vision Transformer for Network Traffic Attack in the Internet of Things}. Electronics.

\bibitem{chaurasia2024tri}
A. Chaurasia, H. C. Harris, P. W. Toohey, et al. (2024). \textit{A generalised vision transformer-based self-supervised model for diagnosing and grading prostate cancer using histological images}. medRxiv.

\bibitem{karagz2024ukp}
Meryem Altin Karagöz, Özkan U. Nalbantoglu, and Geoffrey C. Fox (2024). \textit{Residual Vision Transformer (ResViT) Based Self-Supervised Learning Model for Brain Tumor Classification}. arXiv.org.

\bibitem{lee2025r01}
Hyojin Lee, You Rim Choi, Hyun Kyung Lee, et al. (2025). \textit{Explainable vision transformer for automatic visual sleep staging on multimodal PSG signals}. npj Digit. Medicine.

\bibitem{dmen2024cb9}
Sezer Dümen, Esra Kavalcı Yılmaz, Kemal Adem, et al. (2024). \textit{Performance of vision transformer and swin transformer models for lemon quality classification in fruit juice factories}. European Food Research and Technology.

\bibitem{ferdous2024f89}
Gazi Jannatul Ferdous, Khaleda Akhter Sathi, Md. Azad Hossain, et al. (2024). \textit{SPT-Swin: A Shifted Patch Tokenization Swin Transformer for Image Classification}. IEEE Access.

\bibitem{akan2024izq}
Sara Akan, Songül Varli, and Mohammad Alfrad Nobel Bhuiyan (2024). \textit{An enhanced Swin Transformer for soccer player reidentification}. Scientific Reports.

\bibitem{nahak20242mv}
Pradeep Nahak, D. K. Pratihar, and A. K. Deb (2024). \textit{Tomato maturity stage prediction based on vision transformer and deep convolution neural networks}. International Journal of Hybrid Intelligent Systems.

\bibitem{han2024f96}
Yufei Han, Haoyuan Chen, Linwei Yao, et al. (2024). \textit{MAT-VIT:A Vision Transformer with MAE-Based Self-Supervised Auxiliary Task for Medical Image Classification}. International Conference on Computer Supported Cooperative Work in Design.

\bibitem{zhao2024p8o}
Xiaoping Zhao, Jingjing Xu, Zhichen Lin, et al. (2024). \textit{BiCFormer: Swin Transformer based model for classification of benign and malignant pulmonary nodules}. Measurement science and technology.

\bibitem{li2024qva}
Tao Li, and Yi Zhang (2024). \textit{A Contour-Aware Monocular Depth Estimation Network Using Swin Transformer and Cascaded Multiscale Fusion}. IEEE Sensors Journal.

\bibitem{wang2024ueo}
Yancheng Wang, and Yingzhen Yang (2024). \textit{Efficient Visual Transformer by Learnable Token Merging}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{qi2024f5d}
Haochen Qi, Xiangwei Kong, Zhibo Jin, et al. (2024). \textit{A Vision-Transformer-Based Convex Variational Network for Bridge Pavement Defect Segmentation}. IEEE transactions on intelligent transportation systems (Print).

\bibitem{zhu2024l2i}
Shaojun Zhu, Guotao Chen, Hongguang Chen, et al. (2024). \textit{Squeeze-and-excitation-attention-based mobile vision transformer for grading recognition of bladder prolapse in pelvic MRI images.}. Medical Physics (Lancaster).

\bibitem{roy2024r9y}
Barsha Roy, Md. Farukuzzaman Faruk, Md Nazmul Islam, et al. (2024). \textit{A Cutting-Edge Ensemble of Vision Transformer and ResNet101v2 Based Transfer Learning for the Precise Classification of Leukemia Sub-types from Peripheral Blood Smear Images}. International Conference on Electrical Engineering and Information Communication Technology.

\bibitem{wang2024w4u}
Chunbao Wang, Xiangyu Wang, Zeyu Gao, et al. (2024). \textit{Multiple serous cavity effusion screening based on smear images using vision transformer}. Scientific Reports.

\bibitem{pan202424q}
Guangliang Pan, Qihui Wu, Bo Zhou, et al. (2024). \textit{Spectrum Prediction With Deep 3D Pyramid Vision Transformer Learning}. IEEE Transactions on Wireless Communications.

\bibitem{du2024lml}
Haiying Du, Jie Shen, Jing Wang, et al. (2024). \textit{Vision transformer-based electronic nose for enhanced mixed gases classification}. Measurement science and technology.

\bibitem{luo202432g}
Kevin Luo, and Ie-bin Lian (2024). \textit{Building a Vision Transformer-Based Damage Severity Classifier with Ground-Level Imagery of Homes Affected by California Wildfires}. Fire.

\bibitem{elnabi2025psy}
Samy Abd El-Nabi, Ahmed F. Ibrahim, El-Sayed M. El-Rabaie, et al. (2025). \textit{Driver Drowsiness Detection Using Swin Transformer and Diffusion Models for Robust Image Denoising}. IEEE Access.

\bibitem{ergn2025r6s}
Ebru Ergün (2025). \textit{High precision banana variety identification using vision transformer based feature extraction and support vector machine}. Scientific Reports.

\bibitem{mohsin2025gup}
Muhammad Ahmed Mohsin, Muhammad Jazib, Zeeshan Alam, et al. (2025). \textit{Vision Transformer Based Semantic Communications for Next Generation Wireless Networks}. 2025 IEEE International Conference on Communications Workshops (ICC Workshops).

\bibitem{marcos2024oo2}
Luella Marcos, Paul S. Babyn, and J. Alirezaie (2024). \textit{Pure Vision Transformer (CT-ViT) with Noise2Neighbors Interpolation for Low-Dose CT Image Denoising.}. Journal of imaging informatics in medicine.

\bibitem{peng2024kal}
Xianhui Peng, Chenchen Xu, Peng Zhang, et al. (2024). \textit{Computer vision classification detection of chicken parts based on optimized Swin-Transformer}. CyTA - Journal of Food.

\bibitem{urrea20245k4}
Claudio Urrea, and Maximiliano Vélez (2024). \textit{Enhancing Autonomous Visual Perception in Challenging Environments: Bilateral Models with Vision Transformer and Multilayer Perceptron for Traversable Area Detection}. Technologies.

\bibitem{zhang2024b7v}
Jinnian Zhang, Weijie Chen, Tanmayee Joshi, et al. (2024). \textit{BAE-ViT: An Efficient Multimodal Vision Transformer for Bone Age Estimation}. Tomography.

\bibitem{saleem20249yl}
Hira Saleem, Flora Salim, and Cormac Purcell (2024). \textit{STC-ViT: Spatio Temporal Continuous Vision Transformer for Weather Forecasting}. Unpublished manuscript.

\bibitem{zhou2024toe}
Yang Zhou, Cai Yang, Ping Wang, et al. (2024). \textit{ViT-FuseNet: Multimodal Fusion of Vision Transformer for Vehicle-Infrastructure Cooperative Perception}. IEEE Access.

\bibitem{lijin2024mhk}
P. Lijin, M. Ullah, Anuja Vats, et al. (2024). \textit{PolySegNet: improving polyp segmentation through swin transformer and vision transformer fusion.}. Biomedical Engineering Letters.

\bibitem{huang2024htf}
Lan Huang, Jiong Ma, Hui Yang, et al. (2024). \textit{Research and implementation of multi-disease diagnosis on chest X-ray based on vision transformer}. Quantitative Imaging in Medicine and Surgery.

\bibitem{chen2024cha}
Chuanyu Chen, Yi Luo, Qiuyang Hou, et al. (2024). \textit{A vision transformer-based deep transfer learning nomogram for predicting lymph node metastasis in lung adenocarcinoma.}. Medical Physics (Lancaster).

\bibitem{shahin2024o1c}
Mohammed Shahin, and Mohamed Deriche (2024). \textit{A Novel Framework based on a Hybrid Vision Transformer and Deep Neural Network for Deepfake Detection}. International Multi-Conference on Systems, Signals & Devices.

\bibitem{xu2024wux}
Yang Xu, and Zuqiang Meng (2024). \textit{Interpretable vision transformer based on prototype parts for COVID-19 detection}. IET Image Processing.

\bibitem{park2024d7y}
Joohyuk Park, Yong-Nam Oh, Yongjune Kim, et al. (2024). \textit{Vision Transformer-Based Semantic Communications With Importance-Aware Quantization}. IEEE Internet of Things Journal.

\bibitem{elharrouss20252ng}
O. Elharrouss, Y. Akbari, Noor Almaadeed, et al. (2025). \textit{PDC-ViT : Source Camera Identification using Pixel Difference Convolution and Vision Transformer}. Neural computing & applications (Print).

\bibitem{du2024i6n}
Yongqiang Du, Haoran Liu, Shengjie He, et al. (2024). \textit{InViT: GAN Inversion-Based Vision Transformer for Blind Image Inpainting}. IEEE Access.

\bibitem{guo2024o8u}
Qianyu Guo, Ziqing Yu, Jiaming Fu, et al. (2024). \textit{Force-EvT: A Closer Look at Robotic Gripper Force Measurement with Event-Based Vision Transformer}. 2024 6th International Conference on Reconfigurable Mechanisms and Robots (ReMAR).

\bibitem{zhang2024g0m}
Kunpeng Zhang, Mengyan Lyu, Xinxin Guo, et al. (2024). \textit{Temporal Shift Module-Based Vision Transformer Network for Action Recognition}. IEEE Access.

\bibitem{xu2025tku}
Lu Xu, Rui Shi, and Yijia Zhang (2025). \textit{A Radio Frequency Sensor-Based UAV Detection and Identification System Using Improved Vision Transformer-Based Model}. IEEE Sensors Journal.

\bibitem{li2024m4t}
Yang Li, Doudou Zhang, and Jianli Xiao (2024). \textit{A New Method for Vehicle Logo Recognition Based on Swin Transformer}. arXiv.org.

\end{thebibliography}

\end{document}