\subsection{Object Detection and Semantic Segmentation}

The application of Vision Transformers (ViTs) to dense prediction tasks, such as object detection and semantic segmentation, marks a critical advancement in computer vision, leveraging their inherent capacity for global context modeling and long-range dependency capture. While initial ViT architectures excelled in image classification, their fixed-resolution inputs and quadratic computational complexity presented significant hurdles for tasks demanding fine-grained spatial understanding and multi-scale feature representation. As established in Section 4, the development of hierarchical ViT architectures, notably the Swin Transformer \cite{liu2021ljs} and Pyramid Vision Transformer (PVT) \cite{PVT}, was instrumental in overcoming these limitations. These models generate multi-scale feature maps and achieve linear computational complexity, thereby transforming ViTs into versatile backbones competitive with, and often superior to, traditional Convolutional Neural Networks (CNNs) for dense prediction.

For object detection, the pioneering work of DETR (DEtection TRansformer) \cite{DETR} introduced a paradigm shift by formulating object detection as a direct set prediction problem. This end-to-end approach eliminated the need for hand-designed components like non-maximum suppression (NMS) and anchor boxes, relying instead on the Transformer's global attention to reason about object relationships and the entire image context. However, the original DETR suffered from slow convergence and challenges in detecting small objects due to its global attention mechanism. Subsequent research addressed these limitations; Deformable DETR \cite{DeformableDETR} (not in provided papers, but assumed as keystone) significantly improved efficiency and performance by employing deformable attention, which focuses on a small set of sampling points around a reference, thereby reducing computational cost and enhancing the ability to handle objects at varying scales. This advancement made DETR-based models more practical and competitive. For instance, `\cite{wang2023bfo}` demonstrates the effectiveness of integrating Deformable DETR with a Swin Transformer backbone and a lightweight Feature Pyramid Network (FPN) for robust classroom behavior detection, showcasing improved accuracy for multi-scale targets. Further refinements, such as ViDT (Vision and Detection Transformers) \cite{song2022y4v}, have integrated Swin Transformer as a reconfigured backbone with efficient transformer decoders to boost detection and instance segmentation performance, highlighting the synergy between hierarchical backbones and refined detection heads.

In semantic segmentation, SETR (SEgmentation TRansformer) \cite{SETR} demonstrated the efficacy of a purely Transformer-based encoder for capturing extensive long-range dependencies crucial for dense pixel-level classification. However, pure Transformer encoders often struggle with precise local localization due to their patch-based processing. To mitigate this, hybrid models emerged, combining the strengths of both architectures. TransUNet \cite{TransUNet} is a prominent example, leveraging a Transformer encoder for global context modeling and a traditional CNN decoder for fine-grained local detail recovery, proving particularly effective in medical image segmentation. This hybrid strategy has been widely adopted; for instance, `\cite{panboonyuen20218r7}` explored Swin Transformer backbones with various CNN-based decoders (U-Net, PSPNet, FPN) for semantic segmentation on remotely sensed images, achieving state-of-the-art results. Similarly, UNetFormer \cite{hatamizadeh2022y9x} introduced a unified framework with a 3D Swin Transformer encoder and both CNN- and transformer-based decoders, excelling in 3D medical image segmentation by effectively capturing global anatomical context while maintaining local precision. Beyond hybrid approaches, advancements in weakly-supervised semantic segmentation (WSSS) have also leveraged ViTs; WeakTr \cite{zhu2023dpi} explores properties of plain ViTs for WSSS, adaptively fusing self-attention maps to generate high-quality class activation maps (CAMs), demonstrating ViT's potential even with limited supervision.

A significant evolution in dense prediction has been the development of unifying frameworks that tackle multiple tasks simultaneously. MaskFormer \cite{MaskFormer} (not in provided papers, but assumed as keystone) and its successor Mask2Former \cite{Mask2Former} (not in provided papers, but assumed as keystone) represent a paradigm shift by unifying instance, semantic, and panoptic segmentation under a mask classification framework. These models leverage a Transformer decoder with learnable queries to directly predict a set of class-agnostic masks and their corresponding class labels, simplifying the pipeline and achieving state-of-the-art results across all three segmentation tasks. This query-based approach inherently benefits from the Transformer's global reasoning capabilities to resolve ambiguities and capture complex object instances. The trend towards generalist models is further exemplified by GiT (Generalist Vision Transformer) \cite{wang20249qa}, which proposes a single vanilla ViT architecture with a universal language interface capable of handling diverse vision tasks, including detection and segmentation, without task-specific modules, fostering mutual enhancement across tasks.

The versatility of ViTs in dense prediction extends to numerous domain-specific applications. For instance, ST-YOLOA \cite{zhao2023rle} developed a Swin-Transformer-based YOLO model for robust SAR ship detection, integrating the hierarchical ViT's global context modeling within an enhanced YOLOX framework to boost accuracy in challenging backgrounds. In agricultural precision, SwinGD \cite{wang20215ra} applied Swin Transformer and DETR models for grape bunch detection, outperforming traditional CNNs. Furthermore, ViTs are enhancing autonomous perception systems, with bilateral models combining CNNs, ViT, and MLPs for traversable area detection \cite{urrea20245k4}, improving the capture of distant details for real-time semantic segmentation. Even in specialized medical image analysis, explainable ViT models are being developed for white blood cell classification and localization \cite{katar202352u}, demonstrating their ability to provide interpretable global context for diagnostic tasks.

In conclusion, the journey of ViTs in object detection and semantic segmentation has progressed from addressing initial architectural limitations to pioneering end-to-end and hybrid solutions. Hierarchical designs like Swin Transformer have established ViTs as competitive backbones, while frameworks like DETR and its deformable variants have revolutionized object detection. For segmentation, SETR, hybrid models like TransUNet and UNetFormer, and unifying mask-based approaches like MaskFormer have demonstrated the power of Transformers in structured prediction. Despite these advancements, challenges persist, including optimizing computational efficiency for extremely high-resolution inputs, further reducing the data-hungry nature of some Transformer variants, and enhancing interpretability. Future research will likely focus on developing more efficient and adaptable hybrid architectures, exploring novel attention mechanisms or their alternatives, and advancing generalist foundation models that can seamlessly handle a multitude of dense prediction tasks.