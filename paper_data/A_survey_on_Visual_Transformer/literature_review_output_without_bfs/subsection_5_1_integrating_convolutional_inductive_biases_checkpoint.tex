\subsection{Integrating Convolutional Inductive Biases}
The initial success of Vision Transformers (ViTs) in image recognition, despite their impressive global context modeling, quickly exposed a fundamental architectural challenge: their inherent lack of strong inductive biases for local feature processing and translation equivariance \cite{han2020yk0, hassija2025wq3}. Unlike Convolutional Neural Networks (CNNs), which inherently capture local patterns through their kernel operations, pure ViTs treat images as sequences of patches, relying on self-attention to learn spatial relationships from scratch. This often led to significant data hunger, performance degradation on smaller datasets, and increased computational demands for capturing fine-grained local details. Consequently, a pivotal trend in Vision Transformer research has been the strategic integration of convolutional inductive biases directly into ViT architectures, aiming to harness the complementary strengths of both paradigms. This integration seeks to combine the global reasoning capabilities of self-attention with the local feature extraction, parameter efficiency, and translation equivariance of convolutions.

This integration manifests in several distinct architectural strategies. One common approach involves incorporating convolutional layers within the initial patch embedding or tokenization process. For instance, Convolutional Vision Transformers (CvT) \cite{CvT} introduced convolutions into both the token embedding and the projection layers (query, key, and value) within the Transformer blocks. By replacing linear projections with depth-wise separable convolutions, CvT effectively injects local processing into the attention mechanism itself, enhancing local feature extraction and translation equivariance while retaining the global receptive field of self-attention. This direct integration allowed CvT to achieve improved performance and efficiency compared to pure ViTs, particularly on smaller datasets.

A second strategy involves designing hybrid architectures that strategically combine distinct convolutional and attentional layers or blocks. Co-Scale Conv-Attentional Image Transformers (CoaT) \cite{CoaT} exemplifies this by proposing a multi-scale architecture that leverages both convolutional and attentional layers at different stages. CoaT employs a co-scale attention mechanism that allows for interaction between features extracted by convolutional and self-attention branches, enabling efficient capture of multi-scale representations. This parallel processing and fusion of local and global features proved effective for various vision tasks. More recently, Next-ViT \cite{li2022a4u} further refined this hybrid approach by developing a Next Convolution Block (NCB) and a Next Transformer Block (NTB), which are then stacked using a novel Next Hybrid Strategy (NHS). Next-ViT explicitly aims for efficient deployment in industrial scenarios, demonstrating that by carefully designing and integrating deployment-friendly convolutional and transformer components, models can achieve superior latency/accuracy trade-offs compared to both pure CNNs and ViTs. The NCB efficiently captures local information, while the NTB handles global context, and their strategic interleaving ensures high performance across tasks like image classification, object detection, and semantic segmentation.

Another variant of hybrid integration involves using convolutions to enrich feature representations before or during the attention mechanism. The XFormer \cite{zhao2022koc} model, for example, combines efficient mobile CNNs with ViTs through a novel cross feature attention (XFA) mechanism. Here, convolutions provide a strong foundation for local representations, which are then enriched and fused with global attention through the XFA, leading to lightweight yet high-performing models. This approach highlights how convolutions can serve as powerful local feature extractors, whose outputs are then processed and contextualized by Transformer layers.

The benefits of injecting convolutional inductive biases are multifaceted. These hybrid models often exhibit improved data efficiency, requiring less pre-training data to achieve competitive performance, making them more practical for scenarios with limited labeled datasets. They also tend to be more efficient computationally, particularly for high-resolution inputs, as convolutions can process local information more effectively than global self-attention. Furthermore, the inherent translation equivariance of convolutions contributes to better generalization and robustness. This architectural convergence, where the strengths of CNNs and Transformers are combined, has led to models that are not only more efficient and performant but also more robust across a wider array of vision tasks. While pure ViTs, especially large-scale ones like ViTDet \cite{li2022raj} and Hiera \cite{ryali202339q}, can achieve strong results with extensive pre-training and minimal adaptations, the consistent improvements observed with well-designed hybrid architectures underscore the enduring value of explicitly incorporating convolutional inductive biases. This ongoing integration blurs the lines between these once distinct paradigms, pointing towards a future where optimal vision backbones dynamically leverage the best elements from both to achieve even greater efficiency, generalization, and practical utility.