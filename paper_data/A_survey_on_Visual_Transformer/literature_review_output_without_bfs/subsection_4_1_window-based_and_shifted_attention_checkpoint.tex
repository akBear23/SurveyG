\subsection*{Window-Based and Shifted Attention}

The initial success of Vision Transformers (ViTs) \cite{ViT} was tempered by the quadratic computational complexity of their global self-attention mechanism with respect to image resolution, posing a significant bottleneck for high-resolution images and dense prediction tasks. To overcome this, architectural innovations focused on introducing localized attention mechanisms, where self-attention is computed only within restricted spatial regions. This approach reduces complexity to linear with respect to image size, making ViTs computationally efficient and scalable.

A foundational step in this direction was the adoption of window-based attention, where the input image is partitioned into non-overlapping local windows, and self-attention is computed independently within each window. While this effectively reduces computational cost, it inherently limits the receptive field and prevents information exchange between different windows. The pivotal innovation to address this limitation while maintaining computational efficiency was introduced by the Swin Transformer \cite{Swin}. Swin Transformer proposed a hierarchical architecture that leverages a novel shifted window attention mechanism. In this design, attention is computed within non-overlapping windows in one layer, and then in the subsequent layer, the windows are shifted, allowing for connections between previously isolated regions. This alternating pattern of regular and shifted windowing enables cross-window connections and builds multi-scale feature representations, crucial for tasks requiring dense predictions, without incurring the quadratic cost of global attention. The Swin Transformer quickly became a new standard backbone, demonstrating superior performance across various computer vision tasks, including image classification, object detection, and semantic segmentation.

The hierarchical and window-based design of Swin Transformer has shown enhanced transferability compared to plain ViTs, often requiring less extensive fine-tuning for downstream tasks \cite{zhou2021rtn}. This adaptability has been leveraged in diverse applications, showcasing its versatility. For instance, the Swin Transformer architecture has been successfully adapted for medical image analysis, such as in SwinCross \cite{li20233lv}, which employs a cross-modal 3D Swin Transformer with cross-modal shifted window attention for head-and-neck tumor segmentation in PET/CT images, demonstrating its efficacy in integrating multi-modal data. Similarly, its robust performance has been validated in specialized classification tasks, achieving higher accuracy than traditional CNNs and plain ViTs for mobile-based oral cancer image classification \cite{song2024c99} and outperforming other object detection models for chicken part classification and detection \cite{peng2024kal}.

Despite the effectiveness of window-based architectures, their local nature presented new challenges, particularly concerning pre-training strategies like Masked Autoencoders (MAE). The random masking inherent in MAE pre-training can be difficult to reconcile with pyramid-based ViTs that rely on local window operators. To address this, Uniform Masking (UM) was proposed by \cite{li2022ow4}, enabling efficient and effective MAE pre-training for pyramid-based ViTs with locality. UM ensures uniform sampling across local windows and introduces secondary masking to enhance semantic learning, significantly improving pre-training efficiency and maintaining competitive fine-tuning performance.

While shifted windows are a powerful mechanism for cross-window communication, recent research has explored alternatives and questioned their absolute necessity under certain conditions. For example, \cite{li2022raj} investigated plain, non-hierarchical ViT backbones for object detection and found that competitive results could be achieved using simple window attention (without shifting) when augmented with very few cross-window propagation blocks. This suggests that explicit shifting might be less critical if other sparse mechanisms are introduced to facilitate information flow between windows. Furthermore, the Hiera model \cite{ryali202339q} demonstrated that by leveraging strong visual pretext tasks like MAE pre-training, many "bells-and-whistles" (complex vision-specific components) could be stripped from hierarchical ViTs without sacrificing accuracy. This indicates a potential synergy where powerful self-supervised pre-training can simplify architectural designs, potentially reducing the reliance on intricate windowing strategies for optimal performance.

In conclusion, window-based and particularly shifted attention mechanisms, pioneered by the Swin Transformer, were instrumental in making Vision Transformers computationally efficient and scalable for high-resolution images and dense prediction tasks. These innovations transformed ViTs into practical general-purpose backbones. The ongoing research continues to refine these concepts, exploring more efficient pre-training methods for windowed architectures and re-evaluating the precise mechanisms required for cross-window information exchange, balancing architectural complexity with the power of self-supervised learning.