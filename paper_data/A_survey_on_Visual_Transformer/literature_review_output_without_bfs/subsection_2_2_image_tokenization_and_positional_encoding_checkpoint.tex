\subsection{Image Tokenization and Positional Encoding}
The successful adaptation of Transformer architectures from natural language processing (NLP) to computer vision (CV) fundamentally relies on two critical initial processing steps: image tokenization and positional encoding \cite{han2020yk0, hassija2025wq3}. Unlike Convolutional Neural Networks (CNNs) that inherently leverage inductive biases for spatial locality and translation equivariance, Transformers process input sequences without an intrinsic understanding of spatial relationships or the original arrangement of visual elements. This inherent permutation-invariance of the self-attention mechanism \cite{Vaswani2017Attention} necessitates dedicated mechanisms to transform raw image data into a sequential format and explicitly embed spatial awareness.

The foundational Vision Transformer (ViT) \cite{ViT} introduced a straightforward, yet impactful, tokenization strategy. An input image is first partitioned into a grid of fixed-size, non-overlapping square patches, typically $16 \times 16$ pixels. Each patch is then flattened into a 1D vector and linearly projected into a higher-dimensional embedding space. This process converts the 2D image into a sequence of fixed-length visual tokens, analogous to word embeddings in NLP. A special "class token" (CLS token) is prepended to this sequence, serving as a global representation for classification tasks, similar to its use in BERT \cite{Vaswani2017Attention}. While effective for demonstrating the viability of Transformers in vision, this simple patch embedding has inherent limitations. It treats each patch as an independent entity, largely disregarding fine-grained local structural information and spatial relationships *within* the patch \cite{han2020yk0}. This can be particularly problematic for tasks requiring detailed local feature understanding or when dealing with smaller datasets where such inductive biases are more crucial. The fixed size of these patches also means that the model's receptive field is determined by the patch size, and changing input image resolutions would alter the number of tokens, requiring adaptations or re-training. This initial simplification, which diminishes the capture of local context, became a significant motivation for subsequent research into more sophisticated tokenization methods, such as those incorporating convolutional inductive biases or hierarchical structures, which will be explored in Section 3.3 and Section 5.1.

Complementing tokenization, positional encoding is indispensable for Transformers to comprehend the spatial arrangement of these visual tokens, as the self-attention mechanism itself is permutation-invariant \cite{Vaswani2017Attention}. Without positional information, the model would process an image's patches identically regardless of their original location, losing crucial spatial context. The original Transformer \cite{Vaswani2017Attention} (in NLP) typically employed fixed sinusoidal positional encodings, which are deterministic functions of position. In contrast, the ViT \cite{ViT} opted for learnable 1D positional embeddings. These embeddings are simply vectors, of the same dimension as the patch embeddings, that are added to each visual token based on its sequential index (corresponding to its spatial position in the flattened grid). The rationale behind learnable embeddings was their flexibility to adapt to the specific spatial patterns present in image data during training. It is critical to note that the original ViT paper, despite exploring 2D-aware and relative positional embeddings, found that simple 1D learnable embeddings performed comparably well. This unexpected efficacy was largely attributed to the ViT's extensive pre-training on massive datasets like JFT-300M, which allowed the model to implicitly learn complex spatial relationships and compensate for the lack of stronger explicit inductive biases \cite{hong2022ks6}.

However, a significant drawback of these fixed-size learnable embeddings is their lack of transferability to input images of different resolutions. If an image has a different number of patches than what the model was trained on, the pre-trained positional embeddings become incompatible, necessitating interpolation or re-training, which limits generalization \cite{wang2022gq4}. This rigidity in handling varying input sizes posed a challenge for deploying ViTs in diverse real-world scenarios where image resolutions can vary widely. This limitation spurred research into more flexible positional encoding mechanisms, including relative positional encodings and dynamic methods, which are crucial for enabling ViTs to scale effectively to different input dimensions and will be discussed in Section 4.1.

In summary, the initial strategies of simple patch-based tokenization and learnable 1D absolute positional embeddings in the original ViT were foundational in demonstrating the power of Transformers for vision tasks. These mechanisms were the necessary "glue" to adapt a sequence-processing architecture to 2D image data. However, their inherent limitations—the diminished capture of fine-grained local structural information and the inflexibility of absolute positional embeddings to varying input resolutions—became primary drivers for the extensive architectural innovations and optimizations explored in subsequent sections. These foundational concepts, along with their identified limitations, set the stage for the full Vision Transformer architecture, which is detailed next.