\subsection{Rethinking Token Mixing: Beyond Self-Attention}

While Vision Transformers have revolutionized computer vision, the computational cost and complexity of the self-attention mechanism remain a significant bottleneck, particularly for high-resolution images and real-time applications. This has spurred a wave of radical architectural explorations that challenge the necessity of complex self-attention, instead proposing simpler, more computationally efficient token mixing operations while retaining the overall 'MetaFormer' structure (token mixer + MLP). These works provocatively suggest that the core architectural design of Transformers, rather than the intricate self-attention mechanism itself, might be the primary driver of their success, opening avenues for faster and more hardware-friendly vision models.

Among the most direct challenges to self-attention is the \textit{MetaFormer} architecture, exemplified by \cite{PoolFormer}. This work demonstrated that even a simple spatial pooling operation, when integrated into the MetaFormer block (token mixer followed by an MLP), could achieve competitive performance. \cite{PoolFormer} posited that the overall architectural design principles of Transformers, such as residual connections, normalization layers, and the alternation of token mixing and channel-wise MLP layers, are more critical than the specific self-attention mechanism itself. This finding highlighted the potential for significantly simpler and more efficient token mixers without sacrificing performance.

Building on the premise of efficient global interactions, \cite{GFNet} introduced Global Filter Networks (GFNet), which replace self-attention with global filters applied in the frequency domain. By leveraging 2D Fourier transforms, GFNet achieves global receptive fields with a computational complexity that is linear with respect to the number of tokens, offering a highly efficient alternative to the quadratic complexity of standard self-attention. This approach demonstrated that global information exchange, crucial for Transformer performance, could be achieved through non-attention mechanisms, further supporting the MetaFormer hypothesis.

Another notable exploration into alternative token mixing is the Vision Permutator (ViP) \cite{ViP}. While its title includes "Permutable Self-Attention," ViP's core contribution lies in its permutation-based mixing strategy, which significantly departs from traditional self-attention by performing attention along different axes (height, width, and channel dimensions) sequentially. This design aims to capture global information more efficiently by reducing the computational burden associated with full pairwise token interactions, effectively acting as a more efficient, structured alternative to standard self-attention within the MetaFormer paradigm.

Further diversifying the landscape of non-attention token mixers, \cite{FocalNet} proposed Focal Modulation Networks, which replace self-attention with a focal modulation mechanism. This approach captures multi-range dependencies by dynamically modulating features based on their spatial location and context, without relying on explicit attention weights. FocalNet demonstrated that effective long-range interactions and hierarchical feature learning can be achieved through modulation operations, offering another compelling alternative to self-attention that is both efficient and performs competitively.

Collectively, these architectural innovations \cite{PoolFormer, GFNet, ViP, FocalNet} represent a critical re-evaluation of the core components of Vision Transformers. They consistently demonstrate that high performance can be maintained or even improved by replacing complex self-attention with simpler, more computationally efficient token mixing operations. This body of work underscores that the architectural blueprint of Transformers, characterized by its modularity and the separation of spatial mixing and channel-wise processing, is a powerful paradigm. The ongoing challenge lies in further exploring the trade-offs between the theoretical expressiveness of full self-attention and the practical benefits of these simpler, hardware-friendly alternatives, paving the way for a new generation of efficient and scalable vision models.