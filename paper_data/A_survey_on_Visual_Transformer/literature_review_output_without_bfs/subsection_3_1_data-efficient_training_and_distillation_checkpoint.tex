\subsection*{Data-Efficient Training and Distillation}

The introduction of the Vision Transformer (ViT) by \cite{dosovitskiy2020image} marked a pivotal moment in computer vision, demonstrating the potential of pure self-attention mechanisms for image recognition. However, a significant hurdle to its widespread adoption was its substantial data hunger, often necessitating pre-training on colossal proprietary datasets like JFT-300M to achieve performance competitive with state-of-the-art Convolutional Neural Networks (CNNs). This reliance on massive labeled data posed a critical barrier for researchers and practitioners without access to such resources, making data-efficient training strategies a paramount area of early research.

A foundational breakthrough in addressing ViT's data dependency was the work by Touvron et al. \cite{touvron2021training}, which introduced Data-efficient image Transformers (DeiT). This seminal paper demonstrated that ViTs could achieve competitive performance on standard datasets like ImageNet-1K without requiring extensive pre-training on external, larger datasets. The core innovation of DeiT was the effective application of knowledge distillation, a technique where a smaller "student" model learns from the output of a larger, pre-trained "teacher" model. Specifically, DeiT leveraged a strong CNN teacher (e.g., a RegNet model) to guide the training of a ViT student. The authors explored both "soft" distillation, where the student matches the teacher's softened probability distribution, and "hard" distillation, where the student is trained to predict the teacher's hard class labels. Crucially, DeiT introduced a dedicated "distillation token" alongside the standard class token, allowing the ViT student to learn directly from the teacher's predictions through an additional distillation loss. This mechanism significantly improved the student ViT's performance, enabling it to reach accuracy levels comparable to or even surpassing the teacher CNN, all while being trained solely on ImageNet-1K. This innovation was instrumental in democratizing ViT research, making the architecture accessible to a broader community.

Beyond the initial success of DeiT, subsequent research has further refined and extended distillation techniques for ViTs, often integrating them with other compression or efficiency strategies. For instance, while DeiT primarily focused on distilling class logits, \cite{wang2022pee} proposed Attention Distillation (AttnDistill) specifically for self-supervised Vision Transformer students. They argued that directly distilling information from the teacher's crucial attention mechanism to the student could significantly narrow the performance gap, especially in self-supervised learning (SSL) contexts where traditional logit-based distillation might be suboptimal. AttnDistill demonstrated superior k-NN accuracy on ImageNet-1K compared to existing self-supervised knowledge distillation methods, highlighting the importance of attention-level guidance for ViT students. This approach is particularly valuable as it is independent of the specific SSL algorithm used, offering a versatile method to improve ViT performance on memory and compute-constrained devices.

Knowledge distillation has also been integrated into broader model compression frameworks to enhance ViT efficiency and performance under resource constraints. \cite{yu2022iy0} introduced a Unified Visual Transformer Compression (UVC) framework that seamlessly combines pruning, layer skipping, and knowledge distillation. By formulating a budget-constrained, end-to-end optimization framework that jointly learns model weights, pruning ratios, and skip configurations under a distillation loss, UVC effectively compresses ViT variants like DeiT and T2T-ViT. This demonstrates that distillation is not merely a standalone data-efficiency technique but a powerful component in a multi-faceted approach to creating more practical and deployable ViTs. Similarly, \cite{li2022tl7} explored the challenging domain of fully quantized low-bit Vision Transformers (Q-ViT). They identified information distortion in low-bit quantized self-attention maps as a bottleneck and proposed a distribution-guided distillation (DGD) scheme to rectify this. Their Q-ViT models, leveraging DGD, achieved remarkable performance, even surpassing full-precision counterparts in some cases, while drastically reducing computational and memory costs. This illustrates how distillation can mitigate the performance degradation associated with extreme quantization, thereby making ViTs more viable for edge devices and further reducing their effective data-to-performance ratio by enabling smaller, more efficient models.

In summary, the initial "data hunger" of Vision Transformers was a significant impediment, but early efforts, particularly through knowledge distillation, provided crucial solutions. The advent of DeiT \cite{touvron2021training} demonstrated that ViTs could achieve strong performance on standard datasets by learning from pre-trained CNN teachers, thereby mitigating the need for massive proprietary datasets. Subsequent advancements have extended distillation beyond simple logit matching, exploring attention-based distillation for self-supervised learning \cite{wang2022pee} and integrating it into unified compression frameworks \cite{yu2022iy0} and quantization schemes \cite{li2022tl7}. These innovations collectively transformed ViTs from computationally demanding and data-intensive models into more accessible and practical architectures, laying the groundwork for their broader application across diverse computer vision tasks. While architectural advancements like the Swin Transformer (discussed in Section 4.1) contributed to computational efficiency, it is these dedicated data-efficient training strategies, primarily knowledge distillation, that directly addressed the challenge of learning powerful representations from substantially less labeled data.