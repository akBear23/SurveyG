\subsection{Rethinking Token Mixing: Alternatives to Self-Attention}
While self-attention mechanisms have been instrumental in the success of Vision Transformers (ViTs) by providing global receptive fields, their quadratic computational complexity with respect to sequence length and their initial lack of inherent inductive biases have stimulated extensive research into simpler, more computationally efficient alternatives for global token mixing. This line of inquiry, often unified under the "MetaFormer" architectural paradigm \cite{yu2022metaformer}, provocatively suggests that the overall design—comprising a token mixer followed by a feed-forward network (FFN)—can be highly effective even with substantially simpler mixing operations, challenging the notion that complex self-attention is indispensable for achieving state-of-the-art performance in vision.

Among the earliest and most influential works to question the necessity of self-attention were those proposing MLP-based mixers. The \textit{MLP-Mixer} \cite{tolstikhin2021mlp} demonstrated that a pure multi-layer perceptron (MLP) architecture, without any explicit attention mechanism, could achieve competitive performance on image classification tasks. This model segments an image into patches, then applies channel-mixing MLPs and token-mixing MLPs alternately. The token-mixing MLP operates across all patches, effectively providing a global receptive field through fully connected layers, albeit with a high computational cost if not carefully implemented. Following this, \textit{gMLP} \cite{liu2021pay} further refined the MLP-only approach by introducing a Spatial Gating Unit (SGU) that adaptively controls information flow across spatial locations. Both MLP-Mixer and gMLP were foundational in establishing that the general Transformer-like block structure, rather than the specific self-attention operation, was a key driver of performance, paving the way for further exploration of non-attention mixers.

Building on the surprising efficacy of simpler operations, subsequent research explored even more basic alternatives. The \textit{PoolFormer} architecture \cite{PoolFormer} famously demonstrated that simple pooling operations, such as average or max pooling, could effectively replace self-attention layers within a MetaFormer block while achieving competitive performance. This finding underscored that a parameter-free, local aggregation operation, when stacked within the MetaFormer framework, could surprisingly capture sufficient global context for robust image representation. In a different vein, Global Filter Networks (GFNet) \cite{GFNet} proposed replacing self-attention with global filters applied in the frequency domain, specifically using 2D Fourier transforms. This approach offers a computationally efficient way to achieve global receptive fields by treating tokens as a signal and performing global mixing via frequency-domain multiplication, thereby avoiding the quadratic complexity of self-attention while providing a distinct mechanism for global interaction.

Beyond parameter-free pooling or frequency-domain filters, other works have explored structured or localized alternatives that retain some global interaction without full self-attention. The Vision Permutator (ViP) \cite{ViP} introduced a permutable self-attention mechanism, which simplifies the attention process by performing attention along different axes (height and width) sequentially, rather than across all tokens simultaneously. While still using an attention-like mechanism, its structured permutation significantly reduces computational cost and provides a form of global information exchange with linear complexity. Focal Modulation Networks (FocalNet) \cite{FocalNet} proposed focal modulation, a novel spatial modulation mechanism that captures both fine-grained local context and long-range dependencies in a hierarchical manner. Unlike self-attention, focal modulation explicitly models interactions at different scales through a series of modulators, providing an efficient way to aggregate information across varying receptive fields without the explicit pairwise token comparison of attention.

The exploration of alternatives continues with emerging paradigms, such as state-space models (SSMs). Recent work like \textit{MambaVision} \cite{hatamizadeh2024xr6} proposes a hybrid Mamba-Transformer backbone, where the Mamba architecture, based on structured state-space sequences, is adapted for visual feature modeling. MambaVision demonstrates the feasibility of integrating SSMs as a distinct, non-attention-based token mixer, particularly in its initial layers, to efficiently capture long-range spatial dependencies. This represents a significant new direction, suggesting that sequence modeling mechanisms beyond attention can be effectively adapted for vision, potentially offering superior efficiency and inductive biases for certain tasks.

In conclusion, the research on rethinking token mixing fundamentally challenges the premise that self-attention is an indispensable component of high-performing vision models. Works like \cite{tolstikhin2021mlp, liu2021pay, PoolFormer, GFNet, ViP, FocalNet} have collectively demonstrated that simpler, more efficient operations—ranging from basic MLPs and pooling to frequency-domain filters, structured permutations, and focal modulation—can effectively replace self-attention within the robust MetaFormer paradigm. These models achieve competitive performance while significantly reducing computational overhead, pushing the boundaries of efficiency and architectural simplicity. The emergence of state-space models further diversifies this landscape, indicating a continuous quest for novel token mixing strategies. This shift suggests that the general block-based, token-processing architecture of Transformers is a robust design, capable of leveraging diverse mixing strategies. The ongoing challenge lies in balancing the theoretical expressiveness and adaptive capacity of full self-attention with the practical demands for computational efficiency, hardware friendliness, and inherent inductive biases offered by these increasingly varied alternative token mixing approaches.