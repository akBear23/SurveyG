\subsection{Self-Distillation and Contrastive Learning without Labels}

The inherent data hunger of Vision Transformers (ViTs) for pre-training with vast, labeled datasets has spurred significant research into self-supervised learning (SSL) paradigms. These approaches enable ViTs to acquire robust and semantically meaningful representations from unlabeled data, thereby mitigating the bottleneck of costly human annotation. Within SSL, two prominent and often intertwined methodologies—contrastive learning and self-distillation—have demonstrated exceptional efficacy, revealing profound emergent properties in ViT features that underscore their intrinsic capacity for visual understanding.

Contrastive learning, a foundational SSL paradigm, operates by maximizing the agreement between different augmented views of the same image (positive pairs) while simultaneously pushing apart representations of different images (negative pairs) in the embedding space. Early adaptations for ViTs, such as MoCo-v3 \cite{chen2021mocov3}, successfully integrated the momentum encoder concept with contrastive objectives, demonstrating that ViTs could learn powerful representations competitive with CNNs on ImageNet. Similarly, frameworks inspired by SimCLR \cite{chen2020simple} were adapted, emphasizing the importance of large batch sizes or memory banks for effective negative sampling. While effective, the reliance on explicit negative pairs and the associated computational overhead or architectural complexities (e.g., large memory banks or distributed training for large batches) presented practical challenges for scaling and maintaining training stability, motivating the search for alternative non-contrastive approaches.

In response to these complexities, non-contrastive self-supervised methods emerged, circumventing the explicit need for negative pairs altogether. A pioneering work in this category is BYOL (Bootstrap Your Own Latent) \cite{grill2020bootstrap}, which demonstrated that high-quality representations could be learned by simply predicting the representation of one augmented view from another. BYOL employs a momentum encoder for the teacher network and a crucial predictor head on the student branch. The combination of the predictor network and a stop-gradient operation applied to the teacher's output was instrumental in preventing representational collapse, ensuring the student learned meaningful, non-trivial features. Following BYOL, SimSiam \cite{chen2021exploring} further simplified non-contrastive learning by showing that even without a momentum encoder, a stop-gradient operation alone, when applied to one branch of a Siamese network, could effectively prevent collapse, making the training process more straightforward and efficient.

Building upon these non-contrastive principles, DINO (Self-Distillation with No Labels) \cite{DINO} emerged as a seminal work specifically tailored for ViTs. DINO employs a student-teacher architecture where a student network is trained to match the output distribution of a teacher network for different augmented views of the same image. Crucially, the teacher network's weights are an exponential moving average of the student's weights (a momentum encoder), providing a stable yet evolving target. To prevent representational collapse, a common challenge in non-contrastive methods, DINO incorporates techniques like centering and sharpening the output distributions, which effectively regularize the learning process. This self-distillation encourages the student to learn features invariant to various augmentations and to capture deep semantic information. A remarkable emergent property observed in ViTs trained with DINO is their ability to perform object segmentation without any explicit supervision; the attention maps of the self-attention layers spontaneously highlight object boundaries and coherent semantic regions. This capability illustrates that ViTs can intrinsically learn to parse visual scenes into meaningful entities through unsupervised means, significantly reducing the need for human annotation for tasks like segmentation.

Beyond foundational self-distillation, further refinements and applications have broadened its impact. EsViT \cite{EsViT} extended self-distillation by exploring different view generation strategies and leveraging attention-based distillation to enhance the learning process for ViTs. Furthermore, the concept of distilling knowledge from a self-supervised teacher to a smaller student has gained traction for efficiency and deployment on resource-constrained devices. AttnDistill \cite{wang2022pee} specifically addresses self-supervised knowledge distillation for ViTs by directly distilling information from the crucial attention mechanism of a teacher to a student. This method demonstrates that by guiding the student with the teacher's attention, the performance gap between models can be significantly narrowed, enabling the deployment of high-performing ViTs on memory and compute-constrained devices, even down to tiny ViT models. Similarly, distillation principles are leveraged in contexts like ViT quantization, where a teacher-student framework can rectify issues such as attention distortion in binarized ViTs, improving their performance on resource-limited devices \cite{li20238ti}. While this application of distillation focuses on model compression and enhancing the utility of already learned representations rather than initial self-supervised representation learning, it underscores the versatility of the teacher-student paradigm in improving ViT practicality.

A significant evolution in this space involves the convergence of self-distillation with masked image modeling (MIM), bridging concepts from this subsection with those discussed in Section 4.1. iBOT (Image BERT Pre-training with Online Tokenizer) \cite{zhou2022ibot} exemplifies this hybrid approach. iBOT combines the self-distillation framework of DINO with masked image modeling, where a student ViT is trained to predict the output of a teacher ViT for masked image patches. This dual objective allows iBOT to leverage the strengths of both paradigms: the global context learning from MIM and the robust feature learning from self-distillation, leading to highly effective visual representations that exhibit both fine-grained detail and strong semantic understanding. Such self-supervised techniques are also being adapted for domain-specific applications, such as medical image analysis, where models like MAT-VIT explore MAE-based self-supervised auxiliary tasks within a Vision Transformer framework to leverage abundant unlabeled medical images, improving classification performance in data-scarce scenarios \cite{han2024f96}.

In summary, self-distillation and contrastive learning paradigms have been instrumental in unlocking the full potential of Vision Transformers by enabling them to learn from vast amounts of unlabeled data. While contrastive methods like MoCo-v3 provided early successes, the subsequent development of non-contrastive self-distillation methods such as BYOL, SimSiam, and DINO simplified the training process by avoiding explicit negative pairs, often leading to more stable learning and revealing remarkable emergent properties like unsupervised object segmentation. The continuous refinement of distillation techniques for efficiency and the development of hybrid approaches like iBOT, which integrate self-distillation with masked image modeling, further enhance the representational capacity and practical applicability of ViTs. These advancements not only drastically reduce the reliance on extensive human annotation but also highlight the profound inherent representational capacity of Transformer architectures for visual data, paving the way for more data-efficient, generalizable, and universally applicable vision models.