\subsection{Multi-Scale and Efficient Attention Mechanisms}

The initial promise of Vision Transformers (ViTs) was significantly constrained by their quadratic computational complexity with respect to image resolution and their inherent limitations in capturing multi-scale features, which are indispensable for dense prediction tasks. While hierarchical architectures like the Swin Transformer \cite{Swin} and Pyramid Vision Transformer (PVT) \cite{PVT} (as discussed in Sections 3.1 and 3.2, respectively) laid foundational groundwork by introducing window-based and spatial reduction attention for linear complexity and multi-scale feature generation, subsequent research has delved deeper into refining the core attention mechanism itself to further enhance efficiency and multi-scale processing capabilities. This continuous effort is crucial for enabling ViTs to serve as versatile and deployable backbones across diverse vision applications.

A primary avenue of innovation has focused on optimizing spatial attention to balance global context and local detail more effectively, often by intelligently structuring local interactions. \textit{Multiscale Vision Transformers} (MViT) \cite{MViT} addressed the challenge of scaling to higher resolutions by progressively expanding channel dimensions while reducing spatial resolution within a hierarchical structure, enabling robust performance on high-resolution inputs, including video. This approach efficiently aggregates information across scales. Complementing this, the \textit{Twins} architecture \cite{Twins} refined spatial attention by explicitly combining both global and local attention mechanisms. This hybrid strategy aims to leverage the benefits of fine-grained local processing and broad contextual understanding, thereby creating a robust general-purpose backbone. Further expanding on localized attention, the \textit{CSWin Transformer} \cite{CSWin} introduced cross-shaped window attention, which captures richer contextual information more efficiently than square windows by attending to features along horizontal and vertical strips. Similarly, \textit{RegionViT} \cite{RegionViT} proposed a regional-to-local attention strategy, allowing efficient processing of large images by first attending to broader regional features and then refining with local attention. This hierarchical attention within a single block effectively balances computational cost with comprehensive feature extraction. \textit{Focal Attention} \cite{Focal} also contributes to this theme by efficiently capturing both fine-grained local details and broader contextual information, proving particularly beneficial for tasks like document understanding where varied scales of information are critical. These methods collectively demonstrate a trend towards more sophisticated, yet computationally constrained, local-global attention interactions, moving beyond simple windowing to more adaptive spatial sampling.

Beyond static windowing, a significant advancement in efficient attention involves making the attention mechanism data-dependent and dynamic. \textit{Deformable Attention Transformers} (DAT) \cite{xia2022qga} and its enhanced version DAT++ \cite{xia2023bp7} introduced a novel deformable multi-head attention module. Unlike fixed-grid or window-based attention, deformable attention adaptively allocates the positions of key and value pairs in a data-dependent manner. This flexible scheme allows the model to dynamically focus on relevant regions, overcoming the limitations of handcrafted attention patterns (like those in Swin or PVT) and maintaining the representational power of global attention while significantly reducing computational cost and memory usage. This approach represents a critical step towards more intelligent and adaptive attention mechanisms that can dynamically adjust their receptive fields based on image content, thereby enhancing both efficiency and feature discriminability.

Another crucial direction for efficiency has been the development of linear attention mechanisms and strategies for reducing spatial redundancy. Traditional self-attention's quadratic complexity stems from the softmax operation and the dense interaction matrix. \textit{UFO-ViT} \cite{song20215tk} proposed a novel self-attention mechanism with linear complexity by eliminating non-linearity and factorizing the matrix multiplication without complex linear approximations. This direct approach to linear scaling offers substantial computational benefits, especially for high-resolution inputs. Concurrently, methods focusing on reducing spatial redundancy by pruning or merging tokens have gained traction. The \textit{Localization and Focus Vision Transformer} (LF-ViT) \cite{hu202434n} strategically curtails computational demands by processing a reduced-resolution image in a "Localization" phase. If a definitive prediction is elusive, it triggers a Neighborhood Global Class Attention (NGCA) mechanism to identify and spotlight class-discriminative regions from the original image in a "Focus" phase. This selective processing significantly reduces FLOPs. Similarly, \textit{CP-ViT} \cite{song2022603} introduced a cascade pruning framework that dynamically predicts sparsity in ViT models, progressively pruning uninformative patches and heads. By defining a cumulative score and adjusting pruning ratios based on layer-aware attention range, CP-ViT achieves substantial FLOPs reduction with minimal accuracy loss. Furthermore, \textit{LTM-Transformer} \cite{wang2024ueo} proposes a novel block with Learnable Token Merging (LTM), which reduces FLOPs and inference time by merging tokens in a learnable scheme, compatible with various existing Transformer networks. These token-level optimization strategies demonstrate that efficiency can be gained not just by altering the attention computation itself, but also by intelligently reducing the amount of information that needs to be processed.

The broader landscape of efficient hierarchical designs also continues to evolve. \textit{Hiera} \cite{ryali202339q} exemplifies how architectural simplicity, when combined with strong self-supervised pretraining (like MAE), can yield highly efficient and accurate hierarchical Vision Transformers. By stripping away many "bells-and-whistles" commonly added for supervised performance, Hiera achieves faster inference and training while maintaining competitive accuracy, suggesting that computational overhead can be reduced through a holistic approach encompassing both architectural design and pre-training strategy.

In conclusion, the continuous pursuit of multi-scale and efficient attention mechanisms has been paramount for the practical deployment of Vision Transformers \cite{heidari2024d9k}. From sophisticated spatial attention refinements that balance local and global context \cite{MViT, Twins, CSWin, RegionViT, Focal}, to dynamic and data-dependent mechanisms like deformable attention \cite{xia2022qga, xia2023bp7}, and radical approaches that achieve linear complexity or reduce spatial redundancy through token pruning/merging \cite{song20215tk, hu202434n, song2022603, wang2024ueo}, the field strives to balance the expressive power of global attention with computational feasibility. Unresolved issues include finding the optimal trade-off between incorporating inductive biases (like locality) and maintaining the flexibility of pure attention, as well as exploring novel attention-free or highly simplified architectural components that can further reduce computational overhead without sacrificing performance.