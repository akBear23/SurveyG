\contentsline {section}{\numberline {1}Introduction to Visual Transformers}{3}{section.1}%
\contentsline {subsection}{\numberline {1.1}The Rise of Transformers in Deep Learning}{3}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Motivation for Vision Transformers: Beyond CNNs}{5}{subsection.1.2}%
\contentsline {subsection}{\numberline {1.3}Scope and Structure of the Review}{7}{subsection.1.3}%
\contentsline {section}{\numberline {2}Foundational Vision Transformer Architectures and Early Optimizations}{9}{section.2}%
\contentsline {subsection}{\numberline {2.1}The Original Vision Transformer (ViT) Paradigm}{9}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Data-Efficient Training and Distillation}{11}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Enhancing Stability and Tokenization for Deeper Models}{14}{subsection.2.3}%
\contentsline {section}{\numberline {3}Hierarchical and Efficient Vision Transformer Architectures}{16}{section.3}%
\contentsline {subsection}{\numberline {3.1}Shifted Window-Based Attention for Hierarchical Processing}{16}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Pyramid Structures for Multi-Scale Feature Representation}{19}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Multi-Scale and Efficient Attention Mechanisms}{22}{subsection.3.3}%
\contentsline {section}{\numberline {4}Self-Supervised Learning Paradigms for Vision Transformers}{25}{section.4}%
\contentsline {subsection}{\numberline {4.1}Masked Image Modeling (MIM) for Representation Learning}{25}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Self-Distillation and Contrastive Learning without Labels}{27}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Scaling Self-Supervised ViTs to Foundation Models}{30}{subsection.4.3}%
\contentsline {section}{\numberline {5}Hybrid Architectures and Beyond Self-Attention}{34}{section.5}%
\contentsline {subsection}{\numberline {5.1}Integrating Convolutional Inductive Biases into Transformers}{34}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Rethinking Token Mixing: Alternatives to Self-Attention}{37}{subsection.5.2}%
\contentsline {subsection}{\numberline {5.3}Modernizing CNNs with Vision Transformer Design Principles}{39}{subsection.5.3}%
\contentsline {subsection}{\numberline {5.4}Efficient and Lightweight Hybrid Designs for Deployment}{42}{subsection.5.4}%
\contentsline {section}{\numberline {6}Applications and Domain-Specific Adaptations of Visual Transformers}{42}{section.6}%
\contentsline {subsection}{\numberline {6.1}Object Detection and Semantic Segmentation}{42}{subsection.6.1}%
\contentsline {subsection}{\numberline {6.2}Medical Image Analysis and 3D Segmentation}{45}{subsection.6.2}%
\contentsline {subsection}{\numberline {6.3}Remote Sensing and Environmental Monitoring}{47}{subsection.6.3}%
\contentsline {subsection}{\numberline {6.4}Lightweight and Real-time Applications}{49}{subsection.6.4}%
\contentsline {section}{\numberline {7}Future Directions and Open Challenges}{52}{section.7}%
\contentsline {subsection}{\numberline {7.1}Towards More Efficient and Scalable Architectures}{52}{subsection.7.1}%
\contentsline {subsection}{\numberline {7.2}Beyond Vision: Multimodal and Foundation Models}{55}{subsection.7.2}%
\contentsline {subsection}{\numberline {7.3}Novel Architectures and Beyond Attention Mechanisms}{58}{subsection.7.3}%
\contentsline {subsection}{\numberline {7.4}Ethical Considerations and Societal Impact}{60}{subsection.7.4}%
\contentsline {section}{\numberline {8}Conclusion}{63}{section.8}%
\contentsline {subsection}{\numberline {8.1}Summary of Key Developments}{63}{subsection.8.1}%
\contentsline {subsection}{\numberline {8.2}Unresolved Tensions and Future Outlook}{66}{subsection.8.2}%
\contentsline {section}{References}{71}{section*.2}%
