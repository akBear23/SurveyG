\subsection*{The Rise of Transformers in Deep Learning}

The landscape of deep learning for sequence modeling underwent a profound transformation with the advent of the Transformer architecture, which effectively addressed critical limitations inherent in prior recurrent and convolutional neural networks. Before this paradigm shift, models like Recurrent Neural Networks (RNNs) and their more sophisticated variants, Long Short-Term Memory (LSTM) networks \cite{Hochreiter1997LongSM}, faced significant challenges in capturing long-range dependencies. These challenges stemmed from issues such as vanishing or exploding gradients and their inherently sequential processing nature, which fundamentally hindered parallelization and scalability for longer sequences. Convolutional Neural Networks (CNNs), while powerful for local feature extraction and exhibiting strong inductive biases like locality and translation equivariance, were not primarily designed for global context modeling across extended sequences. They often required complex architectural designs, such as deeper stacks or dilated convolutions, to achieve broader receptive fields, which could still be limited compared to the entire sequence.

The seminal paper "Attention Is All You Need" by Vaswani et al. (2017) introduced the Transformer, a novel architecture that entirely eschewed recurrence and convolutions, relying instead on a multi-head self-attention mechanism to draw global dependencies between input and output elements \cite{Vaswani2017}. This groundbreaking work fundamentally altered the approach to sequence-to-sequence tasks, particularly in Natural Language Processing (NLP). The core innovation of the Transformer lies in its ability to dynamically weigh the importance of different parts of the input sequence when processing each element, irrespective of their positional distance \cite{Vaswani2017, heidari2024d9k}. Unlike RNNs, which process tokens one by one, the self-attention mechanism enables parallel processing of all tokens in a sequence, significantly accelerating training times and improving scalability for longer sequences \cite{han2020yk0}. Beyond self-attention, the Transformer architecture also incorporates crucial components such as positional encodings to inject information about the relative or absolute position of tokens in the sequence, and feed-forward networks for non-linear transformations, all contributing to its robust representational capacity.

The self-attention mechanism computes a weighted sum of all input elements, where the weights are dynamically calculated based on the similarity between a query (representing the current element being processed) and all keys (representing other elements in the sequence). More precisely, the attention weights are derived by computing the scaled dot-product between the query vector ($Q$) and key vectors ($K$) for all elements, followed by a softmax function to normalize these scores, and then multiplying by value vectors ($V$) to obtain the output: $\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$, where $d_k$ is the dimension of the keys \cite{Vaswani2017, heidari2024d9k}. This direct, unconstrained interaction between any two positions in the input sequence ensures that long-range dependencies are effectively captured, a significant improvement over the limited receptive fields of CNNs or the vanishing gradient issues prevalent in RNNs. By allowing each element to attend to all other elements, the Transformer can build a rich contextual representation for every token, reflecting its global relationships within the sequence. Furthermore, the multi-head aspect of self-attention allows the model to jointly attend to information from different representation subspaces at different positions, enriching the model's ability to focus on various aspects of the input simultaneously and enhancing its overall expressiveness \cite{han2020yk0}.

This breakthrough capability of the Transformer to efficiently model global dependencies and process sequences in parallel profoundly impacted deep learning, establishing a new state-of-the-art across numerous NLP tasks, including machine translation, text summarization, and question answering \cite{Vaswani2017}. The subsequent development of large-scale pre-trained models like BERT \cite{Devlin2019BERTPO} further demonstrated the Transformer's power, showcasing its ability to learn highly generalizable language representations. The conceptual elegance and computational efficiency of self-attention laid the critical groundwork for its eventual adaptation and widespread adoption beyond NLP. Critically, the success in NLP prompted researchers to challenge the long-held axiom that CNNs' inherent inductive biases, such as locality and translation equivariance, were indispensable for computer vision tasks \cite{zhou2021rtn}. The Transformer, by design, lacks these built-in biases, and overcoming this perceived limitation became a key conceptual hurdle. This motivated the direct application of a sequence-processing architecture to image data, where images could be re-conceptualized as sequences of patches, allowing the powerful self-attention mechanism to capture global visual relationships \cite{han2020yk0}. This paradigm shift directly challenged the long-standing dominance of recurrent and convolutional networks across various deep learning applications, particularly paving the way for Vision Transformers (ViTs).

The Transformer architecture, with its innovative self-attention mechanism and parallel processing capabilities, thus marked a pivotal moment in deep learning. Its capacity for robust capture of long-range dependencies not only revolutionized NLP but also established a foundational conceptual framework that would eventually inspire a new generation of models in computer vision and other domains, fundamentally altering how deep learning models perceive and process complex data. This conceptual leap set the stage for the subsequent exploration of Vision Transformers, which sought to leverage these advantages for visual understanding, despite the architectural differences from traditional CNNs.