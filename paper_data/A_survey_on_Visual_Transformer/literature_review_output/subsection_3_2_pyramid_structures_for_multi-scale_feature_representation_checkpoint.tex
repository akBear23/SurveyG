\subsection*{Pyramid Structures for Multi-Scale Feature Representation}

The initial success of Vision Transformers (ViTs) \cite{Dosovitskiy2020} primarily in image classification highlighted their powerful global reasoning capabilities. However, their inherent design, which typically produces fixed-resolution feature maps and suffers from quadratic computational complexity due to global self-attention, presented significant challenges for dense prediction tasks like object detection and segmentation. These tasks critically demand fine-grained localization and multi-scale contextual understanding, capabilities traditionally dominated by Convolutional Neural Networks (CNNs) through their hierarchical feature extraction. To bridge this gap, a pivotal direction in ViT research has focused on developing pyramid-like architectures that can generate multi-scale feature representations efficiently and without explicit convolutional layers, thereby expanding ViT's applicability beyond simple image classification.

A seminal contribution in this domain is the Pyramid Vision Transformer (PVT) \cite{Wang2021}. PVT introduced a novel pyramid structure to progressively reduce the resolution of feature maps, moving from high-resolution, fine-grained features at early stages to low-resolution, semantically rich features at deeper layers. This hierarchical design is achieved through a spatial reduction attention mechanism, which efficiently downsamples the key and value matrices in the self-attention computation. This approach allows PVT to capture both low-level details and high-level semantic information, making it a versatile backbone for dense prediction tasks and a direct competitor to CNN-based feature pyramid networks (FPNs). Crucially, PVT maintains the global context modeling strengths of Transformers while significantly reducing computational costs compared to the original ViT's global attention.

Concurrently, the Swin Transformer \cite{Liu2021} emerged as another highly influential hierarchical Vision Transformer, also designed to overcome the computational and multi-scale limitations of earlier ViTs. While its core mechanism of shifted window-based attention is detailed in Subsection 3.1, it is important to note its parallel contribution to generating multi-scale feature representations. By restricting self-attention to non-overlapping local windows and introducing a shifted window mechanism for cross-window connections, Swin Transformer achieves linear computational complexity and a hierarchical feature pyramid. This design has proven exceptionally effective across a wide array of dense prediction tasks, including object detection \cite{xiong2022ec2, xiang2024tww} and semantic segmentation, and even low-level vision tasks like image restoration \cite{Liang2021v6x} and monocular depth estimation \cite{li2024qva}, underscoring the versatility of such hierarchical designs.

Beyond PVT and Swin, other architectures have explored diverse strategies for generating multi-scale features. The Hierarchical Vision Transformer (HiViT) \cite{Zhang2022msa, ryali202339q} further refines hierarchical ViT designs, demonstrating advantageous performance, particularly in self-supervised pre-training methods like masked image modeling. HiViT emphasizes architectural simplicity, arguing that many "bells-and-whistles" added to hierarchical ViTs are unnecessary when combined with strong pre-training, leading to faster and more accurate models. In contrast to fixed pyramid structures, the Deformable Attention Transformer (DAT++) \cite{xia2023bp7} introduces a novel deformable multi-head attention module. This mechanism adaptively allocates key and value positions in a data-dependent way, allowing the model to dynamically focus on relevant regions and overcome the data-agnostic nature of handcrafted attention patterns in some pyramid designs. This offers a more flexible approach to capturing multi-scale and long-range relationships.

Interestingly, some research has challenged the necessity of *inherently hierarchical* ViT backbones for dense prediction. ViTDet \cite{li2022raj} demonstrates that a simple feature pyramid can be effectively built from a *plain*, single-scale Vision Transformer feature map, especially when the ViT is pre-trained with Masked Autoencoders (MAE). This suggests that the rich representations learned by plain ViTs can be adapted for multi-scale tasks with minimal architectural modifications, such as using window attention with few cross-window propagation blocks. This approach simplifies the backbone design while achieving competitive results in object detection.

Furthermore, hybrid approaches have emerged, combining the strengths of ViTs with traditional CNN-based FPNs to enhance multi-scale feature learning. For instance, the Feature Pyramid Vision Transformer (FPViT) \cite{liu2022c56} integrates Transformers with a ResNet backbone and a feature pyramid, allowing the Transformers to capture global contexts from CNN-extracted features while leveraging multi-scale maps for better adaptability in medical image classification. Similarly, Transformer-Based YOLOX \cite{panboonyuen2021b4h} employs a pre-trained ViT as a backbone and integrates an FPN decoder to effectively aggregate multi-level features for object detection, demonstrating superior performance on challenging datasets. Other specialized models like the Visual Saliency Transformer (VST) \cite{Liu2021jpu} leverage multi-level token fusion and upsampling within a pure transformer framework to generate high-resolution saliency maps, while SENet \cite{Hao202488z} incorporates a local information capture module to compensate for the patch-level attention mechanisms in pixel-level tasks like camouflaged object detection and salient object detection, further reinforcing the critical need for fine-grained multi-scale features.

In conclusion, the development of pyramid Vision Transformer architectures, spearheaded by models like PVT and Swin Transformer, represents a significant evolution in computer vision. By ingeniously designing hierarchical structures and employing efficient attention mechanisms, these models have successfully addressed the critical need for multi-scale feature representation and improved computational efficiency. This has expanded ViT's applicability far beyond simple image classification, enabling them to serve as versatile and powerful backbones for a broad spectrum of dense prediction and other complex vision tasks, directly challenging the long-standing dominance of CNNs. The ongoing research continues to explore novel attention mechanisms, architectural simplifications, and hybrid integrations to further enhance their efficiency and adaptability across diverse applications.