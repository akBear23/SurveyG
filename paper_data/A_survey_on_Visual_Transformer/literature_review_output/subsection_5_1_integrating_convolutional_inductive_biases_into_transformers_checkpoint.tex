\subsection{Integrating Convolutional Inductive Biases into Transformers}

While Vision Transformers (ViTs) have demonstrated impressive capabilities in capturing global dependencies, their initial lack of inherent inductive biases, such as translation equivariance and locality, often necessitates vast training data and can lead to sub-optimal performance on smaller datasets or for fine-grained local tasks \cite{dosovitskiy2020image, touvron2021training}. To address these limitations, a significant research direction has focused on hybrid architectures that explicitly embed convolutional layers or integrate convolutional inductive biases within the Transformer framework, aiming to synergistically combine the local feature extraction strengths of Convolutional Neural Networks (CNNs) with the global reasoning capabilities of Transformers. This synergistic integration often leads to improved efficiency, better performance, and enhanced robustness, particularly on smaller datasets where pure ViTs might struggle due to their lack of inherent inductive priors. These hybrid designs represent a pragmatic effort to leverage the best attributes of both paradigms, moving beyond a 'CNN vs. Transformer' dichotomy towards more powerful combined architectures.

Early efforts to imbue Transformers with locality often explored architectural modifications without direct convolutions. For instance, the Swin Transformer \cite{liu2021ljs} introduced a hierarchical architecture that limits self-attention computation to non-overlapping local windows, with shifted windowing enabling cross-window connections. While this design provides a form of local inductive bias and improves efficiency for dense prediction tasks, it achieves locality through windowing rather than explicit convolutional operations. In contrast, other foundational hybrid models directly integrated convolutions. CoaT (Co-scale Conv-Attentional Image Transformers) \cite{xu2021coat} integrates convolution and attention at co-scales, allowing for a dynamic interplay between local and global features. CvT (Introducing Convolutions to Vision Transformers) \cite{wu2021cvt} explicitly embeds convolutions into the Transformer architecture by replacing the linear patch embedding with a strided convolution and using depthwise-separable convolutions for the key, query, and value projection layers within the self-attention mechanism. This approach leverages convolutions for efficient tokenization and local feature aggregation directly within the attention process. ConViT (Improving Vision Transformers with Soft Convolutional Inductive Biases) \cite{d2021convit} introduces "soft convolutional biases" by initializing the attention mechanism to prioritize local neighborhoods, gradually expanding its receptive field in deeper layers, thereby mimicking the inductive bias of convolutions without hard-coding them. LeViT (a Vision Transformer in ConvNet's Clothing for Faster Inference) \cite{graham2021levit} further blurs the lines by optimizing ViTs for speed through the incorporation of attention bias and convolution-like structures, demonstrating that careful architectural design can yield benefits traditionally associated with CNNs.

The integration strategies for convolutional inductive biases can be broadly categorized into several architectural patterns. One common approach involves embedding convolutional layers directly within Transformer blocks or using them for initial feature extraction. For example, some models utilize convolutions for the initial patch embedding, similar to CvT, or integrate them into feed-forward networks (FFNs) to enhance local feature mixing. Another prominent pattern involves parallel CNN and ViT streams, where each branch specializes in different aspects of feature extraction before their outputs are fused. This allows the CNN branch to capture fine-grained local details and translation equivariance, while the Transformer branch focuses on global contextual relationships. For instance, CTNet \cite{deng2021man} proposes a joint framework with separate CNN and ViT streams to extract local structural and global semantic features, respectively, for remote sensing scene classification, fusing them for comprehensive understanding. Similarly, GLNS \cite{liu2022249} for high-resolution SAR image classification utilizes a lightweight CNN and a compact ViT in parallel, fusing their outputs to leverage complementary local and global features.

Beyond full architectural integration, convolutional inductive biases can also be introduced through lightweight adaptation modules or specialized components. Convpass \cite{jie20220pc} proposes "Convolutional Bypasses" as plug-and-play adaptation modules for pre-trained ViTs. These bypasses inject convolutional layers, benefiting from their hard-coded inductive bias, particularly in low-data regimes, without altering the original ViT parameters. This method offers a parameter-efficient way to enhance ViTs with local priors. For pixel-level tasks like camouflaged and salient object detection, SENet \cite{hao202488z} incorporates a "local information capture module" within its ViT-based encoder-decoder structure. This module is specifically designed to compensate for the limitations of patch-level attention in capturing fine-grained local details, which are crucial for precise pixel-level predictions.

These hybrid strategies have proven particularly effective in specialized domains where data scarcity or the need for robust local features is paramount. In remote sensing, for example, the fusion of local and global information is critical for tasks like land use and land cover classification or hyperspectral image analysis. Several works adopt parallel or integrated convolutional modules to enhance local feature extraction and reduce reliance on massive pre-training. P2FEViT \cite{wang202338i} introduces a plug-and-play CNN feature embedding into ViT, allowing synchronous capture and fusion of global context with local multimodal information. ExViT \cite{yao2023sax} extends conventional ViTs for multimodal land use and land cover classification by processing image patches with parallel branches of position-shared ViTs augmented with separable convolution modules, fusing their embeddings via cross-modality attention. GSC-ViT \cite{zhao2024671} for hyperspectral image classification employs a groupwise separable convolution (GSC) module to efficiently capture local spectral-spatial information and a groupwise separable multihead self-attention (GSSA) module for both local and global spatial feature extraction, significantly reducing parameters. Furthermore, the lightweight dual-branch Swin Transformer (LDBST) \cite{zheng202325h} combines a ViT branch with a CNN branch, integrating a Conv-MLP structure into the ViT branch to enhance connections with neighboring windows, showcasing the versatility of these hybrid designs.

In conclusion, the integration of convolutional inductive biases into Transformers represents a pragmatic and effective strategy to overcome the limitations of pure ViTs, particularly concerning data efficiency, local feature extraction, and robustness on diverse datasets. These hybrid designs, ranging from embedding convolutional layers directly within Transformer blocks and attention mechanisms to employing parallel CNN-ViT streams and lightweight convolutional adaptation modules, successfully combine the local feature extraction strengths of CNNs with the global reasoning capabilities of Transformers. This synergistic approach has consistently led to improved performance and enhanced robustness across various vision tasks, especially in scenarios with limited data or requiring fine-grained local understanding. Future research will likely continue to explore more sophisticated and dynamic integration strategies, a deeper theoretical understanding of their combined inductive biases, and the optimal balancing of computational cost with performance gains across diverse applications, moving beyond a simplistic 'CNN vs. Transformer' dichotomy towards more powerful and versatile combined architectures.