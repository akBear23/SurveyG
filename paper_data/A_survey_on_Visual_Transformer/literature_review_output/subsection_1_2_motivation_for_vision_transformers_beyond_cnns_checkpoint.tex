\subsection{Motivation for Vision Transformers: Beyond CNNs}

Convolutional Neural Networks (CNNs) have historically dominated computer vision, largely owing to their strong inductive biases of locality and translation equivariance. These biases, inherent in convolutional operations, enable efficient extraction of local features and parameter sharing, leading to remarkable success in tasks like image classification and object recognition over the past decade \cite{han2020yk0}. However, the very nature of these local operations, while beneficial for capturing fine-grained patterns, inherently restricts CNNs' ability to effectively model global relationships and long-range dependencies across an entire image. This limitation is particularly pronounced when understanding complex scenes or objects where interactions between spatially distant parts are crucial \cite{liu2022c56, gheflati202131i}.

To compensate for this restricted receptive field, CNN architectures often resort to complex designs such as stacking many convolutional layers, employing large kernel sizes, using dilated convolutions, or incorporating pyramid structures to aggregate broader context and simulate a global view \cite{liu2022c56}. While these strategies have pushed the boundaries of CNN performance, they often introduce significant computational overhead, increase model depth, or still rely on an indirect, hierarchical aggregation of information rather than direct global interaction. For instance, in medical imaging, the "inadequacy of long-range relation modeling" in CNNs poses challenges for comprehensive understanding of complex anatomical structures \cite{liu2022c56}, and their "restricted local receptive field limits their ability to learn global context information" in breast ultrasound image classification \cite{gheflati202131i}. The need for a more direct and flexible mechanism to capture these global interactions served as a primary motivation for exploring alternative architectures.

The advent of Vision Transformers (ViTs) offered a fundamentally different paradigm, directly addressing the CNNs' limitations in global context modeling. Inspired by the success of Transformers in Natural Language Processing (NLP) for handling long-range dependencies in sequential data, ViTs re-conceptualized image processing. By treating images as sequences of flattened patches, which are then linearly embedded and processed by a standard Transformer encoder, ViTs leverage the multi-head self-attention mechanism to establish direct relationships between any two patches, irrespective of their spatial distance \cite{han2020yk0}. This design inherently provides a global receptive field from the very first layer, allowing the model to weigh the importance of all input patches when processing each individual patch, thereby enabling a more holistic understanding of the visual scene. This radical departure from convolutional locality represented a significant architectural shift, promising to overcome the architectural constraints that plagued CNNs in capturing broad context.

However, this paradigm shift was not without its own challenges, which in turn motivated subsequent waves of research within the ViT domain. By largely abandoning the strong inductive biases of locality and translation equivariance inherent in CNNs, the original ViTs exhibited a significant dependence on vast amounts of pre-training data to achieve competitive performance \cite{lee2021us0, hong2022ks6, wang202338i}. This "data hunger" stemmed from their more general-purpose architecture, which required extensive exposure to diverse visual patterns to learn robust representations without the built-in priors of convolutions. Furthermore, the initial ViT models, with their uniform patch processing, lacked the inherent hierarchical feature extraction capabilities that CNNs naturally provided, which are crucial for dense prediction tasks like object detection and semantic segmentation \cite{liu2021yw0}. The quadratic computational complexity of global self-attention with respect to image resolution also presented a practical hurdle for high-resolution inputs. These new challenges, arising directly from ViTs' novel approach to global context modeling, became the driving force for subsequent architectural innovations aimed at improving data efficiency, computational scalability, and the ability to generate multi-scale features, thereby making ViTs more robust and versatile for a broader range of computer vision applications.