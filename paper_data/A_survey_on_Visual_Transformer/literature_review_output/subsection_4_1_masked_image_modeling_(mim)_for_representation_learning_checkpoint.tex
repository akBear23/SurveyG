\subsection{Masked Image Modeling (MIM) for Representation Learning}

Masked Image Modeling (MIM) has emerged as a highly effective self-supervised learning paradigm for Vision Transformers (ViTs), drawing direct inspiration from the success of BERT in Natural Language Processing (NLP). This approach trains a ViT to learn rich visual representations by reconstructing masked portions of an image, effectively predicting missing pixels or discrete visual tokens. By forcing the model to infer global context from partial observations, MIM significantly reduces the reliance on extensive human annotations, enabling ViTs to leverage vast amounts of unlabeled data for pre-training. A key distinction often drawn is that MIM's generative reconstruction task encourages learning rich, localized features, whereas contrastive methods (discussed in Section 4.2) tend to yield more globally semantic, linearly separable features.

One of the pioneering works in this domain is \cite{beit}, which introduced BEiT, a BERT-like pre-training framework for image Transformers. BEiT tokenizes images into discrete visual tokens using a discrete Variational AutoEncoder (dVAE) and then trains a standard ViT encoder to predict these tokens for randomly masked image patches. This method demonstrated that a masked language modeling objective could be effectively transferred to the visual domain, yielding strong representations for various downstream tasks. The discrete token prediction objective forces the model to operate at a higher level of semantic abstraction, potentially leading to more semantically meaningful feature learning by abstracting away low-level pixel noise.

Building upon this foundation, \cite{mae} presented Masked Autoencoders (MAE), a highly scalable and efficient MIM approach. MAE trains a Vision Transformer encoder to reconstruct the original raw pixel values of masked image patches. A key innovation of MAE is its asymmetric encoder-decoder architecture: only the visible patches are fed into the encoder, while a lightweight decoder reconstructs the missing pixels from the encoder's output and the masked token embeddings. This design, coupled with a remarkably high masking ratio (e.g., 75\%), forces the model to learn a deep understanding of image semantics and global context, leading to impressive scalability and performance. The simplicity of reconstructing raw pixels, rather than discrete tokens, further streamlines the pre-training pipeline by removing the need for a dVAE tokenizer, a notable trade-off compared to BEiT.

Beyond these foundational works, the MIM landscape has diversified. SimMIM \cite{simmim} demonstrated that a simple linear head for pixel reconstruction could be highly effective, simplifying the decoder design even further. MaskFeat \cite{maskfeat} explored predicting hand-crafted features like Histograms of Oriented Gradients (HOG) instead of raw pixels, suggesting that the reconstruction target itself can be varied to influence learned representations.

A significant challenge for MIM, particularly MAE, has been its application to hierarchical Vision Transformers (e.g., Swin Transformer, PVT) which incorporate local window-based attention. The original MAE design, relying on the global attention of plain ViTs to handle randomly masked sequences, struggled with the local inductive biases of hierarchical architectures. To address this, HiViT \cite{zhang2022msa} proposed a new design for hierarchical ViTs that maintains efficiency and performance in MIM by modifying Swin Transformer to make mask-units serializable. Similarly, Uniform Masking (UM-MAE) \cite{li2022ow4} enabled MAE pre-training for pyramid-based ViTs with locality. UM-MAE introduces a Uniform Sampling strategy that selects one random patch from each 2x2 grid, combined with Secondary Masking, to preserve equivalent elements across local windows, significantly improving pre-training efficiency and transferability for these architectures.

The effectiveness of MIM, particularly MAE, lies in its ability to learn robust and transferable representations. For instance, pre-trained MAE backbones have been shown to be highly effective for downstream tasks like object detection. \cite{li2022raj} demonstrated that plain, non-hierarchical ViT backbones pre-trained with MAE could achieve competitive results in object detection (ViTDet) with minimal architectural adaptations, even outperforming methods based on hierarchical backbones. Similarly, \cite{ryali202339q} leveraged MAE pre-training to simplify hierarchical ViT architectures, showing that a strong visual pretext task can allow for the removal of many "bells-and-whistles" without sacrificing accuracy, resulting in faster and more efficient models like Hiera. This highlights MIM's capacity to imbue even simpler ViT designs with powerful representational capabilities. Furthermore, MIM can serve as a powerful auxiliary task; MAT-VIT \cite{han2024f96} uses an MAE-based self-supervised auxiliary task to improve medical image classification, effectively leveraging both unlabeled and labeled data. In the context of deep reinforcement learning from pixels, reconstruction-based ViT methods, including MIM, have been found to significantly outperform ViT contrastive-learning approaches, suggesting their utility in learning rich visual states for control tasks \cite{tao2022gdr}.

While MIM has proven highly effective in self-supervised pre-training, the computational cost associated with training these large models, even with efficient designs like MAE's asymmetric architecture, remains a significant consideration. The optimal masking strategy, including masking ratio and pattern, continues to be an active area of research to maximize learning efficiency and representation quality. For example, BEiT v2 explores more sophisticated block-wise masking schemes to encourage learning richer semantics. Future work is investigating more adaptive masking techniques or hybrid approaches that combine MIM's generative loss with a discriminative loss on the visible tokens to capture both local detail and high-level semantics. The trade-off between pixel-level and discrete token-level reconstruction also remains a topic of investigation, as each approach offers distinct advantages in terms of simplicity, efficiency, and the semantic richness of the learned features.