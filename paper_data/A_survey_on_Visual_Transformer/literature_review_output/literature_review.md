# A Comprehensive Literature Review with Self-Reflection

**Generated on:** 2025-10-07T17:27:53.299620
**Papers analyzed:** 367

## Papers Included:
1. c8b25fab5608c3e033d34b4483ec47e68ba109b7.pdf [liu2021ljs]
2. 7a9a708ca61c14886aa0dcd6d13dac7879713f5f.pdf [liang2021v6x]
3. d40c77c010c8dbef6142903a02f2a73a85012d5d.pdf [han2020yk0]
4. 0eff37167876356da2163b2e396df2719adf7de9.pdf [chen2021r2y]
5. da74a10824193be9d3889ce0d6ed4c6f8ee48b9e.pdf [mehta20216ad]
6. a09cbcaac305884f043810afc4fa4053099b5970.pdf [li2022raj]
7. 2fe2f849b94cf08b559226bc9d78adcaef5ef186.pdf [chen2022woa]
8. e5cb26148791b57bfd36aa26ce2401e231d01b57.pdf [xia2022qga]
9. 96da196d6f8c947db03d13759f030642f8234abf.pdf [zhou202105h]
10. 751b71158b7dcd2a7949e72a6ad8fb13657a401c.pdf [liu2021jpu]
11. 164e41a60120917d13fb69e183ee3c996b6c9414.pdf [lee2021us0]
12. 5553f9508dd1056ecc20c5b1f367e9a07e2c7e81.pdf [zhang2021fje]
13. 226fcbe55235d873bedb2fcf5b981bd5ec860d4f.pdf [jiang2022zcn]
14. 5c1b7e400ba8b3c3b04522f7bb3af17ad68d0585.pdf [islam2022iss]
15. a4b728dbbf5afdc231afb95ad4e5c2ececdefc48.pdf [li2022a4u]
16. 0b8e8760d7dd64c7439019aeb3b6ac55dd5075d4.pdf [yao202245i]
17. 17534840dc6016229a577a66f108a1564b8a0131.pdf [borhani2022w8x]
18. b8cee43a51c44f8f4448e78e41ecf081987707cf.pdf [mao2021zr1]
19. 44ee4165b8a3811dc7d557f99150ff9e62f3733f.pdf [chen202174h]
20. 50a260631a28bfed18eccf8ebfc75ff34917518f.pdf [jie20220pc]
21. 3c14992a490cc31a7a38f5fab156c9da40a861d4.pdf [fan2022m88]
22. ab70c5e1a338cb470ec39c22a4f10e0f19e61edd.pdf [lin20216a3]
23. 1ee05cd919590eaba129caa0fda5e850c87b75a5.pdf [lin2021utw]
24. 9fb327c55a30b9771a364f45f33f77778756a164.pdf [li2022mco]
25. dfdb2894d50e095ce97f994ed6cee38554c4c84f.pdf [li2022tl7]
26. f27040f1f81144b17ec4c2b30610960e96353002.pdf [yang2021myb]
27. 4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a.pdf [yu2022iy0]
28. 49030ae220c863e9b72ab380ecc749c9d0f0ad13.pdf [zhuang2022qn7]
29. 60b0f9af990349546f284dea666fbf52ebfa7004.pdf [deng2021man]
30. 64d8af9153d68e9b50f616d227663385bece93b9.pdf [wang2021oct]
31. 03384825d373aabe67c4288ef1eae4d1cf89dc00.pdf [wang2022ti0]
32. d28fed119d9293af31776205150b3c34f3adc82b.pdf [li2022ow4]
33. b52844a746dafd8a5051cef49abbbda64a312605.pdf [wang2022da0]
34. 35fccd11326e799ebf724f4150acef12a6538953.pdf [deng2022bil]
35. 0d1e4faa1580266a56ad62ac2fdd72ed0c0bbbe9.pdf [yang20228mm]
36. 8f7ae0526f9c8706a9e0967eb7942d9b3ffb2788.pdf [gheflati202131i]
37. 00f6041bdbdd00299ef317bd86db3660eb039a8c.pdf [tang2022e2i]
38. 5f7fee3db8879d9d4e714ae20627f00fcbdfab38.pdf [yu202236t]
39. 070c917ab1a4d6b924a9613ca18443f260d8d5be.pdf [li2021ra5]
40. 011f59c91bbee6de780d35ebe50fff62087e5b13.pdf [meng2022t3x]
41. f27c847e2909f30745f4a3528b574f5acfd76ea7.pdf [li20223n5]
42. d717fa5a4355b5033e660ad58a7ae7f4ccce6939.pdf [bazi2022tlu]
43. a119cc83788701313d94746baecd2df5dd30199d.pdf [zheng2022gg5]
44. 60f9fd4e6b29e6f11324ba4f6d3349a2d8107a1b.pdf [gao2021uzl]
45. 5ca02297d8d49f03f26148b74fea77272d09c78b.pdf [zheng202218g]
46. aed7e4bc195d838735c320ac40a78f123206831b.pdf [bi20225lu]
47. b66e4257aa8856df537f03f6a12341f489eb6500.pdf [chen2022vac]
48. f9480350e1986957919d49f346ba20dcab8f5b71.pdf [song2022603]
49. 836dd64a4f606931029c5d68e74d81ef5885b622.pdf [li2022rl9]
50. 16fa1a8575ff56781b6b83726906754ed4e5f3a7.pdf [wensel2022lva]
51. 9f7f81b1c82828a45a52df8f0c6a92636af76c7e.pdf [wang2021sav]
52. e678898301a66faab85dfa4c84e51118e434b8f2.pdf [naseem2022c95]
53. e33434a141bb2881a2e60c518a0cda4feed3f19a.pdf [wu20210gs]
54. 9da3fadf092c864f61d6fd1e8eab5a6ca2397194.pdf [lyu2022vd9]
55. 9fab78222c7111702a5702ce5fae0f920722c316.pdf [krishnan2021086]
56. c051ee2ad7ac203a26fa8f50eb6312424c729b27.pdf [yang20210bg]
57. 13f7a106bb3814ad1fab25fd1356e99e91f402d3.pdf [li20229zn]
58. d04fbbec070b7e7af8152f9b7574971d22bf5997.pdf [wang2022n7h]
59. e939b55a6f78bffeb00065aed897950c49d21182.pdf [chen202199v]
60. 6dc8693674a105c6daca5200141c50362e3044fc.pdf [panboonyuen20218r7]
61. 494e200d8a2ef49d21ce3458a25a109ea917ebe4.pdf [liang2022xlx]
62. 39240f94c9915d9f9959c34b1dc68593894531e6.pdf [zhou2021rtn]
63. 8db96d4eaefdac8dff6d98d94a5e3f5b5558c63d.pdf [dubey2021ra5]
64. 428d755f0c8397ee6d04c89787f3455d323d8280.pdf [ayas2022md0]
65. ff00791b780b10336cc02ee366446d16e1c5e17b.pdf [tian2022shu]
66. 957a3d34303b424fe90a279cf5361253c93ac265.pdf [liu2022249]
67. 401c8d72a9b275e88e6ba159d8d646cfb9f397aa.pdf [zhang2021mcp]
68. 7dee2bc2be709c0009b7623b7af78246f32e0a60.pdf [han2021vis]
69. 1b026103e33b4c9eb637bc6f34715e22636b3492.pdf [kim2022m6u]
70. 024c595ba03087399e68e51f87adb4eaf5379701.pdf [zhou2022nln]
71. 9500dbde37e0f8c6b9ea924aacc6591bc3414166.pdf [hu202242d]
72. 977351c92f156db27592e88b14dee2c22d4b312a.pdf [you2022bor]
73. ba12a9915553b3b42df17a33afcfd547821d8cc3.pdf [ren2022ifo]
74. 3efcd3a4c54694a093886981d59e3cffe0dd7149.pdf [wang20215ra]
75. 58fc305734a0d5d849ae69b9233af082d712197e.pdf [xiao202229y]
76. 54911915a13cf0138c06b696e6c604b12acfe228.pdf [jamil20223a4]
77. b8585577d05cebd85d45b7c63f7011851412e794.pdf [bai2022f1v]
78. 956d45f7a8916ec921df522c0641fd4f02beccb7.pdf [li2022th8]
79. 99fbe810d4194684be03458fdfebacb12d8a5c4e.pdf [almalik20223wr]
80. f4e32b928d7cc27447e312bdc052aa75888045aa.pdf [sha2022ae0]
81. 98e702ef2f64ab2643df9e80b1bd034334142e62.pdf [zhang2022msa]
82. 0ef3e7efa5ded21a398d8ec90ff766b467c4fc54.pdf [htten2022lui]
83. a21fa5ff42db4b0bd0fefae3d710cad5f0175400.pdf [hatamizadeh2022y9x]
84. 174919e5a4ef95ff66440d56614ad954c6f27df1.pdf [montazerin2022dgi]
85. 6971aee925639a8bd5b79c821570728ef49060c6.pdf [kojima2022k5c]
86. 15ad149fc1ba4d8b5de189fdfacac9daecc286d0.pdf [kang2022pv3]
87. f66181828b7621892d02480fa1944b5f381be80d.pdf [tian2022qb5]
88. cee8934975dfbe89747af60bbafc95e10a788dc2.pdf [peng2022snr]
89. 69e20583872b4f1384ae3f7dbdbec648c19d3d35.pdf [ho20228q6]
90. 0594eaa8dfe580678a2382aaf77ac3582c872a97.pdf [xia2022dnj]
91. 3a0145f34bcd35f09db23b2edec3ed097894444c.pdf [wang202232c]
92. ce79649b126dfe9e3cbeb1ecd64a80708bbd5538.pdf [mogan202229d]
93. 572ed945b06818472105bd17cfeb355d4e46c5e5.pdf [yang20221ce]
94. 934942934a6a785e2a80daa6421fa79971558b89.pdf [li2022ip7]
95. 3ae8c0b646ddce95ffd09da31c02ed6fdb744e90.pdf [yu20220np]
96. bc8d9f11ad91d54e83ad7cc3900502a913499fcb.pdf [li20229fn]
97. cf6d947d5d2ee72873a5a7b97dde9f881f79a6b6.pdf [huang2022iwe]
98. 9db52b92b48b8855f76d7e4b51b7727a2b3c0271.pdf [qu2022be0]
99. e4add4391dfa2a806a50cc1fbe9a9696dac9501f.pdf [zeng2022ce2]
100. 324f97d033efd97855488cf0b15511799fe7b7f7.pdf [lin20225ad]
101. bf52f09d648d78677f140e63c7c83ca44fcd438f.pdf [reghunath2022z8g]
102. 4eba4dccc7d19c40dfd02eef447a57613c3644e2.pdf [kundu2022z97]
103. 0bdcea2f18e158bbd1723ed27d48e5211545e050.pdf [sun2022cti]
104. 67b7ad5f3c818f42c8359abc87f353202f76f6f2.pdf [li2022gef]
105. 3994996a202f0127a58f57b259324a5283a1ba27.pdf [guo20228rt]
106. 4767600089dd71c7ed74c6a0acaf9e5a6fb345c9.pdf [li202240n]
107. d5b8af7ae526e3a511b1172c26d7caf7421145bb.pdf [jiang2022jlc]
108. 4d491b6fbe529a3986ef50cc34ede7c9ad88126c.pdf [lin2021oan]
109. d9e4af7739d4d5dcd44619bf276dc497f7334e34.pdf [wang2022dl1]
110. d69102eec0fff1084e3d1e24a411103280020a32.pdf [li2022wab]
111. 38bef6f0b4de2f9809b4fd44e8b61f9bebb14db9.pdf [park2022eln]
112. b80d0ce78652ffbf6b3ddebfc567f8594a6ffd4b.pdf [shen2022d6i]
113. 7d4d512445903e8ad75f0dcfa8baec27b1fe8620.pdf [wang20224wo]
114. 7d5274f1155b85a6120491c9374b6983dac96552.pdf [tao2022gdr]
115. 0afbaae7d0bfe98a04a3b7dd1ba69e25f32b24c6.pdf [wu2021nmg]
116. 6bcdf04ccd35ae971e765cdda25127ce005b7713.pdf [liu2022c56]
117. 6fd1002f321240c75dfc86c6bad6c05068a8160a.pdf [wang2022pb8]
118. c4560ab3855cb8ae4b9922458a0b6b94108e2c28.pdf [xiong2022ec2]
119. a0762777d6e46acbcd5b301285c2d9894065ff8f.pdf [sun2022pom]
120. d43950779dc86b728d7e002be6195526d35a26b0.pdf [qi2022yq9]
121. 2e69d97ae22c6a3685cc548f8c19c696d5d7d363.pdf [ma2022vf3]
122. c25091718b22384cebece2da7f30fc1702a07c76.pdf [wang2022tok]
123. cec0cbc2dd6d7975714110632b6bfcb5c1927ec3.pdf [wang2022pee]
124. ef93c81f90154a022e164be2f83c6cf6e602f33d.pdf [jannat20228u6]
125. 6a8d4a97ba3eafcb85d2d2a57d7d514bb592dbd0.pdf [chen2022r27]
126. 64143b37ae41085c4907e344ff3a2362a3051d0c.pdf [wang2021p2r]
127. dcd8617200724f0aa998276be339ff4af589ee42.pdf [sajid2021xb6]
128. 46880aeca86695ca3117cc04f6bd9edaf088111b.pdf [xing2022kqr]
129. 7e0dd543471b66374fbf1639b9894d3d502533b6.pdf [garaiman2022xwd]
130. 845a154dbcde81de52b68d73c78fad5be4af3b20.pdf [wang2022wyu]
131. 6f4093a7ad5378e8cd3b73a52fbec80b784c107d.pdf [hou2022ver]
132. 50405c1ee26c97ed5b9a54c7179317a424e6e471.pdf [agilandeeswari202273m]
133. 1bf9a97a7581d1671d683c19ce36fed6511dc95b.pdf [qin2022cfg]
134. e58c2bc5333ee8f37150d9d15c0428d9658b2e50.pdf [wang2022ohd]
135. 761f55a486e99ab5d3550aee48df34b6b65643c2.pdf [yu2022o30]
136. 2dcc1f5f1ba11f99e09f22c53d02aa66cec35c48.pdf [boukabouya2022ffi]
137. 52a7f15085f1b6815a4de2da26df51bb63470596.pdf [wang2022d7p]
138. 649b706ba282de4eb5a161137f80eb49ed84a0a8.pdf [song20215tk]
139. 186295f7c79e46c0e4e5f40e094267c09714043d.pdf [xie2021th0]
140. d77288fc7de7b15c200ed75118de702caf841ec3.pdf [sun2022bm5]
141. 2e4dbc3dbd400346be60318ae558a0293e65ba81.pdf [jing2022nkb]
142. 3de95f33c2b4f61a9c0f335b4810a966e209a47a.pdf [li2022spu]
143. 7817ecb816da8676ae21b401d60c99e706446f06.pdf [song2022y4v]
144. 3502b661362b278eebacf1037fc3bb4e21963869.pdf [shukla2022jxz]
145. 791d1e306eaa2e87657925ec4f45661baa8da58b.pdf [tran2022bvd]
146. 1d8ecf2c3a78e5dc4de9e4c41961771bbd0033b3.pdf [hong2022ks6]
147. e1f7478294fe01ce271cdef9ba93f4c675d92dc9.pdf [panboonyuen2021b4h]
148. a56f8e42e9efe5290602116b42a247b758052fe4.pdf [zhao2022wi7]
149. 6eb10790689a4cf239d1ee2a3919dced4be6db8f.pdf [wang2022h3u]
150. fc2e7fcdc1bd773f1eb097ae67c8f736108276e3.pdf [liu2021yw0]
151. 16ad38c73f4897e1c58326e3562b74ae1c2fd219.pdf [gul202290q]
152. 371e924dd270a213ee6e8d4104a38875105668df.pdf [zhao2022koc]
153. 0025c4241ffb2cce589dc2dcd82385ff06455542.pdf [yang2022qwh]
154. f2b3ece7cb0c88701ae0055236f6d6da39c8156d.pdf [alquraishi2022j3v]
155. 1eb4bbaa204890a45da2ca713f34fcc2d763c6e1.pdf [jin2021qdw]
156. 1b18750ea0d26575f3e2c18f61e444c2790dbe4c.pdf [lee2022rf1]
157. 08502153c9255399f8ff155e5f75900f121bd2ff.pdf [shi2022evc]
158. cb85a7648d2a4b4953abfc7437a33f91dc2f3673.pdf [zhang20223g5]
159. 90f9b2892a437ca32c4ab26c09b8dbb7de8fa034.pdf [zim202282d]
160. 7ead0cae4e67f390b2eb0083117ea8ab90c53b47.pdf [bao202239k]
161. e4864ea54710b4c8f3b2ca9ac39854bdfd0dae28.pdf [sun2022nny]
162. fe6887ee065c4f64a0c8d6054d1fc562766452de.pdf [munyer2022pfs]
163. 280ea33e67484c442757fe761b75d871a399905d.pdf [fan2022wve]
164. 29f86d6d1eaba6a466c231f6906b18eae4b2b484.pdf [wang2022gq4]
165. d8bf4b494d255c6d9b81e9963f29e40831106e3e.pdf [ali2022dux]
166. abf037290e859a241a5af2c5adf9c08767971683.pdf [chougui2022mpo]
167. dd46070ce18f55a5714e53a096c8219d6934d188.pdf [zhuang2021hqu]
168. 829926d9943c99dd64eea5b4b96541eca1e5e91d.pdf [chen2021d1q]
169. e8dceb26166721014b8ecbd11fd212739c18d315.pdf [hatamizadeh2024xr6]
170. e06b703146c46a6455fd0c33077de1bea5fdd877.pdf [ryali202339q]
171. 3af375031a3e23b7daf2f1ed14b5b61147996ca0.pdf [yao2023sax]
172. d203076c28587895aa344d088b2788dbab5e82a1.pdf [li2023287]
173. f3d0278649454f80ba52c966a979499ee33e26c2.pdf [zhao2024671]
174. 918617dbc02fa4df1999599bcf967acd2ea84d71.pdf [dehghani2023u7e]
175. 51f38bd957fa863022feb5878fa1ba3bea6657cf.pdf [duan2024q7h]
176. 1f389f54324790bfad6fc40ac4e56428757ea92b.pdf [barman2024q21]
177. 05236fa766fc1a38a9eb895e77075fb65be8c258.pdf [jamil20230ll]
178. 0eec6c36da426f78b7091ba7ae8602e129742d30.pdf [paal2024eg1]
179. 689bc24f71f8f22784534c764d59baa93a62c2e0.pdf [zhang2023k43]
180. afb7ff7096cd45de02de96b5937c3416ebca0cb0.pdf [himel2024u0i]
181. 595adb75ddeb90760c79e89b76d99e55079e0708.pdf [xu20235cu]
182. de20c6805b83a2f83ed75784920e91b913d888bb.pdf [chi202331y]
183. c57467e652f3f9131b3e7e40c23059abe395f01d.pdf [patro202303d]
184. 53e5db85e2a7442f20670be2ae25019fcf9d27a2.pdf [pan2023hry]
185. 0682771fd5f611bce2a536bf83587532469a83df.pdf [wang2024mrk]
186. a4ef2c7c321dea04d8ea91ec1ce0710ebce4f848.pdf [tabbakh2023ao7]
187. 243a056d1acb153f70e39cc80a10e7d211a4312f.pdf [dutta2023aet]
188. d8ab87176444f8b0747972310431c647a87de2df.pdf [qiu2024eh4]
189. a1e17a09b0df676f133c2175366ab7fcd0381ac1.pdf [li2023nnd]
190. 77b625e6dec2732b96ca8cf57d7a123497873e1d.pdf [zhao20243f3]
191. e91934d66d9133d854ff0a4cafbe7966584bbf97.pdf [song2024fx9]
192. e37539f5c943a92ef56b49b7fa067bd976e418d4.pdf [cai2023hji]
193. 8fc9a7f23c5cf57e17790ffb99b6acb11693c673.pdf [akinpelu2024d4m]
194. 7ddbb9723420cb474c58d4b6b6834d5ebe112049.pdf [hayat2024e4f]
195. c7c0c84fa505995cd888a8fbaa9def21cb84cca3.pdf [li2024g3z]
196. 1af8c50b4c7329fb2273aa7b755b0f9e8223e2d2.pdf [arshed2023zen]
197. bca0adcba0c26c5a34b05795c5bfbc1948bf2fae.pdf [qin20242eu]
198. c4895869637f73154d608cdd817234b0dbcd3508.pdf [lee2023iwc]
199. 64811427a4427588bb049a6a254446ddd2cafacc.pdf [tagnamas20246ug]
200. 7a0f92968c63613730b0c06b3bc3bd40d2666571.pdf [li2023jft]
201. 136b0ef7bd7f642a285b1d61ddf7abfc7ddc026f.pdf [song2024c99]
202. cf439db0e071f19305ea1755aa108acdde73ed99.pdf [aksoy20240c0]
203. ffc855594cad345ea5a1cce2ee27095bec767bc8.pdf [leem2024j4t]
204. 838d7862215df504dde41496cbe6ee711a12ae9f.pdf [chen2024asi]
205. 9a35a0aac04f4f712f38ea9bbfd2f0b0108242f5.pdf [lin202343q]
206. f6bf7787115affe22c410eb5b2606269912d59a0.pdf [ghahremani202491m]
207. 69e56df0ae079c83607bb48e68f8db39a4177cd0.pdf [wang20249qa]
208. 1528bab641830ea279bdf53ff9e7d11d2b7e5028.pdf [shahin2024g0q]
209. b43bb480caad36ab6fd667570275d42fe9050175.pdf [zhu2023dpi]
210. 1970ace992d742bdf098de08a82817b05ef87477.pdf [yu2023l1g]
211. fec86abbb29b559c1eaff31428f5b59f8070bb67.pdf [ko2024eax]
212. cb8b0eba078098000f004d7e0f97a33189261f30.pdf [yang2024w08]
213. 9b4d81736637392adabe688b6a698cec58f9ce57.pdf [nazih20238nf]
214. 981970d0f586761e7cdd978670c6a8f46990f514.pdf [xia2023bp7]
215. 6bdafb965e94c5240db2c30f20c37c4b4dd0e451.pdf [nag2023cfn]
216. a3d1cebf99262cc20d22863b9540769b49a15ede.pdf [gezici20246lf]
217. f05176e2d3f7a6e95b60f0e96fd9bf20e8b335d2.pdf [ghazouani202342t]
218. 442b5ec3aad4b099e71d6203a62eb7ec7519544c.pdf [wang202338i]
219. 635675452852e838644516e1eeefd1aaa8c8ac07.pdf [guo2024tr7]
220. d2fce7480111d66a74caa801a236f71ab021c42c.pdf [wang2023ski]
221. 5135a8f690c66c3b64928227443c4f9378bd20e1.pdf [zheng202325h]
222. 77eea367f79e69995948699d806683c7731a60b1.pdf [mogan2023ywz]
223. 861f670073679ba05990f3bc6d119b13ab62aca7.pdf [ebert202377v]
224. f93f718bb2d3f5e4354e174e54e71c5596f56ea3.pdf [wang20245bq]
225. c5c9005aae80795e241de18b595c2d01393808f8.pdf [cao20241ng]
226. 14c42c0f2c94e0a1f4aa820886080263f9922047.pdf [yang2024in8]
227. 9998291d71f4e8ddf59f4b016b19df1f848eeed1.pdf [hussain2025qoe]
228. 9fa44f616e796f187d97b9cc324bd839bc21ad81.pdf [shim2023z7g]
229. d62e6ba83d3da5b75e1d428f20f8199e3754bd5d.pdf [alam2024t09]
230. d1faaa1d7d312dd5867683ce60519979de6b3349.pdf [yang2024tti]
231. d68aedc3a957c25e5f19438ec4b64fb53b48422c.pdf [wang20245hx]
232. bb5e6bbd4ad8c613a0bac3db23940c03ce94bec9.pdf [song202479c]
233. 55156532cb9c20fdcaed9ead238f7a2cbaab2527.pdf [li2023lvd]
234. ae7c5ae7d377189c5a5f33a60b33c6cc5ca56124.pdf [ma2023vhi]
235. 3b427c8d3258968b9ac5eaf846d99ac027de9a76.pdf [han202416k]
236. 10e9943b3a974ac6175ffec3228e670ec9d2cc18.pdf [katar202352u]
237. 1783c85f8b951dc11c9dcb4929bfcae2d52ecb63.pdf [hemalatha2024a14]
238. 42bd7d3ca2b66eca5609fe77078fe8951ab9fd5c.pdf [ma2024uan]
239. 7aeadf7b9d839bdd9dbfbff5239f71792ff0c04c.pdf [lai20238ck]
240. b2becca9911c155bf97656df8e5079ca76767ab9.pdf [wang2024luv]
241. 25a69bdd7f87ecc287b4e9a64eb4d6d562371add.pdf [ling2023x36]
242. 50e997b23a534a6fbfd32d63990fa80373ec7c6b.pdf [wang2023bfo]
243. 3ea79430455304c782572dfb6ca3e5230b0351de.pdf [yin2023029]
244. 0284fa05879f1609bf459f15a4d40e22355cd5ae.pdf [mishra2024fbz]
245. 714e21409b8c4f7788ac8c93795249a4e45e51ce.pdf [heidari2024d9k]
246. 2ef224f7a1f545eb8bfdf61c7b849f2676fe6a61.pdf [yu2023fqo]
247. bc99c855d52ba3d432c428fb4096b3a22c04f8bf.pdf [zhao2023pau]
248. 3dee43cea71d5988a72a914121f3455106f89cc7.pdf [pan20249k5]
249. 1871c0bc27bd5cd5f2f6c3d6cedeb4b8c5a89499.pdf [huan202345b]
250. c064efa0419b75ba131ec8470ed80f01e1a80f64.pdf [belal2023x1u]
251. b48a85980deb5f1baa64d862b9f0e4e62124e4de.pdf [li20238ti]
252. 8ec10ffe0c1fc8f6a92d541f0e002e78080b564a.pdf [huo2023e5h]
253. 769ba82ab77fc46db4a594dc64576286d02cf1d7.pdf [kim2023cvz]
254. d1255f6e7e7e14b253fe2245e842d0b45f5e715e.pdf [fan2023whi]
255. 5572237909914e23758115be6b8d7f99a8bd51dc.pdf [zhao2023rle]
256. 21c37fd6d00d6c727ec10f0c871c1ddf6e4f7ed9.pdf [xie20234ve]
257. e38e70580acb204c05096de8da90b7ab1d4bdb6b.pdf [li20233lv]
258. a246677a3d68e8cd7a63d24639a5cf3fc3b9f56e.pdf [ma2023qek]
259. 1397d1bcfeae68078756fdbc3ed7e62ef51a1e32.pdf [tanimola20246cv]
260. 52239e2338cd9f0e0e690203af8f0f1260ef7abf.pdf [chen2023xxw]
261. f996d5ee3b8ad3c60510862a92fd72c6a41777e0.pdf [ranjan20243bn]
262. f1b6f564e7cba8121df0eef0d656673e3bd18621.pdf [fu20232q3]
263. 12106888ec74baf48d1e9fb64e8932c9ce509e84.pdf [shi20235zy]
264. 3798e7f16fe69c29307a9bab4f0f4d779478afc5.pdf [deressa2023lrl]
265. 29a0077d198418bab2ea4d78d04a892ede860d68.pdf [aburass2023qpf]
266. ac9cc0c28838a037e77f4e19433de170f47b3de9.pdf [hassija2025wq3]
267. c4357abf10ff937e4ad62df4289fbbf74f114725.pdf [huang20238er]
268. 0b41c18d0397e14ddacee4143db74a05d774434d.pdf [liu20230kl]
269. f9e1a8754e77a4b5f240d11e9e81e2563a319b89.pdf [he20238sy]
270. 4b9568a9798c527779a1f7479ffb3ba373640e3e.pdf [guo2023dpo]
271. 34ca004eda791c8c4191e0ebe65cc4405a116b08.pdf [wang2023j6b]
272. 409b43b8cd8a2ba69f93e80c2bacc0126238b550.pdf [gopal20237ol]
273. dc5d707a8f9efaf32d2319ec2ca8d2619ef2926e.pdf [liu2023awp]
274. 9017053fb240d0870779b9658082488b392e7cde.pdf [fu20228zq]
275. f27221a15f4c3c8af7ea0ada5bab25831ffe21ea.pdf [sahoo20223yl]
276. 2d421d94bc9eed935870088a6f3218244e36dc97.pdf [ganz20249zr]
277. be1aabb6460d49905575da88d564864da9f80417.pdf [paal2024no4]
278. 0d7d27fbd8193acf8db032441fd22945d26e9952.pdf [hassan20243qi]
279. 0af7ccb9f6cf2edb35674dc97d0ec5d165f8433b.pdf [k2024wyx]
280. f9efe4918b9a20c2e8084d7a5ec624b87ec1f211.pdf [nguyen2024id9]
281. f62cb393ab51505bdd22823e74b7cf28f4801e1c.pdf [almohimeed2024jq1]
282. 4702a22a3c2da1284a88d5e608d38cd106d66736.pdf [hao202488z]
283. 9fcea59a7076064f5ac3949177307c1637473ffd.pdf [yao20244li]
284. 1ec9b653475287e95fdaef2f5247f82a8376c56c.pdf [dong20245zz]
285. 2b48f5f0769bf41b5a3025ace73417d19de65cf1.pdf [zhang2024jha]
286. bfb310434204d99fa5c420ac272966c5ae1c1bd2.pdf [boukhari2024gbb]
287. bbe5dfbecfd1bed7556b9c8269b0d363faa24973.pdf [song2025idg]
288. be28edb064e2050e0852ff376434ee39f7c4b0a3.pdf [zhou2024tps]
289. e25a0b06079966b8e43f8e1f2455913266cb7426.pdf [abbaoui20244wy]
290. ecd9598308161557d6ac35b3e4d32770489e811d.pdf [yang2024nyx]
291. 7dc4b2930870e66caa7ff23b5d447283a6171452.pdf [yang20241kf]
292. b1f0ac0b5e4044251dcdcce78e0046feb43cc7c7.pdf [hu202434n]
293. 903c8a82f5539b3e482efcb23ff59819f04f2c0f.pdf [yang20244dq]
294. 1e95bb5827dc784547a46058793c15effd74dccc.pdf [keresh20249rl]
295. 2104eea2b01b7a4b6ba2d256769328ac4919afad.pdf [hu20247km]
296. 8972ebf4f9ec47ee60c3ed0effd520daff495d5a.pdf [alsulami2024ffb]
297. 0fde8e3287ac04dcf210b73898c8ceb697ad489f.pdf [yang2024wxl]
298. 9a4718faa07a32cf1dce745062181d3342e9b054.pdf [p2024nbn]
299. 6e97c1ba023afc87c1b99881f631af8146230d96.pdf [wu2024tsm]
300. 1d7183d481ae5a396743dde39984f1f0c8f47edf.pdf [dong2024bm2]
301. 9fbf36da9ee60d56a7675042bb0b24798a3b966d.pdf [swapno2025y2b]
302. d80166681f3344a1946b8bfc623f4679d979ee10.pdf [yoo2024u1f]
303. 9285996627124b945ec601a763f6ff884bac3281.pdf [he2024m6j]
304. 3f002ea3ede9d97f1fe29b16691491219c1e626d.pdf [zhang202489a]
305. 9121dcd10df00e5cc51dc94400e0325e0ae47bb9.pdf [zhang2024pd6]
306. 08606a6a8b447909e714be2c3160074fdf1b91ad.pdf [dong20242ow]
307. 0222269c963f0902cc9eae6768a3c5948531488b.pdf [kayacan2024yy7]
308. 8776fd7934dc48df4663dadf30c6da665d84fb19.pdf [liu20248jh]
309. cc24f933b343b6a9701088cf6ae1dbf3299c0c9e.pdf [shi2024r44]
310. 88589b0b2d2d8caa09d8ca94414343455ae87d7c.pdf [xin2024ljt]
311. d629289cc2f3efbe374a66d57690dd84f19d59ec.pdf [zhou2024qty]
312. 70211e2b04fcb4413edfd43e75b9f10e8d130171.pdf [monjezi2024tdt]
313. cc817ba09f1c3c94bae73576463bcaf57c70261a.pdf [baek2025h8e]
314. 77e4ea46e7248408844f62fa3613aa246e35fdf0.pdf [payne2024u8l]
315. 271c6a87213b8b2a26ce3e1a6f20b07cc2f82b81.pdf [qi2024rzy]
316. 05d15576d88f9384738908f98716f91bdb5dbc78.pdf [mercier2024063]
317. 6feea99a1a6c876d6f4f714b463da9c95998dee7.pdf [sikkandar2024p0d]
318. c05744f690ab9db007012a63c3c5c3ca48201c66.pdf [hou2024e4y]
319. cebc29015a2827aa8a98ec39c90c93bca2f14848.pdf [nfor2025o20]
320. 630c9993b6aaf812e1aa3dae8e243af131abd3d0.pdf [xiang2024tww]
321. d14a1677e416bd7ac6fbe01672cd3152fc1f983d.pdf [tian20242kr]
322. 8ed42c0d60eb4c91e99ee184a17719f7c3ce3f3f.pdf [zhou2024r66]
323. c4cf58e104fc8d783dfd0bcf3ccd65f8e4028d7f.pdf [taye20244db]
324. 72e23cdc3accca1f09e2e19446bc475368c912d0.pdf [alohali2024xwz]
325. 7b6d64097d16219c043df64e4576bd7d87656073.pdf [gao20246ks]
326. 0820d2ac35cd55fd60f53c14460cca78fa996f1e.pdf [du2024s3t]
327. b02144ef4ed94df78544959bc97eddef4580dd95.pdf [tiwari2024jm9]
328. b6488fded5fb0728d7c3bd04f3afa3fba68d0450.pdf [du20248pd]
329. 24eba2f5345bc8279bb63f085e75cc26b37d0b1e.pdf [chaurasia2024tri]
330. 8ce6872b26f88e52aeed57f6f7528dee30b5f7b2.pdf [karagz2024ukp]
331. dcf77b8e23bbc56c5c8f76ddc5d3cf68d0a051be.pdf [lee2025r01]
332. f792dcf878e5bf8e7fb5aa31d231474d43462f4a.pdf [dmen2024cb9]
333. 310f5543603bef94d42366878a14161db1bf45de.pdf [ferdous2024f89]
334. f9f524944dd9d1fe4b8956f9a822927b2eec29ad.pdf [akan2024izq]
335. 6ec48cb5fa3d149c9cc8a29373e64b63fe257913.pdf [nahak20242mv]
336. 3c7e07304bab6a860dbbe4ff36a4d87010036d2a.pdf [han2024f96]
337. 819ae728828d50f56f234e35832b1222de081bfc.pdf [zhao2024p8o]
338. 6a9247fe471690218ef04cea5f6f4b59b3b50747.pdf [li2024qva]
339. f462bb00b8c4379c4a4699b66a19ce10da530b08.pdf [wang2024ueo]
340. 52fafbbf20a37ffe863f1fa1d44fde6b4fc3334e.pdf [qi2024f5d]
341. b4be4f2ecb8711755f75dcb09a57d8c9722ac349.pdf [zhu2024l2i]
342. 7492734c76036143baf574d6602bd45a348c416f.pdf [roy2024r9y]
343. eacb2581b480cf4a80bc601c8ea657651b41c0fc.pdf [wang2024w4u]
344. da7d671f61e53601bdfc760d336cc98e5d9c516b.pdf [pan202424q]
345. 2bb98b30fb9aae37858248cd5ac3221ebcf7eeba.pdf [du2024lml]
346. 90e9d0a35bcf7e8251c4c9601a224bd95db14dd9.pdf [luo202432g]
347. 5ce1ebe051a64969caeb879c44c45320a5ca8011.pdf [elnabi2025psy]
348. 3bba1b3376e43a39bf4f1bf4eab558758a37ea2e.pdf [ergn2025r6s]
349. 3b11c8774bbeddd6722d65455bf9bb729a63c1cb.pdf [mohsin2025gup]
350. 04ed3b0f4e21d9f4884bf4db98edf83a34d005a9.pdf [marcos2024oo2]
351. 23ce9c2814d6567efec884b7043977cefcb7602e.pdf [peng2024kal]
352. 5b22bdc6aedf13d812509dd0f768353eb1469a79.pdf [urrea20245k4]
353. d2fd0dc314023ebca1342f1379ba6d79c7ded84a.pdf [zhang2024b7v]
354. 1c96dc4ad4dfa3bf7c6ca9cf0c6197e7ac00272b.pdf [saleem20249yl]
355. c35c316feba84334a4b08feb5281a5f83db0b8b2.pdf [zhou2024toe]
356. c8174af99bc92d96935683beccc4161c65a8aa46.pdf [lijin2024mhk]
357. 05548c4b3db8be40fac236dfa8e21882fe9ce9de.pdf [huang2024htf]
358. bd7a4a9e020b594ebcacb886ddae8731a2885209.pdf [chen2024cha]
359. a7df70e86f049a86b1c555f9a399d3540f466be7.pdf [shahin2024o1c]
360. e4b0c4a1d7ee87ad666214172f329831ade4c25d.pdf [xu2024wux]
361. 6604a900b9a7404a447b2167892a947012a9ffb8.pdf [park2024d7y]
362. 9814001811c4924171787de52e01cc31446e2f97.pdf [elharrouss20252ng]
363. ab10aacab1a2672a154034c589dd0aa801912272.pdf [du2024i6n]
364. 325367f93439652efaa4bc6b50115bbb7371704e.pdf [guo2024o8u]
365. 3c6980902883f03c37332d34ead343e1229062b3.pdf [zhang2024g0m]
366. f2b1b0fb57cccaac51b44477726d510570c4c799.pdf [xu2025tku]
367. 2456506ed87faa667a0c2b8af4028a5a86a49650.pdf [li2024m4t]

## Literature Review

### Introduction to Visual Transformers

\section{Introduction to Visual Transformers}
\label{sec:introduction_to_visual_transformers}



\subsection{The Rise of Transformers in Deep Learning}
\label{sec:1_1_the_rise_of_transformers_in_deep_learning}


The landscape of deep learning for sequence modeling underwent a profound transformation with the advent of the Transformer architecture, which effectively addressed critical limitations inherent in prior recurrent and convolutional neural networks. Before this paradigm shift, models like Recurrent Neural Networks (RNNs) and their more sophisticated variants, Long Short-Term Memory (LSTM) networks [Hochreiter1997LongSM], faced significant challenges in capturing long-range dependencies. These challenges stemmed from issues such as vanishing or exploding gradients and their inherently sequential processing nature, which fundamentally hindered parallelization and scalability for longer sequences. Convolutional Neural Networks (CNNs), while powerful for local feature extraction and exhibiting strong inductive biases like locality and translation equivariance, were not primarily designed for global context modeling across extended sequences. They often required complex architectural designs, such as deeper stacks or dilated convolutions, to achieve broader receptive fields, which could still be limited compared to the entire sequence.

The seminal paper "Attention Is All You Need" by Vaswani et al. (2017) introduced the Transformer, a novel architecture that entirely eschewed recurrence and convolutions, relying instead on a multi-head self-attention mechanism to draw global dependencies between input and output elements [Vaswani2017]. This groundbreaking work fundamentally altered the approach to sequence-to-sequence tasks, particularly in Natural Language Processing (NLP). The core innovation of the Transformer lies in its ability to dynamically weigh the importance of different parts of the input sequence when processing each element, irrespective of their positional distance [Vaswani2017, heidari2024d9k]. Unlike RNNs, which process tokens one by one, the self-attention mechanism enables parallel processing of all tokens in a sequence, significantly accelerating training times and improving scalability for longer sequences [han2020yk0]. Beyond self-attention, the Transformer architecture also incorporates crucial components such as positional encodings to inject information about the relative or absolute position of tokens in the sequence, and feed-forward networks for non-linear transformations, all contributing to its robust representational capacity.

The self-attention mechanism computes a weighted sum of all input elements, where the weights are dynamically calculated based on the similarity between a query (representing the current element being processed) and all keys (representing other elements in the sequence). More precisely, the attention weights are derived by computing the scaled dot-product between the query vector ($Q$) and key vectors ($K$) for all elements, followed by a softmax function to normalize these scores, and then multiplying by value vectors ($V$) to obtain the output: $\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$, where $d_k$ is the dimension of the keys [Vaswani2017, heidari2024d9k]. This direct, unconstrained interaction between any two positions in the input sequence ensures that long-range dependencies are effectively captured, a significant improvement over the limited receptive fields of CNNs or the vanishing gradient issues prevalent in RNNs. By allowing each element to attend to all other elements, the Transformer can build a rich contextual representation for every token, reflecting its global relationships within the sequence. Furthermore, the multi-head aspect of self-attention allows the model to jointly attend to information from different representation subspaces at different positions, enriching the model's ability to focus on various aspects of the input simultaneously and enhancing its overall expressiveness [han2020yk0].

This breakthrough capability of the Transformer to efficiently model global dependencies and process sequences in parallel profoundly impacted deep learning, establishing a new state-of-the-art across numerous NLP tasks, including machine translation, text summarization, and question answering [Vaswani2017]. The subsequent development of large-scale pre-trained models like BERT [Devlin2019BERTPO] further demonstrated the Transformer's power, showcasing its ability to learn highly generalizable language representations. The conceptual elegance and computational efficiency of self-attention laid the critical groundwork for its eventual adaptation and widespread adoption beyond NLP. Critically, the success in NLP prompted researchers to challenge the long-held axiom that CNNs' inherent inductive biases, such as locality and translation equivariance, were indispensable for computer vision tasks [zhou2021rtn]. The Transformer, by design, lacks these built-in biases, and overcoming this perceived limitation became a key conceptual hurdle. This motivated the direct application of a sequence-processing architecture to image data, where images could be re-conceptualized as sequences of patches, allowing the powerful self-attention mechanism to capture global visual relationships [han2020yk0]. This paradigm shift directly challenged the long-standing dominance of recurrent and convolutional networks across various deep learning applications, particularly paving the way for Vision Transformers (ViTs).

The Transformer architecture, with its innovative self-attention mechanism and parallel processing capabilities, thus marked a pivotal moment in deep learning. Its capacity for robust capture of long-range dependencies not only revolutionized NLP but also established a foundational conceptual framework that would eventually inspire a new generation of models in computer vision and other domains, fundamentally altering how deep learning models perceive and process complex data. This conceptual leap set the stage for the subsequent exploration of Vision Transformers, which sought to leverage these advantages for visual understanding, despite the architectural differences from traditional CNNs.
\subsection{Motivation for Vision Transformers: Beyond CNNs}
\label{sec:1_2_motivation_for_vision_transformers:_beyond_cnns}


Convolutional Neural Networks (CNNs) have historically dominated computer vision, largely owing to their strong inductive biases of locality and translation equivariance. These biases, inherent in convolutional operations, enable efficient extraction of local features and parameter sharing, leading to remarkable success in tasks like image classification and object recognition over the past decade [han2020yk0]. However, the very nature of these local operations, while beneficial for capturing fine-grained patterns, inherently restricts CNNs' ability to effectively model global relationships and long-range dependencies across an entire image. This limitation is particularly pronounced when understanding complex scenes or objects where interactions between spatially distant parts are crucial [liu2022c56, gheflati202131i].

To compensate for this restricted receptive field, CNN architectures often resort to complex designs such as stacking many convolutional layers, employing large kernel sizes, using dilated convolutions, or incorporating pyramid structures to aggregate broader context and simulate a global view [liu2022c56]. While these strategies have pushed the boundaries of CNN performance, they often introduce significant computational overhead, increase model depth, or still rely on an indirect, hierarchical aggregation of information rather than direct global interaction. For instance, in medical imaging, the "inadequacy of long-range relation modeling" in CNNs poses challenges for comprehensive understanding of complex anatomical structures [liu2022c56], and their "restricted local receptive field limits their ability to learn global context information" in breast ultrasound image classification [gheflati202131i]. The need for a more direct and flexible mechanism to capture these global interactions served as a primary motivation for exploring alternative architectures.

The advent of Vision Transformers (ViTs) offered a fundamentally different paradigm, directly addressing the CNNs' limitations in global context modeling. Inspired by the success of Transformers in Natural Language Processing (NLP) for handling long-range dependencies in sequential data, ViTs re-conceptualized image processing. By treating images as sequences of flattened patches, which are then linearly embedded and processed by a standard Transformer encoder, ViTs leverage the multi-head self-attention mechanism to establish direct relationships between any two patches, irrespective of their spatial distance [han2020yk0]. This design inherently provides a global receptive field from the very first layer, allowing the model to weigh the importance of all input patches when processing each individual patch, thereby enabling a more holistic understanding of the visual scene. This radical departure from convolutional locality represented a significant architectural shift, promising to overcome the architectural constraints that plagued CNNs in capturing broad context.

However, this paradigm shift was not without its own challenges, which in turn motivated subsequent waves of research within the ViT domain. By largely abandoning the strong inductive biases of locality and translation equivariance inherent in CNNs, the original ViTs exhibited a significant dependence on vast amounts of pre-training data to achieve competitive performance [lee2021us0, hong2022ks6, wang202338i]. This "data hunger" stemmed from their more general-purpose architecture, which required extensive exposure to diverse visual patterns to learn robust representations without the built-in priors of convolutions. Furthermore, the initial ViT models, with their uniform patch processing, lacked the inherent hierarchical feature extraction capabilities that CNNs naturally provided, which are crucial for dense prediction tasks like object detection and semantic segmentation [liu2021yw0]. The quadratic computational complexity of global self-attention with respect to image resolution also presented a practical hurdle for high-resolution inputs. These new challenges, arising directly from ViTs' novel approach to global context modeling, became the driving force for subsequent architectural innovations aimed at improving data efficiency, computational scalability, and the ability to generate multi-scale features, thereby making ViTs more robust and versatile for a broader range of computer vision applications.
\subsection{Scope and Structure of the Review}
\label{sec:1_3_scope__and__structure_of_the_review}


This literature review is meticulously structured to provide a comprehensive and navigable roadmap through the rapidly evolving landscape of Visual Transformer (ViT) research. Its organizational framework is designed to facilitate a pedagogical progression, tracing the intellectual trajectory of ViTs from their foundational concepts to their most advanced applications and future challenges. This approach ensures a coherent narrative that highlights the chronological development, interconnectedness of ideas, and the continuous evolution of this transformative field.

The review commences with \textbf{Section 1: Introduction to Visual Transformers}, which establishes the historical context of deep learning for vision, critically examines the limitations of traditional Convolutional Neural Networks (CNNs), and introduces the paradigm shift instigated by Transformers. This foundational section sets the stage by delineating the overall scope of the review, emphasizing the journey from initial concepts to sophisticated architectures and diverse applications.

Building upon this introduction, \textbf{Section 2: Foundational Vision Transformer Architectures and Early Optimizations} delves into the seminal works that introduced ViTs to computer vision. This section covers the core methodology of processing images with standard Transformer encoders and critically analyzes their initial potential and inherent limitations, such as significant data requirements. It then explores immediate efforts to address these practical constraints through early innovations in training efficiency, stability, and tokenization, including initial investigations into ViT robustness and transferability.

The narrative then transitions to \textbf{Section 3: Hierarchical and Efficient Vision Transformer Architectures}. This pivotal section focuses on architectural innovations that transformed ViTs into versatile backbones capable of handling a broader range of computer vision tasks. It details the development of hierarchical structures, multi-scale feature representations, and efficient attention mechanisms designed to overcome the original ViT's quadratic computational complexity and its limitations in dense prediction tasks. The discussion here encompasses various design techniques aimed at enhancing efficiency and scalability of attention mechanisms.

Following the architectural advancements, \textbf{Section 4: Self-Supervised Learning Paradigms for Vision Transformers} investigates the transformative role of self-supervised learning (SSL) in unlocking ViTs' full potential. This section details various SSL methodologies, including masked image modeling and self-distillation, which enable ViTs to learn powerful visual representations from vast amounts of unlabeled data. It highlights how these sophisticated pre-training strategies mitigate ViT's initial reliance on massive labeled datasets, paving the way for scalable and robust models, and discusses the scaling of these self-supervised ViTs towards foundation models.

\textbf{Section 5: Hybrid Architectures and Beyond Self-Attention} explores the fascinating convergence of ViTs with CNNs and examines radical alternatives to the self-attention mechanism. This section details hybrid architectures that strategically combine the inductive biases of convolutions with the global context modeling of Transformers, aiming for improved efficiency and robustness. Furthermore, it investigates research that questions the absolute necessity of complex self-attention, proposing simpler yet effective token mixing mechanisms, and demonstrates how modern CNNs, inspired by ViT design principles, can achieve competitive performance.

The review then broadens its scope in \textbf{Section 6: Applications and Domain-Specific Adaptations of Visual Transformers}. This section showcases the broad applicability and versatility of ViTs across various computer vision tasks and specialized domains, including object detection, semantic segmentation, medical image analysis, and remote sensing. It illustrates how ViTs have been adapted to excel in these diverse real-world scenarios, often through customized integration strategies, and touches upon their utility in general image classification and emerging 3D vision tasks.

Finally, \textbf{Section 7: Future Directions and Open Challenges} provides a forward-looking perspective on the field. It identifies key future research directions, unresolved theoretical questions, and practical challenges, such as the continuous quest for more efficient and scalable architectures, the potential of multimodal and new foundational models, and the exploration of novel architectures beyond traditional attention mechanisms. This section also critically addresses ethical considerations and the broader societal impact of increasingly powerful Vision Transformers, including concerns related to bias, robustness, and responsible AI development. The review concludes with \textbf{Section 8: Conclusion}, synthesizing the key findings and offering a final outlook on the trajectory of visual intelligence.

Through this structured progression, the review aims to provide readers with a deep understanding of the intellectual journey, current state, and future potential of Visual Transformer research, emphasizing the dynamic interplay between theoretical advancements and practical applications.


### Foundational Vision Transformer Architectures and Early Optimizations

\section{Foundational Vision Transformer Architectures and Early Optimizations}
\label{sec:foundational_vision_transformer_architectures__and__early_optimizations}



\subsection{The Original Vision Transformer (ViT) Paradigm}
\label{sec:2_1_the_original_vision_transformer_(vit)_paradigm}


The Vision Transformer (ViT) architecture operationalized a profound shift in computer vision, directly applying the Transformer architecture, originally developed for natural language processing, to image recognition tasks [dosovitskiy2021image]. This groundbreaking work fundamentally re-conceptualized image processing, moving away from local convolutional operations to a sequence-to-sequence approach, treating images as sequences of flattened patches rather than relying on inherent spatial hierarchies.

At its core, the ViT architecture processes an image by first dividing it into non-overlapping, fixed-size square patches (e.g., 16x16 pixels). Each patch is then flattened into a 1D sequence of pixel values and linearly projected into a higher-dimensional embedding space. To re-introduce crucial spatial information lost during the flattening, learnable positional encodings are added to these patch embeddings, allowing the model to understand the relative positions of different patches within the original image [dosovitskiy2021image]. These embedded patches, along with an additional learnable "class token" (a direct borrowing from BERT's \texttt{[CLS]} token in NLP, intended to aggregate global information for classification), form the input sequence to a standard Transformer encoder. This encoder, composed of multiple layers of multi-head self-attention (MSA) and feed-forward networks (FFN), enables the model to capture long-range dependencies and global contextual relationships across the entire image by allowing each patch to attend to all other patches. The final classification is typically performed by a multi-layer perceptron (MLP) head attached to the output of the class token.

The original ViT demonstrated impressive performance, achieving state-of-the-art results on large-scale image classification benchmarks, notably surpassing CNN-based models when pre-trained on massive datasets such as JFT-300M [dosovitskiy2021image]. This success underscored the power of the self-attention mechanism to learn rich, global representations without relying on strong, hard-coded convolutional inductive biases like locality and translation equivariance.

However, despite its groundbreaking performance, the original ViT paradigm critically highlighted significant limitations. Unlike CNNs, which possess strong, hard-coded inductive biases such as local receptive fields and translation equivariance, the original ViT architecture largely foregoes these explicit priors. While the initial patchification step introduces a rudimentary form of local processing, the subsequent global self-attention mechanism is designed to learn relationships across arbitrary distances without inherent spatial constraints [han2020yk0, huo2023e5h]. This design choice, while enabling unprecedented flexibility and global context modeling, critically meant that ViTs had to learn these fundamental visual priors from data itself. Consequently, ViTs exhibited substantial data hunger, requiring extensive pre-training on colossal datasets to learn robust visual representations from scratch, making them less competitive than CNNs when trained on smaller, more common datasets like ImageNet-1K without such pre-training [yu2022iy0].

Furthermore, the computational cost associated with the original ViT posed practical challenges. The global self-attention mechanism scales quadratically with respect to the number of input tokens (patches) [song20215tk]. For instance, doubling the linear resolution of an image (e.g., from 224x224 to 448x448) quadruples the number of patches, leading to a sixteen-fold increase in the computational cost of the attention layers [yu2022iy0, heidari2024d9k]. This prohibitive computational and memory overhead, especially for high-resolution inputs, limited its practical deployment and accessibility [hu202434n, xia2022dnj]. The reliance on a single `[CLS]` token for global representation, while effective, also represented a direct porting of an NLP mechanism, which would later be critically re-evaluated and often replaced by more vision-centric aggregation strategies in subsequent ViT variants.

In conclusion, the original ViT paradigm successfully demonstrated the viability of Transformers for computer vision, showcasing their unparalleled ability to model global dependencies and learn powerful representations from data. Yet, it simultaneously exposed a fundamental tension: the trade-off between the expressive power of pure self-attention and the practical demands of data efficiency, computational cost, and the need for implicit inductive biases. This initial work laid the foundation for subsequent research to address these limitations, either by developing more data-efficient training strategies or by integrating architectural priors to make Vision Transformers more robust and versatile across various scales and data regimes.
\subsection{Data-Efficient Training and Distillation}
\label{sec:2_2_data-efficient_training__and__distillation}


The initial introduction of Vision Transformers (ViTs) marked a significant paradigm shift in computer vision, demonstrating that pure Transformer architectures could achieve state-of-the-art performance on image recognition tasks [vit]. However, a critical limitation of the original ViT was its substantial data hunger, necessitating pre-training on colossal datasets like JFT-300M to outperform traditional Convolutional Neural Networks (CNNs). This reliance on proprietary, extremely large datasets posed a major barrier to broader adoption and practical application. Consequently, early research efforts focused intensely on developing strategies to mitigate this data dependency, making ViTs more accessible and efficient for researchers and practitioners without access to such extensive resources.

A pivotal advancement in addressing ViT's data hunger was the introduction of data-efficient training techniques, most notably knowledge distillation, exemplified by the Data-efficient Image Transformer (DeiT) [deit]. DeiT demonstrated that a ViT student model could achieve competitive performance, even with significantly smaller training datasets (e.g., ImageNet-1K), by learning from a pre-trained CNN teacher. This teacher-student paradigm effectively transfers the rich representations and strong inductive biases (such as locality and translation equivariance) inherent in CNNs to the ViT, thereby bootstrapping its performance without requiring massive amounts of labeled data. The distillation process in DeiT involved a specialized distillation token that interacts with the class token and patch tokens, learning to reproduce the teacher's output. Crucially, DeiT's success was also attributed to a sophisticated training recipe that included aggressive data augmentation techniques like RandAugment, Mixup, and CutMix. These augmentations effectively expanded the diversity of the training data, preventing overfitting and enabling the ViT to learn robust features from a comparatively smaller dataset, thus making ViTs practical for a wider range of applications.

Complementing distillation, other approaches sought to imbue ViTs with CNN-like inductive biases directly into their architecture to improve data efficiency. For instance, [convit] proposed ConViT, which introduced soft convolutional inductive biases into the self-attention mechanism. By incorporating a gated positional self-attention that leverages local information, ConViT improved the performance of ViTs, particularly when trained on smaller datasets. This method aimed to combine the local feature extraction strengths of CNNs with the global reasoning capabilities of Transformers, making the models more robust to data scarcity by providing better architectural priors. Similarly, [lee2021us0] introduced Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA) as generic add-on modules to enhance the locality inductive bias of ViTs. SPT reorganizes patches to capture local information more effectively, while LSA restricts attention to local neighborhoods, enabling ViTs to learn effectively from scratch even on small-size datasets like Tiny-ImageNet, demonstrating significant performance improvements. These architectural modifications offer an alternative to distillation by intrinsically improving the ViT's ability to learn from limited data.

The concept of distillation itself also evolved, moving beyond CNN teachers. [wang2022pee] explored "Attention Distillation" for self-supervised Vision Transformers (ViT-SSKD), demonstrating that distilling information directly from the attention mechanism of a teacher ViT to a student ViT could significantly narrow the performance gap between them. This approach proved effective for smaller ViT models (e.g., ViT-Tiny and ViT-Small) and was independent of the specific self-supervised learning algorithm, highlighting the versatility and continued relevance of distillation for improving the data efficiency and performance of ViT students, even when the teacher is another ViT. Furthermore, the challenge of training ViTs from scratch extended beyond classification. [hong2022ks6] investigated training ViT-based object detectors from scratch without large-scale ImageNet pre-training. Their findings revealed that specific architectural changes and extended training epochs played critical roles in achieving competitive performance, underscoring that data efficiency for ViTs is not solely about distillation but also about optimizing the architectural design and training regimen for specific tasks when large pre-training datasets are unavailable.

In summary, the early strategies for data-efficient training and distillation were crucial in overcoming the initial barrier of ViT's immense data requirements. Techniques like knowledge distillation [deit, wang2022pee] successfully transferred valuable inductive biases and rich representations, enabling ViTs to achieve strong performance with standard datasets. Concurrently, architectural innovations that integrated convolutional priors [convit] and enhanced locality through refined tokenization and attention mechanisms [lee2021us0] further improved ViT's ability to learn from less data and even train from scratch [hong2022ks6]. These advancements collectively transformed ViTs from a theoretical curiosity requiring massive resources into a practical and widely applicable architecture, accelerating their integration into the broader computer vision community. While these methods successfully addressed the data-hunger problem, making ViTs viable on ImageNet, achieving stable training for deeper and more powerful variants presented a new set of challenges related to optimization and token representation, which are explored in the subsequent section.
\subsection{Enhancing Stability and Tokenization for Deeper Models}
\label{sec:2_3_enhancing_stability__and__tokenization_for_deeper_models}


The introduction of Vision Transformers (ViTs) marked a significant paradigm shift in computer vision, demonstrating the potential of self-attention mechanisms for image recognition [ViT]. However, early ViT models faced considerable challenges, notably their heavy reliance on vast pre-training datasets and inherent instability when scaled to greater depths, which hindered their practical application and ability to train effectively from scratch on standard datasets like ImageNet. Addressing these limitations became crucial for the broader adoption of ViTs.

Initial efforts focused on mitigating the data dependency. For instance, DeiT [DeiT] introduced data-efficient training strategies, including knowledge distillation from a convolutional neural network (CNN) teacher, enabling ViTs to achieve competitive performance on ImageNet with significantly less training data. While this improved data efficiency, the fundamental architectural challenges related to training very deep Transformer models, such as vanishing or exploding gradients and general instability, persisted.

A pivotal advancement in enabling deeper and more stable ViT architectures was presented by CaiT (Class-attention in Image Transformers) [CaiT]. This work directly addressed the instability issues encountered when scaling ViTs to hundreds of layers, a common problem in deep neural networks. CaiT introduced **LayerScale**, a simple yet highly effective mechanism that rescales the residual connections within each Transformer block. By adaptively learning per-channel scaling factors for the output of self-attention and feed-forward layers, LayerScale stabilized the training dynamics, allowing the successful development and training of much deeper ViTs without performance degradation. Furthermore, CaiT proposed **class-attention layers**, where a dedicated class token attends to all image tokens to aggregate global information, but image tokens do not attend to the class token. This asymmetric attention mechanism further enhanced stability and improved the representation learning capabilities of deeper models, demonstrating that ViTs could indeed "go deeper" with appropriate architectural modifications.

Complementing these architectural stability improvements, refinements in the initial tokenization process were explored to better capture fine-grained local image structures and optimize sequence length, thereby facilitating more effective training from scratch. The Tokens-to-Token ViT (T2T-ViT) [T2T-ViT] introduced a novel **Tokens-to-Token (T2T) module** designed to progressively structure local tokens. Unlike the original ViT's approach of simply flattening non-overlapping image patches, the T2T module iteratively aggregates neighboring tokens, effectively modeling local connectivity and reducing the sequence length. This hierarchical tokenization strategy allowed T2T-ViT to capture fine-grained details and local contextual information more efficiently, leading to improved performance. Crucially, this enhanced tokenization, combined with the general advancements in ViT training stability, enabled T2T-ViT to achieve state-of-the-art results when trained from scratch on ImageNet, without requiring extensive pre-training on larger external datasets.

The innovations introduced by CaiT [CaiT] and T2T-ViT [T2T-ViT] collectively marked a significant step forward in the development of Vision Transformers. CaiT's LayerScale and class-attention layers provided the necessary architectural stability for scaling ViTs to unprecedented depths, while T2T-ViT's progressive tokenization improved the capture of local features and reduced computational overhead, making ViTs more amenable to training from scratch on standard datasets. These advancements significantly enhanced the practicality, robustness, and scalability of ViTs, moving them beyond their initial limitations and paving the way for their broader application in diverse computer vision tasks.

Despite these critical advancements, the inherent quadratic complexity of global self-attention, even with reduced sequence lengths from improved tokenization, remained a computational bottleneck for very high-resolution inputs or real-time applications. This limitation spurred further research into more efficient attention mechanisms and hierarchical designs, which would become a subsequent major focus in the evolution of Vision Transformers.


### Hierarchical and Efficient Vision Transformer Architectures

\section{Hierarchical and Efficient Vision Transformer Architectures}
\label{sec:hierarchical__and__efficient_vision_transformer_architectures}



\subsection{Shifted Window-Based Attention for Hierarchical Processing}
\label{sec:3_1_shifted_window-based_attention_for_hierarchical_processing}


The initial Vision Transformer (ViT) architecture, while demonstrating the power of self-attention for image classification, suffered from two significant limitations: a quadratic computational complexity with respect to image size, making it impractical for high-resolution inputs, and an inability to generate multi-scale feature maps, which are crucial for dense prediction tasks like object detection and semantic segmentation. Addressing these challenges, the concept of shifted window-based attention emerged as a pivotal innovation, most notably introduced by the Swin Transformer [liu2021ljs]. This mechanism fundamentally transformed ViTs into efficient, general-purpose backbones capable of hierarchical processing.

The Swin Transformer [liu2021ljs] proposed a hierarchical Vision Transformer that computes representations with shifted windows. Its core innovation lies in partitioning images into non-overlapping local windows and applying self-attention only within these windows, thereby reducing computational complexity from quadratic to linear with respect to image size. Crucially, the 'shifted window' approach introduces cross-window connections by cyclically shifting the window partitions between successive self-attention layers. This ingenious mechanism allows for information flow across different windows, effectively expanding the receptive field and enabling the construction of a hierarchical feature representation akin to those in convolutional neural networks (CNNs). This design made Swin Transformer highly efficient and effective, achieving state-of-the-art results across image classification, object detection, and semantic segmentation, establishing it as a versatile backbone for various vision tasks.

The effectiveness of the Swin Transformer's shifted window design quickly led to its adoption and adaptation across various computer vision domains. For instance, SwinIR [liang2021v6x] leveraged the Swin Transformer as a strong baseline for image restoration tasks, demonstrating its capability in low-level vision by incorporating Residual Swin Transformer Blocks. Similarly, for semantic segmentation of remote sensing images, the Class-Guided Swin Transformer (CG-Swin) [meng2022t3x] utilized Swin as an encoder, further validating its suitability for dense prediction by designing a class-guided Transformer block in the decoder. The Lightweight Dual-Branch Swin Transformer (LDBST) [zheng202325h] for remote sensing scene classification refined the local connection within the Swin framework by integrating a depthwise convolutional layer into the MLP, boosting connections with neighboring windows and demonstrating efforts to make Swin-based architectures more efficient.

While shifted window attention proved highly effective, alternative strategies for achieving multi-scale features and efficient global context were also explored. CrossViT [chen2021r2y], for example, proposed a dual-branch transformer that processes image patches of different sizes and fuses them using a linear-complexity cross-attention mechanism, offering an alternative to Swin's window-shifting for multi-scale interaction. MobileViT [mehta20216ad] aimed for light-weight, mobile-friendly ViTs by presenting "transformers as convolutions," combining CNN strengths with ViTs for efficient global processing. Interestingly, ViTDet [li2022raj] explored plain, non-hierarchical ViT backbones for object detection and found that simple feature pyramids from single-scale feature maps, combined with *non-shifted* window attention aided by very few cross-window propagation blocks, could achieve competitive results. This finding directly challenges the absolute necessity of the *shifted* component for all cross-window interactions, suggesting that sparse global connections can sometimes suffice. Further, the Deformable Attention Transformer (DAT) [xia2022qga] introduced deformable self-attention, where key and value positions are selected in a data-dependent manner, offering a more flexible and focused alternative to fixed windowing for capturing informative features. Hiera [ryali202339q] argued that with strong pretraining like Masked Autoencoders (MAE), a simpler hierarchical vision transformer, stripped of some "bells-and-whistles," could be equally effective and faster, implying that the core hierarchical structure might be more critical than the specific shifted window mechanism for certain applications. Other works, such as GSC-ViT [zhao2024671], achieved a balance between local and global feature extraction through groupwise separable multihead self-attention, echoing the principles of local processing with global interaction.

The success of the Swin Transformer cemented its role as a foundational backbone, influencing many subsequent works. Numerous studies in remote sensing image analysis, including QAGA-Net [song202479c], ODDL-Net [song2025idg], CTNet [deng2021man], P2FEViT [wang202338i], and those exploring quantitative regularization [song2024fx9], have adopted Swin Transformer (or similar hierarchical ViTs like Next-ViT) as their backbone, often integrating it with CNNs or specialized training strategies to address the unique challenges of remote sensing data. This highlights Swin's versatility and its ability to serve as a robust feature extractor. Beyond direct adoption, the principles of hierarchical processing and efficient attention inspired new hybrid architectures. Next-ViT [li2022a4u] and TRT-ViT [xia2022dnj] are examples of "next-generation" vision transformers that combine convolutional and transformer blocks in hierarchical designs, specifically optimized for efficient deployment in industrial scenarios. MambaVision [hatamizadeh2024xr6] represents a recent hybrid Mamba-Transformer backbone that also emphasizes hierarchical architecture for efficient modeling of visual features and capturing long-range spatial dependencies. Even in video action recognition, the need for efficient spatio-temporal processing led to models like TP-ViT [jing2022nkb] and ViT-Shift [zhang2024g0m], which adapt ViT principles to handle temporal dynamics, often benefiting from hierarchical feature extraction.

In conclusion, the introduction of shifted window-based attention, pioneered by the Swin Transformer, marked a paradigm shift in Vision Transformer research. It effectively mitigated the original ViT's limitations of quadratic complexity and fixed-scale processing, making Transformers practical and highly effective for a broad spectrum of computer vision tasks, especially dense prediction. This innovation established a new standard for hierarchical feature representation in ViTs, enabling linear scaling with image size and facilitating cross-window information exchange. While the core shifted window mechanism remains influential, subsequent research has explored various refinements, alternatives, and hybrid architectures, continuously seeking to optimize the balance between computational efficiency, inductive biases, and the expressive power of global attention. The ongoing tension lies in determining the optimal integration of localized processing (like windows or convolutions) with global reasoning, and whether the specific "shifted" mechanism is always necessary or if simpler cross-window connections suffice, particularly when coupled with powerful pre-training strategies.
\subsection{Pyramid Structures for Multi-Scale Feature Representation}
\label{sec:3_2_pyramid_structures_for_multi-scale_feature_representation}


The initial success of Vision Transformers (ViTs) [Dosovitskiy2020] primarily in image classification highlighted their powerful global reasoning capabilities. However, their inherent design, which typically produces fixed-resolution feature maps and suffers from quadratic computational complexity due to global self-attention, presented significant challenges for dense prediction tasks like object detection and segmentation. These tasks critically demand fine-grained localization and multi-scale contextual understanding, capabilities traditionally dominated by Convolutional Neural Networks (CNNs) through their hierarchical feature extraction. To bridge this gap, a pivotal direction in ViT research has focused on developing pyramid-like architectures that can generate multi-scale feature representations efficiently and without explicit convolutional layers, thereby expanding ViT's applicability beyond simple image classification.

A seminal contribution in this domain is the Pyramid Vision Transformer (PVT) [Wang2021]. PVT introduced a novel pyramid structure to progressively reduce the resolution of feature maps, moving from high-resolution, fine-grained features at early stages to low-resolution, semantically rich features at deeper layers. This hierarchical design is achieved through a spatial reduction attention mechanism, which efficiently downsamples the key and value matrices in the self-attention computation. This approach allows PVT to capture both low-level details and high-level semantic information, making it a versatile backbone for dense prediction tasks and a direct competitor to CNN-based feature pyramid networks (FPNs). Crucially, PVT maintains the global context modeling strengths of Transformers while significantly reducing computational costs compared to the original ViT's global attention.

Concurrently, the Swin Transformer [Liu2021] emerged as another highly influential hierarchical Vision Transformer, also designed to overcome the computational and multi-scale limitations of earlier ViTs. While its core mechanism of shifted window-based attention is detailed in Subsection 3.1, it is important to note its parallel contribution to generating multi-scale feature representations. By restricting self-attention to non-overlapping local windows and introducing a shifted window mechanism for cross-window connections, Swin Transformer achieves linear computational complexity and a hierarchical feature pyramid. This design has proven exceptionally effective across a wide array of dense prediction tasks, including object detection [xiong2022ec2, xiang2024tww] and semantic segmentation, and even low-level vision tasks like image restoration [Liang2021v6x] and monocular depth estimation [li2024qva], underscoring the versatility of such hierarchical designs.

Beyond PVT and Swin, other architectures have explored diverse strategies for generating multi-scale features. The Hierarchical Vision Transformer (HiViT) [Zhang2022msa, ryali202339q] further refines hierarchical ViT designs, demonstrating advantageous performance, particularly in self-supervised pre-training methods like masked image modeling. HiViT emphasizes architectural simplicity, arguing that many "bells-and-whistles" added to hierarchical ViTs are unnecessary when combined with strong pre-training, leading to faster and more accurate models. In contrast to fixed pyramid structures, the Deformable Attention Transformer (DAT++) [xia2023bp7] introduces a novel deformable multi-head attention module. This mechanism adaptively allocates key and value positions in a data-dependent way, allowing the model to dynamically focus on relevant regions and overcome the data-agnostic nature of handcrafted attention patterns in some pyramid designs. This offers a more flexible approach to capturing multi-scale and long-range relationships.

Interestingly, some research has challenged the necessity of *inherently hierarchical* ViT backbones for dense prediction. ViTDet [li2022raj] demonstrates that a simple feature pyramid can be effectively built from a *plain*, single-scale Vision Transformer feature map, especially when the ViT is pre-trained with Masked Autoencoders (MAE). This suggests that the rich representations learned by plain ViTs can be adapted for multi-scale tasks with minimal architectural modifications, such as using window attention with few cross-window propagation blocks. This approach simplifies the backbone design while achieving competitive results in object detection.

Furthermore, hybrid approaches have emerged, combining the strengths of ViTs with traditional CNN-based FPNs to enhance multi-scale feature learning. For instance, the Feature Pyramid Vision Transformer (FPViT) [liu2022c56] integrates Transformers with a ResNet backbone and a feature pyramid, allowing the Transformers to capture global contexts from CNN-extracted features while leveraging multi-scale maps for better adaptability in medical image classification. Similarly, Transformer-Based YOLOX [panboonyuen2021b4h] employs a pre-trained ViT as a backbone and integrates an FPN decoder to effectively aggregate multi-level features for object detection, demonstrating superior performance on challenging datasets. Other specialized models like the Visual Saliency Transformer (VST) [Liu2021jpu] leverage multi-level token fusion and upsampling within a pure transformer framework to generate high-resolution saliency maps, while SENet [Hao202488z] incorporates a local information capture module to compensate for the patch-level attention mechanisms in pixel-level tasks like camouflaged object detection and salient object detection, further reinforcing the critical need for fine-grained multi-scale features.

In conclusion, the development of pyramid Vision Transformer architectures, spearheaded by models like PVT and Swin Transformer, represents a significant evolution in computer vision. By ingeniously designing hierarchical structures and employing efficient attention mechanisms, these models have successfully addressed the critical need for multi-scale feature representation and improved computational efficiency. This has expanded ViT's applicability far beyond simple image classification, enabling them to serve as versatile and powerful backbones for a broad spectrum of dense prediction and other complex vision tasks, directly challenging the long-standing dominance of CNNs. The ongoing research continues to explore novel attention mechanisms, architectural simplifications, and hybrid integrations to further enhance their efficiency and adaptability across diverse applications.
\subsection{Multi-Scale and Efficient Attention Mechanisms}
\label{sec:3_3_multi-scale__and__efficient_attention_mechanisms}


The initial promise of Vision Transformers (ViTs) was significantly constrained by their quadratic computational complexity with respect to image resolution and their inherent limitations in capturing multi-scale features, which are indispensable for dense prediction tasks. While hierarchical architectures like the Swin Transformer [Swin] and Pyramid Vision Transformer (PVT) [PVT] (as discussed in Sections 3.1 and 3.2, respectively) laid foundational groundwork by introducing window-based and spatial reduction attention for linear complexity and multi-scale feature generation, subsequent research has delved deeper into refining the core attention mechanism itself to further enhance efficiency and multi-scale processing capabilities. This continuous effort is crucial for enabling ViTs to serve as versatile and deployable backbones across diverse vision applications.

A primary avenue of innovation has focused on optimizing spatial attention to balance global context and local detail more effectively, often by intelligently structuring local interactions. \textit{Multiscale Vision Transformers} (MViT) [MViT] addressed the challenge of scaling to higher resolutions by progressively expanding channel dimensions while reducing spatial resolution within a hierarchical structure, enabling robust performance on high-resolution inputs, including video. This approach efficiently aggregates information across scales. Complementing this, the \textit{Twins} architecture [Twins] refined spatial attention by explicitly combining both global and local attention mechanisms. This hybrid strategy aims to leverage the benefits of fine-grained local processing and broad contextual understanding, thereby creating a robust general-purpose backbone. Further expanding on localized attention, the \textit{CSWin Transformer} [CSWin] introduced cross-shaped window attention, which captures richer contextual information more efficiently than square windows by attending to features along horizontal and vertical strips. Similarly, \textit{RegionViT} [RegionViT] proposed a regional-to-local attention strategy, allowing efficient processing of large images by first attending to broader regional features and then refining with local attention. This hierarchical attention within a single block effectively balances computational cost with comprehensive feature extraction. \textit{Focal Attention} [Focal] also contributes to this theme by efficiently capturing both fine-grained local details and broader contextual information, proving particularly beneficial for tasks like document understanding where varied scales of information are critical. These methods collectively demonstrate a trend towards more sophisticated, yet computationally constrained, local-global attention interactions, moving beyond simple windowing to more adaptive spatial sampling.

Beyond static windowing, a significant advancement in efficient attention involves making the attention mechanism data-dependent and dynamic. \textit{Deformable Attention Transformers} (DAT) [xia2022qga] and its enhanced version DAT++ [xia2023bp7] introduced a novel deformable multi-head attention module. Unlike fixed-grid or window-based attention, deformable attention adaptively allocates the positions of key and value pairs in a data-dependent manner. This flexible scheme allows the model to dynamically focus on relevant regions, overcoming the limitations of handcrafted attention patterns (like those in Swin or PVT) and maintaining the representational power of global attention while significantly reducing computational cost and memory usage. This approach represents a critical step towards more intelligent and adaptive attention mechanisms that can dynamically adjust their receptive fields based on image content, thereby enhancing both efficiency and feature discriminability.

Another crucial direction for efficiency has been the development of linear attention mechanisms and strategies for reducing spatial redundancy. Traditional self-attention's quadratic complexity stems from the softmax operation and the dense interaction matrix. \textit{UFO-ViT} [song20215tk] proposed a novel self-attention mechanism with linear complexity by eliminating non-linearity and factorizing the matrix multiplication without complex linear approximations. This direct approach to linear scaling offers substantial computational benefits, especially for high-resolution inputs. Concurrently, methods focusing on reducing spatial redundancy by pruning or merging tokens have gained traction. The \textit{Localization and Focus Vision Transformer} (LF-ViT) [hu202434n] strategically curtails computational demands by processing a reduced-resolution image in a "Localization" phase. If a definitive prediction is elusive, it triggers a Neighborhood Global Class Attention (NGCA) mechanism to identify and spotlight class-discriminative regions from the original image in a "Focus" phase. This selective processing significantly reduces FLOPs. Similarly, \textit{CP-ViT} [song2022603] introduced a cascade pruning framework that dynamically predicts sparsity in ViT models, progressively pruning uninformative patches and heads. By defining a cumulative score and adjusting pruning ratios based on layer-aware attention range, CP-ViT achieves substantial FLOPs reduction with minimal accuracy loss. Furthermore, \textit{LTM-Transformer} [wang2024ueo] proposes a novel block with Learnable Token Merging (LTM), which reduces FLOPs and inference time by merging tokens in a learnable scheme, compatible with various existing Transformer networks. These token-level optimization strategies demonstrate that efficiency can be gained not just by altering the attention computation itself, but also by intelligently reducing the amount of information that needs to be processed.

The broader landscape of efficient hierarchical designs also continues to evolve. \textit{Hiera} [ryali202339q] exemplifies how architectural simplicity, when combined with strong self-supervised pretraining (like MAE), can yield highly efficient and accurate hierarchical Vision Transformers. By stripping away many "bells-and-whistles" commonly added for supervised performance, Hiera achieves faster inference and training while maintaining competitive accuracy, suggesting that computational overhead can be reduced through a holistic approach encompassing both architectural design and pre-training strategy.

In conclusion, the continuous pursuit of multi-scale and efficient attention mechanisms has been paramount for the practical deployment of Vision Transformers [heidari2024d9k]. From sophisticated spatial attention refinements that balance local and global context [MViT, Twins, CSWin, RegionViT, Focal], to dynamic and data-dependent mechanisms like deformable attention [xia2022qga, xia2023bp7], and radical approaches that achieve linear complexity or reduce spatial redundancy through token pruning/merging [song20215tk, hu202434n, song2022603, wang2024ueo], the field strives to balance the expressive power of global attention with computational feasibility. Unresolved issues include finding the optimal trade-off between incorporating inductive biases (like locality) and maintaining the flexibility of pure attention, as well as exploring novel attention-free or highly simplified architectural components that can further reduce computational overhead without sacrificing performance.


### Self-Supervised Learning Paradigms for Vision Transformers

\section{Self-Supervised Learning Paradigms for Vision Transformers}
\label{sec:self-supervised_learning_paradigms_for_vision_transformers}



\subsection{Masked Image Modeling (MIM) for Representation Learning}
\label{sec:4_1_masked_image_modeling_(mim)_for_representation_learning}


Masked Image Modeling (MIM) has emerged as a highly effective self-supervised learning paradigm for Vision Transformers (ViTs), drawing direct inspiration from the success of BERT in Natural Language Processing (NLP). This approach trains a ViT to learn rich visual representations by reconstructing masked portions of an image, effectively predicting missing pixels or discrete visual tokens. By forcing the model to infer global context from partial observations, MIM significantly reduces the reliance on extensive human annotations, enabling ViTs to leverage vast amounts of unlabeled data for pre-training. A key distinction often drawn is that MIM's generative reconstruction task encourages learning rich, localized features, whereas contrastive methods (discussed in Section 4.2) tend to yield more globally semantic, linearly separable features.

One of the pioneering works in this domain is [beit], which introduced BEiT, a BERT-like pre-training framework for image Transformers. BEiT tokenizes images into discrete visual tokens using a discrete Variational AutoEncoder (dVAE) and then trains a standard ViT encoder to predict these tokens for randomly masked image patches. This method demonstrated that a masked language modeling objective could be effectively transferred to the visual domain, yielding strong representations for various downstream tasks. The discrete token prediction objective forces the model to operate at a higher level of semantic abstraction, potentially leading to more semantically meaningful feature learning by abstracting away low-level pixel noise.

Building upon this foundation, [mae] presented Masked Autoencoders (MAE), a highly scalable and efficient MIM approach. MAE trains a Vision Transformer encoder to reconstruct the original raw pixel values of masked image patches. A key innovation of MAE is its asymmetric encoder-decoder architecture: only the visible patches are fed into the encoder, while a lightweight decoder reconstructs the missing pixels from the encoder's output and the masked token embeddings. This design, coupled with a remarkably high masking ratio (e.g., 75\%), forces the model to learn a deep understanding of image semantics and global context, leading to impressive scalability and performance. The simplicity of reconstructing raw pixels, rather than discrete tokens, further streamlines the pre-training pipeline by removing the need for a dVAE tokenizer, a notable trade-off compared to BEiT.

Beyond these foundational works, the MIM landscape has diversified. SimMIM [simmim] demonstrated that a simple linear head for pixel reconstruction could be highly effective, simplifying the decoder design even further. MaskFeat [maskfeat] explored predicting hand-crafted features like Histograms of Oriented Gradients (HOG) instead of raw pixels, suggesting that the reconstruction target itself can be varied to influence learned representations.

A significant challenge for MIM, particularly MAE, has been its application to hierarchical Vision Transformers (e.g., Swin Transformer, PVT) which incorporate local window-based attention. The original MAE design, relying on the global attention of plain ViTs to handle randomly masked sequences, struggled with the local inductive biases of hierarchical architectures. To address this, HiViT [zhang2022msa] proposed a new design for hierarchical ViTs that maintains efficiency and performance in MIM by modifying Swin Transformer to make mask-units serializable. Similarly, Uniform Masking (UM-MAE) [li2022ow4] enabled MAE pre-training for pyramid-based ViTs with locality. UM-MAE introduces a Uniform Sampling strategy that selects one random patch from each 2x2 grid, combined with Secondary Masking, to preserve equivalent elements across local windows, significantly improving pre-training efficiency and transferability for these architectures.

The effectiveness of MIM, particularly MAE, lies in its ability to learn robust and transferable representations. For instance, pre-trained MAE backbones have been shown to be highly effective for downstream tasks like object detection. [li2022raj] demonstrated that plain, non-hierarchical ViT backbones pre-trained with MAE could achieve competitive results in object detection (ViTDet) with minimal architectural adaptations, even outperforming methods based on hierarchical backbones. Similarly, [ryali202339q] leveraged MAE pre-training to simplify hierarchical ViT architectures, showing that a strong visual pretext task can allow for the removal of many "bells-and-whistles" without sacrificing accuracy, resulting in faster and more efficient models like Hiera. This highlights MIM's capacity to imbue even simpler ViT designs with powerful representational capabilities. Furthermore, MIM can serve as a powerful auxiliary task; MAT-VIT [han2024f96] uses an MAE-based self-supervised auxiliary task to improve medical image classification, effectively leveraging both unlabeled and labeled data. In the context of deep reinforcement learning from pixels, reconstruction-based ViT methods, including MIM, have been found to significantly outperform ViT contrastive-learning approaches, suggesting their utility in learning rich visual states for control tasks [tao2022gdr].

While MIM has proven highly effective in self-supervised pre-training, the computational cost associated with training these large models, even with efficient designs like MAE's asymmetric architecture, remains a significant consideration. The optimal masking strategy, including masking ratio and pattern, continues to be an active area of research to maximize learning efficiency and representation quality. For example, BEiT v2 explores more sophisticated block-wise masking schemes to encourage learning richer semantics. Future work is investigating more adaptive masking techniques or hybrid approaches that combine MIM's generative loss with a discriminative loss on the visible tokens to capture both local detail and high-level semantics. The trade-off between pixel-level and discrete token-level reconstruction also remains a topic of investigation, as each approach offers distinct advantages in terms of simplicity, efficiency, and the semantic richness of the learned features.
\subsection{Self-Distillation and Contrastive Learning without Labels}
\label{sec:4_2_self-distillation__and__contrastive_learning_without_labels}


The inherent data hunger of Vision Transformers (ViTs) for pre-training with vast, labeled datasets has spurred significant research into self-supervised learning (SSL) paradigms. These approaches enable ViTs to acquire robust and semantically meaningful representations from unlabeled data, thereby mitigating the bottleneck of costly human annotation. Within SSL, two prominent and often intertwined methodologiescontrastive learning and self-distillationhave demonstrated exceptional efficacy, revealing profound emergent properties in ViT features that underscore their intrinsic capacity for visual understanding.

Contrastive learning, a foundational SSL paradigm, operates by maximizing the agreement between different augmented views of the same image (positive pairs) while simultaneously pushing apart representations of different images (negative pairs) in the embedding space. Early adaptations for ViTs, such as MoCo-v3 [chen2021mocov3], successfully integrated the momentum encoder concept with contrastive objectives, demonstrating that ViTs could learn powerful representations competitive with CNNs on ImageNet. Similarly, frameworks inspired by SimCLR [chen2020simple] were adapted, emphasizing the importance of large batch sizes or memory banks for effective negative sampling. While effective, the reliance on explicit negative pairs and the associated computational overhead or architectural complexities (e.g., large memory banks or distributed training for large batches) presented practical challenges for scaling and maintaining training stability, motivating the search for alternative non-contrastive approaches.

In response to these complexities, non-contrastive self-supervised methods emerged, circumventing the explicit need for negative pairs altogether. A pioneering work in this category is BYOL (Bootstrap Your Own Latent) [grill2020bootstrap], which demonstrated that high-quality representations could be learned by simply predicting the representation of one augmented view from another. BYOL employs a momentum encoder for the teacher network and a crucial predictor head on the student branch. The combination of the predictor network and a stop-gradient operation applied to the teacher's output was instrumental in preventing representational collapse, ensuring the student learned meaningful, non-trivial features. Following BYOL, SimSiam [chen2021exploring] further simplified non-contrastive learning by showing that even without a momentum encoder, a stop-gradient operation alone, when applied to one branch of a Siamese network, could effectively prevent collapse, making the training process more straightforward and efficient.

Building upon these non-contrastive principles, DINO (Self-Distillation with No Labels) [DINO] emerged as a seminal work specifically tailored for ViTs. DINO employs a student-teacher architecture where a student network is trained to match the output distribution of a teacher network for different augmented views of the same image. Crucially, the teacher network's weights are an exponential moving average of the student's weights (a momentum encoder), providing a stable yet evolving target. To prevent representational collapse, a common challenge in non-contrastive methods, DINO incorporates techniques like centering and sharpening the output distributions, which effectively regularize the learning process. This self-distillation encourages the student to learn features invariant to various augmentations and to capture deep semantic information. A remarkable emergent property observed in ViTs trained with DINO is their ability to perform object segmentation without any explicit supervision; the attention maps of the self-attention layers spontaneously highlight object boundaries and coherent semantic regions. This capability illustrates that ViTs can intrinsically learn to parse visual scenes into meaningful entities through unsupervised means, significantly reducing the need for human annotation for tasks like segmentation.

Beyond foundational self-distillation, further refinements and applications have broadened its impact. EsViT [EsViT] extended self-distillation by exploring different view generation strategies and leveraging attention-based distillation to enhance the learning process for ViTs. Furthermore, the concept of distilling knowledge from a self-supervised teacher to a smaller student has gained traction for efficiency and deployment on resource-constrained devices. AttnDistill [wang2022pee] specifically addresses self-supervised knowledge distillation for ViTs by directly distilling information from the crucial attention mechanism of a teacher to a student. This method demonstrates that by guiding the student with the teacher's attention, the performance gap between models can be significantly narrowed, enabling the deployment of high-performing ViTs on memory and compute-constrained devices, even down to tiny ViT models. Similarly, distillation principles are leveraged in contexts like ViT quantization, where a teacher-student framework can rectify issues such as attention distortion in binarized ViTs, improving their performance on resource-limited devices [li20238ti]. While this application of distillation focuses on model compression and enhancing the utility of already learned representations rather than initial self-supervised representation learning, it underscores the versatility of the teacher-student paradigm in improving ViT practicality.

A significant evolution in this space involves the convergence of self-distillation with masked image modeling (MIM), bridging concepts from this subsection with those discussed in Section 4.1. iBOT (Image BERT Pre-training with Online Tokenizer) [zhou2022ibot] exemplifies this hybrid approach. iBOT combines the self-distillation framework of DINO with masked image modeling, where a student ViT is trained to predict the output of a teacher ViT for masked image patches. This dual objective allows iBOT to leverage the strengths of both paradigms: the global context learning from MIM and the robust feature learning from self-distillation, leading to highly effective visual representations that exhibit both fine-grained detail and strong semantic understanding. Such self-supervised techniques are also being adapted for domain-specific applications, such as medical image analysis, where models like MAT-VIT explore MAE-based self-supervised auxiliary tasks within a Vision Transformer framework to leverage abundant unlabeled medical images, improving classification performance in data-scarce scenarios [han2024f96].

In summary, self-distillation and contrastive learning paradigms have been instrumental in unlocking the full potential of Vision Transformers by enabling them to learn from vast amounts of unlabeled data. While contrastive methods like MoCo-v3 provided early successes, the subsequent development of non-contrastive self-distillation methods such as BYOL, SimSiam, and DINO simplified the training process by avoiding explicit negative pairs, often leading to more stable learning and revealing remarkable emergent properties like unsupervised object segmentation. The continuous refinement of distillation techniques for efficiency and the development of hybrid approaches like iBOT, which integrate self-distillation with masked image modeling, further enhance the representational capacity and practical applicability of ViTs. These advancements not only drastically reduce the reliance on extensive human annotation but also highlight the profound inherent representational capacity of Transformer architectures for visual data, paving the way for more data-efficient, generalizable, and universally applicable vision models.
\subsection{Scaling Self-Supervised ViTs to Foundation Models}
\label{sec:4_3_scaling_self-supervised_vits_to_foundation_models}


The landscape of computer vision is undergoing a profound transformation with the emergence of "Vision Foundation Models," a paradigm driven by the aggressive scaling of self-supervised Vision Transformers (ViTs) to unprecedented sizes. Drawing inspiration from large language models, a foundation model is broadly defined as a large model pre-trained on a vast quantity of diverse data, designed to be adaptable to a wide range of downstream tasks [bommasani2021opportunities]. In vision, this translates to developing highly robust, general-purpose visual backbones capable of transferring effectively across an extremely broad spectrum of tasks with minimal fine-tuning, thereby significantly reducing the need for task-specific model development and accelerating progress towards universal visual intelligence.

The initial success of ViTs [ViT] in image recognition, despite their significant data hunger, underscored the potential of Transformer architectures for visual data. This limitation spurred extensive research into self-supervised learning (SSL) techniques, which became indispensable for enabling ViTs to scale by leveraging vast quantities of unlabeled data. As discussed in previous subsections, Masked Image Modeling (MIM) [MAE, BEiT] and self-distillation methods like DINO [DINO] proved instrumental in allowing ViTs to learn rich, generalizable representations without explicit human annotations. Building on these foundational SSL advancements, the field has now entered an era of aggressive scaling, pushing model capacities and data volumes to new extremes.

A prime illustration of this scaling trend is the work by [ICLR2023], which systematically explored scaling Vision Transformers to up to a billion parameters. This research demonstrated that with sufficient model capacity and effective self-supervised pre-training, particularly MIM, ViTs can learn exceptionally powerful and transferable representations. Crucially, this scaling revealed emergent properties and improved scaling laws, where performance gains continue with increasing model size and data, setting new benchmarks for general-purpose visual backbones. These models are typically pre-trained on massive, diverse, and often curated web-scale datasets, far surpassing the scale of traditional benchmarks like ImageNet. Complementing this, [ICLR2023_DINOv2] pushed the boundaries of self-supervised learning further, focusing on learning highly robust and generalizable visual features without any supervision. DINOv2's success lies in its ability to produce features that are readily usable for a wide array of downstream tasks, significantly reducing the need for task-specific labeled data and extensive fine-tuning. This approach exemplifies how refined SSL strategies, combined with large-scale pre-training, enable models to learn intrinsic visual understanding, often exhibiting remarkable emergent properties such as object segmentation without explicit supervision. Furthermore, research like Hiera [ryali202339q] demonstrates that strong visual pretext tasks, such as MAE, can simplify hierarchical ViT designs, allowing for the removal of architectural "bells-and-whistles" while maintaining or improving accuracy and efficiency post-pre-training, suggesting the power often lies more in the robust SSL pre-training strategy than in architectural complexity alone.

However, this aggressive scaling is not without significant challenges and costs. The engineering hurdles associated with training such colossal models, including distributed computing, memory optimization, and stable optimization techniques, are substantial. Moreover, the exorbitant computational costs and environmental impact of training billion-parameter models raise concerns about accessibility and sustainability, creating a potential barrier to entry for academic research. To mitigate these issues, research into more efficient architectures and compression techniques is vital. For instance, approaches like UFO-ViT [song20215tk] propose linear complexity self-attention mechanisms to alleviate the quadratic computational burden, while DeepViT [zhou202105h] addresses attention collapse in deeper models through "Re-attention" to enable more effective scaling. Furthermore, structured pruning methods like GOHSP [yin2023029] and multi-dimensional compression paradigms [hou2022ver] aim to reduce the model size and computational cost of ViTs for practical deployment without significant accuracy loss, making the benefits of foundation models more accessible.

The hallmark of these Vision Foundation Models is their exceptional transferability and generalization capabilities. A systematic investigation by [zhou2021rtn] revealed consistent advantages of Transformer-based backbones over ConvNets in transfer learning across a majority of downstream tasks, including fine-grained classification, scene recognition, and open-domain classification. This inherent transferability is amplified at the foundation model scale. For instance, Prompt Vision Transformers [zheng202218g] leverage prompt learning to embed domain-specific knowledge, enabling ViTs to generalize effectively to unseen domains. Similarly, Transferable Vision Transformers (TVT) [yang2021myb] demonstrate superior generalization ability and can be further optimized for unsupervised domain adaptation by focusing on transferable and discriminative features through specialized modules. The concept of attention distillation [wang2022pee] further extends the utility of these large foundation models by enabling the transfer of learned knowledge, particularly from attention mechanisms, to smaller student ViTs, making the benefits of scale accessible to more resource-constrained deployments.

The impact of these Vision Foundation Models is transformative across various applications, primarily by providing highly robust, general-purpose visual backbones that significantly reduce the need for developing task-specific models from scratch. Their ability to learn rich, semantically meaningful features from vast, diverse data makes them uniquely suited for domains with distinct data characteristics or limited labeled data. For example, in medical imaging, models leveraging foundation backbones can achieve high accuracy in tasks like white blood cell classification [katar202352u] or even medical image classification with MAE-based auxiliary tasks [han2024f96], often with minimal task-specific data and fine-tuning, demonstrating their strong transferability to data-scarce domains. In autonomous systems, these powerful backbones can be adapted for critical tasks such as traversable area detection [urrea20245k4], leveraging their robust feature learning for complex environmental understanding. Their capacity to discern subtle visual artifacts and latent data distributions also makes them invaluable for challenging adversarial applications like deepfake detection [deressa2023lrl]. Furthermore, their general-purpose nature extends to tasks like camouflaged and salient object detection, where a simple ViT-based encoder-decoder can yield competitive results across both distinct tasks [hao202488z], and to remote sensing image classification, where quantitative regularization can enhance ViT performance even with limited training samples [song2024fx9].

In conclusion, the scaling of self-supervised ViTs to foundation models represents a monumental achievement in computer vision. This trend, driven by advanced SSL techniques like MIM and self-distillation, combined with massive model capacities and diverse unlabeled datasets, is creating highly robust and general-purpose visual backbones. These models offer unprecedented generalization and transferability, significantly streamlining the development of high-performance vision systems across a multitude of tasks. While offering immense potential, future research will continue to focus on further enhancing their efficiency, exploring novel architectural refinements, and critically addressing the ethical considerations inherent in such broadly applicable and powerful AI systems, particularly concerning bias amplification from vast, uncurated web-scale data.


### Hybrid Architectures and Beyond Self-Attention

\section{Hybrid Architectures and Beyond Self-Attention}
\label{sec:hybrid_architectures__and__beyond_self-attention}



\subsection{Integrating Convolutional Inductive Biases into Transformers}
\label{sec:5_1_integrating_convolutional_inductive_biases_into_transformers}


While Vision Transformers (ViTs) have demonstrated impressive capabilities in capturing global dependencies, their initial lack of inherent inductive biases, such as translation equivariance and locality, often necessitates vast training data and can lead to sub-optimal performance on smaller datasets or for fine-grained local tasks [dosovitskiy2020image, touvron2021training]. To address these limitations, a significant research direction has focused on hybrid architectures that explicitly embed convolutional layers or integrate convolutional inductive biases within the Transformer framework, aiming to synergistically combine the local feature extraction strengths of Convolutional Neural Networks (CNNs) with the global reasoning capabilities of Transformers. This synergistic integration often leads to improved efficiency, better performance, and enhanced robustness, particularly on smaller datasets where pure ViTs might struggle due to their lack of inherent inductive priors. These hybrid designs represent a pragmatic effort to leverage the best attributes of both paradigms, moving beyond a 'CNN vs. Transformer' dichotomy towards more powerful combined architectures.

Early efforts to imbue Transformers with locality often explored architectural modifications without direct convolutions. For instance, the Swin Transformer [liu2021ljs] introduced a hierarchical architecture that limits self-attention computation to non-overlapping local windows, with shifted windowing enabling cross-window connections. While this design provides a form of local inductive bias and improves efficiency for dense prediction tasks, it achieves locality through windowing rather than explicit convolutional operations. In contrast, other foundational hybrid models directly integrated convolutions. CoaT (Co-scale Conv-Attentional Image Transformers) [xu2021coat] integrates convolution and attention at co-scales, allowing for a dynamic interplay between local and global features. CvT (Introducing Convolutions to Vision Transformers) [wu2021cvt] explicitly embeds convolutions into the Transformer architecture by replacing the linear patch embedding with a strided convolution and using depthwise-separable convolutions for the key, query, and value projection layers within the self-attention mechanism. This approach leverages convolutions for efficient tokenization and local feature aggregation directly within the attention process. ConViT (Improving Vision Transformers with Soft Convolutional Inductive Biases) [d2021convit] introduces "soft convolutional biases" by initializing the attention mechanism to prioritize local neighborhoods, gradually expanding its receptive field in deeper layers, thereby mimicking the inductive bias of convolutions without hard-coding them. LeViT (a Vision Transformer in ConvNet's Clothing for Faster Inference) [graham2021levit] further blurs the lines by optimizing ViTs for speed through the incorporation of attention bias and convolution-like structures, demonstrating that careful architectural design can yield benefits traditionally associated with CNNs.

The integration strategies for convolutional inductive biases can be broadly categorized into several architectural patterns. One common approach involves embedding convolutional layers directly within Transformer blocks or using them for initial feature extraction. For example, some models utilize convolutions for the initial patch embedding, similar to CvT, or integrate them into feed-forward networks (FFNs) to enhance local feature mixing. Another prominent pattern involves parallel CNN and ViT streams, where each branch specializes in different aspects of feature extraction before their outputs are fused. This allows the CNN branch to capture fine-grained local details and translation equivariance, while the Transformer branch focuses on global contextual relationships. For instance, CTNet [deng2021man] proposes a joint framework with separate CNN and ViT streams to extract local structural and global semantic features, respectively, for remote sensing scene classification, fusing them for comprehensive understanding. Similarly, GLNS [liu2022249] for high-resolution SAR image classification utilizes a lightweight CNN and a compact ViT in parallel, fusing their outputs to leverage complementary local and global features.

Beyond full architectural integration, convolutional inductive biases can also be introduced through lightweight adaptation modules or specialized components. Convpass [jie20220pc] proposes "Convolutional Bypasses" as plug-and-play adaptation modules for pre-trained ViTs. These bypasses inject convolutional layers, benefiting from their hard-coded inductive bias, particularly in low-data regimes, without altering the original ViT parameters. This method offers a parameter-efficient way to enhance ViTs with local priors. For pixel-level tasks like camouflaged and salient object detection, SENet [hao202488z] incorporates a "local information capture module" within its ViT-based encoder-decoder structure. This module is specifically designed to compensate for the limitations of patch-level attention in capturing fine-grained local details, which are crucial for precise pixel-level predictions.

These hybrid strategies have proven particularly effective in specialized domains where data scarcity or the need for robust local features is paramount. In remote sensing, for example, the fusion of local and global information is critical for tasks like land use and land cover classification or hyperspectral image analysis. Several works adopt parallel or integrated convolutional modules to enhance local feature extraction and reduce reliance on massive pre-training. P2FEViT [wang202338i] introduces a plug-and-play CNN feature embedding into ViT, allowing synchronous capture and fusion of global context with local multimodal information. ExViT [yao2023sax] extends conventional ViTs for multimodal land use and land cover classification by processing image patches with parallel branches of position-shared ViTs augmented with separable convolution modules, fusing their embeddings via cross-modality attention. GSC-ViT [zhao2024671] for hyperspectral image classification employs a groupwise separable convolution (GSC) module to efficiently capture local spectral-spatial information and a groupwise separable multihead self-attention (GSSA) module for both local and global spatial feature extraction, significantly reducing parameters. Furthermore, the lightweight dual-branch Swin Transformer (LDBST) [zheng202325h] combines a ViT branch with a CNN branch, integrating a Conv-MLP structure into the ViT branch to enhance connections with neighboring windows, showcasing the versatility of these hybrid designs.

In conclusion, the integration of convolutional inductive biases into Transformers represents a pragmatic and effective strategy to overcome the limitations of pure ViTs, particularly concerning data efficiency, local feature extraction, and robustness on diverse datasets. These hybrid designs, ranging from embedding convolutional layers directly within Transformer blocks and attention mechanisms to employing parallel CNN-ViT streams and lightweight convolutional adaptation modules, successfully combine the local feature extraction strengths of CNNs with the global reasoning capabilities of Transformers. This synergistic approach has consistently led to improved performance and enhanced robustness across various vision tasks, especially in scenarios with limited data or requiring fine-grained local understanding. Future research will likely continue to explore more sophisticated and dynamic integration strategies, a deeper theoretical understanding of their combined inductive biases, and the optimal balancing of computational cost with performance gains across diverse applications, moving beyond a simplistic 'CNN vs. Transformer' dichotomy towards more powerful and versatile combined architectures.
\subsection{Rethinking Token Mixing: Alternatives to Self-Attention}
\label{sec:5_2_rethinking_token_mixing:_alternatives_to_self-attention}

While self-attention mechanisms have been instrumental in the success of Vision Transformers (ViTs) by providing global receptive fields, their quadratic computational complexity with respect to sequence length and their initial lack of inherent inductive biases have stimulated extensive research into simpler, more computationally efficient alternatives for global token mixing. This line of inquiry, often unified under the "MetaFormer" architectural paradigm [yu2022metaformer], provocatively suggests that the overall designcomprising a token mixer followed by a feed-forward network (FFN)can be highly effective even with substantially simpler mixing operations, challenging the notion that complex self-attention is indispensable for achieving state-of-the-art performance in vision.

Among the earliest and most influential works to question the necessity of self-attention were those proposing MLP-based mixers. The \textit{MLP-Mixer} [tolstikhin2021mlp] demonstrated that a pure multi-layer perceptron (MLP) architecture, without any explicit attention mechanism, could achieve competitive performance on image classification tasks. This model segments an image into patches, then applies channel-mixing MLPs and token-mixing MLPs alternately. The token-mixing MLP operates across all patches, effectively providing a global receptive field through fully connected layers, albeit with a high computational cost if not carefully implemented. Following this, \textit{gMLP} [liu2021pay] further refined the MLP-only approach by introducing a Spatial Gating Unit (SGU) that adaptively controls information flow across spatial locations. Both MLP-Mixer and gMLP were foundational in establishing that the general Transformer-like block structure, rather than the specific self-attention operation, was a key driver of performance, paving the way for further exploration of non-attention mixers.

Building on the surprising efficacy of simpler operations, subsequent research explored even more basic alternatives. The \textit{PoolFormer} architecture [PoolFormer] famously demonstrated that simple pooling operations, such as average or max pooling, could effectively replace self-attention layers within a MetaFormer block while achieving competitive performance. This finding underscored that a parameter-free, local aggregation operation, when stacked within the MetaFormer framework, could surprisingly capture sufficient global context for robust image representation. In a different vein, Global Filter Networks (GFNet) [GFNet] proposed replacing self-attention with global filters applied in the frequency domain, specifically using 2D Fourier transforms. This approach offers a computationally efficient way to achieve global receptive fields by treating tokens as a signal and performing global mixing via frequency-domain multiplication, thereby avoiding the quadratic complexity of self-attention while providing a distinct mechanism for global interaction.

Beyond parameter-free pooling or frequency-domain filters, other works have explored structured or localized alternatives that retain some global interaction without full self-attention. The Vision Permutator (ViP) [ViP] introduced a permutable self-attention mechanism, which simplifies the attention process by performing attention along different axes (height and width) sequentially, rather than across all tokens simultaneously. While still using an attention-like mechanism, its structured permutation significantly reduces computational cost and provides a form of global information exchange with linear complexity. Focal Modulation Networks (FocalNet) [FocalNet] proposed focal modulation, a novel spatial modulation mechanism that captures both fine-grained local context and long-range dependencies in a hierarchical manner. Unlike self-attention, focal modulation explicitly models interactions at different scales through a series of modulators, providing an efficient way to aggregate information across varying receptive fields without the explicit pairwise token comparison of attention.

The exploration of alternatives continues with emerging paradigms, such as state-space models (SSMs). Recent work like \textit{MambaVision} [hatamizadeh2024xr6] proposes a hybrid Mamba-Transformer backbone, where the Mamba architecture, based on structured state-space sequences, is adapted for visual feature modeling. MambaVision demonstrates the feasibility of integrating SSMs as a distinct, non-attention-based token mixer, particularly in its initial layers, to efficiently capture long-range spatial dependencies. This represents a significant new direction, suggesting that sequence modeling mechanisms beyond attention can be effectively adapted for vision, potentially offering superior efficiency and inductive biases for certain tasks.

In conclusion, the research on rethinking token mixing fundamentally challenges the premise that self-attention is an indispensable component of high-performing vision models. Works like [tolstikhin2021mlp, liu2021pay, PoolFormer, GFNet, ViP, FocalNet] have collectively demonstrated that simpler, more efficient operationsranging from basic MLPs and pooling to frequency-domain filters, structured permutations, and focal modulationcan effectively replace self-attention within the robust MetaFormer paradigm. These models achieve competitive performance while significantly reducing computational overhead, pushing the boundaries of efficiency and architectural simplicity. The emergence of state-space models further diversifies this landscape, indicating a continuous quest for novel token mixing strategies. This shift suggests that the general block-based, token-processing architecture of Transformers is a robust design, capable of leveraging diverse mixing strategies. The ongoing challenge lies in balancing the theoretical expressiveness and adaptive capacity of full self-attention with the practical demands for computational efficiency, hardware friendliness, and inherent inductive biases offered by these increasingly varied alternative token mixing approaches.
\subsection{Modernizing CNNs with Vision Transformer Design Principles}
\label{sec:5_3_modernizing_cnns_with_vision_transformer_design_principles}


The emergence of Vision Transformers (ViTs) initially presented a significant challenge to the long-standing dominance of Convolutional Neural Networks (CNNs) in computer vision, primarily due to their superior global context modeling capabilities. However, this perceived architectural dichotomy has evolved into a profound convergence, where insights and design principles from ViTs are now being strategically applied to modernize and revitalize traditional CNNs. This trend effectively blurs the lines, fostering a new generation of powerful convolutional networks that benefit from lessons learned in the Transformer era, thereby challenging the absolute necessity of self-attention for state-of-the-art performance.

A crucial precursor to this modernization was the \textit{Swin Transformer} [liu2021ljs]. While fundamentally a Transformer, Swin's hierarchical architecture, employing shifted window-based attention, introduced a more CNN-like inductive bias by processing local regions and progressively building global interactions. This design demonstrated that Transformers could achieve efficiency and scalability comparable to CNNs, inspiring researchers to investigate whether the performance gains of ViTs stemmed primarily from their attention mechanism or from their broader architectural structure and training methodologies. The Swin Transformer thus served as a blueprint, prompting a re-evaluation of CNN design principles.

The most seminal work exemplifying this modernization is the \textit{ConvNeXt} architecture [liu2022convnext]. The authors embarked on a systematic 'metamorphosis' of a standard ResNet, incrementally incorporating design choices prevalent in state-of-the-art Vision Transformers, particularly those observed in Swin Transformer. This meticulous process involved several key modifications:
\begin{itemize}
    \item \textbf{Macro Design:} The overall stage-wise downsampling and channel ratios were adjusted to align with those of Swin Transformer, moving from a large stem to a patchify stem and adopting similar block repetition patterns.
    \item \textbf{ResNeXt-ification:} Efficient depthwise convolutions and inverted bottleneck designs, characteristic of efficient Transformer blocks, were integrated. This allowed for increased channel capacity within blocks while maintaining computational efficiency.
    \item \textbf{Larger Kernel Sizes:} The kernel size of depthwise convolutions was significantly increased (e.g., from 3x3 to 7x7). This allowed convolutional layers to capture a larger receptive field, mimicking the broader context captured by attention mechanisms without incurring their quadratic computational cost.
    \item \textbf{Layer Normalization:} Batch Normalization layers were replaced with Layer Normalization, a standard practice in Transformers that stabilizes training, especially with smaller batch sizes, and improves generalization.
    \item \textbf{Activation Functions:} The ReLU activation function was substituted with GELU, another activation function widely used in Transformers, contributing to improved performance.
    \item \textbf{Downsampling and Pooling:} Adjustments were made to downsampling layers, and the final classification head adopted a single global average pooling layer, streamlining the network's output stage.
\end{itemize}
By systematically applying these ViT-inspired design principles, ConvNeXt demonstrated that a pure convolutional network could achieve competitive or even superior performance to leading Transformers across various benchmarks, including ImageNet classification, COCO object detection, and ADE20K semantic segmentation. This work critically challenged the prevailing notion that self-attention was indispensable for achieving state-of-the-art results, instead highlighting the profound importance of macro-architectural design and training strategies.

Beyond architectural modifications, the influence of Vision Transformers extends to training methodologies. Transformer-inspired self-supervised learning strategies, particularly Masked Image Modeling (MIM), have proven highly effective for pre-training convolutional networks, further blurring the methodological distinctions. \textit{ConvMAE} [gao2022convmae] directly applied the Masked Autoencoder (MAE) paradigm, originally developed for ViTs, to pure CNN architectures. It demonstrated that CNNs, like Transformers, could learn powerful visual representations by reconstructing masked image patches, achieving significant performance gains on downstream tasks. This showed that the effectiveness of MIM was not exclusive to attention-based models but could also benefit architectures with strong inductive biases like convolutions. Building on this, \textit{ConvNeXt V2} [woo2023convnextv2] further showcased this synergy by pre-training the modernized ConvNeXt architecture using a fully convolutional masked autoencoder (FCMAE). This approach not only boosted ConvNeXt's performance but also solidified the argument that the benefits of Transformer-era training techniques are transferable to well-designed CNNs, leading to more robust and data-efficient convolutional backbones.

The revitalization of ConvNets through ViT design principles has found application in various domains. For instance, \textit{GenConViT} [deressa2023lrl] for deepfake detection leverages both ConvNeXt and Swin Transformer models for robust feature extraction, illustrating how modernized CNNs can synergistically contribute to complex vision tasks alongside hierarchical ViTs. This ongoing convergence suggests that the future of powerful vision backbones will likely be a synthesis of the best elements from both worlds, leading to more versatile and robust models that effectively balance global context modeling with local inductive biases. The optimal balance and specific architectural configurations remain an active area of research, continually pushing the boundaries of what pure convolutional networks can achieve when inspired by Transformer innovations.
\subsection{Efficient and Lightweight Hybrid Designs for Deployment}
\label{sec:5_4_efficient__and__lightweight_hybrid_designs_for_deployment}




### Applications and Domain-Specific Adaptations of Visual Transformers

\section{Applications and Domain-Specific Adaptations of Visual Transformers}
\label{sec:applications__and__domain-specific_adaptations_of_visual_transformers}



\subsection{Object Detection and Semantic Segmentation}
\label{sec:6_1_object_detection__and__semantic_segmentation}


Object detection and semantic segmentation are fundamental dense prediction tasks in computer vision, demanding not only accurate classification but also precise localization and pixel-level understanding of objects within an image. While Convolutional Neural Networks (CNNs) historically dominated these areas, the advent of Vision Transformers (ViTs) has introduced powerful new paradigms, successfully adapting the global context modeling capabilities of Transformers to these intricate visual challenges.

A significant breakthrough in object detection was the introduction of the Detection Transformer (DETR) [carion2020end]. This pioneering work revolutionized the object detection pipeline by proposing an end-to-end approach that directly predicts a set of objects without the need for traditional components like Non-Maximum Suppression (NMS) or anchor boxes. DETR frames object detection as a set prediction problem, leveraging a Transformer encoder-decoder architecture to learn direct set predictions of bounding boxes and class labels, thereby simplifying the overall process. However, early pure ViT architectures, such as the original Vision Transformer [dosovitskiy2021image], faced limitations when directly applied to dense prediction tasks due to their fixed-resolution inputs and lack of inherent hierarchical feature representation, which are crucial for capturing objects at various scales and for fine-grained pixel-level analysis.

To address these challenges, subsequent research focused on developing hierarchical Vision Transformers capable of generating multi-scale feature maps, essential for dense prediction. The Swin Transformer [liu2021swin] emerged as a powerful backbone, introducing a hierarchical architecture built upon shifted windows. This design allows for local self-attention within non-overlapping windows while enabling cross-window connections through shifted window partitioning, thereby efficiently capturing both local and global context and achieving state-of-the-art performance across various dense prediction benchmarks. Similarly, the Pyramid Vision Transformer (PVT) [wang2021pyramid] offered another effective solution by constructing a pure Transformer-based pyramid structure. PVT progressively reduces the resolution of feature maps and aggregates contextual information across different scales, making it a versatile backbone for tasks requiring multi-scale feature extraction, such as object detection and semantic segmentation, often surpassing traditional CNN-based methods in accuracy for complex scene analysis.

Further explorations into the architectural design for dense prediction have challenged some established conventions. For instance, ViTDet [li2022raj] investigated the efficacy of plain, non-hierarchical Vision Transformer backbones for object detection. This work demonstrated that with minimal adaptations for fine-tuning, a simple feature pyramid constructed from a single-scale feature map, combined with window attention and limited cross-window propagation, could achieve competitive results on the COCO dataset. This suggests that the complex hierarchical designs, while effective, might not always be strictly indispensable. Pushing this simplification even further, ShiftViT [wang2022da0] proposed an extremely simple alternative to the attention mechanism itself. By replacing attention layers with parameter-free shift operations that exchange channels between neighboring features, ShiftViT achieved performance on par with or even superior to the Swin Transformer in classification, detection, and segmentation, questioning the fundamental role of attention as the sole key to ViT's success in dense prediction.

Beyond pure Transformer designs, hybrid architectures have also proven effective in dense prediction tasks, combining the strengths of ViTs with those of CNNs or other mechanisms. For semantic segmentation, particularly in challenging environments, models integrating Vision Transformers with Multilayer Perceptrons (MLPs) and CNNs have shown promise. For example, [urrea20245k4] developed bilateral models for traversable area detection in autonomous vehicles, leveraging a hybrid approach to enhance prediction accuracy by capturing distant details more effectively while maintaining real-time operational capabilities. These models demonstrate how combining the global context understanding of ViTs with the local feature extraction and inductive biases of CNNs can lead to robust solutions for pixel-level tasks.

In conclusion, Vision Transformers have profoundly impacted object detection and semantic segmentation, moving from pioneering end-to-end models like DETR to sophisticated hierarchical backbones such as Swin Transformer and PVT that generate multi-scale features crucial for dense prediction. While these advancements have significantly improved accuracy and simplified pipelines, ongoing research continues to explore architectural efficiencies, question the necessity of complex attention mechanisms, and investigate optimal hybrid designs. The field is still actively seeking to balance the computational cost and data efficiency of global attention with the need for precise local detail and the inductive biases traditionally offered by CNNs, paving the way for even more versatile and robust dense prediction models.
\subsection{Medical Image Analysis and 3D Segmentation}
\label{sec:6_2_medical_image_analysis__and__3d_segmentation}


Accurate and robust segmentation of medical images, particularly in three dimensions, is paramount for diagnosis, treatment planning, and surgical guidance. This task presents unique challenges due to the volumetric nature of the data, often limited availability of annotated datasets, and the critical need for fine-grained detail alongside broad anatomical context. While Convolutional Neural Networks (CNNs) have traditionally excelled in medical image analysis, their inherent limitations in capturing long-range dependencies across large fields of view have motivated the exploration of Vision Transformers (ViTs).

The initial success of Vision Transformers in general image recognition, as demonstrated by models like [ViT], paved the way for their adaptation to medical imaging. However, pure Transformer architectures often require extensive datasets and can struggle with capturing fine-grained local details crucial for precise segmentation. This led to the development of hybrid CNN-ViT architectures, which strategically combine the strengths of both paradigms. A foundational step in this direction for medical imaging was \textit{TransUNet} [TransUNet], which integrated a Transformer encoder to capture global contextual information with a CNN decoder for precise localization, showcasing the benefits of this hybrid approach for 2D medical image segmentation.

Extending these concepts to the more complex domain of 3D medical imaging, where volumetric data demands efficient processing of spatial and contextual information, became a critical area of research. Early hybrid models for 3D segmentation, such as \textit{SwinBTS} [jiang2022zcn], leveraged the hierarchical feature extraction capabilities of the 3D Swin Transformer as the encoder and decoder backbone within a U-Net-like structure. This approach aimed to address the CNN's weakness in modeling global and remote semantic information by employing the self-attention mechanism, while still utilizing convolutional operations for efficient upsampling and downsampling. \textit{SwinBTS} demonstrated improved performance in 3D multimodal brain tumor segmentation by focusing on extracting contextual data through the Transformer while enhancing detail feature extraction.

Building upon these advancements, more sophisticated hybrid architectures emerged to further refine the integration of CNNs and ViTs. \textit{Swin Unet3D} [cai2023hji] represents a significant step forward by proposing a novel parallel feature extraction mechanism within each stage of its 3D U-Net-like encoder-decoder. Unlike prior models that might use Transformers as a bottleneck or sequentially, \textit{Swin Unet3D} employs parallel 3D Swin Transformer Blocks and 3D Convolutional Blocks. This allows the network to simultaneously learn both long-range global dependencies (via the Transformer) and short-range local details (via the CNN) throughout the entire network, addressing the limitations of both pure CNNs (limited receptive fields) and pure ViTs (high parameters, poor local detail learning with limited data). The outputs from these parallel branches are then fused, notably through multiplication, to combine their distinct representations effectively. This parallel processing strategy for complementary features echoes architectural innovations seen in other domains, such as the two-pathway approach in \textit{TP-VIT} [jing2022nkb] for video action recognition, which processes different types of information (spatial and temporal) in parallel.

The integration of 3D Swin Transformer Blocks, which utilize a 3D windowed and shifted-window multi-head self-attention mechanism, is crucial for efficiently processing volumetric data in \textit{Swin Unet3D}. This design, coupled with depth-wise separable convolutions for local feature learning, provides a better balance between segmentation accuracy and model parameters, which is vital for clinical deployment. While these hybrid models significantly advance segmentation accuracy, the broader field of Vision Transformers continues to explore efficiency improvements. For instance, \textit{ViT-Shift} [zhang2024g0m], though focused on video action recognition, demonstrates how modules like the Temporal Shift Module can be integrated into ViTs to reduce computational costs while preserving performance, a consideration that remains highly relevant for resource-intensive 3D medical imaging tasks.

In conclusion, the evolution of medical image analysis for 3D segmentation has seen a clear intellectual trajectory from pure CNNs to sophisticated hybrid CNN-ViT architectures. These models, exemplified by \textit{TransUNet} [TransUNet], \textit{SwinBTS} [jiang2022zcn], and particularly \textit{Swin Unet3D} [cai2023hji], effectively combine the local inductive biases of CNNs with the global context modeling capabilities of Transformers. Despite these advancements, challenges persist in optimizing the fusion mechanisms, reducing computational overhead for real-time applications, and developing robust self-supervised learning strategies to mitigate the impact of limited annotated medical datasets. Future research will likely focus on further refining these hybrid designs and exploring more data-efficient training paradigms to achieve even more accurate and clinically viable 3D medical image segmentation solutions.
\subsection{Remote Sensing and Environmental Monitoring}
\label{sec:6_3_remote_sensing__and__environmental_monitoring}


Remote sensing is indispensable for environmental monitoring, providing critical data for observing Earth's surface across vast geographical scales. Vision Transformers (ViTs) have emerged as a transformative technology in this domain, offering robust solutions for analyzing complex satellite and radar imagery, particularly for tasks like land use and land cover (LULC) classification and specialized object detection under challenging conditions. Their inherent ability to capture global contextual information across large geographical areas, coupled with their capacity to model long-range dependencies, makes them particularly well-suited for these tasks, often surpassing traditional convolutional neural network (CNN) methods in accuracy and robustness.

One significant application area is **object detection in Synthetic Aperture Radar (SAR) imagery**, which presents unique challenges due to speckle noise, strong scattering, and multi-scale objects against complex backgrounds. Traditional methods often struggle to maintain performance in such cluttered environments. To address this, [zhao2023rle] introduced ST-YOLOA, a hybrid model that integrates the global context modeling capabilities of the Swin Transformer with the efficient YOLOX framework for real-time SAR ship detection. This approach leverages a novel STCNet backbone, an enhanced PANet with SE and CBAM attention for multi-scale feature fusion, and a decoupled detection head, demonstrating a notable accuracy improvement (e.g., 4.83\% over YOLOX on complex datasets) while maintaining real-time performance. Further enhancing SAR ship detection, [li2022th8] proposed ESTDNet, an enhancement Swin Transformer detection network specifically designed to handle the strong scattering and multi-scale nature of ships in SAR images. ESTDNet employs a Feature Enhancement Swin Transformer (FESwin) module, which aggregates contextual information using CNNs before and after the Swin Transformer, and an Adjacent Feature Fusion (AFF) module to optimize feature pyramids, thereby improving spatial-to-channel feature expression and enhancing recognition and localization capabilities. These specialized ViT adaptations highlight their effectiveness in overcoming the inherent difficulties of SAR data.

In the realm of **Land Use and Land Cover (LULC) classification and semantic segmentation**, ViTs are particularly adept at processing multimodal remote sensing data and capturing intricate spatial patterns. The Swin Transformer, with its hierarchical structure and shifted window attention, has proven highly effective for general LULC tasks, outperforming state-of-the-art CNNs on datasets like EuroSat and NWPU-RESISC45 [jannat20228u6]. Building on this, [meng2022t3x] proposed the Class-Guided Swin Transformer (CG-Swin) for semantic segmentation of remote sensing images, leveraging the Swin Transformer as an encoder and designing a class-guided Transformer block for the decoder. This architecture effectively captures long-range dependencies crucial for accurate pixel-level classification in complex scenes, achieving significant breakthroughs on datasets such as ISPRS Vaihingen and Potsdam. For multimodal LULC classification, which often involves combining hyperspectral (HS), LiDAR, and SAR data, [yao2023sax] presented ExViT, an Extended Vision Transformer framework. ExViT utilizes parallel ViT branches for each modality and a cross-modality attention (CMA) module to facilitate information exchange, demonstrating superior discriminative ability and classification performance compared to traditional methods.

Addressing the unique characteristics of **hyperspectral image classification (HSIC)**, [chen2021d1q] introduced a multi-stage Vision Transformer with stacked samples. This approach tackles the challenge of ViTs sometimes ignoring local characteristics while focusing on global information, and mitigates their data hunger through an innovative data augmentation method. Similarly, for **Polarimetric SAR (PolSAR) land cover classification**, where labeled samples are often scarce, [wang2022n7h] proposed a ViT-based method. This model leverages the ViT's powerful global feature representation and employs a Masked Autoencoder (MAE) for pre-training with unlabeled data, effectively overcoming the data scarcity limitation and demonstrating superior performance on datasets like Flevoland and Hainan. The integration of CNN-like inductive biases with ViTs also proves beneficial for remote sensing image classification (RSIC). [wang202338i] developed P2FEViT, a Plug-and-Play CNN Feature Embedded Hybrid Vision Transformer. P2FEViT integrates CNN features into the ViT architecture to synchronously capture and fuse global context with discriminative local feature representations, thereby improving classification capability, reducing ViT's dependence on large-scale pre-training data, and accelerating convergence.

Beyond LULC and SAR, ViTs are also making strides in **agricultural monitoring**, where fine-grained detection and classification are essential. For instance, [wang20215ra] developed SwinGD, a Swin Transformer-based model for robust grape bunch detection in complex vineyard environments, effectively handling irregularly shaped and dense objects. Similarly, [p2024nbn] proposed GNViT, an enhanced Vision Transformer model for groundnut pest classification, achieving near-perfect accuracy (99.95\%) through robust data augmentation and transfer learning. These examples underscore ViTs' ability to capture global contextual information for precise identification tasks in diverse agricultural settings.

In conclusion, Vision Transformers have rapidly become a cornerstone in remote sensing and environmental monitoring. Their unparalleled ability to model global contextual information, often enhanced by specialized attention mechanisms and integrated into efficient or hybrid frameworks, provides powerful solutions for complex tasks. From improving SAR ship detection in cluttered maritime environments to enabling highly accurate multimodal LULC classification and fine-grained agricultural monitoring, ViTs offer significant advancements in accuracy and robustness. Future research in this domain will likely focus on developing more efficient ViT architectures tailored for real-time processing on edge devices, enhancing their robustness against environmental noise and adversarial attacks, and exploring their integration into broader geospatial foundation models for more comprehensive and intelligent environmental insights.
\subsection{Lightweight and Real-time Applications}
\label{sec:6_4_lightweight__and__real-time_applications}

The effective deployment of Vision Transformers (ViTs) in real-time applications and resource-constrained environments represents a critical frontier, demanding a delicate balance between model accuracy, computational efficiency, and minimal latency. The inherent complexity and substantial memory footprint of large ViTs often render them unsuitable for on-device processing in scenarios such as automated plant disease classification, radar-based human activity recognition (HAR), and mobile vision systems. This subsection focuses on how lightweight and efficient ViT models are specifically tailored and deployed to overcome these challenges, enabling advanced deep learning capabilities in practical, embedded contexts where computational resources are severely limited.

General-purpose lightweight ViT architectures provide foundational backbones for a wide array of mobile and edge applications. Models like MobileViT [mehta20216ad], which reinterpret transformers as convolutions, demonstrate superior accuracy with significantly fewer parameters than many traditional CNNs and ViTs. This efficiency makes them highly suitable for tasks requiring real-time inference on edge devices. For instance, the MobileViT-based Tracker (MVT) [gopal20237ol] leverages MobileViT as its core, achieving highly accurate and fast visual object tracking in real-time on resource-constrained hardware. The strength of such general-purpose models lies in their broad applicability, offering a robust starting point for various mobile vision tasks without extensive domain-specific customization, thereby extending advanced visual perception to everyday devices.

For highly specialized real-time applications, however, custom hybrid designs and optimized attention mechanisms are often paramount to meet stringent performance and efficiency requirements. Consider radar-based human activity recognition (HAR), a domain characterized by unique micro-Doppler data and the critical need for low-latency processing on embedded systems. Traditional ViTs are often computationally prohibitive in this context. To address this, \textcite{huan202345b} developed a Lightweight Hybrid Vision Transformer (LH-ViT). This architecture strategically integrates efficient RES-SE blocks for local feature extraction with a novel Radar-ViT module, which employs fold and unfold operations. This specialized attention mechanism drastically reduces the computational demands of multi-head attention, making it particularly adept at capturing global micro-Doppler features efficiently for low-latency HAR. This approach highlights how tailoring the attention mechanism to the specific data modality and application constraints can optimize for both performance and computational cost, a crucial aspect for embedded systems.

Similarly, in automated plant disease classification, the demand for real-time, on-site diagnosis in agricultural settings drives the need for highly efficient models. \textcite{borhani2022w8x} tackled the computational burden by proposing custom, simplified CNN and Transformer building blocks within novel hybrid CNN-ViT architectures. Their systematic investigation demonstrated that these custom hybrid models effectively mitigate the speed deceleration associated with attention mechanisms while maintaining high diagnostic accuracy, crucial for real-time agricultural deployments on portable devices. In contrast, \textcite{tabbakh2023ao7} proposed TLMViT, combining transfer learning models with ViTs for deep feature extraction, showcasing the benefit of multi-model integration for improved performance by leveraging pre-trained knowledge. Another approach, GNViT [p2024nbn], focuses on enhancing a Vision Transformer model for groundnut pest classification through robust data augmentation and transfer learning, achieving high accuracy. While \textcite{p2024nbn} claims "near-perfect accuracy," a critical review notes that such claims require specific metrics (e.g., F1-score, AUC) and dataset context for proper academic assessment, and the implicit requirement for efficient inference for practical field use is a key challenge not explicitly detailed in their architectural innovations. These agricultural examples illustrate a spectrum of solutions: from fine-grained custom block design for specific data types to leveraging transfer learning and multi-model ensembles for robustness, all aimed at enabling practical, real-time field deployments.

Beyond architectural design, achieving real-time performance on extremely resource-constrained edge devices often necessitates further post-design optimizations and sophisticated hardware-software co-design. While the detailed methodologies for pruning, quantization, and hardware acceleration are discussed in Section 5.4, their application is critical for deploying lightweight ViTs in stringent real-time scenarios. For instance, models developed for mobile vision or agricultural monitoring are frequently subjected to aggressive quantization (e.g., 8-bit or even lower precision) to reduce memory footprint and computational cost, enabling their execution on dedicated edge AI accelerators. Such co-design efforts are exemplified by frameworks like EQ-ViT [dong20245zz], which, while an acceleration framework itself, demonstrates how end-to-end optimization, including quantization-aware training and heterogeneous computing, can achieve deterministic low-latency inference (e.g., sub-millisecond) and significant energy efficiency gains for real-time ViT deployment. These advancements highlight that the most impactful gains for real-world applications often stem from a holistic approach that considers the entire deployment pipeline, from architectural choice to hardware-software synergy.

In conclusion, the research landscape for Vision Transformers in lightweight and real-time applications is characterized by a multi-faceted approach driven by specific application needs. This includes the development of versatile general-purpose lightweight models like MobileViT for broad mobile vision tasks, as well as highly specialized hybrid architectures such as LH-ViT for unique data modalities like radar-based HAR. These architectural innovations are often complemented by post-design algorithmic optimizations and rigorous hardware-software co-design to meet the stringent latency, power, and memory constraints of edge devices. Despite these advancements, challenges remain in achieving optimal accuracy, robustness, and generalizability across highly diverse and unpredictable real-world environments, especially for ultra-low-power, extremely resource-constrained edge deployments. Future directions will likely involve further adaptive models that dynamically adjust complexity based on available resources and more robust, application-aware optimization techniques to ensure ViTs can reliably operate in the most demanding real-time scenarios.


### Future Directions and Open Challenges

\section{Future Directions and Open Challenges}
\label{sec:future_directions__and__open_challenges}



\subsection{Towards More Efficient and Scalable Architectures}
\label{sec:7_1_towards_more_efficient__and__scalable_architectures}

The profound capabilities of Vision Transformers (ViTs) are often accompanied by significant computational and memory demands, primarily due to the quadratic complexity of the self-attention mechanism with respect to the number of input tokens. This inherent limitation presents substantial hurdles for processing high-resolution imagery, achieving real-time inference, and deploying models on resource-constrained edge devices. While earlier sections (e.g., Section 3.1 on Swin Transformer) discussed hierarchical designs that mitigate this by introducing windowed attention, the continuous drive for efficiency pushes beyond these established paradigms, focusing on more fundamental architectural and algorithmic innovations. This subsection explores emerging research directions aimed at minimizing the memory footprint and computational overhead while maintaining or improving performance, which is crucial for making ViTs practical for widespread use in real-world applications [heidari2024d9k]. This pursuit directly addresses the fundamental trade-offs between model capacity, speed, and resource consumption, seeking to unlock new application domains for ViTs.

One critical avenue for future efficiency improvement lies in fundamentally re-imagining the self-attention mechanism itself, moving beyond its quadratic scaling. While hierarchical window-based approaches achieve linear complexity with respect to image size, they still involve quadratic complexity within each window. \textit{Linear attention mechanisms} represent a significant research frontier, aiming to reduce complexity by approximating the softmax operation or factorizing the attention matrix, often without explicit non-linearities. These methods typically fall into categories such as kernel-based approximations (e.g., using random Fourier features to approximate the softmax kernel), low-rank matrix factorizations, or explicit removal of non-linearities. For instance, UFO-ViT [song20215tk] proposes a Unit Force Operated Vision Transformer that achieves linear complexity by eliminating non-linearity from the original self-attention and factorizing matrix multiplication. This approach, by modifying only a few lines of code, demonstrates competitive or superior performance on image classification and dense prediction tasks across various model capacities, highlighting the potential of simplified attention computations to push efficiency boundaries. Such efforts are crucial for scaling ViTs to unprecedented input sizes, such as gigapixel images, where even windowed attention might be prohibitive.

Beyond static approximations, researchers are actively exploring \textit{adaptive and sparse attention mechanisms} that dynamically focus computational resources on the most salient image regions. This mitigates the inefficiencies of both dense global attention (high cost, often processing irrelevant features) and fixed sparse attention (data-agnostic limitations). The \textit{Vision Transformer with Deformable Attention} (DAT) [xia2022qga] and its enhanced version DAT++ [xia2023bp7] exemplify this by introducing a deformable multi-head attention module. Here, the positions of key and value pairs are adaptively allocated in a data-dependent manner, allowing the model to dynamically attend to relevant regions. This flexible scheme maintains the representational power of global attention while significantly improving efficiency and performance across various vision tasks. Similarly, LF-ViT [hu202434n] addresses spatial redundancy by strategically curtailing computational demands. It processes a reduced-resolution image, and if a definitive prediction is elusive, a Neighborhood Global Class Attention (NGCA) mechanism identifies class-discriminative regions, which are then used from the original image for enhanced recognition. This two-phase approach, with consistent parameters across phases, significantly reduces FLOPs and amplifies throughput without compromising performance, offering a path towards more intelligent resource allocation.

Another critical direction involves \textit{architectural simplification and streamlining} to reduce overhead and improve inference speed. While Section 5.2 discusses alternative token mixers, this thread focuses on optimizing the *overall structure* of ViTs. Hiera [ryali202339q] exemplifies this by arguing that many "bells-and-whistles" added to modern hierarchical vision transformers for supervised classification performance are unnecessary. By leveraging strong self-supervised pre-training (e.g., Masked Autoencoders, MAE), Hiera demonstrates that a significantly simpler hierarchical ViT can achieve higher accuracy and be substantially faster both at inference and during training. This suggests that future efficient designs might prioritize architectural minimalism, relying on robust pre-training to imbue models with necessary inductive biases, thereby reducing the need for complex, hand-engineered components that add computational overhead.

For practical deployment, especially on mobile and edge devices, \textit{hardware-aware designs and quantization techniques} are paramount. These approaches optimize ViTs not just algorithmically but also for the specific constraints of target hardware, pushing towards ultra-low-power and real-time inference. Hardware accelerators like ViTA [nag2023cfn] are specifically designed for ViT inference on resource-constrained edge devices, employing head-level pipelines and inter-layer MLP optimizations to achieve high hardware utilization and reasonable frame rates with low power consumption. Furthermore, \textit{quantization} is a crucial strategy for reducing model size and computational cost. Q-ViT [li20229zn] proposes a fully differentiable quantization method where both quantization scales and bit-widths are learnable, leveraging head-wise bit-width to squeeze model size while preserving performance. It also identifies Multi-head Self-Attention (MSA) and GELU as key aspects for ViT quantization robustness. Pushing the limits further, Bi-ViT [li20238ti] explores fully-binarized ViTs, addressing attention distortion caused by gradient vanishing and ranking disorder through learnable scaling factors and ranking-aware distillation. Such extreme quantization can yield significant theoretical acceleration in FLOPs, albeit with careful management of accuracy trade-offs. The ultimate goal is often achieved through \textit{algorithm-hardware co-design}, as seen in EQ-ViT [dong20245zz], an end-to-end acceleration framework for real-time ViT inference on platforms like AMD Versal ACAP. This framework combines a novel spatial and heterogeneous accelerator architecture with a comprehensive quantization-aware training strategy, demonstrating significant speedups and energy efficiency gains over existing solutions. Similarly, FPGA-aware automatic acceleration frameworks with mixed-scheme quantization [sun2022nny] are being developed to optimize ViTs for specific FPGA architectures, achieving substantial improvements in frame rate with minimal accuracy drops. These hardware-centric optimizations are particularly synergistic with algorithmic advancements like sparse attention, where reduced precision in less salient regions could yield further computational savings.

In summary, the continuous drive for efficient and scalable ViT architectures is a multifaceted research endeavor that extends beyond current state-of-the-art solutions. It spans from fundamental re-designs of the attention mechanism (e.g., linear attention), through the exploration of adaptive and sparse attention, to architectural streamlining and sophisticated hardware-aware co-design and aggressive quantization strategies. These efforts are critical for overcoming the inherent computational challenges of ViTs, enabling their deployment across a broader spectrum of real-world applications, from high-resolution medical imaging to real-time edge computing, by meticulously balancing model capacity, speed, and resource consumption. The future of ViT efficiency lies in the intelligent integration of these diverse strategies, pushing the boundaries of what is computationally feasible.
\subsection{Beyond Vision: Multimodal and Foundation Models}
\label{sec:7_2_beyond_vision:_multimodal__and__foundation_models}


The trajectory of artificial intelligence is rapidly shifting from single-modality, task-specific models towards more generalized and holistic understanding, spearheaded by the emergence of multimodal and foundation models. This paradigm aims to develop powerful Vision Transformers (ViTs) capable of processing and integrating information from diverse modalities, such as text, audio, or 3D data, thereby moving closer to human-like comprehension. These large-scale "foundation models," pre-trained on vast and varied datasets, are designed to serve as universal backbones, learning rich, transferable representations across different data types and tasks. Their significance lies in their ability to generalize to novel tasks and data distributions with minimal or no fine-tuning, a critical step towards more generalized artificial intelligence.

A pivotal development in this shift has been the rise of Vision-Language Pre-training (VLP), which leverages the scalability of Transformer architectures to learn joint representations of images and text. Models like CLIP (Contrastive Language-Image Pre-training) [radford2021learning] and ALIGN (A Large-scale ImaGe-Nosearch pre-training) [jia2021scaling] exemplify this approach. They are trained on massive datasets of image-text pairs (e.g., 400 million for CLIP, 1.8 billion for ALIGN) using contrastive learning objectives, where the model learns to associate corresponding image and text embeddings while distinguishing them from non-matching pairs. This pre-training enables remarkable zero-shot transfer capabilities, allowing the models to perform tasks like image classification or retrieval on unseen categories without explicit fine-tuning, simply by comparing image features to text prompts. The power of these models lies in their ability to bridge the semantic gap between visual and linguistic domains, demonstrating a foundational understanding that extends beyond raw pixel values.

Building upon these VLP successes, the concept of foundation models has expanded to encompass general-purpose visual backbones. While earlier self-supervised methods like MAE [mae] and DINO [dino] were crucial for learning robust visual features from unlabeled data, models like DINOv2 [dino_v2_2023] represent a further scaling of this paradigm. DINOv2 trains large ViTs on billions of unlabeled images, yielding highly robust and generalizable visual features that can serve as strong backbones for a wide array of downstream vision tasks, often outperforming supervised pre-training. Critically, these models exhibit emergent properties, such as the ability to perform dense pixel-level tasks without explicit supervision, showcasing their deep understanding of visual semantics. Another significant example is the Segment Anything Model (SAM) [segment_anything_2023], a foundation model specifically designed for promptable segmentation. SAM is trained on an unprecedented dataset of 11 million images and 1.1 billion masks, allowing it to segment any object in an image given various prompts (e.g., points, bounding boxes, text). This demonstrates the power of large-scale pre-training to create models with remarkable generalization and interactive capabilities, moving beyond fixed-category segmentation to a more flexible, user-driven approach.

The ultimate goal of multimodal foundation models is to integrate diverse sensory inputs for more complex reasoning and in-context learning. Models such as Flamingo [alayrac2022flamingo] represent a significant step in this direction, combining powerful pre-trained vision encoders (like CLIP) with large language models (LLMs) to enable few-shot learning for vision-language tasks. Flamingo achieves this by using cross-attention layers that condition the LLM on visual features, allowing it to process interleaved image and text inputs and generate coherent responses based on a few examples. This architecture enables capabilities like visual question answering, image captioning, and visual dialogue with unprecedented flexibility. Similarly, generalist agents like Gato [reed2022generalist] demonstrate the potential for a single Transformer to perform a wide range of tasks across different modalities, from playing Atari games to controlling robotic arms, by treating diverse inputs and outputs as a unified sequence. This approach highlights the ambition to create truly general-purpose AI systems that can learn and adapt across domains, moving beyond specialized models to a more unified intelligence. Furthermore, EVA [eva_2023] explores the limits of transfer learning with a unified text-and-image encoder, demonstrating a direct step towards more comprehensive multimodal understanding by leveraging both image-text and image-only data. The integration of deformable convolutions into Transformer-like architectures, as seen in InternImage [internimage_2023], further exemplifies the ongoing exploration of combining strengths from different paradigms to build large-scale vision foundation models.

Despite these significant advancements, several challenges remain. The sheer scale of these models necessitates immense computational resources for training and deployment, raising concerns about their environmental impact and accessibility. Data curation for multimodal pre-training is also a complex task, as biases present in large web-scraped datasets can lead to unfair or harmful model behaviors. Furthermore, while these models demonstrate impressive emergent capabilities, the mechanisms for true cross-modal reasoning, beyond superficial integration or concatenation, are still an active area of research. Ensuring interpretability and robustness in such complex, black-box systems is also critical for their responsible deployment. The future direction points towards even larger, more data-efficient, and ethically aligned foundation models that can seamlessly integrate information from the visual world with other sensory inputs, fostering a more holistic and adaptable understanding.
\subsection{Novel Architectures and Beyond Attention Mechanisms}
\label{sec:7_3_novel_architectures__and__beyond_attention_mechanisms}


The pervasive dominance of the Transformer architecture in visual AI, largely due to its potent self-attention mechanism, has spurred an intense search for alternative architectural paradigms that can overcome its inherent limitations, particularly concerning computational cost, memory footprint, and the explicit capture of local inductive biases. This subsection delves into emerging and entirely new architectural designs and token mixing mechanisms that either move significantly beyond or fundamentally modify the self-attention block, challenging the prevailing Transformer-centric view. The exploration of these novel approaches, such as state-space models (SSMs) and potentially biologically inspired mechanisms, aims to offer superior efficiency, distinct inductive biases, and innovative strategies for capturing long-range dependencies, thereby diversifying the architectural landscape of visual AI.

While initial Vision Transformers (ViTs) demonstrated the power of global attention, subsequent research focused on optimizing these architectures through hierarchical designs and windowed attention to mitigate quadratic complexity and improve performance on dense prediction tasks. Even efforts to replace self-attention with simpler token mixers, such as Fourier transforms or pooling operations, have shown competitive results, suggesting that the broader "meta-architecture" of Transformers might be as crucial as the specific attention mechanism itself. However, these approaches often operate within the established Transformer block structure, prompting a deeper inquiry into fundamentally different computational primitives for sequence modeling in vision.

A promising direction involves the adaptation of State-Space Models (SSMs), which have recently demonstrated remarkable capabilities in efficiently modeling long sequences, particularly in natural language processing. SSMs offer an alternative to attention by processing sequences through a hidden state that evolves over time, enabling efficient capture of long-range dependencies with linear complexity. This mechanism inherently provides a different inductive bias compared to the global pairwise interactions of self-attention, potentially leading to more efficient and effective visual representations.

A significant stride in this direction is presented by \textcite{hatamizadeh2024xr6} with \textit{MambaVision}, a novel hybrid Mamba-Transformer backbone specifically engineered for vision applications. This work directly addresses the challenge of moving beyond traditional attention by redesigning the Mamba formulation to enhance its capability for efficient modeling of visual features. MambaVision's core innovation lies in its ability to process visual information sequentially while maintaining a global context through its state-space representation, offering a compelling alternative to the quadratic complexity of full self-attention. Crucially, \textcite{hatamizadeh2024xr6} demonstrate through extensive ablation studies the feasibility and benefits of integrating Vision Transformers (ViT) with Mamba. Their findings reveal that equipping the Mamba architecture with self-attention blocks, particularly in the final layers, significantly improves its capacity to capture intricate long-range spatial dependencies. This hybrid approach suggests that a synergistic combination of different token mixing mechanismsthe efficient sequential processing of Mamba and the global interaction of self-attentioncan yield superior performance.

The family of MambaVision models introduced by \textcite{hatamizadeh2024xr6} adopts a hierarchical architecture, akin to successful Vision Transformers like Swin, to meet various design criteria and scale effectively. For image classification on the ImageNet-1K dataset, MambaVision variants achieve state-of-the-art (SOTA) performance in terms of both Top-1 accuracy and throughput, underscoring its computational efficiency without sacrificing accuracy. Furthermore, in downstream tasks such as object detection, instance segmentation on MS COCO, and semantic segmentation on ADE20K datasets, MambaVision consistently outperforms comparably sized backbones while demonstrating favorable performance. These results highlight MambaVision's potential to offer better trade-offs between performance, computational cost, and generalizability across diverse visual tasks.

The emergence of architectures like MambaVision represents a critical juncture in visual AI, challenging the long-held assumption that self-attention is the sole or optimal mechanism for global context aggregation. By leveraging the strengths of State-Space Models, MambaVision provides a concrete example of how novel architectural paradigms can offer distinct advantages in efficiency and inductive biases, particularly for capturing long-range dependencies in a more streamlined manner. The hybrid nature of MambaVision also opens up new avenues for research into optimally combining different token mixing strategies, moving beyond a monolithic architectural design towards more modular and functionally specialized components. Unresolved issues include a deeper theoretical understanding of the inductive biases introduced by SSMs in vision, the exploration of other biologically inspired mechanisms, and the identification of optimal hybrid configurations that balance the strengths of diverse architectural primitives. Future directions will likely involve further refinement of SSMs for vision, investigating their interpretability, and exploring novel ways to integrate them with other computational blocks to unlock new breakthroughs in visual representation learning.
\subsection{Ethical Considerations and Societal Impact}
\label{sec:7_4_ethical_considerations__and__societal_impact}


The remarkable ascent of Vision Transformers (ViTs) has unlocked unprecedented capabilities across diverse computer vision tasks, yet this increasing power necessitates a rigorous examination of their profound ethical implications and broader societal impact. As ViTs become more ubiquitous and their architectures more complex, critical concerns emerge regarding algorithmic bias, the potential for misuse in sensitive applications, and the overarching imperative for responsible AI development that prioritizes transparency, interpretability, and robust fairness evaluations. These are not merely technical challenges but fundamental open questions that will shape public trust and the equitable deployment of advanced AI systems.

A primary ethical challenge revolves around the amplification of algorithmic bias, particularly stemming from the vast, often uncurated datasets used for pre-training large ViT models. Unlike traditional CNNs with stronger inductive biases, ViTs' reliance on global attention and their notorious data hunger often necessitate training on massive, unfiltered web-scale datasets (e.g., LAION). These datasets inevitably reflect and encode societal biases, which ViTs can then learn and perpetuate, leading to unfair or discriminatory outcomes. For instance, in high-stakes applications like medical imaging, the vulnerability of ViTs to adversarial attacks [almalik20223wr] raises serious safety concerns. If a ViT model for medical diagnosis is susceptible to subtle perturbations, its reliability in clinical settings is compromised, potentially leading to misdiagnoses or disparate impacts on different patient populations if the model's robustness varies across demographic groups. Furthermore, while efforts to enhance object detection or scene classification in remote sensing [song202479c, song2025idg] aim for improved accuracy, without careful curation and fairness audits of the augmented data, these techniques could inadvertently amplify existing biases, leading to discriminatory outcomes in applications like urban planning or resource allocation. The complexity of hybrid architectures, such as MambaVision [hatamizadeh2024xr6], further complicates the identification and mitigation of these embedded biases, as the sources of learned representations become more opaque.

Beyond bias, the enhanced capabilities and deployment efficiency of advanced ViTs raise significant concerns regarding their potential for misuse, particularly in surveillance and other harmful contexts. Architectures optimized for efficient deployment, such as Next-ViT [li2022a4u] and TRT-ViT [xia2022dnj], achieve impressive latency/accuracy trade-offs, making them highly attractive for real-time applications. While beneficial for many commercial uses, this efficiency also lowers the barrier for deploying powerful vision systems in surveillance infrastructure, potentially infringing on privacy and civil liberties. The application of ViTs in autonomous vehicle safety assessment, as explored by [kang2022pv3], exemplifies both the promise and peril. While ViT-TA can accurately classify critical situations and identify probable causes using attention maps, thereby improving AV safety, the underlying power to analyze complex real-world scenarios in detail also highlights the immense responsibility associated with deploying such systems. Similarly, advancements in face anti-spoofing using ViTs fine-tuned with self-supervised frameworks like DINO [keresh20249rl] enhance biometric security but simultaneously underscore the increasing sophistication of facial recognition technologies, demanding robust ethical guidelines to prevent their weaponization or use in oppressive regimes.

These significant ethical challenges underscore the critical importance of responsible AI development, framing transparency, interpretability, and robust fairness evaluations as crucial future research directions. The "black-box" nature of deep learning is often exacerbated in complex ViT architectures, making it difficult to understand their decision-making processes. This opacity hinders the identification and rectification of biases, necessitating dedicated research into ViT interpretability. Promising avenues include explainable ViTs for medical applications, such as SleepXViT for automatic sleep staging [lee2025r01] and prototype-based ViTs for COVID-19 detection [xu2024wux]. SleepXViT, for instance, provides intuitive explanations by mimicking human "visual scoring" and offering high-resolution heatmaps, thereby fostering trust and facilitating synergy between AI and human experts. Similarly, the interpretable ViT by [xu2024wux] uses prototype parts to explain model decisions, making the inference process transparent and meaningful for critical health applications. These efforts are crucial for moving beyond mere accuracy to ensure that models are trustworthy and accountable.

However, a critical open challenge remains the development of scalable auditing tools specifically designed for billion-parameter ViT foundation models, which are often pre-trained on vast, uncurated datasets. The sheer scale and complexity of these models make traditional fairness audits impractical. Future research must focus on creating inherently fair attention mechanisms that are robust to dataset biases and on developing standardized ethical guidelines and regulatory frameworks for ViT deployment across sensitive domains. The discussion must move beyond generic AI ethics to analyze how ViT-specific propertiessuch as their global attention mechanism, their notorious data hunger, and the emergent properties from self-supervised learning on vast, uncurated datasetsmight introduce novel or exacerbated ethical challenges compared to CNNs. Without proactive measures to embed ethical considerations throughout the AI lifecycle, from data collection and model training to deployment and monitoring, the societal benefits of Vision Transformers could be overshadowed by their unintended negative consequences.

In conclusion, while the rapid advancements in Vision Transformers promise transformative capabilities, they also introduce profound ethical dilemmas that demand immediate and sustained attention. The pervasive risk of algorithmic bias, particularly from large-scale self-supervised pre-training, and the potential for misuse in surveillance and other high-stakes applications necessitate a concerted effort towards responsible AI development. Future research must not only focus on pushing performance boundaries but also prioritize the development of transparent, interpretable, and robust ViT systems. Fostering public trust in these powerful AI technologies hinges on our collective ability to navigate these ethical complexities, ensuring that innovation is guided by a strong commitment to societal well-being and equitable outcomes.


### Conclusion

\section{Conclusion}
\label{sec:conclusion}



\subsection{Summary of Key Developments}
\label{sec:8_1_summary_of_key_developments}


The emergence of Vision Transformers (ViTs) has profoundly reshaped the landscape of computer vision, challenging the long-standing dominance of Convolutional Neural Networks (CNNs). As introduced in Section 2.1, the foundational ViT architecture [dosovitskiy2021image] demonstrated that by segmenting images into patches and processing them with a standard Transformer encoder, impressive performance could be achieved in image classification. This breakthrough underscored the power of global self-attention in capturing long-range dependencies across an entire image, a capability often limited in traditional CNNs. However, the initial ViT paradigm faced significant hurdles: its considerable data hunger, requiring massive pre-training datasets like JFT-300M, and its quadratic computational complexity with respect to image resolution, which hindered its application to high-resolution inputs and dense prediction tasks. Early research, as detailed in Section 2.2, swiftly addressed these limitations. Knowledge distillation, notably exemplified by DeiT [deit], enabled ViTs to achieve competitive performance with significantly smaller datasets by learning from pre-trained CNN teachers, effectively transferring inductive biases. Concurrently, efforts focused on enhancing architectural stability and tokenization for deeper models, as discussed in Section 2.3, paving the way for more robust and practical ViT deployments.

The inherent limitations of the original ViT for dense prediction tasks, which demand multi-scale feature representations and efficient processing of high-resolution inputs, spurred a critical wave of architectural innovation. As elaborated in Section 3, the development of hierarchical Vision Transformers became a pivotal breakthrough. The \textit{Swin Transformer} [liu2021ljs] stands out as a landmark contribution, introducing a hierarchical design coupled with shifted window-based self-attention. This ingenious mechanism addressed the quadratic complexity by localizing attention within non-overlapping windows while enabling cross-window information exchange through shifting, thereby generating multi-scale feature maps that scale linearly with image size. This made Swin Transformer a versatile and efficient backbone, capable of excelling in complex tasks like object detection and semantic segmentation, effectively bridging the performance gap with CNNs in these domains. Further advancements in this direction, such as the Pyramid Vision Transformer (PVT) [wang2021pyramid] and Multiscale Vision Transformers (MViT) [fan2021multiscale], continued to refine multi-scale feature extraction and efficient attention mechanisms. More recently, the Hiera architecture [ryali202339q] demonstrated that, with strong self-supervised pretraining, hierarchical ViTs could achieve high accuracy and speed even with simplified designs, challenging the necessity of overly complex, vision-specific components. This evolution underscored a strategic shift towards architectural designs that balance global context with computational efficiency and multi-scale representation.

A transformative development in overcoming ViT's data dependency was the widespread adoption of self-supervised learning (SSL) paradigms, as thoroughly explored in Section 4. These strategies enabled ViTs to learn powerful visual representations from vast amounts of unlabeled data, significantly reducing the reliance on expensive human annotations. Masked Image Modeling (MIM), inspired by BERT in NLP, emerged as a highly effective approach. Models like Masked Autoencoders (MAE) [mae] demonstrated that reconstructing masked image patches forced ViTs to learn rich, semantic features, particularly with high masking ratios that encourage global understanding. Complementary to MIM, self-distillation and contrastive learning methods, such as DINO [dino], revealed remarkable emergent properties in ViT features, including the ability to perform object segmentation without explicit supervision. By training a student network to match a teacher's output, DINO showcased how ViTs could learn robust and semantically meaningful representations through unsupervised means. The success of these SSL techniques has been instrumental in scaling ViTs to unprecedented sizes, leading to the development of 'Vision Foundation Models' [zhai2022scaling, assran2023dino] that serve as highly robust and general-purpose visual backbones, capable of transferring effectively across a broad spectrum of downstream tasks with minimal fine-tuning.

Beyond architectural refinements and training strategies, a significant research thrust has involved critically re-evaluating the self-attention mechanism itself and exploring hybrid architectures, as detailed in Section 5. This line of inquiry sought to combine the strengths of CNNs, particularly their inductive biases for local feature extraction and computational efficiency, with the global context modeling of Transformers. Hybrid models, such as MobileViT [mehta20216ad] and Next-ViT [li2022a4u], strategically integrated convolutional layers or convolutional inductive biases within the Transformer framework, yielding lightweight and efficient designs suitable for deployment on resource-constrained devices. These models often achieve superior performance by leveraging the best of both worlds, demonstrating that a synergistic approach can outperform pure paradigms in certain contexts. Simultaneously, researchers explored radical alternatives to complex self-attention. Works like UFO-ViT [song20215tk] introduced linear complexity attention mechanisms, while ShiftViT [wang2022da0] provocatively showed that even zero-parameter shift operations could replace attention layers while maintaining competitive performance, suggesting that the overall "MetaFormer" architectural design might be more crucial than attention alone. Other innovations, such as PLG-ViT [ebert202377v] with its parallel local-global self-attention and NomMer [liu2021yw0] with dynamic context nomination, further refined efficient attention by adaptively combining local and global information. This convergence of ideas even led to the modernization of CNNs, exemplified by ConvNeXt [liu2022convnet], which adopted ViT design principles to achieve competitive performance, effectively blurring the lines between these once distinct architectural paradigms. The recent advent of MambaVision [hatamizadeh2024xr6], integrating state-space models, signals a continued exploration of novel token mixing mechanisms beyond traditional attention.

Collectively, these advancements have propelled Vision Transformers from a nascent concept to a mature and versatile technology, finding widespread application across diverse computer vision domains, as showcased in Section 6. From robust object detection and semantic segmentation to specialized tasks in medical image analysis and remote sensing, ViTs, often in their hierarchical or hybrid forms, have demonstrated exceptional adaptability and performance. The continuous evolution reflects an ongoing tension within the field: balancing the expressive power and global receptive field of pure self-attention with the practical demands for computational efficiency, stronger inductive biases, and multi-scale processing. Looking ahead, as discussed in Section 7, future directions will likely involve further convergence of architectural ideas, the exploration of even more novel token mixing mechanisms, and the development of increasingly robust and generalizable multimodal and foundation models. The journey of Vision Transformers underscores a dynamic field driven by continuous innovation, where the pursuit of more powerful, efficient, and universally applicable visual intelligence remains the central objective.
\subsection{Unresolved Tensions and Future Outlook}
\label{sec:8_2_unresolved_tensions__and__future_outlook}


The remarkable ascent of Vision Transformers (ViTs) has profoundly reshaped computer vision, establishing them as formidable contenders, and often successors, to traditional Convolutional Neural Networks (CNNs). However, the field is far from settled, grappling with several fundamental, unresolved tensions that define the frontier of future research. These debates center on achieving an optimal balance between architectural expressivity and computational efficiency, the quest for truly universal visual representations, the practicalities of model deployment, and the seamless integration of ViTs into broader multimodal and foundation models. Navigating these challenges is crucial for unlocking the next generation of visual intelligence.

One of the most persistent architectural tensions lies in reconciling the global context modeling prowess of self-attention with the imperative for computational efficiency and robust local inductive biases. While early ViTs [ViT] showcased the power of global attention, their quadratic complexity and significant data requirements highlighted the need for more efficient designs. This led to a diverse array of innovations, from hierarchical architectures like Swin Transformer [liu2021ljs] that localize attention to windows, to more adaptive variants such as Deformable Attention Transformer [xia2022qga] and linear attention mechanisms like UFO-ViT [song20215tk], all striving to reduce computational overhead while preserving long-range dependencies. However, a deeper meta-question emerged: is complex self-attention truly indispensable? Provocative works like PoolFormer [PoolFormer] and ShiftViT [wang2022da0] demonstrated that even remarkably simple token mixers, or zero-parameter shift operations, within a Transformer-like "MetaFormer" structure could yield competitive results. This suggests that the overall architectural design and the strategic integration of inductive biases, rather than solely the attention mechanism, are significant drivers of ViT's success (as explored in Section 5.2). This perspective is further reinforced by the modernization of CNNs (e.g., ConvNeXt [ConvNeXt]) using ViT-inspired principles, blurring the lines between the two paradigms (as discussed in Section 5.3). The future likely involves a continued exploration of hybrid models (e.g., MobileViT [mehta20216ad], Next-ViT [li2022a4u]) that strategically combine convolutional locality with global attention, and the emergence of entirely new architectural primitives, such as State Space Models in MambaVision [hatamizadeh2024xr6]. This ongoing architectural dialogue, potentially guided by neural architecture search [chen202199v], underscores a continuous quest for token mixers that optimally balance expressivity, efficiency, and inherent inductive biases, moving towards a more unified theory of visual processing.

The pursuit of truly universal visual representations, capable of transferring across an expansive range of tasks and domains with minimal fine-tuning, remains a paramount challenge. Self-supervised learning (SSL) methods, particularly Masked Autoencoders (MAE) [MAE] and DINO [community_10], have been pivotal in mitigating ViT's data hunger and fostering robust representations (as detailed in Section 4). While systematic studies [zhou2021rtn] have demonstrated ViTs' superior transfer learning capabilities compared to ConvNets, the ideal pre-training strategy and architectural design for maximal universality are still under active investigation. Challenges persist in adapting SSL to complex hierarchical architectures, as seen with the need for innovations like Uniform Masking (UM-MAE) [li2022ow4]. Recent efforts, such as ViTDet [li2022raj] and Hiera [ryali202339q], suggest that strong self-supervised pre-training can simplify hierarchical ViT designs, leading to more efficient and accurate models. However, the reliance on massive pre-training datasets remains a bottleneck, with research exploring the feasibility of training ViT-based object detectors from scratch [hong2022ks6] highlighting the need for fundamental architectural changes and extended training. A significant step towards this universality is exemplified by GiT [wang20249qa], which proposes a "Generalist Vision Transformer" framework capable of handling diverse vision tasksfrom image captioning to detection and segmentationthrough a universal language interface, without task-specific modules. This paradigm shift aims to foster mutual enhancement across tasks and reduce the architectural gap between vision and language, suggesting a future where visual backbones are inherently more robust and generalizable, reducing the dependency on colossal datasets or enabling efficient learning from scratch.

Beyond theoretical architectures, the practical deployment of ViTs, especially on resource-constrained edge devices, presents critical challenges related to model complexity and efficiency. While lightweight hybrid designs (e.g., MobileViT [mehta20216ad], as discussed in Section 5.4) offer promising avenues, a holistic approach is essential. This involves not only efficient architectural design but also advanced quantization techniques, such as Q-ViT [li20229zn] which employs differentiable head-wise bit-width allocation, and knowledge distillation methods like AttnDistill [wang2022pee] to transfer knowledge from large teachers to smaller student models. Crucially, algorithm-hardware co-design is emerging as a vital strategy, with specialized accelerators like ViTA [nag2023cfn] and EQ-ViT [dong20245zz] demonstrating how tailoring hardware to ViT inference patterns can achieve real-time performance and energy efficiency on edge platforms. Further architectural innovations like LF-ViT [hu202434n], which reduces spatial redundancy by focusing computation on class-discriminative regions, also contribute to deployment efficiency. The future of ViT deployment hinges on a synergistic integration of these efforts, pushing beyond theoretical FLOPs towards real-world latency, energy consumption, and robustness in diverse operational environments.

Finally, the integration of ViTs into broader multimodal and foundation models represents perhaps the most ambitious frontier. The token-based nature of Transformers inherently positions them as strong candidates for processing and integrating diverse data types, including text, audio, and 3D data. Early multimodal explorations, such as the Visual Saliency Transformer (VST) [liu2021jpu] for RGB-D data and cross-attention mechanisms in models like CrossViT [chen2021r2y] and SwinCross [li20233lv] for fusing features, have laid the groundwork. In specialized domains like remote sensing, models such as ExViT [yao2023sax] and PolSAR-MPIformer [xin2024ljt] extend ViTs to handle complex hyperspectral, LiDAR, and SAR imagery, enabling more comprehensive environmental understanding. The ultimate vision is the development of unified vision-language foundation models, exemplified by TransVG++ [deng2022bil] for visual grounding, which aim to achieve a more holistic understanding of the world by learning rich, transferable representations across an extremely broad range of data types and tasks. As noted by [hassija2025wq3], scaling these models to unprecedented sizes and effectively aligning diverse modalities poses significant challenges in data curation, architectural complexity, and computational cost. Nevertheless, this trajectory represents a profound shift towards truly generalized artificial intelligence, where ViTs serve as a core component for intelligent systems capable of perceiving, reasoning, and interacting with a complex, multi-sensory world.

In conclusion, the Vision Transformer landscape is defined by a dynamic interplay of innovation and persistent challenges. The core unresolved tensionsbalancing attention's power with efficiency, achieving truly universal representations, enabling practical deployment, and advancing multimodal integrationare actively shaping the research agenda. Future work will undoubtedly continue to explore novel architectural paradigms, push the boundaries of self-supervised learning, embrace algorithm-hardware co-design, and advance multimodal integration. This continuous pursuit will ultimately contribute to the development of more intelligent, adaptable, and responsible AI systems for visual perception, moving beyond current limitations towards a more comprehensive understanding of visual and multimodal data.


