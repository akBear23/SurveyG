\subsection{Lightweight and Real-time Applications}
The effective deployment of Vision Transformers (ViTs) in real-time applications and resource-constrained environments represents a critical frontier, demanding a delicate balance between model accuracy, computational efficiency, and minimal latency. The inherent complexity and substantial memory footprint of large ViTs often render them unsuitable for on-device processing in scenarios such as automated plant disease classification, radar-based human activity recognition (HAR), and mobile vision systems. This subsection focuses on how lightweight and efficient ViT models are specifically tailored and deployed to overcome these challenges, enabling advanced deep learning capabilities in practical, embedded contexts where computational resources are severely limited.

General-purpose lightweight ViT architectures provide foundational backbones for a wide array of mobile and edge applications. Models like MobileViT \cite{mehta20216ad}, which reinterpret transformers as convolutions, demonstrate superior accuracy with significantly fewer parameters than many traditional CNNs and ViTs. This efficiency makes them highly suitable for tasks requiring real-time inference on edge devices. For instance, the MobileViT-based Tracker (MVT) \cite{gopal20237ol} leverages MobileViT as its core, achieving highly accurate and fast visual object tracking in real-time on resource-constrained hardware. The strength of such general-purpose models lies in their broad applicability, offering a robust starting point for various mobile vision tasks without extensive domain-specific customization, thereby extending advanced visual perception to everyday devices.

For highly specialized real-time applications, however, custom hybrid designs and optimized attention mechanisms are often paramount to meet stringent performance and efficiency requirements. Consider radar-based human activity recognition (HAR), a domain characterized by unique micro-Doppler data and the critical need for low-latency processing on embedded systems. Traditional ViTs are often computationally prohibitive in this context. To address this, \textcite{huan202345b} developed a Lightweight Hybrid Vision Transformer (LH-ViT). This architecture strategically integrates efficient RES-SE blocks for local feature extraction with a novel Radar-ViT module, which employs fold and unfold operations. This specialized attention mechanism drastically reduces the computational demands of multi-head attention, making it particularly adept at capturing global micro-Doppler features efficiently for low-latency HAR. This approach highlights how tailoring the attention mechanism to the specific data modality and application constraints can optimize for both performance and computational cost, a crucial aspect for embedded systems.

Similarly, in automated plant disease classification, the demand for real-time, on-site diagnosis in agricultural settings drives the need for highly efficient models. \textcite{borhani2022w8x} tackled the computational burden by proposing custom, simplified CNN and Transformer building blocks within novel hybrid CNN-ViT architectures. Their systematic investigation demonstrated that these custom hybrid models effectively mitigate the speed deceleration associated with attention mechanisms while maintaining high diagnostic accuracy, crucial for real-time agricultural deployments on portable devices. In contrast, \textcite{tabbakh2023ao7} proposed TLMViT, combining transfer learning models with ViTs for deep feature extraction, showcasing the benefit of multi-model integration for improved performance by leveraging pre-trained knowledge. Another approach, GNViT \cite{p2024nbn}, focuses on enhancing a Vision Transformer model for groundnut pest classification through robust data augmentation and transfer learning, achieving high accuracy. While \textcite{p2024nbn} claims "near-perfect accuracy," a critical review notes that such claims require specific metrics (e.g., F1-score, AUC) and dataset context for proper academic assessment, and the implicit requirement for efficient inference for practical field use is a key challenge not explicitly detailed in their architectural innovations. These agricultural examples illustrate a spectrum of solutions: from fine-grained custom block design for specific data types to leveraging transfer learning and multi-model ensembles for robustness, all aimed at enabling practical, real-time field deployments.

Beyond architectural design, achieving real-time performance on extremely resource-constrained edge devices often necessitates further post-design optimizations and sophisticated hardware-software co-design. While the detailed methodologies for pruning, quantization, and hardware acceleration are discussed in Section 5.4, their application is critical for deploying lightweight ViTs in stringent real-time scenarios. For instance, models developed for mobile vision or agricultural monitoring are frequently subjected to aggressive quantization (e.g., 8-bit or even lower precision) to reduce memory footprint and computational cost, enabling their execution on dedicated edge AI accelerators. Such co-design efforts are exemplified by frameworks like EQ-ViT \cite{dong20245zz}, which, while an acceleration framework itself, demonstrates how end-to-end optimization, including quantization-aware training and heterogeneous computing, can achieve deterministic low-latency inference (e.g., sub-millisecond) and significant energy efficiency gains for real-time ViT deployment. These advancements highlight that the most impactful gains for real-world applications often stem from a holistic approach that considers the entire deployment pipeline, from architectural choice to hardware-software synergy.

In conclusion, the research landscape for Vision Transformers in lightweight and real-time applications is characterized by a multi-faceted approach driven by specific application needs. This includes the development of versatile general-purpose lightweight models like MobileViT for broad mobile vision tasks, as well as highly specialized hybrid architectures such as LH-ViT for unique data modalities like radar-based HAR. These architectural innovations are often complemented by post-design algorithmic optimizations and rigorous hardware-software co-design to meet the stringent latency, power, and memory constraints of edge devices. Despite these advancements, challenges remain in achieving optimal accuracy, robustness, and generalizability across highly diverse and unpredictable real-world environments, especially for ultra-low-power, extremely resource-constrained edge deployments. Future directions will likely involve further adaptive models that dynamically adjust complexity based on available resources and more robust, application-aware optimization techniques to ensure ViTs can reliably operate in the most demanding real-time scenarios.