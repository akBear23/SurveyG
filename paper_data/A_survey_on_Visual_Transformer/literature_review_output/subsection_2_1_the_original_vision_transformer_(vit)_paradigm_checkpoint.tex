\subsection{The Original Vision Transformer (ViT) Paradigm}

The Vision Transformer (ViT) architecture operationalized a profound shift in computer vision, directly applying the Transformer architecture, originally developed for natural language processing, to image recognition tasks \cite{dosovitskiy2021image}. This groundbreaking work fundamentally re-conceptualized image processing, moving away from local convolutional operations to a sequence-to-sequence approach, treating images as sequences of flattened patches rather than relying on inherent spatial hierarchies.

At its core, the ViT architecture processes an image by first dividing it into non-overlapping, fixed-size square patches (e.g., 16x16 pixels). Each patch is then flattened into a 1D sequence of pixel values and linearly projected into a higher-dimensional embedding space. To re-introduce crucial spatial information lost during the flattening, learnable positional encodings are added to these patch embeddings, allowing the model to understand the relative positions of different patches within the original image \cite{dosovitskiy2021image}. These embedded patches, along with an additional learnable "class token" (a direct borrowing from BERT's \texttt{[CLS]} token in NLP, intended to aggregate global information for classification), form the input sequence to a standard Transformer encoder. This encoder, composed of multiple layers of multi-head self-attention (MSA) and feed-forward networks (FFN), enables the model to capture long-range dependencies and global contextual relationships across the entire image by allowing each patch to attend to all other patches. The final classification is typically performed by a multi-layer perceptron (MLP) head attached to the output of the class token.

The original ViT demonstrated impressive performance, achieving state-of-the-art results on large-scale image classification benchmarks, notably surpassing CNN-based models when pre-trained on massive datasets such as JFT-300M \cite{dosovitskiy2021image}. This success underscored the power of the self-attention mechanism to learn rich, global representations without relying on strong, hard-coded convolutional inductive biases like locality and translation equivariance.

However, despite its groundbreaking performance, the original ViT paradigm critically highlighted significant limitations. Unlike CNNs, which possess strong, hard-coded inductive biases such as local receptive fields and translation equivariance, the original ViT architecture largely foregoes these explicit priors. While the initial patchification step introduces a rudimentary form of local processing, the subsequent global self-attention mechanism is designed to learn relationships across arbitrary distances without inherent spatial constraints \cite{han2020yk0, huo2023e5h}. This design choice, while enabling unprecedented flexibility and global context modeling, critically meant that ViTs had to learn these fundamental visual priors from data itself. Consequently, ViTs exhibited substantial data hunger, requiring extensive pre-training on colossal datasets to learn robust visual representations from scratch, making them less competitive than CNNs when trained on smaller, more common datasets like ImageNet-1K without such pre-training \cite{yu2022iy0}.

Furthermore, the computational cost associated with the original ViT posed practical challenges. The global self-attention mechanism scales quadratically with respect to the number of input tokens (patches) \cite{song20215tk}. For instance, doubling the linear resolution of an image (e.g., from 224x224 to 448x448) quadruples the number of patches, leading to a sixteen-fold increase in the computational cost of the attention layers \cite{yu2022iy0, heidari2024d9k}. This prohibitive computational and memory overhead, especially for high-resolution inputs, limited its practical deployment and accessibility \cite{hu202434n, xia2022dnj}. The reliance on a single `[CLS]` token for global representation, while effective, also represented a direct porting of an NLP mechanism, which would later be critically re-evaluated and often replaced by more vision-centric aggregation strategies in subsequent ViT variants.

In conclusion, the original ViT paradigm successfully demonstrated the viability of Transformers for computer vision, showcasing their unparalleled ability to model global dependencies and learn powerful representations from data. Yet, it simultaneously exposed a fundamental tension: the trade-off between the expressive power of pure self-attention and the practical demands of data efficiency, computational cost, and the need for implicit inductive biases. This initial work laid the foundation for subsequent research to address these limitations, either by developing more data-efficient training strategies or by integrating architectural priors to make Vision Transformers more robust and versatile across various scales and data regimes.