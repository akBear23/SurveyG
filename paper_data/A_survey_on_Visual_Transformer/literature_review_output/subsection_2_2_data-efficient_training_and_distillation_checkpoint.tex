\subsection{Data-Efficient Training and Distillation}

The initial introduction of Vision Transformers (ViTs) marked a significant paradigm shift in computer vision, demonstrating that pure Transformer architectures could achieve state-of-the-art performance on image recognition tasks \cite{vit}. However, a critical limitation of the original ViT was its substantial data hunger, necessitating pre-training on colossal datasets like JFT-300M to outperform traditional Convolutional Neural Networks (CNNs). This reliance on proprietary, extremely large datasets posed a major barrier to broader adoption and practical application. Consequently, early research efforts focused intensely on developing strategies to mitigate this data dependency, making ViTs more accessible and efficient for researchers and practitioners without access to such extensive resources.

A pivotal advancement in addressing ViT's data hunger was the introduction of data-efficient training techniques, most notably knowledge distillation, exemplified by the Data-efficient Image Transformer (DeiT) \cite{deit}. DeiT demonstrated that a ViT student model could achieve competitive performance, even with significantly smaller training datasets (e.g., ImageNet-1K), by learning from a pre-trained CNN teacher. This teacher-student paradigm effectively transfers the rich representations and strong inductive biases (such as locality and translation equivariance) inherent in CNNs to the ViT, thereby bootstrapping its performance without requiring massive amounts of labeled data. The distillation process in DeiT involved a specialized distillation token that interacts with the class token and patch tokens, learning to reproduce the teacher's output. Crucially, DeiT's success was also attributed to a sophisticated training recipe that included aggressive data augmentation techniques like RandAugment, Mixup, and CutMix. These augmentations effectively expanded the diversity of the training data, preventing overfitting and enabling the ViT to learn robust features from a comparatively smaller dataset, thus making ViTs practical for a wider range of applications.

Complementing distillation, other approaches sought to imbue ViTs with CNN-like inductive biases directly into their architecture to improve data efficiency. For instance, \cite{convit} proposed ConViT, which introduced soft convolutional inductive biases into the self-attention mechanism. By incorporating a gated positional self-attention that leverages local information, ConViT improved the performance of ViTs, particularly when trained on smaller datasets. This method aimed to combine the local feature extraction strengths of CNNs with the global reasoning capabilities of Transformers, making the models more robust to data scarcity by providing better architectural priors. Similarly, \cite{lee2021us0} introduced Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA) as generic add-on modules to enhance the locality inductive bias of ViTs. SPT reorganizes patches to capture local information more effectively, while LSA restricts attention to local neighborhoods, enabling ViTs to learn effectively from scratch even on small-size datasets like Tiny-ImageNet, demonstrating significant performance improvements. These architectural modifications offer an alternative to distillation by intrinsically improving the ViT's ability to learn from limited data.

The concept of distillation itself also evolved, moving beyond CNN teachers. \cite{wang2022pee} explored "Attention Distillation" for self-supervised Vision Transformers (ViT-SSKD), demonstrating that distilling information directly from the attention mechanism of a teacher ViT to a student ViT could significantly narrow the performance gap between them. This approach proved effective for smaller ViT models (e.g., ViT-Tiny and ViT-Small) and was independent of the specific self-supervised learning algorithm, highlighting the versatility and continued relevance of distillation for improving the data efficiency and performance of ViT students, even when the teacher is another ViT. Furthermore, the challenge of training ViTs from scratch extended beyond classification. \cite{hong2022ks6} investigated training ViT-based object detectors from scratch without large-scale ImageNet pre-training. Their findings revealed that specific architectural changes and extended training epochs played critical roles in achieving competitive performance, underscoring that data efficiency for ViTs is not solely about distillation but also about optimizing the architectural design and training regimen for specific tasks when large pre-training datasets are unavailable.

In summary, the early strategies for data-efficient training and distillation were crucial in overcoming the initial barrier of ViT's immense data requirements. Techniques like knowledge distillation \cite{deit, wang2022pee} successfully transferred valuable inductive biases and rich representations, enabling ViTs to achieve strong performance with standard datasets. Concurrently, architectural innovations that integrated convolutional priors \cite{convit} and enhanced locality through refined tokenization and attention mechanisms \cite{lee2021us0} further improved ViT's ability to learn from less data and even train from scratch \cite{hong2022ks6}. These advancements collectively transformed ViTs from a theoretical curiosity requiring massive resources into a practical and widely applicable architecture, accelerating their integration into the broader computer vision community. While these methods successfully addressed the data-hunger problem, making ViTs viable on ImageNet, achieving stable training for deeper and more powerful variants presented a new set of challenges related to optimization and token representation, which are explored in the subsequent section.