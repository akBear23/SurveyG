\subsection{Enhancing Stability and Tokenization for Deeper Models}

The introduction of Vision Transformers (ViTs) marked a significant paradigm shift in computer vision, demonstrating the potential of self-attention mechanisms for image recognition \cite{ViT}. However, early ViT models faced considerable challenges, notably their heavy reliance on vast pre-training datasets and inherent instability when scaled to greater depths, which hindered their practical application and ability to train effectively from scratch on standard datasets like ImageNet. Addressing these limitations became crucial for the broader adoption of ViTs.

Initial efforts focused on mitigating the data dependency. For instance, DeiT \cite{DeiT} introduced data-efficient training strategies, including knowledge distillation from a convolutional neural network (CNN) teacher, enabling ViTs to achieve competitive performance on ImageNet with significantly less training data. While this improved data efficiency, the fundamental architectural challenges related to training very deep Transformer models, such as vanishing or exploding gradients and general instability, persisted.

A pivotal advancement in enabling deeper and more stable ViT architectures was presented by CaiT (Class-attention in Image Transformers) \cite{CaiT}. This work directly addressed the instability issues encountered when scaling ViTs to hundreds of layers, a common problem in deep neural networks. CaiT introduced **LayerScale**, a simple yet highly effective mechanism that rescales the residual connections within each Transformer block. By adaptively learning per-channel scaling factors for the output of self-attention and feed-forward layers, LayerScale stabilized the training dynamics, allowing the successful development and training of much deeper ViTs without performance degradation. Furthermore, CaiT proposed **class-attention layers**, where a dedicated class token attends to all image tokens to aggregate global information, but image tokens do not attend to the class token. This asymmetric attention mechanism further enhanced stability and improved the representation learning capabilities of deeper models, demonstrating that ViTs could indeed "go deeper" with appropriate architectural modifications.

Complementing these architectural stability improvements, refinements in the initial tokenization process were explored to better capture fine-grained local image structures and optimize sequence length, thereby facilitating more effective training from scratch. The Tokens-to-Token ViT (T2T-ViT) \cite{T2T-ViT} introduced a novel **Tokens-to-Token (T2T) module** designed to progressively structure local tokens. Unlike the original ViT's approach of simply flattening non-overlapping image patches, the T2T module iteratively aggregates neighboring tokens, effectively modeling local connectivity and reducing the sequence length. This hierarchical tokenization strategy allowed T2T-ViT to capture fine-grained details and local contextual information more efficiently, leading to improved performance. Crucially, this enhanced tokenization, combined with the general advancements in ViT training stability, enabled T2T-ViT to achieve state-of-the-art results when trained from scratch on ImageNet, without requiring extensive pre-training on larger external datasets.

The innovations introduced by CaiT \cite{CaiT} and T2T-ViT \cite{T2T-ViT} collectively marked a significant step forward in the development of Vision Transformers. CaiT's LayerScale and class-attention layers provided the necessary architectural stability for scaling ViTs to unprecedented depths, while T2T-ViT's progressive tokenization improved the capture of local features and reduced computational overhead, making ViTs more amenable to training from scratch on standard datasets. These advancements significantly enhanced the practicality, robustness, and scalability of ViTs, moving them beyond their initial limitations and paving the way for their broader application in diverse computer vision tasks.

Despite these critical advancements, the inherent quadratic complexity of global self-attention, even with reduced sequence lengths from improved tokenization, remained a computational bottleneck for very high-resolution inputs or real-time applications. This limitation spurred further research into more efficient attention mechanisms and hierarchical designs, which would become a subsequent major focus in the evolution of Vision Transformers.