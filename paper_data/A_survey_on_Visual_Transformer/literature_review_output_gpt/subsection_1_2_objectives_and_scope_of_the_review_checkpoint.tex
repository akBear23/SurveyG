\subsection*{Objectives and Scope of the Review}

The primary objective of this literature review is to synthesize existing research on Visual Transformers (ViTs), a novel architecture that has significantly transformed the landscape of computer vision. The review aims to highlight key themes, such as foundational architectures, training optimizations, and recent advancements in hybrid models, while also exploring practical applications across various domains, including image classification, object detection, and segmentation. By clarifying these objectives, this subsection sets expectations for the reader and underscores the importance of understanding the evolution and impact of Visual Transformers in contemporary computer vision.

The journey of Visual Transformers began with the seminal work by Dosovitskiy et al. in 2020, which introduced the Vision Transformer (ViT) framework. This paper demonstrated the feasibility of applying the Transformer architecture to image patches, achieving competitive results on large datasets, albeit with a significant requirement for data, which limited its practical applicability \cite{zheng202325h}. In response, Touvron et al. proposed the Data-efficient Image Transformers (DeiT) in 2021, which introduced knowledge distillation from Convolutional Neural Networks (CNNs) to mitigate the data hunger of ViTs, making them more accessible for researchers with limited datasets \cite{hatamizadeh2024xr6}. This foundational work laid the groundwork for subsequent advancements in the field.

Following the establishment of these foundational architectures, a wave of architectural innovations emerged, focusing on enhancing ViTs to address their limitations and broaden their applicability. For instance, Liu et al. introduced the Swin Transformer, which employs hierarchical feature maps and localized attention mechanisms to capture multi-scale information effectively \cite{hong2022ks6}. Similarly, the Pyramid Vision Transformer (PVT) presented a versatile backbone for dense prediction tasks without relying on convolutions, thus expanding the utility of ViTs in various vision tasks \cite{li2022a4u}. Other notable contributions, such as CoaT and CvT, explored hybrid architectures that integrated convolutional inductive biases, thereby combining the strengths of CNNs and Transformers \cite{deng2021man}. These innovations illustrate a trend toward creating more efficient and versatile models capable of handling diverse vision tasks.

In parallel, advancements in self-supervised learning and large-scale training strategies have significantly impacted the performance of ViTs. The introduction of Masked Autoencoders (MAE) and DINO showcased the potential of self-supervised learning to extract meaningful visual representations without extensive labeled data \cite{song202479c}. These methods have enabled the training of larger models, such as SwinV2 and ViT-H, which push the boundaries of model capacity and input resolution, demonstrating that scaling can lead to state-of-the-art performance \cite{xia2022dnj}.

Despite these advancements, several unresolved issues persist in the literature. The increasing architectural complexity raises questions about the optimal balance between pure Transformer designs and the integration of convolutional priors. Furthermore, while self-supervised methods have shown promise, the computational cost associated with training large models remains a significant barrier, particularly in resource-constrained environments \cite{wang202338i}. 

In conclusion, this review aims to provide a comprehensive overview of the evolution of Visual Transformers, highlighting the critical advancements and ongoing challenges within the field. Future research directions may focus on optimizing the balance between model complexity and performance, as well as developing more efficient training strategies that reduce the reliance on large datasets.
```