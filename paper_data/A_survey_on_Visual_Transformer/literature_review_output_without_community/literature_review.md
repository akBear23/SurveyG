# A Comprehensive Literature Review with Self-Reflection

**Generated on:** 2025-10-07T23:09:20.474440
**Papers analyzed:** 367

## Papers Included:
1. c8b25fab5608c3e033d34b4483ec47e68ba109b7.pdf [liu2021ljs]
2. 7a9a708ca61c14886aa0dcd6d13dac7879713f5f.pdf [liang2021v6x]
3. d40c77c010c8dbef6142903a02f2a73a85012d5d.pdf [han2020yk0]
4. 0eff37167876356da2163b2e396df2719adf7de9.pdf [chen2021r2y]
5. da74a10824193be9d3889ce0d6ed4c6f8ee48b9e.pdf [mehta20216ad]
6. a09cbcaac305884f043810afc4fa4053099b5970.pdf [li2022raj]
7. 2fe2f849b94cf08b559226bc9d78adcaef5ef186.pdf [chen2022woa]
8. e5cb26148791b57bfd36aa26ce2401e231d01b57.pdf [xia2022qga]
9. 96da196d6f8c947db03d13759f030642f8234abf.pdf [zhou202105h]
10. 751b71158b7dcd2a7949e72a6ad8fb13657a401c.pdf [liu2021jpu]
11. 164e41a60120917d13fb69e183ee3c996b6c9414.pdf [lee2021us0]
12. 5553f9508dd1056ecc20c5b1f367e9a07e2c7e81.pdf [zhang2021fje]
13. 226fcbe55235d873bedb2fcf5b981bd5ec860d4f.pdf [jiang2022zcn]
14. 5c1b7e400ba8b3c3b04522f7bb3af17ad68d0585.pdf [islam2022iss]
15. a4b728dbbf5afdc231afb95ad4e5c2ececdefc48.pdf [li2022a4u]
16. 0b8e8760d7dd64c7439019aeb3b6ac55dd5075d4.pdf [yao202245i]
17. 17534840dc6016229a577a66f108a1564b8a0131.pdf [borhani2022w8x]
18. b8cee43a51c44f8f4448e78e41ecf081987707cf.pdf [mao2021zr1]
19. 44ee4165b8a3811dc7d557f99150ff9e62f3733f.pdf [chen202174h]
20. 50a260631a28bfed18eccf8ebfc75ff34917518f.pdf [jie20220pc]
21. 3c14992a490cc31a7a38f5fab156c9da40a861d4.pdf [fan2022m88]
22. ab70c5e1a338cb470ec39c22a4f10e0f19e61edd.pdf [lin20216a3]
23. 1ee05cd919590eaba129caa0fda5e850c87b75a5.pdf [lin2021utw]
24. 9fb327c55a30b9771a364f45f33f77778756a164.pdf [li2022mco]
25. dfdb2894d50e095ce97f994ed6cee38554c4c84f.pdf [li2022tl7]
26. f27040f1f81144b17ec4c2b30610960e96353002.pdf [yang2021myb]
27. 4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a.pdf [yu2022iy0]
28. 49030ae220c863e9b72ab380ecc749c9d0f0ad13.pdf [zhuang2022qn7]
29. 60b0f9af990349546f284dea666fbf52ebfa7004.pdf [deng2021man]
30. 64d8af9153d68e9b50f616d227663385bece93b9.pdf [wang2021oct]
31. 03384825d373aabe67c4288ef1eae4d1cf89dc00.pdf [wang2022ti0]
32. d28fed119d9293af31776205150b3c34f3adc82b.pdf [li2022ow4]
33. b52844a746dafd8a5051cef49abbbda64a312605.pdf [wang2022da0]
34. 35fccd11326e799ebf724f4150acef12a6538953.pdf [deng2022bil]
35. 0d1e4faa1580266a56ad62ac2fdd72ed0c0bbbe9.pdf [yang20228mm]
36. 8f7ae0526f9c8706a9e0967eb7942d9b3ffb2788.pdf [gheflati202131i]
37. 00f6041bdbdd00299ef317bd86db3660eb039a8c.pdf [tang2022e2i]
38. 5f7fee3db8879d9d4e714ae20627f00fcbdfab38.pdf [yu202236t]
39. 070c917ab1a4d6b924a9613ca18443f260d8d5be.pdf [li2021ra5]
40. 011f59c91bbee6de780d35ebe50fff62087e5b13.pdf [meng2022t3x]
41. f27c847e2909f30745f4a3528b574f5acfd76ea7.pdf [li20223n5]
42. d717fa5a4355b5033e660ad58a7ae7f4ccce6939.pdf [bazi2022tlu]
43. a119cc83788701313d94746baecd2df5dd30199d.pdf [zheng2022gg5]
44. 60f9fd4e6b29e6f11324ba4f6d3349a2d8107a1b.pdf [gao2021uzl]
45. 5ca02297d8d49f03f26148b74fea77272d09c78b.pdf [zheng202218g]
46. aed7e4bc195d838735c320ac40a78f123206831b.pdf [bi20225lu]
47. b66e4257aa8856df537f03f6a12341f489eb6500.pdf [chen2022vac]
48. f9480350e1986957919d49f346ba20dcab8f5b71.pdf [song2022603]
49. 836dd64a4f606931029c5d68e74d81ef5885b622.pdf [li2022rl9]
50. 16fa1a8575ff56781b6b83726906754ed4e5f3a7.pdf [wensel2022lva]
51. 9f7f81b1c82828a45a52df8f0c6a92636af76c7e.pdf [wang2021sav]
52. e678898301a66faab85dfa4c84e51118e434b8f2.pdf [naseem2022c95]
53. e33434a141bb2881a2e60c518a0cda4feed3f19a.pdf [wu20210gs]
54. 9da3fadf092c864f61d6fd1e8eab5a6ca2397194.pdf [lyu2022vd9]
55. 9fab78222c7111702a5702ce5fae0f920722c316.pdf [krishnan2021086]
56. c051ee2ad7ac203a26fa8f50eb6312424c729b27.pdf [yang20210bg]
57. 13f7a106bb3814ad1fab25fd1356e99e91f402d3.pdf [li20229zn]
58. d04fbbec070b7e7af8152f9b7574971d22bf5997.pdf [wang2022n7h]
59. e939b55a6f78bffeb00065aed897950c49d21182.pdf [chen202199v]
60. 6dc8693674a105c6daca5200141c50362e3044fc.pdf [panboonyuen20218r7]
61. 494e200d8a2ef49d21ce3458a25a109ea917ebe4.pdf [liang2022xlx]
62. 39240f94c9915d9f9959c34b1dc68593894531e6.pdf [zhou2021rtn]
63. 8db96d4eaefdac8dff6d98d94a5e3f5b5558c63d.pdf [dubey2021ra5]
64. 428d755f0c8397ee6d04c89787f3455d323d8280.pdf [ayas2022md0]
65. ff00791b780b10336cc02ee366446d16e1c5e17b.pdf [tian2022shu]
66. 957a3d34303b424fe90a279cf5361253c93ac265.pdf [liu2022249]
67. 401c8d72a9b275e88e6ba159d8d646cfb9f397aa.pdf [zhang2021mcp]
68. 7dee2bc2be709c0009b7623b7af78246f32e0a60.pdf [han2021vis]
69. 1b026103e33b4c9eb637bc6f34715e22636b3492.pdf [kim2022m6u]
70. 024c595ba03087399e68e51f87adb4eaf5379701.pdf [zhou2022nln]
71. 9500dbde37e0f8c6b9ea924aacc6591bc3414166.pdf [hu202242d]
72. 977351c92f156db27592e88b14dee2c22d4b312a.pdf [you2022bor]
73. ba12a9915553b3b42df17a33afcfd547821d8cc3.pdf [ren2022ifo]
74. 3efcd3a4c54694a093886981d59e3cffe0dd7149.pdf [wang20215ra]
75. 58fc305734a0d5d849ae69b9233af082d712197e.pdf [xiao202229y]
76. 54911915a13cf0138c06b696e6c604b12acfe228.pdf [jamil20223a4]
77. b8585577d05cebd85d45b7c63f7011851412e794.pdf [bai2022f1v]
78. 956d45f7a8916ec921df522c0641fd4f02beccb7.pdf [li2022th8]
79. 99fbe810d4194684be03458fdfebacb12d8a5c4e.pdf [almalik20223wr]
80. f4e32b928d7cc27447e312bdc052aa75888045aa.pdf [sha2022ae0]
81. 98e702ef2f64ab2643df9e80b1bd034334142e62.pdf [zhang2022msa]
82. 0ef3e7efa5ded21a398d8ec90ff766b467c4fc54.pdf [htten2022lui]
83. a21fa5ff42db4b0bd0fefae3d710cad5f0175400.pdf [hatamizadeh2022y9x]
84. 174919e5a4ef95ff66440d56614ad954c6f27df1.pdf [montazerin2022dgi]
85. 6971aee925639a8bd5b79c821570728ef49060c6.pdf [kojima2022k5c]
86. 15ad149fc1ba4d8b5de189fdfacac9daecc286d0.pdf [kang2022pv3]
87. f66181828b7621892d02480fa1944b5f381be80d.pdf [tian2022qb5]
88. cee8934975dfbe89747af60bbafc95e10a788dc2.pdf [peng2022snr]
89. 69e20583872b4f1384ae3f7dbdbec648c19d3d35.pdf [ho20228q6]
90. 0594eaa8dfe580678a2382aaf77ac3582c872a97.pdf [xia2022dnj]
91. 3a0145f34bcd35f09db23b2edec3ed097894444c.pdf [wang202232c]
92. ce79649b126dfe9e3cbeb1ecd64a80708bbd5538.pdf [mogan202229d]
93. 572ed945b06818472105bd17cfeb355d4e46c5e5.pdf [yang20221ce]
94. 934942934a6a785e2a80daa6421fa79971558b89.pdf [li2022ip7]
95. 3ae8c0b646ddce95ffd09da31c02ed6fdb744e90.pdf [yu20220np]
96. bc8d9f11ad91d54e83ad7cc3900502a913499fcb.pdf [li20229fn]
97. cf6d947d5d2ee72873a5a7b97dde9f881f79a6b6.pdf [huang2022iwe]
98. 9db52b92b48b8855f76d7e4b51b7727a2b3c0271.pdf [qu2022be0]
99. e4add4391dfa2a806a50cc1fbe9a9696dac9501f.pdf [zeng2022ce2]
100. 324f97d033efd97855488cf0b15511799fe7b7f7.pdf [lin20225ad]
101. bf52f09d648d78677f140e63c7c83ca44fcd438f.pdf [reghunath2022z8g]
102. 4eba4dccc7d19c40dfd02eef447a57613c3644e2.pdf [kundu2022z97]
103. 0bdcea2f18e158bbd1723ed27d48e5211545e050.pdf [sun2022cti]
104. 67b7ad5f3c818f42c8359abc87f353202f76f6f2.pdf [li2022gef]
105. 3994996a202f0127a58f57b259324a5283a1ba27.pdf [guo20228rt]
106. 4767600089dd71c7ed74c6a0acaf9e5a6fb345c9.pdf [li202240n]
107. d5b8af7ae526e3a511b1172c26d7caf7421145bb.pdf [jiang2022jlc]
108. 4d491b6fbe529a3986ef50cc34ede7c9ad88126c.pdf [lin2021oan]
109. d9e4af7739d4d5dcd44619bf276dc497f7334e34.pdf [wang2022dl1]
110. d69102eec0fff1084e3d1e24a411103280020a32.pdf [li2022wab]
111. 38bef6f0b4de2f9809b4fd44e8b61f9bebb14db9.pdf [park2022eln]
112. b80d0ce78652ffbf6b3ddebfc567f8594a6ffd4b.pdf [shen2022d6i]
113. 7d4d512445903e8ad75f0dcfa8baec27b1fe8620.pdf [wang20224wo]
114. 7d5274f1155b85a6120491c9374b6983dac96552.pdf [tao2022gdr]
115. 0afbaae7d0bfe98a04a3b7dd1ba69e25f32b24c6.pdf [wu2021nmg]
116. 6bcdf04ccd35ae971e765cdda25127ce005b7713.pdf [liu2022c56]
117. 6fd1002f321240c75dfc86c6bad6c05068a8160a.pdf [wang2022pb8]
118. c4560ab3855cb8ae4b9922458a0b6b94108e2c28.pdf [xiong2022ec2]
119. a0762777d6e46acbcd5b301285c2d9894065ff8f.pdf [sun2022pom]
120. d43950779dc86b728d7e002be6195526d35a26b0.pdf [qi2022yq9]
121. 2e69d97ae22c6a3685cc548f8c19c696d5d7d363.pdf [ma2022vf3]
122. c25091718b22384cebece2da7f30fc1702a07c76.pdf [wang2022tok]
123. cec0cbc2dd6d7975714110632b6bfcb5c1927ec3.pdf [wang2022pee]
124. ef93c81f90154a022e164be2f83c6cf6e602f33d.pdf [jannat20228u6]
125. 6a8d4a97ba3eafcb85d2d2a57d7d514bb592dbd0.pdf [chen2022r27]
126. 64143b37ae41085c4907e344ff3a2362a3051d0c.pdf [wang2021p2r]
127. dcd8617200724f0aa998276be339ff4af589ee42.pdf [sajid2021xb6]
128. 46880aeca86695ca3117cc04f6bd9edaf088111b.pdf [xing2022kqr]
129. 7e0dd543471b66374fbf1639b9894d3d502533b6.pdf [garaiman2022xwd]
130. 845a154dbcde81de52b68d73c78fad5be4af3b20.pdf [wang2022wyu]
131. 6f4093a7ad5378e8cd3b73a52fbec80b784c107d.pdf [hou2022ver]
132. 50405c1ee26c97ed5b9a54c7179317a424e6e471.pdf [agilandeeswari202273m]
133. 1bf9a97a7581d1671d683c19ce36fed6511dc95b.pdf [qin2022cfg]
134. e58c2bc5333ee8f37150d9d15c0428d9658b2e50.pdf [wang2022ohd]
135. 761f55a486e99ab5d3550aee48df34b6b65643c2.pdf [yu2022o30]
136. 2dcc1f5f1ba11f99e09f22c53d02aa66cec35c48.pdf [boukabouya2022ffi]
137. 52a7f15085f1b6815a4de2da26df51bb63470596.pdf [wang2022d7p]
138. 649b706ba282de4eb5a161137f80eb49ed84a0a8.pdf [song20215tk]
139. 186295f7c79e46c0e4e5f40e094267c09714043d.pdf [xie2021th0]
140. d77288fc7de7b15c200ed75118de702caf841ec3.pdf [sun2022bm5]
141. 2e4dbc3dbd400346be60318ae558a0293e65ba81.pdf [jing2022nkb]
142. 3de95f33c2b4f61a9c0f335b4810a966e209a47a.pdf [li2022spu]
143. 7817ecb816da8676ae21b401d60c99e706446f06.pdf [song2022y4v]
144. 3502b661362b278eebacf1037fc3bb4e21963869.pdf [shukla2022jxz]
145. 791d1e306eaa2e87657925ec4f45661baa8da58b.pdf [tran2022bvd]
146. 1d8ecf2c3a78e5dc4de9e4c41961771bbd0033b3.pdf [hong2022ks6]
147. e1f7478294fe01ce271cdef9ba93f4c675d92dc9.pdf [panboonyuen2021b4h]
148. a56f8e42e9efe5290602116b42a247b758052fe4.pdf [zhao2022wi7]
149. 6eb10790689a4cf239d1ee2a3919dced4be6db8f.pdf [wang2022h3u]
150. fc2e7fcdc1bd773f1eb097ae67c8f736108276e3.pdf [liu2021yw0]
151. 16ad38c73f4897e1c58326e3562b74ae1c2fd219.pdf [gul202290q]
152. 371e924dd270a213ee6e8d4104a38875105668df.pdf [zhao2022koc]
153. 0025c4241ffb2cce589dc2dcd82385ff06455542.pdf [yang2022qwh]
154. f2b3ece7cb0c88701ae0055236f6d6da39c8156d.pdf [alquraishi2022j3v]
155. 1eb4bbaa204890a45da2ca713f34fcc2d763c6e1.pdf [jin2021qdw]
156. 1b18750ea0d26575f3e2c18f61e444c2790dbe4c.pdf [lee2022rf1]
157. 08502153c9255399f8ff155e5f75900f121bd2ff.pdf [shi2022evc]
158. cb85a7648d2a4b4953abfc7437a33f91dc2f3673.pdf [zhang20223g5]
159. 90f9b2892a437ca32c4ab26c09b8dbb7de8fa034.pdf [zim202282d]
160. 7ead0cae4e67f390b2eb0083117ea8ab90c53b47.pdf [bao202239k]
161. e4864ea54710b4c8f3b2ca9ac39854bdfd0dae28.pdf [sun2022nny]
162. fe6887ee065c4f64a0c8d6054d1fc562766452de.pdf [munyer2022pfs]
163. 280ea33e67484c442757fe761b75d871a399905d.pdf [fan2022wve]
164. 29f86d6d1eaba6a466c231f6906b18eae4b2b484.pdf [wang2022gq4]
165. d8bf4b494d255c6d9b81e9963f29e40831106e3e.pdf [ali2022dux]
166. abf037290e859a241a5af2c5adf9c08767971683.pdf [chougui2022mpo]
167. dd46070ce18f55a5714e53a096c8219d6934d188.pdf [zhuang2021hqu]
168. 829926d9943c99dd64eea5b4b96541eca1e5e91d.pdf [chen2021d1q]
169. e8dceb26166721014b8ecbd11fd212739c18d315.pdf [hatamizadeh2024xr6]
170. e06b703146c46a6455fd0c33077de1bea5fdd877.pdf [ryali202339q]
171. 3af375031a3e23b7daf2f1ed14b5b61147996ca0.pdf [yao2023sax]
172. d203076c28587895aa344d088b2788dbab5e82a1.pdf [li2023287]
173. f3d0278649454f80ba52c966a979499ee33e26c2.pdf [zhao2024671]
174. 918617dbc02fa4df1999599bcf967acd2ea84d71.pdf [dehghani2023u7e]
175. 51f38bd957fa863022feb5878fa1ba3bea6657cf.pdf [duan2024q7h]
176. 1f389f54324790bfad6fc40ac4e56428757ea92b.pdf [barman2024q21]
177. 05236fa766fc1a38a9eb895e77075fb65be8c258.pdf [jamil20230ll]
178. 0eec6c36da426f78b7091ba7ae8602e129742d30.pdf [paal2024eg1]
179. 689bc24f71f8f22784534c764d59baa93a62c2e0.pdf [zhang2023k43]
180. afb7ff7096cd45de02de96b5937c3416ebca0cb0.pdf [himel2024u0i]
181. 595adb75ddeb90760c79e89b76d99e55079e0708.pdf [xu20235cu]
182. de20c6805b83a2f83ed75784920e91b913d888bb.pdf [chi202331y]
183. c57467e652f3f9131b3e7e40c23059abe395f01d.pdf [patro202303d]
184. 53e5db85e2a7442f20670be2ae25019fcf9d27a2.pdf [pan2023hry]
185. 0682771fd5f611bce2a536bf83587532469a83df.pdf [wang2024mrk]
186. a4ef2c7c321dea04d8ea91ec1ce0710ebce4f848.pdf [tabbakh2023ao7]
187. 243a056d1acb153f70e39cc80a10e7d211a4312f.pdf [dutta2023aet]
188. d8ab87176444f8b0747972310431c647a87de2df.pdf [qiu2024eh4]
189. a1e17a09b0df676f133c2175366ab7fcd0381ac1.pdf [li2023nnd]
190. 77b625e6dec2732b96ca8cf57d7a123497873e1d.pdf [zhao20243f3]
191. e91934d66d9133d854ff0a4cafbe7966584bbf97.pdf [song2024fx9]
192. e37539f5c943a92ef56b49b7fa067bd976e418d4.pdf [cai2023hji]
193. 8fc9a7f23c5cf57e17790ffb99b6acb11693c673.pdf [akinpelu2024d4m]
194. 7ddbb9723420cb474c58d4b6b6834d5ebe112049.pdf [hayat2024e4f]
195. c7c0c84fa505995cd888a8fbaa9def21cb84cca3.pdf [li2024g3z]
196. 1af8c50b4c7329fb2273aa7b755b0f9e8223e2d2.pdf [arshed2023zen]
197. bca0adcba0c26c5a34b05795c5bfbc1948bf2fae.pdf [qin20242eu]
198. c4895869637f73154d608cdd817234b0dbcd3508.pdf [lee2023iwc]
199. 64811427a4427588bb049a6a254446ddd2cafacc.pdf [tagnamas20246ug]
200. 7a0f92968c63613730b0c06b3bc3bd40d2666571.pdf [li2023jft]
201. 136b0ef7bd7f642a285b1d61ddf7abfc7ddc026f.pdf [song2024c99]
202. cf439db0e071f19305ea1755aa108acdde73ed99.pdf [aksoy20240c0]
203. ffc855594cad345ea5a1cce2ee27095bec767bc8.pdf [leem2024j4t]
204. 838d7862215df504dde41496cbe6ee711a12ae9f.pdf [chen2024asi]
205. 9a35a0aac04f4f712f38ea9bbfd2f0b0108242f5.pdf [lin202343q]
206. f6bf7787115affe22c410eb5b2606269912d59a0.pdf [ghahremani202491m]
207. 69e56df0ae079c83607bb48e68f8db39a4177cd0.pdf [wang20249qa]
208. 1528bab641830ea279bdf53ff9e7d11d2b7e5028.pdf [shahin2024g0q]
209. b43bb480caad36ab6fd667570275d42fe9050175.pdf [zhu2023dpi]
210. 1970ace992d742bdf098de08a82817b05ef87477.pdf [yu2023l1g]
211. fec86abbb29b559c1eaff31428f5b59f8070bb67.pdf [ko2024eax]
212. cb8b0eba078098000f004d7e0f97a33189261f30.pdf [yang2024w08]
213. 9b4d81736637392adabe688b6a698cec58f9ce57.pdf [nazih20238nf]
214. 981970d0f586761e7cdd978670c6a8f46990f514.pdf [xia2023bp7]
215. 6bdafb965e94c5240db2c30f20c37c4b4dd0e451.pdf [nag2023cfn]
216. a3d1cebf99262cc20d22863b9540769b49a15ede.pdf [gezici20246lf]
217. f05176e2d3f7a6e95b60f0e96fd9bf20e8b335d2.pdf [ghazouani202342t]
218. 442b5ec3aad4b099e71d6203a62eb7ec7519544c.pdf [wang202338i]
219. 635675452852e838644516e1eeefd1aaa8c8ac07.pdf [guo2024tr7]
220. d2fce7480111d66a74caa801a236f71ab021c42c.pdf [wang2023ski]
221. 5135a8f690c66c3b64928227443c4f9378bd20e1.pdf [zheng202325h]
222. 77eea367f79e69995948699d806683c7731a60b1.pdf [mogan2023ywz]
223. 861f670073679ba05990f3bc6d119b13ab62aca7.pdf [ebert202377v]
224. f93f718bb2d3f5e4354e174e54e71c5596f56ea3.pdf [wang20245bq]
225. c5c9005aae80795e241de18b595c2d01393808f8.pdf [cao20241ng]
226. 14c42c0f2c94e0a1f4aa820886080263f9922047.pdf [yang2024in8]
227. 9998291d71f4e8ddf59f4b016b19df1f848eeed1.pdf [hussain2025qoe]
228. 9fa44f616e796f187d97b9cc324bd839bc21ad81.pdf [shim2023z7g]
229. d62e6ba83d3da5b75e1d428f20f8199e3754bd5d.pdf [alam2024t09]
230. d1faaa1d7d312dd5867683ce60519979de6b3349.pdf [yang2024tti]
231. d68aedc3a957c25e5f19438ec4b64fb53b48422c.pdf [wang20245hx]
232. bb5e6bbd4ad8c613a0bac3db23940c03ce94bec9.pdf [song202479c]
233. 55156532cb9c20fdcaed9ead238f7a2cbaab2527.pdf [li2023lvd]
234. ae7c5ae7d377189c5a5f33a60b33c6cc5ca56124.pdf [ma2023vhi]
235. 3b427c8d3258968b9ac5eaf846d99ac027de9a76.pdf [han202416k]
236. 10e9943b3a974ac6175ffec3228e670ec9d2cc18.pdf [katar202352u]
237. 1783c85f8b951dc11c9dcb4929bfcae2d52ecb63.pdf [hemalatha2024a14]
238. 42bd7d3ca2b66eca5609fe77078fe8951ab9fd5c.pdf [ma2024uan]
239. 7aeadf7b9d839bdd9dbfbff5239f71792ff0c04c.pdf [lai20238ck]
240. b2becca9911c155bf97656df8e5079ca76767ab9.pdf [wang2024luv]
241. 25a69bdd7f87ecc287b4e9a64eb4d6d562371add.pdf [ling2023x36]
242. 50e997b23a534a6fbfd32d63990fa80373ec7c6b.pdf [wang2023bfo]
243. 3ea79430455304c782572dfb6ca3e5230b0351de.pdf [yin2023029]
244. 0284fa05879f1609bf459f15a4d40e22355cd5ae.pdf [mishra2024fbz]
245. 714e21409b8c4f7788ac8c93795249a4e45e51ce.pdf [heidari2024d9k]
246. 2ef224f7a1f545eb8bfdf61c7b849f2676fe6a61.pdf [yu2023fqo]
247. bc99c855d52ba3d432c428fb4096b3a22c04f8bf.pdf [zhao2023pau]
248. 3dee43cea71d5988a72a914121f3455106f89cc7.pdf [pan20249k5]
249. 1871c0bc27bd5cd5f2f6c3d6cedeb4b8c5a89499.pdf [huan202345b]
250. c064efa0419b75ba131ec8470ed80f01e1a80f64.pdf [belal2023x1u]
251. b48a85980deb5f1baa64d862b9f0e4e62124e4de.pdf [li20238ti]
252. 8ec10ffe0c1fc8f6a92d541f0e002e78080b564a.pdf [huo2023e5h]
253. 769ba82ab77fc46db4a594dc64576286d02cf1d7.pdf [kim2023cvz]
254. d1255f6e7e7e14b253fe2245e842d0b45f5e715e.pdf [fan2023whi]
255. 5572237909914e23758115be6b8d7f99a8bd51dc.pdf [zhao2023rle]
256. 21c37fd6d00d6c727ec10f0c871c1ddf6e4f7ed9.pdf [xie20234ve]
257. e38e70580acb204c05096de8da90b7ab1d4bdb6b.pdf [li20233lv]
258. a246677a3d68e8cd7a63d24639a5cf3fc3b9f56e.pdf [ma2023qek]
259. 1397d1bcfeae68078756fdbc3ed7e62ef51a1e32.pdf [tanimola20246cv]
260. 52239e2338cd9f0e0e690203af8f0f1260ef7abf.pdf [chen2023xxw]
261. f996d5ee3b8ad3c60510862a92fd72c6a41777e0.pdf [ranjan20243bn]
262. f1b6f564e7cba8121df0eef0d656673e3bd18621.pdf [fu20232q3]
263. 12106888ec74baf48d1e9fb64e8932c9ce509e84.pdf [shi20235zy]
264. 3798e7f16fe69c29307a9bab4f0f4d779478afc5.pdf [deressa2023lrl]
265. 29a0077d198418bab2ea4d78d04a892ede860d68.pdf [aburass2023qpf]
266. ac9cc0c28838a037e77f4e19433de170f47b3de9.pdf [hassija2025wq3]
267. c4357abf10ff937e4ad62df4289fbbf74f114725.pdf [huang20238er]
268. 0b41c18d0397e14ddacee4143db74a05d774434d.pdf [liu20230kl]
269. f9e1a8754e77a4b5f240d11e9e81e2563a319b89.pdf [he20238sy]
270. 4b9568a9798c527779a1f7479ffb3ba373640e3e.pdf [guo2023dpo]
271. 34ca004eda791c8c4191e0ebe65cc4405a116b08.pdf [wang2023j6b]
272. 409b43b8cd8a2ba69f93e80c2bacc0126238b550.pdf [gopal20237ol]
273. dc5d707a8f9efaf32d2319ec2ca8d2619ef2926e.pdf [liu2023awp]
274. 9017053fb240d0870779b9658082488b392e7cde.pdf [fu20228zq]
275. f27221a15f4c3c8af7ea0ada5bab25831ffe21ea.pdf [sahoo20223yl]
276. 2d421d94bc9eed935870088a6f3218244e36dc97.pdf [ganz20249zr]
277. be1aabb6460d49905575da88d564864da9f80417.pdf [paal2024no4]
278. 0d7d27fbd8193acf8db032441fd22945d26e9952.pdf [hassan20243qi]
279. 0af7ccb9f6cf2edb35674dc97d0ec5d165f8433b.pdf [k2024wyx]
280. f9efe4918b9a20c2e8084d7a5ec624b87ec1f211.pdf [nguyen2024id9]
281. f62cb393ab51505bdd22823e74b7cf28f4801e1c.pdf [almohimeed2024jq1]
282. 4702a22a3c2da1284a88d5e608d38cd106d66736.pdf [hao202488z]
283. 9fcea59a7076064f5ac3949177307c1637473ffd.pdf [yao20244li]
284. 1ec9b653475287e95fdaef2f5247f82a8376c56c.pdf [dong20245zz]
285. 2b48f5f0769bf41b5a3025ace73417d19de65cf1.pdf [zhang2024jha]
286. bfb310434204d99fa5c420ac272966c5ae1c1bd2.pdf [boukhari2024gbb]
287. bbe5dfbecfd1bed7556b9c8269b0d363faa24973.pdf [song2025idg]
288. be28edb064e2050e0852ff376434ee39f7c4b0a3.pdf [zhou2024tps]
289. e25a0b06079966b8e43f8e1f2455913266cb7426.pdf [abbaoui20244wy]
290. ecd9598308161557d6ac35b3e4d32770489e811d.pdf [yang2024nyx]
291. 7dc4b2930870e66caa7ff23b5d447283a6171452.pdf [yang20241kf]
292. b1f0ac0b5e4044251dcdcce78e0046feb43cc7c7.pdf [hu202434n]
293. 903c8a82f5539b3e482efcb23ff59819f04f2c0f.pdf [yang20244dq]
294. 1e95bb5827dc784547a46058793c15effd74dccc.pdf [keresh20249rl]
295. 2104eea2b01b7a4b6ba2d256769328ac4919afad.pdf [hu20247km]
296. 8972ebf4f9ec47ee60c3ed0effd520daff495d5a.pdf [alsulami2024ffb]
297. 0fde8e3287ac04dcf210b73898c8ceb697ad489f.pdf [yang2024wxl]
298. 9a4718faa07a32cf1dce745062181d3342e9b054.pdf [p2024nbn]
299. 6e97c1ba023afc87c1b99881f631af8146230d96.pdf [wu2024tsm]
300. 1d7183d481ae5a396743dde39984f1f0c8f47edf.pdf [dong2024bm2]
301. 9fbf36da9ee60d56a7675042bb0b24798a3b966d.pdf [swapno2025y2b]
302. d80166681f3344a1946b8bfc623f4679d979ee10.pdf [yoo2024u1f]
303. 9285996627124b945ec601a763f6ff884bac3281.pdf [he2024m6j]
304. 3f002ea3ede9d97f1fe29b16691491219c1e626d.pdf [zhang202489a]
305. 9121dcd10df00e5cc51dc94400e0325e0ae47bb9.pdf [zhang2024pd6]
306. 08606a6a8b447909e714be2c3160074fdf1b91ad.pdf [dong20242ow]
307. 0222269c963f0902cc9eae6768a3c5948531488b.pdf [kayacan2024yy7]
308. 8776fd7934dc48df4663dadf30c6da665d84fb19.pdf [liu20248jh]
309. cc24f933b343b6a9701088cf6ae1dbf3299c0c9e.pdf [shi2024r44]
310. 88589b0b2d2d8caa09d8ca94414343455ae87d7c.pdf [xin2024ljt]
311. d629289cc2f3efbe374a66d57690dd84f19d59ec.pdf [zhou2024qty]
312. 70211e2b04fcb4413edfd43e75b9f10e8d130171.pdf [monjezi2024tdt]
313. cc817ba09f1c3c94bae73576463bcaf57c70261a.pdf [baek2025h8e]
314. 77e4ea46e7248408844f62fa3613aa246e35fdf0.pdf [payne2024u8l]
315. 271c6a87213b8b2a26ce3e1a6f20b07cc2f82b81.pdf [qi2024rzy]
316. 05d15576d88f9384738908f98716f91bdb5dbc78.pdf [mercier2024063]
317. 6feea99a1a6c876d6f4f714b463da9c95998dee7.pdf [sikkandar2024p0d]
318. c05744f690ab9db007012a63c3c5c3ca48201c66.pdf [hou2024e4y]
319. cebc29015a2827aa8a98ec39c90c93bca2f14848.pdf [nfor2025o20]
320. 630c9993b6aaf812e1aa3dae8e243af131abd3d0.pdf [xiang2024tww]
321. d14a1677e416bd7ac6fbe01672cd3152fc1f983d.pdf [tian20242kr]
322. 8ed42c0d60eb4c91e99ee184a17719f7c3ce3f3f.pdf [zhou2024r66]
323. c4cf58e104fc8d783dfd0bcf3ccd65f8e4028d7f.pdf [taye20244db]
324. 72e23cdc3accca1f09e2e19446bc475368c912d0.pdf [alohali2024xwz]
325. 7b6d64097d16219c043df64e4576bd7d87656073.pdf [gao20246ks]
326. 0820d2ac35cd55fd60f53c14460cca78fa996f1e.pdf [du2024s3t]
327. b02144ef4ed94df78544959bc97eddef4580dd95.pdf [tiwari2024jm9]
328. b6488fded5fb0728d7c3bd04f3afa3fba68d0450.pdf [du20248pd]
329. 24eba2f5345bc8279bb63f085e75cc26b37d0b1e.pdf [chaurasia2024tri]
330. 8ce6872b26f88e52aeed57f6f7528dee30b5f7b2.pdf [karagz2024ukp]
331. dcf77b8e23bbc56c5c8f76ddc5d3cf68d0a051be.pdf [lee2025r01]
332. f792dcf878e5bf8e7fb5aa31d231474d43462f4a.pdf [dmen2024cb9]
333. 310f5543603bef94d42366878a14161db1bf45de.pdf [ferdous2024f89]
334. f9f524944dd9d1fe4b8956f9a822927b2eec29ad.pdf [akan2024izq]
335. 6ec48cb5fa3d149c9cc8a29373e64b63fe257913.pdf [nahak20242mv]
336. 3c7e07304bab6a860dbbe4ff36a4d87010036d2a.pdf [han2024f96]
337. 819ae728828d50f56f234e35832b1222de081bfc.pdf [zhao2024p8o]
338. 6a9247fe471690218ef04cea5f6f4b59b3b50747.pdf [li2024qva]
339. f462bb00b8c4379c4a4699b66a19ce10da530b08.pdf [wang2024ueo]
340. 52fafbbf20a37ffe863f1fa1d44fde6b4fc3334e.pdf [qi2024f5d]
341. b4be4f2ecb8711755f75dcb09a57d8c9722ac349.pdf [zhu2024l2i]
342. 7492734c76036143baf574d6602bd45a348c416f.pdf [roy2024r9y]
343. eacb2581b480cf4a80bc601c8ea657651b41c0fc.pdf [wang2024w4u]
344. da7d671f61e53601bdfc760d336cc98e5d9c516b.pdf [pan202424q]
345. 2bb98b30fb9aae37858248cd5ac3221ebcf7eeba.pdf [du2024lml]
346. 90e9d0a35bcf7e8251c4c9601a224bd95db14dd9.pdf [luo202432g]
347. 5ce1ebe051a64969caeb879c44c45320a5ca8011.pdf [elnabi2025psy]
348. 3bba1b3376e43a39bf4f1bf4eab558758a37ea2e.pdf [ergn2025r6s]
349. 3b11c8774bbeddd6722d65455bf9bb729a63c1cb.pdf [mohsin2025gup]
350. 04ed3b0f4e21d9f4884bf4db98edf83a34d005a9.pdf [marcos2024oo2]
351. 23ce9c2814d6567efec884b7043977cefcb7602e.pdf [peng2024kal]
352. 5b22bdc6aedf13d812509dd0f768353eb1469a79.pdf [urrea20245k4]
353. d2fd0dc314023ebca1342f1379ba6d79c7ded84a.pdf [zhang2024b7v]
354. 1c96dc4ad4dfa3bf7c6ca9cf0c6197e7ac00272b.pdf [saleem20249yl]
355. c35c316feba84334a4b08feb5281a5f83db0b8b2.pdf [zhou2024toe]
356. c8174af99bc92d96935683beccc4161c65a8aa46.pdf [lijin2024mhk]
357. 05548c4b3db8be40fac236dfa8e21882fe9ce9de.pdf [huang2024htf]
358. bd7a4a9e020b594ebcacb886ddae8731a2885209.pdf [chen2024cha]
359. a7df70e86f049a86b1c555f9a399d3540f466be7.pdf [shahin2024o1c]
360. e4b0c4a1d7ee87ad666214172f329831ade4c25d.pdf [xu2024wux]
361. 6604a900b9a7404a447b2167892a947012a9ffb8.pdf [park2024d7y]
362. 9814001811c4924171787de52e01cc31446e2f97.pdf [elharrouss20252ng]
363. ab10aacab1a2672a154034c589dd0aa801912272.pdf [du2024i6n]
364. 325367f93439652efaa4bc6b50115bbb7371704e.pdf [guo2024o8u]
365. 3c6980902883f03c37332d34ead343e1229062b3.pdf [zhang2024g0m]
366. f2b1b0fb57cccaac51b44477726d510570c4c799.pdf [xu2025tku]
367. 2456506ed87faa667a0c2b8af4028a5a86a49650.pdf [li2024m4t]

## Literature Review

### Introduction

\section{Introduction}
\label{sec:introduction}



\subsection{Motivation for Visual Transformers}
\label{sec:1_1_motivation_for_visual_transformers}


The advent of Vision Transformers (ViTs) marks a significant paradigm shift in computer vision, primarily driven by the inherent limitations of Convolutional Neural Networks (CNNs) in capturing global contextual relationships and the groundbreaking success of Transformer architectures in Natural Language Processing (NLP). For an extended period, CNNs were the undisputed standard for visual recognition tasks, owing to their powerful inductive biases such as locality and translation equivariance. These biases, intrinsically embedded through spatially restricted convolutional kernels, are exceptionally effective at extracting local features and constructing hierarchical representations [han2020yk0]. However, this very strength became a critical constraint when models needed to comprehend long-range dependencies and holistic contextual information across an entire image. CNNs, by design, inherently struggle to model interactions between spatially distant parts of an image without resorting to increasingly deep architectures, significantly larger receptive fields achieved through techniques like dilated convolutions, or complex add-on modules such as non-local blocks [Wang2018NonlocalNN, zhou2021rtn]. While these methods attempted to mitigate the issue by expanding the effective receptive field, they often introduced increased computational complexity or did not fundamentally alter the local processing paradigm. As highlighted by [gheflati202131i], CNNs' restricted local receptive fields inherently limit their capacity for global context learning, making it challenging to capture comprehensive image understanding. Similarly, [liu2022249] and [karagz2024ukp] explicitly note that while CNNs excel at representing local spatial features, they find it difficult to capture global information, underscoring a fundamental gap in their representational power for certain tasks.

The landscape of deep learning was profoundly reshaped by the Transformer architecture, introduced by [Vaswani2017] in NLP. This model demonstrated the unparalleled power of self-attention mechanisms to capture global dependencies within sequences, processing information from all input parts simultaneously. Unlike recurrent networks that process sequentially or CNNs with their localized focus, the Transformer's global attention mechanism, largely devoid of strong inductive biases about local connectivity, offered a compelling alternative for learning flexible, context-aware representations. The remarkable ability of Transformers to achieve state-of-the-art performance across diverse language tasks inspired researchers to question whether a similar global approach could unlock new capabilities in computer vision, overcoming the limitations of CNNs in capturing holistic image understanding and offering a less constrained path to feature learning [han2020yk0]. This inspiration was rooted in the desire for models that could intrinsically understand the relationships between any two parts of an image, regardless of their spatial separation, without being constrained by fixed-size kernels.

This profound inspiration culminated in the seminal work by [Dosovitskiy2021], which introduced the Vision Transformer (ViT). This pioneering paper directly adapted the pure Transformer architecture to image recognition by treating image patches as a sequence of tokens, demonstrating that a model built entirely on self-attention, when trained on sufficiently large datasets, could achieve state-of-the-art performance without any convolutional layers. This was a profound challenge to the long-held paradigm of convolutional feature extraction, showcasing that global attention could indeed learn powerful visual representations and capture long-range dependencies more effectively than CNNs. The ability of ViTs to learn highly transferable representations, often outperforming CNNs on various downstream tasks when properly pre-trained, further solidified its revolutionary potential [zhou2021rtn]. For instance, studies like [htten2022lui] have shown that ViT models can achieve equivalent or even superior performance to CNNs in complex industrial visual inspection tasks, even with sparse data, highlighting their robust feature learning capabilities. This successful adaptation of a pure Transformer to vision tasks marked a definitive turning point, setting the stage for a new architectural lineage in computer vision that prioritizes global context and flexible feature learning.
\subsection{Overview of Vision Models: From CNNs to Transformers}
\label{sec:1_2_overview_of_vision_models:_from_cnns_to_transformers}


The trajectory of computer vision research has been marked by a significant paradigm shift, evolving from the long-standing dominance of Convolutional Neural Networks (CNNs) to the recent ascendance of Transformer architectures. For decades, CNNs served as the foundational backbone for visual recognition tasks, owing their success to inherent inductive biases that align well with the hierarchical and local nature of visual data [han2020yk0]. Key among these biases are local receptive fields, which allow neurons to process only a small, localized region of the input; weight sharing, which enables the detection of features regardless of their position; and spatial pooling, which progressively reduces spatial dimensions while retaining essential information. Architectures like AlexNet [alexnet_2012], VGG [vgg_2014], ResNet [resnet_2016], and Inception [inception_2015] exemplified this era, demonstrating remarkable capabilities in tasks ranging from image classification to object detection by building increasingly complex and abstract representations through stacked convolutional layers. Their architectural evolution often involved increasing depth, introducing residual connections to mitigate vanishing gradients, and designing more intricate modules to enhance representational power, all while retaining the core principle of localized feature extraction.

Despite their profound success and continuous advancements, CNNs inherently faced limitations, particularly in effectively modeling global contextual information across an entire image. Their reliance on local operations meant that capturing long-range dependencies required very deep networks, which could be computationally intensive and sometimes struggled to integrate information from widely separated regions efficiently. This inductive bias towards locality, while beneficial for many tasks, could also constrain their ability to understand broader semantic relationships or complex spatial layouts without extensive architectural modifications [han2020yk0, zhou2021rtn].

A pivotal moment arrived with the introduction of the Transformer architecture [attention_is_all_you_need_2017], originally conceived for natural language processing (NLP). Its core innovation, the self-attention mechanism, revolutionized sequence modeling by enabling models to weigh the importance of different parts of an input sequence, fostering a more global understanding of relationships without relying on recurrent or convolutional operations. This mechanism allowed for parallel processing of inputs and offered an unprecedented ability to capture long-range dependencies directly. The profound success of Transformers in NLP inspired researchers to explore their applicability to other domains, including computer vision [heidari2024d9k].

This exploration culminated in the introduction of the Vision Transformer (ViT) [image_is_worth_16x16_words_2021]. The ViT architecture ingeniously adapted the standard Transformer encoder for image recognition by treating images as sequences of non-overlapping patches. Each patch was linearly embedded into a token, and positional embeddings were added to retain spatial information. These visual tokens were then fed into a Transformer encoder, leveraging its global self-attention mechanism to capture relationships between patches across the entire image. This marked a significant paradigm shift, demonstrating that models could achieve competitive performance in image recognition without relying on convolutional layers, instead directly leveraging the Transformer's ability to capture global dependencies. This approach fundamentally altered how visual features were extracted and processed, moving away from predefined spatial hierarchies and towards a more flexible, data-driven understanding of global context [han2020yk0].

The initial success of ViTs, however, was accompanied by practical challenges. Pure ViT models often required significantly larger datasets for pre-training compared to CNNs to achieve comparable performance, largely due to their reduced inherent inductive biases [image_is_worth_16x16_words_2021, zhou2021rtn]. Furthermore, the quadratic computational complexity of global self-attention with respect to the number of tokens (and thus image resolution) posed considerable computational costs, limiting their applicability to high-resolution images or resource-constrained environments. These initial limitations became the primary drivers for an explosion of subsequent research aimed at enhancing the efficiency, data-effectiveness, and architectural flexibility of Vision Transformers.

In conclusion, the transition from CNNs to Transformers represents a fundamental evolution in computer vision, shifting the focus from local, hierarchical feature extraction to global context understanding through attention mechanisms. While CNNs provided a robust foundation, the advent of ViTs offered a new paradigm capable of overcoming some of their inherent limitations, particularly in modeling long-range dependencies. This breakthrough, despite its initial challenges related to data hunger and computational expense, has spurred extensive research into architectural refinements, efficient training methodologies, and hybrid models. These ongoing efforts, which will be explored in detail in the subsequent sections of this review, are continuously solidifying the position of Vision Transformers as versatile and powerful backbones for a wide array of computer vision tasks, fundamentally altering how visual features are extracted and processed.
\subsection{Scope and Organization of the Review}
\label{sec:1_3_scope__and__organization_of_the_review}


This literature review aims to provide a comprehensive and systematically organized overview of Vision Transformers (ViTs), tracing their rapid evolution from foundational principles to their current state as a dominant paradigm in computer vision. By delineating the boundaries and structure of this review, readers are provided with a clear roadmap, ensuring a coherent and comprehensive understanding of the field's progression, its intricate interconnections, and the critical research trajectories that continue to shape its future. This structured approach, similar to recent surveys on ViTs that categorize design techniques and innovative methods [heidari2024d9k, hassija2025wq3], is designed to facilitate a deep understanding of the architectural, methodological, and application-driven advancements.

The review is meticulously structured into seven main sections, each building upon the preceding one to offer a logical and progressive narrative of ViT development:

The journey commences with \textbf{Section 1: Introduction}, which establishes the overarching context for Vision Transformers. This section begins by outlining the motivation behind their emergence, particularly in response to the inherent limitations of traditional Convolutional Neural Networks (CNNs) in capturing global dependencies (Subsection 1.1). It then provides a concise historical overview of vision models, highlighting the pivotal paradigm shift from CNN dominance to the rise of Transformers (Subsection 1.2). This introductory section sets the stage for the detailed exploration that follows, providing essential background for understanding the subsequent technical discussions.

\textbf{Section 2: Foundational Concepts of Vision Transformers}, delves into the core principles that underpin ViTs. It starts with a brief recap of the original Transformer architecture from Natural Language Processing (Subsection 2.1), explaining its key components. Subsequently, it details the pioneering work that adapted this architecture for visual data, introducing the original Vision Transformer (ViT) and its innovative approach to image tokenization and global self-attention (Subsection 2.2). Crucially, this section also critically examines the initial challenges and limitations of pure ViTs, such as their significant data hunger and quadratic computational complexity (Subsection 2.3), thereby establishing the impetus for subsequent research and architectural refinements.

Building upon the identified limitations, \textbf{Section 3: Architectural Enhancements and Efficiency}, explores the crucial innovations designed to make ViTs more practical and efficient. This section is segmented to discuss hierarchical Vision Transformers (Subsection 3.1), which address multi-scale feature learning and computational efficiency; data-efficient training and tokenization strategies (Subsection 3.2), aimed at reducing reliance on massive datasets; and advanced attention mechanisms (Subsection 3.3), which refine the core self-attention process for improved performance and reduced overhead. These advancements collectively broadened the applicability of ViTs beyond initial classification benchmarks.

\textbf{Section 4: Self-Supervised Learning and Vision Foundation Models}, shifts focus to the transformative role of self-supervised learning (SSL) in scaling ViTs. It details powerful SSL techniques like Masked Autoencoders (MAE) for pre-training (Subsection 4.1) and explores other contrastive and knowledge distillation approaches (Subsection 4.2). The section culminates by discussing the profound trend of scaling Vision Transformers to create "Vision Foundation Models" (Subsection 4.3), which aim to learn universal visual representations adaptable to a wide array of downstream tasks with minimal fine-tuning, marking a significant paradigm shift towards general-purpose visual intelligence.

Further addressing practical deployment and architectural synergy, \textbf{Section 5: Hybrid Architectures and Mobile-Friendly Designs}, examines the strategic convergence of convolutional and Transformer architectures. It explores synergistic CNN-Transformer designs (Subsection 5.1) that leverage the complementary strengths of both paradigms. Additionally, it addresses the critical need for efficiency by detailing lightweight and mobile-optimized Vision Transformers (Subsection 5.2) for resource-constrained environments. The section also looks into novel hybrid paradigms (Subsection 5.3), such as the integration of state-space models, showcasing the continuous innovation in architectural design.

\textbf{Section 6: Applications of Visual Transformers}, comprehensively showcases the extensive and diverse utility of ViTs across the spectrum of computer vision tasks. This section illustrates their state-of-the-art performance in core vision tasks like classification and object detection (Subsection 6.1), their adaptation for dense prediction tasks such as segmentation and pose estimation (Subsection 6.2), and their remarkable versatility in specialized and multimodal applications (Subsection 6.3). This breadth of application underscores the profound impact of ViTs on various real-world scenarios.

Finally, \textbf{Section 7: Future Outlook: Challenges and Opportunities}, synthesizes the current state of ViTs by identifying persistent challenges, such as computational cost and data efficiency (Subsection 7.1), and critical aspects like interpretability, robustness, and generalization (Subsection 7.2). It then outlines promising future trends and open problems (Subsection 7.3), including novel architectural explorations and the integration of ViTs into broader multimodal AI systems. This concluding section encourages responsible development and application of this rapidly evolving technology, ensuring beneficial and equitable societal impact. Through this structured exploration, the review aims to provide a comprehensive and critically informed understanding of the Vision Transformer landscape.


### Foundational Concepts of Vision Transformers

\section{Foundational Concepts of Vision Transformers}
\label{sec:foundational_concepts_of_vision_transformers}



\subsection{The Transformer Architecture: A Brief Recap}
\label{sec:2_1_the_transformer_architecture:_a_brief_recap}


The Transformer architecture, introduced by [vaswani2017attention], fundamentally revolutionized sequence modeling in Natural Language Processing (NLP) by moving away from recurrent and convolutional networks. This paradigm shift enabled unprecedented parallel processing capabilities and a more effective capture of long-range dependencies, laying the groundwork for its subsequent adaptation to diverse data modalities, including visual data.

At its core, the Transformer adopts an encoder-decoder structure. The encoder processes the input sequence, generating a rich, context-aware representation, which the decoder then utilizes to generate the output sequence, often in an auto-regressive manner. This entire architecture is built exclusively on attention mechanisms, eliminating the need for sequential processing inherent in Recurrent Neural Networks (RNNs) or the local receptive fields characteristic of Convolutional Neural Networks (CNNs).

The central innovation is the \textbf{self-attention mechanism}, which allows the model to weigh the importance of different parts of the input sequence when processing each element. For every token in the sequence, three distinct vectors are computed: a Query (Q), a Key (K), and a Value (V). The attention scores are calculated by taking the dot product of the Query with all Keys, scaling by the square root of the key dimension to prevent vanishing gradients, and applying a softmax function. These scores then weight the corresponding Value vectors, which are summed to produce the output for that token. This mechanism inherently allows each token to attend to any other token in the sequence, regardless of their distance, thereby effectively capturing long-range dependencies that were challenging for traditional recurrent networks.

To enhance the model's ability to capture diverse relationships and focus on different aspects of the input simultaneously, the Transformer employs \textbf{multi-head attention}. Instead of performing a single attention function, the Q, K, and V vectors are linearly projected multiple times into different lower-dimensional subspaces. Each of these "heads" then independently computes its own scaled dot-product attention. The outputs from all attention heads are concatenated and then linearly transformed back into the desired output dimension. This parallel processing of multiple attention mechanisms allows the model to learn various types of relationships and attend to different positions, enriching the overall representation and improving the model's capacity to model complex data.

A critical aspect of sequence modeling is preserving the order of elements, which self-attention inherently lacks due to its permutation-invariant nature. To address this, the Transformer injects sequence order information through \textbf{positional encoding}. These are vectors added directly to the input embeddings at the bottom of the encoder and decoder stacks. The original Transformer used fixed sinusoidal functions of different frequencies, allowing the model to easily learn relative positions. This mechanism ensures that while the attention mechanism processes tokens in parallel, the model retains crucial information about their relative and absolute positions within the sequence.

Each encoder and decoder block also contains a position-wise fully connected feed-forward network, applied independently and identically to each position. This network typically consists of two linear transformations with a ReLU activation in between. Additionally, layer normalization is applied before each sub-layer (self-attention and feed-forward network), followed by a residual connection, aiding in stable training and gradient flow through deep networks.

The design of the Transformer, particularly its reliance on self-attention, marked a significant departure from previous state-of-the-art models like Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). Unlike RNNs, which process sequences token by token, leading to computational bottlenecks and difficulties with long-range dependencies due to vanishing/exploding gradients, the Transformer processes all tokens in parallel. This parallelization drastically reduces training time. Furthermore, while CNNs capture local features through fixed-size kernels, the Transformer's self-attention mechanism provides a global receptive field from the very first layer, allowing it to directly model relationships between any two tokens irrespective of their distance. This global perspective and parallel processing capability were revolutionary, establishing a new paradigm for sequence modeling that would profoundly influence subsequent research across various domains.
\subsection{The Original Vision Transformer (ViT)}
\label{sec:2_2_the_original_vision_transformer_(vit)}


The remarkable success of the Transformer architecture [vaswani2017attention] in natural language processing (NLP), primarily driven by its potent self-attention mechanism, ignited a fundamental inquiry within the computer vision community: could a similar, convolution-free paradigm effectively process and interpret visual data? This pivotal question was comprehensively addressed by Dosovitskiy et al. [dosovitskiy2021image] with the introduction of the Vision Transformer (ViT). Their pioneering work directly adapted the Transformer for image recognition, unequivocally demonstrating its viability as a powerful and distinct alternative to traditional Convolutional Neural Networks (CNNs) and marking a significant paradigm shift in visual modeling [han2020yk0, huo2023e5h].

The core innovation of the original ViT lies in its elegant yet straightforward approach to re-conceptualizing an image as a sequence of discrete tokens, directly analogous to how words are treated in NLP. The process commences by partitioning an input image into a grid of fixed-size, non-overlapping square patches, typically $16 \times 16$ pixels. Each of these patches is then flattened into a one-dimensional vector and subsequently projected linearly into a fixed-dimension embedding space. This transformation yields a sequence of "visual tokens," where each token represents a localized region of the original image. Crucially, to compensate for the spatial information lost during the flattening and tokenization process, learnable positional embeddings are added to these patch embeddings. These embeddings are vital for retaining the relative spatial arrangement of patches, allowing the model to understand the geometric relationships between different parts of the image.

To facilitate image classification, a special, learnable "class token" is prepended to this sequence of embedded patches and their corresponding positional embeddings. This class token acts as a global representation of the entire image, accumulating information from all other visual tokens as it propagates through the Transformer layers. The augmented sequence then enters a standard Transformer encoder, which consists of multiple identical layers, each comprising a multi-head self-attention (MSA) block and a feed-forward network (FFN). The global self-attention mechanism within the MSA block is the cornerstone of ViT's ability to capture long-range dependencies; it enables each visual token (patch or class token) to attend to every other token in the sequence, thereby integrating global contextual information across the entire image. This stands in stark contrast to CNNs, which primarily rely on local receptive fields and hierarchical processing to build global context. The final state of the class token, after traversing the Transformer encoder, is then fed into a simple Multi-Layer Perceptron (MLP) head, which outputs the predicted class probabilities for the image.

This architectural departure from CNNs demonstrated remarkable empirical performance. Dosovitskiy et al. [dosovitskiy2021image] showcased that ViT achieved state-of-the-art results on large-scale image classification benchmarks, including ImageNet and JFT-300M. A critical finding was that ViTs, when pre-trained on sufficiently vast datasets (e.g., JFT-300M with 300 million images), could significantly outperform CNNs of comparable size. This performance advantage was attributed to the Transformer's ability to learn more flexible and global representations, unconstrained by the strong inductive biases (like locality and translation equivariance) inherent in CNNs. While these inductive biases make CNNs highly data-efficient on smaller datasets, ViT's "data hunger" meant it required extensive pre-training to fully exploit its capacity and generalize effectively [huo2023e5h]. The breakthrough performance of ViT not only challenged the long-standing dominance of CNNs in computer vision but also fundamentally reoriented research, establishing a new and highly influential direction for visual modeling based on global attention mechanisms [han2020yk0]. However, this paradigm shift, while promising, also introduced its own set of initial challenges, particularly concerning its substantial data requirements and the computational implications of its global self-attention mechanism, which necessitated further architectural refinements and training innovations.
\subsection{Initial Challenges and Limitations of Pure ViTs}
\label{sec:2_3_initial_challenges__and__limitations_of_pure_vits}


Despite the groundbreaking success of the Vision Transformer (ViT) in demonstrating that a pure Transformer architecture could achieve state-of-the-art performance in image classification, the initial iterations of these models presented several significant challenges and inherent limitations [ICLR2019]. These drawbacks, which became apparent despite their impressive performance on large-scale datasets, necessitated extensive subsequent research into their optimization and refinement.

A primary limitation of the original pure ViT architecture, as introduced by [ICLR2019], was its substantial data requirement for pre-training. Unlike Convolutional Neural Networks (CNNs) that inherently possess inductive biases such as locality and translation equivariance, pure ViTs lack these built-in priors. Consequently, they must learn these fundamental visual properties from scratch, demanding massive datasets like JFT-300M to achieve competitive performance, making them less accessible for researchers without access to such vast resources. This data hunger was a critical bottleneck, prompting early efforts to make ViTs more data-efficient, such as through knowledge distillation [Touvron2021] or refined tokenization strategies [Yuan2021].

Another significant challenge stemmed from the computational complexity of the global self-attention mechanism, which is central to pure ViTs. For an input image of $H \times W$ resolution, if flattened into $N$ patches, the self-attention operation scales quadratically with the number of patches, $O(N^2)$, or equivalently, $O((HW)^2)$ with respect to the image resolution. This quadratic complexity made pure ViTs computationally prohibitive for high-resolution images and impractical for tasks requiring fine-grained, pixel-level understanding, such as dense prediction. This limitation spurred the development of hierarchical ViTs that employed localized attention mechanisms, like the shifted window attention in Swin Transformer [ICLR2021] or pyramid structures in PVT [ICCV2021], to reduce computational cost and enable multi-scale feature extraction.

Beyond data and computational demands, pure ViTs also exhibited architectural limitations that hindered their scalability and applicability. For instance, deeper pure ViT models were found to suffer from an "attention collapse" issue [zhou202105h]. As the network depth increased, the attention maps across layers tended to become increasingly similar, leading to redundant feature learning and performance saturation rather than improvement. To address this, [zhou202105h] proposed Re-attention to diversify attention maps and enable consistent performance gains in deeper ViTs.

Furthermore, the lack of inherent hierarchical feature representation and strong locality inductive biases in pure ViTs made them less naturally suited for dense prediction tasks like object detection and semantic segmentation, which traditionally benefit from multi-scale feature pyramids and local context modeling. While studies like [li2022raj] demonstrated that plain ViT backbones could be adapted for object detection, they often required significant modifications, such as building simple feature pyramids from single-scale feature maps and incorporating cross-window propagation blocks, to achieve competitive results. This highlighted that the original pure ViT design was not intrinsically optimized for these tasks without architectural augmentations.

In summary, the initial pure ViT architectures, despite their paradigm-shifting performance, were characterized by their substantial data requirements, quadratic computational complexity, and a lack of built-in inductive biases that are naturally present in CNNs. These factors collectively made them less efficient, harder to train effectively on smaller datasets, and less versatile for a broad range of computer vision tasks without significant modifications. These inherent limitations became a fertile ground for subsequent research, driving the evolution towards more efficient, data-agnostic, and task-adaptable Vision Transformer variants.


### Architectural Enhancements and Efficiency

\section{Architectural Enhancements and Efficiency}
\label{sec:architectural_enhancements__and__efficiency}



\subsection{Hierarchical Vision Transformers}
\label{sec:3_1_hierarchical_vision_transformers}


Early Vision Transformers (ViTs) [dosovitskiy2020image] marked a significant paradigm shift in computer vision, yet their direct application to tasks requiring fine-grained spatial understanding, such as dense prediction, or processing of high-resolution images, was hampered by two primary limitations [hassija2025wq3, heidari2024d9k]. Firstly, the global self-attention mechanism exhibited quadratic computational complexity with respect to the number of input tokens (image patches), making it prohibitively expensive for large inputs. Secondly, pure ViTs lacked the inherent multi-scale feature representations and inductive biases (like locality) that Convolutional Neural Networks (CNNs) naturally possess, which are crucial for capturing both global context and local details across various scales. To address these challenges, a critical line of research emerged, focusing on the development of hierarchical Vision Transformers that integrate multi-scale processing and more efficient attention mechanisms, thereby mimicking CNN-like feature pyramids and achieving linear computational complexity.

One of the foundational models in this category is the \textbf{Pyramid Vision Transformer (PVT)} [wang2021pyramid]. PVT introduced a progressive shrinking pyramid structure, akin to feature pyramids in CNNs, by gradually reducing the resolution of feature maps in deeper layers. This hierarchical design allows for the generation of multi-scale features, which are essential for dense prediction tasks. To manage the computational cost, PVT employs a Spatial-Reduction Attention (SRA) module. Unlike global self-attention, SRA reduces the spatial dimension of the key and value matrices before computing attention, effectively lowering the computational complexity from quadratic to linear with respect to image size. This innovation enabled PVT to serve as a robust backbone for tasks like object detection and semantic segmentation, demonstrating the viability of hierarchical Transformers for a broader range of vision applications [hassija2025wq3].

Building upon the success of hierarchical designs, the \textbf{Swin Transformer} [liu2021ljs] emerged as another seminal work, further solidifying the potential of these architectures. Swin Transformer also adopts a hierarchical structure through progressive patch merging, generating multi-scale feature maps. Its core innovation, however, lies in the "shifted window attention" mechanism. Instead of global attention, Swin restricts self-attention computation to non-overlapping local windows within each stage, drastically reducing computational complexity to linear. To facilitate information exchange between windows and capture global dependencies, the windows are shifted between successive Transformer blocks. This elegant solution allows Swin Transformer to achieve state-of-the-art performance across a wide array of vision tasks, including image classification, object detection, and semantic segmentation, effectively establishing hierarchical Transformers as powerful general-purpose vision backbones. While both PVT and Swin Transformer achieved linear complexity, Swin's shifted window approach provided a more dynamic mechanism for cross-window information flow compared to PVT's static spatial reduction, often leading to stronger performance.

Further advancements in hierarchical ViTs have focused on refining attention mechanisms and simplifying architectures. The fixed window sizes and handcrafted attention patterns in models like Swin Transformer and PVT, while efficient, can sometimes limit their ability to model long-range relations adaptively [xia2023bp7]. Addressing this, the \textbf{Deformable Attention Transformer (DAT++)} [xia2023bp7] proposes a novel deformable multi-head attention module. This module adaptively allocates the positions of key and value pairs in a data-dependent manner, allowing the model to dynamically focus on relevant regions and capture more flexible spatial relationships, similar to deformable convolutions in CNNs. This approach enhances the representation power of global attention while maintaining efficiency. Similarly, the \textbf{Dynamic Window Vision Transformer (DW-ViT)} [ren2022ifo] extends the window-based paradigm by moving "beyond fixation." DW-ViT assigns windows of different sizes to different head groups within multi-head self-attention and dynamically fuses the multi-scale information, overcoming the limitation of fixed single-scale windows in models like Swin Transformer and further improving multi-scale modeling capabilities.

In contrast to increasing architectural complexity, some research has explored simplification. \textbf{Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles} [ryali202339q] proposes a streamlined hierarchical ViT design, arguing that many added complexities can be removed without sacrificing accuracy, provided the model is pre-trained with a robust self-supervised pretext task, such as Masked Autoencoders (MAE) [he2022masked]. By leveraging strong pre-training, Hiera achieves competitive or superior accuracy to more complex hierarchical models while being significantly faster during both inference and training. This work underscores the critical interplay between architectural design and effective pre-training strategies, suggesting that inductive biases from hierarchical processing, combined with powerful self-supervision, can lead to highly efficient and performant architectures without excessive overhead. This also opens a comparative perspective on the fundamental role of attention itself; for instance, the \textbf{ShiftViT} [wang2022da0] even explores replacing attention layers entirely with a zero-parameter shift operation, achieving performance comparable to Swin Transformer, suggesting that the hierarchical structure and feature processing might be as crucial as the attention mechanism itself.

In conclusion, the evolution of hierarchical Vision Transformers has been instrumental in overcoming the initial limitations of pure ViTs, particularly their quadratic complexity and lack of multi-scale representations. Models like PVT and Swin Transformer pioneered efficient multi-scale processing through techniques such as spatial-reduction attention and shifted window attention, respectively, making ViTs suitable for dense prediction tasks and high-resolution images. Subsequent innovations, including deformable attention (DAT++) and dynamic windowing (DW-ViT), have further refined these attention mechanisms for greater flexibility and multi-scale understanding. Concurrently, models like Hiera highlight the power of architectural simplification when coupled with strong self-supervised pre-training. Future research in this domain will likely continue to explore the optimal balance between architectural complexity, computational efficiency, and representational power within hierarchical structures, further optimizing attention mechanisms, and deepening the synergy with advanced self-supervised learning paradigms to unlock even more robust and versatile vision backbones.
\subsection{Data-Efficient Training and Tokenization Strategies}
\label{sec:3_2_data-efficient_training__and__tokenization_strategies}


Early Vision Transformers (ViTs) demonstrated impressive capabilities in image recognition, yet their reliance on massive proprietary datasets like JFT-300M for pre-training posed a significant barrier to their widespread adoption and practical application [Dosovitskiy2021]. This section explores key innovations that addressed this limitation, focusing on strategies for data-efficient training and improved tokenization, thereby making ViTs more accessible and performant on standard, smaller datasets.

One of the most impactful approaches to mitigate ViTs' data hunger is knowledge distillation. The work by [Touvron2021] introduced Data-efficient image Transformers (DeiT), which enabled ViTs to be trained effectively on ImageNet-1K, a significantly smaller dataset, without requiring external data. DeiT achieved this by employing a distillation strategy where a ViT student model learns from a powerful convolutional neural network (CNN) teacher. A crucial innovation was the "distillation token," an additional token that interacts with the class token through self-attention and is trained to match the teacher's output, effectively transferring the teacher's knowledge and enabling the ViT to achieve competitive performance with much less training data.

While knowledge distillation provided a powerful mechanism for data-efficient training, another line of research focused on enhancing the initial representation of image patches, thereby reducing the inherent data requirements for learning robust visual features. The original ViT treats non-overlapping image patches as independent tokens, which can lose fine-grained local structural information. To address this, [Yuan2021] proposed Tokens-to-Token ViT (T2T-ViT), which introduced an improved tokenization strategy. Instead of a single-step patch embedding, T2T-ViT employs a multi-stage process where neighboring pixels are progressively aggregated into tokens through a series of self-attention layers. This hierarchical tokenization module effectively captures local structural details and preserves more information from the original image, leading to better performance from scratch on standard benchmarks like ImageNet-1K without relying on massive pre-training datasets or external teacher models.

The advancements in data-efficient training through distillation, as exemplified by DeiT, and improved tokenization strategies, such as T2T-ViT, collectively transformed the landscape of Vision Transformers. DeiT demonstrated that ViTs could achieve state-of-the-art results on ImageNet-1K by effectively mimicking a larger teacher, while T2T-ViT showed that a more sophisticated initial tokenization could inherently boost ViT performance on smaller datasets by better encoding local visual information. These innovations were critical in making ViTs more practical and accessible, moving them beyond the realm of models requiring immense computational resources and proprietary datasets. However, challenges remain in further reducing the data requirements for highly specialized tasks and in developing unified frameworks that seamlessly integrate the benefits of both distillation and advanced tokenization for optimal efficiency across diverse applications.
\subsection{Advanced Attention Mechanisms}
\label{sec:3_3_advanced_attention_mechanisms}


The foundational self-attention mechanism in Vision Transformers, while powerful for capturing global dependencies, inherently suffers from quadratic computational complexity with respect to the number of tokens, posing significant challenges for high-resolution images or deeper architectures. Furthermore, vanilla self-attention often lacks explicit spatial inductive biases, which can sometimes lead to redundancy or "attention collapse" in very deep models, limiting their expressive power. To address these limitations, researchers have developed various advanced attention mechanisms aimed at improving efficiency, flexibility, and expressiveness.

One critical challenge in scaling Vision Transformers to greater depths is the phenomenon of "attention collapse." As models become deeper, the attention maps across different layers can become increasingly similar, hindering the model's ability to learn diverse and effective representations. \textcite{zhou202105h} introduced DeepViT to tackle this issue, observing that in deeper ViTs, self-attention mechanisms often fail to learn distinct concepts, leading to performance saturation. To mitigate this, they proposed Re-attention, a simple yet effective method designed to re-generate attention maps and increase their diversity across layers. This innovation allows for the training of significantly deeper ViT models with consistent performance improvements, enhancing the model's overall expressiveness by ensuring that attention layers continue to contribute unique feature transformations.

Beyond improving expressiveness in deep models, a major thrust in advanced attention research focuses on enhancing efficiency and spatial awareness. The global nature of standard self-attention means every token attends to every other token, which can be computationally expensive and may lead to attention being drawn to irrelevant regions. To make attention more adaptive and efficient, Deformable Attention was introduced, allowing the attention mechanism to sample features at adaptive, learned offsets \textcite{xia2022qga}. Unlike fixed-grid or sparse attention patterns, this data-dependent sampling enables the model to dynamically focus on relevant regions and capture more informative features. This flexible scheme significantly reduces computational overhead by concentrating attention on salient parts of the input, while simultaneously enhancing the model's ability to adapt to varying object shapes and scales, thereby improving spatial awareness and fine-grained detail capture.

This concept of focusing attention on relevant tokens to reduce computation is also explored in approaches like Focal Attention. Such mechanisms typically employ hierarchical attention or explicit token selection strategies to prioritize important visual information. By selectively attending to a subset of tokens or regions, these methods aim to mitigate the quadratic complexity of global attention, making ViTs more scalable for high-resolution inputs. These innovations collectively enhance the model's ability to capture fine-grained details and adapt to varying object shapes and scales, leading to more robust and accurate visual understanding.

In summary, advanced attention mechanisms represent a crucial evolutionary step for Vision Transformers, moving beyond the limitations of vanilla self-attention. By addressing issues like attention collapse through techniques such as Re-attention \textcite{zhou202105h} and improving efficiency and spatial adaptability with data-dependent sampling as seen in Deformable Attention \textcite{xia2022qga}, these refinements enable the development of more powerful, flexible, and scalable ViT architectures. Future directions will likely continue to explore more sophisticated ways to balance global context modeling with local detail, further optimizing computational efficiency, and integrating stronger inductive biases into attention mechanisms for even more robust visual understanding across diverse tasks.


### Self-Supervised Learning and Vision Foundation Models

\section{Self-Supervised Learning and Vision Foundation Models}
\label{sec:self-supervised_learning__and__vision_foundation_models}



\subsection{Masked Autoencoders (MAE) for Pre-training}
\label{sec:4_1_masked_autoencoders_(mae)_for_pre-training}


Vision Transformers (ViTs), despite their remarkable success in various computer vision tasks, initially faced significant challenges related to their data-hungry nature and the difficulty of scaling them to deeper architectures without performance degradation. Early observations, such as those presented in [zhou202105h], highlighted an "attention collapse" issue in deeper ViTs, where attention maps became increasingly similar across layers, hindering effective representation learning and limiting performance gains. This necessitated the development of efficient and scalable self-supervised pre-training strategies to unlock the full potential of ViTs and enable them to learn robust visual representations from unlabeled data.

A pivotal breakthrough in this regard was the introduction of Masked Autoencoders (MAE) by [CVPR2022_Masked_Autoencoders_Are_Scalable_Vision_Learners_2022]. MAE revolutionized pre-training for Vision Transformers by proposing a highly effective self-supervised learning strategy rooted in image reconstruction. The core idea involves masking a substantial portion of image patches, typically around 75\%, and training a Transformer encoder-decoder architecture to reconstruct the missing pixels. The encoder processes only the visible, unmasked patches, leading to significant computational efficiency during pre-training, while a lightweight decoder is responsible for predicting the original pixel values of the masked patches. This design forces the encoder to learn rich, high-level semantic representations from limited visual context, preventing it from relying on trivial low-level statistical regularities.

The MAE approach offers several compelling advantages. Its asymmetric encoder-decoder design, where the encoder operates on a sparse set of visible patches, drastically reduces the computational cost of pre-training compared to prior self-supervised methods that process full images. This efficiency makes MAE exceptionally scalable, enabling the effective pre-training of very large Vision Transformer models on vast amounts of unlabeled data. Consequently, MAE-pretrained ViTs have demonstrated state-of-the-art performance across various downstream tasks, including image classification, object detection, and semantic segmentation, often requiring less fine-tuning data than models trained with other self-supervised or supervised methods. The learned representations are highly robust and generalizable, significantly reducing the reliance on extensive labeled datasets and making large-scale pre-training more feasible and impactful.

The success of MAE extended beyond pure Transformer architectures, demonstrating the generality of its self-supervised reconstruction paradigm. For instance, [ICLR2023_ConvNeXt_V2_Co_designing_and_Scaling_ConvNets_with_Masked_Autoencoders_2023] successfully applied MAE pre-training to modern ConvNets, showing that even CNNs could benefit significantly from this Transformer-inspired self-supervised strategy, achieving improved performance and bridging the gap between the two architectural paradigms. Furthermore, the efficiency and scalability demonstrated by MAE paved the way for the exploration of even grander model scales. Works like [ICLR2023_Scaling_Vision_Transformers_to_1_Billion_Parameters_2023] leveraged similar efficient pre-training principles to scale Vision Transformers to unprecedented sizes, showcasing the potential for massive, general-purpose "Vision Foundation Models." The drive for learning robust, unsupervised features, significantly propelled by MAE's success, continued with advancements such as [ICLR2023_DINOv2_Learning_Robust_Visual_Features_without_Supervision_2023], which further refined self-supervised approaches to learn highly transferable visual features, building upon the foundation of efficient pre-training established by MAE.

In conclusion, Masked Autoencoders marked a significant turning point in the pre-training landscape for Vision Transformers. By providing an efficient, scalable, and effective self-supervised learning strategy, MAE made large-scale ViT pre-training practical and solidified self-supervised learning as a cornerstone for developing powerful Vision Foundation Models. While MAE has proven highly effective, ongoing research continues to explore optimal masking strategies, alternative reconstruction targets, and combinations with other self-supervised objectives to further enhance the learned representations and address the persistent challenge of adapting these powerful, pre-trained models for efficient deployment on resource-constrained devices.
\subsection{Contrastive and Knowledge Distillation Approaches}
\label{sec:4_2_contrastive__and__knowledge_distillation_approaches}


The quest for learning robust, generalizable visual features for Vision Transformers (ViTs) without the prohibitive cost of extensive human annotation has propelled the development of sophisticated self-supervised learning (SSL) paradigms. Beyond masked autoencoding, two other prominent strategies, contrastive learning and advanced knowledge distillation, have significantly contributed to overcoming the data-hungry nature of ViTs, enabling them to learn meaningful representations directly from vast quantities of unlabeled data. These methods enhance ViTs' performance and applicability across diverse downstream tasks, demonstrating powerful alternative strategies for unsupervised representation learning.

Contrastive learning, a foundational SSL approach, operates by teaching the model to distinguish between similar and dissimilar pairs of data points. The core idea is to pull "positive" pairs (different augmented views of the same image) closer in the embedding space while pushing "negative" pairs (views from different images) apart. Early successes with Convolutional Neural Networks (CNNs), such as SimCLR [chen2020simple] and MoCo [he2020momentum], laid the groundwork for adapting these techniques to ViTs. MoCo-v3 [chen2021empirical], for instance, successfully applied a momentum contrast framework to ViTs, demonstrating that these architectures could learn competitive representations through instance discrimination. The effectiveness of contrastive learning stems from its ability to learn representations that are invariant to various data augmentations, thereby capturing essential semantic features without explicit labels.

A highly effective and distinct paradigm, particularly for ViTs, is self-distillation, a specialized form of knowledge distillation where a model learns from itself through a teacher-student framework. This approach has proven exceptionally successful in generating high-quality, transferable features. The original DINO (Self-Distillation with No Labels) [caron2021emerging] was a seminal work, demonstrating that ViTs could learn dense, semantic features by matching the output of a momentum-updated teacher network. This method encourages the student to produce features that are invariant to different views of the same image, effectively learning without explicit supervision. Building upon this, DINOv2 [ICLR2023_DINOv2] scaled this teacher-student self-distillation framework to unprecedented levels, utilizing massive datasets and architectural refinements to produce exceptionally robust, generalizable, and transferable visual features. DINOv2's representations frequently outperform features derived from supervised pre-training on various transfer tasks, significantly reducing the reliance on labeled datasets and making ViTs more accessible and scalable for real-world applications.

Beyond self-distillation, other forms of knowledge distillation have also been explored to enhance self-supervised ViTs, particularly for efficiency and transferability. For instance, Attention Distillation [wang2022pee] investigates distilling knowledge from a self-supervised ViT teacher to a smaller ViT student. This method highlights that directly distilling information from the crucial attention mechanism can significantly narrow the performance gap between teacher and student, outperforming existing self-supervised knowledge distillation methods focused on ConvNets. Such approaches are vital for deploying high-performing self-supervised ViTs on memory and compute-constrained devices, extending the benefits of powerful SSL to more practical scenarios.

A critical analysis reveals fundamental differences in the learning objectives and resulting feature properties between these SSL paradigms and Masked Autoencoders (MAE) discussed in the preceding subsection. While MAE [he2022masked] forces the model to learn rich representations through a high-information-density pixel reconstruction task, often yielding features beneficial for dense prediction, DINO-style self-distillation encourages semantic consistency and invariance to data augmentations, typically leading to features that are highly effective for classification and linear probing. Contrastive learning, on the other hand, focuses on instance-level discrimination, aiming for representations that can distinguish individual images while being robust to transformations. These distinct objectives mean that each SSL paradigm can yield features with different strengths, making them more suitable for specific downstream tasks or offering complementary benefits. For example, while MAE's reconstruction task can be adapted to hierarchical architectures like HiViT [zhang2022msa] and Hiera [ryali202339q] for efficiency, contrastive and distillation methods often focus on the feature space directly. Moreover, some approaches explore hybrid strategies, such as MAT-VIT [han2024f96], which leverages an MAE-based self-supervised auxiliary task alongside a supervised primary task, demonstrating the potential for combining different SSL paradigms to maximize learning from both labeled and unlabeled medical images.

In conclusion, contrastive learning and knowledge distillation approaches, particularly self-distillation as pioneered by DINO and scaled by DINOv2, represent critical advancements in self-supervised learning for Vision Transformers. By leveraging teacher-student frameworks, instance discrimination, and focusing on learning highly robust and generalizable features from unlabeled data, these methods empower ViTs to achieve state-of-the-art performance in various downstream tasks, often surpassing supervised alternatives. The continuous development of such unsupervised representation learning strategies, coupled with innovations in knowledge transfer for efficiency, is fundamental to realizing the full potential of Vision Transformers as efficient, scalable, and universally applicable models, thereby significantly reducing the dependency on costly human annotations and paving the way for the emergence of powerful Vision Foundation Models.
\subsection{Scaling Vision Transformers to Foundation Models}
\label{sec:4_3_scaling_vision_transformers_to_foundation_models}

The quest for general-purpose visual intelligence has ushered in a transformative era, marked by the scaling of Vision Transformers (ViTs) to unprecedented sizes, often comprising billions of parameters, to establish "Vision Foundation Models." These colossal models, meticulously trained on vast datasets using sophisticated self-supervised learning (SSL) techniques, aim to distill universal visual representations. The ultimate objective is to develop versatile backbones that exhibit exceptional adaptability across a broad spectrum of vision tasks, delivering superior performance with significantly reduced task-specific fine-tuning. This paradigm represents a profound shift towards general-purpose visual intelligence, capable of impressive zero-shot or few-shot learning capabilities.

Initial explorations into scaling ViTs encountered significant hurdles. Naively increasing model depth, as explored by [zhou202105h] with DeepViT, revealed an "attention collapse" phenomenon. In deeper layers, attention maps became increasingly homogeneous, hindering performance gains and indicating that simply adding more layers was insufficient without more robust architectural designs and effective training strategies. This challenge underscored the critical need for advancements in how ViTs learn and generalize at scale.

A pivotal breakthrough in enabling the efficient scaling of ViTs came with the advent of Masked Autoencoders (MAE) [CVPR2022_Masked_Autoencoders_Are_Scalable_Vision_Learners]. As discussed in Subsection 4.1, MAE introduced an elegant and highly efficient self-supervised pre-training paradigm. By masking a large portion of image patches and training the Transformer encoder-decoder to reconstruct the missing pixels, MAE effectively mitigated the notorious data-hungry nature of ViTs, allowing them to learn rich, robust representations from vast quantities of unlabeled data. This innovation was instrumental in unlocking the potential for large-scale pre-training. Complementing MAE, methods like DINOv2 [ICLR2023_DINOv2_Learning_Robust_Visual_Features_without_Supervision], elaborated in Subsection 4.2, further refined self-supervised learning by leveraging self-distillation with a teacher-student framework. DINOv2 produced highly generalizable and transferable features without explicit supervision, often surpassing supervised pre-training in transfer tasks and demonstrating impressive zero-shot capabilities.

The combination of efficient self-supervised pre-training and the availability of colossal datasets has been fundamental to the scaling trend. Vision Foundation Models are typically pre-trained on datasets far exceeding ImageNet, such as JFT-300M [bai2022f1v, hong2022ks6], JFT-4B, or the internet-scale LAION-5B. The sheer volume and diversity of these datasets, coupled with scalable SSL methods, allow ViTs to learn highly abstract and universal visual concepts. Empirical studies have consistently demonstrated that performance scales predictably with increased model size, dataset size, and computational resources, a phenomenon often referred to as "scaling laws" in deep learning.

Building upon these foundations, researchers have aggressively pushed the boundaries of ViT size. The work by [ICLR2023_Scaling_Vision_Transformers_to_1_Billion_Parameters] notably demonstrated the efficacy of scaling Vision Transformers to one billion parameters. Their findings unequivocally showed that larger models, when pre-trained on extensive datasets using techniques like MAE, yield superior performance and significantly enhanced generalization across diverse downstream tasks. This marked a crucial step towards realizing truly universal visual backbones. Even more extreme examples, such as Google's ViT-22B, further exemplify the profound capabilities that emerge from massive scaling. These models exhibit emergent properties, including remarkable zero-shot transfer to unseen tasks and improved few-shot learning, which are hallmarks of true foundation models. They can often adapt to new domains with minimal or no task-specific fine-tuning, drastically reducing the effort and labeled data required for new applications.

Beyond pure parameter count, architectural innovations have also contributed to the scalability and efficiency of these large models. For instance, approaches like UFO-ViT [song20215tk] introduce linear attention mechanisms to mitigate the quadratic computational complexity of traditional self-attention, which becomes a bottleneck at high resolutions or extreme depths. Similarly, models like Hiera [ryali202339q] demonstrate that even simplified hierarchical ViT designs, when effectively pre-trained with MAE, can achieve state-of-the-art performance while being significantly faster, thus contributing to more efficient scaling.

While scaling to billions of parameters offers immense representational power, the practical deployment of such massive models remains a critical consideration. The computational cost for inference and the memory footprint can be prohibitive for many real-world scenarios, particularly on resource-constrained edge devices. To address this, research into efficiency techniques is paramount. Quantization methods, such as Q-ViT [li20229zn], enable significant model compression by reducing bit-widths while striving to preserve performance. Furthermore, specialized hardware accelerators like ViTA [nag2023cfn] are being developed to optimize the inference of Vision Transformers, ensuring that these powerful, scaled models can be deployed efficiently in practical applications.

In conclusion, the evolution of Vision Transformers into foundation models represents a profound paradigm shift from task-specific model development to the creation of universally powerful, pre-trained visual intelligence. This trajectory is characterized by extreme architectural scaling, the indispensable role of advanced self-supervised learning for robust feature extraction from colossal datasets, and the resultant emergence of powerful zero-shot and few-shot learning capabilities. Future directions will undoubtedly focus on further enhancing the efficiency of training and inference for these massive models, exploring deeper multimodal integration for richer representations, and critically addressing the ethical implications and potential biases inherent in such large-scale, general-purpose visual systems.


### Hybrid Architectures and Mobile-Friendly Designs

\section{Hybrid Architectures and Mobile-Friendly Designs}
\label{sec:hybrid_architectures__and__mobile-friendly_designs}



\subsection{Synergistic CNN-Transformer Designs}
\label{sec:5_1_synergistic_cnn-transformer_designs}


The evolution of computer vision architectures has increasingly moved towards a synergistic paradigm, strategically combining the inherent strengths of Convolutional Neural Networks (CNNs) with the global context modeling capabilities of Transformers. This convergence is driven by the recognition that while CNNs excel at local feature extraction and possess strong inductive biases like locality and translation equivariance, pure Vision Transformers (ViTs) [dosovitskiy2021image] offer unparalleled global receptive fields and flexibility in capturing long-range dependencies. Hybrid designs aim to mitigate the limitations of each, such as ViTs' high data requirements and quadratic computational complexity, or CNNs' struggle with modeling global relationships, thereby fostering more robust, efficient, and versatile vision backbones.

One prominent direction in this synergy involves modernizing traditional CNN architectures by incorporating Transformer-inspired methodologies and training paradigms. A seminal work in this category is \textit{ConvNeXt V2} [iclr2023]. Building upon the architectural principles of ConvNeXt, which re-examined and optimized CNN designs to resemble Transformers (e.g., using large kernel depthwise convolutions and inverted bottleneck structures), ConvNeXt V2 crucially leveraged the Masked Autoencoder (MAE) pre-training strategy [he2022masked]. This integration demonstrated that CNNs could benefit immensely from self-supervised learning techniques originally developed for Transformers, achieving state-of-the-art performance and effectively bridging the perceived architectural gap. ConvNeXt V2 showcased that the inductive biases of CNNs, when combined with powerful, scalable pre-training, remain highly competitive and efficient.

Conversely, another significant approach integrates powerful CNN components directly into Transformer-like frameworks. \textit{InternImage} [cvpr2023] exemplifies this by embedding deformable convolutions within a Transformer-inspired architecture. Deformable convolutions, a hallmark of advanced CNNs, enable adaptive receptive fields and flexible spatial sampling, which are crucial for handling geometric variations and object deformations effectively. By incorporating this mechanism, InternImage creates robust "Vision Foundation Models" that combine the global context understanding of Transformers with the precise, adaptive local feature extraction characteristic of sophisticated convolutions. This allows the model to dynamically focus on relevant regions, enhancing its ability to capture fine-grained details and adapt to varying object shapes and scales, a limitation often observed in standard fixed-grid attention mechanisms. A related approach is seen in \textit{Deformable Attention Transformer (DAT)} and its enhanced version \textit{DAT++} [xia2022qga, xia2023bp7]. These models integrate a novel deformable multi-head attention module where key and value positions are adaptively allocated in a data-dependent manner, effectively bringing the adaptive sampling power of deformable convolutions directly into the self-attention mechanism, thereby enhancing spatial awareness and reducing computational overhead compared to dense global attention.

Beyond integrating components or pre-training strategies, some hybrid architectures explicitly interleave CNN and Transformer blocks to capitalize on their respective strengths at different stages of feature extraction. \textit{Next-ViT} [li2022a4u] proposes a "Next Hybrid Strategy" (NHS) that stacks "Next Convolution Blocks" (NCB) for local information capture and "Next Transformer Blocks" (NTB) for global information. This design is optimized for efficient deployment in realistic industrial scenarios, addressing the challenge of achieving both high performance and low latency on hardware accelerators like TensorRT. Similarly, \textit{TRT-ViT} [xia2022dnj] provides practical guidelines for TensorRT-oriented network design, suggesting an "early CNN and late Transformer at stage-level" and "early Transformer and late CNN at block-level" strategy. This highlights a nuanced understanding of where and how to best deploy each architectural component for optimal hardware efficiency and performance. These block-level hybrid models demonstrate that a careful arrangement of convolutional and self-attention layers can yield superior latency-accuracy trade-offs across various vision tasks.

Another innovative direction involves fusing local and global processing within the Transformer block itself, drawing inspiration from CNNs' local inductive biases without necessarily using explicit convolutional layers. \textit{PLG-ViT} [ebert202377v] introduces a "Parallel Local-Global Vision Transformer" that merges local window self-attention with global self-attention. This design efficiently represents short- and long-range spatial interactions, bypassing the need for computationally expensive operations like shifted windows while still achieving strong performance in image classification and dense prediction tasks. This approach reflects a deeper integration, where the *concept* of locality, traditionally associated with CNNs, is re-imagined and implemented within the Transformer's attention mechanism.

In summary, the landscape of synergistic CNN-Transformer designs is rich and diverse, reflecting various philosophies of integration. ConvNeXt V2 represents a CNN-centric approach enhanced by Transformer pre-training. InternImage and DAT/DAT++ showcase embedding CNN-inspired adaptive mechanisms into Transformer backbones. Next-ViT and TRT-ViT demonstrate strategic interleaving of CNN and Transformer blocks for deployment efficiency. PLG-ViT illustrates the internal fusion of local and global attention within a Transformer, mimicking CNN-like inductive biases. These models collectively demonstrate that moving beyond a strict CNN-Transformer dichotomy towards a complementary approach unlocks new levels of performance, efficiency, and versatility, pushing the boundaries of architectural design for a wide array of computer vision applications. Future research will likely explore even more sophisticated integration strategies, novel unified pre-training objectives, and dynamic architectural adaptations to further harness the combined power of these two foundational paradigms.
\subsection{Lightweight and Mobile-Optimized Vision Transformers}
\label{sec:5_2_lightweight__and__mobile-optimized_vision_transformers}

Building upon the synergistic CNN-Transformer designs discussed in Section 5.1, this subsection delves into architectures specifically engineered for resource-constrained mobile and edge devices. The deployment of Vision Transformers (ViTs) on platforms such as smartphones, IoT devices, and embedded systems presents significant challenges due to their typically high computational cost, large memory footprint, and substantial power consumption. Addressing these critical deployment challenges, a dedicated research thrust focuses on developing lightweight and mobile-optimized ViT architectures that maintain high accuracy while drastically reducing computational demands and parameter counts, making powerful ViT capabilities accessible for real-time and edge applications.

A pioneering effort in this domain is \textcite{mehta20216ad}'s MobileViT, which directly tackles the trade-off between the spatial inductive biases and efficiency of Convolutional Neural Networks (CNNs) and the global representation learning capabilities of ViTs. MobileViT proposes a novel hybrid architecture that integrates standard convolutional layers for local feature extraction with a lightweight Transformer block that processes information globally, effectively treating "transformers as convolutions." This design allows MobileViT to achieve impressive accuracy, such as 78.4\% top-1 on ImageNet-1k with approximately 6 million parameters, significantly outperforming both CNN-based (e.g., MobileNetv3) and ViT-based (e.g., DeiT) counterparts for similar parameter budgets. Its ability to learn global representations while retaining the efficiency of local processing makes it highly suitable for mobile vision tasks, demonstrating a foundational approach to lightweight hybrid ViTs.

Further advancing the field of deployable hybrid architectures, \textcite{li2022a4u} introduce Next-ViT, explicitly designed for efficient deployment in realistic industrial scenarios, considering factors like TensorRT and CoreML inference latency. Next-ViT proposes a "Next Convolution Block (NCB)" and a "Next Transformer Block (NTB)" that are deployment-friendly, capturing local and global information respectively. The "Next Hybrid Strategy (NHS)" then efficiently stacks these blocks. Unlike many ViTs that optimize for FLOPs or parameter count, Next-ViT prioritizes actual inference speed on target hardware, achieving substantial latency reductions while maintaining superior accuracy. For instance, it surpasses ResNet by 5.5 mAP on COCO detection and accelerates inference speed by 3.6x compared to CSWin under similar performance, highlighting its practical utility for edge computing where real-world latency is paramount. This represents a critical evolution from merely lightweight designs to truly deployment-optimized ones.

Beyond architectural hybridization, another crucial avenue for efficiency lies in redesigning the attention mechanism itself to mitigate its quadratic computational complexity. \textcite{chen2021r2y} introduce CrossViT, which focuses on learning multi-scale feature representations within a dual-branch transformer framework. CrossViT processes image patches of different sizes in separate branches and then fuses these multi-scale tokens using a highly efficient cross-attention mechanism. Crucially, their cross-attention module is designed to operate with linear computational and memory complexity, rather than the quadratic complexity typical of standard self-attention, thereby mitigating a major bottleneck for deploying ViTs on resource-limited hardware. This approach not only enhances representational power by leveraging features at various scales but also ensures that the fusion process remains computationally tractable. Complementing this, \textcite{song20215tk} propose UFO-ViT (Unit Force Operated Vision Transformer), which offers a novel self-attention mechanism with linear complexity by eliminating non-linearity and factorizing matrix multiplication without complex linear approximations. While CrossViT achieves linear scaling through a specialized cross-attention for multi-scale fusion, UFO-ViT fundamentally re-engineers the self-attention block itself to achieve linear complexity, offering a more general solution for reducing the computational burden of attention in any ViT layer.

Finally, while architectural and algorithmic innovations are vital, the ultimate efficiency on edge devices often necessitates hardware-aware optimization. \textcite{nag2023cfn} address this by proposing ViTA, a configurable hardware accelerator specifically designed for inference of Vision Transformer models on highly resource-constrained edge computing devices. ViTA employs a head-level pipeline and inter-layer MLP optimizations to avoid repeated off-chip memory accesses, a common bottleneck in embedded systems. It supports various ViT models with changes solely in its control logic, achieving nearly 90\% hardware utilization efficiency and reasonable frame rates at low power consumption (e.g., 0.88W at 150 MHz). This demonstrates that for true mobile optimization, a holistic approach combining efficient model design with dedicated hardware acceleration is indispensable, pushing the boundaries of what is achievable on the edge.

Collectively, these works demonstrate a clear progression towards making powerful Vision Transformer capabilities accessible for real-time and edge applications. They highlight the importance of hybrid architectures that judiciously combine CNN and Transformer strengths [mehta20216ad, li2022a4u], efficient attention mechanisms that scale linearly with input size [chen2021r2y, song20215tk], and the critical role of hardware-software co-design for practical deployment [nag2023cfn]. Despite these significant advancements, ongoing challenges include developing more sophisticated hardware-aware neural architecture search methods that consider power and memory constraints alongside latency, exploring novel quantization and pruning strategies tailored for these complex hybrid designs, and investigating dynamic execution strategies to adapt to varying computational budgets on the fly.
\subsection{Novel Hybrid Paradigms}
\label{sec:5_3_novel_hybrid_paradigms}


The continuous quest for more efficient, scalable, and powerful vision models has propelled research beyond the established CNN-Transformer hybrid architectures, leading to the exploration of entirely new component integrations. This subsection delves into novel hybrid paradigms that fuse distinct neural network components, moving beyond convolutional and self-attention mechanisms, to leverage their complementary strengths for enhanced performance and efficiency, particularly in modeling long-range dependencies and global context. Such approaches signify a dynamic and evolving research frontier, continuously pushing the boundaries of what is achievable in visual representation learning by incorporating components inspired by advancements in sequence modeling.

A significant recent development in this direction is the integration of State-Space Models (SSMs) into vision backbones, exemplified by architectures like MambaVision [hatamizadeh2024xr6]. Originating from control theory and recently revitalized for deep learning, SSMs offer an alternative mechanism for efficient sequence modeling, particularly adept at capturing long-range dependencies with linear computational complexity. MambaVision, proposed by Hatamizadeh et al. [hatamizadeh2024xr6], presents a hybrid architecture that integrates the Mamba SSM with Vision Transformers (ViT). The core motivation behind MambaVision is to harness Mamba's inherent efficiency in sequence modeling and its superior capacity for capturing long-range dependencies, while simultaneously retaining the robust global context understanding capabilities characteristic of Transformers. This fusion offers a compelling new perspective on designing vision backbones that can efficiently process extensive visual information, addressing some of the quadratic complexity issues of pure self-attention.

Hatamizadeh et al. [hatamizadeh2024xr6] meticulously redesigned the Mamba formulation to optimize its performance specifically for visual features, addressing the unique challenges posed by image data. Through comprehensive ablation studies, they demonstrated the feasibility and substantial benefits of integrating ViT components within the Mamba framework. A pivotal finding was that the strategic incorporation of self-attention blocks in the final layers of the Mamba architecture significantly enhanced its ability to capture intricate long-range spatial dependencies, a critical aspect for achieving high performance across diverse vision tasks. This hybrid design allows MambaVision to benefit from the local processing and linear scaling of Mamba while leveraging the global reasoning of attention where most critical. The efficacy of MambaVision is robustly supported by its empirical results across multiple benchmarks, achieving state-of-the-art (SOTA) performance on ImageNet-1K classification and demonstrating favorable performance in downstream tasks such as object detection, instance segmentation on MS COCO, and semantic segmentation on ADE20K, often outperforming comparably sized backbones.

Another emerging paradigm involves adapting Receptance Weighted Key Value (RWKV) models, initially developed for efficient large language models, to visual perception. Vision-RWKV (VRWKV), introduced by Duan et al. [duan2024q7h], is a notable example. Transformers, while powerful, face limitations in high-resolution image processing due to their quadratic computational complexity. VRWKV addresses this by adapting the RWKV model, which processes sequences in a recurrent manner while maintaining a Transformer-like attention mechanism, but with significantly reduced spatial aggregation complexity. This design allows VRWKV to efficiently handle sparse inputs and demonstrate robust global processing capabilities, scaling effectively to large parameters and extensive datasets without the necessity for windowing operations often employed in hierarchical Transformers.

Duan et al. [duan2024q7h] made necessary modifications to the RWKV architecture to tailor it for vision tasks, enabling it to function as an efficient and scalable visual backbone. Their evaluations demonstrated that VRWKV surpasses the performance of traditional Vision Transformers (ViTs) in image classification, while also exhibiting significantly faster speeds and lower memory usage when processing high-resolution inputs. Furthermore, in dense prediction tasks, VRWKV was shown to outperform window-based models, maintaining comparable speeds. These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks, particularly where high-resolution processing and long-context analysis are critical, without incurring the prohibitive computational costs of full self-attention.

The emergence of models like MambaVision and Vision-RWKV represents a significant paradigm shift, moving beyond the established dichotomy of CNN-Transformer integrations. Both approaches aim to address the computational burdens associated with pure self-attention mechanisms while preserving or enhancing powerful global reasoning capabilities. MambaVision leverages the linear scaling of SSMs for efficient long-range dependency modeling, strategically reintroducing self-attention in later layers to refine global context. In contrast, Vision-RWKV adapts a distinct recurrent-attention mechanism from NLP, offering reduced spatial aggregation complexity and superior high-resolution processing without explicit windowing. While MambaVision focuses on a hybrid SSM-ViT structure, VRWKV presents a more direct adaptation of an efficient sequence model, emphasizing its unique recurrent-attention mechanism. Both paradigms demonstrate the immense potential of exploring non-traditional neural network components and their synergistic combinations for advancing the field of computer vision. Their success unequivocally highlights the value of cross-domain inspiration, particularly from efficient sequence models in NLP, to develop more efficient, powerful, and specialized vision models for future applications.


### Applications of Visual Transformers

\section{Applications of Visual Transformers}
\label{sec:applications_of_visual_transformers}



\subsection{Core Vision Tasks: Classification and Object Detection}
\label{sec:6_1_core_vision_tasks:_classification__and__object_detection}


Vision Transformers (ViTs) have profoundly impacted fundamental computer vision tasks, particularly image classification and object detection, by leveraging their global receptive fields and powerful feature extraction capabilities. This architectural shift has enabled ViTs to capture both broad contextual information and fine-grained details, often leading to new performance benchmarks that frequently surpass traditional Convolutional Neural Networks (CNNs), especially when trained on extensive datasets.

The initial foray of Transformers into vision began with image classification. The seminal Vision Transformer (ViT) by [Dosovitskiy2021] demonstrated that a pure Transformer, by treating image patches as sequential tokens, could effectively classify images. While achieving state-of-the-art results on massive datasets like ImageNet-21K (e.g., ViT-L/16 reaching 88.55\% top-1 accuracy), it initially exhibited a significant dependency on vast training data, often underperforming CNNs on smaller datasets. To mitigate this data-hungry nature, [Touvron2021] introduced Data-efficient Image Transformers (DeiT), employing knowledge distillation to enable ViTs to be trained efficiently on ImageNet-1K, achieving competitive performance (e.g., DeiT-S reaching 83.1\% top-1 accuracy) without requiring massive pre-training. Concurrently, [Yuan2021] proposed Tokens-to-Token ViT (T2T-ViT), which refined the initial tokenization process to better represent local structures, thereby boosting performance on standard datasets from scratch. Further architectural advancements aimed at enhancing the depth and stability of ViTs for classification; [zhou202105h] identified the "attention collapse" issue in deeper ViTs and proposed Re-attention to increase the diversity of attention maps, enabling the training of significantly deeper models with consistent performance gains. More recently, `[ferdous2024f89]` introduced SPT-Swin, a variant that combines shifted patch tokenization with Swin Transformer to address data deficiency and computational complexity, achieving 89.45\% accuracy on ImageNet-1K.

A major breakthrough in efficient pre-training for classification came with [CVPR2022], which introduced Masked Autoencoders (MAE). This self-supervised learning approach reconstructs masked image patches, proving highly scalable and data-efficient. MAE enables ViTs to learn robust representations from unlabeled data, significantly improving their performance and reducing the need for extensive labeled datasets for classification. For instance, a ViT-Large pre-trained with MAE on ImageNet-1K can achieve 87.8\% top-1 accuracy. Building on this, the field has seen the emergence of "Vision Foundation Models," with [ICLR2023] demonstrating the efficacy of scaling ViTs to over a billion parameters, achieving unprecedented classification performance through massive pre-training. Similarly, [ICLR2023] advanced self-supervised learning with DINOv2, which learns highly robust and generalizable visual features without supervision, often outperforming supervised pre-training in transfer tasks and further enhancing classification accuracy. The influence of ViT principles even extended to CNNs, as shown by [ICLR2023], where ConvNeXt V2 leveraged MAE pre-training to significantly boost CNN performance, blurring the lines between the two architectures for classification tasks. For practical deployment, `[song2022603]` proposed CP-ViT, a cascade pruning framework that dynamically predicts sparsity in ViT models, reducing FLOPs by over 40\% while maintaining accuracy, crucial for resource-limited devices.

The success of ViTs quickly extended to object detection, a more complex task requiring both accurate object categorization and precise localization. Early pure ViTs struggled with dense prediction tasks due to their fixed-size patch embeddings and lack of inherent hierarchical feature maps, which are crucial for detecting objects at various scales. This limitation was fundamentally addressed by the introduction of end-to-end Transformer detectors. The seminal work on DEtection TRansformer (DETR) by [Carion2020] revolutionized object detection by formulating it as a direct set prediction problem. DETR eliminated hand-designed components like Non-Maximum Suppression (NMS) by using a set of learned object queries and a bipartite matching loss. While DETR demonstrated the power of Transformers for detection, its slow convergence and high computational cost were initial drawbacks. These were largely overcome by Deformable DETR [Zhu2020], which introduced deformable attention to focus on a small set of key sampling points, significantly accelerating training and improving performance, particularly for small objects.

Following these foundational end-to-end Transformer detectors, architectural innovations focused on integrating hierarchical ViT backbones to better capture multi-scale visual features. The [Liu2021] Swin Transformer, detailed in `[liu2021ljs]`, revolutionized ViT applicability to dense prediction by proposing a hierarchical architecture with shifted window attention. This design limited self-attention computation to non-overlapping local windows while allowing for cross-window connections, effectively generating multi-scale feature maps. Swin Transformers became highly suitable as backbones for object detection frameworks, achieving state-of-the-art results, such as 58.7 box AP and 51.1 mask AP on COCO test-dev with Swin-Large. Similarly, [Wang2021] introduced the Pyramid Vision Transformer (PVT), a pure Transformer-based pyramid structure that progressively reduces feature map resolution, enabling the generation of multi-scale features essential for detecting objects of different sizes. These hierarchical backbones, when integrated with DETR-like heads, led to powerful detectors. For instance, `[wang2023bfo]` combined Deformable DETR with a Swin Transformer and a lightweight Feature Pyramid Network (FPN) to enhance detection accuracy for multi-scale targets, demonstrating a 6.1\% improvement in accuracy on a classroom behavior dataset.

Interestingly, even plain (non-hierarchical) ViT backbones, when properly leveraged, have shown strong performance in object detection. `[li2022raj]` explored the use of plain ViT backbones, pre-trained with MAE, for object detection. Their work, ViTDet, demonstrated that with minimal adaptations like a simple feature pyramid and limited cross-window propagation, these models could achieve competitive results on the COCO dataset, reaching up to 61.3 AP\_box using only ImageNet-1K pre-training. This highlighted that the powerful representations learned by plain ViTs, especially through self-supervised pre-training, could be effectively fine-tuned for localization tasks, challenging the strict necessity of hierarchical backbones for all detection scenarios. The convergence of ViT backbones and end-to-end Transformer detectors is further exemplified by `[song2022y4v]`, which introduced ViDT, an extendable and efficient object detector integrating Vision and Detection Transformers. ViDT reconfigures the Swin Transformer as a standalone detector and employs an efficient Transformer decoder, achieving an excellent AP and latency trade-off on COCO. Furthermore, the advancements in large-scale ViT pre-training, such as those in [ICLR2023] and [ICLR2023], implicitly provide even more robust and generalizable backbones for object detection, allowing downstream detection models to achieve higher accuracy and better generalization across diverse scenarios.

In conclusion, Vision Transformers have established themselves as formidable architectures for core vision tasks. From their initial success in image classification, overcoming challenges related to data efficiency and architectural depth through innovations like DeiT and MAE, to their subsequent adaptation for object detection via end-to-end Transformer designs (DETR) and hierarchical backbones (Swin Transformer), ViTs have consistently pushed performance boundaries. While initial pure ViTs were data-hungry and lacked inherent inductive biases for dense prediction, the continuous evolution has led to more efficient, robust, and scalable models. The ongoing development of massive "Vision Foundation Models" and sophisticated hybrid architectures, leveraging advanced self-supervised learning, signifies a future where increasingly versatile visual intelligence can be deployed across an even wider spectrum of real-world applications, further solidifying ViTs' foundational strength in capturing both global context and fine-grained details.
\subsection{Dense Prediction Tasks: Segmentation and Pose Estimation}
\label{sec:6_2_dense_prediction_tasks:_segmentation__and__pose_estimation}

Dense prediction tasks, encompassing semantic segmentation, instance segmentation, and human pose estimation, demand pixel-level understanding and precise localization, moving significantly beyond image-level classification. Vision Transformers (ViTs), with their inherent ability to model long-range dependencies, have emerged as powerful tools in these fine-grained visual analysis tasks. However, the direct application of vanilla ViTs was initially hindered by their high computational cost, quadratic complexity with respect to image resolution, and lack of inherent multi-scale feature representation, which is crucial for pixel-level tasks [li2023287].

To address these limitations, hierarchical ViT architectures quickly emerged as a foundational solution. The \textit{Swin Transformer} [liu2021swin] introduced shifted window attention, enabling efficient computation by restricting self-attention within local windows while allowing cross-window connections through shifting. This hierarchical design effectively generates multi-scale feature maps, making it highly suitable as a backbone for dense prediction. Similarly, the \textit{Pyramid Vision Transformer} (PVT) [wang2021pyramid] explicitly designed a pyramid structure to produce multi-scale features, directly mimicking the feature pyramid networks (FPNs) commonly used in dense prediction, thereby facilitating their integration into existing frameworks. Further building on this, \textit{HiViT} [zhang2022msa] proposed another hierarchical ViT design, demonstrating improved efficiency and performance for downstream tasks like detection and segmentation when pre-trained with masked image modeling (MIM). These hierarchical designs were pivotal, providing the necessary multi-scale representations that pure ViTs lacked, thereby enabling more coherent and accurate pixel-wise predictions.

As ViT architectures matured, research consolidated strategies for their adaptation to pixel-level tasks, broadly falling into two categories: ViT encoders paired with specialized decoders, and end-to-end Transformer designs. Early and effective approaches often paired hierarchical ViT backbones with established CNN-style decoders. For instance, studies demonstrated the efficacy of integrating Swin Transformer with decoders like U-Net, Pyramid Scene Parsing (PSP) network, or Feature Pyramid Network (FPN) for semantic segmentation, particularly in domains like remote sensing [panboonyuen20218r7]. This hybrid approach leverages the ViT's strong global feature extraction while benefiting from the CNN decoder's inductive biases for local detail and spatial upsampling. In contrast, end-to-end Transformer designs sought to eliminate reliance on hand-crafted CNN components. \textit{Max-DeepLab} [wang2021maxdeeplab] showcased the capacity of ViTs to directly learn pixel-level representations and segmentations for semantic segmentation, leveraging the global receptive field of Transformers to capture broader contextual cues and produce consistent masks. A significant conceptual advance for instance segmentation (and panoptic/semantic segmentation) was \textit{Mask2Former} [cheng2022mask2former], which unified these tasks under a single query-based Transformer framework. Mask2Former uses a masked attention mechanism and a set of learnable queries to directly predict object masks and their classes, demonstrating superior performance on complex scenes and representing a paradigm shift towards more unified, global reasoning for pixel-level tasks.

The versatility of ViTs extends to other dense prediction tasks beyond segmentation. For human pose estimation, which involves localizing keypoints on human bodies, models like \textit{ViTPose} [xu2022vitpose] have showcased how ViTs can effectively model spatial relationships between body parts. Their global attention mechanism is crucial for inferring occluded or less visible keypoints based on overall body context, often outperforming CNN-based methods. Furthermore, ViTs have been adapted for specialized segmentation challenges. For example, \textit{SENet} [hao202488z] proposes a simple yet effective ViT-based encoder-decoder for camouflaged and salient object detection, demonstrating how ViTs can be tailored for specific pixel-level understanding problems. In the medical imaging domain, \textit{SwinBTS} [jiang2022zcn] leverages the Swin Transformer for 3D multimodal brain tumor segmentation, highlighting ViT's applicability in critical, high-dimensional analysis.

The recent advancements in foundational Vision Transformers (as discussed in Section 4) have profoundly amplified their impact on dense prediction. Models pre-trained with self-supervised learning strategies like Masked Autoencoders (MAE) [he2022masked] or advanced knowledge distillation (e.g., DINOv2 [oquab2023dinov2]) provide exceptionally robust and generalizable backbones. When fine-tuned for dense prediction tasks, these large-scale models, often paired with task-specific decoders (e.g., UPerNet heads or Mask2Former heads), achieve unprecedented accuracy and efficiency, significantly reducing the need for extensive labeled data for downstream tasks. Moreover, hybrid architectures (as detailed in Section 5) have further refined feature extraction for dense prediction. Models like \textit{InternImage} [wang2023internimage] and \textit{ConvNeXt V2} [woo2023convnext], which integrate deformable convolutions or enhance CNNs with MAE pre-training, provide backbones that combine the inductive biases of CNNs (e.g., locality, efficiency) with the global context modeling of Transformers. More recently, hybrid Mamba-Transformer backbones like \textit{MambaVision} [hatamizadeh2024xr6] have also demonstrated strong performance on instance and semantic segmentation, showcasing new avenues for efficient long-range dependency modeling. Even efficient variants like \textit{MobileViT V2} [vaswani2023mobilevit] aim to bring these powerful local-global feature interactions to resource-constrained environments, broadening the practical applicability of ViTs for real-time dense prediction.

In conclusion, Vision Transformers have firmly established themselves as formidable architectures for dense prediction tasks. Their evolution from basic image classifiers to sophisticated pixel-level understanding models has been driven by architectural innovations that address computational complexity and multi-scale representation, coupled with specialized decoder designs and end-to-end Transformer frameworks. The recent advent of large-scale, self-supervised pre-trained foundation models has further empowered ViTs, providing exceptionally robust and generalizable backbones that significantly enhance performance across semantic segmentation, instance segmentation, and pose estimation. Future research will likely focus on optimizing the fusion of local and global features, developing more efficient architectures for real-time dense prediction, and exploring novel ways to leverage the rich contextual understanding of foundation models for even finer-grained and more complex pixel-level analyses.
\subsection{Specialized and Multimodal Applications}
\label{sec:6_3_specialized__and__multimodal_applications}


Beyond their foundational success in standard image classification and object detection, Vision Transformers (ViTs) have demonstrated remarkable versatility and adaptability across a diverse array of specialized and complex application areas. This subsection highlights their profound utility in challenging real-world scenarios, ranging from low-level image processing to the analysis of high-dimensional and multimodal data, showcasing their capacity to transcend traditional benchmarks and open new avenues for research and deployment.

One significant domain where ViTs have excelled is image restoration, a crucial low-level vision task demanding precise pixel-level manipulation and global consistency. Traditional convolutional neural networks (CNNs) often struggle with capturing long-range dependencies essential for coherent restoration across an entire image. \textcite{liang2021v6x} addressed this by introducing SwinIR, a robust baseline for tasks such as super-resolution, denoising, and JPEG compression artifact reduction. SwinIR leverages the hierarchical Swin Transformer, which, through its shifted window attention, efficiently captures both local details and global structural information. This hierarchical design allows SwinIR to effectively model non-local correlations within images, crucial for hallucinating high-frequency details in super-resolution or removing noise while preserving fine textures, ultimately achieving superior performance over state-of-the-art CNNs with a reduced parameter count. Similarly, for visual saliency detection, which requires identifying the most visually prominent regions at a pixel level, \textcite{liu2021jpu} proposed the Visual Saliency Transformer (VST). VST re-conceptualizes saliency detection as a convolution-free sequence-to-sequence prediction problem. By employing multi-level token fusion and a novel token upsampling method, VST effectively processes both RGB and RGB-D inputs, demonstrating ViT's inherent capability for fine-grained pixel-level understanding and its adaptability to multimodal inputs through a token-based multi-task decoder. The global attention mechanism of VST allows it to integrate contextual information across the entire image, which is vital for distinguishing salient objects from complex backgrounds, a task where local CNN receptive fields might fall short.

The ability of ViTs to process and fuse information from diverse and high-dimensional data modalities further underscores their adaptability. For hyperspectral image classification (HSIC), which involves analyzing data cubes with hundreds of spectral bands alongside spatial information, ViTs offer a powerful alternative to traditional methods. HSIC presents unique challenges due to its high dimensionality, spectral redundancy, and often limited labeled samples. \textcite{zhao2024671} introduced the Groupwise Separable Convolutional Vision Transformer (GSC-ViT) to address these issues. GSC-ViT integrates a Groupwise Separable Convolution (GSC) module to efficiently extract local spectral-spatial features, mitigating the parameter burden of pure ViTs and enhancing local representation. Concurrently, a Groupwise Separable Multihead Self-Attention (GSSA) module captures both local and global spatial feature dependencies, allowing the model to effectively learn intricate relationships across the spectral and spatial dimensions. This hybrid approach demonstrates surprising classification performance even with fewer training samples, highlighting the ViT's capacity to model complex, high-dimensional data by judiciously combining inductive biases.

Beyond visual imagery, ViTs have proven effective in processing non-standard data types, such as radar signals for human activity recognition (HAR). HAR from radar data is challenging due to the abstract nature of micro-Doppler signatures, which represent subtle motion patterns. \textcite{huan202345b} proposed a Lightweight Hybrid Vision Transformer (LH-ViT) network for radar-based HAR. While the architectural details of LH-ViT are discussed in Section 5.2, its application here demonstrates how ViTs can effectively capture global temporal patterns within micro-Doppler spectrograms, which are crucial for distinguishing different human activities. The global attention mechanism allows the model to correlate features across the entire time-frequency representation, providing a more holistic understanding of the activity compared to local processing methods. The emphasis on a lightweight design further underscores the practical considerations for deploying such models in real-time or embedded radar systems.

Furthermore, ViTs are increasingly pivotal in multimodal data fusion, where information from heterogeneous sources must be synergistically combined for robust understanding. For multimodal land use and land cover (LULC) classification, which often involves integrating optical, hyperspectral, LiDAR, and Synthetic Aperture Radar (SAR) data, \textcite{yao2023sax} developed the Extended Vision Transformer (ExViT). This framework extends conventional ViTs with parallel branches, each tailored for a specific modality (e.g., hyperspectral and LiDAR/SAR data), utilizing position-shared ViTs and separable convolutions for efficient feature extraction. Critically, ExViT employs a cross-modality attention (CMA) module to facilitate dynamic information exchange and alignment between these heterogeneous modalities. This deep fusion mechanism, culminating in a robust decision-level fusion, leverages the ViT's token-based architecture to seamlessly integrate diverse data representations, leading to significantly improved classification accuracy and robustness compared to single-modality or simpler fusion approaches.

Collectively, these applications underscore the remarkable versatility of Vision Transformers. They illustrate how ViTs can be meticulously engineered, often through hybrid architectures, specialized attention mechanisms, and novel tokenization strategies, to effectively process diverse data typesfrom standard images and spectral cubes to radar micro-Doppler mapsand tackle complex tasks like restoration, saliency detection, high-dimensional classification, and multimodal fusion. The continuous drive to optimize ViTs for efficiency and integrate them with domain-specific inductive biases (e.g., convolutions for local features) highlights a key development direction. Future research will likely focus on further enhancing their efficiency for edge deployment, exploring more sophisticated multimodal fusion strategies, and adapting them to novel data modalities and real-time inference challenges in increasingly complex environments, thereby expanding their utility across an even broader spectrum of scientific and industrial applications.


### Future Outlook: Challenges and Opportunities

\section{Future Outlook: Challenges and Opportunities}
\label{sec:future_outlook:_challenges__and__opportunities}



\subsection{Persistent Challenges: Computational Cost and Data Efficiency}
\label{sec:7_1_persistent_challenges:_computational_cost__and__data_efficiency}


Despite the remarkable progress in Vision Transformers (ViTs) across diverse computer vision tasks, their substantial computational cost and persistent reliance on extensive datasets remain critical hurdles, dictating their broader adoption and sustainability, particularly in resource-constrained environments and for specialized applications. While earlier sections have detailed numerous advancements, these challenges are far from resolved, continuously driving innovation to balance performance with practicality.

The foundational ViT [Dosovitskiy2021] introduced a quadratic computational complexity with respect to image resolution due to its global self-attention mechanism, alongside a significant appetite for massive pre-training datasets like JFT-300M. Subsequent architectural innovations, such as the hierarchical Swin Transformer [Liu2021] and Pyramid Vision Transformer (PVT) [Wang2021], mitigated this by restricting attention to local windows or employing spatial reduction. Similarly, advanced attention mechanisms like Focal Attention [FocalAttention2022] aimed to capture context efficiently. However, these solutions often introduce trade-offs; local attention, while efficient, can sacrifice the model's inherent ability to capture true global dependencies, which was a core advantage of the original Transformer. Furthermore, even linear complexity can be prohibitive for processing gigapixel images in domains like digital pathology, highlighting that architectural fixes alone are insufficient.

The quest for computational efficiency extends beyond architectural design into post-training optimization and hardware-aware deployment. Model quantization, which reduces the precision of weights and activations, is crucial for inference on edge devices. Yet, ViTs present unique challenges for quantization due to the sensitivity of components like Layer Normalization and the non-uniform distribution of attention maps [lin2021utw]. While methods like Q-ViT [li20229zn] and FQ-ViT [lin2021utw] have pushed the limits of fully differentiable and post-training quantization, achieving near-lossless accuracy at lower bit-widths, the inherent complexity of ViT operations still demands specialized techniques to prevent severe performance degradation. Concurrently, model pruning techniques, which remove redundant parameters or operations, are vital. Research explores multi-dimensional compression, pruning attention heads, neurons, and even input sequences [song2022603, hou2022ver, yin2023029]. These methods, such as CP-ViT [song2022603] and GOHSP [yin2023029], aim to identify and remove deleterious components while preserving accuracy, but the challenge lies in developing robust, generalized pruning criteria that do not require extensive retraining or compromise the model's representational power. Ultimately, for deployment in highly resource-constrained environments, dedicated hardware accelerators like ViTA [nag2023cfn] are becoming indispensable, demonstrating that a holistic hardware-software co-design approach is necessary to truly overcome the computational bottleneck.

Data efficiency, another critical challenge, has seen significant breakthroughs with self-supervised learning (SSL) and knowledge distillation. Data-efficient Image Transformers (DeiT) [Touvron2021] demonstrated competitive performance with ImageNet-1K pre-training through distillation from a CNN teacher, while Tokens-to-Token ViT (T2T-ViT) [Yuan2021] improved initial tokenization. The advent of Masked Autoencoders (MAE) [he2022masked] and advanced self-distillation techniques like DINOv2 [oquab2023dinov2] further revolutionized pre-training by enabling ViTs to learn robust features from vast quantities of *unlabeled* data. However, even with these advancements, the "data hunger" persists in different forms. Large foundation models, while powerful, still necessitate colossal datasets for pre-training, which may not be universally accessible or ethically diverse. Moreover, adapting these general-purpose models to specialized domains (e.g., medical imaging, remote sensing) where labeled data is scarce remains a significant hurdle. For instance, MAT-VIT [han2024f96] explores MAE-based auxiliary tasks to leverage unlabeled medical images, highlighting the ongoing need for domain-specific data efficiency strategies.

The traditional "pre-train and fine-tune" paradigm, while effective, also presents challenges. Training ViT-based object detectors from scratch, as explored by [hong2022ks6], reveals that simply switching backbones from CNNs to ViTs does not generalize well, emphasizing the deep reliance of ViTs on large-scale pre-training. Furthermore, optimization techniques commonly used in deep learning do not always translate seamlessly to ViTs; for example, gradient accumulation, often used to simulate larger batch sizes, was found to decrease accuracy and increase training time for Swin Transformers [aburass2023qpf], underscoring the need for ViT-specific optimization strategies. Even in knowledge distillation, researchers continue to refine techniques, with methods like Attention Distillation [wang2022pee] showing that self-supervised ViT students require more nuanced guidance to effectively close the performance gap with teachers. The emergence of "simple" hierarchical ViTs like Hiera [ryali202339q], which strip away architectural "bells-and-whistles" when combined with strong SSL, further suggests that the true drivers of efficiency and performance might lie in the pre-training strategy rather than complex architectural designs.

In conclusion, the computational cost and data efficiency of Vision Transformers are not static problems but dynamic challenges that evolve with architectural innovations and deployment contexts. While significant strides have been made through hierarchical designs, advanced attention, quantization, pruning, and self-supervised learning, these solutions often introduce new trade-offs or highlight the need for further refinement. The continuous efforts to develop more parameter-efficient models, optimize training strategies, and improve generalization from limited data, especially for specialized tasks and resource-constrained environments, remain a fertile and critical ground for future research, pushing towards truly sustainable and ubiquitous visual AI.
\subsection{Interpretability, Robustness, and Generalization}
\label{sec:7_2_interpretability,_robustness,__and__generalization}

The trustworthy deployment of Vision Transformers (ViTs) in real-world applications critically hinges on their interpretability, robustness, and generalization capabilities. These aspects are paramount for fostering confidence in AI systems, ensuring their reliable and safe operation in diverse environments, and upholding ethical standards, especially in sensitive domains.

A fundamental challenge in ViTs, as with many deep learning models, lies in interpreting their complex decision-making processes. Unlike Convolutional Neural Networks (CNNs) where feature maps often correspond to spatially localized patterns, the global self-attention mechanism in Transformers makes direct interpretation more elusive. Early attempts to interpret ViTs often relied on visualizing raw attention maps, but this approach has been critiqued for its limitations; raw attention scores do not directly represent feature importance or causal contributions to the output [jain2019attention]. More rigorous explainable AI (XAI) methods adapted for Transformers include attention rollout [abnar2020quantifying], which propagates attention through layers to aggregate relevance, and Layer-wise Relevance Propagation (LRP) [bach2015pixel], which decomposes the prediction backward through the network to assign relevance scores to input pixels. Gradient-based attribution methods, such as Grad-CAM [selvaraju2017grad] and its variants, have also been applied to ViTs to highlight salient regions influencing decisions. While these methods offer valuable insights, a comprehensive, human-understandable explanation of ViT reasoning, particularly in safety-critical applications, remains an active research area. Furthermore, some studies, such as [wang2022da0], suggest that the attention mechanism itself might not be the sole or even primary driver of ViT success, demonstrating that it can be replaced by simpler shift operations with comparable performance. This raises questions about the true mechanistic role of attention and, consequently, the validity of solely relying on attention-based explanations. Efforts like Re-attention [zhou202105h] aimed to diversify attention maps in deeper ViTs to prevent "attention collapse" and improve representation learning, which indirectly aids in making attention patterns more informative, but does not fundamentally solve the interpretability challenge.

Beyond interpretability, ensuring the robustness of ViTs against adversarial attacks and distribution shifts is crucial. Robustness is a multi-faceted concept, encompassing resilience to imperceptible adversarial perturbations, robustness to common image corruptions (e.g., noise, blur), and generalization to out-of-distribution (OOD) data. While initial ViTs demonstrated strong performance on benchmark datasets, their susceptibility to adversarial attacks was quickly identified, akin to CNNs [mao2021zr1, almalik20223wr]. For instance, \textcite{almalik20223wr} proposed Self-Ensembling Vision Transformer (SEViT) to enhance adversarial robustness in medical image classification by leveraging intermediate feature representations and combining multiple classifiers.

Significant strides in improving ViT robustness and generalization have been made through advanced pre-training paradigms, particularly self-supervised learning (SSL). As discussed in Section 4, methods like Masked Autoencoders (MAE) [mae2022] and DINOv2 [dino2023] have enabled ViTs to learn powerful, transferable representations from vast amounts of unlabeled data. These SSL approaches force models to learn rich semantic features by reconstructing masked patches or performing self-distillation, leading to representations that are inherently more robust to variations and distribution shifts compared to purely supervised pre-training. For example, DINOv2 [dino2023] has shown remarkable performance on various downstream tasks and improved transferability, often outperforming supervised counterparts, by producing robust visual features without explicit supervision. The systematic evaluation by \textcite{mao2021zr1} further highlights that certain ViT components can be detrimental to robustness, and by leveraging robust building blocks and techniques like position-aware attention scaling and patch-wise augmentation, they proposed Robust Vision Transformer (RVT) which achieved superior performance on benchmarks like ImageNet-C (common corruptions), ImageNet-R (renditions), and ImageNet-Sketch (sketches), demonstrating enhanced resilience to various distribution shifts.

The pursuit of "Vision Foundation Models," as explored in Section 4.3, further exemplifies this drive towards universal robustness and generalization. Models scaled to billions of parameters, such as those by \textcite{scalingvit2023} and \textcite{internimage2023}, aim to serve as versatile backbones capable of adapting to a wide array of vision tasks with minimal fine-tuning. This extreme scaling, often coupled with advanced self-supervised pre-training, is hypothesized to imbue models with a deeper understanding of visual semantics, thereby enhancing their generalization to novel situations and improving their resilience to variations in input data.

Moreover, the integration of inductive biases from CNNs into Transformer architectures has also contributed to developing more robust and generalizable models. Hybrid architectures, such as ConvNeXt V2 [convnextv22023] (discussed in Section 5.1), which co-designs CNNs with MAE pre-training, and InternImage [internimage2023] (also from Section 5.1), which incorporates deformable convolutions into large-scale vision foundation models, represent a synergistic approach. By combining the local feature extraction strengths of CNNs with the global context modeling of Transformers, these models aim to achieve a more comprehensive and robust understanding of visual data, often exhibiting better performance on out-of-distribution tasks. For instance, \textcite{zhou2021rtn} systematically investigated the transfer learning ability of ConvNets and vision transformers across 15 downstream tasks, observing consistent advantages for Transformer-based backbones on 13 tasks, particularly noting their robustness in multi-task learning and their reliance on whole-network fine-tuning for optimal transfer. Practical considerations for real-world deployment also necessitate efficient architectures that maintain performance, contributing to operational robustness. While not directly addressing adversarial or OOD robustness, efficient ViT variants like MobileViT V2 [mobilevitv22023] (from Section 5.2) ensure reliable performance within resource constraints, which is a form of practical robustness for deployment.

Despite significant strides in enhancing the robustness and generalization of ViTs through scaling, advanced self-supervised learning, and hybrid designs, the challenge of achieving true interpretability remains complex. While methods offer glimpses into model behavior, a holistic, causal understanding of ViT decisions, especially in safety-critical applications, is still an active research area. Future work must continue to explore novel techniques for transparent model design, robust evaluation against diverse adversarial threats and OOD data, and the development of ViTs that are not only high-performing but also inherently understandable and trustworthy.
\subsection{Future Trends and Open Problems}
\label{sec:7_3_future_trends__and__open_problems}


The evolution of Vision Transformers (ViTs) is rapidly accelerating, pushing the frontiers of artificial intelligence towards more comprehensive, intelligent, and human-like visual systems. This subsection looks ahead, identifying cutting-edge research directions and unresolved questions that will define the next generation of ViT architectures. While Section 7.1 and 7.2 address persistent challenges related to efficiency, robustness, and interpretability, this section focuses on more speculative, transformative trends and deeper, unsolved theoretical problems that extend beyond incremental improvements.

A significant future trend lies in the exploration of **novel architectural paradigms that fundamentally rethink the self-attention mechanism**. While ViTs excel at global context modeling, their quadratic computational complexity remains a bottleneck. The integration of ViTs with State-Space Models (SSMs), such as Mamba, represents a particularly exciting frontier. MambaVision [hatamizadeh2024xr6] exemplifies this by redesigning the Mamba formulation for visual features and strategically incorporating self-attention blocks in later layers to effectively capture long-range spatial dependencies. This hybrid approach demonstrates state-of-the-art performance and throughput, suggesting a promising path for combining the strengths of different architectures to overcome individual limitations. Beyond such integrations, a more radical open question is whether attention is truly indispensable. Research like ShiftViT [wang2022da0] explores this by replacing attention layers with a zero-parameter shift operation, achieving competitive performance and prompting a re-evaluation of the core mechanisms driving ViT success. This suggests a future where attention might be replaced or heavily augmented by simpler, more efficient operations, as also highlighted by broader surveys on attention mechanism redesigns [heidari2024d9k]. Furthermore, the development of adaptive architectures that can dynamically adjust their computational patterns based on input characteristics, such as Dynamic Window Vision Transformer (DW-ViT) [ren2022ifo] which assigns windows of different sizes to various attention heads, offers a pathway to more flexible and robust visual understanding beyond fixed windowing.

The ultimate ambition for many researchers is the development of truly **universal and multimodal visual foundation models** capable of sophisticated reasoning. Building upon the success of large-scale pre-training [ICLR2023_Scaling_Vision_Transformers_to_1_Billion_Parameters, ICLR2023_DINOv2], the field is now focused on creating generalist backbones that transcend individual tasks and modalities. GiT [wang20249qa] represents a significant step, proposing a vanilla ViT framework with a universal language interface that can simultaneously handle diverse vision tasksfrom image captioning to object detection and segmentationwithout task-specific modifications. This approach aims to narrow the architectural gap between vision and language models, fostering mutual enhancement across tasks. The extension of these models to genuinely multimodal reasoning, integrating vision not only with language but also with audio, 3D data, and other sensor modalities, is paramount. This requires novel architectural designs and pre-training strategies to effectively fuse information from disparate sources, enabling a more holistic and human-like understanding. For instance, unifying 2D and 3D vision remains a significant challenge, with efforts like Simple3D-Former [wang2022gq4] demonstrating that a standard 2D ViT can be minimally customized to perform robustly on 3D tasks, suggesting a path towards more transferable architectures. In practical applications, multimodal fusion is already yielding benefits, such as ViT-FuseNet [zhou2024toe] for vehicle-infrastructure cooperative perception (LiDAR and camera) and NF-DVT [pan20249k5] for monocular 3D object detection (RGB and depth), but the grand challenge is to generalize these fusion capabilities across *any* combination of modalities.

Beyond architectural and multimodal advancements, several **deeper open problems in visual intelligence** persist, representing grand challenges for the field. One critical area is **compositional generalization and relational reasoning**. While ViTs can learn complex patterns, their ability to reason about novel combinations of objects and their relationships, similar to human cognition, remains limited. RelViT [ma2022vf3] offers a concept-guided approach to improve relational reasoning and systematic generalization, but achieving robust compositional understanding across diverse, unseen scenarios is a profound theoretical and practical hurdle. This involves understanding how ViTs can move beyond statistical correlations to infer causal relationships within visual scenes, a key component of true intelligence. Another significant challenge is **lifelong or continual learning**, where ViTs can adapt to new tasks and data streams without catastrophically forgetting previously acquired knowledge. This is essential for deploying models in dynamic real-world environments that evolve over time. Furthermore, integrating ViTs into **embodied AI systems** that interact with the physical world, requiring real-time perception, planning, and action, presents complex challenges in bridging the gap between static image understanding and dynamic environmental interaction.

Finally, as ViTs become increasingly powerful and ubiquitous, the **ethical implications and the need for trustworthy AI systems** become paramount [hassija2025wq3]. While Section 7.2 discusses interpretability and robustness as ongoing challenges, the future demands a proactive approach to designing ViTs that are inherently fair, transparent, and privacy-preserving. Developing generalizable interpretability methods for massive, multimodal foundation models is a major open problem. For instance, prototype-based interpretable ViTs like ProtoViT [ma2024uan] offer local explanations by comparing image parts to learned prototypes, but scaling such fine-grained interpretability to the complexity of universal foundation models remains an active research area. Mitigating biases embedded in training data, ensuring equitable performance across diverse demographic groups, and developing robust privacy-preserving mechanisms for visual data are not just technical hurdles but critical societal imperatives for the responsible deployment of future vision systems.

In conclusion, the future of Vision Transformers is characterized by a relentless pursuit of greater efficiency through novel architectural paradigms, the development of truly generalist and multimodal foundation models, and a deeper engagement with fundamental problems of visual intelligence such as compositional and causal reasoning. Crucially, addressing the ethical dimensions of these powerful systems will be indispensable for realizing their transformative potential in a responsible and beneficial manner.


