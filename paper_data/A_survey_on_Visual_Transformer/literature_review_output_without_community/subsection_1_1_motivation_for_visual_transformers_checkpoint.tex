\subsection*{Motivation for Visual Transformers}

The advent of Vision Transformers (ViTs) marks a significant paradigm shift in computer vision, primarily driven by the inherent limitations of Convolutional Neural Networks (CNNs) in capturing global contextual relationships and the groundbreaking success of Transformer architectures in Natural Language Processing (NLP). For an extended period, CNNs were the undisputed standard for visual recognition tasks, owing to their powerful inductive biases such as locality and translation equivariance. These biases, intrinsically embedded through spatially restricted convolutional kernels, are exceptionally effective at extracting local features and constructing hierarchical representations \cite{han2020yk0}. However, this very strength became a critical constraint when models needed to comprehend long-range dependencies and holistic contextual information across an entire image. CNNs, by design, inherently struggle to model interactions between spatially distant parts of an image without resorting to increasingly deep architectures, significantly larger receptive fields achieved through techniques like dilated convolutions, or complex add-on modules such as non-local blocks \cite{Wang2018NonlocalNN, zhou2021rtn}. While these methods attempted to mitigate the issue by expanding the effective receptive field, they often introduced increased computational complexity or did not fundamentally alter the local processing paradigm. As highlighted by \cite{gheflati202131i}, CNNs' restricted local receptive fields inherently limit their capacity for global context learning, making it challenging to capture comprehensive image understanding. Similarly, \cite{liu2022249} and \cite{karagz2024ukp} explicitly note that while CNNs excel at representing local spatial features, they find it difficult to capture global information, underscoring a fundamental gap in their representational power for certain tasks.

The landscape of deep learning was profoundly reshaped by the Transformer architecture, introduced by \cite{Vaswani2017} in NLP. This model demonstrated the unparalleled power of self-attention mechanisms to capture global dependencies within sequences, processing information from all input parts simultaneously. Unlike recurrent networks that process sequentially or CNNs with their localized focus, the Transformer's global attention mechanism, largely devoid of strong inductive biases about local connectivity, offered a compelling alternative for learning flexible, context-aware representations. The remarkable ability of Transformers to achieve state-of-the-art performance across diverse language tasks inspired researchers to question whether a similar global approach could unlock new capabilities in computer vision, overcoming the limitations of CNNs in capturing holistic image understanding and offering a less constrained path to feature learning \cite{han2020yk0}. This inspiration was rooted in the desire for models that could intrinsically understand the relationships between any two parts of an image, regardless of their spatial separation, without being constrained by fixed-size kernels.

This profound inspiration culminated in the seminal work by \cite{Dosovitskiy2021}, which introduced the Vision Transformer (ViT). This pioneering paper directly adapted the pure Transformer architecture to image recognition by treating image patches as a sequence of tokens, demonstrating that a model built entirely on self-attention, when trained on sufficiently large datasets, could achieve state-of-the-art performance without any convolutional layers. This was a profound challenge to the long-held paradigm of convolutional feature extraction, showcasing that global attention could indeed learn powerful visual representations and capture long-range dependencies more effectively than CNNs. The ability of ViTs to learn highly transferable representations, often outperforming CNNs on various downstream tasks when properly pre-trained, further solidified its revolutionary potential \cite{zhou2021rtn}. For instance, studies like \cite{htten2022lui} have shown that ViT models can achieve equivalent or even superior performance to CNNs in complex industrial visual inspection tasks, even with sparse data, highlighting their robust feature learning capabilities. This successful adaptation of a pure Transformer to vision tasks marked a definitive turning point, setting the stage for a new architectural lineage in computer vision that prioritizes global context and flexible feature learning.