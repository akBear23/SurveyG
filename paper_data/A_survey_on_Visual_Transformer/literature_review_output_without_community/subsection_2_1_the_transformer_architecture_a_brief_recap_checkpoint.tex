\subsection{The Transformer Architecture: A Brief Recap}

The Transformer architecture, introduced by \cite{vaswani2017attention}, fundamentally revolutionized sequence modeling in Natural Language Processing (NLP) by moving away from recurrent and convolutional networks. This paradigm shift enabled unprecedented parallel processing capabilities and a more effective capture of long-range dependencies, laying the groundwork for its subsequent adaptation to diverse data modalities, including visual data.

At its core, the Transformer adopts an encoder-decoder structure. The encoder processes the input sequence, generating a rich, context-aware representation, which the decoder then utilizes to generate the output sequence, often in an auto-regressive manner. This entire architecture is built exclusively on attention mechanisms, eliminating the need for sequential processing inherent in Recurrent Neural Networks (RNNs) or the local receptive fields characteristic of Convolutional Neural Networks (CNNs).

The central innovation is the \textbf{self-attention mechanism}, which allows the model to weigh the importance of different parts of the input sequence when processing each element. For every token in the sequence, three distinct vectors are computed: a Query (Q), a Key (K), and a Value (V). The attention scores are calculated by taking the dot product of the Query with all Keys, scaling by the square root of the key dimension to prevent vanishing gradients, and applying a softmax function. These scores then weight the corresponding Value vectors, which are summed to produce the output for that token. This mechanism inherently allows each token to attend to any other token in the sequence, regardless of their distance, thereby effectively capturing long-range dependencies that were challenging for traditional recurrent networks.

To enhance the model's ability to capture diverse relationships and focus on different aspects of the input simultaneously, the Transformer employs \textbf{multi-head attention}. Instead of performing a single attention function, the Q, K, and V vectors are linearly projected multiple times into different lower-dimensional subspaces. Each of these "heads" then independently computes its own scaled dot-product attention. The outputs from all attention heads are concatenated and then linearly transformed back into the desired output dimension. This parallel processing of multiple attention mechanisms allows the model to learn various types of relationships and attend to different positions, enriching the overall representation and improving the model's capacity to model complex data.

A critical aspect of sequence modeling is preserving the order of elements, which self-attention inherently lacks due to its permutation-invariant nature. To address this, the Transformer injects sequence order information through \textbf{positional encoding}. These are vectors added directly to the input embeddings at the bottom of the encoder and decoder stacks. The original Transformer used fixed sinusoidal functions of different frequencies, allowing the model to easily learn relative positions. This mechanism ensures that while the attention mechanism processes tokens in parallel, the model retains crucial information about their relative and absolute positions within the sequence.

Each encoder and decoder block also contains a position-wise fully connected feed-forward network, applied independently and identically to each position. This network typically consists of two linear transformations with a ReLU activation in between. Additionally, layer normalization is applied before each sub-layer (self-attention and feed-forward network), followed by a residual connection, aiding in stable training and gradient flow through deep networks.

The design of the Transformer, particularly its reliance on self-attention, marked a significant departure from previous state-of-the-art models like Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). Unlike RNNs, which process sequences token by token, leading to computational bottlenecks and difficulties with long-range dependencies due to vanishing/exploding gradients, the Transformer processes all tokens in parallel. This parallelization drastically reduces training time. Furthermore, while CNNs capture local features through fixed-size kernels, the Transformer's self-attention mechanism provides a global receptive field from the very first layer, allowing it to directly model relationships between any two tokens irrespective of their distance. This global perspective and parallel processing capability were revolutionary, establishing a new paradigm for sequence modeling that would profoundly influence subsequent research across various domains.