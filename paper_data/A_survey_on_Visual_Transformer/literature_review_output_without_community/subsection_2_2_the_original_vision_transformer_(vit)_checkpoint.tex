\subsection*{The Original Vision Transformer (ViT)}

The remarkable success of the Transformer architecture \cite{vaswani2017attention} in natural language processing (NLP), primarily driven by its potent self-attention mechanism, ignited a fundamental inquiry within the computer vision community: could a similar, convolution-free paradigm effectively process and interpret visual data? This pivotal question was comprehensively addressed by Dosovitskiy et al. \cite{dosovitskiy2021image} with the introduction of the Vision Transformer (ViT). Their pioneering work directly adapted the Transformer for image recognition, unequivocally demonstrating its viability as a powerful and distinct alternative to traditional Convolutional Neural Networks (CNNs) and marking a significant paradigm shift in visual modeling \cite{han2020yk0, huo2023e5h}.

The core innovation of the original ViT lies in its elegant yet straightforward approach to re-conceptualizing an image as a sequence of discrete tokens, directly analogous to how words are treated in NLP. The process commences by partitioning an input image into a grid of fixed-size, non-overlapping square patches, typically $16 \times 16$ pixels. Each of these patches is then flattened into a one-dimensional vector and subsequently projected linearly into a fixed-dimension embedding space. This transformation yields a sequence of "visual tokens," where each token represents a localized region of the original image. Crucially, to compensate for the spatial information lost during the flattening and tokenization process, learnable positional embeddings are added to these patch embeddings. These embeddings are vital for retaining the relative spatial arrangement of patches, allowing the model to understand the geometric relationships between different parts of the image.

To facilitate image classification, a special, learnable "class token" is prepended to this sequence of embedded patches and their corresponding positional embeddings. This class token acts as a global representation of the entire image, accumulating information from all other visual tokens as it propagates through the Transformer layers. The augmented sequence then enters a standard Transformer encoder, which consists of multiple identical layers, each comprising a multi-head self-attention (MSA) block and a feed-forward network (FFN). The global self-attention mechanism within the MSA block is the cornerstone of ViT's ability to capture long-range dependencies; it enables each visual token (patch or class token) to attend to every other token in the sequence, thereby integrating global contextual information across the entire image. This stands in stark contrast to CNNs, which primarily rely on local receptive fields and hierarchical processing to build global context. The final state of the class token, after traversing the Transformer encoder, is then fed into a simple Multi-Layer Perceptron (MLP) head, which outputs the predicted class probabilities for the image.

This architectural departure from CNNs demonstrated remarkable empirical performance. Dosovitskiy et al. \cite{dosovitskiy2021image} showcased that ViT achieved state-of-the-art results on large-scale image classification benchmarks, including ImageNet and JFT-300M. A critical finding was that ViTs, when pre-trained on sufficiently vast datasets (e.g., JFT-300M with 300 million images), could significantly outperform CNNs of comparable size. This performance advantage was attributed to the Transformer's ability to learn more flexible and global representations, unconstrained by the strong inductive biases (like locality and translation equivariance) inherent in CNNs. While these inductive biases make CNNs highly data-efficient on smaller datasets, ViT's "data hunger" meant it required extensive pre-training to fully exploit its capacity and generalize effectively \cite{huo2023e5h}. The breakthrough performance of ViT not only challenged the long-standing dominance of CNNs in computer vision but also fundamentally reoriented research, establishing a new and highly influential direction for visual modeling based on global attention mechanisms \cite{han2020yk0}. However, this paradigm shift, while promising, also introduced its own set of initial challenges, particularly concerning its substantial data requirements and the computational implications of its global self-attention mechanism, which necessitated further architectural refinements and training innovations.