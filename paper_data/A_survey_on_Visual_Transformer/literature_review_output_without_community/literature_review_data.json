{
  "title": "A Comprehensive Literature Review with Self-Reflection",
  "papers_processed": 367,
  "paper_list": [
    "c8b25fab5608c3e033d34b4483ec47e68ba109b7.pdf",
    "7a9a708ca61c14886aa0dcd6d13dac7879713f5f.pdf",
    "d40c77c010c8dbef6142903a02f2a73a85012d5d.pdf",
    "0eff37167876356da2163b2e396df2719adf7de9.pdf",
    "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e.pdf",
    "a09cbcaac305884f043810afc4fa4053099b5970.pdf",
    "2fe2f849b94cf08b559226bc9d78adcaef5ef186.pdf",
    "e5cb26148791b57bfd36aa26ce2401e231d01b57.pdf",
    "96da196d6f8c947db03d13759f030642f8234abf.pdf",
    "751b71158b7dcd2a7949e72a6ad8fb13657a401c.pdf",
    "164e41a60120917d13fb69e183ee3c996b6c9414.pdf",
    "5553f9508dd1056ecc20c5b1f367e9a07e2c7e81.pdf",
    "226fcbe55235d873bedb2fcf5b981bd5ec860d4f.pdf",
    "5c1b7e400ba8b3c3b04522f7bb3af17ad68d0585.pdf",
    "a4b728dbbf5afdc231afb95ad4e5c2ececdefc48.pdf",
    "0b8e8760d7dd64c7439019aeb3b6ac55dd5075d4.pdf",
    "17534840dc6016229a577a66f108a1564b8a0131.pdf",
    "b8cee43a51c44f8f4448e78e41ecf081987707cf.pdf",
    "44ee4165b8a3811dc7d557f99150ff9e62f3733f.pdf",
    "50a260631a28bfed18eccf8ebfc75ff34917518f.pdf",
    "3c14992a490cc31a7a38f5fab156c9da40a861d4.pdf",
    "ab70c5e1a338cb470ec39c22a4f10e0f19e61edd.pdf",
    "1ee05cd919590eaba129caa0fda5e850c87b75a5.pdf",
    "9fb327c55a30b9771a364f45f33f77778756a164.pdf",
    "dfdb2894d50e095ce97f994ed6cee38554c4c84f.pdf",
    "f27040f1f81144b17ec4c2b30610960e96353002.pdf",
    "4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a.pdf",
    "49030ae220c863e9b72ab380ecc749c9d0f0ad13.pdf",
    "60b0f9af990349546f284dea666fbf52ebfa7004.pdf",
    "64d8af9153d68e9b50f616d227663385bece93b9.pdf",
    "03384825d373aabe67c4288ef1eae4d1cf89dc00.pdf",
    "d28fed119d9293af31776205150b3c34f3adc82b.pdf",
    "b52844a746dafd8a5051cef49abbbda64a312605.pdf",
    "35fccd11326e799ebf724f4150acef12a6538953.pdf",
    "0d1e4faa1580266a56ad62ac2fdd72ed0c0bbbe9.pdf",
    "8f7ae0526f9c8706a9e0967eb7942d9b3ffb2788.pdf",
    "00f6041bdbdd00299ef317bd86db3660eb039a8c.pdf",
    "5f7fee3db8879d9d4e714ae20627f00fcbdfab38.pdf",
    "070c917ab1a4d6b924a9613ca18443f260d8d5be.pdf",
    "011f59c91bbee6de780d35ebe50fff62087e5b13.pdf",
    "f27c847e2909f30745f4a3528b574f5acfd76ea7.pdf",
    "d717fa5a4355b5033e660ad58a7ae7f4ccce6939.pdf",
    "a119cc83788701313d94746baecd2df5dd30199d.pdf",
    "60f9fd4e6b29e6f11324ba4f6d3349a2d8107a1b.pdf",
    "5ca02297d8d49f03f26148b74fea77272d09c78b.pdf",
    "aed7e4bc195d838735c320ac40a78f123206831b.pdf",
    "b66e4257aa8856df537f03f6a12341f489eb6500.pdf",
    "f9480350e1986957919d49f346ba20dcab8f5b71.pdf",
    "836dd64a4f606931029c5d68e74d81ef5885b622.pdf",
    "16fa1a8575ff56781b6b83726906754ed4e5f3a7.pdf",
    "9f7f81b1c82828a45a52df8f0c6a92636af76c7e.pdf",
    "e678898301a66faab85dfa4c84e51118e434b8f2.pdf",
    "e33434a141bb2881a2e60c518a0cda4feed3f19a.pdf",
    "9da3fadf092c864f61d6fd1e8eab5a6ca2397194.pdf",
    "9fab78222c7111702a5702ce5fae0f920722c316.pdf",
    "c051ee2ad7ac203a26fa8f50eb6312424c729b27.pdf",
    "13f7a106bb3814ad1fab25fd1356e99e91f402d3.pdf",
    "d04fbbec070b7e7af8152f9b7574971d22bf5997.pdf",
    "e939b55a6f78bffeb00065aed897950c49d21182.pdf",
    "6dc8693674a105c6daca5200141c50362e3044fc.pdf",
    "494e200d8a2ef49d21ce3458a25a109ea917ebe4.pdf",
    "39240f94c9915d9f9959c34b1dc68593894531e6.pdf",
    "8db96d4eaefdac8dff6d98d94a5e3f5b5558c63d.pdf",
    "428d755f0c8397ee6d04c89787f3455d323d8280.pdf",
    "ff00791b780b10336cc02ee366446d16e1c5e17b.pdf",
    "957a3d34303b424fe90a279cf5361253c93ac265.pdf",
    "401c8d72a9b275e88e6ba159d8d646cfb9f397aa.pdf",
    "7dee2bc2be709c0009b7623b7af78246f32e0a60.pdf",
    "1b026103e33b4c9eb637bc6f34715e22636b3492.pdf",
    "024c595ba03087399e68e51f87adb4eaf5379701.pdf",
    "9500dbde37e0f8c6b9ea924aacc6591bc3414166.pdf",
    "977351c92f156db27592e88b14dee2c22d4b312a.pdf",
    "ba12a9915553b3b42df17a33afcfd547821d8cc3.pdf",
    "3efcd3a4c54694a093886981d59e3cffe0dd7149.pdf",
    "58fc305734a0d5d849ae69b9233af082d712197e.pdf",
    "54911915a13cf0138c06b696e6c604b12acfe228.pdf",
    "b8585577d05cebd85d45b7c63f7011851412e794.pdf",
    "956d45f7a8916ec921df522c0641fd4f02beccb7.pdf",
    "99fbe810d4194684be03458fdfebacb12d8a5c4e.pdf",
    "f4e32b928d7cc27447e312bdc052aa75888045aa.pdf",
    "98e702ef2f64ab2643df9e80b1bd034334142e62.pdf",
    "0ef3e7efa5ded21a398d8ec90ff766b467c4fc54.pdf",
    "a21fa5ff42db4b0bd0fefae3d710cad5f0175400.pdf",
    "174919e5a4ef95ff66440d56614ad954c6f27df1.pdf",
    "6971aee925639a8bd5b79c821570728ef49060c6.pdf",
    "15ad149fc1ba4d8b5de189fdfacac9daecc286d0.pdf",
    "f66181828b7621892d02480fa1944b5f381be80d.pdf",
    "cee8934975dfbe89747af60bbafc95e10a788dc2.pdf",
    "69e20583872b4f1384ae3f7dbdbec648c19d3d35.pdf",
    "0594eaa8dfe580678a2382aaf77ac3582c872a97.pdf",
    "3a0145f34bcd35f09db23b2edec3ed097894444c.pdf",
    "ce79649b126dfe9e3cbeb1ecd64a80708bbd5538.pdf",
    "572ed945b06818472105bd17cfeb355d4e46c5e5.pdf",
    "934942934a6a785e2a80daa6421fa79971558b89.pdf",
    "3ae8c0b646ddce95ffd09da31c02ed6fdb744e90.pdf",
    "bc8d9f11ad91d54e83ad7cc3900502a913499fcb.pdf",
    "cf6d947d5d2ee72873a5a7b97dde9f881f79a6b6.pdf",
    "9db52b92b48b8855f76d7e4b51b7727a2b3c0271.pdf",
    "e4add4391dfa2a806a50cc1fbe9a9696dac9501f.pdf",
    "324f97d033efd97855488cf0b15511799fe7b7f7.pdf",
    "bf52f09d648d78677f140e63c7c83ca44fcd438f.pdf",
    "4eba4dccc7d19c40dfd02eef447a57613c3644e2.pdf",
    "0bdcea2f18e158bbd1723ed27d48e5211545e050.pdf",
    "67b7ad5f3c818f42c8359abc87f353202f76f6f2.pdf",
    "3994996a202f0127a58f57b259324a5283a1ba27.pdf",
    "4767600089dd71c7ed74c6a0acaf9e5a6fb345c9.pdf",
    "d5b8af7ae526e3a511b1172c26d7caf7421145bb.pdf",
    "4d491b6fbe529a3986ef50cc34ede7c9ad88126c.pdf",
    "d9e4af7739d4d5dcd44619bf276dc497f7334e34.pdf",
    "d69102eec0fff1084e3d1e24a411103280020a32.pdf",
    "38bef6f0b4de2f9809b4fd44e8b61f9bebb14db9.pdf",
    "b80d0ce78652ffbf6b3ddebfc567f8594a6ffd4b.pdf",
    "7d4d512445903e8ad75f0dcfa8baec27b1fe8620.pdf",
    "7d5274f1155b85a6120491c9374b6983dac96552.pdf",
    "0afbaae7d0bfe98a04a3b7dd1ba69e25f32b24c6.pdf",
    "6bcdf04ccd35ae971e765cdda25127ce005b7713.pdf",
    "6fd1002f321240c75dfc86c6bad6c05068a8160a.pdf",
    "c4560ab3855cb8ae4b9922458a0b6b94108e2c28.pdf",
    "a0762777d6e46acbcd5b301285c2d9894065ff8f.pdf",
    "d43950779dc86b728d7e002be6195526d35a26b0.pdf",
    "2e69d97ae22c6a3685cc548f8c19c696d5d7d363.pdf",
    "c25091718b22384cebece2da7f30fc1702a07c76.pdf",
    "cec0cbc2dd6d7975714110632b6bfcb5c1927ec3.pdf",
    "ef93c81f90154a022e164be2f83c6cf6e602f33d.pdf",
    "6a8d4a97ba3eafcb85d2d2a57d7d514bb592dbd0.pdf",
    "64143b37ae41085c4907e344ff3a2362a3051d0c.pdf",
    "dcd8617200724f0aa998276be339ff4af589ee42.pdf",
    "46880aeca86695ca3117cc04f6bd9edaf088111b.pdf",
    "7e0dd543471b66374fbf1639b9894d3d502533b6.pdf",
    "845a154dbcde81de52b68d73c78fad5be4af3b20.pdf",
    "6f4093a7ad5378e8cd3b73a52fbec80b784c107d.pdf",
    "50405c1ee26c97ed5b9a54c7179317a424e6e471.pdf",
    "1bf9a97a7581d1671d683c19ce36fed6511dc95b.pdf",
    "e58c2bc5333ee8f37150d9d15c0428d9658b2e50.pdf",
    "761f55a486e99ab5d3550aee48df34b6b65643c2.pdf",
    "2dcc1f5f1ba11f99e09f22c53d02aa66cec35c48.pdf",
    "52a7f15085f1b6815a4de2da26df51bb63470596.pdf",
    "649b706ba282de4eb5a161137f80eb49ed84a0a8.pdf",
    "186295f7c79e46c0e4e5f40e094267c09714043d.pdf",
    "d77288fc7de7b15c200ed75118de702caf841ec3.pdf",
    "2e4dbc3dbd400346be60318ae558a0293e65ba81.pdf",
    "3de95f33c2b4f61a9c0f335b4810a966e209a47a.pdf",
    "7817ecb816da8676ae21b401d60c99e706446f06.pdf",
    "3502b661362b278eebacf1037fc3bb4e21963869.pdf",
    "791d1e306eaa2e87657925ec4f45661baa8da58b.pdf",
    "1d8ecf2c3a78e5dc4de9e4c41961771bbd0033b3.pdf",
    "e1f7478294fe01ce271cdef9ba93f4c675d92dc9.pdf",
    "a56f8e42e9efe5290602116b42a247b758052fe4.pdf",
    "6eb10790689a4cf239d1ee2a3919dced4be6db8f.pdf",
    "fc2e7fcdc1bd773f1eb097ae67c8f736108276e3.pdf",
    "16ad38c73f4897e1c58326e3562b74ae1c2fd219.pdf",
    "371e924dd270a213ee6e8d4104a38875105668df.pdf",
    "0025c4241ffb2cce589dc2dcd82385ff06455542.pdf",
    "f2b3ece7cb0c88701ae0055236f6d6da39c8156d.pdf",
    "1eb4bbaa204890a45da2ca713f34fcc2d763c6e1.pdf",
    "1b18750ea0d26575f3e2c18f61e444c2790dbe4c.pdf",
    "08502153c9255399f8ff155e5f75900f121bd2ff.pdf",
    "cb85a7648d2a4b4953abfc7437a33f91dc2f3673.pdf",
    "90f9b2892a437ca32c4ab26c09b8dbb7de8fa034.pdf",
    "7ead0cae4e67f390b2eb0083117ea8ab90c53b47.pdf",
    "e4864ea54710b4c8f3b2ca9ac39854bdfd0dae28.pdf",
    "fe6887ee065c4f64a0c8d6054d1fc562766452de.pdf",
    "280ea33e67484c442757fe761b75d871a399905d.pdf",
    "29f86d6d1eaba6a466c231f6906b18eae4b2b484.pdf",
    "d8bf4b494d255c6d9b81e9963f29e40831106e3e.pdf",
    "abf037290e859a241a5af2c5adf9c08767971683.pdf",
    "dd46070ce18f55a5714e53a096c8219d6934d188.pdf",
    "829926d9943c99dd64eea5b4b96541eca1e5e91d.pdf",
    "e8dceb26166721014b8ecbd11fd212739c18d315.pdf",
    "e06b703146c46a6455fd0c33077de1bea5fdd877.pdf",
    "3af375031a3e23b7daf2f1ed14b5b61147996ca0.pdf",
    "d203076c28587895aa344d088b2788dbab5e82a1.pdf",
    "f3d0278649454f80ba52c966a979499ee33e26c2.pdf",
    "918617dbc02fa4df1999599bcf967acd2ea84d71.pdf",
    "51f38bd957fa863022feb5878fa1ba3bea6657cf.pdf",
    "1f389f54324790bfad6fc40ac4e56428757ea92b.pdf",
    "05236fa766fc1a38a9eb895e77075fb65be8c258.pdf",
    "0eec6c36da426f78b7091ba7ae8602e129742d30.pdf",
    "689bc24f71f8f22784534c764d59baa93a62c2e0.pdf",
    "afb7ff7096cd45de02de96b5937c3416ebca0cb0.pdf",
    "595adb75ddeb90760c79e89b76d99e55079e0708.pdf",
    "de20c6805b83a2f83ed75784920e91b913d888bb.pdf",
    "c57467e652f3f9131b3e7e40c23059abe395f01d.pdf",
    "53e5db85e2a7442f20670be2ae25019fcf9d27a2.pdf",
    "0682771fd5f611bce2a536bf83587532469a83df.pdf",
    "a4ef2c7c321dea04d8ea91ec1ce0710ebce4f848.pdf",
    "243a056d1acb153f70e39cc80a10e7d211a4312f.pdf",
    "d8ab87176444f8b0747972310431c647a87de2df.pdf",
    "a1e17a09b0df676f133c2175366ab7fcd0381ac1.pdf",
    "77b625e6dec2732b96ca8cf57d7a123497873e1d.pdf",
    "e91934d66d9133d854ff0a4cafbe7966584bbf97.pdf",
    "e37539f5c943a92ef56b49b7fa067bd976e418d4.pdf",
    "8fc9a7f23c5cf57e17790ffb99b6acb11693c673.pdf",
    "7ddbb9723420cb474c58d4b6b6834d5ebe112049.pdf",
    "c7c0c84fa505995cd888a8fbaa9def21cb84cca3.pdf",
    "1af8c50b4c7329fb2273aa7b755b0f9e8223e2d2.pdf",
    "bca0adcba0c26c5a34b05795c5bfbc1948bf2fae.pdf",
    "c4895869637f73154d608cdd817234b0dbcd3508.pdf",
    "64811427a4427588bb049a6a254446ddd2cafacc.pdf",
    "7a0f92968c63613730b0c06b3bc3bd40d2666571.pdf",
    "136b0ef7bd7f642a285b1d61ddf7abfc7ddc026f.pdf",
    "cf439db0e071f19305ea1755aa108acdde73ed99.pdf",
    "ffc855594cad345ea5a1cce2ee27095bec767bc8.pdf",
    "838d7862215df504dde41496cbe6ee711a12ae9f.pdf",
    "9a35a0aac04f4f712f38ea9bbfd2f0b0108242f5.pdf",
    "f6bf7787115affe22c410eb5b2606269912d59a0.pdf",
    "69e56df0ae079c83607bb48e68f8db39a4177cd0.pdf",
    "1528bab641830ea279bdf53ff9e7d11d2b7e5028.pdf",
    "b43bb480caad36ab6fd667570275d42fe9050175.pdf",
    "1970ace992d742bdf098de08a82817b05ef87477.pdf",
    "fec86abbb29b559c1eaff31428f5b59f8070bb67.pdf",
    "cb8b0eba078098000f004d7e0f97a33189261f30.pdf",
    "9b4d81736637392adabe688b6a698cec58f9ce57.pdf",
    "981970d0f586761e7cdd978670c6a8f46990f514.pdf",
    "6bdafb965e94c5240db2c30f20c37c4b4dd0e451.pdf",
    "a3d1cebf99262cc20d22863b9540769b49a15ede.pdf",
    "f05176e2d3f7a6e95b60f0e96fd9bf20e8b335d2.pdf",
    "442b5ec3aad4b099e71d6203a62eb7ec7519544c.pdf",
    "635675452852e838644516e1eeefd1aaa8c8ac07.pdf",
    "d2fce7480111d66a74caa801a236f71ab021c42c.pdf",
    "5135a8f690c66c3b64928227443c4f9378bd20e1.pdf",
    "77eea367f79e69995948699d806683c7731a60b1.pdf",
    "861f670073679ba05990f3bc6d119b13ab62aca7.pdf",
    "f93f718bb2d3f5e4354e174e54e71c5596f56ea3.pdf",
    "c5c9005aae80795e241de18b595c2d01393808f8.pdf",
    "14c42c0f2c94e0a1f4aa820886080263f9922047.pdf",
    "9998291d71f4e8ddf59f4b016b19df1f848eeed1.pdf",
    "9fa44f616e796f187d97b9cc324bd839bc21ad81.pdf",
    "d62e6ba83d3da5b75e1d428f20f8199e3754bd5d.pdf",
    "d1faaa1d7d312dd5867683ce60519979de6b3349.pdf",
    "d68aedc3a957c25e5f19438ec4b64fb53b48422c.pdf",
    "bb5e6bbd4ad8c613a0bac3db23940c03ce94bec9.pdf",
    "55156532cb9c20fdcaed9ead238f7a2cbaab2527.pdf",
    "ae7c5ae7d377189c5a5f33a60b33c6cc5ca56124.pdf",
    "3b427c8d3258968b9ac5eaf846d99ac027de9a76.pdf",
    "10e9943b3a974ac6175ffec3228e670ec9d2cc18.pdf",
    "1783c85f8b951dc11c9dcb4929bfcae2d52ecb63.pdf",
    "42bd7d3ca2b66eca5609fe77078fe8951ab9fd5c.pdf",
    "7aeadf7b9d839bdd9dbfbff5239f71792ff0c04c.pdf",
    "b2becca9911c155bf97656df8e5079ca76767ab9.pdf",
    "25a69bdd7f87ecc287b4e9a64eb4d6d562371add.pdf",
    "50e997b23a534a6fbfd32d63990fa80373ec7c6b.pdf",
    "3ea79430455304c782572dfb6ca3e5230b0351de.pdf",
    "0284fa05879f1609bf459f15a4d40e22355cd5ae.pdf",
    "714e21409b8c4f7788ac8c93795249a4e45e51ce.pdf",
    "2ef224f7a1f545eb8bfdf61c7b849f2676fe6a61.pdf",
    "bc99c855d52ba3d432c428fb4096b3a22c04f8bf.pdf",
    "3dee43cea71d5988a72a914121f3455106f89cc7.pdf",
    "1871c0bc27bd5cd5f2f6c3d6cedeb4b8c5a89499.pdf",
    "c064efa0419b75ba131ec8470ed80f01e1a80f64.pdf",
    "b48a85980deb5f1baa64d862b9f0e4e62124e4de.pdf",
    "8ec10ffe0c1fc8f6a92d541f0e002e78080b564a.pdf",
    "769ba82ab77fc46db4a594dc64576286d02cf1d7.pdf",
    "d1255f6e7e7e14b253fe2245e842d0b45f5e715e.pdf",
    "5572237909914e23758115be6b8d7f99a8bd51dc.pdf",
    "21c37fd6d00d6c727ec10f0c871c1ddf6e4f7ed9.pdf",
    "e38e70580acb204c05096de8da90b7ab1d4bdb6b.pdf",
    "a246677a3d68e8cd7a63d24639a5cf3fc3b9f56e.pdf",
    "1397d1bcfeae68078756fdbc3ed7e62ef51a1e32.pdf",
    "52239e2338cd9f0e0e690203af8f0f1260ef7abf.pdf",
    "f996d5ee3b8ad3c60510862a92fd72c6a41777e0.pdf",
    "f1b6f564e7cba8121df0eef0d656673e3bd18621.pdf",
    "12106888ec74baf48d1e9fb64e8932c9ce509e84.pdf",
    "3798e7f16fe69c29307a9bab4f0f4d779478afc5.pdf",
    "29a0077d198418bab2ea4d78d04a892ede860d68.pdf",
    "ac9cc0c28838a037e77f4e19433de170f47b3de9.pdf",
    "c4357abf10ff937e4ad62df4289fbbf74f114725.pdf",
    "0b41c18d0397e14ddacee4143db74a05d774434d.pdf",
    "f9e1a8754e77a4b5f240d11e9e81e2563a319b89.pdf",
    "4b9568a9798c527779a1f7479ffb3ba373640e3e.pdf",
    "34ca004eda791c8c4191e0ebe65cc4405a116b08.pdf",
    "409b43b8cd8a2ba69f93e80c2bacc0126238b550.pdf",
    "dc5d707a8f9efaf32d2319ec2ca8d2619ef2926e.pdf",
    "9017053fb240d0870779b9658082488b392e7cde.pdf",
    "f27221a15f4c3c8af7ea0ada5bab25831ffe21ea.pdf",
    "2d421d94bc9eed935870088a6f3218244e36dc97.pdf",
    "be1aabb6460d49905575da88d564864da9f80417.pdf",
    "0d7d27fbd8193acf8db032441fd22945d26e9952.pdf",
    "0af7ccb9f6cf2edb35674dc97d0ec5d165f8433b.pdf",
    "f9efe4918b9a20c2e8084d7a5ec624b87ec1f211.pdf",
    "f62cb393ab51505bdd22823e74b7cf28f4801e1c.pdf",
    "4702a22a3c2da1284a88d5e608d38cd106d66736.pdf",
    "9fcea59a7076064f5ac3949177307c1637473ffd.pdf",
    "1ec9b653475287e95fdaef2f5247f82a8376c56c.pdf",
    "2b48f5f0769bf41b5a3025ace73417d19de65cf1.pdf",
    "bfb310434204d99fa5c420ac272966c5ae1c1bd2.pdf",
    "bbe5dfbecfd1bed7556b9c8269b0d363faa24973.pdf",
    "be28edb064e2050e0852ff376434ee39f7c4b0a3.pdf",
    "e25a0b06079966b8e43f8e1f2455913266cb7426.pdf",
    "ecd9598308161557d6ac35b3e4d32770489e811d.pdf",
    "7dc4b2930870e66caa7ff23b5d447283a6171452.pdf",
    "b1f0ac0b5e4044251dcdcce78e0046feb43cc7c7.pdf",
    "903c8a82f5539b3e482efcb23ff59819f04f2c0f.pdf",
    "1e95bb5827dc784547a46058793c15effd74dccc.pdf",
    "2104eea2b01b7a4b6ba2d256769328ac4919afad.pdf",
    "8972ebf4f9ec47ee60c3ed0effd520daff495d5a.pdf",
    "0fde8e3287ac04dcf210b73898c8ceb697ad489f.pdf",
    "9a4718faa07a32cf1dce745062181d3342e9b054.pdf",
    "6e97c1ba023afc87c1b99881f631af8146230d96.pdf",
    "1d7183d481ae5a396743dde39984f1f0c8f47edf.pdf",
    "9fbf36da9ee60d56a7675042bb0b24798a3b966d.pdf",
    "d80166681f3344a1946b8bfc623f4679d979ee10.pdf",
    "9285996627124b945ec601a763f6ff884bac3281.pdf",
    "3f002ea3ede9d97f1fe29b16691491219c1e626d.pdf",
    "9121dcd10df00e5cc51dc94400e0325e0ae47bb9.pdf",
    "08606a6a8b447909e714be2c3160074fdf1b91ad.pdf",
    "0222269c963f0902cc9eae6768a3c5948531488b.pdf",
    "8776fd7934dc48df4663dadf30c6da665d84fb19.pdf",
    "cc24f933b343b6a9701088cf6ae1dbf3299c0c9e.pdf",
    "88589b0b2d2d8caa09d8ca94414343455ae87d7c.pdf",
    "d629289cc2f3efbe374a66d57690dd84f19d59ec.pdf",
    "70211e2b04fcb4413edfd43e75b9f10e8d130171.pdf",
    "cc817ba09f1c3c94bae73576463bcaf57c70261a.pdf",
    "77e4ea46e7248408844f62fa3613aa246e35fdf0.pdf",
    "271c6a87213b8b2a26ce3e1a6f20b07cc2f82b81.pdf",
    "05d15576d88f9384738908f98716f91bdb5dbc78.pdf",
    "6feea99a1a6c876d6f4f714b463da9c95998dee7.pdf",
    "c05744f690ab9db007012a63c3c5c3ca48201c66.pdf",
    "cebc29015a2827aa8a98ec39c90c93bca2f14848.pdf",
    "630c9993b6aaf812e1aa3dae8e243af131abd3d0.pdf",
    "d14a1677e416bd7ac6fbe01672cd3152fc1f983d.pdf",
    "8ed42c0d60eb4c91e99ee184a17719f7c3ce3f3f.pdf",
    "c4cf58e104fc8d783dfd0bcf3ccd65f8e4028d7f.pdf",
    "72e23cdc3accca1f09e2e19446bc475368c912d0.pdf",
    "7b6d64097d16219c043df64e4576bd7d87656073.pdf",
    "0820d2ac35cd55fd60f53c14460cca78fa996f1e.pdf",
    "b02144ef4ed94df78544959bc97eddef4580dd95.pdf",
    "b6488fded5fb0728d7c3bd04f3afa3fba68d0450.pdf",
    "24eba2f5345bc8279bb63f085e75cc26b37d0b1e.pdf",
    "8ce6872b26f88e52aeed57f6f7528dee30b5f7b2.pdf",
    "dcf77b8e23bbc56c5c8f76ddc5d3cf68d0a051be.pdf",
    "f792dcf878e5bf8e7fb5aa31d231474d43462f4a.pdf",
    "310f5543603bef94d42366878a14161db1bf45de.pdf",
    "f9f524944dd9d1fe4b8956f9a822927b2eec29ad.pdf",
    "6ec48cb5fa3d149c9cc8a29373e64b63fe257913.pdf",
    "3c7e07304bab6a860dbbe4ff36a4d87010036d2a.pdf",
    "819ae728828d50f56f234e35832b1222de081bfc.pdf",
    "6a9247fe471690218ef04cea5f6f4b59b3b50747.pdf",
    "f462bb00b8c4379c4a4699b66a19ce10da530b08.pdf",
    "52fafbbf20a37ffe863f1fa1d44fde6b4fc3334e.pdf",
    "b4be4f2ecb8711755f75dcb09a57d8c9722ac349.pdf",
    "7492734c76036143baf574d6602bd45a348c416f.pdf",
    "eacb2581b480cf4a80bc601c8ea657651b41c0fc.pdf",
    "da7d671f61e53601bdfc760d336cc98e5d9c516b.pdf",
    "2bb98b30fb9aae37858248cd5ac3221ebcf7eeba.pdf",
    "90e9d0a35bcf7e8251c4c9601a224bd95db14dd9.pdf",
    "5ce1ebe051a64969caeb879c44c45320a5ca8011.pdf",
    "3bba1b3376e43a39bf4f1bf4eab558758a37ea2e.pdf",
    "3b11c8774bbeddd6722d65455bf9bb729a63c1cb.pdf",
    "04ed3b0f4e21d9f4884bf4db98edf83a34d005a9.pdf",
    "23ce9c2814d6567efec884b7043977cefcb7602e.pdf",
    "5b22bdc6aedf13d812509dd0f768353eb1469a79.pdf",
    "d2fd0dc314023ebca1342f1379ba6d79c7ded84a.pdf",
    "1c96dc4ad4dfa3bf7c6ca9cf0c6197e7ac00272b.pdf",
    "c35c316feba84334a4b08feb5281a5f83db0b8b2.pdf",
    "c8174af99bc92d96935683beccc4161c65a8aa46.pdf",
    "05548c4b3db8be40fac236dfa8e21882fe9ce9de.pdf",
    "bd7a4a9e020b594ebcacb886ddae8731a2885209.pdf",
    "a7df70e86f049a86b1c555f9a399d3540f466be7.pdf",
    "e4b0c4a1d7ee87ad666214172f329831ade4c25d.pdf",
    "6604a900b9a7404a447b2167892a947012a9ffb8.pdf",
    "9814001811c4924171787de52e01cc31446e2f97.pdf",
    "ab10aacab1a2672a154034c589dd0aa801912272.pdf",
    "325367f93439652efaa4bc6b50115bbb7371704e.pdf",
    "3c6980902883f03c37332d34ead343e1229062b3.pdf",
    "f2b1b0fb57cccaac51b44477726d510570c4c799.pdf",
    "2456506ed87faa667a0c2b8af4028a5a86a49650.pdf"
  ],
  "citations_map": {
    "c8b25fab5608c3e033d34b4483ec47e68ba109b7.pdf": "liu2021ljs",
    "7a9a708ca61c14886aa0dcd6d13dac7879713f5f.pdf": "liang2021v6x",
    "d40c77c010c8dbef6142903a02f2a73a85012d5d.pdf": "han2020yk0",
    "0eff37167876356da2163b2e396df2719adf7de9.pdf": "chen2021r2y",
    "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e.pdf": "mehta20216ad",
    "a09cbcaac305884f043810afc4fa4053099b5970.pdf": "li2022raj",
    "2fe2f849b94cf08b559226bc9d78adcaef5ef186.pdf": "chen2022woa",
    "e5cb26148791b57bfd36aa26ce2401e231d01b57.pdf": "xia2022qga",
    "96da196d6f8c947db03d13759f030642f8234abf.pdf": "zhou202105h",
    "751b71158b7dcd2a7949e72a6ad8fb13657a401c.pdf": "liu2021jpu",
    "164e41a60120917d13fb69e183ee3c996b6c9414.pdf": "lee2021us0",
    "5553f9508dd1056ecc20c5b1f367e9a07e2c7e81.pdf": "zhang2021fje",
    "226fcbe55235d873bedb2fcf5b981bd5ec860d4f.pdf": "jiang2022zcn",
    "5c1b7e400ba8b3c3b04522f7bb3af17ad68d0585.pdf": "islam2022iss",
    "a4b728dbbf5afdc231afb95ad4e5c2ececdefc48.pdf": "li2022a4u",
    "0b8e8760d7dd64c7439019aeb3b6ac55dd5075d4.pdf": "yao202245i",
    "17534840dc6016229a577a66f108a1564b8a0131.pdf": "borhani2022w8x",
    "b8cee43a51c44f8f4448e78e41ecf081987707cf.pdf": "mao2021zr1",
    "44ee4165b8a3811dc7d557f99150ff9e62f3733f.pdf": "chen202174h",
    "50a260631a28bfed18eccf8ebfc75ff34917518f.pdf": "jie20220pc",
    "3c14992a490cc31a7a38f5fab156c9da40a861d4.pdf": "fan2022m88",
    "ab70c5e1a338cb470ec39c22a4f10e0f19e61edd.pdf": "lin20216a3",
    "1ee05cd919590eaba129caa0fda5e850c87b75a5.pdf": "lin2021utw",
    "9fb327c55a30b9771a364f45f33f77778756a164.pdf": "li2022mco",
    "dfdb2894d50e095ce97f994ed6cee38554c4c84f.pdf": "li2022tl7",
    "f27040f1f81144b17ec4c2b30610960e96353002.pdf": "yang2021myb",
    "4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a.pdf": "yu2022iy0",
    "49030ae220c863e9b72ab380ecc749c9d0f0ad13.pdf": "zhuang2022qn7",
    "60b0f9af990349546f284dea666fbf52ebfa7004.pdf": "deng2021man",
    "64d8af9153d68e9b50f616d227663385bece93b9.pdf": "wang2021oct",
    "03384825d373aabe67c4288ef1eae4d1cf89dc00.pdf": "wang2022ti0",
    "d28fed119d9293af31776205150b3c34f3adc82b.pdf": "li2022ow4",
    "b52844a746dafd8a5051cef49abbbda64a312605.pdf": "wang2022da0",
    "35fccd11326e799ebf724f4150acef12a6538953.pdf": "deng2022bil",
    "0d1e4faa1580266a56ad62ac2fdd72ed0c0bbbe9.pdf": "yang20228mm",
    "8f7ae0526f9c8706a9e0967eb7942d9b3ffb2788.pdf": "gheflati202131i",
    "00f6041bdbdd00299ef317bd86db3660eb039a8c.pdf": "tang2022e2i",
    "5f7fee3db8879d9d4e714ae20627f00fcbdfab38.pdf": "yu202236t",
    "070c917ab1a4d6b924a9613ca18443f260d8d5be.pdf": "li2021ra5",
    "011f59c91bbee6de780d35ebe50fff62087e5b13.pdf": "meng2022t3x",
    "f27c847e2909f30745f4a3528b574f5acfd76ea7.pdf": "li20223n5",
    "d717fa5a4355b5033e660ad58a7ae7f4ccce6939.pdf": "bazi2022tlu",
    "a119cc83788701313d94746baecd2df5dd30199d.pdf": "zheng2022gg5",
    "60f9fd4e6b29e6f11324ba4f6d3349a2d8107a1b.pdf": "gao2021uzl",
    "5ca02297d8d49f03f26148b74fea77272d09c78b.pdf": "zheng202218g",
    "aed7e4bc195d838735c320ac40a78f123206831b.pdf": "bi20225lu",
    "b66e4257aa8856df537f03f6a12341f489eb6500.pdf": "chen2022vac",
    "f9480350e1986957919d49f346ba20dcab8f5b71.pdf": "song2022603",
    "836dd64a4f606931029c5d68e74d81ef5885b622.pdf": "li2022rl9",
    "16fa1a8575ff56781b6b83726906754ed4e5f3a7.pdf": "wensel2022lva",
    "9f7f81b1c82828a45a52df8f0c6a92636af76c7e.pdf": "wang2021sav",
    "e678898301a66faab85dfa4c84e51118e434b8f2.pdf": "naseem2022c95",
    "e33434a141bb2881a2e60c518a0cda4feed3f19a.pdf": "wu20210gs",
    "9da3fadf092c864f61d6fd1e8eab5a6ca2397194.pdf": "lyu2022vd9",
    "9fab78222c7111702a5702ce5fae0f920722c316.pdf": "krishnan2021086",
    "c051ee2ad7ac203a26fa8f50eb6312424c729b27.pdf": "yang20210bg",
    "13f7a106bb3814ad1fab25fd1356e99e91f402d3.pdf": "li20229zn",
    "d04fbbec070b7e7af8152f9b7574971d22bf5997.pdf": "wang2022n7h",
    "e939b55a6f78bffeb00065aed897950c49d21182.pdf": "chen202199v",
    "6dc8693674a105c6daca5200141c50362e3044fc.pdf": "panboonyuen20218r7",
    "494e200d8a2ef49d21ce3458a25a109ea917ebe4.pdf": "liang2022xlx",
    "39240f94c9915d9f9959c34b1dc68593894531e6.pdf": "zhou2021rtn",
    "8db96d4eaefdac8dff6d98d94a5e3f5b5558c63d.pdf": "dubey2021ra5",
    "428d755f0c8397ee6d04c89787f3455d323d8280.pdf": "ayas2022md0",
    "ff00791b780b10336cc02ee366446d16e1c5e17b.pdf": "tian2022shu",
    "957a3d34303b424fe90a279cf5361253c93ac265.pdf": "liu2022249",
    "401c8d72a9b275e88e6ba159d8d646cfb9f397aa.pdf": "zhang2021mcp",
    "7dee2bc2be709c0009b7623b7af78246f32e0a60.pdf": "han2021vis",
    "1b026103e33b4c9eb637bc6f34715e22636b3492.pdf": "kim2022m6u",
    "024c595ba03087399e68e51f87adb4eaf5379701.pdf": "zhou2022nln",
    "9500dbde37e0f8c6b9ea924aacc6591bc3414166.pdf": "hu202242d",
    "977351c92f156db27592e88b14dee2c22d4b312a.pdf": "you2022bor",
    "ba12a9915553b3b42df17a33afcfd547821d8cc3.pdf": "ren2022ifo",
    "3efcd3a4c54694a093886981d59e3cffe0dd7149.pdf": "wang20215ra",
    "58fc305734a0d5d849ae69b9233af082d712197e.pdf": "xiao202229y",
    "54911915a13cf0138c06b696e6c604b12acfe228.pdf": "jamil20223a4",
    "b8585577d05cebd85d45b7c63f7011851412e794.pdf": "bai2022f1v",
    "956d45f7a8916ec921df522c0641fd4f02beccb7.pdf": "li2022th8",
    "99fbe810d4194684be03458fdfebacb12d8a5c4e.pdf": "almalik20223wr",
    "f4e32b928d7cc27447e312bdc052aa75888045aa.pdf": "sha2022ae0",
    "98e702ef2f64ab2643df9e80b1bd034334142e62.pdf": "zhang2022msa",
    "0ef3e7efa5ded21a398d8ec90ff766b467c4fc54.pdf": "htten2022lui",
    "a21fa5ff42db4b0bd0fefae3d710cad5f0175400.pdf": "hatamizadeh2022y9x",
    "174919e5a4ef95ff66440d56614ad954c6f27df1.pdf": "montazerin2022dgi",
    "6971aee925639a8bd5b79c821570728ef49060c6.pdf": "kojima2022k5c",
    "15ad149fc1ba4d8b5de189fdfacac9daecc286d0.pdf": "kang2022pv3",
    "f66181828b7621892d02480fa1944b5f381be80d.pdf": "tian2022qb5",
    "cee8934975dfbe89747af60bbafc95e10a788dc2.pdf": "peng2022snr",
    "69e20583872b4f1384ae3f7dbdbec648c19d3d35.pdf": "ho20228q6",
    "0594eaa8dfe580678a2382aaf77ac3582c872a97.pdf": "xia2022dnj",
    "3a0145f34bcd35f09db23b2edec3ed097894444c.pdf": "wang202232c",
    "ce79649b126dfe9e3cbeb1ecd64a80708bbd5538.pdf": "mogan202229d",
    "572ed945b06818472105bd17cfeb355d4e46c5e5.pdf": "yang20221ce",
    "934942934a6a785e2a80daa6421fa79971558b89.pdf": "li2022ip7",
    "3ae8c0b646ddce95ffd09da31c02ed6fdb744e90.pdf": "yu20220np",
    "bc8d9f11ad91d54e83ad7cc3900502a913499fcb.pdf": "li20229fn",
    "cf6d947d5d2ee72873a5a7b97dde9f881f79a6b6.pdf": "huang2022iwe",
    "9db52b92b48b8855f76d7e4b51b7727a2b3c0271.pdf": "qu2022be0",
    "e4add4391dfa2a806a50cc1fbe9a9696dac9501f.pdf": "zeng2022ce2",
    "324f97d033efd97855488cf0b15511799fe7b7f7.pdf": "lin20225ad",
    "bf52f09d648d78677f140e63c7c83ca44fcd438f.pdf": "reghunath2022z8g",
    "4eba4dccc7d19c40dfd02eef447a57613c3644e2.pdf": "kundu2022z97",
    "0bdcea2f18e158bbd1723ed27d48e5211545e050.pdf": "sun2022cti",
    "67b7ad5f3c818f42c8359abc87f353202f76f6f2.pdf": "li2022gef",
    "3994996a202f0127a58f57b259324a5283a1ba27.pdf": "guo20228rt",
    "4767600089dd71c7ed74c6a0acaf9e5a6fb345c9.pdf": "li202240n",
    "d5b8af7ae526e3a511b1172c26d7caf7421145bb.pdf": "jiang2022jlc",
    "4d491b6fbe529a3986ef50cc34ede7c9ad88126c.pdf": "lin2021oan",
    "d9e4af7739d4d5dcd44619bf276dc497f7334e34.pdf": "wang2022dl1",
    "d69102eec0fff1084e3d1e24a411103280020a32.pdf": "li2022wab",
    "38bef6f0b4de2f9809b4fd44e8b61f9bebb14db9.pdf": "park2022eln",
    "b80d0ce78652ffbf6b3ddebfc567f8594a6ffd4b.pdf": "shen2022d6i",
    "7d4d512445903e8ad75f0dcfa8baec27b1fe8620.pdf": "wang20224wo",
    "7d5274f1155b85a6120491c9374b6983dac96552.pdf": "tao2022gdr",
    "0afbaae7d0bfe98a04a3b7dd1ba69e25f32b24c6.pdf": "wu2021nmg",
    "6bcdf04ccd35ae971e765cdda25127ce005b7713.pdf": "liu2022c56",
    "6fd1002f321240c75dfc86c6bad6c05068a8160a.pdf": "wang2022pb8",
    "c4560ab3855cb8ae4b9922458a0b6b94108e2c28.pdf": "xiong2022ec2",
    "a0762777d6e46acbcd5b301285c2d9894065ff8f.pdf": "sun2022pom",
    "d43950779dc86b728d7e002be6195526d35a26b0.pdf": "qi2022yq9",
    "2e69d97ae22c6a3685cc548f8c19c696d5d7d363.pdf": "ma2022vf3",
    "c25091718b22384cebece2da7f30fc1702a07c76.pdf": "wang2022tok",
    "cec0cbc2dd6d7975714110632b6bfcb5c1927ec3.pdf": "wang2022pee",
    "ef93c81f90154a022e164be2f83c6cf6e602f33d.pdf": "jannat20228u6",
    "6a8d4a97ba3eafcb85d2d2a57d7d514bb592dbd0.pdf": "chen2022r27",
    "64143b37ae41085c4907e344ff3a2362a3051d0c.pdf": "wang2021p2r",
    "dcd8617200724f0aa998276be339ff4af589ee42.pdf": "sajid2021xb6",
    "46880aeca86695ca3117cc04f6bd9edaf088111b.pdf": "xing2022kqr",
    "7e0dd543471b66374fbf1639b9894d3d502533b6.pdf": "garaiman2022xwd",
    "845a154dbcde81de52b68d73c78fad5be4af3b20.pdf": "wang2022wyu",
    "6f4093a7ad5378e8cd3b73a52fbec80b784c107d.pdf": "hou2022ver",
    "50405c1ee26c97ed5b9a54c7179317a424e6e471.pdf": "agilandeeswari202273m",
    "1bf9a97a7581d1671d683c19ce36fed6511dc95b.pdf": "qin2022cfg",
    "e58c2bc5333ee8f37150d9d15c0428d9658b2e50.pdf": "wang2022ohd",
    "761f55a486e99ab5d3550aee48df34b6b65643c2.pdf": "yu2022o30",
    "2dcc1f5f1ba11f99e09f22c53d02aa66cec35c48.pdf": "boukabouya2022ffi",
    "52a7f15085f1b6815a4de2da26df51bb63470596.pdf": "wang2022d7p",
    "649b706ba282de4eb5a161137f80eb49ed84a0a8.pdf": "song20215tk",
    "186295f7c79e46c0e4e5f40e094267c09714043d.pdf": "xie2021th0",
    "d77288fc7de7b15c200ed75118de702caf841ec3.pdf": "sun2022bm5",
    "2e4dbc3dbd400346be60318ae558a0293e65ba81.pdf": "jing2022nkb",
    "3de95f33c2b4f61a9c0f335b4810a966e209a47a.pdf": "li2022spu",
    "7817ecb816da8676ae21b401d60c99e706446f06.pdf": "song2022y4v",
    "3502b661362b278eebacf1037fc3bb4e21963869.pdf": "shukla2022jxz",
    "791d1e306eaa2e87657925ec4f45661baa8da58b.pdf": "tran2022bvd",
    "1d8ecf2c3a78e5dc4de9e4c41961771bbd0033b3.pdf": "hong2022ks6",
    "e1f7478294fe01ce271cdef9ba93f4c675d92dc9.pdf": "panboonyuen2021b4h",
    "a56f8e42e9efe5290602116b42a247b758052fe4.pdf": "zhao2022wi7",
    "6eb10790689a4cf239d1ee2a3919dced4be6db8f.pdf": "wang2022h3u",
    "fc2e7fcdc1bd773f1eb097ae67c8f736108276e3.pdf": "liu2021yw0",
    "16ad38c73f4897e1c58326e3562b74ae1c2fd219.pdf": "gul202290q",
    "371e924dd270a213ee6e8d4104a38875105668df.pdf": "zhao2022koc",
    "0025c4241ffb2cce589dc2dcd82385ff06455542.pdf": "yang2022qwh",
    "f2b3ece7cb0c88701ae0055236f6d6da39c8156d.pdf": "alquraishi2022j3v",
    "1eb4bbaa204890a45da2ca713f34fcc2d763c6e1.pdf": "jin2021qdw",
    "1b18750ea0d26575f3e2c18f61e444c2790dbe4c.pdf": "lee2022rf1",
    "08502153c9255399f8ff155e5f75900f121bd2ff.pdf": "shi2022evc",
    "cb85a7648d2a4b4953abfc7437a33f91dc2f3673.pdf": "zhang20223g5",
    "90f9b2892a437ca32c4ab26c09b8dbb7de8fa034.pdf": "zim202282d",
    "7ead0cae4e67f390b2eb0083117ea8ab90c53b47.pdf": "bao202239k",
    "e4864ea54710b4c8f3b2ca9ac39854bdfd0dae28.pdf": "sun2022nny",
    "fe6887ee065c4f64a0c8d6054d1fc562766452de.pdf": "munyer2022pfs",
    "280ea33e67484c442757fe761b75d871a399905d.pdf": "fan2022wve",
    "29f86d6d1eaba6a466c231f6906b18eae4b2b484.pdf": "wang2022gq4",
    "d8bf4b494d255c6d9b81e9963f29e40831106e3e.pdf": "ali2022dux",
    "abf037290e859a241a5af2c5adf9c08767971683.pdf": "chougui2022mpo",
    "dd46070ce18f55a5714e53a096c8219d6934d188.pdf": "zhuang2021hqu",
    "829926d9943c99dd64eea5b4b96541eca1e5e91d.pdf": "chen2021d1q",
    "e8dceb26166721014b8ecbd11fd212739c18d315.pdf": "hatamizadeh2024xr6",
    "e06b703146c46a6455fd0c33077de1bea5fdd877.pdf": "ryali202339q",
    "3af375031a3e23b7daf2f1ed14b5b61147996ca0.pdf": "yao2023sax",
    "d203076c28587895aa344d088b2788dbab5e82a1.pdf": "li2023287",
    "f3d0278649454f80ba52c966a979499ee33e26c2.pdf": "zhao2024671",
    "918617dbc02fa4df1999599bcf967acd2ea84d71.pdf": "dehghani2023u7e",
    "51f38bd957fa863022feb5878fa1ba3bea6657cf.pdf": "duan2024q7h",
    "1f389f54324790bfad6fc40ac4e56428757ea92b.pdf": "barman2024q21",
    "05236fa766fc1a38a9eb895e77075fb65be8c258.pdf": "jamil20230ll",
    "0eec6c36da426f78b7091ba7ae8602e129742d30.pdf": "paal2024eg1",
    "689bc24f71f8f22784534c764d59baa93a62c2e0.pdf": "zhang2023k43",
    "afb7ff7096cd45de02de96b5937c3416ebca0cb0.pdf": "himel2024u0i",
    "595adb75ddeb90760c79e89b76d99e55079e0708.pdf": "xu20235cu",
    "de20c6805b83a2f83ed75784920e91b913d888bb.pdf": "chi202331y",
    "c57467e652f3f9131b3e7e40c23059abe395f01d.pdf": "patro202303d",
    "53e5db85e2a7442f20670be2ae25019fcf9d27a2.pdf": "pan2023hry",
    "0682771fd5f611bce2a536bf83587532469a83df.pdf": "wang2024mrk",
    "a4ef2c7c321dea04d8ea91ec1ce0710ebce4f848.pdf": "tabbakh2023ao7",
    "243a056d1acb153f70e39cc80a10e7d211a4312f.pdf": "dutta2023aet",
    "d8ab87176444f8b0747972310431c647a87de2df.pdf": "qiu2024eh4",
    "a1e17a09b0df676f133c2175366ab7fcd0381ac1.pdf": "li2023nnd",
    "77b625e6dec2732b96ca8cf57d7a123497873e1d.pdf": "zhao20243f3",
    "e91934d66d9133d854ff0a4cafbe7966584bbf97.pdf": "song2024fx9",
    "e37539f5c943a92ef56b49b7fa067bd976e418d4.pdf": "cai2023hji",
    "8fc9a7f23c5cf57e17790ffb99b6acb11693c673.pdf": "akinpelu2024d4m",
    "7ddbb9723420cb474c58d4b6b6834d5ebe112049.pdf": "hayat2024e4f",
    "c7c0c84fa505995cd888a8fbaa9def21cb84cca3.pdf": "li2024g3z",
    "1af8c50b4c7329fb2273aa7b755b0f9e8223e2d2.pdf": "arshed2023zen",
    "bca0adcba0c26c5a34b05795c5bfbc1948bf2fae.pdf": "qin20242eu",
    "c4895869637f73154d608cdd817234b0dbcd3508.pdf": "lee2023iwc",
    "64811427a4427588bb049a6a254446ddd2cafacc.pdf": "tagnamas20246ug",
    "7a0f92968c63613730b0c06b3bc3bd40d2666571.pdf": "li2023jft",
    "136b0ef7bd7f642a285b1d61ddf7abfc7ddc026f.pdf": "song2024c99",
    "cf439db0e071f19305ea1755aa108acdde73ed99.pdf": "aksoy20240c0",
    "ffc855594cad345ea5a1cce2ee27095bec767bc8.pdf": "leem2024j4t",
    "838d7862215df504dde41496cbe6ee711a12ae9f.pdf": "chen2024asi",
    "9a35a0aac04f4f712f38ea9bbfd2f0b0108242f5.pdf": "lin202343q",
    "f6bf7787115affe22c410eb5b2606269912d59a0.pdf": "ghahremani202491m",
    "69e56df0ae079c83607bb48e68f8db39a4177cd0.pdf": "wang20249qa",
    "1528bab641830ea279bdf53ff9e7d11d2b7e5028.pdf": "shahin2024g0q",
    "b43bb480caad36ab6fd667570275d42fe9050175.pdf": "zhu2023dpi",
    "1970ace992d742bdf098de08a82817b05ef87477.pdf": "yu2023l1g",
    "fec86abbb29b559c1eaff31428f5b59f8070bb67.pdf": "ko2024eax",
    "cb8b0eba078098000f004d7e0f97a33189261f30.pdf": "yang2024w08",
    "9b4d81736637392adabe688b6a698cec58f9ce57.pdf": "nazih20238nf",
    "981970d0f586761e7cdd978670c6a8f46990f514.pdf": "xia2023bp7",
    "6bdafb965e94c5240db2c30f20c37c4b4dd0e451.pdf": "nag2023cfn",
    "a3d1cebf99262cc20d22863b9540769b49a15ede.pdf": "gezici20246lf",
    "f05176e2d3f7a6e95b60f0e96fd9bf20e8b335d2.pdf": "ghazouani202342t",
    "442b5ec3aad4b099e71d6203a62eb7ec7519544c.pdf": "wang202338i",
    "635675452852e838644516e1eeefd1aaa8c8ac07.pdf": "guo2024tr7",
    "d2fce7480111d66a74caa801a236f71ab021c42c.pdf": "wang2023ski",
    "5135a8f690c66c3b64928227443c4f9378bd20e1.pdf": "zheng202325h",
    "77eea367f79e69995948699d806683c7731a60b1.pdf": "mogan2023ywz",
    "861f670073679ba05990f3bc6d119b13ab62aca7.pdf": "ebert202377v",
    "f93f718bb2d3f5e4354e174e54e71c5596f56ea3.pdf": "wang20245bq",
    "c5c9005aae80795e241de18b595c2d01393808f8.pdf": "cao20241ng",
    "14c42c0f2c94e0a1f4aa820886080263f9922047.pdf": "yang2024in8",
    "9998291d71f4e8ddf59f4b016b19df1f848eeed1.pdf": "hussain2025qoe",
    "9fa44f616e796f187d97b9cc324bd839bc21ad81.pdf": "shim2023z7g",
    "d62e6ba83d3da5b75e1d428f20f8199e3754bd5d.pdf": "alam2024t09",
    "d1faaa1d7d312dd5867683ce60519979de6b3349.pdf": "yang2024tti",
    "d68aedc3a957c25e5f19438ec4b64fb53b48422c.pdf": "wang20245hx",
    "bb5e6bbd4ad8c613a0bac3db23940c03ce94bec9.pdf": "song202479c",
    "55156532cb9c20fdcaed9ead238f7a2cbaab2527.pdf": "li2023lvd",
    "ae7c5ae7d377189c5a5f33a60b33c6cc5ca56124.pdf": "ma2023vhi",
    "3b427c8d3258968b9ac5eaf846d99ac027de9a76.pdf": "han202416k",
    "10e9943b3a974ac6175ffec3228e670ec9d2cc18.pdf": "katar202352u",
    "1783c85f8b951dc11c9dcb4929bfcae2d52ecb63.pdf": "hemalatha2024a14",
    "42bd7d3ca2b66eca5609fe77078fe8951ab9fd5c.pdf": "ma2024uan",
    "7aeadf7b9d839bdd9dbfbff5239f71792ff0c04c.pdf": "lai20238ck",
    "b2becca9911c155bf97656df8e5079ca76767ab9.pdf": "wang2024luv",
    "25a69bdd7f87ecc287b4e9a64eb4d6d562371add.pdf": "ling2023x36",
    "50e997b23a534a6fbfd32d63990fa80373ec7c6b.pdf": "wang2023bfo",
    "3ea79430455304c782572dfb6ca3e5230b0351de.pdf": "yin2023029",
    "0284fa05879f1609bf459f15a4d40e22355cd5ae.pdf": "mishra2024fbz",
    "714e21409b8c4f7788ac8c93795249a4e45e51ce.pdf": "heidari2024d9k",
    "2ef224f7a1f545eb8bfdf61c7b849f2676fe6a61.pdf": "yu2023fqo",
    "bc99c855d52ba3d432c428fb4096b3a22c04f8bf.pdf": "zhao2023pau",
    "3dee43cea71d5988a72a914121f3455106f89cc7.pdf": "pan20249k5",
    "1871c0bc27bd5cd5f2f6c3d6cedeb4b8c5a89499.pdf": "huan202345b",
    "c064efa0419b75ba131ec8470ed80f01e1a80f64.pdf": "belal2023x1u",
    "b48a85980deb5f1baa64d862b9f0e4e62124e4de.pdf": "li20238ti",
    "8ec10ffe0c1fc8f6a92d541f0e002e78080b564a.pdf": "huo2023e5h",
    "769ba82ab77fc46db4a594dc64576286d02cf1d7.pdf": "kim2023cvz",
    "d1255f6e7e7e14b253fe2245e842d0b45f5e715e.pdf": "fan2023whi",
    "5572237909914e23758115be6b8d7f99a8bd51dc.pdf": "zhao2023rle",
    "21c37fd6d00d6c727ec10f0c871c1ddf6e4f7ed9.pdf": "xie20234ve",
    "e38e70580acb204c05096de8da90b7ab1d4bdb6b.pdf": "li20233lv",
    "a246677a3d68e8cd7a63d24639a5cf3fc3b9f56e.pdf": "ma2023qek",
    "1397d1bcfeae68078756fdbc3ed7e62ef51a1e32.pdf": "tanimola20246cv",
    "52239e2338cd9f0e0e690203af8f0f1260ef7abf.pdf": "chen2023xxw",
    "f996d5ee3b8ad3c60510862a92fd72c6a41777e0.pdf": "ranjan20243bn",
    "f1b6f564e7cba8121df0eef0d656673e3bd18621.pdf": "fu20232q3",
    "12106888ec74baf48d1e9fb64e8932c9ce509e84.pdf": "shi20235zy",
    "3798e7f16fe69c29307a9bab4f0f4d779478afc5.pdf": "deressa2023lrl",
    "29a0077d198418bab2ea4d78d04a892ede860d68.pdf": "aburass2023qpf",
    "ac9cc0c28838a037e77f4e19433de170f47b3de9.pdf": "hassija2025wq3",
    "c4357abf10ff937e4ad62df4289fbbf74f114725.pdf": "huang20238er",
    "0b41c18d0397e14ddacee4143db74a05d774434d.pdf": "liu20230kl",
    "f9e1a8754e77a4b5f240d11e9e81e2563a319b89.pdf": "he20238sy",
    "4b9568a9798c527779a1f7479ffb3ba373640e3e.pdf": "guo2023dpo",
    "34ca004eda791c8c4191e0ebe65cc4405a116b08.pdf": "wang2023j6b",
    "409b43b8cd8a2ba69f93e80c2bacc0126238b550.pdf": "gopal20237ol",
    "dc5d707a8f9efaf32d2319ec2ca8d2619ef2926e.pdf": "liu2023awp",
    "9017053fb240d0870779b9658082488b392e7cde.pdf": "fu20228zq",
    "f27221a15f4c3c8af7ea0ada5bab25831ffe21ea.pdf": "sahoo20223yl",
    "2d421d94bc9eed935870088a6f3218244e36dc97.pdf": "ganz20249zr",
    "be1aabb6460d49905575da88d564864da9f80417.pdf": "paal2024no4",
    "0d7d27fbd8193acf8db032441fd22945d26e9952.pdf": "hassan20243qi",
    "0af7ccb9f6cf2edb35674dc97d0ec5d165f8433b.pdf": "k2024wyx",
    "f9efe4918b9a20c2e8084d7a5ec624b87ec1f211.pdf": "nguyen2024id9",
    "f62cb393ab51505bdd22823e74b7cf28f4801e1c.pdf": "almohimeed2024jq1",
    "4702a22a3c2da1284a88d5e608d38cd106d66736.pdf": "hao202488z",
    "9fcea59a7076064f5ac3949177307c1637473ffd.pdf": "yao20244li",
    "1ec9b653475287e95fdaef2f5247f82a8376c56c.pdf": "dong20245zz",
    "2b48f5f0769bf41b5a3025ace73417d19de65cf1.pdf": "zhang2024jha",
    "bfb310434204d99fa5c420ac272966c5ae1c1bd2.pdf": "boukhari2024gbb",
    "bbe5dfbecfd1bed7556b9c8269b0d363faa24973.pdf": "song2025idg",
    "be28edb064e2050e0852ff376434ee39f7c4b0a3.pdf": "zhou2024tps",
    "e25a0b06079966b8e43f8e1f2455913266cb7426.pdf": "abbaoui20244wy",
    "ecd9598308161557d6ac35b3e4d32770489e811d.pdf": "yang2024nyx",
    "7dc4b2930870e66caa7ff23b5d447283a6171452.pdf": "yang20241kf",
    "b1f0ac0b5e4044251dcdcce78e0046feb43cc7c7.pdf": "hu202434n",
    "903c8a82f5539b3e482efcb23ff59819f04f2c0f.pdf": "yang20244dq",
    "1e95bb5827dc784547a46058793c15effd74dccc.pdf": "keresh20249rl",
    "2104eea2b01b7a4b6ba2d256769328ac4919afad.pdf": "hu20247km",
    "8972ebf4f9ec47ee60c3ed0effd520daff495d5a.pdf": "alsulami2024ffb",
    "0fde8e3287ac04dcf210b73898c8ceb697ad489f.pdf": "yang2024wxl",
    "9a4718faa07a32cf1dce745062181d3342e9b054.pdf": "p2024nbn",
    "6e97c1ba023afc87c1b99881f631af8146230d96.pdf": "wu2024tsm",
    "1d7183d481ae5a396743dde39984f1f0c8f47edf.pdf": "dong2024bm2",
    "9fbf36da9ee60d56a7675042bb0b24798a3b966d.pdf": "swapno2025y2b",
    "d80166681f3344a1946b8bfc623f4679d979ee10.pdf": "yoo2024u1f",
    "9285996627124b945ec601a763f6ff884bac3281.pdf": "he2024m6j",
    "3f002ea3ede9d97f1fe29b16691491219c1e626d.pdf": "zhang202489a",
    "9121dcd10df00e5cc51dc94400e0325e0ae47bb9.pdf": "zhang2024pd6",
    "08606a6a8b447909e714be2c3160074fdf1b91ad.pdf": "dong20242ow",
    "0222269c963f0902cc9eae6768a3c5948531488b.pdf": "kayacan2024yy7",
    "8776fd7934dc48df4663dadf30c6da665d84fb19.pdf": "liu20248jh",
    "cc24f933b343b6a9701088cf6ae1dbf3299c0c9e.pdf": "shi2024r44",
    "88589b0b2d2d8caa09d8ca94414343455ae87d7c.pdf": "xin2024ljt",
    "d629289cc2f3efbe374a66d57690dd84f19d59ec.pdf": "zhou2024qty",
    "70211e2b04fcb4413edfd43e75b9f10e8d130171.pdf": "monjezi2024tdt",
    "cc817ba09f1c3c94bae73576463bcaf57c70261a.pdf": "baek2025h8e",
    "77e4ea46e7248408844f62fa3613aa246e35fdf0.pdf": "payne2024u8l",
    "271c6a87213b8b2a26ce3e1a6f20b07cc2f82b81.pdf": "qi2024rzy",
    "05d15576d88f9384738908f98716f91bdb5dbc78.pdf": "mercier2024063",
    "6feea99a1a6c876d6f4f714b463da9c95998dee7.pdf": "sikkandar2024p0d",
    "c05744f690ab9db007012a63c3c5c3ca48201c66.pdf": "hou2024e4y",
    "cebc29015a2827aa8a98ec39c90c93bca2f14848.pdf": "nfor2025o20",
    "630c9993b6aaf812e1aa3dae8e243af131abd3d0.pdf": "xiang2024tww",
    "d14a1677e416bd7ac6fbe01672cd3152fc1f983d.pdf": "tian20242kr",
    "8ed42c0d60eb4c91e99ee184a17719f7c3ce3f3f.pdf": "zhou2024r66",
    "c4cf58e104fc8d783dfd0bcf3ccd65f8e4028d7f.pdf": "taye20244db",
    "72e23cdc3accca1f09e2e19446bc475368c912d0.pdf": "alohali2024xwz",
    "7b6d64097d16219c043df64e4576bd7d87656073.pdf": "gao20246ks",
    "0820d2ac35cd55fd60f53c14460cca78fa996f1e.pdf": "du2024s3t",
    "b02144ef4ed94df78544959bc97eddef4580dd95.pdf": "tiwari2024jm9",
    "b6488fded5fb0728d7c3bd04f3afa3fba68d0450.pdf": "du20248pd",
    "24eba2f5345bc8279bb63f085e75cc26b37d0b1e.pdf": "chaurasia2024tri",
    "8ce6872b26f88e52aeed57f6f7528dee30b5f7b2.pdf": "karagz2024ukp",
    "dcf77b8e23bbc56c5c8f76ddc5d3cf68d0a051be.pdf": "lee2025r01",
    "f792dcf878e5bf8e7fb5aa31d231474d43462f4a.pdf": "dmen2024cb9",
    "310f5543603bef94d42366878a14161db1bf45de.pdf": "ferdous2024f89",
    "f9f524944dd9d1fe4b8956f9a822927b2eec29ad.pdf": "akan2024izq",
    "6ec48cb5fa3d149c9cc8a29373e64b63fe257913.pdf": "nahak20242mv",
    "3c7e07304bab6a860dbbe4ff36a4d87010036d2a.pdf": "han2024f96",
    "819ae728828d50f56f234e35832b1222de081bfc.pdf": "zhao2024p8o",
    "6a9247fe471690218ef04cea5f6f4b59b3b50747.pdf": "li2024qva",
    "f462bb00b8c4379c4a4699b66a19ce10da530b08.pdf": "wang2024ueo",
    "52fafbbf20a37ffe863f1fa1d44fde6b4fc3334e.pdf": "qi2024f5d",
    "b4be4f2ecb8711755f75dcb09a57d8c9722ac349.pdf": "zhu2024l2i",
    "7492734c76036143baf574d6602bd45a348c416f.pdf": "roy2024r9y",
    "eacb2581b480cf4a80bc601c8ea657651b41c0fc.pdf": "wang2024w4u",
    "da7d671f61e53601bdfc760d336cc98e5d9c516b.pdf": "pan202424q",
    "2bb98b30fb9aae37858248cd5ac3221ebcf7eeba.pdf": "du2024lml",
    "90e9d0a35bcf7e8251c4c9601a224bd95db14dd9.pdf": "luo202432g",
    "5ce1ebe051a64969caeb879c44c45320a5ca8011.pdf": "elnabi2025psy",
    "3bba1b3376e43a39bf4f1bf4eab558758a37ea2e.pdf": "ergn2025r6s",
    "3b11c8774bbeddd6722d65455bf9bb729a63c1cb.pdf": "mohsin2025gup",
    "04ed3b0f4e21d9f4884bf4db98edf83a34d005a9.pdf": "marcos2024oo2",
    "23ce9c2814d6567efec884b7043977cefcb7602e.pdf": "peng2024kal",
    "5b22bdc6aedf13d812509dd0f768353eb1469a79.pdf": "urrea20245k4",
    "d2fd0dc314023ebca1342f1379ba6d79c7ded84a.pdf": "zhang2024b7v",
    "1c96dc4ad4dfa3bf7c6ca9cf0c6197e7ac00272b.pdf": "saleem20249yl",
    "c35c316feba84334a4b08feb5281a5f83db0b8b2.pdf": "zhou2024toe",
    "c8174af99bc92d96935683beccc4161c65a8aa46.pdf": "lijin2024mhk",
    "05548c4b3db8be40fac236dfa8e21882fe9ce9de.pdf": "huang2024htf",
    "bd7a4a9e020b594ebcacb886ddae8731a2885209.pdf": "chen2024cha",
    "a7df70e86f049a86b1c555f9a399d3540f466be7.pdf": "shahin2024o1c",
    "e4b0c4a1d7ee87ad666214172f329831ade4c25d.pdf": "xu2024wux",
    "6604a900b9a7404a447b2167892a947012a9ffb8.pdf": "park2024d7y",
    "9814001811c4924171787de52e01cc31446e2f97.pdf": "elharrouss20252ng",
    "ab10aacab1a2672a154034c589dd0aa801912272.pdf": "du2024i6n",
    "325367f93439652efaa4bc6b50115bbb7371704e.pdf": "guo2024o8u",
    "3c6980902883f03c37332d34ead343e1229062b3.pdf": "zhang2024g0m",
    "f2b1b0fb57cccaac51b44477726d510570c4c799.pdf": "xu2025tku",
    "2456506ed87faa667a0c2b8af4028a5a86a49650.pdf": "li2024m4t"
  },
  "sections": {
    "Introduction": "\\section{Introduction}\n\\label{sec:introduction}\n\n\n\n\\subsection{Motivation for Visual Transformers}\n\\label{sec:1_1_motivation_for_visual_transformers}\n\n\nThe advent of Vision Transformers (ViTs) marks a significant paradigm shift in computer vision, primarily driven by the inherent limitations of Convolutional Neural Networks (CNNs) in capturing global contextual relationships and the groundbreaking success of Transformer architectures in Natural Language Processing (NLP). For an extended period, CNNs were the undisputed standard for visual recognition tasks, owing to their powerful inductive biases such as locality and translation equivariance. These biases, intrinsically embedded through spatially restricted convolutional kernels, are exceptionally effective at extracting local features and constructing hierarchical representations \\cite{han2020yk0}. However, this very strength became a critical constraint when models needed to comprehend long-range dependencies and holistic contextual information across an entire image. CNNs, by design, inherently struggle to model interactions between spatially distant parts of an image without resorting to increasingly deep architectures, significantly larger receptive fields achieved through techniques like dilated convolutions, or complex add-on modules such as non-local blocks \\cite{Wang2018NonlocalNN, zhou2021rtn}. While these methods attempted to mitigate the issue by expanding the effective receptive field, they often introduced increased computational complexity or did not fundamentally alter the local processing paradigm. As highlighted by \\cite{gheflati202131i}, CNNs' restricted local receptive fields inherently limit their capacity for global context learning, making it challenging to capture comprehensive image understanding. Similarly, \\cite{liu2022249} and \\cite{karagz2024ukp} explicitly note that while CNNs excel at representing local spatial features, they find it difficult to capture global information, underscoring a fundamental gap in their representational power for certain tasks.\n\nThe landscape of deep learning was profoundly reshaped by the Transformer architecture, introduced by \\cite{Vaswani2017} in NLP. This model demonstrated the unparalleled power of self-attention mechanisms to capture global dependencies within sequences, processing information from all input parts simultaneously. Unlike recurrent networks that process sequentially or CNNs with their localized focus, the Transformer's global attention mechanism, largely devoid of strong inductive biases about local connectivity, offered a compelling alternative for learning flexible, context-aware representations. The remarkable ability of Transformers to achieve state-of-the-art performance across diverse language tasks inspired researchers to question whether a similar global approach could unlock new capabilities in computer vision, overcoming the limitations of CNNs in capturing holistic image understanding and offering a less constrained path to feature learning \\cite{han2020yk0}. This inspiration was rooted in the desire for models that could intrinsically understand the relationships between any two parts of an image, regardless of their spatial separation, without being constrained by fixed-size kernels.\n\nThis profound inspiration culminated in the seminal work by \\cite{Dosovitskiy2021}, which introduced the Vision Transformer (ViT). This pioneering paper directly adapted the pure Transformer architecture to image recognition by treating image patches as a sequence of tokens, demonstrating that a model built entirely on self-attention, when trained on sufficiently large datasets, could achieve state-of-the-art performance without any convolutional layers. This was a profound challenge to the long-held paradigm of convolutional feature extraction, showcasing that global attention could indeed learn powerful visual representations and capture long-range dependencies more effectively than CNNs. The ability of ViTs to learn highly transferable representations, often outperforming CNNs on various downstream tasks when properly pre-trained, further solidified its revolutionary potential \\cite{zhou2021rtn}. For instance, studies like \\cite{htten2022lui} have shown that ViT models can achieve equivalent or even superior performance to CNNs in complex industrial visual inspection tasks, even with sparse data, highlighting their robust feature learning capabilities. This successful adaptation of a pure Transformer to vision tasks marked a definitive turning point, setting the stage for a new architectural lineage in computer vision that prioritizes global context and flexible feature learning.\n\\subsection{Overview of Vision Models: From CNNs to Transformers}\n\\label{sec:1_2_overview_of_vision_models:_from_cnns_to_transformers}\n\n\nThe trajectory of computer vision research has been marked by a significant paradigm shift, evolving from the long-standing dominance of Convolutional Neural Networks (CNNs) to the recent ascendance of Transformer architectures. For decades, CNNs served as the foundational backbone for visual recognition tasks, owing their success to inherent inductive biases that align well with the hierarchical and local nature of visual data \\cite{han2020yk0}. Key among these biases are local receptive fields, which allow neurons to process only a small, localized region of the input; weight sharing, which enables the detection of features regardless of their position; and spatial pooling, which progressively reduces spatial dimensions while retaining essential information. Architectures like AlexNet \\cite{alexnet_2012}, VGG \\cite{vgg_2014}, ResNet \\cite{resnet_2016}, and Inception \\cite{inception_2015} exemplified this era, demonstrating remarkable capabilities in tasks ranging from image classification to object detection by building increasingly complex and abstract representations through stacked convolutional layers. Their architectural evolution often involved increasing depth, introducing residual connections to mitigate vanishing gradients, and designing more intricate modules to enhance representational power, all while retaining the core principle of localized feature extraction.\n\nDespite their profound success and continuous advancements, CNNs inherently faced limitations, particularly in effectively modeling global contextual information across an entire image. Their reliance on local operations meant that capturing long-range dependencies required very deep networks, which could be computationally intensive and sometimes struggled to integrate information from widely separated regions efficiently. This inductive bias towards locality, while beneficial for many tasks, could also constrain their ability to understand broader semantic relationships or complex spatial layouts without extensive architectural modifications \\cite{han2020yk0, zhou2021rtn}.\n\nA pivotal moment arrived with the introduction of the Transformer architecture \\cite{attention_is_all_you_need_2017}, originally conceived for natural language processing (NLP). Its core innovation, the self-attention mechanism, revolutionized sequence modeling by enabling models to weigh the importance of different parts of an input sequence, fostering a more global understanding of relationships without relying on recurrent or convolutional operations. This mechanism allowed for parallel processing of inputs and offered an unprecedented ability to capture long-range dependencies directly. The profound success of Transformers in NLP inspired researchers to explore their applicability to other domains, including computer vision \\cite{heidari2024d9k}.\n\nThis exploration culminated in the introduction of the Vision Transformer (ViT) \\cite{image_is_worth_16x16_words_2021}. The ViT architecture ingeniously adapted the standard Transformer encoder for image recognition by treating images as sequences of non-overlapping patches. Each patch was linearly embedded into a token, and positional embeddings were added to retain spatial information. These visual tokens were then fed into a Transformer encoder, leveraging its global self-attention mechanism to capture relationships between patches across the entire image. This marked a significant paradigm shift, demonstrating that models could achieve competitive performance in image recognition without relying on convolutional layers, instead directly leveraging the Transformer's ability to capture global dependencies. This approach fundamentally altered how visual features were extracted and processed, moving away from predefined spatial hierarchies and towards a more flexible, data-driven understanding of global context \\cite{han2020yk0}.\n\nThe initial success of ViTs, however, was accompanied by practical challenges. Pure ViT models often required significantly larger datasets for pre-training compared to CNNs to achieve comparable performance, largely due to their reduced inherent inductive biases \\cite{image_is_worth_16x16_words_2021, zhou2021rtn}. Furthermore, the quadratic computational complexity of global self-attention with respect to the number of tokens (and thus image resolution) posed considerable computational costs, limiting their applicability to high-resolution images or resource-constrained environments. These initial limitations became the primary drivers for an explosion of subsequent research aimed at enhancing the efficiency, data-effectiveness, and architectural flexibility of Vision Transformers.\n\nIn conclusion, the transition from CNNs to Transformers represents a fundamental evolution in computer vision, shifting the focus from local, hierarchical feature extraction to global context understanding through attention mechanisms. While CNNs provided a robust foundation, the advent of ViTs offered a new paradigm capable of overcoming some of their inherent limitations, particularly in modeling long-range dependencies. This breakthrough, despite its initial challenges related to data hunger and computational expense, has spurred extensive research into architectural refinements, efficient training methodologies, and hybrid models. These ongoing efforts, which will be explored in detail in the subsequent sections of this review, are continuously solidifying the position of Vision Transformers as versatile and powerful backbones for a wide array of computer vision tasks, fundamentally altering how visual features are extracted and processed.\n\\subsection{Scope and Organization of the Review}\n\\label{sec:1_3_scope__and__organization_of_the_review}\n\n\nThis literature review aims to provide a comprehensive and systematically organized overview of Vision Transformers (ViTs), tracing their rapid evolution from foundational principles to their current state as a dominant paradigm in computer vision. By delineating the boundaries and structure of this review, readers are provided with a clear roadmap, ensuring a coherent and comprehensive understanding of the field's progression, its intricate interconnections, and the critical research trajectories that continue to shape its future. This structured approach, similar to recent surveys on ViTs that categorize design techniques and innovative methods \\cite{heidari2024d9k, hassija2025wq3}, is designed to facilitate a deep understanding of the architectural, methodological, and application-driven advancements.\n\nThe review is meticulously structured into seven main sections, each building upon the preceding one to offer a logical and progressive narrative of ViT development:\n\nThe journey commences with \\textbf{Section 1: Introduction}, which establishes the overarching context for Vision Transformers. This section begins by outlining the motivation behind their emergence, particularly in response to the inherent limitations of traditional Convolutional Neural Networks (CNNs) in capturing global dependencies (Subsection 1.1). It then provides a concise historical overview of vision models, highlighting the pivotal paradigm shift from CNN dominance to the rise of Transformers (Subsection 1.2). This introductory section sets the stage for the detailed exploration that follows, providing essential background for understanding the subsequent technical discussions.\n\n\\textbf{Section 2: Foundational Concepts of Vision Transformers}, delves into the core principles that underpin ViTs. It starts with a brief recap of the original Transformer architecture from Natural Language Processing (Subsection 2.1), explaining its key components. Subsequently, it details the pioneering work that adapted this architecture for visual data, introducing the original Vision Transformer (ViT) and its innovative approach to image tokenization and global self-attention (Subsection 2.2). Crucially, this section also critically examines the initial challenges and limitations of pure ViTs, such as their significant data hunger and quadratic computational complexity (Subsection 2.3), thereby establishing the impetus for subsequent research and architectural refinements.\n\nBuilding upon the identified limitations, \\textbf{Section 3: Architectural Enhancements and Efficiency}, explores the crucial innovations designed to make ViTs more practical and efficient. This section is segmented to discuss hierarchical Vision Transformers (Subsection 3.1), which address multi-scale feature learning and computational efficiency; data-efficient training and tokenization strategies (Subsection 3.2), aimed at reducing reliance on massive datasets; and advanced attention mechanisms (Subsection 3.3), which refine the core self-attention process for improved performance and reduced overhead. These advancements collectively broadened the applicability of ViTs beyond initial classification benchmarks.\n\n\\textbf{Section 4: Self-Supervised Learning and Vision Foundation Models}, shifts focus to the transformative role of self-supervised learning (SSL) in scaling ViTs. It details powerful SSL techniques like Masked Autoencoders (MAE) for pre-training (Subsection 4.1) and explores other contrastive and knowledge distillation approaches (Subsection 4.2). The section culminates by discussing the profound trend of scaling Vision Transformers to create \"Vision Foundation Models\" (Subsection 4.3), which aim to learn universal visual representations adaptable to a wide array of downstream tasks with minimal fine-tuning, marking a significant paradigm shift towards general-purpose visual intelligence.\n\nFurther addressing practical deployment and architectural synergy, \\textbf{Section 5: Hybrid Architectures and Mobile-Friendly Designs}, examines the strategic convergence of convolutional and Transformer architectures. It explores synergistic CNN-Transformer designs (Subsection 5.1) that leverage the complementary strengths of both paradigms. Additionally, it addresses the critical need for efficiency by detailing lightweight and mobile-optimized Vision Transformers (Subsection 5.2) for resource-constrained environments. The section also looks into novel hybrid paradigms (Subsection 5.3), such as the integration of state-space models, showcasing the continuous innovation in architectural design.\n\n\\textbf{Section 6: Applications of Visual Transformers}, comprehensively showcases the extensive and diverse utility of ViTs across the spectrum of computer vision tasks. This section illustrates their state-of-the-art performance in core vision tasks like classification and object detection (Subsection 6.1), their adaptation for dense prediction tasks such as segmentation and pose estimation (Subsection 6.2), and their remarkable versatility in specialized and multimodal applications (Subsection 6.3). This breadth of application underscores the profound impact of ViTs on various real-world scenarios.\n\nFinally, \\textbf{Section 7: Future Outlook: Challenges and Opportunities}, synthesizes the current state of ViTs by identifying persistent challenges, such as computational cost and data efficiency (Subsection 7.1), and critical aspects like interpretability, robustness, and generalization (Subsection 7.2). It then outlines promising future trends and open problems (Subsection 7.3), including novel architectural explorations and the integration of ViTs into broader multimodal AI systems. This concluding section encourages responsible development and application of this rapidly evolving technology, ensuring beneficial and equitable societal impact. Through this structured exploration, the review aims to provide a comprehensive and critically informed understanding of the Vision Transformer landscape.\n",
    "Foundational Concepts of Vision Transformers": "\\section{Foundational Concepts of Vision Transformers}\n\\label{sec:foundational_concepts_of_vision_transformers}\n\n\n\n\\subsection{The Transformer Architecture: A Brief Recap}\n\\label{sec:2_1_the_transformer_architecture:_a_brief_recap}\n\n\nThe Transformer architecture, introduced by \\cite{vaswani2017attention}, fundamentally revolutionized sequence modeling in Natural Language Processing (NLP) by moving away from recurrent and convolutional networks. This paradigm shift enabled unprecedented parallel processing capabilities and a more effective capture of long-range dependencies, laying the groundwork for its subsequent adaptation to diverse data modalities, including visual data.\n\nAt its core, the Transformer adopts an encoder-decoder structure. The encoder processes the input sequence, generating a rich, context-aware representation, which the decoder then utilizes to generate the output sequence, often in an auto-regressive manner. This entire architecture is built exclusively on attention mechanisms, eliminating the need for sequential processing inherent in Recurrent Neural Networks (RNNs) or the local receptive fields characteristic of Convolutional Neural Networks (CNNs).\n\nThe central innovation is the \\textbf{self-attention mechanism}, which allows the model to weigh the importance of different parts of the input sequence when processing each element. For every token in the sequence, three distinct vectors are computed: a Query (Q), a Key (K), and a Value (V). The attention scores are calculated by taking the dot product of the Query with all Keys, scaling by the square root of the key dimension to prevent vanishing gradients, and applying a softmax function. These scores then weight the corresponding Value vectors, which are summed to produce the output for that token. This mechanism inherently allows each token to attend to any other token in the sequence, regardless of their distance, thereby effectively capturing long-range dependencies that were challenging for traditional recurrent networks.\n\nTo enhance the model's ability to capture diverse relationships and focus on different aspects of the input simultaneously, the Transformer employs \\textbf{multi-head attention}. Instead of performing a single attention function, the Q, K, and V vectors are linearly projected multiple times into different lower-dimensional subspaces. Each of these \"heads\" then independently computes its own scaled dot-product attention. The outputs from all attention heads are concatenated and then linearly transformed back into the desired output dimension. This parallel processing of multiple attention mechanisms allows the model to learn various types of relationships and attend to different positions, enriching the overall representation and improving the model's capacity to model complex data.\n\nA critical aspect of sequence modeling is preserving the order of elements, which self-attention inherently lacks due to its permutation-invariant nature. To address this, the Transformer injects sequence order information through \\textbf{positional encoding}. These are vectors added directly to the input embeddings at the bottom of the encoder and decoder stacks. The original Transformer used fixed sinusoidal functions of different frequencies, allowing the model to easily learn relative positions. This mechanism ensures that while the attention mechanism processes tokens in parallel, the model retains crucial information about their relative and absolute positions within the sequence.\n\nEach encoder and decoder block also contains a position-wise fully connected feed-forward network, applied independently and identically to each position. This network typically consists of two linear transformations with a ReLU activation in between. Additionally, layer normalization is applied before each sub-layer (self-attention and feed-forward network), followed by a residual connection, aiding in stable training and gradient flow through deep networks.\n\nThe design of the Transformer, particularly its reliance on self-attention, marked a significant departure from previous state-of-the-art models like Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). Unlike RNNs, which process sequences token by token, leading to computational bottlenecks and difficulties with long-range dependencies due to vanishing/exploding gradients, the Transformer processes all tokens in parallel. This parallelization drastically reduces training time. Furthermore, while CNNs capture local features through fixed-size kernels, the Transformer's self-attention mechanism provides a global receptive field from the very first layer, allowing it to directly model relationships between any two tokens irrespective of their distance. This global perspective and parallel processing capability were revolutionary, establishing a new paradigm for sequence modeling that would profoundly influence subsequent research across various domains.\n\\subsection{The Original Vision Transformer (ViT)}\n\\label{sec:2_2_the_original_vision_transformer_(vit)}\n\n\nThe remarkable success of the Transformer architecture \\cite{vaswani2017attention} in natural language processing (NLP), primarily driven by its potent self-attention mechanism, ignited a fundamental inquiry within the computer vision community: could a similar, convolution-free paradigm effectively process and interpret visual data? This pivotal question was comprehensively addressed by Dosovitskiy et al. \\cite{dosovitskiy2021image} with the introduction of the Vision Transformer (ViT). Their pioneering work directly adapted the Transformer for image recognition, unequivocally demonstrating its viability as a powerful and distinct alternative to traditional Convolutional Neural Networks (CNNs) and marking a significant paradigm shift in visual modeling \\cite{han2020yk0, huo2023e5h}.\n\nThe core innovation of the original ViT lies in its elegant yet straightforward approach to re-conceptualizing an image as a sequence of discrete tokens, directly analogous to how words are treated in NLP. The process commences by partitioning an input image into a grid of fixed-size, non-overlapping square patches, typically $16 \\times 16$ pixels. Each of these patches is then flattened into a one-dimensional vector and subsequently projected linearly into a fixed-dimension embedding space. This transformation yields a sequence of \"visual tokens,\" where each token represents a localized region of the original image. Crucially, to compensate for the spatial information lost during the flattening and tokenization process, learnable positional embeddings are added to these patch embeddings. These embeddings are vital for retaining the relative spatial arrangement of patches, allowing the model to understand the geometric relationships between different parts of the image.\n\nTo facilitate image classification, a special, learnable \"class token\" is prepended to this sequence of embedded patches and their corresponding positional embeddings. This class token acts as a global representation of the entire image, accumulating information from all other visual tokens as it propagates through the Transformer layers. The augmented sequence then enters a standard Transformer encoder, which consists of multiple identical layers, each comprising a multi-head self-attention (MSA) block and a feed-forward network (FFN). The global self-attention mechanism within the MSA block is the cornerstone of ViT's ability to capture long-range dependencies; it enables each visual token (patch or class token) to attend to every other token in the sequence, thereby integrating global contextual information across the entire image. This stands in stark contrast to CNNs, which primarily rely on local receptive fields and hierarchical processing to build global context. The final state of the class token, after traversing the Transformer encoder, is then fed into a simple Multi-Layer Perceptron (MLP) head, which outputs the predicted class probabilities for the image.\n\nThis architectural departure from CNNs demonstrated remarkable empirical performance. Dosovitskiy et al. \\cite{dosovitskiy2021image} showcased that ViT achieved state-of-the-art results on large-scale image classification benchmarks, including ImageNet and JFT-300M. A critical finding was that ViTs, when pre-trained on sufficiently vast datasets (e.g., JFT-300M with 300 million images), could significantly outperform CNNs of comparable size. This performance advantage was attributed to the Transformer's ability to learn more flexible and global representations, unconstrained by the strong inductive biases (like locality and translation equivariance) inherent in CNNs. While these inductive biases make CNNs highly data-efficient on smaller datasets, ViT's \"data hunger\" meant it required extensive pre-training to fully exploit its capacity and generalize effectively \\cite{huo2023e5h}. The breakthrough performance of ViT not only challenged the long-standing dominance of CNNs in computer vision but also fundamentally reoriented research, establishing a new and highly influential direction for visual modeling based on global attention mechanisms \\cite{han2020yk0}. However, this paradigm shift, while promising, also introduced its own set of initial challenges, particularly concerning its substantial data requirements and the computational implications of its global self-attention mechanism, which necessitated further architectural refinements and training innovations.\n\\subsection{Initial Challenges and Limitations of Pure ViTs}\n\\label{sec:2_3_initial_challenges__and__limitations_of_pure_vits}\n\n\nDespite the groundbreaking success of the Vision Transformer (ViT) in demonstrating that a pure Transformer architecture could achieve state-of-the-art performance in image classification, the initial iterations of these models presented several significant challenges and inherent limitations \\cite{ICLR2019}. These drawbacks, which became apparent despite their impressive performance on large-scale datasets, necessitated extensive subsequent research into their optimization and refinement.\n\nA primary limitation of the original pure ViT architecture, as introduced by \\cite{ICLR2019}, was its substantial data requirement for pre-training. Unlike Convolutional Neural Networks (CNNs) that inherently possess inductive biases such as locality and translation equivariance, pure ViTs lack these built-in priors. Consequently, they must learn these fundamental visual properties from scratch, demanding massive datasets like JFT-300M to achieve competitive performance, making them less accessible for researchers without access to such vast resources. This data hunger was a critical bottleneck, prompting early efforts to make ViTs more data-efficient, such as through knowledge distillation \\cite{Touvron2021} or refined tokenization strategies \\cite{Yuan2021}.\n\nAnother significant challenge stemmed from the computational complexity of the global self-attention mechanism, which is central to pure ViTs. For an input image of $H \\times W$ resolution, if flattened into $N$ patches, the self-attention operation scales quadratically with the number of patches, $O(N^2)$, or equivalently, $O((HW)^2)$ with respect to the image resolution. This quadratic complexity made pure ViTs computationally prohibitive for high-resolution images and impractical for tasks requiring fine-grained, pixel-level understanding, such as dense prediction. This limitation spurred the development of hierarchical ViTs that employed localized attention mechanisms, like the shifted window attention in Swin Transformer \\cite{ICLR2021} or pyramid structures in PVT \\cite{ICCV2021}, to reduce computational cost and enable multi-scale feature extraction.\n\nBeyond data and computational demands, pure ViTs also exhibited architectural limitations that hindered their scalability and applicability. For instance, deeper pure ViT models were found to suffer from an \"attention collapse\" issue \\cite{zhou202105h}. As the network depth increased, the attention maps across layers tended to become increasingly similar, leading to redundant feature learning and performance saturation rather than improvement. To address this, \\cite{zhou202105h} proposed Re-attention to diversify attention maps and enable consistent performance gains in deeper ViTs.\n\nFurthermore, the lack of inherent hierarchical feature representation and strong locality inductive biases in pure ViTs made them less naturally suited for dense prediction tasks like object detection and semantic segmentation, which traditionally benefit from multi-scale feature pyramids and local context modeling. While studies like \\cite{li2022raj} demonstrated that plain ViT backbones could be adapted for object detection, they often required significant modifications, such as building simple feature pyramids from single-scale feature maps and incorporating cross-window propagation blocks, to achieve competitive results. This highlighted that the original pure ViT design was not intrinsically optimized for these tasks without architectural augmentations.\n\nIn summary, the initial pure ViT architectures, despite their paradigm-shifting performance, were characterized by their substantial data requirements, quadratic computational complexity, and a lack of built-in inductive biases that are naturally present in CNNs. These factors collectively made them less efficient, harder to train effectively on smaller datasets, and less versatile for a broad range of computer vision tasks without significant modifications. These inherent limitations became a fertile ground for subsequent research, driving the evolution towards more efficient, data-agnostic, and task-adaptable Vision Transformer variants.\n",
    "Architectural Enhancements and Efficiency": "\\section{Architectural Enhancements and Efficiency}\n\\label{sec:architectural_enhancements__and__efficiency}\n\n\n\n\\subsection{Hierarchical Vision Transformers}\n\\label{sec:3_1_hierarchical_vision_transformers}\n\n\nEarly Vision Transformers (ViTs) \\cite{dosovitskiy2020image} marked a significant paradigm shift in computer vision, yet their direct application to tasks requiring fine-grained spatial understanding, such as dense prediction, or processing of high-resolution images, was hampered by two primary limitations \\cite{hassija2025wq3, heidari2024d9k}. Firstly, the global self-attention mechanism exhibited quadratic computational complexity with respect to the number of input tokens (image patches), making it prohibitively expensive for large inputs. Secondly, pure ViTs lacked the inherent multi-scale feature representations and inductive biases (like locality) that Convolutional Neural Networks (CNNs) naturally possess, which are crucial for capturing both global context and local details across various scales. To address these challenges, a critical line of research emerged, focusing on the development of hierarchical Vision Transformers that integrate multi-scale processing and more efficient attention mechanisms, thereby mimicking CNN-like feature pyramids and achieving linear computational complexity.\n\nOne of the foundational models in this category is the \\textbf{Pyramid Vision Transformer (PVT)} \\cite{wang2021pyramid}. PVT introduced a progressive shrinking pyramid structure, akin to feature pyramids in CNNs, by gradually reducing the resolution of feature maps in deeper layers. This hierarchical design allows for the generation of multi-scale features, which are essential for dense prediction tasks. To manage the computational cost, PVT employs a Spatial-Reduction Attention (SRA) module. Unlike global self-attention, SRA reduces the spatial dimension of the key and value matrices before computing attention, effectively lowering the computational complexity from quadratic to linear with respect to image size. This innovation enabled PVT to serve as a robust backbone for tasks like object detection and semantic segmentation, demonstrating the viability of hierarchical Transformers for a broader range of vision applications \\cite{hassija2025wq3}.\n\nBuilding upon the success of hierarchical designs, the \\textbf{Swin Transformer} \\cite{liu2021ljs} emerged as another seminal work, further solidifying the potential of these architectures. Swin Transformer also adopts a hierarchical structure through progressive patch merging, generating multi-scale feature maps. Its core innovation, however, lies in the \"shifted window attention\" mechanism. Instead of global attention, Swin restricts self-attention computation to non-overlapping local windows within each stage, drastically reducing computational complexity to linear. To facilitate information exchange between windows and capture global dependencies, the windows are shifted between successive Transformer blocks. This elegant solution allows Swin Transformer to achieve state-of-the-art performance across a wide array of vision tasks, including image classification, object detection, and semantic segmentation, effectively establishing hierarchical Transformers as powerful general-purpose vision backbones. While both PVT and Swin Transformer achieved linear complexity, Swin's shifted window approach provided a more dynamic mechanism for cross-window information flow compared to PVT's static spatial reduction, often leading to stronger performance.\n\nFurther advancements in hierarchical ViTs have focused on refining attention mechanisms and simplifying architectures. The fixed window sizes and handcrafted attention patterns in models like Swin Transformer and PVT, while efficient, can sometimes limit their ability to model long-range relations adaptively \\cite{xia2023bp7}. Addressing this, the \\textbf{Deformable Attention Transformer (DAT++)} \\cite{xia2023bp7} proposes a novel deformable multi-head attention module. This module adaptively allocates the positions of key and value pairs in a data-dependent manner, allowing the model to dynamically focus on relevant regions and capture more flexible spatial relationships, similar to deformable convolutions in CNNs. This approach enhances the representation power of global attention while maintaining efficiency. Similarly, the \\textbf{Dynamic Window Vision Transformer (DW-ViT)} \\cite{ren2022ifo} extends the window-based paradigm by moving \"beyond fixation.\" DW-ViT assigns windows of different sizes to different head groups within multi-head self-attention and dynamically fuses the multi-scale information, overcoming the limitation of fixed single-scale windows in models like Swin Transformer and further improving multi-scale modeling capabilities.\n\nIn contrast to increasing architectural complexity, some research has explored simplification. \\textbf{Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles} \\cite{ryali202339q} proposes a streamlined hierarchical ViT design, arguing that many added complexities can be removed without sacrificing accuracy, provided the model is pre-trained with a robust self-supervised pretext task, such as Masked Autoencoders (MAE) \\cite{he2022masked}. By leveraging strong pre-training, Hiera achieves competitive or superior accuracy to more complex hierarchical models while being significantly faster during both inference and training. This work underscores the critical interplay between architectural design and effective pre-training strategies, suggesting that inductive biases from hierarchical processing, combined with powerful self-supervision, can lead to highly efficient and performant architectures without excessive overhead. This also opens a comparative perspective on the fundamental role of attention itself; for instance, the \\textbf{ShiftViT} \\cite{wang2022da0} even explores replacing attention layers entirely with a zero-parameter shift operation, achieving performance comparable to Swin Transformer, suggesting that the hierarchical structure and feature processing might be as crucial as the attention mechanism itself.\n\nIn conclusion, the evolution of hierarchical Vision Transformers has been instrumental in overcoming the initial limitations of pure ViTs, particularly their quadratic complexity and lack of multi-scale representations. Models like PVT and Swin Transformer pioneered efficient multi-scale processing through techniques such as spatial-reduction attention and shifted window attention, respectively, making ViTs suitable for dense prediction tasks and high-resolution images. Subsequent innovations, including deformable attention (DAT++) and dynamic windowing (DW-ViT), have further refined these attention mechanisms for greater flexibility and multi-scale understanding. Concurrently, models like Hiera highlight the power of architectural simplification when coupled with strong self-supervised pre-training. Future research in this domain will likely continue to explore the optimal balance between architectural complexity, computational efficiency, and representational power within hierarchical structures, further optimizing attention mechanisms, and deepening the synergy with advanced self-supervised learning paradigms to unlock even more robust and versatile vision backbones.\n\\subsection{Data-Efficient Training and Tokenization Strategies}\n\\label{sec:3_2_data-efficient_training__and__tokenization_strategies}\n\n\nEarly Vision Transformers (ViTs) demonstrated impressive capabilities in image recognition, yet their reliance on massive proprietary datasets like JFT-300M for pre-training posed a significant barrier to their widespread adoption and practical application \\cite{Dosovitskiy2021}. This section explores key innovations that addressed this limitation, focusing on strategies for data-efficient training and improved tokenization, thereby making ViTs more accessible and performant on standard, smaller datasets.\n\nOne of the most impactful approaches to mitigate ViTs' data hunger is knowledge distillation. The work by \\cite{Touvron2021} introduced Data-efficient image Transformers (DeiT), which enabled ViTs to be trained effectively on ImageNet-1K, a significantly smaller dataset, without requiring external data. DeiT achieved this by employing a distillation strategy where a ViT student model learns from a powerful convolutional neural network (CNN) teacher. A crucial innovation was the \"distillation token,\" an additional token that interacts with the class token through self-attention and is trained to match the teacher's output, effectively transferring the teacher's knowledge and enabling the ViT to achieve competitive performance with much less training data.\n\nWhile knowledge distillation provided a powerful mechanism for data-efficient training, another line of research focused on enhancing the initial representation of image patches, thereby reducing the inherent data requirements for learning robust visual features. The original ViT treats non-overlapping image patches as independent tokens, which can lose fine-grained local structural information. To address this, \\cite{Yuan2021} proposed Tokens-to-Token ViT (T2T-ViT), which introduced an improved tokenization strategy. Instead of a single-step patch embedding, T2T-ViT employs a multi-stage process where neighboring pixels are progressively aggregated into tokens through a series of self-attention layers. This hierarchical tokenization module effectively captures local structural details and preserves more information from the original image, leading to better performance from scratch on standard benchmarks like ImageNet-1K without relying on massive pre-training datasets or external teacher models.\n\nThe advancements in data-efficient training through distillation, as exemplified by DeiT, and improved tokenization strategies, such as T2T-ViT, collectively transformed the landscape of Vision Transformers. DeiT demonstrated that ViTs could achieve state-of-the-art results on ImageNet-1K by effectively mimicking a larger teacher, while T2T-ViT showed that a more sophisticated initial tokenization could inherently boost ViT performance on smaller datasets by better encoding local visual information. These innovations were critical in making ViTs more practical and accessible, moving them beyond the realm of models requiring immense computational resources and proprietary datasets. However, challenges remain in further reducing the data requirements for highly specialized tasks and in developing unified frameworks that seamlessly integrate the benefits of both distillation and advanced tokenization for optimal efficiency across diverse applications.\n\\subsection{Advanced Attention Mechanisms}\n\\label{sec:3_3_advanced_attention_mechanisms}\n\n\nThe foundational self-attention mechanism in Vision Transformers, while powerful for capturing global dependencies, inherently suffers from quadratic computational complexity with respect to the number of tokens, posing significant challenges for high-resolution images or deeper architectures. Furthermore, vanilla self-attention often lacks explicit spatial inductive biases, which can sometimes lead to redundancy or \"attention collapse\" in very deep models, limiting their expressive power. To address these limitations, researchers have developed various advanced attention mechanisms aimed at improving efficiency, flexibility, and expressiveness.\n\nOne critical challenge in scaling Vision Transformers to greater depths is the phenomenon of \"attention collapse.\" As models become deeper, the attention maps across different layers can become increasingly similar, hindering the model's ability to learn diverse and effective representations. \\textcite{zhou202105h} introduced DeepViT to tackle this issue, observing that in deeper ViTs, self-attention mechanisms often fail to learn distinct concepts, leading to performance saturation. To mitigate this, they proposed Re-attention, a simple yet effective method designed to re-generate attention maps and increase their diversity across layers. This innovation allows for the training of significantly deeper ViT models with consistent performance improvements, enhancing the model's overall expressiveness by ensuring that attention layers continue to contribute unique feature transformations.\n\nBeyond improving expressiveness in deep models, a major thrust in advanced attention research focuses on enhancing efficiency and spatial awareness. The global nature of standard self-attention means every token attends to every other token, which can be computationally expensive and may lead to attention being drawn to irrelevant regions. To make attention more adaptive and efficient, Deformable Attention was introduced, allowing the attention mechanism to sample features at adaptive, learned offsets \\textcite{xia2022qga}. Unlike fixed-grid or sparse attention patterns, this data-dependent sampling enables the model to dynamically focus on relevant regions and capture more informative features. This flexible scheme significantly reduces computational overhead by concentrating attention on salient parts of the input, while simultaneously enhancing the model's ability to adapt to varying object shapes and scales, thereby improving spatial awareness and fine-grained detail capture.\n\nThis concept of focusing attention on relevant tokens to reduce computation is also explored in approaches like Focal Attention. Such mechanisms typically employ hierarchical attention or explicit token selection strategies to prioritize important visual information. By selectively attending to a subset of tokens or regions, these methods aim to mitigate the quadratic complexity of global attention, making ViTs more scalable for high-resolution inputs. These innovations collectively enhance the model's ability to capture fine-grained details and adapt to varying object shapes and scales, leading to more robust and accurate visual understanding.\n\nIn summary, advanced attention mechanisms represent a crucial evolutionary step for Vision Transformers, moving beyond the limitations of vanilla self-attention. By addressing issues like attention collapse through techniques such as Re-attention \\textcite{zhou202105h} and improving efficiency and spatial adaptability with data-dependent sampling as seen in Deformable Attention \\textcite{xia2022qga}, these refinements enable the development of more powerful, flexible, and scalable ViT architectures. Future directions will likely continue to explore more sophisticated ways to balance global context modeling with local detail, further optimizing computational efficiency, and integrating stronger inductive biases into attention mechanisms for even more robust visual understanding across diverse tasks.\n",
    "Self-Supervised Learning and Vision Foundation Models": "\\section{Self-Supervised Learning and Vision Foundation Models}\n\\label{sec:self-supervised_learning__and__vision_foundation_models}\n\n\n\n\\subsection{Masked Autoencoders (MAE) for Pre-training}\n\\label{sec:4_1_masked_autoencoders_(mae)_for_pre-training}\n\n\nVision Transformers (ViTs), despite their remarkable success in various computer vision tasks, initially faced significant challenges related to their data-hungry nature and the difficulty of scaling them to deeper architectures without performance degradation. Early observations, such as those presented in \\cite{zhou202105h}, highlighted an \"attention collapse\" issue in deeper ViTs, where attention maps became increasingly similar across layers, hindering effective representation learning and limiting performance gains. This necessitated the development of efficient and scalable self-supervised pre-training strategies to unlock the full potential of ViTs and enable them to learn robust visual representations from unlabeled data.\n\nA pivotal breakthrough in this regard was the introduction of Masked Autoencoders (MAE) by \\cite{CVPR2022_Masked_Autoencoders_Are_Scalable_Vision_Learners_2022}. MAE revolutionized pre-training for Vision Transformers by proposing a highly effective self-supervised learning strategy rooted in image reconstruction. The core idea involves masking a substantial portion of image patches, typically around 75\\%, and training a Transformer encoder-decoder architecture to reconstruct the missing pixels. The encoder processes only the visible, unmasked patches, leading to significant computational efficiency during pre-training, while a lightweight decoder is responsible for predicting the original pixel values of the masked patches. This design forces the encoder to learn rich, high-level semantic representations from limited visual context, preventing it from relying on trivial low-level statistical regularities.\n\nThe MAE approach offers several compelling advantages. Its asymmetric encoder-decoder design, where the encoder operates on a sparse set of visible patches, drastically reduces the computational cost of pre-training compared to prior self-supervised methods that process full images. This efficiency makes MAE exceptionally scalable, enabling the effective pre-training of very large Vision Transformer models on vast amounts of unlabeled data. Consequently, MAE-pretrained ViTs have demonstrated state-of-the-art performance across various downstream tasks, including image classification, object detection, and semantic segmentation, often requiring less fine-tuning data than models trained with other self-supervised or supervised methods. The learned representations are highly robust and generalizable, significantly reducing the reliance on extensive labeled datasets and making large-scale pre-training more feasible and impactful.\n\nThe success of MAE extended beyond pure Transformer architectures, demonstrating the generality of its self-supervised reconstruction paradigm. For instance, \\cite{ICLR2023_ConvNeXt_V2_Co_designing_and_Scaling_ConvNets_with_Masked_Autoencoders_2023} successfully applied MAE pre-training to modern ConvNets, showing that even CNNs could benefit significantly from this Transformer-inspired self-supervised strategy, achieving improved performance and bridging the gap between the two architectural paradigms. Furthermore, the efficiency and scalability demonstrated by MAE paved the way for the exploration of even grander model scales. Works like \\cite{ICLR2023_Scaling_Vision_Transformers_to_1_Billion_Parameters_2023} leveraged similar efficient pre-training principles to scale Vision Transformers to unprecedented sizes, showcasing the potential for massive, general-purpose \"Vision Foundation Models.\" The drive for learning robust, unsupervised features, significantly propelled by MAE's success, continued with advancements such as \\cite{ICLR2023_DINOv2_Learning_Robust_Visual_Features_without_Supervision_2023}, which further refined self-supervised approaches to learn highly transferable visual features, building upon the foundation of efficient pre-training established by MAE.\n\nIn conclusion, Masked Autoencoders marked a significant turning point in the pre-training landscape for Vision Transformers. By providing an efficient, scalable, and effective self-supervised learning strategy, MAE made large-scale ViT pre-training practical and solidified self-supervised learning as a cornerstone for developing powerful Vision Foundation Models. While MAE has proven highly effective, ongoing research continues to explore optimal masking strategies, alternative reconstruction targets, and combinations with other self-supervised objectives to further enhance the learned representations and address the persistent challenge of adapting these powerful, pre-trained models for efficient deployment on resource-constrained devices.\n\\subsection{Contrastive and Knowledge Distillation Approaches}\n\\label{sec:4_2_contrastive__and__knowledge_distillation_approaches}\n\n\nThe quest for learning robust, generalizable visual features for Vision Transformers (ViTs) without the prohibitive cost of extensive human annotation has propelled the development of sophisticated self-supervised learning (SSL) paradigms. Beyond masked autoencoding, two other prominent strategies, contrastive learning and advanced knowledge distillation, have significantly contributed to overcoming the data-hungry nature of ViTs, enabling them to learn meaningful representations directly from vast quantities of unlabeled data. These methods enhance ViTs' performance and applicability across diverse downstream tasks, demonstrating powerful alternative strategies for unsupervised representation learning.\n\nContrastive learning, a foundational SSL approach, operates by teaching the model to distinguish between similar and dissimilar pairs of data points. The core idea is to pull \"positive\" pairs (different augmented views of the same image) closer in the embedding space while pushing \"negative\" pairs (views from different images) apart. Early successes with Convolutional Neural Networks (CNNs), such as SimCLR \\cite{chen2020simple} and MoCo \\cite{he2020momentum}, laid the groundwork for adapting these techniques to ViTs. MoCo-v3 \\cite{chen2021empirical}, for instance, successfully applied a momentum contrast framework to ViTs, demonstrating that these architectures could learn competitive representations through instance discrimination. The effectiveness of contrastive learning stems from its ability to learn representations that are invariant to various data augmentations, thereby capturing essential semantic features without explicit labels.\n\nA highly effective and distinct paradigm, particularly for ViTs, is self-distillation, a specialized form of knowledge distillation where a model learns from itself through a teacher-student framework. This approach has proven exceptionally successful in generating high-quality, transferable features. The original DINO (Self-Distillation with No Labels) \\cite{caron2021emerging} was a seminal work, demonstrating that ViTs could learn dense, semantic features by matching the output of a momentum-updated teacher network. This method encourages the student to produce features that are invariant to different views of the same image, effectively learning without explicit supervision. Building upon this, DINOv2 \\cite{ICLR2023_DINOv2} scaled this teacher-student self-distillation framework to unprecedented levels, utilizing massive datasets and architectural refinements to produce exceptionally robust, generalizable, and transferable visual features. DINOv2's representations frequently outperform features derived from supervised pre-training on various transfer tasks, significantly reducing the reliance on labeled datasets and making ViTs more accessible and scalable for real-world applications.\n\nBeyond self-distillation, other forms of knowledge distillation have also been explored to enhance self-supervised ViTs, particularly for efficiency and transferability. For instance, Attention Distillation \\cite{wang2022pee} investigates distilling knowledge from a self-supervised ViT teacher to a smaller ViT student. This method highlights that directly distilling information from the crucial attention mechanism can significantly narrow the performance gap between teacher and student, outperforming existing self-supervised knowledge distillation methods focused on ConvNets. Such approaches are vital for deploying high-performing self-supervised ViTs on memory and compute-constrained devices, extending the benefits of powerful SSL to more practical scenarios.\n\nA critical analysis reveals fundamental differences in the learning objectives and resulting feature properties between these SSL paradigms and Masked Autoencoders (MAE) discussed in the preceding subsection. While MAE \\cite{he2022masked} forces the model to learn rich representations through a high-information-density pixel reconstruction task, often yielding features beneficial for dense prediction, DINO-style self-distillation encourages semantic consistency and invariance to data augmentations, typically leading to features that are highly effective for classification and linear probing. Contrastive learning, on the other hand, focuses on instance-level discrimination, aiming for representations that can distinguish individual images while being robust to transformations. These distinct objectives mean that each SSL paradigm can yield features with different strengths, making them more suitable for specific downstream tasks or offering complementary benefits. For example, while MAE's reconstruction task can be adapted to hierarchical architectures like HiViT \\cite{zhang2022msa} and Hiera \\cite{ryali202339q} for efficiency, contrastive and distillation methods often focus on the feature space directly. Moreover, some approaches explore hybrid strategies, such as MAT-VIT \\cite{han2024f96}, which leverages an MAE-based self-supervised auxiliary task alongside a supervised primary task, demonstrating the potential for combining different SSL paradigms to maximize learning from both labeled and unlabeled medical images.\n\nIn conclusion, contrastive learning and knowledge distillation approaches, particularly self-distillation as pioneered by DINO and scaled by DINOv2, represent critical advancements in self-supervised learning for Vision Transformers. By leveraging teacher-student frameworks, instance discrimination, and focusing on learning highly robust and generalizable features from unlabeled data, these methods empower ViTs to achieve state-of-the-art performance in various downstream tasks, often surpassing supervised alternatives. The continuous development of such unsupervised representation learning strategies, coupled with innovations in knowledge transfer for efficiency, is fundamental to realizing the full potential of Vision Transformers as efficient, scalable, and universally applicable models, thereby significantly reducing the dependency on costly human annotations and paving the way for the emergence of powerful Vision Foundation Models.\n\\subsection{Scaling Vision Transformers to Foundation Models}\n\\label{sec:4_3_scaling_vision_transformers_to_foundation_models}\n\nThe quest for general-purpose visual intelligence has ushered in a transformative era, marked by the scaling of Vision Transformers (ViTs) to unprecedented sizes, often comprising billions of parameters, to establish \"Vision Foundation Models.\" These colossal models, meticulously trained on vast datasets using sophisticated self-supervised learning (SSL) techniques, aim to distill universal visual representations. The ultimate objective is to develop versatile backbones that exhibit exceptional adaptability across a broad spectrum of vision tasks, delivering superior performance with significantly reduced task-specific fine-tuning. This paradigm represents a profound shift towards general-purpose visual intelligence, capable of impressive zero-shot or few-shot learning capabilities.\n\nInitial explorations into scaling ViTs encountered significant hurdles. Naively increasing model depth, as explored by \\cite{zhou202105h} with DeepViT, revealed an \"attention collapse\" phenomenon. In deeper layers, attention maps became increasingly homogeneous, hindering performance gains and indicating that simply adding more layers was insufficient without more robust architectural designs and effective training strategies. This challenge underscored the critical need for advancements in how ViTs learn and generalize at scale.\n\nA pivotal breakthrough in enabling the efficient scaling of ViTs came with the advent of Masked Autoencoders (MAE) \\cite{CVPR2022_Masked_Autoencoders_Are_Scalable_Vision_Learners}. As discussed in Subsection 4.1, MAE introduced an elegant and highly efficient self-supervised pre-training paradigm. By masking a large portion of image patches and training the Transformer encoder-decoder to reconstruct the missing pixels, MAE effectively mitigated the notorious data-hungry nature of ViTs, allowing them to learn rich, robust representations from vast quantities of unlabeled data. This innovation was instrumental in unlocking the potential for large-scale pre-training. Complementing MAE, methods like DINOv2 \\cite{ICLR2023_DINOv2_Learning_Robust_Visual_Features_without_Supervision}, elaborated in Subsection 4.2, further refined self-supervised learning by leveraging self-distillation with a teacher-student framework. DINOv2 produced highly generalizable and transferable features without explicit supervision, often surpassing supervised pre-training in transfer tasks and demonstrating impressive zero-shot capabilities.\n\nThe combination of efficient self-supervised pre-training and the availability of colossal datasets has been fundamental to the scaling trend. Vision Foundation Models are typically pre-trained on datasets far exceeding ImageNet, such as JFT-300M \\cite{bai2022f1v, hong2022ks6}, JFT-4B, or the internet-scale LAION-5B. The sheer volume and diversity of these datasets, coupled with scalable SSL methods, allow ViTs to learn highly abstract and universal visual concepts. Empirical studies have consistently demonstrated that performance scales predictably with increased model size, dataset size, and computational resources, a phenomenon often referred to as \"scaling laws\" in deep learning.\n\nBuilding upon these foundations, researchers have aggressively pushed the boundaries of ViT size. The work by \\cite{ICLR2023_Scaling_Vision_Transformers_to_1_Billion_Parameters} notably demonstrated the efficacy of scaling Vision Transformers to one billion parameters. Their findings unequivocally showed that larger models, when pre-trained on extensive datasets using techniques like MAE, yield superior performance and significantly enhanced generalization across diverse downstream tasks. This marked a crucial step towards realizing truly universal visual backbones. Even more extreme examples, such as Google's ViT-22B, further exemplify the profound capabilities that emerge from massive scaling. These models exhibit emergent properties, including remarkable zero-shot transfer to unseen tasks and improved few-shot learning, which are hallmarks of true foundation models. They can often adapt to new domains with minimal or no task-specific fine-tuning, drastically reducing the effort and labeled data required for new applications.\n\nBeyond pure parameter count, architectural innovations have also contributed to the scalability and efficiency of these large models. For instance, approaches like UFO-ViT \\cite{song20215tk} introduce linear attention mechanisms to mitigate the quadratic computational complexity of traditional self-attention, which becomes a bottleneck at high resolutions or extreme depths. Similarly, models like Hiera \\cite{ryali202339q} demonstrate that even simplified hierarchical ViT designs, when effectively pre-trained with MAE, can achieve state-of-the-art performance while being significantly faster, thus contributing to more efficient scaling.\n\nWhile scaling to billions of parameters offers immense representational power, the practical deployment of such massive models remains a critical consideration. The computational cost for inference and the memory footprint can be prohibitive for many real-world scenarios, particularly on resource-constrained edge devices. To address this, research into efficiency techniques is paramount. Quantization methods, such as Q-ViT \\cite{li20229zn}, enable significant model compression by reducing bit-widths while striving to preserve performance. Furthermore, specialized hardware accelerators like ViTA \\cite{nag2023cfn} are being developed to optimize the inference of Vision Transformers, ensuring that these powerful, scaled models can be deployed efficiently in practical applications.\n\nIn conclusion, the evolution of Vision Transformers into foundation models represents a profound paradigm shift from task-specific model development to the creation of universally powerful, pre-trained visual intelligence. This trajectory is characterized by extreme architectural scaling, the indispensable role of advanced self-supervised learning for robust feature extraction from colossal datasets, and the resultant emergence of powerful zero-shot and few-shot learning capabilities. Future directions will undoubtedly focus on further enhancing the efficiency of training and inference for these massive models, exploring deeper multimodal integration for richer representations, and critically addressing the ethical implications and potential biases inherent in such large-scale, general-purpose visual systems.\n",
    "Hybrid Architectures and Mobile-Friendly Designs": "\\section{Hybrid Architectures and Mobile-Friendly Designs}\n\\label{sec:hybrid_architectures__and__mobile-friendly_designs}\n\n\n\n\\subsection{Synergistic CNN-Transformer Designs}\n\\label{sec:5_1_synergistic_cnn-transformer_designs}\n\n\nThe evolution of computer vision architectures has increasingly moved towards a synergistic paradigm, strategically combining the inherent strengths of Convolutional Neural Networks (CNNs) with the global context modeling capabilities of Transformers. This convergence is driven by the recognition that while CNNs excel at local feature extraction and possess strong inductive biases like locality and translation equivariance, pure Vision Transformers (ViTs) \\cite{dosovitskiy2021image} offer unparalleled global receptive fields and flexibility in capturing long-range dependencies. Hybrid designs aim to mitigate the limitations of each, such as ViTs' high data requirements and quadratic computational complexity, or CNNs' struggle with modeling global relationships, thereby fostering more robust, efficient, and versatile vision backbones.\n\nOne prominent direction in this synergy involves modernizing traditional CNN architectures by incorporating Transformer-inspired methodologies and training paradigms. A seminal work in this category is \\textit{ConvNeXt V2} \\cite{iclr2023}. Building upon the architectural principles of ConvNeXt, which re-examined and optimized CNN designs to resemble Transformers (e.g., using large kernel depthwise convolutions and inverted bottleneck structures), ConvNeXt V2 crucially leveraged the Masked Autoencoder (MAE) pre-training strategy \\cite{he2022masked}. This integration demonstrated that CNNs could benefit immensely from self-supervised learning techniques originally developed for Transformers, achieving state-of-the-art performance and effectively bridging the perceived architectural gap. ConvNeXt V2 showcased that the inductive biases of CNNs, when combined with powerful, scalable pre-training, remain highly competitive and efficient.\n\nConversely, another significant approach integrates powerful CNN components directly into Transformer-like frameworks. \\textit{InternImage} \\cite{cvpr2023} exemplifies this by embedding deformable convolutions within a Transformer-inspired architecture. Deformable convolutions, a hallmark of advanced CNNs, enable adaptive receptive fields and flexible spatial sampling, which are crucial for handling geometric variations and object deformations effectively. By incorporating this mechanism, InternImage creates robust \"Vision Foundation Models\" that combine the global context understanding of Transformers with the precise, adaptive local feature extraction characteristic of sophisticated convolutions. This allows the model to dynamically focus on relevant regions, enhancing its ability to capture fine-grained details and adapt to varying object shapes and scales, a limitation often observed in standard fixed-grid attention mechanisms. A related approach is seen in \\textit{Deformable Attention Transformer (DAT)} and its enhanced version \\textit{DAT++} \\cite{xia2022qga, xia2023bp7}. These models integrate a novel deformable multi-head attention module where key and value positions are adaptively allocated in a data-dependent manner, effectively bringing the adaptive sampling power of deformable convolutions directly into the self-attention mechanism, thereby enhancing spatial awareness and reducing computational overhead compared to dense global attention.\n\nBeyond integrating components or pre-training strategies, some hybrid architectures explicitly interleave CNN and Transformer blocks to capitalize on their respective strengths at different stages of feature extraction. \\textit{Next-ViT} \\cite{li2022a4u} proposes a \"Next Hybrid Strategy\" (NHS) that stacks \"Next Convolution Blocks\" (NCB) for local information capture and \"Next Transformer Blocks\" (NTB) for global information. This design is optimized for efficient deployment in realistic industrial scenarios, addressing the challenge of achieving both high performance and low latency on hardware accelerators like TensorRT. Similarly, \\textit{TRT-ViT} \\cite{xia2022dnj} provides practical guidelines for TensorRT-oriented network design, suggesting an \"early CNN and late Transformer at stage-level\" and \"early Transformer and late CNN at block-level\" strategy. This highlights a nuanced understanding of where and how to best deploy each architectural component for optimal hardware efficiency and performance. These block-level hybrid models demonstrate that a careful arrangement of convolutional and self-attention layers can yield superior latency-accuracy trade-offs across various vision tasks.\n\nAnother innovative direction involves fusing local and global processing within the Transformer block itself, drawing inspiration from CNNs' local inductive biases without necessarily using explicit convolutional layers. \\textit{PLG-ViT} \\cite{ebert202377v} introduces a \"Parallel Local-Global Vision Transformer\" that merges local window self-attention with global self-attention. This design efficiently represents short- and long-range spatial interactions, bypassing the need for computationally expensive operations like shifted windows while still achieving strong performance in image classification and dense prediction tasks. This approach reflects a deeper integration, where the *concept* of locality, traditionally associated with CNNs, is re-imagined and implemented within the Transformer's attention mechanism.\n\nIn summary, the landscape of synergistic CNN-Transformer designs is rich and diverse, reflecting various philosophies of integration. ConvNeXt V2 represents a CNN-centric approach enhanced by Transformer pre-training. InternImage and DAT/DAT++ showcase embedding CNN-inspired adaptive mechanisms into Transformer backbones. Next-ViT and TRT-ViT demonstrate strategic interleaving of CNN and Transformer blocks for deployment efficiency. PLG-ViT illustrates the internal fusion of local and global attention within a Transformer, mimicking CNN-like inductive biases. These models collectively demonstrate that moving beyond a strict CNN-Transformer dichotomy towards a complementary approach unlocks new levels of performance, efficiency, and versatility, pushing the boundaries of architectural design for a wide array of computer vision applications. Future research will likely explore even more sophisticated integration strategies, novel unified pre-training objectives, and dynamic architectural adaptations to further harness the combined power of these two foundational paradigms.\n\\subsection{Lightweight and Mobile-Optimized Vision Transformers}\n\\label{sec:5_2_lightweight__and__mobile-optimized_vision_transformers}\n\nBuilding upon the synergistic CNN-Transformer designs discussed in Section 5.1, this subsection delves into architectures specifically engineered for resource-constrained mobile and edge devices. The deployment of Vision Transformers (ViTs) on platforms such as smartphones, IoT devices, and embedded systems presents significant challenges due to their typically high computational cost, large memory footprint, and substantial power consumption. Addressing these critical deployment challenges, a dedicated research thrust focuses on developing lightweight and mobile-optimized ViT architectures that maintain high accuracy while drastically reducing computational demands and parameter counts, making powerful ViT capabilities accessible for real-time and edge applications.\n\nA pioneering effort in this domain is \\textcite{mehta20216ad}'s MobileViT, which directly tackles the trade-off between the spatial inductive biases and efficiency of Convolutional Neural Networks (CNNs) and the global representation learning capabilities of ViTs. MobileViT proposes a novel hybrid architecture that integrates standard convolutional layers for local feature extraction with a lightweight Transformer block that processes information globally, effectively treating \"transformers as convolutions.\" This design allows MobileViT to achieve impressive accuracy, such as 78.4\\% top-1 on ImageNet-1k with approximately 6 million parameters, significantly outperforming both CNN-based (e.g., MobileNetv3) and ViT-based (e.g., DeiT) counterparts for similar parameter budgets. Its ability to learn global representations while retaining the efficiency of local processing makes it highly suitable for mobile vision tasks, demonstrating a foundational approach to lightweight hybrid ViTs.\n\nFurther advancing the field of deployable hybrid architectures, \\textcite{li2022a4u} introduce Next-ViT, explicitly designed for efficient deployment in realistic industrial scenarios, considering factors like TensorRT and CoreML inference latency. Next-ViT proposes a \"Next Convolution Block (NCB)\" and a \"Next Transformer Block (NTB)\" that are deployment-friendly, capturing local and global information respectively. The \"Next Hybrid Strategy (NHS)\" then efficiently stacks these blocks. Unlike many ViTs that optimize for FLOPs or parameter count, Next-ViT prioritizes actual inference speed on target hardware, achieving substantial latency reductions while maintaining superior accuracy. For instance, it surpasses ResNet by 5.5 mAP on COCO detection and accelerates inference speed by 3.6x compared to CSWin under similar performance, highlighting its practical utility for edge computing where real-world latency is paramount. This represents a critical evolution from merely lightweight designs to truly deployment-optimized ones.\n\nBeyond architectural hybridization, another crucial avenue for efficiency lies in redesigning the attention mechanism itself to mitigate its quadratic computational complexity. \\textcite{chen2021r2y} introduce CrossViT, which focuses on learning multi-scale feature representations within a dual-branch transformer framework. CrossViT processes image patches of different sizes in separate branches and then fuses these multi-scale tokens using a highly efficient cross-attention mechanism. Crucially, their cross-attention module is designed to operate with linear computational and memory complexity, rather than the quadratic complexity typical of standard self-attention, thereby mitigating a major bottleneck for deploying ViTs on resource-limited hardware. This approach not only enhances representational power by leveraging features at various scales but also ensures that the fusion process remains computationally tractable. Complementing this, \\textcite{song20215tk} propose UFO-ViT (Unit Force Operated Vision Transformer), which offers a novel self-attention mechanism with linear complexity by eliminating non-linearity and factorizing matrix multiplication without complex linear approximations. While CrossViT achieves linear scaling through a specialized cross-attention for multi-scale fusion, UFO-ViT fundamentally re-engineers the self-attention block itself to achieve linear complexity, offering a more general solution for reducing the computational burden of attention in any ViT layer.\n\nFinally, while architectural and algorithmic innovations are vital, the ultimate efficiency on edge devices often necessitates hardware-aware optimization. \\textcite{nag2023cfn} address this by proposing ViTA, a configurable hardware accelerator specifically designed for inference of Vision Transformer models on highly resource-constrained edge computing devices. ViTA employs a head-level pipeline and inter-layer MLP optimizations to avoid repeated off-chip memory accesses, a common bottleneck in embedded systems. It supports various ViT models with changes solely in its control logic, achieving nearly 90\\% hardware utilization efficiency and reasonable frame rates at low power consumption (e.g., 0.88W at 150 MHz). This demonstrates that for true mobile optimization, a holistic approach combining efficient model design with dedicated hardware acceleration is indispensable, pushing the boundaries of what is achievable on the edge.\n\nCollectively, these works demonstrate a clear progression towards making powerful Vision Transformer capabilities accessible for real-time and edge applications. They highlight the importance of hybrid architectures that judiciously combine CNN and Transformer strengths \\cite{mehta20216ad, li2022a4u}, efficient attention mechanisms that scale linearly with input size \\cite{chen2021r2y, song20215tk}, and the critical role of hardware-software co-design for practical deployment \\cite{nag2023cfn}. Despite these significant advancements, ongoing challenges include developing more sophisticated hardware-aware neural architecture search methods that consider power and memory constraints alongside latency, exploring novel quantization and pruning strategies tailored for these complex hybrid designs, and investigating dynamic execution strategies to adapt to varying computational budgets on the fly.\n\\subsection{Novel Hybrid Paradigms}\n\\label{sec:5_3_novel_hybrid_paradigms}\n\n\nThe continuous quest for more efficient, scalable, and powerful vision models has propelled research beyond the established CNN-Transformer hybrid architectures, leading to the exploration of entirely new component integrations. This subsection delves into novel hybrid paradigms that fuse distinct neural network components, moving beyond convolutional and self-attention mechanisms, to leverage their complementary strengths for enhanced performance and efficiency, particularly in modeling long-range dependencies and global context. Such approaches signify a dynamic and evolving research frontier, continuously pushing the boundaries of what is achievable in visual representation learning by incorporating components inspired by advancements in sequence modeling.\n\nA significant recent development in this direction is the integration of State-Space Models (SSMs) into vision backbones, exemplified by architectures like MambaVision \\cite{hatamizadeh2024xr6}. Originating from control theory and recently revitalized for deep learning, SSMs offer an alternative mechanism for efficient sequence modeling, particularly adept at capturing long-range dependencies with linear computational complexity. MambaVision, proposed by Hatamizadeh et al. \\cite{hatamizadeh2024xr6}, presents a hybrid architecture that integrates the Mamba SSM with Vision Transformers (ViT). The core motivation behind MambaVision is to harness Mamba's inherent efficiency in sequence modeling and its superior capacity for capturing long-range dependencies, while simultaneously retaining the robust global context understanding capabilities characteristic of Transformers. This fusion offers a compelling new perspective on designing vision backbones that can efficiently process extensive visual information, addressing some of the quadratic complexity issues of pure self-attention.\n\nHatamizadeh et al. \\cite{hatamizadeh2024xr6} meticulously redesigned the Mamba formulation to optimize its performance specifically for visual features, addressing the unique challenges posed by image data. Through comprehensive ablation studies, they demonstrated the feasibility and substantial benefits of integrating ViT components within the Mamba framework. A pivotal finding was that the strategic incorporation of self-attention blocks in the final layers of the Mamba architecture significantly enhanced its ability to capture intricate long-range spatial dependencies, a critical aspect for achieving high performance across diverse vision tasks. This hybrid design allows MambaVision to benefit from the local processing and linear scaling of Mamba while leveraging the global reasoning of attention where most critical. The efficacy of MambaVision is robustly supported by its empirical results across multiple benchmarks, achieving state-of-the-art (SOTA) performance on ImageNet-1K classification and demonstrating favorable performance in downstream tasks such as object detection, instance segmentation on MS COCO, and semantic segmentation on ADE20K, often outperforming comparably sized backbones.\n\nAnother emerging paradigm involves adapting Receptance Weighted Key Value (RWKV) models, initially developed for efficient large language models, to visual perception. Vision-RWKV (VRWKV), introduced by Duan et al. \\cite{duan2024q7h}, is a notable example. Transformers, while powerful, face limitations in high-resolution image processing due to their quadratic computational complexity. VRWKV addresses this by adapting the RWKV model, which processes sequences in a recurrent manner while maintaining a Transformer-like attention mechanism, but with significantly reduced spatial aggregation complexity. This design allows VRWKV to efficiently handle sparse inputs and demonstrate robust global processing capabilities, scaling effectively to large parameters and extensive datasets without the necessity for windowing operations often employed in hierarchical Transformers.\n\nDuan et al. \\cite{duan2024q7h} made necessary modifications to the RWKV architecture to tailor it for vision tasks, enabling it to function as an efficient and scalable visual backbone. Their evaluations demonstrated that VRWKV surpasses the performance of traditional Vision Transformers (ViTs) in image classification, while also exhibiting significantly faster speeds and lower memory usage when processing high-resolution inputs. Furthermore, in dense prediction tasks, VRWKV was shown to outperform window-based models, maintaining comparable speeds. These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks, particularly where high-resolution processing and long-context analysis are critical, without incurring the prohibitive computational costs of full self-attention.\n\nThe emergence of models like MambaVision and Vision-RWKV represents a significant paradigm shift, moving beyond the established dichotomy of CNN-Transformer integrations. Both approaches aim to address the computational burdens associated with pure self-attention mechanisms while preserving or enhancing powerful global reasoning capabilities. MambaVision leverages the linear scaling of SSMs for efficient long-range dependency modeling, strategically reintroducing self-attention in later layers to refine global context. In contrast, Vision-RWKV adapts a distinct recurrent-attention mechanism from NLP, offering reduced spatial aggregation complexity and superior high-resolution processing without explicit windowing. While MambaVision focuses on a hybrid SSM-ViT structure, VRWKV presents a more direct adaptation of an efficient sequence model, emphasizing its unique recurrent-attention mechanism. Both paradigms demonstrate the immense potential of exploring non-traditional neural network components and their synergistic combinations for advancing the field of computer vision. Their success unequivocally highlights the value of cross-domain inspiration, particularly from efficient sequence models in NLP, to develop more efficient, powerful, and specialized vision models for future applications.\n",
    "Applications of Visual Transformers": "\\section{Applications of Visual Transformers}\n\\label{sec:applications_of_visual_transformers}\n\n\n\n\\subsection{Core Vision Tasks: Classification and Object Detection}\n\\label{sec:6_1_core_vision_tasks:_classification__and__object_detection}\n\n\nVision Transformers (ViTs) have profoundly impacted fundamental computer vision tasks, particularly image classification and object detection, by leveraging their global receptive fields and powerful feature extraction capabilities. This architectural shift has enabled ViTs to capture both broad contextual information and fine-grained details, often leading to new performance benchmarks that frequently surpass traditional Convolutional Neural Networks (CNNs), especially when trained on extensive datasets.\n\nThe initial foray of Transformers into vision began with image classification. The seminal Vision Transformer (ViT) by \\cite{Dosovitskiy2021} demonstrated that a pure Transformer, by treating image patches as sequential tokens, could effectively classify images. While achieving state-of-the-art results on massive datasets like ImageNet-21K (e.g., ViT-L/16 reaching 88.55\\% top-1 accuracy), it initially exhibited a significant dependency on vast training data, often underperforming CNNs on smaller datasets. To mitigate this data-hungry nature, \\cite{Touvron2021} introduced Data-efficient Image Transformers (DeiT), employing knowledge distillation to enable ViTs to be trained efficiently on ImageNet-1K, achieving competitive performance (e.g., DeiT-S reaching 83.1\\% top-1 accuracy) without requiring massive pre-training. Concurrently, \\cite{Yuan2021} proposed Tokens-to-Token ViT (T2T-ViT), which refined the initial tokenization process to better represent local structures, thereby boosting performance on standard datasets from scratch. Further architectural advancements aimed at enhancing the depth and stability of ViTs for classification; \\cite{zhou202105h} identified the \"attention collapse\" issue in deeper ViTs and proposed Re-attention to increase the diversity of attention maps, enabling the training of significantly deeper models with consistent performance gains. More recently, `[ferdous2024f89]` introduced SPT-Swin, a variant that combines shifted patch tokenization with Swin Transformer to address data deficiency and computational complexity, achieving 89.45\\% accuracy on ImageNet-1K.\n\nA major breakthrough in efficient pre-training for classification came with \\cite{CVPR2022}, which introduced Masked Autoencoders (MAE). This self-supervised learning approach reconstructs masked image patches, proving highly scalable and data-efficient. MAE enables ViTs to learn robust representations from unlabeled data, significantly improving their performance and reducing the need for extensive labeled datasets for classification. For instance, a ViT-Large pre-trained with MAE on ImageNet-1K can achieve 87.8\\% top-1 accuracy. Building on this, the field has seen the emergence of \"Vision Foundation Models,\" with \\cite{ICLR2023} demonstrating the efficacy of scaling ViTs to over a billion parameters, achieving unprecedented classification performance through massive pre-training. Similarly, \\cite{ICLR2023} advanced self-supervised learning with DINOv2, which learns highly robust and generalizable visual features without supervision, often outperforming supervised pre-training in transfer tasks and further enhancing classification accuracy. The influence of ViT principles even extended to CNNs, as shown by \\cite{ICLR2023}, where ConvNeXt V2 leveraged MAE pre-training to significantly boost CNN performance, blurring the lines between the two architectures for classification tasks. For practical deployment, `[song2022603]` proposed CP-ViT, a cascade pruning framework that dynamically predicts sparsity in ViT models, reducing FLOPs by over 40\\% while maintaining accuracy, crucial for resource-limited devices.\n\nThe success of ViTs quickly extended to object detection, a more complex task requiring both accurate object categorization and precise localization. Early pure ViTs struggled with dense prediction tasks due to their fixed-size patch embeddings and lack of inherent hierarchical feature maps, which are crucial for detecting objects at various scales. This limitation was fundamentally addressed by the introduction of end-to-end Transformer detectors. The seminal work on DEtection TRansformer (DETR) by \\cite{Carion2020} revolutionized object detection by formulating it as a direct set prediction problem. DETR eliminated hand-designed components like Non-Maximum Suppression (NMS) by using a set of learned object queries and a bipartite matching loss. While DETR demonstrated the power of Transformers for detection, its slow convergence and high computational cost were initial drawbacks. These were largely overcome by Deformable DETR \\cite{Zhu2020}, which introduced deformable attention to focus on a small set of key sampling points, significantly accelerating training and improving performance, particularly for small objects.\n\nFollowing these foundational end-to-end Transformer detectors, architectural innovations focused on integrating hierarchical ViT backbones to better capture multi-scale visual features. The \\cite{Liu2021} Swin Transformer, detailed in `[liu2021ljs]`, revolutionized ViT applicability to dense prediction by proposing a hierarchical architecture with shifted window attention. This design limited self-attention computation to non-overlapping local windows while allowing for cross-window connections, effectively generating multi-scale feature maps. Swin Transformers became highly suitable as backbones for object detection frameworks, achieving state-of-the-art results, such as 58.7 box AP and 51.1 mask AP on COCO test-dev with Swin-Large. Similarly, \\cite{Wang2021} introduced the Pyramid Vision Transformer (PVT), a pure Transformer-based pyramid structure that progressively reduces feature map resolution, enabling the generation of multi-scale features essential for detecting objects of different sizes. These hierarchical backbones, when integrated with DETR-like heads, led to powerful detectors. For instance, `[wang2023bfo]` combined Deformable DETR with a Swin Transformer and a lightweight Feature Pyramid Network (FPN) to enhance detection accuracy for multi-scale targets, demonstrating a 6.1\\% improvement in accuracy on a classroom behavior dataset.\n\nInterestingly, even plain (non-hierarchical) ViT backbones, when properly leveraged, have shown strong performance in object detection. `[li2022raj]` explored the use of plain ViT backbones, pre-trained with MAE, for object detection. Their work, ViTDet, demonstrated that with minimal adaptations like a simple feature pyramid and limited cross-window propagation, these models could achieve competitive results on the COCO dataset, reaching up to 61.3 AP\\_box using only ImageNet-1K pre-training. This highlighted that the powerful representations learned by plain ViTs, especially through self-supervised pre-training, could be effectively fine-tuned for localization tasks, challenging the strict necessity of hierarchical backbones for all detection scenarios. The convergence of ViT backbones and end-to-end Transformer detectors is further exemplified by `[song2022y4v]`, which introduced ViDT, an extendable and efficient object detector integrating Vision and Detection Transformers. ViDT reconfigures the Swin Transformer as a standalone detector and employs an efficient Transformer decoder, achieving an excellent AP and latency trade-off on COCO. Furthermore, the advancements in large-scale ViT pre-training, such as those in \\cite{ICLR2023} and \\cite{ICLR2023}, implicitly provide even more robust and generalizable backbones for object detection, allowing downstream detection models to achieve higher accuracy and better generalization across diverse scenarios.\n\nIn conclusion, Vision Transformers have established themselves as formidable architectures for core vision tasks. From their initial success in image classification, overcoming challenges related to data efficiency and architectural depth through innovations like DeiT and MAE, to their subsequent adaptation for object detection via end-to-end Transformer designs (DETR) and hierarchical backbones (Swin Transformer), ViTs have consistently pushed performance boundaries. While initial pure ViTs were data-hungry and lacked inherent inductive biases for dense prediction, the continuous evolution has led to more efficient, robust, and scalable models. The ongoing development of massive \"Vision Foundation Models\" and sophisticated hybrid architectures, leveraging advanced self-supervised learning, signifies a future where increasingly versatile visual intelligence can be deployed across an even wider spectrum of real-world applications, further solidifying ViTs' foundational strength in capturing both global context and fine-grained details.\n\\subsection{Dense Prediction Tasks: Segmentation and Pose Estimation}\n\\label{sec:6_2_dense_prediction_tasks:_segmentation__and__pose_estimation}\n\nDense prediction tasks, encompassing semantic segmentation, instance segmentation, and human pose estimation, demand pixel-level understanding and precise localization, moving significantly beyond image-level classification. Vision Transformers (ViTs), with their inherent ability to model long-range dependencies, have emerged as powerful tools in these fine-grained visual analysis tasks. However, the direct application of vanilla ViTs was initially hindered by their high computational cost, quadratic complexity with respect to image resolution, and lack of inherent multi-scale feature representation, which is crucial for pixel-level tasks \\cite{li2023287}.\n\nTo address these limitations, hierarchical ViT architectures quickly emerged as a foundational solution. The \\textit{Swin Transformer} \\cite{liu2021swin} introduced shifted window attention, enabling efficient computation by restricting self-attention within local windows while allowing cross-window connections through shifting. This hierarchical design effectively generates multi-scale feature maps, making it highly suitable as a backbone for dense prediction. Similarly, the \\textit{Pyramid Vision Transformer} (PVT) \\cite{wang2021pyramid} explicitly designed a pyramid structure to produce multi-scale features, directly mimicking the feature pyramid networks (FPNs) commonly used in dense prediction, thereby facilitating their integration into existing frameworks. Further building on this, \\textit{HiViT} \\cite{zhang2022msa} proposed another hierarchical ViT design, demonstrating improved efficiency and performance for downstream tasks like detection and segmentation when pre-trained with masked image modeling (MIM). These hierarchical designs were pivotal, providing the necessary multi-scale representations that pure ViTs lacked, thereby enabling more coherent and accurate pixel-wise predictions.\n\nAs ViT architectures matured, research consolidated strategies for their adaptation to pixel-level tasks, broadly falling into two categories: ViT encoders paired with specialized decoders, and end-to-end Transformer designs. Early and effective approaches often paired hierarchical ViT backbones with established CNN-style decoders. For instance, studies demonstrated the efficacy of integrating Swin Transformer with decoders like U-Net, Pyramid Scene Parsing (PSP) network, or Feature Pyramid Network (FPN) for semantic segmentation, particularly in domains like remote sensing \\cite{panboonyuen20218r7}. This hybrid approach leverages the ViT's strong global feature extraction while benefiting from the CNN decoder's inductive biases for local detail and spatial upsampling. In contrast, end-to-end Transformer designs sought to eliminate reliance on hand-crafted CNN components. \\textit{Max-DeepLab} \\cite{wang2021maxdeeplab} showcased the capacity of ViTs to directly learn pixel-level representations and segmentations for semantic segmentation, leveraging the global receptive field of Transformers to capture broader contextual cues and produce consistent masks. A significant conceptual advance for instance segmentation (and panoptic/semantic segmentation) was \\textit{Mask2Former} \\cite{cheng2022mask2former}, which unified these tasks under a single query-based Transformer framework. Mask2Former uses a masked attention mechanism and a set of learnable queries to directly predict object masks and their classes, demonstrating superior performance on complex scenes and representing a paradigm shift towards more unified, global reasoning for pixel-level tasks.\n\nThe versatility of ViTs extends to other dense prediction tasks beyond segmentation. For human pose estimation, which involves localizing keypoints on human bodies, models like \\textit{ViTPose} \\cite{xu2022vitpose} have showcased how ViTs can effectively model spatial relationships between body parts. Their global attention mechanism is crucial for inferring occluded or less visible keypoints based on overall body context, often outperforming CNN-based methods. Furthermore, ViTs have been adapted for specialized segmentation challenges. For example, \\textit{SENet} \\cite{hao202488z} proposes a simple yet effective ViT-based encoder-decoder for camouflaged and salient object detection, demonstrating how ViTs can be tailored for specific pixel-level understanding problems. In the medical imaging domain, \\textit{SwinBTS} \\cite{jiang2022zcn} leverages the Swin Transformer for 3D multimodal brain tumor segmentation, highlighting ViT's applicability in critical, high-dimensional analysis.\n\nThe recent advancements in foundational Vision Transformers (as discussed in Section 4) have profoundly amplified their impact on dense prediction. Models pre-trained with self-supervised learning strategies like Masked Autoencoders (MAE) \\cite{he2022masked} or advanced knowledge distillation (e.g., DINOv2 \\cite{oquab2023dinov2}) provide exceptionally robust and generalizable backbones. When fine-tuned for dense prediction tasks, these large-scale models, often paired with task-specific decoders (e.g., UPerNet heads or Mask2Former heads), achieve unprecedented accuracy and efficiency, significantly reducing the need for extensive labeled data for downstream tasks. Moreover, hybrid architectures (as detailed in Section 5) have further refined feature extraction for dense prediction. Models like \\textit{InternImage} \\cite{wang2023internimage} and \\textit{ConvNeXt V2} \\cite{woo2023convnext}, which integrate deformable convolutions or enhance CNNs with MAE pre-training, provide backbones that combine the inductive biases of CNNs (e.g., locality, efficiency) with the global context modeling of Transformers. More recently, hybrid Mamba-Transformer backbones like \\textit{MambaVision} \\cite{hatamizadeh2024xr6} have also demonstrated strong performance on instance and semantic segmentation, showcasing new avenues for efficient long-range dependency modeling. Even efficient variants like \\textit{MobileViT V2} \\cite{vaswani2023mobilevit} aim to bring these powerful local-global feature interactions to resource-constrained environments, broadening the practical applicability of ViTs for real-time dense prediction.\n\nIn conclusion, Vision Transformers have firmly established themselves as formidable architectures for dense prediction tasks. Their evolution from basic image classifiers to sophisticated pixel-level understanding models has been driven by architectural innovations that address computational complexity and multi-scale representation, coupled with specialized decoder designs and end-to-end Transformer frameworks. The recent advent of large-scale, self-supervised pre-trained foundation models has further empowered ViTs, providing exceptionally robust and generalizable backbones that significantly enhance performance across semantic segmentation, instance segmentation, and pose estimation. Future research will likely focus on optimizing the fusion of local and global features, developing more efficient architectures for real-time dense prediction, and exploring novel ways to leverage the rich contextual understanding of foundation models for even finer-grained and more complex pixel-level analyses.\n\\subsection{Specialized and Multimodal Applications}\n\\label{sec:6_3_specialized__and__multimodal_applications}\n\n\nBeyond their foundational success in standard image classification and object detection, Vision Transformers (ViTs) have demonstrated remarkable versatility and adaptability across a diverse array of specialized and complex application areas. This subsection highlights their profound utility in challenging real-world scenarios, ranging from low-level image processing to the analysis of high-dimensional and multimodal data, showcasing their capacity to transcend traditional benchmarks and open new avenues for research and deployment.\n\nOne significant domain where ViTs have excelled is image restoration, a crucial low-level vision task demanding precise pixel-level manipulation and global consistency. Traditional convolutional neural networks (CNNs) often struggle with capturing long-range dependencies essential for coherent restoration across an entire image. \\textcite{liang2021v6x} addressed this by introducing SwinIR, a robust baseline for tasks such as super-resolution, denoising, and JPEG compression artifact reduction. SwinIR leverages the hierarchical Swin Transformer, which, through its shifted window attention, efficiently captures both local details and global structural information. This hierarchical design allows SwinIR to effectively model non-local correlations within images, crucial for hallucinating high-frequency details in super-resolution or removing noise while preserving fine textures, ultimately achieving superior performance over state-of-the-art CNNs with a reduced parameter count. Similarly, for visual saliency detection, which requires identifying the most visually prominent regions at a pixel level, \\textcite{liu2021jpu} proposed the Visual Saliency Transformer (VST). VST re-conceptualizes saliency detection as a convolution-free sequence-to-sequence prediction problem. By employing multi-level token fusion and a novel token upsampling method, VST effectively processes both RGB and RGB-D inputs, demonstrating ViT's inherent capability for fine-grained pixel-level understanding and its adaptability to multimodal inputs through a token-based multi-task decoder. The global attention mechanism of VST allows it to integrate contextual information across the entire image, which is vital for distinguishing salient objects from complex backgrounds, a task where local CNN receptive fields might fall short.\n\nThe ability of ViTs to process and fuse information from diverse and high-dimensional data modalities further underscores their adaptability. For hyperspectral image classification (HSIC), which involves analyzing data cubes with hundreds of spectral bands alongside spatial information, ViTs offer a powerful alternative to traditional methods. HSIC presents unique challenges due to its high dimensionality, spectral redundancy, and often limited labeled samples. \\textcite{zhao2024671} introduced the Groupwise Separable Convolutional Vision Transformer (GSC-ViT) to address these issues. GSC-ViT integrates a Groupwise Separable Convolution (GSC) module to efficiently extract local spectral-spatial features, mitigating the parameter burden of pure ViTs and enhancing local representation. Concurrently, a Groupwise Separable Multihead Self-Attention (GSSA) module captures both local and global spatial feature dependencies, allowing the model to effectively learn intricate relationships across the spectral and spatial dimensions. This hybrid approach demonstrates surprising classification performance even with fewer training samples, highlighting the ViT's capacity to model complex, high-dimensional data by judiciously combining inductive biases.\n\nBeyond visual imagery, ViTs have proven effective in processing non-standard data types, such as radar signals for human activity recognition (HAR). HAR from radar data is challenging due to the abstract nature of micro-Doppler signatures, which represent subtle motion patterns. \\textcite{huan202345b} proposed a Lightweight Hybrid Vision Transformer (LH-ViT) network for radar-based HAR. While the architectural details of LH-ViT are discussed in Section 5.2, its application here demonstrates how ViTs can effectively capture global temporal patterns within micro-Doppler spectrograms, which are crucial for distinguishing different human activities. The global attention mechanism allows the model to correlate features across the entire time-frequency representation, providing a more holistic understanding of the activity compared to local processing methods. The emphasis on a lightweight design further underscores the practical considerations for deploying such models in real-time or embedded radar systems.\n\nFurthermore, ViTs are increasingly pivotal in multimodal data fusion, where information from heterogeneous sources must be synergistically combined for robust understanding. For multimodal land use and land cover (LULC) classification, which often involves integrating optical, hyperspectral, LiDAR, and Synthetic Aperture Radar (SAR) data, \\textcite{yao2023sax} developed the Extended Vision Transformer (ExViT). This framework extends conventional ViTs with parallel branches, each tailored for a specific modality (e.g., hyperspectral and LiDAR/SAR data), utilizing position-shared ViTs and separable convolutions for efficient feature extraction. Critically, ExViT employs a cross-modality attention (CMA) module to facilitate dynamic information exchange and alignment between these heterogeneous modalities. This deep fusion mechanism, culminating in a robust decision-level fusion, leverages the ViT's token-based architecture to seamlessly integrate diverse data representations, leading to significantly improved classification accuracy and robustness compared to single-modality or simpler fusion approaches.\n\nCollectively, these applications underscore the remarkable versatility of Vision Transformers. They illustrate how ViTs can be meticulously engineered, often through hybrid architectures, specialized attention mechanisms, and novel tokenization strategies, to effectively process diverse data typesfrom standard images and spectral cubes to radar micro-Doppler mapsand tackle complex tasks like restoration, saliency detection, high-dimensional classification, and multimodal fusion. The continuous drive to optimize ViTs for efficiency and integrate them with domain-specific inductive biases (e.g., convolutions for local features) highlights a key development direction. Future research will likely focus on further enhancing their efficiency for edge deployment, exploring more sophisticated multimodal fusion strategies, and adapting them to novel data modalities and real-time inference challenges in increasingly complex environments, thereby expanding their utility across an even broader spectrum of scientific and industrial applications.\n",
    "Future Outlook: Challenges and Opportunities": "\\section{Future Outlook: Challenges and Opportunities}\n\\label{sec:future_outlook:_challenges__and__opportunities}\n\n\n\n\\subsection{Persistent Challenges: Computational Cost and Data Efficiency}\n\\label{sec:7_1_persistent_challenges:_computational_cost__and__data_efficiency}\n\n\nDespite the remarkable progress in Vision Transformers (ViTs) across diverse computer vision tasks, their substantial computational cost and persistent reliance on extensive datasets remain critical hurdles, dictating their broader adoption and sustainability, particularly in resource-constrained environments and for specialized applications. While earlier sections have detailed numerous advancements, these challenges are far from resolved, continuously driving innovation to balance performance with practicality.\n\nThe foundational ViT \\cite{Dosovitskiy2021} introduced a quadratic computational complexity with respect to image resolution due to its global self-attention mechanism, alongside a significant appetite for massive pre-training datasets like JFT-300M. Subsequent architectural innovations, such as the hierarchical Swin Transformer \\cite{Liu2021} and Pyramid Vision Transformer (PVT) \\cite{Wang2021}, mitigated this by restricting attention to local windows or employing spatial reduction. Similarly, advanced attention mechanisms like Focal Attention \\cite{FocalAttention2022} aimed to capture context efficiently. However, these solutions often introduce trade-offs; local attention, while efficient, can sacrifice the model's inherent ability to capture true global dependencies, which was a core advantage of the original Transformer. Furthermore, even linear complexity can be prohibitive for processing gigapixel images in domains like digital pathology, highlighting that architectural fixes alone are insufficient.\n\nThe quest for computational efficiency extends beyond architectural design into post-training optimization and hardware-aware deployment. Model quantization, which reduces the precision of weights and activations, is crucial for inference on edge devices. Yet, ViTs present unique challenges for quantization due to the sensitivity of components like Layer Normalization and the non-uniform distribution of attention maps \\cite{lin2021utw}. While methods like Q-ViT \\cite{li20229zn} and FQ-ViT \\cite{lin2021utw} have pushed the limits of fully differentiable and post-training quantization, achieving near-lossless accuracy at lower bit-widths, the inherent complexity of ViT operations still demands specialized techniques to prevent severe performance degradation. Concurrently, model pruning techniques, which remove redundant parameters or operations, are vital. Research explores multi-dimensional compression, pruning attention heads, neurons, and even input sequences \\cite{song2022603, hou2022ver, yin2023029}. These methods, such as CP-ViT \\cite{song2022603} and GOHSP \\cite{yin2023029}, aim to identify and remove deleterious components while preserving accuracy, but the challenge lies in developing robust, generalized pruning criteria that do not require extensive retraining or compromise the model's representational power. Ultimately, for deployment in highly resource-constrained environments, dedicated hardware accelerators like ViTA \\cite{nag2023cfn} are becoming indispensable, demonstrating that a holistic hardware-software co-design approach is necessary to truly overcome the computational bottleneck.\n\nData efficiency, another critical challenge, has seen significant breakthroughs with self-supervised learning (SSL) and knowledge distillation. Data-efficient Image Transformers (DeiT) \\cite{Touvron2021} demonstrated competitive performance with ImageNet-1K pre-training through distillation from a CNN teacher, while Tokens-to-Token ViT (T2T-ViT) \\cite{Yuan2021} improved initial tokenization. The advent of Masked Autoencoders (MAE) \\cite{he2022masked} and advanced self-distillation techniques like DINOv2 \\cite{oquab2023dinov2} further revolutionized pre-training by enabling ViTs to learn robust features from vast quantities of *unlabeled* data. However, even with these advancements, the \"data hunger\" persists in different forms. Large foundation models, while powerful, still necessitate colossal datasets for pre-training, which may not be universally accessible or ethically diverse. Moreover, adapting these general-purpose models to specialized domains (e.g., medical imaging, remote sensing) where labeled data is scarce remains a significant hurdle. For instance, MAT-VIT \\cite{han2024f96} explores MAE-based auxiliary tasks to leverage unlabeled medical images, highlighting the ongoing need for domain-specific data efficiency strategies.\n\nThe traditional \"pre-train and fine-tune\" paradigm, while effective, also presents challenges. Training ViT-based object detectors from scratch, as explored by \\cite{hong2022ks6}, reveals that simply switching backbones from CNNs to ViTs does not generalize well, emphasizing the deep reliance of ViTs on large-scale pre-training. Furthermore, optimization techniques commonly used in deep learning do not always translate seamlessly to ViTs; for example, gradient accumulation, often used to simulate larger batch sizes, was found to decrease accuracy and increase training time for Swin Transformers \\cite{aburass2023qpf}, underscoring the need for ViT-specific optimization strategies. Even in knowledge distillation, researchers continue to refine techniques, with methods like Attention Distillation \\cite{wang2022pee} showing that self-supervised ViT students require more nuanced guidance to effectively close the performance gap with teachers. The emergence of \"simple\" hierarchical ViTs like Hiera \\cite{ryali202339q}, which strip away architectural \"bells-and-whistles\" when combined with strong SSL, further suggests that the true drivers of efficiency and performance might lie in the pre-training strategy rather than complex architectural designs.\n\nIn conclusion, the computational cost and data efficiency of Vision Transformers are not static problems but dynamic challenges that evolve with architectural innovations and deployment contexts. While significant strides have been made through hierarchical designs, advanced attention, quantization, pruning, and self-supervised learning, these solutions often introduce new trade-offs or highlight the need for further refinement. The continuous efforts to develop more parameter-efficient models, optimize training strategies, and improve generalization from limited data, especially for specialized tasks and resource-constrained environments, remain a fertile and critical ground for future research, pushing towards truly sustainable and ubiquitous visual AI.\n\\subsection{Interpretability, Robustness, and Generalization}\n\\label{sec:7_2_interpretability,_robustness,__and__generalization}\n\nThe trustworthy deployment of Vision Transformers (ViTs) in real-world applications critically hinges on their interpretability, robustness, and generalization capabilities. These aspects are paramount for fostering confidence in AI systems, ensuring their reliable and safe operation in diverse environments, and upholding ethical standards, especially in sensitive domains.\n\nA fundamental challenge in ViTs, as with many deep learning models, lies in interpreting their complex decision-making processes. Unlike Convolutional Neural Networks (CNNs) where feature maps often correspond to spatially localized patterns, the global self-attention mechanism in Transformers makes direct interpretation more elusive. Early attempts to interpret ViTs often relied on visualizing raw attention maps, but this approach has been critiqued for its limitations; raw attention scores do not directly represent feature importance or causal contributions to the output \\cite{jain2019attention}. More rigorous explainable AI (XAI) methods adapted for Transformers include attention rollout \\cite{abnar2020quantifying}, which propagates attention through layers to aggregate relevance, and Layer-wise Relevance Propagation (LRP) \\cite{bach2015pixel}, which decomposes the prediction backward through the network to assign relevance scores to input pixels. Gradient-based attribution methods, such as Grad-CAM \\cite{selvaraju2017grad} and its variants, have also been applied to ViTs to highlight salient regions influencing decisions. While these methods offer valuable insights, a comprehensive, human-understandable explanation of ViT reasoning, particularly in safety-critical applications, remains an active research area. Furthermore, some studies, such as \\cite{wang2022da0}, suggest that the attention mechanism itself might not be the sole or even primary driver of ViT success, demonstrating that it can be replaced by simpler shift operations with comparable performance. This raises questions about the true mechanistic role of attention and, consequently, the validity of solely relying on attention-based explanations. Efforts like Re-attention \\cite{zhou202105h} aimed to diversify attention maps in deeper ViTs to prevent \"attention collapse\" and improve representation learning, which indirectly aids in making attention patterns more informative, but does not fundamentally solve the interpretability challenge.\n\nBeyond interpretability, ensuring the robustness of ViTs against adversarial attacks and distribution shifts is crucial. Robustness is a multi-faceted concept, encompassing resilience to imperceptible adversarial perturbations, robustness to common image corruptions (e.g., noise, blur), and generalization to out-of-distribution (OOD) data. While initial ViTs demonstrated strong performance on benchmark datasets, their susceptibility to adversarial attacks was quickly identified, akin to CNNs \\cite{mao2021zr1, almalik20223wr}. For instance, \\textcite{almalik20223wr} proposed Self-Ensembling Vision Transformer (SEViT) to enhance adversarial robustness in medical image classification by leveraging intermediate feature representations and combining multiple classifiers.\n\nSignificant strides in improving ViT robustness and generalization have been made through advanced pre-training paradigms, particularly self-supervised learning (SSL). As discussed in Section 4, methods like Masked Autoencoders (MAE) \\cite{mae2022} and DINOv2 \\cite{dino2023} have enabled ViTs to learn powerful, transferable representations from vast amounts of unlabeled data. These SSL approaches force models to learn rich semantic features by reconstructing masked patches or performing self-distillation, leading to representations that are inherently more robust to variations and distribution shifts compared to purely supervised pre-training. For example, DINOv2 \\cite{dino2023} has shown remarkable performance on various downstream tasks and improved transferability, often outperforming supervised counterparts, by producing robust visual features without explicit supervision. The systematic evaluation by \\textcite{mao2021zr1} further highlights that certain ViT components can be detrimental to robustness, and by leveraging robust building blocks and techniques like position-aware attention scaling and patch-wise augmentation, they proposed Robust Vision Transformer (RVT) which achieved superior performance on benchmarks like ImageNet-C (common corruptions), ImageNet-R (renditions), and ImageNet-Sketch (sketches), demonstrating enhanced resilience to various distribution shifts.\n\nThe pursuit of \"Vision Foundation Models,\" as explored in Section 4.3, further exemplifies this drive towards universal robustness and generalization. Models scaled to billions of parameters, such as those by \\textcite{scalingvit2023} and \\textcite{internimage2023}, aim to serve as versatile backbones capable of adapting to a wide array of vision tasks with minimal fine-tuning. This extreme scaling, often coupled with advanced self-supervised pre-training, is hypothesized to imbue models with a deeper understanding of visual semantics, thereby enhancing their generalization to novel situations and improving their resilience to variations in input data.\n\nMoreover, the integration of inductive biases from CNNs into Transformer architectures has also contributed to developing more robust and generalizable models. Hybrid architectures, such as ConvNeXt V2 \\cite{convnextv22023} (discussed in Section 5.1), which co-designs CNNs with MAE pre-training, and InternImage \\cite{internimage2023} (also from Section 5.1), which incorporates deformable convolutions into large-scale vision foundation models, represent a synergistic approach. By combining the local feature extraction strengths of CNNs with the global context modeling of Transformers, these models aim to achieve a more comprehensive and robust understanding of visual data, often exhibiting better performance on out-of-distribution tasks. For instance, \\textcite{zhou2021rtn} systematically investigated the transfer learning ability of ConvNets and vision transformers across 15 downstream tasks, observing consistent advantages for Transformer-based backbones on 13 tasks, particularly noting their robustness in multi-task learning and their reliance on whole-network fine-tuning for optimal transfer. Practical considerations for real-world deployment also necessitate efficient architectures that maintain performance, contributing to operational robustness. While not directly addressing adversarial or OOD robustness, efficient ViT variants like MobileViT V2 \\cite{mobilevitv22023} (from Section 5.2) ensure reliable performance within resource constraints, which is a form of practical robustness for deployment.\n\nDespite significant strides in enhancing the robustness and generalization of ViTs through scaling, advanced self-supervised learning, and hybrid designs, the challenge of achieving true interpretability remains complex. While methods offer glimpses into model behavior, a holistic, causal understanding of ViT decisions, especially in safety-critical applications, is still an active research area. Future work must continue to explore novel techniques for transparent model design, robust evaluation against diverse adversarial threats and OOD data, and the development of ViTs that are not only high-performing but also inherently understandable and trustworthy.\n\\subsection{Future Trends and Open Problems}\n\\label{sec:7_3_future_trends__and__open_problems}\n\n\nThe evolution of Vision Transformers (ViTs) is rapidly accelerating, pushing the frontiers of artificial intelligence towards more comprehensive, intelligent, and human-like visual systems. This subsection looks ahead, identifying cutting-edge research directions and unresolved questions that will define the next generation of ViT architectures. While Section 7.1 and 7.2 address persistent challenges related to efficiency, robustness, and interpretability, this section focuses on more speculative, transformative trends and deeper, unsolved theoretical problems that extend beyond incremental improvements.\n\nA significant future trend lies in the exploration of **novel architectural paradigms that fundamentally rethink the self-attention mechanism**. While ViTs excel at global context modeling, their quadratic computational complexity remains a bottleneck. The integration of ViTs with State-Space Models (SSMs), such as Mamba, represents a particularly exciting frontier. MambaVision \\cite{hatamizadeh2024xr6} exemplifies this by redesigning the Mamba formulation for visual features and strategically incorporating self-attention blocks in later layers to effectively capture long-range spatial dependencies. This hybrid approach demonstrates state-of-the-art performance and throughput, suggesting a promising path for combining the strengths of different architectures to overcome individual limitations. Beyond such integrations, a more radical open question is whether attention is truly indispensable. Research like ShiftViT \\cite{wang2022da0} explores this by replacing attention layers with a zero-parameter shift operation, achieving competitive performance and prompting a re-evaluation of the core mechanisms driving ViT success. This suggests a future where attention might be replaced or heavily augmented by simpler, more efficient operations, as also highlighted by broader surveys on attention mechanism redesigns \\cite{heidari2024d9k}. Furthermore, the development of adaptive architectures that can dynamically adjust their computational patterns based on input characteristics, such as Dynamic Window Vision Transformer (DW-ViT) \\cite{ren2022ifo} which assigns windows of different sizes to various attention heads, offers a pathway to more flexible and robust visual understanding beyond fixed windowing.\n\nThe ultimate ambition for many researchers is the development of truly **universal and multimodal visual foundation models** capable of sophisticated reasoning. Building upon the success of large-scale pre-training \\cite{ICLR2023_Scaling_Vision_Transformers_to_1_Billion_Parameters, ICLR2023_DINOv2}, the field is now focused on creating generalist backbones that transcend individual tasks and modalities. GiT \\cite{wang20249qa} represents a significant step, proposing a vanilla ViT framework with a universal language interface that can simultaneously handle diverse vision tasksfrom image captioning to object detection and segmentationwithout task-specific modifications. This approach aims to narrow the architectural gap between vision and language models, fostering mutual enhancement across tasks. The extension of these models to genuinely multimodal reasoning, integrating vision not only with language but also with audio, 3D data, and other sensor modalities, is paramount. This requires novel architectural designs and pre-training strategies to effectively fuse information from disparate sources, enabling a more holistic and human-like understanding. For instance, unifying 2D and 3D vision remains a significant challenge, with efforts like Simple3D-Former \\cite{wang2022gq4} demonstrating that a standard 2D ViT can be minimally customized to perform robustly on 3D tasks, suggesting a path towards more transferable architectures. In practical applications, multimodal fusion is already yielding benefits, such as ViT-FuseNet \\cite{zhou2024toe} for vehicle-infrastructure cooperative perception (LiDAR and camera) and NF-DVT \\cite{pan20249k5} for monocular 3D object detection (RGB and depth), but the grand challenge is to generalize these fusion capabilities across *any* combination of modalities.\n\nBeyond architectural and multimodal advancements, several **deeper open problems in visual intelligence** persist, representing grand challenges for the field. One critical area is **compositional generalization and relational reasoning**. While ViTs can learn complex patterns, their ability to reason about novel combinations of objects and their relationships, similar to human cognition, remains limited. RelViT \\cite{ma2022vf3} offers a concept-guided approach to improve relational reasoning and systematic generalization, but achieving robust compositional understanding across diverse, unseen scenarios is a profound theoretical and practical hurdle. This involves understanding how ViTs can move beyond statistical correlations to infer causal relationships within visual scenes, a key component of true intelligence. Another significant challenge is **lifelong or continual learning**, where ViTs can adapt to new tasks and data streams without catastrophically forgetting previously acquired knowledge. This is essential for deploying models in dynamic real-world environments that evolve over time. Furthermore, integrating ViTs into **embodied AI systems** that interact with the physical world, requiring real-time perception, planning, and action, presents complex challenges in bridging the gap between static image understanding and dynamic environmental interaction.\n\nFinally, as ViTs become increasingly powerful and ubiquitous, the **ethical implications and the need for trustworthy AI systems** become paramount \\cite{hassija2025wq3}. While Section 7.2 discusses interpretability and robustness as ongoing challenges, the future demands a proactive approach to designing ViTs that are inherently fair, transparent, and privacy-preserving. Developing generalizable interpretability methods for massive, multimodal foundation models is a major open problem. For instance, prototype-based interpretable ViTs like ProtoViT \\cite{ma2024uan} offer local explanations by comparing image parts to learned prototypes, but scaling such fine-grained interpretability to the complexity of universal foundation models remains an active research area. Mitigating biases embedded in training data, ensuring equitable performance across diverse demographic groups, and developing robust privacy-preserving mechanisms for visual data are not just technical hurdles but critical societal imperatives for the responsible deployment of future vision systems.\n\nIn conclusion, the future of Vision Transformers is characterized by a relentless pursuit of greater efficiency through novel architectural paradigms, the development of truly generalist and multimodal foundation models, and a deeper engagement with fundamental problems of visual intelligence such as compositional and causal reasoning. Crucially, addressing the ethical dimensions of these powerful systems will be indispensable for realizing their transformative potential in a responsible and beneficial manner.\n"
  },
  "subsections": {
    "Motivation for Visual Transformers": "\\subsection*{Motivation for Visual Transformers}\n\nThe advent of Vision Transformers (ViTs) marks a significant paradigm shift in computer vision, primarily driven by the inherent limitations of Convolutional Neural Networks (CNNs) in capturing global contextual relationships and the groundbreaking success of Transformer architectures in Natural Language Processing (NLP). For an extended period, CNNs were the undisputed standard for visual recognition tasks, owing to their powerful inductive biases such as locality and translation equivariance. These biases, intrinsically embedded through spatially restricted convolutional kernels, are exceptionally effective at extracting local features and constructing hierarchical representations \\cite{han2020yk0}. However, this very strength became a critical constraint when models needed to comprehend long-range dependencies and holistic contextual information across an entire image. CNNs, by design, inherently struggle to model interactions between spatially distant parts of an image without resorting to increasingly deep architectures, significantly larger receptive fields achieved through techniques like dilated convolutions, or complex add-on modules such as non-local blocks \\cite{Wang2018NonlocalNN, zhou2021rtn}. While these methods attempted to mitigate the issue by expanding the effective receptive field, they often introduced increased computational complexity or did not fundamentally alter the local processing paradigm. As highlighted by \\cite{gheflati202131i}, CNNs' restricted local receptive fields inherently limit their capacity for global context learning, making it challenging to capture comprehensive image understanding. Similarly, \\cite{liu2022249} and \\cite{karagz2024ukp} explicitly note that while CNNs excel at representing local spatial features, they find it difficult to capture global information, underscoring a fundamental gap in their representational power for certain tasks.\n\nThe landscape of deep learning was profoundly reshaped by the Transformer architecture, introduced by \\cite{Vaswani2017} in NLP. This model demonstrated the unparalleled power of self-attention mechanisms to capture global dependencies within sequences, processing information from all input parts simultaneously. Unlike recurrent networks that process sequentially or CNNs with their localized focus, the Transformer's global attention mechanism, largely devoid of strong inductive biases about local connectivity, offered a compelling alternative for learning flexible, context-aware representations. The remarkable ability of Transformers to achieve state-of-the-art performance across diverse language tasks inspired researchers to question whether a similar global approach could unlock new capabilities in computer vision, overcoming the limitations of CNNs in capturing holistic image understanding and offering a less constrained path to feature learning \\cite{han2020yk0}. This inspiration was rooted in the desire for models that could intrinsically understand the relationships between any two parts of an image, regardless of their spatial separation, without being constrained by fixed-size kernels.\n\nThis profound inspiration culminated in the seminal work by \\cite{Dosovitskiy2021}, which introduced the Vision Transformer (ViT). This pioneering paper directly adapted the pure Transformer architecture to image recognition by treating image patches as a sequence of tokens, demonstrating that a model built entirely on self-attention, when trained on sufficiently large datasets, could achieve state-of-the-art performance without any convolutional layers. This was a profound challenge to the long-held paradigm of convolutional feature extraction, showcasing that global attention could indeed learn powerful visual representations and capture long-range dependencies more effectively than CNNs. The ability of ViTs to learn highly transferable representations, often outperforming CNNs on various downstream tasks when properly pre-trained, further solidified its revolutionary potential \\cite{zhou2021rtn}. For instance, studies like \\cite{htten2022lui} have shown that ViT models can achieve equivalent or even superior performance to CNNs in complex industrial visual inspection tasks, even with sparse data, highlighting their robust feature learning capabilities. This successful adaptation of a pure Transformer to vision tasks marked a definitive turning point, setting the stage for a new architectural lineage in computer vision that prioritizes global context and flexible feature learning.",
    "Overview of Vision Models: From CNNs to Transformers": "\\subsection{Overview of Vision Models: From CNNs to Transformers}\n\nThe trajectory of computer vision research has been marked by a significant paradigm shift, evolving from the long-standing dominance of Convolutional Neural Networks (CNNs) to the recent ascendance of Transformer architectures. For decades, CNNs served as the foundational backbone for visual recognition tasks, owing their success to inherent inductive biases that align well with the hierarchical and local nature of visual data \\cite{han2020yk0}. Key among these biases are local receptive fields, which allow neurons to process only a small, localized region of the input; weight sharing, which enables the detection of features regardless of their position; and spatial pooling, which progressively reduces spatial dimensions while retaining essential information. Architectures like AlexNet \\cite{alexnet_2012}, VGG \\cite{vgg_2014}, ResNet \\cite{resnet_2016}, and Inception \\cite{inception_2015} exemplified this era, demonstrating remarkable capabilities in tasks ranging from image classification to object detection by building increasingly complex and abstract representations through stacked convolutional layers. Their architectural evolution often involved increasing depth, introducing residual connections to mitigate vanishing gradients, and designing more intricate modules to enhance representational power, all while retaining the core principle of localized feature extraction.\n\nDespite their profound success and continuous advancements, CNNs inherently faced limitations, particularly in effectively modeling global contextual information across an entire image. Their reliance on local operations meant that capturing long-range dependencies required very deep networks, which could be computationally intensive and sometimes struggled to integrate information from widely separated regions efficiently. This inductive bias towards locality, while beneficial for many tasks, could also constrain their ability to understand broader semantic relationships or complex spatial layouts without extensive architectural modifications \\cite{han2020yk0, zhou2021rtn}.\n\nA pivotal moment arrived with the introduction of the Transformer architecture \\cite{attention_is_all_you_need_2017}, originally conceived for natural language processing (NLP). Its core innovation, the self-attention mechanism, revolutionized sequence modeling by enabling models to weigh the importance of different parts of an input sequence, fostering a more global understanding of relationships without relying on recurrent or convolutional operations. This mechanism allowed for parallel processing of inputs and offered an unprecedented ability to capture long-range dependencies directly. The profound success of Transformers in NLP inspired researchers to explore their applicability to other domains, including computer vision \\cite{heidari2024d9k}.\n\nThis exploration culminated in the introduction of the Vision Transformer (ViT) \\cite{image_is_worth_16x16_words_2021}. The ViT architecture ingeniously adapted the standard Transformer encoder for image recognition by treating images as sequences of non-overlapping patches. Each patch was linearly embedded into a token, and positional embeddings were added to retain spatial information. These visual tokens were then fed into a Transformer encoder, leveraging its global self-attention mechanism to capture relationships between patches across the entire image. This marked a significant paradigm shift, demonstrating that models could achieve competitive performance in image recognition without relying on convolutional layers, instead directly leveraging the Transformer's ability to capture global dependencies. This approach fundamentally altered how visual features were extracted and processed, moving away from predefined spatial hierarchies and towards a more flexible, data-driven understanding of global context \\cite{han2020yk0}.\n\nThe initial success of ViTs, however, was accompanied by practical challenges. Pure ViT models often required significantly larger datasets for pre-training compared to CNNs to achieve comparable performance, largely due to their reduced inherent inductive biases \\cite{image_is_worth_16x16_words_2021, zhou2021rtn}. Furthermore, the quadratic computational complexity of global self-attention with respect to the number of tokens (and thus image resolution) posed considerable computational costs, limiting their applicability to high-resolution images or resource-constrained environments. These initial limitations became the primary drivers for an explosion of subsequent research aimed at enhancing the efficiency, data-effectiveness, and architectural flexibility of Vision Transformers.\n\nIn conclusion, the transition from CNNs to Transformers represents a fundamental evolution in computer vision, shifting the focus from local, hierarchical feature extraction to global context understanding through attention mechanisms. While CNNs provided a robust foundation, the advent of ViTs offered a new paradigm capable of overcoming some of their inherent limitations, particularly in modeling long-range dependencies. This breakthrough, despite its initial challenges related to data hunger and computational expense, has spurred extensive research into architectural refinements, efficient training methodologies, and hybrid models. These ongoing efforts, which will be explored in detail in the subsequent sections of this review, are continuously solidifying the position of Vision Transformers as versatile and powerful backbones for a wide array of computer vision tasks, fundamentally altering how visual features are extracted and processed.",
    "Scope and Organization of the Review": "\\subsection*{Scope and Organization of the Review}\n\nThis literature review aims to provide a comprehensive and systematically organized overview of Vision Transformers (ViTs), tracing their rapid evolution from foundational principles to their current state as a dominant paradigm in computer vision. By delineating the boundaries and structure of this review, readers are provided with a clear roadmap, ensuring a coherent and comprehensive understanding of the field's progression, its intricate interconnections, and the critical research trajectories that continue to shape its future. This structured approach, similar to recent surveys on ViTs that categorize design techniques and innovative methods \\cite{heidari2024d9k, hassija2025wq3}, is designed to facilitate a deep understanding of the architectural, methodological, and application-driven advancements.\n\nThe review is meticulously structured into seven main sections, each building upon the preceding one to offer a logical and progressive narrative of ViT development:\n\nThe journey commences with \\textbf{Section 1: Introduction}, which establishes the overarching context for Vision Transformers. This section begins by outlining the motivation behind their emergence, particularly in response to the inherent limitations of traditional Convolutional Neural Networks (CNNs) in capturing global dependencies (Subsection 1.1). It then provides a concise historical overview of vision models, highlighting the pivotal paradigm shift from CNN dominance to the rise of Transformers (Subsection 1.2). This introductory section sets the stage for the detailed exploration that follows, providing essential background for understanding the subsequent technical discussions.\n\n\\textbf{Section 2: Foundational Concepts of Vision Transformers}, delves into the core principles that underpin ViTs. It starts with a brief recap of the original Transformer architecture from Natural Language Processing (Subsection 2.1), explaining its key components. Subsequently, it details the pioneering work that adapted this architecture for visual data, introducing the original Vision Transformer (ViT) and its innovative approach to image tokenization and global self-attention (Subsection 2.2). Crucially, this section also critically examines the initial challenges and limitations of pure ViTs, such as their significant data hunger and quadratic computational complexity (Subsection 2.3), thereby establishing the impetus for subsequent research and architectural refinements.\n\nBuilding upon the identified limitations, \\textbf{Section 3: Architectural Enhancements and Efficiency}, explores the crucial innovations designed to make ViTs more practical and efficient. This section is segmented to discuss hierarchical Vision Transformers (Subsection 3.1), which address multi-scale feature learning and computational efficiency; data-efficient training and tokenization strategies (Subsection 3.2), aimed at reducing reliance on massive datasets; and advanced attention mechanisms (Subsection 3.3), which refine the core self-attention process for improved performance and reduced overhead. These advancements collectively broadened the applicability of ViTs beyond initial classification benchmarks.\n\n\\textbf{Section 4: Self-Supervised Learning and Vision Foundation Models}, shifts focus to the transformative role of self-supervised learning (SSL) in scaling ViTs. It details powerful SSL techniques like Masked Autoencoders (MAE) for pre-training (Subsection 4.1) and explores other contrastive and knowledge distillation approaches (Subsection 4.2). The section culminates by discussing the profound trend of scaling Vision Transformers to create \"Vision Foundation Models\" (Subsection 4.3), which aim to learn universal visual representations adaptable to a wide array of downstream tasks with minimal fine-tuning, marking a significant paradigm shift towards general-purpose visual intelligence.\n\nFurther addressing practical deployment and architectural synergy, \\textbf{Section 5: Hybrid Architectures and Mobile-Friendly Designs}, examines the strategic convergence of convolutional and Transformer architectures. It explores synergistic CNN-Transformer designs (Subsection 5.1) that leverage the complementary strengths of both paradigms. Additionally, it addresses the critical need for efficiency by detailing lightweight and mobile-optimized Vision Transformers (Subsection 5.2) for resource-constrained environments. The section also looks into novel hybrid paradigms (Subsection 5.3), such as the integration of state-space models, showcasing the continuous innovation in architectural design.\n\n\\textbf{Section 6: Applications of Visual Transformers}, comprehensively showcases the extensive and diverse utility of ViTs across the spectrum of computer vision tasks. This section illustrates their state-of-the-art performance in core vision tasks like classification and object detection (Subsection 6.1), their adaptation for dense prediction tasks such as segmentation and pose estimation (Subsection 6.2), and their remarkable versatility in specialized and multimodal applications (Subsection 6.3). This breadth of application underscores the profound impact of ViTs on various real-world scenarios.\n\nFinally, \\textbf{Section 7: Future Outlook: Challenges and Opportunities}, synthesizes the current state of ViTs by identifying persistent challenges, such as computational cost and data efficiency (Subsection 7.1), and critical aspects like interpretability, robustness, and generalization (Subsection 7.2). It then outlines promising future trends and open problems (Subsection 7.3), including novel architectural explorations and the integration of ViTs into broader multimodal AI systems. This concluding section encourages responsible development and application of this rapidly evolving technology, ensuring beneficial and equitable societal impact. Through this structured exploration, the review aims to provide a comprehensive and critically informed understanding of the Vision Transformer landscape.",
    "The Transformer Architecture: A Brief Recap": "\\subsection{The Transformer Architecture: A Brief Recap}\n\nThe Transformer architecture, introduced by \\cite{vaswani2017attention}, fundamentally revolutionized sequence modeling in Natural Language Processing (NLP) by moving away from recurrent and convolutional networks. This paradigm shift enabled unprecedented parallel processing capabilities and a more effective capture of long-range dependencies, laying the groundwork for its subsequent adaptation to diverse data modalities, including visual data.\n\nAt its core, the Transformer adopts an encoder-decoder structure. The encoder processes the input sequence, generating a rich, context-aware representation, which the decoder then utilizes to generate the output sequence, often in an auto-regressive manner. This entire architecture is built exclusively on attention mechanisms, eliminating the need for sequential processing inherent in Recurrent Neural Networks (RNNs) or the local receptive fields characteristic of Convolutional Neural Networks (CNNs).\n\nThe central innovation is the \\textbf{self-attention mechanism}, which allows the model to weigh the importance of different parts of the input sequence when processing each element. For every token in the sequence, three distinct vectors are computed: a Query (Q), a Key (K), and a Value (V). The attention scores are calculated by taking the dot product of the Query with all Keys, scaling by the square root of the key dimension to prevent vanishing gradients, and applying a softmax function. These scores then weight the corresponding Value vectors, which are summed to produce the output for that token. This mechanism inherently allows each token to attend to any other token in the sequence, regardless of their distance, thereby effectively capturing long-range dependencies that were challenging for traditional recurrent networks.\n\nTo enhance the model's ability to capture diverse relationships and focus on different aspects of the input simultaneously, the Transformer employs \\textbf{multi-head attention}. Instead of performing a single attention function, the Q, K, and V vectors are linearly projected multiple times into different lower-dimensional subspaces. Each of these \"heads\" then independently computes its own scaled dot-product attention. The outputs from all attention heads are concatenated and then linearly transformed back into the desired output dimension. This parallel processing of multiple attention mechanisms allows the model to learn various types of relationships and attend to different positions, enriching the overall representation and improving the model's capacity to model complex data.\n\nA critical aspect of sequence modeling is preserving the order of elements, which self-attention inherently lacks due to its permutation-invariant nature. To address this, the Transformer injects sequence order information through \\textbf{positional encoding}. These are vectors added directly to the input embeddings at the bottom of the encoder and decoder stacks. The original Transformer used fixed sinusoidal functions of different frequencies, allowing the model to easily learn relative positions. This mechanism ensures that while the attention mechanism processes tokens in parallel, the model retains crucial information about their relative and absolute positions within the sequence.\n\nEach encoder and decoder block also contains a position-wise fully connected feed-forward network, applied independently and identically to each position. This network typically consists of two linear transformations with a ReLU activation in between. Additionally, layer normalization is applied before each sub-layer (self-attention and feed-forward network), followed by a residual connection, aiding in stable training and gradient flow through deep networks.\n\nThe design of the Transformer, particularly its reliance on self-attention, marked a significant departure from previous state-of-the-art models like Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). Unlike RNNs, which process sequences token by token, leading to computational bottlenecks and difficulties with long-range dependencies due to vanishing/exploding gradients, the Transformer processes all tokens in parallel. This parallelization drastically reduces training time. Furthermore, while CNNs capture local features through fixed-size kernels, the Transformer's self-attention mechanism provides a global receptive field from the very first layer, allowing it to directly model relationships between any two tokens irrespective of their distance. This global perspective and parallel processing capability were revolutionary, establishing a new paradigm for sequence modeling that would profoundly influence subsequent research across various domains.",
    "The Original Vision Transformer (ViT)": "\\subsection*{The Original Vision Transformer (ViT)}\n\nThe remarkable success of the Transformer architecture \\cite{vaswani2017attention} in natural language processing (NLP), primarily driven by its potent self-attention mechanism, ignited a fundamental inquiry within the computer vision community: could a similar, convolution-free paradigm effectively process and interpret visual data? This pivotal question was comprehensively addressed by Dosovitskiy et al. \\cite{dosovitskiy2021image} with the introduction of the Vision Transformer (ViT). Their pioneering work directly adapted the Transformer for image recognition, unequivocally demonstrating its viability as a powerful and distinct alternative to traditional Convolutional Neural Networks (CNNs) and marking a significant paradigm shift in visual modeling \\cite{han2020yk0, huo2023e5h}.\n\nThe core innovation of the original ViT lies in its elegant yet straightforward approach to re-conceptualizing an image as a sequence of discrete tokens, directly analogous to how words are treated in NLP. The process commences by partitioning an input image into a grid of fixed-size, non-overlapping square patches, typically $16 \\times 16$ pixels. Each of these patches is then flattened into a one-dimensional vector and subsequently projected linearly into a fixed-dimension embedding space. This transformation yields a sequence of \"visual tokens,\" where each token represents a localized region of the original image. Crucially, to compensate for the spatial information lost during the flattening and tokenization process, learnable positional embeddings are added to these patch embeddings. These embeddings are vital for retaining the relative spatial arrangement of patches, allowing the model to understand the geometric relationships between different parts of the image.\n\nTo facilitate image classification, a special, learnable \"class token\" is prepended to this sequence of embedded patches and their corresponding positional embeddings. This class token acts as a global representation of the entire image, accumulating information from all other visual tokens as it propagates through the Transformer layers. The augmented sequence then enters a standard Transformer encoder, which consists of multiple identical layers, each comprising a multi-head self-attention (MSA) block and a feed-forward network (FFN). The global self-attention mechanism within the MSA block is the cornerstone of ViT's ability to capture long-range dependencies; it enables each visual token (patch or class token) to attend to every other token in the sequence, thereby integrating global contextual information across the entire image. This stands in stark contrast to CNNs, which primarily rely on local receptive fields and hierarchical processing to build global context. The final state of the class token, after traversing the Transformer encoder, is then fed into a simple Multi-Layer Perceptron (MLP) head, which outputs the predicted class probabilities for the image.\n\nThis architectural departure from CNNs demonstrated remarkable empirical performance. Dosovitskiy et al. \\cite{dosovitskiy2021image} showcased that ViT achieved state-of-the-art results on large-scale image classification benchmarks, including ImageNet and JFT-300M. A critical finding was that ViTs, when pre-trained on sufficiently vast datasets (e.g., JFT-300M with 300 million images), could significantly outperform CNNs of comparable size. This performance advantage was attributed to the Transformer's ability to learn more flexible and global representations, unconstrained by the strong inductive biases (like locality and translation equivariance) inherent in CNNs. While these inductive biases make CNNs highly data-efficient on smaller datasets, ViT's \"data hunger\" meant it required extensive pre-training to fully exploit its capacity and generalize effectively \\cite{huo2023e5h}. The breakthrough performance of ViT not only challenged the long-standing dominance of CNNs in computer vision but also fundamentally reoriented research, establishing a new and highly influential direction for visual modeling based on global attention mechanisms \\cite{han2020yk0}. However, this paradigm shift, while promising, also introduced its own set of initial challenges, particularly concerning its substantial data requirements and the computational implications of its global self-attention mechanism, which necessitated further architectural refinements and training innovations.",
    "Initial Challenges and Limitations of Pure ViTs": "\\subsection*{Initial Challenges and Limitations of Pure ViTs}\n\nDespite the groundbreaking success of the Vision Transformer (ViT) in demonstrating that a pure Transformer architecture could achieve state-of-the-art performance in image classification, the initial iterations of these models presented several significant challenges and inherent limitations \\cite{ICLR2019}. These drawbacks, which became apparent despite their impressive performance on large-scale datasets, necessitated extensive subsequent research into their optimization and refinement.\n\nA primary limitation of the original pure ViT architecture, as introduced by \\cite{ICLR2019}, was its substantial data requirement for pre-training. Unlike Convolutional Neural Networks (CNNs) that inherently possess inductive biases such as locality and translation equivariance, pure ViTs lack these built-in priors. Consequently, they must learn these fundamental visual properties from scratch, demanding massive datasets like JFT-300M to achieve competitive performance, making them less accessible for researchers without access to such vast resources. This data hunger was a critical bottleneck, prompting early efforts to make ViTs more data-efficient, such as through knowledge distillation \\cite{Touvron2021} or refined tokenization strategies \\cite{Yuan2021}.\n\nAnother significant challenge stemmed from the computational complexity of the global self-attention mechanism, which is central to pure ViTs. For an input image of $H \\times W$ resolution, if flattened into $N$ patches, the self-attention operation scales quadratically with the number of patches, $O(N^2)$, or equivalently, $O((HW)^2)$ with respect to the image resolution. This quadratic complexity made pure ViTs computationally prohibitive for high-resolution images and impractical for tasks requiring fine-grained, pixel-level understanding, such as dense prediction. This limitation spurred the development of hierarchical ViTs that employed localized attention mechanisms, like the shifted window attention in Swin Transformer \\cite{ICLR2021} or pyramid structures in PVT \\cite{ICCV2021}, to reduce computational cost and enable multi-scale feature extraction.\n\nBeyond data and computational demands, pure ViTs also exhibited architectural limitations that hindered their scalability and applicability. For instance, deeper pure ViT models were found to suffer from an \"attention collapse\" issue \\cite{zhou202105h}. As the network depth increased, the attention maps across layers tended to become increasingly similar, leading to redundant feature learning and performance saturation rather than improvement. To address this, \\cite{zhou202105h} proposed Re-attention to diversify attention maps and enable consistent performance gains in deeper ViTs.\n\nFurthermore, the lack of inherent hierarchical feature representation and strong locality inductive biases in pure ViTs made them less naturally suited for dense prediction tasks like object detection and semantic segmentation, which traditionally benefit from multi-scale feature pyramids and local context modeling. While studies like \\cite{li2022raj} demonstrated that plain ViT backbones could be adapted for object detection, they often required significant modifications, such as building simple feature pyramids from single-scale feature maps and incorporating cross-window propagation blocks, to achieve competitive results. This highlighted that the original pure ViT design was not intrinsically optimized for these tasks without architectural augmentations.\n\nIn summary, the initial pure ViT architectures, despite their paradigm-shifting performance, were characterized by their substantial data requirements, quadratic computational complexity, and a lack of built-in inductive biases that are naturally present in CNNs. These factors collectively made them less efficient, harder to train effectively on smaller datasets, and less versatile for a broad range of computer vision tasks without significant modifications. These inherent limitations became a fertile ground for subsequent research, driving the evolution towards more efficient, data-agnostic, and task-adaptable Vision Transformer variants.",
    "Hierarchical Vision Transformers": "\\subsection*{Hierarchical Vision Transformers}\n\nEarly Vision Transformers (ViTs) \\cite{dosovitskiy2020image} marked a significant paradigm shift in computer vision, yet their direct application to tasks requiring fine-grained spatial understanding, such as dense prediction, or processing of high-resolution images, was hampered by two primary limitations \\cite{hassija2025wq3, heidari2024d9k}. Firstly, the global self-attention mechanism exhibited quadratic computational complexity with respect to the number of input tokens (image patches), making it prohibitively expensive for large inputs. Secondly, pure ViTs lacked the inherent multi-scale feature representations and inductive biases (like locality) that Convolutional Neural Networks (CNNs) naturally possess, which are crucial for capturing both global context and local details across various scales. To address these challenges, a critical line of research emerged, focusing on the development of hierarchical Vision Transformers that integrate multi-scale processing and more efficient attention mechanisms, thereby mimicking CNN-like feature pyramids and achieving linear computational complexity.\n\nOne of the foundational models in this category is the \\textbf{Pyramid Vision Transformer (PVT)} \\cite{wang2021pyramid}. PVT introduced a progressive shrinking pyramid structure, akin to feature pyramids in CNNs, by gradually reducing the resolution of feature maps in deeper layers. This hierarchical design allows for the generation of multi-scale features, which are essential for dense prediction tasks. To manage the computational cost, PVT employs a Spatial-Reduction Attention (SRA) module. Unlike global self-attention, SRA reduces the spatial dimension of the key and value matrices before computing attention, effectively lowering the computational complexity from quadratic to linear with respect to image size. This innovation enabled PVT to serve as a robust backbone for tasks like object detection and semantic segmentation, demonstrating the viability of hierarchical Transformers for a broader range of vision applications \\cite{hassija2025wq3}.\n\nBuilding upon the success of hierarchical designs, the \\textbf{Swin Transformer} \\cite{liu2021ljs} emerged as another seminal work, further solidifying the potential of these architectures. Swin Transformer also adopts a hierarchical structure through progressive patch merging, generating multi-scale feature maps. Its core innovation, however, lies in the \"shifted window attention\" mechanism. Instead of global attention, Swin restricts self-attention computation to non-overlapping local windows within each stage, drastically reducing computational complexity to linear. To facilitate information exchange between windows and capture global dependencies, the windows are shifted between successive Transformer blocks. This elegant solution allows Swin Transformer to achieve state-of-the-art performance across a wide array of vision tasks, including image classification, object detection, and semantic segmentation, effectively establishing hierarchical Transformers as powerful general-purpose vision backbones. While both PVT and Swin Transformer achieved linear complexity, Swin's shifted window approach provided a more dynamic mechanism for cross-window information flow compared to PVT's static spatial reduction, often leading to stronger performance.\n\nFurther advancements in hierarchical ViTs have focused on refining attention mechanisms and simplifying architectures. The fixed window sizes and handcrafted attention patterns in models like Swin Transformer and PVT, while efficient, can sometimes limit their ability to model long-range relations adaptively \\cite{xia2023bp7}. Addressing this, the \\textbf{Deformable Attention Transformer (DAT++)} \\cite{xia2023bp7} proposes a novel deformable multi-head attention module. This module adaptively allocates the positions of key and value pairs in a data-dependent manner, allowing the model to dynamically focus on relevant regions and capture more flexible spatial relationships, similar to deformable convolutions in CNNs. This approach enhances the representation power of global attention while maintaining efficiency. Similarly, the \\textbf{Dynamic Window Vision Transformer (DW-ViT)} \\cite{ren2022ifo} extends the window-based paradigm by moving \"beyond fixation.\" DW-ViT assigns windows of different sizes to different head groups within multi-head self-attention and dynamically fuses the multi-scale information, overcoming the limitation of fixed single-scale windows in models like Swin Transformer and further improving multi-scale modeling capabilities.\n\nIn contrast to increasing architectural complexity, some research has explored simplification. \\textbf{Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles} \\cite{ryali202339q} proposes a streamlined hierarchical ViT design, arguing that many added complexities can be removed without sacrificing accuracy, provided the model is pre-trained with a robust self-supervised pretext task, such as Masked Autoencoders (MAE) \\cite{he2022masked}. By leveraging strong pre-training, Hiera achieves competitive or superior accuracy to more complex hierarchical models while being significantly faster during both inference and training. This work underscores the critical interplay between architectural design and effective pre-training strategies, suggesting that inductive biases from hierarchical processing, combined with powerful self-supervision, can lead to highly efficient and performant architectures without excessive overhead. This also opens a comparative perspective on the fundamental role of attention itself; for instance, the \\textbf{ShiftViT} \\cite{wang2022da0} even explores replacing attention layers entirely with a zero-parameter shift operation, achieving performance comparable to Swin Transformer, suggesting that the hierarchical structure and feature processing might be as crucial as the attention mechanism itself.\n\nIn conclusion, the evolution of hierarchical Vision Transformers has been instrumental in overcoming the initial limitations of pure ViTs, particularly their quadratic complexity and lack of multi-scale representations. Models like PVT and Swin Transformer pioneered efficient multi-scale processing through techniques such as spatial-reduction attention and shifted window attention, respectively, making ViTs suitable for dense prediction tasks and high-resolution images. Subsequent innovations, including deformable attention (DAT++) and dynamic windowing (DW-ViT), have further refined these attention mechanisms for greater flexibility and multi-scale understanding. Concurrently, models like Hiera highlight the power of architectural simplification when coupled with strong self-supervised pre-training. Future research in this domain will likely continue to explore the optimal balance between architectural complexity, computational efficiency, and representational power within hierarchical structures, further optimizing attention mechanisms, and deepening the synergy with advanced self-supervised learning paradigms to unlock even more robust and versatile vision backbones.",
    "Data-Efficient Training and Tokenization Strategies": "\\subsection{Data-Efficient Training and Tokenization Strategies}\n\nEarly Vision Transformers (ViTs) demonstrated impressive capabilities in image recognition, yet their reliance on massive proprietary datasets like JFT-300M for pre-training posed a significant barrier to their widespread adoption and practical application \\cite{Dosovitskiy2021}. This section explores key innovations that addressed this limitation, focusing on strategies for data-efficient training and improved tokenization, thereby making ViTs more accessible and performant on standard, smaller datasets.\n\nOne of the most impactful approaches to mitigate ViTs' data hunger is knowledge distillation. The work by \\cite{Touvron2021} introduced Data-efficient image Transformers (DeiT), which enabled ViTs to be trained effectively on ImageNet-1K, a significantly smaller dataset, without requiring external data. DeiT achieved this by employing a distillation strategy where a ViT student model learns from a powerful convolutional neural network (CNN) teacher. A crucial innovation was the \"distillation token,\" an additional token that interacts with the class token through self-attention and is trained to match the teacher's output, effectively transferring the teacher's knowledge and enabling the ViT to achieve competitive performance with much less training data.\n\nWhile knowledge distillation provided a powerful mechanism for data-efficient training, another line of research focused on enhancing the initial representation of image patches, thereby reducing the inherent data requirements for learning robust visual features. The original ViT treats non-overlapping image patches as independent tokens, which can lose fine-grained local structural information. To address this, \\cite{Yuan2021} proposed Tokens-to-Token ViT (T2T-ViT), which introduced an improved tokenization strategy. Instead of a single-step patch embedding, T2T-ViT employs a multi-stage process where neighboring pixels are progressively aggregated into tokens through a series of self-attention layers. This hierarchical tokenization module effectively captures local structural details and preserves more information from the original image, leading to better performance from scratch on standard benchmarks like ImageNet-1K without relying on massive pre-training datasets or external teacher models.\n\nThe advancements in data-efficient training through distillation, as exemplified by DeiT, and improved tokenization strategies, such as T2T-ViT, collectively transformed the landscape of Vision Transformers. DeiT demonstrated that ViTs could achieve state-of-the-art results on ImageNet-1K by effectively mimicking a larger teacher, while T2T-ViT showed that a more sophisticated initial tokenization could inherently boost ViT performance on smaller datasets by better encoding local visual information. These innovations were critical in making ViTs more practical and accessible, moving them beyond the realm of models requiring immense computational resources and proprietary datasets. However, challenges remain in further reducing the data requirements for highly specialized tasks and in developing unified frameworks that seamlessly integrate the benefits of both distillation and advanced tokenization for optimal efficiency across diverse applications.",
    "Advanced Attention Mechanisms": "\\subsection*{Advanced Attention Mechanisms}\n\nThe foundational self-attention mechanism in Vision Transformers, while powerful for capturing global dependencies, inherently suffers from quadratic computational complexity with respect to the number of tokens, posing significant challenges for high-resolution images or deeper architectures. Furthermore, vanilla self-attention often lacks explicit spatial inductive biases, which can sometimes lead to redundancy or \"attention collapse\" in very deep models, limiting their expressive power. To address these limitations, researchers have developed various advanced attention mechanisms aimed at improving efficiency, flexibility, and expressiveness.\n\nOne critical challenge in scaling Vision Transformers to greater depths is the phenomenon of \"attention collapse.\" As models become deeper, the attention maps across different layers can become increasingly similar, hindering the model's ability to learn diverse and effective representations. \\textcite{zhou202105h} introduced DeepViT to tackle this issue, observing that in deeper ViTs, self-attention mechanisms often fail to learn distinct concepts, leading to performance saturation. To mitigate this, they proposed Re-attention, a simple yet effective method designed to re-generate attention maps and increase their diversity across layers. This innovation allows for the training of significantly deeper ViT models with consistent performance improvements, enhancing the model's overall expressiveness by ensuring that attention layers continue to contribute unique feature transformations.\n\nBeyond improving expressiveness in deep models, a major thrust in advanced attention research focuses on enhancing efficiency and spatial awareness. The global nature of standard self-attention means every token attends to every other token, which can be computationally expensive and may lead to attention being drawn to irrelevant regions. To make attention more adaptive and efficient, Deformable Attention was introduced, allowing the attention mechanism to sample features at adaptive, learned offsets \\textcite{xia2022qga}. Unlike fixed-grid or sparse attention patterns, this data-dependent sampling enables the model to dynamically focus on relevant regions and capture more informative features. This flexible scheme significantly reduces computational overhead by concentrating attention on salient parts of the input, while simultaneously enhancing the model's ability to adapt to varying object shapes and scales, thereby improving spatial awareness and fine-grained detail capture.\n\nThis concept of focusing attention on relevant tokens to reduce computation is also explored in approaches like Focal Attention. Such mechanisms typically employ hierarchical attention or explicit token selection strategies to prioritize important visual information. By selectively attending to a subset of tokens or regions, these methods aim to mitigate the quadratic complexity of global attention, making ViTs more scalable for high-resolution inputs. These innovations collectively enhance the model's ability to capture fine-grained details and adapt to varying object shapes and scales, leading to more robust and accurate visual understanding.\n\nIn summary, advanced attention mechanisms represent a crucial evolutionary step for Vision Transformers, moving beyond the limitations of vanilla self-attention. By addressing issues like attention collapse through techniques such as Re-attention \\textcite{zhou202105h} and improving efficiency and spatial adaptability with data-dependent sampling as seen in Deformable Attention \\textcite{xia2022qga}, these refinements enable the development of more powerful, flexible, and scalable ViT architectures. Future directions will likely continue to explore more sophisticated ways to balance global context modeling with local detail, further optimizing computational efficiency, and integrating stronger inductive biases into attention mechanisms for even more robust visual understanding across diverse tasks.",
    "Masked Autoencoders (MAE) for Pre-training": "\\subsection{Masked Autoencoders (MAE) for Pre-training}\n\nVision Transformers (ViTs), despite their remarkable success in various computer vision tasks, initially faced significant challenges related to their data-hungry nature and the difficulty of scaling them to deeper architectures without performance degradation. Early observations, such as those presented in \\cite{zhou202105h}, highlighted an \"attention collapse\" issue in deeper ViTs, where attention maps became increasingly similar across layers, hindering effective representation learning and limiting performance gains. This necessitated the development of efficient and scalable self-supervised pre-training strategies to unlock the full potential of ViTs and enable them to learn robust visual representations from unlabeled data.\n\nA pivotal breakthrough in this regard was the introduction of Masked Autoencoders (MAE) by \\cite{CVPR2022_Masked_Autoencoders_Are_Scalable_Vision_Learners_2022}. MAE revolutionized pre-training for Vision Transformers by proposing a highly effective self-supervised learning strategy rooted in image reconstruction. The core idea involves masking a substantial portion of image patches, typically around 75\\%, and training a Transformer encoder-decoder architecture to reconstruct the missing pixels. The encoder processes only the visible, unmasked patches, leading to significant computational efficiency during pre-training, while a lightweight decoder is responsible for predicting the original pixel values of the masked patches. This design forces the encoder to learn rich, high-level semantic representations from limited visual context, preventing it from relying on trivial low-level statistical regularities.\n\nThe MAE approach offers several compelling advantages. Its asymmetric encoder-decoder design, where the encoder operates on a sparse set of visible patches, drastically reduces the computational cost of pre-training compared to prior self-supervised methods that process full images. This efficiency makes MAE exceptionally scalable, enabling the effective pre-training of very large Vision Transformer models on vast amounts of unlabeled data. Consequently, MAE-pretrained ViTs have demonstrated state-of-the-art performance across various downstream tasks, including image classification, object detection, and semantic segmentation, often requiring less fine-tuning data than models trained with other self-supervised or supervised methods. The learned representations are highly robust and generalizable, significantly reducing the reliance on extensive labeled datasets and making large-scale pre-training more feasible and impactful.\n\nThe success of MAE extended beyond pure Transformer architectures, demonstrating the generality of its self-supervised reconstruction paradigm. For instance, \\cite{ICLR2023_ConvNeXt_V2_Co_designing_and_Scaling_ConvNets_with_Masked_Autoencoders_2023} successfully applied MAE pre-training to modern ConvNets, showing that even CNNs could benefit significantly from this Transformer-inspired self-supervised strategy, achieving improved performance and bridging the gap between the two architectural paradigms. Furthermore, the efficiency and scalability demonstrated by MAE paved the way for the exploration of even grander model scales. Works like \\cite{ICLR2023_Scaling_Vision_Transformers_to_1_Billion_Parameters_2023} leveraged similar efficient pre-training principles to scale Vision Transformers to unprecedented sizes, showcasing the potential for massive, general-purpose \"Vision Foundation Models.\" The drive for learning robust, unsupervised features, significantly propelled by MAE's success, continued with advancements such as \\cite{ICLR2023_DINOv2_Learning_Robust_Visual_Features_without_Supervision_2023}, which further refined self-supervised approaches to learn highly transferable visual features, building upon the foundation of efficient pre-training established by MAE.\n\nIn conclusion, Masked Autoencoders marked a significant turning point in the pre-training landscape for Vision Transformers. By providing an efficient, scalable, and effective self-supervised learning strategy, MAE made large-scale ViT pre-training practical and solidified self-supervised learning as a cornerstone for developing powerful Vision Foundation Models. While MAE has proven highly effective, ongoing research continues to explore optimal masking strategies, alternative reconstruction targets, and combinations with other self-supervised objectives to further enhance the learned representations and address the persistent challenge of adapting these powerful, pre-trained models for efficient deployment on resource-constrained devices.",
    "Contrastive and Knowledge Distillation Approaches": "\\subsection*{Contrastive and Knowledge Distillation Approaches}\n\nThe quest for learning robust, generalizable visual features for Vision Transformers (ViTs) without the prohibitive cost of extensive human annotation has propelled the development of sophisticated self-supervised learning (SSL) paradigms. Beyond masked autoencoding, two other prominent strategies, contrastive learning and advanced knowledge distillation, have significantly contributed to overcoming the data-hungry nature of ViTs, enabling them to learn meaningful representations directly from vast quantities of unlabeled data. These methods enhance ViTs' performance and applicability across diverse downstream tasks, demonstrating powerful alternative strategies for unsupervised representation learning.\n\nContrastive learning, a foundational SSL approach, operates by teaching the model to distinguish between similar and dissimilar pairs of data points. The core idea is to pull \"positive\" pairs (different augmented views of the same image) closer in the embedding space while pushing \"negative\" pairs (views from different images) apart. Early successes with Convolutional Neural Networks (CNNs), such as SimCLR \\cite{chen2020simple} and MoCo \\cite{he2020momentum}, laid the groundwork for adapting these techniques to ViTs. MoCo-v3 \\cite{chen2021empirical}, for instance, successfully applied a momentum contrast framework to ViTs, demonstrating that these architectures could learn competitive representations through instance discrimination. The effectiveness of contrastive learning stems from its ability to learn representations that are invariant to various data augmentations, thereby capturing essential semantic features without explicit labels.\n\nA highly effective and distinct paradigm, particularly for ViTs, is self-distillation, a specialized form of knowledge distillation where a model learns from itself through a teacher-student framework. This approach has proven exceptionally successful in generating high-quality, transferable features. The original DINO (Self-Distillation with No Labels) \\cite{caron2021emerging} was a seminal work, demonstrating that ViTs could learn dense, semantic features by matching the output of a momentum-updated teacher network. This method encourages the student to produce features that are invariant to different views of the same image, effectively learning without explicit supervision. Building upon this, DINOv2 \\cite{ICLR2023_DINOv2} scaled this teacher-student self-distillation framework to unprecedented levels, utilizing massive datasets and architectural refinements to produce exceptionally robust, generalizable, and transferable visual features. DINOv2's representations frequently outperform features derived from supervised pre-training on various transfer tasks, significantly reducing the reliance on labeled datasets and making ViTs more accessible and scalable for real-world applications.\n\nBeyond self-distillation, other forms of knowledge distillation have also been explored to enhance self-supervised ViTs, particularly for efficiency and transferability. For instance, Attention Distillation \\cite{wang2022pee} investigates distilling knowledge from a self-supervised ViT teacher to a smaller ViT student. This method highlights that directly distilling information from the crucial attention mechanism can significantly narrow the performance gap between teacher and student, outperforming existing self-supervised knowledge distillation methods focused on ConvNets. Such approaches are vital for deploying high-performing self-supervised ViTs on memory and compute-constrained devices, extending the benefits of powerful SSL to more practical scenarios.\n\nA critical analysis reveals fundamental differences in the learning objectives and resulting feature properties between these SSL paradigms and Masked Autoencoders (MAE) discussed in the preceding subsection. While MAE \\cite{he2022masked} forces the model to learn rich representations through a high-information-density pixel reconstruction task, often yielding features beneficial for dense prediction, DINO-style self-distillation encourages semantic consistency and invariance to data augmentations, typically leading to features that are highly effective for classification and linear probing. Contrastive learning, on the other hand, focuses on instance-level discrimination, aiming for representations that can distinguish individual images while being robust to transformations. These distinct objectives mean that each SSL paradigm can yield features with different strengths, making them more suitable for specific downstream tasks or offering complementary benefits. For example, while MAE's reconstruction task can be adapted to hierarchical architectures like HiViT \\cite{zhang2022msa} and Hiera \\cite{ryali202339q} for efficiency, contrastive and distillation methods often focus on the feature space directly. Moreover, some approaches explore hybrid strategies, such as MAT-VIT \\cite{han2024f96}, which leverages an MAE-based self-supervised auxiliary task alongside a supervised primary task, demonstrating the potential for combining different SSL paradigms to maximize learning from both labeled and unlabeled medical images.\n\nIn conclusion, contrastive learning and knowledge distillation approaches, particularly self-distillation as pioneered by DINO and scaled by DINOv2, represent critical advancements in self-supervised learning for Vision Transformers. By leveraging teacher-student frameworks, instance discrimination, and focusing on learning highly robust and generalizable features from unlabeled data, these methods empower ViTs to achieve state-of-the-art performance in various downstream tasks, often surpassing supervised alternatives. The continuous development of such unsupervised representation learning strategies, coupled with innovations in knowledge transfer for efficiency, is fundamental to realizing the full potential of Vision Transformers as efficient, scalable, and universally applicable models, thereby significantly reducing the dependency on costly human annotations and paving the way for the emergence of powerful Vision Foundation Models.",
    "Scaling Vision Transformers to Foundation Models": "\\subsection{Scaling Vision Transformers to Foundation Models}\nThe quest for general-purpose visual intelligence has ushered in a transformative era, marked by the scaling of Vision Transformers (ViTs) to unprecedented sizes, often comprising billions of parameters, to establish \"Vision Foundation Models.\" These colossal models, meticulously trained on vast datasets using sophisticated self-supervised learning (SSL) techniques, aim to distill universal visual representations. The ultimate objective is to develop versatile backbones that exhibit exceptional adaptability across a broad spectrum of vision tasks, delivering superior performance with significantly reduced task-specific fine-tuning. This paradigm represents a profound shift towards general-purpose visual intelligence, capable of impressive zero-shot or few-shot learning capabilities.\n\nInitial explorations into scaling ViTs encountered significant hurdles. Naively increasing model depth, as explored by \\cite{zhou202105h} with DeepViT, revealed an \"attention collapse\" phenomenon. In deeper layers, attention maps became increasingly homogeneous, hindering performance gains and indicating that simply adding more layers was insufficient without more robust architectural designs and effective training strategies. This challenge underscored the critical need for advancements in how ViTs learn and generalize at scale.\n\nA pivotal breakthrough in enabling the efficient scaling of ViTs came with the advent of Masked Autoencoders (MAE) \\cite{CVPR2022_Masked_Autoencoders_Are_Scalable_Vision_Learners}. As discussed in Subsection 4.1, MAE introduced an elegant and highly efficient self-supervised pre-training paradigm. By masking a large portion of image patches and training the Transformer encoder-decoder to reconstruct the missing pixels, MAE effectively mitigated the notorious data-hungry nature of ViTs, allowing them to learn rich, robust representations from vast quantities of unlabeled data. This innovation was instrumental in unlocking the potential for large-scale pre-training. Complementing MAE, methods like DINOv2 \\cite{ICLR2023_DINOv2_Learning_Robust_Visual_Features_without_Supervision}, elaborated in Subsection 4.2, further refined self-supervised learning by leveraging self-distillation with a teacher-student framework. DINOv2 produced highly generalizable and transferable features without explicit supervision, often surpassing supervised pre-training in transfer tasks and demonstrating impressive zero-shot capabilities.\n\nThe combination of efficient self-supervised pre-training and the availability of colossal datasets has been fundamental to the scaling trend. Vision Foundation Models are typically pre-trained on datasets far exceeding ImageNet, such as JFT-300M \\cite{bai2022f1v, hong2022ks6}, JFT-4B, or the internet-scale LAION-5B. The sheer volume and diversity of these datasets, coupled with scalable SSL methods, allow ViTs to learn highly abstract and universal visual concepts. Empirical studies have consistently demonstrated that performance scales predictably with increased model size, dataset size, and computational resources, a phenomenon often referred to as \"scaling laws\" in deep learning.\n\nBuilding upon these foundations, researchers have aggressively pushed the boundaries of ViT size. The work by \\cite{ICLR2023_Scaling_Vision_Transformers_to_1_Billion_Parameters} notably demonstrated the efficacy of scaling Vision Transformers to one billion parameters. Their findings unequivocally showed that larger models, when pre-trained on extensive datasets using techniques like MAE, yield superior performance and significantly enhanced generalization across diverse downstream tasks. This marked a crucial step towards realizing truly universal visual backbones. Even more extreme examples, such as Google's ViT-22B, further exemplify the profound capabilities that emerge from massive scaling. These models exhibit emergent properties, including remarkable zero-shot transfer to unseen tasks and improved few-shot learning, which are hallmarks of true foundation models. They can often adapt to new domains with minimal or no task-specific fine-tuning, drastically reducing the effort and labeled data required for new applications.\n\nBeyond pure parameter count, architectural innovations have also contributed to the scalability and efficiency of these large models. For instance, approaches like UFO-ViT \\cite{song20215tk} introduce linear attention mechanisms to mitigate the quadratic computational complexity of traditional self-attention, which becomes a bottleneck at high resolutions or extreme depths. Similarly, models like Hiera \\cite{ryali202339q} demonstrate that even simplified hierarchical ViT designs, when effectively pre-trained with MAE, can achieve state-of-the-art performance while being significantly faster, thus contributing to more efficient scaling.\n\nWhile scaling to billions of parameters offers immense representational power, the practical deployment of such massive models remains a critical consideration. The computational cost for inference and the memory footprint can be prohibitive for many real-world scenarios, particularly on resource-constrained edge devices. To address this, research into efficiency techniques is paramount. Quantization methods, such as Q-ViT \\cite{li20229zn}, enable significant model compression by reducing bit-widths while striving to preserve performance. Furthermore, specialized hardware accelerators like ViTA \\cite{nag2023cfn} are being developed to optimize the inference of Vision Transformers, ensuring that these powerful, scaled models can be deployed efficiently in practical applications.\n\nIn conclusion, the evolution of Vision Transformers into foundation models represents a profound paradigm shift from task-specific model development to the creation of universally powerful, pre-trained visual intelligence. This trajectory is characterized by extreme architectural scaling, the indispensable role of advanced self-supervised learning for robust feature extraction from colossal datasets, and the resultant emergence of powerful zero-shot and few-shot learning capabilities. Future directions will undoubtedly focus on further enhancing the efficiency of training and inference for these massive models, exploring deeper multimodal integration for richer representations, and critically addressing the ethical implications and potential biases inherent in such large-scale, general-purpose visual systems.",
    "Synergistic CNN-Transformer Designs": "\\subsection{Synergistic CNN-Transformer Designs}\n\nThe evolution of computer vision architectures has increasingly moved towards a synergistic paradigm, strategically combining the inherent strengths of Convolutional Neural Networks (CNNs) with the global context modeling capabilities of Transformers. This convergence is driven by the recognition that while CNNs excel at local feature extraction and possess strong inductive biases like locality and translation equivariance, pure Vision Transformers (ViTs) \\cite{dosovitskiy2021image} offer unparalleled global receptive fields and flexibility in capturing long-range dependencies. Hybrid designs aim to mitigate the limitations of each, such as ViTs' high data requirements and quadratic computational complexity, or CNNs' struggle with modeling global relationships, thereby fostering more robust, efficient, and versatile vision backbones.\n\nOne prominent direction in this synergy involves modernizing traditional CNN architectures by incorporating Transformer-inspired methodologies and training paradigms. A seminal work in this category is \\textit{ConvNeXt V2} \\cite{iclr2023}. Building upon the architectural principles of ConvNeXt, which re-examined and optimized CNN designs to resemble Transformers (e.g., using large kernel depthwise convolutions and inverted bottleneck structures), ConvNeXt V2 crucially leveraged the Masked Autoencoder (MAE) pre-training strategy \\cite{he2022masked}. This integration demonstrated that CNNs could benefit immensely from self-supervised learning techniques originally developed for Transformers, achieving state-of-the-art performance and effectively bridging the perceived architectural gap. ConvNeXt V2 showcased that the inductive biases of CNNs, when combined with powerful, scalable pre-training, remain highly competitive and efficient.\n\nConversely, another significant approach integrates powerful CNN components directly into Transformer-like frameworks. \\textit{InternImage} \\cite{cvpr2023} exemplifies this by embedding deformable convolutions within a Transformer-inspired architecture. Deformable convolutions, a hallmark of advanced CNNs, enable adaptive receptive fields and flexible spatial sampling, which are crucial for handling geometric variations and object deformations effectively. By incorporating this mechanism, InternImage creates robust \"Vision Foundation Models\" that combine the global context understanding of Transformers with the precise, adaptive local feature extraction characteristic of sophisticated convolutions. This allows the model to dynamically focus on relevant regions, enhancing its ability to capture fine-grained details and adapt to varying object shapes and scales, a limitation often observed in standard fixed-grid attention mechanisms. A related approach is seen in \\textit{Deformable Attention Transformer (DAT)} and its enhanced version \\textit{DAT++} \\cite{xia2022qga, xia2023bp7}. These models integrate a novel deformable multi-head attention module where key and value positions are adaptively allocated in a data-dependent manner, effectively bringing the adaptive sampling power of deformable convolutions directly into the self-attention mechanism, thereby enhancing spatial awareness and reducing computational overhead compared to dense global attention.\n\nBeyond integrating components or pre-training strategies, some hybrid architectures explicitly interleave CNN and Transformer blocks to capitalize on their respective strengths at different stages of feature extraction. \\textit{Next-ViT} \\cite{li2022a4u} proposes a \"Next Hybrid Strategy\" (NHS) that stacks \"Next Convolution Blocks\" (NCB) for local information capture and \"Next Transformer Blocks\" (NTB) for global information. This design is optimized for efficient deployment in realistic industrial scenarios, addressing the challenge of achieving both high performance and low latency on hardware accelerators like TensorRT. Similarly, \\textit{TRT-ViT} \\cite{xia2022dnj} provides practical guidelines for TensorRT-oriented network design, suggesting an \"early CNN and late Transformer at stage-level\" and \"early Transformer and late CNN at block-level\" strategy. This highlights a nuanced understanding of where and how to best deploy each architectural component for optimal hardware efficiency and performance. These block-level hybrid models demonstrate that a careful arrangement of convolutional and self-attention layers can yield superior latency-accuracy trade-offs across various vision tasks.\n\nAnother innovative direction involves fusing local and global processing within the Transformer block itself, drawing inspiration from CNNs' local inductive biases without necessarily using explicit convolutional layers. \\textit{PLG-ViT} \\cite{ebert202377v} introduces a \"Parallel Local-Global Vision Transformer\" that merges local window self-attention with global self-attention. This design efficiently represents short- and long-range spatial interactions, bypassing the need for computationally expensive operations like shifted windows while still achieving strong performance in image classification and dense prediction tasks. This approach reflects a deeper integration, where the *concept* of locality, traditionally associated with CNNs, is re-imagined and implemented within the Transformer's attention mechanism.\n\nIn summary, the landscape of synergistic CNN-Transformer designs is rich and diverse, reflecting various philosophies of integration. ConvNeXt V2 represents a CNN-centric approach enhanced by Transformer pre-training. InternImage and DAT/DAT++ showcase embedding CNN-inspired adaptive mechanisms into Transformer backbones. Next-ViT and TRT-ViT demonstrate strategic interleaving of CNN and Transformer blocks for deployment efficiency. PLG-ViT illustrates the internal fusion of local and global attention within a Transformer, mimicking CNN-like inductive biases. These models collectively demonstrate that moving beyond a strict CNN-Transformer dichotomy towards a complementary approach unlocks new levels of performance, efficiency, and versatility, pushing the boundaries of architectural design for a wide array of computer vision applications. Future research will likely explore even more sophisticated integration strategies, novel unified pre-training objectives, and dynamic architectural adaptations to further harness the combined power of these two foundational paradigms.",
    "Lightweight and Mobile-Optimized Vision Transformers": "\\subsection{Lightweight and Mobile-Optimized Vision Transformers}\nBuilding upon the synergistic CNN-Transformer designs discussed in Section 5.1, this subsection delves into architectures specifically engineered for resource-constrained mobile and edge devices. The deployment of Vision Transformers (ViTs) on platforms such as smartphones, IoT devices, and embedded systems presents significant challenges due to their typically high computational cost, large memory footprint, and substantial power consumption. Addressing these critical deployment challenges, a dedicated research thrust focuses on developing lightweight and mobile-optimized ViT architectures that maintain high accuracy while drastically reducing computational demands and parameter counts, making powerful ViT capabilities accessible for real-time and edge applications.\n\nA pioneering effort in this domain is \\textcite{mehta20216ad}'s MobileViT, which directly tackles the trade-off between the spatial inductive biases and efficiency of Convolutional Neural Networks (CNNs) and the global representation learning capabilities of ViTs. MobileViT proposes a novel hybrid architecture that integrates standard convolutional layers for local feature extraction with a lightweight Transformer block that processes information globally, effectively treating \"transformers as convolutions.\" This design allows MobileViT to achieve impressive accuracy, such as 78.4\\% top-1 on ImageNet-1k with approximately 6 million parameters, significantly outperforming both CNN-based (e.g., MobileNetv3) and ViT-based (e.g., DeiT) counterparts for similar parameter budgets. Its ability to learn global representations while retaining the efficiency of local processing makes it highly suitable for mobile vision tasks, demonstrating a foundational approach to lightweight hybrid ViTs.\n\nFurther advancing the field of deployable hybrid architectures, \\textcite{li2022a4u} introduce Next-ViT, explicitly designed for efficient deployment in realistic industrial scenarios, considering factors like TensorRT and CoreML inference latency. Next-ViT proposes a \"Next Convolution Block (NCB)\" and a \"Next Transformer Block (NTB)\" that are deployment-friendly, capturing local and global information respectively. The \"Next Hybrid Strategy (NHS)\" then efficiently stacks these blocks. Unlike many ViTs that optimize for FLOPs or parameter count, Next-ViT prioritizes actual inference speed on target hardware, achieving substantial latency reductions while maintaining superior accuracy. For instance, it surpasses ResNet by 5.5 mAP on COCO detection and accelerates inference speed by 3.6x compared to CSWin under similar performance, highlighting its practical utility for edge computing where real-world latency is paramount. This represents a critical evolution from merely lightweight designs to truly deployment-optimized ones.\n\nBeyond architectural hybridization, another crucial avenue for efficiency lies in redesigning the attention mechanism itself to mitigate its quadratic computational complexity. \\textcite{chen2021r2y} introduce CrossViT, which focuses on learning multi-scale feature representations within a dual-branch transformer framework. CrossViT processes image patches of different sizes in separate branches and then fuses these multi-scale tokens using a highly efficient cross-attention mechanism. Crucially, their cross-attention module is designed to operate with linear computational and memory complexity, rather than the quadratic complexity typical of standard self-attention, thereby mitigating a major bottleneck for deploying ViTs on resource-limited hardware. This approach not only enhances representational power by leveraging features at various scales but also ensures that the fusion process remains computationally tractable. Complementing this, \\textcite{song20215tk} propose UFO-ViT (Unit Force Operated Vision Transformer), which offers a novel self-attention mechanism with linear complexity by eliminating non-linearity and factorizing matrix multiplication without complex linear approximations. While CrossViT achieves linear scaling through a specialized cross-attention for multi-scale fusion, UFO-ViT fundamentally re-engineers the self-attention block itself to achieve linear complexity, offering a more general solution for reducing the computational burden of attention in any ViT layer.\n\nFinally, while architectural and algorithmic innovations are vital, the ultimate efficiency on edge devices often necessitates hardware-aware optimization. \\textcite{nag2023cfn} address this by proposing ViTA, a configurable hardware accelerator specifically designed for inference of Vision Transformer models on highly resource-constrained edge computing devices. ViTA employs a head-level pipeline and inter-layer MLP optimizations to avoid repeated off-chip memory accesses, a common bottleneck in embedded systems. It supports various ViT models with changes solely in its control logic, achieving nearly 90\\% hardware utilization efficiency and reasonable frame rates at low power consumption (e.g., 0.88W at 150 MHz). This demonstrates that for true mobile optimization, a holistic approach combining efficient model design with dedicated hardware acceleration is indispensable, pushing the boundaries of what is achievable on the edge.\n\nCollectively, these works demonstrate a clear progression towards making powerful Vision Transformer capabilities accessible for real-time and edge applications. They highlight the importance of hybrid architectures that judiciously combine CNN and Transformer strengths \\cite{mehta20216ad, li2022a4u}, efficient attention mechanisms that scale linearly with input size \\cite{chen2021r2y, song20215tk}, and the critical role of hardware-software co-design for practical deployment \\cite{nag2023cfn}. Despite these significant advancements, ongoing challenges include developing more sophisticated hardware-aware neural architecture search methods that consider power and memory constraints alongside latency, exploring novel quantization and pruning strategies tailored for these complex hybrid designs, and investigating dynamic execution strategies to adapt to varying computational budgets on the fly.",
    "Novel Hybrid Paradigms": "\\subsection{Novel Hybrid Paradigms}\n\nThe continuous quest for more efficient, scalable, and powerful vision models has propelled research beyond the established CNN-Transformer hybrid architectures, leading to the exploration of entirely new component integrations. This subsection delves into novel hybrid paradigms that fuse distinct neural network components, moving beyond convolutional and self-attention mechanisms, to leverage their complementary strengths for enhanced performance and efficiency, particularly in modeling long-range dependencies and global context. Such approaches signify a dynamic and evolving research frontier, continuously pushing the boundaries of what is achievable in visual representation learning by incorporating components inspired by advancements in sequence modeling.\n\nA significant recent development in this direction is the integration of State-Space Models (SSMs) into vision backbones, exemplified by architectures like MambaVision \\cite{hatamizadeh2024xr6}. Originating from control theory and recently revitalized for deep learning, SSMs offer an alternative mechanism for efficient sequence modeling, particularly adept at capturing long-range dependencies with linear computational complexity. MambaVision, proposed by Hatamizadeh et al. \\cite{hatamizadeh2024xr6}, presents a hybrid architecture that integrates the Mamba SSM with Vision Transformers (ViT). The core motivation behind MambaVision is to harness Mamba's inherent efficiency in sequence modeling and its superior capacity for capturing long-range dependencies, while simultaneously retaining the robust global context understanding capabilities characteristic of Transformers. This fusion offers a compelling new perspective on designing vision backbones that can efficiently process extensive visual information, addressing some of the quadratic complexity issues of pure self-attention.\n\nHatamizadeh et al. \\cite{hatamizadeh2024xr6} meticulously redesigned the Mamba formulation to optimize its performance specifically for visual features, addressing the unique challenges posed by image data. Through comprehensive ablation studies, they demonstrated the feasibility and substantial benefits of integrating ViT components within the Mamba framework. A pivotal finding was that the strategic incorporation of self-attention blocks in the final layers of the Mamba architecture significantly enhanced its ability to capture intricate long-range spatial dependencies, a critical aspect for achieving high performance across diverse vision tasks. This hybrid design allows MambaVision to benefit from the local processing and linear scaling of Mamba while leveraging the global reasoning of attention where most critical. The efficacy of MambaVision is robustly supported by its empirical results across multiple benchmarks, achieving state-of-the-art (SOTA) performance on ImageNet-1K classification and demonstrating favorable performance in downstream tasks such as object detection, instance segmentation on MS COCO, and semantic segmentation on ADE20K, often outperforming comparably sized backbones.\n\nAnother emerging paradigm involves adapting Receptance Weighted Key Value (RWKV) models, initially developed for efficient large language models, to visual perception. Vision-RWKV (VRWKV), introduced by Duan et al. \\cite{duan2024q7h}, is a notable example. Transformers, while powerful, face limitations in high-resolution image processing due to their quadratic computational complexity. VRWKV addresses this by adapting the RWKV model, which processes sequences in a recurrent manner while maintaining a Transformer-like attention mechanism, but with significantly reduced spatial aggregation complexity. This design allows VRWKV to efficiently handle sparse inputs and demonstrate robust global processing capabilities, scaling effectively to large parameters and extensive datasets without the necessity for windowing operations often employed in hierarchical Transformers.\n\nDuan et al. \\cite{duan2024q7h} made necessary modifications to the RWKV architecture to tailor it for vision tasks, enabling it to function as an efficient and scalable visual backbone. Their evaluations demonstrated that VRWKV surpasses the performance of traditional Vision Transformers (ViTs) in image classification, while also exhibiting significantly faster speeds and lower memory usage when processing high-resolution inputs. Furthermore, in dense prediction tasks, VRWKV was shown to outperform window-based models, maintaining comparable speeds. These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks, particularly where high-resolution processing and long-context analysis are critical, without incurring the prohibitive computational costs of full self-attention.\n\nThe emergence of models like MambaVision and Vision-RWKV represents a significant paradigm shift, moving beyond the established dichotomy of CNN-Transformer integrations. Both approaches aim to address the computational burdens associated with pure self-attention mechanisms while preserving or enhancing powerful global reasoning capabilities. MambaVision leverages the linear scaling of SSMs for efficient long-range dependency modeling, strategically reintroducing self-attention in later layers to refine global context. In contrast, Vision-RWKV adapts a distinct recurrent-attention mechanism from NLP, offering reduced spatial aggregation complexity and superior high-resolution processing without explicit windowing. While MambaVision focuses on a hybrid SSM-ViT structure, VRWKV presents a more direct adaptation of an efficient sequence model, emphasizing its unique recurrent-attention mechanism. Both paradigms demonstrate the immense potential of exploring non-traditional neural network components and their synergistic combinations for advancing the field of computer vision. Their success unequivocally highlights the value of cross-domain inspiration, particularly from efficient sequence models in NLP, to develop more efficient, powerful, and specialized vision models for future applications.",
    "Core Vision Tasks: Classification and Object Detection": "\\subsection*{Core Vision Tasks: Classification and Object Detection}\n\nVision Transformers (ViTs) have profoundly impacted fundamental computer vision tasks, particularly image classification and object detection, by leveraging their global receptive fields and powerful feature extraction capabilities. This architectural shift has enabled ViTs to capture both broad contextual information and fine-grained details, often leading to new performance benchmarks that frequently surpass traditional Convolutional Neural Networks (CNNs), especially when trained on extensive datasets.\n\nThe initial foray of Transformers into vision began with image classification. The seminal Vision Transformer (ViT) by \\cite{Dosovitskiy2021} demonstrated that a pure Transformer, by treating image patches as sequential tokens, could effectively classify images. While achieving state-of-the-art results on massive datasets like ImageNet-21K (e.g., ViT-L/16 reaching 88.55\\% top-1 accuracy), it initially exhibited a significant dependency on vast training data, often underperforming CNNs on smaller datasets. To mitigate this data-hungry nature, \\cite{Touvron2021} introduced Data-efficient Image Transformers (DeiT), employing knowledge distillation to enable ViTs to be trained efficiently on ImageNet-1K, achieving competitive performance (e.g., DeiT-S reaching 83.1\\% top-1 accuracy) without requiring massive pre-training. Concurrently, \\cite{Yuan2021} proposed Tokens-to-Token ViT (T2T-ViT), which refined the initial tokenization process to better represent local structures, thereby boosting performance on standard datasets from scratch. Further architectural advancements aimed at enhancing the depth and stability of ViTs for classification; \\cite{zhou202105h} identified the \"attention collapse\" issue in deeper ViTs and proposed Re-attention to increase the diversity of attention maps, enabling the training of significantly deeper models with consistent performance gains. More recently, `[ferdous2024f89]` introduced SPT-Swin, a variant that combines shifted patch tokenization with Swin Transformer to address data deficiency and computational complexity, achieving 89.45\\% accuracy on ImageNet-1K.\n\nA major breakthrough in efficient pre-training for classification came with \\cite{CVPR2022}, which introduced Masked Autoencoders (MAE). This self-supervised learning approach reconstructs masked image patches, proving highly scalable and data-efficient. MAE enables ViTs to learn robust representations from unlabeled data, significantly improving their performance and reducing the need for extensive labeled datasets for classification. For instance, a ViT-Large pre-trained with MAE on ImageNet-1K can achieve 87.8\\% top-1 accuracy. Building on this, the field has seen the emergence of \"Vision Foundation Models,\" with \\cite{ICLR2023} demonstrating the efficacy of scaling ViTs to over a billion parameters, achieving unprecedented classification performance through massive pre-training. Similarly, \\cite{ICLR2023} advanced self-supervised learning with DINOv2, which learns highly robust and generalizable visual features without supervision, often outperforming supervised pre-training in transfer tasks and further enhancing classification accuracy. The influence of ViT principles even extended to CNNs, as shown by \\cite{ICLR2023}, where ConvNeXt V2 leveraged MAE pre-training to significantly boost CNN performance, blurring the lines between the two architectures for classification tasks. For practical deployment, `[song2022603]` proposed CP-ViT, a cascade pruning framework that dynamically predicts sparsity in ViT models, reducing FLOPs by over 40\\% while maintaining accuracy, crucial for resource-limited devices.\n\nThe success of ViTs quickly extended to object detection, a more complex task requiring both accurate object categorization and precise localization. Early pure ViTs struggled with dense prediction tasks due to their fixed-size patch embeddings and lack of inherent hierarchical feature maps, which are crucial for detecting objects at various scales. This limitation was fundamentally addressed by the introduction of end-to-end Transformer detectors. The seminal work on DEtection TRansformer (DETR) by \\cite{Carion2020} revolutionized object detection by formulating it as a direct set prediction problem. DETR eliminated hand-designed components like Non-Maximum Suppression (NMS) by using a set of learned object queries and a bipartite matching loss. While DETR demonstrated the power of Transformers for detection, its slow convergence and high computational cost were initial drawbacks. These were largely overcome by Deformable DETR \\cite{Zhu2020}, which introduced deformable attention to focus on a small set of key sampling points, significantly accelerating training and improving performance, particularly for small objects.\n\nFollowing these foundational end-to-end Transformer detectors, architectural innovations focused on integrating hierarchical ViT backbones to better capture multi-scale visual features. The \\cite{Liu2021} Swin Transformer, detailed in `[liu2021ljs]`, revolutionized ViT applicability to dense prediction by proposing a hierarchical architecture with shifted window attention. This design limited self-attention computation to non-overlapping local windows while allowing for cross-window connections, effectively generating multi-scale feature maps. Swin Transformers became highly suitable as backbones for object detection frameworks, achieving state-of-the-art results, such as 58.7 box AP and 51.1 mask AP on COCO test-dev with Swin-Large. Similarly, \\cite{Wang2021} introduced the Pyramid Vision Transformer (PVT), a pure Transformer-based pyramid structure that progressively reduces feature map resolution, enabling the generation of multi-scale features essential for detecting objects of different sizes. These hierarchical backbones, when integrated with DETR-like heads, led to powerful detectors. For instance, `[wang2023bfo]` combined Deformable DETR with a Swin Transformer and a lightweight Feature Pyramid Network (FPN) to enhance detection accuracy for multi-scale targets, demonstrating a 6.1\\% improvement in accuracy on a classroom behavior dataset.\n\nInterestingly, even plain (non-hierarchical) ViT backbones, when properly leveraged, have shown strong performance in object detection. `[li2022raj]` explored the use of plain ViT backbones, pre-trained with MAE, for object detection. Their work, ViTDet, demonstrated that with minimal adaptations like a simple feature pyramid and limited cross-window propagation, these models could achieve competitive results on the COCO dataset, reaching up to 61.3 AP\\_box using only ImageNet-1K pre-training. This highlighted that the powerful representations learned by plain ViTs, especially through self-supervised pre-training, could be effectively fine-tuned for localization tasks, challenging the strict necessity of hierarchical backbones for all detection scenarios. The convergence of ViT backbones and end-to-end Transformer detectors is further exemplified by `[song2022y4v]`, which introduced ViDT, an extendable and efficient object detector integrating Vision and Detection Transformers. ViDT reconfigures the Swin Transformer as a standalone detector and employs an efficient Transformer decoder, achieving an excellent AP and latency trade-off on COCO. Furthermore, the advancements in large-scale ViT pre-training, such as those in \\cite{ICLR2023} and \\cite{ICLR2023}, implicitly provide even more robust and generalizable backbones for object detection, allowing downstream detection models to achieve higher accuracy and better generalization across diverse scenarios.\n\nIn conclusion, Vision Transformers have established themselves as formidable architectures for core vision tasks. From their initial success in image classification, overcoming challenges related to data efficiency and architectural depth through innovations like DeiT and MAE, to their subsequent adaptation for object detection via end-to-end Transformer designs (DETR) and hierarchical backbones (Swin Transformer), ViTs have consistently pushed performance boundaries. While initial pure ViTs were data-hungry and lacked inherent inductive biases for dense prediction, the continuous evolution has led to more efficient, robust, and scalable models. The ongoing development of massive \"Vision Foundation Models\" and sophisticated hybrid architectures, leveraging advanced self-supervised learning, signifies a future where increasingly versatile visual intelligence can be deployed across an even wider spectrum of real-world applications, further solidifying ViTs' foundational strength in capturing both global context and fine-grained details.",
    "Dense Prediction Tasks: Segmentation and Pose Estimation": "\\subsection{Dense Prediction Tasks: Segmentation and Pose Estimation}\nDense prediction tasks, encompassing semantic segmentation, instance segmentation, and human pose estimation, demand pixel-level understanding and precise localization, moving significantly beyond image-level classification. Vision Transformers (ViTs), with their inherent ability to model long-range dependencies, have emerged as powerful tools in these fine-grained visual analysis tasks. However, the direct application of vanilla ViTs was initially hindered by their high computational cost, quadratic complexity with respect to image resolution, and lack of inherent multi-scale feature representation, which is crucial for pixel-level tasks \\cite{li2023287}.\n\nTo address these limitations, hierarchical ViT architectures quickly emerged as a foundational solution. The \\textit{Swin Transformer} \\cite{liu2021swin} introduced shifted window attention, enabling efficient computation by restricting self-attention within local windows while allowing cross-window connections through shifting. This hierarchical design effectively generates multi-scale feature maps, making it highly suitable as a backbone for dense prediction. Similarly, the \\textit{Pyramid Vision Transformer} (PVT) \\cite{wang2021pyramid} explicitly designed a pyramid structure to produce multi-scale features, directly mimicking the feature pyramid networks (FPNs) commonly used in dense prediction, thereby facilitating their integration into existing frameworks. Further building on this, \\textit{HiViT} \\cite{zhang2022msa} proposed another hierarchical ViT design, demonstrating improved efficiency and performance for downstream tasks like detection and segmentation when pre-trained with masked image modeling (MIM). These hierarchical designs were pivotal, providing the necessary multi-scale representations that pure ViTs lacked, thereby enabling more coherent and accurate pixel-wise predictions.\n\nAs ViT architectures matured, research consolidated strategies for their adaptation to pixel-level tasks, broadly falling into two categories: ViT encoders paired with specialized decoders, and end-to-end Transformer designs. Early and effective approaches often paired hierarchical ViT backbones with established CNN-style decoders. For instance, studies demonstrated the efficacy of integrating Swin Transformer with decoders like U-Net, Pyramid Scene Parsing (PSP) network, or Feature Pyramid Network (FPN) for semantic segmentation, particularly in domains like remote sensing \\cite{panboonyuen20218r7}. This hybrid approach leverages the ViT's strong global feature extraction while benefiting from the CNN decoder's inductive biases for local detail and spatial upsampling. In contrast, end-to-end Transformer designs sought to eliminate reliance on hand-crafted CNN components. \\textit{Max-DeepLab} \\cite{wang2021maxdeeplab} showcased the capacity of ViTs to directly learn pixel-level representations and segmentations for semantic segmentation, leveraging the global receptive field of Transformers to capture broader contextual cues and produce consistent masks. A significant conceptual advance for instance segmentation (and panoptic/semantic segmentation) was \\textit{Mask2Former} \\cite{cheng2022mask2former}, which unified these tasks under a single query-based Transformer framework. Mask2Former uses a masked attention mechanism and a set of learnable queries to directly predict object masks and their classes, demonstrating superior performance on complex scenes and representing a paradigm shift towards more unified, global reasoning for pixel-level tasks.\n\nThe versatility of ViTs extends to other dense prediction tasks beyond segmentation. For human pose estimation, which involves localizing keypoints on human bodies, models like \\textit{ViTPose} \\cite{xu2022vitpose} have showcased how ViTs can effectively model spatial relationships between body parts. Their global attention mechanism is crucial for inferring occluded or less visible keypoints based on overall body context, often outperforming CNN-based methods. Furthermore, ViTs have been adapted for specialized segmentation challenges. For example, \\textit{SENet} \\cite{hao202488z} proposes a simple yet effective ViT-based encoder-decoder for camouflaged and salient object detection, demonstrating how ViTs can be tailored for specific pixel-level understanding problems. In the medical imaging domain, \\textit{SwinBTS} \\cite{jiang2022zcn} leverages the Swin Transformer for 3D multimodal brain tumor segmentation, highlighting ViT's applicability in critical, high-dimensional analysis.\n\nThe recent advancements in foundational Vision Transformers (as discussed in Section 4) have profoundly amplified their impact on dense prediction. Models pre-trained with self-supervised learning strategies like Masked Autoencoders (MAE) \\cite{he2022masked} or advanced knowledge distillation (e.g., DINOv2 \\cite{oquab2023dinov2}) provide exceptionally robust and generalizable backbones. When fine-tuned for dense prediction tasks, these large-scale models, often paired with task-specific decoders (e.g., UPerNet heads or Mask2Former heads), achieve unprecedented accuracy and efficiency, significantly reducing the need for extensive labeled data for downstream tasks. Moreover, hybrid architectures (as detailed in Section 5) have further refined feature extraction for dense prediction. Models like \\textit{InternImage} \\cite{wang2023internimage} and \\textit{ConvNeXt V2} \\cite{woo2023convnext}, which integrate deformable convolutions or enhance CNNs with MAE pre-training, provide backbones that combine the inductive biases of CNNs (e.g., locality, efficiency) with the global context modeling of Transformers. More recently, hybrid Mamba-Transformer backbones like \\textit{MambaVision} \\cite{hatamizadeh2024xr6} have also demonstrated strong performance on instance and semantic segmentation, showcasing new avenues for efficient long-range dependency modeling. Even efficient variants like \\textit{MobileViT V2} \\cite{vaswani2023mobilevit} aim to bring these powerful local-global feature interactions to resource-constrained environments, broadening the practical applicability of ViTs for real-time dense prediction.\n\nIn conclusion, Vision Transformers have firmly established themselves as formidable architectures for dense prediction tasks. Their evolution from basic image classifiers to sophisticated pixel-level understanding models has been driven by architectural innovations that address computational complexity and multi-scale representation, coupled with specialized decoder designs and end-to-end Transformer frameworks. The recent advent of large-scale, self-supervised pre-trained foundation models has further empowered ViTs, providing exceptionally robust and generalizable backbones that significantly enhance performance across semantic segmentation, instance segmentation, and pose estimation. Future research will likely focus on optimizing the fusion of local and global features, developing more efficient architectures for real-time dense prediction, and exploring novel ways to leverage the rich contextual understanding of foundation models for even finer-grained and more complex pixel-level analyses.",
    "Specialized and Multimodal Applications": "\\subsection*{Specialized and Multimodal Applications}\n\nBeyond their foundational success in standard image classification and object detection, Vision Transformers (ViTs) have demonstrated remarkable versatility and adaptability across a diverse array of specialized and complex application areas. This subsection highlights their profound utility in challenging real-world scenarios, ranging from low-level image processing to the analysis of high-dimensional and multimodal data, showcasing their capacity to transcend traditional benchmarks and open new avenues for research and deployment.\n\nOne significant domain where ViTs have excelled is image restoration, a crucial low-level vision task demanding precise pixel-level manipulation and global consistency. Traditional convolutional neural networks (CNNs) often struggle with capturing long-range dependencies essential for coherent restoration across an entire image. \\textcite{liang2021v6x} addressed this by introducing SwinIR, a robust baseline for tasks such as super-resolution, denoising, and JPEG compression artifact reduction. SwinIR leverages the hierarchical Swin Transformer, which, through its shifted window attention, efficiently captures both local details and global structural information. This hierarchical design allows SwinIR to effectively model non-local correlations within images, crucial for hallucinating high-frequency details in super-resolution or removing noise while preserving fine textures, ultimately achieving superior performance over state-of-the-art CNNs with a reduced parameter count. Similarly, for visual saliency detection, which requires identifying the most visually prominent regions at a pixel level, \\textcite{liu2021jpu} proposed the Visual Saliency Transformer (VST). VST re-conceptualizes saliency detection as a convolution-free sequence-to-sequence prediction problem. By employing multi-level token fusion and a novel token upsampling method, VST effectively processes both RGB and RGB-D inputs, demonstrating ViT's inherent capability for fine-grained pixel-level understanding and its adaptability to multimodal inputs through a token-based multi-task decoder. The global attention mechanism of VST allows it to integrate contextual information across the entire image, which is vital for distinguishing salient objects from complex backgrounds, a task where local CNN receptive fields might fall short.\n\nThe ability of ViTs to process and fuse information from diverse and high-dimensional data modalities further underscores their adaptability. For hyperspectral image classification (HSIC), which involves analyzing data cubes with hundreds of spectral bands alongside spatial information, ViTs offer a powerful alternative to traditional methods. HSIC presents unique challenges due to its high dimensionality, spectral redundancy, and often limited labeled samples. \\textcite{zhao2024671} introduced the Groupwise Separable Convolutional Vision Transformer (GSC-ViT) to address these issues. GSC-ViT integrates a Groupwise Separable Convolution (GSC) module to efficiently extract local spectral-spatial features, mitigating the parameter burden of pure ViTs and enhancing local representation. Concurrently, a Groupwise Separable Multihead Self-Attention (GSSA) module captures both local and global spatial feature dependencies, allowing the model to effectively learn intricate relationships across the spectral and spatial dimensions. This hybrid approach demonstrates surprising classification performance even with fewer training samples, highlighting the ViT's capacity to model complex, high-dimensional data by judiciously combining inductive biases.\n\nBeyond visual imagery, ViTs have proven effective in processing non-standard data types, such as radar signals for human activity recognition (HAR). HAR from radar data is challenging due to the abstract nature of micro-Doppler signatures, which represent subtle motion patterns. \\textcite{huan202345b} proposed a Lightweight Hybrid Vision Transformer (LH-ViT) network for radar-based HAR. While the architectural details of LH-ViT are discussed in Section 5.2, its application here demonstrates how ViTs can effectively capture global temporal patterns within micro-Doppler spectrograms, which are crucial for distinguishing different human activities. The global attention mechanism allows the model to correlate features across the entire time-frequency representation, providing a more holistic understanding of the activity compared to local processing methods. The emphasis on a lightweight design further underscores the practical considerations for deploying such models in real-time or embedded radar systems.\n\nFurthermore, ViTs are increasingly pivotal in multimodal data fusion, where information from heterogeneous sources must be synergistically combined for robust understanding. For multimodal land use and land cover (LULC) classification, which often involves integrating optical, hyperspectral, LiDAR, and Synthetic Aperture Radar (SAR) data, \\textcite{yao2023sax} developed the Extended Vision Transformer (ExViT). This framework extends conventional ViTs with parallel branches, each tailored for a specific modality (e.g., hyperspectral and LiDAR/SAR data), utilizing position-shared ViTs and separable convolutions for efficient feature extraction. Critically, ExViT employs a cross-modality attention (CMA) module to facilitate dynamic information exchange and alignment between these heterogeneous modalities. This deep fusion mechanism, culminating in a robust decision-level fusion, leverages the ViT's token-based architecture to seamlessly integrate diverse data representations, leading to significantly improved classification accuracy and robustness compared to single-modality or simpler fusion approaches.\n\nCollectively, these applications underscore the remarkable versatility of Vision Transformers. They illustrate how ViTs can be meticulously engineered, often through hybrid architectures, specialized attention mechanisms, and novel tokenization strategies, to effectively process diverse data typesfrom standard images and spectral cubes to radar micro-Doppler mapsand tackle complex tasks like restoration, saliency detection, high-dimensional classification, and multimodal fusion. The continuous drive to optimize ViTs for efficiency and integrate them with domain-specific inductive biases (e.g., convolutions for local features) highlights a key development direction. Future research will likely focus on further enhancing their efficiency for edge deployment, exploring more sophisticated multimodal fusion strategies, and adapting them to novel data modalities and real-time inference challenges in increasingly complex environments, thereby expanding their utility across an even broader spectrum of scientific and industrial applications.",
    "Persistent Challenges: Computational Cost and Data Efficiency": "\\subsection*{Persistent Challenges: Computational Cost and Data Efficiency}\n\nDespite the remarkable progress in Vision Transformers (ViTs) across diverse computer vision tasks, their substantial computational cost and persistent reliance on extensive datasets remain critical hurdles, dictating their broader adoption and sustainability, particularly in resource-constrained environments and for specialized applications. While earlier sections have detailed numerous advancements, these challenges are far from resolved, continuously driving innovation to balance performance with practicality.\n\nThe foundational ViT \\cite{Dosovitskiy2021} introduced a quadratic computational complexity with respect to image resolution due to its global self-attention mechanism, alongside a significant appetite for massive pre-training datasets like JFT-300M. Subsequent architectural innovations, such as the hierarchical Swin Transformer \\cite{Liu2021} and Pyramid Vision Transformer (PVT) \\cite{Wang2021}, mitigated this by restricting attention to local windows or employing spatial reduction. Similarly, advanced attention mechanisms like Focal Attention \\cite{FocalAttention2022} aimed to capture context efficiently. However, these solutions often introduce trade-offs; local attention, while efficient, can sacrifice the model's inherent ability to capture true global dependencies, which was a core advantage of the original Transformer. Furthermore, even linear complexity can be prohibitive for processing gigapixel images in domains like digital pathology, highlighting that architectural fixes alone are insufficient.\n\nThe quest for computational efficiency extends beyond architectural design into post-training optimization and hardware-aware deployment. Model quantization, which reduces the precision of weights and activations, is crucial for inference on edge devices. Yet, ViTs present unique challenges for quantization due to the sensitivity of components like Layer Normalization and the non-uniform distribution of attention maps \\cite{lin2021utw}. While methods like Q-ViT \\cite{li20229zn} and FQ-ViT \\cite{lin2021utw} have pushed the limits of fully differentiable and post-training quantization, achieving near-lossless accuracy at lower bit-widths, the inherent complexity of ViT operations still demands specialized techniques to prevent severe performance degradation. Concurrently, model pruning techniques, which remove redundant parameters or operations, are vital. Research explores multi-dimensional compression, pruning attention heads, neurons, and even input sequences \\cite{song2022603, hou2022ver, yin2023029}. These methods, such as CP-ViT \\cite{song2022603} and GOHSP \\cite{yin2023029}, aim to identify and remove deleterious components while preserving accuracy, but the challenge lies in developing robust, generalized pruning criteria that do not require extensive retraining or compromise the model's representational power. Ultimately, for deployment in highly resource-constrained environments, dedicated hardware accelerators like ViTA \\cite{nag2023cfn} are becoming indispensable, demonstrating that a holistic hardware-software co-design approach is necessary to truly overcome the computational bottleneck.\n\nData efficiency, another critical challenge, has seen significant breakthroughs with self-supervised learning (SSL) and knowledge distillation. Data-efficient Image Transformers (DeiT) \\cite{Touvron2021} demonstrated competitive performance with ImageNet-1K pre-training through distillation from a CNN teacher, while Tokens-to-Token ViT (T2T-ViT) \\cite{Yuan2021} improved initial tokenization. The advent of Masked Autoencoders (MAE) \\cite{he2022masked} and advanced self-distillation techniques like DINOv2 \\cite{oquab2023dinov2} further revolutionized pre-training by enabling ViTs to learn robust features from vast quantities of *unlabeled* data. However, even with these advancements, the \"data hunger\" persists in different forms. Large foundation models, while powerful, still necessitate colossal datasets for pre-training, which may not be universally accessible or ethically diverse. Moreover, adapting these general-purpose models to specialized domains (e.g., medical imaging, remote sensing) where labeled data is scarce remains a significant hurdle. For instance, MAT-VIT \\cite{han2024f96} explores MAE-based auxiliary tasks to leverage unlabeled medical images, highlighting the ongoing need for domain-specific data efficiency strategies.\n\nThe traditional \"pre-train and fine-tune\" paradigm, while effective, also presents challenges. Training ViT-based object detectors from scratch, as explored by \\cite{hong2022ks6}, reveals that simply switching backbones from CNNs to ViTs does not generalize well, emphasizing the deep reliance of ViTs on large-scale pre-training. Furthermore, optimization techniques commonly used in deep learning do not always translate seamlessly to ViTs; for example, gradient accumulation, often used to simulate larger batch sizes, was found to decrease accuracy and increase training time for Swin Transformers \\cite{aburass2023qpf}, underscoring the need for ViT-specific optimization strategies. Even in knowledge distillation, researchers continue to refine techniques, with methods like Attention Distillation \\cite{wang2022pee} showing that self-supervised ViT students require more nuanced guidance to effectively close the performance gap with teachers. The emergence of \"simple\" hierarchical ViTs like Hiera \\cite{ryali202339q}, which strip away architectural \"bells-and-whistles\" when combined with strong SSL, further suggests that the true drivers of efficiency and performance might lie in the pre-training strategy rather than complex architectural designs.\n\nIn conclusion, the computational cost and data efficiency of Vision Transformers are not static problems but dynamic challenges that evolve with architectural innovations and deployment contexts. While significant strides have been made through hierarchical designs, advanced attention, quantization, pruning, and self-supervised learning, these solutions often introduce new trade-offs or highlight the need for further refinement. The continuous efforts to develop more parameter-efficient models, optimize training strategies, and improve generalization from limited data, especially for specialized tasks and resource-constrained environments, remain a fertile and critical ground for future research, pushing towards truly sustainable and ubiquitous visual AI.",
    "Interpretability, Robustness, and Generalization": "\\subsection{Interpretability, Robustness, and Generalization}\nThe trustworthy deployment of Vision Transformers (ViTs) in real-world applications critically hinges on their interpretability, robustness, and generalization capabilities. These aspects are paramount for fostering confidence in AI systems, ensuring their reliable and safe operation in diverse environments, and upholding ethical standards, especially in sensitive domains.\n\nA fundamental challenge in ViTs, as with many deep learning models, lies in interpreting their complex decision-making processes. Unlike Convolutional Neural Networks (CNNs) where feature maps often correspond to spatially localized patterns, the global self-attention mechanism in Transformers makes direct interpretation more elusive. Early attempts to interpret ViTs often relied on visualizing raw attention maps, but this approach has been critiqued for its limitations; raw attention scores do not directly represent feature importance or causal contributions to the output \\cite{jain2019attention}. More rigorous explainable AI (XAI) methods adapted for Transformers include attention rollout \\cite{abnar2020quantifying}, which propagates attention through layers to aggregate relevance, and Layer-wise Relevance Propagation (LRP) \\cite{bach2015pixel}, which decomposes the prediction backward through the network to assign relevance scores to input pixels. Gradient-based attribution methods, such as Grad-CAM \\cite{selvaraju2017grad} and its variants, have also been applied to ViTs to highlight salient regions influencing decisions. While these methods offer valuable insights, a comprehensive, human-understandable explanation of ViT reasoning, particularly in safety-critical applications, remains an active research area. Furthermore, some studies, such as \\cite{wang2022da0}, suggest that the attention mechanism itself might not be the sole or even primary driver of ViT success, demonstrating that it can be replaced by simpler shift operations with comparable performance. This raises questions about the true mechanistic role of attention and, consequently, the validity of solely relying on attention-based explanations. Efforts like Re-attention \\cite{zhou202105h} aimed to diversify attention maps in deeper ViTs to prevent \"attention collapse\" and improve representation learning, which indirectly aids in making attention patterns more informative, but does not fundamentally solve the interpretability challenge.\n\nBeyond interpretability, ensuring the robustness of ViTs against adversarial attacks and distribution shifts is crucial. Robustness is a multi-faceted concept, encompassing resilience to imperceptible adversarial perturbations, robustness to common image corruptions (e.g., noise, blur), and generalization to out-of-distribution (OOD) data. While initial ViTs demonstrated strong performance on benchmark datasets, their susceptibility to adversarial attacks was quickly identified, akin to CNNs \\cite{mao2021zr1, almalik20223wr}. For instance, \\textcite{almalik20223wr} proposed Self-Ensembling Vision Transformer (SEViT) to enhance adversarial robustness in medical image classification by leveraging intermediate feature representations and combining multiple classifiers.\n\nSignificant strides in improving ViT robustness and generalization have been made through advanced pre-training paradigms, particularly self-supervised learning (SSL). As discussed in Section 4, methods like Masked Autoencoders (MAE) \\cite{mae2022} and DINOv2 \\cite{dino2023} have enabled ViTs to learn powerful, transferable representations from vast amounts of unlabeled data. These SSL approaches force models to learn rich semantic features by reconstructing masked patches or performing self-distillation, leading to representations that are inherently more robust to variations and distribution shifts compared to purely supervised pre-training. For example, DINOv2 \\cite{dino2023} has shown remarkable performance on various downstream tasks and improved transferability, often outperforming supervised counterparts, by producing robust visual features without explicit supervision. The systematic evaluation by \\textcite{mao2021zr1} further highlights that certain ViT components can be detrimental to robustness, and by leveraging robust building blocks and techniques like position-aware attention scaling and patch-wise augmentation, they proposed Robust Vision Transformer (RVT) which achieved superior performance on benchmarks like ImageNet-C (common corruptions), ImageNet-R (renditions), and ImageNet-Sketch (sketches), demonstrating enhanced resilience to various distribution shifts.\n\nThe pursuit of \"Vision Foundation Models,\" as explored in Section 4.3, further exemplifies this drive towards universal robustness and generalization. Models scaled to billions of parameters, such as those by \\textcite{scalingvit2023} and \\textcite{internimage2023}, aim to serve as versatile backbones capable of adapting to a wide array of vision tasks with minimal fine-tuning. This extreme scaling, often coupled with advanced self-supervised pre-training, is hypothesized to imbue models with a deeper understanding of visual semantics, thereby enhancing their generalization to novel situations and improving their resilience to variations in input data.\n\nMoreover, the integration of inductive biases from CNNs into Transformer architectures has also contributed to developing more robust and generalizable models. Hybrid architectures, such as ConvNeXt V2 \\cite{convnextv22023} (discussed in Section 5.1), which co-designs CNNs with MAE pre-training, and InternImage \\cite{internimage2023} (also from Section 5.1), which incorporates deformable convolutions into large-scale vision foundation models, represent a synergistic approach. By combining the local feature extraction strengths of CNNs with the global context modeling of Transformers, these models aim to achieve a more comprehensive and robust understanding of visual data, often exhibiting better performance on out-of-distribution tasks. For instance, \\textcite{zhou2021rtn} systematically investigated the transfer learning ability of ConvNets and vision transformers across 15 downstream tasks, observing consistent advantages for Transformer-based backbones on 13 tasks, particularly noting their robustness in multi-task learning and their reliance on whole-network fine-tuning for optimal transfer. Practical considerations for real-world deployment also necessitate efficient architectures that maintain performance, contributing to operational robustness. While not directly addressing adversarial or OOD robustness, efficient ViT variants like MobileViT V2 \\cite{mobilevitv22023} (from Section 5.2) ensure reliable performance within resource constraints, which is a form of practical robustness for deployment.\n\nDespite significant strides in enhancing the robustness and generalization of ViTs through scaling, advanced self-supervised learning, and hybrid designs, the challenge of achieving true interpretability remains complex. While methods offer glimpses into model behavior, a holistic, causal understanding of ViT decisions, especially in safety-critical applications, is still an active research area. Future work must continue to explore novel techniques for transparent model design, robust evaluation against diverse adversarial threats and OOD data, and the development of ViTs that are not only high-performing but also inherently understandable and trustworthy.",
    "Future Trends and Open Problems": "\\subsection{Future Trends and Open Problems}\n\nThe evolution of Vision Transformers (ViTs) is rapidly accelerating, pushing the frontiers of artificial intelligence towards more comprehensive, intelligent, and human-like visual systems. This subsection looks ahead, identifying cutting-edge research directions and unresolved questions that will define the next generation of ViT architectures. While Section 7.1 and 7.2 address persistent challenges related to efficiency, robustness, and interpretability, this section focuses on more speculative, transformative trends and deeper, unsolved theoretical problems that extend beyond incremental improvements.\n\nA significant future trend lies in the exploration of **novel architectural paradigms that fundamentally rethink the self-attention mechanism**. While ViTs excel at global context modeling, their quadratic computational complexity remains a bottleneck. The integration of ViTs with State-Space Models (SSMs), such as Mamba, represents a particularly exciting frontier. MambaVision \\cite{hatamizadeh2024xr6} exemplifies this by redesigning the Mamba formulation for visual features and strategically incorporating self-attention blocks in later layers to effectively capture long-range spatial dependencies. This hybrid approach demonstrates state-of-the-art performance and throughput, suggesting a promising path for combining the strengths of different architectures to overcome individual limitations. Beyond such integrations, a more radical open question is whether attention is truly indispensable. Research like ShiftViT \\cite{wang2022da0} explores this by replacing attention layers with a zero-parameter shift operation, achieving competitive performance and prompting a re-evaluation of the core mechanisms driving ViT success. This suggests a future where attention might be replaced or heavily augmented by simpler, more efficient operations, as also highlighted by broader surveys on attention mechanism redesigns \\cite{heidari2024d9k}. Furthermore, the development of adaptive architectures that can dynamically adjust their computational patterns based on input characteristics, such as Dynamic Window Vision Transformer (DW-ViT) \\cite{ren2022ifo} which assigns windows of different sizes to various attention heads, offers a pathway to more flexible and robust visual understanding beyond fixed windowing.\n\nThe ultimate ambition for many researchers is the development of truly **universal and multimodal visual foundation models** capable of sophisticated reasoning. Building upon the success of large-scale pre-training \\cite{ICLR2023_Scaling_Vision_Transformers_to_1_Billion_Parameters, ICLR2023_DINOv2}, the field is now focused on creating generalist backbones that transcend individual tasks and modalities. GiT \\cite{wang20249qa} represents a significant step, proposing a vanilla ViT framework with a universal language interface that can simultaneously handle diverse vision tasksfrom image captioning to object detection and segmentationwithout task-specific modifications. This approach aims to narrow the architectural gap between vision and language models, fostering mutual enhancement across tasks. The extension of these models to genuinely multimodal reasoning, integrating vision not only with language but also with audio, 3D data, and other sensor modalities, is paramount. This requires novel architectural designs and pre-training strategies to effectively fuse information from disparate sources, enabling a more holistic and human-like understanding. For instance, unifying 2D and 3D vision remains a significant challenge, with efforts like Simple3D-Former \\cite{wang2022gq4} demonstrating that a standard 2D ViT can be minimally customized to perform robustly on 3D tasks, suggesting a path towards more transferable architectures. In practical applications, multimodal fusion is already yielding benefits, such as ViT-FuseNet \\cite{zhou2024toe} for vehicle-infrastructure cooperative perception (LiDAR and camera) and NF-DVT \\cite{pan20249k5} for monocular 3D object detection (RGB and depth), but the grand challenge is to generalize these fusion capabilities across *any* combination of modalities.\n\nBeyond architectural and multimodal advancements, several **deeper open problems in visual intelligence** persist, representing grand challenges for the field. One critical area is **compositional generalization and relational reasoning**. While ViTs can learn complex patterns, their ability to reason about novel combinations of objects and their relationships, similar to human cognition, remains limited. RelViT \\cite{ma2022vf3} offers a concept-guided approach to improve relational reasoning and systematic generalization, but achieving robust compositional understanding across diverse, unseen scenarios is a profound theoretical and practical hurdle. This involves understanding how ViTs can move beyond statistical correlations to infer causal relationships within visual scenes, a key component of true intelligence. Another significant challenge is **lifelong or continual learning**, where ViTs can adapt to new tasks and data streams without catastrophically forgetting previously acquired knowledge. This is essential for deploying models in dynamic real-world environments that evolve over time. Furthermore, integrating ViTs into **embodied AI systems** that interact with the physical world, requiring real-time perception, planning, and action, presents complex challenges in bridging the gap between static image understanding and dynamic environmental interaction.\n\nFinally, as ViTs become increasingly powerful and ubiquitous, the **ethical implications and the need for trustworthy AI systems** become paramount \\cite{hassija2025wq3}. While Section 7.2 discusses interpretability and robustness as ongoing challenges, the future demands a proactive approach to designing ViTs that are inherently fair, transparent, and privacy-preserving. Developing generalizable interpretability methods for massive, multimodal foundation models is a major open problem. For instance, prototype-based interpretable ViTs like ProtoViT \\cite{ma2024uan} offer local explanations by comparing image parts to learned prototypes, but scaling such fine-grained interpretability to the complexity of universal foundation models remains an active research area. Mitigating biases embedded in training data, ensuring equitable performance across diverse demographic groups, and developing robust privacy-preserving mechanisms for visual data are not just technical hurdles but critical societal imperatives for the responsible deployment of future vision systems.\n\nIn conclusion, the future of Vision Transformers is characterized by a relentless pursuit of greater efficiency through novel architectural paradigms, the development of truly generalist and multimodal foundation models, and a deeper engagement with fundamental problems of visual intelligence such as compositional and causal reasoning. Crucially, addressing the ethical dimensions of these powerful systems will be indispensable for realizing their transformative potential in a responsible and beneficial manner."
  },
  "latex_document": "\\documentclass[12pt,a4paper]{article}\n    \\usepackage[utf8]{inputenc}\n    \\usepackage[T1]{fontenc}\n    \\usepackage{amsmath,amsfonts,amssymb}\n    \\usepackage{graphicx}\n    \\usepackage[margin=2.5cm]{geometry}\n    \\usepackage{setspace}\n    \\usepackage{natbib}\n    \\usepackage{url}\n    \\usepackage{hyperref}\n    \\usepackage{booktabs}\n    \\usepackage{longtable}\n    \\usepackage{array}\n    \\usepackage{multirow}\n    \\usepackage{wrapfig}\n    \\usepackage{float}\n    \\usepackage{colortbl}\n    \\usepackage{pdflscape}\n    \\usepackage{tabu}\n    \\usepackage{threeparttable}\n    \\usepackage{threeparttablex}\n    \\usepackage[normalem]{ulem}\n    \\usepackage{makecell}\n    \\usepackage{xcolor}\n\n    % Set line spacing\n    \\doublespacing\n\n    % Configure hyperref\n    \\hypersetup{\n        colorlinks=true,\n        linkcolor=blue,\n        filecolor=magenta,      \n        urlcolor=cyan,\n        citecolor=red,\n    }\n\n    % Title and author information\n    \\title{A Comprehensive Literature Review with Self-Reflection}\n    \\author{Literature Review}\n    \\date{\\today}\n\n    \\begin{document}\n\n    \\maketitle\n\n    % Abstract (optional)\n    \\begin{abstract}\n    This literature review provides a comprehensive analysis of recent research in the field. The review synthesizes findings from 367 research papers, identifying key themes, methodological approaches, and future research directions.\n    \\end{abstract}\n\n    \\newpage\n    \\tableofcontents\n    \\newpage\n\n    \\label{sec:introduction}\n\n\\section{Introduction}\n\\label{sec:introduction}\n\n\\subsection{Motivation for Visual Transformers}\n\\label{sec:1\\_1\\_motivation\\_for\\_visual\\_transformers}\n\nThe advent of Vision Transformers (ViTs) marks a significant paradigm shift in computer vision, primarily driven by the inherent limitations of Convolutional Neural Networks (CNNs) in capturing global contextual relationships and the groundbreaking success of Transformer architectures in Natural Language Processing (NLP). For an extended period, CNNs were the undisputed standard for visual recognition tasks, owing to their powerful inductive biases such as locality and translation equivariance. These biases, intrinsically embedded through spatially restricted convolutional kernels, are exceptionally effective at extracting local features and constructing hierarchical representations \\cite{han2020yk0}. However, this very strength became a critical constraint when models needed to comprehend long-range dependencies and holistic contextual information across an entire image. CNNs, by design, inherently struggle to model interactions between spatially distant parts of an image without resorting to increasingly deep architectures, significantly larger receptive fields achieved through techniques like dilated convolutions, or complex add-on modules such as non-local blocks \\cite{Wang2018NonlocalNN, zhou2021rtn}. While these methods attempted to mitigate the issue by expanding the effective receptive field, they often introduced increased computational complexity or did not fundamentally alter the local processing paradigm. As highlighted by \\cite{gheflati202131i}, CNNs' restricted local receptive fields inherently limit their capacity for global context learning, making it challenging to capture comprehensive image understanding. Similarly, \\cite{liu2022249} and \\cite{karagz2024ukp} explicitly note that while CNNs excel at representing local spatial features, they find it difficult to capture global information, underscoring a fundamental gap in their representational power for certain tasks.\n\nThe landscape of deep learning was profoundly reshaped by the Transformer architecture, introduced by \\cite{Vaswani2017} in NLP. This model demonstrated the unparalleled power of self-attention mechanisms to capture global dependencies within sequences, processing information from all input parts simultaneously. Unlike recurrent networks that process sequentially or CNNs with their localized focus, the Transformer's global attention mechanism, largely devoid of strong inductive biases about local connectivity, offered a compelling alternative for learning flexible, context-aware representations. The remarkable ability of Transformers to achieve state-of-the-art performance across diverse language tasks inspired researchers to question whether a similar global approach could unlock new capabilities in computer vision, overcoming the limitations of CNNs in capturing holistic image understanding and offering a less constrained path to feature learning \\cite{han2020yk0}. This inspiration was rooted in the desire for models that could intrinsically understand the relationships between any two parts of an image, regardless of their spatial separation, without being constrained by fixed-size kernels.\n\nThis profound inspiration culminated in the seminal work by \\cite{Dosovitskiy2021}, which introduced the Vision Transformer (ViT). This pioneering paper directly adapted the pure Transformer architecture to image recognition by treating image patches as a sequence of tokens, demonstrating that a model built entirely on self-attention, when trained on sufficiently large datasets, could achieve state-of-the-art performance without any convolutional layers. This was a profound challenge to the long-held paradigm of convolutional feature extraction, showcasing that global attention could indeed learn powerful visual representations and capture long-range dependencies more effectively than CNNs. The ability of ViTs to learn highly transferable representations, often outperforming CNNs on various downstream tasks when properly pre-trained, further solidified its revolutionary potential \\cite{zhou2021rtn}. For instance, studies like \\cite{htten2022lui} have shown that ViT models can achieve equivalent or even superior performance to CNNs in complex industrial visual inspection tasks, even with sparse data, highlighting their robust feature learning capabilities. This successful adaptation of a pure Transformer to vision tasks marked a definitive turning point, setting the stage for a new architectural lineage in computer vision that prioritizes global context and flexible feature learning.\n\\subsection{Overview of Vision Models: From CNNs to Transformers}\n\\label{sec:1\\_2\\_overview\\_of\\_vision\\_models:\\_from\\_cnns\\_to\\_transformers}\n\nThe trajectory of computer vision research has been marked by a significant paradigm shift, evolving from the long-standing dominance of Convolutional Neural Networks (CNNs) to the recent ascendance of Transformer architectures. For decades, CNNs served as the foundational backbone for visual recognition tasks, owing their success to inherent inductive biases that align well with the hierarchical and local nature of visual data \\cite{han2020yk0}. Key among these biases are local receptive fields, which allow neurons to process only a small, localized region of the input; weight sharing, which enables the detection of features regardless of their position; and spatial pooling, which progressively reduces spatial dimensions while retaining essential information. Architectures like AlexNet \\cite{alexnet\\_2012}, VGG \\cite{vgg\\_2014}, ResNet \\cite{resnet\\_2016}, and Inception \\cite{inception\\_2015} exemplified this era, demonstrating remarkable capabilities in tasks ranging from image classification to object detection by building increasingly complex and abstract representations through stacked convolutional layers. Their architectural evolution often involved increasing depth, introducing residual connections to mitigate vanishing gradients, and designing more intricate modules to enhance representational power, all while retaining the core principle of localized feature extraction.\n\nDespite their profound success and continuous advancements, CNNs inherently faced limitations, particularly in effectively modeling global contextual information across an entire image. Their reliance on local operations meant that capturing long-range dependencies required very deep networks, which could be computationally intensive and sometimes struggled to integrate information from widely separated regions efficiently. This inductive bias towards locality, while beneficial for many tasks, could also constrain their ability to understand broader semantic relationships or complex spatial layouts without extensive architectural modifications \\cite{han2020yk0, zhou2021rtn}.\n\nA pivotal moment arrived with the introduction of the Transformer architecture \\cite{attention\\_is\\_all\\_you\\_need\\_2017}, originally conceived for natural language processing (NLP). Its core innovation, the self-attention mechanism, revolutionized sequence modeling by enabling models to weigh the importance of different parts of an input sequence, fostering a more global understanding of relationships without relying on recurrent or convolutional operations. This mechanism allowed for parallel processing of inputs and offered an unprecedented ability to capture long-range dependencies directly. The profound success of Transformers in NLP inspired researchers to explore their applicability to other domains, including computer vision \\cite{heidari2024d9k}.\n\nThis exploration culminated in the introduction of the Vision Transformer (ViT) \\cite{image\\_is\\_worth\\_16x16\\_words\\_2021}. The ViT architecture ingeniously adapted the standard Transformer encoder for image recognition by treating images as sequences of non-overlapping patches. Each patch was linearly embedded into a token, and positional embeddings were added to retain spatial information. These visual tokens were then fed into a Transformer encoder, leveraging its global self-attention mechanism to capture relationships between patches across the entire image. This marked a significant paradigm shift, demonstrating that models could achieve competitive performance in image recognition without relying on convolutional layers, instead directly leveraging the Transformer's ability to capture global dependencies. This approach fundamentally altered how visual features were extracted and processed, moving away from predefined spatial hierarchies and towards a more flexible, data-driven understanding of global context \\cite{han2020yk0}.\n\nThe initial success of ViTs, however, was accompanied by practical challenges. Pure ViT models often required significantly larger datasets for pre-training compared to CNNs to achieve comparable performance, largely due to their reduced inherent inductive biases \\cite{image\\_is\\_worth\\_16x16\\_words\\_2021, zhou2021rtn}. Furthermore, the quadratic computational complexity of global self-attention with respect to the number of tokens (and thus image resolution) posed considerable computational costs, limiting their applicability to high-resolution images or resource-constrained environments. These initial limitations became the primary drivers for an explosion of subsequent research aimed at enhancing the efficiency, data-effectiveness, and architectural flexibility of Vision Transformers.\n\nIn conclusion, the transition from CNNs to Transformers represents a fundamental evolution in computer vision, shifting the focus from local, hierarchical feature extraction to global context understanding through attention mechanisms. While CNNs provided a robust foundation, the advent of ViTs offered a new paradigm capable of overcoming some of their inherent limitations, particularly in modeling long-range dependencies. This breakthrough, despite its initial challenges related to data hunger and computational expense, has spurred extensive research into architectural refinements, efficient training methodologies, and hybrid models. These ongoing efforts, which will be explored in detail in the subsequent sections of this review, are continuously solidifying the position of Vision Transformers as versatile and powerful backbones for a wide array of computer vision tasks, fundamentally altering how visual features are extracted and processed.\n\\subsection{Scope and Organization of the Review}\n\\label{sec:1\\_3\\_scope\\_\\_and\\_\\_organization\\_of\\_the\\_review}\n\nThis literature review aims to provide a comprehensive and systematically organized overview of Vision Transformers (ViTs), tracing their rapid evolution from foundational principles to their current state as a dominant paradigm in computer vision. By delineating the boundaries and structure of this review, readers are provided with a clear roadmap, ensuring a coherent and comprehensive understanding of the field's progression, its intricate interconnections, and the critical research trajectories that continue to shape its future. This structured approach, similar to recent surveys on ViTs that categorize design techniques and innovative methods \\cite{heidari2024d9k, hassija2025wq3}, is designed to facilitate a deep understanding of the architectural, methodological, and application-driven advancements.\n\nThe review is meticulously structured into seven main sections, each building upon the preceding one to offer a logical and progressive narrative of ViT development:\n\nThe journey commences with \\textbf{Section 1: Introduction}, which establishes the overarching context for Vision Transformers. This section begins by outlining the motivation behind their emergence, particularly in response to the inherent limitations of traditional Convolutional Neural Networks (CNNs) in capturing global dependencies (Subsection 1.1). It then provides a concise historical overview of vision models, highlighting the pivotal paradigm shift from CNN dominance to the rise of Transformers (Subsection 1.2). This introductory section sets the stage for the detailed exploration that follows, providing essential background for understanding the subsequent technical discussions.\n\n\\textbf{Section 2: Foundational Concepts of Vision Transformers}, delves into the core principles that underpin ViTs. It starts with a brief recap of the original Transformer architecture from Natural Language Processing (Subsection 2.1), explaining its key components. Subsequently, it details the pioneering work that adapted this architecture for visual data, introducing the original Vision Transformer (ViT) and its innovative approach to image tokenization and global self-attention (Subsection 2.2). Crucially, this section also critically examines the initial challenges and limitations of pure ViTs, such as their significant data hunger and quadratic computational complexity (Subsection 2.3), thereby establishing the impetus for subsequent research and architectural refinements.\n\nBuilding upon the identified limitations, \\textbf{Section 3: Architectural Enhancements and Efficiency}, explores the crucial innovations designed to make ViTs more practical and efficient. This section is segmented to discuss hierarchical Vision Transformers (Subsection 3.1), which address multi-scale feature learning and computational efficiency; data-efficient training and tokenization strategies (Subsection 3.2), aimed at reducing reliance on massive datasets; and advanced attention mechanisms (Subsection 3.3), which refine the core self-attention process for improved performance and reduced overhead. These advancements collectively broadened the applicability of ViTs beyond initial classification benchmarks.\n\n\\textbf{Section 4: Self-Supervised Learning and Vision Foundation Models}, shifts focus to the transformative role of self-supervised learning (SSL) in scaling ViTs. It details powerful SSL techniques like Masked Autoencoders (MAE) for pre-training (Subsection 4.1) and explores other contrastive and knowledge distillation approaches (Subsection 4.2). The section culminates by discussing the profound trend of scaling Vision Transformers to create \"Vision Foundation Models\" (Subsection 4.3), which aim to learn universal visual representations adaptable to a wide array of downstream tasks with minimal fine-tuning, marking a significant paradigm shift towards general-purpose visual intelligence.\n\nFurther addressing practical deployment and architectural synergy, \\textbf{Section 5: Hybrid Architectures and Mobile-Friendly Designs}, examines the strategic convergence of convolutional and Transformer architectures. It explores synergistic CNN-Transformer designs (Subsection 5.1) that leverage the complementary strengths of both paradigms. Additionally, it addresses the critical need for efficiency by detailing lightweight and mobile-optimized Vision Transformers (Subsection 5.2) for resource-constrained environments. The section also looks into novel hybrid paradigms (Subsection 5.3), such as the integration of state-space models, showcasing the continuous innovation in architectural design.\n\n\\textbf{Section 6: Applications of Visual Transformers}, comprehensively showcases the extensive and diverse utility of ViTs across the spectrum of computer vision tasks. This section illustrates their state-of-the-art performance in core vision tasks like classification and object detection (Subsection 6.1), their adaptation for dense prediction tasks such as segmentation and pose estimation (Subsection 6.2), and their remarkable versatility in specialized and multimodal applications (Subsection 6.3). This breadth of application underscores the profound impact of ViTs on various real-world scenarios.\n\nFinally, \\textbf{Section 7: Future Outlook: Challenges and Opportunities}, synthesizes the current state of ViTs by identifying persistent challenges, such as computational cost and data efficiency (Subsection 7.1), and critical aspects like interpretability, robustness, and generalization (Subsection 7.2). It then outlines promising future trends and open problems (Subsection 7.3), including novel architectural explorations and the integration of ViTs into broader multimodal AI systems. This concluding section encourages responsible development and application of this rapidly evolving technology, ensuring beneficial and equitable societal impact. Through this structured exploration, the review aims to provide a comprehensive and critically informed understanding of the Vision Transformer landscape.\n\n\n\\label{sec:foundational_concepts_of_vision_transformers}\n\n\\section{Foundational Concepts of Vision Transformers}\n\\label{sec:foundational\\_concepts\\_of\\_vision\\_transformers}\n\n\\subsection{The Transformer Architecture: A Brief Recap}\n\\label{sec:2\\_1\\_the\\_transformer\\_architecture:\\_a\\_brief\\_recap}\n\nThe Transformer architecture, introduced by \\cite{vaswani2017attention}, fundamentally revolutionized sequence modeling in Natural Language Processing (NLP) by moving away from recurrent and convolutional networks. This paradigm shift enabled unprecedented parallel processing capabilities and a more effective capture of long-range dependencies, laying the groundwork for its subsequent adaptation to diverse data modalities, including visual data.\n\nAt its core, the Transformer adopts an encoder-decoder structure. The encoder processes the input sequence, generating a rich, context-aware representation, which the decoder then utilizes to generate the output sequence, often in an auto-regressive manner. This entire architecture is built exclusively on attention mechanisms, eliminating the need for sequential processing inherent in Recurrent Neural Networks (RNNs) or the local receptive fields characteristic of Convolutional Neural Networks (CNNs).\n\nThe central innovation is the \\textbf{self-attention mechanism}, which allows the model to weigh the importance of different parts of the input sequence when processing each element. For every token in the sequence, three distinct vectors are computed: a Query (Q), a Key (K), and a Value (V). The attention scores are calculated by taking the dot product of the Query with all Keys, scaling by the square root of the key dimension to prevent vanishing gradients, and applying a softmax function. These scores then weight the corresponding Value vectors, which are summed to produce the output for that token. This mechanism inherently allows each token to attend to any other token in the sequence, regardless of their distance, thereby effectively capturing long-range dependencies that were challenging for traditional recurrent networks.\n\nTo enhance the model's ability to capture diverse relationships and focus on different aspects of the input simultaneously, the Transformer employs \\textbf{multi-head attention}. Instead of performing a single attention function, the Q, K, and V vectors are linearly projected multiple times into different lower-dimensional subspaces. Each of these \"heads\" then independently computes its own scaled dot-product attention. The outputs from all attention heads are concatenated and then linearly transformed back into the desired output dimension. This parallel processing of multiple attention mechanisms allows the model to learn various types of relationships and attend to different positions, enriching the overall representation and improving the model's capacity to model complex data.\n\nA critical aspect of sequence modeling is preserving the order of elements, which self-attention inherently lacks due to its permutation-invariant nature. To address this, the Transformer injects sequence order information through \\textbf{positional encoding}. These are vectors added directly to the input embeddings at the bottom of the encoder and decoder stacks. The original Transformer used fixed sinusoidal functions of different frequencies, allowing the model to easily learn relative positions. This mechanism ensures that while the attention mechanism processes tokens in parallel, the model retains crucial information about their relative and absolute positions within the sequence.\n\nEach encoder and decoder block also contains a position-wise fully connected feed-forward network, applied independently and identically to each position. This network typically consists of two linear transformations with a ReLU activation in between. Additionally, layer normalization is applied before each sub-layer (self-attention and feed-forward network), followed by a residual connection, aiding in stable training and gradient flow through deep networks.\n\nThe design of the Transformer, particularly its reliance on self-attention, marked a significant departure from previous state-of-the-art models like Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). Unlike RNNs, which process sequences token by token, leading to computational bottlenecks and difficulties with long-range dependencies due to vanishing/exploding gradients, the Transformer processes all tokens in parallel. This parallelization drastically reduces training time. Furthermore, while CNNs capture local features through fixed-size kernels, the Transformer's self-attention mechanism provides a global receptive field from the very first layer, allowing it to directly model relationships between any two tokens irrespective of their distance. This global perspective and parallel processing capability were revolutionary, establishing a new paradigm for sequence modeling that would profoundly influence subsequent research across various domains.\n\\subsection{The Original Vision Transformer (ViT)}\n\\label{sec:2\\_2\\_the\\_original\\_vision\\_transformer\\_(vit)}\n\nThe remarkable success of the Transformer architecture \\cite{vaswani2017attention} in natural language processing (NLP), primarily driven by its potent self-attention mechanism, ignited a fundamental inquiry within the computer vision community: could a similar, convolution-free paradigm effectively process and interpret visual data? This pivotal question was comprehensively addressed by Dosovitskiy et al. \\cite{dosovitskiy2021image} with the introduction of the Vision Transformer (ViT). Their pioneering work directly adapted the Transformer for image recognition, unequivocally demonstrating its viability as a powerful and distinct alternative to traditional Convolutional Neural Networks (CNNs) and marking a significant paradigm shift in visual modeling \\cite{han2020yk0, huo2023e5h}.\n\nThe core innovation of the original ViT lies in its elegant yet straightforward approach to re-conceptualizing an image as a sequence of discrete tokens, directly analogous to how words are treated in NLP. The process commences by partitioning an input image into a grid of fixed-size, non-overlapping square patches, typically $16 \\times 16$ pixels. Each of these patches is then flattened into a one-dimensional vector and subsequently projected linearly into a fixed-dimension embedding space. This transformation yields a sequence of \"visual tokens,\" where each token represents a localized region of the original image. Crucially, to compensate for the spatial information lost during the flattening and tokenization process, learnable positional embeddings are added to these patch embeddings. These embeddings are vital for retaining the relative spatial arrangement of patches, allowing the model to understand the geometric relationships between different parts of the image.\n\nTo facilitate image classification, a special, learnable \"class token\" is prepended to this sequence of embedded patches and their corresponding positional embeddings. This class token acts as a global representation of the entire image, accumulating information from all other visual tokens as it propagates through the Transformer layers. The augmented sequence then enters a standard Transformer encoder, which consists of multiple identical layers, each comprising a multi-head self-attention (MSA) block and a feed-forward network (FFN). The global self-attention mechanism within the MSA block is the cornerstone of ViT's ability to capture long-range dependencies; it enables each visual token (patch or class token) to attend to every other token in the sequence, thereby integrating global contextual information across the entire image. This stands in stark contrast to CNNs, which primarily rely on local receptive fields and hierarchical processing to build global context. The final state of the class token, after traversing the Transformer encoder, is then fed into a simple Multi-Layer Perceptron (MLP) head, which outputs the predicted class probabilities for the image.\n\nThis architectural departure from CNNs demonstrated remarkable empirical performance. Dosovitskiy et al. \\cite{dosovitskiy2021image} showcased that ViT achieved state-of-the-art results on large-scale image classification benchmarks, including ImageNet and JFT-300M. A critical finding was that ViTs, when pre-trained on sufficiently vast datasets (e.g., JFT-300M with 300 million images), could significantly outperform CNNs of comparable size. This performance advantage was attributed to the Transformer's ability to learn more flexible and global representations, unconstrained by the strong inductive biases (like locality and translation equivariance) inherent in CNNs. While these inductive biases make CNNs highly data-efficient on smaller datasets, ViT's \"data hunger\" meant it required extensive pre-training to fully exploit its capacity and generalize effectively \\cite{huo2023e5h}. The breakthrough performance of ViT not only challenged the long-standing dominance of CNNs in computer vision but also fundamentally reoriented research, establishing a new and highly influential direction for visual modeling based on global attention mechanisms \\cite{han2020yk0}. However, this paradigm shift, while promising, also introduced its own set of initial challenges, particularly concerning its substantial data requirements and the computational implications of its global self-attention mechanism, which necessitated further architectural refinements and training innovations.\n\\subsection{Initial Challenges and Limitations of Pure ViTs}\n\\label{sec:2\\_3\\_initial\\_challenges\\_\\_and\\_\\_limitations\\_of\\_pure\\_vits}\n\nDespite the groundbreaking success of the Vision Transformer (ViT) in demonstrating that a pure Transformer architecture could achieve state-of-the-art performance in image classification, the initial iterations of these models presented several significant challenges and inherent limitations \\cite{ICLR2019}. These drawbacks, which became apparent despite their impressive performance on large-scale datasets, necessitated extensive subsequent research into their optimization and refinement.\n\nA primary limitation of the original pure ViT architecture, as introduced by \\cite{ICLR2019}, was its substantial data requirement for pre-training. Unlike Convolutional Neural Networks (CNNs) that inherently possess inductive biases such as locality and translation equivariance, pure ViTs lack these built-in priors. Consequently, they must learn these fundamental visual properties from scratch, demanding massive datasets like JFT-300M to achieve competitive performance, making them less accessible for researchers without access to such vast resources. This data hunger was a critical bottleneck, prompting early efforts to make ViTs more data-efficient, such as through knowledge distillation \\cite{Touvron2021} or refined tokenization strategies \\cite{Yuan2021}.\n\nAnother significant challenge stemmed from the computational complexity of the global self-attention mechanism, which is central to pure ViTs. For an input image of $H \\times W$ resolution, if flattened into $N$ patches, the self-attention operation scales quadratically with the number of patches, $O(N^2)$, or equivalently, $O((HW)^2)$ with respect to the image resolution. This quadratic complexity made pure ViTs computationally prohibitive for high-resolution images and impractical for tasks requiring fine-grained, pixel-level understanding, such as dense prediction. This limitation spurred the development of hierarchical ViTs that employed localized attention mechanisms, like the shifted window attention in Swin Transformer \\cite{ICLR2021} or pyramid structures in PVT \\cite{ICCV2021}, to reduce computational cost and enable multi-scale feature extraction.\n\nBeyond data and computational demands, pure ViTs also exhibited architectural limitations that hindered their scalability and applicability. For instance, deeper pure ViT models were found to suffer from an \"attention collapse\" issue \\cite{zhou202105h}. As the network depth increased, the attention maps across layers tended to become increasingly similar, leading to redundant feature learning and performance saturation rather than improvement. To address this, \\cite{zhou202105h} proposed Re-attention to diversify attention maps and enable consistent performance gains in deeper ViTs.\n\nFurthermore, the lack of inherent hierarchical feature representation and strong locality inductive biases in pure ViTs made them less naturally suited for dense prediction tasks like object detection and semantic segmentation, which traditionally benefit from multi-scale feature pyramids and local context modeling. While studies like \\cite{li2022raj} demonstrated that plain ViT backbones could be adapted for object detection, they often required significant modifications, such as building simple feature pyramids from single-scale feature maps and incorporating cross-window propagation blocks, to achieve competitive results. This highlighted that the original pure ViT design was not intrinsically optimized for these tasks without architectural augmentations.\n\nIn summary, the initial pure ViT architectures, despite their paradigm-shifting performance, were characterized by their substantial data requirements, quadratic computational complexity, and a lack of built-in inductive biases that are naturally present in CNNs. These factors collectively made them less efficient, harder to train effectively on smaller datasets, and less versatile for a broad range of computer vision tasks without significant modifications. These inherent limitations became a fertile ground for subsequent research, driving the evolution towards more efficient, data-agnostic, and task-adaptable Vision Transformer variants.\n\n\n\\label{sec:architectural_enhancements_and_efficiency}\n\n\\section{Architectural Enhancements and Efficiency}\n\\label{sec:architectural\\_enhancements\\_\\_and\\_\\_efficiency}\n\n\\subsection{Hierarchical Vision Transformers}\n\\label{sec:3\\_1\\_hierarchical\\_vision\\_transformers}\n\nEarly Vision Transformers (ViTs) \\cite{dosovitskiy2020image} marked a significant paradigm shift in computer vision, yet their direct application to tasks requiring fine-grained spatial understanding, such as dense prediction, or processing of high-resolution images, was hampered by two primary limitations \\cite{hassija2025wq3, heidari2024d9k}. Firstly, the global self-attention mechanism exhibited quadratic computational complexity with respect to the number of input tokens (image patches), making it prohibitively expensive for large inputs. Secondly, pure ViTs lacked the inherent multi-scale feature representations and inductive biases (like locality) that Convolutional Neural Networks (CNNs) naturally possess, which are crucial for capturing both global context and local details across various scales. To address these challenges, a critical line of research emerged, focusing on the development of hierarchical Vision Transformers that integrate multi-scale processing and more efficient attention mechanisms, thereby mimicking CNN-like feature pyramids and achieving linear computational complexity.\n\nOne of the foundational models in this category is the \\textbf{Pyramid Vision Transformer (PVT)} \\cite{wang2021pyramid}. PVT introduced a progressive shrinking pyramid structure, akin to feature pyramids in CNNs, by gradually reducing the resolution of feature maps in deeper layers. This hierarchical design allows for the generation of multi-scale features, which are essential for dense prediction tasks. To manage the computational cost, PVT employs a Spatial-Reduction Attention (SRA) module. Unlike global self-attention, SRA reduces the spatial dimension of the key and value matrices before computing attention, effectively lowering the computational complexity from quadratic to linear with respect to image size. This innovation enabled PVT to serve as a robust backbone for tasks like object detection and semantic segmentation, demonstrating the viability of hierarchical Transformers for a broader range of vision applications \\cite{hassija2025wq3}.\n\nBuilding upon the success of hierarchical designs, the \\textbf{Swin Transformer} \\cite{liu2021ljs} emerged as another seminal work, further solidifying the potential of these architectures. Swin Transformer also adopts a hierarchical structure through progressive patch merging, generating multi-scale feature maps. Its core innovation, however, lies in the \"shifted window attention\" mechanism. Instead of global attention, Swin restricts self-attention computation to non-overlapping local windows within each stage, drastically reducing computational complexity to linear. To facilitate information exchange between windows and capture global dependencies, the windows are shifted between successive Transformer blocks. This elegant solution allows Swin Transformer to achieve state-of-the-art performance across a wide array of vision tasks, including image classification, object detection, and semantic segmentation, effectively establishing hierarchical Transformers as powerful general-purpose vision backbones. While both PVT and Swin Transformer achieved linear complexity, Swin's shifted window approach provided a more dynamic mechanism for cross-window information flow compared to PVT's static spatial reduction, often leading to stronger performance.\n\nFurther advancements in hierarchical ViTs have focused on refining attention mechanisms and simplifying architectures. The fixed window sizes and handcrafted attention patterns in models like Swin Transformer and PVT, while efficient, can sometimes limit their ability to model long-range relations adaptively \\cite{xia2023bp7}. Addressing this, the \\textbf{Deformable Attention Transformer (DAT++)} \\cite{xia2023bp7} proposes a novel deformable multi-head attention module. This module adaptively allocates the positions of key and value pairs in a data-dependent manner, allowing the model to dynamically focus on relevant regions and capture more flexible spatial relationships, similar to deformable convolutions in CNNs. This approach enhances the representation power of global attention while maintaining efficiency. Similarly, the \\textbf{Dynamic Window Vision Transformer (DW-ViT)} \\cite{ren2022ifo} extends the window-based paradigm by moving \"beyond fixation.\" DW-ViT assigns windows of different sizes to different head groups within multi-head self-attention and dynamically fuses the multi-scale information, overcoming the limitation of fixed single-scale windows in models like Swin Transformer and further improving multi-scale modeling capabilities.\n\nIn contrast to increasing architectural complexity, some research has explored simplification. \\textbf{Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles} \\cite{ryali202339q} proposes a streamlined hierarchical ViT design, arguing that many added complexities can be removed without sacrificing accuracy, provided the model is pre-trained with a robust self-supervised pretext task, such as Masked Autoencoders (MAE) \\cite{he2022masked}. By leveraging strong pre-training, Hiera achieves competitive or superior accuracy to more complex hierarchical models while being significantly faster during both inference and training. This work underscores the critical interplay between architectural design and effective pre-training strategies, suggesting that inductive biases from hierarchical processing, combined with powerful self-supervision, can lead to highly efficient and performant architectures without excessive overhead. This also opens a comparative perspective on the fundamental role of attention itself; for instance, the \\textbf{ShiftViT} \\cite{wang2022da0} even explores replacing attention layers entirely with a zero-parameter shift operation, achieving performance comparable to Swin Transformer, suggesting that the hierarchical structure and feature processing might be as crucial as the attention mechanism itself.\n\nIn conclusion, the evolution of hierarchical Vision Transformers has been instrumental in overcoming the initial limitations of pure ViTs, particularly their quadratic complexity and lack of multi-scale representations. Models like PVT and Swin Transformer pioneered efficient multi-scale processing through techniques such as spatial-reduction attention and shifted window attention, respectively, making ViTs suitable for dense prediction tasks and high-resolution images. Subsequent innovations, including deformable attention (DAT++) and dynamic windowing (DW-ViT), have further refined these attention mechanisms for greater flexibility and multi-scale understanding. Concurrently, models like Hiera highlight the power of architectural simplification when coupled with strong self-supervised pre-training. Future research in this domain will likely continue to explore the optimal balance between architectural complexity, computational efficiency, and representational power within hierarchical structures, further optimizing attention mechanisms, and deepening the synergy with advanced self-supervised learning paradigms to unlock even more robust and versatile vision backbones.\n\\subsection{Data-Efficient Training and Tokenization Strategies}\n\\label{sec:3\\_2\\_data-efficient\\_training\\_\\_and\\_\\_tokenization\\_strategies}\n\nEarly Vision Transformers (ViTs) demonstrated impressive capabilities in image recognition, yet their reliance on massive proprietary datasets like JFT-300M for pre-training posed a significant barrier to their widespread adoption and practical application \\cite{Dosovitskiy2021}. This section explores key innovations that addressed this limitation, focusing on strategies for data-efficient training and improved tokenization, thereby making ViTs more accessible and performant on standard, smaller datasets.\n\nOne of the most impactful approaches to mitigate ViTs' data hunger is knowledge distillation. The work by \\cite{Touvron2021} introduced Data-efficient image Transformers (DeiT), which enabled ViTs to be trained effectively on ImageNet-1K, a significantly smaller dataset, without requiring external data. DeiT achieved this by employing a distillation strategy where a ViT student model learns from a powerful convolutional neural network (CNN) teacher. A crucial innovation was the \"distillation token,\" an additional token that interacts with the class token through self-attention and is trained to match the teacher's output, effectively transferring the teacher's knowledge and enabling the ViT to achieve competitive performance with much less training data.\n\nWhile knowledge distillation provided a powerful mechanism for data-efficient training, another line of research focused on enhancing the initial representation of image patches, thereby reducing the inherent data requirements for learning robust visual features. The original ViT treats non-overlapping image patches as independent tokens, which can lose fine-grained local structural information. To address this, \\cite{Yuan2021} proposed Tokens-to-Token ViT (T2T-ViT), which introduced an improved tokenization strategy. Instead of a single-step patch embedding, T2T-ViT employs a multi-stage process where neighboring pixels are progressively aggregated into tokens through a series of self-attention layers. This hierarchical tokenization module effectively captures local structural details and preserves more information from the original image, leading to better performance from scratch on standard benchmarks like ImageNet-1K without relying on massive pre-training datasets or external teacher models.\n\nThe advancements in data-efficient training through distillation, as exemplified by DeiT, and improved tokenization strategies, such as T2T-ViT, collectively transformed the landscape of Vision Transformers. DeiT demonstrated that ViTs could achieve state-of-the-art results on ImageNet-1K by effectively mimicking a larger teacher, while T2T-ViT showed that a more sophisticated initial tokenization could inherently boost ViT performance on smaller datasets by better encoding local visual information. These innovations were critical in making ViTs more practical and accessible, moving them beyond the realm of models requiring immense computational resources and proprietary datasets. However, challenges remain in further reducing the data requirements for highly specialized tasks and in developing unified frameworks that seamlessly integrate the benefits of both distillation and advanced tokenization for optimal efficiency across diverse applications.\n\\subsection{Advanced Attention Mechanisms}\n\\label{sec:3\\_3\\_advanced\\_attention\\_mechanisms}\n\nThe foundational self-attention mechanism in Vision Transformers, while powerful for capturing global dependencies, inherently suffers from quadratic computational complexity with respect to the number of tokens, posing significant challenges for high-resolution images or deeper architectures. Furthermore, vanilla self-attention often lacks explicit spatial inductive biases, which can sometimes lead to redundancy or \"attention collapse\" in very deep models, limiting their expressive power. To address these limitations, researchers have developed various advanced attention mechanisms aimed at improving efficiency, flexibility, and expressiveness.\n\nOne critical challenge in scaling Vision Transformers to greater depths is the phenomenon of \"attention collapse.\" As models become deeper, the attention maps across different layers can become increasingly similar, hindering the model's ability to learn diverse and effective representations. \\textcite{zhou202105h} introduced DeepViT to tackle this issue, observing that in deeper ViTs, self-attention mechanisms often fail to learn distinct concepts, leading to performance saturation. To mitigate this, they proposed Re-attention, a simple yet effective method designed to re-generate attention maps and increase their diversity across layers. This innovation allows for the training of significantly deeper ViT models with consistent performance improvements, enhancing the model's overall expressiveness by ensuring that attention layers continue to contribute unique feature transformations.\n\nBeyond improving expressiveness in deep models, a major thrust in advanced attention research focuses on enhancing efficiency and spatial awareness. The global nature of standard self-attention means every token attends to every other token, which can be computationally expensive and may lead to attention being drawn to irrelevant regions. To make attention more adaptive and efficient, Deformable Attention was introduced, allowing the attention mechanism to sample features at adaptive, learned offsets \\textcite{xia2022qga}. Unlike fixed-grid or sparse attention patterns, this data-dependent sampling enables the model to dynamically focus on relevant regions and capture more informative features. This flexible scheme significantly reduces computational overhead by concentrating attention on salient parts of the input, while simultaneously enhancing the model's ability to adapt to varying object shapes and scales, thereby improving spatial awareness and fine-grained detail capture.\n\nThis concept of focusing attention on relevant tokens to reduce computation is also explored in approaches like Focal Attention. Such mechanisms typically employ hierarchical attention or explicit token selection strategies to prioritize important visual information. By selectively attending to a subset of tokens or regions, these methods aim to mitigate the quadratic complexity of global attention, making ViTs more scalable for high-resolution inputs. These innovations collectively enhance the model's ability to capture fine-grained details and adapt to varying object shapes and scales, leading to more robust and accurate visual understanding.\n\nIn summary, advanced attention mechanisms represent a crucial evolutionary step for Vision Transformers, moving beyond the limitations of vanilla self-attention. By addressing issues like attention collapse through techniques such as Re-attention \\textcite{zhou202105h} and improving efficiency and spatial adaptability with data-dependent sampling as seen in Deformable Attention \\textcite{xia2022qga}, these refinements enable the development of more powerful, flexible, and scalable ViT architectures. Future directions will likely continue to explore more sophisticated ways to balance global context modeling with local detail, further optimizing computational efficiency, and integrating stronger inductive biases into attention mechanisms for even more robust visual understanding across diverse tasks.\n\n\n\\label{sec:self-supervised_learning_and_vision_foundation_models}\n\n\\section{Self-Supervised Learning and Vision Foundation Models}\n\\label{sec:self-supervised\\_learning\\_\\_and\\_\\_vision\\_foundation\\_models}\n\n\\subsection{Masked Autoencoders (MAE) for Pre-training}\n\\label{sec:4\\_1\\_masked\\_autoencoders\\_(mae)\\_for\\_pre-training}\n\nVision Transformers (ViTs), despite their remarkable success in various computer vision tasks, initially faced significant challenges related to their data-hungry nature and the difficulty of scaling them to deeper architectures without performance degradation. Early observations, such as those presented in \\cite{zhou202105h}, highlighted an \"attention collapse\" issue in deeper ViTs, where attention maps became increasingly similar across layers, hindering effective representation learning and limiting performance gains. This necessitated the development of efficient and scalable self-supervised pre-training strategies to unlock the full potential of ViTs and enable them to learn robust visual representations from unlabeled data.\n\nA pivotal breakthrough in this regard was the introduction of Masked Autoencoders (MAE) by \\cite{CVPR2022\\_Masked\\_Autoencoders\\_Are\\_Scalable\\_Vision\\_Learners\\_2022}. MAE revolutionized pre-training for Vision Transformers by proposing a highly effective self-supervised learning strategy rooted in image reconstruction. The core idea involves masking a substantial portion of image patches, typically around 75\\\\%, and training a Transformer encoder-decoder architecture to reconstruct the missing pixels. The encoder processes only the visible, unmasked patches, leading to significant computational efficiency during pre-training, while a lightweight decoder is responsible for predicting the original pixel values of the masked patches. This design forces the encoder to learn rich, high-level semantic representations from limited visual context, preventing it from relying on trivial low-level statistical regularities.\n\nThe MAE approach offers several compelling advantages. Its asymmetric encoder-decoder design, where the encoder operates on a sparse set of visible patches, drastically reduces the computational cost of pre-training compared to prior self-supervised methods that process full images. This efficiency makes MAE exceptionally scalable, enabling the effective pre-training of very large Vision Transformer models on vast amounts of unlabeled data. Consequently, MAE-pretrained ViTs have demonstrated state-of-the-art performance across various downstream tasks, including image classification, object detection, and semantic segmentation, often requiring less fine-tuning data than models trained with other self-supervised or supervised methods. The learned representations are highly robust and generalizable, significantly reducing the reliance on extensive labeled datasets and making large-scale pre-training more feasible and impactful.\n\nThe success of MAE extended beyond pure Transformer architectures, demonstrating the generality of its self-supervised reconstruction paradigm. For instance, \\cite{ICLR2023\\_ConvNeXt\\_V2\\_Co\\_designing\\_and\\_Scaling\\_ConvNets\\_with\\_Masked\\_Autoencoders\\_2023} successfully applied MAE pre-training to modern ConvNets, showing that even CNNs could benefit significantly from this Transformer-inspired self-supervised strategy, achieving improved performance and bridging the gap between the two architectural paradigms. Furthermore, the efficiency and scalability demonstrated by MAE paved the way for the exploration of even grander model scales. Works like \\cite{ICLR2023\\_Scaling\\_Vision\\_Transformers\\_to\\_1\\_Billion\\_Parameters\\_2023} leveraged similar efficient pre-training principles to scale Vision Transformers to unprecedented sizes, showcasing the potential for massive, general-purpose \"Vision Foundation Models.\" The drive for learning robust, unsupervised features, significantly propelled by MAE's success, continued with advancements such as \\cite{ICLR2023\\_DINOv2\\_Learning\\_Robust\\_Visual\\_Features\\_without\\_Supervision\\_2023}, which further refined self-supervised approaches to learn highly transferable visual features, building upon the foundation of efficient pre-training established by MAE.\n\nIn conclusion, Masked Autoencoders marked a significant turning point in the pre-training landscape for Vision Transformers. By providing an efficient, scalable, and effective self-supervised learning strategy, MAE made large-scale ViT pre-training practical and solidified self-supervised learning as a cornerstone for developing powerful Vision Foundation Models. While MAE has proven highly effective, ongoing research continues to explore optimal masking strategies, alternative reconstruction targets, and combinations with other self-supervised objectives to further enhance the learned representations and address the persistent challenge of adapting these powerful, pre-trained models for efficient deployment on resource-constrained devices.\n\\subsection{Contrastive and Knowledge Distillation Approaches}\n\\label{sec:4\\_2\\_contrastive\\_\\_and\\_\\_knowledge\\_distillation\\_approaches}\n\nThe quest for learning robust, generalizable visual features for Vision Transformers (ViTs) without the prohibitive cost of extensive human annotation has propelled the development of sophisticated self-supervised learning (SSL) paradigms. Beyond masked autoencoding, two other prominent strategies, contrastive learning and advanced knowledge distillation, have significantly contributed to overcoming the data-hungry nature of ViTs, enabling them to learn meaningful representations directly from vast quantities of unlabeled data. These methods enhance ViTs' performance and applicability across diverse downstream tasks, demonstrating powerful alternative strategies for unsupervised representation learning.\n\nContrastive learning, a foundational SSL approach, operates by teaching the model to distinguish between similar and dissimilar pairs of data points. The core idea is to pull \"positive\" pairs (different augmented views of the same image) closer in the embedding space while pushing \"negative\" pairs (views from different images) apart. Early successes with Convolutional Neural Networks (CNNs), such as SimCLR \\cite{chen2020simple} and MoCo \\cite{he2020momentum}, laid the groundwork for adapting these techniques to ViTs. MoCo-v3 \\cite{chen2021empirical}, for instance, successfully applied a momentum contrast framework to ViTs, demonstrating that these architectures could learn competitive representations through instance discrimination. The effectiveness of contrastive learning stems from its ability to learn representations that are invariant to various data augmentations, thereby capturing essential semantic features without explicit labels.\n\nA highly effective and distinct paradigm, particularly for ViTs, is self-distillation, a specialized form of knowledge distillation where a model learns from itself through a teacher-student framework. This approach has proven exceptionally successful in generating high-quality, transferable features. The original DINO (Self-Distillation with No Labels) \\cite{caron2021emerging} was a seminal work, demonstrating that ViTs could learn dense, semantic features by matching the output of a momentum-updated teacher network. This method encourages the student to produce features that are invariant to different views of the same image, effectively learning without explicit supervision. Building upon this, DINOv2 \\cite{ICLR2023\\_DINOv2} scaled this teacher-student self-distillation framework to unprecedented levels, utilizing massive datasets and architectural refinements to produce exceptionally robust, generalizable, and transferable visual features. DINOv2's representations frequently outperform features derived from supervised pre-training on various transfer tasks, significantly reducing the reliance on labeled datasets and making ViTs more accessible and scalable for real-world applications.\n\nBeyond self-distillation, other forms of knowledge distillation have also been explored to enhance self-supervised ViTs, particularly for efficiency and transferability. For instance, Attention Distillation \\cite{wang2022pee} investigates distilling knowledge from a self-supervised ViT teacher to a smaller ViT student. This method highlights that directly distilling information from the crucial attention mechanism can significantly narrow the performance gap between teacher and student, outperforming existing self-supervised knowledge distillation methods focused on ConvNets. Such approaches are vital for deploying high-performing self-supervised ViTs on memory and compute-constrained devices, extending the benefits of powerful SSL to more practical scenarios.\n\nA critical analysis reveals fundamental differences in the learning objectives and resulting feature properties between these SSL paradigms and Masked Autoencoders (MAE) discussed in the preceding subsection. While MAE \\cite{he2022masked} forces the model to learn rich representations through a high-information-density pixel reconstruction task, often yielding features beneficial for dense prediction, DINO-style self-distillation encourages semantic consistency and invariance to data augmentations, typically leading to features that are highly effective for classification and linear probing. Contrastive learning, on the other hand, focuses on instance-level discrimination, aiming for representations that can distinguish individual images while being robust to transformations. These distinct objectives mean that each SSL paradigm can yield features with different strengths, making them more suitable for specific downstream tasks or offering complementary benefits. For example, while MAE's reconstruction task can be adapted to hierarchical architectures like HiViT \\cite{zhang2022msa} and Hiera \\cite{ryali202339q} for efficiency, contrastive and distillation methods often focus on the feature space directly. Moreover, some approaches explore hybrid strategies, such as MAT-VIT \\cite{han2024f96}, which leverages an MAE-based self-supervised auxiliary task alongside a supervised primary task, demonstrating the potential for combining different SSL paradigms to maximize learning from both labeled and unlabeled medical images.\n\nIn conclusion, contrastive learning and knowledge distillation approaches, particularly self-distillation as pioneered by DINO and scaled by DINOv2, represent critical advancements in self-supervised learning for Vision Transformers. By leveraging teacher-student frameworks, instance discrimination, and focusing on learning highly robust and generalizable features from unlabeled data, these methods empower ViTs to achieve state-of-the-art performance in various downstream tasks, often surpassing supervised alternatives. The continuous development of such unsupervised representation learning strategies, coupled with innovations in knowledge transfer for efficiency, is fundamental to realizing the full potential of Vision Transformers as efficient, scalable, and universally applicable models, thereby significantly reducing the dependency on costly human annotations and paving the way for the emergence of powerful Vision Foundation Models.\n\\subsection{Scaling Vision Transformers to Foundation Models}\n\\label{sec:4\\_3\\_scaling\\_vision\\_transformers\\_to\\_foundation\\_models}\n\nThe quest for general-purpose visual intelligence has ushered in a transformative era, marked by the scaling of Vision Transformers (ViTs) to unprecedented sizes, often comprising billions of parameters, to establish \"Vision Foundation Models.\" These colossal models, meticulously trained on vast datasets using sophisticated self-supervised learning (SSL) techniques, aim to distill universal visual representations. The ultimate objective is to develop versatile backbones that exhibit exceptional adaptability across a broad spectrum of vision tasks, delivering superior performance with significantly reduced task-specific fine-tuning. This paradigm represents a profound shift towards general-purpose visual intelligence, capable of impressive zero-shot or few-shot learning capabilities.\n\nInitial explorations into scaling ViTs encountered significant hurdles. Naively increasing model depth, as explored by \\cite{zhou202105h} with DeepViT, revealed an \"attention collapse\" phenomenon. In deeper layers, attention maps became increasingly homogeneous, hindering performance gains and indicating that simply adding more layers was insufficient without more robust architectural designs and effective training strategies. This challenge underscored the critical need for advancements in how ViTs learn and generalize at scale.\n\nA pivotal breakthrough in enabling the efficient scaling of ViTs came with the advent of Masked Autoencoders (MAE) \\cite{CVPR2022\\_Masked\\_Autoencoders\\_Are\\_Scalable\\_Vision\\_Learners}. As discussed in Subsection 4.1, MAE introduced an elegant and highly efficient self-supervised pre-training paradigm. By masking a large portion of image patches and training the Transformer encoder-decoder to reconstruct the missing pixels, MAE effectively mitigated the notorious data-hungry nature of ViTs, allowing them to learn rich, robust representations from vast quantities of unlabeled data. This innovation was instrumental in unlocking the potential for large-scale pre-training. Complementing MAE, methods like DINOv2 \\cite{ICLR2023\\_DINOv2\\_Learning\\_Robust\\_Visual\\_Features\\_without\\_Supervision}, elaborated in Subsection 4.2, further refined self-supervised learning by leveraging self-distillation with a teacher-student framework. DINOv2 produced highly generalizable and transferable features without explicit supervision, often surpassing supervised pre-training in transfer tasks and demonstrating impressive zero-shot capabilities.\n\nThe combination of efficient self-supervised pre-training and the availability of colossal datasets has been fundamental to the scaling trend. Vision Foundation Models are typically pre-trained on datasets far exceeding ImageNet, such as JFT-300M \\cite{bai2022f1v, hong2022ks6}, JFT-4B, or the internet-scale LAION-5B. The sheer volume and diversity of these datasets, coupled with scalable SSL methods, allow ViTs to learn highly abstract and universal visual concepts. Empirical studies have consistently demonstrated that performance scales predictably with increased model size, dataset size, and computational resources, a phenomenon often referred to as \"scaling laws\" in deep learning.\n\nBuilding upon these foundations, researchers have aggressively pushed the boundaries of ViT size. The work by \\cite{ICLR2023\\_Scaling\\_Vision\\_Transformers\\_to\\_1\\_Billion\\_Parameters} notably demonstrated the efficacy of scaling Vision Transformers to one billion parameters. Their findings unequivocally showed that larger models, when pre-trained on extensive datasets using techniques like MAE, yield superior performance and significantly enhanced generalization across diverse downstream tasks. This marked a crucial step towards realizing truly universal visual backbones. Even more extreme examples, such as Google's ViT-22B, further exemplify the profound capabilities that emerge from massive scaling. These models exhibit emergent properties, including remarkable zero-shot transfer to unseen tasks and improved few-shot learning, which are hallmarks of true foundation models. They can often adapt to new domains with minimal or no task-specific fine-tuning, drastically reducing the effort and labeled data required for new applications.\n\nBeyond pure parameter count, architectural innovations have also contributed to the scalability and efficiency of these large models. For instance, approaches like UFO-ViT \\cite{song20215tk} introduce linear attention mechanisms to mitigate the quadratic computational complexity of traditional self-attention, which becomes a bottleneck at high resolutions or extreme depths. Similarly, models like Hiera \\cite{ryali202339q} demonstrate that even simplified hierarchical ViT designs, when effectively pre-trained with MAE, can achieve state-of-the-art performance while being significantly faster, thus contributing to more efficient scaling.\n\nWhile scaling to billions of parameters offers immense representational power, the practical deployment of such massive models remains a critical consideration. The computational cost for inference and the memory footprint can be prohibitive for many real-world scenarios, particularly on resource-constrained edge devices. To address this, research into efficiency techniques is paramount. Quantization methods, such as Q-ViT \\cite{li20229zn}, enable significant model compression by reducing bit-widths while striving to preserve performance. Furthermore, specialized hardware accelerators like ViTA \\cite{nag2023cfn} are being developed to optimize the inference of Vision Transformers, ensuring that these powerful, scaled models can be deployed efficiently in practical applications.\n\nIn conclusion, the evolution of Vision Transformers into foundation models represents a profound paradigm shift from task-specific model development to the creation of universally powerful, pre-trained visual intelligence. This trajectory is characterized by extreme architectural scaling, the indispensable role of advanced self-supervised learning for robust feature extraction from colossal datasets, and the resultant emergence of powerful zero-shot and few-shot learning capabilities. Future directions will undoubtedly focus on further enhancing the efficiency of training and inference for these massive models, exploring deeper multimodal integration for richer representations, and critically addressing the ethical implications and potential biases inherent in such large-scale, general-purpose visual systems.\n\n\n\\label{sec:hybrid_architectures_and_mobile-friendly_designs}\n\n\\section{Hybrid Architectures and Mobile-Friendly Designs}\n\\label{sec:hybrid\\_architectures\\_\\_and\\_\\_mobile-friendly\\_designs}\n\n\\subsection{Synergistic CNN-Transformer Designs}\n\\label{sec:5\\_1\\_synergistic\\_cnn-transformer\\_designs}\n\nThe evolution of computer vision architectures has increasingly moved towards a synergistic paradigm, strategically combining the inherent strengths of Convolutional Neural Networks (CNNs) with the global context modeling capabilities of Transformers. This convergence is driven by the recognition that while CNNs excel at local feature extraction and possess strong inductive biases like locality and translation equivariance, pure Vision Transformers (ViTs) \\cite{dosovitskiy2021image} offer unparalleled global receptive fields and flexibility in capturing long-range dependencies. Hybrid designs aim to mitigate the limitations of each, such as ViTs' high data requirements and quadratic computational complexity, or CNNs' struggle with modeling global relationships, thereby fostering more robust, efficient, and versatile vision backbones.\n\nOne prominent direction in this synergy involves modernizing traditional CNN architectures by incorporating Transformer-inspired methodologies and training paradigms. A seminal work in this category is \\textit{ConvNeXt V2} \\cite{iclr2023}. Building upon the architectural principles of ConvNeXt, which re-examined and optimized CNN designs to resemble Transformers (e.g., using large kernel depthwise convolutions and inverted bottleneck structures), ConvNeXt V2 crucially leveraged the Masked Autoencoder (MAE) pre-training strategy \\cite{he2022masked}. This integration demonstrated that CNNs could benefit immensely from self-supervised learning techniques originally developed for Transformers, achieving state-of-the-art performance and effectively bridging the perceived architectural gap. ConvNeXt V2 showcased that the inductive biases of CNNs, when combined with powerful, scalable pre-training, remain highly competitive and efficient.\n\nConversely, another significant approach integrates powerful CNN components directly into Transformer-like frameworks. \\textit{InternImage} \\cite{cvpr2023} exemplifies this by embedding deformable convolutions within a Transformer-inspired architecture. Deformable convolutions, a hallmark of advanced CNNs, enable adaptive receptive fields and flexible spatial sampling, which are crucial for handling geometric variations and object deformations effectively. By incorporating this mechanism, InternImage creates robust \"Vision Foundation Models\" that combine the global context understanding of Transformers with the precise, adaptive local feature extraction characteristic of sophisticated convolutions. This allows the model to dynamically focus on relevant regions, enhancing its ability to capture fine-grained details and adapt to varying object shapes and scales, a limitation often observed in standard fixed-grid attention mechanisms. A related approach is seen in \\textit{Deformable Attention Transformer (DAT)} and its enhanced version \\textit{DAT++} \\cite{xia2022qga, xia2023bp7}. These models integrate a novel deformable multi-head attention module where key and value positions are adaptively allocated in a data-dependent manner, effectively bringing the adaptive sampling power of deformable convolutions directly into the self-attention mechanism, thereby enhancing spatial awareness and reducing computational overhead compared to dense global attention.\n\nBeyond integrating components or pre-training strategies, some hybrid architectures explicitly interleave CNN and Transformer blocks to capitalize on their respective strengths at different stages of feature extraction. \\textit{Next-ViT} \\cite{li2022a4u} proposes a \"Next Hybrid Strategy\" (NHS) that stacks \"Next Convolution Blocks\" (NCB) for local information capture and \"Next Transformer Blocks\" (NTB) for global information. This design is optimized for efficient deployment in realistic industrial scenarios, addressing the challenge of achieving both high performance and low latency on hardware accelerators like TensorRT. Similarly, \\textit{TRT-ViT} \\cite{xia2022dnj} provides practical guidelines for TensorRT-oriented network design, suggesting an \"early CNN and late Transformer at stage-level\" and \"early Transformer and late CNN at block-level\" strategy. This highlights a nuanced understanding of where and how to best deploy each architectural component for optimal hardware efficiency and performance. These block-level hybrid models demonstrate that a careful arrangement of convolutional and self-attention layers can yield superior latency-accuracy trade-offs across various vision tasks.\n\nAnother innovative direction involves fusing local and global processing within the Transformer block itself, drawing inspiration from CNNs' local inductive biases without necessarily using explicit convolutional layers. \\textit{PLG-ViT} \\cite{ebert202377v} introduces a \"Parallel Local-Global Vision Transformer\" that merges local window self-attention with global self-attention. This design efficiently represents short- and long-range spatial interactions, bypassing the need for computationally expensive operations like shifted windows while still achieving strong performance in image classification and dense prediction tasks. This approach reflects a deeper integration, where the \\textit{concept} of locality, traditionally associated with CNNs, is re-imagined and implemented within the Transformer's attention mechanism.\n\nIn summary, the landscape of synergistic CNN-Transformer designs is rich and diverse, reflecting various philosophies of integration. ConvNeXt V2 represents a CNN-centric approach enhanced by Transformer pre-training. InternImage and DAT/DAT++ showcase embedding CNN-inspired adaptive mechanisms into Transformer backbones. Next-ViT and TRT-ViT demonstrate strategic interleaving of CNN and Transformer blocks for deployment efficiency. PLG-ViT illustrates the internal fusion of local and global attention within a Transformer, mimicking CNN-like inductive biases. These models collectively demonstrate that moving beyond a strict CNN-Transformer dichotomy towards a complementary approach unlocks new levels of performance, efficiency, and versatility, pushing the boundaries of architectural design for a wide array of computer vision applications. Future research will likely explore even more sophisticated integration strategies, novel unified pre-training objectives, and dynamic architectural adaptations to further harness the combined power of these two foundational paradigms.\n\\subsection{Lightweight and Mobile-Optimized Vision Transformers}\n\\label{sec:5\\_2\\_lightweight\\_\\_and\\_\\_mobile-optimized\\_vision\\_transformers}\n\nBuilding upon the synergistic CNN-Transformer designs discussed in Section 5.1, this subsection delves into architectures specifically engineered for resource-constrained mobile and edge devices. The deployment of Vision Transformers (ViTs) on platforms such as smartphones, IoT devices, and embedded systems presents significant challenges due to their typically high computational cost, large memory footprint, and substantial power consumption. Addressing these critical deployment challenges, a dedicated research thrust focuses on developing lightweight and mobile-optimized ViT architectures that maintain high accuracy while drastically reducing computational demands and parameter counts, making powerful ViT capabilities accessible for real-time and edge applications.\n\nA pioneering effort in this domain is \\textcite{mehta20216ad}'s MobileViT, which directly tackles the trade-off between the spatial inductive biases and efficiency of Convolutional Neural Networks (CNNs) and the global representation learning capabilities of ViTs. MobileViT proposes a novel hybrid architecture that integrates standard convolutional layers for local feature extraction with a lightweight Transformer block that processes information globally, effectively treating \"transformers as convolutions.\" This design allows MobileViT to achieve impressive accuracy, such as 78.4\\\\% top-1 on ImageNet-1k with approximately 6 million parameters, significantly outperforming both CNN-based (e.g., MobileNetv3) and ViT-based (e.g., DeiT) counterparts for similar parameter budgets. Its ability to learn global representations while retaining the efficiency of local processing makes it highly suitable for mobile vision tasks, demonstrating a foundational approach to lightweight hybrid ViTs.\n\nFurther advancing the field of deployable hybrid architectures, \\textcite{li2022a4u} introduce Next-ViT, explicitly designed for efficient deployment in realistic industrial scenarios, considering factors like TensorRT and CoreML inference latency. Next-ViT proposes a \"Next Convolution Block (NCB)\" and a \"Next Transformer Block (NTB)\" that are deployment-friendly, capturing local and global information respectively. The \"Next Hybrid Strategy (NHS)\" then efficiently stacks these blocks. Unlike many ViTs that optimize for FLOPs or parameter count, Next-ViT prioritizes actual inference speed on target hardware, achieving substantial latency reductions while maintaining superior accuracy. For instance, it surpasses ResNet by 5.5 mAP on COCO detection and accelerates inference speed by 3.6x compared to CSWin under similar performance, highlighting its practical utility for edge computing where real-world latency is paramount. This represents a critical evolution from merely lightweight designs to truly deployment-optimized ones.\n\nBeyond architectural hybridization, another crucial avenue for efficiency lies in redesigning the attention mechanism itself to mitigate its quadratic computational complexity. \\textcite{chen2021r2y} introduce CrossViT, which focuses on learning multi-scale feature representations within a dual-branch transformer framework. CrossViT processes image patches of different sizes in separate branches and then fuses these multi-scale tokens using a highly efficient cross-attention mechanism. Crucially, their cross-attention module is designed to operate with linear computational and memory complexity, rather than the quadratic complexity typical of standard self-attention, thereby mitigating a major bottleneck for deploying ViTs on resource-limited hardware. This approach not only enhances representational power by leveraging features at various scales but also ensures that the fusion process remains computationally tractable. Complementing this, \\textcite{song20215tk} propose UFO-ViT (Unit Force Operated Vision Transformer), which offers a novel self-attention mechanism with linear complexity by eliminating non-linearity and factorizing matrix multiplication without complex linear approximations. While CrossViT achieves linear scaling through a specialized cross-attention for multi-scale fusion, UFO-ViT fundamentally re-engineers the self-attention block itself to achieve linear complexity, offering a more general solution for reducing the computational burden of attention in any ViT layer.\n\nFinally, while architectural and algorithmic innovations are vital, the ultimate efficiency on edge devices often necessitates hardware-aware optimization. \\textcite{nag2023cfn} address this by proposing ViTA, a configurable hardware accelerator specifically designed for inference of Vision Transformer models on highly resource-constrained edge computing devices. ViTA employs a head-level pipeline and inter-layer MLP optimizations to avoid repeated off-chip memory accesses, a common bottleneck in embedded systems. It supports various ViT models with changes solely in its control logic, achieving nearly 90\\\\% hardware utilization efficiency and reasonable frame rates at low power consumption (e.g., 0.88W at 150 MHz). This demonstrates that for true mobile optimization, a holistic approach combining efficient model design with dedicated hardware acceleration is indispensable, pushing the boundaries of what is achievable on the edge.\n\nCollectively, these works demonstrate a clear progression towards making powerful Vision Transformer capabilities accessible for real-time and edge applications. They highlight the importance of hybrid architectures that judiciously combine CNN and Transformer strengths \\cite{mehta20216ad, li2022a4u}, efficient attention mechanisms that scale linearly with input size \\cite{chen2021r2y, song20215tk}, and the critical role of hardware-software co-design for practical deployment \\cite{nag2023cfn}. Despite these significant advancements, ongoing challenges include developing more sophisticated hardware-aware neural architecture search methods that consider power and memory constraints alongside latency, exploring novel quantization and pruning strategies tailored for these complex hybrid designs, and investigating dynamic execution strategies to adapt to varying computational budgets on the fly.\n\\subsection{Novel Hybrid Paradigms}\n\\label{sec:5\\_3\\_novel\\_hybrid\\_paradigms}\n\nThe continuous quest for more efficient, scalable, and powerful vision models has propelled research beyond the established CNN-Transformer hybrid architectures, leading to the exploration of entirely new component integrations. This subsection delves into novel hybrid paradigms that fuse distinct neural network components, moving beyond convolutional and self-attention mechanisms, to leverage their complementary strengths for enhanced performance and efficiency, particularly in modeling long-range dependencies and global context. Such approaches signify a dynamic and evolving research frontier, continuously pushing the boundaries of what is achievable in visual representation learning by incorporating components inspired by advancements in sequence modeling.\n\nA significant recent development in this direction is the integration of State-Space Models (SSMs) into vision backbones, exemplified by architectures like MambaVision \\cite{hatamizadeh2024xr6}. Originating from control theory and recently revitalized for deep learning, SSMs offer an alternative mechanism for efficient sequence modeling, particularly adept at capturing long-range dependencies with linear computational complexity. MambaVision, proposed by Hatamizadeh et al. \\cite{hatamizadeh2024xr6}, presents a hybrid architecture that integrates the Mamba SSM with Vision Transformers (ViT). The core motivation behind MambaVision is to harness Mamba's inherent efficiency in sequence modeling and its superior capacity for capturing long-range dependencies, while simultaneously retaining the robust global context understanding capabilities characteristic of Transformers. This fusion offers a compelling new perspective on designing vision backbones that can efficiently process extensive visual information, addressing some of the quadratic complexity issues of pure self-attention.\n\nHatamizadeh et al. \\cite{hatamizadeh2024xr6} meticulously redesigned the Mamba formulation to optimize its performance specifically for visual features, addressing the unique challenges posed by image data. Through comprehensive ablation studies, they demonstrated the feasibility and substantial benefits of integrating ViT components within the Mamba framework. A pivotal finding was that the strategic incorporation of self-attention blocks in the final layers of the Mamba architecture significantly enhanced its ability to capture intricate long-range spatial dependencies, a critical aspect for achieving high performance across diverse vision tasks. This hybrid design allows MambaVision to benefit from the local processing and linear scaling of Mamba while leveraging the global reasoning of attention where most critical. The efficacy of MambaVision is robustly supported by its empirical results across multiple benchmarks, achieving state-of-the-art (SOTA) performance on ImageNet-1K classification and demonstrating favorable performance in downstream tasks such as object detection, instance segmentation on MS COCO, and semantic segmentation on ADE20K, often outperforming comparably sized backbones.\n\nAnother emerging paradigm involves adapting Receptance Weighted Key Value (RWKV) models, initially developed for efficient large language models, to visual perception. Vision-RWKV (VRWKV), introduced by Duan et al. \\cite{duan2024q7h}, is a notable example. Transformers, while powerful, face limitations in high-resolution image processing due to their quadratic computational complexity. VRWKV addresses this by adapting the RWKV model, which processes sequences in a recurrent manner while maintaining a Transformer-like attention mechanism, but with significantly reduced spatial aggregation complexity. This design allows VRWKV to efficiently handle sparse inputs and demonstrate robust global processing capabilities, scaling effectively to large parameters and extensive datasets without the necessity for windowing operations often employed in hierarchical Transformers.\n\nDuan et al. \\cite{duan2024q7h} made necessary modifications to the RWKV architecture to tailor it for vision tasks, enabling it to function as an efficient and scalable visual backbone. Their evaluations demonstrated that VRWKV surpasses the performance of traditional Vision Transformers (ViTs) in image classification, while also exhibiting significantly faster speeds and lower memory usage when processing high-resolution inputs. Furthermore, in dense prediction tasks, VRWKV was shown to outperform window-based models, maintaining comparable speeds. These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks, particularly where high-resolution processing and long-context analysis are critical, without incurring the prohibitive computational costs of full self-attention.\n\nThe emergence of models like MambaVision and Vision-RWKV represents a significant paradigm shift, moving beyond the established dichotomy of CNN-Transformer integrations. Both approaches aim to address the computational burdens associated with pure self-attention mechanisms while preserving or enhancing powerful global reasoning capabilities. MambaVision leverages the linear scaling of SSMs for efficient long-range dependency modeling, strategically reintroducing self-attention in later layers to refine global context. In contrast, Vision-RWKV adapts a distinct recurrent-attention mechanism from NLP, offering reduced spatial aggregation complexity and superior high-resolution processing without explicit windowing. While MambaVision focuses on a hybrid SSM-ViT structure, VRWKV presents a more direct adaptation of an efficient sequence model, emphasizing its unique recurrent-attention mechanism. Both paradigms demonstrate the immense potential of exploring non-traditional neural network components and their synergistic combinations for advancing the field of computer vision. Their success unequivocally highlights the value of cross-domain inspiration, particularly from efficient sequence models in NLP, to develop more efficient, powerful, and specialized vision models for future applications.\n\n\n\\label{sec:applications_of_visual_transformers}\n\n\\section{Applications of Visual Transformers}\n\\label{sec:applications\\_of\\_visual\\_transformers}\n\n\\subsection{Core Vision Tasks: Classification and Object Detection}\n\\label{sec:6\\_1\\_core\\_vision\\_tasks:\\_classification\\_\\_and\\_\\_object\\_detection}\n\nVision Transformers (ViTs) have profoundly impacted fundamental computer vision tasks, particularly image classification and object detection, by leveraging their global receptive fields and powerful feature extraction capabilities. This architectural shift has enabled ViTs to capture both broad contextual information and fine-grained details, often leading to new performance benchmarks that frequently surpass traditional Convolutional Neural Networks (CNNs), especially when trained on extensive datasets.\n\nThe initial foray of Transformers into vision began with image classification. The seminal Vision Transformer (ViT) by \\cite{Dosovitskiy2021} demonstrated that a pure Transformer, by treating image patches as sequential tokens, could effectively classify images. While achieving state-of-the-art results on massive datasets like ImageNet-21K (e.g., ViT-L/16 reaching 88.55\\\\% top-1 accuracy), it initially exhibited a significant dependency on vast training data, often underperforming CNNs on smaller datasets. To mitigate this data-hungry nature, \\cite{Touvron2021} introduced Data-efficient Image Transformers (DeiT), employing knowledge distillation to enable ViTs to be trained efficiently on ImageNet-1K, achieving competitive performance (e.g., DeiT-S reaching 83.1\\\\% top-1 accuracy) without requiring massive pre-training. Concurrently, \\cite{Yuan2021} proposed Tokens-to-Token ViT (T2T-ViT), which refined the initial tokenization process to better represent local structures, thereby boosting performance on standard datasets from scratch. Further architectural advancements aimed at enhancing the depth and stability of ViTs for classification; \\cite{zhou202105h} identified the \"attention collapse\" issue in deeper ViTs and proposed Re-attention to increase the diversity of attention maps, enabling the training of significantly deeper models with consistent performance gains. More recently, \\texttt{[ferdous2024f89]} introduced SPT-Swin, a variant that combines shifted patch tokenization with Swin Transformer to address data deficiency and computational complexity, achieving 89.45\\\\% accuracy on ImageNet-1K.\n\nA major breakthrough in efficient pre-training for classification came with \\cite{CVPR2022}, which introduced Masked Autoencoders (MAE). This self-supervised learning approach reconstructs masked image patches, proving highly scalable and data-efficient. MAE enables ViTs to learn robust representations from unlabeled data, significantly improving their performance and reducing the need for extensive labeled datasets for classification. For instance, a ViT-Large pre-trained with MAE on ImageNet-1K can achieve 87.8\\\\% top-1 accuracy. Building on this, the field has seen the emergence of \"Vision Foundation Models,\" with \\cite{ICLR2023} demonstrating the efficacy of scaling ViTs to over a billion parameters, achieving unprecedented classification performance through massive pre-training. Similarly, \\cite{ICLR2023} advanced self-supervised learning with DINOv2, which learns highly robust and generalizable visual features without supervision, often outperforming supervised pre-training in transfer tasks and further enhancing classification accuracy. The influence of ViT principles even extended to CNNs, as shown by \\cite{ICLR2023}, where ConvNeXt V2 leveraged MAE pre-training to significantly boost CNN performance, blurring the lines between the two architectures for classification tasks. For practical deployment, \\texttt{[song2022603]} proposed CP-ViT, a cascade pruning framework that dynamically predicts sparsity in ViT models, reducing FLOPs by over 40\\\\% while maintaining accuracy, crucial for resource-limited devices.\n\nThe success of ViTs quickly extended to object detection, a more complex task requiring both accurate object categorization and precise localization. Early pure ViTs struggled with dense prediction tasks due to their fixed-size patch embeddings and lack of inherent hierarchical feature maps, which are crucial for detecting objects at various scales. This limitation was fundamentally addressed by the introduction of end-to-end Transformer detectors. The seminal work on DEtection TRansformer (DETR) by \\cite{Carion2020} revolutionized object detection by formulating it as a direct set prediction problem. DETR eliminated hand-designed components like Non-Maximum Suppression (NMS) by using a set of learned object queries and a bipartite matching loss. While DETR demonstrated the power of Transformers for detection, its slow convergence and high computational cost were initial drawbacks. These were largely overcome by Deformable DETR \\cite{Zhu2020}, which introduced deformable attention to focus on a small set of key sampling points, significantly accelerating training and improving performance, particularly for small objects.\n\nFollowing these foundational end-to-end Transformer detectors, architectural innovations focused on integrating hierarchical ViT backbones to better capture multi-scale visual features. The \\cite{Liu2021} Swin Transformer, detailed in \\texttt{[liu2021ljs]}, revolutionized ViT applicability to dense prediction by proposing a hierarchical architecture with shifted window attention. This design limited self-attention computation to non-overlapping local windows while allowing for cross-window connections, effectively generating multi-scale feature maps. Swin Transformers became highly suitable as backbones for object detection frameworks, achieving state-of-the-art results, such as 58.7 box AP and 51.1 mask AP on COCO test-dev with Swin-Large. Similarly, \\cite{Wang2021} introduced the Pyramid Vision Transformer (PVT), a pure Transformer-based pyramid structure that progressively reduces feature map resolution, enabling the generation of multi-scale features essential for detecting objects of different sizes. These hierarchical backbones, when integrated with DETR-like heads, led to powerful detectors. For instance, \\texttt{[wang2023bfo]} combined Deformable DETR with a Swin Transformer and a lightweight Feature Pyramid Network (FPN) to enhance detection accuracy for multi-scale targets, demonstrating a 6.1\\\\% improvement in accuracy on a classroom behavior dataset.\n\nInterestingly, even plain (non-hierarchical) ViT backbones, when properly leveraged, have shown strong performance in object detection. \\texttt{[li2022raj]} explored the use of plain ViT backbones, pre-trained with MAE, for object detection. Their work, ViTDet, demonstrated that with minimal adaptations like a simple feature pyramid and limited cross-window propagation, these models could achieve competitive results on the COCO dataset, reaching up to 61.3 AP\\\\_box using only ImageNet-1K pre-training. This highlighted that the powerful representations learned by plain ViTs, especially through self-supervised pre-training, could be effectively fine-tuned for localization tasks, challenging the strict necessity of hierarchical backbones for all detection scenarios. The convergence of ViT backbones and end-to-end Transformer detectors is further exemplified by \\texttt{[song2022y4v]}, which introduced ViDT, an extendable and efficient object detector integrating Vision and Detection Transformers. ViDT reconfigures the Swin Transformer as a standalone detector and employs an efficient Transformer decoder, achieving an excellent AP and latency trade-off on COCO. Furthermore, the advancements in large-scale ViT pre-training, such as those in \\cite{ICLR2023} and \\cite{ICLR2023}, implicitly provide even more robust and generalizable backbones for object detection, allowing downstream detection models to achieve higher accuracy and better generalization across diverse scenarios.\n\nIn conclusion, Vision Transformers have established themselves as formidable architectures for core vision tasks. From their initial success in image classification, overcoming challenges related to data efficiency and architectural depth through innovations like DeiT and MAE, to their subsequent adaptation for object detection via end-to-end Transformer designs (DETR) and hierarchical backbones (Swin Transformer), ViTs have consistently pushed performance boundaries. While initial pure ViTs were data-hungry and lacked inherent inductive biases for dense prediction, the continuous evolution has led to more efficient, robust, and scalable models. The ongoing development of massive \"Vision Foundation Models\" and sophisticated hybrid architectures, leveraging advanced self-supervised learning, signifies a future where increasingly versatile visual intelligence can be deployed across an even wider spectrum of real-world applications, further solidifying ViTs' foundational strength in capturing both global context and fine-grained details.\n\\subsection{Dense Prediction Tasks: Segmentation and Pose Estimation}\n\\label{sec:6\\_2\\_dense\\_prediction\\_tasks:\\_segmentation\\_\\_and\\_\\_pose\\_estimation}\n\nDense prediction tasks, encompassing semantic segmentation, instance segmentation, and human pose estimation, demand pixel-level understanding and precise localization, moving significantly beyond image-level classification. Vision Transformers (ViTs), with their inherent ability to model long-range dependencies, have emerged as powerful tools in these fine-grained visual analysis tasks. However, the direct application of vanilla ViTs was initially hindered by their high computational cost, quadratic complexity with respect to image resolution, and lack of inherent multi-scale feature representation, which is crucial for pixel-level tasks \\cite{li2023287}.\n\nTo address these limitations, hierarchical ViT architectures quickly emerged as a foundational solution. The \\textit{Swin Transformer} \\cite{liu2021swin} introduced shifted window attention, enabling efficient computation by restricting self-attention within local windows while allowing cross-window connections through shifting. This hierarchical design effectively generates multi-scale feature maps, making it highly suitable as a backbone for dense prediction. Similarly, the \\textit{Pyramid Vision Transformer} (PVT) \\cite{wang2021pyramid} explicitly designed a pyramid structure to produce multi-scale features, directly mimicking the feature pyramid networks (FPNs) commonly used in dense prediction, thereby facilitating their integration into existing frameworks. Further building on this, \\textit{HiViT} \\cite{zhang2022msa} proposed another hierarchical ViT design, demonstrating improved efficiency and performance for downstream tasks like detection and segmentation when pre-trained with masked image modeling (MIM). These hierarchical designs were pivotal, providing the necessary multi-scale representations that pure ViTs lacked, thereby enabling more coherent and accurate pixel-wise predictions.\n\nAs ViT architectures matured, research consolidated strategies for their adaptation to pixel-level tasks, broadly falling into two categories: ViT encoders paired with specialized decoders, and end-to-end Transformer designs. Early and effective approaches often paired hierarchical ViT backbones with established CNN-style decoders. For instance, studies demonstrated the efficacy of integrating Swin Transformer with decoders like U-Net, Pyramid Scene Parsing (PSP) network, or Feature Pyramid Network (FPN) for semantic segmentation, particularly in domains like remote sensing \\cite{panboonyuen20218r7}. This hybrid approach leverages the ViT's strong global feature extraction while benefiting from the CNN decoder's inductive biases for local detail and spatial upsampling. In contrast, end-to-end Transformer designs sought to eliminate reliance on hand-crafted CNN components. \\textit{Max-DeepLab} \\cite{wang2021maxdeeplab} showcased the capacity of ViTs to directly learn pixel-level representations and segmentations for semantic segmentation, leveraging the global receptive field of Transformers to capture broader contextual cues and produce consistent masks. A significant conceptual advance for instance segmentation (and panoptic/semantic segmentation) was \\textit{Mask2Former} \\cite{cheng2022mask2former}, which unified these tasks under a single query-based Transformer framework. Mask2Former uses a masked attention mechanism and a set of learnable queries to directly predict object masks and their classes, demonstrating superior performance on complex scenes and representing a paradigm shift towards more unified, global reasoning for pixel-level tasks.\n\nThe versatility of ViTs extends to other dense prediction tasks beyond segmentation. For human pose estimation, which involves localizing keypoints on human bodies, models like \\textit{ViTPose} \\cite{xu2022vitpose} have showcased how ViTs can effectively model spatial relationships between body parts. Their global attention mechanism is crucial for inferring occluded or less visible keypoints based on overall body context, often outperforming CNN-based methods. Furthermore, ViTs have been adapted for specialized segmentation challenges. For example, \\textit{SENet} \\cite{hao202488z} proposes a simple yet effective ViT-based encoder-decoder for camouflaged and salient object detection, demonstrating how ViTs can be tailored for specific pixel-level understanding problems. In the medical imaging domain, \\textit{SwinBTS} \\cite{jiang2022zcn} leverages the Swin Transformer for 3D multimodal brain tumor segmentation, highlighting ViT's applicability in critical, high-dimensional analysis.\n\nThe recent advancements in foundational Vision Transformers (as discussed in Section 4) have profoundly amplified their impact on dense prediction. Models pre-trained with self-supervised learning strategies like Masked Autoencoders (MAE) \\cite{he2022masked} or advanced knowledge distillation (e.g., DINOv2 \\cite{oquab2023dinov2}) provide exceptionally robust and generalizable backbones. When fine-tuned for dense prediction tasks, these large-scale models, often paired with task-specific decoders (e.g., UPerNet heads or Mask2Former heads), achieve unprecedented accuracy and efficiency, significantly reducing the need for extensive labeled data for downstream tasks. Moreover, hybrid architectures (as detailed in Section 5) have further refined feature extraction for dense prediction. Models like \\textit{InternImage} \\cite{wang2023internimage} and \\textit{ConvNeXt V2} \\cite{woo2023convnext}, which integrate deformable convolutions or enhance CNNs with MAE pre-training, provide backbones that combine the inductive biases of CNNs (e.g., locality, efficiency) with the global context modeling of Transformers. More recently, hybrid Mamba-Transformer backbones like \\textit{MambaVision} \\cite{hatamizadeh2024xr6} have also demonstrated strong performance on instance and semantic segmentation, showcasing new avenues for efficient long-range dependency modeling. Even efficient variants like \\textit{MobileViT V2} \\cite{vaswani2023mobilevit} aim to bring these powerful local-global feature interactions to resource-constrained environments, broadening the practical applicability of ViTs for real-time dense prediction.\n\nIn conclusion, Vision Transformers have firmly established themselves as formidable architectures for dense prediction tasks. Their evolution from basic image classifiers to sophisticated pixel-level understanding models has been driven by architectural innovations that address computational complexity and multi-scale representation, coupled with specialized decoder designs and end-to-end Transformer frameworks. The recent advent of large-scale, self-supervised pre-trained foundation models has further empowered ViTs, providing exceptionally robust and generalizable backbones that significantly enhance performance across semantic segmentation, instance segmentation, and pose estimation. Future research will likely focus on optimizing the fusion of local and global features, developing more efficient architectures for real-time dense prediction, and exploring novel ways to leverage the rich contextual understanding of foundation models for even finer-grained and more complex pixel-level analyses.\n\\subsection{Specialized and Multimodal Applications}\n\\label{sec:6\\_3\\_specialized\\_\\_and\\_\\_multimodal\\_applications}\n\nBeyond their foundational success in standard image classification and object detection, Vision Transformers (ViTs) have demonstrated remarkable versatility and adaptability across a diverse array of specialized and complex application areas. This subsection highlights their profound utility in challenging real-world scenarios, ranging from low-level image processing to the analysis of high-dimensional and multimodal data, showcasing their capacity to transcend traditional benchmarks and open new avenues for research and deployment.\n\nOne significant domain where ViTs have excelled is image restoration, a crucial low-level vision task demanding precise pixel-level manipulation and global consistency. Traditional convolutional neural networks (CNNs) often struggle with capturing long-range dependencies essential for coherent restoration across an entire image. \\textcite{liang2021v6x} addressed this by introducing SwinIR, a robust baseline for tasks such as super-resolution, denoising, and JPEG compression artifact reduction. SwinIR leverages the hierarchical Swin Transformer, which, through its shifted window attention, efficiently captures both local details and global structural information. This hierarchical design allows SwinIR to effectively model non-local correlations within images, crucial for hallucinating high-frequency details in super-resolution or removing noise while preserving fine textures, ultimately achieving superior performance over state-of-the-art CNNs with a reduced parameter count. Similarly, for visual saliency detection, which requires identifying the most visually prominent regions at a pixel level, \\textcite{liu2021jpu} proposed the Visual Saliency Transformer (VST). VST re-conceptualizes saliency detection as a convolution-free sequence-to-sequence prediction problem. By employing multi-level token fusion and a novel token upsampling method, VST effectively processes both RGB and RGB-D inputs, demonstrating ViT's inherent capability for fine-grained pixel-level understanding and its adaptability to multimodal inputs through a token-based multi-task decoder. The global attention mechanism of VST allows it to integrate contextual information across the entire image, which is vital for distinguishing salient objects from complex backgrounds, a task where local CNN receptive fields might fall short.\n\nThe ability of ViTs to process and fuse information from diverse and high-dimensional data modalities further underscores their adaptability. For hyperspectral image classification (HSIC), which involves analyzing data cubes with hundreds of spectral bands alongside spatial information, ViTs offer a powerful alternative to traditional methods. HSIC presents unique challenges due to its high dimensionality, spectral redundancy, and often limited labeled samples. \\textcite{zhao2024671} introduced the Groupwise Separable Convolutional Vision Transformer (GSC-ViT) to address these issues. GSC-ViT integrates a Groupwise Separable Convolution (GSC) module to efficiently extract local spectral-spatial features, mitigating the parameter burden of pure ViTs and enhancing local representation. Concurrently, a Groupwise Separable Multihead Self-Attention (GSSA) module captures both local and global spatial feature dependencies, allowing the model to effectively learn intricate relationships across the spectral and spatial dimensions. This hybrid approach demonstrates surprising classification performance even with fewer training samples, highlighting the ViT's capacity to model complex, high-dimensional data by judiciously combining inductive biases.\n\nBeyond visual imagery, ViTs have proven effective in processing non-standard data types, such as radar signals for human activity recognition (HAR). HAR from radar data is challenging due to the abstract nature of micro-Doppler signatures, which represent subtle motion patterns. \\textcite{huan202345b} proposed a Lightweight Hybrid Vision Transformer (LH-ViT) network for radar-based HAR. While the architectural details of LH-ViT are discussed in Section 5.2, its application here demonstrates how ViTs can effectively capture global temporal patterns within micro-Doppler spectrograms, which are crucial for distinguishing different human activities. The global attention mechanism allows the model to correlate features across the entire time-frequency representation, providing a more holistic understanding of the activity compared to local processing methods. The emphasis on a lightweight design further underscores the practical considerations for deploying such models in real-time or embedded radar systems.\n\nFurthermore, ViTs are increasingly pivotal in multimodal data fusion, where information from heterogeneous sources must be synergistically combined for robust understanding. For multimodal land use and land cover (LULC) classification, which often involves integrating optical, hyperspectral, LiDAR, and Synthetic Aperture Radar (SAR) data, \\textcite{yao2023sax} developed the Extended Vision Transformer (ExViT). This framework extends conventional ViTs with parallel branches, each tailored for a specific modality (e.g., hyperspectral and LiDAR/SAR data), utilizing position-shared ViTs and separable convolutions for efficient feature extraction. Critically, ExViT employs a cross-modality attention (CMA) module to facilitate dynamic information exchange and alignment between these heterogeneous modalities. This deep fusion mechanism, culminating in a robust decision-level fusion, leverages the ViT's token-based architecture to seamlessly integrate diverse data representations, leading to significantly improved classification accuracy and robustness compared to single-modality or simpler fusion approaches.\n\nCollectively, these applications underscore the remarkable versatility of Vision Transformers. They illustrate how ViTs can be meticulously engineered, often through hybrid architectures, specialized attention mechanisms, and novel tokenization strategies, to effectively process diverse data typesfrom standard images and spectral cubes to radar micro-Doppler mapsand tackle complex tasks like restoration, saliency detection, high-dimensional classification, and multimodal fusion. The continuous drive to optimize ViTs for efficiency and integrate them with domain-specific inductive biases (e.g., convolutions for local features) highlights a key development direction. Future research will likely focus on further enhancing their efficiency for edge deployment, exploring more sophisticated multimodal fusion strategies, and adapting them to novel data modalities and real-time inference challenges in increasingly complex environments, thereby expanding their utility across an even broader spectrum of scientific and industrial applications.\n\n\n\\label{sec:future_outlook:_challenges_and_opportunities}\n\n\\section{Future Outlook: Challenges and Opportunities}\n\\label{sec:future\\_outlook:\\_challenges\\_\\_and\\_\\_opportunities}\n\n\\subsection{Persistent Challenges: Computational Cost and Data Efficiency}\n\\label{sec:7\\_1\\_persistent\\_challenges:\\_computational\\_cost\\_\\_and\\_\\_data\\_efficiency}\n\nDespite the remarkable progress in Vision Transformers (ViTs) across diverse computer vision tasks, their substantial computational cost and persistent reliance on extensive datasets remain critical hurdles, dictating their broader adoption and sustainability, particularly in resource-constrained environments and for specialized applications. While earlier sections have detailed numerous advancements, these challenges are far from resolved, continuously driving innovation to balance performance with practicality.\n\nThe foundational ViT \\cite{Dosovitskiy2021} introduced a quadratic computational complexity with respect to image resolution due to its global self-attention mechanism, alongside a significant appetite for massive pre-training datasets like JFT-300M. Subsequent architectural innovations, such as the hierarchical Swin Transformer \\cite{Liu2021} and Pyramid Vision Transformer (PVT) \\cite{Wang2021}, mitigated this by restricting attention to local windows or employing spatial reduction. Similarly, advanced attention mechanisms like Focal Attention \\cite{FocalAttention2022} aimed to capture context efficiently. However, these solutions often introduce trade-offs; local attention, while efficient, can sacrifice the model's inherent ability to capture true global dependencies, which was a core advantage of the original Transformer. Furthermore, even linear complexity can be prohibitive for processing gigapixel images in domains like digital pathology, highlighting that architectural fixes alone are insufficient.\n\nThe quest for computational efficiency extends beyond architectural design into post-training optimization and hardware-aware deployment. Model quantization, which reduces the precision of weights and activations, is crucial for inference on edge devices. Yet, ViTs present unique challenges for quantization due to the sensitivity of components like Layer Normalization and the non-uniform distribution of attention maps \\cite{lin2021utw}. While methods like Q-ViT \\cite{li20229zn} and FQ-ViT \\cite{lin2021utw} have pushed the limits of fully differentiable and post-training quantization, achieving near-lossless accuracy at lower bit-widths, the inherent complexity of ViT operations still demands specialized techniques to prevent severe performance degradation. Concurrently, model pruning techniques, which remove redundant parameters or operations, are vital. Research explores multi-dimensional compression, pruning attention heads, neurons, and even input sequences \\cite{song2022603, hou2022ver, yin2023029}. These methods, such as CP-ViT \\cite{song2022603} and GOHSP \\cite{yin2023029}, aim to identify and remove deleterious components while preserving accuracy, but the challenge lies in developing robust, generalized pruning criteria that do not require extensive retraining or compromise the model's representational power. Ultimately, for deployment in highly resource-constrained environments, dedicated hardware accelerators like ViTA \\cite{nag2023cfn} are becoming indispensable, demonstrating that a holistic hardware-software co-design approach is necessary to truly overcome the computational bottleneck.\n\nData efficiency, another critical challenge, has seen significant breakthroughs with self-supervised learning (SSL) and knowledge distillation. Data-efficient Image Transformers (DeiT) \\cite{Touvron2021} demonstrated competitive performance with ImageNet-1K pre-training through distillation from a CNN teacher, while Tokens-to-Token ViT (T2T-ViT) \\cite{Yuan2021} improved initial tokenization. The advent of Masked Autoencoders (MAE) \\cite{he2022masked} and advanced self-distillation techniques like DINOv2 \\cite{oquab2023dinov2} further revolutionized pre-training by enabling ViTs to learn robust features from vast quantities of \\textit{unlabeled} data. However, even with these advancements, the \"data hunger\" persists in different forms. Large foundation models, while powerful, still necessitate colossal datasets for pre-training, which may not be universally accessible or ethically diverse. Moreover, adapting these general-purpose models to specialized domains (e.g., medical imaging, remote sensing) where labeled data is scarce remains a significant hurdle. For instance, MAT-VIT \\cite{han2024f96} explores MAE-based auxiliary tasks to leverage unlabeled medical images, highlighting the ongoing need for domain-specific data efficiency strategies.\n\nThe traditional \"pre-train and fine-tune\" paradigm, while effective, also presents challenges. Training ViT-based object detectors from scratch, as explored by \\cite{hong2022ks6}, reveals that simply switching backbones from CNNs to ViTs does not generalize well, emphasizing the deep reliance of ViTs on large-scale pre-training. Furthermore, optimization techniques commonly used in deep learning do not always translate seamlessly to ViTs; for example, gradient accumulation, often used to simulate larger batch sizes, was found to decrease accuracy and increase training time for Swin Transformers \\cite{aburass2023qpf}, underscoring the need for ViT-specific optimization strategies. Even in knowledge distillation, researchers continue to refine techniques, with methods like Attention Distillation \\cite{wang2022pee} showing that self-supervised ViT students require more nuanced guidance to effectively close the performance gap with teachers. The emergence of \"simple\" hierarchical ViTs like Hiera \\cite{ryali202339q}, which strip away architectural \"bells-and-whistles\" when combined with strong SSL, further suggests that the true drivers of efficiency and performance might lie in the pre-training strategy rather than complex architectural designs.\n\nIn conclusion, the computational cost and data efficiency of Vision Transformers are not static problems but dynamic challenges that evolve with architectural innovations and deployment contexts. While significant strides have been made through hierarchical designs, advanced attention, quantization, pruning, and self-supervised learning, these solutions often introduce new trade-offs or highlight the need for further refinement. The continuous efforts to develop more parameter-efficient models, optimize training strategies, and improve generalization from limited data, especially for specialized tasks and resource-constrained environments, remain a fertile and critical ground for future research, pushing towards truly sustainable and ubiquitous visual AI.\n\\subsection{Interpretability, Robustness, and Generalization}\n\\label{sec:7\\_2\\_interpretability,\\_robustness,\\_\\_and\\_\\_generalization}\n\nThe trustworthy deployment of Vision Transformers (ViTs) in real-world applications critically hinges on their interpretability, robustness, and generalization capabilities. These aspects are paramount for fostering confidence in AI systems, ensuring their reliable and safe operation in diverse environments, and upholding ethical standards, especially in sensitive domains.\n\nA fundamental challenge in ViTs, as with many deep learning models, lies in interpreting their complex decision-making processes. Unlike Convolutional Neural Networks (CNNs) where feature maps often correspond to spatially localized patterns, the global self-attention mechanism in Transformers makes direct interpretation more elusive. Early attempts to interpret ViTs often relied on visualizing raw attention maps, but this approach has been critiqued for its limitations; raw attention scores do not directly represent feature importance or causal contributions to the output \\cite{jain2019attention}. More rigorous explainable AI (XAI) methods adapted for Transformers include attention rollout \\cite{abnar2020quantifying}, which propagates attention through layers to aggregate relevance, and Layer-wise Relevance Propagation (LRP) \\cite{bach2015pixel}, which decomposes the prediction backward through the network to assign relevance scores to input pixels. Gradient-based attribution methods, such as Grad-CAM \\cite{selvaraju2017grad} and its variants, have also been applied to ViTs to highlight salient regions influencing decisions. While these methods offer valuable insights, a comprehensive, human-understandable explanation of ViT reasoning, particularly in safety-critical applications, remains an active research area. Furthermore, some studies, such as \\cite{wang2022da0}, suggest that the attention mechanism itself might not be the sole or even primary driver of ViT success, demonstrating that it can be replaced by simpler shift operations with comparable performance. This raises questions about the true mechanistic role of attention and, consequently, the validity of solely relying on attention-based explanations. Efforts like Re-attention \\cite{zhou202105h} aimed to diversify attention maps in deeper ViTs to prevent \"attention collapse\" and improve representation learning, which indirectly aids in making attention patterns more informative, but does not fundamentally solve the interpretability challenge.\n\nBeyond interpretability, ensuring the robustness of ViTs against adversarial attacks and distribution shifts is crucial. Robustness is a multi-faceted concept, encompassing resilience to imperceptible adversarial perturbations, robustness to common image corruptions (e.g., noise, blur), and generalization to out-of-distribution (OOD) data. While initial ViTs demonstrated strong performance on benchmark datasets, their susceptibility to adversarial attacks was quickly identified, akin to CNNs \\cite{mao2021zr1, almalik20223wr}. For instance, \\textcite{almalik20223wr} proposed Self-Ensembling Vision Transformer (SEViT) to enhance adversarial robustness in medical image classification by leveraging intermediate feature representations and combining multiple classifiers.\n\nSignificant strides in improving ViT robustness and generalization have been made through advanced pre-training paradigms, particularly self-supervised learning (SSL). As discussed in Section 4, methods like Masked Autoencoders (MAE) \\cite{mae2022} and DINOv2 \\cite{dino2023} have enabled ViTs to learn powerful, transferable representations from vast amounts of unlabeled data. These SSL approaches force models to learn rich semantic features by reconstructing masked patches or performing self-distillation, leading to representations that are inherently more robust to variations and distribution shifts compared to purely supervised pre-training. For example, DINOv2 \\cite{dino2023} has shown remarkable performance on various downstream tasks and improved transferability, often outperforming supervised counterparts, by producing robust visual features without explicit supervision. The systematic evaluation by \\textcite{mao2021zr1} further highlights that certain ViT components can be detrimental to robustness, and by leveraging robust building blocks and techniques like position-aware attention scaling and patch-wise augmentation, they proposed Robust Vision Transformer (RVT) which achieved superior performance on benchmarks like ImageNet-C (common corruptions), ImageNet-R (renditions), and ImageNet-Sketch (sketches), demonstrating enhanced resilience to various distribution shifts.\n\nThe pursuit of \"Vision Foundation Models,\" as explored in Section 4.3, further exemplifies this drive towards universal robustness and generalization. Models scaled to billions of parameters, such as those by \\textcite{scalingvit2023} and \\textcite{internimage2023}, aim to serve as versatile backbones capable of adapting to a wide array of vision tasks with minimal fine-tuning. This extreme scaling, often coupled with advanced self-supervised pre-training, is hypothesized to imbue models with a deeper understanding of visual semantics, thereby enhancing their generalization to novel situations and improving their resilience to variations in input data.\n\nMoreover, the integration of inductive biases from CNNs into Transformer architectures has also contributed to developing more robust and generalizable models. Hybrid architectures, such as ConvNeXt V2 \\cite{convnextv22023} (discussed in Section 5.1), which co-designs CNNs with MAE pre-training, and InternImage \\cite{internimage2023} (also from Section 5.1), which incorporates deformable convolutions into large-scale vision foundation models, represent a synergistic approach. By combining the local feature extraction strengths of CNNs with the global context modeling of Transformers, these models aim to achieve a more comprehensive and robust understanding of visual data, often exhibiting better performance on out-of-distribution tasks. For instance, \\textcite{zhou2021rtn} systematically investigated the transfer learning ability of ConvNets and vision transformers across 15 downstream tasks, observing consistent advantages for Transformer-based backbones on 13 tasks, particularly noting their robustness in multi-task learning and their reliance on whole-network fine-tuning for optimal transfer. Practical considerations for real-world deployment also necessitate efficient architectures that maintain performance, contributing to operational robustness. While not directly addressing adversarial or OOD robustness, efficient ViT variants like MobileViT V2 \\cite{mobilevitv22023} (from Section 5.2) ensure reliable performance within resource constraints, which is a form of practical robustness for deployment.\n\nDespite significant strides in enhancing the robustness and generalization of ViTs through scaling, advanced self-supervised learning, and hybrid designs, the challenge of achieving true interpretability remains complex. While methods offer glimpses into model behavior, a holistic, causal understanding of ViT decisions, especially in safety-critical applications, is still an active research area. Future work must continue to explore novel techniques for transparent model design, robust evaluation against diverse adversarial threats and OOD data, and the development of ViTs that are not only high-performing but also inherently understandable and trustworthy.\n\\subsection{Future Trends and Open Problems}\n\\label{sec:7\\_3\\_future\\_trends\\_\\_and\\_\\_open\\_problems}\n\nThe evolution of Vision Transformers (ViTs) is rapidly accelerating, pushing the frontiers of artificial intelligence towards more comprehensive, intelligent, and human-like visual systems. This subsection looks ahead, identifying cutting-edge research directions and unresolved questions that will define the next generation of ViT architectures. While Section 7.1 and 7.2 address persistent challenges related to efficiency, robustness, and interpretability, this section focuses on more speculative, transformative trends and deeper, unsolved theoretical problems that extend beyond incremental improvements.\n\nA significant future trend lies in the exploration of \\textbf{novel architectural paradigms that fundamentally rethink the self-attention mechanism}. While ViTs excel at global context modeling, their quadratic computational complexity remains a bottleneck. The integration of ViTs with State-Space Models (SSMs), such as Mamba, represents a particularly exciting frontier. MambaVision \\cite{hatamizadeh2024xr6} exemplifies this by redesigning the Mamba formulation for visual features and strategically incorporating self-attention blocks in later layers to effectively capture long-range spatial dependencies. This hybrid approach demonstrates state-of-the-art performance and throughput, suggesting a promising path for combining the strengths of different architectures to overcome individual limitations. Beyond such integrations, a more radical open question is whether attention is truly indispensable. Research like ShiftViT \\cite{wang2022da0} explores this by replacing attention layers with a zero-parameter shift operation, achieving competitive performance and prompting a re-evaluation of the core mechanisms driving ViT success. This suggests a future where attention might be replaced or heavily augmented by simpler, more efficient operations, as also highlighted by broader surveys on attention mechanism redesigns \\cite{heidari2024d9k}. Furthermore, the development of adaptive architectures that can dynamically adjust their computational patterns based on input characteristics, such as Dynamic Window Vision Transformer (DW-ViT) \\cite{ren2022ifo} which assigns windows of different sizes to various attention heads, offers a pathway to more flexible and robust visual understanding beyond fixed windowing.\n\nThe ultimate ambition for many researchers is the development of truly \\textbf{universal and multimodal visual foundation models} capable of sophisticated reasoning. Building upon the success of large-scale pre-training \\cite{ICLR2023\\_Scaling\\_Vision\\_Transformers\\_to\\_1\\_Billion\\_Parameters, ICLR2023\\_DINOv2}, the field is now focused on creating generalist backbones that transcend individual tasks and modalities. GiT \\cite{wang20249qa} represents a significant step, proposing a vanilla ViT framework with a universal language interface that can simultaneously handle diverse vision tasksfrom image captioning to object detection and segmentationwithout task-specific modifications. This approach aims to narrow the architectural gap between vision and language models, fostering mutual enhancement across tasks. The extension of these models to genuinely multimodal reasoning, integrating vision not only with language but also with audio, 3D data, and other sensor modalities, is paramount. This requires novel architectural designs and pre-training strategies to effectively fuse information from disparate sources, enabling a more holistic and human-like understanding. For instance, unifying 2D and 3D vision remains a significant challenge, with efforts like Simple3D-Former \\cite{wang2022gq4} demonstrating that a standard 2D ViT can be minimally customized to perform robustly on 3D tasks, suggesting a path towards more transferable architectures. In practical applications, multimodal fusion is already yielding benefits, such as ViT-FuseNet \\cite{zhou2024toe} for vehicle-infrastructure cooperative perception (LiDAR and camera) and NF-DVT \\cite{pan20249k5} for monocular 3D object detection (RGB and depth), but the grand challenge is to generalize these fusion capabilities across \\textit{any} combination of modalities.\n\nBeyond architectural and multimodal advancements, several \\textbf{deeper open problems in visual intelligence} persist, representing grand challenges for the field. One critical area is \\textbf{compositional generalization and relational reasoning}. While ViTs can learn complex patterns, their ability to reason about novel combinations of objects and their relationships, similar to human cognition, remains limited. RelViT \\cite{ma2022vf3} offers a concept-guided approach to improve relational reasoning and systematic generalization, but achieving robust compositional understanding across diverse, unseen scenarios is a profound theoretical and practical hurdle. This involves understanding how ViTs can move beyond statistical correlations to infer causal relationships within visual scenes, a key component of true intelligence. Another significant challenge is \\textbf{lifelong or continual learning}, where ViTs can adapt to new tasks and data streams without catastrophically forgetting previously acquired knowledge. This is essential for deploying models in dynamic real-world environments that evolve over time. Furthermore, integrating ViTs into \\textbf{embodied AI systems} that interact with the physical world, requiring real-time perception, planning, and action, presents complex challenges in bridging the gap between static image understanding and dynamic environmental interaction.\n\nFinally, as ViTs become increasingly powerful and ubiquitous, the \\textbf{ethical implications and the need for trustworthy AI systems} become paramount \\cite{hassija2025wq3}. While Section 7.2 discusses interpretability and robustness as ongoing challenges, the future demands a proactive approach to designing ViTs that are inherently fair, transparent, and privacy-preserving. Developing generalizable interpretability methods for massive, multimodal foundation models is a major open problem. For instance, prototype-based interpretable ViTs like ProtoViT \\cite{ma2024uan} offer local explanations by comparing image parts to learned prototypes, but scaling such fine-grained interpretability to the complexity of universal foundation models remains an active research area. Mitigating biases embedded in training data, ensuring equitable performance across diverse demographic groups, and developing robust privacy-preserving mechanisms for visual data are not just technical hurdles but critical societal imperatives for the responsible deployment of future vision systems.\n\nIn conclusion, the future of Vision Transformers is characterized by a relentless pursuit of greater efficiency through novel architectural paradigms, the development of truly generalist and multimodal foundation models, and a deeper engagement with fundamental problems of visual intelligence such as compositional and causal reasoning. Crucially, addressing the ethical dimensions of these powerful systems will be indispensable for realizing their transformative potential in a responsible and beneficial manner.\n\n\n\\newpage\n\\section*{References}\n\\addcontentsline{toc}{section}{References}\n\n\\begin{thebibliography}{367}\n\n\\bibitem{liu2021ljs}\nZe Liu, Yutong Lin, Yue Cao, et al. (2021). \\textit{Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}. IEEE International Conference on Computer Vision.\n\n\\bibitem{liang2021v6x}\nJingyun Liang, Jie Cao, Guolei Sun, et al. (2021). \\textit{SwinIR: Image Restoration Using Swin Transformer}. 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW).\n\n\\bibitem{han2020yk0}\nKai Han, Yunhe Wang, Hanting Chen, et al. (2020). \\textit{A Survey on Vision Transformer}. IEEE Transactions on Pattern Analysis and Machine Intelligence.\n\n\\bibitem{chen2021r2y}\nChun-Fu Chen, Quanfu Fan, and Rameswar Panda (2021). \\textit{CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification}. IEEE International Conference on Computer Vision.\n\n\\bibitem{mehta20216ad}\nSachin Mehta, and Mohammad Rastegari (2021). \\textit{MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer}. International Conference on Learning Representations.\n\n\\bibitem{li2022raj}\nYanghao Li, Hanzi Mao, Ross B. Girshick, et al. (2022). \\textit{Exploring Plain Vision Transformer Backbones for Object Detection}. European Conference on Computer Vision.\n\n\\bibitem{chen2022woa}\nShoufa Chen, Chongjian Ge, Zhan Tong, et al. (2022). \\textit{AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition}. Neural Information Processing Systems.\n\n\\bibitem{xia2022qga}\nZhuofan Xia, Xuran Pan, S. Song, et al. (2022). \\textit{Vision Transformer with Deformable Attention}. Computer Vision and Pattern Recognition.\n\n\\bibitem{zhou202105h}\nDaquan Zhou, Bingyi Kang, Xiaojie Jin, et al. (2021). \\textit{DeepViT: Towards Deeper Vision Transformer}. arXiv.org.\n\n\\bibitem{liu2021jpu}\nNian Liu, Ni Zhang, Kaiyuan Wan, et al. (2021). \\textit{Visual Saliency Transformer}. IEEE International Conference on Computer Vision.\n\n\\bibitem{lee2021us0}\nSeung Hoon Lee, Seunghyun Lee, and B. Song (2021). \\textit{Vision Transformer for Small-Size Datasets}. arXiv.org.\n\n\\bibitem{zhang2021fje}\nBo Zhang, Shuyang Gu, Bo Zhang, et al. (2021). \\textit{StyleSwin: Transformer-based GAN for High-resolution Image Generation}. Computer Vision and Pattern Recognition.\n\n\\bibitem{jiang2022zcn}\nYun Jiang, Yuan Zhang, Xinyi Lin, et al. (2022). \\textit{SwinBTS: A Method for 3D Multimodal Brain Tumor Segmentation Using Swin Transformer}. Brain Science.\n\n\\bibitem{islam2022iss}\nMd. Nazmul Islam, Madina Hasan, Md. Kabir Hossain, et al. (2022). \\textit{Vision transformer and explainable transfer learning models for auto detection of kidney cyst, stone and tumor from CT-radiography}. Scientific Reports.\n\n\\bibitem{li2022a4u}\nJiashi Li, Xin Xia, W. Li, et al. (2022). \\textit{Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios}. arXiv.org.\n\n\\bibitem{yao202245i}\nTing Yao, Yingwei Pan, Yehao Li, et al. (2022). \\textit{Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning}. European Conference on Computer Vision.\n\n\\bibitem{borhani2022w8x}\nY. Borhani, Javad Khoramdel, and E. Najafi (2022). \\textit{A deep learning based approach for automated plant disease classification using vision transformer}. Scientific Reports.\n\n\\bibitem{mao2021zr1}\nXiaofeng Mao, Gege Qi, Yuefeng Chen, et al. (2021). \\textit{Towards Robust Vision Transformer}. Computer Vision and Pattern Recognition.\n\n\\bibitem{chen202174h}\nJunyu Chen, Yufan He, E. Frey, et al. (2021). \\textit{ViT-V-Net: Vision Transformer for Unsupervised Volumetric Medical Image Registration}. arXiv.org.\n\n\\bibitem{jie20220pc}\nShibo Jie, and Zhi-Hong Deng (2022). \\textit{Convolutional Bypasses Are Better Vision Transformer Adapters}. European Conference on Artificial Intelligence.\n\n\\bibitem{fan2022m88}\nChi-Mao Fan, Tsung-Jung Liu, and Kuan-Hsien Liu (2022). \\textit{SUNet: Swin Transformer UNet for Image Denoising}. International Symposium on Circuits and Systems.\n\n\\bibitem{lin20216a3}\nHezheng Lin, Xingyi Cheng, Xiangyu Wu, et al. (2021). \\textit{CAT: Cross Attention in Vision Transformer}. IEEE International Conference on Multimedia and Expo.\n\n\\bibitem{lin2021utw}\nYang Lin, Tianyu Zhang, Peiqin Sun, et al. (2021). \\textit{FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer}. International Joint Conference on Artificial Intelligence.\n\n\\bibitem{li2022mco}\nZhikai Li, and Qingyi Gu (2022). \\textit{I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference}. IEEE International Conference on Computer Vision.\n\n\\bibitem{li2022tl7}\nYanjing Li, Sheng Xu, Baochang Zhang, et al. (2022). \\textit{Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer}. Neural Information Processing Systems.\n\n\\bibitem{yang2021myb}\nJinyu Yang, Jingjing Liu, N. Xu, et al. (2021). \\textit{TVT: Transferable Vision Transformer for Unsupervised Domain Adaptation}. IEEE Workshop/Winter Conference on Applications of Computer Vision.\n\n\\bibitem{yu2022iy0}\nShixing Yu, Tianlong Chen, Jiayi Shen, et al. (2022). \\textit{Unified Visual Transformer Compression}. International Conference on Learning Representations.\n\n\\bibitem{zhuang2022qn7}\nWanyi Zhuang, Qi Chu, Zhentao Tan, et al. (2022). \\textit{UIA-ViT: Unsupervised Inconsistency-Aware Method based on Vision Transformer for Face Forgery Detection}. European Conference on Computer Vision.\n\n\\bibitem{deng2021man}\nPeifang Deng, Kejie Xu, and Hong Huang (2021). \\textit{When CNNs Meet Vision Transformer: A Joint Framework for Remote Sensing Scene Classification}. IEEE Geoscience and Remote Sensing Letters.\n\n\\bibitem{wang2021oct}\nJun Wang, Xiaohan Yu, and Yongsheng Gao (2021). \\textit{Feature Fusion Vision Transformer for Fine-Grained Visual Categorization}. British Machine Vision Conference.\n\n\\bibitem{wang2022ti0}\nTeng Wang, Lei Gong, Chao Wang, et al. (2022). \\textit{ViA: A Novel Vision-Transformer Accelerator Based on FPGA}. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems.\n\n\\bibitem{li2022ow4}\nXiang Li, Wenhai Wang, Lingfeng Yang, et al. (2022). \\textit{Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality}. arXiv.org.\n\n\\bibitem{wang2022da0}\nGuangting Wang, Yucheng Zhao, Chuanxin Tang, et al. (2022). \\textit{When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{deng2022bil}\nJiajun Deng, Zhengyuan Yang, Daqing Liu, et al. (2022). \\textit{TransVG++: End-to-End Visual Grounding With Language Conditioned Vision Transformer}. IEEE Transactions on Pattern Analysis and Machine Intelligence.\n\n\\bibitem{yang20228mm}\nShusheng Yang, Xinggang Wang, Yu Li, et al. (2022). \\textit{Temporally Efficient Vision Transformer for Video Instance Segmentation}. Computer Vision and Pattern Recognition.\n\n\\bibitem{gheflati202131i}\nBehnaz Gheflati, and H. Rivaz (2021). \\textit{Vision Transformer for Classification of Breast Ultrasound Images}. arXiv.org.\n\n\\bibitem{tang2022e2i}\nXinyu Tang, Zengbing Xu, and Zhigang Wang (2022). \\textit{A Novel Fault Diagnosis Method of Rolling Bearing Based on Integrated Vision Transformer Model}. Italian National Conference on Sensors.\n\n\\bibitem{yu202236t}\nXiaohan Yu, Jun Wang, Yang Zhao, et al. (2022). \\textit{Mix-ViT: Mixing attentive vision transformer for ultra-fine-grained visual categorization}. Pattern Recognition.\n\n\\bibitem{li2021ra5}\nHanting Li, Ming-Fa Sui, Feng Zhao, et al. (2021). \\textit{MViT: Mask Vision Transformer for Facial Expression Recognition in the wild}. arXiv.org.\n\n\\bibitem{meng2022t3x}\nXiaoliang Meng, Yuechi Yang, Libo Wang, et al. (2022). \\textit{Class-Guided Swin Transformer for Semantic Segmentation of Remote Sensing Imagery}. IEEE Geoscience and Remote Sensing Letters.\n\n\\bibitem{li20223n5}\nZ. Li, Mengshu Sun, Alec Lu, et al. (2022). \\textit{Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization}. International Conference on Field-Programmable Logic and Applications.\n\n\\bibitem{bazi2022tlu}\nY. Bazi, Mohamad Mahmoud Al Rahhal, M. L. Mekhalfi, et al. (2022). \\textit{Bi-Modal Transformer-Based Approach for Visual Question Answering in Remote Sensing Imagery}. IEEE Transactions on Geoscience and Remote Sensing.\n\n\\bibitem{zheng2022gg5}\nHao Zheng, Guohui Wang, and Xuchen Li (2022). \\textit{Swin-MLP: a strawberry appearance quality identification method by Swin Transformer and multi-layer perceptron}. Journal of Food Measurement & Characterization.\n\n\\bibitem{gao2021uzl}\nXiaohong W. Gao, Y. Qian, and Alice Gao (2021). \\textit{COVID-VIT: Classification of COVID-19 from CT chest images based on vision transformer models}. arXiv.org.\n\n\\bibitem{zheng202218g}\nZangwei Zheng, Xiangyu Yue, Kai Wang, et al. (2022). \\textit{Prompt Vision Transformer for Domain Generalization}. arXiv.org.\n\n\\bibitem{bi20225lu}\nChunguang Bi, Nan Hu, Yiqiang Zou, et al. (2022). \\textit{Development of Deep Learning Methodology for Maize Seed Variety Recognition Based on Improved Swin Transformer}. Agronomy.\n\n\\bibitem{chen2022vac}\nYihan Chen, Xingyu Gu, Zhen Liu, et al. (2022). \\textit{A Fast Inference Vision Transformer for Automatic Pavement Image Classification and Its Visual Interpretation Method}. Remote Sensing.\n\n\\bibitem{song2022603}\nZhuoran Song, Yihong Xu, Zhezhi He, et al. (2022). \\textit{CP-ViT: Cascade Vision Transformer Pruning via Progressive Sparsity Prediction}. arXiv.org.\n\n\\bibitem{li2022rl9}\nTao Li, Zheng Zhang, Lishen Pei, et al. (2022). \\textit{HashFormer: Vision Transformer Based Deep Hashing for Image Retrieval}. IEEE Signal Processing Letters.\n\n\\bibitem{wensel2022lva}\nJames Wensel, Hayat Ullah, and Arslan Munir (2022). \\textit{ViT-ReT: Vision and Recurrent Transformer Neural Networks for Human Activity Recognition in Videos}. IEEE Access.\n\n\\bibitem{wang2021sav}\nWenxiao Wang, Lu-yuan Yao, Long Chen, et al. (2021). \\textit{CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention}. arXiv.org.\n\n\\bibitem{naseem2022c95}\nUsman Naseem, Matloob Khushi, and Jinman Kim (2022). \\textit{Vision-Language Transformer for Interpretable Pathology Visual Question Answering}. IEEE journal of biomedical and health informatics.\n\n\\bibitem{wu20210gs}\nYanan Wu, Shouliang Qi, Yu Sun, et al. (2021). \\textit{A vision transformer for emphysema classification using CT images}. Physics in Medicine and Biology.\n\n\\bibitem{lyu2022vd9}\nYanjun Lyu, Xiao-Wen Yu, Dajiang Zhu, et al. (2022). \\textit{Classification of Alzheimer's Disease via Vision Transformer: Classification of Alzheimer's Disease via Vision Transformer}. Petra.\n\n\\bibitem{krishnan2021086}\nKoushik Sivarama Krishnan, and Karthik Sivarama Krishnan (2021). \\textit{Vision Transformer based COVID-19 Detection using Chest X-rays}. 2021 6th International Conference on Signal Processing, Computing and Control (ISPCC).\n\n\\bibitem{yang20210bg}\nHuanrui Yang, Hongxu Yin, Maying Shen, et al. (2021). \\textit{Global Vision Transformer Pruning with Hessian-Aware Saliency}. Computer Vision and Pattern Recognition.\n\n\\bibitem{li20229zn}\nZhexin Li, Tong Yang, Peisong Wang, et al. (2022). \\textit{Q-ViT: Fully Differentiable Quantization for Vision Transformer}. arXiv.org.\n\n\\bibitem{wang2022n7h}\nHongmiao Wang, Cheng Xing, Junjun Yin, et al. (2022). \\textit{Land Cover Classification for Polarimetric SAR Images Based on Vision Transformer}. Remote Sensing.\n\n\\bibitem{chen202199v}\nMinghao Chen, Kan Wu, Bolin Ni, et al. (2021). \\textit{Searching the Search Space of Vision Transformer}. Neural Information Processing Systems.\n\n\\bibitem{panboonyuen20218r7}\nTeerapong Panboonyuen, Kulsawasd Jitkajornwanich, S. Lawawirojwong, et al. (2021). \\textit{Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images}. Remote Sensing.\n\n\\bibitem{liang2022xlx}\nJunjie Liang, Cihui Yang, Jingting Zhong, et al. (2022). \\textit{BTSwin-Unet: 3D U-shaped Symmetrical Swin Transformer-based Network for Brain Tumor Segmentation with Self-supervised Pre-training}. Neural Processing Letters.\n\n\\bibitem{zhou2021rtn}\nHong-Yu Zhou, Chi-Ken Lu, Sibei Yang, et al. (2021). \\textit{ConvNets vs. Transformers: Whose Visual Representations are More Transferable?}. 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW).\n\n\\bibitem{dubey2021ra5}\nS. Dubey, S. Singh, and Wei Chu (2021). \\textit{Vision Transformer Hashing for Image Retrieval}. IEEE International Conference on Multimedia and Expo.\n\n\\bibitem{ayas2022md0}\nSelen Ayas, and Esra Tunc-Gormus (2022). \\textit{SpectralSWIN: a spectral-swin transformer network for hyperspectral image classification}. International Journal of Remote Sensing.\n\n\\bibitem{tian2022shu}\nJialin Tian, Xing Xu, Fumin Shen, et al. (2022). \\textit{TVT: Three-Way Vision Transformer through Multi-Modal Hypersphere Learning for Zero-Shot Sketch-Based Image Retrieval}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{liu2022249}\nXingyu Liu, Yuehua Wu, Wenkai Liang, et al. (2022). \\textit{High Resolution SAR Image Classification Using Global-Local Network Structure Based on Vision Transformer and CNN}. IEEE Geoscience and Remote Sensing Letters.\n\n\\bibitem{zhang2021mcp}\nYuan Zhang, Jian Cao, Ling Zhang, et al. (2021). \\textit{A free lunch from ViT: adaptive attention multi-scale fusion Transformer for fine-grained visual recognition}. IEEE International Conference on Acoustics, Speech, and Signal Processing.\n\n\\bibitem{han2021vis}\nQi Han, Zejia Fan, Qi Dai, et al. (2021). \\textit{Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight}. arXiv.org.\n\n\\bibitem{kim2022m6u}\nSangwon Kim, J. Nam, and ByoungChul Ko (2022). \\textit{Facial Expression Recognition Based on Squeeze Vision Transformer}. Italian National Conference on Sensors.\n\n\\bibitem{zhou2022nln}\nXiaoli Zhou, Chaowei Tang, Pan Huang, et al. (2022). \\textit{ASI-DBNet: An Adaptive Sparse Interactive ResNet-Vision Transformer Dual-Branch Network for the Grading of Brain Cancer Histopathological Images}. Interdisciplinary Sciences Computational Life Sciences.\n\n\\bibitem{hu202242d}\nZhongxu Hu, Yiran Zhang, Yang Xing, et al. (2022). \\textit{Toward Human-Centered Automated Driving: A Novel Spatiotemporal Vision Transformer-Enabled Head Tracker}. IEEE Vehicular Technology Magazine.\n\n\\bibitem{you2022bor}\nHaoran You, Yunyang Xiong, Xiaoliang Dai, et al. (2022). \\textit{Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference}. Computer Vision and Pattern Recognition.\n\n\\bibitem{ren2022ifo}\nPengzhen Ren, Changlin Li, Guangrun Wang, et al. (2022). \\textit{Beyond Fixation: Dynamic Window Visual Transformer}. Computer Vision and Pattern Recognition.\n\n\\bibitem{wang20215ra}\nJinhai Wang, Zongyin Zhang, Lufeng Luo, et al. (2021). \\textit{SwinGD: A Robust Grape Bunch Detection Model Based on Swin Transformer in Complex Vineyard Environment}. Horticulturae.\n\n\\bibitem{xiao202229y}\nXiao Xiao, Wenliang Guo, Rui Chen, et al. (2022). \\textit{A Swin Transformer-Based Encoding Booster Integrated in U-Shaped Network for Building Extraction}. Remote Sensing.\n\n\\bibitem{jamil20223a4}\nSonain Jamil, Muhammad Sohail Abbas, and Anisha Roy (2022). \\textit{Distinguishing Malicious Drones Using Vision Transformer}. Applied Informatics.\n\n\\bibitem{bai2022f1v}\nLong Bai, Liangyu Wang, Tong Chen, et al. (2022). \\textit{Transformer-Based Disease Identification for Small-Scale Imbalanced Capsule Endoscopy Dataset}. Electronics.\n\n\\bibitem{li2022th8}\nKuoyang Li, Min Zhang, Maiping Xu, et al. (2022). \\textit{Ship Detection in SAR Images Based on Feature Enhancement Swin Transformer and Adjacent Feature Fusion}. Remote Sensing.\n\n\\bibitem{almalik20223wr}\nFaris Almalik, Mohammad Yaqub, and Karthik Nandakumar (2022). \\textit{Self-Ensembling Vision Transformer (SEViT) for Robust Medical Image Classification}. International Conference on Medical Image Computing and Computer-Assisted Intervention.\n\n\\bibitem{sha2022ae0}\nZ. Sha, and Jianfeng Li (2022). \\textit{MITformer: A Multiinstance Vision Transformer for Remote Sensing Scene Classification}. IEEE Geoscience and Remote Sensing Letters.\n\n\\bibitem{zhang2022msa}\nXiaosong Zhang, Yunjie Tian, Wei Huang, et al. (2022). \\textit{HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling}. arXiv.org.\n\n\\bibitem{htten2022lui}\nNils Htten, R. Meyes, and Tobias Meisen (2022). \\textit{Vision Transformer in Industrial Visual Inspection}. Applied Sciences.\n\n\\bibitem{hatamizadeh2022y9x}\nAli Hatamizadeh, Ziyue Xu, Dong Yang, et al. (2022). \\textit{UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation}. arXiv.org.\n\n\\bibitem{montazerin2022dgi}\nMansooreh Montazerin, Soheil Zabihi, E. Rahimian, et al. (2022). \\textit{ViT-HGR: Vision Transformer-based Hand Gesture Recognition from High Density Surface EMG Signals}. Annual International Conference of the IEEE Engineering in Medicine and Biology Society.\n\n\\bibitem{kojima2022k5c}\nTakeshi Kojima, Yutaka Matsuo, and Yusuke Iwasawa (2022). \\textit{Robustifying Vision Transformer without Retraining from Scratch by Test-Time Class-Conditional Feature Alignment}. International Joint Conference on Artificial Intelligence.\n\n\\bibitem{kang2022pv3}\nMinhee Kang, Wooseop Lee, Keeyeon Hwang, et al. (2022). \\textit{Vision Transformer for Detecting Critical Situations AndExtracting Functional Scenario for Automated Vehicle Safety Assessment}. Social Science Research Network.\n\n\\bibitem{tian2022qb5}\nGeng Tian, Ziwei Wang, Chang Wang, et al. (2022). \\textit{A deep ensemble learning-based automated detection of COVID-19 using lung CT images and Vision Transformer and ConvNeXt}. Frontiers in Microbiology.\n\n\\bibitem{peng2022snr}\nLihong Peng, Chang Wang, Geng Tian, et al. (2022). \\textit{Analysis of CT scan images for COVID-19 pneumonia based on a deep ensemble framework with DenseNet, Swin transformer, and RegNet}. Frontiers in Microbiology.\n\n\\bibitem{ho20228q6}\nChi M. K. Ho, K. Yow, Zhongwen Zhu, et al. (2022). \\textit{Network Intrusion Detection via Flow-to-Image Conversion and Vision Transformer Classification}. IEEE Access.\n\n\\bibitem{xia2022dnj}\nXin Xia, Jiashi Li, Jie Wu, et al. (2022). \\textit{TRT-ViT: TensorRT-oriented Vision Transformer}. arXiv.org.\n\n\\bibitem{wang202232c}\nZhenmin Wang, Haoyu Chen, Q. Zhong, et al. (2022). \\textit{Recognition of penetration state in GTAW based on vision transformer using weld pool image}. The International Journal of Advanced Manufacturing Technology.\n\n\\bibitem{mogan202229d}\nJashila Nair Mogan, C. Lee, K. Lim, et al. (2022). \\textit{Gait-ViT: Gait Recognition with Vision Transformer}. Italian National Conference on Sensors.\n\n\\bibitem{yang20221ce}\nYuguang Yang, HongMei Fu, Shang Gao, et al. (2022). \\textit{Intrusion detection: A model based on the improved vision transformer}. Transactions on Emerging Telecommunications Technologies.\n\n\\bibitem{li2022ip7}\nNannan Li, Yaran Chen, Weifan Li, et al. (2022). \\textit{BViT: Broad Attention-Based Vision Transformer}. IEEE Transactions on Neural Networks and Learning Systems.\n\n\\bibitem{yu20220np}\nTan Yu, Gangming Zhao, Ping Li, et al. (2022). \\textit{BOAT: Bilateral Local Attention Vision Transformer}. British Machine Vision Conference.\n\n\\bibitem{li20229fn}\nJiacheng Li, Menglin Wang, and Xiaojin Gong (2022). \\textit{Transformer Based Multi-Grained Features for Unsupervised Person Re-Identification}. 2023 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW).\n\n\\bibitem{huang2022iwe}\nJiahao Huang, Xiaodan Xing, Zhifan Gao, et al. (2022). \\textit{Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI}. International Conference on Medical Image Computing and Computer-Assisted Intervention.\n\n\\bibitem{qu2022be0}\nMengxue Qu, Yu Wu, Wu Liu, et al. (2022). \\textit{SiRi: A Simple Selective Retraining Mechanism for Transformer-based Visual Grounding}. European Conference on Computer Vision.\n\n\\bibitem{zeng2022ce2}\nWenyuan Zeng, Meng Li, Wenjie Xiong, et al. (2022). \\textit{MPCViT: Searching for Accurate and Efficient MPC-Friendly Vision Transformer with Heterogeneous Attention}. IEEE International Conference on Computer Vision.\n\n\\bibitem{lin20225ad}\nJi Lin, Haifeng Lin, and Fang Wang (2022). \\textit{STPM_SAHI: A Small-Target Forest Fire Detection Model Based on Swin Transformer and Slicing Aided Hyper Inference}. Forests.\n\n\\bibitem{reghunath2022z8g}\nL. Reghunath, and R. Rajan (2022). \\textit{Transformer-based ensemble method for multiple predominant instruments recognition in polyphonic music}. EURASIP Journal on Audio, Speech, and Music Processing.\n\n\\bibitem{kundu2022z97}\nDipanjali Kundu, Umme Raihan Siddiqi, and Md. Mahbubur Rahman (2022). \\textit{Vision Transformer based Deep Learning Model for Monkeypox Detection}. 2022 25th International Conference on Computer and Information Technology (ICCIT).\n\n\\bibitem{sun2022cti}\nFan Sun, Wujie Zhou, Lv Ye, et al. (2022). \\textit{Hierarchical Decoding Network Based on Swin Transformer for Detecting Salient Objects in RGB-T Images}. IEEE Signal Processing Letters.\n\n\\bibitem{li2022gef}\nYupeng Li, Huimin Lu, Yifan Wang, et al. (2022). \\textit{ViT-Cap: A Novel Vision Transformer-Based Capsule Network Model for Finger Vein Recognition}. Applied Sciences.\n\n\\bibitem{guo20228rt}\nBangwei Guo, Xingyu Li, Miao Yang, et al. (2022). \\textit{Predicting microsatellite instability and key biomarkers in colorectal cancer from H&Estained images: achieving stateoftheart predictive performance with fewer data using Swin Transformer}. The Journal of Pathology: Clinical Research.\n\n\\bibitem{li202240n}\nAo Li, Yaqin Zhao, and Zhaoxiang Zheng (2022). \\textit{Novel Recursive BiFPN Combining with Swin Transformer for Wildland Fire Smoke Detection}. Forests.\n\n\\bibitem{jiang2022jlc}\nXiaoben Jiang, Yu Zhu, Gan Cai, et al. (2022). \\textit{MXT: A New Variant of Pyramid Vision Transformer for Multi-label Chest X-ray Image Classification}. Cognitive Computation.\n\n\\bibitem{lin2021oan}\nYang Lin, Tianyu Zhang, Peiqin Sun, et al. (2021). \\textit{FQ-ViT: Fully Quantized Vision Transformer without Retraining}. arXiv.org.\n\n\\bibitem{wang2022dl1}\nJing Wang, Haotian Fa, X. Hou, et al. (2022). \\textit{MSTRIQ: No Reference Image Quality Assessment Based on Swin Transformer with Multi-Stage Fusion}. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW).\n\n\\bibitem{li2022wab}\nHan Li, Sufang Li, Jiguo Yu, et al. (2022). \\textit{Plant disease and insect pest identification based on vision transformer}. Other Conferences.\n\n\\bibitem{park2022eln}\nSangjoon Park, and Jong-Chul Ye (2022). \\textit{Multi-Task Distributed Learning Using Vision Transformer With Random Patch Permutation}. IEEE Transactions on Medical Imaging.\n\n\\bibitem{shen2022d6i}\nYifan Shen, Li Liu, Zhihao Tang, et al. (2022). \\textit{Explainable Survival Analysis with Convolution-Involved Vision Transformer}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{wang20224wo}\nAili Wang, Shuang Xing, Yan Zhao, et al. (2022). \\textit{A Hyperspectral Image Classification Method Based on Adaptive Spectral Spatial Kernel Combined with Improved Vision Transformer}. Remote Sensing.\n\n\\bibitem{tao2022gdr}\nTianxin Tao, Daniele Reda, and M. V. D. Panne (2022). \\textit{Evaluating Vision Transformer Methods for Deep Reinforcement Learning from Pixels}. arXiv.org.\n\n\\bibitem{wu2021nmg}\nShupei Wu, Youqiang Sun, and He Huang (2021). \\textit{Multi-granularity Feature Extraction Based on Vision Transformer for Tomato Leaf Disease Recognition}. 2021 3rd International Academic Exchange Conference on Science and Technology Innovation (IAECST).\n\n\\bibitem{liu2022c56}\nJinwei Liu, Yan Li, Guitao Cao, et al. (2022). \\textit{Feature Pyramid Vision Transformer for MedMNIST Classification Decathlon}. IEEE International Joint Conference on Neural Network.\n\n\\bibitem{wang2022pb8}\nYangtao Wang, Yanzhao Xie, Lisheng Fan, et al. (2022). \\textit{STMG: Swin transformer for multi-label image recognition with graph convolution network}. Neural computing & applications (Print).\n\n\\bibitem{xiong2022ec2}\nZinan Xiong, Chenxi Wang, Ying Li, et al. (2022). \\textit{Swin-Pose: Swin Transformer Based Human Pose Estimation}. Conference on Multimedia Information Processing and Retrieval.\n\n\\bibitem{sun2022pom}\nRuinan Sun, and Yu Pang (2022). \\textit{Efficient Lung Cancer Image Classification and Segmentation Algorithm Based on Improved Swin Transformer}. arXiv.org.\n\n\\bibitem{qi2022yq9}\nZheng Qi, AprilPyone Maungmaung, Yuma Kinoshita, et al. (2022). \\textit{Privacy-Preserving Image Classification Using Vision Transformer}. European Signal Processing Conference.\n\n\\bibitem{ma2022vf3}\nXiaojian Ma, Weili Nie, Zhiding Yu, et al. (2022). \\textit{RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning}. International Conference on Learning Representations.\n\n\\bibitem{wang2022tok}\nZiyang Wang, Will Zhao, Zixuan Ni, et al. (2022). \\textit{Adversarial Vision Transformer for Medical Image Semantic Segmentation with Limited Annotations}. British Machine Vision Conference.\n\n\\bibitem{wang2022pee}\nKai Wang, Fei Yang, and Joost van de Weijer (2022). \\textit{Attention Distillation: self-supervised vision transformer students need more guidance}. British Machine Vision Conference.\n\n\\bibitem{jannat20228u6}\nFatema-E- Jannat, and A. Willis (2022). \\textit{Improving Classification of Remotely Sensed Images with the Swin Transformer}. SoutheastCon.\n\n\\bibitem{chen2022r27}\nYuzhong Chen, Zhe Xiao, Lin Zhao, et al. (2022). \\textit{Mask-guided Vision Transformer (MG-ViT) for Few-Shot Learning}. arXiv.org.\n\n\\bibitem{wang2021p2r}\nHaoran Wang, Yanju Ji, Kaiwen Song, et al. (2021). \\textit{ViT-P: Classification of Genitourinary Syndrome of Menopause From OCT Images Based on Vision Transformer Models}. IEEE Transactions on Instrumentation and Measurement.\n\n\\bibitem{sajid2021xb6}\nUsman Sajid, Xiangyu Chen, Hasan Sajid, et al. (2021). \\textit{Audio-Visual Transformer Based Crowd Counting}. 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW).\n\n\\bibitem{xing2022kqr}\nW. Xing, and K. Egiazarian (2022). \\textit{Residual Swin Transformer Channel Attention Network for Image Demosaicing}. European Workshop on Visual Information Processing.\n\n\\bibitem{garaiman2022xwd}\nA. Garaiman, F. Nooralahzadeh, C. Mihai, et al. (2022). \\textit{Vision transformer assisting rheumatologists in screening for capillaroscopy changes in systemic sclerosis: an artificial intelligence model}. Rheumatology.\n\n\\bibitem{wang2022wyu}\nZiyang Wang, Nanqing Dong, and I. Voiculescu (2022). \\textit{Computationally-Efficient Vision Transformer for Medical Image Semantic Segmentation Via Dual Pseudo-Label Supervision}. International Conference on Information Photonics.\n\n\\bibitem{hou2022ver}\nZejiang Hou, and S. Kung (2022). \\textit{Multi-Dimensional Vision Transformer Compression via Dependency Guided Gaussian Process Search}. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW).\n\n\\bibitem{agilandeeswari202273m}\nL. Agilandeeswari, and S. D. Meena (2022). \\textit{SWIN transformer based contrastive self-supervised learning for animal detection and classification}. Multimedia tools and applications.\n\n\\bibitem{qin2022cfg}\nHaonan Qin, Weiying Xie, Yunsong Li, et al. (2022). \\textit{HTD-VIT: Spectral-Spatial Joint Hyperspectral Target Detection with Vision Transformer}. IEEE International Geoscience and Remote Sensing Symposium.\n\n\\bibitem{wang2022ohd}\nBoyuan Wang (2022). \\textit{Automatic Mushroom Species Classification Model for Foodborne Disease Prevention Based on Vision Transformer}. Journal of Food Quality.\n\n\\bibitem{yu2022o30}\nHyunwoo Yu, J. Shim, Jaeho Kwak, et al. (2022). \\textit{Vision Transformer-Based Retina Vessel Segmentation with Deep Adaptive Gamma Correction}. IEEE International Conference on Acoustics, Speech, and Signal Processing.\n\n\\bibitem{boukabouya2022ffi}\nRayene Amina Boukabouya, A. Moussaoui, and Mohamed Berrimi (2022). \\textit{Vision Transformer Based Models for Plant Disease Detection and Diagnosis}. International Symposium on Information and Automation.\n\n\\bibitem{wang2022d7p}\nNan Wang, Xiangjun Meng, Xiangchao Meng, et al. (2022). \\textit{Convolution-Embedded Vision Transformer With Elastic Positional Encoding for Pansharpening}. IEEE Transactions on Geoscience and Remote Sensing.\n\n\\bibitem{song20215tk}\nJeonggeun Song (2021). \\textit{UFO-ViT: High Performance Linear Vision Transformer without Softmax}. arXiv.org.\n\n\\bibitem{xie2021th0}\nJiangtao Xie, Rui Zeng, Qilong Wang, et al. (2021). \\textit{So-ViT: Mind Visual Tokens for Vision Transformer}. arXiv.org.\n\n\\bibitem{sun2022bm5}\nYu-shan Sun, Hao Zheng, Guo-cheng Zhang, et al. (2022). \\textit{DP-ViT: A Dual-Path Vision Transformer for Real-Time Sonar Target Detection}. Remote Sensing.\n\n\\bibitem{jing2022nkb}\nYanhao Jing, and Feng Wang (2022). \\textit{TP-VIT: A Two-Pathway Vision Transformer for Video Action Recognition}. IEEE International Conference on Acoustics, Speech, and Signal Processing.\n\n\\bibitem{li2022spu}\nRui Li, Weihua Li, Yi Yang, et al. (2022). \\textit{Swinv2-Imagen: Hierarchical Vision Transformer Diffusion Models for Text-to-Image Generation}. Neural computing & applications (Print).\n\n\\bibitem{song2022y4v}\nHwanjun Song, Deqing Sun, Sanghyuk Chun, et al. (2022). \\textit{An Extendable, Efficient and Effective Transformer-based Object Detector}. arXiv.org.\n\n\\bibitem{shukla2022jxz}\nNeha Shukla, Anand Pandey, A. P. Shukla, et al. (2022). \\textit{ECG-ViT: A Transformer-Based ECG Classifier for Energy-Constraint Wearable Devices}. J. Sensors.\n\n\\bibitem{tran2022bvd}\nNguyen H. Tran, Ta Duc Huy, S. T. Duong, et al. (2022). \\textit{Improving Local Features with Relevant Spatial Information by Vision Transformer for Crowd Counting}. British Machine Vision Conference.\n\n\\bibitem{hong2022ks6}\nWeixiang Hong, Jiangwei Lao, Wang Ren, et al. (2022). \\textit{Training Object Detectors from Scratch: An Empirical Study in the Era of Vision Transformer}. Computer Vision and Pattern Recognition.\n\n\\bibitem{panboonyuen2021b4h}\nTeerapong Panboonyuen, Sittinun Thongbai, W. Wongweeranimit, et al. (2021). \\textit{Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama}. Inf..\n\n\\bibitem{zhao2022wi7}\nHong Zhao, Zhiwen Chen, Lan Guo, et al. (2022). \\textit{Video captioning based on vision transformer and reinforcement learning}. PeerJ Computer Science.\n\n\\bibitem{wang2022h3u}\nYuchen Wang, L. Qing, Zhengyong Wang, et al. (2022). \\textit{Multi-Level Transformer-Based Social Relation Recognition}. Italian National Conference on Sensors.\n\n\\bibitem{liu2021yw0}\nHao Liu, Xinghua Jiang, Xin Li, et al. (2021). \\textit{NomMer: Nominate Synergistic Context in Vision Transformer for Visual Recognition}. Computer Vision and Pattern Recognition.\n\n\\bibitem{gul202290q}\nA. Gul, Ozdemir Cetin, Christoph Reich, et al. (2022). \\textit{Histopathological image classification based on self-supervised vision transformer and weak labels}. Medical Imaging.\n\n\\bibitem{zhao2022koc}\nYoupeng Zhao, Huadong Tang, Yingying Jiang, et al. (2022). \\textit{Lightweight Vision Transformer with Cross Feature Attention}. arXiv.org.\n\n\\bibitem{yang2022qwh}\nYali Yang, Yuanping Xu, Chaolong Zhang, et al. (2022). \\textit{Hierarchical Vision Transformer with Channel Attention for RGB-D Image Segmentation}. International Symposium on Signal Processing Systems.\n\n\\bibitem{alquraishi2022j3v}\nM. S. Al-Quraishi, I. Elamvazuthi, T. Tang, et al. (2022). \\textit{Decoding the Users Movements Preparation From EEG Signals Using Vision Transformer Architecture}. IEEE Access.\n\n\\bibitem{jin2021qdw}\nWeiqiang Jin, Hang Yu, and Xiangfeng Luo (2021). \\textit{CvT-ASSD: Convolutional vision-Transformer Based Attentive Single Shot MultiBox Detector}. IEEE International Conference on Tools with Artificial Intelligence.\n\n\\bibitem{lee2022rf1}\nK. Lee, Bhavin Jawade, D. Mohan, et al. (2022). \\textit{Attribute De-biased Vision Transformer (AD-ViT) for Long-Term Person Re-identification}. Advanced Video and Signal Based Surveillance.\n\n\\bibitem{shi2022evc}\nYongtao Shi, Xiaodong Zhao, Fan Zhang, et al. (2022). \\textit{Non-Intrusive Load Monitoring Based on Swin-Transformer with Adaptive Scaling Recurrence Plot}. Energies.\n\n\\bibitem{zhang20223g5}\nHuaqi Zhang, Huang Chen, Jin Qin, et al. (2022). \\textit{MC-ViT: Multi-path cross-scale vision transformer for thymoma histopathology whole slide image typing}. Frontiers in Oncology.\n\n\\bibitem{zim202282d}\nAbid Hasan Zim, Aeyan Ashraf, Aquib Iqbal, et al. (2022). \\textit{A Vision Transformer-Based Approach to Bearing Fault Classification via Vibration Signals}. Asia-Pacific Signal and Information Processing Association Annual Summit and Conference.\n\n\\bibitem{bao202239k}\nShuai Bao, Jiping Liu, Liang Wang, et al. (2022). \\textit{Landslide Susceptibility Mapping by Fusing Convolutional Neural Networks and Vision Transformer}. Italian National Conference on Sensors.\n\n\\bibitem{sun2022nny}\nMengshu Sun, Z. Li, Alec Lu, et al. (2022). \\textit{FPGA-aware automatic acceleration framework for vision transformer with mixed-scheme quantization: late breaking results}. Design Automation Conference.\n\n\\bibitem{munyer2022pfs}\nTravis J. E. Munyer, D. Brinkman, Xin Zhong, et al. (2022). \\textit{Foreign Object Debris Detection for Airport Pavement Images Based on Self-Supervised Localization and Vision Transformer}. 2022 International Conference on Computational Science and Computational Intelligence (CSCI).\n\n\\bibitem{fan2022wve}\nHong-wei Fan, Ningge Ma, Xu-hui Zhang, et al. (2022). \\textit{New intelligent fault diagnosis approach of rolling bearing based on improved vibration gray texture image and vision transformer}. Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science.\n\n\\bibitem{wang2022gq4}\nYi Wang, Zhiwen Fan, Tianlong Chen, et al. (2022). \\textit{Can We Solve 3D Vision Tasks Starting from A 2D Vision Transformer?}. arXiv.org.\n\n\\bibitem{ali2022dux}\nLuqman Ali, Hamad Al Jassmi, Wasif Khan, et al. (2022). \\textit{Crack45K: Integration of Vision Transformer with Tubularity Flow Field (TuFF) and Sliding-Window Approach for Crack-Segmentation in Pavement Structures}. Buildings.\n\n\\bibitem{chougui2022mpo}\nAbdeldjalil Chougui, Achraf Moussaoui, and A. Moussaoui (2022). \\textit{Plant-Leaf Diseases Classification using CNN, CBAM and Vision Transformer}. International Symposium on Information and Automation.\n\n\\bibitem{zhuang2021hqu}\nLi Zhuang (2021). \\textit{Deep-Learning-Based Diagnosis of Cassava Leaf Diseases Using Vision Transformer}. Artificial Intelligence and Cloud Computing Conference.\n\n\\bibitem{chen2021d1q}\nXiaoyue Chen, Sei-ichiro Kamata, and Weilian Zhou (2021). \\textit{Hyperspectral Image Classification Based on Multi-stage Vision Transformer with Stacked Samples}. IEEE Region 10 Conference.\n\n\\bibitem{hatamizadeh2024xr6}\nAli Hatamizadeh, and Jan Kautz (2024). \\textit{MambaVision: A Hybrid Mamba-Transformer Vision Backbone}. Computer Vision and Pattern Recognition.\n\n\\bibitem{ryali202339q}\nChaitanya K. Ryali, Yuan-Ting Hu, Daniel Bolya, et al. (2023). \\textit{Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles}. International Conference on Machine Learning.\n\n\\bibitem{yao2023sax}\nJing Yao, Bing Zhang, Chenyu Li, et al. (2023). \\textit{Extended Vision Transformer (ExViT) for Land Use and Land Cover Classification: A Multimodal Deep Learning Framework}. IEEE Transactions on Geoscience and Remote Sensing.\n\n\\bibitem{li2023287}\nXiangtai Li, Henghui Ding, Wenwei Zhang, et al. (2023). \\textit{Transformer-Based Visual Segmentation: A Survey}. IEEE Transactions on Pattern Analysis and Machine Intelligence.\n\n\\bibitem{zhao2024671}\nZhuoyi Zhao, Xiang Xu, Shutao Li, et al. (2024). \\textit{Hyperspectral Image Classification Using Groupwise Separable Convolutional Vision Transformer Network}. IEEE Transactions on Geoscience and Remote Sensing.\n\n\\bibitem{dehghani2023u7e}\nMostafa Dehghani, Basil Mustafa, J. Djolonga, et al. (2023). \\textit{Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution}. Neural Information Processing Systems.\n\n\\bibitem{duan2024q7h}\nYuchen Duan, Weiyun Wang, Zhe Chen, et al. (2024). \\textit{Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures}. International Conference on Learning Representations.\n\n\\bibitem{barman2024q21}\nUtpal Barman, Parismita Sarma, Mirzanur Rahman, et al. (2024). \\textit{ViT-SmartAgri: Vision Transformer and Smartphone-Based Plant Disease Detection for Smart Agriculture}. Agronomy.\n\n\\bibitem{jamil20230ll}\nSonain Jamil, and Anisha Roy (2023). \\textit{An efficient and robust Phonocardiography (PCG)-based Valvular Heart Diseases (VHD) detection framework using Vision Transformer (ViT)}. Comput. Biol. Medicine.\n\n\\bibitem{paal2024eg1}\nIshak Paal, Melek Alaftekin, and F. Zengul (2024). \\textit{Enhancing Skin Cancer Diagnosis Using Swin Transformer with Hybrid Shifted Window-Based Multi-head Self-attention and SwiGLU-Based MLP}. Journal of imaging informatics in medicine.\n\n\\bibitem{zhang2023k43}\nXiaosong Zhang, Yunjie Tian, Lingxi Xie, et al. (2023). \\textit{HiViT: A Simpler and More Efficient Design of Hierarchical Vision Transformer}. International Conference on Learning Representations.\n\n\\bibitem{himel2024u0i}\nGalib Muhammad Shahriar Himel, Md. Masudul Islam, Kh Abdullah Al-Aff, et al. (2024). \\textit{Skin Cancer Segmentation and Classification Using Vision Transformer for Automatic Analysis in Dermatoscopy-Based Noninvasive Digital System}. International Journal of Biomedical Imaging.\n\n\\bibitem{xu20235cu}\nQin Xu, Jiahui Wang, Bo Jiang, et al. (2023). \\textit{Fine-Grained Visual Classification via Internal Ensemble Learning Transformer}. IEEE transactions on multimedia.\n\n\\bibitem{chi202331y}\nKaichen Chi, Yuan Yuan, and Qi Wang (2023). \\textit{Trinity-Net: Gradient-Guided Swin Transformer-Based Remote Sensing Image Dehazing and Beyond}. IEEE Transactions on Geoscience and Remote Sensing.\n\n\\bibitem{patro202303d}\nBadri N. Patro, Vinay P. Namboodiri, and Vijay Srinivas Agneeswaran (2023). \\textit{SpectFormer: Frequency and Attention is what you need in a Vision Transformer}. IEEE Workshop/Winter Conference on Applications of Computer Vision.\n\n\\bibitem{pan2023hry}\nXuran Pan, Tianzhu Ye, Zhuofan Xia, et al. (2023). \\textit{Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention}. Computer Vision and Pattern Recognition.\n\n\\bibitem{wang2024mrk}\nZiyang Wang, and Chao Ma (2024). \\textit{Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for Scribble-based Medical Image Segmentation}. arXiv.org.\n\n\\bibitem{tabbakh2023ao7}\nA. Tabbakh, and Soubhagya Sankar Barpanda (2023). \\textit{A Deep Features Extraction Model Based on the Transfer Learning Model and Vision Transformer TLMViT for Plant Disease Classification}. IEEE Access.\n\n\\bibitem{dutta2023aet}\nPramit Dutta, Khaleda Akhter Sathi, Md.Azad Hossain, et al. (2023). \\textit{Conv-ViT: A Convolution and Vision Transformer-Based Hybrid Feature Extraction Method for Retinal Disease Detection}. Journal of Imaging.\n\n\\bibitem{qiu2024eh4}\nYuhang Qiu, Honghui Chen, Xingbo Dong, et al. (2024). \\textit{IFViT: Interpretable Fixed-Length Representation for Fingerprint Matching via Vision Transformer}. IEEE Transactions on Information Forensics and Security.\n\n\\bibitem{li2023nnd}\nGuoqiang Li, Yuchao Wang, Qing Zhao, et al. (2023). \\textit{PMVT: a lightweight vision transformer for plant disease identification on mobile devices}. Frontiers in Plant Science.\n\n\\bibitem{zhao20243f3}\nHu Zhao, Keyan Ren, Tianyi Yue, et al. (2024). \\textit{TransFG: A Cross-View Geo-Localization of Satellite and UAVs Imagery Pipeline Using Transformer-Based Feature Aggregation and Gradient Guidance}. IEEE Transactions on Geoscience and Remote Sensing.\n\n\\bibitem{song2024fx9}\nHuaxiang Song, Yuxuan Yuan, Zhiwei Ouyang, et al. (2024). \\textit{Quantitative regularization in robust vision transformer for remote sensing image classification}. Photogrammetric Record.\n\n\\bibitem{cai2023hji}\nYimin Cai, Yuqing Long, Zhenggong Han, et al. (2023). \\textit{Swin Unet3D: a three-dimensional medical image segmentation network combining vision transformer and convolution}. BMC Medical Informatics and Decision Making.\n\n\\bibitem{akinpelu2024d4m}\nS. Akinpelu, Serestina Viriri, and A. Adegun (2024). \\textit{An enhanced speech emotion recognition using vision transformer}. Scientific Reports.\n\n\\bibitem{hayat2024e4f}\nMansoor Hayat, Nouman Ahmad, Anam Nasir, et al. (2024). \\textit{Hybrid Deep Learning EfficientNetV2 and Vision Transformer (EffNetV2-ViT) Model for Breast Cancer Histopathological Image Classification}. IEEE Access.\n\n\\bibitem{li2024g3z}\nYongxin Li, Mengyuan Liu, You Wu, et al. (2024). \\textit{Learning Adaptive and View-Invariant Vision Transformer for Real-Time UAV Tracking}. International Conference on Machine Learning.\n\n\\bibitem{arshed2023zen}\nMuhammad Asad Arshed, Shahzad Mumtaz, Muhammad Ibrahim, et al. (2023). \\textit{Multi-Class Skin Cancer Classification Using Vision Transformer Networks and Convolutional Neural Network-Based Pre-Trained Models}. Inf..\n\n\\bibitem{qin20242eu}\nS. Qin, Taiyue Qi, Tang Deng, et al. (2024). \\textit{Image segmentation using Vision Transformer for tunnel defect assessment}. Comput. Aided Civ. Infrastructure Eng..\n\n\\bibitem{lee2023iwc}\nC. Lee, K. Lim, Yu Xuan Song, et al. (2023). \\textit{Plant-CNN-ViT: Plant Classification with Ensemble of Convolutional Neural Networks and Vision Transformer}. Plants.\n\n\\bibitem{tagnamas20246ug}\nJaouad Tagnamas, Hiba Ramadan, Ali Yahyaouy, et al. (2024). \\textit{Multi-task approach based on combined CNN-transformer for efficient segmentation and classification of breast tumors in ultrasound images}. Visual Computing for Industry, Biomedicine, and Art.\n\n\\bibitem{li2023jft}\nShuiwang Li, Yangxiang Yang, Dan Zeng, et al. (2023). \\textit{Adaptive and Background-Aware Vision Transformer for Real-Time UAV Tracking}. IEEE International Conference on Computer Vision.\n\n\\bibitem{song2024c99}\nBofan Song, D. Kc, Rubin Yuchan Yang, et al. (2024). \\textit{Classification of Mobile-Based Oral Cancer Images Using the Vision Transformer and the Swin Transformer}. Cancers.\n\n\\bibitem{aksoy20240c0}\nSerra Aksoy, P. Demirciolu, and I. Bogrekci (2024). \\textit{Enhancing Melanoma Diagnosis with Advanced Deep Learning Models Focusing on Vision Transformer, Swin Transformer, and ConvNeXt}. Dermatopathology.\n\n\\bibitem{leem2024j4t}\nSaebom Leem, and Hyunseok Seo (2024). \\textit{Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{chen2024asi}\nShiming Chen, W. Hou, Salman H. Khan, et al. (2024). \\textit{Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning}. Computer Vision and Pattern Recognition.\n\n\\bibitem{lin202343q}\nFudong Lin, Summer Crawford, Kaleb Guillot, et al. (2023). \\textit{MMST-ViT: Climate Change-aware Crop Yield Prediction via Multi-Modal Spatial-Temporal Vision Transformer}. IEEE International Conference on Computer Vision.\n\n\\bibitem{ghahremani202491m}\nMorteza Ghahremani, Mohammad Khateri, Bailiang Jian, et al. (2024). \\textit{H-ViT: A Hierarchical Vision Transformer for Deformable Image Registration}. Computer Vision and Pattern Recognition.\n\n\\bibitem{wang20249qa}\nHaiyang Wang, Hao Tang, Li Jiang, et al. (2024). \\textit{GiT: Towards Generalist Vision Transformer through Universal Language Interface}. European Conference on Computer Vision.\n\n\\bibitem{shahin2024g0q}\nMohammad Shahin, F. F. Chen, Mazdak Maghanaki, et al. (2024). \\textit{Improving the Concrete Crack Detection Process via a Hybrid Visual Transformer Algorithm}. Italian National Conference on Sensors.\n\n\\bibitem{zhu2023dpi}\nLiang Zhu, Yingyue Li, Jiemin Fang, et al. (2023). \\textit{WeakTr: Exploring Plain Vision Transformer for Weakly-supervised Semantic Segmentation}. arXiv.org.\n\n\\bibitem{yu2023l1g}\nZitong Yu, Rizhao Cai, Yawen Cui, et al. (2023). \\textit{Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing}. International Journal of Computer Vision.\n\n\\bibitem{ko2024eax}\nJinsol Ko, Soyeon Park, and H. G. Woo (2024). \\textit{Optimization of vision transformer-based detection of lung diseases from chest X-ray images}. BMC Medical Informatics Decis. Mak..\n\n\\bibitem{yang2024w08}\nQiying Yang, and Rongzuo Guo (2024). \\textit{An Unsupervised Method for Industrial Image Anomaly Detection with Vision Transformer-Based Autoencoder}. Italian National Conference on Sensors.\n\n\\bibitem{nazih20238nf}\nWaleed Nazih, Ahmad O. Aseeri, Osama Youssef Atallah, et al. (2023). \\textit{Vision Transformer Model for Predicting the Severity of Diabetic Retinopathy in Fundus Photography-Based Retina Images}. IEEE Access.\n\n\\bibitem{xia2023bp7}\nZhuofan Xia, Xuran Pan, Shiji Song, et al. (2023). \\textit{DAT++: Spatially Dynamic Vision Transformer with Deformable Attention}. arXiv.org.\n\n\\bibitem{nag2023cfn}\nShashank Nag, G. Datta, Souvik Kundu, et al. (2023). \\textit{ViTA: A Vision Transformer Inference Accelerator for Edge Applications}. International Symposium on Circuits and Systems.\n\n\\bibitem{gezici20246lf}\nAbdul Haluk Batur Gezici, and Emre Sefer (2024). \\textit{Deep Transformer-Based Asset Price and Direction Prediction}. IEEE Access.\n\n\\bibitem{ghazouani202342t}\nFethi Ghazouani, Pierre Vera, and Su Ruan (2023). \\textit{Efficient brain tumor segmentation using Swin transformer and enhanced local self-attention}. International Journal of Computer Assisted Radiology and Surgery.\n\n\\bibitem{wang202338i}\nGuanqun Wang, He Chen, Liang Chen, et al. (2023). \\textit{P2FEViT: Plug-and-Play CNN Feature Embedded Hybrid Vision Transformer for Remote Sensing Image Classification}. Remote Sensing.\n\n\\bibitem{guo2024tr7}\nYu Guo, Zhi Zhang, and Yuzhen Huang (2024). \\textit{Dual Class Token Vision Transformer for Direction of Arrival Estimation in Low SNR}. IEEE Signal Processing Letters.\n\n\\bibitem{wang2023ski}\nWei Wang, Xin Yang, and Jinhui Tang (2023). \\textit{Vision Transformer With Hybrid Shifted Windows for Gastrointestinal Endoscopy Image Classification}. IEEE transactions on circuits and systems for video technology (Print).\n\n\\bibitem{zheng202325h}\nFujian Zheng, Shuai Lin, Wei Zhou, et al. (2023). \\textit{A Lightweight Dual-Branch Swin Transformer for Remote Sensing Scene Classification}. Remote Sensing.\n\n\\bibitem{mogan2023ywz}\nJashila Nair Mogan, C. Lee, K. Lim, et al. (2023). \\textit{Gait-CNN-ViT: Multi-Model Gait Recognition with Convolutional Neural Networks and Vision Transformer}. Italian National Conference on Sensors.\n\n\\bibitem{ebert202377v}\nNikolas Ebert, D. Stricker, and Oliver Wasenmller (2023). \\textit{PLG-ViT: Vision Transformer with Parallel Local and Global Self-Attention}. Italian National Conference on Sensors.\n\n\\bibitem{wang20245bq}\nYong Wang, Cheng Lu, Hailun Lian, et al. (2024). \\textit{Speech Swin-Transformer: Exploring a Hierarchical Transformer with Shifted Windows for Speech Emotion Recognition}. IEEE International Conference on Acoustics, Speech, and Signal Processing.\n\n\\bibitem{cao20241ng}\nJie Cao, Tingting Xu, Yu-he Deng, et al. (2024). \\textit{Galaxy morphology classification based on Convolutional vision Transformer (CvT)}. Astronomy &amp; Astrophysics.\n\n\\bibitem{yang2024in8}\nYaoming Yang, Zhili Cai, Shuxia Qiu, et al. (2024). \\textit{Vision transformer with masked autoencoders for referable diabetic retinopathy classification based on large-size retina image}. PLoS ONE.\n\n\\bibitem{hussain2025qoe}\nTahir Hussain, Hayaru Shouno, Abid Hussain, et al. (2025). \\textit{EFFResNet-ViT: A Fusion-Based Convolutional and Vision Transformer Model for Explainable Medical Image Classification}. IEEE Access.\n\n\\bibitem{shim2023z7g}\nD. Shim, and H. J. Kim (2023). \\textit{SwinDepth: Unsupervised Depth Estimation using Monocular Sequences via Swin Transformer and Densely Cascaded Network}. IEEE International Conference on Robotics and Automation.\n\n\\bibitem{alam2024t09}\nTaukir Alam, Wei-Cheng Yeh, Fang Rong Hsu, et al. (2024). \\textit{An Integrated Approach using YOLOv8 and ResNet, SeResNet & Vision Transformer (ViT) Algorithms based on ROI Fracture Prediction in X-ray Images of the Elbow.}. Current medical imaging.\n\n\\bibitem{yang2024tti}\nRuiping Yang, Liu Kun, Shaohua Xu, et al. (2024). \\textit{ViT-UperNet: a hybrid vision transformer with unified-perceptual-parsing network for medical image segmentation}. Complex &amp; Intelligent Systems.\n\n\\bibitem{wang20245hx}\nDong Wang, Jian Lian, and Wanzhen Jiao (2024). \\textit{Multi-label classification of retinal disease via a novel vision transformer model}. Frontiers in Neuroscience.\n\n\\bibitem{song202479c}\nHuaxiang Song, Hanjun Xia, Wenhui Wang, et al. (2024). \\textit{QAGA-Net: enhanced vision transformer-based object detection for remote sensing images}. International Journal of Intelligent Computing and Cybernetics.\n\n\\bibitem{li2023lvd}\nXiaoye Li, and Bin-Bin Zhang (2023). \\textit{FV-ViT: Vision Transformer for Finger Vein Recognition}. IEEE Access.\n\n\\bibitem{ma2023vhi}\nXiaochen Ma, Bo Du, Xianggen Liu, et al. (2023). \\textit{IML-ViT: Image Manipulation Localization by Vision Transformer}. arXiv.org.\n\n\\bibitem{han202416k}\nHuiyan Han, H. Zeng, Liqun Kuang, et al. (2024). \\textit{A human activity recognition method based on Vision Transformer}. Scientific Reports.\n\n\\bibitem{katar202352u}\nOuzhan Katar, and Ozal Yildirim (2023). \\textit{An Explainable Vision Transformer Model Based White Blood Cells Classification and Localization}. Diagnostics.\n\n\\bibitem{hemalatha2024a14}\nS. Hemalatha, and Jayachandiran Jai Jaganath Babu (2024). \\textit{A Multitask Learning-Based Vision Transformer for Plant Disease Localization and Classification}. International Journal of Computational Intelligence Systems.\n\n\\bibitem{ma2024uan}\nChiyu Ma, Jon Donnelly, Wenjun Liu, et al. (2024). \\textit{Interpretable Image Classification with Adaptive Prototype-based Vision Transformers}. Neural Information Processing Systems.\n\n\\bibitem{lai20238ck}\nD. K. Lai, Zi-Han Yu, Tommy Yau-Nam Leung, et al. (2023). \\textit{Vision Transformers (ViT) for Blanket-Penetrating Sleep Posture Recognition Using a Triple Ultra-Wideband (UWB) Radar System}. Italian National Conference on Sensors.\n\n\\bibitem{wang2024luv}\nZhikan Wang, Zhongyao Cheng, Jiajie Xiong, et al. (2024). \\textit{A Timely Survey on Vision Transformer for Deepfake Detection}. arXiv.org.\n\n\\bibitem{ling2023x36}\nZhixin Ling, Zhen Xing, Xiangdong Zhou, et al. (2023). \\textit{PanoSwin: a Pano-style Swin Transformer for Panorama Understanding}. Computer Vision and Pattern Recognition.\n\n\\bibitem{wang2023bfo}\nZhifeng Wang, Jialong Yao, Chunyan Zeng, et al. (2023). \\textit{Students' Classroom Behavior Detection System Incorporating Deformable DETR with Swin Transformer and Light-Weight Feature Pyramid Network}. Syst..\n\n\\bibitem{yin2023029}\nMiao Yin, Burak Uzkent, Yilin Shen, et al. (2023). \\textit{GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous Structured Pruning for Vision Transformer}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{mishra2024fbz}\nSwapneel Mishra, Saumya Seth, Shrishti Jain, et al. (2024). \\textit{Image Caption Generation using Vision Transformer and GPT Architecture}. 2024 2nd International Conference on Advancement in Computation & Computer Technologies (InCACCT).\n\n\\bibitem{heidari2024d9k}\nMoein Heidari, Reza Azad, Sina Ghorbani Kolahi, et al. (2024). \\textit{Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights}. arXiv.org.\n\n\\bibitem{yu2023fqo}\nSheng Yu, Dihua Zhai, and Yuanqing Xia (2023). \\textit{A Novel Robotic Pushing and Grasping Method Based on Vision Transformer and Convolution}. IEEE Transactions on Neural Networks and Learning Systems.\n\n\\bibitem{zhao2023pau}\nQihao Zhao, Yangyu Huang, Wei Hu, et al. (2023). \\textit{MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer}. International Conference on Learning Representations.\n\n\\bibitem{pan20249k5}\nC. Pan, Junran Peng, and Zhaoxiang Zhang (2024). \\textit{Depth-Guided Vision Transformer With Normalizing Flows for Monocular 3D Object Detection}. IEEE/CAA Journal of Automatica Sinica.\n\n\\bibitem{huan202345b}\nSha Huan, Zhaoyue Wang, Xiaoqiang Wang, et al. (2023). \\textit{A lightweight hybrid vision transformer network for radar-based human activity recognition}. Scientific Reports.\n\n\\bibitem{belal2023x1u}\nMohamad Mulham Belal, and Dr. Divya Meena Sundaram (2023). \\textit{Global-Local Attention-Based Butterfly Vision Transformer for Visualization-Based Malware Classification}. IEEE Access.\n\n\\bibitem{li20238ti}\nYanjing Li, Sheng Xu, Mingbao Lin, et al. (2023). \\textit{Bi-ViT: Pushing the Limit of Vision Transformer Quantization}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{huo2023e5h}\nYingzi Huo, Kai Jin, Jiahong Cai, et al. (2023). \\textit{Vision Transformer (ViT)-based Applications in Image Classification}. 2023 IEEE 9th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS).\n\n\\bibitem{kim2023cvz}\nJiseob Kim, Kyuhong Shim, Junhan Kim, et al. (2023). \\textit{Vision Transformer-Based Feature Extraction for Generalized Zero-Shot Learning}. IEEE International Conference on Acoustics, Speech, and Signal Processing.\n\n\\bibitem{fan2023whi}\nChunyu Fan, Q. Su, Zhifeng Xiao, et al. (2023). \\textit{ViT-FRD: A Vision Transformer Model for Cardiac MRI Image Segmentation Based on Feature Recombination Distillation}. IEEE Access.\n\n\\bibitem{zhao2023rle}\nKai Zhao, Ruitao Lu, Siyu Wang, et al. (2023). \\textit{ST-YOLOA: a Swin-transformer-based YOLO model with an attention mechanism for SAR ship detection under complex background}. Frontiers in Neurorobotics.\n\n\\bibitem{xie20234ve}\nTao Xie, Kun Dai, Zhiqiang Jiang, et al. (2023). \\textit{ViT-MVT: A Unified Vision Transformer Network for Multiple Vision Tasks}. IEEE Transactions on Neural Networks and Learning Systems.\n\n\\bibitem{li20233lv}\nGary Y. Li, Junyu Chen, Se-In Jang, et al. (2023). \\textit{SwinCross: Cross-modal Swin Transformer for Head-and-Neck Tumor Segmentation in PET/CT Images}. Medical Physics (Lancaster).\n\n\\bibitem{ma2023qek}\nXiaochen Ma, Bo Du, Zhuohang Jiang, et al. (2023). \\textit{IML-ViT: Benchmarking Image Manipulation Localization by Vision Transformer}. Unpublished manuscript.\n\n\\bibitem{tanimola20246cv}\nOluwatosin Tanimola, Olamilekan Shobayo, O. Popoola, et al. (2024). \\textit{Breast Cancer Classification Using Fine-Tuned SWIN Transformer Model on Mammographic Images}. Analytics.\n\n\\bibitem{chen2023xxw}\nTiansheng Chen, and L. Mo (2023). \\textit{Swin-Fusion: Swin-Transformer with Feature Fusion for Human Action Recognition}. Neural Processing Letters.\n\n\\bibitem{ranjan20243bn}\nNavin Ranjan, and Andreas E. Savakis (2024). \\textit{LRP-QViT: Mixed-Precision Vision Transformer Quantization via Layer-wise Relevance Propagation}. arXiv.org.\n\n\\bibitem{fu20232q3}\nXiangqu Fu, Qirui Ren, Hao Wu, et al. (2023). \\textit{P3 ViT: A CIM-Based High-Utilization Architecture With Dynamic Pruning and Two-Way Ping-Pong Macro for Vision Transformer}. IEEE Transactions on Circuits and Systems Part 1: Regular Papers.\n\n\\bibitem{shi20235zy}\nChaojun Shi, Shiwei Zhao, Kecheng Zhang, et al. (2023). \\textit{Face-based age estimation using improved Swin Transformer with attention-based convolution}. Frontiers in Neuroscience.\n\n\\bibitem{deressa2023lrl}\nDeressa Wodajo Deressa, Hannes Mareen, Peter Lambert, et al. (2023). \\textit{GenConViT: Deepfake Video Detection Using Generative Convolutional Vision Transformer}. Applied Sciences.\n\n\\bibitem{aburass2023qpf}\nSanad Aburass, and O. Dorgham (2023). \\textit{Performance Evaluation of Swin Vision Transformer Model using Gradient Accumulation Optimization Technique}. arXiv.org.\n\n\\bibitem{hassija2025wq3}\nVikas Hassija, Balamurugan Palanisamy, Arpita Chatterjee, et al. (2025). \\textit{Transformers for Vision: A Survey on Innovative Methods for Computer Vision}. IEEE Access.\n\n\\bibitem{huang20238er}\nYihang Huang, and Wan Li (2023). \\textit{Resizer Swin Transformer-Based Classification Using sMRI for Alzheimers Disease}. Applied Sciences.\n\n\\bibitem{liu20230kl}\nJianwei Liu, Shirui Lyu, Denis Hadjivelichkov, et al. (2023). \\textit{ViT-A*: Legged Robot Path Planning using Vision Transformer A*}. IEEE-RAS International Conference on Humanoid Robots.\n\n\\bibitem{he20238sy}\nRu He, Xiaomin Wang, Huazhen Chen, et al. (2023). \\textit{VHR-BirdPose: Vision Transformer-Based HRNet for Bird Pose Estimation with Attention Mechanism}. Electronics.\n\n\\bibitem{guo2023dpo}\nYangyang Guo, Wenhao Hong, Jiaxin Wu, et al. (2023). \\textit{Vision-Based Cow Tracking and Feeding Monitoring for Autonomous Livestock Farming: The YOLOv5s-CA+DeepSORT-Vision Transformer}. IEEE robotics & automation magazine.\n\n\\bibitem{wang2023j6b}\nYun Wang, Shuai Shi, and Jie Chen (2023). \\textit{Efficient Blind Hyperspectral Unmixing with Non-Local Spatial Information Based on Swin Transformer}. IEEE International Geoscience and Remote Sensing Symposium.\n\n\\bibitem{gopal20237ol}\nGoutam Yelluru Gopal, and Maria A. Amer (2023). \\textit{Mobile Vision Transformer-based Visual Object Tracking}. British Machine Vision Conference.\n\n\\bibitem{liu2023awp}\nZhiyang Liu, Pengyu Yin, and Zhenhua Ren (2023). \\textit{An Efficient FPGA-Based Accelerator for Swin Transformer}. arXiv.org.\n\n\\bibitem{fu20228zq}\nZujun Fu (2022). \\textit{Vision Transformer: Vit and its Derivatives}. arXiv.org.\n\n\\bibitem{sahoo20223yl}\nP. Sahoo, S. Saha, S. Mondal, et al. (2022). \\textit{Vision Transformer Based COVID-19 Detection Using Chest CT-scan images}. 2022 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI).\n\n\\bibitem{ganz20249zr}\nRoy Ganz, Yair Kittenplon, Aviad Aberdam, et al. (2024). \\textit{Question Aware Vision Transformer for Multimodal Reasoning}. Computer Vision and Pattern Recognition.\n\n\\bibitem{paal2024no4}\nIshak Paal, and Ismail Kunduracioglu (2024). \\textit{Data-Efficient Vision Transformer Models for Robust Classification of Sugarcane}. Journal of Soft Computing and Decision Analytics.\n\n\\bibitem{hassan20243qi}\nNada M. Hassan, Safwat Hamad, and Khaled Mahar (2024). \\textit{YOLO-based CAD framework with ViT transformer for breast mass detection and classification in CESM and FFDM images}. Neural computing & applications (Print).\n\n\\bibitem{k2024wyx}\nAbinaya K, and S. B (2024). \\textit{A Deep Learning-Based Approach for Cervical Cancer Classification Using 3D CNN and Vision Transformer.}. Journal of imaging informatics in medicine.\n\n\\bibitem{nguyen2024id9}\nXuan-Bac Nguyen, Hoang-Quan Nguyen, Samuel Yen-Chi Chen, et al. (2024). \\textit{QClusformer: A Quantum Transformer-based Framework for Unsupervised Visual Clustering}. International Conference on Quantum Computing and Engineering.\n\n\\bibitem{almohimeed2024jq1}\nAbdulaziz Almohimeed, Mohamed Shehata, Nora El-Rashidy, et al. (2024). \\textit{ViT-PSO-SVM: Cervical Cancer Predication Based on Integrating Vision Transformer with Particle Swarm Optimization and Support Vector Machine}. Bioengineering.\n\n\\bibitem{hao202488z}\nChao Hao, Zitong Yu, Xin Liu, et al. (2024). \\textit{A Simple Yet Effective Network Based on Vision Transformer for Camouflaged Object and Salient Object Detection}. IEEE Transactions on Image Processing.\n\n\\bibitem{yao20244li}\nHaiming Yao, Wei Luo, Jianan Lou, et al. (2024). \\textit{Scalable Industrial Visual Anomaly Detection With Partial Semantics Aggregation Vision Transformer}. IEEE Transactions on Instrumentation and Measurement.\n\n\\bibitem{dong20245zz}\nPeiyan Dong, Jinming Zhuang, Zhuoping Yang, et al. (2024). \\textit{EQ-ViT: Algorithm-Hardware Co-Design for End-to-End Acceleration of Real-Time Vision Transformer Inference on Versal ACAP Architecture}. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems.\n\n\\bibitem{zhang2024jha}\nHaoyu Zhang, Raghavendra Ramachandra, Kiran B. Raja, et al. (2024). \\textit{Generalized Single-Image-Based Morphing Attack Detection Using Deep Representations from Vision Transformer}. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW).\n\n\\bibitem{boukhari2024gbb}\nD. E. Boukhari (2024). \\textit{Facial Beauty Prediction Based on Vision Transformer}. International Journal of Electrical and Electronic Engineering &amp; Telecommunications.\n\n\\bibitem{song2025idg}\nHuaxiang Song, Junping Xie, Yunyang Wang, et al. (2025). \\textit{Optimized Data Distribution Learning for Enhancing Vision TransformerBased Object Detection in Remote Sensing Images}. Photogrammetric Record.\n\n\\bibitem{zhou2024tps}\nHeng Zhou, Jingmin Yang, Shanghui Deng, et al. (2024). \\textit{VTIL: A multi-layer indoor location algorithm for RSSI images based on vision transformer}. Engineering Research Express.\n\n\\bibitem{abbaoui20244wy}\nWafae Abbaoui, Sara Retal, Soumia Ziti, et al. (2024). \\textit{Automated Ischemic Stroke Classification from MRI Scans: Using a Vision Transformer Approach}. Journal of Clinical Medicine.\n\n\\bibitem{yang2024nyx}\nXiangyang Yang, Dan Zeng, Xucheng Wang, et al. (2024). \\textit{Adaptively Bypassing Vision Transformer Blocks for Efficient Visual Tracking}. Pattern Recognition.\n\n\\bibitem{yang20241kf}\nZhiding Yang, and Weimin Huang (2024). \\textit{SWHFormer: A Vision Transformer for Significant Wave Height Estimation From Nautical Radar Images}. IEEE Transactions on Geoscience and Remote Sensing.\n\n\\bibitem{hu202434n}\nYoubing Hu, Yun Cheng, Anqi Lu, et al. (2024). \\textit{LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{yang20244dq}\nBin Yang, Binghan Zhang, Yilong Han, et al. (2024). \\textit{Vision transformer-based visual language understanding of the construction process}. Alexandria Engineering Journal.\n\n\\bibitem{keresh20249rl}\nArman Keresh, and Pakizar Shamoi (2024). \\textit{Liveness Detection in Computer Vision: Transformer-Based Self-Supervised Learning for Face Anti-Spoofing}. IEEE Access.\n\n\\bibitem{hu20247km}\nYuxin Hu, Han Zhou, Ning Cao, et al. (2024). \\textit{Synthetic CT generation based on CBCT using improved vision transformer CycleGAN}. Scientific Reports.\n\n\\bibitem{alsulami2024ffb}\nAbdulkream A Alsulami, Aishah Albarakati, A. A. Al-Ghamdi, et al. (2024). \\textit{Identification of Anomalies in Lung and Colon Cancer Using Computer Vision-Based Swin Transformer with Ensemble Model on Histopathological Images}. Bioengineering.\n\n\\bibitem{yang2024wxl}\nLu Yang, Songtao Guo, Defang Liu, et al. (2024). \\textit{ConViTML: A Convolutional Vision Transformer-Based Meta-Learning Framework for Real-Time Edge Network Traffic Classification}. IEEE Transactions on Network and Service Management.\n\n\\bibitem{p2024nbn}\nVenkatasaichandrakanth P, and I. M (2024). \\textit{GNViT- An enhanced image-based groundnut pest classification using Vision Transformer (ViT) model}. PLoS ONE.\n\n\\bibitem{wu2024tsm}\nXinhao Wu, Sirui Xu, Ming-Yu Gao, et al. (2024). \\textit{A new ECT image reconstruction algorithm based on Vision transformer (ViT)}. Flow Measurement and Instrumentation.\n\n\\bibitem{dong2024bm2}\nQiwei Dong, Xiaoru Xie, and Zhongfeng Wang (2024). \\textit{SWAT: An Efficient Swin Transformer Accelerator Based on FPGA}. Asia and South Pacific Design Automation Conference.\n\n\\bibitem{swapno2025y2b}\nS. M. M. Swapno, S. N. Nobel, Md Babul Islam, et al. (2025). \\textit{ViT-SENet-Tom: machine learning-based novel hybrid squeeze-excitation network and vision transformer framework for tomato fruits classification}. Neural computing & applications (Print).\n\n\\bibitem{yoo2024u1f}\nDayeon Yoo, Jeesu Kim, and Jinwoo Yoo (2024). \\textit{FSwin Transformer: Feature-Space Window Attention Vision Transformer for Image Classification}. IEEE Access.\n\n\\bibitem{he2024m6j}\nKan He, Wei Zhang, Xuejun Zong, et al. (2024). \\textit{Network Intrusion Detection Based on Feature Image and Deformable Vision Transformer Classification}. IEEE Access.\n\n\\bibitem{zhang202489a}\nZichen Zhang, Jing Li, C. Cai, et al. (2024). \\textit{Bearing Fault Diagnosis Based on Image Information Fusion and Vision Transformer Transfer Learning Model}. Applied Sciences.\n\n\\bibitem{zhang2024pd6}\nYueqi Zhang, Lichen Feng, Hongwei Shan, et al. (2024). \\textit{A 109-GOPs/W FPGA-Based Vision Transformer Accelerator With Weight-Loop Dataflow Featuring Data Reusing and Resource Saving}. IEEE transactions on circuits and systems for video technology (Print).\n\n\\bibitem{dong20242ow}\nXinlong Dong, Peicheng Shi, Yueyue Tang, et al. (2024). \\textit{Vehicle Classification Algorithm Based on Improved Vision Transformer}. World Electric Vehicle Journal.\n\n\\bibitem{kayacan2024yy7}\nYavuz Emre Kayacan, and I. Erer (2024). \\textit{A Vision-Transformer-Based Approach to Clutter Removal in GPR: DC-ViT}. IEEE Geoscience and Remote Sensing Letters.\n\n\\bibitem{liu20248jh}\nJintao Liu, Alfredo Toln Becerra, Jos Fernando Bienvenido-Barcena, et al. (2024). \\textit{CFFI-Vit: Enhanced Vision Transformer for the Accurate Classification of Fish Feeding Intensity in Aquaculture}. Journal of Marine Science and Engineering.\n\n\\bibitem{shi2024r44}\nHuihong Shi, Xin Cheng, Wendong Mao, et al. (2024). \\textit{P2-ViT: Power-of-Two Post-Training Quantization and Acceleration for Fully Quantized Vision Transformer}. IEEE Transactions on Very Large Scale Integration (VLSI) Systems.\n\n\\bibitem{xin2024ljt}\nXinyue Xin, Ming Li, Yan Wu, et al. (2024). \\textit{PolSAR-MPIformer: A Vision Transformer Based on Mixed Patch Interaction for Dual-Frequency PolSAR Image Adaptive Fusion Classification}. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing.\n\n\\bibitem{zhou2024qty}\nJian Zhou, Guochuan Zhao, and Yonglong Li (2024). \\textit{Vison Transformer-Based Automatic Crack Detection on Dam Surface}. Water.\n\n\\bibitem{monjezi2024tdt}\nEhsan Monjezi, G. Akbarizadeh, and Karim Ansari-Asl (2024). \\textit{RI-ViT: A Multi-Scale Hybrid Method Based on Vision Transformer for Breast Cancer Detection in Histopathological Images}. IEEE Access.\n\n\\bibitem{baek2025h8e}\nEu-tteum Baek (2025). \\textit{Attention Score-Based Multi-Vision Transformer Technique for Plant Disease Classification}. Italian National Conference on Sensors.\n\n\\bibitem{payne2024u8l}\nDavid L. Payne, Xuan Xu, Farshid Faraji, et al. (2024). \\textit{Automated Detection of Cervical Spinal Stenosis and Cord Compression via Vision Transformer and Rules-Based Classification}. American Journal of Neuroradiology.\n\n\\bibitem{qi2024rzy}\nNan Qi, Yan Piao, Hao Zhang, et al. (2024). \\textit{Seizure prediction based on improved vision transformer model for EEG channel optimization}. Computer Methods in Biomechanics and Biomedical Engineering.\n\n\\bibitem{mercier2024063}\nJ. Mercier, O. Ertz, and E. Bocher (2024). \\textit{Quantifying Dwell Time With Location-based Augmented Reality: Dynamic AOI Analysis on Mobile Eye Tracking Data With Vision Transformer}. Journal of Eye Movement Research.\n\n\\bibitem{sikkandar2024p0d}\nMohamed Yacin Sikkandar, S. Sundaram, Ahmad Alassaf, et al. (2024). \\textit{Utilizing adaptive deformable convolution and position embedding for colon polyp segmentation with a visual transformer}. Scientific Reports.\n\n\\bibitem{hou2024e4y}\nMingyang Hou, Zhiyong Huang, Zhi Yu, et al. (2024). \\textit{CSwT-SR: Conv-Swin Transformer for Blind Remote Sensing Image Super-Resolution With Amplitude-Phase Learning and Structural Detail Alternating Learning}. IEEE Transactions on Geoscience and Remote Sensing.\n\n\\bibitem{nfor2025o20}\nKintoh Allen Nfor, Tagne Poupi Theodore Armand, Kenesbaeva Periyzat Ismaylovna, et al. (2025). \\textit{An Explainable CNN and Vision Transformer-Based Approach for Real-Time Food Recognition}. Nutrients.\n\n\\bibitem{xiang2024tww}\nChangcheng Xiang, Duofen Yin, Fei Song, et al. (2024). \\textit{A Fast and Robust Safety Helmet Network Based on a Mutilscale Swin Transformer}. Buildings.\n\n\\bibitem{tian20242kr}\nYuan Tian, Jingxuan Zhu, Huang Yao, et al. (2024). \\textit{Facial Expression Recognition Based on Vision Transformer with Hybrid Local Attention}. Applied Sciences.\n\n\\bibitem{zhou2024r66}\nNan Zhou, Mingming Xu, Biaoqun Shen, et al. (2024). \\textit{ViT-UNet: A Vision Transformer Based UNet Model for Coastal Wetland Classification Based on High Spatial Resolution Imagery}. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing.\n\n\\bibitem{taye20244db}\nGizatie Desalegn Taye, Zewdie Habtie Sisay, Genet Worku Gebeyhu, et al. (2024). \\textit{Thoracic computed tomography (CT) image-based identification and severity classification of COVID-19 cases using vision transformer (ViT)}. Discover Applied Sciences.\n\n\\bibitem{alohali2024xwz}\nManal Abdullah Alohali, Nora El-Rashidy, Saad Alaklabi, et al. (2024). \\textit{Swin-GA-RF: genetic algorithm-based Swin Transformer and random forest for enhancing cervical cancer classification}. Frontiers in Oncology.\n\n\\bibitem{gao20246ks}\nZhenchang Gao, Shanshan Chen, Jinxian Huang, et al. (2024). \\textit{Real-time quantitative detection of hydrocolloid adulteration in meat based on Swin Transformer and smartphone.}. Journal of Food Science.\n\n\\bibitem{du2024s3t}\nYufeng Du, Rongyun Zhang, Peicheng Shi, et al. (2024). \\textit{ST-LaneNet: Lane Line Detection Method Based on Swin Transformer and LaneNet}. Chinese Journal of Mechanical Engineering.\n\n\\bibitem{tiwari2024jm9}\nR. Tiwari, Himani Maheshwari, Vinay Gautam, et al. (2024). \\textit{CurrencyNet: A Vision Transformer-Based Approach for Indian Currency Note Classification with Optimizer Exploration}. 2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT).\n\n\\bibitem{du20248pd}\nChunlai Du, Yanhui Guo, and Yuhang Zhang (2024). \\textit{A Deep Learning-Based Intrusion Detection Model Integrating Convolutional Neural Network and Vision Transformer for Network Traffic Attack in the Internet of Things}. Electronics.\n\n\\bibitem{chaurasia2024tri}\nA. Chaurasia, H. C. Harris, P. W. Toohey, et al. (2024). \\textit{A generalised vision transformer-based self-supervised model for diagnosing and grading prostate cancer using histological images}. medRxiv.\n\n\\bibitem{karagz2024ukp}\nMeryem Altin Karagz, zkan U. Nalbantoglu, and Geoffrey C. Fox (2024). \\textit{Residual Vision Transformer (ResViT) Based Self-Supervised Learning Model for Brain Tumor Classification}. arXiv.org.\n\n\\bibitem{lee2025r01}\nHyojin Lee, You Rim Choi, Hyun Kyung Lee, et al. (2025). \\textit{Explainable vision transformer for automatic visual sleep staging on multimodal PSG signals}. npj Digit. Medicine.\n\n\\bibitem{dmen2024cb9}\nSezer Dmen, Esra Kavalc Ylmaz, Kemal Adem, et al. (2024). \\textit{Performance of vision transformer and swin transformer models for lemon quality classification in fruit juice factories}. European Food Research and Technology.\n\n\\bibitem{ferdous2024f89}\nGazi Jannatul Ferdous, Khaleda Akhter Sathi, Md. Azad Hossain, et al. (2024). \\textit{SPT-Swin: A Shifted Patch Tokenization Swin Transformer for Image Classification}. IEEE Access.\n\n\\bibitem{akan2024izq}\nSara Akan, Songl Varli, and Mohammad Alfrad Nobel Bhuiyan (2024). \\textit{An enhanced Swin Transformer for soccer player reidentification}. Scientific Reports.\n\n\\bibitem{nahak20242mv}\nPradeep Nahak, D. K. Pratihar, and A. K. Deb (2024). \\textit{Tomato maturity stage prediction based on vision transformer and deep convolution neural networks}. International Journal of Hybrid Intelligent Systems.\n\n\\bibitem{han2024f96}\nYufei Han, Haoyuan Chen, Linwei Yao, et al. (2024). \\textit{MAT-VIT:A Vision Transformer with MAE-Based Self-Supervised Auxiliary Task for Medical Image Classification}. International Conference on Computer Supported Cooperative Work in Design.\n\n\\bibitem{zhao2024p8o}\nXiaoping Zhao, Jingjing Xu, Zhichen Lin, et al. (2024). \\textit{BiCFormer: Swin Transformer based model for classification of benign and malignant pulmonary nodules}. Measurement science and technology.\n\n\\bibitem{li2024qva}\nTao Li, and Yi Zhang (2024). \\textit{A Contour-Aware Monocular Depth Estimation Network Using Swin Transformer and Cascaded Multiscale Fusion}. IEEE Sensors Journal.\n\n\\bibitem{wang2024ueo}\nYancheng Wang, and Yingzhen Yang (2024). \\textit{Efficient Visual Transformer by Learnable Token Merging}. IEEE Transactions on Pattern Analysis and Machine Intelligence.\n\n\\bibitem{qi2024f5d}\nHaochen Qi, Xiangwei Kong, Zhibo Jin, et al. (2024). \\textit{A Vision-Transformer-Based Convex Variational Network for Bridge Pavement Defect Segmentation}. IEEE transactions on intelligent transportation systems (Print).\n\n\\bibitem{zhu2024l2i}\nShaojun Zhu, Guotao Chen, Hongguang Chen, et al. (2024). \\textit{Squeeze-and-excitation-attention-based mobile vision transformer for grading recognition of bladder prolapse in pelvic MRI images.}. Medical Physics (Lancaster).\n\n\\bibitem{roy2024r9y}\nBarsha Roy, Md. Farukuzzaman Faruk, Md Nazmul Islam, et al. (2024). \\textit{A Cutting-Edge Ensemble of Vision Transformer and ResNet101v2 Based Transfer Learning for the Precise Classification of Leukemia Sub-types from Peripheral Blood Smear Images}. International Conference on Electrical Engineering and Information Communication Technology.\n\n\\bibitem{wang2024w4u}\nChunbao Wang, Xiangyu Wang, Zeyu Gao, et al. (2024). \\textit{Multiple serous cavity effusion screening based on smear images using vision transformer}. Scientific Reports.\n\n\\bibitem{pan202424q}\nGuangliang Pan, Qihui Wu, Bo Zhou, et al. (2024). \\textit{Spectrum Prediction With Deep 3D Pyramid Vision Transformer Learning}. IEEE Transactions on Wireless Communications.\n\n\\bibitem{du2024lml}\nHaiying Du, Jie Shen, Jing Wang, et al. (2024). \\textit{Vision transformer-based electronic nose for enhanced mixed gases classification}. Measurement science and technology.\n\n\\bibitem{luo202432g}\nKevin Luo, and Ie-bin Lian (2024). \\textit{Building a Vision Transformer-Based Damage Severity Classifier with Ground-Level Imagery of Homes Affected by California Wildfires}. Fire.\n\n\\bibitem{elnabi2025psy}\nSamy Abd El-Nabi, Ahmed F. Ibrahim, El-Sayed M. El-Rabaie, et al. (2025). \\textit{Driver Drowsiness Detection Using Swin Transformer and Diffusion Models for Robust Image Denoising}. IEEE Access.\n\n\\bibitem{ergn2025r6s}\nEbru Ergn (2025). \\textit{High precision banana variety identification using vision transformer based feature extraction and support vector machine}. Scientific Reports.\n\n\\bibitem{mohsin2025gup}\nMuhammad Ahmed Mohsin, Muhammad Jazib, Zeeshan Alam, et al. (2025). \\textit{Vision Transformer Based Semantic Communications for Next Generation Wireless Networks}. 2025 IEEE International Conference on Communications Workshops (ICC Workshops).\n\n\\bibitem{marcos2024oo2}\nLuella Marcos, Paul S. Babyn, and J. Alirezaie (2024). \\textit{Pure Vision Transformer (CT-ViT) with Noise2Neighbors Interpolation for Low-Dose CT Image Denoising.}. Journal of imaging informatics in medicine.\n\n\\bibitem{peng2024kal}\nXianhui Peng, Chenchen Xu, Peng Zhang, et al. (2024). \\textit{Computer vision classification detection of chicken parts based on optimized Swin-Transformer}. CyTA - Journal of Food.\n\n\\bibitem{urrea20245k4}\nClaudio Urrea, and Maximiliano Vlez (2024). \\textit{Enhancing Autonomous Visual Perception in Challenging Environments: Bilateral Models with Vision Transformer and Multilayer Perceptron for Traversable Area Detection}. Technologies.\n\n\\bibitem{zhang2024b7v}\nJinnian Zhang, Weijie Chen, Tanmayee Joshi, et al. (2024). \\textit{BAE-ViT: An Efficient Multimodal Vision Transformer for Bone Age Estimation}. Tomography.\n\n\\bibitem{saleem20249yl}\nHira Saleem, Flora Salim, and Cormac Purcell (2024). \\textit{STC-ViT: Spatio Temporal Continuous Vision Transformer for Weather Forecasting}. Unpublished manuscript.\n\n\\bibitem{zhou2024toe}\nYang Zhou, Cai Yang, Ping Wang, et al. (2024). \\textit{ViT-FuseNet: Multimodal Fusion of Vision Transformer for Vehicle-Infrastructure Cooperative Perception}. IEEE Access.\n\n\\bibitem{lijin2024mhk}\nP. Lijin, M. Ullah, Anuja Vats, et al. (2024). \\textit{PolySegNet: improving polyp segmentation through swin transformer and vision transformer fusion.}. Biomedical Engineering Letters.\n\n\\bibitem{huang2024htf}\nLan Huang, Jiong Ma, Hui Yang, et al. (2024). \\textit{Research and implementation of multi-disease diagnosis on chest X-ray based on vision transformer}. Quantitative Imaging in Medicine and Surgery.\n\n\\bibitem{chen2024cha}\nChuanyu Chen, Yi Luo, Qiuyang Hou, et al. (2024). \\textit{A vision transformer-based deep transfer learning nomogram for predicting lymph node metastasis in lung adenocarcinoma.}. Medical Physics (Lancaster).\n\n\\bibitem{shahin2024o1c}\nMohammed Shahin, and Mohamed Deriche (2024). \\textit{A Novel Framework based on a Hybrid Vision Transformer and Deep Neural Network for Deepfake Detection}. International Multi-Conference on Systems, Signals & Devices.\n\n\\bibitem{xu2024wux}\nYang Xu, and Zuqiang Meng (2024). \\textit{Interpretable vision transformer based on prototype parts for COVID-19 detection}. IET Image Processing.\n\n\\bibitem{park2024d7y}\nJoohyuk Park, Yong-Nam Oh, Yongjune Kim, et al. (2024). \\textit{Vision Transformer-Based Semantic Communications With Importance-Aware Quantization}. IEEE Internet of Things Journal.\n\n\\bibitem{elharrouss20252ng}\nO. Elharrouss, Y. Akbari, Noor Almaadeed, et al. (2025). \\textit{PDC-ViT : Source Camera Identification using Pixel Difference Convolution and Vision Transformer}. Neural computing & applications (Print).\n\n\\bibitem{du2024i6n}\nYongqiang Du, Haoran Liu, Shengjie He, et al. (2024). \\textit{InViT: GAN Inversion-Based Vision Transformer for Blind Image Inpainting}. IEEE Access.\n\n\\bibitem{guo2024o8u}\nQianyu Guo, Ziqing Yu, Jiaming Fu, et al. (2024). \\textit{Force-EvT: A Closer Look at Robotic Gripper Force Measurement with Event-Based Vision Transformer}. 2024 6th International Conference on Reconfigurable Mechanisms and Robots (ReMAR).\n\n\\bibitem{zhang2024g0m}\nKunpeng Zhang, Mengyan Lyu, Xinxin Guo, et al. (2024). \\textit{Temporal Shift Module-Based Vision Transformer Network for Action Recognition}. IEEE Access.\n\n\\bibitem{xu2025tku}\nLu Xu, Rui Shi, and Yijia Zhang (2025). \\textit{A Radio Frequency Sensor-Based UAV Detection and Identification System Using Improved Vision Transformer-Based Model}. IEEE Sensors Journal.\n\n\\bibitem{li2024m4t}\nYang Li, Doudou Zhang, and Jianli Xiao (2024). \\textit{A New Method for Vehicle Logo Recognition Based on Swin Transformer}. arXiv.org.\n\n\\end{thebibliography}\n\n\\end{document}",
  "generation_date": "2025-10-07T23:09:20.474440",
  "processed_papers_data": [
    {
      "success": true,
      "doc_id": "443475d8dbaa2b6222b0a68744bb4b26",
      "summary": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer.",
      "intriguing_abstract": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/c8b25fab5608c3e033d34b4483ec47e68ba109b7.pdf",
      "citation_key": "liu2021ljs",
      "metadata": {
        "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
        "authors": [
          "Ze Liu",
          "Yutong Lin",
          "Yue Cao",
          "Han Hu",
          "Yixuan Wei",
          "Zheng Zhang",
          "Stephen Lin",
          "B. Guo"
        ],
        "published_date": "2021",
        "abstract": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c8b25fab5608c3e033d34b4483ec47e68ba109b7.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 24050,
        "score": 6012.5,
        "summary": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer.",
        "keywords": []
      },
      "file_name": "c8b25fab5608c3e033d34b4483ec47e68ba109b7.pdf"
    },
    {
      "success": true,
      "doc_id": "f38fda2e06f7458a501c86051fece4b5",
      "summary": "Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by up to 0.140.45dB, while the total number of parameters can be reduced by up to 67%.",
      "intriguing_abstract": "Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by up to 0.140.45dB, while the total number of parameters can be reduced by up to 67%.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/7a9a708ca61c14886aa0dcd6d13dac7879713f5f.pdf",
      "citation_key": "liang2021v6x",
      "metadata": {
        "title": "SwinIR: Image Restoration Using Swin Transformer",
        "authors": [
          "Jingyun Liang",
          "Jie Cao",
          "Guolei Sun",
          "K. Zhang",
          "L. Gool",
          "R. Timofte"
        ],
        "published_date": "2021",
        "abstract": "Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by up to 0.140.45dB, while the total number of parameters can be reduced by up to 67%.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7a9a708ca61c14886aa0dcd6d13dac7879713f5f.pdf",
        "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
        "citationCount": 3340,
        "score": 835.0,
        "summary": "Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by up to 0.140.45dB, while the total number of parameters can be reduced by up to 67%.",
        "keywords": []
      },
      "file_name": "7a9a708ca61c14886aa0dcd6d13dac7879713f5f.pdf"
    },
    {
      "success": true,
      "doc_id": "717ffff2c145973c08ec49e55007baf0",
      "summary": "Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",
      "intriguing_abstract": "Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d40c77c010c8dbef6142903a02f2a73a85012d5d.pdf",
      "citation_key": "han2020yk0",
      "metadata": {
        "title": "A Survey on Vision Transformer",
        "authors": [
          "Kai Han",
          "Yunhe Wang",
          "Hanting Chen",
          "Xinghao Chen",
          "Jianyuan Guo",
          "Zhenhua Liu",
          "Yehui Tang",
          "An Xiao",
          "Chunjing Xu",
          "Yixing Xu",
          "Zhaohui Yang",
          "Yiman Zhang",
          "D. Tao"
        ],
        "published_date": "2020",
        "abstract": "Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d40c77c010c8dbef6142903a02f2a73a85012d5d.pdf",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citationCount": 2562,
        "score": 512.4,
        "summary": "Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",
        "keywords": []
      },
      "file_name": "d40c77c010c8dbef6142903a02f2a73a85012d5d.pdf"
    },
    {
      "success": true,
      "doc_id": "637f49f83eadc33faabb010e57fdd26e",
      "summary": "The recently developed vision transformer (ViT) has achieved promising results on image classification compared to convolutional neural networks. Inspired by this, in this paper, we study how to learn multi-scale feature representations in transformer models for image classification. To this end, we propose a dual-branch transformer to com-bine image patches (i.e., tokens in a transformer) of different sizes to produce stronger image features. Our approach processes small-patch and large-patch tokens with two separate branches of different computational complexity and these tokens are then fused purely by attention multiple times to complement each other. Furthermore, to reduce computation, we develop a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches. Our proposed cross-attention only requires linear time for both computational and memory complexity instead of quadratic time otherwise. Extensive experiments demonstrate that our approach performs better than or on par with several concurrent works on vision transformer, in addition to efficient CNN models. For example, on the ImageNet1K dataset, with some architectural changes, our approach outperforms the recent DeiT by a large margin of 2% with a small to moderate increase in FLOPs and model parameters. Our source codes and models are available at https://github.com/IBM/CrossViT.",
      "intriguing_abstract": "The recently developed vision transformer (ViT) has achieved promising results on image classification compared to convolutional neural networks. Inspired by this, in this paper, we study how to learn multi-scale feature representations in transformer models for image classification. To this end, we propose a dual-branch transformer to com-bine image patches (i.e., tokens in a transformer) of different sizes to produce stronger image features. Our approach processes small-patch and large-patch tokens with two separate branches of different computational complexity and these tokens are then fused purely by attention multiple times to complement each other. Furthermore, to reduce computation, we develop a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches. Our proposed cross-attention only requires linear time for both computational and memory complexity instead of quadratic time otherwise. Extensive experiments demonstrate that our approach performs better than or on par with several concurrent works on vision transformer, in addition to efficient CNN models. For example, on the ImageNet1K dataset, with some architectural changes, our approach outperforms the recent DeiT by a large margin of 2% with a small to moderate increase in FLOPs and model parameters. Our source codes and models are available at https://github.com/IBM/CrossViT.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/0eff37167876356da2163b2e396df2719adf7de9.pdf",
      "citation_key": "chen2021r2y",
      "metadata": {
        "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
        "authors": [
          "Chun-Fu Chen",
          "Quanfu Fan",
          "Rameswar Panda"
        ],
        "published_date": "2021",
        "abstract": "The recently developed vision transformer (ViT) has achieved promising results on image classification compared to convolutional neural networks. Inspired by this, in this paper, we study how to learn multi-scale feature representations in transformer models for image classification. To this end, we propose a dual-branch transformer to com-bine image patches (i.e., tokens in a transformer) of different sizes to produce stronger image features. Our approach processes small-patch and large-patch tokens with two separate branches of different computational complexity and these tokens are then fused purely by attention multiple times to complement each other. Furthermore, to reduce computation, we develop a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches. Our proposed cross-attention only requires linear time for both computational and memory complexity instead of quadratic time otherwise. Extensive experiments demonstrate that our approach performs better than or on par with several concurrent works on vision transformer, in addition to efficient CNN models. For example, on the ImageNet1K dataset, with some architectural changes, our approach outperforms the recent DeiT by a large margin of 2% with a small to moderate increase in FLOPs and model parameters. Our source codes and models are available at https://github.com/IBM/CrossViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0eff37167876356da2163b2e396df2719adf7de9.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 1640,
        "score": 410.0,
        "summary": "The recently developed vision transformer (ViT) has achieved promising results on image classification compared to convolutional neural networks. Inspired by this, in this paper, we study how to learn multi-scale feature representations in transformer models for image classification. To this end, we propose a dual-branch transformer to com-bine image patches (i.e., tokens in a transformer) of different sizes to produce stronger image features. Our approach processes small-patch and large-patch tokens with two separate branches of different computational complexity and these tokens are then fused purely by attention multiple times to complement each other. Furthermore, to reduce computation, we develop a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches. Our proposed cross-attention only requires linear time for both computational and memory complexity instead of quadratic time otherwise. Extensive experiments demonstrate that our approach performs better than or on par with several concurrent works on vision transformer, in addition to efficient CNN models. For example, on the ImageNet1K dataset, with some architectural changes, our approach outperforms the recent DeiT by a large margin of 2% with a small to moderate increase in FLOPs and model parameters. Our source codes and models are available at https://github.com/IBM/CrossViT.",
        "keywords": []
      },
      "file_name": "0eff37167876356da2163b2e396df2719adf7de9.pdf"
    },
    {
      "success": true,
      "doc_id": "54ef7e6b67559056c7193f9723bf1304",
      "summary": "Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets",
      "intriguing_abstract": "Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/da74a10824193be9d3889ce0d6ed4c6f8ee48b9e.pdf",
      "citation_key": "mehta20216ad",
      "metadata": {
        "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer",
        "authors": [
          "Sachin Mehta",
          "Mohammad Rastegari"
        ],
        "published_date": "2021",
        "abstract": "Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/da74a10824193be9d3889ce0d6ed4c6f8ee48b9e.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 1482,
        "score": 370.5,
        "summary": "Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets",
        "keywords": []
      },
      "file_name": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e.pdf"
    },
    {
      "success": true,
      "doc_id": "f12cdbd03409495b3a19b11f918b8fba",
      "summary": "We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors. Code for ViTDet is available in Detectron2.",
      "intriguing_abstract": "We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors. Code for ViTDet is available in Detectron2.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/a09cbcaac305884f043810afc4fa4053099b5970.pdf",
      "citation_key": "li2022raj",
      "metadata": {
        "title": "Exploring Plain Vision Transformer Backbones for Object Detection",
        "authors": [
          "Yanghao Li",
          "Hanzi Mao",
          "Ross B. Girshick",
          "Kaiming He"
        ],
        "published_date": "2022",
        "abstract": "We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors. Code for ViTDet is available in Detectron2.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a09cbcaac305884f043810afc4fa4053099b5970.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 918,
        "score": 306.0,
        "summary": "We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors. Code for ViTDet is available in Detectron2.",
        "keywords": []
      },
      "file_name": "a09cbcaac305884f043810afc4fa4053099b5970.pdf"
    },
    {
      "success": true,
      "doc_id": "075143edd39ade50ac4d7b9b3c7d310a",
      "summary": "Pretraining Vision Transformers (ViTs) has achieved great success in visual recognition. A following scenario is to adapt a ViT to various image and video recognition tasks. The adaptation is challenging because of heavy computation and memory storage. Each model needs an independent and complete finetuning process to adapt to different tasks, which limits its transferability to different visual domains. To address this challenge, we propose an effective adaptation approach for Transformer, namely AdaptFormer, which can adapt the pre-trained ViTs into many different image and video tasks efficiently. It possesses several benefits more appealing than prior arts. Firstly, AdaptFormer introduces lightweight modules that only add less than 2% extra parameters to a ViT, while it is able to increase the ViT's transferability without updating its original pre-trained parameters, significantly outperforming the existing 100\\% fully fine-tuned models on action recognition benchmarks. Secondly, it can be plug-and-play in different Transformers and scalable to many visual tasks. Thirdly, extensive experiments on five image and video datasets show that AdaptFormer largely improves ViTs in the target domains. For example, when updating just 1.5% extra parameters, it achieves about 10% and 19% relative improvement compared to the fully fine-tuned models on Something-Something~v2 and HMDB51, respectively. Code is available at https://github.com/ShoufaChen/AdaptFormer.",
      "intriguing_abstract": "Pretraining Vision Transformers (ViTs) has achieved great success in visual recognition. A following scenario is to adapt a ViT to various image and video recognition tasks. The adaptation is challenging because of heavy computation and memory storage. Each model needs an independent and complete finetuning process to adapt to different tasks, which limits its transferability to different visual domains. To address this challenge, we propose an effective adaptation approach for Transformer, namely AdaptFormer, which can adapt the pre-trained ViTs into many different image and video tasks efficiently. It possesses several benefits more appealing than prior arts. Firstly, AdaptFormer introduces lightweight modules that only add less than 2% extra parameters to a ViT, while it is able to increase the ViT's transferability without updating its original pre-trained parameters, significantly outperforming the existing 100\\% fully fine-tuned models on action recognition benchmarks. Secondly, it can be plug-and-play in different Transformers and scalable to many visual tasks. Thirdly, extensive experiments on five image and video datasets show that AdaptFormer largely improves ViTs in the target domains. For example, when updating just 1.5% extra parameters, it achieves about 10% and 19% relative improvement compared to the fully fine-tuned models on Something-Something~v2 and HMDB51, respectively. Code is available at https://github.com/ShoufaChen/AdaptFormer.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/2fe2f849b94cf08b559226bc9d78adcaef5ef186.pdf",
      "citation_key": "chen2022woa",
      "metadata": {
        "title": "AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition",
        "authors": [
          "Shoufa Chen",
          "Chongjian Ge",
          "Zhan Tong",
          "Jiangliu Wang",
          "Yibing Song",
          "Jue Wang",
          "Ping Luo"
        ],
        "published_date": "2022",
        "abstract": "Pretraining Vision Transformers (ViTs) has achieved great success in visual recognition. A following scenario is to adapt a ViT to various image and video recognition tasks. The adaptation is challenging because of heavy computation and memory storage. Each model needs an independent and complete finetuning process to adapt to different tasks, which limits its transferability to different visual domains. To address this challenge, we propose an effective adaptation approach for Transformer, namely AdaptFormer, which can adapt the pre-trained ViTs into many different image and video tasks efficiently. It possesses several benefits more appealing than prior arts. Firstly, AdaptFormer introduces lightweight modules that only add less than 2% extra parameters to a ViT, while it is able to increase the ViT's transferability without updating its original pre-trained parameters, significantly outperforming the existing 100\\% fully fine-tuned models on action recognition benchmarks. Secondly, it can be plug-and-play in different Transformers and scalable to many visual tasks. Thirdly, extensive experiments on five image and video datasets show that AdaptFormer largely improves ViTs in the target domains. For example, when updating just 1.5% extra parameters, it achieves about 10% and 19% relative improvement compared to the fully fine-tuned models on Something-Something~v2 and HMDB51, respectively. Code is available at https://github.com/ShoufaChen/AdaptFormer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2fe2f849b94cf08b559226bc9d78adcaef5ef186.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 806,
        "score": 268.66666666666663,
        "summary": "Pretraining Vision Transformers (ViTs) has achieved great success in visual recognition. A following scenario is to adapt a ViT to various image and video recognition tasks. The adaptation is challenging because of heavy computation and memory storage. Each model needs an independent and complete finetuning process to adapt to different tasks, which limits its transferability to different visual domains. To address this challenge, we propose an effective adaptation approach for Transformer, namely AdaptFormer, which can adapt the pre-trained ViTs into many different image and video tasks efficiently. It possesses several benefits more appealing than prior arts. Firstly, AdaptFormer introduces lightweight modules that only add less than 2% extra parameters to a ViT, while it is able to increase the ViT's transferability without updating its original pre-trained parameters, significantly outperforming the existing 100\\% fully fine-tuned models on action recognition benchmarks. Secondly, it can be plug-and-play in different Transformers and scalable to many visual tasks. Thirdly, extensive experiments on five image and video datasets show that AdaptFormer largely improves ViTs in the target domains. For example, when updating just 1.5% extra parameters, it achieves about 10% and 19% relative improvement compared to the fully fine-tuned models on Something-Something~v2 and HMDB51, respectively. Code is available at https://github.com/ShoufaChen/AdaptFormer.",
        "keywords": []
      },
      "file_name": "2fe2f849b94cf08b559226bc9d78adcaef5ef186.pdf"
    },
    {
      "success": true,
      "doc_id": "75c2d3f79356cef3a8f11809513a657d",
      "summary": "Transformers have recently shown superior performances on various vision tasks. The large, sometimes even global, receptive field endows Transformer models with higher representation power over their CNN counterparts. Nevertheless, simply enlarging receptive field also gives rise to several concerns. On the one hand, using dense attention e.g., in ViT, leads to excessive memory and computational cost, and features can be influenced by irrelevant parts which are beyond the region of interests. On the other hand, the sparse attention adopted in PVT or Swin Transformer is data agnostic and may limit the ability to model long range relations. To mitigate these issues, we propose a novel deformable selfattention module, where the positions of key and value pairs in selfattention are selected in a data-dependent way. This flexible scheme enables the self-attention module to focus on relevant re-gions and capture more informative features. On this basis, we present Deformable Attention Transformer, a general backbone model with deformable attention for both image classification and dense prediction tasks. Extensive experi-ments show that our models achieve consistently improved results on comprehensive benchmarks. Code is available at https://github.com/LeapLabTHU/DAT.",
      "intriguing_abstract": "Transformers have recently shown superior performances on various vision tasks. The large, sometimes even global, receptive field endows Transformer models with higher representation power over their CNN counterparts. Nevertheless, simply enlarging receptive field also gives rise to several concerns. On the one hand, using dense attention e.g., in ViT, leads to excessive memory and computational cost, and features can be influenced by irrelevant parts which are beyond the region of interests. On the other hand, the sparse attention adopted in PVT or Swin Transformer is data agnostic and may limit the ability to model long range relations. To mitigate these issues, we propose a novel deformable selfattention module, where the positions of key and value pairs in selfattention are selected in a data-dependent way. This flexible scheme enables the self-attention module to focus on relevant re-gions and capture more informative features. On this basis, we present Deformable Attention Transformer, a general backbone model with deformable attention for both image classification and dense prediction tasks. Extensive experi-ments show that our models achieve consistently improved results on comprehensive benchmarks. Code is available at https://github.com/LeapLabTHU/DAT.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/e5cb26148791b57bfd36aa26ce2401e231d01b57.pdf",
      "citation_key": "xia2022qga",
      "metadata": {
        "title": "Vision Transformer with Deformable Attention",
        "authors": [
          "Zhuofan Xia",
          "Xuran Pan",
          "S. Song",
          "Li Erran Li",
          "Gao Huang"
        ],
        "published_date": "2022",
        "abstract": "Transformers have recently shown superior performances on various vision tasks. The large, sometimes even global, receptive field endows Transformer models with higher representation power over their CNN counterparts. Nevertheless, simply enlarging receptive field also gives rise to several concerns. On the one hand, using dense attention e.g., in ViT, leads to excessive memory and computational cost, and features can be influenced by irrelevant parts which are beyond the region of interests. On the other hand, the sparse attention adopted in PVT or Swin Transformer is data agnostic and may limit the ability to model long range relations. To mitigate these issues, we propose a novel deformable selfattention module, where the positions of key and value pairs in selfattention are selected in a data-dependent way. This flexible scheme enables the self-attention module to focus on relevant re-gions and capture more informative features. On this basis, we present Deformable Attention Transformer, a general backbone model with deformable attention for both image classification and dense prediction tasks. Extensive experi-ments show that our models achieve consistently improved results on comprehensive benchmarks. Code is available at https://github.com/LeapLabTHU/DAT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e5cb26148791b57bfd36aa26ce2401e231d01b57.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 560,
        "score": 186.66666666666666,
        "summary": "Transformers have recently shown superior performances on various vision tasks. The large, sometimes even global, receptive field endows Transformer models with higher representation power over their CNN counterparts. Nevertheless, simply enlarging receptive field also gives rise to several concerns. On the one hand, using dense attention e.g., in ViT, leads to excessive memory and computational cost, and features can be influenced by irrelevant parts which are beyond the region of interests. On the other hand, the sparse attention adopted in PVT or Swin Transformer is data agnostic and may limit the ability to model long range relations. To mitigate these issues, we propose a novel deformable selfattention module, where the positions of key and value pairs in selfattention are selected in a data-dependent way. This flexible scheme enables the self-attention module to focus on relevant re-gions and capture more informative features. On this basis, we present Deformable Attention Transformer, a general backbone model with deformable attention for both image classification and dense prediction tasks. Extensive experi-ments show that our models achieve consistently improved results on comprehensive benchmarks. Code is available at https://github.com/LeapLabTHU/DAT.",
        "keywords": []
      },
      "file_name": "e5cb26148791b57bfd36aa26ce2401e231d01b57.pdf"
    },
    {
      "success": true,
      "doc_id": "5e339f4421e9afa9d672728abcaf4f04",
      "summary": "Vision transformers (ViTs) have been successfully applied in image classification tasks recently. In this paper, we show that, unlike convolution neural networks (CNNs)that can be improved by stacking more convolutional layers, the performance of ViTs saturate fast when scaled to be deeper. More specifically, we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper, the attention maps gradually become similar and even much the same after certain layers. In other words, the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs, the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation, we propose a simple yet effective method, named Re-attention, to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably, when training a deep ViT model with 32 transformer blocks, the Top-1 classification accuracy can be improved by 1.6% on ImageNet. Code is publicly available at https://github.com/zhoudaquan/dvit_repo.",
      "intriguing_abstract": "Vision transformers (ViTs) have been successfully applied in image classification tasks recently. In this paper, we show that, unlike convolution neural networks (CNNs)that can be improved by stacking more convolutional layers, the performance of ViTs saturate fast when scaled to be deeper. More specifically, we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper, the attention maps gradually become similar and even much the same after certain layers. In other words, the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs, the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation, we propose a simple yet effective method, named Re-attention, to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably, when training a deep ViT model with 32 transformer blocks, the Top-1 classification accuracy can be improved by 1.6% on ImageNet. Code is publicly available at https://github.com/zhoudaquan/dvit_repo.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/96da196d6f8c947db03d13759f030642f8234abf.pdf",
      "citation_key": "zhou202105h",
      "metadata": {
        "title": "DeepViT: Towards Deeper Vision Transformer",
        "authors": [
          "Daquan Zhou",
          "Bingyi Kang",
          "Xiaojie Jin",
          "Linjie Yang",
          "Xiaochen Lian",
          "Qibin Hou",
          "Jiashi Feng"
        ],
        "published_date": "2021",
        "abstract": "Vision transformers (ViTs) have been successfully applied in image classification tasks recently. In this paper, we show that, unlike convolution neural networks (CNNs)that can be improved by stacking more convolutional layers, the performance of ViTs saturate fast when scaled to be deeper. More specifically, we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper, the attention maps gradually become similar and even much the same after certain layers. In other words, the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs, the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation, we propose a simple yet effective method, named Re-attention, to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably, when training a deep ViT model with 32 transformer blocks, the Top-1 classification accuracy can be improved by 1.6% on ImageNet. Code is publicly available at https://github.com/zhoudaquan/dvit_repo.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/96da196d6f8c947db03d13759f030642f8234abf.pdf",
        "venue": "arXiv.org",
        "citationCount": 548,
        "score": 137.0,
        "summary": "Vision transformers (ViTs) have been successfully applied in image classification tasks recently. In this paper, we show that, unlike convolution neural networks (CNNs)that can be improved by stacking more convolutional layers, the performance of ViTs saturate fast when scaled to be deeper. More specifically, we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper, the attention maps gradually become similar and even much the same after certain layers. In other words, the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs, the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation, we propose a simple yet effective method, named Re-attention, to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably, when training a deep ViT model with 32 transformer blocks, the Top-1 classification accuracy can be improved by 1.6% on ImageNet. Code is publicly available at https://github.com/zhoudaquan/dvit_repo.",
        "keywords": []
      },
      "file_name": "96da196d6f8c947db03d13759f030642f8234abf.pdf"
    },
    {
      "success": true,
      "doc_id": "34c9b9466a75114d76fd8f630e6c9fc7",
      "summary": "Existing state-of-the-art saliency detection methods heavily rely on CNN-based architectures. Alternatively, we rethink this task from a convolution-free sequence-to-sequence perspective and predict saliency by modeling long-range dependencies, which can not be achieved by convolution. Specifically, we develop a novel unified model based on a pure transformer, namely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient object detection (SOD). It takes image patches as inputs and leverages the transformer to propagate global contexts among image patches. Unlike conventional architectures used in Vision Transformer (ViT), we leverage multi-level token fusion and propose a new token upsampling method under the transformer framework to get high-resolution detection results. We also develop a token-based multi-task decoder to simultaneously perform saliency and boundary detection by introducing task-related tokens and a novel patch-task-attention mechanism. Experimental results show that our model outperforms existing methods on both RGB and RGB-D SOD benchmark datasets. Most importantly, our whole framework not only provides a new perspective for the SOD field but also shows a new paradigm for transformer-based dense prediction models. Code is available at https://github.com/nnizhang/VST.",
      "intriguing_abstract": "Existing state-of-the-art saliency detection methods heavily rely on CNN-based architectures. Alternatively, we rethink this task from a convolution-free sequence-to-sequence perspective and predict saliency by modeling long-range dependencies, which can not be achieved by convolution. Specifically, we develop a novel unified model based on a pure transformer, namely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient object detection (SOD). It takes image patches as inputs and leverages the transformer to propagate global contexts among image patches. Unlike conventional architectures used in Vision Transformer (ViT), we leverage multi-level token fusion and propose a new token upsampling method under the transformer framework to get high-resolution detection results. We also develop a token-based multi-task decoder to simultaneously perform saliency and boundary detection by introducing task-related tokens and a novel patch-task-attention mechanism. Experimental results show that our model outperforms existing methods on both RGB and RGB-D SOD benchmark datasets. Most importantly, our whole framework not only provides a new perspective for the SOD field but also shows a new paradigm for transformer-based dense prediction models. Code is available at https://github.com/nnizhang/VST.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/751b71158b7dcd2a7949e72a6ad8fb13657a401c.pdf",
      "citation_key": "liu2021jpu",
      "metadata": {
        "title": "Visual Saliency Transformer",
        "authors": [
          "Nian Liu",
          "Ni Zhang",
          "Kaiyuan Wan",
          "Junwei Han",
          "Ling Shao"
        ],
        "published_date": "2021",
        "abstract": "Existing state-of-the-art saliency detection methods heavily rely on CNN-based architectures. Alternatively, we rethink this task from a convolution-free sequence-to-sequence perspective and predict saliency by modeling long-range dependencies, which can not be achieved by convolution. Specifically, we develop a novel unified model based on a pure transformer, namely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient object detection (SOD). It takes image patches as inputs and leverages the transformer to propagate global contexts among image patches. Unlike conventional architectures used in Vision Transformer (ViT), we leverage multi-level token fusion and propose a new token upsampling method under the transformer framework to get high-resolution detection results. We also develop a token-based multi-task decoder to simultaneously perform saliency and boundary detection by introducing task-related tokens and a novel patch-task-attention mechanism. Experimental results show that our model outperforms existing methods on both RGB and RGB-D SOD benchmark datasets. Most importantly, our whole framework not only provides a new perspective for the SOD field but also shows a new paradigm for transformer-based dense prediction models. Code is available at https://github.com/nnizhang/VST.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/751b71158b7dcd2a7949e72a6ad8fb13657a401c.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 391,
        "score": 97.75,
        "summary": "Existing state-of-the-art saliency detection methods heavily rely on CNN-based architectures. Alternatively, we rethink this task from a convolution-free sequence-to-sequence perspective and predict saliency by modeling long-range dependencies, which can not be achieved by convolution. Specifically, we develop a novel unified model based on a pure transformer, namely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient object detection (SOD). It takes image patches as inputs and leverages the transformer to propagate global contexts among image patches. Unlike conventional architectures used in Vision Transformer (ViT), we leverage multi-level token fusion and propose a new token upsampling method under the transformer framework to get high-resolution detection results. We also develop a token-based multi-task decoder to simultaneously perform saliency and boundary detection by introducing task-related tokens and a novel patch-task-attention mechanism. Experimental results show that our model outperforms existing methods on both RGB and RGB-D SOD benchmark datasets. Most importantly, our whole framework not only provides a new perspective for the SOD field but also shows a new paradigm for transformer-based dense prediction models. Code is available at https://github.com/nnizhang/VST.",
        "keywords": []
      },
      "file_name": "751b71158b7dcd2a7949e72a6ad8fb13657a401c.pdf"
    },
    {
      "success": true,
      "doc_id": "2a6e378e47cfc160db5cc2a339b90abd",
      "summary": "Recently, the Vision Transformer (ViT), which applied the transformer structure to the image classification task, has outperformed convolutional neural networks. However, the high performance of the ViT results from pre-training using a large-size dataset such as JFT-300M, and its dependence on a large dataset is interpreted as due to low locality inductive bias. This paper proposes Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA), which effectively solve the lack of locality inductive bias and enable it to learn from scratch even on small-size datasets. Moreover, SPT and LSA are generic and effective add-on modules that are easily applicable to various ViTs. Experimental results show that when both SPT and LSA were applied to the ViTs, the performance improved by an average of 2.96% in Tiny-ImageNet, which is a representative small-size dataset. Especially, Swin Transformer achieved an overwhelming performance improvement of 4.08% thanks to the proposed SPT and LSA.",
      "intriguing_abstract": "Recently, the Vision Transformer (ViT), which applied the transformer structure to the image classification task, has outperformed convolutional neural networks. However, the high performance of the ViT results from pre-training using a large-size dataset such as JFT-300M, and its dependence on a large dataset is interpreted as due to low locality inductive bias. This paper proposes Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA), which effectively solve the lack of locality inductive bias and enable it to learn from scratch even on small-size datasets. Moreover, SPT and LSA are generic and effective add-on modules that are easily applicable to various ViTs. Experimental results show that when both SPT and LSA were applied to the ViTs, the performance improved by an average of 2.96% in Tiny-ImageNet, which is a representative small-size dataset. Especially, Swin Transformer achieved an overwhelming performance improvement of 4.08% thanks to the proposed SPT and LSA.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/164e41a60120917d13fb69e183ee3c996b6c9414.pdf",
      "citation_key": "lee2021us0",
      "metadata": {
        "title": "Vision Transformer for Small-Size Datasets",
        "authors": [
          "Seung Hoon Lee",
          "Seunghyun Lee",
          "B. Song"
        ],
        "published_date": "2021",
        "abstract": "Recently, the Vision Transformer (ViT), which applied the transformer structure to the image classification task, has outperformed convolutional neural networks. However, the high performance of the ViT results from pre-training using a large-size dataset such as JFT-300M, and its dependence on a large dataset is interpreted as due to low locality inductive bias. This paper proposes Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA), which effectively solve the lack of locality inductive bias and enable it to learn from scratch even on small-size datasets. Moreover, SPT and LSA are generic and effective add-on modules that are easily applicable to various ViTs. Experimental results show that when both SPT and LSA were applied to the ViTs, the performance improved by an average of 2.96% in Tiny-ImageNet, which is a representative small-size dataset. Especially, Swin Transformer achieved an overwhelming performance improvement of 4.08% thanks to the proposed SPT and LSA.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/164e41a60120917d13fb69e183ee3c996b6c9414.pdf",
        "venue": "arXiv.org",
        "citationCount": 250,
        "score": 62.5,
        "summary": "Recently, the Vision Transformer (ViT), which applied the transformer structure to the image classification task, has outperformed convolutional neural networks. However, the high performance of the ViT results from pre-training using a large-size dataset such as JFT-300M, and its dependence on a large dataset is interpreted as due to low locality inductive bias. This paper proposes Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA), which effectively solve the lack of locality inductive bias and enable it to learn from scratch even on small-size datasets. Moreover, SPT and LSA are generic and effective add-on modules that are easily applicable to various ViTs. Experimental results show that when both SPT and LSA were applied to the ViTs, the performance improved by an average of 2.96% in Tiny-ImageNet, which is a representative small-size dataset. Especially, Swin Transformer achieved an overwhelming performance improvement of 4.08% thanks to the proposed SPT and LSA.",
        "keywords": []
      },
      "file_name": "164e41a60120917d13fb69e183ee3c996b6c9414.pdf"
    },
    {
      "success": true,
      "doc_id": "1b67ef9b6fce448c3ffcffba71aa2e17",
      "summary": "Despite the tantalizing success in a broad of vision tasks, transformers have not yet demonstrated on-par ability as ConvNets in high-resolution image generative modeling. In this paper, we seek to explore using pure transformers to build a generative adversarial network for high-resolution image synthesis. To this end, we believe that local attention is crucial to strike the balance between computational efficiency and modeling capacity. Hence, the proposed generator adopts Swin transformer in a style-based architecture. To achieve a larger receptive field, we propose double attention which simultaneously leverages the context of the local and the shifted windows, leading to improved generation quality. Moreover, we show that offering the knowledge of the absolute position that has been lost in window-based transformers greatly benefits the generation quality. The proposed StyleSwin is scalable to high resolutions, with both the coarse geometry and fine structures benefit from the strong expressivity of transformers. However, blocking artifacts occur during high-resolution synthesis because performing the local attention in a block-wise manner may break the spatial coherency. To solve this, we empirically investigate various solutions, among which we find that employing a wavelet discriminator to examine the spectral discrepancy effectively suppresses the artifacts. Extensive experiments show the superiority over prior transformer-based GANs, especially on high resolutions, e.g., $1024 \\times$ 1024. The StyleSwin, without complex training strategies, excels over StyleGAN on CelebA-HQ 1024, and achieves on-par performance on FFHQ-1024, proving the promise of using transformers for high-resolution image generation. The code and pretrained models are available at https://github.com/microsoft/StyleSwin.",
      "intriguing_abstract": "Despite the tantalizing success in a broad of vision tasks, transformers have not yet demonstrated on-par ability as ConvNets in high-resolution image generative modeling. In this paper, we seek to explore using pure transformers to build a generative adversarial network for high-resolution image synthesis. To this end, we believe that local attention is crucial to strike the balance between computational efficiency and modeling capacity. Hence, the proposed generator adopts Swin transformer in a style-based architecture. To achieve a larger receptive field, we propose double attention which simultaneously leverages the context of the local and the shifted windows, leading to improved generation quality. Moreover, we show that offering the knowledge of the absolute position that has been lost in window-based transformers greatly benefits the generation quality. The proposed StyleSwin is scalable to high resolutions, with both the coarse geometry and fine structures benefit from the strong expressivity of transformers. However, blocking artifacts occur during high-resolution synthesis because performing the local attention in a block-wise manner may break the spatial coherency. To solve this, we empirically investigate various solutions, among which we find that employing a wavelet discriminator to examine the spectral discrepancy effectively suppresses the artifacts. Extensive experiments show the superiority over prior transformer-based GANs, especially on high resolutions, e.g., $1024 \\times$ 1024. The StyleSwin, without complex training strategies, excels over StyleGAN on CelebA-HQ 1024, and achieves on-par performance on FFHQ-1024, proving the promise of using transformers for high-resolution image generation. The code and pretrained models are available at https://github.com/microsoft/StyleSwin.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/5553f9508dd1056ecc20c5b1f367e9a07e2c7e81.pdf",
      "citation_key": "zhang2021fje",
      "metadata": {
        "title": "StyleSwin: Transformer-based GAN for High-resolution Image Generation",
        "authors": [
          "Bo Zhang",
          "Shuyang Gu",
          "Bo Zhang",
          "Jianmin Bao",
          "Dong Chen",
          "Fang Wen",
          "Yong Wang",
          "B. Guo"
        ],
        "published_date": "2021",
        "abstract": "Despite the tantalizing success in a broad of vision tasks, transformers have not yet demonstrated on-par ability as ConvNets in high-resolution image generative modeling. In this paper, we seek to explore using pure transformers to build a generative adversarial network for high-resolution image synthesis. To this end, we believe that local attention is crucial to strike the balance between computational efficiency and modeling capacity. Hence, the proposed generator adopts Swin transformer in a style-based architecture. To achieve a larger receptive field, we propose double attention which simultaneously leverages the context of the local and the shifted windows, leading to improved generation quality. Moreover, we show that offering the knowledge of the absolute position that has been lost in window-based transformers greatly benefits the generation quality. The proposed StyleSwin is scalable to high resolutions, with both the coarse geometry and fine structures benefit from the strong expressivity of transformers. However, blocking artifacts occur during high-resolution synthesis because performing the local attention in a block-wise manner may break the spatial coherency. To solve this, we empirically investigate various solutions, among which we find that employing a wavelet discriminator to examine the spectral discrepancy effectively suppresses the artifacts. Extensive experiments show the superiority over prior transformer-based GANs, especially on high resolutions, e.g., $1024 \\times$ 1024. The StyleSwin, without complex training strategies, excels over StyleGAN on CelebA-HQ 1024, and achieves on-par performance on FFHQ-1024, proving the promise of using transformers for high-resolution image generation. The code and pretrained models are available at https://github.com/microsoft/StyleSwin.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/5553f9508dd1056ecc20c5b1f367e9a07e2c7e81.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 249,
        "score": 62.25,
        "summary": "Despite the tantalizing success in a broad of vision tasks, transformers have not yet demonstrated on-par ability as ConvNets in high-resolution image generative modeling. In this paper, we seek to explore using pure transformers to build a generative adversarial network for high-resolution image synthesis. To this end, we believe that local attention is crucial to strike the balance between computational efficiency and modeling capacity. Hence, the proposed generator adopts Swin transformer in a style-based architecture. To achieve a larger receptive field, we propose double attention which simultaneously leverages the context of the local and the shifted windows, leading to improved generation quality. Moreover, we show that offering the knowledge of the absolute position that has been lost in window-based transformers greatly benefits the generation quality. The proposed StyleSwin is scalable to high resolutions, with both the coarse geometry and fine structures benefit from the strong expressivity of transformers. However, blocking artifacts occur during high-resolution synthesis because performing the local attention in a block-wise manner may break the spatial coherency. To solve this, we empirically investigate various solutions, among which we find that employing a wavelet discriminator to examine the spectral discrepancy effectively suppresses the artifacts. Extensive experiments show the superiority over prior transformer-based GANs, especially on high resolutions, e.g., $1024 \\times$ 1024. The StyleSwin, without complex training strategies, excels over StyleGAN on CelebA-HQ 1024, and achieves on-par performance on FFHQ-1024, proving the promise of using transformers for high-resolution image generation. The code and pretrained models are available at https://github.com/microsoft/StyleSwin.",
        "keywords": []
      },
      "file_name": "5553f9508dd1056ecc20c5b1f367e9a07e2c7e81.pdf"
    },
    {
      "success": true,
      "doc_id": "069a467588a518d4d92a9a388ca3bf58",
      "summary": "Brain tumor semantic segmentation is a critical medical image processing work, which aids clinicians in diagnosing patients and determining the extent of lesions. Convolutional neural networks (CNNs) have demonstrated exceptional performance in computer vision tasks in recent years. For 3D medical image tasks, deep convolutional neural networks based on an encoderdecoder structure and skip-connection have been frequently used. However, CNNs have the drawback of being unable to learn global and remote semantic information well. On the other hand, the transformer has recently found success in natural language processing and computer vision as a result of its usage of a self-attention mechanism for global information modeling. For demanding prediction tasks, such as 3D medical picture segmentation, local and global characteristics are critical. We propose SwinBTS, a new 3D medical picture segmentation approach, which combines a transformer, convolutional neural network, and encoderdecoder structure to define the 3D brain tumor semantic segmentation job as a sequence-to-sequence prediction challenge in this research. To extract contextual data, the 3D Swin Transformer is utilized as the networks encoder and decoder, and convolutional operations are employed for upsampling and downsampling. Finally, we achieve segmentation results using an improved Transformer module that we built for increasing detail feature extraction. Extensive experimental results on the BraTS 2019, BraTS 2020, and BraTS 2021 datasets reveal that SwinBTS outperforms state-of-the-art 3D algorithms for brain tumor segmentation on 3D MRI scanned images.",
      "intriguing_abstract": "Brain tumor semantic segmentation is a critical medical image processing work, which aids clinicians in diagnosing patients and determining the extent of lesions. Convolutional neural networks (CNNs) have demonstrated exceptional performance in computer vision tasks in recent years. For 3D medical image tasks, deep convolutional neural networks based on an encoderdecoder structure and skip-connection have been frequently used. However, CNNs have the drawback of being unable to learn global and remote semantic information well. On the other hand, the transformer has recently found success in natural language processing and computer vision as a result of its usage of a self-attention mechanism for global information modeling. For demanding prediction tasks, such as 3D medical picture segmentation, local and global characteristics are critical. We propose SwinBTS, a new 3D medical picture segmentation approach, which combines a transformer, convolutional neural network, and encoderdecoder structure to define the 3D brain tumor semantic segmentation job as a sequence-to-sequence prediction challenge in this research. To extract contextual data, the 3D Swin Transformer is utilized as the networks encoder and decoder, and convolutional operations are employed for upsampling and downsampling. Finally, we achieve segmentation results using an improved Transformer module that we built for increasing detail feature extraction. Extensive experimental results on the BraTS 2019, BraTS 2020, and BraTS 2021 datasets reveal that SwinBTS outperforms state-of-the-art 3D algorithms for brain tumor segmentation on 3D MRI scanned images.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/226fcbe55235d873bedb2fcf5b981bd5ec860d4f.pdf",
      "citation_key": "jiang2022zcn",
      "metadata": {
        "title": "SwinBTS: A Method for 3D Multimodal Brain Tumor Segmentation Using Swin Transformer",
        "authors": [
          "Yun Jiang",
          "Yuan Zhang",
          "Xinyi Lin",
          "Jinkun Dong",
          "Tongtong Cheng",
          "Jing Liang"
        ],
        "published_date": "2022",
        "abstract": "Brain tumor semantic segmentation is a critical medical image processing work, which aids clinicians in diagnosing patients and determining the extent of lesions. Convolutional neural networks (CNNs) have demonstrated exceptional performance in computer vision tasks in recent years. For 3D medical image tasks, deep convolutional neural networks based on an encoderdecoder structure and skip-connection have been frequently used. However, CNNs have the drawback of being unable to learn global and remote semantic information well. On the other hand, the transformer has recently found success in natural language processing and computer vision as a result of its usage of a self-attention mechanism for global information modeling. For demanding prediction tasks, such as 3D medical picture segmentation, local and global characteristics are critical. We propose SwinBTS, a new 3D medical picture segmentation approach, which combines a transformer, convolutional neural network, and encoderdecoder structure to define the 3D brain tumor semantic segmentation job as a sequence-to-sequence prediction challenge in this research. To extract contextual data, the 3D Swin Transformer is utilized as the networks encoder and decoder, and convolutional operations are employed for upsampling and downsampling. Finally, we achieve segmentation results using an improved Transformer module that we built for increasing detail feature extraction. Extensive experimental results on the BraTS 2019, BraTS 2020, and BraTS 2021 datasets reveal that SwinBTS outperforms state-of-the-art 3D algorithms for brain tumor segmentation on 3D MRI scanned images.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/226fcbe55235d873bedb2fcf5b981bd5ec860d4f.pdf",
        "venue": "Brain Science",
        "citationCount": 181,
        "score": 60.33333333333333,
        "summary": "Brain tumor semantic segmentation is a critical medical image processing work, which aids clinicians in diagnosing patients and determining the extent of lesions. Convolutional neural networks (CNNs) have demonstrated exceptional performance in computer vision tasks in recent years. For 3D medical image tasks, deep convolutional neural networks based on an encoderdecoder structure and skip-connection have been frequently used. However, CNNs have the drawback of being unable to learn global and remote semantic information well. On the other hand, the transformer has recently found success in natural language processing and computer vision as a result of its usage of a self-attention mechanism for global information modeling. For demanding prediction tasks, such as 3D medical picture segmentation, local and global characteristics are critical. We propose SwinBTS, a new 3D medical picture segmentation approach, which combines a transformer, convolutional neural network, and encoderdecoder structure to define the 3D brain tumor semantic segmentation job as a sequence-to-sequence prediction challenge in this research. To extract contextual data, the 3D Swin Transformer is utilized as the networks encoder and decoder, and convolutional operations are employed for upsampling and downsampling. Finally, we achieve segmentation results using an improved Transformer module that we built for increasing detail feature extraction. Extensive experimental results on the BraTS 2019, BraTS 2020, and BraTS 2021 datasets reveal that SwinBTS outperforms state-of-the-art 3D algorithms for brain tumor segmentation on 3D MRI scanned images.",
        "keywords": []
      },
      "file_name": "226fcbe55235d873bedb2fcf5b981bd5ec860d4f.pdf"
    },
    {
      "success": true,
      "doc_id": "763bc9b0c92fff9022504759cfe279fb",
      "summary": "Renal failure, a public health concern, and the scarcity of nephrologists around the globe have necessitated the development of an AI-based system to auto-diagnose kidney diseases. This research deals with the three major renal diseases categories: kidney stones, cysts, and tumors, and gathered and annotated a total of 12,446 CT whole abdomen and urogram images in order to construct an AI-based kidney diseases diagnostic system and contribute to the AI communitys research scope e.g., modeling digital-twin of renal functions. The collected images were exposed to exploratory data analysis, which revealed that the images from all of the classes had the same type of mean color distribution. Furthermore, six machine learning models were built, three of which are based on the state-of-the-art variants of the Vision transformers EANet, CCT, and Swin transformers, while the other three are based on well-known deep learning models Resnet, VGG16, and Inception v3, which were adjusted in the last layers. While the VGG16 and CCT models performed admirably, the swin transformer outperformed all of them in terms of accuracy, with an accuracy of 99.30 percent. The F1 score and precision and recall comparison reveal that the Swin transformer outperforms all other models and that it is the quickest to train. The study also revealed the blackbox of the VGG16, Resnet50, and Inception models, demonstrating that VGG16 is superior than Resnet50 and Inceptionv3 in terms of monitoring the necessary anatomy abnormalities. We believe that the superior accuracy of our Swin transformer-based model and the VGG16-based model can both be useful in diagnosing kidney tumors, cysts, and stones.",
      "intriguing_abstract": "Renal failure, a public health concern, and the scarcity of nephrologists around the globe have necessitated the development of an AI-based system to auto-diagnose kidney diseases. This research deals with the three major renal diseases categories: kidney stones, cysts, and tumors, and gathered and annotated a total of 12,446 CT whole abdomen and urogram images in order to construct an AI-based kidney diseases diagnostic system and contribute to the AI communitys research scope e.g., modeling digital-twin of renal functions. The collected images were exposed to exploratory data analysis, which revealed that the images from all of the classes had the same type of mean color distribution. Furthermore, six machine learning models were built, three of which are based on the state-of-the-art variants of the Vision transformers EANet, CCT, and Swin transformers, while the other three are based on well-known deep learning models Resnet, VGG16, and Inception v3, which were adjusted in the last layers. While the VGG16 and CCT models performed admirably, the swin transformer outperformed all of them in terms of accuracy, with an accuracy of 99.30 percent. The F1 score and precision and recall comparison reveal that the Swin transformer outperforms all other models and that it is the quickest to train. The study also revealed the blackbox of the VGG16, Resnet50, and Inception models, demonstrating that VGG16 is superior than Resnet50 and Inceptionv3 in terms of monitoring the necessary anatomy abnormalities. We believe that the superior accuracy of our Swin transformer-based model and the VGG16-based model can both be useful in diagnosing kidney tumors, cysts, and stones.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/5c1b7e400ba8b3c3b04522f7bb3af17ad68d0585.pdf",
      "citation_key": "islam2022iss",
      "metadata": {
        "title": "Vision transformer and explainable transfer learning models for auto detection of kidney cyst, stone and tumor from CT-radiography",
        "authors": [
          "Md. Nazmul Islam",
          "Madina Hasan",
          "Md. Kabir Hossain",
          "Md. Golam Rabiul Alam",
          "Md. Zia Uddin",
          "A. Soylu"
        ],
        "published_date": "2022",
        "abstract": "Renal failure, a public health concern, and the scarcity of nephrologists around the globe have necessitated the development of an AI-based system to auto-diagnose kidney diseases. This research deals with the three major renal diseases categories: kidney stones, cysts, and tumors, and gathered and annotated a total of 12,446 CT whole abdomen and urogram images in order to construct an AI-based kidney diseases diagnostic system and contribute to the AI communitys research scope e.g., modeling digital-twin of renal functions. The collected images were exposed to exploratory data analysis, which revealed that the images from all of the classes had the same type of mean color distribution. Furthermore, six machine learning models were built, three of which are based on the state-of-the-art variants of the Vision transformers EANet, CCT, and Swin transformers, while the other three are based on well-known deep learning models Resnet, VGG16, and Inception v3, which were adjusted in the last layers. While the VGG16 and CCT models performed admirably, the swin transformer outperformed all of them in terms of accuracy, with an accuracy of 99.30 percent. The F1 score and precision and recall comparison reveal that the Swin transformer outperforms all other models and that it is the quickest to train. The study also revealed the blackbox of the VGG16, Resnet50, and Inception models, demonstrating that VGG16 is superior than Resnet50 and Inceptionv3 in terms of monitoring the necessary anatomy abnormalities. We believe that the superior accuracy of our Swin transformer-based model and the VGG16-based model can both be useful in diagnosing kidney tumors, cysts, and stones.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/5c1b7e400ba8b3c3b04522f7bb3af17ad68d0585.pdf",
        "venue": "Scientific Reports",
        "citationCount": 177,
        "score": 59.0,
        "summary": "Renal failure, a public health concern, and the scarcity of nephrologists around the globe have necessitated the development of an AI-based system to auto-diagnose kidney diseases. This research deals with the three major renal diseases categories: kidney stones, cysts, and tumors, and gathered and annotated a total of 12,446 CT whole abdomen and urogram images in order to construct an AI-based kidney diseases diagnostic system and contribute to the AI communitys research scope e.g., modeling digital-twin of renal functions. The collected images were exposed to exploratory data analysis, which revealed that the images from all of the classes had the same type of mean color distribution. Furthermore, six machine learning models were built, three of which are based on the state-of-the-art variants of the Vision transformers EANet, CCT, and Swin transformers, while the other three are based on well-known deep learning models Resnet, VGG16, and Inception v3, which were adjusted in the last layers. While the VGG16 and CCT models performed admirably, the swin transformer outperformed all of them in terms of accuracy, with an accuracy of 99.30 percent. The F1 score and precision and recall comparison reveal that the Swin transformer outperforms all other models and that it is the quickest to train. The study also revealed the blackbox of the VGG16, Resnet50, and Inception models, demonstrating that VGG16 is superior than Resnet50 and Inceptionv3 in terms of monitoring the necessary anatomy abnormalities. We believe that the superior accuracy of our Swin transformer-based model and the VGG16-based model can both be useful in diagnosing kidney tumors, cysts, and stones.",
        "keywords": []
      },
      "file_name": "5c1b7e400ba8b3c3b04522f7bb3af17ad68d0585.pdf"
    },
    {
      "success": true,
      "doc_id": "c855cec2e5f42ab52d0be6152a375c6b",
      "summary": "Due to the complex attention mechanisms and model design, most existing vision Transformers (ViTs) can not perform as efficiently as convolutional neural networks (CNNs) in realistic industrial deployment scenarios, e.g. TensorRT and CoreML. This poses a distinct challenge: Can a visual neural network be designed to infer as fast as CNNs and perform as powerful as ViTs? Recent works have tried to design CNN-Transformer hybrid architectures to address this issue, yet the overall performance of these works is far away from satisfactory. To end these, we propose a next generation vision Transformer for efficient deployment in realistic industrial scenarios, namely Next-ViT, which dominates both CNNs and ViTs from the perspective of latency/accuracy trade-off. In this work, the Next Convolution Block (NCB) and Next Transformer Block (NTB) are respectively developed to capture local and global information with deployment-friendly mechanisms. Then, Next Hybrid Strategy (NHS) is designed to stack NCB and NTB in an efficient hybrid paradigm, which boosts performance in various downstream tasks. Extensive experiments show that Next-ViT significantly outperforms existing CNNs, ViTs and CNN-Transformer hybrid architectures with respect to the latency/accuracy trade-off across various vision tasks. On TensorRT, Next-ViT surpasses ResNet by 5.5 mAP (from 40.4 to 45.9) on COCO detection and 7.7% mIoU (from 38.8% to 46.5%) on ADE20K segmentation under similar latency. Meanwhile, it achieves comparable performance with CSWin, while the inference speed is accelerated by 3.6x. On CoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on COCO detection and 3.5% mIoU (from 45.1% to 48.6%) on ADE20K segmentation under similar latency. Our code and models are made public at: https://github.com/bytedance/Next-ViT",
      "intriguing_abstract": "Due to the complex attention mechanisms and model design, most existing vision Transformers (ViTs) can not perform as efficiently as convolutional neural networks (CNNs) in realistic industrial deployment scenarios, e.g. TensorRT and CoreML. This poses a distinct challenge: Can a visual neural network be designed to infer as fast as CNNs and perform as powerful as ViTs? Recent works have tried to design CNN-Transformer hybrid architectures to address this issue, yet the overall performance of these works is far away from satisfactory. To end these, we propose a next generation vision Transformer for efficient deployment in realistic industrial scenarios, namely Next-ViT, which dominates both CNNs and ViTs from the perspective of latency/accuracy trade-off. In this work, the Next Convolution Block (NCB) and Next Transformer Block (NTB) are respectively developed to capture local and global information with deployment-friendly mechanisms. Then, Next Hybrid Strategy (NHS) is designed to stack NCB and NTB in an efficient hybrid paradigm, which boosts performance in various downstream tasks. Extensive experiments show that Next-ViT significantly outperforms existing CNNs, ViTs and CNN-Transformer hybrid architectures with respect to the latency/accuracy trade-off across various vision tasks. On TensorRT, Next-ViT surpasses ResNet by 5.5 mAP (from 40.4 to 45.9) on COCO detection and 7.7% mIoU (from 38.8% to 46.5%) on ADE20K segmentation under similar latency. Meanwhile, it achieves comparable performance with CSWin, while the inference speed is accelerated by 3.6x. On CoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on COCO detection and 3.5% mIoU (from 45.1% to 48.6%) on ADE20K segmentation under similar latency. Our code and models are made public at: https://github.com/bytedance/Next-ViT",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/a4b728dbbf5afdc231afb95ad4e5c2ececdefc48.pdf",
      "citation_key": "li2022a4u",
      "metadata": {
        "title": "Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios",
        "authors": [
          "Jiashi Li",
          "Xin Xia",
          "W. Li",
          "Huixia Li",
          "Xing Wang",
          "Xuefeng Xiao",
          "Rui Wang",
          "Minghang Zheng",
          "Xin Pan"
        ],
        "published_date": "2022",
        "abstract": "Due to the complex attention mechanisms and model design, most existing vision Transformers (ViTs) can not perform as efficiently as convolutional neural networks (CNNs) in realistic industrial deployment scenarios, e.g. TensorRT and CoreML. This poses a distinct challenge: Can a visual neural network be designed to infer as fast as CNNs and perform as powerful as ViTs? Recent works have tried to design CNN-Transformer hybrid architectures to address this issue, yet the overall performance of these works is far away from satisfactory. To end these, we propose a next generation vision Transformer for efficient deployment in realistic industrial scenarios, namely Next-ViT, which dominates both CNNs and ViTs from the perspective of latency/accuracy trade-off. In this work, the Next Convolution Block (NCB) and Next Transformer Block (NTB) are respectively developed to capture local and global information with deployment-friendly mechanisms. Then, Next Hybrid Strategy (NHS) is designed to stack NCB and NTB in an efficient hybrid paradigm, which boosts performance in various downstream tasks. Extensive experiments show that Next-ViT significantly outperforms existing CNNs, ViTs and CNN-Transformer hybrid architectures with respect to the latency/accuracy trade-off across various vision tasks. On TensorRT, Next-ViT surpasses ResNet by 5.5 mAP (from 40.4 to 45.9) on COCO detection and 7.7% mIoU (from 38.8% to 46.5%) on ADE20K segmentation under similar latency. Meanwhile, it achieves comparable performance with CSWin, while the inference speed is accelerated by 3.6x. On CoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on COCO detection and 3.5% mIoU (from 45.1% to 48.6%) on ADE20K segmentation under similar latency. Our code and models are made public at: https://github.com/bytedance/Next-ViT",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a4b728dbbf5afdc231afb95ad4e5c2ececdefc48.pdf",
        "venue": "arXiv.org",
        "citationCount": 168,
        "score": 56.0,
        "summary": "Due to the complex attention mechanisms and model design, most existing vision Transformers (ViTs) can not perform as efficiently as convolutional neural networks (CNNs) in realistic industrial deployment scenarios, e.g. TensorRT and CoreML. This poses a distinct challenge: Can a visual neural network be designed to infer as fast as CNNs and perform as powerful as ViTs? Recent works have tried to design CNN-Transformer hybrid architectures to address this issue, yet the overall performance of these works is far away from satisfactory. To end these, we propose a next generation vision Transformer for efficient deployment in realistic industrial scenarios, namely Next-ViT, which dominates both CNNs and ViTs from the perspective of latency/accuracy trade-off. In this work, the Next Convolution Block (NCB) and Next Transformer Block (NTB) are respectively developed to capture local and global information with deployment-friendly mechanisms. Then, Next Hybrid Strategy (NHS) is designed to stack NCB and NTB in an efficient hybrid paradigm, which boosts performance in various downstream tasks. Extensive experiments show that Next-ViT significantly outperforms existing CNNs, ViTs and CNN-Transformer hybrid architectures with respect to the latency/accuracy trade-off across various vision tasks. On TensorRT, Next-ViT surpasses ResNet by 5.5 mAP (from 40.4 to 45.9) on COCO detection and 7.7% mIoU (from 38.8% to 46.5%) on ADE20K segmentation under similar latency. Meanwhile, it achieves comparable performance with CSWin, while the inference speed is accelerated by 3.6x. On CoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on COCO detection and 3.5% mIoU (from 45.1% to 48.6%) on ADE20K segmentation under similar latency. Our code and models are made public at: https://github.com/bytedance/Next-ViT",
        "keywords": []
      },
      "file_name": "a4b728dbbf5afdc231afb95ad4e5c2ececdefc48.pdf"
    },
    {
      "success": true,
      "doc_id": "02ba81b7e46795be8b0f3d79866ad901",
      "summary": "Multi-scale Vision Transformer (ViT) has emerged as a powerful backbone for computer vision tasks, while the self-attention computation in Transformer scales quadratically w.r.t. the input patch number. Thus, existing solutions commonly employ down-sampling operations (e.g., average pooling) over keys/values to dramatically reduce the computational cost. In this work, we argue that such over-aggressive down-sampling design is not invertible and inevitably causes information dropping especially for high-frequency components in objects (e.g., texture details). Motivated by the wavelet theory, we construct a new Wavelet Vision Transformer (\\textbf{Wave-ViT}) that formulates the invertible down-sampling with wavelet transforms and self-attention learning in a unified way. This proposal enables self-attention learning with lossless down-sampling over keys/values, facilitating the pursuing of a better efficiency-vs-accuracy trade-off. Furthermore, inverse wavelet transforms are leveraged to strengthen self-attention outputs by aggregating local contexts with enlarged receptive field. We validate the superiority of Wave-ViT through extensive experiments over multiple vision tasks (e.g., image recognition, object detection and instance segmentation). Its performances surpass state-of-the-art ViT backbones with comparable FLOPs. Source code is available at \\url{https://github.com/YehLi/ImageNetModel}.",
      "intriguing_abstract": "Multi-scale Vision Transformer (ViT) has emerged as a powerful backbone for computer vision tasks, while the self-attention computation in Transformer scales quadratically w.r.t. the input patch number. Thus, existing solutions commonly employ down-sampling operations (e.g., average pooling) over keys/values to dramatically reduce the computational cost. In this work, we argue that such over-aggressive down-sampling design is not invertible and inevitably causes information dropping especially for high-frequency components in objects (e.g., texture details). Motivated by the wavelet theory, we construct a new Wavelet Vision Transformer (\\textbf{Wave-ViT}) that formulates the invertible down-sampling with wavelet transforms and self-attention learning in a unified way. This proposal enables self-attention learning with lossless down-sampling over keys/values, facilitating the pursuing of a better efficiency-vs-accuracy trade-off. Furthermore, inverse wavelet transforms are leveraged to strengthen self-attention outputs by aggregating local contexts with enlarged receptive field. We validate the superiority of Wave-ViT through extensive experiments over multiple vision tasks (e.g., image recognition, object detection and instance segmentation). Its performances surpass state-of-the-art ViT backbones with comparable FLOPs. Source code is available at \\url{https://github.com/YehLi/ImageNetModel}.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/0b8e8760d7dd64c7439019aeb3b6ac55dd5075d4.pdf",
      "citation_key": "yao202245i",
      "metadata": {
        "title": "Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning",
        "authors": [
          "Ting Yao",
          "Yingwei Pan",
          "Yehao Li",
          "C. Ngo",
          "Tao Mei"
        ],
        "published_date": "2022",
        "abstract": "Multi-scale Vision Transformer (ViT) has emerged as a powerful backbone for computer vision tasks, while the self-attention computation in Transformer scales quadratically w.r.t. the input patch number. Thus, existing solutions commonly employ down-sampling operations (e.g., average pooling) over keys/values to dramatically reduce the computational cost. In this work, we argue that such over-aggressive down-sampling design is not invertible and inevitably causes information dropping especially for high-frequency components in objects (e.g., texture details). Motivated by the wavelet theory, we construct a new Wavelet Vision Transformer (\\textbf{Wave-ViT}) that formulates the invertible down-sampling with wavelet transforms and self-attention learning in a unified way. This proposal enables self-attention learning with lossless down-sampling over keys/values, facilitating the pursuing of a better efficiency-vs-accuracy trade-off. Furthermore, inverse wavelet transforms are leveraged to strengthen self-attention outputs by aggregating local contexts with enlarged receptive field. We validate the superiority of Wave-ViT through extensive experiments over multiple vision tasks (e.g., image recognition, object detection and instance segmentation). Its performances surpass state-of-the-art ViT backbones with comparable FLOPs. Source code is available at \\url{https://github.com/YehLi/ImageNetModel}.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0b8e8760d7dd64c7439019aeb3b6ac55dd5075d4.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 166,
        "score": 55.33333333333333,
        "summary": "Multi-scale Vision Transformer (ViT) has emerged as a powerful backbone for computer vision tasks, while the self-attention computation in Transformer scales quadratically w.r.t. the input patch number. Thus, existing solutions commonly employ down-sampling operations (e.g., average pooling) over keys/values to dramatically reduce the computational cost. In this work, we argue that such over-aggressive down-sampling design is not invertible and inevitably causes information dropping especially for high-frequency components in objects (e.g., texture details). Motivated by the wavelet theory, we construct a new Wavelet Vision Transformer (\\textbf{Wave-ViT}) that formulates the invertible down-sampling with wavelet transforms and self-attention learning in a unified way. This proposal enables self-attention learning with lossless down-sampling over keys/values, facilitating the pursuing of a better efficiency-vs-accuracy trade-off. Furthermore, inverse wavelet transforms are leveraged to strengthen self-attention outputs by aggregating local contexts with enlarged receptive field. We validate the superiority of Wave-ViT through extensive experiments over multiple vision tasks (e.g., image recognition, object detection and instance segmentation). Its performances surpass state-of-the-art ViT backbones with comparable FLOPs. Source code is available at \\url{https://github.com/YehLi/ImageNetModel}.",
        "keywords": []
      },
      "file_name": "0b8e8760d7dd64c7439019aeb3b6ac55dd5075d4.pdf"
    },
    {
      "success": true,
      "doc_id": "e4b2bbb7d1f60fde7962f929b2982cdd",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the critical technical problem of automated, real-time plant disease classification \\cite{borhani2022w8x}.\n    *   This problem is important due to the significant agricultural product loss caused by plant diseases, and the need for early diagnosis to enable preventive measures.\n    *   It is challenging because farmers often lack the expertise to accurately diagnose diseases, especially those with tiny features. Furthermore, existing deep learning models, particularly large pre-trained Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs), are often computationally heavy, leading to slow prediction times which are unsuitable for real-time applications, and require extensive, domain-specific datasets for optimal performance \\cite{borhani2022w8x}.\n\n*   **Related Work & Positioning**\n    *   Existing approaches in plant disease classification primarily utilize CNNs (e.g., VGG16, ResNet-50, EfficientNet-B0, AlexNet, SqueezeNet) and, more recently, ViTs \\cite{borhani2022w8x}. Some works incorporate attention mechanisms (e.g., CBAM) or image preprocessing techniques like segmentation.\n    *   Limitations of previous solutions include:\n        *   Many studies focus on specific plants or diseases, limiting generalizability.\n        *   Datasets often feature simplified backgrounds, which may not reflect real-world field conditions.\n        *   Heavy pre-designed architectures (like full ViT models) are computationally expensive and slow, making them impractical for real-time applications.\n        *   Previous ViT applications in agriculture often used the main, unmodified ViT versions without studying prediction speed, which is crucial for real-time deployment \\cite{borhani2022w8x}.\n    *   This work positions itself by proposing *lightweight* ViT-based and *hybrid* CNN-ViT models specifically designed for *real-time* multi-plant disease classification, explicitly addressing the speed-accuracy trade-off and evaluating performance across diverse datasets \\cite{borhani2022w8x}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method involves developing lightweight deep learning models based on custom-designed Vision Transformer (ViT) blocks, Convolutional Neural Network (CNN) blocks, and novel hybrid architectures combining both \\cite{borhani2022w8x}.\n    *   **Custom Building Blocks**:\n        *   **CNN Block**: Consists of two 3x3 convolutional layers (without padding/activation), followed by a Leaky ReLU activation and a 2x2 max pooling layer. This design is inspired by VGG for efficiency.\n        *   **Transformer Block**: Incorporates layer normalization, a multi-head attention layer (four heads, projection dimension 64), skip connections, and a multi-layer perceptron (MLP) with GELU activation.\n    *   **Novel Architectures**: Eight distinct models are investigated: CNN-only (1 or 2 blocks), Transformer-only (1 or 2 blocks), and four hybrid models that combine CNN and Transformer blocks in various sequences (e.g., CNN then Transformer, Transformer then CNN) \\cite{borhani2022w8x}.\n    *   The approach is novel due to its focus on *lightweight* design for real-time performance, the *custom-built* and simplified CNN and Transformer blocks, and the exploration of *hybrid* CNN-ViT combinations to achieve an optimal balance between accuracy and prediction speed.\n\n*   **Key Technical Contributions**\n    *   Development of novel lightweight ViT-based architectures tailored for efficient plant disease classification \\cite{borhani2022w8x}.\n    *   Introduction and empirical validation of hybrid CNN-ViT models, designed to leverage the feature extraction capabilities of CNNs and the global context understanding of ViTs.\n    *   Design of custom, simplified CNN and Transformer building blocks that contribute to the lightweight nature of the proposed models.\n    *   A systematic investigation into the trade-off between accuracy gains from attention mechanisms and their impact on prediction speed, demonstrating how hybrid models can effectively mitigate speed deceleration \\cite{borhani2022w8x}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: All eight proposed model structures were trained and evaluated on three distinct datasets with varying characteristics and sizes \\cite{borhani2022w8x}. The effect of different image resolutions (50x50, 100x100, 200x200) on model performance and speed was also studied.\n    *   **Datasets**:\n        *   **Wheat Rust Classification Dataset (WRCD)**: A small dataset with ~3.7k images across 3 classes (healthy, yellow rust, brown rust), uniformly distributed.\n        *   **Rice Leaf Disease Dataset (RLDD)**: A very small dataset of 120 images across 3 classes (Bacterial Leaf Disease, Brown Spot, Leaf Smut), challenging due to limited samples and symptom similarity.\n        *   **Plant Village**: A large-scale dataset containing 54,306 images across 38 classes (14 crop species and 26 diseases), characterized by specific image conditions (simple backgrounds) and imbalanced class distribution.\n    *   **Key Performance Metrics**: Models were evaluated using accuracy, F1-score, recall, precision, Giga floating-point operations per second (GFlops) for computational cost, and prediction speed \\cite{borhani2022w8x}.\n    *   **Comparison Results (from abstract/intro)**: The study concluded that while attention blocks generally increase accuracy, they also decelerate prediction speed. Crucially, combining attention blocks with CNN blocks in hybrid architectures was found to compensate for this speed reduction, offering a better balance between high accuracy and fast prediction \\cite{borhani2022w8x}. High accuracies were achieved even on images with occluded disease parts.\n\n*   **Limitations & Scope**\n    *   The Plant Village dataset, while large, consists of images taken under specific conditions (e.g., simple backgrounds), which might not fully represent the complexity of real-world agricultural field scenarios \\cite{borhani2022w8x}.\n    *   The paper implicitly suggests that even lightweight models might be \"extreme\" for certain situations, indicating that the optimal model choice depends on specific application constraints and available computational resources.\n    *   The scope of applicability is primarily focused on image-based classification of plant diseases under controlled or semi-controlled conditions, with an emphasis on real-time inference.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art in automated plant disease classification by demonstrating the effectiveness of *lightweight* and *hybrid* deep learning architectures for *real-time* applications \\cite{borhani2022w8x}.\n    *   It provides a practical and efficient solution for farmers and agricultural systems, enabling timely disease diagnosis and intervention.\n    *   The research offers valuable insights into the performance characteristics and trade-offs between CNNs and ViTs, particularly highlighting how hybrid models can achieve a superior balance of accuracy and computational efficiency.\n    *   The findings pave the way for future research into optimized, resource-efficient deep learning models suitable for deployment on edge devices or mobile platforms in agricultural settings.",
      "intriguing_abstract": "Plant diseases devastate agricultural yields globally, necessitating rapid and accurate diagnosis to enable timely intervention. Current deep learning solutions, particularly large Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs), often suffer from computational intensity and slow inference, rendering them impractical for real-time field deployment. This paper introduces a novel suite of *lightweight* deep learning architectures, comprising custom-designed CNN and Transformer blocks, alongside innovative *hybrid CNN-ViT models*, specifically engineered for *real-time plant disease classification*.\n\nWe systematically investigate the intricate trade-off between model accuracy and *prediction speed*, a critical factor for practical applications. Our custom-built blocks and hybrid configurations demonstrate that while *attention mechanisms* enhance accuracy, their computational cost can be effectively mitigated by strategic integration with CNN layers. Experimental validation across diverse datasets reveals that our *hybrid architectures* achieve a superior balance, delivering high classification accuracy with significantly improved *computational efficiency* and rapid inference speeds. This work offers a crucial advancement towards deployable, resource-efficient solutions for automated agricultural disease management, paving the way for intelligent farming systems.",
      "keywords": [
        "Automated real-time plant disease classification",
        "Lightweight Vision Transformers (ViTs)",
        "Hybrid CNN-ViT architectures",
        "Prediction speed",
        "Speed-accuracy trade-off",
        "Custom deep learning blocks",
        "Multi-plant disease classification",
        "Computational efficiency",
        "Attention mechanisms",
        "Agricultural applications",
        "Early disease diagnosis"
      ],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/17534840dc6016229a577a66f108a1564b8a0131.pdf",
      "citation_key": "borhani2022w8x",
      "metadata": {
        "title": "A deep learning based approach for automated plant disease classification using vision transformer",
        "authors": [
          "Y. Borhani",
          "Javad Khoramdel",
          "E. Najafi"
        ],
        "published_date": "2022",
        "abstract": "Plant disease can diminish a considerable portion of the agricultural products on each farm. The main goal of this work is to provide visual information for the farmers to enable them to take the necessary preventive measures. A lightweight deep learning approach is proposed based on the Vision Transformer (ViT) for real-time automated plant disease classification. In addition to the ViT, the classical convolutional neural network (CNN) methods and the combination of CNN and ViT have been implemented for the plant disease classification. The models have been trained and evaluated on multiple datasets. Based on the comparison between the obtained results, it is concluded that although attention blocks increase the accuracy, they decelerate the prediction. Combining attention blocks with CNN blocks can compensate for the speed.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/17534840dc6016229a577a66f108a1564b8a0131.pdf",
        "venue": "Scientific Reports",
        "citationCount": 165,
        "score": 55.0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the critical technical problem of automated, real-time plant disease classification \\cite{borhani2022w8x}.\n    *   This problem is important due to the significant agricultural product loss caused by plant diseases, and the need for early diagnosis to enable preventive measures.\n    *   It is challenging because farmers often lack the expertise to accurately diagnose diseases, especially those with tiny features. Furthermore, existing deep learning models, particularly large pre-trained Vision Transformers (ViTs) and Convolutional Neural Networks (CNNs), are often computationally heavy, leading to slow prediction times which are unsuitable for real-time applications, and require extensive, domain-specific datasets for optimal performance \\cite{borhani2022w8x}.\n\n*   **Related Work & Positioning**\n    *   Existing approaches in plant disease classification primarily utilize CNNs (e.g., VGG16, ResNet-50, EfficientNet-B0, AlexNet, SqueezeNet) and, more recently, ViTs \\cite{borhani2022w8x}. Some works incorporate attention mechanisms (e.g., CBAM) or image preprocessing techniques like segmentation.\n    *   Limitations of previous solutions include:\n        *   Many studies focus on specific plants or diseases, limiting generalizability.\n        *   Datasets often feature simplified backgrounds, which may not reflect real-world field conditions.\n        *   Heavy pre-designed architectures (like full ViT models) are computationally expensive and slow, making them impractical for real-time applications.\n        *   Previous ViT applications in agriculture often used the main, unmodified ViT versions without studying prediction speed, which is crucial for real-time deployment \\cite{borhani2022w8x}.\n    *   This work positions itself by proposing *lightweight* ViT-based and *hybrid* CNN-ViT models specifically designed for *real-time* multi-plant disease classification, explicitly addressing the speed-accuracy trade-off and evaluating performance across diverse datasets \\cite{borhani2022w8x}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method involves developing lightweight deep learning models based on custom-designed Vision Transformer (ViT) blocks, Convolutional Neural Network (CNN) blocks, and novel hybrid architectures combining both \\cite{borhani2022w8x}.\n    *   **Custom Building Blocks**:\n        *   **CNN Block**: Consists of two 3x3 convolutional layers (without padding/activation), followed by a Leaky ReLU activation and a 2x2 max pooling layer. This design is inspired by VGG for efficiency.\n        *   **Transformer Block**: Incorporates layer normalization, a multi-head attention layer (four heads, projection dimension 64), skip connections, and a multi-layer perceptron (MLP) with GELU activation.\n    *   **Novel Architectures**: Eight distinct models are investigated: CNN-only (1 or 2 blocks), Transformer-only (1 or 2 blocks), and four hybrid models that combine CNN and Transformer blocks in various sequences (e.g., CNN then Transformer, Transformer then CNN) \\cite{borhani2022w8x}.\n    *   The approach is novel due to its focus on *lightweight* design for real-time performance, the *custom-built* and simplified CNN and Transformer blocks, and the exploration of *hybrid* CNN-ViT combinations to achieve an optimal balance between accuracy and prediction speed.\n\n*   **Key Technical Contributions**\n    *   Development of novel lightweight ViT-based architectures tailored for efficient plant disease classification \\cite{borhani2022w8x}.\n    *   Introduction and empirical validation of hybrid CNN-ViT models, designed to leverage the feature extraction capabilities of CNNs and the global context understanding of ViTs.\n    *   Design of custom, simplified CNN and Transformer building blocks that contribute to the lightweight nature of the proposed models.\n    *   A systematic investigation into the trade-off between accuracy gains from attention mechanisms and their impact on prediction speed, demonstrating how hybrid models can effectively mitigate speed deceleration \\cite{borhani2022w8x}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: All eight proposed model structures were trained and evaluated on three distinct datasets with varying characteristics and sizes \\cite{borhani2022w8x}. The effect of different image resolutions (50x50, 100x100, 200x200) on model performance and speed was also studied.\n    *   **Datasets**:\n        *   **Wheat Rust Classification Dataset (WRCD)**: A small dataset with ~3.7k images across 3 classes (healthy, yellow rust, brown rust), uniformly distributed.\n        *   **Rice Leaf Disease Dataset (RLDD)**: A very small dataset of 120 images across 3 classes (Bacterial Leaf Disease, Brown Spot, Leaf Smut), challenging due to limited samples and symptom similarity.\n        *   **Plant Village**: A large-scale dataset containing 54,306 images across 38 classes (14 crop species and 26 diseases), characterized by specific image conditions (simple backgrounds) and imbalanced class distribution.\n    *   **Key Performance Metrics**: Models were evaluated using accuracy, F1-score, recall, precision, Giga floating-point operations per second (GFlops) for computational cost, and prediction speed \\cite{borhani2022w8x}.\n    *   **Comparison Results (from abstract/intro)**: The study concluded that while attention blocks generally increase accuracy, they also decelerate prediction speed. Crucially, combining attention blocks with CNN blocks in hybrid architectures was found to compensate for this speed reduction, offering a better balance between high accuracy and fast prediction \\cite{borhani2022w8x}. High accuracies were achieved even on images with occluded disease parts.\n\n*   **Limitations & Scope**\n    *   The Plant Village dataset, while large, consists of images taken under specific conditions (e.g., simple backgrounds), which might not fully represent the complexity of real-world agricultural field scenarios \\cite{borhani2022w8x}.\n    *   The paper implicitly suggests that even lightweight models might be \"extreme\" for certain situations, indicating that the optimal model choice depends on specific application constraints and available computational resources.\n    *   The scope of applicability is primarily focused on image-based classification of plant diseases under controlled or semi-controlled conditions, with an emphasis on real-time inference.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art in automated plant disease classification by demonstrating the effectiveness of *lightweight* and *hybrid* deep learning architectures for *real-time* applications \\cite{borhani2022w8x}.\n    *   It provides a practical and efficient solution for farmers and agricultural systems, enabling timely disease diagnosis and intervention.\n    *   The research offers valuable insights into the performance characteristics and trade-offs between CNNs and ViTs, particularly highlighting how hybrid models can achieve a superior balance of accuracy and computational efficiency.\n    *   The findings pave the way for future research into optimized, resource-efficient deep learning models suitable for deployment on edge devices or mobile platforms in agricultural settings.",
        "keywords": [
          "Automated real-time plant disease classification",
          "Lightweight Vision Transformers (ViTs)",
          "Hybrid CNN-ViT architectures",
          "Prediction speed",
          "Speed-accuracy trade-off",
          "Custom deep learning blocks",
          "Multi-plant disease classification",
          "Computational efficiency",
          "Attention mechanisms",
          "Agricultural applications",
          "Early disease diagnosis"
        ],
        "paper_type": "based on the abstract and introduction:\n\nthe abstract explicitly states: \"a lightweight deep learning approach is **proposed** based on the vision transformer (vit) for real-time automated plant disease classification.\" it also mentions \"classical convolutional neural network (cnn) methods and the combination of cnn and vit\".\n\nthe introduction further elaborates on the problem (plant disease diagnosis) and discusses the use of cnns and the vision transformer (vit) structure, hinting at a novel combination or application.\n\nthis aligns perfectly with the criteria for a **technical** paper:\n*   **abstract mentions:** \"propose\", \"develop\", \"present\", \"algorithm\", \"method\" (specifically \"approach\" and \"methods\").\n*   **introduction discusses:** \"technical problem\" (plant disease classification), \"proposed solution\" (a deep learning approach using vit, potentially combined with cnns).\n\ntherefore, this paper is a **technical** paper."
      },
      "file_name": "17534840dc6016229a577a66f108a1564b8a0131.pdf"
    },
    {
      "success": true,
      "doc_id": "eb4a59a17dcd870e72e03c8408e2effd",
      "summary": "Recent advances on Vision Transformer (ViT) and its improved variants have shown that self-attention-based networks surpass traditional Convolutional Neural Networks (CNNs) in most vision tasks. However, existing ViTs focus on the standard accuracy and computation cost, lacking the investigation of the intrinsic influence on model robustness and generalization. In this work, we conduct systematic evaluation on components of ViTs in terms of their impact on robustness to adversarial examples, common corruptions and distribution shifts. We find some components can be harmful to robustness. By leveraging robust components as building blocks of ViTs, we propose Robust Vision Transformer (RVT), which is a new vision transformer and has superior performance with strong robustness. Inspired by the findings during the evaluation, we further propose two new plug-and-play techniques called position-aware attention scaling and patch-wise augmentation to augment our RVT, which we abbreviate as RVT*. The experimental results of RVT on ImageNet and six robustness benchmarks demonstrate its advanced robustness and generalization ability compared with previous ViTs and state-of-the-art CNNs. Furthermore, RVT-S* achieves Top-1 rank on multiple robustness leaderboards including ImageNet-C, ImageNet-Sketch and ImageNet-R.",
      "intriguing_abstract": "Recent advances on Vision Transformer (ViT) and its improved variants have shown that self-attention-based networks surpass traditional Convolutional Neural Networks (CNNs) in most vision tasks. However, existing ViTs focus on the standard accuracy and computation cost, lacking the investigation of the intrinsic influence on model robustness and generalization. In this work, we conduct systematic evaluation on components of ViTs in terms of their impact on robustness to adversarial examples, common corruptions and distribution shifts. We find some components can be harmful to robustness. By leveraging robust components as building blocks of ViTs, we propose Robust Vision Transformer (RVT), which is a new vision transformer and has superior performance with strong robustness. Inspired by the findings during the evaluation, we further propose two new plug-and-play techniques called position-aware attention scaling and patch-wise augmentation to augment our RVT, which we abbreviate as RVT*. The experimental results of RVT on ImageNet and six robustness benchmarks demonstrate its advanced robustness and generalization ability compared with previous ViTs and state-of-the-art CNNs. Furthermore, RVT-S* achieves Top-1 rank on multiple robustness leaderboards including ImageNet-C, ImageNet-Sketch and ImageNet-R.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/b8cee43a51c44f8f4448e78e41ecf081987707cf.pdf",
      "citation_key": "mao2021zr1",
      "metadata": {
        "title": "Towards Robust Vision Transformer",
        "authors": [
          "Xiaofeng Mao",
          "Gege Qi",
          "Yuefeng Chen",
          "Xiaodan Li",
          "Ranjie Duan",
          "Shaokai Ye",
          "Yuan He",
          "Hui Xue"
        ],
        "published_date": "2021",
        "abstract": "Recent advances on Vision Transformer (ViT) and its improved variants have shown that self-attention-based networks surpass traditional Convolutional Neural Networks (CNNs) in most vision tasks. However, existing ViTs focus on the standard accuracy and computation cost, lacking the investigation of the intrinsic influence on model robustness and generalization. In this work, we conduct systematic evaluation on components of ViTs in terms of their impact on robustness to adversarial examples, common corruptions and distribution shifts. We find some components can be harmful to robustness. By leveraging robust components as building blocks of ViTs, we propose Robust Vision Transformer (RVT), which is a new vision transformer and has superior performance with strong robustness. Inspired by the findings during the evaluation, we further propose two new plug-and-play techniques called position-aware attention scaling and patch-wise augmentation to augment our RVT, which we abbreviate as RVT*. The experimental results of RVT on ImageNet and six robustness benchmarks demonstrate its advanced robustness and generalization ability compared with previous ViTs and state-of-the-art CNNs. Furthermore, RVT-S* achieves Top-1 rank on multiple robustness leaderboards including ImageNet-C, ImageNet-Sketch and ImageNet-R.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b8cee43a51c44f8f4448e78e41ecf081987707cf.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 205,
        "score": 51.25,
        "summary": "Recent advances on Vision Transformer (ViT) and its improved variants have shown that self-attention-based networks surpass traditional Convolutional Neural Networks (CNNs) in most vision tasks. However, existing ViTs focus on the standard accuracy and computation cost, lacking the investigation of the intrinsic influence on model robustness and generalization. In this work, we conduct systematic evaluation on components of ViTs in terms of their impact on robustness to adversarial examples, common corruptions and distribution shifts. We find some components can be harmful to robustness. By leveraging robust components as building blocks of ViTs, we propose Robust Vision Transformer (RVT), which is a new vision transformer and has superior performance with strong robustness. Inspired by the findings during the evaluation, we further propose two new plug-and-play techniques called position-aware attention scaling and patch-wise augmentation to augment our RVT, which we abbreviate as RVT*. The experimental results of RVT on ImageNet and six robustness benchmarks demonstrate its advanced robustness and generalization ability compared with previous ViTs and state-of-the-art CNNs. Furthermore, RVT-S* achieves Top-1 rank on multiple robustness leaderboards including ImageNet-C, ImageNet-Sketch and ImageNet-R.",
        "keywords": []
      },
      "file_name": "b8cee43a51c44f8f4448e78e41ecf081987707cf.pdf"
    },
    {
      "success": true,
      "doc_id": "678fc21add7ed5c3f82faf8c134f9e79",
      "summary": "In the last decade, convolutional neural networks (ConvNets) have dominated and achieved state-of-the-art performances in a variety of medical imaging applications. However, the performances of ConvNets are still limited by lacking the understanding of long-range spatial relations in an image. The recently proposed Vision Transformer (ViT) for image classification uses a purely self-attention-based model that learns long-range spatial relations to focus on the relevant parts of an image. Nevertheless, ViT emphasizes the low-resolution features because of the consecutive downsamplings, result in a lack of detailed localization information, making it unsuitable for image registration. Recently, several ViT-based image segmentation methods have been combined with ConvNets to improve the recovery of detailed localization information. Inspired by them, we present ViT-V-Net, which bridges ViT and ConvNet to provide volumetric medical image registration. The experimental results presented here demonstrate that the proposed architecture achieves superior performance to several top-performing registration methods.",
      "intriguing_abstract": "In the last decade, convolutional neural networks (ConvNets) have dominated and achieved state-of-the-art performances in a variety of medical imaging applications. However, the performances of ConvNets are still limited by lacking the understanding of long-range spatial relations in an image. The recently proposed Vision Transformer (ViT) for image classification uses a purely self-attention-based model that learns long-range spatial relations to focus on the relevant parts of an image. Nevertheless, ViT emphasizes the low-resolution features because of the consecutive downsamplings, result in a lack of detailed localization information, making it unsuitable for image registration. Recently, several ViT-based image segmentation methods have been combined with ConvNets to improve the recovery of detailed localization information. Inspired by them, we present ViT-V-Net, which bridges ViT and ConvNet to provide volumetric medical image registration. The experimental results presented here demonstrate that the proposed architecture achieves superior performance to several top-performing registration methods.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/44ee4165b8a3811dc7d557f99150ff9e62f3733f.pdf",
      "citation_key": "chen202174h",
      "metadata": {
        "title": "ViT-V-Net: Vision Transformer for Unsupervised Volumetric Medical Image Registration",
        "authors": [
          "Junyu Chen",
          "Yufan He",
          "E. Frey",
          "Ye Li",
          "Yong Du"
        ],
        "published_date": "2021",
        "abstract": "In the last decade, convolutional neural networks (ConvNets) have dominated and achieved state-of-the-art performances in a variety of medical imaging applications. However, the performances of ConvNets are still limited by lacking the understanding of long-range spatial relations in an image. The recently proposed Vision Transformer (ViT) for image classification uses a purely self-attention-based model that learns long-range spatial relations to focus on the relevant parts of an image. Nevertheless, ViT emphasizes the low-resolution features because of the consecutive downsamplings, result in a lack of detailed localization information, making it unsuitable for image registration. Recently, several ViT-based image segmentation methods have been combined with ConvNets to improve the recovery of detailed localization information. Inspired by them, we present ViT-V-Net, which bridges ViT and ConvNet to provide volumetric medical image registration. The experimental results presented here demonstrate that the proposed architecture achieves superior performance to several top-performing registration methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/44ee4165b8a3811dc7d557f99150ff9e62f3733f.pdf",
        "venue": "arXiv.org",
        "citationCount": 202,
        "score": 50.5,
        "summary": "In the last decade, convolutional neural networks (ConvNets) have dominated and achieved state-of-the-art performances in a variety of medical imaging applications. However, the performances of ConvNets are still limited by lacking the understanding of long-range spatial relations in an image. The recently proposed Vision Transformer (ViT) for image classification uses a purely self-attention-based model that learns long-range spatial relations to focus on the relevant parts of an image. Nevertheless, ViT emphasizes the low-resolution features because of the consecutive downsamplings, result in a lack of detailed localization information, making it unsuitable for image registration. Recently, several ViT-based image segmentation methods have been combined with ConvNets to improve the recovery of detailed localization information. Inspired by them, we present ViT-V-Net, which bridges ViT and ConvNet to provide volumetric medical image registration. The experimental results presented here demonstrate that the proposed architecture achieves superior performance to several top-performing registration methods.",
        "keywords": []
      },
      "file_name": "44ee4165b8a3811dc7d557f99150ff9e62f3733f.pdf"
    },
    {
      "success": true,
      "doc_id": "51485aa4e1f57eb56e207063c2bbde9c",
      "summary": "The pretrain-then-finetune paradigm has been widely adopted in computer vision. But as the size of Vision Transformer (ViT) grows exponentially, the full finetuning becomes prohibitive in view of the heavier storage overhead. Motivated by parameter-efficient transfer learning (PETL) on language transformers, recent studies attempt to insert lightweight adaptation modules (e.g., adapter layers or prompt tokens) to pretrained ViT and only finetune these modules while the pretrained weights are frozen. However, these modules were originally proposed to finetune language models and did not take into account the prior knowledge specifically for visual tasks. In this paper, we propose to construct Convolutional Bypasses (Convpass) in ViT as adaptation modules, introducing only a small amount (less than 0.5% of model parameters) of trainable parameters to adapt the large ViT. Different from other PETL methods, Convpass benefits from the hard-coded inductive bias of convolutional layers and thus is more suitable for visual tasks, especially in the low-data regime. Experimental results on VTAB-1K benchmark and few-shot learning datasets show that Convpass outperforms current language-oriented adaptation modules, demonstrating the necessity to tailor vision-oriented adaptation modules for adapting vision models.",
      "intriguing_abstract": "The pretrain-then-finetune paradigm has been widely adopted in computer vision. But as the size of Vision Transformer (ViT) grows exponentially, the full finetuning becomes prohibitive in view of the heavier storage overhead. Motivated by parameter-efficient transfer learning (PETL) on language transformers, recent studies attempt to insert lightweight adaptation modules (e.g., adapter layers or prompt tokens) to pretrained ViT and only finetune these modules while the pretrained weights are frozen. However, these modules were originally proposed to finetune language models and did not take into account the prior knowledge specifically for visual tasks. In this paper, we propose to construct Convolutional Bypasses (Convpass) in ViT as adaptation modules, introducing only a small amount (less than 0.5% of model parameters) of trainable parameters to adapt the large ViT. Different from other PETL methods, Convpass benefits from the hard-coded inductive bias of convolutional layers and thus is more suitable for visual tasks, especially in the low-data regime. Experimental results on VTAB-1K benchmark and few-shot learning datasets show that Convpass outperforms current language-oriented adaptation modules, demonstrating the necessity to tailor vision-oriented adaptation modules for adapting vision models.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/50a260631a28bfed18eccf8ebfc75ff34917518f.pdf",
      "citation_key": "jie20220pc",
      "metadata": {
        "title": "Convolutional Bypasses Are Better Vision Transformer Adapters",
        "authors": [
          "Shibo Jie",
          "Zhi-Hong Deng"
        ],
        "published_date": "2022",
        "abstract": "The pretrain-then-finetune paradigm has been widely adopted in computer vision. But as the size of Vision Transformer (ViT) grows exponentially, the full finetuning becomes prohibitive in view of the heavier storage overhead. Motivated by parameter-efficient transfer learning (PETL) on language transformers, recent studies attempt to insert lightweight adaptation modules (e.g., adapter layers or prompt tokens) to pretrained ViT and only finetune these modules while the pretrained weights are frozen. However, these modules were originally proposed to finetune language models and did not take into account the prior knowledge specifically for visual tasks. In this paper, we propose to construct Convolutional Bypasses (Convpass) in ViT as adaptation modules, introducing only a small amount (less than 0.5% of model parameters) of trainable parameters to adapt the large ViT. Different from other PETL methods, Convpass benefits from the hard-coded inductive bias of convolutional layers and thus is more suitable for visual tasks, especially in the low-data regime. Experimental results on VTAB-1K benchmark and few-shot learning datasets show that Convpass outperforms current language-oriented adaptation modules, demonstrating the necessity to tailor vision-oriented adaptation modules for adapting vision models.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/50a260631a28bfed18eccf8ebfc75ff34917518f.pdf",
        "venue": "European Conference on Artificial Intelligence",
        "citationCount": 150,
        "score": 50.0,
        "summary": "The pretrain-then-finetune paradigm has been widely adopted in computer vision. But as the size of Vision Transformer (ViT) grows exponentially, the full finetuning becomes prohibitive in view of the heavier storage overhead. Motivated by parameter-efficient transfer learning (PETL) on language transformers, recent studies attempt to insert lightweight adaptation modules (e.g., adapter layers or prompt tokens) to pretrained ViT and only finetune these modules while the pretrained weights are frozen. However, these modules were originally proposed to finetune language models and did not take into account the prior knowledge specifically for visual tasks. In this paper, we propose to construct Convolutional Bypasses (Convpass) in ViT as adaptation modules, introducing only a small amount (less than 0.5% of model parameters) of trainable parameters to adapt the large ViT. Different from other PETL methods, Convpass benefits from the hard-coded inductive bias of convolutional layers and thus is more suitable for visual tasks, especially in the low-data regime. Experimental results on VTAB-1K benchmark and few-shot learning datasets show that Convpass outperforms current language-oriented adaptation modules, demonstrating the necessity to tailor vision-oriented adaptation modules for adapting vision models.",
        "keywords": []
      },
      "file_name": "50a260631a28bfed18eccf8ebfc75ff34917518f.pdf"
    },
    {
      "success": true,
      "doc_id": "ab9cabd7e107e2948d429f3f8678e3e0",
      "summary": "Image restoration is a challenging ill-posed problem which also has been a long-standing issue. In the past few years, the convolution neural networks (CNNs) almost dominated the computer vision and had achieved considerable success in different levels of vision tasks including image restoration. However, recently the Swin Transformer-based model also shows impressive performance, even surpasses the CNN-based methods to become the state-of-the-art on high-level vision tasks. In this paper, we proposed a restoration model called SUNet which uses the Swin Transformer layer as our basic block and then is applied to UNet architecture for image denoising. The source code and pre-trained models are available at https://github.com/FanChiMao/SUNet.",
      "intriguing_abstract": "Image restoration is a challenging ill-posed problem which also has been a long-standing issue. In the past few years, the convolution neural networks (CNNs) almost dominated the computer vision and had achieved considerable success in different levels of vision tasks including image restoration. However, recently the Swin Transformer-based model also shows impressive performance, even surpasses the CNN-based methods to become the state-of-the-art on high-level vision tasks. In this paper, we proposed a restoration model called SUNet which uses the Swin Transformer layer as our basic block and then is applied to UNet architecture for image denoising. The source code and pre-trained models are available at https://github.com/FanChiMao/SUNet.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/3c14992a490cc31a7a38f5fab156c9da40a861d4.pdf",
      "citation_key": "fan2022m88",
      "metadata": {
        "title": "SUNet: Swin Transformer UNet for Image Denoising",
        "authors": [
          "Chi-Mao Fan",
          "Tsung-Jung Liu",
          "Kuan-Hsien Liu"
        ],
        "published_date": "2022",
        "abstract": "Image restoration is a challenging ill-posed problem which also has been a long-standing issue. In the past few years, the convolution neural networks (CNNs) almost dominated the computer vision and had achieved considerable success in different levels of vision tasks including image restoration. However, recently the Swin Transformer-based model also shows impressive performance, even surpasses the CNN-based methods to become the state-of-the-art on high-level vision tasks. In this paper, we proposed a restoration model called SUNet which uses the Swin Transformer layer as our basic block and then is applied to UNet architecture for image denoising. The source code and pre-trained models are available at https://github.com/FanChiMao/SUNet.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3c14992a490cc31a7a38f5fab156c9da40a861d4.pdf",
        "venue": "International Symposium on Circuits and Systems",
        "citationCount": 139,
        "score": 46.33333333333333,
        "summary": "Image restoration is a challenging ill-posed problem which also has been a long-standing issue. In the past few years, the convolution neural networks (CNNs) almost dominated the computer vision and had achieved considerable success in different levels of vision tasks including image restoration. However, recently the Swin Transformer-based model also shows impressive performance, even surpasses the CNN-based methods to become the state-of-the-art on high-level vision tasks. In this paper, we proposed a restoration model called SUNet which uses the Swin Transformer layer as our basic block and then is applied to UNet architecture for image denoising. The source code and pre-trained models are available at https://github.com/FanChiMao/SUNet.",
        "keywords": []
      },
      "file_name": "3c14992a490cc31a7a38f5fab156c9da40a861d4.pdf"
    },
    {
      "success": true,
      "doc_id": "d185933053ac91031c999b94337ad605",
      "summary": "Since Transformer has found widespread use in NLP, the potential of Transformer in CV has been realized and has inspired many new approaches. However, the computation required for replacing word tokens with image patches for Transformer after the tokenization of the image is vast(e.g., ViT), which bottlenecks model training and inference. In this paper, we propose a new attention mechanism in Transformer termed Cross Attention, which alternates attention inner the image patch instead of the whole image to capture local information and apply attention between image patches which are divided from single-channel feature maps to capture global information. Both operations have less computation than standard self-attention in Transformer. Based on that, we build a hierarchical network called Cross Attention Transformer(CAT) for other vision tasks. Our model achieves 82.8% on ImageNet-1K, and improves the performance of other methods on COCO and ADE20K, illustrating that our network has the potential to serve as general backbones. The code and models are avalible at https://github.com/linhezheng19/CAT.",
      "intriguing_abstract": "Since Transformer has found widespread use in NLP, the potential of Transformer in CV has been realized and has inspired many new approaches. However, the computation required for replacing word tokens with image patches for Transformer after the tokenization of the image is vast(e.g., ViT), which bottlenecks model training and inference. In this paper, we propose a new attention mechanism in Transformer termed Cross Attention, which alternates attention inner the image patch instead of the whole image to capture local information and apply attention between image patches which are divided from single-channel feature maps to capture global information. Both operations have less computation than standard self-attention in Transformer. Based on that, we build a hierarchical network called Cross Attention Transformer(CAT) for other vision tasks. Our model achieves 82.8% on ImageNet-1K, and improves the performance of other methods on COCO and ADE20K, illustrating that our network has the potential to serve as general backbones. The code and models are avalible at https://github.com/linhezheng19/CAT.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/ab70c5e1a338cb470ec39c22a4f10e0f19e61edd.pdf",
      "citation_key": "lin20216a3",
      "metadata": {
        "title": "CAT: Cross Attention in Vision Transformer",
        "authors": [
          "Hezheng Lin",
          "Xingyi Cheng",
          "Xiangyu Wu",
          "Fan Yang",
          "Dong Shen",
          "Zhongyuan Wang",
          "Qing Song",
          "Wei Yuan"
        ],
        "published_date": "2021",
        "abstract": "Since Transformer has found widespread use in NLP, the potential of Transformer in CV has been realized and has inspired many new approaches. However, the computation required for replacing word tokens with image patches for Transformer after the tokenization of the image is vast(e.g., ViT), which bottlenecks model training and inference. In this paper, we propose a new attention mechanism in Transformer termed Cross Attention, which alternates attention inner the image patch instead of the whole image to capture local information and apply attention between image patches which are divided from single-channel feature maps to capture global information. Both operations have less computation than standard self-attention in Transformer. Based on that, we build a hierarchical network called Cross Attention Transformer(CAT) for other vision tasks. Our model achieves 82.8% on ImageNet-1K, and improves the performance of other methods on COCO and ADE20K, illustrating that our network has the potential to serve as general backbones. The code and models are avalible at https://github.com/linhezheng19/CAT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ab70c5e1a338cb470ec39c22a4f10e0f19e61edd.pdf",
        "venue": "IEEE International Conference on Multimedia and Expo",
        "citationCount": 182,
        "score": 45.5,
        "summary": "Since Transformer has found widespread use in NLP, the potential of Transformer in CV has been realized and has inspired many new approaches. However, the computation required for replacing word tokens with image patches for Transformer after the tokenization of the image is vast(e.g., ViT), which bottlenecks model training and inference. In this paper, we propose a new attention mechanism in Transformer termed Cross Attention, which alternates attention inner the image patch instead of the whole image to capture local information and apply attention between image patches which are divided from single-channel feature maps to capture global information. Both operations have less computation than standard self-attention in Transformer. Based on that, we build a hierarchical network called Cross Attention Transformer(CAT) for other vision tasks. Our model achieves 82.8% on ImageNet-1K, and improves the performance of other methods on COCO and ADE20K, illustrating that our network has the potential to serve as general backbones. The code and models are avalible at https://github.com/linhezheng19/CAT.",
        "keywords": []
      },
      "file_name": "ab70c5e1a338cb470ec39c22a4f10e0f19e61edd.pdf"
    },
    {
      "success": true,
      "doc_id": "5b1485d0cd34fa2d24c989bbd7a27f10",
      "summary": "Network quantization significantly reduces model inference complexity and has been widely used in real-world deployments. However, most existing quantization methods have been developed mainly on Convolutional Neural Networks (CNNs), and suffer severe degradation when applied to fully quantized vision transformers. In this work, we demonstrate that many of these difficulties arise because of serious inter-channel variation in LayerNorm inputs, and present, Power-of-Two Factor (PTF), a systematic method to reduce the performance degradation and inference complexity of fully quantized vision transformers. In addition, observing an extreme non-uniform distribution in attention maps, we propose Log-Int-Softmax (LIS) to sustain that and simplify inference by using 4-bit quantization and the BitShift operator. Comprehensive experiments on various transformer-based architectures and benchmarks show that our Fully Quantized Vision Transformer (FQ-ViT) outperforms previous works while even using lower bit-width on attention maps. For instance, we reach 84.89% top-1 accuracy with ViT-L on ImageNet and 50.8 mAP with Cascade Mask R-CNN (Swin-S) on COCO. To our knowledge, we are the first to achieve lossless accuracy degradation (~1%) on fully quantized vision transformers. The code is available at https://github.com/megvii-research/FQ-ViT.",
      "intriguing_abstract": "Network quantization significantly reduces model inference complexity and has been widely used in real-world deployments. However, most existing quantization methods have been developed mainly on Convolutional Neural Networks (CNNs), and suffer severe degradation when applied to fully quantized vision transformers. In this work, we demonstrate that many of these difficulties arise because of serious inter-channel variation in LayerNorm inputs, and present, Power-of-Two Factor (PTF), a systematic method to reduce the performance degradation and inference complexity of fully quantized vision transformers. In addition, observing an extreme non-uniform distribution in attention maps, we propose Log-Int-Softmax (LIS) to sustain that and simplify inference by using 4-bit quantization and the BitShift operator. Comprehensive experiments on various transformer-based architectures and benchmarks show that our Fully Quantized Vision Transformer (FQ-ViT) outperforms previous works while even using lower bit-width on attention maps. For instance, we reach 84.89% top-1 accuracy with ViT-L on ImageNet and 50.8 mAP with Cascade Mask R-CNN (Swin-S) on COCO. To our knowledge, we are the first to achieve lossless accuracy degradation (~1%) on fully quantized vision transformers. The code is available at https://github.com/megvii-research/FQ-ViT.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/1ee05cd919590eaba129caa0fda5e850c87b75a5.pdf",
      "citation_key": "lin2021utw",
      "metadata": {
        "title": "FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer",
        "authors": [
          "Yang Lin",
          "Tianyu Zhang",
          "Peiqin Sun",
          "Zheng Li",
          "Shuchang Zhou"
        ],
        "published_date": "2021",
        "abstract": "Network quantization significantly reduces model inference complexity and has been widely used in real-world deployments. However, most existing quantization methods have been developed mainly on Convolutional Neural Networks (CNNs), and suffer severe degradation when applied to fully quantized vision transformers. In this work, we demonstrate that many of these difficulties arise because of serious inter-channel variation in LayerNorm inputs, and present, Power-of-Two Factor (PTF), a systematic method to reduce the performance degradation and inference complexity of fully quantized vision transformers. In addition, observing an extreme non-uniform distribution in attention maps, we propose Log-Int-Softmax (LIS) to sustain that and simplify inference by using 4-bit quantization and the BitShift operator. Comprehensive experiments on various transformer-based architectures and benchmarks show that our Fully Quantized Vision Transformer (FQ-ViT) outperforms previous works while even using lower bit-width on attention maps. For instance, we reach 84.89% top-1 accuracy with ViT-L on ImageNet and 50.8 mAP with Cascade Mask R-CNN (Swin-S) on COCO. To our knowledge, we are the first to achieve lossless accuracy degradation (~1%) on fully quantized vision transformers. The code is available at https://github.com/megvii-research/FQ-ViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1ee05cd919590eaba129caa0fda5e850c87b75a5.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 179,
        "score": 44.75,
        "summary": "Network quantization significantly reduces model inference complexity and has been widely used in real-world deployments. However, most existing quantization methods have been developed mainly on Convolutional Neural Networks (CNNs), and suffer severe degradation when applied to fully quantized vision transformers. In this work, we demonstrate that many of these difficulties arise because of serious inter-channel variation in LayerNorm inputs, and present, Power-of-Two Factor (PTF), a systematic method to reduce the performance degradation and inference complexity of fully quantized vision transformers. In addition, observing an extreme non-uniform distribution in attention maps, we propose Log-Int-Softmax (LIS) to sustain that and simplify inference by using 4-bit quantization and the BitShift operator. Comprehensive experiments on various transformer-based architectures and benchmarks show that our Fully Quantized Vision Transformer (FQ-ViT) outperforms previous works while even using lower bit-width on attention maps. For instance, we reach 84.89% top-1 accuracy with ViT-L on ImageNet and 50.8 mAP with Cascade Mask R-CNN (Swin-S) on COCO. To our knowledge, we are the first to achieve lossless accuracy degradation (~1%) on fully quantized vision transformers. The code is available at https://github.com/megvii-research/FQ-ViT.",
        "keywords": []
      },
      "file_name": "1ee05cd919590eaba129caa0fda5e850c87b75a5.pdf"
    },
    {
      "success": true,
      "doc_id": "f7cb0279442c0a6539a70cd2f43c5e10",
      "summary": "Vision Transformers (ViTs) have achieved state-of-the-art performance on various computer vision applications. However, these models have considerable storage and computational overheads, making their deployment and efficient inference on edge devices challenging. Quantization is a promising approach to reducing model complexity, and the dyadic arithmetic pipeline can allow the quantized models to perform efficient integer-only inference. Unfortunately, dyadic arithmetic is based on the homogeneity condition in convolutional neural networks, which is not applicable to the non-linear components in ViTs, making integer-only inference of ViTs an open issue. In this paper, we propose I-ViT, an integer-only quantization scheme for ViTs, to enable ViTs to perform the entire computational graph of inference with integer arithmetic and bit-shifting, and without any floating-point arithmetic. In I-ViT, linear operations (e.g., MatMul and Dense) follow the integer-only pipeline with dyadic arithmetic, and non-linear operations (e.g., Softmax, GELU, and LayerNorm) are approximated by the proposed light-weight integer-only arithmetic methods. More specifically, I-ViT applies the proposed Shiftmax and ShiftGELU, which are designed to use integer bit-shifting to approximate the corresponding floating-point operations. We evaluate I-ViT on various benchmark models and the results show that integer-only INT8 quantization achieves comparable (or even slightly higher) accuracy to the full-precision (FP) baseline. Furthermore, we utilize TVM for practical hardware deployment on the GPUs integer arithmetic units, achieving 3.72 ~ 4.11 inference speedup compared to the FP model. Code of both Pytorch and TVM is released at https://github.com/zkkli/I-ViT.",
      "intriguing_abstract": "Vision Transformers (ViTs) have achieved state-of-the-art performance on various computer vision applications. However, these models have considerable storage and computational overheads, making their deployment and efficient inference on edge devices challenging. Quantization is a promising approach to reducing model complexity, and the dyadic arithmetic pipeline can allow the quantized models to perform efficient integer-only inference. Unfortunately, dyadic arithmetic is based on the homogeneity condition in convolutional neural networks, which is not applicable to the non-linear components in ViTs, making integer-only inference of ViTs an open issue. In this paper, we propose I-ViT, an integer-only quantization scheme for ViTs, to enable ViTs to perform the entire computational graph of inference with integer arithmetic and bit-shifting, and without any floating-point arithmetic. In I-ViT, linear operations (e.g., MatMul and Dense) follow the integer-only pipeline with dyadic arithmetic, and non-linear operations (e.g., Softmax, GELU, and LayerNorm) are approximated by the proposed light-weight integer-only arithmetic methods. More specifically, I-ViT applies the proposed Shiftmax and ShiftGELU, which are designed to use integer bit-shifting to approximate the corresponding floating-point operations. We evaluate I-ViT on various benchmark models and the results show that integer-only INT8 quantization achieves comparable (or even slightly higher) accuracy to the full-precision (FP) baseline. Furthermore, we utilize TVM for practical hardware deployment on the GPUs integer arithmetic units, achieving 3.72 ~ 4.11 inference speedup compared to the FP model. Code of both Pytorch and TVM is released at https://github.com/zkkli/I-ViT.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/9fb327c55a30b9771a364f45f33f77778756a164.pdf",
      "citation_key": "li2022mco",
      "metadata": {
        "title": "I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference",
        "authors": [
          "Zhikai Li",
          "Qingyi Gu"
        ],
        "published_date": "2022",
        "abstract": "Vision Transformers (ViTs) have achieved state-of-the-art performance on various computer vision applications. However, these models have considerable storage and computational overheads, making their deployment and efficient inference on edge devices challenging. Quantization is a promising approach to reducing model complexity, and the dyadic arithmetic pipeline can allow the quantized models to perform efficient integer-only inference. Unfortunately, dyadic arithmetic is based on the homogeneity condition in convolutional neural networks, which is not applicable to the non-linear components in ViTs, making integer-only inference of ViTs an open issue. In this paper, we propose I-ViT, an integer-only quantization scheme for ViTs, to enable ViTs to perform the entire computational graph of inference with integer arithmetic and bit-shifting, and without any floating-point arithmetic. In I-ViT, linear operations (e.g., MatMul and Dense) follow the integer-only pipeline with dyadic arithmetic, and non-linear operations (e.g., Softmax, GELU, and LayerNorm) are approximated by the proposed light-weight integer-only arithmetic methods. More specifically, I-ViT applies the proposed Shiftmax and ShiftGELU, which are designed to use integer bit-shifting to approximate the corresponding floating-point operations. We evaluate I-ViT on various benchmark models and the results show that integer-only INT8 quantization achieves comparable (or even slightly higher) accuracy to the full-precision (FP) baseline. Furthermore, we utilize TVM for practical hardware deployment on the GPUs integer arithmetic units, achieving 3.72 ~ 4.11 inference speedup compared to the FP model. Code of both Pytorch and TVM is released at https://github.com/zkkli/I-ViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9fb327c55a30b9771a364f45f33f77778756a164.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 122,
        "score": 40.666666666666664,
        "summary": "Vision Transformers (ViTs) have achieved state-of-the-art performance on various computer vision applications. However, these models have considerable storage and computational overheads, making their deployment and efficient inference on edge devices challenging. Quantization is a promising approach to reducing model complexity, and the dyadic arithmetic pipeline can allow the quantized models to perform efficient integer-only inference. Unfortunately, dyadic arithmetic is based on the homogeneity condition in convolutional neural networks, which is not applicable to the non-linear components in ViTs, making integer-only inference of ViTs an open issue. In this paper, we propose I-ViT, an integer-only quantization scheme for ViTs, to enable ViTs to perform the entire computational graph of inference with integer arithmetic and bit-shifting, and without any floating-point arithmetic. In I-ViT, linear operations (e.g., MatMul and Dense) follow the integer-only pipeline with dyadic arithmetic, and non-linear operations (e.g., Softmax, GELU, and LayerNorm) are approximated by the proposed light-weight integer-only arithmetic methods. More specifically, I-ViT applies the proposed Shiftmax and ShiftGELU, which are designed to use integer bit-shifting to approximate the corresponding floating-point operations. We evaluate I-ViT on various benchmark models and the results show that integer-only INT8 quantization achieves comparable (or even slightly higher) accuracy to the full-precision (FP) baseline. Furthermore, we utilize TVM for practical hardware deployment on the GPUs integer arithmetic units, achieving 3.72 ~ 4.11 inference speedup compared to the FP model. Code of both Pytorch and TVM is released at https://github.com/zkkli/I-ViT.",
        "keywords": []
      },
      "file_name": "9fb327c55a30b9771a364f45f33f77778756a164.pdf"
    },
    {
      "success": true,
      "doc_id": "1150323eb888a10d260baa0db3cefc01",
      "summary": "The large pre-trained vision transformers (ViTs) have demonstrated remarkable performance on various visual tasks, but suffer from expensive computational and memory cost problems when deployed on resource-constrained devices. Among the powerful compression approaches, quantization extremely reduces the computation and memory consumption by low-bit parameters and bit-wise operations. However, low-bit ViTs remain largely unexplored and usually suffer from a significant performance drop compared with the real-valued counterparts. In this work, through extensive empirical analysis, we first identify the bottleneck for severe performance drop comes from the information distortion of the low-bit quantized self-attention map. We then develop an information rectification module (IRM) and a distribution guided distillation (DGD) scheme for fully quantized vision transformers (Q-ViT) to effectively eliminate such distortion, leading to a fully quantized ViTs. We evaluate our methods on popular DeiT and Swin backbones. Extensive experimental results show that our method achieves a much better performance than the prior arts. For example, our Q-ViT can theoretically accelerates the ViT-S by 6.14x and achieves about 80.9% Top-1 accuracy, even surpassing the full-precision counterpart by 1.0% on ImageNet dataset. Our codes and models are attached on https://github.com/YanjingLi0202/Q-ViT",
      "intriguing_abstract": "The large pre-trained vision transformers (ViTs) have demonstrated remarkable performance on various visual tasks, but suffer from expensive computational and memory cost problems when deployed on resource-constrained devices. Among the powerful compression approaches, quantization extremely reduces the computation and memory consumption by low-bit parameters and bit-wise operations. However, low-bit ViTs remain largely unexplored and usually suffer from a significant performance drop compared with the real-valued counterparts. In this work, through extensive empirical analysis, we first identify the bottleneck for severe performance drop comes from the information distortion of the low-bit quantized self-attention map. We then develop an information rectification module (IRM) and a distribution guided distillation (DGD) scheme for fully quantized vision transformers (Q-ViT) to effectively eliminate such distortion, leading to a fully quantized ViTs. We evaluate our methods on popular DeiT and Swin backbones. Extensive experimental results show that our method achieves a much better performance than the prior arts. For example, our Q-ViT can theoretically accelerates the ViT-S by 6.14x and achieves about 80.9% Top-1 accuracy, even surpassing the full-precision counterpart by 1.0% on ImageNet dataset. Our codes and models are attached on https://github.com/YanjingLi0202/Q-ViT",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/dfdb2894d50e095ce97f994ed6cee38554c4c84f.pdf",
      "citation_key": "li2022tl7",
      "metadata": {
        "title": "Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer",
        "authors": [
          "Yanjing Li",
          "Sheng Xu",
          "Baochang Zhang",
          "Xianbin Cao",
          "Penglei Gao",
          "Guodong Guo"
        ],
        "published_date": "2022",
        "abstract": "The large pre-trained vision transformers (ViTs) have demonstrated remarkable performance on various visual tasks, but suffer from expensive computational and memory cost problems when deployed on resource-constrained devices. Among the powerful compression approaches, quantization extremely reduces the computation and memory consumption by low-bit parameters and bit-wise operations. However, low-bit ViTs remain largely unexplored and usually suffer from a significant performance drop compared with the real-valued counterparts. In this work, through extensive empirical analysis, we first identify the bottleneck for severe performance drop comes from the information distortion of the low-bit quantized self-attention map. We then develop an information rectification module (IRM) and a distribution guided distillation (DGD) scheme for fully quantized vision transformers (Q-ViT) to effectively eliminate such distortion, leading to a fully quantized ViTs. We evaluate our methods on popular DeiT and Swin backbones. Extensive experimental results show that our method achieves a much better performance than the prior arts. For example, our Q-ViT can theoretically accelerates the ViT-S by 6.14x and achieves about 80.9% Top-1 accuracy, even surpassing the full-precision counterpart by 1.0% on ImageNet dataset. Our codes and models are attached on https://github.com/YanjingLi0202/Q-ViT",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/dfdb2894d50e095ce97f994ed6cee38554c4c84f.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 115,
        "score": 38.33333333333333,
        "summary": "The large pre-trained vision transformers (ViTs) have demonstrated remarkable performance on various visual tasks, but suffer from expensive computational and memory cost problems when deployed on resource-constrained devices. Among the powerful compression approaches, quantization extremely reduces the computation and memory consumption by low-bit parameters and bit-wise operations. However, low-bit ViTs remain largely unexplored and usually suffer from a significant performance drop compared with the real-valued counterparts. In this work, through extensive empirical analysis, we first identify the bottleneck for severe performance drop comes from the information distortion of the low-bit quantized self-attention map. We then develop an information rectification module (IRM) and a distribution guided distillation (DGD) scheme for fully quantized vision transformers (Q-ViT) to effectively eliminate such distortion, leading to a fully quantized ViTs. We evaluate our methods on popular DeiT and Swin backbones. Extensive experimental results show that our method achieves a much better performance than the prior arts. For example, our Q-ViT can theoretically accelerates the ViT-S by 6.14x and achieves about 80.9% Top-1 accuracy, even surpassing the full-precision counterpart by 1.0% on ImageNet dataset. Our codes and models are attached on https://github.com/YanjingLi0202/Q-ViT",
        "keywords": []
      },
      "file_name": "dfdb2894d50e095ce97f994ed6cee38554c4c84f.pdf"
    },
    {
      "success": true,
      "doc_id": "ac9355fcf0eb1cdd2bad84359f8339bb",
      "summary": "Unsupervised domain adaptation (UDA) aims to transfer the knowledge learnt from a labeled source domain to an unlabeled target domain. Previous work is mainly built upon convolutional neural networks (CNNs) to learn domain-invariant representations. With the recent exponential increase in applying Vision Transformer (ViT) to vision tasks, the capability of ViT in adapting cross-domain knowledge, however, remains unexplored in the literature. To fill this gap, this paper first comprehensively investigates the performance of ViT on a variety of domain adaptation tasks. Surprisingly, ViT demonstrates superior generalization ability, while the performance can be further improved by incorporating adversarial adaptation. Notwithstanding, directly using CNNs-based adaptation strategies fails to take the advantage of ViTs intrinsic merits (e.g., attention mechanism and sequential image representation) which play an important role in knowledge transfer. To remedy this, we propose an unified framework, namely Transferable Vision Transformer (TVT), to fully exploit the transferability of ViT for domain adaptation. Specifically, we delicately devise a novel and effective unit, which we term Transferability Adaption Module (TAM). By injecting learned trans- ferabilities into attention blocks, TAM compels ViT focus on both transferable and discriminative features. Besides, we leverage discriminative clustering to enhance feature diversity and separation which are undermined during adversarial domain alignment. To verify its versatility, we perform extensive studies of TVT on four benchmarks and the experimental results demonstrate that TVT attains significant improvements compared to existing state-of-the-art UDA methods.",
      "intriguing_abstract": "Unsupervised domain adaptation (UDA) aims to transfer the knowledge learnt from a labeled source domain to an unlabeled target domain. Previous work is mainly built upon convolutional neural networks (CNNs) to learn domain-invariant representations. With the recent exponential increase in applying Vision Transformer (ViT) to vision tasks, the capability of ViT in adapting cross-domain knowledge, however, remains unexplored in the literature. To fill this gap, this paper first comprehensively investigates the performance of ViT on a variety of domain adaptation tasks. Surprisingly, ViT demonstrates superior generalization ability, while the performance can be further improved by incorporating adversarial adaptation. Notwithstanding, directly using CNNs-based adaptation strategies fails to take the advantage of ViTs intrinsic merits (e.g., attention mechanism and sequential image representation) which play an important role in knowledge transfer. To remedy this, we propose an unified framework, namely Transferable Vision Transformer (TVT), to fully exploit the transferability of ViT for domain adaptation. Specifically, we delicately devise a novel and effective unit, which we term Transferability Adaption Module (TAM). By injecting learned trans- ferabilities into attention blocks, TAM compels ViT focus on both transferable and discriminative features. Besides, we leverage discriminative clustering to enhance feature diversity and separation which are undermined during adversarial domain alignment. To verify its versatility, we perform extensive studies of TVT on four benchmarks and the experimental results demonstrate that TVT attains significant improvements compared to existing state-of-the-art UDA methods.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f27040f1f81144b17ec4c2b30610960e96353002.pdf",
      "citation_key": "yang2021myb",
      "metadata": {
        "title": "TVT: Transferable Vision Transformer for Unsupervised Domain Adaptation",
        "authors": [
          "Jinyu Yang",
          "Jingjing Liu",
          "N. Xu",
          "Junzhou Huang"
        ],
        "published_date": "2021",
        "abstract": "Unsupervised domain adaptation (UDA) aims to transfer the knowledge learnt from a labeled source domain to an unlabeled target domain. Previous work is mainly built upon convolutional neural networks (CNNs) to learn domain-invariant representations. With the recent exponential increase in applying Vision Transformer (ViT) to vision tasks, the capability of ViT in adapting cross-domain knowledge, however, remains unexplored in the literature. To fill this gap, this paper first comprehensively investigates the performance of ViT on a variety of domain adaptation tasks. Surprisingly, ViT demonstrates superior generalization ability, while the performance can be further improved by incorporating adversarial adaptation. Notwithstanding, directly using CNNs-based adaptation strategies fails to take the advantage of ViTs intrinsic merits (e.g., attention mechanism and sequential image representation) which play an important role in knowledge transfer. To remedy this, we propose an unified framework, namely Transferable Vision Transformer (TVT), to fully exploit the transferability of ViT for domain adaptation. Specifically, we delicately devise a novel and effective unit, which we term Transferability Adaption Module (TAM). By injecting learned trans- ferabilities into attention blocks, TAM compels ViT focus on both transferable and discriminative features. Besides, we leverage discriminative clustering to enhance feature diversity and separation which are undermined during adversarial domain alignment. To verify its versatility, we perform extensive studies of TVT on four benchmarks and the experimental results demonstrate that TVT attains significant improvements compared to existing state-of-the-art UDA methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f27040f1f81144b17ec4c2b30610960e96353002.pdf",
        "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
        "citationCount": 142,
        "score": 35.5,
        "summary": "Unsupervised domain adaptation (UDA) aims to transfer the knowledge learnt from a labeled source domain to an unlabeled target domain. Previous work is mainly built upon convolutional neural networks (CNNs) to learn domain-invariant representations. With the recent exponential increase in applying Vision Transformer (ViT) to vision tasks, the capability of ViT in adapting cross-domain knowledge, however, remains unexplored in the literature. To fill this gap, this paper first comprehensively investigates the performance of ViT on a variety of domain adaptation tasks. Surprisingly, ViT demonstrates superior generalization ability, while the performance can be further improved by incorporating adversarial adaptation. Notwithstanding, directly using CNNs-based adaptation strategies fails to take the advantage of ViTs intrinsic merits (e.g., attention mechanism and sequential image representation) which play an important role in knowledge transfer. To remedy this, we propose an unified framework, namely Transferable Vision Transformer (TVT), to fully exploit the transferability of ViT for domain adaptation. Specifically, we delicately devise a novel and effective unit, which we term Transferability Adaption Module (TAM). By injecting learned trans- ferabilities into attention blocks, TAM compels ViT focus on both transferable and discriminative features. Besides, we leverage discriminative clustering to enhance feature diversity and separation which are undermined during adversarial domain alignment. To verify its versatility, we perform extensive studies of TVT on four benchmarks and the experimental results demonstrate that TVT attains significant improvements compared to existing state-of-the-art UDA methods.",
        "keywords": []
      },
      "file_name": "f27040f1f81144b17ec4c2b30610960e96353002.pdf"
    },
    {
      "success": true,
      "doc_id": "ec3168459fe817b7e4839a2687f0a143",
      "summary": "Vision transformers (ViTs) have gained popularity recently. Even without customized image operators such as convolutions, ViTs can yield competitive performance when properly trained on massive data. However, the computational overhead of ViTs remains prohibitive, due to stacking multi-head self-attention modules and else. Compared to the vast literature and prevailing success in compressing convolutional neural networks, the study of Vision Transformer compression has also just emerged, and existing works focused on one or two aspects of compression. This paper proposes a unified ViT compression framework that seamlessly assembles three effective techniques: pruning, layer skipping, and knowledge distillation. We formulate a budget-constrained, end-to-end optimization framework, targeting jointly learning model weights, layer-wise pruning ratios/masks, and skip configurations, under a distillation loss. The optimization problem is then solved using the primal-dual algorithm. Experiments are conducted with several ViT variants, e.g. DeiT and T2T-ViT backbones on the ImageNet dataset, and our approach consistently outperforms recent competitors. For example, DeiT-Tiny can be trimmed down to 50\\% of the original FLOPs almost without losing accuracy. Codes are available online:~\\url{https://github.com/VITA-Group/UVC}.",
      "intriguing_abstract": "Vision transformers (ViTs) have gained popularity recently. Even without customized image operators such as convolutions, ViTs can yield competitive performance when properly trained on massive data. However, the computational overhead of ViTs remains prohibitive, due to stacking multi-head self-attention modules and else. Compared to the vast literature and prevailing success in compressing convolutional neural networks, the study of Vision Transformer compression has also just emerged, and existing works focused on one or two aspects of compression. This paper proposes a unified ViT compression framework that seamlessly assembles three effective techniques: pruning, layer skipping, and knowledge distillation. We formulate a budget-constrained, end-to-end optimization framework, targeting jointly learning model weights, layer-wise pruning ratios/masks, and skip configurations, under a distillation loss. The optimization problem is then solved using the primal-dual algorithm. Experiments are conducted with several ViT variants, e.g. DeiT and T2T-ViT backbones on the ImageNet dataset, and our approach consistently outperforms recent competitors. For example, DeiT-Tiny can be trimmed down to 50\\% of the original FLOPs almost without losing accuracy. Codes are available online:~\\url{https://github.com/VITA-Group/UVC}.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a.pdf",
      "citation_key": "yu2022iy0",
      "metadata": {
        "title": "Unified Visual Transformer Compression",
        "authors": [
          "Shixing Yu",
          "Tianlong Chen",
          "Jiayi Shen",
          "Huan Yuan",
          "Jianchao Tan",
          "Sen Yang",
          "Ji Liu",
          "Zhangyang Wang"
        ],
        "published_date": "2022",
        "abstract": "Vision transformers (ViTs) have gained popularity recently. Even without customized image operators such as convolutions, ViTs can yield competitive performance when properly trained on massive data. However, the computational overhead of ViTs remains prohibitive, due to stacking multi-head self-attention modules and else. Compared to the vast literature and prevailing success in compressing convolutional neural networks, the study of Vision Transformer compression has also just emerged, and existing works focused on one or two aspects of compression. This paper proposes a unified ViT compression framework that seamlessly assembles three effective techniques: pruning, layer skipping, and knowledge distillation. We formulate a budget-constrained, end-to-end optimization framework, targeting jointly learning model weights, layer-wise pruning ratios/masks, and skip configurations, under a distillation loss. The optimization problem is then solved using the primal-dual algorithm. Experiments are conducted with several ViT variants, e.g. DeiT and T2T-ViT backbones on the ImageNet dataset, and our approach consistently outperforms recent competitors. For example, DeiT-Tiny can be trimmed down to 50\\% of the original FLOPs almost without losing accuracy. Codes are available online:~\\url{https://github.com/VITA-Group/UVC}.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 103,
        "score": 34.33333333333333,
        "summary": "Vision transformers (ViTs) have gained popularity recently. Even without customized image operators such as convolutions, ViTs can yield competitive performance when properly trained on massive data. However, the computational overhead of ViTs remains prohibitive, due to stacking multi-head self-attention modules and else. Compared to the vast literature and prevailing success in compressing convolutional neural networks, the study of Vision Transformer compression has also just emerged, and existing works focused on one or two aspects of compression. This paper proposes a unified ViT compression framework that seamlessly assembles three effective techniques: pruning, layer skipping, and knowledge distillation. We formulate a budget-constrained, end-to-end optimization framework, targeting jointly learning model weights, layer-wise pruning ratios/masks, and skip configurations, under a distillation loss. The optimization problem is then solved using the primal-dual algorithm. Experiments are conducted with several ViT variants, e.g. DeiT and T2T-ViT backbones on the ImageNet dataset, and our approach consistently outperforms recent competitors. For example, DeiT-Tiny can be trimmed down to 50\\% of the original FLOPs almost without losing accuracy. Codes are available online:~\\url{https://github.com/VITA-Group/UVC}.",
        "keywords": []
      },
      "file_name": "4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a.pdf"
    },
    {
      "success": true,
      "doc_id": "2e535a75bda9630a47411e7a039ec34f",
      "summary": "Intra-frame inconsistency has been proved to be effective for the generalization of face forgery detection. However, learning to focus on these inconsistency requires extra pixel-level forged location annotations. Acquiring such annotations is non-trivial. Some existing methods generate large-scale synthesized data with location annotations, which is only composed of real images and cannot capture the properties of forgery regions. Others generate forgery location labels by subtracting paired real and fake images, yet such paired data is difficult to collected and the generated label is usually discontinuous. To overcome these limitations, we propose a novel Unsupervised Inconsistency-Aware method based on Vision Transformer, called UIA-ViT, which only makes use of video-level labels and can learn inconsistency-aware feature without pixel-level annotations. Due to the self-attention mechanism, the attention map among patch embeddings naturally represents the consistency relation, making the vision Transformer suitable for the consistency representation learning. Based on vision Transformer, we propose two key components: Unsupervised Patch Consistency Learning (UPCL) and Progressive Consistency Weighted Assemble (PCWA). UPCL is designed for learning the consistency-related representation with progressive optimized pseudo annotations. PCWA enhances the final classification embedding with previous patch embeddings optimized by UPCL to further improve the detection performance. Extensive experiments demonstrate the effectiveness of the proposed method.",
      "intriguing_abstract": "Intra-frame inconsistency has been proved to be effective for the generalization of face forgery detection. However, learning to focus on these inconsistency requires extra pixel-level forged location annotations. Acquiring such annotations is non-trivial. Some existing methods generate large-scale synthesized data with location annotations, which is only composed of real images and cannot capture the properties of forgery regions. Others generate forgery location labels by subtracting paired real and fake images, yet such paired data is difficult to collected and the generated label is usually discontinuous. To overcome these limitations, we propose a novel Unsupervised Inconsistency-Aware method based on Vision Transformer, called UIA-ViT, which only makes use of video-level labels and can learn inconsistency-aware feature without pixel-level annotations. Due to the self-attention mechanism, the attention map among patch embeddings naturally represents the consistency relation, making the vision Transformer suitable for the consistency representation learning. Based on vision Transformer, we propose two key components: Unsupervised Patch Consistency Learning (UPCL) and Progressive Consistency Weighted Assemble (PCWA). UPCL is designed for learning the consistency-related representation with progressive optimized pseudo annotations. PCWA enhances the final classification embedding with previous patch embeddings optimized by UPCL to further improve the detection performance. Extensive experiments demonstrate the effectiveness of the proposed method.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/49030ae220c863e9b72ab380ecc749c9d0f0ad13.pdf",
      "citation_key": "zhuang2022qn7",
      "metadata": {
        "title": "UIA-ViT: Unsupervised Inconsistency-Aware Method based on Vision Transformer for Face Forgery Detection",
        "authors": [
          "Wanyi Zhuang",
          "Qi Chu",
          "Zhentao Tan",
          "Qiankun Liu",
          "Haojie Yuan",
          "Changtao Miao",
          "Zixiang Luo",
          "Nenghai Yu"
        ],
        "published_date": "2022",
        "abstract": "Intra-frame inconsistency has been proved to be effective for the generalization of face forgery detection. However, learning to focus on these inconsistency requires extra pixel-level forged location annotations. Acquiring such annotations is non-trivial. Some existing methods generate large-scale synthesized data with location annotations, which is only composed of real images and cannot capture the properties of forgery regions. Others generate forgery location labels by subtracting paired real and fake images, yet such paired data is difficult to collected and the generated label is usually discontinuous. To overcome these limitations, we propose a novel Unsupervised Inconsistency-Aware method based on Vision Transformer, called UIA-ViT, which only makes use of video-level labels and can learn inconsistency-aware feature without pixel-level annotations. Due to the self-attention mechanism, the attention map among patch embeddings naturally represents the consistency relation, making the vision Transformer suitable for the consistency representation learning. Based on vision Transformer, we propose two key components: Unsupervised Patch Consistency Learning (UPCL) and Progressive Consistency Weighted Assemble (PCWA). UPCL is designed for learning the consistency-related representation with progressive optimized pseudo annotations. PCWA enhances the final classification embedding with previous patch embeddings optimized by UPCL to further improve the detection performance. Extensive experiments demonstrate the effectiveness of the proposed method.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/49030ae220c863e9b72ab380ecc749c9d0f0ad13.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 92,
        "score": 30.666666666666664,
        "summary": "Intra-frame inconsistency has been proved to be effective for the generalization of face forgery detection. However, learning to focus on these inconsistency requires extra pixel-level forged location annotations. Acquiring such annotations is non-trivial. Some existing methods generate large-scale synthesized data with location annotations, which is only composed of real images and cannot capture the properties of forgery regions. Others generate forgery location labels by subtracting paired real and fake images, yet such paired data is difficult to collected and the generated label is usually discontinuous. To overcome these limitations, we propose a novel Unsupervised Inconsistency-Aware method based on Vision Transformer, called UIA-ViT, which only makes use of video-level labels and can learn inconsistency-aware feature without pixel-level annotations. Due to the self-attention mechanism, the attention map among patch embeddings naturally represents the consistency relation, making the vision Transformer suitable for the consistency representation learning. Based on vision Transformer, we propose two key components: Unsupervised Patch Consistency Learning (UPCL) and Progressive Consistency Weighted Assemble (PCWA). UPCL is designed for learning the consistency-related representation with progressive optimized pseudo annotations. PCWA enhances the final classification embedding with previous patch embeddings optimized by UPCL to further improve the detection performance. Extensive experiments demonstrate the effectiveness of the proposed method.",
        "keywords": []
      },
      "file_name": "49030ae220c863e9b72ab380ecc749c9d0f0ad13.pdf"
    },
    {
      "success": true,
      "doc_id": "afa6055ea589194ae149dadfd7b9d0fa",
      "summary": "Scene classification is an indispensable part of remote sensing image interpretation, and various convolutional neural network (CNN)-based methods have been explored to improve classification accuracy. Although they have shown good classification performance on high-resolution remote sensing (HRRS) images, discriminative ability of extracted features is still limited. In this letter, a high-performance joint framework combined CNNs and vision transformer (ViT) (CTNet) is proposed to further boost the discriminative ability of features for HRRS scene classification. The CTNet method contains two modules, including the stream of ViT (T-stream) and the stream of CNNs (C-stream). For the T-stream, flattened image patches are sent into pretrained ViT model to mine semantic features in HRRS images. To complement with T-stream, pretrained CNN is transferred to extract local structural features in the C-stream. Then, semantic features and structural features are concatenated to predict labels of unknown samples. Finally, a joint loss function is developed to optimize the joint model and increase the intraclass aggregation. The highest accuracies on the aerial image dataset (AID) and Northwestern Polytechnical University (NWPU)-RESISC45 datasets obtained by the CTNet method are 97.70% and 95.49%, respectively. The classification results reveal that the proposed method achieves high classification performance compared with other state-of-the-art (SOTA) methods.",
      "intriguing_abstract": "Scene classification is an indispensable part of remote sensing image interpretation, and various convolutional neural network (CNN)-based methods have been explored to improve classification accuracy. Although they have shown good classification performance on high-resolution remote sensing (HRRS) images, discriminative ability of extracted features is still limited. In this letter, a high-performance joint framework combined CNNs and vision transformer (ViT) (CTNet) is proposed to further boost the discriminative ability of features for HRRS scene classification. The CTNet method contains two modules, including the stream of ViT (T-stream) and the stream of CNNs (C-stream). For the T-stream, flattened image patches are sent into pretrained ViT model to mine semantic features in HRRS images. To complement with T-stream, pretrained CNN is transferred to extract local structural features in the C-stream. Then, semantic features and structural features are concatenated to predict labels of unknown samples. Finally, a joint loss function is developed to optimize the joint model and increase the intraclass aggregation. The highest accuracies on the aerial image dataset (AID) and Northwestern Polytechnical University (NWPU)-RESISC45 datasets obtained by the CTNet method are 97.70% and 95.49%, respectively. The classification results reveal that the proposed method achieves high classification performance compared with other state-of-the-art (SOTA) methods.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/60b0f9af990349546f284dea666fbf52ebfa7004.pdf",
      "citation_key": "deng2021man",
      "metadata": {
        "title": "When CNNs Meet Vision Transformer: A Joint Framework for Remote Sensing Scene Classification",
        "authors": [
          "Peifang Deng",
          "Kejie Xu",
          "Hong Huang"
        ],
        "published_date": "2021",
        "abstract": "Scene classification is an indispensable part of remote sensing image interpretation, and various convolutional neural network (CNN)-based methods have been explored to improve classification accuracy. Although they have shown good classification performance on high-resolution remote sensing (HRRS) images, discriminative ability of extracted features is still limited. In this letter, a high-performance joint framework combined CNNs and vision transformer (ViT) (CTNet) is proposed to further boost the discriminative ability of features for HRRS scene classification. The CTNet method contains two modules, including the stream of ViT (T-stream) and the stream of CNNs (C-stream). For the T-stream, flattened image patches are sent into pretrained ViT model to mine semantic features in HRRS images. To complement with T-stream, pretrained CNN is transferred to extract local structural features in the C-stream. Then, semantic features and structural features are concatenated to predict labels of unknown samples. Finally, a joint loss function is developed to optimize the joint model and increase the intraclass aggregation. The highest accuracies on the aerial image dataset (AID) and Northwestern Polytechnical University (NWPU)-RESISC45 datasets obtained by the CTNet method are 97.70% and 95.49%, respectively. The classification results reveal that the proposed method achieves high classification performance compared with other state-of-the-art (SOTA) methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/60b0f9af990349546f284dea666fbf52ebfa7004.pdf",
        "venue": "IEEE Geoscience and Remote Sensing Letters",
        "citationCount": 121,
        "score": 30.25,
        "summary": "Scene classification is an indispensable part of remote sensing image interpretation, and various convolutional neural network (CNN)-based methods have been explored to improve classification accuracy. Although they have shown good classification performance on high-resolution remote sensing (HRRS) images, discriminative ability of extracted features is still limited. In this letter, a high-performance joint framework combined CNNs and vision transformer (ViT) (CTNet) is proposed to further boost the discriminative ability of features for HRRS scene classification. The CTNet method contains two modules, including the stream of ViT (T-stream) and the stream of CNNs (C-stream). For the T-stream, flattened image patches are sent into pretrained ViT model to mine semantic features in HRRS images. To complement with T-stream, pretrained CNN is transferred to extract local structural features in the C-stream. Then, semantic features and structural features are concatenated to predict labels of unknown samples. Finally, a joint loss function is developed to optimize the joint model and increase the intraclass aggregation. The highest accuracies on the aerial image dataset (AID) and Northwestern Polytechnical University (NWPU)-RESISC45 datasets obtained by the CTNet method are 97.70% and 95.49%, respectively. The classification results reveal that the proposed method achieves high classification performance compared with other state-of-the-art (SOTA) methods.",
        "keywords": []
      },
      "file_name": "60b0f9af990349546f284dea666fbf52ebfa7004.pdf"
    },
    {
      "success": true,
      "doc_id": "53262b50e6a8594df455e75e43948d1b",
      "summary": "The core for tackling the fine-grained visual categorization (FGVC) is to learn subtle yet discriminative features. Most previous works achieve this by explicitly selecting the discriminative parts or integrating the attention mechanism via CNN-based approaches.However, these methods enhance the computational complexity and make the modeldominated by the regions containing the most of the objects. Recently, vision trans-former (ViT) has achieved SOTA performance on general image recognition tasks. Theself-attention mechanism aggregates and weights the information from all patches to the classification token, making it perfectly suitable for FGVC. Nonetheless, the classifi-cation token in the deep layer pays more attention to the global information, lacking the local and low-level features that are essential for FGVC. In this work, we proposea novel pure transformer-based framework Feature Fusion Vision Transformer (FFVT)where we aggregate the important tokens from each transformer layer to compensate thelocal, low-level and middle-level information. We design a novel token selection mod-ule called mutual attention weight selection (MAWS) to guide the network effectively and efficiently towards selecting discriminative tokens without introducing extra param-eters. We verify the effectiveness of FFVT on three benchmarks where FFVT achieves the state-of-the-art performance.",
      "intriguing_abstract": "The core for tackling the fine-grained visual categorization (FGVC) is to learn subtle yet discriminative features. Most previous works achieve this by explicitly selecting the discriminative parts or integrating the attention mechanism via CNN-based approaches.However, these methods enhance the computational complexity and make the modeldominated by the regions containing the most of the objects. Recently, vision trans-former (ViT) has achieved SOTA performance on general image recognition tasks. Theself-attention mechanism aggregates and weights the information from all patches to the classification token, making it perfectly suitable for FGVC. Nonetheless, the classifi-cation token in the deep layer pays more attention to the global information, lacking the local and low-level features that are essential for FGVC. In this work, we proposea novel pure transformer-based framework Feature Fusion Vision Transformer (FFVT)where we aggregate the important tokens from each transformer layer to compensate thelocal, low-level and middle-level information. We design a novel token selection mod-ule called mutual attention weight selection (MAWS) to guide the network effectively and efficiently towards selecting discriminative tokens without introducing extra param-eters. We verify the effectiveness of FFVT on three benchmarks where FFVT achieves the state-of-the-art performance.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/64d8af9153d68e9b50f616d227663385bece93b9.pdf",
      "citation_key": "wang2021oct",
      "metadata": {
        "title": "Feature Fusion Vision Transformer for Fine-Grained Visual Categorization",
        "authors": [
          "Jun Wang",
          "Xiaohan Yu",
          "Yongsheng Gao"
        ],
        "published_date": "2021",
        "abstract": "The core for tackling the fine-grained visual categorization (FGVC) is to learn subtle yet discriminative features. Most previous works achieve this by explicitly selecting the discriminative parts or integrating the attention mechanism via CNN-based approaches.However, these methods enhance the computational complexity and make the modeldominated by the regions containing the most of the objects. Recently, vision trans-former (ViT) has achieved SOTA performance on general image recognition tasks. Theself-attention mechanism aggregates and weights the information from all patches to the classification token, making it perfectly suitable for FGVC. Nonetheless, the classifi-cation token in the deep layer pays more attention to the global information, lacking the local and low-level features that are essential for FGVC. In this work, we proposea novel pure transformer-based framework Feature Fusion Vision Transformer (FFVT)where we aggregate the important tokens from each transformer layer to compensate thelocal, low-level and middle-level information. We design a novel token selection mod-ule called mutual attention weight selection (MAWS) to guide the network effectively and efficiently towards selecting discriminative tokens without introducing extra param-eters. We verify the effectiveness of FFVT on three benchmarks where FFVT achieves the state-of-the-art performance.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/64d8af9153d68e9b50f616d227663385bece93b9.pdf",
        "venue": "British Machine Vision Conference",
        "citationCount": 116,
        "score": 29.0,
        "summary": "The core for tackling the fine-grained visual categorization (FGVC) is to learn subtle yet discriminative features. Most previous works achieve this by explicitly selecting the discriminative parts or integrating the attention mechanism via CNN-based approaches.However, these methods enhance the computational complexity and make the modeldominated by the regions containing the most of the objects. Recently, vision trans-former (ViT) has achieved SOTA performance on general image recognition tasks. Theself-attention mechanism aggregates and weights the information from all patches to the classification token, making it perfectly suitable for FGVC. Nonetheless, the classifi-cation token in the deep layer pays more attention to the global information, lacking the local and low-level features that are essential for FGVC. In this work, we proposea novel pure transformer-based framework Feature Fusion Vision Transformer (FFVT)where we aggregate the important tokens from each transformer layer to compensate thelocal, low-level and middle-level information. We design a novel token selection mod-ule called mutual attention weight selection (MAWS) to guide the network effectively and efficiently towards selecting discriminative tokens without introducing extra param-eters. We verify the effectiveness of FFVT on three benchmarks where FFVT achieves the state-of-the-art performance.",
        "keywords": []
      },
      "file_name": "64d8af9153d68e9b50f616d227663385bece93b9.pdf"
    },
    {
      "success": true,
      "doc_id": "568976ef4ad5ccd47829b0df5d77e3fa",
      "summary": "Since Google proposed Transformer in 2017, it has made significant natural language processing (NLP) development. However, the increasing cost is a large amount of calculation and parameters. Previous researchers designed and proposed some accelerator structures for transformer models in field-programmable gate array (FPGA) to deal with NLP tasks efficiently. Now, the development of Transformer has also affected computer vision (CV) and has rapidly surpassed convolution neural networks (CNNs) in various image tasks. And there are apparent differences between the image data used in CV and the sequence data in NLP. The details in the models contained with transformer units in these two fields are also different. The difference in terms of data brings about the problem of the locality. The difference in the model structure brings about the problem of path dependence, which is not noticed in the existing related accelerator design. Therefore, in this work, we propose the ViA, a novel vision transformer (ViT) accelerator architecture based on FPGA, to execute the transformer application efficiently and avoid the cost of these challenges. By analyzing the data structure in the ViT, we design an appropriate partition strategy to reduce the impact of data locality in the image and improve the efficiency of computation and memory access. Meanwhile, by observing the computing flow of the ViT, we use the half-layer mapping and throughput analysis to reduce the impact of path dependence caused by the shortcut mechanism and fully utilize hardware resources to execute the Transformer efficiently. Based on optimization strategies, we design two reuse processing engines with the internal stream, different from the previous overlap or stream design patterns. In the stage of the experiment, we implement the ViA architecture in Xilinx Alveo U50 FPGA and finally achieved ~5.2 times improvement of energy efficiency compared with NVIDIA Tesla V100, and 410 times improvement of performance compared with related accelerators based on FPGA, that obtained nearly 309.6 GOP/s computing performance in the peek.",
      "intriguing_abstract": "Since Google proposed Transformer in 2017, it has made significant natural language processing (NLP) development. However, the increasing cost is a large amount of calculation and parameters. Previous researchers designed and proposed some accelerator structures for transformer models in field-programmable gate array (FPGA) to deal with NLP tasks efficiently. Now, the development of Transformer has also affected computer vision (CV) and has rapidly surpassed convolution neural networks (CNNs) in various image tasks. And there are apparent differences between the image data used in CV and the sequence data in NLP. The details in the models contained with transformer units in these two fields are also different. The difference in terms of data brings about the problem of the locality. The difference in the model structure brings about the problem of path dependence, which is not noticed in the existing related accelerator design. Therefore, in this work, we propose the ViA, a novel vision transformer (ViT) accelerator architecture based on FPGA, to execute the transformer application efficiently and avoid the cost of these challenges. By analyzing the data structure in the ViT, we design an appropriate partition strategy to reduce the impact of data locality in the image and improve the efficiency of computation and memory access. Meanwhile, by observing the computing flow of the ViT, we use the half-layer mapping and throughput analysis to reduce the impact of path dependence caused by the shortcut mechanism and fully utilize hardware resources to execute the Transformer efficiently. Based on optimization strategies, we design two reuse processing engines with the internal stream, different from the previous overlap or stream design patterns. In the stage of the experiment, we implement the ViA architecture in Xilinx Alveo U50 FPGA and finally achieved ~5.2 times improvement of energy efficiency compared with NVIDIA Tesla V100, and 410 times improvement of performance compared with related accelerators based on FPGA, that obtained nearly 309.6 GOP/s computing performance in the peek.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/03384825d373aabe67c4288ef1eae4d1cf89dc00.pdf",
      "citation_key": "wang2022ti0",
      "metadata": {
        "title": "ViA: A Novel Vision-Transformer Accelerator Based on FPGA",
        "authors": [
          "Teng Wang",
          "Lei Gong",
          "Chao Wang",
          "Y. Yang",
          "Yingxue Gao",
          "Xuehai Zhou",
          "Huaping Chen"
        ],
        "published_date": "2022",
        "abstract": "Since Google proposed Transformer in 2017, it has made significant natural language processing (NLP) development. However, the increasing cost is a large amount of calculation and parameters. Previous researchers designed and proposed some accelerator structures for transformer models in field-programmable gate array (FPGA) to deal with NLP tasks efficiently. Now, the development of Transformer has also affected computer vision (CV) and has rapidly surpassed convolution neural networks (CNNs) in various image tasks. And there are apparent differences between the image data used in CV and the sequence data in NLP. The details in the models contained with transformer units in these two fields are also different. The difference in terms of data brings about the problem of the locality. The difference in the model structure brings about the problem of path dependence, which is not noticed in the existing related accelerator design. Therefore, in this work, we propose the ViA, a novel vision transformer (ViT) accelerator architecture based on FPGA, to execute the transformer application efficiently and avoid the cost of these challenges. By analyzing the data structure in the ViT, we design an appropriate partition strategy to reduce the impact of data locality in the image and improve the efficiency of computation and memory access. Meanwhile, by observing the computing flow of the ViT, we use the half-layer mapping and throughput analysis to reduce the impact of path dependence caused by the shortcut mechanism and fully utilize hardware resources to execute the Transformer efficiently. Based on optimization strategies, we design two reuse processing engines with the internal stream, different from the previous overlap or stream design patterns. In the stage of the experiment, we implement the ViA architecture in Xilinx Alveo U50 FPGA and finally achieved ~5.2 times improvement of energy efficiency compared with NVIDIA Tesla V100, and 410 times improvement of performance compared with related accelerators based on FPGA, that obtained nearly 309.6 GOP/s computing performance in the peek.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/03384825d373aabe67c4288ef1eae4d1cf89dc00.pdf",
        "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
        "citationCount": 84,
        "score": 28.0,
        "summary": "Since Google proposed Transformer in 2017, it has made significant natural language processing (NLP) development. However, the increasing cost is a large amount of calculation and parameters. Previous researchers designed and proposed some accelerator structures for transformer models in field-programmable gate array (FPGA) to deal with NLP tasks efficiently. Now, the development of Transformer has also affected computer vision (CV) and has rapidly surpassed convolution neural networks (CNNs) in various image tasks. And there are apparent differences between the image data used in CV and the sequence data in NLP. The details in the models contained with transformer units in these two fields are also different. The difference in terms of data brings about the problem of the locality. The difference in the model structure brings about the problem of path dependence, which is not noticed in the existing related accelerator design. Therefore, in this work, we propose the ViA, a novel vision transformer (ViT) accelerator architecture based on FPGA, to execute the transformer application efficiently and avoid the cost of these challenges. By analyzing the data structure in the ViT, we design an appropriate partition strategy to reduce the impact of data locality in the image and improve the efficiency of computation and memory access. Meanwhile, by observing the computing flow of the ViT, we use the half-layer mapping and throughput analysis to reduce the impact of path dependence caused by the shortcut mechanism and fully utilize hardware resources to execute the Transformer efficiently. Based on optimization strategies, we design two reuse processing engines with the internal stream, different from the previous overlap or stream design patterns. In the stage of the experiment, we implement the ViA architecture in Xilinx Alveo U50 FPGA and finally achieved ~5.2 times improvement of energy efficiency compared with NVIDIA Tesla V100, and 410 times improvement of performance compared with related accelerators based on FPGA, that obtained nearly 309.6 GOP/s computing performance in the peek.",
        "keywords": []
      },
      "file_name": "03384825d373aabe67c4288ef1eae4d1cf89dc00.pdf"
    },
    {
      "success": true,
      "doc_id": "229270b292f006af9e62f6bde81712e8",
      "summary": "Masked AutoEncoder (MAE) has recently led the trends of visual self-supervision area by an elegant asymmetric encoder-decoder design, which significantly optimizes both the pre-training efficiency and fine-tuning accuracy. Notably, the success of the asymmetric structure relies on the\"global\"property of Vanilla Vision Transformer (ViT), whose self-attention mechanism reasons over arbitrary subset of discrete image patches. However, it is still unclear how the advanced Pyramid-based ViTs (e.g., PVT, Swin) can be adopted in MAE pre-training as they commonly introduce operators within\"local\"windows, making it difficult to handle the random sequence of partial vision tokens. In this paper, we propose Uniform Masking (UM), successfully enabling MAE pre-training for Pyramid-based ViTs with locality (termed\"UM-MAE\"for short). Specifically, UM includes a Uniform Sampling (US) that strictly samples $1$ random patch from each $2 \\times 2$ grid, and a Secondary Masking (SM) which randomly masks a portion of (usually $25\\%$) the already sampled regions as learnable tokens. US preserves equivalent elements across multiple non-overlapped local windows, resulting in the smooth support for popular Pyramid-based ViTs; whilst SM is designed for better transferable visual representations since US reduces the difficulty of pixel recovery pre-task that hinders the semantic learning. We demonstrate that UM-MAE significantly improves the pre-training efficiency (e.g., it speeds up and reduces the GPU memory by $\\sim 2\\times$) of Pyramid-based ViTs, but maintains the competitive fine-tuning performance across downstream tasks. For example using HTC++ detector, the pre-trained Swin-Large backbone self-supervised under UM-MAE only in ImageNet-1K can even outperform the one supervised in ImageNet-22K. The codes are available at https://github.com/implus/UM-MAE.",
      "intriguing_abstract": "Masked AutoEncoder (MAE) has recently led the trends of visual self-supervision area by an elegant asymmetric encoder-decoder design, which significantly optimizes both the pre-training efficiency and fine-tuning accuracy. Notably, the success of the asymmetric structure relies on the\"global\"property of Vanilla Vision Transformer (ViT), whose self-attention mechanism reasons over arbitrary subset of discrete image patches. However, it is still unclear how the advanced Pyramid-based ViTs (e.g., PVT, Swin) can be adopted in MAE pre-training as they commonly introduce operators within\"local\"windows, making it difficult to handle the random sequence of partial vision tokens. In this paper, we propose Uniform Masking (UM), successfully enabling MAE pre-training for Pyramid-based ViTs with locality (termed\"UM-MAE\"for short). Specifically, UM includes a Uniform Sampling (US) that strictly samples $1$ random patch from each $2 \\times 2$ grid, and a Secondary Masking (SM) which randomly masks a portion of (usually $25\\%$) the already sampled regions as learnable tokens. US preserves equivalent elements across multiple non-overlapped local windows, resulting in the smooth support for popular Pyramid-based ViTs; whilst SM is designed for better transferable visual representations since US reduces the difficulty of pixel recovery pre-task that hinders the semantic learning. We demonstrate that UM-MAE significantly improves the pre-training efficiency (e.g., it speeds up and reduces the GPU memory by $\\sim 2\\times$) of Pyramid-based ViTs, but maintains the competitive fine-tuning performance across downstream tasks. For example using HTC++ detector, the pre-trained Swin-Large backbone self-supervised under UM-MAE only in ImageNet-1K can even outperform the one supervised in ImageNet-22K. The codes are available at https://github.com/implus/UM-MAE.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d28fed119d9293af31776205150b3c34f3adc82b.pdf",
      "citation_key": "li2022ow4",
      "metadata": {
        "title": "Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality",
        "authors": [
          "Xiang Li",
          "Wenhai Wang",
          "Lingfeng Yang",
          "Jian Yang"
        ],
        "published_date": "2022",
        "abstract": "Masked AutoEncoder (MAE) has recently led the trends of visual self-supervision area by an elegant asymmetric encoder-decoder design, which significantly optimizes both the pre-training efficiency and fine-tuning accuracy. Notably, the success of the asymmetric structure relies on the\"global\"property of Vanilla Vision Transformer (ViT), whose self-attention mechanism reasons over arbitrary subset of discrete image patches. However, it is still unclear how the advanced Pyramid-based ViTs (e.g., PVT, Swin) can be adopted in MAE pre-training as they commonly introduce operators within\"local\"windows, making it difficult to handle the random sequence of partial vision tokens. In this paper, we propose Uniform Masking (UM), successfully enabling MAE pre-training for Pyramid-based ViTs with locality (termed\"UM-MAE\"for short). Specifically, UM includes a Uniform Sampling (US) that strictly samples $1$ random patch from each $2 \\times 2$ grid, and a Secondary Masking (SM) which randomly masks a portion of (usually $25\\%$) the already sampled regions as learnable tokens. US preserves equivalent elements across multiple non-overlapped local windows, resulting in the smooth support for popular Pyramid-based ViTs; whilst SM is designed for better transferable visual representations since US reduces the difficulty of pixel recovery pre-task that hinders the semantic learning. We demonstrate that UM-MAE significantly improves the pre-training efficiency (e.g., it speeds up and reduces the GPU memory by $\\sim 2\\times$) of Pyramid-based ViTs, but maintains the competitive fine-tuning performance across downstream tasks. For example using HTC++ detector, the pre-trained Swin-Large backbone self-supervised under UM-MAE only in ImageNet-1K can even outperform the one supervised in ImageNet-22K. The codes are available at https://github.com/implus/UM-MAE.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d28fed119d9293af31776205150b3c34f3adc82b.pdf",
        "venue": "arXiv.org",
        "citationCount": 81,
        "score": 27.0,
        "summary": "Masked AutoEncoder (MAE) has recently led the trends of visual self-supervision area by an elegant asymmetric encoder-decoder design, which significantly optimizes both the pre-training efficiency and fine-tuning accuracy. Notably, the success of the asymmetric structure relies on the\"global\"property of Vanilla Vision Transformer (ViT), whose self-attention mechanism reasons over arbitrary subset of discrete image patches. However, it is still unclear how the advanced Pyramid-based ViTs (e.g., PVT, Swin) can be adopted in MAE pre-training as they commonly introduce operators within\"local\"windows, making it difficult to handle the random sequence of partial vision tokens. In this paper, we propose Uniform Masking (UM), successfully enabling MAE pre-training for Pyramid-based ViTs with locality (termed\"UM-MAE\"for short). Specifically, UM includes a Uniform Sampling (US) that strictly samples $1$ random patch from each $2 \\times 2$ grid, and a Secondary Masking (SM) which randomly masks a portion of (usually $25\\%$) the already sampled regions as learnable tokens. US preserves equivalent elements across multiple non-overlapped local windows, resulting in the smooth support for popular Pyramid-based ViTs; whilst SM is designed for better transferable visual representations since US reduces the difficulty of pixel recovery pre-task that hinders the semantic learning. We demonstrate that UM-MAE significantly improves the pre-training efficiency (e.g., it speeds up and reduces the GPU memory by $\\sim 2\\times$) of Pyramid-based ViTs, but maintains the competitive fine-tuning performance across downstream tasks. For example using HTC++ detector, the pre-trained Swin-Large backbone self-supervised under UM-MAE only in ImageNet-1K can even outperform the one supervised in ImageNet-22K. The codes are available at https://github.com/implus/UM-MAE.",
        "keywords": []
      },
      "file_name": "d28fed119d9293af31776205150b3c34f3adc82b.pdf"
    },
    {
      "success": true,
      "doc_id": "6ba73d37835e6a92de887bafaa41e26b",
      "summary": "Attention mechanism has been widely believed as the key to success of vision transformers (ViTs), since it provides a flexible and powerful way to model spatial relationships. However, is the attention mechanism truly an indispensable part of ViT? Can it be replaced by some other alternatives? To demystify the role of attention mechanism, we simplify it into an extremely simple case: ZERO FLOP and ZERO parameter. Concretely, we revisit the shift operation. It does not contain any parameter or arithmetic calculation. The only operation is to exchange a small portion of the channels between neighboring features. Based on this simple operation, we construct a new backbone network, namely ShiftViT, where the attention layers in ViT are substituted by shift operations. Surprisingly, ShiftViT works quite well in several mainstream tasks, e.g., classification, detection, and segmentation. The performance is on par with or even better than the strong baseline Swin Transformer. These results suggest that the attention mechanism might not be the vital factor that makes ViT successful. It can be even replaced by a zero-parameter operation. We should pay more attentions to the remaining parts of ViT in the future work. Code is available at github.com/microsoft/SPACH.",
      "intriguing_abstract": "Attention mechanism has been widely believed as the key to success of vision transformers (ViTs), since it provides a flexible and powerful way to model spatial relationships. However, is the attention mechanism truly an indispensable part of ViT? Can it be replaced by some other alternatives? To demystify the role of attention mechanism, we simplify it into an extremely simple case: ZERO FLOP and ZERO parameter. Concretely, we revisit the shift operation. It does not contain any parameter or arithmetic calculation. The only operation is to exchange a small portion of the channels between neighboring features. Based on this simple operation, we construct a new backbone network, namely ShiftViT, where the attention layers in ViT are substituted by shift operations. Surprisingly, ShiftViT works quite well in several mainstream tasks, e.g., classification, detection, and segmentation. The performance is on par with or even better than the strong baseline Swin Transformer. These results suggest that the attention mechanism might not be the vital factor that makes ViT successful. It can be even replaced by a zero-parameter operation. We should pay more attentions to the remaining parts of ViT in the future work. Code is available at github.com/microsoft/SPACH.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/b52844a746dafd8a5051cef49abbbda64a312605.pdf",
      "citation_key": "wang2022da0",
      "metadata": {
        "title": "When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism",
        "authors": [
          "Guangting Wang",
          "Yucheng Zhao",
          "Chuanxin Tang",
          "Chong Luo",
          "Wenjun Zeng"
        ],
        "published_date": "2022",
        "abstract": "Attention mechanism has been widely believed as the key to success of vision transformers (ViTs), since it provides a flexible and powerful way to model spatial relationships. However, is the attention mechanism truly an indispensable part of ViT? Can it be replaced by some other alternatives? To demystify the role of attention mechanism, we simplify it into an extremely simple case: ZERO FLOP and ZERO parameter. Concretely, we revisit the shift operation. It does not contain any parameter or arithmetic calculation. The only operation is to exchange a small portion of the channels between neighboring features. Based on this simple operation, we construct a new backbone network, namely ShiftViT, where the attention layers in ViT are substituted by shift operations. Surprisingly, ShiftViT works quite well in several mainstream tasks, e.g., classification, detection, and segmentation. The performance is on par with or even better than the strong baseline Swin Transformer. These results suggest that the attention mechanism might not be the vital factor that makes ViT successful. It can be even replaced by a zero-parameter operation. We should pay more attentions to the remaining parts of ViT in the future work. Code is available at github.com/microsoft/SPACH.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b52844a746dafd8a5051cef49abbbda64a312605.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 78,
        "score": 26.0,
        "summary": "Attention mechanism has been widely believed as the key to success of vision transformers (ViTs), since it provides a flexible and powerful way to model spatial relationships. However, is the attention mechanism truly an indispensable part of ViT? Can it be replaced by some other alternatives? To demystify the role of attention mechanism, we simplify it into an extremely simple case: ZERO FLOP and ZERO parameter. Concretely, we revisit the shift operation. It does not contain any parameter or arithmetic calculation. The only operation is to exchange a small portion of the channels between neighboring features. Based on this simple operation, we construct a new backbone network, namely ShiftViT, where the attention layers in ViT are substituted by shift operations. Surprisingly, ShiftViT works quite well in several mainstream tasks, e.g., classification, detection, and segmentation. The performance is on par with or even better than the strong baseline Swin Transformer. These results suggest that the attention mechanism might not be the vital factor that makes ViT successful. It can be even replaced by a zero-parameter operation. We should pay more attentions to the remaining parts of ViT in the future work. Code is available at github.com/microsoft/SPACH.",
        "keywords": []
      },
      "file_name": "b52844a746dafd8a5051cef49abbbda64a312605.pdf"
    },
    {
      "success": true,
      "doc_id": "3289bff6b09d142d4af3586e449d46cf",
      "summary": "In this work, we explore neat yet effective Transformer-based frameworks for visual grounding. The previous methods generally address the core problem of visual grounding, i.e., multi-modal fusion and reasoning, with manually-designed mechanisms. Such heuristic designs are not only complicated but also make models easily overfit specific data distributions. To avoid this, we first propose TransVG, which establishes multi-modal correspondences by Transformers and localizes referred regions by directly regressing box coordinates. We empirically show that complicated fusion modules can be replaced by a simple stack of Transformer encoder layers with higher performance. However, the core fusion Transformer in TransVG is stand-alone against uni-modal encoders, and thus should be trained from scratch on limited visual grounding data, which makes it hard to be optimized and leads to sub-optimal performance. To this end, we further introduce TransVG++ to make two-fold improvements. For one thing, we upgrade our framework to a purely Transformer-based one by leveraging Vision Transformer (ViT) for vision feature encoding. For another, we devise Language Conditioned Vision Transformer that removes external fusion modules and reuses the uni-modal ViT for vision-language fusion at the intermediate layers. We conduct extensive experiments on five prevalent datasets, and report a series of state-of-the-art records.",
      "intriguing_abstract": "In this work, we explore neat yet effective Transformer-based frameworks for visual grounding. The previous methods generally address the core problem of visual grounding, i.e., multi-modal fusion and reasoning, with manually-designed mechanisms. Such heuristic designs are not only complicated but also make models easily overfit specific data distributions. To avoid this, we first propose TransVG, which establishes multi-modal correspondences by Transformers and localizes referred regions by directly regressing box coordinates. We empirically show that complicated fusion modules can be replaced by a simple stack of Transformer encoder layers with higher performance. However, the core fusion Transformer in TransVG is stand-alone against uni-modal encoders, and thus should be trained from scratch on limited visual grounding data, which makes it hard to be optimized and leads to sub-optimal performance. To this end, we further introduce TransVG++ to make two-fold improvements. For one thing, we upgrade our framework to a purely Transformer-based one by leveraging Vision Transformer (ViT) for vision feature encoding. For another, we devise Language Conditioned Vision Transformer that removes external fusion modules and reuses the uni-modal ViT for vision-language fusion at the intermediate layers. We conduct extensive experiments on five prevalent datasets, and report a series of state-of-the-art records.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/35fccd11326e799ebf724f4150acef12a6538953.pdf",
      "citation_key": "deng2022bil",
      "metadata": {
        "title": "TransVG++: End-to-End Visual Grounding With Language Conditioned Vision Transformer",
        "authors": [
          "Jiajun Deng",
          "Zhengyuan Yang",
          "Daqing Liu",
          "Tianlang Chen",
          "Wen-gang Zhou",
          "Yanyong Zhang",
          "Houqiang Li",
          "Wanli Ouyang"
        ],
        "published_date": "2022",
        "abstract": "In this work, we explore neat yet effective Transformer-based frameworks for visual grounding. The previous methods generally address the core problem of visual grounding, i.e., multi-modal fusion and reasoning, with manually-designed mechanisms. Such heuristic designs are not only complicated but also make models easily overfit specific data distributions. To avoid this, we first propose TransVG, which establishes multi-modal correspondences by Transformers and localizes referred regions by directly regressing box coordinates. We empirically show that complicated fusion modules can be replaced by a simple stack of Transformer encoder layers with higher performance. However, the core fusion Transformer in TransVG is stand-alone against uni-modal encoders, and thus should be trained from scratch on limited visual grounding data, which makes it hard to be optimized and leads to sub-optimal performance. To this end, we further introduce TransVG++ to make two-fold improvements. For one thing, we upgrade our framework to a purely Transformer-based one by leveraging Vision Transformer (ViT) for vision feature encoding. For another, we devise Language Conditioned Vision Transformer that removes external fusion modules and reuses the uni-modal ViT for vision-language fusion at the intermediate layers. We conduct extensive experiments on five prevalent datasets, and report a series of state-of-the-art records.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/35fccd11326e799ebf724f4150acef12a6538953.pdf",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citationCount": 73,
        "score": 24.333333333333332,
        "summary": "In this work, we explore neat yet effective Transformer-based frameworks for visual grounding. The previous methods generally address the core problem of visual grounding, i.e., multi-modal fusion and reasoning, with manually-designed mechanisms. Such heuristic designs are not only complicated but also make models easily overfit specific data distributions. To avoid this, we first propose TransVG, which establishes multi-modal correspondences by Transformers and localizes referred regions by directly regressing box coordinates. We empirically show that complicated fusion modules can be replaced by a simple stack of Transformer encoder layers with higher performance. However, the core fusion Transformer in TransVG is stand-alone against uni-modal encoders, and thus should be trained from scratch on limited visual grounding data, which makes it hard to be optimized and leads to sub-optimal performance. To this end, we further introduce TransVG++ to make two-fold improvements. For one thing, we upgrade our framework to a purely Transformer-based one by leveraging Vision Transformer (ViT) for vision feature encoding. For another, we devise Language Conditioned Vision Transformer that removes external fusion modules and reuses the uni-modal ViT for vision-language fusion at the intermediate layers. We conduct extensive experiments on five prevalent datasets, and report a series of state-of-the-art records.",
        "keywords": []
      },
      "file_name": "35fccd11326e799ebf724f4150acef12a6538953.pdf"
    },
    {
      "success": true,
      "doc_id": "cfb659def9b2e92d767e130724fba50e",
      "summary": "Recently vision transformer has achieved tremendous success on image-level visual recognition tasks. To effectively and efficiently model the crucial temporal information within a video clip, we propose a Temporally Efficient Vision Transformer (TeViT) for video instance segmentation (VIS). Different from previous transformer-based VIS methods, TeViT is nearly convolution-free, which contains a transformer backbone and a query-based video instance segmentation head. In the backbone stage, we propose a nearly parameter-free messenger shift mechanism for early temporal context fusion. In the head stages, we propose a parameter-shared spatiotemporal query interaction mechanism to build the one-to-one correspondence between video instances and queries. Thus, TeViT fully utilizes both frame-level and instance-level temporal context information and obtains strong temporal modeling capacity with negligible extra computational cost. On three widely adopted VIS benchmarks, i.e., YouTube-VIS-2019, YouTube-VIS-2021, and OVIS, TeViT obtains state-of-the-art results and maintains high inference speed, e.g., 46.6 AP with 68.9 FPS on YouTube-VIS-2019. Code is available at https://github.com/hustvl/TeViT.",
      "intriguing_abstract": "Recently vision transformer has achieved tremendous success on image-level visual recognition tasks. To effectively and efficiently model the crucial temporal information within a video clip, we propose a Temporally Efficient Vision Transformer (TeViT) for video instance segmentation (VIS). Different from previous transformer-based VIS methods, TeViT is nearly convolution-free, which contains a transformer backbone and a query-based video instance segmentation head. In the backbone stage, we propose a nearly parameter-free messenger shift mechanism for early temporal context fusion. In the head stages, we propose a parameter-shared spatiotemporal query interaction mechanism to build the one-to-one correspondence between video instances and queries. Thus, TeViT fully utilizes both frame-level and instance-level temporal context information and obtains strong temporal modeling capacity with negligible extra computational cost. On three widely adopted VIS benchmarks, i.e., YouTube-VIS-2019, YouTube-VIS-2021, and OVIS, TeViT obtains state-of-the-art results and maintains high inference speed, e.g., 46.6 AP with 68.9 FPS on YouTube-VIS-2019. Code is available at https://github.com/hustvl/TeViT.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/0d1e4faa1580266a56ad62ac2fdd72ed0c0bbbe9.pdf",
      "citation_key": "yang20228mm",
      "metadata": {
        "title": "Temporally Efficient Vision Transformer for Video Instance Segmentation",
        "authors": [
          "Shusheng Yang",
          "Xinggang Wang",
          "Yu Li",
          "Yuxin Fang",
          "Jiemin Fang",
          "Wenyu Liu",
          "Xun Zhao",
          "Ying Shan"
        ],
        "published_date": "2022",
        "abstract": "Recently vision transformer has achieved tremendous success on image-level visual recognition tasks. To effectively and efficiently model the crucial temporal information within a video clip, we propose a Temporally Efficient Vision Transformer (TeViT) for video instance segmentation (VIS). Different from previous transformer-based VIS methods, TeViT is nearly convolution-free, which contains a transformer backbone and a query-based video instance segmentation head. In the backbone stage, we propose a nearly parameter-free messenger shift mechanism for early temporal context fusion. In the head stages, we propose a parameter-shared spatiotemporal query interaction mechanism to build the one-to-one correspondence between video instances and queries. Thus, TeViT fully utilizes both frame-level and instance-level temporal context information and obtains strong temporal modeling capacity with negligible extra computational cost. On three widely adopted VIS benchmarks, i.e., YouTube-VIS-2019, YouTube-VIS-2021, and OVIS, TeViT obtains state-of-the-art results and maintains high inference speed, e.g., 46.6 AP with 68.9 FPS on YouTube-VIS-2019. Code is available at https://github.com/hustvl/TeViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0d1e4faa1580266a56ad62ac2fdd72ed0c0bbbe9.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 73,
        "score": 24.333333333333332,
        "summary": "Recently vision transformer has achieved tremendous success on image-level visual recognition tasks. To effectively and efficiently model the crucial temporal information within a video clip, we propose a Temporally Efficient Vision Transformer (TeViT) for video instance segmentation (VIS). Different from previous transformer-based VIS methods, TeViT is nearly convolution-free, which contains a transformer backbone and a query-based video instance segmentation head. In the backbone stage, we propose a nearly parameter-free messenger shift mechanism for early temporal context fusion. In the head stages, we propose a parameter-shared spatiotemporal query interaction mechanism to build the one-to-one correspondence between video instances and queries. Thus, TeViT fully utilizes both frame-level and instance-level temporal context information and obtains strong temporal modeling capacity with negligible extra computational cost. On three widely adopted VIS benchmarks, i.e., YouTube-VIS-2019, YouTube-VIS-2021, and OVIS, TeViT obtains state-of-the-art results and maintains high inference speed, e.g., 46.6 AP with 68.9 FPS on YouTube-VIS-2019. Code is available at https://github.com/hustvl/TeViT.",
        "keywords": []
      },
      "file_name": "0d1e4faa1580266a56ad62ac2fdd72ed0c0bbbe9.pdf"
    },
    {
      "success": true,
      "doc_id": "bfc28c07de806b6ae2d8841f063165ed",
      "summary": "Medical ultrasound (US) imaging has become a prominent modality for breast cancer imaging due to its ease-of-use, low-cost and safety. In the past decade, convolutional neural networks (CNNs) have emerged as the method of choice in vision applications and have shown excellent potential in automatic classification of US images. Despite their success, their restricted local receptive field limits their ability to learn global context information. Recently, Vision Transformer (ViT) designs that are based on self-attention between image patches have shown great potential to be an alternative to CNNs. In this study, for the first time, we utilize ViT to classify breast US images using different augmentation strategies. The results are provided as classification accuracy and Area Under the Curve (AUC) metrics, and the performance is compared with the state-of-the-art CNNs. The results indicate that the ViT models have comparable efficiency with or even better than the CNNs in classification of US breast images.",
      "intriguing_abstract": "Medical ultrasound (US) imaging has become a prominent modality for breast cancer imaging due to its ease-of-use, low-cost and safety. In the past decade, convolutional neural networks (CNNs) have emerged as the method of choice in vision applications and have shown excellent potential in automatic classification of US images. Despite their success, their restricted local receptive field limits their ability to learn global context information. Recently, Vision Transformer (ViT) designs that are based on self-attention between image patches have shown great potential to be an alternative to CNNs. In this study, for the first time, we utilize ViT to classify breast US images using different augmentation strategies. The results are provided as classification accuracy and Area Under the Curve (AUC) metrics, and the performance is compared with the state-of-the-art CNNs. The results indicate that the ViT models have comparable efficiency with or even better than the CNNs in classification of US breast images.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/8f7ae0526f9c8706a9e0967eb7942d9b3ffb2788.pdf",
      "citation_key": "gheflati202131i",
      "metadata": {
        "title": "Vision Transformer for Classification of Breast Ultrasound Images",
        "authors": [
          "Behnaz Gheflati",
          "H. Rivaz"
        ],
        "published_date": "2021",
        "abstract": "Medical ultrasound (US) imaging has become a prominent modality for breast cancer imaging due to its ease-of-use, low-cost and safety. In the past decade, convolutional neural networks (CNNs) have emerged as the method of choice in vision applications and have shown excellent potential in automatic classification of US images. Despite their success, their restricted local receptive field limits their ability to learn global context information. Recently, Vision Transformer (ViT) designs that are based on self-attention between image patches have shown great potential to be an alternative to CNNs. In this study, for the first time, we utilize ViT to classify breast US images using different augmentation strategies. The results are provided as classification accuracy and Area Under the Curve (AUC) metrics, and the performance is compared with the state-of-the-art CNNs. The results indicate that the ViT models have comparable efficiency with or even better than the CNNs in classification of US breast images.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/8f7ae0526f9c8706a9e0967eb7942d9b3ffb2788.pdf",
        "venue": "arXiv.org",
        "citationCount": 94,
        "score": 23.5,
        "summary": "Medical ultrasound (US) imaging has become a prominent modality for breast cancer imaging due to its ease-of-use, low-cost and safety. In the past decade, convolutional neural networks (CNNs) have emerged as the method of choice in vision applications and have shown excellent potential in automatic classification of US images. Despite their success, their restricted local receptive field limits their ability to learn global context information. Recently, Vision Transformer (ViT) designs that are based on self-attention between image patches have shown great potential to be an alternative to CNNs. In this study, for the first time, we utilize ViT to classify breast US images using different augmentation strategies. The results are provided as classification accuracy and Area Under the Curve (AUC) metrics, and the performance is compared with the state-of-the-art CNNs. The results indicate that the ViT models have comparable efficiency with or even better than the CNNs in classification of US breast images.",
        "keywords": []
      },
      "file_name": "8f7ae0526f9c8706a9e0967eb7942d9b3ffb2788.pdf"
    },
    {
      "success": true,
      "doc_id": "1f3f0764e9bb868778140684cf7466c9",
      "summary": "In order to improve the diagnosis accuracy and generalization of bearing faults, an integrated vision transformer (ViT) model based on wavelet transform and the soft voting method is proposed in this paper. Firstly, the discrete wavelet transform (DWT) was utilized to decompose the vibration signal into the subsignals in the different frequency bands, and then these different subsignals were transformed into a timefrequency representation (TFR) map by the continuous wavelet transform (CWT) method. Secondly, the TFR maps were input with respective to the multiple individual ViT models for preliminary diagnosis analysis. Finally, the final diagnosis decision was obtained by using the soft voting method to fuse all the preliminary diagnosis results. Through multifaceted diagnosis tests of rolling bearings on different datasets, the diagnosis results demonstrate that the proposed integrated ViT model based on the soft voting method can diagnose the different fault categories and fault severities of bearings accurately, and has a higher diagnostic accuracy and generalization ability by comparison analysis with integrated CNN and individual ViT.",
      "intriguing_abstract": "In order to improve the diagnosis accuracy and generalization of bearing faults, an integrated vision transformer (ViT) model based on wavelet transform and the soft voting method is proposed in this paper. Firstly, the discrete wavelet transform (DWT) was utilized to decompose the vibration signal into the subsignals in the different frequency bands, and then these different subsignals were transformed into a timefrequency representation (TFR) map by the continuous wavelet transform (CWT) method. Secondly, the TFR maps were input with respective to the multiple individual ViT models for preliminary diagnosis analysis. Finally, the final diagnosis decision was obtained by using the soft voting method to fuse all the preliminary diagnosis results. Through multifaceted diagnosis tests of rolling bearings on different datasets, the diagnosis results demonstrate that the proposed integrated ViT model based on the soft voting method can diagnose the different fault categories and fault severities of bearings accurately, and has a higher diagnostic accuracy and generalization ability by comparison analysis with integrated CNN and individual ViT.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/00f6041bdbdd00299ef317bd86db3660eb039a8c.pdf",
      "citation_key": "tang2022e2i",
      "metadata": {
        "title": "A Novel Fault Diagnosis Method of Rolling Bearing Based on Integrated Vision Transformer Model",
        "authors": [
          "Xinyu Tang",
          "Zengbing Xu",
          "Zhigang Wang"
        ],
        "published_date": "2022",
        "abstract": "In order to improve the diagnosis accuracy and generalization of bearing faults, an integrated vision transformer (ViT) model based on wavelet transform and the soft voting method is proposed in this paper. Firstly, the discrete wavelet transform (DWT) was utilized to decompose the vibration signal into the subsignals in the different frequency bands, and then these different subsignals were transformed into a timefrequency representation (TFR) map by the continuous wavelet transform (CWT) method. Secondly, the TFR maps were input with respective to the multiple individual ViT models for preliminary diagnosis analysis. Finally, the final diagnosis decision was obtained by using the soft voting method to fuse all the preliminary diagnosis results. Through multifaceted diagnosis tests of rolling bearings on different datasets, the diagnosis results demonstrate that the proposed integrated ViT model based on the soft voting method can diagnose the different fault categories and fault severities of bearings accurately, and has a higher diagnostic accuracy and generalization ability by comparison analysis with integrated CNN and individual ViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/00f6041bdbdd00299ef317bd86db3660eb039a8c.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 68,
        "score": 22.666666666666664,
        "summary": "In order to improve the diagnosis accuracy and generalization of bearing faults, an integrated vision transformer (ViT) model based on wavelet transform and the soft voting method is proposed in this paper. Firstly, the discrete wavelet transform (DWT) was utilized to decompose the vibration signal into the subsignals in the different frequency bands, and then these different subsignals were transformed into a timefrequency representation (TFR) map by the continuous wavelet transform (CWT) method. Secondly, the TFR maps were input with respective to the multiple individual ViT models for preliminary diagnosis analysis. Finally, the final diagnosis decision was obtained by using the soft voting method to fuse all the preliminary diagnosis results. Through multifaceted diagnosis tests of rolling bearings on different datasets, the diagnosis results demonstrate that the proposed integrated ViT model based on the soft voting method can diagnose the different fault categories and fault severities of bearings accurately, and has a higher diagnostic accuracy and generalization ability by comparison analysis with integrated CNN and individual ViT.",
        "keywords": []
      },
      "file_name": "00f6041bdbdd00299ef317bd86db3660eb039a8c.pdf"
    },
    {
      "success": true,
      "doc_id": "6f670b9183a2ee8d458431558b1b05e3",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/5f7fee3db8879d9d4e714ae20627f00fcbdfab38.pdf",
      "citation_key": "yu202236t",
      "metadata": {
        "title": "Mix-ViT: Mixing attentive vision transformer for ultra-fine-grained visual categorization",
        "authors": [
          "Xiaohan Yu",
          "Jun Wang",
          "Yang Zhao",
          "Yongsheng Gao"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/5f7fee3db8879d9d4e714ae20627f00fcbdfab38.pdf",
        "venue": "Pattern Recognition",
        "citationCount": 66,
        "score": 22.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "5f7fee3db8879d9d4e714ae20627f00fcbdfab38.pdf"
    },
    {
      "success": true,
      "doc_id": "c59346b76f51517e39931914b7466459",
      "summary": "Facial Expression Recognition (FER) in the wild is an extremely challenging task in computer vision due to variant backgrounds, low-quality facial images, and the subjectiveness of annotators. These uncertainties make it difficult for neural networks to learn robust features on limited-scale datasets. Moreover, the networks can be easily distributed by the above factors and perform incorrect decisions. Recently, vision transformer (ViT) and data-efficient image transformers (DeiT) present their significant performance in traditional classification tasks. The self-attention mechanism makes transformers obtain a global receptive field in the first layer which dramatically enhances the feature extraction capability. In this work, we first propose a novel pure transformer-based mask vision transformer (MVT) for FER in the wild, which consists of two modules: a transformer-based mask generation network (MGN) to generate a mask that can filter out complex backgrounds and occlusion of face images, and a dynamic relabeling module to rectify incorrect labels in FER datasets in the wild. Extensive experimental results demonstrate that our MVT outperforms state-of-the-art methods on RAF-DB with 88.62%, FERPlus with 89.22%, and AffectNet-7 with 64.57%, respectively, and achieves a comparable result on AffectNet-8 with 61.40%.",
      "intriguing_abstract": "Facial Expression Recognition (FER) in the wild is an extremely challenging task in computer vision due to variant backgrounds, low-quality facial images, and the subjectiveness of annotators. These uncertainties make it difficult for neural networks to learn robust features on limited-scale datasets. Moreover, the networks can be easily distributed by the above factors and perform incorrect decisions. Recently, vision transformer (ViT) and data-efficient image transformers (DeiT) present their significant performance in traditional classification tasks. The self-attention mechanism makes transformers obtain a global receptive field in the first layer which dramatically enhances the feature extraction capability. In this work, we first propose a novel pure transformer-based mask vision transformer (MVT) for FER in the wild, which consists of two modules: a transformer-based mask generation network (MGN) to generate a mask that can filter out complex backgrounds and occlusion of face images, and a dynamic relabeling module to rectify incorrect labels in FER datasets in the wild. Extensive experimental results demonstrate that our MVT outperforms state-of-the-art methods on RAF-DB with 88.62%, FERPlus with 89.22%, and AffectNet-7 with 64.57%, respectively, and achieves a comparable result on AffectNet-8 with 61.40%.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/070c917ab1a4d6b924a9613ca18443f260d8d5be.pdf",
      "citation_key": "li2021ra5",
      "metadata": {
        "title": "MViT: Mask Vision Transformer for Facial Expression Recognition in the wild",
        "authors": [
          "Hanting Li",
          "Ming-Fa Sui",
          "Feng Zhao",
          "Zhengjun Zha",
          "Feng Wu"
        ],
        "published_date": "2021",
        "abstract": "Facial Expression Recognition (FER) in the wild is an extremely challenging task in computer vision due to variant backgrounds, low-quality facial images, and the subjectiveness of annotators. These uncertainties make it difficult for neural networks to learn robust features on limited-scale datasets. Moreover, the networks can be easily distributed by the above factors and perform incorrect decisions. Recently, vision transformer (ViT) and data-efficient image transformers (DeiT) present their significant performance in traditional classification tasks. The self-attention mechanism makes transformers obtain a global receptive field in the first layer which dramatically enhances the feature extraction capability. In this work, we first propose a novel pure transformer-based mask vision transformer (MVT) for FER in the wild, which consists of two modules: a transformer-based mask generation network (MGN) to generate a mask that can filter out complex backgrounds and occlusion of face images, and a dynamic relabeling module to rectify incorrect labels in FER datasets in the wild. Extensive experimental results demonstrate that our MVT outperforms state-of-the-art methods on RAF-DB with 88.62%, FERPlus with 89.22%, and AffectNet-7 with 64.57%, respectively, and achieves a comparable result on AffectNet-8 with 61.40%.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/070c917ab1a4d6b924a9613ca18443f260d8d5be.pdf",
        "venue": "arXiv.org",
        "citationCount": 83,
        "score": 20.75,
        "summary": "Facial Expression Recognition (FER) in the wild is an extremely challenging task in computer vision due to variant backgrounds, low-quality facial images, and the subjectiveness of annotators. These uncertainties make it difficult for neural networks to learn robust features on limited-scale datasets. Moreover, the networks can be easily distributed by the above factors and perform incorrect decisions. Recently, vision transformer (ViT) and data-efficient image transformers (DeiT) present their significant performance in traditional classification tasks. The self-attention mechanism makes transformers obtain a global receptive field in the first layer which dramatically enhances the feature extraction capability. In this work, we first propose a novel pure transformer-based mask vision transformer (MVT) for FER in the wild, which consists of two modules: a transformer-based mask generation network (MGN) to generate a mask that can filter out complex backgrounds and occlusion of face images, and a dynamic relabeling module to rectify incorrect labels in FER datasets in the wild. Extensive experimental results demonstrate that our MVT outperforms state-of-the-art methods on RAF-DB with 88.62%, FERPlus with 89.22%, and AffectNet-7 with 64.57%, respectively, and achieves a comparable result on AffectNet-8 with 61.40%.",
        "keywords": []
      },
      "file_name": "070c917ab1a4d6b924a9613ca18443f260d8d5be.pdf"
    },
    {
      "success": true,
      "doc_id": "5a7a70cf9f6d1d94f52deeeefcedd390",
      "summary": "Semantic segmentation of remote sensing images plays a crucial role in a wide variety of practical applications, including land cover mapping, environmental protection, and economic assessment. In the last decade, convolutional neural network (CNN) is the mainstream deep learning-based method of semantic segmentation. Compared with conventional methods, CNN-based methods learn semantic features automatically, thereby achieving strong representation capability. However, the local receptive field of the convolution operation limits CNN-based methods from capturing long-range dependencies. In contrast, Vision Transformer (ViT) demonstrates its great potential in modeling long-range dependencies and obtains superior results in semantic segmentation. Inspired by this, in this letter, we propose a class-guided Swin Transformer (CG-Swin) for semantic segmentation of remote sensing images. Specifically, we adopt a Transformer-based encoderdecoder structure, which introduces the Swin Transformer backbone as the encoder and designs a class-guided Transformer block to construct the decoder. The experimental results on ISPRS Vaihingen and Potsdam datasets demonstrate the significant breakthrough of the proposed method over ten benchmarks, outperforming both advanced CNN-based and recent Transformer-based approaches.",
      "intriguing_abstract": "Semantic segmentation of remote sensing images plays a crucial role in a wide variety of practical applications, including land cover mapping, environmental protection, and economic assessment. In the last decade, convolutional neural network (CNN) is the mainstream deep learning-based method of semantic segmentation. Compared with conventional methods, CNN-based methods learn semantic features automatically, thereby achieving strong representation capability. However, the local receptive field of the convolution operation limits CNN-based methods from capturing long-range dependencies. In contrast, Vision Transformer (ViT) demonstrates its great potential in modeling long-range dependencies and obtains superior results in semantic segmentation. Inspired by this, in this letter, we propose a class-guided Swin Transformer (CG-Swin) for semantic segmentation of remote sensing images. Specifically, we adopt a Transformer-based encoderdecoder structure, which introduces the Swin Transformer backbone as the encoder and designs a class-guided Transformer block to construct the decoder. The experimental results on ISPRS Vaihingen and Potsdam datasets demonstrate the significant breakthrough of the proposed method over ten benchmarks, outperforming both advanced CNN-based and recent Transformer-based approaches.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/011f59c91bbee6de780d35ebe50fff62087e5b13.pdf",
      "citation_key": "meng2022t3x",
      "metadata": {
        "title": "Class-Guided Swin Transformer for Semantic Segmentation of Remote Sensing Imagery",
        "authors": [
          "Xiaoliang Meng",
          "Yuechi Yang",
          "Libo Wang",
          "Teng Wang",
          "Rui Li",
          "Ce Zhang"
        ],
        "published_date": "2022",
        "abstract": "Semantic segmentation of remote sensing images plays a crucial role in a wide variety of practical applications, including land cover mapping, environmental protection, and economic assessment. In the last decade, convolutional neural network (CNN) is the mainstream deep learning-based method of semantic segmentation. Compared with conventional methods, CNN-based methods learn semantic features automatically, thereby achieving strong representation capability. However, the local receptive field of the convolution operation limits CNN-based methods from capturing long-range dependencies. In contrast, Vision Transformer (ViT) demonstrates its great potential in modeling long-range dependencies and obtains superior results in semantic segmentation. Inspired by this, in this letter, we propose a class-guided Swin Transformer (CG-Swin) for semantic segmentation of remote sensing images. Specifically, we adopt a Transformer-based encoderdecoder structure, which introduces the Swin Transformer backbone as the encoder and designs a class-guided Transformer block to construct the decoder. The experimental results on ISPRS Vaihingen and Potsdam datasets demonstrate the significant breakthrough of the proposed method over ten benchmarks, outperforming both advanced CNN-based and recent Transformer-based approaches.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/011f59c91bbee6de780d35ebe50fff62087e5b13.pdf",
        "venue": "IEEE Geoscience and Remote Sensing Letters",
        "citationCount": 60,
        "score": 20.0,
        "summary": "Semantic segmentation of remote sensing images plays a crucial role in a wide variety of practical applications, including land cover mapping, environmental protection, and economic assessment. In the last decade, convolutional neural network (CNN) is the mainstream deep learning-based method of semantic segmentation. Compared with conventional methods, CNN-based methods learn semantic features automatically, thereby achieving strong representation capability. However, the local receptive field of the convolution operation limits CNN-based methods from capturing long-range dependencies. In contrast, Vision Transformer (ViT) demonstrates its great potential in modeling long-range dependencies and obtains superior results in semantic segmentation. Inspired by this, in this letter, we propose a class-guided Swin Transformer (CG-Swin) for semantic segmentation of remote sensing images. Specifically, we adopt a Transformer-based encoderdecoder structure, which introduces the Swin Transformer backbone as the encoder and designs a class-guided Transformer block to construct the decoder. The experimental results on ISPRS Vaihingen and Potsdam datasets demonstrate the significant breakthrough of the proposed method over ten benchmarks, outperforming both advanced CNN-based and recent Transformer-based approaches.",
        "keywords": []
      },
      "file_name": "011f59c91bbee6de780d35ebe50fff62087e5b13.pdf"
    },
    {
      "success": true,
      "doc_id": "0f77fca834cffc4ddbed9dd67b63afe1",
      "summary": "Vision transformers (ViTs) are emerging with significantly improved accuracy in computer vision tasks. However, their complex architecture and enormous computation/storage demand impose urgent needs for new hardware accelerator design methodology. This work proposes an FPGA-aware automatic ViT acceleration framework based on the proposed mixed-scheme quantization. To the best of our knowledge, this is the first FPGA-based ViT acceleration framework exploring model quantization. Compared with state-of-the-art ViT quantization work (algorithmic approach only without hardware acceleration), our quantization achieves 0.47% to 1.36% higher Top-l accuracy under the same bit-width. Compared with the 32-bit floating-point baseline FPGA accelerator, our accelerator achieves around 5.6x improvement on the frame rate (i.e., 56.8 FPS vs. 10.0 FPS) with 0.71% accuracy drop on ImageNet dataset for DeiT-base.",
      "intriguing_abstract": "Vision transformers (ViTs) are emerging with significantly improved accuracy in computer vision tasks. However, their complex architecture and enormous computation/storage demand impose urgent needs for new hardware accelerator design methodology. This work proposes an FPGA-aware automatic ViT acceleration framework based on the proposed mixed-scheme quantization. To the best of our knowledge, this is the first FPGA-based ViT acceleration framework exploring model quantization. Compared with state-of-the-art ViT quantization work (algorithmic approach only without hardware acceleration), our quantization achieves 0.47% to 1.36% higher Top-l accuracy under the same bit-width. Compared with the 32-bit floating-point baseline FPGA accelerator, our accelerator achieves around 5.6x improvement on the frame rate (i.e., 56.8 FPS vs. 10.0 FPS) with 0.71% accuracy drop on ImageNet dataset for DeiT-base.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f27c847e2909f30745f4a3528b574f5acfd76ea7.pdf",
      "citation_key": "li20223n5",
      "metadata": {
        "title": "Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization",
        "authors": [
          "Z. Li",
          "Mengshu Sun",
          "Alec Lu",
          "Haoyu Ma",
          "Geng Yuan",
          "Yanyue Xie",
          "Hao Tang",
          "Yanyu Li",
          "M. Leeser",
          "Zhangyang Wang",
          "Xue Lin",
          "Zhenman Fang"
        ],
        "published_date": "2022",
        "abstract": "Vision transformers (ViTs) are emerging with significantly improved accuracy in computer vision tasks. However, their complex architecture and enormous computation/storage demand impose urgent needs for new hardware accelerator design methodology. This work proposes an FPGA-aware automatic ViT acceleration framework based on the proposed mixed-scheme quantization. To the best of our knowledge, this is the first FPGA-based ViT acceleration framework exploring model quantization. Compared with state-of-the-art ViT quantization work (algorithmic approach only without hardware acceleration), our quantization achieves 0.47% to 1.36% higher Top-l accuracy under the same bit-width. Compared with the 32-bit floating-point baseline FPGA accelerator, our accelerator achieves around 5.6x improvement on the frame rate (i.e., 56.8 FPS vs. 10.0 FPS) with 0.71% accuracy drop on ImageNet dataset for DeiT-base.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f27c847e2909f30745f4a3528b574f5acfd76ea7.pdf",
        "venue": "International Conference on Field-Programmable Logic and Applications",
        "citationCount": 59,
        "score": 19.666666666666664,
        "summary": "Vision transformers (ViTs) are emerging with significantly improved accuracy in computer vision tasks. However, their complex architecture and enormous computation/storage demand impose urgent needs for new hardware accelerator design methodology. This work proposes an FPGA-aware automatic ViT acceleration framework based on the proposed mixed-scheme quantization. To the best of our knowledge, this is the first FPGA-based ViT acceleration framework exploring model quantization. Compared with state-of-the-art ViT quantization work (algorithmic approach only without hardware acceleration), our quantization achieves 0.47% to 1.36% higher Top-l accuracy under the same bit-width. Compared with the 32-bit floating-point baseline FPGA accelerator, our accelerator achieves around 5.6x improvement on the frame rate (i.e., 56.8 FPS vs. 10.0 FPS) with 0.71% accuracy drop on ImageNet dataset for DeiT-base.",
        "keywords": []
      },
      "file_name": "f27c847e2909f30745f4a3528b574f5acfd76ea7.pdf"
    },
    {
      "success": true,
      "doc_id": "d12b6d8bd7c639f8659f99431482f528",
      "summary": "Recently, vision-language models based on transformers are gaining popularity for joint modeling of visual and textual modalities. In particular, they show impressive results when transferred to several downstream tasks such as zero and few-shot classification. In this article, we propose a visual question answering (VQA) approach for remote sensing images based on these models. The VQA task attempts to provide answers to image-related questions. In contrast, VQA has gained popularity in computer vision, in remote sensing, it is not widespread. First, we use the contrastive language image pretraining (CLIP) network for embedding the image patches and question words into a sequence of visual and textual representations. Then, we learn attention mechanisms to capture the intradependencies and interdependencies within and between these representations. Afterward, we generate the final answer by averaging the predictions of two classifiers mounted on the top of the resulting contextual representations. In the experiments, we study the performance of the proposed approach on two datasets acquired with Sentinel-2 and aerial sensors. In particular, we demonstrate that our approach can achieve better results with reduced training size compared with the recent state-of-the-art.",
      "intriguing_abstract": "Recently, vision-language models based on transformers are gaining popularity for joint modeling of visual and textual modalities. In particular, they show impressive results when transferred to several downstream tasks such as zero and few-shot classification. In this article, we propose a visual question answering (VQA) approach for remote sensing images based on these models. The VQA task attempts to provide answers to image-related questions. In contrast, VQA has gained popularity in computer vision, in remote sensing, it is not widespread. First, we use the contrastive language image pretraining (CLIP) network for embedding the image patches and question words into a sequence of visual and textual representations. Then, we learn attention mechanisms to capture the intradependencies and interdependencies within and between these representations. Afterward, we generate the final answer by averaging the predictions of two classifiers mounted on the top of the resulting contextual representations. In the experiments, we study the performance of the proposed approach on two datasets acquired with Sentinel-2 and aerial sensors. In particular, we demonstrate that our approach can achieve better results with reduced training size compared with the recent state-of-the-art.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d717fa5a4355b5033e660ad58a7ae7f4ccce6939.pdf",
      "citation_key": "bazi2022tlu",
      "metadata": {
        "title": "Bi-Modal Transformer-Based Approach for Visual Question Answering in Remote Sensing Imagery",
        "authors": [
          "Y. Bazi",
          "Mohamad Mahmoud Al Rahhal",
          "M. L. Mekhalfi",
          "M. Zuair",
          "F. Melgani"
        ],
        "published_date": "2022",
        "abstract": "Recently, vision-language models based on transformers are gaining popularity for joint modeling of visual and textual modalities. In particular, they show impressive results when transferred to several downstream tasks such as zero and few-shot classification. In this article, we propose a visual question answering (VQA) approach for remote sensing images based on these models. The VQA task attempts to provide answers to image-related questions. In contrast, VQA has gained popularity in computer vision, in remote sensing, it is not widespread. First, we use the contrastive language image pretraining (CLIP) network for embedding the image patches and question words into a sequence of visual and textual representations. Then, we learn attention mechanisms to capture the intradependencies and interdependencies within and between these representations. Afterward, we generate the final answer by averaging the predictions of two classifiers mounted on the top of the resulting contextual representations. In the experiments, we study the performance of the proposed approach on two datasets acquired with Sentinel-2 and aerial sensors. In particular, we demonstrate that our approach can achieve better results with reduced training size compared with the recent state-of-the-art.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d717fa5a4355b5033e660ad58a7ae7f4ccce6939.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 58,
        "score": 19.333333333333332,
        "summary": "Recently, vision-language models based on transformers are gaining popularity for joint modeling of visual and textual modalities. In particular, they show impressive results when transferred to several downstream tasks such as zero and few-shot classification. In this article, we propose a visual question answering (VQA) approach for remote sensing images based on these models. The VQA task attempts to provide answers to image-related questions. In contrast, VQA has gained popularity in computer vision, in remote sensing, it is not widespread. First, we use the contrastive language image pretraining (CLIP) network for embedding the image patches and question words into a sequence of visual and textual representations. Then, we learn attention mechanisms to capture the intradependencies and interdependencies within and between these representations. Afterward, we generate the final answer by averaging the predictions of two classifiers mounted on the top of the resulting contextual representations. In the experiments, we study the performance of the proposed approach on two datasets acquired with Sentinel-2 and aerial sensors. In particular, we demonstrate that our approach can achieve better results with reduced training size compared with the recent state-of-the-art.",
        "keywords": []
      },
      "file_name": "d717fa5a4355b5033e660ad58a7ae7f4ccce6939.pdf"
    },
    {
      "success": true,
      "doc_id": "a51772db7009f862ad332a069dec0168",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/a119cc83788701313d94746baecd2df5dd30199d.pdf",
      "citation_key": "zheng2022gg5",
      "metadata": {
        "title": "Swin-MLP: a strawberry appearance quality identification method by Swin Transformer and multi-layer perceptron",
        "authors": [
          "Hao Zheng",
          "Guohui Wang",
          "Xuchen Li"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a119cc83788701313d94746baecd2df5dd30199d.pdf",
        "venue": "Journal of Food Measurement & Characterization",
        "citationCount": 57,
        "score": 19.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "a119cc83788701313d94746baecd2df5dd30199d.pdf"
    },
    {
      "success": true,
      "doc_id": "c8d12739075c2c1a58b35c3cbd41505e",
      "summary": "This paper is responding to the MIA-COV19 challenge to classify COVID from non-COVID based on CT lung images. The COVID-19 virus has devastated the world in the last eighteen months by infecting more than 182 million people and causing over 3.9 million deaths. The overarching aim is to predict the diagnosis of the COVID-19 virus from chest radiographs, through the development of explainable vision transformer deep learning techniques, leading to population screening in a more rapid, accurate and transparent way. In this competition, there are 5381 three-dimensional (3D) datasets in total, including 1552 for training, 374 for evaluation and 3455 for testing. While most of the data volumes are in axial view, there are a number of subjects' data are in coronal or sagittal views with 1 or 2 slices are in axial view. Hence, while 3D data based classification is investigated, in this competition, 2D images remains the main focus. Two deep learning methods are studied, which are vision transformer (ViT) based on attention models and DenseNet that is built upon conventional convolutional neural network (CNN). Initial evaluation results based on validation datasets whereby the ground truth is known indicate that ViT performs better than DenseNet with F1 scores being 0.76 and 0.72 respectively. Codes are available at GitHub at <https://github/xiaohong1/COVID-ViT>.",
      "intriguing_abstract": "This paper is responding to the MIA-COV19 challenge to classify COVID from non-COVID based on CT lung images. The COVID-19 virus has devastated the world in the last eighteen months by infecting more than 182 million people and causing over 3.9 million deaths. The overarching aim is to predict the diagnosis of the COVID-19 virus from chest radiographs, through the development of explainable vision transformer deep learning techniques, leading to population screening in a more rapid, accurate and transparent way. In this competition, there are 5381 three-dimensional (3D) datasets in total, including 1552 for training, 374 for evaluation and 3455 for testing. While most of the data volumes are in axial view, there are a number of subjects' data are in coronal or sagittal views with 1 or 2 slices are in axial view. Hence, while 3D data based classification is investigated, in this competition, 2D images remains the main focus. Two deep learning methods are studied, which are vision transformer (ViT) based on attention models and DenseNet that is built upon conventional convolutional neural network (CNN). Initial evaluation results based on validation datasets whereby the ground truth is known indicate that ViT performs better than DenseNet with F1 scores being 0.76 and 0.72 respectively. Codes are available at GitHub at <https://github/xiaohong1/COVID-ViT>.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/60f9fd4e6b29e6f11324ba4f6d3349a2d8107a1b.pdf",
      "citation_key": "gao2021uzl",
      "metadata": {
        "title": "COVID-VIT: Classification of COVID-19 from CT chest images based on vision transformer models",
        "authors": [
          "Xiaohong W. Gao",
          "Y. Qian",
          "Alice Gao"
        ],
        "published_date": "2021",
        "abstract": "This paper is responding to the MIA-COV19 challenge to classify COVID from non-COVID based on CT lung images. The COVID-19 virus has devastated the world in the last eighteen months by infecting more than 182 million people and causing over 3.9 million deaths. The overarching aim is to predict the diagnosis of the COVID-19 virus from chest radiographs, through the development of explainable vision transformer deep learning techniques, leading to population screening in a more rapid, accurate and transparent way. In this competition, there are 5381 three-dimensional (3D) datasets in total, including 1552 for training, 374 for evaluation and 3455 for testing. While most of the data volumes are in axial view, there are a number of subjects' data are in coronal or sagittal views with 1 or 2 slices are in axial view. Hence, while 3D data based classification is investigated, in this competition, 2D images remains the main focus. Two deep learning methods are studied, which are vision transformer (ViT) based on attention models and DenseNet that is built upon conventional convolutional neural network (CNN). Initial evaluation results based on validation datasets whereby the ground truth is known indicate that ViT performs better than DenseNet with F1 scores being 0.76 and 0.72 respectively. Codes are available at GitHub at <https://github/xiaohong1/COVID-ViT>.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/60f9fd4e6b29e6f11324ba4f6d3349a2d8107a1b.pdf",
        "venue": "arXiv.org",
        "citationCount": 75,
        "score": 18.75,
        "summary": "This paper is responding to the MIA-COV19 challenge to classify COVID from non-COVID based on CT lung images. The COVID-19 virus has devastated the world in the last eighteen months by infecting more than 182 million people and causing over 3.9 million deaths. The overarching aim is to predict the diagnosis of the COVID-19 virus from chest radiographs, through the development of explainable vision transformer deep learning techniques, leading to population screening in a more rapid, accurate and transparent way. In this competition, there are 5381 three-dimensional (3D) datasets in total, including 1552 for training, 374 for evaluation and 3455 for testing. While most of the data volumes are in axial view, there are a number of subjects' data are in coronal or sagittal views with 1 or 2 slices are in axial view. Hence, while 3D data based classification is investigated, in this competition, 2D images remains the main focus. Two deep learning methods are studied, which are vision transformer (ViT) based on attention models and DenseNet that is built upon conventional convolutional neural network (CNN). Initial evaluation results based on validation datasets whereby the ground truth is known indicate that ViT performs better than DenseNet with F1 scores being 0.76 and 0.72 respectively. Codes are available at GitHub at <https://github/xiaohong1/COVID-ViT>.",
        "keywords": []
      },
      "file_name": "60f9fd4e6b29e6f11324ba4f6d3349a2d8107a1b.pdf"
    },
    {
      "success": true,
      "doc_id": "7ac92d63ee7a3d1fef91de25e71364ea",
      "summary": "Though vision transformers (ViTs) have exhibited impressive ability for representation learning, we empirically find that they cannot generalize well to unseen domains with previous domain generalization algorithms. In this paper, we propose a novel approach DoPrompt based on prompt learning to embed the knowledge of source domains in domain prompts for target domain prediction. Specifically, domain prompts are prepended before ViT input tokens from the corresponding source domain. Each domain prompt learns domain-specific knowledge efficiently since it is optimized only for one domain. Meanwhile, we train a prompt adapter to produce a suitable prompt for each input image based on the learned source domain prompts. At test time, the adapted prompt generated by the prompt adapter can exploit the similarity between the feature of the out-of-domain image and source domains to properly integrate the source domain knowledge. Extensive experiments are conducted on four benchmark datasets. Our approach achieves 1.4% improvements in the averaged accuracy, which is 3.5 times the improvement of the state-of-the-art algorithm with a ViT backbone.",
      "intriguing_abstract": "Though vision transformers (ViTs) have exhibited impressive ability for representation learning, we empirically find that they cannot generalize well to unseen domains with previous domain generalization algorithms. In this paper, we propose a novel approach DoPrompt based on prompt learning to embed the knowledge of source domains in domain prompts for target domain prediction. Specifically, domain prompts are prepended before ViT input tokens from the corresponding source domain. Each domain prompt learns domain-specific knowledge efficiently since it is optimized only for one domain. Meanwhile, we train a prompt adapter to produce a suitable prompt for each input image based on the learned source domain prompts. At test time, the adapted prompt generated by the prompt adapter can exploit the similarity between the feature of the out-of-domain image and source domains to properly integrate the source domain knowledge. Extensive experiments are conducted on four benchmark datasets. Our approach achieves 1.4% improvements in the averaged accuracy, which is 3.5 times the improvement of the state-of-the-art algorithm with a ViT backbone.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/5ca02297d8d49f03f26148b74fea77272d09c78b.pdf",
      "citation_key": "zheng202218g",
      "metadata": {
        "title": "Prompt Vision Transformer for Domain Generalization",
        "authors": [
          "Zangwei Zheng",
          "Xiangyu Yue",
          "Kai Wang",
          "Yang You"
        ],
        "published_date": "2022",
        "abstract": "Though vision transformers (ViTs) have exhibited impressive ability for representation learning, we empirically find that they cannot generalize well to unseen domains with previous domain generalization algorithms. In this paper, we propose a novel approach DoPrompt based on prompt learning to embed the knowledge of source domains in domain prompts for target domain prediction. Specifically, domain prompts are prepended before ViT input tokens from the corresponding source domain. Each domain prompt learns domain-specific knowledge efficiently since it is optimized only for one domain. Meanwhile, we train a prompt adapter to produce a suitable prompt for each input image based on the learned source domain prompts. At test time, the adapted prompt generated by the prompt adapter can exploit the similarity between the feature of the out-of-domain image and source domains to properly integrate the source domain knowledge. Extensive experiments are conducted on four benchmark datasets. Our approach achieves 1.4% improvements in the averaged accuracy, which is 3.5 times the improvement of the state-of-the-art algorithm with a ViT backbone.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/5ca02297d8d49f03f26148b74fea77272d09c78b.pdf",
        "venue": "arXiv.org",
        "citationCount": 56,
        "score": 18.666666666666664,
        "summary": "Though vision transformers (ViTs) have exhibited impressive ability for representation learning, we empirically find that they cannot generalize well to unseen domains with previous domain generalization algorithms. In this paper, we propose a novel approach DoPrompt based on prompt learning to embed the knowledge of source domains in domain prompts for target domain prediction. Specifically, domain prompts are prepended before ViT input tokens from the corresponding source domain. Each domain prompt learns domain-specific knowledge efficiently since it is optimized only for one domain. Meanwhile, we train a prompt adapter to produce a suitable prompt for each input image based on the learned source domain prompts. At test time, the adapted prompt generated by the prompt adapter can exploit the similarity between the feature of the out-of-domain image and source domains to properly integrate the source domain knowledge. Extensive experiments are conducted on four benchmark datasets. Our approach achieves 1.4% improvements in the averaged accuracy, which is 3.5 times the improvement of the state-of-the-art algorithm with a ViT backbone.",
        "keywords": []
      },
      "file_name": "5ca02297d8d49f03f26148b74fea77272d09c78b.pdf"
    },
    {
      "success": true,
      "doc_id": "947ac8df43f26a2dc1a7a371bc456594",
      "summary": "In order to solve the problems of high subjectivity, frequent error occurrence and easy damage of traditional corn seed identification methods, this paper combines deep learning with machine vision and the utilization of the basis of the Swin Transformer to improve maize seed recognition. The study was focused on feature attention and multi-scale feature fusion learning. Firstly, input the seed image into the network to obtain shallow features and deep features; secondly, a feature attention layer was introduced to give weights to different stages of features to strengthen and suppress; and finally, the shallow features and deep features were fused to construct multi-scale fusion features of corn seed images, and the seed images are divided into 19 varieties through a classifier. The experimental results showed that the average precision, recall and F1 values of the MFSwin Transformer model on the test set were 96.53%, 96.46%, and 96.47%, respectively, and the parameter memory is 12.83 M. Compared to other models, the MFSwin Transformer model achieved the highest classification accuracy results. Therefore, the neural network proposed in this paper can classify corn seeds accurately and efficiently, could meet the high-precision classification requirements of corn seed images, and provide a reference tool for seed identification.",
      "intriguing_abstract": "In order to solve the problems of high subjectivity, frequent error occurrence and easy damage of traditional corn seed identification methods, this paper combines deep learning with machine vision and the utilization of the basis of the Swin Transformer to improve maize seed recognition. The study was focused on feature attention and multi-scale feature fusion learning. Firstly, input the seed image into the network to obtain shallow features and deep features; secondly, a feature attention layer was introduced to give weights to different stages of features to strengthen and suppress; and finally, the shallow features and deep features were fused to construct multi-scale fusion features of corn seed images, and the seed images are divided into 19 varieties through a classifier. The experimental results showed that the average precision, recall and F1 values of the MFSwin Transformer model on the test set were 96.53%, 96.46%, and 96.47%, respectively, and the parameter memory is 12.83 M. Compared to other models, the MFSwin Transformer model achieved the highest classification accuracy results. Therefore, the neural network proposed in this paper can classify corn seeds accurately and efficiently, could meet the high-precision classification requirements of corn seed images, and provide a reference tool for seed identification.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/aed7e4bc195d838735c320ac40a78f123206831b.pdf",
      "citation_key": "bi20225lu",
      "metadata": {
        "title": "Development of Deep Learning Methodology for Maize Seed Variety Recognition Based on Improved Swin Transformer",
        "authors": [
          "Chunguang Bi",
          "Nan Hu",
          "Yiqiang Zou",
          "Shuo Zhang",
          "Suzhen Xu",
          "Helong Yu"
        ],
        "published_date": "2022",
        "abstract": "In order to solve the problems of high subjectivity, frequent error occurrence and easy damage of traditional corn seed identification methods, this paper combines deep learning with machine vision and the utilization of the basis of the Swin Transformer to improve maize seed recognition. The study was focused on feature attention and multi-scale feature fusion learning. Firstly, input the seed image into the network to obtain shallow features and deep features; secondly, a feature attention layer was introduced to give weights to different stages of features to strengthen and suppress; and finally, the shallow features and deep features were fused to construct multi-scale fusion features of corn seed images, and the seed images are divided into 19 varieties through a classifier. The experimental results showed that the average precision, recall and F1 values of the MFSwin Transformer model on the test set were 96.53%, 96.46%, and 96.47%, respectively, and the parameter memory is 12.83 M. Compared to other models, the MFSwin Transformer model achieved the highest classification accuracy results. Therefore, the neural network proposed in this paper can classify corn seeds accurately and efficiently, could meet the high-precision classification requirements of corn seed images, and provide a reference tool for seed identification.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/aed7e4bc195d838735c320ac40a78f123206831b.pdf",
        "venue": "Agronomy",
        "citationCount": 55,
        "score": 18.333333333333332,
        "summary": "In order to solve the problems of high subjectivity, frequent error occurrence and easy damage of traditional corn seed identification methods, this paper combines deep learning with machine vision and the utilization of the basis of the Swin Transformer to improve maize seed recognition. The study was focused on feature attention and multi-scale feature fusion learning. Firstly, input the seed image into the network to obtain shallow features and deep features; secondly, a feature attention layer was introduced to give weights to different stages of features to strengthen and suppress; and finally, the shallow features and deep features were fused to construct multi-scale fusion features of corn seed images, and the seed images are divided into 19 varieties through a classifier. The experimental results showed that the average precision, recall and F1 values of the MFSwin Transformer model on the test set were 96.53%, 96.46%, and 96.47%, respectively, and the parameter memory is 12.83 M. Compared to other models, the MFSwin Transformer model achieved the highest classification accuracy results. Therefore, the neural network proposed in this paper can classify corn seeds accurately and efficiently, could meet the high-precision classification requirements of corn seed images, and provide a reference tool for seed identification.",
        "keywords": []
      },
      "file_name": "aed7e4bc195d838735c320ac40a78f123206831b.pdf"
    },
    {
      "success": true,
      "doc_id": "5f2d92e4860ced4f43e319c3a56f441e",
      "summary": "Traditional automatic pavement distress detection methods using convolutional neural networks (CNNs) require a great deal of time and resources for computing and are poor in terms of interpretability. Therefore, inspired by the successful application of Transformer architecture in natural language processing (NLP) tasks, a novel Transformer method called LeViT was introduced for automatic asphalt pavement image classification. LeViT consists of convolutional layers, transformer stages where Multi-layer Perception (MLP) and multi-head self-attention blocks alternate using the residual connection, and two classifier heads. To conduct the proposed methods, three different sources of pavement image datasets and pre-trained weights based on ImageNet were attained. The performance of the proposed model was compared with six state-of-the-art (SOTA) deep learning models. All of them were trained based on transfer learning strategy. Compared to the tested SOTA methods, LeViT has less than 1/8 of the parameters of the original Vision Transformer (ViT) and 1/2 of ResNet and InceptionNet. Experimental results show that after training for 100 epochs with a 16 batch-size, the proposed method acquired 91.56% accuracy, 91.72% precision, 91.56% recall, and 91.45% F1-score in the Chinese asphalt pavement dataset and 99.17% accuracy, 99.19% precision, 99.17% recall, and 99.17% F1-score in the German asphalt pavement dataset, which is the best performance among all the tested SOTA models. Moreover, it shows superiority in inference speed (86 ms/step), which is approximately 25% of the original ViT method and 80% of some prevailing CNN-based models, including DenseNet, VGG, and ResNet. Overall, the proposed method can achieve competitive performance with fewer computation costs. In addition, a visualization method combining Grad-CAM and Attention Rollout was proposed to analyze the classification results and explore what has been learned in every MLP and attention block of LeViT, which improved the interpretability of the proposed pavement image classification model.",
      "intriguing_abstract": "Traditional automatic pavement distress detection methods using convolutional neural networks (CNNs) require a great deal of time and resources for computing and are poor in terms of interpretability. Therefore, inspired by the successful application of Transformer architecture in natural language processing (NLP) tasks, a novel Transformer method called LeViT was introduced for automatic asphalt pavement image classification. LeViT consists of convolutional layers, transformer stages where Multi-layer Perception (MLP) and multi-head self-attention blocks alternate using the residual connection, and two classifier heads. To conduct the proposed methods, three different sources of pavement image datasets and pre-trained weights based on ImageNet were attained. The performance of the proposed model was compared with six state-of-the-art (SOTA) deep learning models. All of them were trained based on transfer learning strategy. Compared to the tested SOTA methods, LeViT has less than 1/8 of the parameters of the original Vision Transformer (ViT) and 1/2 of ResNet and InceptionNet. Experimental results show that after training for 100 epochs with a 16 batch-size, the proposed method acquired 91.56% accuracy, 91.72% precision, 91.56% recall, and 91.45% F1-score in the Chinese asphalt pavement dataset and 99.17% accuracy, 99.19% precision, 99.17% recall, and 99.17% F1-score in the German asphalt pavement dataset, which is the best performance among all the tested SOTA models. Moreover, it shows superiority in inference speed (86 ms/step), which is approximately 25% of the original ViT method and 80% of some prevailing CNN-based models, including DenseNet, VGG, and ResNet. Overall, the proposed method can achieve competitive performance with fewer computation costs. In addition, a visualization method combining Grad-CAM and Attention Rollout was proposed to analyze the classification results and explore what has been learned in every MLP and attention block of LeViT, which improved the interpretability of the proposed pavement image classification model.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/b66e4257aa8856df537f03f6a12341f489eb6500.pdf",
      "citation_key": "chen2022vac",
      "metadata": {
        "title": "A Fast Inference Vision Transformer for Automatic Pavement Image Classification and Its Visual Interpretation Method",
        "authors": [
          "Yihan Chen",
          "Xingyu Gu",
          "Zhen Liu",
          "Jia-Yun Liang"
        ],
        "published_date": "2022",
        "abstract": "Traditional automatic pavement distress detection methods using convolutional neural networks (CNNs) require a great deal of time and resources for computing and are poor in terms of interpretability. Therefore, inspired by the successful application of Transformer architecture in natural language processing (NLP) tasks, a novel Transformer method called LeViT was introduced for automatic asphalt pavement image classification. LeViT consists of convolutional layers, transformer stages where Multi-layer Perception (MLP) and multi-head self-attention blocks alternate using the residual connection, and two classifier heads. To conduct the proposed methods, three different sources of pavement image datasets and pre-trained weights based on ImageNet were attained. The performance of the proposed model was compared with six state-of-the-art (SOTA) deep learning models. All of them were trained based on transfer learning strategy. Compared to the tested SOTA methods, LeViT has less than 1/8 of the parameters of the original Vision Transformer (ViT) and 1/2 of ResNet and InceptionNet. Experimental results show that after training for 100 epochs with a 16 batch-size, the proposed method acquired 91.56% accuracy, 91.72% precision, 91.56% recall, and 91.45% F1-score in the Chinese asphalt pavement dataset and 99.17% accuracy, 99.19% precision, 99.17% recall, and 99.17% F1-score in the German asphalt pavement dataset, which is the best performance among all the tested SOTA models. Moreover, it shows superiority in inference speed (86 ms/step), which is approximately 25% of the original ViT method and 80% of some prevailing CNN-based models, including DenseNet, VGG, and ResNet. Overall, the proposed method can achieve competitive performance with fewer computation costs. In addition, a visualization method combining Grad-CAM and Attention Rollout was proposed to analyze the classification results and explore what has been learned in every MLP and attention block of LeViT, which improved the interpretability of the proposed pavement image classification model.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b66e4257aa8856df537f03f6a12341f489eb6500.pdf",
        "venue": "Remote Sensing",
        "citationCount": 52,
        "score": 17.333333333333332,
        "summary": "Traditional automatic pavement distress detection methods using convolutional neural networks (CNNs) require a great deal of time and resources for computing and are poor in terms of interpretability. Therefore, inspired by the successful application of Transformer architecture in natural language processing (NLP) tasks, a novel Transformer method called LeViT was introduced for automatic asphalt pavement image classification. LeViT consists of convolutional layers, transformer stages where Multi-layer Perception (MLP) and multi-head self-attention blocks alternate using the residual connection, and two classifier heads. To conduct the proposed methods, three different sources of pavement image datasets and pre-trained weights based on ImageNet were attained. The performance of the proposed model was compared with six state-of-the-art (SOTA) deep learning models. All of them were trained based on transfer learning strategy. Compared to the tested SOTA methods, LeViT has less than 1/8 of the parameters of the original Vision Transformer (ViT) and 1/2 of ResNet and InceptionNet. Experimental results show that after training for 100 epochs with a 16 batch-size, the proposed method acquired 91.56% accuracy, 91.72% precision, 91.56% recall, and 91.45% F1-score in the Chinese asphalt pavement dataset and 99.17% accuracy, 99.19% precision, 99.17% recall, and 99.17% F1-score in the German asphalt pavement dataset, which is the best performance among all the tested SOTA models. Moreover, it shows superiority in inference speed (86 ms/step), which is approximately 25% of the original ViT method and 80% of some prevailing CNN-based models, including DenseNet, VGG, and ResNet. Overall, the proposed method can achieve competitive performance with fewer computation costs. In addition, a visualization method combining Grad-CAM and Attention Rollout was proposed to analyze the classification results and explore what has been learned in every MLP and attention block of LeViT, which improved the interpretability of the proposed pavement image classification model.",
        "keywords": []
      },
      "file_name": "b66e4257aa8856df537f03f6a12341f489eb6500.pdf"
    },
    {
      "success": true,
      "doc_id": "57752050d6d51304359589171132f2cd",
      "summary": "Vision transformer (ViT) has achieved competitive accuracy on a variety of computer vision applications, but its computational cost impedes the deployment on resource-limited mobile devices. We explore the sparsity in ViT and observe that informative patches and heads are sufficient for accurate image recognition. In this paper, we propose a cascade pruning framework named CP-ViT by predicting sparsity in ViT models progressively and dynamically to reduce computational redundancy while minimizing the accuracy loss. Specifically, we define the cumulative score to reserve the informative patches and heads across the ViT model for better accuracy. We also propose the dynamic pruning ratio adjustment technique based on layer-aware attention range. CP-ViT has great general applicability for practical deployment, which can be applied to a wide range of ViT models and can achieve superior accuracy with or without fine-tuning. Extensive experiments on ImageNet, CIFAR-10, and CIFAR-100 with various pre-trained models have demonstrated the effectiveness and efficiency of CP-ViT. By progressively pruning 50\\% patches, our CP-ViT method reduces over 40\\% FLOPs while maintaining accuracy loss within 1\\%.",
      "intriguing_abstract": "Vision transformer (ViT) has achieved competitive accuracy on a variety of computer vision applications, but its computational cost impedes the deployment on resource-limited mobile devices. We explore the sparsity in ViT and observe that informative patches and heads are sufficient for accurate image recognition. In this paper, we propose a cascade pruning framework named CP-ViT by predicting sparsity in ViT models progressively and dynamically to reduce computational redundancy while minimizing the accuracy loss. Specifically, we define the cumulative score to reserve the informative patches and heads across the ViT model for better accuracy. We also propose the dynamic pruning ratio adjustment technique based on layer-aware attention range. CP-ViT has great general applicability for practical deployment, which can be applied to a wide range of ViT models and can achieve superior accuracy with or without fine-tuning. Extensive experiments on ImageNet, CIFAR-10, and CIFAR-100 with various pre-trained models have demonstrated the effectiveness and efficiency of CP-ViT. By progressively pruning 50\\% patches, our CP-ViT method reduces over 40\\% FLOPs while maintaining accuracy loss within 1\\%.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f9480350e1986957919d49f346ba20dcab8f5b71.pdf",
      "citation_key": "song2022603",
      "metadata": {
        "title": "CP-ViT: Cascade Vision Transformer Pruning via Progressive Sparsity Prediction",
        "authors": [
          "Zhuoran Song",
          "Yihong Xu",
          "Zhezhi He",
          "Li Jiang",
          "Naifeng Jing",
          "Xiaoyao Liang"
        ],
        "published_date": "2022",
        "abstract": "Vision transformer (ViT) has achieved competitive accuracy on a variety of computer vision applications, but its computational cost impedes the deployment on resource-limited mobile devices. We explore the sparsity in ViT and observe that informative patches and heads are sufficient for accurate image recognition. In this paper, we propose a cascade pruning framework named CP-ViT by predicting sparsity in ViT models progressively and dynamically to reduce computational redundancy while minimizing the accuracy loss. Specifically, we define the cumulative score to reserve the informative patches and heads across the ViT model for better accuracy. We also propose the dynamic pruning ratio adjustment technique based on layer-aware attention range. CP-ViT has great general applicability for practical deployment, which can be applied to a wide range of ViT models and can achieve superior accuracy with or without fine-tuning. Extensive experiments on ImageNet, CIFAR-10, and CIFAR-100 with various pre-trained models have demonstrated the effectiveness and efficiency of CP-ViT. By progressively pruning 50\\% patches, our CP-ViT method reduces over 40\\% FLOPs while maintaining accuracy loss within 1\\%.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f9480350e1986957919d49f346ba20dcab8f5b71.pdf",
        "venue": "arXiv.org",
        "citationCount": 51,
        "score": 17.0,
        "summary": "Vision transformer (ViT) has achieved competitive accuracy on a variety of computer vision applications, but its computational cost impedes the deployment on resource-limited mobile devices. We explore the sparsity in ViT and observe that informative patches and heads are sufficient for accurate image recognition. In this paper, we propose a cascade pruning framework named CP-ViT by predicting sparsity in ViT models progressively and dynamically to reduce computational redundancy while minimizing the accuracy loss. Specifically, we define the cumulative score to reserve the informative patches and heads across the ViT model for better accuracy. We also propose the dynamic pruning ratio adjustment technique based on layer-aware attention range. CP-ViT has great general applicability for practical deployment, which can be applied to a wide range of ViT models and can achieve superior accuracy with or without fine-tuning. Extensive experiments on ImageNet, CIFAR-10, and CIFAR-100 with various pre-trained models have demonstrated the effectiveness and efficiency of CP-ViT. By progressively pruning 50\\% patches, our CP-ViT method reduces over 40\\% FLOPs while maintaining accuracy loss within 1\\%.",
        "keywords": []
      },
      "file_name": "f9480350e1986957919d49f346ba20dcab8f5b71.pdf"
    },
    {
      "success": true,
      "doc_id": "d0a8e3073e0cedde3111865a112baee3",
      "summary": "Deep image hashing aims to map an input image to compact binary codes by deep neural network, to enable efficient image retrieval across large-scale dataset. Due to the explosive growth of modern data, deep hashing has gained growing attention from research community. Recently, convolutional neural networks like ResNet have dominated in deep hashing. Nevertheless, motivated by the recent advancements of vision transformers, we propose a pure transformer-based framework, called as HashFormer, to tackle the deep hashing task. Specifically, we utilize vision transformer (ViT) as our backbone, and treat binary codes as the intermediate representations for our surrogate task, i.e., image classification. In addition, we observe that the binary codes suitable for classification are sub-optimal for retrieval. To mitigate this problem, we present a novel average precision loss, which enables us to directly optimize the retrieval accuracy. To the best of our knowledge, our work is one of the pioneer works to address deep hashing learning problems without convolutional neural networks (CNNs). We perform comprehensive experiments on three widely-studied datasets: CIFAR-10, NUSWIDE and ImageNet. The proposed method demonstrates promising results against existing state-of-the-art works, validating the advantages and merits of our HashFormer.",
      "intriguing_abstract": "Deep image hashing aims to map an input image to compact binary codes by deep neural network, to enable efficient image retrieval across large-scale dataset. Due to the explosive growth of modern data, deep hashing has gained growing attention from research community. Recently, convolutional neural networks like ResNet have dominated in deep hashing. Nevertheless, motivated by the recent advancements of vision transformers, we propose a pure transformer-based framework, called as HashFormer, to tackle the deep hashing task. Specifically, we utilize vision transformer (ViT) as our backbone, and treat binary codes as the intermediate representations for our surrogate task, i.e., image classification. In addition, we observe that the binary codes suitable for classification are sub-optimal for retrieval. To mitigate this problem, we present a novel average precision loss, which enables us to directly optimize the retrieval accuracy. To the best of our knowledge, our work is one of the pioneer works to address deep hashing learning problems without convolutional neural networks (CNNs). We perform comprehensive experiments on three widely-studied datasets: CIFAR-10, NUSWIDE and ImageNet. The proposed method demonstrates promising results against existing state-of-the-art works, validating the advantages and merits of our HashFormer.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/836dd64a4f606931029c5d68e74d81ef5885b622.pdf",
      "citation_key": "li2022rl9",
      "metadata": {
        "title": "HashFormer: Vision Transformer Based Deep Hashing for Image Retrieval",
        "authors": [
          "Tao Li",
          "Zheng Zhang",
          "Lishen Pei",
          "Yan Gan"
        ],
        "published_date": "2022",
        "abstract": "Deep image hashing aims to map an input image to compact binary codes by deep neural network, to enable efficient image retrieval across large-scale dataset. Due to the explosive growth of modern data, deep hashing has gained growing attention from research community. Recently, convolutional neural networks like ResNet have dominated in deep hashing. Nevertheless, motivated by the recent advancements of vision transformers, we propose a pure transformer-based framework, called as HashFormer, to tackle the deep hashing task. Specifically, we utilize vision transformer (ViT) as our backbone, and treat binary codes as the intermediate representations for our surrogate task, i.e., image classification. In addition, we observe that the binary codes suitable for classification are sub-optimal for retrieval. To mitigate this problem, we present a novel average precision loss, which enables us to directly optimize the retrieval accuracy. To the best of our knowledge, our work is one of the pioneer works to address deep hashing learning problems without convolutional neural networks (CNNs). We perform comprehensive experiments on three widely-studied datasets: CIFAR-10, NUSWIDE and ImageNet. The proposed method demonstrates promising results against existing state-of-the-art works, validating the advantages and merits of our HashFormer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/836dd64a4f606931029c5d68e74d81ef5885b622.pdf",
        "venue": "IEEE Signal Processing Letters",
        "citationCount": 51,
        "score": 17.0,
        "summary": "Deep image hashing aims to map an input image to compact binary codes by deep neural network, to enable efficient image retrieval across large-scale dataset. Due to the explosive growth of modern data, deep hashing has gained growing attention from research community. Recently, convolutional neural networks like ResNet have dominated in deep hashing. Nevertheless, motivated by the recent advancements of vision transformers, we propose a pure transformer-based framework, called as HashFormer, to tackle the deep hashing task. Specifically, we utilize vision transformer (ViT) as our backbone, and treat binary codes as the intermediate representations for our surrogate task, i.e., image classification. In addition, we observe that the binary codes suitable for classification are sub-optimal for retrieval. To mitigate this problem, we present a novel average precision loss, which enables us to directly optimize the retrieval accuracy. To the best of our knowledge, our work is one of the pioneer works to address deep hashing learning problems without convolutional neural networks (CNNs). We perform comprehensive experiments on three widely-studied datasets: CIFAR-10, NUSWIDE and ImageNet. The proposed method demonstrates promising results against existing state-of-the-art works, validating the advantages and merits of our HashFormer.",
        "keywords": []
      },
      "file_name": "836dd64a4f606931029c5d68e74d81ef5885b622.pdf"
    },
    {
      "success": true,
      "doc_id": "fd25a628776cad93c0e9afe18a34dabe",
      "summary": "Human activity recognition is an emerging and important area in computer vision which seeks to determine the activity an individual or group of individuals are performing. The applications of this field ranges from generating highlight videos in sports, to intelligent surveillance and gesture recognition. Most activity recognition systems rely on a combination of convolutional neural networks (CNNs) to perform feature extraction from the data and recurrent neural networks (RNNs) to determine the time dependent nature of the data. This paper proposes and designs two transformer neural networks for human activity recognition: a recurrent transformer (ReT), a specialized neural network used to make predictions on sequences of data, as well as a vision transformer (ViT), a transformer optimized for extracting salient features from images, to improve speed and scalability of activity recognition. We have provided an extensive comparison of the proposed transformer neural networks with the contemporary CNN and RNN-based human activity recognition models in terms of speed and accuracy for four publicly available human action datasets. Experimental results reveal that the proposed ViT-ReT framework attains a speedup of $2\\times $ over the baseline ResNet50-LSTM approach while attaining nearly the same level of accuracy. Furthermore, results show that the proposed ViT-ReT framework attains significant improvements over the state-of-the-art human action recognition methods in terms of both model accuracy and runtime for each of the datasets used in our experiments, thus verifying the suitability of the proposed ViT-ReT framework for human activity recognition in resource-constrained and real-time environments.",
      "intriguing_abstract": "Human activity recognition is an emerging and important area in computer vision which seeks to determine the activity an individual or group of individuals are performing. The applications of this field ranges from generating highlight videos in sports, to intelligent surveillance and gesture recognition. Most activity recognition systems rely on a combination of convolutional neural networks (CNNs) to perform feature extraction from the data and recurrent neural networks (RNNs) to determine the time dependent nature of the data. This paper proposes and designs two transformer neural networks for human activity recognition: a recurrent transformer (ReT), a specialized neural network used to make predictions on sequences of data, as well as a vision transformer (ViT), a transformer optimized for extracting salient features from images, to improve speed and scalability of activity recognition. We have provided an extensive comparison of the proposed transformer neural networks with the contemporary CNN and RNN-based human activity recognition models in terms of speed and accuracy for four publicly available human action datasets. Experimental results reveal that the proposed ViT-ReT framework attains a speedup of $2\\times $ over the baseline ResNet50-LSTM approach while attaining nearly the same level of accuracy. Furthermore, results show that the proposed ViT-ReT framework attains significant improvements over the state-of-the-art human action recognition methods in terms of both model accuracy and runtime for each of the datasets used in our experiments, thus verifying the suitability of the proposed ViT-ReT framework for human activity recognition in resource-constrained and real-time environments.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/16fa1a8575ff56781b6b83726906754ed4e5f3a7.pdf",
      "citation_key": "wensel2022lva",
      "metadata": {
        "title": "ViT-ReT: Vision and Recurrent Transformer Neural Networks for Human Activity Recognition in Videos",
        "authors": [
          "James Wensel",
          "Hayat Ullah",
          "Arslan Munir"
        ],
        "published_date": "2022",
        "abstract": "Human activity recognition is an emerging and important area in computer vision which seeks to determine the activity an individual or group of individuals are performing. The applications of this field ranges from generating highlight videos in sports, to intelligent surveillance and gesture recognition. Most activity recognition systems rely on a combination of convolutional neural networks (CNNs) to perform feature extraction from the data and recurrent neural networks (RNNs) to determine the time dependent nature of the data. This paper proposes and designs two transformer neural networks for human activity recognition: a recurrent transformer (ReT), a specialized neural network used to make predictions on sequences of data, as well as a vision transformer (ViT), a transformer optimized for extracting salient features from images, to improve speed and scalability of activity recognition. We have provided an extensive comparison of the proposed transformer neural networks with the contemporary CNN and RNN-based human activity recognition models in terms of speed and accuracy for four publicly available human action datasets. Experimental results reveal that the proposed ViT-ReT framework attains a speedup of $2\\times $ over the baseline ResNet50-LSTM approach while attaining nearly the same level of accuracy. Furthermore, results show that the proposed ViT-ReT framework attains significant improvements over the state-of-the-art human action recognition methods in terms of both model accuracy and runtime for each of the datasets used in our experiments, thus verifying the suitability of the proposed ViT-ReT framework for human activity recognition in resource-constrained and real-time environments.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/16fa1a8575ff56781b6b83726906754ed4e5f3a7.pdf",
        "venue": "IEEE Access",
        "citationCount": 51,
        "score": 17.0,
        "summary": "Human activity recognition is an emerging and important area in computer vision which seeks to determine the activity an individual or group of individuals are performing. The applications of this field ranges from generating highlight videos in sports, to intelligent surveillance and gesture recognition. Most activity recognition systems rely on a combination of convolutional neural networks (CNNs) to perform feature extraction from the data and recurrent neural networks (RNNs) to determine the time dependent nature of the data. This paper proposes and designs two transformer neural networks for human activity recognition: a recurrent transformer (ReT), a specialized neural network used to make predictions on sequences of data, as well as a vision transformer (ViT), a transformer optimized for extracting salient features from images, to improve speed and scalability of activity recognition. We have provided an extensive comparison of the proposed transformer neural networks with the contemporary CNN and RNN-based human activity recognition models in terms of speed and accuracy for four publicly available human action datasets. Experimental results reveal that the proposed ViT-ReT framework attains a speedup of $2\\times $ over the baseline ResNet50-LSTM approach while attaining nearly the same level of accuracy. Furthermore, results show that the proposed ViT-ReT framework attains significant improvements over the state-of-the-art human action recognition methods in terms of both model accuracy and runtime for each of the datasets used in our experiments, thus verifying the suitability of the proposed ViT-ReT framework for human activity recognition in resource-constrained and real-time environments.",
        "keywords": []
      },
      "file_name": "16fa1a8575ff56781b6b83726906754ed4e5f3a7.pdf"
    },
    {
      "success": true,
      "doc_id": "e2f818b5633ec2ffa54ef65d4a28fd19",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/9f7f81b1c82828a45a52df8f0c6a92636af76c7e.pdf",
      "citation_key": "wang2021sav",
      "metadata": {
        "title": "CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention",
        "authors": [
          "Wenxiao Wang",
          "Lu-yuan Yao",
          "Long Chen",
          "Deng Cai",
          "Xiaofei He",
          "Wei Liu"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9f7f81b1c82828a45a52df8f0c6a92636af76c7e.pdf",
        "venue": "arXiv.org",
        "citationCount": 64,
        "score": 16.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "9f7f81b1c82828a45a52df8f0c6a92636af76c7e.pdf"
    },
    {
      "success": true,
      "doc_id": "bdbc043e8db1880462033c814771b42a",
      "summary": "Pathology visual question answering (PathVQA) attempts to answer a medical question posed by pathology images. Despite its great potential in healthcare, it is not widely adopted because it requires interactions on both the image (vision) and question (language) to generate an answer. Existing methods focused on treating vision and language features independently, which were unable to capture the high and low-level interactions that are required for VQA. Further, these methods failed to offer capabilities to interpret the retrieved answers, which are obscure to humans where the models interpretability to justify the retrieved answers has remained largely unexplored. Motivated by these limitations, we introduce a vision-language transformer that embeds vision (images) and language (questions) features for an interpretable PathVQA. We present an interpretable transformer-based Path-VQA (TraP-VQA), where we embed transformers encoder layers with vision and language features extracted using pre-trained CNN and domain-specific language model (LM), respectively. A decoder layer is then embedded to upsample the encoded features for the final prediction for PathVQA. Our experiments showed that our TraP-VQA outperformed the state-of-the-art comparative methods with public PathVQA dataset. Our experiments validated the robustness of our model on another medical VQA dataset, and the ablation study demonstrated the capability of our integrated transformer-based vision-language model for PathVQA. Finally, we present the visualization results of both text and images, which explain the reason for a retrieved answer in PathVQA.",
      "intriguing_abstract": "Pathology visual question answering (PathVQA) attempts to answer a medical question posed by pathology images. Despite its great potential in healthcare, it is not widely adopted because it requires interactions on both the image (vision) and question (language) to generate an answer. Existing methods focused on treating vision and language features independently, which were unable to capture the high and low-level interactions that are required for VQA. Further, these methods failed to offer capabilities to interpret the retrieved answers, which are obscure to humans where the models interpretability to justify the retrieved answers has remained largely unexplored. Motivated by these limitations, we introduce a vision-language transformer that embeds vision (images) and language (questions) features for an interpretable PathVQA. We present an interpretable transformer-based Path-VQA (TraP-VQA), where we embed transformers encoder layers with vision and language features extracted using pre-trained CNN and domain-specific language model (LM), respectively. A decoder layer is then embedded to upsample the encoded features for the final prediction for PathVQA. Our experiments showed that our TraP-VQA outperformed the state-of-the-art comparative methods with public PathVQA dataset. Our experiments validated the robustness of our model on another medical VQA dataset, and the ablation study demonstrated the capability of our integrated transformer-based vision-language model for PathVQA. Finally, we present the visualization results of both text and images, which explain the reason for a retrieved answer in PathVQA.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/e678898301a66faab85dfa4c84e51118e434b8f2.pdf",
      "citation_key": "naseem2022c95",
      "metadata": {
        "title": "Vision-Language Transformer for Interpretable Pathology Visual Question Answering",
        "authors": [
          "Usman Naseem",
          "Matloob Khushi",
          "Jinman Kim"
        ],
        "published_date": "2022",
        "abstract": "Pathology visual question answering (PathVQA) attempts to answer a medical question posed by pathology images. Despite its great potential in healthcare, it is not widely adopted because it requires interactions on both the image (vision) and question (language) to generate an answer. Existing methods focused on treating vision and language features independently, which were unable to capture the high and low-level interactions that are required for VQA. Further, these methods failed to offer capabilities to interpret the retrieved answers, which are obscure to humans where the models interpretability to justify the retrieved answers has remained largely unexplored. Motivated by these limitations, we introduce a vision-language transformer that embeds vision (images) and language (questions) features for an interpretable PathVQA. We present an interpretable transformer-based Path-VQA (TraP-VQA), where we embed transformers encoder layers with vision and language features extracted using pre-trained CNN and domain-specific language model (LM), respectively. A decoder layer is then embedded to upsample the encoded features for the final prediction for PathVQA. Our experiments showed that our TraP-VQA outperformed the state-of-the-art comparative methods with public PathVQA dataset. Our experiments validated the robustness of our model on another medical VQA dataset, and the ablation study demonstrated the capability of our integrated transformer-based vision-language model for PathVQA. Finally, we present the visualization results of both text and images, which explain the reason for a retrieved answer in PathVQA.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e678898301a66faab85dfa4c84e51118e434b8f2.pdf",
        "venue": "IEEE journal of biomedical and health informatics",
        "citationCount": 46,
        "score": 15.333333333333332,
        "summary": "Pathology visual question answering (PathVQA) attempts to answer a medical question posed by pathology images. Despite its great potential in healthcare, it is not widely adopted because it requires interactions on both the image (vision) and question (language) to generate an answer. Existing methods focused on treating vision and language features independently, which were unable to capture the high and low-level interactions that are required for VQA. Further, these methods failed to offer capabilities to interpret the retrieved answers, which are obscure to humans where the models interpretability to justify the retrieved answers has remained largely unexplored. Motivated by these limitations, we introduce a vision-language transformer that embeds vision (images) and language (questions) features for an interpretable PathVQA. We present an interpretable transformer-based Path-VQA (TraP-VQA), where we embed transformers encoder layers with vision and language features extracted using pre-trained CNN and domain-specific language model (LM), respectively. A decoder layer is then embedded to upsample the encoded features for the final prediction for PathVQA. Our experiments showed that our TraP-VQA outperformed the state-of-the-art comparative methods with public PathVQA dataset. Our experiments validated the robustness of our model on another medical VQA dataset, and the ablation study demonstrated the capability of our integrated transformer-based vision-language model for PathVQA. Finally, we present the visualization results of both text and images, which explain the reason for a retrieved answer in PathVQA.",
        "keywords": []
      },
      "file_name": "e678898301a66faab85dfa4c84e51118e434b8f2.pdf"
    },
    {
      "success": true,
      "doc_id": "0d21d277ea7b8562382cf959b44fe4d7",
      "summary": "Objective. Emphysema is characterized by the destruction and permanent enlargement of the alveoli in the lung. According to visual CT appearance, emphysema can be divided into three subtypes: centrilobular emphysema (CLE), panlobular emphysema (PLE), and paraseptal emphysema (PSE). Automating emphysema classification can help precisely determine the patterns of lung destruction and provide a quantitative evaluation. Approach. We propose a vision transformer (ViT) model to classify the emphysema subtypes via CT images. First, large patches (61  61) are cropped from CT images which contain the area of normal lung parenchyma, CLE, PLE, and PSE. After resizing, the large patch is divided into small patches and these small patches are converted to a sequence of patch embeddings by flattening and linear embedding. A class embedding is concatenated to the patch embedding, and the positional embedding is added to the resulting embeddings described above. Then, the obtained embedding is fed into the transformer encoder blocks to generate the final representation. Finally, the learnable class embedding is fed to a softmax layer to classify the emphysema. Main results. To overcome the lack of massive data, the transformer encoder blocks (pre-trained on ImageNet) are transferred and fine-tuned in our ViT model. The average accuracy of the pre-trained ViT model achieves 95.95% in our labs own dataset, which is higher than that of AlexNet, Inception-V3, MobileNet-V2, ResNet34, and ResNet50. Meanwhile, the pre-trained ViT model outperforms the ViT model without the pre-training. The accuracy of our pre-trained ViT model is higher than or comparable to that by available methods for the public dataset. Significance. The results demonstrated that the proposed ViT model can accurately classify the subtypes of emphysema using CT images. The ViT model can help make an effective computer-aided diagnosis of emphysema, and the ViT method can be extended to other medical applications.",
      "intriguing_abstract": "Objective. Emphysema is characterized by the destruction and permanent enlargement of the alveoli in the lung. According to visual CT appearance, emphysema can be divided into three subtypes: centrilobular emphysema (CLE), panlobular emphysema (PLE), and paraseptal emphysema (PSE). Automating emphysema classification can help precisely determine the patterns of lung destruction and provide a quantitative evaluation. Approach. We propose a vision transformer (ViT) model to classify the emphysema subtypes via CT images. First, large patches (61  61) are cropped from CT images which contain the area of normal lung parenchyma, CLE, PLE, and PSE. After resizing, the large patch is divided into small patches and these small patches are converted to a sequence of patch embeddings by flattening and linear embedding. A class embedding is concatenated to the patch embedding, and the positional embedding is added to the resulting embeddings described above. Then, the obtained embedding is fed into the transformer encoder blocks to generate the final representation. Finally, the learnable class embedding is fed to a softmax layer to classify the emphysema. Main results. To overcome the lack of massive data, the transformer encoder blocks (pre-trained on ImageNet) are transferred and fine-tuned in our ViT model. The average accuracy of the pre-trained ViT model achieves 95.95% in our labs own dataset, which is higher than that of AlexNet, Inception-V3, MobileNet-V2, ResNet34, and ResNet50. Meanwhile, the pre-trained ViT model outperforms the ViT model without the pre-training. The accuracy of our pre-trained ViT model is higher than or comparable to that by available methods for the public dataset. Significance. The results demonstrated that the proposed ViT model can accurately classify the subtypes of emphysema using CT images. The ViT model can help make an effective computer-aided diagnosis of emphysema, and the ViT method can be extended to other medical applications.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/e33434a141bb2881a2e60c518a0cda4feed3f19a.pdf",
      "citation_key": "wu20210gs",
      "metadata": {
        "title": "A vision transformer for emphysema classification using CT images",
        "authors": [
          "Yanan Wu",
          "Shouliang Qi",
          "Yu Sun",
          "Shuyue Xia",
          "Yudong Yao",
          "W. Qian"
        ],
        "published_date": "2021",
        "abstract": "Objective. Emphysema is characterized by the destruction and permanent enlargement of the alveoli in the lung. According to visual CT appearance, emphysema can be divided into three subtypes: centrilobular emphysema (CLE), panlobular emphysema (PLE), and paraseptal emphysema (PSE). Automating emphysema classification can help precisely determine the patterns of lung destruction and provide a quantitative evaluation. Approach. We propose a vision transformer (ViT) model to classify the emphysema subtypes via CT images. First, large patches (61  61) are cropped from CT images which contain the area of normal lung parenchyma, CLE, PLE, and PSE. After resizing, the large patch is divided into small patches and these small patches are converted to a sequence of patch embeddings by flattening and linear embedding. A class embedding is concatenated to the patch embedding, and the positional embedding is added to the resulting embeddings described above. Then, the obtained embedding is fed into the transformer encoder blocks to generate the final representation. Finally, the learnable class embedding is fed to a softmax layer to classify the emphysema. Main results. To overcome the lack of massive data, the transformer encoder blocks (pre-trained on ImageNet) are transferred and fine-tuned in our ViT model. The average accuracy of the pre-trained ViT model achieves 95.95% in our labs own dataset, which is higher than that of AlexNet, Inception-V3, MobileNet-V2, ResNet34, and ResNet50. Meanwhile, the pre-trained ViT model outperforms the ViT model without the pre-training. The accuracy of our pre-trained ViT model is higher than or comparable to that by available methods for the public dataset. Significance. The results demonstrated that the proposed ViT model can accurately classify the subtypes of emphysema using CT images. The ViT model can help make an effective computer-aided diagnosis of emphysema, and the ViT method can be extended to other medical applications.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e33434a141bb2881a2e60c518a0cda4feed3f19a.pdf",
        "venue": "Physics in Medicine and Biology",
        "citationCount": 61,
        "score": 15.25,
        "summary": "Objective. Emphysema is characterized by the destruction and permanent enlargement of the alveoli in the lung. According to visual CT appearance, emphysema can be divided into three subtypes: centrilobular emphysema (CLE), panlobular emphysema (PLE), and paraseptal emphysema (PSE). Automating emphysema classification can help precisely determine the patterns of lung destruction and provide a quantitative evaluation. Approach. We propose a vision transformer (ViT) model to classify the emphysema subtypes via CT images. First, large patches (61  61) are cropped from CT images which contain the area of normal lung parenchyma, CLE, PLE, and PSE. After resizing, the large patch is divided into small patches and these small patches are converted to a sequence of patch embeddings by flattening and linear embedding. A class embedding is concatenated to the patch embedding, and the positional embedding is added to the resulting embeddings described above. Then, the obtained embedding is fed into the transformer encoder blocks to generate the final representation. Finally, the learnable class embedding is fed to a softmax layer to classify the emphysema. Main results. To overcome the lack of massive data, the transformer encoder blocks (pre-trained on ImageNet) are transferred and fine-tuned in our ViT model. The average accuracy of the pre-trained ViT model achieves 95.95% in our labs own dataset, which is higher than that of AlexNet, Inception-V3, MobileNet-V2, ResNet34, and ResNet50. Meanwhile, the pre-trained ViT model outperforms the ViT model without the pre-training. The accuracy of our pre-trained ViT model is higher than or comparable to that by available methods for the public dataset. Significance. The results demonstrated that the proposed ViT model can accurately classify the subtypes of emphysema using CT images. The ViT model can help make an effective computer-aided diagnosis of emphysema, and the ViT method can be extended to other medical applications.",
        "keywords": []
      },
      "file_name": "e33434a141bb2881a2e60c518a0cda4feed3f19a.pdf"
    },
    {
      "success": true,
      "doc_id": "22e95efca991d48d8d3f5f82c838806b",
      "summary": "Deep models are powerful in capturing the complex and non-linear relationship buried in brain imaging data. However, the huge number of parameters in deep models can easily overfit given limited imaging data samples. In this work, we proposed a cross-domain transfer learning method to solve the insufficient data problem in brain imaging domain by leveraging the knowledge learned in natural image domain. Specifically, we employed ViT as the backbone and firstly pretrained it using ImageNet-21K dataset and then transferred to the brain imaging dataset. A slice-wise convolution embedding method was developed to improve the standard patch operation in vanilla ViT. Our method was evaluated based on AD/CN classification task. We also conducted extensive experiments to compare the transfer performance with different transfer strategies, models, and sample size. The results suggest that the proposed method can effectively transfer the knowledge learned in natural image domain to brain imaging area and may provide a promising way to take advantages of the pretrained model in data-intensive applications. Moreover, the proposed cross-domain transfer learning method can obtain comparable classification performance compared to most recent studies.",
      "intriguing_abstract": "Deep models are powerful in capturing the complex and non-linear relationship buried in brain imaging data. However, the huge number of parameters in deep models can easily overfit given limited imaging data samples. In this work, we proposed a cross-domain transfer learning method to solve the insufficient data problem in brain imaging domain by leveraging the knowledge learned in natural image domain. Specifically, we employed ViT as the backbone and firstly pretrained it using ImageNet-21K dataset and then transferred to the brain imaging dataset. A slice-wise convolution embedding method was developed to improve the standard patch operation in vanilla ViT. Our method was evaluated based on AD/CN classification task. We also conducted extensive experiments to compare the transfer performance with different transfer strategies, models, and sample size. The results suggest that the proposed method can effectively transfer the knowledge learned in natural image domain to brain imaging area and may provide a promising way to take advantages of the pretrained model in data-intensive applications. Moreover, the proposed cross-domain transfer learning method can obtain comparable classification performance compared to most recent studies.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/9da3fadf092c864f61d6fd1e8eab5a6ca2397194.pdf",
      "citation_key": "lyu2022vd9",
      "metadata": {
        "title": "Classification of Alzheimer's Disease via Vision Transformer: Classification of Alzheimer's Disease via Vision Transformer",
        "authors": [
          "Yanjun Lyu",
          "Xiao-Wen Yu",
          "Dajiang Zhu",
          "Lu Zhang"
        ],
        "published_date": "2022",
        "abstract": "Deep models are powerful in capturing the complex and non-linear relationship buried in brain imaging data. However, the huge number of parameters in deep models can easily overfit given limited imaging data samples. In this work, we proposed a cross-domain transfer learning method to solve the insufficient data problem in brain imaging domain by leveraging the knowledge learned in natural image domain. Specifically, we employed ViT as the backbone and firstly pretrained it using ImageNet-21K dataset and then transferred to the brain imaging dataset. A slice-wise convolution embedding method was developed to improve the standard patch operation in vanilla ViT. Our method was evaluated based on AD/CN classification task. We also conducted extensive experiments to compare the transfer performance with different transfer strategies, models, and sample size. The results suggest that the proposed method can effectively transfer the knowledge learned in natural image domain to brain imaging area and may provide a promising way to take advantages of the pretrained model in data-intensive applications. Moreover, the proposed cross-domain transfer learning method can obtain comparable classification performance compared to most recent studies.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9da3fadf092c864f61d6fd1e8eab5a6ca2397194.pdf",
        "venue": "Petra",
        "citationCount": 45,
        "score": 15.0,
        "summary": "Deep models are powerful in capturing the complex and non-linear relationship buried in brain imaging data. However, the huge number of parameters in deep models can easily overfit given limited imaging data samples. In this work, we proposed a cross-domain transfer learning method to solve the insufficient data problem in brain imaging domain by leveraging the knowledge learned in natural image domain. Specifically, we employed ViT as the backbone and firstly pretrained it using ImageNet-21K dataset and then transferred to the brain imaging dataset. A slice-wise convolution embedding method was developed to improve the standard patch operation in vanilla ViT. Our method was evaluated based on AD/CN classification task. We also conducted extensive experiments to compare the transfer performance with different transfer strategies, models, and sample size. The results suggest that the proposed method can effectively transfer the knowledge learned in natural image domain to brain imaging area and may provide a promising way to take advantages of the pretrained model in data-intensive applications. Moreover, the proposed cross-domain transfer learning method can obtain comparable classification performance compared to most recent studies.",
        "keywords": []
      },
      "file_name": "9da3fadf092c864f61d6fd1e8eab5a6ca2397194.pdf"
    },
    {
      "success": true,
      "doc_id": "3eb55a9ee877b70de151e069c718d6bb",
      "summary": "COVID-19 is a global pandemic, and detecting them is a momentous task for medical professionals today due to its rapid mutations. Current methods of examining chest X-rays and CT scan requires profound knowledge and are time consuming, which suggests that it shrinks the precious time of medical practitioners when peoples lives are at stake. This study tries to assist this process by achieving state-of-the-art performance in classifying chest X-rays by fine-tuning Vision Transformer(ViT). The proposed approach uses pretrained models, fine-tuned for detecting the presence of COVID-19 disease on chest X-rays. This approach achieves an accuracy score of 97.61%, precision score of 95.34%, recall score of 93.84% and, fl-score of 94.58%. This result signifies the performance of transformer-based models on chest X-ray.",
      "intriguing_abstract": "COVID-19 is a global pandemic, and detecting them is a momentous task for medical professionals today due to its rapid mutations. Current methods of examining chest X-rays and CT scan requires profound knowledge and are time consuming, which suggests that it shrinks the precious time of medical practitioners when peoples lives are at stake. This study tries to assist this process by achieving state-of-the-art performance in classifying chest X-rays by fine-tuning Vision Transformer(ViT). The proposed approach uses pretrained models, fine-tuned for detecting the presence of COVID-19 disease on chest X-rays. This approach achieves an accuracy score of 97.61%, precision score of 95.34%, recall score of 93.84% and, fl-score of 94.58%. This result signifies the performance of transformer-based models on chest X-ray.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/9fab78222c7111702a5702ce5fae0f920722c316.pdf",
      "citation_key": "krishnan2021086",
      "metadata": {
        "title": "Vision Transformer based COVID-19 Detection using Chest X-rays",
        "authors": [
          "Koushik Sivarama Krishnan",
          "Karthik Sivarama Krishnan"
        ],
        "published_date": "2021",
        "abstract": "COVID-19 is a global pandemic, and detecting them is a momentous task for medical professionals today due to its rapid mutations. Current methods of examining chest X-rays and CT scan requires profound knowledge and are time consuming, which suggests that it shrinks the precious time of medical practitioners when peoples lives are at stake. This study tries to assist this process by achieving state-of-the-art performance in classifying chest X-rays by fine-tuning Vision Transformer(ViT). The proposed approach uses pretrained models, fine-tuned for detecting the presence of COVID-19 disease on chest X-rays. This approach achieves an accuracy score of 97.61%, precision score of 95.34%, recall score of 93.84% and, fl-score of 94.58%. This result signifies the performance of transformer-based models on chest X-ray.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9fab78222c7111702a5702ce5fae0f920722c316.pdf",
        "venue": "2021 6th International Conference on Signal Processing, Computing and Control (ISPCC)",
        "citationCount": 59,
        "score": 14.75,
        "summary": "COVID-19 is a global pandemic, and detecting them is a momentous task for medical professionals today due to its rapid mutations. Current methods of examining chest X-rays and CT scan requires profound knowledge and are time consuming, which suggests that it shrinks the precious time of medical practitioners when peoples lives are at stake. This study tries to assist this process by achieving state-of-the-art performance in classifying chest X-rays by fine-tuning Vision Transformer(ViT). The proposed approach uses pretrained models, fine-tuned for detecting the presence of COVID-19 disease on chest X-rays. This approach achieves an accuracy score of 97.61%, precision score of 95.34%, recall score of 93.84% and, fl-score of 94.58%. This result signifies the performance of transformer-based models on chest X-ray.",
        "keywords": []
      },
      "file_name": "9fab78222c7111702a5702ce5fae0f920722c316.pdf"
    },
    {
      "success": true,
      "doc_id": "0e9fe425f31c75072479425e0199cf36",
      "summary": "Transformers yield state-of-the-art results across many tasks. However, their heuristically designed architecture impose huge computational costs during inference. This work aims on challenging the common design philosophy of the Vision Transformer (ViT) model with uniform dimension across all the stacked blocks in a model stage, where we redistribute the parameters both across transformer blocks and between different structures within the block via the first systematic attempt on global structural pruning. Dealing with diverse ViT structural components, we derive a novel Hessian-based structural pruning criteria comparable across all layers and structures, with latency-aware regularization for direct latency reduction. Performing iterative pruning on the DeiT-Base model leads to a new architecture family called NViT (Novel ViT), with a novel parameter redistribution that utilizes parameters more efficiently. On ImageNet-1K, NViT-Base achieves a $2.6\\times FLOPs$ reduction, $5.1\\times$ parameter reduction, and $1.9\\times run$-time speedup over the DeiT-Base model in a near lossless manner. Smaller NViT variants achieve more than 1% accuracy gain at the same throughput of the DeiT Small/Tiny variants, as well as a lossless $3.3\\times parameter$ reduction over the SWIN-Small model. These results outperform prior art by a large margin. Further analysis is provided on the parameter redistribution insight of NViT, where we show the high prunability of ViT models, distinct sensitivity within ViT block, and unique parameter distribution trend across stacked ViT blocks. Our insights provide viability for a simple yet effective parameter redistribution rule towards more efficient ViTs for off-the-shelf performance boost.",
      "intriguing_abstract": "Transformers yield state-of-the-art results across many tasks. However, their heuristically designed architecture impose huge computational costs during inference. This work aims on challenging the common design philosophy of the Vision Transformer (ViT) model with uniform dimension across all the stacked blocks in a model stage, where we redistribute the parameters both across transformer blocks and between different structures within the block via the first systematic attempt on global structural pruning. Dealing with diverse ViT structural components, we derive a novel Hessian-based structural pruning criteria comparable across all layers and structures, with latency-aware regularization for direct latency reduction. Performing iterative pruning on the DeiT-Base model leads to a new architecture family called NViT (Novel ViT), with a novel parameter redistribution that utilizes parameters more efficiently. On ImageNet-1K, NViT-Base achieves a $2.6\\times FLOPs$ reduction, $5.1\\times$ parameter reduction, and $1.9\\times run$-time speedup over the DeiT-Base model in a near lossless manner. Smaller NViT variants achieve more than 1% accuracy gain at the same throughput of the DeiT Small/Tiny variants, as well as a lossless $3.3\\times parameter$ reduction over the SWIN-Small model. These results outperform prior art by a large margin. Further analysis is provided on the parameter redistribution insight of NViT, where we show the high prunability of ViT models, distinct sensitivity within ViT block, and unique parameter distribution trend across stacked ViT blocks. Our insights provide viability for a simple yet effective parameter redistribution rule towards more efficient ViTs for off-the-shelf performance boost.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/c051ee2ad7ac203a26fa8f50eb6312424c729b27.pdf",
      "citation_key": "yang20210bg",
      "metadata": {
        "title": "Global Vision Transformer Pruning with Hessian-Aware Saliency",
        "authors": [
          "Huanrui Yang",
          "Hongxu Yin",
          "Maying Shen",
          "Pavlo Molchanov",
          "Hai Helen Li",
          "Jan Kautz"
        ],
        "published_date": "2021",
        "abstract": "Transformers yield state-of-the-art results across many tasks. However, their heuristically designed architecture impose huge computational costs during inference. This work aims on challenging the common design philosophy of the Vision Transformer (ViT) model with uniform dimension across all the stacked blocks in a model stage, where we redistribute the parameters both across transformer blocks and between different structures within the block via the first systematic attempt on global structural pruning. Dealing with diverse ViT structural components, we derive a novel Hessian-based structural pruning criteria comparable across all layers and structures, with latency-aware regularization for direct latency reduction. Performing iterative pruning on the DeiT-Base model leads to a new architecture family called NViT (Novel ViT), with a novel parameter redistribution that utilizes parameters more efficiently. On ImageNet-1K, NViT-Base achieves a $2.6\\times FLOPs$ reduction, $5.1\\times$ parameter reduction, and $1.9\\times run$-time speedup over the DeiT-Base model in a near lossless manner. Smaller NViT variants achieve more than 1% accuracy gain at the same throughput of the DeiT Small/Tiny variants, as well as a lossless $3.3\\times parameter$ reduction over the SWIN-Small model. These results outperform prior art by a large margin. Further analysis is provided on the parameter redistribution insight of NViT, where we show the high prunability of ViT models, distinct sensitivity within ViT block, and unique parameter distribution trend across stacked ViT blocks. Our insights provide viability for a simple yet effective parameter redistribution rule towards more efficient ViTs for off-the-shelf performance boost.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c051ee2ad7ac203a26fa8f50eb6312424c729b27.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 59,
        "score": 14.75,
        "summary": "Transformers yield state-of-the-art results across many tasks. However, their heuristically designed architecture impose huge computational costs during inference. This work aims on challenging the common design philosophy of the Vision Transformer (ViT) model with uniform dimension across all the stacked blocks in a model stage, where we redistribute the parameters both across transformer blocks and between different structures within the block via the first systematic attempt on global structural pruning. Dealing with diverse ViT structural components, we derive a novel Hessian-based structural pruning criteria comparable across all layers and structures, with latency-aware regularization for direct latency reduction. Performing iterative pruning on the DeiT-Base model leads to a new architecture family called NViT (Novel ViT), with a novel parameter redistribution that utilizes parameters more efficiently. On ImageNet-1K, NViT-Base achieves a $2.6\\times FLOPs$ reduction, $5.1\\times$ parameter reduction, and $1.9\\times run$-time speedup over the DeiT-Base model in a near lossless manner. Smaller NViT variants achieve more than 1% accuracy gain at the same throughput of the DeiT Small/Tiny variants, as well as a lossless $3.3\\times parameter$ reduction over the SWIN-Small model. These results outperform prior art by a large margin. Further analysis is provided on the parameter redistribution insight of NViT, where we show the high prunability of ViT models, distinct sensitivity within ViT block, and unique parameter distribution trend across stacked ViT blocks. Our insights provide viability for a simple yet effective parameter redistribution rule towards more efficient ViTs for off-the-shelf performance boost.",
        "keywords": []
      },
      "file_name": "c051ee2ad7ac203a26fa8f50eb6312424c729b27.pdf"
    },
    {
      "success": true,
      "doc_id": "8d272e24f2f1c833ddc3959caf2f7643",
      "summary": "In this paper, we propose a fully differentiable quantization method for vision transformer (ViT) named as Q-ViT, in which both of the quantization scales and bit-widths are learnable parameters. Specifically, based on our observation that heads in ViT display different quantization robustness, we leverage head-wise bit-width to squeeze the size of Q-ViT while preserving performance. In addition, we propose a novel technique named switchable scale to resolve the convergence problem in the joint training of quantization scales and bit-widths. In this way, Q-ViT pushes the limits of ViT quantization to 3-bit without heavy performance drop. Moreover, we analyze the quantization robustness of every architecture component of ViT and show that the Multi-head Self-Attention (MSA) and the Gaussian Error Linear Units (GELU) are the key aspects for ViT quantization. This study provides some insights for further research about ViT quantization. Extensive experiments on different ViT models, such as DeiT and Swin Transformer show the effectiveness of our quantization method. In particular, our method outperforms the state-of-the-art uniform quantization method by 1.5% on DeiT-Tiny.",
      "intriguing_abstract": "In this paper, we propose a fully differentiable quantization method for vision transformer (ViT) named as Q-ViT, in which both of the quantization scales and bit-widths are learnable parameters. Specifically, based on our observation that heads in ViT display different quantization robustness, we leverage head-wise bit-width to squeeze the size of Q-ViT while preserving performance. In addition, we propose a novel technique named switchable scale to resolve the convergence problem in the joint training of quantization scales and bit-widths. In this way, Q-ViT pushes the limits of ViT quantization to 3-bit without heavy performance drop. Moreover, we analyze the quantization robustness of every architecture component of ViT and show that the Multi-head Self-Attention (MSA) and the Gaussian Error Linear Units (GELU) are the key aspects for ViT quantization. This study provides some insights for further research about ViT quantization. Extensive experiments on different ViT models, such as DeiT and Swin Transformer show the effectiveness of our quantization method. In particular, our method outperforms the state-of-the-art uniform quantization method by 1.5% on DeiT-Tiny.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/13f7a106bb3814ad1fab25fd1356e99e91f402d3.pdf",
      "citation_key": "li20229zn",
      "metadata": {
        "title": "Q-ViT: Fully Differentiable Quantization for Vision Transformer",
        "authors": [
          "Zhexin Li",
          "Tong Yang",
          "Peisong Wang",
          "Jian Cheng"
        ],
        "published_date": "2022",
        "abstract": "In this paper, we propose a fully differentiable quantization method for vision transformer (ViT) named as Q-ViT, in which both of the quantization scales and bit-widths are learnable parameters. Specifically, based on our observation that heads in ViT display different quantization robustness, we leverage head-wise bit-width to squeeze the size of Q-ViT while preserving performance. In addition, we propose a novel technique named switchable scale to resolve the convergence problem in the joint training of quantization scales and bit-widths. In this way, Q-ViT pushes the limits of ViT quantization to 3-bit without heavy performance drop. Moreover, we analyze the quantization robustness of every architecture component of ViT and show that the Multi-head Self-Attention (MSA) and the Gaussian Error Linear Units (GELU) are the key aspects for ViT quantization. This study provides some insights for further research about ViT quantization. Extensive experiments on different ViT models, such as DeiT and Swin Transformer show the effectiveness of our quantization method. In particular, our method outperforms the state-of-the-art uniform quantization method by 1.5% on DeiT-Tiny.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/13f7a106bb3814ad1fab25fd1356e99e91f402d3.pdf",
        "venue": "arXiv.org",
        "citationCount": 44,
        "score": 14.666666666666666,
        "summary": "In this paper, we propose a fully differentiable quantization method for vision transformer (ViT) named as Q-ViT, in which both of the quantization scales and bit-widths are learnable parameters. Specifically, based on our observation that heads in ViT display different quantization robustness, we leverage head-wise bit-width to squeeze the size of Q-ViT while preserving performance. In addition, we propose a novel technique named switchable scale to resolve the convergence problem in the joint training of quantization scales and bit-widths. In this way, Q-ViT pushes the limits of ViT quantization to 3-bit without heavy performance drop. Moreover, we analyze the quantization robustness of every architecture component of ViT and show that the Multi-head Self-Attention (MSA) and the Gaussian Error Linear Units (GELU) are the key aspects for ViT quantization. This study provides some insights for further research about ViT quantization. Extensive experiments on different ViT models, such as DeiT and Swin Transformer show the effectiveness of our quantization method. In particular, our method outperforms the state-of-the-art uniform quantization method by 1.5% on DeiT-Tiny.",
        "keywords": []
      },
      "file_name": "13f7a106bb3814ad1fab25fd1356e99e91f402d3.pdf"
    },
    {
      "success": true,
      "doc_id": "1979bf1851a150d170bdb6151c7aae15",
      "summary": "Deep learning methods have been widely studied for Polarimetric synthetic aperture radar (PolSAR) land cover classification. The scarcity of PolSAR labeled samples and the small receptive field of the model limit the performance of deep learning methods for land cover classification. In this paper, a vision Transformer (ViT)-based classification method is proposed. The ViT structure can extract features from the global range of images based on a self-attention block. The powerful feature representation capability of the model is equivalent to a flexible receptive field, which is suitable for PolSAR image classification at different resolutions. In addition, because of the lack of labeled data, the Mask Autoencoder method is used to pre-train the proposed model with unlabeled data. Experiments are carried out on the Flevoland dataset acquired by NASA/JPL AIRSAR and the Hainan dataset acquired by the Aerial Remote Sensing System of the Chinese Academy of Sciences. The experimental results on both datasets demonstrate the superiority of the proposed method.",
      "intriguing_abstract": "Deep learning methods have been widely studied for Polarimetric synthetic aperture radar (PolSAR) land cover classification. The scarcity of PolSAR labeled samples and the small receptive field of the model limit the performance of deep learning methods for land cover classification. In this paper, a vision Transformer (ViT)-based classification method is proposed. The ViT structure can extract features from the global range of images based on a self-attention block. The powerful feature representation capability of the model is equivalent to a flexible receptive field, which is suitable for PolSAR image classification at different resolutions. In addition, because of the lack of labeled data, the Mask Autoencoder method is used to pre-train the proposed model with unlabeled data. Experiments are carried out on the Flevoland dataset acquired by NASA/JPL AIRSAR and the Hainan dataset acquired by the Aerial Remote Sensing System of the Chinese Academy of Sciences. The experimental results on both datasets demonstrate the superiority of the proposed method.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d04fbbec070b7e7af8152f9b7574971d22bf5997.pdf",
      "citation_key": "wang2022n7h",
      "metadata": {
        "title": "Land Cover Classification for Polarimetric SAR Images Based on Vision Transformer",
        "authors": [
          "Hongmiao Wang",
          "Cheng Xing",
          "Junjun Yin",
          "Jian Yang"
        ],
        "published_date": "2022",
        "abstract": "Deep learning methods have been widely studied for Polarimetric synthetic aperture radar (PolSAR) land cover classification. The scarcity of PolSAR labeled samples and the small receptive field of the model limit the performance of deep learning methods for land cover classification. In this paper, a vision Transformer (ViT)-based classification method is proposed. The ViT structure can extract features from the global range of images based on a self-attention block. The powerful feature representation capability of the model is equivalent to a flexible receptive field, which is suitable for PolSAR image classification at different resolutions. In addition, because of the lack of labeled data, the Mask Autoencoder method is used to pre-train the proposed model with unlabeled data. Experiments are carried out on the Flevoland dataset acquired by NASA/JPL AIRSAR and the Hainan dataset acquired by the Aerial Remote Sensing System of the Chinese Academy of Sciences. The experimental results on both datasets demonstrate the superiority of the proposed method.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d04fbbec070b7e7af8152f9b7574971d22bf5997.pdf",
        "venue": "Remote Sensing",
        "citationCount": 43,
        "score": 14.333333333333332,
        "summary": "Deep learning methods have been widely studied for Polarimetric synthetic aperture radar (PolSAR) land cover classification. The scarcity of PolSAR labeled samples and the small receptive field of the model limit the performance of deep learning methods for land cover classification. In this paper, a vision Transformer (ViT)-based classification method is proposed. The ViT structure can extract features from the global range of images based on a self-attention block. The powerful feature representation capability of the model is equivalent to a flexible receptive field, which is suitable for PolSAR image classification at different resolutions. In addition, because of the lack of labeled data, the Mask Autoencoder method is used to pre-train the proposed model with unlabeled data. Experiments are carried out on the Flevoland dataset acquired by NASA/JPL AIRSAR and the Hainan dataset acquired by the Aerial Remote Sensing System of the Chinese Academy of Sciences. The experimental results on both datasets demonstrate the superiority of the proposed method.",
        "keywords": []
      },
      "file_name": "d04fbbec070b7e7af8152f9b7574971d22bf5997.pdf"
    },
    {
      "success": true,
      "doc_id": "4c570f702cde4a89522770e9de5463a4",
      "summary": "Vision Transformer has shown great visual representation power in substantial vision tasks such as recognition and detection, and thus been attracting fast-growing efforts on manually designing more effective architectures. In this paper, we propose to use neural architecture search to automate this process, by searching not only the architecture but also the search space. The central idea is to gradually evolve different search dimensions guided by their E-T Error computed using a weight-sharing supernet. Moreover, we provide design guidelines of general vision transformers with extensive analysis according to the space searching process, which could promote the understanding of vision transformer. Remarkably, the searched models, named S3 (short for Searching the Search Space), from the searched space achieve superior performance to recently proposed models, such as Swin, DeiT and ViT, when evaluated on ImageNet. The effectiveness of S3 is also illustrated on object detection, semantic segmentation and visual question answering, demonstrating its generality to downstream vision and vision-language tasks. Code and models will be available at https://github.com/microsoft/Cream.",
      "intriguing_abstract": "Vision Transformer has shown great visual representation power in substantial vision tasks such as recognition and detection, and thus been attracting fast-growing efforts on manually designing more effective architectures. In this paper, we propose to use neural architecture search to automate this process, by searching not only the architecture but also the search space. The central idea is to gradually evolve different search dimensions guided by their E-T Error computed using a weight-sharing supernet. Moreover, we provide design guidelines of general vision transformers with extensive analysis according to the space searching process, which could promote the understanding of vision transformer. Remarkably, the searched models, named S3 (short for Searching the Search Space), from the searched space achieve superior performance to recently proposed models, such as Swin, DeiT and ViT, when evaluated on ImageNet. The effectiveness of S3 is also illustrated on object detection, semantic segmentation and visual question answering, demonstrating its generality to downstream vision and vision-language tasks. Code and models will be available at https://github.com/microsoft/Cream.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/e939b55a6f78bffeb00065aed897950c49d21182.pdf",
      "citation_key": "chen202199v",
      "metadata": {
        "title": "Searching the Search Space of Vision Transformer",
        "authors": [
          "Minghao Chen",
          "Kan Wu",
          "Bolin Ni",
          "Houwen Peng",
          "Bei Liu",
          "Jianlong Fu",
          "Hongyang Chao",
          "Haibin Ling"
        ],
        "published_date": "2021",
        "abstract": "Vision Transformer has shown great visual representation power in substantial vision tasks such as recognition and detection, and thus been attracting fast-growing efforts on manually designing more effective architectures. In this paper, we propose to use neural architecture search to automate this process, by searching not only the architecture but also the search space. The central idea is to gradually evolve different search dimensions guided by their E-T Error computed using a weight-sharing supernet. Moreover, we provide design guidelines of general vision transformers with extensive analysis according to the space searching process, which could promote the understanding of vision transformer. Remarkably, the searched models, named S3 (short for Searching the Search Space), from the searched space achieve superior performance to recently proposed models, such as Swin, DeiT and ViT, when evaluated on ImageNet. The effectiveness of S3 is also illustrated on object detection, semantic segmentation and visual question answering, demonstrating its generality to downstream vision and vision-language tasks. Code and models will be available at https://github.com/microsoft/Cream.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e939b55a6f78bffeb00065aed897950c49d21182.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 57,
        "score": 14.25,
        "summary": "Vision Transformer has shown great visual representation power in substantial vision tasks such as recognition and detection, and thus been attracting fast-growing efforts on manually designing more effective architectures. In this paper, we propose to use neural architecture search to automate this process, by searching not only the architecture but also the search space. The central idea is to gradually evolve different search dimensions guided by their E-T Error computed using a weight-sharing supernet. Moreover, we provide design guidelines of general vision transformers with extensive analysis according to the space searching process, which could promote the understanding of vision transformer. Remarkably, the searched models, named S3 (short for Searching the Search Space), from the searched space achieve superior performance to recently proposed models, such as Swin, DeiT and ViT, when evaluated on ImageNet. The effectiveness of S3 is also illustrated on object detection, semantic segmentation and visual question answering, demonstrating its generality to downstream vision and vision-language tasks. Code and models will be available at https://github.com/microsoft/Cream.",
        "keywords": []
      },
      "file_name": "e939b55a6f78bffeb00065aed897950c49d21182.pdf"
    },
    {
      "success": true,
      "doc_id": "d2696dc6ae6e1d0a00e170920e300c54",
      "summary": "Transformers have demonstrated remarkable accomplishments in several natural language processing (NLP) tasks as well as image processing tasks. Herein, we present a deep-learning (DL) model that is capable of improving the semantic segmentation network in two ways. First, utilizing the pre-training Swin Transformer (SwinTF) under Vision Transformer (ViT) as a backbone, the model weights downstream tasks by joining task layers upon the pretrained encoder. Secondly, decoder designs are applied to our DL network with three decoder designs, U-Net, pyramid scene parsing (PSP) network, and feature pyramid network (FPN), to perform pixel-level segmentation. The results are compared with other image labeling state of the art (SOTA) methods, such as global convolutional network (GCN) and ViT. Extensive experiments show that our Swin Transformer (SwinTF) with decoder designs reached a new state of the art on the Thailand Isan Landsat-8 corpus (89.8% F1 score), Thailand North Landsat-8 corpus (63.12% F1 score), and competitive results on ISPRS Vaihingen. Moreover, both our best-proposed methods (SwinTF-PSP and SwinTF-FPN) even outperformed SwinTF with supervised pre-training ViT on the ImageNet-1K in the Thailand, Landsat-8, and ISPRS Vaihingen corpora.",
      "intriguing_abstract": "Transformers have demonstrated remarkable accomplishments in several natural language processing (NLP) tasks as well as image processing tasks. Herein, we present a deep-learning (DL) model that is capable of improving the semantic segmentation network in two ways. First, utilizing the pre-training Swin Transformer (SwinTF) under Vision Transformer (ViT) as a backbone, the model weights downstream tasks by joining task layers upon the pretrained encoder. Secondly, decoder designs are applied to our DL network with three decoder designs, U-Net, pyramid scene parsing (PSP) network, and feature pyramid network (FPN), to perform pixel-level segmentation. The results are compared with other image labeling state of the art (SOTA) methods, such as global convolutional network (GCN) and ViT. Extensive experiments show that our Swin Transformer (SwinTF) with decoder designs reached a new state of the art on the Thailand Isan Landsat-8 corpus (89.8% F1 score), Thailand North Landsat-8 corpus (63.12% F1 score), and competitive results on ISPRS Vaihingen. Moreover, both our best-proposed methods (SwinTF-PSP and SwinTF-FPN) even outperformed SwinTF with supervised pre-training ViT on the ImageNet-1K in the Thailand, Landsat-8, and ISPRS Vaihingen corpora.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/6dc8693674a105c6daca5200141c50362e3044fc.pdf",
      "citation_key": "panboonyuen20218r7",
      "metadata": {
        "title": "Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images",
        "authors": [
          "Teerapong Panboonyuen",
          "Kulsawasd Jitkajornwanich",
          "S. Lawawirojwong",
          "Panu Srestasathiern",
          "P. Vateekul"
        ],
        "published_date": "2021",
        "abstract": "Transformers have demonstrated remarkable accomplishments in several natural language processing (NLP) tasks as well as image processing tasks. Herein, we present a deep-learning (DL) model that is capable of improving the semantic segmentation network in two ways. First, utilizing the pre-training Swin Transformer (SwinTF) under Vision Transformer (ViT) as a backbone, the model weights downstream tasks by joining task layers upon the pretrained encoder. Secondly, decoder designs are applied to our DL network with three decoder designs, U-Net, pyramid scene parsing (PSP) network, and feature pyramid network (FPN), to perform pixel-level segmentation. The results are compared with other image labeling state of the art (SOTA) methods, such as global convolutional network (GCN) and ViT. Extensive experiments show that our Swin Transformer (SwinTF) with decoder designs reached a new state of the art on the Thailand Isan Landsat-8 corpus (89.8% F1 score), Thailand North Landsat-8 corpus (63.12% F1 score), and competitive results on ISPRS Vaihingen. Moreover, both our best-proposed methods (SwinTF-PSP and SwinTF-FPN) even outperformed SwinTF with supervised pre-training ViT on the ImageNet-1K in the Thailand, Landsat-8, and ISPRS Vaihingen corpora.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6dc8693674a105c6daca5200141c50362e3044fc.pdf",
        "venue": "Remote Sensing",
        "citationCount": 57,
        "score": 14.25,
        "summary": "Transformers have demonstrated remarkable accomplishments in several natural language processing (NLP) tasks as well as image processing tasks. Herein, we present a deep-learning (DL) model that is capable of improving the semantic segmentation network in two ways. First, utilizing the pre-training Swin Transformer (SwinTF) under Vision Transformer (ViT) as a backbone, the model weights downstream tasks by joining task layers upon the pretrained encoder. Secondly, decoder designs are applied to our DL network with three decoder designs, U-Net, pyramid scene parsing (PSP) network, and feature pyramid network (FPN), to perform pixel-level segmentation. The results are compared with other image labeling state of the art (SOTA) methods, such as global convolutional network (GCN) and ViT. Extensive experiments show that our Swin Transformer (SwinTF) with decoder designs reached a new state of the art on the Thailand Isan Landsat-8 corpus (89.8% F1 score), Thailand North Landsat-8 corpus (63.12% F1 score), and competitive results on ISPRS Vaihingen. Moreover, both our best-proposed methods (SwinTF-PSP and SwinTF-FPN) even outperformed SwinTF with supervised pre-training ViT on the ImageNet-1K in the Thailand, Landsat-8, and ISPRS Vaihingen corpora.",
        "keywords": []
      },
      "file_name": "6dc8693674a105c6daca5200141c50362e3044fc.pdf"
    },
    {
      "success": true,
      "doc_id": "cb98bcd6f8ac1420ff3a4c2166adf225",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/494e200d8a2ef49d21ce3458a25a109ea917ebe4.pdf",
      "citation_key": "liang2022xlx",
      "metadata": {
        "title": "BTSwin-Unet: 3D U-shaped Symmetrical Swin Transformer-based Network for Brain Tumor Segmentation with Self-supervised Pre-training",
        "authors": [
          "Junjie Liang",
          "Cihui Yang",
          "Jingting Zhong",
          "Xiaoli Ye"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/494e200d8a2ef49d21ce3458a25a109ea917ebe4.pdf",
        "venue": "Neural Processing Letters",
        "citationCount": 42,
        "score": 14.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "494e200d8a2ef49d21ce3458a25a109ea917ebe4.pdf"
    },
    {
      "success": true,
      "doc_id": "a461a3f35ea2fe9f7720440951639e4f",
      "summary": "Vision transformers have attracted much attention from computer vision researchers as they are not restricted to the spatial inductive bias of ConvNets. However, although Transformer-based backbones have achieved much progress on ImageNet classification, it is still unclear whether the learned representations are as transferable as or even more transferable than ConvNets features. To address this point, we systematically investigate the transfer learning ability of ConvNets and vision transformers in 15 single-task and multi-task performance evaluations. We observe consistent advantages of Transformer-based backbones on 13 downstream tasks (out of 15), including but not limited to fine-grained classification, scene recognition (classification, segmentation and depth estimation), open-domain classification, face recognition, etc. More specifically, we find that two ViT models heavily rely on whole network fine-tuning to achieve performance gains while Swin Transformer does not have such a requirement. Moreover, vision transformers behave more robustly in multi-task learning, i.e., bringing more improvements when managing mutually beneficial tasks and reducing performance losses when tackling irrelevant tasks. We hope our discoveries can facilitate the exploration and exploitation of vision transformers in the future.",
      "intriguing_abstract": "Vision transformers have attracted much attention from computer vision researchers as they are not restricted to the spatial inductive bias of ConvNets. However, although Transformer-based backbones have achieved much progress on ImageNet classification, it is still unclear whether the learned representations are as transferable as or even more transferable than ConvNets features. To address this point, we systematically investigate the transfer learning ability of ConvNets and vision transformers in 15 single-task and multi-task performance evaluations. We observe consistent advantages of Transformer-based backbones on 13 downstream tasks (out of 15), including but not limited to fine-grained classification, scene recognition (classification, segmentation and depth estimation), open-domain classification, face recognition, etc. More specifically, we find that two ViT models heavily rely on whole network fine-tuning to achieve performance gains while Swin Transformer does not have such a requirement. Moreover, vision transformers behave more robustly in multi-task learning, i.e., bringing more improvements when managing mutually beneficial tasks and reducing performance losses when tackling irrelevant tasks. We hope our discoveries can facilitate the exploration and exploitation of vision transformers in the future.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/39240f94c9915d9f9959c34b1dc68593894531e6.pdf",
      "citation_key": "zhou2021rtn",
      "metadata": {
        "title": "ConvNets vs. Transformers: Whose Visual Representations are More Transferable?",
        "authors": [
          "Hong-Yu Zhou",
          "Chi-Ken Lu",
          "Sibei Yang",
          "Yizhou Yu"
        ],
        "published_date": "2021",
        "abstract": "Vision transformers have attracted much attention from computer vision researchers as they are not restricted to the spatial inductive bias of ConvNets. However, although Transformer-based backbones have achieved much progress on ImageNet classification, it is still unclear whether the learned representations are as transferable as or even more transferable than ConvNets features. To address this point, we systematically investigate the transfer learning ability of ConvNets and vision transformers in 15 single-task and multi-task performance evaluations. We observe consistent advantages of Transformer-based backbones on 13 downstream tasks (out of 15), including but not limited to fine-grained classification, scene recognition (classification, segmentation and depth estimation), open-domain classification, face recognition, etc. More specifically, we find that two ViT models heavily rely on whole network fine-tuning to achieve performance gains while Swin Transformer does not have such a requirement. Moreover, vision transformers behave more robustly in multi-task learning, i.e., bringing more improvements when managing mutually beneficial tasks and reducing performance losses when tackling irrelevant tasks. We hope our discoveries can facilitate the exploration and exploitation of vision transformers in the future.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/39240f94c9915d9f9959c34b1dc68593894531e6.pdf",
        "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
        "citationCount": 55,
        "score": 13.75,
        "summary": "Vision transformers have attracted much attention from computer vision researchers as they are not restricted to the spatial inductive bias of ConvNets. However, although Transformer-based backbones have achieved much progress on ImageNet classification, it is still unclear whether the learned representations are as transferable as or even more transferable than ConvNets features. To address this point, we systematically investigate the transfer learning ability of ConvNets and vision transformers in 15 single-task and multi-task performance evaluations. We observe consistent advantages of Transformer-based backbones on 13 downstream tasks (out of 15), including but not limited to fine-grained classification, scene recognition (classification, segmentation and depth estimation), open-domain classification, face recognition, etc. More specifically, we find that two ViT models heavily rely on whole network fine-tuning to achieve performance gains while Swin Transformer does not have such a requirement. Moreover, vision transformers behave more robustly in multi-task learning, i.e., bringing more improvements when managing mutually beneficial tasks and reducing performance losses when tackling irrelevant tasks. We hope our discoveries can facilitate the exploration and exploitation of vision transformers in the future.",
        "keywords": []
      },
      "file_name": "39240f94c9915d9f9959c34b1dc68593894531e6.pdf"
    },
    {
      "success": true,
      "doc_id": "8c714628ddbbac49947e0b451e8fe6d2",
      "summary": "Recently, Transformer has emerged as a new architecture in deep learning by utilizing self-attention without convolution. Transformer is also extended to Vision Transformer (ViT) for the visual recognition with a promising performance on ImageNet. In this paper, we propose a Vision Transformer Hashing (VTS) for image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone network and add the hashing head. The proposed VTS model is fine tuned for hashing under six different image retrieval frameworks with their objective functions. We perform the extensive experiments on CIFAR10, ImageNet, NUS-Wide, and COCO datasets. The proposed VTS based image retrieval outperforms the recent state-of-the-art hashing techniques with a significant margin. We also find the proposed VTS model as the backbone network is better than the existing networks, such as AlexNet and ResNet. The code is released at https://github.com/shivram1987/VisionTransformerHashing.",
      "intriguing_abstract": "Recently, Transformer has emerged as a new architecture in deep learning by utilizing self-attention without convolution. Transformer is also extended to Vision Transformer (ViT) for the visual recognition with a promising performance on ImageNet. In this paper, we propose a Vision Transformer Hashing (VTS) for image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone network and add the hashing head. The proposed VTS model is fine tuned for hashing under six different image retrieval frameworks with their objective functions. We perform the extensive experiments on CIFAR10, ImageNet, NUS-Wide, and COCO datasets. The proposed VTS based image retrieval outperforms the recent state-of-the-art hashing techniques with a significant margin. We also find the proposed VTS model as the backbone network is better than the existing networks, such as AlexNet and ResNet. The code is released at https://github.com/shivram1987/VisionTransformerHashing.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/8db96d4eaefdac8dff6d98d94a5e3f5b5558c63d.pdf",
      "citation_key": "dubey2021ra5",
      "metadata": {
        "title": "Vision Transformer Hashing for Image Retrieval",
        "authors": [
          "S. Dubey",
          "S. Singh",
          "Wei Chu"
        ],
        "published_date": "2021",
        "abstract": "Recently, Transformer has emerged as a new architecture in deep learning by utilizing self-attention without convolution. Transformer is also extended to Vision Transformer (ViT) for the visual recognition with a promising performance on ImageNet. In this paper, we propose a Vision Transformer Hashing (VTS) for image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone network and add the hashing head. The proposed VTS model is fine tuned for hashing under six different image retrieval frameworks with their objective functions. We perform the extensive experiments on CIFAR10, ImageNet, NUS-Wide, and COCO datasets. The proposed VTS based image retrieval outperforms the recent state-of-the-art hashing techniques with a significant margin. We also find the proposed VTS model as the backbone network is better than the existing networks, such as AlexNet and ResNet. The code is released at https://github.com/shivram1987/VisionTransformerHashing.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/8db96d4eaefdac8dff6d98d94a5e3f5b5558c63d.pdf",
        "venue": "IEEE International Conference on Multimedia and Expo",
        "citationCount": 55,
        "score": 13.75,
        "summary": "Recently, Transformer has emerged as a new architecture in deep learning by utilizing self-attention without convolution. Transformer is also extended to Vision Transformer (ViT) for the visual recognition with a promising performance on ImageNet. In this paper, we propose a Vision Transformer Hashing (VTS) for image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone network and add the hashing head. The proposed VTS model is fine tuned for hashing under six different image retrieval frameworks with their objective functions. We perform the extensive experiments on CIFAR10, ImageNet, NUS-Wide, and COCO datasets. The proposed VTS based image retrieval outperforms the recent state-of-the-art hashing techniques with a significant margin. We also find the proposed VTS model as the backbone network is better than the existing networks, such as AlexNet and ResNet. The code is released at https://github.com/shivram1987/VisionTransformerHashing.",
        "keywords": []
      },
      "file_name": "8db96d4eaefdac8dff6d98d94a5e3f5b5558c63d.pdf"
    },
    {
      "success": true,
      "doc_id": "0ba582c9bcf652f0127b13f17ee0b3b7",
      "summary": "ABSTRACT Hyperspectral image (HSI) classification has received extensive attention by the development of deep learning and has achieved great success. However, most of the deep learning-based approaches tend to extract features of spatial content by disrupting spectral information or to extract sequential spectral features in short-range context. On the other hand, Transformers-based models address this problem by learning long-range relationship. This study introduces a novel spectral-swin transformer (SpectralSWIN) network. The proposed network effectively projects the HSI data from spectral characteristics into spatial and spectral feature representation. Specifically, SpectralSWIN network makes use of a newly proposed swin-spectral module (SSM) for processing the spatial and spectral features concurrently. As far as we know, this is the first time that a transformer backbone designed for vision domain has been proposed for HSI classification. Extensive experiments conducted on two different HSI prove the superiority and effectiveness of the proposed method over the state-of-the-art methods in terms of both quantitative and visual evaluations.",
      "intriguing_abstract": "ABSTRACT Hyperspectral image (HSI) classification has received extensive attention by the development of deep learning and has achieved great success. However, most of the deep learning-based approaches tend to extract features of spatial content by disrupting spectral information or to extract sequential spectral features in short-range context. On the other hand, Transformers-based models address this problem by learning long-range relationship. This study introduces a novel spectral-swin transformer (SpectralSWIN) network. The proposed network effectively projects the HSI data from spectral characteristics into spatial and spectral feature representation. Specifically, SpectralSWIN network makes use of a newly proposed swin-spectral module (SSM) for processing the spatial and spectral features concurrently. As far as we know, this is the first time that a transformer backbone designed for vision domain has been proposed for HSI classification. Extensive experiments conducted on two different HSI prove the superiority and effectiveness of the proposed method over the state-of-the-art methods in terms of both quantitative and visual evaluations.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/428d755f0c8397ee6d04c89787f3455d323d8280.pdf",
      "citation_key": "ayas2022md0",
      "metadata": {
        "title": "SpectralSWIN: a spectral-swin transformer network for hyperspectral image classification",
        "authors": [
          "Selen Ayas",
          "Esra Tunc-Gormus"
        ],
        "published_date": "2022",
        "abstract": "ABSTRACT Hyperspectral image (HSI) classification has received extensive attention by the development of deep learning and has achieved great success. However, most of the deep learning-based approaches tend to extract features of spatial content by disrupting spectral information or to extract sequential spectral features in short-range context. On the other hand, Transformers-based models address this problem by learning long-range relationship. This study introduces a novel spectral-swin transformer (SpectralSWIN) network. The proposed network effectively projects the HSI data from spectral characteristics into spatial and spectral feature representation. Specifically, SpectralSWIN network makes use of a newly proposed swin-spectral module (SSM) for processing the spatial and spectral features concurrently. As far as we know, this is the first time that a transformer backbone designed for vision domain has been proposed for HSI classification. Extensive experiments conducted on two different HSI prove the superiority and effectiveness of the proposed method over the state-of-the-art methods in terms of both quantitative and visual evaluations.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/428d755f0c8397ee6d04c89787f3455d323d8280.pdf",
        "venue": "International Journal of Remote Sensing",
        "citationCount": 41,
        "score": 13.666666666666666,
        "summary": "ABSTRACT Hyperspectral image (HSI) classification has received extensive attention by the development of deep learning and has achieved great success. However, most of the deep learning-based approaches tend to extract features of spatial content by disrupting spectral information or to extract sequential spectral features in short-range context. On the other hand, Transformers-based models address this problem by learning long-range relationship. This study introduces a novel spectral-swin transformer (SpectralSWIN) network. The proposed network effectively projects the HSI data from spectral characteristics into spatial and spectral feature representation. Specifically, SpectralSWIN network makes use of a newly proposed swin-spectral module (SSM) for processing the spatial and spectral features concurrently. As far as we know, this is the first time that a transformer backbone designed for vision domain has been proposed for HSI classification. Extensive experiments conducted on two different HSI prove the superiority and effectiveness of the proposed method over the state-of-the-art methods in terms of both quantitative and visual evaluations.",
        "keywords": []
      },
      "file_name": "428d755f0c8397ee6d04c89787f3455d323d8280.pdf"
    },
    {
      "success": true,
      "doc_id": "a94453fd142ad8361ef4706182b3f768",
      "summary": "In this paper, we study the zero-shot sketch-based image retrieval (ZS-SBIR) task, which retrieves natural images related to sketch queries from unseen categories. In the literature, convolutional neural networks (CNNs) have become the de-facto standard and they are either trained end-to-end or used to extract pre-trained features for images and sketches. However, CNNs are limited in modeling the global structural information of objects due to the intrinsic locality of convolution operations. To this end, we propose a Transformer-based approach called Three-Way Vision Transformer (TVT) to leverage the ability of Vision Transformer (ViT) to model global contexts due to the global self-attention mechanism. Going beyond simply applying ViT to this task, we propose a token-based strategy of adding fusion and distillation tokens and making them complementary to each other. Specifically, we integrate three ViTs, which are pre-trained on data of each modality, into a three-way pipeline through the processes of distillation and multi-modal hypersphere learning. The distillation process is proposed to supervise fusion ViT (ViT with an extra fusion token) with soft targets from modality-specific ViTs, which prevents fusion ViT from catastrophic forgetting. Furthermore, our method learns a multi-modal hypersphere by performing inter- and intra-modal alignment without loss of uniformity, which aims to bridge the modal gap between modalities of sketch and image and avoid the collapse in dimensions. Extensive experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and QuickDraw, demonstrate the superiority of our TVT method over the state-of-the-art ZS-SBIR methods.",
      "intriguing_abstract": "In this paper, we study the zero-shot sketch-based image retrieval (ZS-SBIR) task, which retrieves natural images related to sketch queries from unseen categories. In the literature, convolutional neural networks (CNNs) have become the de-facto standard and they are either trained end-to-end or used to extract pre-trained features for images and sketches. However, CNNs are limited in modeling the global structural information of objects due to the intrinsic locality of convolution operations. To this end, we propose a Transformer-based approach called Three-Way Vision Transformer (TVT) to leverage the ability of Vision Transformer (ViT) to model global contexts due to the global self-attention mechanism. Going beyond simply applying ViT to this task, we propose a token-based strategy of adding fusion and distillation tokens and making them complementary to each other. Specifically, we integrate three ViTs, which are pre-trained on data of each modality, into a three-way pipeline through the processes of distillation and multi-modal hypersphere learning. The distillation process is proposed to supervise fusion ViT (ViT with an extra fusion token) with soft targets from modality-specific ViTs, which prevents fusion ViT from catastrophic forgetting. Furthermore, our method learns a multi-modal hypersphere by performing inter- and intra-modal alignment without loss of uniformity, which aims to bridge the modal gap between modalities of sketch and image and avoid the collapse in dimensions. Extensive experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and QuickDraw, demonstrate the superiority of our TVT method over the state-of-the-art ZS-SBIR methods.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/ff00791b780b10336cc02ee366446d16e1c5e17b.pdf",
      "citation_key": "tian2022shu",
      "metadata": {
        "title": "TVT: Three-Way Vision Transformer through Multi-Modal Hypersphere Learning for Zero-Shot Sketch-Based Image Retrieval",
        "authors": [
          "Jialin Tian",
          "Xing Xu",
          "Fumin Shen",
          "Yang Yang",
          "Heng Tao Shen"
        ],
        "published_date": "2022",
        "abstract": "In this paper, we study the zero-shot sketch-based image retrieval (ZS-SBIR) task, which retrieves natural images related to sketch queries from unseen categories. In the literature, convolutional neural networks (CNNs) have become the de-facto standard and they are either trained end-to-end or used to extract pre-trained features for images and sketches. However, CNNs are limited in modeling the global structural information of objects due to the intrinsic locality of convolution operations. To this end, we propose a Transformer-based approach called Three-Way Vision Transformer (TVT) to leverage the ability of Vision Transformer (ViT) to model global contexts due to the global self-attention mechanism. Going beyond simply applying ViT to this task, we propose a token-based strategy of adding fusion and distillation tokens and making them complementary to each other. Specifically, we integrate three ViTs, which are pre-trained on data of each modality, into a three-way pipeline through the processes of distillation and multi-modal hypersphere learning. The distillation process is proposed to supervise fusion ViT (ViT with an extra fusion token) with soft targets from modality-specific ViTs, which prevents fusion ViT from catastrophic forgetting. Furthermore, our method learns a multi-modal hypersphere by performing inter- and intra-modal alignment without loss of uniformity, which aims to bridge the modal gap between modalities of sketch and image and avoid the collapse in dimensions. Extensive experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and QuickDraw, demonstrate the superiority of our TVT method over the state-of-the-art ZS-SBIR methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ff00791b780b10336cc02ee366446d16e1c5e17b.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 40,
        "score": 13.333333333333332,
        "summary": "In this paper, we study the zero-shot sketch-based image retrieval (ZS-SBIR) task, which retrieves natural images related to sketch queries from unseen categories. In the literature, convolutional neural networks (CNNs) have become the de-facto standard and they are either trained end-to-end or used to extract pre-trained features for images and sketches. However, CNNs are limited in modeling the global structural information of objects due to the intrinsic locality of convolution operations. To this end, we propose a Transformer-based approach called Three-Way Vision Transformer (TVT) to leverage the ability of Vision Transformer (ViT) to model global contexts due to the global self-attention mechanism. Going beyond simply applying ViT to this task, we propose a token-based strategy of adding fusion and distillation tokens and making them complementary to each other. Specifically, we integrate three ViTs, which are pre-trained on data of each modality, into a three-way pipeline through the processes of distillation and multi-modal hypersphere learning. The distillation process is proposed to supervise fusion ViT (ViT with an extra fusion token) with soft targets from modality-specific ViTs, which prevents fusion ViT from catastrophic forgetting. Furthermore, our method learns a multi-modal hypersphere by performing inter- and intra-modal alignment without loss of uniformity, which aims to bridge the modal gap between modalities of sketch and image and avoid the collapse in dimensions. Extensive experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and QuickDraw, demonstrate the superiority of our TVT method over the state-of-the-art ZS-SBIR methods.",
        "keywords": []
      },
      "file_name": "ff00791b780b10336cc02ee366446d16e1c5e17b.pdf"
    },
    {
      "success": true,
      "doc_id": "d737805b6d1c25089e1eac6e8f89b0af",
      "summary": "High-resolution (HR) synthetic aperture radar (SAR) image classification is a challenging task for the limitation of its complex semantic scenes and coherent speckles. Convolutional neural networks (CNNs) have been proven the superior local spatial features representation capability for SAR images. However, it is hard to capture global information of images by convolutions. To solve such issues, this letter proposes an end-to-end network named globallocal network structure (GLNS) for HR SAR classification. In the GLNS framework, a lightweight CNN and a compact vision transformer (ViT) are designed to learn local and global features, and two types of features are fused in quality to mine complementary information through the fusion net. Then, our research devolves the twofold loss function to reduce the interclass distance of SAR images, which brings more compactness to classification features and less interference of coherent speckles. Experimental results on real HR SAR images indicate that the proposed method has more strong feature extraction capability and noise resistance performance. This method achieves the highest classification accuracy on both datasets compared with other related approaches based on CNN.",
      "intriguing_abstract": "High-resolution (HR) synthetic aperture radar (SAR) image classification is a challenging task for the limitation of its complex semantic scenes and coherent speckles. Convolutional neural networks (CNNs) have been proven the superior local spatial features representation capability for SAR images. However, it is hard to capture global information of images by convolutions. To solve such issues, this letter proposes an end-to-end network named globallocal network structure (GLNS) for HR SAR classification. In the GLNS framework, a lightweight CNN and a compact vision transformer (ViT) are designed to learn local and global features, and two types of features are fused in quality to mine complementary information through the fusion net. Then, our research devolves the twofold loss function to reduce the interclass distance of SAR images, which brings more compactness to classification features and less interference of coherent speckles. Experimental results on real HR SAR images indicate that the proposed method has more strong feature extraction capability and noise resistance performance. This method achieves the highest classification accuracy on both datasets compared with other related approaches based on CNN.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/957a3d34303b424fe90a279cf5361253c93ac265.pdf",
      "citation_key": "liu2022249",
      "metadata": {
        "title": "High Resolution SAR Image Classification Using Global-Local Network Structure Based on Vision Transformer and CNN",
        "authors": [
          "Xingyu Liu",
          "Yuehua Wu",
          "Wenkai Liang",
          "Yi Cao",
          "Ming Li"
        ],
        "published_date": "2022",
        "abstract": "High-resolution (HR) synthetic aperture radar (SAR) image classification is a challenging task for the limitation of its complex semantic scenes and coherent speckles. Convolutional neural networks (CNNs) have been proven the superior local spatial features representation capability for SAR images. However, it is hard to capture global information of images by convolutions. To solve such issues, this letter proposes an end-to-end network named globallocal network structure (GLNS) for HR SAR classification. In the GLNS framework, a lightweight CNN and a compact vision transformer (ViT) are designed to learn local and global features, and two types of features are fused in quality to mine complementary information through the fusion net. Then, our research devolves the twofold loss function to reduce the interclass distance of SAR images, which brings more compactness to classification features and less interference of coherent speckles. Experimental results on real HR SAR images indicate that the proposed method has more strong feature extraction capability and noise resistance performance. This method achieves the highest classification accuracy on both datasets compared with other related approaches based on CNN.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/957a3d34303b424fe90a279cf5361253c93ac265.pdf",
        "venue": "IEEE Geoscience and Remote Sensing Letters",
        "citationCount": 40,
        "score": 13.333333333333332,
        "summary": "High-resolution (HR) synthetic aperture radar (SAR) image classification is a challenging task for the limitation of its complex semantic scenes and coherent speckles. Convolutional neural networks (CNNs) have been proven the superior local spatial features representation capability for SAR images. However, it is hard to capture global information of images by convolutions. To solve such issues, this letter proposes an end-to-end network named globallocal network structure (GLNS) for HR SAR classification. In the GLNS framework, a lightweight CNN and a compact vision transformer (ViT) are designed to learn local and global features, and two types of features are fused in quality to mine complementary information through the fusion net. Then, our research devolves the twofold loss function to reduce the interclass distance of SAR images, which brings more compactness to classification features and less interference of coherent speckles. Experimental results on real HR SAR images indicate that the proposed method has more strong feature extraction capability and noise resistance performance. This method achieves the highest classification accuracy on both datasets compared with other related approaches based on CNN.",
        "keywords": []
      },
      "file_name": "957a3d34303b424fe90a279cf5361253c93ac265.pdf"
    },
    {
      "success": true,
      "doc_id": "33447448b31348b65ecf1ab2f9b8e440",
      "summary": "Learning subtle representation about object parts plays a vital role in fine-grained visual recognition (FGVR) field. The vision transformer (ViT) achieves promising results on computer vision due to its attention mechanism. Nonetheless, with the fixed size of patches in ViT, the class token in deep layer focuses on the global receptive field and cannot generate multi-granularity features for FGVR. To capture region attention without box annotations and compensate for ViT shortcomings in FGVR, we propose a novel method named Adaptive attention multi-scale Fusion Transformer (AFTrans). The Selective Attention Collection Module (SACM) in our approach leverages attention weights in ViT and filters them adaptively to correspond with the relative importance of input patches. The multiple scales (global and local) pipeline is supervised by our weights sharing encoder and can be easily trained end-to-end. Comprehensive experiments demonstrate that AFTrans can achieve SOTA performance on three published fine-grained benchmarks: CUB-200-2011, Stanford Dogs and iNat2017.",
      "intriguing_abstract": "Learning subtle representation about object parts plays a vital role in fine-grained visual recognition (FGVR) field. The vision transformer (ViT) achieves promising results on computer vision due to its attention mechanism. Nonetheless, with the fixed size of patches in ViT, the class token in deep layer focuses on the global receptive field and cannot generate multi-granularity features for FGVR. To capture region attention without box annotations and compensate for ViT shortcomings in FGVR, we propose a novel method named Adaptive attention multi-scale Fusion Transformer (AFTrans). The Selective Attention Collection Module (SACM) in our approach leverages attention weights in ViT and filters them adaptively to correspond with the relative importance of input patches. The multiple scales (global and local) pipeline is supervised by our weights sharing encoder and can be easily trained end-to-end. Comprehensive experiments demonstrate that AFTrans can achieve SOTA performance on three published fine-grained benchmarks: CUB-200-2011, Stanford Dogs and iNat2017.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/401c8d72a9b275e88e6ba159d8d646cfb9f397aa.pdf",
      "citation_key": "zhang2021mcp",
      "metadata": {
        "title": "A free lunch from ViT: adaptive attention multi-scale fusion Transformer for fine-grained visual recognition",
        "authors": [
          "Yuan Zhang",
          "Jian Cao",
          "Ling Zhang",
          "Xiangcheng Liu",
          "Zhiyi Wang",
          "Feng Ling",
          "Weiqian Chen"
        ],
        "published_date": "2021",
        "abstract": "Learning subtle representation about object parts plays a vital role in fine-grained visual recognition (FGVR) field. The vision transformer (ViT) achieves promising results on computer vision due to its attention mechanism. Nonetheless, with the fixed size of patches in ViT, the class token in deep layer focuses on the global receptive field and cannot generate multi-granularity features for FGVR. To capture region attention without box annotations and compensate for ViT shortcomings in FGVR, we propose a novel method named Adaptive attention multi-scale Fusion Transformer (AFTrans). The Selective Attention Collection Module (SACM) in our approach leverages attention weights in ViT and filters them adaptively to correspond with the relative importance of input patches. The multiple scales (global and local) pipeline is supervised by our weights sharing encoder and can be easily trained end-to-end. Comprehensive experiments demonstrate that AFTrans can achieve SOTA performance on three published fine-grained benchmarks: CUB-200-2011, Stanford Dogs and iNat2017.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/401c8d72a9b275e88e6ba159d8d646cfb9f397aa.pdf",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "citationCount": 53,
        "score": 13.25,
        "summary": "Learning subtle representation about object parts plays a vital role in fine-grained visual recognition (FGVR) field. The vision transformer (ViT) achieves promising results on computer vision due to its attention mechanism. Nonetheless, with the fixed size of patches in ViT, the class token in deep layer focuses on the global receptive field and cannot generate multi-granularity features for FGVR. To capture region attention without box annotations and compensate for ViT shortcomings in FGVR, we propose a novel method named Adaptive attention multi-scale Fusion Transformer (AFTrans). The Selective Attention Collection Module (SACM) in our approach leverages attention weights in ViT and filters them adaptively to correspond with the relative importance of input patches. The multiple scales (global and local) pipeline is supervised by our weights sharing encoder and can be easily trained end-to-end. Comprehensive experiments demonstrate that AFTrans can achieve SOTA performance on three published fine-grained benchmarks: CUB-200-2011, Stanford Dogs and iNat2017.",
        "keywords": []
      },
      "file_name": "401c8d72a9b275e88e6ba159d8d646cfb9f397aa.pdf"
    },
    {
      "success": true,
      "doc_id": "728ea14e7ede88ab024991fd38c6ca82",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/7dee2bc2be709c0009b7623b7af78246f32e0a60.pdf",
      "citation_key": "han2021vis",
      "metadata": {
        "title": "Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight",
        "authors": [
          "Qi Han",
          "Zejia Fan",
          "Qi Dai",
          "Lei Sun",
          "Mingg-Ming Cheng",
          "Jiaying Liu",
          "Jingdong Wang"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7dee2bc2be709c0009b7623b7af78246f32e0a60.pdf",
        "venue": "arXiv.org",
        "citationCount": 53,
        "score": 13.25,
        "summary": "",
        "keywords": []
      },
      "file_name": "7dee2bc2be709c0009b7623b7af78246f32e0a60.pdf"
    },
    {
      "success": true,
      "doc_id": "d6d46992a7feb02577a6d469743a6ef6",
      "summary": "In recent image classification approaches, a vision transformer (ViT) has shown an excellent performance beyond that of a convolutional neural network. A ViT achieves a high classification for natural images because it properly preserves the global image features. Conversely, a ViT still has many limitations in facial expression recognition (FER), which requires the detection of subtle changes in expression, because it can lose the local features of the image. Therefore, in this paper, we propose Squeeze ViT, a method for reducing the computational complexity by reducing the number of feature dimensions while increasing the FER performance by concurrently combining global and local features. To measure the FER performance of Squeeze ViT, experiments were conducted on lab-controlled FER datasets and a wild FER dataset. Through comparative experiments with previous state-of-the-art approaches, we proved that the proposed method achieves an excellent performance on both types of datasets.",
      "intriguing_abstract": "In recent image classification approaches, a vision transformer (ViT) has shown an excellent performance beyond that of a convolutional neural network. A ViT achieves a high classification for natural images because it properly preserves the global image features. Conversely, a ViT still has many limitations in facial expression recognition (FER), which requires the detection of subtle changes in expression, because it can lose the local features of the image. Therefore, in this paper, we propose Squeeze ViT, a method for reducing the computational complexity by reducing the number of feature dimensions while increasing the FER performance by concurrently combining global and local features. To measure the FER performance of Squeeze ViT, experiments were conducted on lab-controlled FER datasets and a wild FER dataset. Through comparative experiments with previous state-of-the-art approaches, we proved that the proposed method achieves an excellent performance on both types of datasets.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/1b026103e33b4c9eb637bc6f34715e22636b3492.pdf",
      "citation_key": "kim2022m6u",
      "metadata": {
        "title": "Facial Expression Recognition Based on Squeeze Vision Transformer",
        "authors": [
          "Sangwon Kim",
          "J. Nam",
          "ByoungChul Ko"
        ],
        "published_date": "2022",
        "abstract": "In recent image classification approaches, a vision transformer (ViT) has shown an excellent performance beyond that of a convolutional neural network. A ViT achieves a high classification for natural images because it properly preserves the global image features. Conversely, a ViT still has many limitations in facial expression recognition (FER), which requires the detection of subtle changes in expression, because it can lose the local features of the image. Therefore, in this paper, we propose Squeeze ViT, a method for reducing the computational complexity by reducing the number of feature dimensions while increasing the FER performance by concurrently combining global and local features. To measure the FER performance of Squeeze ViT, experiments were conducted on lab-controlled FER datasets and a wild FER dataset. Through comparative experiments with previous state-of-the-art approaches, we proved that the proposed method achieves an excellent performance on both types of datasets.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1b026103e33b4c9eb637bc6f34715e22636b3492.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 39,
        "score": 13.0,
        "summary": "In recent image classification approaches, a vision transformer (ViT) has shown an excellent performance beyond that of a convolutional neural network. A ViT achieves a high classification for natural images because it properly preserves the global image features. Conversely, a ViT still has many limitations in facial expression recognition (FER), which requires the detection of subtle changes in expression, because it can lose the local features of the image. Therefore, in this paper, we propose Squeeze ViT, a method for reducing the computational complexity by reducing the number of feature dimensions while increasing the FER performance by concurrently combining global and local features. To measure the FER performance of Squeeze ViT, experiments were conducted on lab-controlled FER datasets and a wild FER dataset. Through comparative experiments with previous state-of-the-art approaches, we proved that the proposed method achieves an excellent performance on both types of datasets.",
        "keywords": []
      },
      "file_name": "1b026103e33b4c9eb637bc6f34715e22636b3492.pdf"
    },
    {
      "success": true,
      "doc_id": "52a829eec4b8c1521ffda02400e64dea",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/024c595ba03087399e68e51f87adb4eaf5379701.pdf",
      "citation_key": "zhou2022nln",
      "metadata": {
        "title": "ASI-DBNet: An Adaptive Sparse Interactive ResNet-Vision Transformer Dual-Branch Network for the Grading of Brain Cancer Histopathological Images",
        "authors": [
          "Xiaoli Zhou",
          "Chaowei Tang",
          "Pan Huang",
          "Sukun Tian",
          "F. Mercaldo",
          "A. Santone"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/024c595ba03087399e68e51f87adb4eaf5379701.pdf",
        "venue": "Interdisciplinary Sciences Computational Life Sciences",
        "citationCount": 39,
        "score": 13.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "024c595ba03087399e68e51f87adb4eaf5379701.pdf"
    },
    {
      "success": true,
      "doc_id": "cf513cc8e8a12c20fc7456925f4b6f0d",
      "summary": "Accurate dynamic driver head pose tracking is of great importance for driverautomotive collaboration, intelligent copilot, head-up display (HUD), and other human-centered automated driving applications. To further advance this technology, this article proposes a low-cost and markerless head-tracking system using a deep learning-based dynamic head pose estimation model. The proposed system requires only a red, green, blue (RGB) camera without other hardware or markers. To enhance the accuracy of the drivers head pose estimation, a spatiotemporal vision transformer (ST-ViT) model, which takes an image pair as the input instead of a single frame, is proposed. Compared to a standard transformer, the ST-ViT contains a spatialconvolutional vision transformer and a temporal transformer, which can improve the model performance. To handle the error fluctuation of the head pose estimation model, this article proposes an adaptive Kalman filter (AKF). By analyzing the error distribution of the estimation model and the user experience of the head tracker, the proposed AKF includes an adaptive observation noise coefficient; this can adaptively moderate the smoothness of the curve. Comprehensive experiments show that the proposed system is feasible and effective, and it achieves a state-of-the-art performance.",
      "intriguing_abstract": "Accurate dynamic driver head pose tracking is of great importance for driverautomotive collaboration, intelligent copilot, head-up display (HUD), and other human-centered automated driving applications. To further advance this technology, this article proposes a low-cost and markerless head-tracking system using a deep learning-based dynamic head pose estimation model. The proposed system requires only a red, green, blue (RGB) camera without other hardware or markers. To enhance the accuracy of the drivers head pose estimation, a spatiotemporal vision transformer (ST-ViT) model, which takes an image pair as the input instead of a single frame, is proposed. Compared to a standard transformer, the ST-ViT contains a spatialconvolutional vision transformer and a temporal transformer, which can improve the model performance. To handle the error fluctuation of the head pose estimation model, this article proposes an adaptive Kalman filter (AKF). By analyzing the error distribution of the estimation model and the user experience of the head tracker, the proposed AKF includes an adaptive observation noise coefficient; this can adaptively moderate the smoothness of the curve. Comprehensive experiments show that the proposed system is feasible and effective, and it achieves a state-of-the-art performance.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/9500dbde37e0f8c6b9ea924aacc6591bc3414166.pdf",
      "citation_key": "hu202242d",
      "metadata": {
        "title": "Toward Human-Centered Automated Driving: A Novel Spatiotemporal Vision Transformer-Enabled Head Tracker",
        "authors": [
          "Zhongxu Hu",
          "Yiran Zhang",
          "Yang Xing",
          "Yifan Zhao",
          "Dongpu Cao",
          "Chen Lv"
        ],
        "published_date": "2022",
        "abstract": "Accurate dynamic driver head pose tracking is of great importance for driverautomotive collaboration, intelligent copilot, head-up display (HUD), and other human-centered automated driving applications. To further advance this technology, this article proposes a low-cost and markerless head-tracking system using a deep learning-based dynamic head pose estimation model. The proposed system requires only a red, green, blue (RGB) camera without other hardware or markers. To enhance the accuracy of the drivers head pose estimation, a spatiotemporal vision transformer (ST-ViT) model, which takes an image pair as the input instead of a single frame, is proposed. Compared to a standard transformer, the ST-ViT contains a spatialconvolutional vision transformer and a temporal transformer, which can improve the model performance. To handle the error fluctuation of the head pose estimation model, this article proposes an adaptive Kalman filter (AKF). By analyzing the error distribution of the estimation model and the user experience of the head tracker, the proposed AKF includes an adaptive observation noise coefficient; this can adaptively moderate the smoothness of the curve. Comprehensive experiments show that the proposed system is feasible and effective, and it achieves a state-of-the-art performance.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9500dbde37e0f8c6b9ea924aacc6591bc3414166.pdf",
        "venue": "IEEE Vehicular Technology Magazine",
        "citationCount": 38,
        "score": 12.666666666666666,
        "summary": "Accurate dynamic driver head pose tracking is of great importance for driverautomotive collaboration, intelligent copilot, head-up display (HUD), and other human-centered automated driving applications. To further advance this technology, this article proposes a low-cost and markerless head-tracking system using a deep learning-based dynamic head pose estimation model. The proposed system requires only a red, green, blue (RGB) camera without other hardware or markers. To enhance the accuracy of the drivers head pose estimation, a spatiotemporal vision transformer (ST-ViT) model, which takes an image pair as the input instead of a single frame, is proposed. Compared to a standard transformer, the ST-ViT contains a spatialconvolutional vision transformer and a temporal transformer, which can improve the model performance. To handle the error fluctuation of the head pose estimation model, this article proposes an adaptive Kalman filter (AKF). By analyzing the error distribution of the estimation model and the user experience of the head tracker, the proposed AKF includes an adaptive observation noise coefficient; this can adaptively moderate the smoothness of the curve. Comprehensive experiments show that the proposed system is feasible and effective, and it achieves a state-of-the-art performance.",
        "keywords": []
      },
      "file_name": "9500dbde37e0f8c6b9ea924aacc6591bc3414166.pdf"
    },
    {
      "success": true,
      "doc_id": "976c42e667cd78cd572011a627d9b860",
      "summary": "Vision Transformers (ViTs) have shown impressive per-formance but still require a high computation cost as compared to convolutional neural networks (CNNs), one rea-son is that ViTs' attention measures global similarities and thus has a quadratic complexity with the number of in-put tokens. Existing efficient ViTs adopt local attention or linear attention, which sacrifice ViTs' capabilities of capturing either global or local context. In this work, we ask an important research question: Can ViTs learn both global and local context while being more efficient during inference? To this end, we propose a framework called Castling- ViT, which trains ViTs using both linear-angular attention and masked softmax-based quadratic attention, but then switches to having only linear-angular attention during inference. Our Castling- ViT leverages angular ker-nels to measure the similarities between queries and keys via spectral angles. And we further simplify it with two techniques: (1) a novel linear-angular attention mechanism: we decompose the angular kernels into linear terms and high-order residuals, and only keep the linear terms; and (2) we adopt two parameterized modules to approximate high-order residuals: a depthwise convolution and an aux-iliary masked softmax attention to help learn global and lo-cal information, where the masks for softmax attention are regularized to gradually become zeros and thus incur no overhead during inference. Extensive experiments validate the effectiveness of our Castling- ViT, e.g., achieving up to a 1.8% higher accuracy or 40% MACs reduction on classification and 1.2 higher mAP on detection under comparable FLOPs, as compared to ViTs with vanilla softmax-based at-tentions. Project page is available at here.",
      "intriguing_abstract": "Vision Transformers (ViTs) have shown impressive per-formance but still require a high computation cost as compared to convolutional neural networks (CNNs), one rea-son is that ViTs' attention measures global similarities and thus has a quadratic complexity with the number of in-put tokens. Existing efficient ViTs adopt local attention or linear attention, which sacrifice ViTs' capabilities of capturing either global or local context. In this work, we ask an important research question: Can ViTs learn both global and local context while being more efficient during inference? To this end, we propose a framework called Castling- ViT, which trains ViTs using both linear-angular attention and masked softmax-based quadratic attention, but then switches to having only linear-angular attention during inference. Our Castling- ViT leverages angular ker-nels to measure the similarities between queries and keys via spectral angles. And we further simplify it with two techniques: (1) a novel linear-angular attention mechanism: we decompose the angular kernels into linear terms and high-order residuals, and only keep the linear terms; and (2) we adopt two parameterized modules to approximate high-order residuals: a depthwise convolution and an aux-iliary masked softmax attention to help learn global and lo-cal information, where the masks for softmax attention are regularized to gradually become zeros and thus incur no overhead during inference. Extensive experiments validate the effectiveness of our Castling- ViT, e.g., achieving up to a 1.8% higher accuracy or 40% MACs reduction on classification and 1.2 higher mAP on detection under comparable FLOPs, as compared to ViTs with vanilla softmax-based at-tentions. Project page is available at here.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/977351c92f156db27592e88b14dee2c22d4b312a.pdf",
      "citation_key": "you2022bor",
      "metadata": {
        "title": "Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference",
        "authors": [
          "Haoran You",
          "Yunyang Xiong",
          "Xiaoliang Dai",
          "Bichen Wu",
          "Peizhao Zhang",
          "Haoqi Fan",
          "Pter Vajda",
          "Yingyan Lin"
        ],
        "published_date": "2022",
        "abstract": "Vision Transformers (ViTs) have shown impressive per-formance but still require a high computation cost as compared to convolutional neural networks (CNNs), one rea-son is that ViTs' attention measures global similarities and thus has a quadratic complexity with the number of in-put tokens. Existing efficient ViTs adopt local attention or linear attention, which sacrifice ViTs' capabilities of capturing either global or local context. In this work, we ask an important research question: Can ViTs learn both global and local context while being more efficient during inference? To this end, we propose a framework called Castling- ViT, which trains ViTs using both linear-angular attention and masked softmax-based quadratic attention, but then switches to having only linear-angular attention during inference. Our Castling- ViT leverages angular ker-nels to measure the similarities between queries and keys via spectral angles. And we further simplify it with two techniques: (1) a novel linear-angular attention mechanism: we decompose the angular kernels into linear terms and high-order residuals, and only keep the linear terms; and (2) we adopt two parameterized modules to approximate high-order residuals: a depthwise convolution and an aux-iliary masked softmax attention to help learn global and lo-cal information, where the masks for softmax attention are regularized to gradually become zeros and thus incur no overhead during inference. Extensive experiments validate the effectiveness of our Castling- ViT, e.g., achieving up to a 1.8% higher accuracy or 40% MACs reduction on classification and 1.2 higher mAP on detection under comparable FLOPs, as compared to ViTs with vanilla softmax-based at-tentions. Project page is available at here.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/977351c92f156db27592e88b14dee2c22d4b312a.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 37,
        "score": 12.333333333333332,
        "summary": "Vision Transformers (ViTs) have shown impressive per-formance but still require a high computation cost as compared to convolutional neural networks (CNNs), one rea-son is that ViTs' attention measures global similarities and thus has a quadratic complexity with the number of in-put tokens. Existing efficient ViTs adopt local attention or linear attention, which sacrifice ViTs' capabilities of capturing either global or local context. In this work, we ask an important research question: Can ViTs learn both global and local context while being more efficient during inference? To this end, we propose a framework called Castling- ViT, which trains ViTs using both linear-angular attention and masked softmax-based quadratic attention, but then switches to having only linear-angular attention during inference. Our Castling- ViT leverages angular ker-nels to measure the similarities between queries and keys via spectral angles. And we further simplify it with two techniques: (1) a novel linear-angular attention mechanism: we decompose the angular kernels into linear terms and high-order residuals, and only keep the linear terms; and (2) we adopt two parameterized modules to approximate high-order residuals: a depthwise convolution and an aux-iliary masked softmax attention to help learn global and lo-cal information, where the masks for softmax attention are regularized to gradually become zeros and thus incur no overhead during inference. Extensive experiments validate the effectiveness of our Castling- ViT, e.g., achieving up to a 1.8% higher accuracy or 40% MACs reduction on classification and 1.2 higher mAP on detection under comparable FLOPs, as compared to ViTs with vanilla softmax-based at-tentions. Project page is available at here.",
        "keywords": []
      },
      "file_name": "977351c92f156db27592e88b14dee2c22d4b312a.pdf"
    },
    {
      "success": true,
      "doc_id": "2f6c41f468c129a7cb1f3effda68f127",
      "summary": "Recently, a surge of interest in visual transformers is to reduce the computational cost by limiting the calculation of self-attention to a local window. Most current work uses a fixed single-scale window for modeling by default, ignoring the impact of window size on model performance. How-ever, this may limit the modeling potential of these window-based models for multi-scale information. In this paper, we propose a novel method, named Dynamic Window Vision Transformer (DW-ViT). The dynamic window strategy proposed by DW- ViT goes beyond the model that employs a fixed single window setting. To the best of our knowl-edge, we are the first to use dynamic multi-scale windows to explore the upper limit of the effect of window settings on model performance. In DW- ViT, multi-scale information is obtained by assigning windows of different sizes to different head groups of window multi-head self-attention. Then, the information is dynamically fused by assigning different weights to the multi-scale window branches. We con-ducted a detailed performance evaluation on three datasets, ImageNet-1K, ADE20K, and COCO. Compared with re-lated state-of-the-art (SoTA) methods, DW- ViT obtains the best performance. Specifically, compared with the current SoTA Swin Transformers [31], DW-ViT has achieved con-sistent and substantial improvements on all three datasets with similar parameters and computational costs. In addition, DW-ViT exhibits good scalability and can be easily inserted into any window-based visual transformers.11Code release: https://github.com/pzhren/DW-ViT. This work was done when the first author interned at Dark Matter AI..",
      "intriguing_abstract": "Recently, a surge of interest in visual transformers is to reduce the computational cost by limiting the calculation of self-attention to a local window. Most current work uses a fixed single-scale window for modeling by default, ignoring the impact of window size on model performance. How-ever, this may limit the modeling potential of these window-based models for multi-scale information. In this paper, we propose a novel method, named Dynamic Window Vision Transformer (DW-ViT). The dynamic window strategy proposed by DW- ViT goes beyond the model that employs a fixed single window setting. To the best of our knowl-edge, we are the first to use dynamic multi-scale windows to explore the upper limit of the effect of window settings on model performance. In DW- ViT, multi-scale information is obtained by assigning windows of different sizes to different head groups of window multi-head self-attention. Then, the information is dynamically fused by assigning different weights to the multi-scale window branches. We con-ducted a detailed performance evaluation on three datasets, ImageNet-1K, ADE20K, and COCO. Compared with re-lated state-of-the-art (SoTA) methods, DW- ViT obtains the best performance. Specifically, compared with the current SoTA Swin Transformers [31], DW-ViT has achieved con-sistent and substantial improvements on all three datasets with similar parameters and computational costs. In addition, DW-ViT exhibits good scalability and can be easily inserted into any window-based visual transformers.11Code release: https://github.com/pzhren/DW-ViT. This work was done when the first author interned at Dark Matter AI..",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/ba12a9915553b3b42df17a33afcfd547821d8cc3.pdf",
      "citation_key": "ren2022ifo",
      "metadata": {
        "title": "Beyond Fixation: Dynamic Window Visual Transformer",
        "authors": [
          "Pengzhen Ren",
          "Changlin Li",
          "Guangrun Wang",
          "Yun Xiao",
          "Qing Du Xiaodan Liang Xiaojun Chang"
        ],
        "published_date": "2022",
        "abstract": "Recently, a surge of interest in visual transformers is to reduce the computational cost by limiting the calculation of self-attention to a local window. Most current work uses a fixed single-scale window for modeling by default, ignoring the impact of window size on model performance. How-ever, this may limit the modeling potential of these window-based models for multi-scale information. In this paper, we propose a novel method, named Dynamic Window Vision Transformer (DW-ViT). The dynamic window strategy proposed by DW- ViT goes beyond the model that employs a fixed single window setting. To the best of our knowl-edge, we are the first to use dynamic multi-scale windows to explore the upper limit of the effect of window settings on model performance. In DW- ViT, multi-scale information is obtained by assigning windows of different sizes to different head groups of window multi-head self-attention. Then, the information is dynamically fused by assigning different weights to the multi-scale window branches. We con-ducted a detailed performance evaluation on three datasets, ImageNet-1K, ADE20K, and COCO. Compared with re-lated state-of-the-art (SoTA) methods, DW- ViT obtains the best performance. Specifically, compared with the current SoTA Swin Transformers [31], DW-ViT has achieved con-sistent and substantial improvements on all three datasets with similar parameters and computational costs. In addition, DW-ViT exhibits good scalability and can be easily inserted into any window-based visual transformers.11Code release: https://github.com/pzhren/DW-ViT. This work was done when the first author interned at Dark Matter AI..",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ba12a9915553b3b42df17a33afcfd547821d8cc3.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 37,
        "score": 12.333333333333332,
        "summary": "Recently, a surge of interest in visual transformers is to reduce the computational cost by limiting the calculation of self-attention to a local window. Most current work uses a fixed single-scale window for modeling by default, ignoring the impact of window size on model performance. How-ever, this may limit the modeling potential of these window-based models for multi-scale information. In this paper, we propose a novel method, named Dynamic Window Vision Transformer (DW-ViT). The dynamic window strategy proposed by DW- ViT goes beyond the model that employs a fixed single window setting. To the best of our knowl-edge, we are the first to use dynamic multi-scale windows to explore the upper limit of the effect of window settings on model performance. In DW- ViT, multi-scale information is obtained by assigning windows of different sizes to different head groups of window multi-head self-attention. Then, the information is dynamically fused by assigning different weights to the multi-scale window branches. We con-ducted a detailed performance evaluation on three datasets, ImageNet-1K, ADE20K, and COCO. Compared with re-lated state-of-the-art (SoTA) methods, DW- ViT obtains the best performance. Specifically, compared with the current SoTA Swin Transformers [31], DW-ViT has achieved con-sistent and substantial improvements on all three datasets with similar parameters and computational costs. In addition, DW-ViT exhibits good scalability and can be easily inserted into any window-based visual transformers.11Code release: https://github.com/pzhren/DW-ViT. This work was done when the first author interned at Dark Matter AI..",
        "keywords": []
      },
      "file_name": "ba12a9915553b3b42df17a33afcfd547821d8cc3.pdf"
    },
    {
      "success": true,
      "doc_id": "08b74d333ce5e13e96871fc40bc626c4",
      "summary": "Accurate recognition of fruits in the orchard is an important step for robot picking in the natural environment, since many CNN models have a low recognition rate when dealing with irregularly shaped and very dense fruits, such as a grape bunch. It is a new trend to use a transformer structure and apply it to a computer vision domain for image processing. This paper provides Swin Transformer and DETR models to achieve grape bunch detection. Additionally, they are compared with traditional CNN models, such as Faster-RCNN, SSD, and YOLO. In addition, the optimal number of stages for a Swin Transformer through experiments is selected. Furthermore, the latest YOLOX model is also used to make a comparison with the Swin Transformer, and the experimental results show that YOLOX has higher accuracy and better detection effect. The above models are trained under red grape datasets collected under natural light. In addition, the dataset is expanded through image data augmentation to achieve a better training effect. After 200 epochs of training, SwinGD obtained an exciting mAP value of 94% when IoU = 0.5. In case of overexposure, overdarkness, and occlusion, SwinGD can recognize more accurately and robustly compared with other models. At the same time, SwinGD still has a better effect when dealing with dense grape bunches. Furthermore, 100 pictures of grapes containing 655 grape bunches are downloaded from Baidu pictures to detect the effect. The Swin Transformer has an accuracy of 91.5%. In order to verify the universality of SwinGD, we conducted a test under green grape images. The experimental results show that SwinGD has a good effect in practical application. The success of SwinGD provides a new solution for precision harvesting in agriculture.",
      "intriguing_abstract": "Accurate recognition of fruits in the orchard is an important step for robot picking in the natural environment, since many CNN models have a low recognition rate when dealing with irregularly shaped and very dense fruits, such as a grape bunch. It is a new trend to use a transformer structure and apply it to a computer vision domain for image processing. This paper provides Swin Transformer and DETR models to achieve grape bunch detection. Additionally, they are compared with traditional CNN models, such as Faster-RCNN, SSD, and YOLO. In addition, the optimal number of stages for a Swin Transformer through experiments is selected. Furthermore, the latest YOLOX model is also used to make a comparison with the Swin Transformer, and the experimental results show that YOLOX has higher accuracy and better detection effect. The above models are trained under red grape datasets collected under natural light. In addition, the dataset is expanded through image data augmentation to achieve a better training effect. After 200 epochs of training, SwinGD obtained an exciting mAP value of 94% when IoU = 0.5. In case of overexposure, overdarkness, and occlusion, SwinGD can recognize more accurately and robustly compared with other models. At the same time, SwinGD still has a better effect when dealing with dense grape bunches. Furthermore, 100 pictures of grapes containing 655 grape bunches are downloaded from Baidu pictures to detect the effect. The Swin Transformer has an accuracy of 91.5%. In order to verify the universality of SwinGD, we conducted a test under green grape images. The experimental results show that SwinGD has a good effect in practical application. The success of SwinGD provides a new solution for precision harvesting in agriculture.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/3efcd3a4c54694a093886981d59e3cffe0dd7149.pdf",
      "citation_key": "wang20215ra",
      "metadata": {
        "title": "SwinGD: A Robust Grape Bunch Detection Model Based on Swin Transformer in Complex Vineyard Environment",
        "authors": [
          "Jinhai Wang",
          "Zongyin Zhang",
          "Lufeng Luo",
          "Wenbo Zhu",
          "Jianwen Chen",
          "Wen Wang"
        ],
        "published_date": "2021",
        "abstract": "Accurate recognition of fruits in the orchard is an important step for robot picking in the natural environment, since many CNN models have a low recognition rate when dealing with irregularly shaped and very dense fruits, such as a grape bunch. It is a new trend to use a transformer structure and apply it to a computer vision domain for image processing. This paper provides Swin Transformer and DETR models to achieve grape bunch detection. Additionally, they are compared with traditional CNN models, such as Faster-RCNN, SSD, and YOLO. In addition, the optimal number of stages for a Swin Transformer through experiments is selected. Furthermore, the latest YOLOX model is also used to make a comparison with the Swin Transformer, and the experimental results show that YOLOX has higher accuracy and better detection effect. The above models are trained under red grape datasets collected under natural light. In addition, the dataset is expanded through image data augmentation to achieve a better training effect. After 200 epochs of training, SwinGD obtained an exciting mAP value of 94% when IoU = 0.5. In case of overexposure, overdarkness, and occlusion, SwinGD can recognize more accurately and robustly compared with other models. At the same time, SwinGD still has a better effect when dealing with dense grape bunches. Furthermore, 100 pictures of grapes containing 655 grape bunches are downloaded from Baidu pictures to detect the effect. The Swin Transformer has an accuracy of 91.5%. In order to verify the universality of SwinGD, we conducted a test under green grape images. The experimental results show that SwinGD has a good effect in practical application. The success of SwinGD provides a new solution for precision harvesting in agriculture.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3efcd3a4c54694a093886981d59e3cffe0dd7149.pdf",
        "venue": "Horticulturae",
        "citationCount": 49,
        "score": 12.25,
        "summary": "Accurate recognition of fruits in the orchard is an important step for robot picking in the natural environment, since many CNN models have a low recognition rate when dealing with irregularly shaped and very dense fruits, such as a grape bunch. It is a new trend to use a transformer structure and apply it to a computer vision domain for image processing. This paper provides Swin Transformer and DETR models to achieve grape bunch detection. Additionally, they are compared with traditional CNN models, such as Faster-RCNN, SSD, and YOLO. In addition, the optimal number of stages for a Swin Transformer through experiments is selected. Furthermore, the latest YOLOX model is also used to make a comparison with the Swin Transformer, and the experimental results show that YOLOX has higher accuracy and better detection effect. The above models are trained under red grape datasets collected under natural light. In addition, the dataset is expanded through image data augmentation to achieve a better training effect. After 200 epochs of training, SwinGD obtained an exciting mAP value of 94% when IoU = 0.5. In case of overexposure, overdarkness, and occlusion, SwinGD can recognize more accurately and robustly compared with other models. At the same time, SwinGD still has a better effect when dealing with dense grape bunches. Furthermore, 100 pictures of grapes containing 655 grape bunches are downloaded from Baidu pictures to detect the effect. The Swin Transformer has an accuracy of 91.5%. In order to verify the universality of SwinGD, we conducted a test under green grape images. The experimental results show that SwinGD has a good effect in practical application. The success of SwinGD provides a new solution for precision harvesting in agriculture.",
        "keywords": []
      },
      "file_name": "3efcd3a4c54694a093886981d59e3cffe0dd7149.pdf"
    },
    {
      "success": true,
      "doc_id": "7f27bdccb7aa19de38d834e65a77ab70",
      "summary": "Building extraction is a popular topic in remote sensing image processing. Efficient building extraction algorithms can identify and segment building areas to provide informative data for downstream tasks. Currently, building extraction is mainly achieved by deep convolutional neural networks (CNNs) based on the U-shaped encoderdecoder architecture. However, the local perceptive field of the convolutional operation poses a challenge for CNNs to fully capture the semantic information of large buildings, especially in high-resolution remote sensing images. Considering the recent success of the Transformer in computer vision tasks, in this paper, first we propose a shifted-window (swin) Transformer-based encoding booster. The proposed encoding booster includes a swin Transformer pyramid containing patch merging layers for down-sampling, which enables our encoding booster to extract semantics from multi-level features at different scales. Most importantly, the receptive field is significantly expanded by the global self-attention mechanism of the swin Transformer, allowing the encoding booster to capture the large-scale semantic information effectively and transcend the limitations of CNNs. Furthermore, we integrate the encoding booster in a specially designed U-shaped network through a novel manner, named the Swin Transformer-based Encoding Booster- U-shaped Network (STEB-UNet), to achieve the feature-level fusion of local and large-scale semantics. Remarkably, compared with other Transformer-included networks, the computational complexity and memory requirement of the STEB-UNet are significantly reduced due to the swin design, making the network training much easier. Experimental results show that the STEB-UNet can effectively discriminate and extract buildings of different scales and demonstrate higher accuracy than the state-of-the-art networks on public datasets.",
      "intriguing_abstract": "Building extraction is a popular topic in remote sensing image processing. Efficient building extraction algorithms can identify and segment building areas to provide informative data for downstream tasks. Currently, building extraction is mainly achieved by deep convolutional neural networks (CNNs) based on the U-shaped encoderdecoder architecture. However, the local perceptive field of the convolutional operation poses a challenge for CNNs to fully capture the semantic information of large buildings, especially in high-resolution remote sensing images. Considering the recent success of the Transformer in computer vision tasks, in this paper, first we propose a shifted-window (swin) Transformer-based encoding booster. The proposed encoding booster includes a swin Transformer pyramid containing patch merging layers for down-sampling, which enables our encoding booster to extract semantics from multi-level features at different scales. Most importantly, the receptive field is significantly expanded by the global self-attention mechanism of the swin Transformer, allowing the encoding booster to capture the large-scale semantic information effectively and transcend the limitations of CNNs. Furthermore, we integrate the encoding booster in a specially designed U-shaped network through a novel manner, named the Swin Transformer-based Encoding Booster- U-shaped Network (STEB-UNet), to achieve the feature-level fusion of local and large-scale semantics. Remarkably, compared with other Transformer-included networks, the computational complexity and memory requirement of the STEB-UNet are significantly reduced due to the swin design, making the network training much easier. Experimental results show that the STEB-UNet can effectively discriminate and extract buildings of different scales and demonstrate higher accuracy than the state-of-the-art networks on public datasets.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/58fc305734a0d5d849ae69b9233af082d712197e.pdf",
      "citation_key": "xiao202229y",
      "metadata": {
        "title": "A Swin Transformer-Based Encoding Booster Integrated in U-Shaped Network for Building Extraction",
        "authors": [
          "Xiao Xiao",
          "Wenliang Guo",
          "Rui Chen",
          "Yilong Hui",
          "J. Wang",
          "Hongyu Zhao"
        ],
        "published_date": "2022",
        "abstract": "Building extraction is a popular topic in remote sensing image processing. Efficient building extraction algorithms can identify and segment building areas to provide informative data for downstream tasks. Currently, building extraction is mainly achieved by deep convolutional neural networks (CNNs) based on the U-shaped encoderdecoder architecture. However, the local perceptive field of the convolutional operation poses a challenge for CNNs to fully capture the semantic information of large buildings, especially in high-resolution remote sensing images. Considering the recent success of the Transformer in computer vision tasks, in this paper, first we propose a shifted-window (swin) Transformer-based encoding booster. The proposed encoding booster includes a swin Transformer pyramid containing patch merging layers for down-sampling, which enables our encoding booster to extract semantics from multi-level features at different scales. Most importantly, the receptive field is significantly expanded by the global self-attention mechanism of the swin Transformer, allowing the encoding booster to capture the large-scale semantic information effectively and transcend the limitations of CNNs. Furthermore, we integrate the encoding booster in a specially designed U-shaped network through a novel manner, named the Swin Transformer-based Encoding Booster- U-shaped Network (STEB-UNet), to achieve the feature-level fusion of local and large-scale semantics. Remarkably, compared with other Transformer-included networks, the computational complexity and memory requirement of the STEB-UNet are significantly reduced due to the swin design, making the network training much easier. Experimental results show that the STEB-UNet can effectively discriminate and extract buildings of different scales and demonstrate higher accuracy than the state-of-the-art networks on public datasets.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/58fc305734a0d5d849ae69b9233af082d712197e.pdf",
        "venue": "Remote Sensing",
        "citationCount": 36,
        "score": 12.0,
        "summary": "Building extraction is a popular topic in remote sensing image processing. Efficient building extraction algorithms can identify and segment building areas to provide informative data for downstream tasks. Currently, building extraction is mainly achieved by deep convolutional neural networks (CNNs) based on the U-shaped encoderdecoder architecture. However, the local perceptive field of the convolutional operation poses a challenge for CNNs to fully capture the semantic information of large buildings, especially in high-resolution remote sensing images. Considering the recent success of the Transformer in computer vision tasks, in this paper, first we propose a shifted-window (swin) Transformer-based encoding booster. The proposed encoding booster includes a swin Transformer pyramid containing patch merging layers for down-sampling, which enables our encoding booster to extract semantics from multi-level features at different scales. Most importantly, the receptive field is significantly expanded by the global self-attention mechanism of the swin Transformer, allowing the encoding booster to capture the large-scale semantic information effectively and transcend the limitations of CNNs. Furthermore, we integrate the encoding booster in a specially designed U-shaped network through a novel manner, named the Swin Transformer-based Encoding Booster- U-shaped Network (STEB-UNet), to achieve the feature-level fusion of local and large-scale semantics. Remarkably, compared with other Transformer-included networks, the computational complexity and memory requirement of the STEB-UNet are significantly reduced due to the swin design, making the network training much easier. Experimental results show that the STEB-UNet can effectively discriminate and extract buildings of different scales and demonstrate higher accuracy than the state-of-the-art networks on public datasets.",
        "keywords": []
      },
      "file_name": "58fc305734a0d5d849ae69b9233af082d712197e.pdf"
    },
    {
      "success": true,
      "doc_id": "dbfd5739e1b2779f2feda08761262055",
      "summary": "Drones are commonly used in numerous applications, such as surveillance, navigation, spraying pesticides in autonomous agricultural systems, various military services, etc., due to their variable sizes and workloads. However, malicious drones that carry harmful objects are often adversely used to intrude restricted areas and attack critical public places. Thus, the timely detection of malicious drones can prevent potential harm. This article proposes a vision transformer (ViT) based framework to distinguish between drones and malicious drones. In the proposed ViT based model, drone images are split into fixed-size patches; then, linearly embeddings and position embeddings are applied, and the resulting sequence of vectors is finally fed to a standard ViT encoder. During classification, an additional learnable classification token associated to the sequence is used. The proposed framework is compared with several handcrafted and deep convolutional neural networks (D-CNN), which reveal that the proposed model has achieved an accuracy of 98.3%, outperforming various handcrafted and D-CNNs models. Additionally, the superiority of the proposed model is illustrated by comparing it with the existing state-of-the-art drone-detection methods.",
      "intriguing_abstract": "Drones are commonly used in numerous applications, such as surveillance, navigation, spraying pesticides in autonomous agricultural systems, various military services, etc., due to their variable sizes and workloads. However, malicious drones that carry harmful objects are often adversely used to intrude restricted areas and attack critical public places. Thus, the timely detection of malicious drones can prevent potential harm. This article proposes a vision transformer (ViT) based framework to distinguish between drones and malicious drones. In the proposed ViT based model, drone images are split into fixed-size patches; then, linearly embeddings and position embeddings are applied, and the resulting sequence of vectors is finally fed to a standard ViT encoder. During classification, an additional learnable classification token associated to the sequence is used. The proposed framework is compared with several handcrafted and deep convolutional neural networks (D-CNN), which reveal that the proposed model has achieved an accuracy of 98.3%, outperforming various handcrafted and D-CNNs models. Additionally, the superiority of the proposed model is illustrated by comparing it with the existing state-of-the-art drone-detection methods.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/54911915a13cf0138c06b696e6c604b12acfe228.pdf",
      "citation_key": "jamil20223a4",
      "metadata": {
        "title": "Distinguishing Malicious Drones Using Vision Transformer",
        "authors": [
          "Sonain Jamil",
          "Muhammad Sohail Abbas",
          "Anisha Roy"
        ],
        "published_date": "2022",
        "abstract": "Drones are commonly used in numerous applications, such as surveillance, navigation, spraying pesticides in autonomous agricultural systems, various military services, etc., due to their variable sizes and workloads. However, malicious drones that carry harmful objects are often adversely used to intrude restricted areas and attack critical public places. Thus, the timely detection of malicious drones can prevent potential harm. This article proposes a vision transformer (ViT) based framework to distinguish between drones and malicious drones. In the proposed ViT based model, drone images are split into fixed-size patches; then, linearly embeddings and position embeddings are applied, and the resulting sequence of vectors is finally fed to a standard ViT encoder. During classification, an additional learnable classification token associated to the sequence is used. The proposed framework is compared with several handcrafted and deep convolutional neural networks (D-CNN), which reveal that the proposed model has achieved an accuracy of 98.3%, outperforming various handcrafted and D-CNNs models. Additionally, the superiority of the proposed model is illustrated by comparing it with the existing state-of-the-art drone-detection methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/54911915a13cf0138c06b696e6c604b12acfe228.pdf",
        "venue": "Applied Informatics",
        "citationCount": 36,
        "score": 12.0,
        "summary": "Drones are commonly used in numerous applications, such as surveillance, navigation, spraying pesticides in autonomous agricultural systems, various military services, etc., due to their variable sizes and workloads. However, malicious drones that carry harmful objects are often adversely used to intrude restricted areas and attack critical public places. Thus, the timely detection of malicious drones can prevent potential harm. This article proposes a vision transformer (ViT) based framework to distinguish between drones and malicious drones. In the proposed ViT based model, drone images are split into fixed-size patches; then, linearly embeddings and position embeddings are applied, and the resulting sequence of vectors is finally fed to a standard ViT encoder. During classification, an additional learnable classification token associated to the sequence is used. The proposed framework is compared with several handcrafted and deep convolutional neural networks (D-CNN), which reveal that the proposed model has achieved an accuracy of 98.3%, outperforming various handcrafted and D-CNNs models. Additionally, the superiority of the proposed model is illustrated by comparing it with the existing state-of-the-art drone-detection methods.",
        "keywords": []
      },
      "file_name": "54911915a13cf0138c06b696e6c604b12acfe228.pdf"
    },
    {
      "success": true,
      "doc_id": "eb9b1392fbae84424b88bd8047d9cab0",
      "summary": "Vision Transformer (ViT) is emerging as a new leader in computer vision with its outstanding performance in many tasks (e.g., ImageNet-22k, JFT-300M). However, the success of ViT relies on pretraining on large datasets. It is difficult for us to use ViT to train from scratch on a small-scale imbalanced capsule endoscopic image dataset. This paper adopts a Transformer neural network with a spatial pooling configuration. Transfomers self-attention mechanism enables it to capture long-range information effectively, and the exploration of ViT spatial structure by pooling can further improve the performance of ViT on our small-scale capsule endoscopy dataset. We trained from scratch on two publicly available datasets for capsule endoscopy disease classification, obtained 79.15% accuracy on the multi-classification task of the Kvasir-Capsule dataset, and 98.63% accuracy on the binary classification task of the Red Lesion Endoscopy dataset.",
      "intriguing_abstract": "Vision Transformer (ViT) is emerging as a new leader in computer vision with its outstanding performance in many tasks (e.g., ImageNet-22k, JFT-300M). However, the success of ViT relies on pretraining on large datasets. It is difficult for us to use ViT to train from scratch on a small-scale imbalanced capsule endoscopic image dataset. This paper adopts a Transformer neural network with a spatial pooling configuration. Transfomers self-attention mechanism enables it to capture long-range information effectively, and the exploration of ViT spatial structure by pooling can further improve the performance of ViT on our small-scale capsule endoscopy dataset. We trained from scratch on two publicly available datasets for capsule endoscopy disease classification, obtained 79.15% accuracy on the multi-classification task of the Kvasir-Capsule dataset, and 98.63% accuracy on the binary classification task of the Red Lesion Endoscopy dataset.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/b8585577d05cebd85d45b7c63f7011851412e794.pdf",
      "citation_key": "bai2022f1v",
      "metadata": {
        "title": "Transformer-Based Disease Identification for Small-Scale Imbalanced Capsule Endoscopy Dataset",
        "authors": [
          "Long Bai",
          "Liangyu Wang",
          "Tong Chen",
          "Yuanhao Zhao",
          "Hongliang Ren"
        ],
        "published_date": "2022",
        "abstract": "Vision Transformer (ViT) is emerging as a new leader in computer vision with its outstanding performance in many tasks (e.g., ImageNet-22k, JFT-300M). However, the success of ViT relies on pretraining on large datasets. It is difficult for us to use ViT to train from scratch on a small-scale imbalanced capsule endoscopic image dataset. This paper adopts a Transformer neural network with a spatial pooling configuration. Transfomers self-attention mechanism enables it to capture long-range information effectively, and the exploration of ViT spatial structure by pooling can further improve the performance of ViT on our small-scale capsule endoscopy dataset. We trained from scratch on two publicly available datasets for capsule endoscopy disease classification, obtained 79.15% accuracy on the multi-classification task of the Kvasir-Capsule dataset, and 98.63% accuracy on the binary classification task of the Red Lesion Endoscopy dataset.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b8585577d05cebd85d45b7c63f7011851412e794.pdf",
        "venue": "Electronics",
        "citationCount": 36,
        "score": 12.0,
        "summary": "Vision Transformer (ViT) is emerging as a new leader in computer vision with its outstanding performance in many tasks (e.g., ImageNet-22k, JFT-300M). However, the success of ViT relies on pretraining on large datasets. It is difficult for us to use ViT to train from scratch on a small-scale imbalanced capsule endoscopic image dataset. This paper adopts a Transformer neural network with a spatial pooling configuration. Transfomers self-attention mechanism enables it to capture long-range information effectively, and the exploration of ViT spatial structure by pooling can further improve the performance of ViT on our small-scale capsule endoscopy dataset. We trained from scratch on two publicly available datasets for capsule endoscopy disease classification, obtained 79.15% accuracy on the multi-classification task of the Kvasir-Capsule dataset, and 98.63% accuracy on the binary classification task of the Red Lesion Endoscopy dataset.",
        "keywords": []
      },
      "file_name": "b8585577d05cebd85d45b7c63f7011851412e794.pdf"
    },
    {
      "success": true,
      "doc_id": "298533417051d5ab8d2c53d637ad65b4",
      "summary": "Convolutional neural networks (CNNs) have achieved milestones in object detection of synthetic aperture radar (SAR) images. Recently, vision transformers and their variants have shown great promise in detection tasks. However, ship detection in SAR images remains a substantial challenge because of the characteristics of strong scattering, multi-scale, and complex backgrounds of ship objects in SAR images. This paper proposes an enhancement Swin transformer detection network, named ESTDNet, to complete the ship detection in SAR images to solve the above problems. We adopt the Swin transformer of Cascade-R-CNN (Cascade R-CNN Swin) as a benchmark model in ESTDNet. Based on this, we built two modules in ESTDNet: the feature enhancement Swin transformer (FESwin) module for improving feature extraction capability and the adjacent feature fusion (AFF) module for optimizing feature pyramids. Firstly, the FESwin module is employed as the backbone network, aggregating contextual information about perceptions before and after the Swin transformer model using CNN. It uses single-point channel information interaction as the primary and local spatial information interaction as the secondary for scale fusion based on capturing visual dependence through self-attention, which improves spatial-to-channel feature expression and increases the utilization of ship information from SAR images. Secondly, the AFF module is a weighted selection fusion of each high-level feature in the feature pyramid with its adjacent shallow-level features using learnable adaptive weights, allowing the ship information of SAR images to be focused on the feature maps at more scales and improving the recognition and localization capability for ships in SAR images. Finally, the ablation study conducted on the SSDD dataset validates the effectiveness of the two components proposed in the ESTDNet detector. Moreover, the experiments executed on two public datasets consisting of SSDD and SARShip demonstrate that the ESTDNet detector outperforms the state-of-the-art methods, which provides a new idea for ship detection in SAR images.",
      "intriguing_abstract": "Convolutional neural networks (CNNs) have achieved milestones in object detection of synthetic aperture radar (SAR) images. Recently, vision transformers and their variants have shown great promise in detection tasks. However, ship detection in SAR images remains a substantial challenge because of the characteristics of strong scattering, multi-scale, and complex backgrounds of ship objects in SAR images. This paper proposes an enhancement Swin transformer detection network, named ESTDNet, to complete the ship detection in SAR images to solve the above problems. We adopt the Swin transformer of Cascade-R-CNN (Cascade R-CNN Swin) as a benchmark model in ESTDNet. Based on this, we built two modules in ESTDNet: the feature enhancement Swin transformer (FESwin) module for improving feature extraction capability and the adjacent feature fusion (AFF) module for optimizing feature pyramids. Firstly, the FESwin module is employed as the backbone network, aggregating contextual information about perceptions before and after the Swin transformer model using CNN. It uses single-point channel information interaction as the primary and local spatial information interaction as the secondary for scale fusion based on capturing visual dependence through self-attention, which improves spatial-to-channel feature expression and increases the utilization of ship information from SAR images. Secondly, the AFF module is a weighted selection fusion of each high-level feature in the feature pyramid with its adjacent shallow-level features using learnable adaptive weights, allowing the ship information of SAR images to be focused on the feature maps at more scales and improving the recognition and localization capability for ships in SAR images. Finally, the ablation study conducted on the SSDD dataset validates the effectiveness of the two components proposed in the ESTDNet detector. Moreover, the experiments executed on two public datasets consisting of SSDD and SARShip demonstrate that the ESTDNet detector outperforms the state-of-the-art methods, which provides a new idea for ship detection in SAR images.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/956d45f7a8916ec921df522c0641fd4f02beccb7.pdf",
      "citation_key": "li2022th8",
      "metadata": {
        "title": "Ship Detection in SAR Images Based on Feature Enhancement Swin Transformer and Adjacent Feature Fusion",
        "authors": [
          "Kuoyang Li",
          "Min Zhang",
          "Maiping Xu",
          "R. Tang",
          "Liang Wang",
          "Hai Wang"
        ],
        "published_date": "2022",
        "abstract": "Convolutional neural networks (CNNs) have achieved milestones in object detection of synthetic aperture radar (SAR) images. Recently, vision transformers and their variants have shown great promise in detection tasks. However, ship detection in SAR images remains a substantial challenge because of the characteristics of strong scattering, multi-scale, and complex backgrounds of ship objects in SAR images. This paper proposes an enhancement Swin transformer detection network, named ESTDNet, to complete the ship detection in SAR images to solve the above problems. We adopt the Swin transformer of Cascade-R-CNN (Cascade R-CNN Swin) as a benchmark model in ESTDNet. Based on this, we built two modules in ESTDNet: the feature enhancement Swin transformer (FESwin) module for improving feature extraction capability and the adjacent feature fusion (AFF) module for optimizing feature pyramids. Firstly, the FESwin module is employed as the backbone network, aggregating contextual information about perceptions before and after the Swin transformer model using CNN. It uses single-point channel information interaction as the primary and local spatial information interaction as the secondary for scale fusion based on capturing visual dependence through self-attention, which improves spatial-to-channel feature expression and increases the utilization of ship information from SAR images. Secondly, the AFF module is a weighted selection fusion of each high-level feature in the feature pyramid with its adjacent shallow-level features using learnable adaptive weights, allowing the ship information of SAR images to be focused on the feature maps at more scales and improving the recognition and localization capability for ships in SAR images. Finally, the ablation study conducted on the SSDD dataset validates the effectiveness of the two components proposed in the ESTDNet detector. Moreover, the experiments executed on two public datasets consisting of SSDD and SARShip demonstrate that the ESTDNet detector outperforms the state-of-the-art methods, which provides a new idea for ship detection in SAR images.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/956d45f7a8916ec921df522c0641fd4f02beccb7.pdf",
        "venue": "Remote Sensing",
        "citationCount": 35,
        "score": 11.666666666666666,
        "summary": "Convolutional neural networks (CNNs) have achieved milestones in object detection of synthetic aperture radar (SAR) images. Recently, vision transformers and their variants have shown great promise in detection tasks. However, ship detection in SAR images remains a substantial challenge because of the characteristics of strong scattering, multi-scale, and complex backgrounds of ship objects in SAR images. This paper proposes an enhancement Swin transformer detection network, named ESTDNet, to complete the ship detection in SAR images to solve the above problems. We adopt the Swin transformer of Cascade-R-CNN (Cascade R-CNN Swin) as a benchmark model in ESTDNet. Based on this, we built two modules in ESTDNet: the feature enhancement Swin transformer (FESwin) module for improving feature extraction capability and the adjacent feature fusion (AFF) module for optimizing feature pyramids. Firstly, the FESwin module is employed as the backbone network, aggregating contextual information about perceptions before and after the Swin transformer model using CNN. It uses single-point channel information interaction as the primary and local spatial information interaction as the secondary for scale fusion based on capturing visual dependence through self-attention, which improves spatial-to-channel feature expression and increases the utilization of ship information from SAR images. Secondly, the AFF module is a weighted selection fusion of each high-level feature in the feature pyramid with its adjacent shallow-level features using learnable adaptive weights, allowing the ship information of SAR images to be focused on the feature maps at more scales and improving the recognition and localization capability for ships in SAR images. Finally, the ablation study conducted on the SSDD dataset validates the effectiveness of the two components proposed in the ESTDNet detector. Moreover, the experiments executed on two public datasets consisting of SSDD and SARShip demonstrate that the ESTDNet detector outperforms the state-of-the-art methods, which provides a new idea for ship detection in SAR images.",
        "keywords": []
      },
      "file_name": "956d45f7a8916ec921df522c0641fd4f02beccb7.pdf"
    },
    {
      "success": true,
      "doc_id": "8cec8f1b812e58ee3ba6c2d3dbec8c5b",
      "summary": "Vision Transformers (ViT) are competing to replace Convolutional Neural Networks (CNN) for various computer vision tasks in medical imaging such as classification and segmentation. While the vulnerability of CNNs to adversarial attacks is a well-known problem, recent works have shown that ViTs are also susceptible to such attacks and suffer significant performance degradation under attack. The vulnerability of ViTs to carefully engineered adversarial samples raises serious concerns about their safety in clinical settings. In this paper, we propose a novel self-ensembling method to enhance the robustness of ViT in the presence of adversarial attacks. The proposed Self-Ensembling Vision Transformer (SEViT) leverages the fact that feature representations learned by initial blocks of a ViT are relatively unaffected by adversarial perturbations. Learning multiple classifiers based on these intermediate feature representations and combining these predictions with that of the final ViT classifier can provide robustness against adversarial attacks. Measuring the consistency between the various predictions can also help detect adversarial samples. Experiments on two modalities (chest X-ray and fundoscopy) demonstrate the efficacy of SEViT architecture to defend against various adversarial attacks in the gray-box (attacker has full knowledge of the target model, but not the defense mechanism) setting. Code: https://github.com/faresmalik/SEViT",
      "intriguing_abstract": "Vision Transformers (ViT) are competing to replace Convolutional Neural Networks (CNN) for various computer vision tasks in medical imaging such as classification and segmentation. While the vulnerability of CNNs to adversarial attacks is a well-known problem, recent works have shown that ViTs are also susceptible to such attacks and suffer significant performance degradation under attack. The vulnerability of ViTs to carefully engineered adversarial samples raises serious concerns about their safety in clinical settings. In this paper, we propose a novel self-ensembling method to enhance the robustness of ViT in the presence of adversarial attacks. The proposed Self-Ensembling Vision Transformer (SEViT) leverages the fact that feature representations learned by initial blocks of a ViT are relatively unaffected by adversarial perturbations. Learning multiple classifiers based on these intermediate feature representations and combining these predictions with that of the final ViT classifier can provide robustness against adversarial attacks. Measuring the consistency between the various predictions can also help detect adversarial samples. Experiments on two modalities (chest X-ray and fundoscopy) demonstrate the efficacy of SEViT architecture to defend against various adversarial attacks in the gray-box (attacker has full knowledge of the target model, but not the defense mechanism) setting. Code: https://github.com/faresmalik/SEViT",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/99fbe810d4194684be03458fdfebacb12d8a5c4e.pdf",
      "citation_key": "almalik20223wr",
      "metadata": {
        "title": "Self-Ensembling Vision Transformer (SEViT) for Robust Medical Image Classification",
        "authors": [
          "Faris Almalik",
          "Mohammad Yaqub",
          "Karthik Nandakumar"
        ],
        "published_date": "2022",
        "abstract": "Vision Transformers (ViT) are competing to replace Convolutional Neural Networks (CNN) for various computer vision tasks in medical imaging such as classification and segmentation. While the vulnerability of CNNs to adversarial attacks is a well-known problem, recent works have shown that ViTs are also susceptible to such attacks and suffer significant performance degradation under attack. The vulnerability of ViTs to carefully engineered adversarial samples raises serious concerns about their safety in clinical settings. In this paper, we propose a novel self-ensembling method to enhance the robustness of ViT in the presence of adversarial attacks. The proposed Self-Ensembling Vision Transformer (SEViT) leverages the fact that feature representations learned by initial blocks of a ViT are relatively unaffected by adversarial perturbations. Learning multiple classifiers based on these intermediate feature representations and combining these predictions with that of the final ViT classifier can provide robustness against adversarial attacks. Measuring the consistency between the various predictions can also help detect adversarial samples. Experiments on two modalities (chest X-ray and fundoscopy) demonstrate the efficacy of SEViT architecture to defend against various adversarial attacks in the gray-box (attacker has full knowledge of the target model, but not the defense mechanism) setting. Code: https://github.com/faresmalik/SEViT",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/99fbe810d4194684be03458fdfebacb12d8a5c4e.pdf",
        "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
        "citationCount": 35,
        "score": 11.666666666666666,
        "summary": "Vision Transformers (ViT) are competing to replace Convolutional Neural Networks (CNN) for various computer vision tasks in medical imaging such as classification and segmentation. While the vulnerability of CNNs to adversarial attacks is a well-known problem, recent works have shown that ViTs are also susceptible to such attacks and suffer significant performance degradation under attack. The vulnerability of ViTs to carefully engineered adversarial samples raises serious concerns about their safety in clinical settings. In this paper, we propose a novel self-ensembling method to enhance the robustness of ViT in the presence of adversarial attacks. The proposed Self-Ensembling Vision Transformer (SEViT) leverages the fact that feature representations learned by initial blocks of a ViT are relatively unaffected by adversarial perturbations. Learning multiple classifiers based on these intermediate feature representations and combining these predictions with that of the final ViT classifier can provide robustness against adversarial attacks. Measuring the consistency between the various predictions can also help detect adversarial samples. Experiments on two modalities (chest X-ray and fundoscopy) demonstrate the efficacy of SEViT architecture to defend against various adversarial attacks in the gray-box (attacker has full knowledge of the target model, but not the defense mechanism) setting. Code: https://github.com/faresmalik/SEViT",
        "keywords": []
      },
      "file_name": "99fbe810d4194684be03458fdfebacb12d8a5c4e.pdf"
    },
    {
      "success": true,
      "doc_id": "060140e72c49490af08c7351c5506fa8",
      "summary": "The latest vision transformer (ViT) has stronger contextual feature representation capability than the existing convolutional neural networks and thus has the potential to depict the remote sensing scenes, which usually have more complicated object distribution and spatial arrangement than ground image scenes. However, recent researches reflect that while ViT learns global features, it also ignores the key local features, which poses a bottleneck for understanding remote sensing scenes. In this letter, we tackle this challenge by proposing a novel multiinstance vision transformer (MITformer). Its originality mainly lies in the classic multiple instance learning (MIL) formulation, where each image patch embedded in ViT is regarded as an instance and each image is regarded as a bag. The benefit of designing ViT under MIL formulation is straightforward, as it helps highlight the feature response of key local regions of remote sensing scenes. Moreover, to enhance the feature propagation of local features, attention-based multilayer perceptron (AMLP) head is embedded at the end of each encoder unit. Finally, to minimize the potential semantic prediction differences between the classic ViT and our MIL head, a semantic consistency loss is designed. Experiments on three remote sensing scene classification benchmarks show that our proposed MITformer outperforms the existing state-of-the-art methods and validate the effectiveness of each component in our MITformer.",
      "intriguing_abstract": "The latest vision transformer (ViT) has stronger contextual feature representation capability than the existing convolutional neural networks and thus has the potential to depict the remote sensing scenes, which usually have more complicated object distribution and spatial arrangement than ground image scenes. However, recent researches reflect that while ViT learns global features, it also ignores the key local features, which poses a bottleneck for understanding remote sensing scenes. In this letter, we tackle this challenge by proposing a novel multiinstance vision transformer (MITformer). Its originality mainly lies in the classic multiple instance learning (MIL) formulation, where each image patch embedded in ViT is regarded as an instance and each image is regarded as a bag. The benefit of designing ViT under MIL formulation is straightforward, as it helps highlight the feature response of key local regions of remote sensing scenes. Moreover, to enhance the feature propagation of local features, attention-based multilayer perceptron (AMLP) head is embedded at the end of each encoder unit. Finally, to minimize the potential semantic prediction differences between the classic ViT and our MIL head, a semantic consistency loss is designed. Experiments on three remote sensing scene classification benchmarks show that our proposed MITformer outperforms the existing state-of-the-art methods and validate the effectiveness of each component in our MITformer.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f4e32b928d7cc27447e312bdc052aa75888045aa.pdf",
      "citation_key": "sha2022ae0",
      "metadata": {
        "title": "MITformer: A Multiinstance Vision Transformer for Remote Sensing Scene Classification",
        "authors": [
          "Z. Sha",
          "Jianfeng Li"
        ],
        "published_date": "2022",
        "abstract": "The latest vision transformer (ViT) has stronger contextual feature representation capability than the existing convolutional neural networks and thus has the potential to depict the remote sensing scenes, which usually have more complicated object distribution and spatial arrangement than ground image scenes. However, recent researches reflect that while ViT learns global features, it also ignores the key local features, which poses a bottleneck for understanding remote sensing scenes. In this letter, we tackle this challenge by proposing a novel multiinstance vision transformer (MITformer). Its originality mainly lies in the classic multiple instance learning (MIL) formulation, where each image patch embedded in ViT is regarded as an instance and each image is regarded as a bag. The benefit of designing ViT under MIL formulation is straightforward, as it helps highlight the feature response of key local regions of remote sensing scenes. Moreover, to enhance the feature propagation of local features, attention-based multilayer perceptron (AMLP) head is embedded at the end of each encoder unit. Finally, to minimize the potential semantic prediction differences between the classic ViT and our MIL head, a semantic consistency loss is designed. Experiments on three remote sensing scene classification benchmarks show that our proposed MITformer outperforms the existing state-of-the-art methods and validate the effectiveness of each component in our MITformer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f4e32b928d7cc27447e312bdc052aa75888045aa.pdf",
        "venue": "IEEE Geoscience and Remote Sensing Letters",
        "citationCount": 34,
        "score": 11.333333333333332,
        "summary": "The latest vision transformer (ViT) has stronger contextual feature representation capability than the existing convolutional neural networks and thus has the potential to depict the remote sensing scenes, which usually have more complicated object distribution and spatial arrangement than ground image scenes. However, recent researches reflect that while ViT learns global features, it also ignores the key local features, which poses a bottleneck for understanding remote sensing scenes. In this letter, we tackle this challenge by proposing a novel multiinstance vision transformer (MITformer). Its originality mainly lies in the classic multiple instance learning (MIL) formulation, where each image patch embedded in ViT is regarded as an instance and each image is regarded as a bag. The benefit of designing ViT under MIL formulation is straightforward, as it helps highlight the feature response of key local regions of remote sensing scenes. Moreover, to enhance the feature propagation of local features, attention-based multilayer perceptron (AMLP) head is embedded at the end of each encoder unit. Finally, to minimize the potential semantic prediction differences between the classic ViT and our MIL head, a semantic consistency loss is designed. Experiments on three remote sensing scene classification benchmarks show that our proposed MITformer outperforms the existing state-of-the-art methods and validate the effectiveness of each component in our MITformer.",
        "keywords": []
      },
      "file_name": "f4e32b928d7cc27447e312bdc052aa75888045aa.pdf"
    },
    {
      "success": true,
      "doc_id": "7e4731ea012a2ce92d7155a7c728d2dd",
      "summary": "Recently, masked image modeling (MIM) has offered a new methodology of self-supervised pre-training of vision transformers. A key idea of efficient implementation is to discard the masked image patches (or tokens) throughout the target network (encoder), which requires the encoder to be a plain vision transformer (e.g., ViT), albeit hierarchical vision transformers (e.g., Swin Transformer) have potentially better properties in formulating vision inputs. In this paper, we offer a new design of hierarchical vision transformers named HiViT (short for Hierarchical ViT) that enjoys both high efficiency and good performance in MIM. The key is to remove the unnecessary\"local inter-unit operations\", deriving structurally simple hierarchical vision transformers in which mask-units can be serialized like plain vision transformers. For this purpose, we start with Swin Transformer and (i) set the masking unit size to be the token size in the main stage of Swin Transformer, (ii) switch off inter-unit self-attentions before the main stage, and (iii) eliminate all operations after the main stage. Empirical studies demonstrate the advantageous performance of HiViT in terms of fully-supervised, self-supervised, and transfer learning. In particular, in running MAE on ImageNet-1K, HiViT-B reports a +0.6% accuracy gain over ViT-B and a 1.9$\\times$ speed-up over Swin-B, and the performance gain generalizes to downstream tasks of detection and segmentation. Code will be made publicly available.",
      "intriguing_abstract": "Recently, masked image modeling (MIM) has offered a new methodology of self-supervised pre-training of vision transformers. A key idea of efficient implementation is to discard the masked image patches (or tokens) throughout the target network (encoder), which requires the encoder to be a plain vision transformer (e.g., ViT), albeit hierarchical vision transformers (e.g., Swin Transformer) have potentially better properties in formulating vision inputs. In this paper, we offer a new design of hierarchical vision transformers named HiViT (short for Hierarchical ViT) that enjoys both high efficiency and good performance in MIM. The key is to remove the unnecessary\"local inter-unit operations\", deriving structurally simple hierarchical vision transformers in which mask-units can be serialized like plain vision transformers. For this purpose, we start with Swin Transformer and (i) set the masking unit size to be the token size in the main stage of Swin Transformer, (ii) switch off inter-unit self-attentions before the main stage, and (iii) eliminate all operations after the main stage. Empirical studies demonstrate the advantageous performance of HiViT in terms of fully-supervised, self-supervised, and transfer learning. In particular, in running MAE on ImageNet-1K, HiViT-B reports a +0.6% accuracy gain over ViT-B and a 1.9$\\times$ speed-up over Swin-B, and the performance gain generalizes to downstream tasks of detection and segmentation. Code will be made publicly available.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/98e702ef2f64ab2643df9e80b1bd034334142e62.pdf",
      "citation_key": "zhang2022msa",
      "metadata": {
        "title": "HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling",
        "authors": [
          "Xiaosong Zhang",
          "Yunjie Tian",
          "Wei Huang",
          "Qixiang Ye",
          "Qi Dai",
          "Lingxi Xie",
          "Qi Tian"
        ],
        "published_date": "2022",
        "abstract": "Recently, masked image modeling (MIM) has offered a new methodology of self-supervised pre-training of vision transformers. A key idea of efficient implementation is to discard the masked image patches (or tokens) throughout the target network (encoder), which requires the encoder to be a plain vision transformer (e.g., ViT), albeit hierarchical vision transformers (e.g., Swin Transformer) have potentially better properties in formulating vision inputs. In this paper, we offer a new design of hierarchical vision transformers named HiViT (short for Hierarchical ViT) that enjoys both high efficiency and good performance in MIM. The key is to remove the unnecessary\"local inter-unit operations\", deriving structurally simple hierarchical vision transformers in which mask-units can be serialized like plain vision transformers. For this purpose, we start with Swin Transformer and (i) set the masking unit size to be the token size in the main stage of Swin Transformer, (ii) switch off inter-unit self-attentions before the main stage, and (iii) eliminate all operations after the main stage. Empirical studies demonstrate the advantageous performance of HiViT in terms of fully-supervised, self-supervised, and transfer learning. In particular, in running MAE on ImageNet-1K, HiViT-B reports a +0.6% accuracy gain over ViT-B and a 1.9$\\times$ speed-up over Swin-B, and the performance gain generalizes to downstream tasks of detection and segmentation. Code will be made publicly available.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/98e702ef2f64ab2643df9e80b1bd034334142e62.pdf",
        "venue": "arXiv.org",
        "citationCount": 33,
        "score": 11.0,
        "summary": "Recently, masked image modeling (MIM) has offered a new methodology of self-supervised pre-training of vision transformers. A key idea of efficient implementation is to discard the masked image patches (or tokens) throughout the target network (encoder), which requires the encoder to be a plain vision transformer (e.g., ViT), albeit hierarchical vision transformers (e.g., Swin Transformer) have potentially better properties in formulating vision inputs. In this paper, we offer a new design of hierarchical vision transformers named HiViT (short for Hierarchical ViT) that enjoys both high efficiency and good performance in MIM. The key is to remove the unnecessary\"local inter-unit operations\", deriving structurally simple hierarchical vision transformers in which mask-units can be serialized like plain vision transformers. For this purpose, we start with Swin Transformer and (i) set the masking unit size to be the token size in the main stage of Swin Transformer, (ii) switch off inter-unit self-attentions before the main stage, and (iii) eliminate all operations after the main stage. Empirical studies demonstrate the advantageous performance of HiViT in terms of fully-supervised, self-supervised, and transfer learning. In particular, in running MAE on ImageNet-1K, HiViT-B reports a +0.6% accuracy gain over ViT-B and a 1.9$\\times$ speed-up over Swin-B, and the performance gain generalizes to downstream tasks of detection and segmentation. Code will be made publicly available.",
        "keywords": []
      },
      "file_name": "98e702ef2f64ab2643df9e80b1bd034334142e62.pdf"
    },
    {
      "success": true,
      "doc_id": "224a60547ee0444576ffca875b8ddac2",
      "summary": "Artificial intelligence as an approach to visual inspection in industrial applications has been considered for decades. Recent successes, driven by advances in deep learning, present a potential paradigm shift and have the potential to facilitate an automated visual inspection, even under complex environmental conditions. Thereby, convolutional neural networks (CNN) have been the de facto standard in deep-learning-based computer vision (CV) for the last 10 years. Recently, attention-based vision transformer architectures emerged and surpassed the performance of CNNs on benchmark datasets, regarding regular CV tasks, such as image classification, object detection, or segmentation. Nevertheless, despite their outstanding results, the application of vision transformers to real world visual inspection is sparse. We suspect that this is likely due to the assumption that they require enormous amounts of data to be effective. In this study, we evaluate this assumption. For this, we perform a systematic comparison of seven widely-used state-of-the-art CNN and transformer based architectures trained in three different use cases in the domain of visual damage assessment for railway freight car maintenance. We show that vision transformer models achieve at least equivalent performance to CNNs in industrial applications with sparse data available, and significantly surpass them in increasingly complex tasks.",
      "intriguing_abstract": "Artificial intelligence as an approach to visual inspection in industrial applications has been considered for decades. Recent successes, driven by advances in deep learning, present a potential paradigm shift and have the potential to facilitate an automated visual inspection, even under complex environmental conditions. Thereby, convolutional neural networks (CNN) have been the de facto standard in deep-learning-based computer vision (CV) for the last 10 years. Recently, attention-based vision transformer architectures emerged and surpassed the performance of CNNs on benchmark datasets, regarding regular CV tasks, such as image classification, object detection, or segmentation. Nevertheless, despite their outstanding results, the application of vision transformers to real world visual inspection is sparse. We suspect that this is likely due to the assumption that they require enormous amounts of data to be effective. In this study, we evaluate this assumption. For this, we perform a systematic comparison of seven widely-used state-of-the-art CNN and transformer based architectures trained in three different use cases in the domain of visual damage assessment for railway freight car maintenance. We show that vision transformer models achieve at least equivalent performance to CNNs in industrial applications with sparse data available, and significantly surpass them in increasingly complex tasks.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/0ef3e7efa5ded21a398d8ec90ff766b467c4fc54.pdf",
      "citation_key": "htten2022lui",
      "metadata": {
        "title": "Vision Transformer in Industrial Visual Inspection",
        "authors": [
          "Nils Htten",
          "R. Meyes",
          "Tobias Meisen"
        ],
        "published_date": "2022",
        "abstract": "Artificial intelligence as an approach to visual inspection in industrial applications has been considered for decades. Recent successes, driven by advances in deep learning, present a potential paradigm shift and have the potential to facilitate an automated visual inspection, even under complex environmental conditions. Thereby, convolutional neural networks (CNN) have been the de facto standard in deep-learning-based computer vision (CV) for the last 10 years. Recently, attention-based vision transformer architectures emerged and surpassed the performance of CNNs on benchmark datasets, regarding regular CV tasks, such as image classification, object detection, or segmentation. Nevertheless, despite their outstanding results, the application of vision transformers to real world visual inspection is sparse. We suspect that this is likely due to the assumption that they require enormous amounts of data to be effective. In this study, we evaluate this assumption. For this, we perform a systematic comparison of seven widely-used state-of-the-art CNN and transformer based architectures trained in three different use cases in the domain of visual damage assessment for railway freight car maintenance. We show that vision transformer models achieve at least equivalent performance to CNNs in industrial applications with sparse data available, and significantly surpass them in increasingly complex tasks.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0ef3e7efa5ded21a398d8ec90ff766b467c4fc54.pdf",
        "venue": "Applied Sciences",
        "citationCount": 32,
        "score": 10.666666666666666,
        "summary": "Artificial intelligence as an approach to visual inspection in industrial applications has been considered for decades. Recent successes, driven by advances in deep learning, present a potential paradigm shift and have the potential to facilitate an automated visual inspection, even under complex environmental conditions. Thereby, convolutional neural networks (CNN) have been the de facto standard in deep-learning-based computer vision (CV) for the last 10 years. Recently, attention-based vision transformer architectures emerged and surpassed the performance of CNNs on benchmark datasets, regarding regular CV tasks, such as image classification, object detection, or segmentation. Nevertheless, despite their outstanding results, the application of vision transformers to real world visual inspection is sparse. We suspect that this is likely due to the assumption that they require enormous amounts of data to be effective. In this study, we evaluate this assumption. For this, we perform a systematic comparison of seven widely-used state-of-the-art CNN and transformer based architectures trained in three different use cases in the domain of visual damage assessment for railway freight car maintenance. We show that vision transformer models achieve at least equivalent performance to CNNs in industrial applications with sparse data available, and significantly surpass them in increasingly complex tasks.",
        "keywords": []
      },
      "file_name": "0ef3e7efa5ded21a398d8ec90ff766b467c4fc54.pdf"
    },
    {
      "success": true,
      "doc_id": "f085aebd72c8e612650611de08820d0b",
      "summary": "Vision Transformers (ViT)s have recently become popular due to their outstanding modeling capabilities, in particular for capturing long-range information, and scalability to dataset and model sizes which has led to state-of-the-art performance in various computer vision and medical image analysis tasks. In this work, we introduce a unified framework consisting of two architectures, dubbed UNetFormer, with a 3D Swin Transformer-based encoder and Convolutional Neural Network (CNN) and transformer-based decoders. In the proposed model, the encoder is linked to the decoder via skip connections at five different resolutions with deep supervision. The design of proposed architecture allows for meeting a wide range of trade-off requirements between accuracy and computational cost. In addition, we present a methodology for self-supervised pre-training of the encoder backbone via learning to predict randomly masked volumetric tokens using contextual information of visible tokens. We pre-train our framework on a cohort of $5050$ CT images, gathered from publicly available CT datasets, and present a systematic investigation of various components such as masking ratio and patch size that affect the representation learning capability and performance of downstream tasks. We validate the effectiveness of our pre-training approach by fine-tuning and testing our model on liver and liver tumor segmentation task using the Medical Segmentation Decathlon (MSD) dataset and achieve state-of-the-art performance in terms of various segmentation metrics. To demonstrate its generalizability, we train and test the model on BraTS 21 dataset for brain tumor segmentation using MRI images and outperform other methods in terms of Dice score. Code: https://github.com/Project-MONAI/research-contributions",
      "intriguing_abstract": "Vision Transformers (ViT)s have recently become popular due to their outstanding modeling capabilities, in particular for capturing long-range information, and scalability to dataset and model sizes which has led to state-of-the-art performance in various computer vision and medical image analysis tasks. In this work, we introduce a unified framework consisting of two architectures, dubbed UNetFormer, with a 3D Swin Transformer-based encoder and Convolutional Neural Network (CNN) and transformer-based decoders. In the proposed model, the encoder is linked to the decoder via skip connections at five different resolutions with deep supervision. The design of proposed architecture allows for meeting a wide range of trade-off requirements between accuracy and computational cost. In addition, we present a methodology for self-supervised pre-training of the encoder backbone via learning to predict randomly masked volumetric tokens using contextual information of visible tokens. We pre-train our framework on a cohort of $5050$ CT images, gathered from publicly available CT datasets, and present a systematic investigation of various components such as masking ratio and patch size that affect the representation learning capability and performance of downstream tasks. We validate the effectiveness of our pre-training approach by fine-tuning and testing our model on liver and liver tumor segmentation task using the Medical Segmentation Decathlon (MSD) dataset and achieve state-of-the-art performance in terms of various segmentation metrics. To demonstrate its generalizability, we train and test the model on BraTS 21 dataset for brain tumor segmentation using MRI images and outperform other methods in terms of Dice score. Code: https://github.com/Project-MONAI/research-contributions",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/a21fa5ff42db4b0bd0fefae3d710cad5f0175400.pdf",
      "citation_key": "hatamizadeh2022y9x",
      "metadata": {
        "title": "UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation",
        "authors": [
          "Ali Hatamizadeh",
          "Ziyue Xu",
          "Dong Yang",
          "Wenqi Li",
          "H. Roth",
          "Daguang Xu"
        ],
        "published_date": "2022",
        "abstract": "Vision Transformers (ViT)s have recently become popular due to their outstanding modeling capabilities, in particular for capturing long-range information, and scalability to dataset and model sizes which has led to state-of-the-art performance in various computer vision and medical image analysis tasks. In this work, we introduce a unified framework consisting of two architectures, dubbed UNetFormer, with a 3D Swin Transformer-based encoder and Convolutional Neural Network (CNN) and transformer-based decoders. In the proposed model, the encoder is linked to the decoder via skip connections at five different resolutions with deep supervision. The design of proposed architecture allows for meeting a wide range of trade-off requirements between accuracy and computational cost. In addition, we present a methodology for self-supervised pre-training of the encoder backbone via learning to predict randomly masked volumetric tokens using contextual information of visible tokens. We pre-train our framework on a cohort of $5050$ CT images, gathered from publicly available CT datasets, and present a systematic investigation of various components such as masking ratio and patch size that affect the representation learning capability and performance of downstream tasks. We validate the effectiveness of our pre-training approach by fine-tuning and testing our model on liver and liver tumor segmentation task using the Medical Segmentation Decathlon (MSD) dataset and achieve state-of-the-art performance in terms of various segmentation metrics. To demonstrate its generalizability, we train and test the model on BraTS 21 dataset for brain tumor segmentation using MRI images and outperform other methods in terms of Dice score. Code: https://github.com/Project-MONAI/research-contributions",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a21fa5ff42db4b0bd0fefae3d710cad5f0175400.pdf",
        "venue": "arXiv.org",
        "citationCount": 31,
        "score": 10.333333333333332,
        "summary": "Vision Transformers (ViT)s have recently become popular due to their outstanding modeling capabilities, in particular for capturing long-range information, and scalability to dataset and model sizes which has led to state-of-the-art performance in various computer vision and medical image analysis tasks. In this work, we introduce a unified framework consisting of two architectures, dubbed UNetFormer, with a 3D Swin Transformer-based encoder and Convolutional Neural Network (CNN) and transformer-based decoders. In the proposed model, the encoder is linked to the decoder via skip connections at five different resolutions with deep supervision. The design of proposed architecture allows for meeting a wide range of trade-off requirements between accuracy and computational cost. In addition, we present a methodology for self-supervised pre-training of the encoder backbone via learning to predict randomly masked volumetric tokens using contextual information of visible tokens. We pre-train our framework on a cohort of $5050$ CT images, gathered from publicly available CT datasets, and present a systematic investigation of various components such as masking ratio and patch size that affect the representation learning capability and performance of downstream tasks. We validate the effectiveness of our pre-training approach by fine-tuning and testing our model on liver and liver tumor segmentation task using the Medical Segmentation Decathlon (MSD) dataset and achieve state-of-the-art performance in terms of various segmentation metrics. To demonstrate its generalizability, we train and test the model on BraTS 21 dataset for brain tumor segmentation using MRI images and outperform other methods in terms of Dice score. Code: https://github.com/Project-MONAI/research-contributions",
        "keywords": []
      },
      "file_name": "a21fa5ff42db4b0bd0fefae3d710cad5f0175400.pdf"
    },
    {
      "success": true,
      "doc_id": "cd1c403b6e9b0e802087ded6d61d5f99",
      "summary": "Recently, there has been a surge of significant interest on application of Deep Learning (DL) models to autonomously perform hand gesture recognition using surface Electromyogram (sEMG) signals. Many of the existing DL models are, however, designed to be applied on sparse sEMG signals. Furthermore, due to the complex structure of these models, typically, we are faced with memory constraint issues, require large training times and a large number of training samples, and; there is the need to resort to data augmentation and/or transfer learning. In this paper, for the first time (to the best of our knowledge), we investigate and design a Vision Transformer (ViT) based architecture to perform hand gesture recognition from High Density (HD-sEMG) signals. Intuitively speaking, we capitalize on the recent breakthrough role of the transformer architecture in tackling different com-plex problems together with its potential for employing more input parallelization via its attention mechanism. The proposed Vision Transformer-based Hand Gesture Recognition (ViT-HGR) framework can overcome the aforementioned training time problems and can accurately classify a large number of hand gestures from scratch without any need for data augmentation and/or transfer learning. The efficiency of the proposed ViT-HGR framework is evaluated using a recently-released HD-sEMG dataset consisting of 65 isometric hand gestures. Our experiments with 64-sample (31.25 ms) window size yield average test accuracy of 84.62  3.07%, where only 78,210 learnable parameters are utilized in the model. The compact structure of the proposed ViT-based ViT-HGR framework (i.e., having significantly reduced number of trainable parameters) shows great potentials for its practical application for prosthetic control.",
      "intriguing_abstract": "Recently, there has been a surge of significant interest on application of Deep Learning (DL) models to autonomously perform hand gesture recognition using surface Electromyogram (sEMG) signals. Many of the existing DL models are, however, designed to be applied on sparse sEMG signals. Furthermore, due to the complex structure of these models, typically, we are faced with memory constraint issues, require large training times and a large number of training samples, and; there is the need to resort to data augmentation and/or transfer learning. In this paper, for the first time (to the best of our knowledge), we investigate and design a Vision Transformer (ViT) based architecture to perform hand gesture recognition from High Density (HD-sEMG) signals. Intuitively speaking, we capitalize on the recent breakthrough role of the transformer architecture in tackling different com-plex problems together with its potential for employing more input parallelization via its attention mechanism. The proposed Vision Transformer-based Hand Gesture Recognition (ViT-HGR) framework can overcome the aforementioned training time problems and can accurately classify a large number of hand gestures from scratch without any need for data augmentation and/or transfer learning. The efficiency of the proposed ViT-HGR framework is evaluated using a recently-released HD-sEMG dataset consisting of 65 isometric hand gestures. Our experiments with 64-sample (31.25 ms) window size yield average test accuracy of 84.62  3.07%, where only 78,210 learnable parameters are utilized in the model. The compact structure of the proposed ViT-based ViT-HGR framework (i.e., having significantly reduced number of trainable parameters) shows great potentials for its practical application for prosthetic control.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/174919e5a4ef95ff66440d56614ad954c6f27df1.pdf",
      "citation_key": "montazerin2022dgi",
      "metadata": {
        "title": "ViT-HGR: Vision Transformer-based Hand Gesture Recognition from High Density Surface EMG Signals",
        "authors": [
          "Mansooreh Montazerin",
          "Soheil Zabihi",
          "E. Rahimian",
          "Arash Mohammadi",
          "Farnoosh Naderkhani"
        ],
        "published_date": "2022",
        "abstract": "Recently, there has been a surge of significant interest on application of Deep Learning (DL) models to autonomously perform hand gesture recognition using surface Electromyogram (sEMG) signals. Many of the existing DL models are, however, designed to be applied on sparse sEMG signals. Furthermore, due to the complex structure of these models, typically, we are faced with memory constraint issues, require large training times and a large number of training samples, and; there is the need to resort to data augmentation and/or transfer learning. In this paper, for the first time (to the best of our knowledge), we investigate and design a Vision Transformer (ViT) based architecture to perform hand gesture recognition from High Density (HD-sEMG) signals. Intuitively speaking, we capitalize on the recent breakthrough role of the transformer architecture in tackling different com-plex problems together with its potential for employing more input parallelization via its attention mechanism. The proposed Vision Transformer-based Hand Gesture Recognition (ViT-HGR) framework can overcome the aforementioned training time problems and can accurately classify a large number of hand gestures from scratch without any need for data augmentation and/or transfer learning. The efficiency of the proposed ViT-HGR framework is evaluated using a recently-released HD-sEMG dataset consisting of 65 isometric hand gestures. Our experiments with 64-sample (31.25 ms) window size yield average test accuracy of 84.62  3.07%, where only 78,210 learnable parameters are utilized in the model. The compact structure of the proposed ViT-based ViT-HGR framework (i.e., having significantly reduced number of trainable parameters) shows great potentials for its practical application for prosthetic control.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/174919e5a4ef95ff66440d56614ad954c6f27df1.pdf",
        "venue": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society",
        "citationCount": 30,
        "score": 10.0,
        "summary": "Recently, there has been a surge of significant interest on application of Deep Learning (DL) models to autonomously perform hand gesture recognition using surface Electromyogram (sEMG) signals. Many of the existing DL models are, however, designed to be applied on sparse sEMG signals. Furthermore, due to the complex structure of these models, typically, we are faced with memory constraint issues, require large training times and a large number of training samples, and; there is the need to resort to data augmentation and/or transfer learning. In this paper, for the first time (to the best of our knowledge), we investigate and design a Vision Transformer (ViT) based architecture to perform hand gesture recognition from High Density (HD-sEMG) signals. Intuitively speaking, we capitalize on the recent breakthrough role of the transformer architecture in tackling different com-plex problems together with its potential for employing more input parallelization via its attention mechanism. The proposed Vision Transformer-based Hand Gesture Recognition (ViT-HGR) framework can overcome the aforementioned training time problems and can accurately classify a large number of hand gestures from scratch without any need for data augmentation and/or transfer learning. The efficiency of the proposed ViT-HGR framework is evaluated using a recently-released HD-sEMG dataset consisting of 65 isometric hand gestures. Our experiments with 64-sample (31.25 ms) window size yield average test accuracy of 84.62  3.07%, where only 78,210 learnable parameters are utilized in the model. The compact structure of the proposed ViT-based ViT-HGR framework (i.e., having significantly reduced number of trainable parameters) shows great potentials for its practical application for prosthetic control.",
        "keywords": []
      },
      "file_name": "174919e5a4ef95ff66440d56614ad954c6f27df1.pdf"
    },
    {
      "success": true,
      "doc_id": "f5fd7c0f9904c841da4508d2d8b018f8",
      "summary": "Vision Transformer (ViT) is becoming more popular in image processing. Specifically, we investigate the effectiveness of test-time adaptation (TTA) on ViT, a technique that has emerged to correct its prediction during test-time by itself. First, we benchmark various test-time adaptation approaches on ViT-B16 and ViT-L16. It is shown that the TTA is effective on ViT and the prior-convention (sensibly selecting modulation parameters) is not necessary when using proper loss function. Based on the observation, we propose a new test-time adaptation method called class-conditional feature alignment (CFA), which minimizes both the class-conditional distribution differences and the whole distribution differences of the hidden representation between the source and target in an online manner. Experiments of image classification tasks on common corruption (CIFAR-10-C, CIFAR-100-C, and ImageNet-C) and domain adaptation (digits datasets and ImageNet-Sketch) show that CFA stably outperforms the existing baselines on various datasets. We also verify that CFA is model agnostic by experimenting on ResNet, MLP-Mixer, and several ViT variants (ViT-AugReg, DeiT, and BeiT). Using BeiT backbone, CFA achieves 19.8% top-1 error rate on ImageNet-C, outperforming the existing test-time adaptation baseline 44.0%. This is a state-of-the-art result among TTA methods that do not need to alter training phase.",
      "intriguing_abstract": "Vision Transformer (ViT) is becoming more popular in image processing. Specifically, we investigate the effectiveness of test-time adaptation (TTA) on ViT, a technique that has emerged to correct its prediction during test-time by itself. First, we benchmark various test-time adaptation approaches on ViT-B16 and ViT-L16. It is shown that the TTA is effective on ViT and the prior-convention (sensibly selecting modulation parameters) is not necessary when using proper loss function. Based on the observation, we propose a new test-time adaptation method called class-conditional feature alignment (CFA), which minimizes both the class-conditional distribution differences and the whole distribution differences of the hidden representation between the source and target in an online manner. Experiments of image classification tasks on common corruption (CIFAR-10-C, CIFAR-100-C, and ImageNet-C) and domain adaptation (digits datasets and ImageNet-Sketch) show that CFA stably outperforms the existing baselines on various datasets. We also verify that CFA is model agnostic by experimenting on ResNet, MLP-Mixer, and several ViT variants (ViT-AugReg, DeiT, and BeiT). Using BeiT backbone, CFA achieves 19.8% top-1 error rate on ImageNet-C, outperforming the existing test-time adaptation baseline 44.0%. This is a state-of-the-art result among TTA methods that do not need to alter training phase.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/6971aee925639a8bd5b79c821570728ef49060c6.pdf",
      "citation_key": "kojima2022k5c",
      "metadata": {
        "title": "Robustifying Vision Transformer without Retraining from Scratch by Test-Time Class-Conditional Feature Alignment",
        "authors": [
          "Takeshi Kojima",
          "Yutaka Matsuo",
          "Yusuke Iwasawa"
        ],
        "published_date": "2022",
        "abstract": "Vision Transformer (ViT) is becoming more popular in image processing. Specifically, we investigate the effectiveness of test-time adaptation (TTA) on ViT, a technique that has emerged to correct its prediction during test-time by itself. First, we benchmark various test-time adaptation approaches on ViT-B16 and ViT-L16. It is shown that the TTA is effective on ViT and the prior-convention (sensibly selecting modulation parameters) is not necessary when using proper loss function. Based on the observation, we propose a new test-time adaptation method called class-conditional feature alignment (CFA), which minimizes both the class-conditional distribution differences and the whole distribution differences of the hidden representation between the source and target in an online manner. Experiments of image classification tasks on common corruption (CIFAR-10-C, CIFAR-100-C, and ImageNet-C) and domain adaptation (digits datasets and ImageNet-Sketch) show that CFA stably outperforms the existing baselines on various datasets. We also verify that CFA is model agnostic by experimenting on ResNet, MLP-Mixer, and several ViT variants (ViT-AugReg, DeiT, and BeiT). Using BeiT backbone, CFA achieves 19.8% top-1 error rate on ImageNet-C, outperforming the existing test-time adaptation baseline 44.0%. This is a state-of-the-art result among TTA methods that do not need to alter training phase.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6971aee925639a8bd5b79c821570728ef49060c6.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 30,
        "score": 10.0,
        "summary": "Vision Transformer (ViT) is becoming more popular in image processing. Specifically, we investigate the effectiveness of test-time adaptation (TTA) on ViT, a technique that has emerged to correct its prediction during test-time by itself. First, we benchmark various test-time adaptation approaches on ViT-B16 and ViT-L16. It is shown that the TTA is effective on ViT and the prior-convention (sensibly selecting modulation parameters) is not necessary when using proper loss function. Based on the observation, we propose a new test-time adaptation method called class-conditional feature alignment (CFA), which minimizes both the class-conditional distribution differences and the whole distribution differences of the hidden representation between the source and target in an online manner. Experiments of image classification tasks on common corruption (CIFAR-10-C, CIFAR-100-C, and ImageNet-C) and domain adaptation (digits datasets and ImageNet-Sketch) show that CFA stably outperforms the existing baselines on various datasets. We also verify that CFA is model agnostic by experimenting on ResNet, MLP-Mixer, and several ViT variants (ViT-AugReg, DeiT, and BeiT). Using BeiT backbone, CFA achieves 19.8% top-1 error rate on ImageNet-C, outperforming the existing test-time adaptation baseline 44.0%. This is a state-of-the-art result among TTA methods that do not need to alter training phase.",
        "keywords": []
      },
      "file_name": "6971aee925639a8bd5b79c821570728ef49060c6.pdf"
    },
    {
      "success": true,
      "doc_id": "8c55d1bf529f950e2f4c8e3c914d0fe6",
      "summary": "Automated Vehicles (AVs) are attracting attention as a safer mobility option thanks to the recent advancement of various sensing technologies that realize a much quicker PerceptionReaction Time than Human-Driven Vehicles (HVs). However, AVs are not entirely free from the risk of accidents, and we currently lack a systematic and reliable method to improve AV safety functions. The manual composition of accident scenarios does not scale. Simulation-based methods do not fully cover the peculiar AV accident patterns that can occur in the real world. Artificial Intelligence (AI) techniques are employed to identify the moments of accidents from ego-vehicle videos. However, most AI-based approaches fall short in accounting for the probable causes of the accidents. Neither of these AI-driven methods offer details for authoring accident scenarios used for AV safety testing. In this paper, we present a customized Vision Transformer (named ViT-TA) that accurately classifies the critical situations around traffic accidents and automatically points out the objects as probable causes based on an Attention map. Using 24,740 frames from Dashcam Accident Dataset (DAD) as training data, ViT-TA detected critical moments at Time-To-Collision (TTC)  1 s with 34.92 higher accuracy than the state-of-the-art approach. ViT-TAs Attention map highlighting the critical objects helped us understand how the situations unfold to put the hypothetical ego vehicles with AV functions at risk. Based on the ViT-TA-assisted interpretation, we systematized the composition of Functional scenarios conceptualized by the PEGASUS project for describing a high-level plan to improve AVs capability of evading critical situations. We propose a novel framework for automatically deriving Logical and Concrete scenarios specified with 6-Layer situational variables defined by the PEGASUS project. We believe our work is vital towards systematically generating highly reliable and trustworthy safety improvement plans for AVs in a scalable manner.",
      "intriguing_abstract": "Automated Vehicles (AVs) are attracting attention as a safer mobility option thanks to the recent advancement of various sensing technologies that realize a much quicker PerceptionReaction Time than Human-Driven Vehicles (HVs). However, AVs are not entirely free from the risk of accidents, and we currently lack a systematic and reliable method to improve AV safety functions. The manual composition of accident scenarios does not scale. Simulation-based methods do not fully cover the peculiar AV accident patterns that can occur in the real world. Artificial Intelligence (AI) techniques are employed to identify the moments of accidents from ego-vehicle videos. However, most AI-based approaches fall short in accounting for the probable causes of the accidents. Neither of these AI-driven methods offer details for authoring accident scenarios used for AV safety testing. In this paper, we present a customized Vision Transformer (named ViT-TA) that accurately classifies the critical situations around traffic accidents and automatically points out the objects as probable causes based on an Attention map. Using 24,740 frames from Dashcam Accident Dataset (DAD) as training data, ViT-TA detected critical moments at Time-To-Collision (TTC)  1 s with 34.92 higher accuracy than the state-of-the-art approach. ViT-TAs Attention map highlighting the critical objects helped us understand how the situations unfold to put the hypothetical ego vehicles with AV functions at risk. Based on the ViT-TA-assisted interpretation, we systematized the composition of Functional scenarios conceptualized by the PEGASUS project for describing a high-level plan to improve AVs capability of evading critical situations. We propose a novel framework for automatically deriving Logical and Concrete scenarios specified with 6-Layer situational variables defined by the PEGASUS project. We believe our work is vital towards systematically generating highly reliable and trustworthy safety improvement plans for AVs in a scalable manner.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/15ad149fc1ba4d8b5de189fdfacac9daecc286d0.pdf",
      "citation_key": "kang2022pv3",
      "metadata": {
        "title": "Vision Transformer for Detecting Critical Situations AndExtracting Functional Scenario for Automated Vehicle Safety Assessment",
        "authors": [
          "Minhee Kang",
          "Wooseop Lee",
          "Keeyeon Hwang",
          "Young Yoon"
        ],
        "published_date": "2022",
        "abstract": "Automated Vehicles (AVs) are attracting attention as a safer mobility option thanks to the recent advancement of various sensing technologies that realize a much quicker PerceptionReaction Time than Human-Driven Vehicles (HVs). However, AVs are not entirely free from the risk of accidents, and we currently lack a systematic and reliable method to improve AV safety functions. The manual composition of accident scenarios does not scale. Simulation-based methods do not fully cover the peculiar AV accident patterns that can occur in the real world. Artificial Intelligence (AI) techniques are employed to identify the moments of accidents from ego-vehicle videos. However, most AI-based approaches fall short in accounting for the probable causes of the accidents. Neither of these AI-driven methods offer details for authoring accident scenarios used for AV safety testing. In this paper, we present a customized Vision Transformer (named ViT-TA) that accurately classifies the critical situations around traffic accidents and automatically points out the objects as probable causes based on an Attention map. Using 24,740 frames from Dashcam Accident Dataset (DAD) as training data, ViT-TA detected critical moments at Time-To-Collision (TTC)  1 s with 34.92 higher accuracy than the state-of-the-art approach. ViT-TAs Attention map highlighting the critical objects helped us understand how the situations unfold to put the hypothetical ego vehicles with AV functions at risk. Based on the ViT-TA-assisted interpretation, we systematized the composition of Functional scenarios conceptualized by the PEGASUS project for describing a high-level plan to improve AVs capability of evading critical situations. We propose a novel framework for automatically deriving Logical and Concrete scenarios specified with 6-Layer situational variables defined by the PEGASUS project. We believe our work is vital towards systematically generating highly reliable and trustworthy safety improvement plans for AVs in a scalable manner.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/15ad149fc1ba4d8b5de189fdfacac9daecc286d0.pdf",
        "venue": "Social Science Research Network",
        "citationCount": 30,
        "score": 10.0,
        "summary": "Automated Vehicles (AVs) are attracting attention as a safer mobility option thanks to the recent advancement of various sensing technologies that realize a much quicker PerceptionReaction Time than Human-Driven Vehicles (HVs). However, AVs are not entirely free from the risk of accidents, and we currently lack a systematic and reliable method to improve AV safety functions. The manual composition of accident scenarios does not scale. Simulation-based methods do not fully cover the peculiar AV accident patterns that can occur in the real world. Artificial Intelligence (AI) techniques are employed to identify the moments of accidents from ego-vehicle videos. However, most AI-based approaches fall short in accounting for the probable causes of the accidents. Neither of these AI-driven methods offer details for authoring accident scenarios used for AV safety testing. In this paper, we present a customized Vision Transformer (named ViT-TA) that accurately classifies the critical situations around traffic accidents and automatically points out the objects as probable causes based on an Attention map. Using 24,740 frames from Dashcam Accident Dataset (DAD) as training data, ViT-TA detected critical moments at Time-To-Collision (TTC)  1 s with 34.92 higher accuracy than the state-of-the-art approach. ViT-TAs Attention map highlighting the critical objects helped us understand how the situations unfold to put the hypothetical ego vehicles with AV functions at risk. Based on the ViT-TA-assisted interpretation, we systematized the composition of Functional scenarios conceptualized by the PEGASUS project for describing a high-level plan to improve AVs capability of evading critical situations. We propose a novel framework for automatically deriving Logical and Concrete scenarios specified with 6-Layer situational variables defined by the PEGASUS project. We believe our work is vital towards systematically generating highly reliable and trustworthy safety improvement plans for AVs in a scalable manner.",
        "keywords": []
      },
      "file_name": "15ad149fc1ba4d8b5de189fdfacac9daecc286d0.pdf"
    },
    {
      "success": true,
      "doc_id": "11bd6e3472082d3977eed3d48fb5dfc3",
      "summary": "Since the outbreak of COVID-19, hundreds of millions of people have been infected, causing millions of deaths, and resulting in a heavy impact on the daily life of countless people. Accurately identifying patients and taking timely isolation measures are necessary ways to stop the spread of COVID-19. Besides the nucleic acid test, lung CT image detection is also a path to quickly identify COVID-19 patients. In this context, deep learning technology can help radiologists identify COVID-19 patients from CT images rapidly. In this paper, we propose a deep learning ensemble framework called VitCNX which combines Vision Transformer and ConvNeXt for COVID-19 CT image identification. We compared our proposed model VitCNX with EfficientNetV2, DenseNet, ResNet-50, and Swin-Transformer which are state-of-the-art deep learning models in the field of image classification, and two individual models which we used for the ensemble (Vision Transformer and ConvNeXt) in binary and three-classification experiments. In the binary classification experiment, VitCNX achieves the best recall of 0.9907, accuracy of 0.9821, F1-score of 0.9855, AUC of 0.9985, and AUPR of 0.9991, which outperforms the other six models. Equally, in the three-classification experiment, VitCNX computes the best precision of 0.9668, an accuracy of 0.9696, and an F1-score of 0.9631, further demonstrating its excellent image classification capability. We hope our proposed VitCNX model could contribute to the recognition of COVID-19 patients.",
      "intriguing_abstract": "Since the outbreak of COVID-19, hundreds of millions of people have been infected, causing millions of deaths, and resulting in a heavy impact on the daily life of countless people. Accurately identifying patients and taking timely isolation measures are necessary ways to stop the spread of COVID-19. Besides the nucleic acid test, lung CT image detection is also a path to quickly identify COVID-19 patients. In this context, deep learning technology can help radiologists identify COVID-19 patients from CT images rapidly. In this paper, we propose a deep learning ensemble framework called VitCNX which combines Vision Transformer and ConvNeXt for COVID-19 CT image identification. We compared our proposed model VitCNX with EfficientNetV2, DenseNet, ResNet-50, and Swin-Transformer which are state-of-the-art deep learning models in the field of image classification, and two individual models which we used for the ensemble (Vision Transformer and ConvNeXt) in binary and three-classification experiments. In the binary classification experiment, VitCNX achieves the best recall of 0.9907, accuracy of 0.9821, F1-score of 0.9855, AUC of 0.9985, and AUPR of 0.9991, which outperforms the other six models. Equally, in the three-classification experiment, VitCNX computes the best precision of 0.9668, an accuracy of 0.9696, and an F1-score of 0.9631, further demonstrating its excellent image classification capability. We hope our proposed VitCNX model could contribute to the recognition of COVID-19 patients.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f66181828b7621892d02480fa1944b5f381be80d.pdf",
      "citation_key": "tian2022qb5",
      "metadata": {
        "title": "A deep ensemble learning-based automated detection of COVID-19 using lung CT images and Vision Transformer and ConvNeXt",
        "authors": [
          "Geng Tian",
          "Ziwei Wang",
          "Chang Wang",
          "Jianhua Chen",
          "Guangyi Liu",
          "He Xu",
          "Yuankang Lu",
          "Zhuoran Han",
          "Yubo Zhao",
          "Zejun Li",
          "Xueming Luo",
          "Lihong Peng"
        ],
        "published_date": "2022",
        "abstract": "Since the outbreak of COVID-19, hundreds of millions of people have been infected, causing millions of deaths, and resulting in a heavy impact on the daily life of countless people. Accurately identifying patients and taking timely isolation measures are necessary ways to stop the spread of COVID-19. Besides the nucleic acid test, lung CT image detection is also a path to quickly identify COVID-19 patients. In this context, deep learning technology can help radiologists identify COVID-19 patients from CT images rapidly. In this paper, we propose a deep learning ensemble framework called VitCNX which combines Vision Transformer and ConvNeXt for COVID-19 CT image identification. We compared our proposed model VitCNX with EfficientNetV2, DenseNet, ResNet-50, and Swin-Transformer which are state-of-the-art deep learning models in the field of image classification, and two individual models which we used for the ensemble (Vision Transformer and ConvNeXt) in binary and three-classification experiments. In the binary classification experiment, VitCNX achieves the best recall of 0.9907, accuracy of 0.9821, F1-score of 0.9855, AUC of 0.9985, and AUPR of 0.9991, which outperforms the other six models. Equally, in the three-classification experiment, VitCNX computes the best precision of 0.9668, an accuracy of 0.9696, and an F1-score of 0.9631, further demonstrating its excellent image classification capability. We hope our proposed VitCNX model could contribute to the recognition of COVID-19 patients.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f66181828b7621892d02480fa1944b5f381be80d.pdf",
        "venue": "Frontiers in Microbiology",
        "citationCount": 29,
        "score": 9.666666666666666,
        "summary": "Since the outbreak of COVID-19, hundreds of millions of people have been infected, causing millions of deaths, and resulting in a heavy impact on the daily life of countless people. Accurately identifying patients and taking timely isolation measures are necessary ways to stop the spread of COVID-19. Besides the nucleic acid test, lung CT image detection is also a path to quickly identify COVID-19 patients. In this context, deep learning technology can help radiologists identify COVID-19 patients from CT images rapidly. In this paper, we propose a deep learning ensemble framework called VitCNX which combines Vision Transformer and ConvNeXt for COVID-19 CT image identification. We compared our proposed model VitCNX with EfficientNetV2, DenseNet, ResNet-50, and Swin-Transformer which are state-of-the-art deep learning models in the field of image classification, and two individual models which we used for the ensemble (Vision Transformer and ConvNeXt) in binary and three-classification experiments. In the binary classification experiment, VitCNX achieves the best recall of 0.9907, accuracy of 0.9821, F1-score of 0.9855, AUC of 0.9985, and AUPR of 0.9991, which outperforms the other six models. Equally, in the three-classification experiment, VitCNX computes the best precision of 0.9668, an accuracy of 0.9696, and an F1-score of 0.9631, further demonstrating its excellent image classification capability. We hope our proposed VitCNX model could contribute to the recognition of COVID-19 patients.",
        "keywords": []
      },
      "file_name": "f66181828b7621892d02480fa1944b5f381be80d.pdf"
    },
    {
      "success": true,
      "doc_id": "44d0f9eba47fe5153ae5eb6fe46b6ee4",
      "summary": "COVID-19 has caused enormous challenges to global economy and public health. The identification of patients with the COVID-19 infection by CT scan images helps prevent its pandemic. Manual screening COVID-19-related CT images spends a lot of time and resources. Artificial intelligence techniques including deep learning can effectively aid doctors and medical workers to screen the COVID-19 patients. In this study, we developed an ensemble deep learning framework, DeepDSR, by combining DenseNet, Swin transformer, and RegNet for COVID-19 image identification. First, we integrate three available COVID-19-related CT image datasets to one larger dataset. Second, we pretrain weights of DenseNet, Swin Transformer, and RegNet on the ImageNet dataset based on transformer learning. Third, we continue to train DenseNet, Swin Transformer, and RegNet on the integrated larger image dataset. Finally, the classification results are obtained by integrating results from the above three models and the soft voting approach. The proposed DeepDSR model is compared to three state-of-the-art deep learning models (EfficientNetV2, ResNet, and Vision transformer) and three individual models (DenseNet, Swin transformer, and RegNet) for binary classification and three-classification problems. The results show that DeepDSR computes the best precision of 0.9833, recall of 0.9895, accuracy of 0.9894, F1-score of 0.9864, AUC of 0.9991 and AUPR of 0.9986 under binary classification problem, and significantly outperforms other methods. Furthermore, DeepDSR obtains the best precision of 0.9740, recall of 0.9653, accuracy of 0.9737, and F1-score of 0.9695 under three-classification problem, further suggesting its powerful image identification ability. We anticipate that the proposed DeepDSR framework contributes to the diagnosis of COVID-19.",
      "intriguing_abstract": "COVID-19 has caused enormous challenges to global economy and public health. The identification of patients with the COVID-19 infection by CT scan images helps prevent its pandemic. Manual screening COVID-19-related CT images spends a lot of time and resources. Artificial intelligence techniques including deep learning can effectively aid doctors and medical workers to screen the COVID-19 patients. In this study, we developed an ensemble deep learning framework, DeepDSR, by combining DenseNet, Swin transformer, and RegNet for COVID-19 image identification. First, we integrate three available COVID-19-related CT image datasets to one larger dataset. Second, we pretrain weights of DenseNet, Swin Transformer, and RegNet on the ImageNet dataset based on transformer learning. Third, we continue to train DenseNet, Swin Transformer, and RegNet on the integrated larger image dataset. Finally, the classification results are obtained by integrating results from the above three models and the soft voting approach. The proposed DeepDSR model is compared to three state-of-the-art deep learning models (EfficientNetV2, ResNet, and Vision transformer) and three individual models (DenseNet, Swin transformer, and RegNet) for binary classification and three-classification problems. The results show that DeepDSR computes the best precision of 0.9833, recall of 0.9895, accuracy of 0.9894, F1-score of 0.9864, AUC of 0.9991 and AUPR of 0.9986 under binary classification problem, and significantly outperforms other methods. Furthermore, DeepDSR obtains the best precision of 0.9740, recall of 0.9653, accuracy of 0.9737, and F1-score of 0.9695 under three-classification problem, further suggesting its powerful image identification ability. We anticipate that the proposed DeepDSR framework contributes to the diagnosis of COVID-19.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/cee8934975dfbe89747af60bbafc95e10a788dc2.pdf",
      "citation_key": "peng2022snr",
      "metadata": {
        "title": "Analysis of CT scan images for COVID-19 pneumonia based on a deep ensemble framework with DenseNet, Swin transformer, and RegNet",
        "authors": [
          "Lihong Peng",
          "Chang Wang",
          "Geng Tian",
          "Guangyi Liu",
          "Gan Li",
          "Yuankang Lu",
          "Jialiang Yang",
          "Min Chen",
          "Zejun Li"
        ],
        "published_date": "2022",
        "abstract": "COVID-19 has caused enormous challenges to global economy and public health. The identification of patients with the COVID-19 infection by CT scan images helps prevent its pandemic. Manual screening COVID-19-related CT images spends a lot of time and resources. Artificial intelligence techniques including deep learning can effectively aid doctors and medical workers to screen the COVID-19 patients. In this study, we developed an ensemble deep learning framework, DeepDSR, by combining DenseNet, Swin transformer, and RegNet for COVID-19 image identification. First, we integrate three available COVID-19-related CT image datasets to one larger dataset. Second, we pretrain weights of DenseNet, Swin Transformer, and RegNet on the ImageNet dataset based on transformer learning. Third, we continue to train DenseNet, Swin Transformer, and RegNet on the integrated larger image dataset. Finally, the classification results are obtained by integrating results from the above three models and the soft voting approach. The proposed DeepDSR model is compared to three state-of-the-art deep learning models (EfficientNetV2, ResNet, and Vision transformer) and three individual models (DenseNet, Swin transformer, and RegNet) for binary classification and three-classification problems. The results show that DeepDSR computes the best precision of 0.9833, recall of 0.9895, accuracy of 0.9894, F1-score of 0.9864, AUC of 0.9991 and AUPR of 0.9986 under binary classification problem, and significantly outperforms other methods. Furthermore, DeepDSR obtains the best precision of 0.9740, recall of 0.9653, accuracy of 0.9737, and F1-score of 0.9695 under three-classification problem, further suggesting its powerful image identification ability. We anticipate that the proposed DeepDSR framework contributes to the diagnosis of COVID-19.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/cee8934975dfbe89747af60bbafc95e10a788dc2.pdf",
        "venue": "Frontiers in Microbiology",
        "citationCount": 29,
        "score": 9.666666666666666,
        "summary": "COVID-19 has caused enormous challenges to global economy and public health. The identification of patients with the COVID-19 infection by CT scan images helps prevent its pandemic. Manual screening COVID-19-related CT images spends a lot of time and resources. Artificial intelligence techniques including deep learning can effectively aid doctors and medical workers to screen the COVID-19 patients. In this study, we developed an ensemble deep learning framework, DeepDSR, by combining DenseNet, Swin transformer, and RegNet for COVID-19 image identification. First, we integrate three available COVID-19-related CT image datasets to one larger dataset. Second, we pretrain weights of DenseNet, Swin Transformer, and RegNet on the ImageNet dataset based on transformer learning. Third, we continue to train DenseNet, Swin Transformer, and RegNet on the integrated larger image dataset. Finally, the classification results are obtained by integrating results from the above three models and the soft voting approach. The proposed DeepDSR model is compared to three state-of-the-art deep learning models (EfficientNetV2, ResNet, and Vision transformer) and three individual models (DenseNet, Swin transformer, and RegNet) for binary classification and three-classification problems. The results show that DeepDSR computes the best precision of 0.9833, recall of 0.9895, accuracy of 0.9894, F1-score of 0.9864, AUC of 0.9991 and AUPR of 0.9986 under binary classification problem, and significantly outperforms other methods. Furthermore, DeepDSR obtains the best precision of 0.9740, recall of 0.9653, accuracy of 0.9737, and F1-score of 0.9695 under three-classification problem, further suggesting its powerful image identification ability. We anticipate that the proposed DeepDSR framework contributes to the diagnosis of COVID-19.",
        "keywords": []
      },
      "file_name": "cee8934975dfbe89747af60bbafc95e10a788dc2.pdf"
    },
    {
      "success": true,
      "doc_id": "f7e8c7a8ce5b3e15c2a43b08dc03e47e",
      "summary": "In recent years, computer networks have become an indispensable part of our life, and these networks are vulnerable to various type of network attacks, compromising the security of our data and the freedom of our communications. In this paper, we propose a new intrusion detection method that uses image conversion from network data flow to produce an RGB image that can be classified using advanced deep learning models. In this method, we proposed to use the decision tree algorithm to identify the important features, and a windowing and overlapping mechanism to convert the varying input size to a standard size image for the classifier. We then use a Vision Transfomer (ViT) classifier to classify the resulting image. Our experimental results show that we can achieve 98.5% accuracy in binary classification on the CIC IDS2017 dataset, and 96.3% on the UNSW-NB15 dataset, which is 8.09% higher than the next best algorithm, the Deep Belief Network with Improved Kernel-Based Extreme Learning (DBN-KELM) method. For multi-class classification, our proposed method can achieve a testing accuracy of 96.4%, which is 5.6% higher than the next best method, the DBN-KELM.",
      "intriguing_abstract": "In recent years, computer networks have become an indispensable part of our life, and these networks are vulnerable to various type of network attacks, compromising the security of our data and the freedom of our communications. In this paper, we propose a new intrusion detection method that uses image conversion from network data flow to produce an RGB image that can be classified using advanced deep learning models. In this method, we proposed to use the decision tree algorithm to identify the important features, and a windowing and overlapping mechanism to convert the varying input size to a standard size image for the classifier. We then use a Vision Transfomer (ViT) classifier to classify the resulting image. Our experimental results show that we can achieve 98.5% accuracy in binary classification on the CIC IDS2017 dataset, and 96.3% on the UNSW-NB15 dataset, which is 8.09% higher than the next best algorithm, the Deep Belief Network with Improved Kernel-Based Extreme Learning (DBN-KELM) method. For multi-class classification, our proposed method can achieve a testing accuracy of 96.4%, which is 5.6% higher than the next best method, the DBN-KELM.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/69e20583872b4f1384ae3f7dbdbec648c19d3d35.pdf",
      "citation_key": "ho20228q6",
      "metadata": {
        "title": "Network Intrusion Detection via Flow-to-Image Conversion and Vision Transformer Classification",
        "authors": [
          "Chi M. K. Ho",
          "K. Yow",
          "Zhongwen Zhu",
          "Sarang Aravamuthan"
        ],
        "published_date": "2022",
        "abstract": "In recent years, computer networks have become an indispensable part of our life, and these networks are vulnerable to various type of network attacks, compromising the security of our data and the freedom of our communications. In this paper, we propose a new intrusion detection method that uses image conversion from network data flow to produce an RGB image that can be classified using advanced deep learning models. In this method, we proposed to use the decision tree algorithm to identify the important features, and a windowing and overlapping mechanism to convert the varying input size to a standard size image for the classifier. We then use a Vision Transfomer (ViT) classifier to classify the resulting image. Our experimental results show that we can achieve 98.5% accuracy in binary classification on the CIC IDS2017 dataset, and 96.3% on the UNSW-NB15 dataset, which is 8.09% higher than the next best algorithm, the Deep Belief Network with Improved Kernel-Based Extreme Learning (DBN-KELM) method. For multi-class classification, our proposed method can achieve a testing accuracy of 96.4%, which is 5.6% higher than the next best method, the DBN-KELM.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/69e20583872b4f1384ae3f7dbdbec648c19d3d35.pdf",
        "venue": "IEEE Access",
        "citationCount": 29,
        "score": 9.666666666666666,
        "summary": "In recent years, computer networks have become an indispensable part of our life, and these networks are vulnerable to various type of network attacks, compromising the security of our data and the freedom of our communications. In this paper, we propose a new intrusion detection method that uses image conversion from network data flow to produce an RGB image that can be classified using advanced deep learning models. In this method, we proposed to use the decision tree algorithm to identify the important features, and a windowing and overlapping mechanism to convert the varying input size to a standard size image for the classifier. We then use a Vision Transfomer (ViT) classifier to classify the resulting image. Our experimental results show that we can achieve 98.5% accuracy in binary classification on the CIC IDS2017 dataset, and 96.3% on the UNSW-NB15 dataset, which is 8.09% higher than the next best algorithm, the Deep Belief Network with Improved Kernel-Based Extreme Learning (DBN-KELM) method. For multi-class classification, our proposed method can achieve a testing accuracy of 96.4%, which is 5.6% higher than the next best method, the DBN-KELM.",
        "keywords": []
      },
      "file_name": "69e20583872b4f1384ae3f7dbdbec648c19d3d35.pdf"
    },
    {
      "success": true,
      "doc_id": "98bccc02a16548e6848d75eaf923c945",
      "summary": "We revisit the existing excellent Transformers from the perspective of practical application. Most of them are not even as efficient as the basic ResNets series and deviate from the realistic deployment scenario. It may be due to the current criterion to measure computation efficiency, such as FLOPs or parameters is one-sided, sub-optimal, and hardware-insensitive. Thus, this paper directly treats the TensorRT latency on the specific hardware as an efficiency metric, which provides more comprehensive feedback involving computational capacity, memory cost, and bandwidth. Based on a series of controlled experiments, this work derives four practical guidelines for TensorRT-oriented and deployment-friendly network design, e.g., early CNN and late Transformer at stage-level, early Transformer and late CNN at block-level. Accordingly, a family of TensortRT-oriented Transformers is presented, abbreviated as TRT-ViT. Extensive experiments demonstrate that TRT-ViT significantly outperforms existing ConvNets and vision Transformers with respect to the latency/accuracy trade-off across diverse visual tasks, e.g., image classification, object detection and semantic segmentation. For example, at 82.7% ImageNet-1k top-1 accuracy, TRT-ViT is 2.7$\\times$ faster than CSWin and 2.0$\\times$ faster than Twins. On the MS-COCO object detection task, TRT-ViT achieves comparable performance with Twins, while the inference speed is increased by 2.8$\\times$.",
      "intriguing_abstract": "We revisit the existing excellent Transformers from the perspective of practical application. Most of them are not even as efficient as the basic ResNets series and deviate from the realistic deployment scenario. It may be due to the current criterion to measure computation efficiency, such as FLOPs or parameters is one-sided, sub-optimal, and hardware-insensitive. Thus, this paper directly treats the TensorRT latency on the specific hardware as an efficiency metric, which provides more comprehensive feedback involving computational capacity, memory cost, and bandwidth. Based on a series of controlled experiments, this work derives four practical guidelines for TensorRT-oriented and deployment-friendly network design, e.g., early CNN and late Transformer at stage-level, early Transformer and late CNN at block-level. Accordingly, a family of TensortRT-oriented Transformers is presented, abbreviated as TRT-ViT. Extensive experiments demonstrate that TRT-ViT significantly outperforms existing ConvNets and vision Transformers with respect to the latency/accuracy trade-off across diverse visual tasks, e.g., image classification, object detection and semantic segmentation. For example, at 82.7% ImageNet-1k top-1 accuracy, TRT-ViT is 2.7$\\times$ faster than CSWin and 2.0$\\times$ faster than Twins. On the MS-COCO object detection task, TRT-ViT achieves comparable performance with Twins, while the inference speed is increased by 2.8$\\times$.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/0594eaa8dfe580678a2382aaf77ac3582c872a97.pdf",
      "citation_key": "xia2022dnj",
      "metadata": {
        "title": "TRT-ViT: TensorRT-oriented Vision Transformer",
        "authors": [
          "Xin Xia",
          "Jiashi Li",
          "Jie Wu",
          "Xing Wang",
          "Ming-Yu Wang",
          "Xuefeng Xiao",
          "Minghang Zheng",
          "Rui Wang"
        ],
        "published_date": "2022",
        "abstract": "We revisit the existing excellent Transformers from the perspective of practical application. Most of them are not even as efficient as the basic ResNets series and deviate from the realistic deployment scenario. It may be due to the current criterion to measure computation efficiency, such as FLOPs or parameters is one-sided, sub-optimal, and hardware-insensitive. Thus, this paper directly treats the TensorRT latency on the specific hardware as an efficiency metric, which provides more comprehensive feedback involving computational capacity, memory cost, and bandwidth. Based on a series of controlled experiments, this work derives four practical guidelines for TensorRT-oriented and deployment-friendly network design, e.g., early CNN and late Transformer at stage-level, early Transformer and late CNN at block-level. Accordingly, a family of TensortRT-oriented Transformers is presented, abbreviated as TRT-ViT. Extensive experiments demonstrate that TRT-ViT significantly outperforms existing ConvNets and vision Transformers with respect to the latency/accuracy trade-off across diverse visual tasks, e.g., image classification, object detection and semantic segmentation. For example, at 82.7% ImageNet-1k top-1 accuracy, TRT-ViT is 2.7$\\times$ faster than CSWin and 2.0$\\times$ faster than Twins. On the MS-COCO object detection task, TRT-ViT achieves comparable performance with Twins, while the inference speed is increased by 2.8$\\times$.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0594eaa8dfe580678a2382aaf77ac3582c872a97.pdf",
        "venue": "arXiv.org",
        "citationCount": 28,
        "score": 9.333333333333332,
        "summary": "We revisit the existing excellent Transformers from the perspective of practical application. Most of them are not even as efficient as the basic ResNets series and deviate from the realistic deployment scenario. It may be due to the current criterion to measure computation efficiency, such as FLOPs or parameters is one-sided, sub-optimal, and hardware-insensitive. Thus, this paper directly treats the TensorRT latency on the specific hardware as an efficiency metric, which provides more comprehensive feedback involving computational capacity, memory cost, and bandwidth. Based on a series of controlled experiments, this work derives four practical guidelines for TensorRT-oriented and deployment-friendly network design, e.g., early CNN and late Transformer at stage-level, early Transformer and late CNN at block-level. Accordingly, a family of TensortRT-oriented Transformers is presented, abbreviated as TRT-ViT. Extensive experiments demonstrate that TRT-ViT significantly outperforms existing ConvNets and vision Transformers with respect to the latency/accuracy trade-off across diverse visual tasks, e.g., image classification, object detection and semantic segmentation. For example, at 82.7% ImageNet-1k top-1 accuracy, TRT-ViT is 2.7$\\times$ faster than CSWin and 2.0$\\times$ faster than Twins. On the MS-COCO object detection task, TRT-ViT achieves comparable performance with Twins, while the inference speed is increased by 2.8$\\times$.",
        "keywords": []
      },
      "file_name": "0594eaa8dfe580678a2382aaf77ac3582c872a97.pdf"
    },
    {
      "success": true,
      "doc_id": "53bc4e1ed60f1ccaa17e048081599158",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/3a0145f34bcd35f09db23b2edec3ed097894444c.pdf",
      "citation_key": "wang202232c",
      "metadata": {
        "title": "Recognition of penetration state in GTAW based on vision transformer using weld pool image",
        "authors": [
          "Zhenmin Wang",
          "Haoyu Chen",
          "Q. Zhong",
          "Sanbao Lin",
          "Jianwen Wu",
          "Mengjia Xu",
          "Qin Zhang"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3a0145f34bcd35f09db23b2edec3ed097894444c.pdf",
        "venue": "The International Journal of Advanced Manufacturing Technology",
        "citationCount": 28,
        "score": 9.333333333333332,
        "summary": "",
        "keywords": []
      },
      "file_name": "3a0145f34bcd35f09db23b2edec3ed097894444c.pdf"
    },
    {
      "success": true,
      "doc_id": "f51b073683d23a179fef385e7913b587",
      "summary": "Identifying an individual based on their physical/behavioral characteristics is known as biometric recognition. Gait is one of the most reliable biometrics due to its advantages, such as being perceivable at a long distance and difficult to replicate. The existing works mostly leverage Convolutional Neural Networks for gait recognition. The Convolutional Neural Networks perform well in image recognition tasks; however, they lack the attention mechanism to emphasize more on the significant regions of the image. The attention mechanism encodes information in the image patches, which facilitates the model to learn the substantial features in the specific regions. In light of this, this work employs the Vision Transformer (ViT) with an attention mechanism for gait recognition, referred to as Gait-ViT. In the proposed Gait-ViT, the gait energy image is first obtained by averaging the series of images over the gait cycle. The images are then split into patches and transformed into sequences by flattening and patch embedding. Position embedding, along with patch embedding, are applied on the sequence of patches to restore the positional information of the patches. Subsequently, the sequence of vectors is fed to the Transformer encoder to produce the final gait representation. As for the classification, the first element of the sequence is sent to the multi-layer perceptron to predict the class label. The proposed method obtained 99.93% on CASIA-B, 100% on OU-ISIR D and 99.51% on OU-LP, which exhibit the ability of the Vision Transformer model to outperform the state-of-the-art methods.",
      "intriguing_abstract": "Identifying an individual based on their physical/behavioral characteristics is known as biometric recognition. Gait is one of the most reliable biometrics due to its advantages, such as being perceivable at a long distance and difficult to replicate. The existing works mostly leverage Convolutional Neural Networks for gait recognition. The Convolutional Neural Networks perform well in image recognition tasks; however, they lack the attention mechanism to emphasize more on the significant regions of the image. The attention mechanism encodes information in the image patches, which facilitates the model to learn the substantial features in the specific regions. In light of this, this work employs the Vision Transformer (ViT) with an attention mechanism for gait recognition, referred to as Gait-ViT. In the proposed Gait-ViT, the gait energy image is first obtained by averaging the series of images over the gait cycle. The images are then split into patches and transformed into sequences by flattening and patch embedding. Position embedding, along with patch embedding, are applied on the sequence of patches to restore the positional information of the patches. Subsequently, the sequence of vectors is fed to the Transformer encoder to produce the final gait representation. As for the classification, the first element of the sequence is sent to the multi-layer perceptron to predict the class label. The proposed method obtained 99.93% on CASIA-B, 100% on OU-ISIR D and 99.51% on OU-LP, which exhibit the ability of the Vision Transformer model to outperform the state-of-the-art methods.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/ce79649b126dfe9e3cbeb1ecd64a80708bbd5538.pdf",
      "citation_key": "mogan202229d",
      "metadata": {
        "title": "Gait-ViT: Gait Recognition with Vision Transformer",
        "authors": [
          "Jashila Nair Mogan",
          "C. Lee",
          "K. Lim",
          "K. Anbananthen"
        ],
        "published_date": "2022",
        "abstract": "Identifying an individual based on their physical/behavioral characteristics is known as biometric recognition. Gait is one of the most reliable biometrics due to its advantages, such as being perceivable at a long distance and difficult to replicate. The existing works mostly leverage Convolutional Neural Networks for gait recognition. The Convolutional Neural Networks perform well in image recognition tasks; however, they lack the attention mechanism to emphasize more on the significant regions of the image. The attention mechanism encodes information in the image patches, which facilitates the model to learn the substantial features in the specific regions. In light of this, this work employs the Vision Transformer (ViT) with an attention mechanism for gait recognition, referred to as Gait-ViT. In the proposed Gait-ViT, the gait energy image is first obtained by averaging the series of images over the gait cycle. The images are then split into patches and transformed into sequences by flattening and patch embedding. Position embedding, along with patch embedding, are applied on the sequence of patches to restore the positional information of the patches. Subsequently, the sequence of vectors is fed to the Transformer encoder to produce the final gait representation. As for the classification, the first element of the sequence is sent to the multi-layer perceptron to predict the class label. The proposed method obtained 99.93% on CASIA-B, 100% on OU-ISIR D and 99.51% on OU-LP, which exhibit the ability of the Vision Transformer model to outperform the state-of-the-art methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ce79649b126dfe9e3cbeb1ecd64a80708bbd5538.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 27,
        "score": 9.0,
        "summary": "Identifying an individual based on their physical/behavioral characteristics is known as biometric recognition. Gait is one of the most reliable biometrics due to its advantages, such as being perceivable at a long distance and difficult to replicate. The existing works mostly leverage Convolutional Neural Networks for gait recognition. The Convolutional Neural Networks perform well in image recognition tasks; however, they lack the attention mechanism to emphasize more on the significant regions of the image. The attention mechanism encodes information in the image patches, which facilitates the model to learn the substantial features in the specific regions. In light of this, this work employs the Vision Transformer (ViT) with an attention mechanism for gait recognition, referred to as Gait-ViT. In the proposed Gait-ViT, the gait energy image is first obtained by averaging the series of images over the gait cycle. The images are then split into patches and transformed into sequences by flattening and patch embedding. Position embedding, along with patch embedding, are applied on the sequence of patches to restore the positional information of the patches. Subsequently, the sequence of vectors is fed to the Transformer encoder to produce the final gait representation. As for the classification, the first element of the sequence is sent to the multi-layer perceptron to predict the class label. The proposed method obtained 99.93% on CASIA-B, 100% on OU-ISIR D and 99.51% on OU-LP, which exhibit the ability of the Vision Transformer model to outperform the state-of-the-art methods.",
        "keywords": []
      },
      "file_name": "ce79649b126dfe9e3cbeb1ecd64a80708bbd5538.pdf"
    },
    {
      "success": true,
      "doc_id": "9d15476bc23f9d169ff09f735be71a46",
      "summary": "We propose an intrusion detection model based on an improved vision transformer (ViT). More specifically, the model uses an attention mechanism to process data, which overcomes the flaw of the shortterm memory in recurrent neural network (RNN) and the difficulty of learning remote dependency in convolutional neural network. It supports parallelization and has a faster computing speed than RNN. A sliding window mechanism is presented to improve the capability of modeling local features for ViT. The hierarchical focal loss function is used to improve the classification effect, and solve the issue of the data imbalance. The public intrusion detection dataset NSLKDD is used for experimental simulations. By experimental simulations, the accuracy is up to 99.68%, the falsepositive rate is 0.22%, and the recall rate is 99.57%, which show that the improved ViT has better accuracy, false positive rate, and recall rate than existing intrusion detection models.",
      "intriguing_abstract": "We propose an intrusion detection model based on an improved vision transformer (ViT). More specifically, the model uses an attention mechanism to process data, which overcomes the flaw of the shortterm memory in recurrent neural network (RNN) and the difficulty of learning remote dependency in convolutional neural network. It supports parallelization and has a faster computing speed than RNN. A sliding window mechanism is presented to improve the capability of modeling local features for ViT. The hierarchical focal loss function is used to improve the classification effect, and solve the issue of the data imbalance. The public intrusion detection dataset NSLKDD is used for experimental simulations. By experimental simulations, the accuracy is up to 99.68%, the falsepositive rate is 0.22%, and the recall rate is 99.57%, which show that the improved ViT has better accuracy, false positive rate, and recall rate than existing intrusion detection models.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/572ed945b06818472105bd17cfeb355d4e46c5e5.pdf",
      "citation_key": "yang20221ce",
      "metadata": {
        "title": "Intrusion detection: A model based on the improved vision transformer",
        "authors": [
          "Yuguang Yang",
          "HongMei Fu",
          "Shang Gao",
          "Yihua Zhou",
          "Wei-Min shi"
        ],
        "published_date": "2022",
        "abstract": "We propose an intrusion detection model based on an improved vision transformer (ViT). More specifically, the model uses an attention mechanism to process data, which overcomes the flaw of the shortterm memory in recurrent neural network (RNN) and the difficulty of learning remote dependency in convolutional neural network. It supports parallelization and has a faster computing speed than RNN. A sliding window mechanism is presented to improve the capability of modeling local features for ViT. The hierarchical focal loss function is used to improve the classification effect, and solve the issue of the data imbalance. The public intrusion detection dataset NSLKDD is used for experimental simulations. By experimental simulations, the accuracy is up to 99.68%, the falsepositive rate is 0.22%, and the recall rate is 99.57%, which show that the improved ViT has better accuracy, false positive rate, and recall rate than existing intrusion detection models.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/572ed945b06818472105bd17cfeb355d4e46c5e5.pdf",
        "venue": "Transactions on Emerging Telecommunications Technologies",
        "citationCount": 27,
        "score": 9.0,
        "summary": "We propose an intrusion detection model based on an improved vision transformer (ViT). More specifically, the model uses an attention mechanism to process data, which overcomes the flaw of the shortterm memory in recurrent neural network (RNN) and the difficulty of learning remote dependency in convolutional neural network. It supports parallelization and has a faster computing speed than RNN. A sliding window mechanism is presented to improve the capability of modeling local features for ViT. The hierarchical focal loss function is used to improve the classification effect, and solve the issue of the data imbalance. The public intrusion detection dataset NSLKDD is used for experimental simulations. By experimental simulations, the accuracy is up to 99.68%, the falsepositive rate is 0.22%, and the recall rate is 99.57%, which show that the improved ViT has better accuracy, false positive rate, and recall rate than existing intrusion detection models.",
        "keywords": []
      },
      "file_name": "572ed945b06818472105bd17cfeb355d4e46c5e5.pdf"
    },
    {
      "success": true,
      "doc_id": "043f6582c2d5850e4eedfb5b22e5404f",
      "summary": "Recent works have demonstrated that transformer can achieve promising performance in computer vision, by exploiting the relationship among image patches with self-attention. They only consider the attention in a single feature layer, but ignore the complementarity of attention in different layers. In this article, we propose broad attention to improve the performance by incorporating the attention relationship of different layers for vision transformer (ViT), which is called BViT. The broad attention is implemented by broad connection and parameter-free attention. Broad connection of each transformer layer promotes the transmission and integration of information for BViT. Without introducing additional trainable parameters, parameter-free attention jointly focuses on the already available attention information in different layers for extracting useful information and building their relationship. Experiments on image classification tasks demonstrate that BViT delivers superior accuracy of 75.0%/81.6% top-1 accuracy on ImageNet with 5M/22M parameters. Moreover, we transfer BViT to downstream object recognition benchmarks to achieve 98.9% and 89.9% on CIFAR10 and CIFAR100, respectively, that exceed ViT with fewer parameters. For the generalization test, the broad attention in Swin Transformer, T2T-ViT and LVT also brings an improvement of more than 1%. To sum up, broad attention is promising to promote the performance of attention-based models. Code and pretrained models are available at https://github.com/DRL/BViT.",
      "intriguing_abstract": "Recent works have demonstrated that transformer can achieve promising performance in computer vision, by exploiting the relationship among image patches with self-attention. They only consider the attention in a single feature layer, but ignore the complementarity of attention in different layers. In this article, we propose broad attention to improve the performance by incorporating the attention relationship of different layers for vision transformer (ViT), which is called BViT. The broad attention is implemented by broad connection and parameter-free attention. Broad connection of each transformer layer promotes the transmission and integration of information for BViT. Without introducing additional trainable parameters, parameter-free attention jointly focuses on the already available attention information in different layers for extracting useful information and building their relationship. Experiments on image classification tasks demonstrate that BViT delivers superior accuracy of 75.0%/81.6% top-1 accuracy on ImageNet with 5M/22M parameters. Moreover, we transfer BViT to downstream object recognition benchmarks to achieve 98.9% and 89.9% on CIFAR10 and CIFAR100, respectively, that exceed ViT with fewer parameters. For the generalization test, the broad attention in Swin Transformer, T2T-ViT and LVT also brings an improvement of more than 1%. To sum up, broad attention is promising to promote the performance of attention-based models. Code and pretrained models are available at https://github.com/DRL/BViT.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/934942934a6a785e2a80daa6421fa79971558b89.pdf",
      "citation_key": "li2022ip7",
      "metadata": {
        "title": "BViT: Broad Attention-Based Vision Transformer",
        "authors": [
          "Nannan Li",
          "Yaran Chen",
          "Weifan Li",
          "Zixiang Ding",
          "Dong Zhao"
        ],
        "published_date": "2022",
        "abstract": "Recent works have demonstrated that transformer can achieve promising performance in computer vision, by exploiting the relationship among image patches with self-attention. They only consider the attention in a single feature layer, but ignore the complementarity of attention in different layers. In this article, we propose broad attention to improve the performance by incorporating the attention relationship of different layers for vision transformer (ViT), which is called BViT. The broad attention is implemented by broad connection and parameter-free attention. Broad connection of each transformer layer promotes the transmission and integration of information for BViT. Without introducing additional trainable parameters, parameter-free attention jointly focuses on the already available attention information in different layers for extracting useful information and building their relationship. Experiments on image classification tasks demonstrate that BViT delivers superior accuracy of 75.0%/81.6% top-1 accuracy on ImageNet with 5M/22M parameters. Moreover, we transfer BViT to downstream object recognition benchmarks to achieve 98.9% and 89.9% on CIFAR10 and CIFAR100, respectively, that exceed ViT with fewer parameters. For the generalization test, the broad attention in Swin Transformer, T2T-ViT and LVT also brings an improvement of more than 1%. To sum up, broad attention is promising to promote the performance of attention-based models. Code and pretrained models are available at https://github.com/DRL/BViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/934942934a6a785e2a80daa6421fa79971558b89.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 27,
        "score": 9.0,
        "summary": "Recent works have demonstrated that transformer can achieve promising performance in computer vision, by exploiting the relationship among image patches with self-attention. They only consider the attention in a single feature layer, but ignore the complementarity of attention in different layers. In this article, we propose broad attention to improve the performance by incorporating the attention relationship of different layers for vision transformer (ViT), which is called BViT. The broad attention is implemented by broad connection and parameter-free attention. Broad connection of each transformer layer promotes the transmission and integration of information for BViT. Without introducing additional trainable parameters, parameter-free attention jointly focuses on the already available attention information in different layers for extracting useful information and building their relationship. Experiments on image classification tasks demonstrate that BViT delivers superior accuracy of 75.0%/81.6% top-1 accuracy on ImageNet with 5M/22M parameters. Moreover, we transfer BViT to downstream object recognition benchmarks to achieve 98.9% and 89.9% on CIFAR10 and CIFAR100, respectively, that exceed ViT with fewer parameters. For the generalization test, the broad attention in Swin Transformer, T2T-ViT and LVT also brings an improvement of more than 1%. To sum up, broad attention is promising to promote the performance of attention-based models. Code and pretrained models are available at https://github.com/DRL/BViT.",
        "keywords": []
      },
      "file_name": "934942934a6a785e2a80daa6421fa79971558b89.pdf"
    },
    {
      "success": true,
      "doc_id": "d79a8292e1b275ada3ac04f6b2a174bc",
      "summary": "Vision Transformers achieved outstanding performance in many computer vision tasks. Early Vision Transformers such as ViT and DeiT adopt global self-attention, which is computationally expensive when the number of patches is large. To improve efficiency, recent Vision Transformers adopt local self-attention mechanisms, where self-attention is computed within local windows. Despite the fact that window-based local self-attention significantly boosts efficiency, it fails to capture the relationships between distant but similar patches in the image plane. To overcome this limitation of image-space local attention, in this paper, we further exploit the locality of patches in the feature space. We group the patches into multiple clusters using their features, and self-attention is computed within every cluster. Such feature-space local attention effectively captures the connections between patches across different local windows but still relevant. We propose a Bilateral lOcal Attention vision Transformer (BOAT), which integrates feature-space local attention with image-space local attention. We further integrate BOAT with both Swin and CSWin models, and extensive experiments on several benchmark datasets demonstrate that our BOAT-CSWin model clearly and consistently outperforms existing state-of-the-art CNN models and vision Transformers.",
      "intriguing_abstract": "Vision Transformers achieved outstanding performance in many computer vision tasks. Early Vision Transformers such as ViT and DeiT adopt global self-attention, which is computationally expensive when the number of patches is large. To improve efficiency, recent Vision Transformers adopt local self-attention mechanisms, where self-attention is computed within local windows. Despite the fact that window-based local self-attention significantly boosts efficiency, it fails to capture the relationships between distant but similar patches in the image plane. To overcome this limitation of image-space local attention, in this paper, we further exploit the locality of patches in the feature space. We group the patches into multiple clusters using their features, and self-attention is computed within every cluster. Such feature-space local attention effectively captures the connections between patches across different local windows but still relevant. We propose a Bilateral lOcal Attention vision Transformer (BOAT), which integrates feature-space local attention with image-space local attention. We further integrate BOAT with both Swin and CSWin models, and extensive experiments on several benchmark datasets demonstrate that our BOAT-CSWin model clearly and consistently outperforms existing state-of-the-art CNN models and vision Transformers.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/3ae8c0b646ddce95ffd09da31c02ed6fdb744e90.pdf",
      "citation_key": "yu20220np",
      "metadata": {
        "title": "BOAT: Bilateral Local Attention Vision Transformer",
        "authors": [
          "Tan Yu",
          "Gangming Zhao",
          "Ping Li",
          "Yizhou Yu"
        ],
        "published_date": "2022",
        "abstract": "Vision Transformers achieved outstanding performance in many computer vision tasks. Early Vision Transformers such as ViT and DeiT adopt global self-attention, which is computationally expensive when the number of patches is large. To improve efficiency, recent Vision Transformers adopt local self-attention mechanisms, where self-attention is computed within local windows. Despite the fact that window-based local self-attention significantly boosts efficiency, it fails to capture the relationships between distant but similar patches in the image plane. To overcome this limitation of image-space local attention, in this paper, we further exploit the locality of patches in the feature space. We group the patches into multiple clusters using their features, and self-attention is computed within every cluster. Such feature-space local attention effectively captures the connections between patches across different local windows but still relevant. We propose a Bilateral lOcal Attention vision Transformer (BOAT), which integrates feature-space local attention with image-space local attention. We further integrate BOAT with both Swin and CSWin models, and extensive experiments on several benchmark datasets demonstrate that our BOAT-CSWin model clearly and consistently outperforms existing state-of-the-art CNN models and vision Transformers.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3ae8c0b646ddce95ffd09da31c02ed6fdb744e90.pdf",
        "venue": "British Machine Vision Conference",
        "citationCount": 27,
        "score": 9.0,
        "summary": "Vision Transformers achieved outstanding performance in many computer vision tasks. Early Vision Transformers such as ViT and DeiT adopt global self-attention, which is computationally expensive when the number of patches is large. To improve efficiency, recent Vision Transformers adopt local self-attention mechanisms, where self-attention is computed within local windows. Despite the fact that window-based local self-attention significantly boosts efficiency, it fails to capture the relationships between distant but similar patches in the image plane. To overcome this limitation of image-space local attention, in this paper, we further exploit the locality of patches in the feature space. We group the patches into multiple clusters using their features, and self-attention is computed within every cluster. Such feature-space local attention effectively captures the connections between patches across different local windows but still relevant. We propose a Bilateral lOcal Attention vision Transformer (BOAT), which integrates feature-space local attention with image-space local attention. We further integrate BOAT with both Swin and CSWin models, and extensive experiments on several benchmark datasets demonstrate that our BOAT-CSWin model clearly and consistently outperforms existing state-of-the-art CNN models and vision Transformers.",
        "keywords": []
      },
      "file_name": "3ae8c0b646ddce95ffd09da31c02ed6fdb744e90.pdf"
    },
    {
      "success": true,
      "doc_id": "1cbf13ac221c945a8d0609aeac22d088",
      "summary": "Multi-grained features extracted from convolutional neural networks (CNNs) have demonstrated their strong dis-crimination ability in supervised person re-identification (Re-ID) tasks. Inspired by them, this work investigates the way of extracting multi-grained features from a pure transformer network to address the unsupervised Re-ID problem that is label-free but much more challenging. To this end, we build a dual-branch network architecture based upon a modified Vision Transformer (ViT). The local tokens output in each branch are reshaped and then uniformly partitioned into multiple stripes to generate part-level features, while the global tokens of two branches are averaged to produce a global feature. Further, based upon offline-online associated camera-aware proxies (02CAP) that is a top-performing unsupervised Re-ID method, we define offline and online contrastive learning losses with respect to both global and part-level features to conduct unsupervised learning. Extensive experiments on three person Re-ID datasets show that the proposed method outperforms state-of-the-art unsupervised methods by a considerable margin, greatly mitigating the gap to supervised counterparts. Code will be available soon at https://github.com/RikoLi/WACV23-workshop-TMGF.",
      "intriguing_abstract": "Multi-grained features extracted from convolutional neural networks (CNNs) have demonstrated their strong dis-crimination ability in supervised person re-identification (Re-ID) tasks. Inspired by them, this work investigates the way of extracting multi-grained features from a pure transformer network to address the unsupervised Re-ID problem that is label-free but much more challenging. To this end, we build a dual-branch network architecture based upon a modified Vision Transformer (ViT). The local tokens output in each branch are reshaped and then uniformly partitioned into multiple stripes to generate part-level features, while the global tokens of two branches are averaged to produce a global feature. Further, based upon offline-online associated camera-aware proxies (02CAP) that is a top-performing unsupervised Re-ID method, we define offline and online contrastive learning losses with respect to both global and part-level features to conduct unsupervised learning. Extensive experiments on three person Re-ID datasets show that the proposed method outperforms state-of-the-art unsupervised methods by a considerable margin, greatly mitigating the gap to supervised counterparts. Code will be available soon at https://github.com/RikoLi/WACV23-workshop-TMGF.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/bc8d9f11ad91d54e83ad7cc3900502a913499fcb.pdf",
      "citation_key": "li20229fn",
      "metadata": {
        "title": "Transformer Based Multi-Grained Features for Unsupervised Person Re-Identification",
        "authors": [
          "Jiacheng Li",
          "Menglin Wang",
          "Xiaojin Gong"
        ],
        "published_date": "2022",
        "abstract": "Multi-grained features extracted from convolutional neural networks (CNNs) have demonstrated their strong dis-crimination ability in supervised person re-identification (Re-ID) tasks. Inspired by them, this work investigates the way of extracting multi-grained features from a pure transformer network to address the unsupervised Re-ID problem that is label-free but much more challenging. To this end, we build a dual-branch network architecture based upon a modified Vision Transformer (ViT). The local tokens output in each branch are reshaped and then uniformly partitioned into multiple stripes to generate part-level features, while the global tokens of two branches are averaged to produce a global feature. Further, based upon offline-online associated camera-aware proxies (02CAP) that is a top-performing unsupervised Re-ID method, we define offline and online contrastive learning losses with respect to both global and part-level features to conduct unsupervised learning. Extensive experiments on three person Re-ID datasets show that the proposed method outperforms state-of-the-art unsupervised methods by a considerable margin, greatly mitigating the gap to supervised counterparts. Code will be available soon at https://github.com/RikoLi/WACV23-workshop-TMGF.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/bc8d9f11ad91d54e83ad7cc3900502a913499fcb.pdf",
        "venue": "2023 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)",
        "citationCount": 27,
        "score": 9.0,
        "summary": "Multi-grained features extracted from convolutional neural networks (CNNs) have demonstrated their strong dis-crimination ability in supervised person re-identification (Re-ID) tasks. Inspired by them, this work investigates the way of extracting multi-grained features from a pure transformer network to address the unsupervised Re-ID problem that is label-free but much more challenging. To this end, we build a dual-branch network architecture based upon a modified Vision Transformer (ViT). The local tokens output in each branch are reshaped and then uniformly partitioned into multiple stripes to generate part-level features, while the global tokens of two branches are averaged to produce a global feature. Further, based upon offline-online associated camera-aware proxies (02CAP) that is a top-performing unsupervised Re-ID method, we define offline and online contrastive learning losses with respect to both global and part-level features to conduct unsupervised learning. Extensive experiments on three person Re-ID datasets show that the proposed method outperforms state-of-the-art unsupervised methods by a considerable margin, greatly mitigating the gap to supervised counterparts. Code will be available soon at https://github.com/RikoLi/WACV23-workshop-TMGF.",
        "keywords": []
      },
      "file_name": "bc8d9f11ad91d54e83ad7cc3900502a913499fcb.pdf"
    },
    {
      "success": true,
      "doc_id": "f10a37c93f354e1b16ba2e24c8741396",
      "summary": ". Fast MRI aims to reconstruct a high delity image from partially observed measurements. Exuberant development in fast MRI using deep learning has been witnessed recently. Meanwhile, novel deep learning paradigms, e.g., Transformer based models, are fast-growing in natural language processing and promptly developed for computer vision and medical image analysis due to their prominent performance. Nevertheless, due to the complexity of the Transformer, the application of fast MRI may not be straightforward. The main obstacle is the computational cost of the self-attention layer, which is the core part of the Transformer, can be expensive for high resolution MRI inputs. In this study, we propose a new Transformer architecture for solving fast MRI that coupled Shifted Windows Transformer with U-Net to reduce the network complexity. We incorporate deformable attention to construe the explainability of our reconstruction model. We empirically demonstrate that our method achieves consistently superior performance on the fast MRI task. Besides, compared to state-of-the-art Transformer models, our method has fewer network parameters while revealing explainability. The code is publicly available at https://github.com/ayanglab/SDAUT.",
      "intriguing_abstract": ". Fast MRI aims to reconstruct a high delity image from partially observed measurements. Exuberant development in fast MRI using deep learning has been witnessed recently. Meanwhile, novel deep learning paradigms, e.g., Transformer based models, are fast-growing in natural language processing and promptly developed for computer vision and medical image analysis due to their prominent performance. Nevertheless, due to the complexity of the Transformer, the application of fast MRI may not be straightforward. The main obstacle is the computational cost of the self-attention layer, which is the core part of the Transformer, can be expensive for high resolution MRI inputs. In this study, we propose a new Transformer architecture for solving fast MRI that coupled Shifted Windows Transformer with U-Net to reduce the network complexity. We incorporate deformable attention to construe the explainability of our reconstruction model. We empirically demonstrate that our method achieves consistently superior performance on the fast MRI task. Besides, compared to state-of-the-art Transformer models, our method has fewer network parameters while revealing explainability. The code is publicly available at https://github.com/ayanglab/SDAUT.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/cf6d947d5d2ee72873a5a7b97dde9f881f79a6b6.pdf",
      "citation_key": "huang2022iwe",
      "metadata": {
        "title": "Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI",
        "authors": [
          "Jiahao Huang",
          "Xiaodan Xing",
          "Zhifan Gao",
          "Guang Yang"
        ],
        "published_date": "2022",
        "abstract": ". Fast MRI aims to reconstruct a high delity image from partially observed measurements. Exuberant development in fast MRI using deep learning has been witnessed recently. Meanwhile, novel deep learning paradigms, e.g., Transformer based models, are fast-growing in natural language processing and promptly developed for computer vision and medical image analysis due to their prominent performance. Nevertheless, due to the complexity of the Transformer, the application of fast MRI may not be straightforward. The main obstacle is the computational cost of the self-attention layer, which is the core part of the Transformer, can be expensive for high resolution MRI inputs. In this study, we propose a new Transformer architecture for solving fast MRI that coupled Shifted Windows Transformer with U-Net to reduce the network complexity. We incorporate deformable attention to construe the explainability of our reconstruction model. We empirically demonstrate that our method achieves consistently superior performance on the fast MRI task. Besides, compared to state-of-the-art Transformer models, our method has fewer network parameters while revealing explainability. The code is publicly available at https://github.com/ayanglab/SDAUT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/cf6d947d5d2ee72873a5a7b97dde9f881f79a6b6.pdf",
        "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
        "citationCount": 27,
        "score": 9.0,
        "summary": ". Fast MRI aims to reconstruct a high delity image from partially observed measurements. Exuberant development in fast MRI using deep learning has been witnessed recently. Meanwhile, novel deep learning paradigms, e.g., Transformer based models, are fast-growing in natural language processing and promptly developed for computer vision and medical image analysis due to their prominent performance. Nevertheless, due to the complexity of the Transformer, the application of fast MRI may not be straightforward. The main obstacle is the computational cost of the self-attention layer, which is the core part of the Transformer, can be expensive for high resolution MRI inputs. In this study, we propose a new Transformer architecture for solving fast MRI that coupled Shifted Windows Transformer with U-Net to reduce the network complexity. We incorporate deformable attention to construe the explainability of our reconstruction model. We empirically demonstrate that our method achieves consistently superior performance on the fast MRI task. Besides, compared to state-of-the-art Transformer models, our method has fewer network parameters while revealing explainability. The code is publicly available at https://github.com/ayanglab/SDAUT.",
        "keywords": []
      },
      "file_name": "cf6d947d5d2ee72873a5a7b97dde9f881f79a6b6.pdf"
    },
    {
      "success": true,
      "doc_id": "d6b790ab8219d16648785916d715f9dc",
      "summary": "In this paper, we investigate how to achieve better visual grounding with modern vision-language transformers, and propose a simple yet powerful Selective Retraining (SiRi) mechanism for this challenging task. Particularly, SiRi conveys a significant principle to the research of visual grounding, i.e., a better initialized vision-language encoder would help the model converge to a better local minimum, advancing the performance accordingly. In specific, we continually update the parameters of the encoder as the training goes on, while periodically re-initialize rest of the parameters to compel the model to be better optimized based on an enhanced encoder. SiRi can significantly outperform previous approaches on three popular benchmarks. Specifically, our method achieves 83.04% Top1 accuracy on RefCOCO+ testA, outperforming the state-of-the-art approaches (training from scratch) by more than 10.21%. Additionally, we reveal that SiRi performs surprisingly superior even with limited training data. We also extend it to transformer-based visual grounding models and other vision-language tasks to verify the validity.",
      "intriguing_abstract": "In this paper, we investigate how to achieve better visual grounding with modern vision-language transformers, and propose a simple yet powerful Selective Retraining (SiRi) mechanism for this challenging task. Particularly, SiRi conveys a significant principle to the research of visual grounding, i.e., a better initialized vision-language encoder would help the model converge to a better local minimum, advancing the performance accordingly. In specific, we continually update the parameters of the encoder as the training goes on, while periodically re-initialize rest of the parameters to compel the model to be better optimized based on an enhanced encoder. SiRi can significantly outperform previous approaches on three popular benchmarks. Specifically, our method achieves 83.04% Top1 accuracy on RefCOCO+ testA, outperforming the state-of-the-art approaches (training from scratch) by more than 10.21%. Additionally, we reveal that SiRi performs surprisingly superior even with limited training data. We also extend it to transformer-based visual grounding models and other vision-language tasks to verify the validity.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/9db52b92b48b8855f76d7e4b51b7727a2b3c0271.pdf",
      "citation_key": "qu2022be0",
      "metadata": {
        "title": "SiRi: A Simple Selective Retraining Mechanism for Transformer-based Visual Grounding",
        "authors": [
          "Mengxue Qu",
          "Yu Wu",
          "Wu Liu",
          "Qiqi Gong",
          "Xiaodan Liang",
          "Olga Russakovsky",
          "Yao Zhao",
          "Yunchao Wei"
        ],
        "published_date": "2022",
        "abstract": "In this paper, we investigate how to achieve better visual grounding with modern vision-language transformers, and propose a simple yet powerful Selective Retraining (SiRi) mechanism for this challenging task. Particularly, SiRi conveys a significant principle to the research of visual grounding, i.e., a better initialized vision-language encoder would help the model converge to a better local minimum, advancing the performance accordingly. In specific, we continually update the parameters of the encoder as the training goes on, while periodically re-initialize rest of the parameters to compel the model to be better optimized based on an enhanced encoder. SiRi can significantly outperform previous approaches on three popular benchmarks. Specifically, our method achieves 83.04% Top1 accuracy on RefCOCO+ testA, outperforming the state-of-the-art approaches (training from scratch) by more than 10.21%. Additionally, we reveal that SiRi performs surprisingly superior even with limited training data. We also extend it to transformer-based visual grounding models and other vision-language tasks to verify the validity.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9db52b92b48b8855f76d7e4b51b7727a2b3c0271.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 26,
        "score": 8.666666666666666,
        "summary": "In this paper, we investigate how to achieve better visual grounding with modern vision-language transformers, and propose a simple yet powerful Selective Retraining (SiRi) mechanism for this challenging task. Particularly, SiRi conveys a significant principle to the research of visual grounding, i.e., a better initialized vision-language encoder would help the model converge to a better local minimum, advancing the performance accordingly. In specific, we continually update the parameters of the encoder as the training goes on, while periodically re-initialize rest of the parameters to compel the model to be better optimized based on an enhanced encoder. SiRi can significantly outperform previous approaches on three popular benchmarks. Specifically, our method achieves 83.04% Top1 accuracy on RefCOCO+ testA, outperforming the state-of-the-art approaches (training from scratch) by more than 10.21%. Additionally, we reveal that SiRi performs surprisingly superior even with limited training data. We also extend it to transformer-based visual grounding models and other vision-language tasks to verify the validity.",
        "keywords": []
      },
      "file_name": "9db52b92b48b8855f76d7e4b51b7727a2b3c0271.pdf"
    },
    {
      "success": true,
      "doc_id": "3c95b5543550f22bb57ba12ca73eaf6f",
      "summary": "Secure multi-party computation (MPC) enables computation directly on encrypted data and protects both data and model privacy in deep learning inference. However, existing neural network architectures, including Vision Transformers (ViTs), are not designed or optimized for MPC and incur significant latency overhead. We observe Softmax accounts for the major latency bottleneck due to a high communication complexity, but can be selectively replaced or linearized without compromising the model accuracy. Hence, in this paper, we propose an MPC-friendly ViT, dubbed MPCViT, to enable accurate yet efficient ViT inference in MPC. Based on a systematic latency and accuracy evaluation of the Softmax attention and other attention variants, we propose a heterogeneous attention optimization space. We also develop a simple yet effective MPC-aware neural architecture search algorithm for fast Pareto optimization. To further boost the inference efficiency, we propose MPCViT+, to jointly optimize the Softmax attention and other network components, including GeLU, matrix multiplication, etc. With extensive experiments, we demonstrate that MPCViT achieves 1.9%, 1.3% and 3.6% higher accuracy with 6.2, 2.9 and 1.9 latency reduction compared with baseline ViT, MPCFormer and THE-X on the Tiny-ImageNet dataset, respectively. MPCViT+ further achieves a better Pareto front compared with MPCViT. The code and models for evaluation are available at https://github.com/PKU-SEC-Lab/mpcvit.",
      "intriguing_abstract": "Secure multi-party computation (MPC) enables computation directly on encrypted data and protects both data and model privacy in deep learning inference. However, existing neural network architectures, including Vision Transformers (ViTs), are not designed or optimized for MPC and incur significant latency overhead. We observe Softmax accounts for the major latency bottleneck due to a high communication complexity, but can be selectively replaced or linearized without compromising the model accuracy. Hence, in this paper, we propose an MPC-friendly ViT, dubbed MPCViT, to enable accurate yet efficient ViT inference in MPC. Based on a systematic latency and accuracy evaluation of the Softmax attention and other attention variants, we propose a heterogeneous attention optimization space. We also develop a simple yet effective MPC-aware neural architecture search algorithm for fast Pareto optimization. To further boost the inference efficiency, we propose MPCViT+, to jointly optimize the Softmax attention and other network components, including GeLU, matrix multiplication, etc. With extensive experiments, we demonstrate that MPCViT achieves 1.9%, 1.3% and 3.6% higher accuracy with 6.2, 2.9 and 1.9 latency reduction compared with baseline ViT, MPCFormer and THE-X on the Tiny-ImageNet dataset, respectively. MPCViT+ further achieves a better Pareto front compared with MPCViT. The code and models for evaluation are available at https://github.com/PKU-SEC-Lab/mpcvit.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/e4add4391dfa2a806a50cc1fbe9a9696dac9501f.pdf",
      "citation_key": "zeng2022ce2",
      "metadata": {
        "title": "MPCViT: Searching for Accurate and Efficient MPC-Friendly Vision Transformer with Heterogeneous Attention",
        "authors": [
          "Wenyuan Zeng",
          "Meng Li",
          "Wenjie Xiong",
          "Tong Tong",
          "Wen-jie Lu",
          "Jin Tan",
          "Runsheng Wang",
          "Ru Huang"
        ],
        "published_date": "2022",
        "abstract": "Secure multi-party computation (MPC) enables computation directly on encrypted data and protects both data and model privacy in deep learning inference. However, existing neural network architectures, including Vision Transformers (ViTs), are not designed or optimized for MPC and incur significant latency overhead. We observe Softmax accounts for the major latency bottleneck due to a high communication complexity, but can be selectively replaced or linearized without compromising the model accuracy. Hence, in this paper, we propose an MPC-friendly ViT, dubbed MPCViT, to enable accurate yet efficient ViT inference in MPC. Based on a systematic latency and accuracy evaluation of the Softmax attention and other attention variants, we propose a heterogeneous attention optimization space. We also develop a simple yet effective MPC-aware neural architecture search algorithm for fast Pareto optimization. To further boost the inference efficiency, we propose MPCViT+, to jointly optimize the Softmax attention and other network components, including GeLU, matrix multiplication, etc. With extensive experiments, we demonstrate that MPCViT achieves 1.9%, 1.3% and 3.6% higher accuracy with 6.2, 2.9 and 1.9 latency reduction compared with baseline ViT, MPCFormer and THE-X on the Tiny-ImageNet dataset, respectively. MPCViT+ further achieves a better Pareto front compared with MPCViT. The code and models for evaluation are available at https://github.com/PKU-SEC-Lab/mpcvit.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e4add4391dfa2a806a50cc1fbe9a9696dac9501f.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 26,
        "score": 8.666666666666666,
        "summary": "Secure multi-party computation (MPC) enables computation directly on encrypted data and protects both data and model privacy in deep learning inference. However, existing neural network architectures, including Vision Transformers (ViTs), are not designed or optimized for MPC and incur significant latency overhead. We observe Softmax accounts for the major latency bottleneck due to a high communication complexity, but can be selectively replaced or linearized without compromising the model accuracy. Hence, in this paper, we propose an MPC-friendly ViT, dubbed MPCViT, to enable accurate yet efficient ViT inference in MPC. Based on a systematic latency and accuracy evaluation of the Softmax attention and other attention variants, we propose a heterogeneous attention optimization space. We also develop a simple yet effective MPC-aware neural architecture search algorithm for fast Pareto optimization. To further boost the inference efficiency, we propose MPCViT+, to jointly optimize the Softmax attention and other network components, including GeLU, matrix multiplication, etc. With extensive experiments, we demonstrate that MPCViT achieves 1.9%, 1.3% and 3.6% higher accuracy with 6.2, 2.9 and 1.9 latency reduction compared with baseline ViT, MPCFormer and THE-X on the Tiny-ImageNet dataset, respectively. MPCViT+ further achieves a better Pareto front compared with MPCViT. The code and models for evaluation are available at https://github.com/PKU-SEC-Lab/mpcvit.",
        "keywords": []
      },
      "file_name": "e4add4391dfa2a806a50cc1fbe9a9696dac9501f.pdf"
    },
    {
      "success": true,
      "doc_id": "ad49c03a38cfe5f1418343958f577878",
      "summary": "Forest fires seriously destroy the worlds forest resources and endanger biodiversity. The traditional forest fire target detection models based on convolutional neural networks (CNNs) lack the ability to deal with the relationship between visual elements and objects. They also have low detection accuracy for small-target forest fires. Therefore, this paper proposes an improved small-target forest fire detection model, STPM_SAHI. We use the latest technology in the field of computer vision, the Swin Transformer backbone network, to extract the features of forest fires. Its self-attention mechanism can capture the global information of forest fires to obtain larger receptive fields and contextual information. We integrated the Swin Transformer backbone network into the Mask R-CNN detection framework, and PAFPN was used to replace the original FPN as the feature fusion network, which can reduce the propagation path of the main feature layer and eliminate the impact of down-sampling fusion. After the improved model was trained, the average precision (AP0.5) of forest fire target detection at different scales reached 89.4. Then, Slicing Aided Hyper Inference technology was integrated into the improved forest fire detection model, which solved the problem that small-target forest fires pixels only account for a small proportion and lack sufficient details, which are difficult to be detected by the traditional target detection models. The detection accuracy of small-target forest fires was significantly improved. The average precision (AP0.5) increased by 8.1. Through an ablation experiment, we have proved the effectiveness of each module of the improved forest fire detection model. Furthermore, the forest fire detection accuracy is significantly better than that of the mainstream models. Our model can also detect forest fire targets with very small pixels. Our model is very suitable for small-target forest fire detection. The detection accuracy of forest fire targets at different scales is also very high and meets the needs of real-time forest fire detection.",
      "intriguing_abstract": "Forest fires seriously destroy the worlds forest resources and endanger biodiversity. The traditional forest fire target detection models based on convolutional neural networks (CNNs) lack the ability to deal with the relationship between visual elements and objects. They also have low detection accuracy for small-target forest fires. Therefore, this paper proposes an improved small-target forest fire detection model, STPM_SAHI. We use the latest technology in the field of computer vision, the Swin Transformer backbone network, to extract the features of forest fires. Its self-attention mechanism can capture the global information of forest fires to obtain larger receptive fields and contextual information. We integrated the Swin Transformer backbone network into the Mask R-CNN detection framework, and PAFPN was used to replace the original FPN as the feature fusion network, which can reduce the propagation path of the main feature layer and eliminate the impact of down-sampling fusion. After the improved model was trained, the average precision (AP0.5) of forest fire target detection at different scales reached 89.4. Then, Slicing Aided Hyper Inference technology was integrated into the improved forest fire detection model, which solved the problem that small-target forest fires pixels only account for a small proportion and lack sufficient details, which are difficult to be detected by the traditional target detection models. The detection accuracy of small-target forest fires was significantly improved. The average precision (AP0.5) increased by 8.1. Through an ablation experiment, we have proved the effectiveness of each module of the improved forest fire detection model. Furthermore, the forest fire detection accuracy is significantly better than that of the mainstream models. Our model can also detect forest fire targets with very small pixels. Our model is very suitable for small-target forest fire detection. The detection accuracy of forest fire targets at different scales is also very high and meets the needs of real-time forest fire detection.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/324f97d033efd97855488cf0b15511799fe7b7f7.pdf",
      "citation_key": "lin20225ad",
      "metadata": {
        "title": "STPM_SAHI: A Small-Target Forest Fire Detection Model Based on Swin Transformer and Slicing Aided Hyper Inference",
        "authors": [
          "Ji Lin",
          "Haifeng Lin",
          "Fang Wang"
        ],
        "published_date": "2022",
        "abstract": "Forest fires seriously destroy the worlds forest resources and endanger biodiversity. The traditional forest fire target detection models based on convolutional neural networks (CNNs) lack the ability to deal with the relationship between visual elements and objects. They also have low detection accuracy for small-target forest fires. Therefore, this paper proposes an improved small-target forest fire detection model, STPM_SAHI. We use the latest technology in the field of computer vision, the Swin Transformer backbone network, to extract the features of forest fires. Its self-attention mechanism can capture the global information of forest fires to obtain larger receptive fields and contextual information. We integrated the Swin Transformer backbone network into the Mask R-CNN detection framework, and PAFPN was used to replace the original FPN as the feature fusion network, which can reduce the propagation path of the main feature layer and eliminate the impact of down-sampling fusion. After the improved model was trained, the average precision (AP0.5) of forest fire target detection at different scales reached 89.4. Then, Slicing Aided Hyper Inference technology was integrated into the improved forest fire detection model, which solved the problem that small-target forest fires pixels only account for a small proportion and lack sufficient details, which are difficult to be detected by the traditional target detection models. The detection accuracy of small-target forest fires was significantly improved. The average precision (AP0.5) increased by 8.1. Through an ablation experiment, we have proved the effectiveness of each module of the improved forest fire detection model. Furthermore, the forest fire detection accuracy is significantly better than that of the mainstream models. Our model can also detect forest fire targets with very small pixels. Our model is very suitable for small-target forest fire detection. The detection accuracy of forest fire targets at different scales is also very high and meets the needs of real-time forest fire detection.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/324f97d033efd97855488cf0b15511799fe7b7f7.pdf",
        "venue": "Forests",
        "citationCount": 25,
        "score": 8.333333333333332,
        "summary": "Forest fires seriously destroy the worlds forest resources and endanger biodiversity. The traditional forest fire target detection models based on convolutional neural networks (CNNs) lack the ability to deal with the relationship between visual elements and objects. They also have low detection accuracy for small-target forest fires. Therefore, this paper proposes an improved small-target forest fire detection model, STPM_SAHI. We use the latest technology in the field of computer vision, the Swin Transformer backbone network, to extract the features of forest fires. Its self-attention mechanism can capture the global information of forest fires to obtain larger receptive fields and contextual information. We integrated the Swin Transformer backbone network into the Mask R-CNN detection framework, and PAFPN was used to replace the original FPN as the feature fusion network, which can reduce the propagation path of the main feature layer and eliminate the impact of down-sampling fusion. After the improved model was trained, the average precision (AP0.5) of forest fire target detection at different scales reached 89.4. Then, Slicing Aided Hyper Inference technology was integrated into the improved forest fire detection model, which solved the problem that small-target forest fires pixels only account for a small proportion and lack sufficient details, which are difficult to be detected by the traditional target detection models. The detection accuracy of small-target forest fires was significantly improved. The average precision (AP0.5) increased by 8.1. Through an ablation experiment, we have proved the effectiveness of each module of the improved forest fire detection model. Furthermore, the forest fire detection accuracy is significantly better than that of the mainstream models. Our model can also detect forest fire targets with very small pixels. Our model is very suitable for small-target forest fire detection. The detection accuracy of forest fire targets at different scales is also very high and meets the needs of real-time forest fire detection.",
        "keywords": []
      },
      "file_name": "324f97d033efd97855488cf0b15511799fe7b7f7.pdf"
    },
    {
      "success": true,
      "doc_id": "7d21bc66fa48355f0ca08de0e0037459",
      "summary": "Multiple predominant instrument recognition in polyphonic music is addressed using decision level fusion of three transformer-based architectures on an ensemble of visual representations. The ensemble consists of Mel-spectrogram, modgdgram, and tempogram. Predominant instrument recognition refers to the problem where the prominent instrument is identified from a mixture of instruments being played together. We experimented with two transformer architectures like Vision transformer (Vi-T) and Shifted window transformer (Swin-T) for the proposed task. The performance of the proposed system is compared with that of the state-of-the-art Hans model, convolutional neural networks (CNN), and deep neural networks (DNN). Transformer networks learn the distinctive local characteristics from the visual representations and classify the instrument to the group where it belongs. The proposed system is systematically evaluated using the IRMAS dataset with eleven classes. A wave generative adversarial network (WaveGAN) architecture is also employed to generate audio files for data augmentation. We train our networks from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from the variable-length test audio file without any sliding window analysis and aggregation strategy as in existing algorithms. The ensemble voting scheme using Swin-T reports a micro and macro F1 score of 0.66 and 0.62, respectively. These metrics are 3.12% and 12.72% relatively higher than those obtained by the state-of-the-art Hans model. The architectural choice of transformers with ensemble voting on Mel-spectro-/modgd-/tempogram has merit in recognizing the predominant instruments in polyphonic music.",
      "intriguing_abstract": "Multiple predominant instrument recognition in polyphonic music is addressed using decision level fusion of three transformer-based architectures on an ensemble of visual representations. The ensemble consists of Mel-spectrogram, modgdgram, and tempogram. Predominant instrument recognition refers to the problem where the prominent instrument is identified from a mixture of instruments being played together. We experimented with two transformer architectures like Vision transformer (Vi-T) and Shifted window transformer (Swin-T) for the proposed task. The performance of the proposed system is compared with that of the state-of-the-art Hans model, convolutional neural networks (CNN), and deep neural networks (DNN). Transformer networks learn the distinctive local characteristics from the visual representations and classify the instrument to the group where it belongs. The proposed system is systematically evaluated using the IRMAS dataset with eleven classes. A wave generative adversarial network (WaveGAN) architecture is also employed to generate audio files for data augmentation. We train our networks from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from the variable-length test audio file without any sliding window analysis and aggregation strategy as in existing algorithms. The ensemble voting scheme using Swin-T reports a micro and macro F1 score of 0.66 and 0.62, respectively. These metrics are 3.12% and 12.72% relatively higher than those obtained by the state-of-the-art Hans model. The architectural choice of transformers with ensemble voting on Mel-spectro-/modgd-/tempogram has merit in recognizing the predominant instruments in polyphonic music.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/bf52f09d648d78677f140e63c7c83ca44fcd438f.pdf",
      "citation_key": "reghunath2022z8g",
      "metadata": {
        "title": "Transformer-based ensemble method for multiple predominant instruments recognition in polyphonic music",
        "authors": [
          "L. Reghunath",
          "R. Rajan"
        ],
        "published_date": "2022",
        "abstract": "Multiple predominant instrument recognition in polyphonic music is addressed using decision level fusion of three transformer-based architectures on an ensemble of visual representations. The ensemble consists of Mel-spectrogram, modgdgram, and tempogram. Predominant instrument recognition refers to the problem where the prominent instrument is identified from a mixture of instruments being played together. We experimented with two transformer architectures like Vision transformer (Vi-T) and Shifted window transformer (Swin-T) for the proposed task. The performance of the proposed system is compared with that of the state-of-the-art Hans model, convolutional neural networks (CNN), and deep neural networks (DNN). Transformer networks learn the distinctive local characteristics from the visual representations and classify the instrument to the group where it belongs. The proposed system is systematically evaluated using the IRMAS dataset with eleven classes. A wave generative adversarial network (WaveGAN) architecture is also employed to generate audio files for data augmentation. We train our networks from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from the variable-length test audio file without any sliding window analysis and aggregation strategy as in existing algorithms. The ensemble voting scheme using Swin-T reports a micro and macro F1 score of 0.66 and 0.62, respectively. These metrics are 3.12% and 12.72% relatively higher than those obtained by the state-of-the-art Hans model. The architectural choice of transformers with ensemble voting on Mel-spectro-/modgd-/tempogram has merit in recognizing the predominant instruments in polyphonic music.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/bf52f09d648d78677f140e63c7c83ca44fcd438f.pdf",
        "venue": "EURASIP Journal on Audio, Speech, and Music Processing",
        "citationCount": 25,
        "score": 8.333333333333332,
        "summary": "Multiple predominant instrument recognition in polyphonic music is addressed using decision level fusion of three transformer-based architectures on an ensemble of visual representations. The ensemble consists of Mel-spectrogram, modgdgram, and tempogram. Predominant instrument recognition refers to the problem where the prominent instrument is identified from a mixture of instruments being played together. We experimented with two transformer architectures like Vision transformer (Vi-T) and Shifted window transformer (Swin-T) for the proposed task. The performance of the proposed system is compared with that of the state-of-the-art Hans model, convolutional neural networks (CNN), and deep neural networks (DNN). Transformer networks learn the distinctive local characteristics from the visual representations and classify the instrument to the group where it belongs. The proposed system is systematically evaluated using the IRMAS dataset with eleven classes. A wave generative adversarial network (WaveGAN) architecture is also employed to generate audio files for data augmentation. We train our networks from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from the variable-length test audio file without any sliding window analysis and aggregation strategy as in existing algorithms. The ensemble voting scheme using Swin-T reports a micro and macro F1 score of 0.66 and 0.62, respectively. These metrics are 3.12% and 12.72% relatively higher than those obtained by the state-of-the-art Hans model. The architectural choice of transformers with ensemble voting on Mel-spectro-/modgd-/tempogram has merit in recognizing the predominant instruments in polyphonic music.",
        "keywords": []
      },
      "file_name": "bf52f09d648d78677f140e63c7c83ca44fcd438f.pdf"
    },
    {
      "success": true,
      "doc_id": "c05f0549abb6b4774e16cb2644626f52",
      "summary": "Images of skin lesions may be used to detect this virus, which is a reliable method for identifying the pox virus group. However, early identification and prediction are difficult due to the viruss resemblance to other pox viruses. An intelligent computer-aided detection system may be a great alternative to relying on labor-intensive human identification. Therefore, in this research an machine learning and deep learning classification method for monkeypox prediction has been proposed and trained, and tested over 1300 skin lesion images. A comparative analysis of machine learning algorithms (K-NN and SVM) and Deep learning algorithms (Vision Transformer, RestNet50) to establish the efficacy of this study. Layered Convolutional Neural Network (CNN) with transfer learning and pretrained models such as RestNet50 integrated, together with customized hyperparameters for extracting the features from the input images. The feed-forward, which is also completely integrated, helped the algorithm divide the visuals into two categorieschickenpox and monkeypox. Among the ML model, the K-NN achieves the best accuracy of 84%. However, The Vision Transformer(ViT) outperforms the other models with an accuracy of 93%. In Addition to it, we analyze our pretrained model to achieve the desired outcome based on the relevant existing model as already established to the end user.",
      "intriguing_abstract": "Images of skin lesions may be used to detect this virus, which is a reliable method for identifying the pox virus group. However, early identification and prediction are difficult due to the viruss resemblance to other pox viruses. An intelligent computer-aided detection system may be a great alternative to relying on labor-intensive human identification. Therefore, in this research an machine learning and deep learning classification method for monkeypox prediction has been proposed and trained, and tested over 1300 skin lesion images. A comparative analysis of machine learning algorithms (K-NN and SVM) and Deep learning algorithms (Vision Transformer, RestNet50) to establish the efficacy of this study. Layered Convolutional Neural Network (CNN) with transfer learning and pretrained models such as RestNet50 integrated, together with customized hyperparameters for extracting the features from the input images. The feed-forward, which is also completely integrated, helped the algorithm divide the visuals into two categorieschickenpox and monkeypox. Among the ML model, the K-NN achieves the best accuracy of 84%. However, The Vision Transformer(ViT) outperforms the other models with an accuracy of 93%. In Addition to it, we analyze our pretrained model to achieve the desired outcome based on the relevant existing model as already established to the end user.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/4eba4dccc7d19c40dfd02eef447a57613c3644e2.pdf",
      "citation_key": "kundu2022z97",
      "metadata": {
        "title": "Vision Transformer based Deep Learning Model for Monkeypox Detection",
        "authors": [
          "Dipanjali Kundu",
          "Umme Raihan Siddiqi",
          "Md. Mahbubur Rahman"
        ],
        "published_date": "2022",
        "abstract": "Images of skin lesions may be used to detect this virus, which is a reliable method for identifying the pox virus group. However, early identification and prediction are difficult due to the viruss resemblance to other pox viruses. An intelligent computer-aided detection system may be a great alternative to relying on labor-intensive human identification. Therefore, in this research an machine learning and deep learning classification method for monkeypox prediction has been proposed and trained, and tested over 1300 skin lesion images. A comparative analysis of machine learning algorithms (K-NN and SVM) and Deep learning algorithms (Vision Transformer, RestNet50) to establish the efficacy of this study. Layered Convolutional Neural Network (CNN) with transfer learning and pretrained models such as RestNet50 integrated, together with customized hyperparameters for extracting the features from the input images. The feed-forward, which is also completely integrated, helped the algorithm divide the visuals into two categorieschickenpox and monkeypox. Among the ML model, the K-NN achieves the best accuracy of 84%. However, The Vision Transformer(ViT) outperforms the other models with an accuracy of 93%. In Addition to it, we analyze our pretrained model to achieve the desired outcome based on the relevant existing model as already established to the end user.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/4eba4dccc7d19c40dfd02eef447a57613c3644e2.pdf",
        "venue": "2022 25th International Conference on Computer and Information Technology (ICCIT)",
        "citationCount": 25,
        "score": 8.333333333333332,
        "summary": "Images of skin lesions may be used to detect this virus, which is a reliable method for identifying the pox virus group. However, early identification and prediction are difficult due to the viruss resemblance to other pox viruses. An intelligent computer-aided detection system may be a great alternative to relying on labor-intensive human identification. Therefore, in this research an machine learning and deep learning classification method for monkeypox prediction has been proposed and trained, and tested over 1300 skin lesion images. A comparative analysis of machine learning algorithms (K-NN and SVM) and Deep learning algorithms (Vision Transformer, RestNet50) to establish the efficacy of this study. Layered Convolutional Neural Network (CNN) with transfer learning and pretrained models such as RestNet50 integrated, together with customized hyperparameters for extracting the features from the input images. The feed-forward, which is also completely integrated, helped the algorithm divide the visuals into two categorieschickenpox and monkeypox. Among the ML model, the K-NN achieves the best accuracy of 84%. However, The Vision Transformer(ViT) outperforms the other models with an accuracy of 93%. In Addition to it, we analyze our pretrained model to achieve the desired outcome based on the relevant existing model as already established to the end user.",
        "keywords": []
      },
      "file_name": "4eba4dccc7d19c40dfd02eef447a57613c3644e2.pdf"
    },
    {
      "success": true,
      "doc_id": "1efa9d9b9a4d42853693aafdf5c1fe06",
      "summary": "Although conventional deep convolutional neural networks are effective for contextual semantic segmentation of objects, recent vision transformers can capture global information of an image and are better at capturing semantic associations over longer ranges. In addition, some existing saliency detection methods disregard the guidance of high-level semantic information for low-level features during decoding, and only use layer-by-layer transmission for encoding. Therefore, we propose a hierarchical decoding network based on a swin transformer to perform redgreenblue and thermal (RGB-T) salient object detection (SOD). First, a sinecosine fusion module performs multimodality intersections and exploits complementarity. As a second fusion stage, an advanced semantic information guidance module adjusts high-level semantic information and low-level detailed characteristics. Finally, a global saliency perception module fuses cross-layer information in a top-down path. Comprehensive experiments demonstrate that the proposed network outperforms 12 state-of-the-art methods on three RGB-T SOD datasets.",
      "intriguing_abstract": "Although conventional deep convolutional neural networks are effective for contextual semantic segmentation of objects, recent vision transformers can capture global information of an image and are better at capturing semantic associations over longer ranges. In addition, some existing saliency detection methods disregard the guidance of high-level semantic information for low-level features during decoding, and only use layer-by-layer transmission for encoding. Therefore, we propose a hierarchical decoding network based on a swin transformer to perform redgreenblue and thermal (RGB-T) salient object detection (SOD). First, a sinecosine fusion module performs multimodality intersections and exploits complementarity. As a second fusion stage, an advanced semantic information guidance module adjusts high-level semantic information and low-level detailed characteristics. Finally, a global saliency perception module fuses cross-layer information in a top-down path. Comprehensive experiments demonstrate that the proposed network outperforms 12 state-of-the-art methods on three RGB-T SOD datasets.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/0bdcea2f18e158bbd1723ed27d48e5211545e050.pdf",
      "citation_key": "sun2022cti",
      "metadata": {
        "title": "Hierarchical Decoding Network Based on Swin Transformer for Detecting Salient Objects in RGB-T Images",
        "authors": [
          "Fan Sun",
          "Wujie Zhou",
          "Lv Ye",
          "Lu Yu"
        ],
        "published_date": "2022",
        "abstract": "Although conventional deep convolutional neural networks are effective for contextual semantic segmentation of objects, recent vision transformers can capture global information of an image and are better at capturing semantic associations over longer ranges. In addition, some existing saliency detection methods disregard the guidance of high-level semantic information for low-level features during decoding, and only use layer-by-layer transmission for encoding. Therefore, we propose a hierarchical decoding network based on a swin transformer to perform redgreenblue and thermal (RGB-T) salient object detection (SOD). First, a sinecosine fusion module performs multimodality intersections and exploits complementarity. As a second fusion stage, an advanced semantic information guidance module adjusts high-level semantic information and low-level detailed characteristics. Finally, a global saliency perception module fuses cross-layer information in a top-down path. Comprehensive experiments demonstrate that the proposed network outperforms 12 state-of-the-art methods on three RGB-T SOD datasets.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0bdcea2f18e158bbd1723ed27d48e5211545e050.pdf",
        "venue": "IEEE Signal Processing Letters",
        "citationCount": 24,
        "score": 8.0,
        "summary": "Although conventional deep convolutional neural networks are effective for contextual semantic segmentation of objects, recent vision transformers can capture global information of an image and are better at capturing semantic associations over longer ranges. In addition, some existing saliency detection methods disregard the guidance of high-level semantic information for low-level features during decoding, and only use layer-by-layer transmission for encoding. Therefore, we propose a hierarchical decoding network based on a swin transformer to perform redgreenblue and thermal (RGB-T) salient object detection (SOD). First, a sinecosine fusion module performs multimodality intersections and exploits complementarity. As a second fusion stage, an advanced semantic information guidance module adjusts high-level semantic information and low-level detailed characteristics. Finally, a global saliency perception module fuses cross-layer information in a top-down path. Comprehensive experiments demonstrate that the proposed network outperforms 12 state-of-the-art methods on three RGB-T SOD datasets.",
        "keywords": []
      },
      "file_name": "0bdcea2f18e158bbd1723ed27d48e5211545e050.pdf"
    },
    {
      "success": true,
      "doc_id": "ea573710fcb4600b0b19c985c5397036",
      "summary": "Finger vein recognition has been widely studied due to its advantages, such as high security, convenience, and living body recognition. At present, the performance of the most advanced finger vein recognition methods largely depends on the quality of finger vein images. However, when collecting finger vein images, due to the possible deviation of finger position, ambient lighting and other factors, the quality of the captured images is often relatively low, which directly affects the performance of finger vein recognition. In this study, we proposed a new model for finger vein recognition that combined the vision transformer architecture with the capsule network (ViT-Cap). The model can explore finger vein image information based on global and local attention and selectively focus on the important finger vein feature information. First, we split-finger vein images into patches and then linearly embedded each of the patches. Second, the resulting vector sequence was fed into a transformer encoder to extract the finger vein features. Third, the feature vectors generated by the vision transformer module were fed into the capsule module for further training. We tested the proposed method on four publicly available finger vein databases. Experimental results showed that the average recognition accuracy of the algorithm based on the proposed model was above 96%, which was better than the original vision transformer, capsule network, and other advanced finger vein recognition algorithms. Moreover, the equal error rate (EER) of our model achieved state-of-the-art performance, especially reaching less than 0.3% under the test of FV-USM datasets which proved the effectiveness and reliability of the proposed model in finger vein recognition.",
      "intriguing_abstract": "Finger vein recognition has been widely studied due to its advantages, such as high security, convenience, and living body recognition. At present, the performance of the most advanced finger vein recognition methods largely depends on the quality of finger vein images. However, when collecting finger vein images, due to the possible deviation of finger position, ambient lighting and other factors, the quality of the captured images is often relatively low, which directly affects the performance of finger vein recognition. In this study, we proposed a new model for finger vein recognition that combined the vision transformer architecture with the capsule network (ViT-Cap). The model can explore finger vein image information based on global and local attention and selectively focus on the important finger vein feature information. First, we split-finger vein images into patches and then linearly embedded each of the patches. Second, the resulting vector sequence was fed into a transformer encoder to extract the finger vein features. Third, the feature vectors generated by the vision transformer module were fed into the capsule module for further training. We tested the proposed method on four publicly available finger vein databases. Experimental results showed that the average recognition accuracy of the algorithm based on the proposed model was above 96%, which was better than the original vision transformer, capsule network, and other advanced finger vein recognition algorithms. Moreover, the equal error rate (EER) of our model achieved state-of-the-art performance, especially reaching less than 0.3% under the test of FV-USM datasets which proved the effectiveness and reliability of the proposed model in finger vein recognition.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/67b7ad5f3c818f42c8359abc87f353202f76f6f2.pdf",
      "citation_key": "li2022gef",
      "metadata": {
        "title": "ViT-Cap: A Novel Vision Transformer-Based Capsule Network Model for Finger Vein Recognition",
        "authors": [
          "Yupeng Li",
          "Huimin Lu",
          "Yifan Wang",
          "Ruoran Gao",
          "Chengcheng Zhao"
        ],
        "published_date": "2022",
        "abstract": "Finger vein recognition has been widely studied due to its advantages, such as high security, convenience, and living body recognition. At present, the performance of the most advanced finger vein recognition methods largely depends on the quality of finger vein images. However, when collecting finger vein images, due to the possible deviation of finger position, ambient lighting and other factors, the quality of the captured images is often relatively low, which directly affects the performance of finger vein recognition. In this study, we proposed a new model for finger vein recognition that combined the vision transformer architecture with the capsule network (ViT-Cap). The model can explore finger vein image information based on global and local attention and selectively focus on the important finger vein feature information. First, we split-finger vein images into patches and then linearly embedded each of the patches. Second, the resulting vector sequence was fed into a transformer encoder to extract the finger vein features. Third, the feature vectors generated by the vision transformer module were fed into the capsule module for further training. We tested the proposed method on four publicly available finger vein databases. Experimental results showed that the average recognition accuracy of the algorithm based on the proposed model was above 96%, which was better than the original vision transformer, capsule network, and other advanced finger vein recognition algorithms. Moreover, the equal error rate (EER) of our model achieved state-of-the-art performance, especially reaching less than 0.3% under the test of FV-USM datasets which proved the effectiveness and reliability of the proposed model in finger vein recognition.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/67b7ad5f3c818f42c8359abc87f353202f76f6f2.pdf",
        "venue": "Applied Sciences",
        "citationCount": 23,
        "score": 7.666666666666666,
        "summary": "Finger vein recognition has been widely studied due to its advantages, such as high security, convenience, and living body recognition. At present, the performance of the most advanced finger vein recognition methods largely depends on the quality of finger vein images. However, when collecting finger vein images, due to the possible deviation of finger position, ambient lighting and other factors, the quality of the captured images is often relatively low, which directly affects the performance of finger vein recognition. In this study, we proposed a new model for finger vein recognition that combined the vision transformer architecture with the capsule network (ViT-Cap). The model can explore finger vein image information based on global and local attention and selectively focus on the important finger vein feature information. First, we split-finger vein images into patches and then linearly embedded each of the patches. Second, the resulting vector sequence was fed into a transformer encoder to extract the finger vein features. Third, the feature vectors generated by the vision transformer module were fed into the capsule module for further training. We tested the proposed method on four publicly available finger vein databases. Experimental results showed that the average recognition accuracy of the algorithm based on the proposed model was above 96%, which was better than the original vision transformer, capsule network, and other advanced finger vein recognition algorithms. Moreover, the equal error rate (EER) of our model achieved state-of-the-art performance, especially reaching less than 0.3% under the test of FV-USM datasets which proved the effectiveness and reliability of the proposed model in finger vein recognition.",
        "keywords": []
      },
      "file_name": "67b7ad5f3c818f42c8359abc87f353202f76f6f2.pdf"
    },
    {
      "success": true,
      "doc_id": "8133aaee79bebca76dfe5addcfe55bbd",
      "summary": "Many artificial intelligence models have been developed to predict clinically relevant biomarkers for colorectal cancer (CRC), including microsatellite instability (MSI). However, existing deep learning networks require large training datasets, which are often hard to obtain. In this study, based on the latest Hierarchical Vision Transformer using Shifted Windows (Swin Transformer [SwinT]), we developed an efficient workflow to predict biomarkers in CRC (MSI, hypermutation, chromosomal instability, CpG island methylator phenotype, and BRAF and TP53 mutation) that required relatively small datasets. Our SwinT workflow substantially achieved the stateoftheart (SOTA) predictive performance in an intrastudy crossvalidation experiment on the Cancer Genome Atlas colon and rectal cancer dataset (TCGACRCDX). It also demonstrated excellent generalizability in crossstudy external validation and delivered a SOTA area under the receiver operating characteristic curve (AUROC) of 0.90 for MSI, using the Molecular and Cellular Oncology dataset for training (N = 1,065) and the TCGACRCDX (N = 462) for testing. A similar performance (AUROC = 0.91) was reported in a recent study, using ~8,000 training samples (ResNet18) on the same testing dataset. SwinT was extremely efficient when using small training datasets and exhibited robust predictive performance with 200500 training samples. Our findings indicate that SwinT could be 510 times more efficient than existing algorithms for MSI prediction based on ResNet18 and ShuffleNet. Furthermore, the SwinT models demonstrated their capability in accurately predicting MSI and BRAF mutation status, which could exclude and therefore reduce samples before subsequent standard testing in a cascading diagnostic workflow, in turn reducing turnaround time and costs.",
      "intriguing_abstract": "Many artificial intelligence models have been developed to predict clinically relevant biomarkers for colorectal cancer (CRC), including microsatellite instability (MSI). However, existing deep learning networks require large training datasets, which are often hard to obtain. In this study, based on the latest Hierarchical Vision Transformer using Shifted Windows (Swin Transformer [SwinT]), we developed an efficient workflow to predict biomarkers in CRC (MSI, hypermutation, chromosomal instability, CpG island methylator phenotype, and BRAF and TP53 mutation) that required relatively small datasets. Our SwinT workflow substantially achieved the stateoftheart (SOTA) predictive performance in an intrastudy crossvalidation experiment on the Cancer Genome Atlas colon and rectal cancer dataset (TCGACRCDX). It also demonstrated excellent generalizability in crossstudy external validation and delivered a SOTA area under the receiver operating characteristic curve (AUROC) of 0.90 for MSI, using the Molecular and Cellular Oncology dataset for training (N = 1,065) and the TCGACRCDX (N = 462) for testing. A similar performance (AUROC = 0.91) was reported in a recent study, using ~8,000 training samples (ResNet18) on the same testing dataset. SwinT was extremely efficient when using small training datasets and exhibited robust predictive performance with 200500 training samples. Our findings indicate that SwinT could be 510 times more efficient than existing algorithms for MSI prediction based on ResNet18 and ShuffleNet. Furthermore, the SwinT models demonstrated their capability in accurately predicting MSI and BRAF mutation status, which could exclude and therefore reduce samples before subsequent standard testing in a cascading diagnostic workflow, in turn reducing turnaround time and costs.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/3994996a202f0127a58f57b259324a5283a1ba27.pdf",
      "citation_key": "guo20228rt",
      "metadata": {
        "title": "Predicting microsatellite instability and key biomarkers in colorectal cancer from H&Estained images: achieving stateoftheart predictive performance with fewer data using Swin Transformer",
        "authors": [
          "Bangwei Guo",
          "Xingyu Li",
          "Miao Yang",
          "J. Jonnagaddala",
          "Hong Zhang",
          "Xuesong Xu"
        ],
        "published_date": "2022",
        "abstract": "Many artificial intelligence models have been developed to predict clinically relevant biomarkers for colorectal cancer (CRC), including microsatellite instability (MSI). However, existing deep learning networks require large training datasets, which are often hard to obtain. In this study, based on the latest Hierarchical Vision Transformer using Shifted Windows (Swin Transformer [SwinT]), we developed an efficient workflow to predict biomarkers in CRC (MSI, hypermutation, chromosomal instability, CpG island methylator phenotype, and BRAF and TP53 mutation) that required relatively small datasets. Our SwinT workflow substantially achieved the stateoftheart (SOTA) predictive performance in an intrastudy crossvalidation experiment on the Cancer Genome Atlas colon and rectal cancer dataset (TCGACRCDX). It also demonstrated excellent generalizability in crossstudy external validation and delivered a SOTA area under the receiver operating characteristic curve (AUROC) of 0.90 for MSI, using the Molecular and Cellular Oncology dataset for training (N = 1,065) and the TCGACRCDX (N = 462) for testing. A similar performance (AUROC = 0.91) was reported in a recent study, using ~8,000 training samples (ResNet18) on the same testing dataset. SwinT was extremely efficient when using small training datasets and exhibited robust predictive performance with 200500 training samples. Our findings indicate that SwinT could be 510 times more efficient than existing algorithms for MSI prediction based on ResNet18 and ShuffleNet. Furthermore, the SwinT models demonstrated their capability in accurately predicting MSI and BRAF mutation status, which could exclude and therefore reduce samples before subsequent standard testing in a cascading diagnostic workflow, in turn reducing turnaround time and costs.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3994996a202f0127a58f57b259324a5283a1ba27.pdf",
        "venue": "The Journal of Pathology: Clinical Research",
        "citationCount": 23,
        "score": 7.666666666666666,
        "summary": "Many artificial intelligence models have been developed to predict clinically relevant biomarkers for colorectal cancer (CRC), including microsatellite instability (MSI). However, existing deep learning networks require large training datasets, which are often hard to obtain. In this study, based on the latest Hierarchical Vision Transformer using Shifted Windows (Swin Transformer [SwinT]), we developed an efficient workflow to predict biomarkers in CRC (MSI, hypermutation, chromosomal instability, CpG island methylator phenotype, and BRAF and TP53 mutation) that required relatively small datasets. Our SwinT workflow substantially achieved the stateoftheart (SOTA) predictive performance in an intrastudy crossvalidation experiment on the Cancer Genome Atlas colon and rectal cancer dataset (TCGACRCDX). It also demonstrated excellent generalizability in crossstudy external validation and delivered a SOTA area under the receiver operating characteristic curve (AUROC) of 0.90 for MSI, using the Molecular and Cellular Oncology dataset for training (N = 1,065) and the TCGACRCDX (N = 462) for testing. A similar performance (AUROC = 0.91) was reported in a recent study, using ~8,000 training samples (ResNet18) on the same testing dataset. SwinT was extremely efficient when using small training datasets and exhibited robust predictive performance with 200500 training samples. Our findings indicate that SwinT could be 510 times more efficient than existing algorithms for MSI prediction based on ResNet18 and ShuffleNet. Furthermore, the SwinT models demonstrated their capability in accurately predicting MSI and BRAF mutation status, which could exclude and therefore reduce samples before subsequent standard testing in a cascading diagnostic workflow, in turn reducing turnaround time and costs.",
        "keywords": []
      },
      "file_name": "3994996a202f0127a58f57b259324a5283a1ba27.pdf"
    },
    {
      "success": true,
      "doc_id": "8ff7f31d3065a0aff2a3ac6ed34b7cc8",
      "summary": "The technologies and models based on machine vision are widely used for early wildfire detection. Due to the broadness of wild scene and the occlusion of the vegetation, smoke is more easily detected than flame. However, the shapes of the smoke blown by the wind change constantly and the smoke colors from different combustors vary greatly. Therefore, the existing target detection networks have limitations in detecting wildland fire smoke, such as low detection accuracy and high false alarm rate. This paper designs the attention model Recursive Bidirectional Feature Pyramid Network (RBiFPN for short) for the fusion and enhancement of smoke features. We introduce RBiFPN into the backbone network of YOLOV5 frame to better distinguish the subtle difference between clouds and smoke. In addition, we replace the classification head of YOLOV5 with Swin Transformer, which helps to change the receptive fields of the network with the size of smoke regions and enhance the capability of modeling local features and global features. We tested the proposed model on the dataset containing a large number of interference objects such as clouds and fog. The experimental results show that our model can detect wildfire smoke with a higher performance than the state-of-the-art methods.",
      "intriguing_abstract": "The technologies and models based on machine vision are widely used for early wildfire detection. Due to the broadness of wild scene and the occlusion of the vegetation, smoke is more easily detected than flame. However, the shapes of the smoke blown by the wind change constantly and the smoke colors from different combustors vary greatly. Therefore, the existing target detection networks have limitations in detecting wildland fire smoke, such as low detection accuracy and high false alarm rate. This paper designs the attention model Recursive Bidirectional Feature Pyramid Network (RBiFPN for short) for the fusion and enhancement of smoke features. We introduce RBiFPN into the backbone network of YOLOV5 frame to better distinguish the subtle difference between clouds and smoke. In addition, we replace the classification head of YOLOV5 with Swin Transformer, which helps to change the receptive fields of the network with the size of smoke regions and enhance the capability of modeling local features and global features. We tested the proposed model on the dataset containing a large number of interference objects such as clouds and fog. The experimental results show that our model can detect wildfire smoke with a higher performance than the state-of-the-art methods.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/4767600089dd71c7ed74c6a0acaf9e5a6fb345c9.pdf",
      "citation_key": "li202240n",
      "metadata": {
        "title": "Novel Recursive BiFPN Combining with Swin Transformer for Wildland Fire Smoke Detection",
        "authors": [
          "Ao Li",
          "Yaqin Zhao",
          "Zhaoxiang Zheng"
        ],
        "published_date": "2022",
        "abstract": "The technologies and models based on machine vision are widely used for early wildfire detection. Due to the broadness of wild scene and the occlusion of the vegetation, smoke is more easily detected than flame. However, the shapes of the smoke blown by the wind change constantly and the smoke colors from different combustors vary greatly. Therefore, the existing target detection networks have limitations in detecting wildland fire smoke, such as low detection accuracy and high false alarm rate. This paper designs the attention model Recursive Bidirectional Feature Pyramid Network (RBiFPN for short) for the fusion and enhancement of smoke features. We introduce RBiFPN into the backbone network of YOLOV5 frame to better distinguish the subtle difference between clouds and smoke. In addition, we replace the classification head of YOLOV5 with Swin Transformer, which helps to change the receptive fields of the network with the size of smoke regions and enhance the capability of modeling local features and global features. We tested the proposed model on the dataset containing a large number of interference objects such as clouds and fog. The experimental results show that our model can detect wildfire smoke with a higher performance than the state-of-the-art methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/4767600089dd71c7ed74c6a0acaf9e5a6fb345c9.pdf",
        "venue": "Forests",
        "citationCount": 23,
        "score": 7.666666666666666,
        "summary": "The technologies and models based on machine vision are widely used for early wildfire detection. Due to the broadness of wild scene and the occlusion of the vegetation, smoke is more easily detected than flame. However, the shapes of the smoke blown by the wind change constantly and the smoke colors from different combustors vary greatly. Therefore, the existing target detection networks have limitations in detecting wildland fire smoke, such as low detection accuracy and high false alarm rate. This paper designs the attention model Recursive Bidirectional Feature Pyramid Network (RBiFPN for short) for the fusion and enhancement of smoke features. We introduce RBiFPN into the backbone network of YOLOV5 frame to better distinguish the subtle difference between clouds and smoke. In addition, we replace the classification head of YOLOV5 with Swin Transformer, which helps to change the receptive fields of the network with the size of smoke regions and enhance the capability of modeling local features and global features. We tested the proposed model on the dataset containing a large number of interference objects such as clouds and fog. The experimental results show that our model can detect wildfire smoke with a higher performance than the state-of-the-art methods.",
        "keywords": []
      },
      "file_name": "4767600089dd71c7ed74c6a0acaf9e5a6fb345c9.pdf"
    },
    {
      "success": true,
      "doc_id": "532ba8eae32ee0b2f8eb73a33b4380b0",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d5b8af7ae526e3a511b1172c26d7caf7421145bb.pdf",
      "citation_key": "jiang2022jlc",
      "metadata": {
        "title": "MXT: A New Variant of Pyramid Vision Transformer for Multi-label Chest X-ray Image Classification",
        "authors": [
          "Xiaoben Jiang",
          "Yu Zhu",
          "Gan Cai",
          "Bingbing Zheng",
          "Dawei Yang"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d5b8af7ae526e3a511b1172c26d7caf7421145bb.pdf",
        "venue": "Cognitive Computation",
        "citationCount": 23,
        "score": 7.666666666666666,
        "summary": "",
        "keywords": []
      },
      "file_name": "d5b8af7ae526e3a511b1172c26d7caf7421145bb.pdf"
    },
    {
      "success": true,
      "doc_id": "81ed34b852300375b60dbb0599522575",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/4d491b6fbe529a3986ef50cc34ede7c9ad88126c.pdf",
      "citation_key": "lin2021oan",
      "metadata": {
        "title": "FQ-ViT: Fully Quantized Vision Transformer without Retraining",
        "authors": [
          "Yang Lin",
          "Tianyu Zhang",
          "Peiqin Sun",
          "Zheng Li",
          "Shuchang Zhou"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/4d491b6fbe529a3986ef50cc34ede7c9ad88126c.pdf",
        "venue": "arXiv.org",
        "citationCount": 30,
        "score": 7.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "4d491b6fbe529a3986ef50cc34ede7c9ad88126c.pdf"
    },
    {
      "success": true,
      "doc_id": "7f6ec868badef08a0bfd2bc1362da8b0",
      "summary": "Measuring the perceptual quality of images automatically is an essential task in the area of computer vision, as degradations on image quality can exist in many processes from image acquisition, transmission to enhancing. Many Image Quality Assessment(IQA) algorithms have been designed to tackle this problem. However, it still remains unsettled due to the various types of image distortions and the lack of large-scale human-rated datasets. In this paper, we propose a novel algorithm based on the Swin Transformer [31] with fused features from multiple stages, which aggregates information from both local and global features to better predict the quality. To address the issues of small-scale datasets, relative rankings of images have been taken into account together with regression loss to simultaneously optimize the model. Furthermore, effective data augmentation strategies are also used to improve the performance. In comparisons with previous works, experiments are carried out on two standard IQA datasets and a challenge dataset. The results demonstrate the effectiveness of our work. The proposed method outperforms other methods on standard datasets and ranks 2nd in the no-reference track of NTIRE 2022 Perceptual Image Quality Assessment Challenge [53]. It verifies that our method is promising in solving diverse IQA problems and thus can be used to real-word applications.",
      "intriguing_abstract": "Measuring the perceptual quality of images automatically is an essential task in the area of computer vision, as degradations on image quality can exist in many processes from image acquisition, transmission to enhancing. Many Image Quality Assessment(IQA) algorithms have been designed to tackle this problem. However, it still remains unsettled due to the various types of image distortions and the lack of large-scale human-rated datasets. In this paper, we propose a novel algorithm based on the Swin Transformer [31] with fused features from multiple stages, which aggregates information from both local and global features to better predict the quality. To address the issues of small-scale datasets, relative rankings of images have been taken into account together with regression loss to simultaneously optimize the model. Furthermore, effective data augmentation strategies are also used to improve the performance. In comparisons with previous works, experiments are carried out on two standard IQA datasets and a challenge dataset. The results demonstrate the effectiveness of our work. The proposed method outperforms other methods on standard datasets and ranks 2nd in the no-reference track of NTIRE 2022 Perceptual Image Quality Assessment Challenge [53]. It verifies that our method is promising in solving diverse IQA problems and thus can be used to real-word applications.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d9e4af7739d4d5dcd44619bf276dc497f7334e34.pdf",
      "citation_key": "wang2022dl1",
      "metadata": {
        "title": "MSTRIQ: No Reference Image Quality Assessment Based on Swin Transformer with Multi-Stage Fusion",
        "authors": [
          "Jing Wang",
          "Haotian Fa",
          "X. Hou",
          "Yitian Xu",
          "Tao Li",
          "X. Lu",
          "Lean Fu"
        ],
        "published_date": "2022",
        "abstract": "Measuring the perceptual quality of images automatically is an essential task in the area of computer vision, as degradations on image quality can exist in many processes from image acquisition, transmission to enhancing. Many Image Quality Assessment(IQA) algorithms have been designed to tackle this problem. However, it still remains unsettled due to the various types of image distortions and the lack of large-scale human-rated datasets. In this paper, we propose a novel algorithm based on the Swin Transformer [31] with fused features from multiple stages, which aggregates information from both local and global features to better predict the quality. To address the issues of small-scale datasets, relative rankings of images have been taken into account together with regression loss to simultaneously optimize the model. Furthermore, effective data augmentation strategies are also used to improve the performance. In comparisons with previous works, experiments are carried out on two standard IQA datasets and a challenge dataset. The results demonstrate the effectiveness of our work. The proposed method outperforms other methods on standard datasets and ranks 2nd in the no-reference track of NTIRE 2022 Perceptual Image Quality Assessment Challenge [53]. It verifies that our method is promising in solving diverse IQA problems and thus can be used to real-word applications.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d9e4af7739d4d5dcd44619bf276dc497f7334e34.pdf",
        "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
        "citationCount": 22,
        "score": 7.333333333333333,
        "summary": "Measuring the perceptual quality of images automatically is an essential task in the area of computer vision, as degradations on image quality can exist in many processes from image acquisition, transmission to enhancing. Many Image Quality Assessment(IQA) algorithms have been designed to tackle this problem. However, it still remains unsettled due to the various types of image distortions and the lack of large-scale human-rated datasets. In this paper, we propose a novel algorithm based on the Swin Transformer [31] with fused features from multiple stages, which aggregates information from both local and global features to better predict the quality. To address the issues of small-scale datasets, relative rankings of images have been taken into account together with regression loss to simultaneously optimize the model. Furthermore, effective data augmentation strategies are also used to improve the performance. In comparisons with previous works, experiments are carried out on two standard IQA datasets and a challenge dataset. The results demonstrate the effectiveness of our work. The proposed method outperforms other methods on standard datasets and ranks 2nd in the no-reference track of NTIRE 2022 Perceptual Image Quality Assessment Challenge [53]. It verifies that our method is promising in solving diverse IQA problems and thus can be used to real-word applications.",
        "keywords": []
      },
      "file_name": "d9e4af7739d4d5dcd44619bf276dc497f7334e34.pdf"
    },
    {
      "success": true,
      "doc_id": "ff76bbb5f4ab1aa64f39bbc2aebb755e",
      "summary": "With the rapid development of precision agriculture and smart agriculture, the need to build an automatic identification and detection system for diseases and insect pests is increasing. Using computers to correctly label plant diseases and insect pests is an important prerequisite for achieving accurate classification of plant diseases and insect pests and ensuring system performance. In order to improve the accuracy of computer classification of plant pests and diseases, this paper proposes an automatic pest identification method based on the Vision Transformer (ViT). In order to avoid training overfitting, the plant diseases and insect pests data sets are enhanced by methods such as Histogram Equalization, Laplacian, Gamma Transformation, CLAHE, Retinex-SSR, and Retinex-MSR. Then use the enhanced data set to train the constructed ViT neural network, so as to realize the automatic classification of plant diseases and insect pests. The simulation results show that the constructed ViT network has a test recognition accuracy rate of 96.71% on the plant disease and insect pest public data set Plant_Village, which is about 1.00% higher than the Plant disease and pest identification method based on traditional convolutional neural networks such as GoogleNet and EfficentNetV2.",
      "intriguing_abstract": "With the rapid development of precision agriculture and smart agriculture, the need to build an automatic identification and detection system for diseases and insect pests is increasing. Using computers to correctly label plant diseases and insect pests is an important prerequisite for achieving accurate classification of plant diseases and insect pests and ensuring system performance. In order to improve the accuracy of computer classification of plant pests and diseases, this paper proposes an automatic pest identification method based on the Vision Transformer (ViT). In order to avoid training overfitting, the plant diseases and insect pests data sets are enhanced by methods such as Histogram Equalization, Laplacian, Gamma Transformation, CLAHE, Retinex-SSR, and Retinex-MSR. Then use the enhanced data set to train the constructed ViT neural network, so as to realize the automatic classification of plant diseases and insect pests. The simulation results show that the constructed ViT network has a test recognition accuracy rate of 96.71% on the plant disease and insect pest public data set Plant_Village, which is about 1.00% higher than the Plant disease and pest identification method based on traditional convolutional neural networks such as GoogleNet and EfficentNetV2.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d69102eec0fff1084e3d1e24a411103280020a32.pdf",
      "citation_key": "li2022wab",
      "metadata": {
        "title": "Plant disease and insect pest identification based on vision transformer",
        "authors": [
          "Han Li",
          "Sufang Li",
          "Jiguo Yu",
          "Yubing Han",
          "Anming Dong"
        ],
        "published_date": "2022",
        "abstract": "With the rapid development of precision agriculture and smart agriculture, the need to build an automatic identification and detection system for diseases and insect pests is increasing. Using computers to correctly label plant diseases and insect pests is an important prerequisite for achieving accurate classification of plant diseases and insect pests and ensuring system performance. In order to improve the accuracy of computer classification of plant pests and diseases, this paper proposes an automatic pest identification method based on the Vision Transformer (ViT). In order to avoid training overfitting, the plant diseases and insect pests data sets are enhanced by methods such as Histogram Equalization, Laplacian, Gamma Transformation, CLAHE, Retinex-SSR, and Retinex-MSR. Then use the enhanced data set to train the constructed ViT neural network, so as to realize the automatic classification of plant diseases and insect pests. The simulation results show that the constructed ViT network has a test recognition accuracy rate of 96.71% on the plant disease and insect pest public data set Plant_Village, which is about 1.00% higher than the Plant disease and pest identification method based on traditional convolutional neural networks such as GoogleNet and EfficentNetV2.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d69102eec0fff1084e3d1e24a411103280020a32.pdf",
        "venue": "Other Conferences",
        "citationCount": 22,
        "score": 7.333333333333333,
        "summary": "With the rapid development of precision agriculture and smart agriculture, the need to build an automatic identification and detection system for diseases and insect pests is increasing. Using computers to correctly label plant diseases and insect pests is an important prerequisite for achieving accurate classification of plant diseases and insect pests and ensuring system performance. In order to improve the accuracy of computer classification of plant pests and diseases, this paper proposes an automatic pest identification method based on the Vision Transformer (ViT). In order to avoid training overfitting, the plant diseases and insect pests data sets are enhanced by methods such as Histogram Equalization, Laplacian, Gamma Transformation, CLAHE, Retinex-SSR, and Retinex-MSR. Then use the enhanced data set to train the constructed ViT neural network, so as to realize the automatic classification of plant diseases and insect pests. The simulation results show that the constructed ViT network has a test recognition accuracy rate of 96.71% on the plant disease and insect pest public data set Plant_Village, which is about 1.00% higher than the Plant disease and pest identification method based on traditional convolutional neural networks such as GoogleNet and EfficentNetV2.",
        "keywords": []
      },
      "file_name": "d69102eec0fff1084e3d1e24a411103280020a32.pdf"
    },
    {
      "success": true,
      "doc_id": "1bfe968f7c0e2d89c7a73f7d7c0f7d11",
      "summary": "The widespread application of artificial intelligence in health research is currently hampered by limitations in data availability. Distributed learning methods such as federated learning (FL) and split learning (SL) are introduced to solve this problem as well as data management and ownership issues with their different strengths and weaknesses. The recent proposal of federated split task-agnostic (F eSTA) learning tries to reconcile the distinct merits of FL and SL by enabling the multi-task collaboration between participants through Vision Transformer (ViT) architecture, but they suffer from higher communication overhead. To address this, here we present a multi-task distributed learning using ViT with random patch permutation, dubbed <inline-formula> <tex-math notation=\"LaTeX\">${p}$ </tex-math></inline-formula>-F eSTA. Instead of using a CNN-based head as in F eSTA, <inline-formula> <tex-math notation=\"LaTeX\">${p}$ </tex-math></inline-formula>-F eSTA adopts a simple patch embedder with random permutation, improving the multi-task learning performance without sacrificing privacy. Experimental results confirm that the proposed method significantly enhances the benefit of multi-task collaboration, communication efficiency, and privacy preservation, shedding light on practical multi-task distributed learning in the field of medical imaging.",
      "intriguing_abstract": "The widespread application of artificial intelligence in health research is currently hampered by limitations in data availability. Distributed learning methods such as federated learning (FL) and split learning (SL) are introduced to solve this problem as well as data management and ownership issues with their different strengths and weaknesses. The recent proposal of federated split task-agnostic (F eSTA) learning tries to reconcile the distinct merits of FL and SL by enabling the multi-task collaboration between participants through Vision Transformer (ViT) architecture, but they suffer from higher communication overhead. To address this, here we present a multi-task distributed learning using ViT with random patch permutation, dubbed <inline-formula> <tex-math notation=\"LaTeX\">${p}$ </tex-math></inline-formula>-F eSTA. Instead of using a CNN-based head as in F eSTA, <inline-formula> <tex-math notation=\"LaTeX\">${p}$ </tex-math></inline-formula>-F eSTA adopts a simple patch embedder with random permutation, improving the multi-task learning performance without sacrificing privacy. Experimental results confirm that the proposed method significantly enhances the benefit of multi-task collaboration, communication efficiency, and privacy preservation, shedding light on practical multi-task distributed learning in the field of medical imaging.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/38bef6f0b4de2f9809b4fd44e8b61f9bebb14db9.pdf",
      "citation_key": "park2022eln",
      "metadata": {
        "title": "Multi-Task Distributed Learning Using Vision Transformer With Random Patch Permutation",
        "authors": [
          "Sangjoon Park",
          "Jong-Chul Ye"
        ],
        "published_date": "2022",
        "abstract": "The widespread application of artificial intelligence in health research is currently hampered by limitations in data availability. Distributed learning methods such as federated learning (FL) and split learning (SL) are introduced to solve this problem as well as data management and ownership issues with their different strengths and weaknesses. The recent proposal of federated split task-agnostic (F eSTA) learning tries to reconcile the distinct merits of FL and SL by enabling the multi-task collaboration between participants through Vision Transformer (ViT) architecture, but they suffer from higher communication overhead. To address this, here we present a multi-task distributed learning using ViT with random patch permutation, dubbed <inline-formula> <tex-math notation=\"LaTeX\">${p}$ </tex-math></inline-formula>-F eSTA. Instead of using a CNN-based head as in F eSTA, <inline-formula> <tex-math notation=\"LaTeX\">${p}$ </tex-math></inline-formula>-F eSTA adopts a simple patch embedder with random permutation, improving the multi-task learning performance without sacrificing privacy. Experimental results confirm that the proposed method significantly enhances the benefit of multi-task collaboration, communication efficiency, and privacy preservation, shedding light on practical multi-task distributed learning in the field of medical imaging.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/38bef6f0b4de2f9809b4fd44e8b61f9bebb14db9.pdf",
        "venue": "IEEE Transactions on Medical Imaging",
        "citationCount": 22,
        "score": 7.333333333333333,
        "summary": "The widespread application of artificial intelligence in health research is currently hampered by limitations in data availability. Distributed learning methods such as federated learning (FL) and split learning (SL) are introduced to solve this problem as well as data management and ownership issues with their different strengths and weaknesses. The recent proposal of federated split task-agnostic (F eSTA) learning tries to reconcile the distinct merits of FL and SL by enabling the multi-task collaboration between participants through Vision Transformer (ViT) architecture, but they suffer from higher communication overhead. To address this, here we present a multi-task distributed learning using ViT with random patch permutation, dubbed <inline-formula> <tex-math notation=\"LaTeX\">${p}$ </tex-math></inline-formula>-F eSTA. Instead of using a CNN-based head as in F eSTA, <inline-formula> <tex-math notation=\"LaTeX\">${p}$ </tex-math></inline-formula>-F eSTA adopts a simple patch embedder with random permutation, improving the multi-task learning performance without sacrificing privacy. Experimental results confirm that the proposed method significantly enhances the benefit of multi-task collaboration, communication efficiency, and privacy preservation, shedding light on practical multi-task distributed learning in the field of medical imaging.",
        "keywords": []
      },
      "file_name": "38bef6f0b4de2f9809b4fd44e8b61f9bebb14db9.pdf"
    },
    {
      "success": true,
      "doc_id": "301df0d9ae92d07d208c7c8898731899",
      "summary": "Image-based survival prediction models can facilitate doctors in diagnosing and treating cancer patients. With the advance of digital pathology technologies, the big whole slide images (WSIs) provide increasing resolution and more details for diagnosis. However, the gigabyte-size WSIs would make most models computationally infeasible. To this end, instead of using the complete WSIs, most of existing models only use a pre-selected subset of key patches or patch clusters as input, which might fail to completely capture the patient's tumor morphology. In this work, we aim to develop a novel survival analysis model to fully utilize the complete WSI information. We show that the use of a Vision Transformer (ViT) backbone, together with convolution operations involved in it, is an effective framework to improve the prediction performance. Additionally, we present a post-hoc explainable method to identify the most salient patches and distinct morphology features, making the model more faithful and the results easier to comprehend by human users. Evaluations on two large cancer datasets show that our proposed model is more effective and has better interpretability for survival prediction.",
      "intriguing_abstract": "Image-based survival prediction models can facilitate doctors in diagnosing and treating cancer patients. With the advance of digital pathology technologies, the big whole slide images (WSIs) provide increasing resolution and more details for diagnosis. However, the gigabyte-size WSIs would make most models computationally infeasible. To this end, instead of using the complete WSIs, most of existing models only use a pre-selected subset of key patches or patch clusters as input, which might fail to completely capture the patient's tumor morphology. In this work, we aim to develop a novel survival analysis model to fully utilize the complete WSI information. We show that the use of a Vision Transformer (ViT) backbone, together with convolution operations involved in it, is an effective framework to improve the prediction performance. Additionally, we present a post-hoc explainable method to identify the most salient patches and distinct morphology features, making the model more faithful and the results easier to comprehend by human users. Evaluations on two large cancer datasets show that our proposed model is more effective and has better interpretability for survival prediction.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/b80d0ce78652ffbf6b3ddebfc567f8594a6ffd4b.pdf",
      "citation_key": "shen2022d6i",
      "metadata": {
        "title": "Explainable Survival Analysis with Convolution-Involved Vision Transformer",
        "authors": [
          "Yifan Shen",
          "Li Liu",
          "Zhihao Tang",
          "Zongyi Chen",
          "Guixiang Ma",
          "Jiyan Dong",
          "Xi Zhang",
          "Lin Yang",
          "Q. Zheng"
        ],
        "published_date": "2022",
        "abstract": "Image-based survival prediction models can facilitate doctors in diagnosing and treating cancer patients. With the advance of digital pathology technologies, the big whole slide images (WSIs) provide increasing resolution and more details for diagnosis. However, the gigabyte-size WSIs would make most models computationally infeasible. To this end, instead of using the complete WSIs, most of existing models only use a pre-selected subset of key patches or patch clusters as input, which might fail to completely capture the patient's tumor morphology. In this work, we aim to develop a novel survival analysis model to fully utilize the complete WSI information. We show that the use of a Vision Transformer (ViT) backbone, together with convolution operations involved in it, is an effective framework to improve the prediction performance. Additionally, we present a post-hoc explainable method to identify the most salient patches and distinct morphology features, making the model more faithful and the results easier to comprehend by human users. Evaluations on two large cancer datasets show that our proposed model is more effective and has better interpretability for survival prediction.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b80d0ce78652ffbf6b3ddebfc567f8594a6ffd4b.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 22,
        "score": 7.333333333333333,
        "summary": "Image-based survival prediction models can facilitate doctors in diagnosing and treating cancer patients. With the advance of digital pathology technologies, the big whole slide images (WSIs) provide increasing resolution and more details for diagnosis. However, the gigabyte-size WSIs would make most models computationally infeasible. To this end, instead of using the complete WSIs, most of existing models only use a pre-selected subset of key patches or patch clusters as input, which might fail to completely capture the patient's tumor morphology. In this work, we aim to develop a novel survival analysis model to fully utilize the complete WSI information. We show that the use of a Vision Transformer (ViT) backbone, together with convolution operations involved in it, is an effective framework to improve the prediction performance. Additionally, we present a post-hoc explainable method to identify the most salient patches and distinct morphology features, making the model more faithful and the results easier to comprehend by human users. Evaluations on two large cancer datasets show that our proposed model is more effective and has better interpretability for survival prediction.",
        "keywords": []
      },
      "file_name": "b80d0ce78652ffbf6b3ddebfc567f8594a6ffd4b.pdf"
    },
    {
      "success": true,
      "doc_id": "dde11a9656d3382db1d20073a3ed0931",
      "summary": "In recent years, methods based on deep convolutional neural networks (CNNs) have dominated the classification task of hyperspectral images. Although CNN-based HSI classification methods have the advantages of spatial feature extraction, HSI images are characterized by approximately continuous spectral information, usually containing hundreds of spectral bands. CNN cannot mine and represent the sequence properties of spectral features well, and the transformer model of attention mechanism proves its advantages in processing sequence data. This study proposes a new spectral spatial kernel combined with the improved Vision Transformer (ViT) to jointly extract spatial spectral features to complete classification task. First, the hyperspectral data are dimensionally reduced by PCA; then, the shallow features are extracted with an spectral spatial kernel, and the extracted features are input into the improved ViT model. The improved ViT introduces a re-attention mechanism and a local mechanism based on the original ViT. The re-attention mechanism can increase the diversity of attention maps at different levels. The local mechanism is introduced into ViT to make full use of the local and global information of the data to improve the classification accuracy. Finally, a multi-layer perceptron is used to obtain the classification result. Among them, the Focal Loss function is used to increase the loss weight of small-class samples and difficult-to-classify samples in HSI data samples and reduce the loss weight of easy-to-classify samples, so that the network can learn more useful hyperspectral image information. In addition, using the Apollo optimizer to train the HSI classification model to better update and compute network parameters that affect model training and model output, thereby minimizing the loss function. We evaluated the classification performance of the proposed method on four different datasets, and achieved good classification results on urban land object classification, crop classification and mineral classification, respectively. Compared with the state-of-the-art backbone network, the method achieves a significant improvement and achieves very good classification accuracy.",
      "intriguing_abstract": "In recent years, methods based on deep convolutional neural networks (CNNs) have dominated the classification task of hyperspectral images. Although CNN-based HSI classification methods have the advantages of spatial feature extraction, HSI images are characterized by approximately continuous spectral information, usually containing hundreds of spectral bands. CNN cannot mine and represent the sequence properties of spectral features well, and the transformer model of attention mechanism proves its advantages in processing sequence data. This study proposes a new spectral spatial kernel combined with the improved Vision Transformer (ViT) to jointly extract spatial spectral features to complete classification task. First, the hyperspectral data are dimensionally reduced by PCA; then, the shallow features are extracted with an spectral spatial kernel, and the extracted features are input into the improved ViT model. The improved ViT introduces a re-attention mechanism and a local mechanism based on the original ViT. The re-attention mechanism can increase the diversity of attention maps at different levels. The local mechanism is introduced into ViT to make full use of the local and global information of the data to improve the classification accuracy. Finally, a multi-layer perceptron is used to obtain the classification result. Among them, the Focal Loss function is used to increase the loss weight of small-class samples and difficult-to-classify samples in HSI data samples and reduce the loss weight of easy-to-classify samples, so that the network can learn more useful hyperspectral image information. In addition, using the Apollo optimizer to train the HSI classification model to better update and compute network parameters that affect model training and model output, thereby minimizing the loss function. We evaluated the classification performance of the proposed method on four different datasets, and achieved good classification results on urban land object classification, crop classification and mineral classification, respectively. Compared with the state-of-the-art backbone network, the method achieves a significant improvement and achieves very good classification accuracy.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/7d4d512445903e8ad75f0dcfa8baec27b1fe8620.pdf",
      "citation_key": "wang20224wo",
      "metadata": {
        "title": "A Hyperspectral Image Classification Method Based on Adaptive Spectral Spatial Kernel Combined with Improved Vision Transformer",
        "authors": [
          "Aili Wang",
          "Shuang Xing",
          "Yan Zhao",
          "Haibin Wu",
          "Y. Iwahori"
        ],
        "published_date": "2022",
        "abstract": "In recent years, methods based on deep convolutional neural networks (CNNs) have dominated the classification task of hyperspectral images. Although CNN-based HSI classification methods have the advantages of spatial feature extraction, HSI images are characterized by approximately continuous spectral information, usually containing hundreds of spectral bands. CNN cannot mine and represent the sequence properties of spectral features well, and the transformer model of attention mechanism proves its advantages in processing sequence data. This study proposes a new spectral spatial kernel combined with the improved Vision Transformer (ViT) to jointly extract spatial spectral features to complete classification task. First, the hyperspectral data are dimensionally reduced by PCA; then, the shallow features are extracted with an spectral spatial kernel, and the extracted features are input into the improved ViT model. The improved ViT introduces a re-attention mechanism and a local mechanism based on the original ViT. The re-attention mechanism can increase the diversity of attention maps at different levels. The local mechanism is introduced into ViT to make full use of the local and global information of the data to improve the classification accuracy. Finally, a multi-layer perceptron is used to obtain the classification result. Among them, the Focal Loss function is used to increase the loss weight of small-class samples and difficult-to-classify samples in HSI data samples and reduce the loss weight of easy-to-classify samples, so that the network can learn more useful hyperspectral image information. In addition, using the Apollo optimizer to train the HSI classification model to better update and compute network parameters that affect model training and model output, thereby minimizing the loss function. We evaluated the classification performance of the proposed method on four different datasets, and achieved good classification results on urban land object classification, crop classification and mineral classification, respectively. Compared with the state-of-the-art backbone network, the method achieves a significant improvement and achieves very good classification accuracy.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7d4d512445903e8ad75f0dcfa8baec27b1fe8620.pdf",
        "venue": "Remote Sensing",
        "citationCount": 21,
        "score": 7.0,
        "summary": "In recent years, methods based on deep convolutional neural networks (CNNs) have dominated the classification task of hyperspectral images. Although CNN-based HSI classification methods have the advantages of spatial feature extraction, HSI images are characterized by approximately continuous spectral information, usually containing hundreds of spectral bands. CNN cannot mine and represent the sequence properties of spectral features well, and the transformer model of attention mechanism proves its advantages in processing sequence data. This study proposes a new spectral spatial kernel combined with the improved Vision Transformer (ViT) to jointly extract spatial spectral features to complete classification task. First, the hyperspectral data are dimensionally reduced by PCA; then, the shallow features are extracted with an spectral spatial kernel, and the extracted features are input into the improved ViT model. The improved ViT introduces a re-attention mechanism and a local mechanism based on the original ViT. The re-attention mechanism can increase the diversity of attention maps at different levels. The local mechanism is introduced into ViT to make full use of the local and global information of the data to improve the classification accuracy. Finally, a multi-layer perceptron is used to obtain the classification result. Among them, the Focal Loss function is used to increase the loss weight of small-class samples and difficult-to-classify samples in HSI data samples and reduce the loss weight of easy-to-classify samples, so that the network can learn more useful hyperspectral image information. In addition, using the Apollo optimizer to train the HSI classification model to better update and compute network parameters that affect model training and model output, thereby minimizing the loss function. We evaluated the classification performance of the proposed method on four different datasets, and achieved good classification results on urban land object classification, crop classification and mineral classification, respectively. Compared with the state-of-the-art backbone network, the method achieves a significant improvement and achieves very good classification accuracy.",
        "keywords": []
      },
      "file_name": "7d4d512445903e8ad75f0dcfa8baec27b1fe8620.pdf"
    },
    {
      "success": true,
      "doc_id": "29dd1c5ee536c41a4301880462e827c9",
      "summary": "Vision Transformers (ViT) have recently demonstrated the significant potential of transformer architectures for computer vision. To what extent can image-based deep reinforcement learning also benefit from ViT architectures, as compared to standard convolutional neural network (CNN) architectures? To answer this question, we evaluate ViT training methods for image-based reinforcement learning (RL) control tasks and compare these results to a leading convolutional-network architecture method, RAD. For training the ViT encoder, we consider several recently-proposed self-supervised losses that are treated as auxiliary tasks, as well as a baseline with no additional loss terms. We find that the CNN architectures trained using RAD still generally provide superior performance. For the ViT methods, all three types of auxiliary tasks that we consider provide a benefit over plain ViT training. Furthermore, ViT reconstruction-based tasks are found to significantly outperform ViT contrastive-learning.",
      "intriguing_abstract": "Vision Transformers (ViT) have recently demonstrated the significant potential of transformer architectures for computer vision. To what extent can image-based deep reinforcement learning also benefit from ViT architectures, as compared to standard convolutional neural network (CNN) architectures? To answer this question, we evaluate ViT training methods for image-based reinforcement learning (RL) control tasks and compare these results to a leading convolutional-network architecture method, RAD. For training the ViT encoder, we consider several recently-proposed self-supervised losses that are treated as auxiliary tasks, as well as a baseline with no additional loss terms. We find that the CNN architectures trained using RAD still generally provide superior performance. For the ViT methods, all three types of auxiliary tasks that we consider provide a benefit over plain ViT training. Furthermore, ViT reconstruction-based tasks are found to significantly outperform ViT contrastive-learning.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/7d5274f1155b85a6120491c9374b6983dac96552.pdf",
      "citation_key": "tao2022gdr",
      "metadata": {
        "title": "Evaluating Vision Transformer Methods for Deep Reinforcement Learning from Pixels",
        "authors": [
          "Tianxin Tao",
          "Daniele Reda",
          "M. V. D. Panne"
        ],
        "published_date": "2022",
        "abstract": "Vision Transformers (ViT) have recently demonstrated the significant potential of transformer architectures for computer vision. To what extent can image-based deep reinforcement learning also benefit from ViT architectures, as compared to standard convolutional neural network (CNN) architectures? To answer this question, we evaluate ViT training methods for image-based reinforcement learning (RL) control tasks and compare these results to a leading convolutional-network architecture method, RAD. For training the ViT encoder, we consider several recently-proposed self-supervised losses that are treated as auxiliary tasks, as well as a baseline with no additional loss terms. We find that the CNN architectures trained using RAD still generally provide superior performance. For the ViT methods, all three types of auxiliary tasks that we consider provide a benefit over plain ViT training. Furthermore, ViT reconstruction-based tasks are found to significantly outperform ViT contrastive-learning.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7d5274f1155b85a6120491c9374b6983dac96552.pdf",
        "venue": "arXiv.org",
        "citationCount": 21,
        "score": 7.0,
        "summary": "Vision Transformers (ViT) have recently demonstrated the significant potential of transformer architectures for computer vision. To what extent can image-based deep reinforcement learning also benefit from ViT architectures, as compared to standard convolutional neural network (CNN) architectures? To answer this question, we evaluate ViT training methods for image-based reinforcement learning (RL) control tasks and compare these results to a leading convolutional-network architecture method, RAD. For training the ViT encoder, we consider several recently-proposed self-supervised losses that are treated as auxiliary tasks, as well as a baseline with no additional loss terms. We find that the CNN architectures trained using RAD still generally provide superior performance. For the ViT methods, all three types of auxiliary tasks that we consider provide a benefit over plain ViT training. Furthermore, ViT reconstruction-based tasks are found to significantly outperform ViT contrastive-learning.",
        "keywords": []
      },
      "file_name": "7d5274f1155b85a6120491c9374b6983dac96552.pdf"
    },
    {
      "success": true,
      "doc_id": "20cdcced052f7197542eef7cc054dd2f",
      "summary": "At present, the task of identifying crop diseases is mainly to simply distinguish the types of different crop diseases. However, the current classifiers cannot solve problems, such as accurate identification of similar disease categories. Compared with convolutional neural network (CNN), the recent vision transformer (VIT) has achieved good results on image tasks. Inspired by this, this paper proposed a multi-granularity feature extraction model based on vision transformer. By combining image block information of different scales, the model can learn image information from different granularities. At the same time, in order to further grasp the important areas, this paper developed a feature selection module. Through experimental comparison, the scheme has an accuracy improvement of nearly 2% compared with other classification models, and the model parameters have not improved much.",
      "intriguing_abstract": "At present, the task of identifying crop diseases is mainly to simply distinguish the types of different crop diseases. However, the current classifiers cannot solve problems, such as accurate identification of similar disease categories. Compared with convolutional neural network (CNN), the recent vision transformer (VIT) has achieved good results on image tasks. Inspired by this, this paper proposed a multi-granularity feature extraction model based on vision transformer. By combining image block information of different scales, the model can learn image information from different granularities. At the same time, in order to further grasp the important areas, this paper developed a feature selection module. Through experimental comparison, the scheme has an accuracy improvement of nearly 2% compared with other classification models, and the model parameters have not improved much.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/0afbaae7d0bfe98a04a3b7dd1ba69e25f32b24c6.pdf",
      "citation_key": "wu2021nmg",
      "metadata": {
        "title": "Multi-granularity Feature Extraction Based on Vision Transformer for Tomato Leaf Disease Recognition",
        "authors": [
          "Shupei Wu",
          "Youqiang Sun",
          "He Huang"
        ],
        "published_date": "2021",
        "abstract": "At present, the task of identifying crop diseases is mainly to simply distinguish the types of different crop diseases. However, the current classifiers cannot solve problems, such as accurate identification of similar disease categories. Compared with convolutional neural network (CNN), the recent vision transformer (VIT) has achieved good results on image tasks. Inspired by this, this paper proposed a multi-granularity feature extraction model based on vision transformer. By combining image block information of different scales, the model can learn image information from different granularities. At the same time, in order to further grasp the important areas, this paper developed a feature selection module. Through experimental comparison, the scheme has an accuracy improvement of nearly 2% compared with other classification models, and the model parameters have not improved much.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0afbaae7d0bfe98a04a3b7dd1ba69e25f32b24c6.pdf",
        "venue": "2021 3rd International Academic Exchange Conference on Science and Technology Innovation (IAECST)",
        "citationCount": 27,
        "score": 6.75,
        "summary": "At present, the task of identifying crop diseases is mainly to simply distinguish the types of different crop diseases. However, the current classifiers cannot solve problems, such as accurate identification of similar disease categories. Compared with convolutional neural network (CNN), the recent vision transformer (VIT) has achieved good results on image tasks. Inspired by this, this paper proposed a multi-granularity feature extraction model based on vision transformer. By combining image block information of different scales, the model can learn image information from different granularities. At the same time, in order to further grasp the important areas, this paper developed a feature selection module. Through experimental comparison, the scheme has an accuracy improvement of nearly 2% compared with other classification models, and the model parameters have not improved much.",
        "keywords": []
      },
      "file_name": "0afbaae7d0bfe98a04a3b7dd1ba69e25f32b24c6.pdf"
    },
    {
      "success": true,
      "doc_id": "c979822f06ce1480ce14611bc8d8e931",
      "summary": "MedMNIST is a medical dataset proposed to block the need for medical knowledge, but there is currently no model that can generalize well on all its sub-datasets. Owing to the inadequacy of long-range relation modeling, models based on convolutional neural networks (CNNs) cannot fully learn the information of images. Besides, relying only on high-level features limits the generalization effect as well. All of these remain challenges for MedMNIST Classification Decathlon. In this paper, we proposed Feature Pyramid Vision Transformer (FPViT), a strong alternative for MedMNIST Classification Decathlon. Our FPViT exhibits enhanced feature learning and modeling capabilities, which merits both residual network (ResNet) and Vision Transformer (ViT). Transformers in our model take the features extracted by ResNet as sequences to capture global contexts which compensate for the lack of locality of convolution operations. Moreover, the feature pyramid designed in our model effectively utilizes the multi-scale feature maps from basic layers of ResNet. These multi-scale features from low-level to high level enable our model to have better adaptability. And, the final prediction is based on the multi-scale ViT and the original ResNet heads. Through experiments, our FPViT can achieve superior classification and generalization on MedMNIST than state-of-the-art methods.",
      "intriguing_abstract": "MedMNIST is a medical dataset proposed to block the need for medical knowledge, but there is currently no model that can generalize well on all its sub-datasets. Owing to the inadequacy of long-range relation modeling, models based on convolutional neural networks (CNNs) cannot fully learn the information of images. Besides, relying only on high-level features limits the generalization effect as well. All of these remain challenges for MedMNIST Classification Decathlon. In this paper, we proposed Feature Pyramid Vision Transformer (FPViT), a strong alternative for MedMNIST Classification Decathlon. Our FPViT exhibits enhanced feature learning and modeling capabilities, which merits both residual network (ResNet) and Vision Transformer (ViT). Transformers in our model take the features extracted by ResNet as sequences to capture global contexts which compensate for the lack of locality of convolution operations. Moreover, the feature pyramid designed in our model effectively utilizes the multi-scale feature maps from basic layers of ResNet. These multi-scale features from low-level to high level enable our model to have better adaptability. And, the final prediction is based on the multi-scale ViT and the original ResNet heads. Through experiments, our FPViT can achieve superior classification and generalization on MedMNIST than state-of-the-art methods.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/6bcdf04ccd35ae971e765cdda25127ce005b7713.pdf",
      "citation_key": "liu2022c56",
      "metadata": {
        "title": "Feature Pyramid Vision Transformer for MedMNIST Classification Decathlon",
        "authors": [
          "Jinwei Liu",
          "Yan Li",
          "Guitao Cao",
          "Yong Liu",
          "W. Cao"
        ],
        "published_date": "2022",
        "abstract": "MedMNIST is a medical dataset proposed to block the need for medical knowledge, but there is currently no model that can generalize well on all its sub-datasets. Owing to the inadequacy of long-range relation modeling, models based on convolutional neural networks (CNNs) cannot fully learn the information of images. Besides, relying only on high-level features limits the generalization effect as well. All of these remain challenges for MedMNIST Classification Decathlon. In this paper, we proposed Feature Pyramid Vision Transformer (FPViT), a strong alternative for MedMNIST Classification Decathlon. Our FPViT exhibits enhanced feature learning and modeling capabilities, which merits both residual network (ResNet) and Vision Transformer (ViT). Transformers in our model take the features extracted by ResNet as sequences to capture global contexts which compensate for the lack of locality of convolution operations. Moreover, the feature pyramid designed in our model effectively utilizes the multi-scale feature maps from basic layers of ResNet. These multi-scale features from low-level to high level enable our model to have better adaptability. And, the final prediction is based on the multi-scale ViT and the original ResNet heads. Through experiments, our FPViT can achieve superior classification and generalization on MedMNIST than state-of-the-art methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6bcdf04ccd35ae971e765cdda25127ce005b7713.pdf",
        "venue": "IEEE International Joint Conference on Neural Network",
        "citationCount": 20,
        "score": 6.666666666666666,
        "summary": "MedMNIST is a medical dataset proposed to block the need for medical knowledge, but there is currently no model that can generalize well on all its sub-datasets. Owing to the inadequacy of long-range relation modeling, models based on convolutional neural networks (CNNs) cannot fully learn the information of images. Besides, relying only on high-level features limits the generalization effect as well. All of these remain challenges for MedMNIST Classification Decathlon. In this paper, we proposed Feature Pyramid Vision Transformer (FPViT), a strong alternative for MedMNIST Classification Decathlon. Our FPViT exhibits enhanced feature learning and modeling capabilities, which merits both residual network (ResNet) and Vision Transformer (ViT). Transformers in our model take the features extracted by ResNet as sequences to capture global contexts which compensate for the lack of locality of convolution operations. Moreover, the feature pyramid designed in our model effectively utilizes the multi-scale feature maps from basic layers of ResNet. These multi-scale features from low-level to high level enable our model to have better adaptability. And, the final prediction is based on the multi-scale ViT and the original ResNet heads. Through experiments, our FPViT can achieve superior classification and generalization on MedMNIST than state-of-the-art methods.",
        "keywords": []
      },
      "file_name": "6bcdf04ccd35ae971e765cdda25127ce005b7713.pdf"
    },
    {
      "success": true,
      "doc_id": "1959384c9fcde95dde1f3149f243befb",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/6fd1002f321240c75dfc86c6bad6c05068a8160a.pdf",
      "citation_key": "wang2022pb8",
      "metadata": {
        "title": "STMG: Swin transformer for multi-label image recognition with graph convolution network",
        "authors": [
          "Yangtao Wang",
          "Yanzhao Xie",
          "Lisheng Fan",
          "Guangxin Hu"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6fd1002f321240c75dfc86c6bad6c05068a8160a.pdf",
        "venue": "Neural computing & applications (Print)",
        "citationCount": 19,
        "score": 6.333333333333333,
        "summary": "",
        "keywords": []
      },
      "file_name": "6fd1002f321240c75dfc86c6bad6c05068a8160a.pdf"
    },
    {
      "success": true,
      "doc_id": "686b62fa7228632fda4456a0de9c42d4",
      "summary": "Convolutional neural networks (CNNs) have been widely utilized in many computer vision tasks. However, CNNs have a fixed reception field and lack the ability of long-range perception, which is crucial to human pose estimation. Transformer architecture has been adopted to computer vision applications recently and is proven to be a highly effective architecture. We are interested in exploring its capability in human pose estimation, and thus propose a novel model based on transformer, enhanced with a feature pyramid fusion structure. More specifically, we use pre-trained Swin Transformer to extract features, and leverage a feature pyramid structure to extract and fuse feature maps from different stages. The experiment results of our study have demonstrated that the proposed transformer-based model can achieve better performance compared to the state-of-the-art CNN-based models.",
      "intriguing_abstract": "Convolutional neural networks (CNNs) have been widely utilized in many computer vision tasks. However, CNNs have a fixed reception field and lack the ability of long-range perception, which is crucial to human pose estimation. Transformer architecture has been adopted to computer vision applications recently and is proven to be a highly effective architecture. We are interested in exploring its capability in human pose estimation, and thus propose a novel model based on transformer, enhanced with a feature pyramid fusion structure. More specifically, we use pre-trained Swin Transformer to extract features, and leverage a feature pyramid structure to extract and fuse feature maps from different stages. The experiment results of our study have demonstrated that the proposed transformer-based model can achieve better performance compared to the state-of-the-art CNN-based models.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/c4560ab3855cb8ae4b9922458a0b6b94108e2c28.pdf",
      "citation_key": "xiong2022ec2",
      "metadata": {
        "title": "Swin-Pose: Swin Transformer Based Human Pose Estimation",
        "authors": [
          "Zinan Xiong",
          "Chenxi Wang",
          "Ying Li",
          "Yan Luo",
          "Yu Cao"
        ],
        "published_date": "2022",
        "abstract": "Convolutional neural networks (CNNs) have been widely utilized in many computer vision tasks. However, CNNs have a fixed reception field and lack the ability of long-range perception, which is crucial to human pose estimation. Transformer architecture has been adopted to computer vision applications recently and is proven to be a highly effective architecture. We are interested in exploring its capability in human pose estimation, and thus propose a novel model based on transformer, enhanced with a feature pyramid fusion structure. More specifically, we use pre-trained Swin Transformer to extract features, and leverage a feature pyramid structure to extract and fuse feature maps from different stages. The experiment results of our study have demonstrated that the proposed transformer-based model can achieve better performance compared to the state-of-the-art CNN-based models.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c4560ab3855cb8ae4b9922458a0b6b94108e2c28.pdf",
        "venue": "Conference on Multimedia Information Processing and Retrieval",
        "citationCount": 19,
        "score": 6.333333333333333,
        "summary": "Convolutional neural networks (CNNs) have been widely utilized in many computer vision tasks. However, CNNs have a fixed reception field and lack the ability of long-range perception, which is crucial to human pose estimation. Transformer architecture has been adopted to computer vision applications recently and is proven to be a highly effective architecture. We are interested in exploring its capability in human pose estimation, and thus propose a novel model based on transformer, enhanced with a feature pyramid fusion structure. More specifically, we use pre-trained Swin Transformer to extract features, and leverage a feature pyramid structure to extract and fuse feature maps from different stages. The experiment results of our study have demonstrated that the proposed transformer-based model can achieve better performance compared to the state-of-the-art CNN-based models.",
        "keywords": []
      },
      "file_name": "c4560ab3855cb8ae4b9922458a0b6b94108e2c28.pdf"
    },
    {
      "success": true,
      "doc_id": "101d72fa652cb2fb822fa869961c56c2",
      "summary": "With the development of computer technology, various models have emerged in artificial intelligence. The transformer model has been applied to the field of computer vision (CV) after its success in natural language processing (NLP). Radiologists continue to face multiple challenges in today's rapidly evolving medical field, such as increased workload and increased diagnostic demands. Although there are some conventional methods for lung cancer detection before, their accuracy still needs to be improved, especially in realistic diagnostic scenarios. This paper creatively proposes a segmentation method based on efficient transformer and applies it to medical image analysis. The algorithm completes the task of lung cancer classification and segmentation by analyzing lung cancer data, and aims to provide efficient technical support for medical staff. In addition, we evaluated and compared the results in various aspects. For the classification mission, the max accuracy of Swin-T by regular training and Swin-B in two resolutions by pre-training can be up to 82.3%. For the segmentation mission, we use pre-training to help the model improve the accuracy of our experiments. The accuracy of the three models reaches over 95%. The experiments demonstrate that the algorithm can be well applied to lung cancer classification and segmentation missions.",
      "intriguing_abstract": "With the development of computer technology, various models have emerged in artificial intelligence. The transformer model has been applied to the field of computer vision (CV) after its success in natural language processing (NLP). Radiologists continue to face multiple challenges in today's rapidly evolving medical field, such as increased workload and increased diagnostic demands. Although there are some conventional methods for lung cancer detection before, their accuracy still needs to be improved, especially in realistic diagnostic scenarios. This paper creatively proposes a segmentation method based on efficient transformer and applies it to medical image analysis. The algorithm completes the task of lung cancer classification and segmentation by analyzing lung cancer data, and aims to provide efficient technical support for medical staff. In addition, we evaluated and compared the results in various aspects. For the classification mission, the max accuracy of Swin-T by regular training and Swin-B in two resolutions by pre-training can be up to 82.3%. For the segmentation mission, we use pre-training to help the model improve the accuracy of our experiments. The accuracy of the three models reaches over 95%. The experiments demonstrate that the algorithm can be well applied to lung cancer classification and segmentation missions.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/a0762777d6e46acbcd5b301285c2d9894065ff8f.pdf",
      "citation_key": "sun2022pom",
      "metadata": {
        "title": "Efficient Lung Cancer Image Classification and Segmentation Algorithm Based on Improved Swin Transformer",
        "authors": [
          "Ruinan Sun",
          "Yu Pang"
        ],
        "published_date": "2022",
        "abstract": "With the development of computer technology, various models have emerged in artificial intelligence. The transformer model has been applied to the field of computer vision (CV) after its success in natural language processing (NLP). Radiologists continue to face multiple challenges in today's rapidly evolving medical field, such as increased workload and increased diagnostic demands. Although there are some conventional methods for lung cancer detection before, their accuracy still needs to be improved, especially in realistic diagnostic scenarios. This paper creatively proposes a segmentation method based on efficient transformer and applies it to medical image analysis. The algorithm completes the task of lung cancer classification and segmentation by analyzing lung cancer data, and aims to provide efficient technical support for medical staff. In addition, we evaluated and compared the results in various aspects. For the classification mission, the max accuracy of Swin-T by regular training and Swin-B in two resolutions by pre-training can be up to 82.3%. For the segmentation mission, we use pre-training to help the model improve the accuracy of our experiments. The accuracy of the three models reaches over 95%. The experiments demonstrate that the algorithm can be well applied to lung cancer classification and segmentation missions.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a0762777d6e46acbcd5b301285c2d9894065ff8f.pdf",
        "venue": "arXiv.org",
        "citationCount": 19,
        "score": 6.333333333333333,
        "summary": "With the development of computer technology, various models have emerged in artificial intelligence. The transformer model has been applied to the field of computer vision (CV) after its success in natural language processing (NLP). Radiologists continue to face multiple challenges in today's rapidly evolving medical field, such as increased workload and increased diagnostic demands. Although there are some conventional methods for lung cancer detection before, their accuracy still needs to be improved, especially in realistic diagnostic scenarios. This paper creatively proposes a segmentation method based on efficient transformer and applies it to medical image analysis. The algorithm completes the task of lung cancer classification and segmentation by analyzing lung cancer data, and aims to provide efficient technical support for medical staff. In addition, we evaluated and compared the results in various aspects. For the classification mission, the max accuracy of Swin-T by regular training and Swin-B in two resolutions by pre-training can be up to 82.3%. For the segmentation mission, we use pre-training to help the model improve the accuracy of our experiments. The accuracy of the three models reaches over 95%. The experiments demonstrate that the algorithm can be well applied to lung cancer classification and segmentation missions.",
        "keywords": []
      },
      "file_name": "a0762777d6e46acbcd5b301285c2d9894065ff8f.pdf"
    },
    {
      "success": true,
      "doc_id": "bebded63f16ef2a4b9346eb8a822c715",
      "summary": "In this paper, we propose a privacy-preserving image classification method that is based on the combined use of encrypted images and the vision transformer (ViT). The proposed method allows us not only to apply images without visual information to ViT models for both training and testing but to also maintain a high classification accuracy. ViT utilizes patch embedding and position embedding for image patches, so this architecture is shown to reduce the influence of block-wise image transformation. In an experiment, the proposed method for privacy-preserving image classification is demonstrated to outperform state-of-the-art methods in terms of classification accuracy and robustness against various attacks.",
      "intriguing_abstract": "In this paper, we propose a privacy-preserving image classification method that is based on the combined use of encrypted images and the vision transformer (ViT). The proposed method allows us not only to apply images without visual information to ViT models for both training and testing but to also maintain a high classification accuracy. ViT utilizes patch embedding and position embedding for image patches, so this architecture is shown to reduce the influence of block-wise image transformation. In an experiment, the proposed method for privacy-preserving image classification is demonstrated to outperform state-of-the-art methods in terms of classification accuracy and robustness against various attacks.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d43950779dc86b728d7e002be6195526d35a26b0.pdf",
      "citation_key": "qi2022yq9",
      "metadata": {
        "title": "Privacy-Preserving Image Classification Using Vision Transformer",
        "authors": [
          "Zheng Qi",
          "AprilPyone Maungmaung",
          "Yuma Kinoshita",
          "H. Kiya"
        ],
        "published_date": "2022",
        "abstract": "In this paper, we propose a privacy-preserving image classification method that is based on the combined use of encrypted images and the vision transformer (ViT). The proposed method allows us not only to apply images without visual information to ViT models for both training and testing but to also maintain a high classification accuracy. ViT utilizes patch embedding and position embedding for image patches, so this architecture is shown to reduce the influence of block-wise image transformation. In an experiment, the proposed method for privacy-preserving image classification is demonstrated to outperform state-of-the-art methods in terms of classification accuracy and robustness against various attacks.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d43950779dc86b728d7e002be6195526d35a26b0.pdf",
        "venue": "European Signal Processing Conference",
        "citationCount": 19,
        "score": 6.333333333333333,
        "summary": "In this paper, we propose a privacy-preserving image classification method that is based on the combined use of encrypted images and the vision transformer (ViT). The proposed method allows us not only to apply images without visual information to ViT models for both training and testing but to also maintain a high classification accuracy. ViT utilizes patch embedding and position embedding for image patches, so this architecture is shown to reduce the influence of block-wise image transformation. In an experiment, the proposed method for privacy-preserving image classification is demonstrated to outperform state-of-the-art methods in terms of classification accuracy and robustness against various attacks.",
        "keywords": []
      },
      "file_name": "d43950779dc86b728d7e002be6195526d35a26b0.pdf"
    },
    {
      "success": true,
      "doc_id": "47c2fb60e06af1573b749aace08df4e5",
      "summary": "Reasoning about visual relationships is central to how humans interpret the visual world. This task remains challenging for current deep learning algorithms since it requires addressing three key technical problems jointly: 1) identifying object entities and their properties, 2) inferring semantic relations between pairs of entities, and 3) generalizing to novel object-relation combinations, i.e., systematic generalization. In this work, we use vision transformers (ViTs) as our base model for visual reasoning and make better use of concepts defined as object entities and their relations to improve the reasoning ability of ViTs. Specifically, we introduce a novel concept-feature dictionary to allow flexible image feature retrieval at training time with concept keys. This dictionary enables two new concept-guided auxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a local task for facilitating semantic object-centric correspondence learning. To examine the systematic generalization of visual reasoning models, we introduce systematic splits for the standard HICO and GQA benchmarks. We show the resulting model, Concept-guided Vision Transformer (or RelViT for short) significantly outperforms prior approaches on HICO and GQA by 16% and 13% in the original split, and by 43% and 18% in the systematic split. Our ablation analyses also reveal our model's compatibility with multiple ViT variants and robustness to hyper-parameters.",
      "intriguing_abstract": "Reasoning about visual relationships is central to how humans interpret the visual world. This task remains challenging for current deep learning algorithms since it requires addressing three key technical problems jointly: 1) identifying object entities and their properties, 2) inferring semantic relations between pairs of entities, and 3) generalizing to novel object-relation combinations, i.e., systematic generalization. In this work, we use vision transformers (ViTs) as our base model for visual reasoning and make better use of concepts defined as object entities and their relations to improve the reasoning ability of ViTs. Specifically, we introduce a novel concept-feature dictionary to allow flexible image feature retrieval at training time with concept keys. This dictionary enables two new concept-guided auxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a local task for facilitating semantic object-centric correspondence learning. To examine the systematic generalization of visual reasoning models, we introduce systematic splits for the standard HICO and GQA benchmarks. We show the resulting model, Concept-guided Vision Transformer (or RelViT for short) significantly outperforms prior approaches on HICO and GQA by 16% and 13% in the original split, and by 43% and 18% in the systematic split. Our ablation analyses also reveal our model's compatibility with multiple ViT variants and robustness to hyper-parameters.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/2e69d97ae22c6a3685cc548f8c19c696d5d7d363.pdf",
      "citation_key": "ma2022vf3",
      "metadata": {
        "title": "RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning",
        "authors": [
          "Xiaojian Ma",
          "Weili Nie",
          "Zhiding Yu",
          "Huaizu Jiang",
          "Chaowei Xiao",
          "Yuke Zhu",
          "Song-Chun Zhu",
          "Anima Anandkumar"
        ],
        "published_date": "2022",
        "abstract": "Reasoning about visual relationships is central to how humans interpret the visual world. This task remains challenging for current deep learning algorithms since it requires addressing three key technical problems jointly: 1) identifying object entities and their properties, 2) inferring semantic relations between pairs of entities, and 3) generalizing to novel object-relation combinations, i.e., systematic generalization. In this work, we use vision transformers (ViTs) as our base model for visual reasoning and make better use of concepts defined as object entities and their relations to improve the reasoning ability of ViTs. Specifically, we introduce a novel concept-feature dictionary to allow flexible image feature retrieval at training time with concept keys. This dictionary enables two new concept-guided auxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a local task for facilitating semantic object-centric correspondence learning. To examine the systematic generalization of visual reasoning models, we introduce systematic splits for the standard HICO and GQA benchmarks. We show the resulting model, Concept-guided Vision Transformer (or RelViT for short) significantly outperforms prior approaches on HICO and GQA by 16% and 13% in the original split, and by 43% and 18% in the systematic split. Our ablation analyses also reveal our model's compatibility with multiple ViT variants and robustness to hyper-parameters.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2e69d97ae22c6a3685cc548f8c19c696d5d7d363.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 19,
        "score": 6.333333333333333,
        "summary": "Reasoning about visual relationships is central to how humans interpret the visual world. This task remains challenging for current deep learning algorithms since it requires addressing three key technical problems jointly: 1) identifying object entities and their properties, 2) inferring semantic relations between pairs of entities, and 3) generalizing to novel object-relation combinations, i.e., systematic generalization. In this work, we use vision transformers (ViTs) as our base model for visual reasoning and make better use of concepts defined as object entities and their relations to improve the reasoning ability of ViTs. Specifically, we introduce a novel concept-feature dictionary to allow flexible image feature retrieval at training time with concept keys. This dictionary enables two new concept-guided auxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a local task for facilitating semantic object-centric correspondence learning. To examine the systematic generalization of visual reasoning models, we introduce systematic splits for the standard HICO and GQA benchmarks. We show the resulting model, Concept-guided Vision Transformer (or RelViT for short) significantly outperforms prior approaches on HICO and GQA by 16% and 13% in the original split, and by 43% and 18% in the systematic split. Our ablation analyses also reveal our model's compatibility with multiple ViT variants and robustness to hyper-parameters.",
        "keywords": []
      },
      "file_name": "2e69d97ae22c6a3685cc548f8c19c696d5d7d363.pdf"
    },
    {
      "success": true,
      "doc_id": "cd9863d7bdc2faeb1bb0c90815808694",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/c25091718b22384cebece2da7f30fc1702a07c76.pdf",
      "citation_key": "wang2022tok",
      "metadata": {
        "title": "Adversarial Vision Transformer for Medical Image Semantic Segmentation with Limited Annotations",
        "authors": [
          "Ziyang Wang",
          "Will Zhao",
          "Zixuan Ni",
          "Yuchen Zheng"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c25091718b22384cebece2da7f30fc1702a07c76.pdf",
        "venue": "British Machine Vision Conference",
        "citationCount": 19,
        "score": 6.333333333333333,
        "summary": "",
        "keywords": []
      },
      "file_name": "c25091718b22384cebece2da7f30fc1702a07c76.pdf"
    },
    {
      "success": true,
      "doc_id": "51d04a4b15a49ab456da5198eca0a081",
      "summary": "Self-supervised learning has been widely applied to train high-quality vision transformers. Unleashing their excellent performance on memory and compute constraint devices is therefore an important research topic. However, how to distill knowledge from one self-supervised ViT to another has not yet been explored. Moreover, the existing self-supervised knowledge distillation (SSKD) methods focus on ConvNet based architectures are suboptimal for ViT knowledge distillation. In this paper, we study knowledge distillation of self-supervised vision transformers (ViT-SSKD). We show that directly distilling information from the crucial attention mechanism from teacher to student can significantly narrow the performance gap between both. In experiments on ImageNet-Subset and ImageNet-1K, we show that our method AttnDistill outperforms existing self-supervised knowledge distillation (SSKD) methods and achieves state-of-the-art k-NN accuracy compared with self-supervised learning (SSL) methods learning from scratch (with the ViT-S model). We are also the first to apply the tiny ViT-T model on self-supervised learning. Moreover, AttnDistill is independent of self-supervised learning algorithms, it can be adapted to ViT based SSL methods to improve the performance in future research. The code is here: https://github.com/wangkai930418/attndistill",
      "intriguing_abstract": "Self-supervised learning has been widely applied to train high-quality vision transformers. Unleashing their excellent performance on memory and compute constraint devices is therefore an important research topic. However, how to distill knowledge from one self-supervised ViT to another has not yet been explored. Moreover, the existing self-supervised knowledge distillation (SSKD) methods focus on ConvNet based architectures are suboptimal for ViT knowledge distillation. In this paper, we study knowledge distillation of self-supervised vision transformers (ViT-SSKD). We show that directly distilling information from the crucial attention mechanism from teacher to student can significantly narrow the performance gap between both. In experiments on ImageNet-Subset and ImageNet-1K, we show that our method AttnDistill outperforms existing self-supervised knowledge distillation (SSKD) methods and achieves state-of-the-art k-NN accuracy compared with self-supervised learning (SSL) methods learning from scratch (with the ViT-S model). We are also the first to apply the tiny ViT-T model on self-supervised learning. Moreover, AttnDistill is independent of self-supervised learning algorithms, it can be adapted to ViT based SSL methods to improve the performance in future research. The code is here: https://github.com/wangkai930418/attndistill",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/cec0cbc2dd6d7975714110632b6bfcb5c1927ec3.pdf",
      "citation_key": "wang2022pee",
      "metadata": {
        "title": "Attention Distillation: self-supervised vision transformer students need more guidance",
        "authors": [
          "Kai Wang",
          "Fei Yang",
          "Joost van de Weijer"
        ],
        "published_date": "2022",
        "abstract": "Self-supervised learning has been widely applied to train high-quality vision transformers. Unleashing their excellent performance on memory and compute constraint devices is therefore an important research topic. However, how to distill knowledge from one self-supervised ViT to another has not yet been explored. Moreover, the existing self-supervised knowledge distillation (SSKD) methods focus on ConvNet based architectures are suboptimal for ViT knowledge distillation. In this paper, we study knowledge distillation of self-supervised vision transformers (ViT-SSKD). We show that directly distilling information from the crucial attention mechanism from teacher to student can significantly narrow the performance gap between both. In experiments on ImageNet-Subset and ImageNet-1K, we show that our method AttnDistill outperforms existing self-supervised knowledge distillation (SSKD) methods and achieves state-of-the-art k-NN accuracy compared with self-supervised learning (SSL) methods learning from scratch (with the ViT-S model). We are also the first to apply the tiny ViT-T model on self-supervised learning. Moreover, AttnDistill is independent of self-supervised learning algorithms, it can be adapted to ViT based SSL methods to improve the performance in future research. The code is here: https://github.com/wangkai930418/attndistill",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/cec0cbc2dd6d7975714110632b6bfcb5c1927ec3.pdf",
        "venue": "British Machine Vision Conference",
        "citationCount": 19,
        "score": 6.333333333333333,
        "summary": "Self-supervised learning has been widely applied to train high-quality vision transformers. Unleashing their excellent performance on memory and compute constraint devices is therefore an important research topic. However, how to distill knowledge from one self-supervised ViT to another has not yet been explored. Moreover, the existing self-supervised knowledge distillation (SSKD) methods focus on ConvNet based architectures are suboptimal for ViT knowledge distillation. In this paper, we study knowledge distillation of self-supervised vision transformers (ViT-SSKD). We show that directly distilling information from the crucial attention mechanism from teacher to student can significantly narrow the performance gap between both. In experiments on ImageNet-Subset and ImageNet-1K, we show that our method AttnDistill outperforms existing self-supervised knowledge distillation (SSKD) methods and achieves state-of-the-art k-NN accuracy compared with self-supervised learning (SSL) methods learning from scratch (with the ViT-S model). We are also the first to apply the tiny ViT-T model on self-supervised learning. Moreover, AttnDistill is independent of self-supervised learning algorithms, it can be adapted to ViT based SSL methods to improve the performance in future research. The code is here: https://github.com/wangkai930418/attndistill",
        "keywords": []
      },
      "file_name": "cec0cbc2dd6d7975714110632b6bfcb5c1927ec3.pdf"
    },
    {
      "success": true,
      "doc_id": "76345212e9e060aebdf6dc41b970e6e5",
      "summary": "With the recent developments of transformer-based architecture in the image classification domain, the initial Vision Transformer (ViT) model has shown promising results compared to traditional CNN models. Inspired by this, this article reports on the efficacy of transformer-based models on remote sensing images for land cover classification. Our approach applies a variation of the vision transformer named the Swin (Shifted Window) Transformer model for analysis. This is a hierarchical transformer model that computes the representation with shifted windows. Results include an extensive study on the performance of this transformer for three different remote sensing datasets: EuroSat, NWPU-RESISC45, and AID. Findings indicate that the Swin architecture outperforms current state-of-the-art approaches for accurately classifying remote sensing images. Comparative analyses provide insights on the specific margin of improvement and an understanding of the prospect these transformer architectures have for improving image classification tasks of this type.",
      "intriguing_abstract": "With the recent developments of transformer-based architecture in the image classification domain, the initial Vision Transformer (ViT) model has shown promising results compared to traditional CNN models. Inspired by this, this article reports on the efficacy of transformer-based models on remote sensing images for land cover classification. Our approach applies a variation of the vision transformer named the Swin (Shifted Window) Transformer model for analysis. This is a hierarchical transformer model that computes the representation with shifted windows. Results include an extensive study on the performance of this transformer for three different remote sensing datasets: EuroSat, NWPU-RESISC45, and AID. Findings indicate that the Swin architecture outperforms current state-of-the-art approaches for accurately classifying remote sensing images. Comparative analyses provide insights on the specific margin of improvement and an understanding of the prospect these transformer architectures have for improving image classification tasks of this type.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/ef93c81f90154a022e164be2f83c6cf6e602f33d.pdf",
      "citation_key": "jannat20228u6",
      "metadata": {
        "title": "Improving Classification of Remotely Sensed Images with the Swin Transformer",
        "authors": [
          "Fatema-E- Jannat",
          "A. Willis"
        ],
        "published_date": "2022",
        "abstract": "With the recent developments of transformer-based architecture in the image classification domain, the initial Vision Transformer (ViT) model has shown promising results compared to traditional CNN models. Inspired by this, this article reports on the efficacy of transformer-based models on remote sensing images for land cover classification. Our approach applies a variation of the vision transformer named the Swin (Shifted Window) Transformer model for analysis. This is a hierarchical transformer model that computes the representation with shifted windows. Results include an extensive study on the performance of this transformer for three different remote sensing datasets: EuroSat, NWPU-RESISC45, and AID. Findings indicate that the Swin architecture outperforms current state-of-the-art approaches for accurately classifying remote sensing images. Comparative analyses provide insights on the specific margin of improvement and an understanding of the prospect these transformer architectures have for improving image classification tasks of this type.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ef93c81f90154a022e164be2f83c6cf6e602f33d.pdf",
        "venue": "SoutheastCon",
        "citationCount": 18,
        "score": 6.0,
        "summary": "With the recent developments of transformer-based architecture in the image classification domain, the initial Vision Transformer (ViT) model has shown promising results compared to traditional CNN models. Inspired by this, this article reports on the efficacy of transformer-based models on remote sensing images for land cover classification. Our approach applies a variation of the vision transformer named the Swin (Shifted Window) Transformer model for analysis. This is a hierarchical transformer model that computes the representation with shifted windows. Results include an extensive study on the performance of this transformer for three different remote sensing datasets: EuroSat, NWPU-RESISC45, and AID. Findings indicate that the Swin architecture outperforms current state-of-the-art approaches for accurately classifying remote sensing images. Comparative analyses provide insights on the specific margin of improvement and an understanding of the prospect these transformer architectures have for improving image classification tasks of this type.",
        "keywords": []
      },
      "file_name": "ef93c81f90154a022e164be2f83c6cf6e602f33d.pdf"
    },
    {
      "success": true,
      "doc_id": "7cc000bf76b38953d8425b9c6eedcd0a",
      "summary": "Learning with little data is challenging but often inevitable in various application scenarios where the labeled data is limited and costly. Recently, few-shot learning (FSL) gained increasing attention because of its generalizability of prior knowledge to new tasks that contain only a few samples. However, for data-intensive models such as vision transformer (ViT), current fine-tuning based FSL approaches are inefficient in knowledge generalization and thus degenerate the downstream task performances. In this paper, we propose a novel mask-guided vision transformer (MG-ViT) to achieve an effective and efficient FSL on ViT model. The key idea is to apply a mask on image patches to screen out the task-irrelevant ones and to guide the ViT to focus on task-relevant and discriminative patches during FSL. Particularly, MG-ViT only introduces an additional mask operation and a residual connection, enabling the inheritance of parameters from pre-trained ViT without any other cost. To optimally select representative few-shot samples, we also include an active learning based sample selection method to further improve the generalizability of MG-ViT based FSL. We evaluate the proposed MG-ViT on both Agri-ImageNet classification task and ACFR apple detection task with gradient-weighted class activation mapping (Grad-CAM) as the mask. The experimental results show that the MG-ViT model significantly improves the performance when compared with general fine-tuning based ViT models, providing novel insights and a concrete approach towards generalizing data-intensive and large-scale deep learning models for FSL.",
      "intriguing_abstract": "Learning with little data is challenging but often inevitable in various application scenarios where the labeled data is limited and costly. Recently, few-shot learning (FSL) gained increasing attention because of its generalizability of prior knowledge to new tasks that contain only a few samples. However, for data-intensive models such as vision transformer (ViT), current fine-tuning based FSL approaches are inefficient in knowledge generalization and thus degenerate the downstream task performances. In this paper, we propose a novel mask-guided vision transformer (MG-ViT) to achieve an effective and efficient FSL on ViT model. The key idea is to apply a mask on image patches to screen out the task-irrelevant ones and to guide the ViT to focus on task-relevant and discriminative patches during FSL. Particularly, MG-ViT only introduces an additional mask operation and a residual connection, enabling the inheritance of parameters from pre-trained ViT without any other cost. To optimally select representative few-shot samples, we also include an active learning based sample selection method to further improve the generalizability of MG-ViT based FSL. We evaluate the proposed MG-ViT on both Agri-ImageNet classification task and ACFR apple detection task with gradient-weighted class activation mapping (Grad-CAM) as the mask. The experimental results show that the MG-ViT model significantly improves the performance when compared with general fine-tuning based ViT models, providing novel insights and a concrete approach towards generalizing data-intensive and large-scale deep learning models for FSL.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/6a8d4a97ba3eafcb85d2d2a57d7d514bb592dbd0.pdf",
      "citation_key": "chen2022r27",
      "metadata": {
        "title": "Mask-guided Vision Transformer (MG-ViT) for Few-Shot Learning",
        "authors": [
          "Yuzhong Chen",
          "Zhe Xiao",
          "Lin Zhao",
          "Lu Zhang",
          "Haixing Dai",
          "David Liu",
          "Zihao Wu",
          "Changhe Li",
          "Tuo Zhang",
          "Changying Li",
          "Dajiang Zhu",
          "Tianming Liu",
          "Xi Jiang"
        ],
        "published_date": "2022",
        "abstract": "Learning with little data is challenging but often inevitable in various application scenarios where the labeled data is limited and costly. Recently, few-shot learning (FSL) gained increasing attention because of its generalizability of prior knowledge to new tasks that contain only a few samples. However, for data-intensive models such as vision transformer (ViT), current fine-tuning based FSL approaches are inefficient in knowledge generalization and thus degenerate the downstream task performances. In this paper, we propose a novel mask-guided vision transformer (MG-ViT) to achieve an effective and efficient FSL on ViT model. The key idea is to apply a mask on image patches to screen out the task-irrelevant ones and to guide the ViT to focus on task-relevant and discriminative patches during FSL. Particularly, MG-ViT only introduces an additional mask operation and a residual connection, enabling the inheritance of parameters from pre-trained ViT without any other cost. To optimally select representative few-shot samples, we also include an active learning based sample selection method to further improve the generalizability of MG-ViT based FSL. We evaluate the proposed MG-ViT on both Agri-ImageNet classification task and ACFR apple detection task with gradient-weighted class activation mapping (Grad-CAM) as the mask. The experimental results show that the MG-ViT model significantly improves the performance when compared with general fine-tuning based ViT models, providing novel insights and a concrete approach towards generalizing data-intensive and large-scale deep learning models for FSL.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6a8d4a97ba3eafcb85d2d2a57d7d514bb592dbd0.pdf",
        "venue": "arXiv.org",
        "citationCount": 18,
        "score": 6.0,
        "summary": "Learning with little data is challenging but often inevitable in various application scenarios where the labeled data is limited and costly. Recently, few-shot learning (FSL) gained increasing attention because of its generalizability of prior knowledge to new tasks that contain only a few samples. However, for data-intensive models such as vision transformer (ViT), current fine-tuning based FSL approaches are inefficient in knowledge generalization and thus degenerate the downstream task performances. In this paper, we propose a novel mask-guided vision transformer (MG-ViT) to achieve an effective and efficient FSL on ViT model. The key idea is to apply a mask on image patches to screen out the task-irrelevant ones and to guide the ViT to focus on task-relevant and discriminative patches during FSL. Particularly, MG-ViT only introduces an additional mask operation and a residual connection, enabling the inheritance of parameters from pre-trained ViT without any other cost. To optimally select representative few-shot samples, we also include an active learning based sample selection method to further improve the generalizability of MG-ViT based FSL. We evaluate the proposed MG-ViT on both Agri-ImageNet classification task and ACFR apple detection task with gradient-weighted class activation mapping (Grad-CAM) as the mask. The experimental results show that the MG-ViT model significantly improves the performance when compared with general fine-tuning based ViT models, providing novel insights and a concrete approach towards generalizing data-intensive and large-scale deep learning models for FSL.",
        "keywords": []
      },
      "file_name": "6a8d4a97ba3eafcb85d2d2a57d7d514bb592dbd0.pdf"
    },
    {
      "success": true,
      "doc_id": "bc754b856017dbf0ae320eb30da2bfa5",
      "summary": "Genitourinary syndrome of menopause (GSM) is a disease caused by a physiological decline in estrogen levels, and it can negatively affect a womans overall health and quality of life in terms of sexual function. Real-time optical biopsy images can now be obtained with optical coherence tomography (OCT) systems. In this study, we introduce vision transformer (ViT) to the field of medical OCT images for the first time and propose a deep learning-based approach for GSM lesion screening. Specifically, we first build a GSM dataset to train and evaluate the experimental model performance. The study aims to propose a method that combines null convolution with a deep convolutional adversarial generative network classifier to generate the samples needed for training to alleviate the hindrance of such problems, in response to certain practical problems, such as category imbalance that occur during data collection. Next, the experiments present ViT PLUS (ViT-P) for the vaginal OCT image classification task used, which effectively improves the shortcomings of ViT in extracting Patch Embedding using a multibranch convolutional neural network combined with a channel attention mechanism. The clinical images acquired by the OCT device are then automatically classified on the basis of the OCT device to reduce the medical workload of gynecologists. Experimental results show that the ViT-P model outperforms the CNN model and ViT for case screening in the GSM and UCSD datasets, and the accuracy can reach 99.9% and 99.69%, respectively.",
      "intriguing_abstract": "Genitourinary syndrome of menopause (GSM) is a disease caused by a physiological decline in estrogen levels, and it can negatively affect a womans overall health and quality of life in terms of sexual function. Real-time optical biopsy images can now be obtained with optical coherence tomography (OCT) systems. In this study, we introduce vision transformer (ViT) to the field of medical OCT images for the first time and propose a deep learning-based approach for GSM lesion screening. Specifically, we first build a GSM dataset to train and evaluate the experimental model performance. The study aims to propose a method that combines null convolution with a deep convolutional adversarial generative network classifier to generate the samples needed for training to alleviate the hindrance of such problems, in response to certain practical problems, such as category imbalance that occur during data collection. Next, the experiments present ViT PLUS (ViT-P) for the vaginal OCT image classification task used, which effectively improves the shortcomings of ViT in extracting Patch Embedding using a multibranch convolutional neural network combined with a channel attention mechanism. The clinical images acquired by the OCT device are then automatically classified on the basis of the OCT device to reduce the medical workload of gynecologists. Experimental results show that the ViT-P model outperforms the CNN model and ViT for case screening in the GSM and UCSD datasets, and the accuracy can reach 99.9% and 99.69%, respectively.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/64143b37ae41085c4907e344ff3a2362a3051d0c.pdf",
      "citation_key": "wang2021p2r",
      "metadata": {
        "title": "ViT-P: Classification of Genitourinary Syndrome of Menopause From OCT Images Based on Vision Transformer Models",
        "authors": [
          "Haoran Wang",
          "Yanju Ji",
          "Kaiwen Song",
          "Mingyang Sun",
          "Peitong Lv",
          "Tianyu Zhang"
        ],
        "published_date": "2021",
        "abstract": "Genitourinary syndrome of menopause (GSM) is a disease caused by a physiological decline in estrogen levels, and it can negatively affect a womans overall health and quality of life in terms of sexual function. Real-time optical biopsy images can now be obtained with optical coherence tomography (OCT) systems. In this study, we introduce vision transformer (ViT) to the field of medical OCT images for the first time and propose a deep learning-based approach for GSM lesion screening. Specifically, we first build a GSM dataset to train and evaluate the experimental model performance. The study aims to propose a method that combines null convolution with a deep convolutional adversarial generative network classifier to generate the samples needed for training to alleviate the hindrance of such problems, in response to certain practical problems, such as category imbalance that occur during data collection. Next, the experiments present ViT PLUS (ViT-P) for the vaginal OCT image classification task used, which effectively improves the shortcomings of ViT in extracting Patch Embedding using a multibranch convolutional neural network combined with a channel attention mechanism. The clinical images acquired by the OCT device are then automatically classified on the basis of the OCT device to reduce the medical workload of gynecologists. Experimental results show that the ViT-P model outperforms the CNN model and ViT for case screening in the GSM and UCSD datasets, and the accuracy can reach 99.9% and 99.69%, respectively.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/64143b37ae41085c4907e344ff3a2362a3051d0c.pdf",
        "venue": "IEEE Transactions on Instrumentation and Measurement",
        "citationCount": 23,
        "score": 5.75,
        "summary": "Genitourinary syndrome of menopause (GSM) is a disease caused by a physiological decline in estrogen levels, and it can negatively affect a womans overall health and quality of life in terms of sexual function. Real-time optical biopsy images can now be obtained with optical coherence tomography (OCT) systems. In this study, we introduce vision transformer (ViT) to the field of medical OCT images for the first time and propose a deep learning-based approach for GSM lesion screening. Specifically, we first build a GSM dataset to train and evaluate the experimental model performance. The study aims to propose a method that combines null convolution with a deep convolutional adversarial generative network classifier to generate the samples needed for training to alleviate the hindrance of such problems, in response to certain practical problems, such as category imbalance that occur during data collection. Next, the experiments present ViT PLUS (ViT-P) for the vaginal OCT image classification task used, which effectively improves the shortcomings of ViT in extracting Patch Embedding using a multibranch convolutional neural network combined with a channel attention mechanism. The clinical images acquired by the OCT device are then automatically classified on the basis of the OCT device to reduce the medical workload of gynecologists. Experimental results show that the ViT-P model outperforms the CNN model and ViT for case screening in the GSM and UCSD datasets, and the accuracy can reach 99.9% and 99.69%, respectively.",
        "keywords": []
      },
      "file_name": "64143b37ae41085c4907e344ff3a2362a3051d0c.pdf"
    },
    {
      "success": true,
      "doc_id": "bb64f1df81863756bd76dc4ba57667f0",
      "summary": "Crowd estimation is a very challenging problem. The most recent study tries to exploit auditory information to aid the visual models, however, the performance is limited due to the lack of an effective approach for feature extraction and integration. The paper proposes a new audiovisual multi-task network to address the critical challenges in crowd counting by effectively utilizing both visual and audio inputs for better modalities association and productive feature extraction. The proposed network introduces the notion of auxiliary and explicit image patch-importance ranking (PIR) and patch-wise crowd estimate (PCE) information to produce a third (run-time) modality. These modalities (audio, visual, run-time) undergo a transformer-inspired cross-modality co-attention mechanism to finally output the crowd estimate. To acquire rich visual features, we propose a multi-branch structure with transformer-style fusion in-between. Extensive experimental evaluations show that the proposed scheme outperforms the state-of-the-art networks under all evaluation settings with up to 33.8% improvement. We also analyze and compare the vision-only variant of our network and empirically demonstrate its superiority over previous approaches.",
      "intriguing_abstract": "Crowd estimation is a very challenging problem. The most recent study tries to exploit auditory information to aid the visual models, however, the performance is limited due to the lack of an effective approach for feature extraction and integration. The paper proposes a new audiovisual multi-task network to address the critical challenges in crowd counting by effectively utilizing both visual and audio inputs for better modalities association and productive feature extraction. The proposed network introduces the notion of auxiliary and explicit image patch-importance ranking (PIR) and patch-wise crowd estimate (PCE) information to produce a third (run-time) modality. These modalities (audio, visual, run-time) undergo a transformer-inspired cross-modality co-attention mechanism to finally output the crowd estimate. To acquire rich visual features, we propose a multi-branch structure with transformer-style fusion in-between. Extensive experimental evaluations show that the proposed scheme outperforms the state-of-the-art networks under all evaluation settings with up to 33.8% improvement. We also analyze and compare the vision-only variant of our network and empirically demonstrate its superiority over previous approaches.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/dcd8617200724f0aa998276be339ff4af589ee42.pdf",
      "citation_key": "sajid2021xb6",
      "metadata": {
        "title": "Audio-Visual Transformer Based Crowd Counting",
        "authors": [
          "Usman Sajid",
          "Xiangyu Chen",
          "Hasan Sajid",
          "Taejoon Kim",
          "Guanghui Wang"
        ],
        "published_date": "2021",
        "abstract": "Crowd estimation is a very challenging problem. The most recent study tries to exploit auditory information to aid the visual models, however, the performance is limited due to the lack of an effective approach for feature extraction and integration. The paper proposes a new audiovisual multi-task network to address the critical challenges in crowd counting by effectively utilizing both visual and audio inputs for better modalities association and productive feature extraction. The proposed network introduces the notion of auxiliary and explicit image patch-importance ranking (PIR) and patch-wise crowd estimate (PCE) information to produce a third (run-time) modality. These modalities (audio, visual, run-time) undergo a transformer-inspired cross-modality co-attention mechanism to finally output the crowd estimate. To acquire rich visual features, we propose a multi-branch structure with transformer-style fusion in-between. Extensive experimental evaluations show that the proposed scheme outperforms the state-of-the-art networks under all evaluation settings with up to 33.8% improvement. We also analyze and compare the vision-only variant of our network and empirically demonstrate its superiority over previous approaches.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/dcd8617200724f0aa998276be339ff4af589ee42.pdf",
        "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
        "citationCount": 23,
        "score": 5.75,
        "summary": "Crowd estimation is a very challenging problem. The most recent study tries to exploit auditory information to aid the visual models, however, the performance is limited due to the lack of an effective approach for feature extraction and integration. The paper proposes a new audiovisual multi-task network to address the critical challenges in crowd counting by effectively utilizing both visual and audio inputs for better modalities association and productive feature extraction. The proposed network introduces the notion of auxiliary and explicit image patch-importance ranking (PIR) and patch-wise crowd estimate (PCE) information to produce a third (run-time) modality. These modalities (audio, visual, run-time) undergo a transformer-inspired cross-modality co-attention mechanism to finally output the crowd estimate. To acquire rich visual features, we propose a multi-branch structure with transformer-style fusion in-between. Extensive experimental evaluations show that the proposed scheme outperforms the state-of-the-art networks under all evaluation settings with up to 33.8% improvement. We also analyze and compare the vision-only variant of our network and empirically demonstrate its superiority over previous approaches.",
        "keywords": []
      },
      "file_name": "dcd8617200724f0aa998276be339ff4af589ee42.pdf"
    },
    {
      "success": true,
      "doc_id": "86f790491368df90141d0787e660a293",
      "summary": "Image demosaicing is problem of interpolating full-resolution color images from raw sensor (color filter array) data. During last decade, deep neural networks have been widely used in image restoration, and in particular, in demosaicing, attaining significant performance improvement. In recent years, vision transformers have been designed and successfully used in various computer vision applications. One of the recent methods of image restoration based on a Swin Transformer (ST), SwinIR, demonstrates state-of-the-art performance with a smaller number of parameters than neural network-based methods. Inspired by the success of SwinIR, we propose in this paper a novel Swin Transformer-based network for image demosaicing, called RSTCANet. To extract image features, RSTCANet stacks several residual Swin Transformer Channel Attention blocks (RSTCAB), introducing the channel attention for each two successive ST blocks. Extensive experiments demonstrate that RSTCANet outperforms state-of-the-art image demosaicing methods, and has a smaller number of parameters. The source code is available at https://github.com/xingwz/RSTCANet.",
      "intriguing_abstract": "Image demosaicing is problem of interpolating full-resolution color images from raw sensor (color filter array) data. During last decade, deep neural networks have been widely used in image restoration, and in particular, in demosaicing, attaining significant performance improvement. In recent years, vision transformers have been designed and successfully used in various computer vision applications. One of the recent methods of image restoration based on a Swin Transformer (ST), SwinIR, demonstrates state-of-the-art performance with a smaller number of parameters than neural network-based methods. Inspired by the success of SwinIR, we propose in this paper a novel Swin Transformer-based network for image demosaicing, called RSTCANet. To extract image features, RSTCANet stacks several residual Swin Transformer Channel Attention blocks (RSTCAB), introducing the channel attention for each two successive ST blocks. Extensive experiments demonstrate that RSTCANet outperforms state-of-the-art image demosaicing methods, and has a smaller number of parameters. The source code is available at https://github.com/xingwz/RSTCANet.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/46880aeca86695ca3117cc04f6bd9edaf088111b.pdf",
      "citation_key": "xing2022kqr",
      "metadata": {
        "title": "Residual Swin Transformer Channel Attention Network for Image Demosaicing",
        "authors": [
          "W. Xing",
          "K. Egiazarian"
        ],
        "published_date": "2022",
        "abstract": "Image demosaicing is problem of interpolating full-resolution color images from raw sensor (color filter array) data. During last decade, deep neural networks have been widely used in image restoration, and in particular, in demosaicing, attaining significant performance improvement. In recent years, vision transformers have been designed and successfully used in various computer vision applications. One of the recent methods of image restoration based on a Swin Transformer (ST), SwinIR, demonstrates state-of-the-art performance with a smaller number of parameters than neural network-based methods. Inspired by the success of SwinIR, we propose in this paper a novel Swin Transformer-based network for image demosaicing, called RSTCANet. To extract image features, RSTCANet stacks several residual Swin Transformer Channel Attention blocks (RSTCAB), introducing the channel attention for each two successive ST blocks. Extensive experiments demonstrate that RSTCANet outperforms state-of-the-art image demosaicing methods, and has a smaller number of parameters. The source code is available at https://github.com/xingwz/RSTCANet.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/46880aeca86695ca3117cc04f6bd9edaf088111b.pdf",
        "venue": "European Workshop on Visual Information Processing",
        "citationCount": 17,
        "score": 5.666666666666666,
        "summary": "Image demosaicing is problem of interpolating full-resolution color images from raw sensor (color filter array) data. During last decade, deep neural networks have been widely used in image restoration, and in particular, in demosaicing, attaining significant performance improvement. In recent years, vision transformers have been designed and successfully used in various computer vision applications. One of the recent methods of image restoration based on a Swin Transformer (ST), SwinIR, demonstrates state-of-the-art performance with a smaller number of parameters than neural network-based methods. Inspired by the success of SwinIR, we propose in this paper a novel Swin Transformer-based network for image demosaicing, called RSTCANet. To extract image features, RSTCANet stacks several residual Swin Transformer Channel Attention blocks (RSTCAB), introducing the channel attention for each two successive ST blocks. Extensive experiments demonstrate that RSTCANet outperforms state-of-the-art image demosaicing methods, and has a smaller number of parameters. The source code is available at https://github.com/xingwz/RSTCANet.",
        "keywords": []
      },
      "file_name": "46880aeca86695ca3117cc04f6bd9edaf088111b.pdf"
    },
    {
      "success": true,
      "doc_id": "bd31551bf6f4d2ed1319e89714310c95",
      "summary": "Abstract Objectives The first objective of this study was to implement and assess the performance and reliability of a vision transformer (ViT)-based deep-learning model, an off-the-shelf artificial intelligence solution, for identifying distinct signs of microangiopathy in nailfold capilloroscopy (NFC) images of patients with SSc. The second objective was to compare the ViTs analysis performance with that of practising rheumatologists. Methods NFC images of patients prospectively enrolled in our European Scleroderma Trials and Research group (EUSTAR) and Very Early Diagnosis of Systemic Sclerosis (VEDOSS) local registries were used. The primary outcome investigated was the ViTs classification performance for identifying disease-associated changes (enlarged capillaries, giant capillaries, capillary loss, microhaemorrhages) and the presence of the scleroderma pattern in these images using a cross-fold validation setting. The secondary outcome involved a comparison of the ViTs performance vs that of rheumatologists on a reliability set, consisting of a subset of 464 NFC images with majority votederived ground-truth labels. Results We analysed 17126 NFC images derived from 234 EUSTAR and 55 VEDOSS patients. The ViT had good performance in identifying the various microangiopathic changes in capillaries by NFC [area under the curve (AUC) from 81.8% to 84.5%]. In the reliability set, the rheumatologists reached a higher average accuracy, as well as a better trade-off between sensitivity and specificity compared with the ViT. However, the annotators performance was variable, and one out of four rheumatologists showed equal or lower classification measures compared with the ViT. Conclusions The ViT is a modern, well-performing and readily available tool for assessing patterns of microangiopathy on NFC images, and it may assist rheumatologists in generating consistent and high-quality NFC reports; however, the final diagnosis of a scleroderma pattern in any individual case needs the judgement of an experienced observer.",
      "intriguing_abstract": "Abstract Objectives The first objective of this study was to implement and assess the performance and reliability of a vision transformer (ViT)-based deep-learning model, an off-the-shelf artificial intelligence solution, for identifying distinct signs of microangiopathy in nailfold capilloroscopy (NFC) images of patients with SSc. The second objective was to compare the ViTs analysis performance with that of practising rheumatologists. Methods NFC images of patients prospectively enrolled in our European Scleroderma Trials and Research group (EUSTAR) and Very Early Diagnosis of Systemic Sclerosis (VEDOSS) local registries were used. The primary outcome investigated was the ViTs classification performance for identifying disease-associated changes (enlarged capillaries, giant capillaries, capillary loss, microhaemorrhages) and the presence of the scleroderma pattern in these images using a cross-fold validation setting. The secondary outcome involved a comparison of the ViTs performance vs that of rheumatologists on a reliability set, consisting of a subset of 464 NFC images with majority votederived ground-truth labels. Results We analysed 17126 NFC images derived from 234 EUSTAR and 55 VEDOSS patients. The ViT had good performance in identifying the various microangiopathic changes in capillaries by NFC [area under the curve (AUC) from 81.8% to 84.5%]. In the reliability set, the rheumatologists reached a higher average accuracy, as well as a better trade-off between sensitivity and specificity compared with the ViT. However, the annotators performance was variable, and one out of four rheumatologists showed equal or lower classification measures compared with the ViT. Conclusions The ViT is a modern, well-performing and readily available tool for assessing patterns of microangiopathy on NFC images, and it may assist rheumatologists in generating consistent and high-quality NFC reports; however, the final diagnosis of a scleroderma pattern in any individual case needs the judgement of an experienced observer.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/7e0dd543471b66374fbf1639b9894d3d502533b6.pdf",
      "citation_key": "garaiman2022xwd",
      "metadata": {
        "title": "Vision transformer assisting rheumatologists in screening for capillaroscopy changes in systemic sclerosis: an artificial intelligence model",
        "authors": [
          "A. Garaiman",
          "F. Nooralahzadeh",
          "C. Mihai",
          "Nicolas Andres Perez Gonzalez",
          "N. Gkikopoulos",
          "M. Becker",
          "O. Distler",
          "M. Krauthammer",
          "B. Maurer"
        ],
        "published_date": "2022",
        "abstract": "Abstract Objectives The first objective of this study was to implement and assess the performance and reliability of a vision transformer (ViT)-based deep-learning model, an off-the-shelf artificial intelligence solution, for identifying distinct signs of microangiopathy in nailfold capilloroscopy (NFC) images of patients with SSc. The second objective was to compare the ViTs analysis performance with that of practising rheumatologists. Methods NFC images of patients prospectively enrolled in our European Scleroderma Trials and Research group (EUSTAR) and Very Early Diagnosis of Systemic Sclerosis (VEDOSS) local registries were used. The primary outcome investigated was the ViTs classification performance for identifying disease-associated changes (enlarged capillaries, giant capillaries, capillary loss, microhaemorrhages) and the presence of the scleroderma pattern in these images using a cross-fold validation setting. The secondary outcome involved a comparison of the ViTs performance vs that of rheumatologists on a reliability set, consisting of a subset of 464 NFC images with majority votederived ground-truth labels. Results We analysed 17126 NFC images derived from 234 EUSTAR and 55 VEDOSS patients. The ViT had good performance in identifying the various microangiopathic changes in capillaries by NFC [area under the curve (AUC) from 81.8% to 84.5%]. In the reliability set, the rheumatologists reached a higher average accuracy, as well as a better trade-off between sensitivity and specificity compared with the ViT. However, the annotators performance was variable, and one out of four rheumatologists showed equal or lower classification measures compared with the ViT. Conclusions The ViT is a modern, well-performing and readily available tool for assessing patterns of microangiopathy on NFC images, and it may assist rheumatologists in generating consistent and high-quality NFC reports; however, the final diagnosis of a scleroderma pattern in any individual case needs the judgement of an experienced observer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7e0dd543471b66374fbf1639b9894d3d502533b6.pdf",
        "venue": "Rheumatology",
        "citationCount": 17,
        "score": 5.666666666666666,
        "summary": "Abstract Objectives The first objective of this study was to implement and assess the performance and reliability of a vision transformer (ViT)-based deep-learning model, an off-the-shelf artificial intelligence solution, for identifying distinct signs of microangiopathy in nailfold capilloroscopy (NFC) images of patients with SSc. The second objective was to compare the ViTs analysis performance with that of practising rheumatologists. Methods NFC images of patients prospectively enrolled in our European Scleroderma Trials and Research group (EUSTAR) and Very Early Diagnosis of Systemic Sclerosis (VEDOSS) local registries were used. The primary outcome investigated was the ViTs classification performance for identifying disease-associated changes (enlarged capillaries, giant capillaries, capillary loss, microhaemorrhages) and the presence of the scleroderma pattern in these images using a cross-fold validation setting. The secondary outcome involved a comparison of the ViTs performance vs that of rheumatologists on a reliability set, consisting of a subset of 464 NFC images with majority votederived ground-truth labels. Results We analysed 17126 NFC images derived from 234 EUSTAR and 55 VEDOSS patients. The ViT had good performance in identifying the various microangiopathic changes in capillaries by NFC [area under the curve (AUC) from 81.8% to 84.5%]. In the reliability set, the rheumatologists reached a higher average accuracy, as well as a better trade-off between sensitivity and specificity compared with the ViT. However, the annotators performance was variable, and one out of four rheumatologists showed equal or lower classification measures compared with the ViT. Conclusions The ViT is a modern, well-performing and readily available tool for assessing patterns of microangiopathy on NFC images, and it may assist rheumatologists in generating consistent and high-quality NFC reports; however, the final diagnosis of a scleroderma pattern in any individual case needs the judgement of an experienced observer.",
        "keywords": []
      },
      "file_name": "7e0dd543471b66374fbf1639b9894d3d502533b6.pdf"
    },
    {
      "success": true,
      "doc_id": "0916fd539a79821537cfc2136824d970",
      "summary": "Ubiquitous accumulation of large volumes of data, and increased availability of annotated medical data in particular, has made it possible to show the many and varied benefits of deep learning to the semantic segmentation of medical images. Nevertheless, data access and annotation come at a high cost in clinician time. The power of Vision Transformer (ViT) is well-documented for generic computer vision tasks involving millions of images of every day objects, of which only relatively few have been annotated. Its translation to relatively more modest (i.e. thousands of images of) medical data is not immediately straightforward. This paper presents practical avenues for training a Computationally-Efficient Semi-Supervised Vision Transformer (CESS-ViT) for medical image segmentation task.We propose a self-attention-based image segmentation network which requires only limited computational resources. Additionally, we develop a dual pseudo-label supervision scheme for use with semi-supervision in a simple pure ViT.Our method has been evaluated on a publicly available cardiac MRI dataset with direct comparison against other semi-supervised methods. Our results illustrate the proposed ViT-based semi-supervised method outperforms the existing methods in the semantic segmentation of cardiac ventricles.",
      "intriguing_abstract": "Ubiquitous accumulation of large volumes of data, and increased availability of annotated medical data in particular, has made it possible to show the many and varied benefits of deep learning to the semantic segmentation of medical images. Nevertheless, data access and annotation come at a high cost in clinician time. The power of Vision Transformer (ViT) is well-documented for generic computer vision tasks involving millions of images of every day objects, of which only relatively few have been annotated. Its translation to relatively more modest (i.e. thousands of images of) medical data is not immediately straightforward. This paper presents practical avenues for training a Computationally-Efficient Semi-Supervised Vision Transformer (CESS-ViT) for medical image segmentation task.We propose a self-attention-based image segmentation network which requires only limited computational resources. Additionally, we develop a dual pseudo-label supervision scheme for use with semi-supervision in a simple pure ViT.Our method has been evaluated on a publicly available cardiac MRI dataset with direct comparison against other semi-supervised methods. Our results illustrate the proposed ViT-based semi-supervised method outperforms the existing methods in the semantic segmentation of cardiac ventricles.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/845a154dbcde81de52b68d73c78fad5be4af3b20.pdf",
      "citation_key": "wang2022wyu",
      "metadata": {
        "title": "Computationally-Efficient Vision Transformer for Medical Image Semantic Segmentation Via Dual Pseudo-Label Supervision",
        "authors": [
          "Ziyang Wang",
          "Nanqing Dong",
          "I. Voiculescu"
        ],
        "published_date": "2022",
        "abstract": "Ubiquitous accumulation of large volumes of data, and increased availability of annotated medical data in particular, has made it possible to show the many and varied benefits of deep learning to the semantic segmentation of medical images. Nevertheless, data access and annotation come at a high cost in clinician time. The power of Vision Transformer (ViT) is well-documented for generic computer vision tasks involving millions of images of every day objects, of which only relatively few have been annotated. Its translation to relatively more modest (i.e. thousands of images of) medical data is not immediately straightforward. This paper presents practical avenues for training a Computationally-Efficient Semi-Supervised Vision Transformer (CESS-ViT) for medical image segmentation task.We propose a self-attention-based image segmentation network which requires only limited computational resources. Additionally, we develop a dual pseudo-label supervision scheme for use with semi-supervision in a simple pure ViT.Our method has been evaluated on a publicly available cardiac MRI dataset with direct comparison against other semi-supervised methods. Our results illustrate the proposed ViT-based semi-supervised method outperforms the existing methods in the semantic segmentation of cardiac ventricles.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/845a154dbcde81de52b68d73c78fad5be4af3b20.pdf",
        "venue": "International Conference on Information Photonics",
        "citationCount": 17,
        "score": 5.666666666666666,
        "summary": "Ubiquitous accumulation of large volumes of data, and increased availability of annotated medical data in particular, has made it possible to show the many and varied benefits of deep learning to the semantic segmentation of medical images. Nevertheless, data access and annotation come at a high cost in clinician time. The power of Vision Transformer (ViT) is well-documented for generic computer vision tasks involving millions of images of every day objects, of which only relatively few have been annotated. Its translation to relatively more modest (i.e. thousands of images of) medical data is not immediately straightforward. This paper presents practical avenues for training a Computationally-Efficient Semi-Supervised Vision Transformer (CESS-ViT) for medical image segmentation task.We propose a self-attention-based image segmentation network which requires only limited computational resources. Additionally, we develop a dual pseudo-label supervision scheme for use with semi-supervision in a simple pure ViT.Our method has been evaluated on a publicly available cardiac MRI dataset with direct comparison against other semi-supervised methods. Our results illustrate the proposed ViT-based semi-supervised method outperforms the existing methods in the semantic segmentation of cardiac ventricles.",
        "keywords": []
      },
      "file_name": "845a154dbcde81de52b68d73c78fad5be4af3b20.pdf"
    },
    {
      "success": true,
      "doc_id": "64d1dbabde7cd2ce3d64ca52b5146667",
      "summary": "Vision transformers (ViT) have recently attracted considerable attentions, but the huge computational cost remains an issue for practical deployment. Previous ViT pruning methods tend to prune the model along one dimension solely, which may suffer from excessive reduction and lead to sub-optimal model quality. In contrast, we advocate a multi-dimensional ViT compression paradigm, and propose to harness the redundancy reduction from attention head, neuron and sequence dimensions jointly. Firstly, we propose a statistical dependence based pruning criterion that is generalizable to different dimensions for identifying the deleterious components. Moreover, we cast the multidimensional ViT compression as an optimization problem, objective of which is to learn an optimal pruning policy across the three dimensions while maximizing the compressed models accuracy under a computational budget. The problem is solved by an adapted Gaussian process search with expected improvement. Experimental results show that our method effectively reduces the computational cost of various ViT models. For example, our method reduces 40% FLOPs without top-1 accuracy loss for DeiT and T2T-ViT models on the ImageNet dataset, outperforming previous state-of-the-art ViT pruning methods.",
      "intriguing_abstract": "Vision transformers (ViT) have recently attracted considerable attentions, but the huge computational cost remains an issue for practical deployment. Previous ViT pruning methods tend to prune the model along one dimension solely, which may suffer from excessive reduction and lead to sub-optimal model quality. In contrast, we advocate a multi-dimensional ViT compression paradigm, and propose to harness the redundancy reduction from attention head, neuron and sequence dimensions jointly. Firstly, we propose a statistical dependence based pruning criterion that is generalizable to different dimensions for identifying the deleterious components. Moreover, we cast the multidimensional ViT compression as an optimization problem, objective of which is to learn an optimal pruning policy across the three dimensions while maximizing the compressed models accuracy under a computational budget. The problem is solved by an adapted Gaussian process search with expected improvement. Experimental results show that our method effectively reduces the computational cost of various ViT models. For example, our method reduces 40% FLOPs without top-1 accuracy loss for DeiT and T2T-ViT models on the ImageNet dataset, outperforming previous state-of-the-art ViT pruning methods.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/6f4093a7ad5378e8cd3b73a52fbec80b784c107d.pdf",
      "citation_key": "hou2022ver",
      "metadata": {
        "title": "Multi-Dimensional Vision Transformer Compression via Dependency Guided Gaussian Process Search",
        "authors": [
          "Zejiang Hou",
          "S. Kung"
        ],
        "published_date": "2022",
        "abstract": "Vision transformers (ViT) have recently attracted considerable attentions, but the huge computational cost remains an issue for practical deployment. Previous ViT pruning methods tend to prune the model along one dimension solely, which may suffer from excessive reduction and lead to sub-optimal model quality. In contrast, we advocate a multi-dimensional ViT compression paradigm, and propose to harness the redundancy reduction from attention head, neuron and sequence dimensions jointly. Firstly, we propose a statistical dependence based pruning criterion that is generalizable to different dimensions for identifying the deleterious components. Moreover, we cast the multidimensional ViT compression as an optimization problem, objective of which is to learn an optimal pruning policy across the three dimensions while maximizing the compressed models accuracy under a computational budget. The problem is solved by an adapted Gaussian process search with expected improvement. Experimental results show that our method effectively reduces the computational cost of various ViT models. For example, our method reduces 40% FLOPs without top-1 accuracy loss for DeiT and T2T-ViT models on the ImageNet dataset, outperforming previous state-of-the-art ViT pruning methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6f4093a7ad5378e8cd3b73a52fbec80b784c107d.pdf",
        "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
        "citationCount": 17,
        "score": 5.666666666666666,
        "summary": "Vision transformers (ViT) have recently attracted considerable attentions, but the huge computational cost remains an issue for practical deployment. Previous ViT pruning methods tend to prune the model along one dimension solely, which may suffer from excessive reduction and lead to sub-optimal model quality. In contrast, we advocate a multi-dimensional ViT compression paradigm, and propose to harness the redundancy reduction from attention head, neuron and sequence dimensions jointly. Firstly, we propose a statistical dependence based pruning criterion that is generalizable to different dimensions for identifying the deleterious components. Moreover, we cast the multidimensional ViT compression as an optimization problem, objective of which is to learn an optimal pruning policy across the three dimensions while maximizing the compressed models accuracy under a computational budget. The problem is solved by an adapted Gaussian process search with expected improvement. Experimental results show that our method effectively reduces the computational cost of various ViT models. For example, our method reduces 40% FLOPs without top-1 accuracy loss for DeiT and T2T-ViT models on the ImageNet dataset, outperforming previous state-of-the-art ViT pruning methods.",
        "keywords": []
      },
      "file_name": "6f4093a7ad5378e8cd3b73a52fbec80b784c107d.pdf"
    },
    {
      "success": true,
      "doc_id": "97277e61a9972d97cfd43c2bb5f735e1",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/50405c1ee26c97ed5b9a54c7179317a424e6e471.pdf",
      "citation_key": "agilandeeswari202273m",
      "metadata": {
        "title": "SWIN transformer based contrastive self-supervised learning for animal detection and classification",
        "authors": [
          "L. Agilandeeswari",
          "S. D. Meena"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/50405c1ee26c97ed5b9a54c7179317a424e6e471.pdf",
        "venue": "Multimedia tools and applications",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "",
        "keywords": []
      },
      "file_name": "50405c1ee26c97ed5b9a54c7179317a424e6e471.pdf"
    },
    {
      "success": true,
      "doc_id": "5b4d833cb32d907e817759a6fb107141",
      "summary": "In hyperspectral images (HSIs), spatial context provides complementary information to abundant spectral features. In this paper, a united spectral-spatial framework named HTD-ViT based on vision transformer (ViT) is proposed for HTD tasks. The HTD-ViT leverages the ViT to learn discriminative spectral-spatial features of each pixel and its neighboring pixels. Meanwhile, the spectral-spatial sequence construction operation uses spectrums in the cross region centered on the selected pixel to produce the corresponding spectral-spatial sequence for ViT processing. Furthermore, the spectral-spatial sample selection procedure based on coarse detection addresses the issue of lacking well-labeled training instances in the HTD tasks. Finally, the spectral-spatial pixel-level detection combines the discriminative feature from the spectral and the spatial domains to suppress the background. In contrast to traditional spatial-spectral feature extraction methods that stack the original spectral feature with spatial neighborhood information directly, joint spectral-spatial inference in HTD-ViT can effectively discover the underlying contextual and structure information in HSIs. Experiments on real HSIs verify the effectiveness of HTD-ViT, which takes full advantage of both the variable spectral and spatial features.",
      "intriguing_abstract": "In hyperspectral images (HSIs), spatial context provides complementary information to abundant spectral features. In this paper, a united spectral-spatial framework named HTD-ViT based on vision transformer (ViT) is proposed for HTD tasks. The HTD-ViT leverages the ViT to learn discriminative spectral-spatial features of each pixel and its neighboring pixels. Meanwhile, the spectral-spatial sequence construction operation uses spectrums in the cross region centered on the selected pixel to produce the corresponding spectral-spatial sequence for ViT processing. Furthermore, the spectral-spatial sample selection procedure based on coarse detection addresses the issue of lacking well-labeled training instances in the HTD tasks. Finally, the spectral-spatial pixel-level detection combines the discriminative feature from the spectral and the spatial domains to suppress the background. In contrast to traditional spatial-spectral feature extraction methods that stack the original spectral feature with spatial neighborhood information directly, joint spectral-spatial inference in HTD-ViT can effectively discover the underlying contextual and structure information in HSIs. Experiments on real HSIs verify the effectiveness of HTD-ViT, which takes full advantage of both the variable spectral and spatial features.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/1bf9a97a7581d1671d683c19ce36fed6511dc95b.pdf",
      "citation_key": "qin2022cfg",
      "metadata": {
        "title": "HTD-VIT: Spectral-Spatial Joint Hyperspectral Target Detection with Vision Transformer",
        "authors": [
          "Haonan Qin",
          "Weiying Xie",
          "Yunsong Li",
          "Qian Du"
        ],
        "published_date": "2022",
        "abstract": "In hyperspectral images (HSIs), spatial context provides complementary information to abundant spectral features. In this paper, a united spectral-spatial framework named HTD-ViT based on vision transformer (ViT) is proposed for HTD tasks. The HTD-ViT leverages the ViT to learn discriminative spectral-spatial features of each pixel and its neighboring pixels. Meanwhile, the spectral-spatial sequence construction operation uses spectrums in the cross region centered on the selected pixel to produce the corresponding spectral-spatial sequence for ViT processing. Furthermore, the spectral-spatial sample selection procedure based on coarse detection addresses the issue of lacking well-labeled training instances in the HTD tasks. Finally, the spectral-spatial pixel-level detection combines the discriminative feature from the spectral and the spatial domains to suppress the background. In contrast to traditional spatial-spectral feature extraction methods that stack the original spectral feature with spatial neighborhood information directly, joint spectral-spatial inference in HTD-ViT can effectively discover the underlying contextual and structure information in HSIs. Experiments on real HSIs verify the effectiveness of HTD-ViT, which takes full advantage of both the variable spectral and spatial features.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1bf9a97a7581d1671d683c19ce36fed6511dc95b.pdf",
        "venue": "IEEE International Geoscience and Remote Sensing Symposium",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "In hyperspectral images (HSIs), spatial context provides complementary information to abundant spectral features. In this paper, a united spectral-spatial framework named HTD-ViT based on vision transformer (ViT) is proposed for HTD tasks. The HTD-ViT leverages the ViT to learn discriminative spectral-spatial features of each pixel and its neighboring pixels. Meanwhile, the spectral-spatial sequence construction operation uses spectrums in the cross region centered on the selected pixel to produce the corresponding spectral-spatial sequence for ViT processing. Furthermore, the spectral-spatial sample selection procedure based on coarse detection addresses the issue of lacking well-labeled training instances in the HTD tasks. Finally, the spectral-spatial pixel-level detection combines the discriminative feature from the spectral and the spatial domains to suppress the background. In contrast to traditional spatial-spectral feature extraction methods that stack the original spectral feature with spatial neighborhood information directly, joint spectral-spatial inference in HTD-ViT can effectively discover the underlying contextual and structure information in HSIs. Experiments on real HSIs verify the effectiveness of HTD-ViT, which takes full advantage of both the variable spectral and spatial features.",
        "keywords": []
      },
      "file_name": "1bf9a97a7581d1671d683c19ce36fed6511dc95b.pdf"
    },
    {
      "success": true,
      "doc_id": "382b23b9abb2bf02ffb13aaf1f245097",
      "summary": "Mushrooms are the fleshy, spore-bearing structure of certain fungi, produced by a group of mycelia and buried in a substratum. Mushrooms are classified as edible, medicinal, and poisonous. However, many poisoning incidents occur yearly by consuming wild mushrooms. Thousands of poisoning incidents are reported each year globally, and 80% of these are from unidentified species of mushrooms. Mushroom poisoning is one of the most serious food safety issues worldwide. Motivated by this problem, this study uses an open-source mushroom dataset and employs several data augmentation approaches to decrease the probability of model overfitting. We propose a novel deep learning pipeline (ViT-Mushroom) for mushroom classification using the Vision Transformer large network (ViT-L/32). We compared the performance of our method against that of a convolutional neural network (CNN). We visualized the high-dimensional outputs of the ViT-L/32 model to achieve the interpretability of ViT-L/32 using the t-distributed stochastic neighbor embedding (t-SNE) method. The results show that ViT-L/32 is the best on the testing dataset, with an accuracy score of 95.97%. These results surpass previous approaches in reducing intraclass variability and generating well-separated feature embeddings. The proposed method is a promising deep learning model capable of automatically classifying mushroom species, helping wild mushroom consumers avoid eating toxic mushrooms, safeguarding food safety, and preventing public health incidents of food poisoning. The results will offer valuable resources for food scientists, nutritionists, and the public health sector regarding the safety and quality of mushrooms.",
      "intriguing_abstract": "Mushrooms are the fleshy, spore-bearing structure of certain fungi, produced by a group of mycelia and buried in a substratum. Mushrooms are classified as edible, medicinal, and poisonous. However, many poisoning incidents occur yearly by consuming wild mushrooms. Thousands of poisoning incidents are reported each year globally, and 80% of these are from unidentified species of mushrooms. Mushroom poisoning is one of the most serious food safety issues worldwide. Motivated by this problem, this study uses an open-source mushroom dataset and employs several data augmentation approaches to decrease the probability of model overfitting. We propose a novel deep learning pipeline (ViT-Mushroom) for mushroom classification using the Vision Transformer large network (ViT-L/32). We compared the performance of our method against that of a convolutional neural network (CNN). We visualized the high-dimensional outputs of the ViT-L/32 model to achieve the interpretability of ViT-L/32 using the t-distributed stochastic neighbor embedding (t-SNE) method. The results show that ViT-L/32 is the best on the testing dataset, with an accuracy score of 95.97%. These results surpass previous approaches in reducing intraclass variability and generating well-separated feature embeddings. The proposed method is a promising deep learning model capable of automatically classifying mushroom species, helping wild mushroom consumers avoid eating toxic mushrooms, safeguarding food safety, and preventing public health incidents of food poisoning. The results will offer valuable resources for food scientists, nutritionists, and the public health sector regarding the safety and quality of mushrooms.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/e58c2bc5333ee8f37150d9d15c0428d9658b2e50.pdf",
      "citation_key": "wang2022ohd",
      "metadata": {
        "title": "Automatic Mushroom Species Classification Model for Foodborne Disease Prevention Based on Vision Transformer",
        "authors": [
          "Boyuan Wang"
        ],
        "published_date": "2022",
        "abstract": "Mushrooms are the fleshy, spore-bearing structure of certain fungi, produced by a group of mycelia and buried in a substratum. Mushrooms are classified as edible, medicinal, and poisonous. However, many poisoning incidents occur yearly by consuming wild mushrooms. Thousands of poisoning incidents are reported each year globally, and 80% of these are from unidentified species of mushrooms. Mushroom poisoning is one of the most serious food safety issues worldwide. Motivated by this problem, this study uses an open-source mushroom dataset and employs several data augmentation approaches to decrease the probability of model overfitting. We propose a novel deep learning pipeline (ViT-Mushroom) for mushroom classification using the Vision Transformer large network (ViT-L/32). We compared the performance of our method against that of a convolutional neural network (CNN). We visualized the high-dimensional outputs of the ViT-L/32 model to achieve the interpretability of ViT-L/32 using the t-distributed stochastic neighbor embedding (t-SNE) method. The results show that ViT-L/32 is the best on the testing dataset, with an accuracy score of 95.97%. These results surpass previous approaches in reducing intraclass variability and generating well-separated feature embeddings. The proposed method is a promising deep learning model capable of automatically classifying mushroom species, helping wild mushroom consumers avoid eating toxic mushrooms, safeguarding food safety, and preventing public health incidents of food poisoning. The results will offer valuable resources for food scientists, nutritionists, and the public health sector regarding the safety and quality of mushrooms.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e58c2bc5333ee8f37150d9d15c0428d9658b2e50.pdf",
        "venue": "Journal of Food Quality",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "Mushrooms are the fleshy, spore-bearing structure of certain fungi, produced by a group of mycelia and buried in a substratum. Mushrooms are classified as edible, medicinal, and poisonous. However, many poisoning incidents occur yearly by consuming wild mushrooms. Thousands of poisoning incidents are reported each year globally, and 80% of these are from unidentified species of mushrooms. Mushroom poisoning is one of the most serious food safety issues worldwide. Motivated by this problem, this study uses an open-source mushroom dataset and employs several data augmentation approaches to decrease the probability of model overfitting. We propose a novel deep learning pipeline (ViT-Mushroom) for mushroom classification using the Vision Transformer large network (ViT-L/32). We compared the performance of our method against that of a convolutional neural network (CNN). We visualized the high-dimensional outputs of the ViT-L/32 model to achieve the interpretability of ViT-L/32 using the t-distributed stochastic neighbor embedding (t-SNE) method. The results show that ViT-L/32 is the best on the testing dataset, with an accuracy score of 95.97%. These results surpass previous approaches in reducing intraclass variability and generating well-separated feature embeddings. The proposed method is a promising deep learning model capable of automatically classifying mushroom species, helping wild mushroom consumers avoid eating toxic mushrooms, safeguarding food safety, and preventing public health incidents of food poisoning. The results will offer valuable resources for food scientists, nutritionists, and the public health sector regarding the safety and quality of mushrooms.",
        "keywords": []
      },
      "file_name": "e58c2bc5333ee8f37150d9d15c0428d9658b2e50.pdf"
    },
    {
      "success": true,
      "doc_id": "47d9a3a506eda0188926192848da5cbf",
      "summary": "Accurate segmentation of the retina vessel is essential for the early diagnosis of eye-related diseases. Recently, convolutional neural networks have shown remarkable performance in retina vessel segmentation. However, the complexity of edge structural information and the changeable intensity distribution depending on retina images reduce the performance of the segmentation tasks. This paper proposes two novel deep learning-based modules, channel attention vision transformer (CAViT) and deep adaptive gamma correction (DAGC), to tackle these issues. The CAViT jointly applies the efficient channel attention (ECA) and the vision transformer (ViT), in which the channel attention module considers the interdependency among feature channels and the ViT discriminates meaningful edge structures by considering the global context. The DAGC module provides the optimal gamma correction value for each input image by jointly training a CNN model with the segmentation network so that all the retina images are mapped to a unified intensity distribution. The experimental results show that our proposed method achieves superior performance compared to conventional methods on widely used datasets, DRIVE and CHASE DB1.",
      "intriguing_abstract": "Accurate segmentation of the retina vessel is essential for the early diagnosis of eye-related diseases. Recently, convolutional neural networks have shown remarkable performance in retina vessel segmentation. However, the complexity of edge structural information and the changeable intensity distribution depending on retina images reduce the performance of the segmentation tasks. This paper proposes two novel deep learning-based modules, channel attention vision transformer (CAViT) and deep adaptive gamma correction (DAGC), to tackle these issues. The CAViT jointly applies the efficient channel attention (ECA) and the vision transformer (ViT), in which the channel attention module considers the interdependency among feature channels and the ViT discriminates meaningful edge structures by considering the global context. The DAGC module provides the optimal gamma correction value for each input image by jointly training a CNN model with the segmentation network so that all the retina images are mapped to a unified intensity distribution. The experimental results show that our proposed method achieves superior performance compared to conventional methods on widely used datasets, DRIVE and CHASE DB1.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/761f55a486e99ab5d3550aee48df34b6b65643c2.pdf",
      "citation_key": "yu2022o30",
      "metadata": {
        "title": "Vision Transformer-Based Retina Vessel Segmentation with Deep Adaptive Gamma Correction",
        "authors": [
          "Hyunwoo Yu",
          "J. Shim",
          "Jaeho Kwak",
          "J. Song",
          "Suk-Ju Kang"
        ],
        "published_date": "2022",
        "abstract": "Accurate segmentation of the retina vessel is essential for the early diagnosis of eye-related diseases. Recently, convolutional neural networks have shown remarkable performance in retina vessel segmentation. However, the complexity of edge structural information and the changeable intensity distribution depending on retina images reduce the performance of the segmentation tasks. This paper proposes two novel deep learning-based modules, channel attention vision transformer (CAViT) and deep adaptive gamma correction (DAGC), to tackle these issues. The CAViT jointly applies the efficient channel attention (ECA) and the vision transformer (ViT), in which the channel attention module considers the interdependency among feature channels and the ViT discriminates meaningful edge structures by considering the global context. The DAGC module provides the optimal gamma correction value for each input image by jointly training a CNN model with the segmentation network so that all the retina images are mapped to a unified intensity distribution. The experimental results show that our proposed method achieves superior performance compared to conventional methods on widely used datasets, DRIVE and CHASE DB1.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/761f55a486e99ab5d3550aee48df34b6b65643c2.pdf",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "Accurate segmentation of the retina vessel is essential for the early diagnosis of eye-related diseases. Recently, convolutional neural networks have shown remarkable performance in retina vessel segmentation. However, the complexity of edge structural information and the changeable intensity distribution depending on retina images reduce the performance of the segmentation tasks. This paper proposes two novel deep learning-based modules, channel attention vision transformer (CAViT) and deep adaptive gamma correction (DAGC), to tackle these issues. The CAViT jointly applies the efficient channel attention (ECA) and the vision transformer (ViT), in which the channel attention module considers the interdependency among feature channels and the ViT discriminates meaningful edge structures by considering the global context. The DAGC module provides the optimal gamma correction value for each input image by jointly training a CNN model with the segmentation network so that all the retina images are mapped to a unified intensity distribution. The experimental results show that our proposed method achieves superior performance compared to conventional methods on widely used datasets, DRIVE and CHASE DB1.",
        "keywords": []
      },
      "file_name": "761f55a486e99ab5d3550aee48df34b6b65643c2.pdf"
    },
    {
      "success": true,
      "doc_id": "8d871a9201a545e7da9cdbade4051929",
      "summary": "Plant health is one of the most interesting aspects in the natural cycle, it needs to be conserved to keep the life of the organisms. Several plant diseases could be observed at early stages in the leaf level, where immediate interventions should be taken to prevent the progression of the disease. The use of deep learning has dramatically increased recently, owing to its remarkable performance in multiple applications in different research areas. In this study, we focus on the detection of tomato diseases at the leaf stage using recent deep learning architectures. Several deep learning models are put in comparative experiments to achieve a stable and robust classification performance with high precision that outperforms previous SOTA results. Vision Transformers (ViT) models reported the top classification re-sults, with an accuracy of 96.7%, 98.52%, 99.1% and 99.7%. The research funding will help in the early automatic detection of diseases in the leaf plants, thus providing necessary treatments and maintaining the natural cycle.",
      "intriguing_abstract": "Plant health is one of the most interesting aspects in the natural cycle, it needs to be conserved to keep the life of the organisms. Several plant diseases could be observed at early stages in the leaf level, where immediate interventions should be taken to prevent the progression of the disease. The use of deep learning has dramatically increased recently, owing to its remarkable performance in multiple applications in different research areas. In this study, we focus on the detection of tomato diseases at the leaf stage using recent deep learning architectures. Several deep learning models are put in comparative experiments to achieve a stable and robust classification performance with high precision that outperforms previous SOTA results. Vision Transformers (ViT) models reported the top classification re-sults, with an accuracy of 96.7%, 98.52%, 99.1% and 99.7%. The research funding will help in the early automatic detection of diseases in the leaf plants, thus providing necessary treatments and maintaining the natural cycle.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/2dcc1f5f1ba11f99e09f22c53d02aa66cec35c48.pdf",
      "citation_key": "boukabouya2022ffi",
      "metadata": {
        "title": "Vision Transformer Based Models for Plant Disease Detection and Diagnosis",
        "authors": [
          "Rayene Amina Boukabouya",
          "A. Moussaoui",
          "Mohamed Berrimi"
        ],
        "published_date": "2022",
        "abstract": "Plant health is one of the most interesting aspects in the natural cycle, it needs to be conserved to keep the life of the organisms. Several plant diseases could be observed at early stages in the leaf level, where immediate interventions should be taken to prevent the progression of the disease. The use of deep learning has dramatically increased recently, owing to its remarkable performance in multiple applications in different research areas. In this study, we focus on the detection of tomato diseases at the leaf stage using recent deep learning architectures. Several deep learning models are put in comparative experiments to achieve a stable and robust classification performance with high precision that outperforms previous SOTA results. Vision Transformers (ViT) models reported the top classification re-sults, with an accuracy of 96.7%, 98.52%, 99.1% and 99.7%. The research funding will help in the early automatic detection of diseases in the leaf plants, thus providing necessary treatments and maintaining the natural cycle.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2dcc1f5f1ba11f99e09f22c53d02aa66cec35c48.pdf",
        "venue": "International Symposium on Information and Automation",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "Plant health is one of the most interesting aspects in the natural cycle, it needs to be conserved to keep the life of the organisms. Several plant diseases could be observed at early stages in the leaf level, where immediate interventions should be taken to prevent the progression of the disease. The use of deep learning has dramatically increased recently, owing to its remarkable performance in multiple applications in different research areas. In this study, we focus on the detection of tomato diseases at the leaf stage using recent deep learning architectures. Several deep learning models are put in comparative experiments to achieve a stable and robust classification performance with high precision that outperforms previous SOTA results. Vision Transformers (ViT) models reported the top classification re-sults, with an accuracy of 96.7%, 98.52%, 99.1% and 99.7%. The research funding will help in the early automatic detection of diseases in the leaf plants, thus providing necessary treatments and maintaining the natural cycle.",
        "keywords": []
      },
      "file_name": "2dcc1f5f1ba11f99e09f22c53d02aa66cec35c48.pdf"
    },
    {
      "success": true,
      "doc_id": "8762c9a27397d8cb746b2067e7158b7b",
      "summary": "Transformer, especially vision transformer (ViT), is attracting increasing attention in various computer vision (CV) tasks. However, two urgent problems exist for the ViT: 1) owing to its attending to an image in the patch level, the ViT seems to have a better performance in fetching global representations but is limited in extracting local features, which is an inherent advantage for the convolutional neural network (CNN) and 2) the learnable positional encoding plays a positive role, but limits the cross-resolution ability of the network. Specifically, the pre-trained model could only generate images of the same size during training. To conquer the two problems, we propose a novel convolution-embedded ViT with elastic positional encoding in this article. On one hand, we propose a joint CNN and self-attention (CSA) network to collaboratively extract local and global features. On the other hand, we propose to integrate the elastic CNN-based positional encoder into the framework to solve the rigid limitation of the ViT in cross resolution issues and improve the performance. Extensive experiments were conducted on IKONOS and WorldView-2 with 4- and 8-band multispectral (MS) images, respectively. The visual and numerical results show the competitive performance of the proposed method.",
      "intriguing_abstract": "Transformer, especially vision transformer (ViT), is attracting increasing attention in various computer vision (CV) tasks. However, two urgent problems exist for the ViT: 1) owing to its attending to an image in the patch level, the ViT seems to have a better performance in fetching global representations but is limited in extracting local features, which is an inherent advantage for the convolutional neural network (CNN) and 2) the learnable positional encoding plays a positive role, but limits the cross-resolution ability of the network. Specifically, the pre-trained model could only generate images of the same size during training. To conquer the two problems, we propose a novel convolution-embedded ViT with elastic positional encoding in this article. On one hand, we propose a joint CNN and self-attention (CSA) network to collaboratively extract local and global features. On the other hand, we propose to integrate the elastic CNN-based positional encoder into the framework to solve the rigid limitation of the ViT in cross resolution issues and improve the performance. Extensive experiments were conducted on IKONOS and WorldView-2 with 4- and 8-band multispectral (MS) images, respectively. The visual and numerical results show the competitive performance of the proposed method.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/52a7f15085f1b6815a4de2da26df51bb63470596.pdf",
      "citation_key": "wang2022d7p",
      "metadata": {
        "title": "Convolution-Embedded Vision Transformer With Elastic Positional Encoding for Pansharpening",
        "authors": [
          "Nan Wang",
          "Xiangjun Meng",
          "Xiangchao Meng",
          "Feng Shao"
        ],
        "published_date": "2022",
        "abstract": "Transformer, especially vision transformer (ViT), is attracting increasing attention in various computer vision (CV) tasks. However, two urgent problems exist for the ViT: 1) owing to its attending to an image in the patch level, the ViT seems to have a better performance in fetching global representations but is limited in extracting local features, which is an inherent advantage for the convolutional neural network (CNN) and 2) the learnable positional encoding plays a positive role, but limits the cross-resolution ability of the network. Specifically, the pre-trained model could only generate images of the same size during training. To conquer the two problems, we propose a novel convolution-embedded ViT with elastic positional encoding in this article. On one hand, we propose a joint CNN and self-attention (CSA) network to collaboratively extract local and global features. On the other hand, we propose to integrate the elastic CNN-based positional encoder into the framework to solve the rigid limitation of the ViT in cross resolution issues and improve the performance. Extensive experiments were conducted on IKONOS and WorldView-2 with 4- and 8-band multispectral (MS) images, respectively. The visual and numerical results show the competitive performance of the proposed method.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/52a7f15085f1b6815a4de2da26df51bb63470596.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "Transformer, especially vision transformer (ViT), is attracting increasing attention in various computer vision (CV) tasks. However, two urgent problems exist for the ViT: 1) owing to its attending to an image in the patch level, the ViT seems to have a better performance in fetching global representations but is limited in extracting local features, which is an inherent advantage for the convolutional neural network (CNN) and 2) the learnable positional encoding plays a positive role, but limits the cross-resolution ability of the network. Specifically, the pre-trained model could only generate images of the same size during training. To conquer the two problems, we propose a novel convolution-embedded ViT with elastic positional encoding in this article. On one hand, we propose a joint CNN and self-attention (CSA) network to collaboratively extract local and global features. On the other hand, we propose to integrate the elastic CNN-based positional encoder into the framework to solve the rigid limitation of the ViT in cross resolution issues and improve the performance. Extensive experiments were conducted on IKONOS and WorldView-2 with 4- and 8-band multispectral (MS) images, respectively. The visual and numerical results show the competitive performance of the proposed method.",
        "keywords": []
      },
      "file_name": "52a7f15085f1b6815a4de2da26df51bb63470596.pdf"
    },
    {
      "success": true,
      "doc_id": "2cabac0465d3a90b4e24339090daf996",
      "summary": "Vision transformers have become one of the most important models for computer vision tasks. Although they outperform prior works, they require heavy computational resources on a scale that is quadratic to $N$. This is a major drawback of the traditional self-attention (SA) algorithm. Here, we propose the Unit Force Operated Vision Transformer (UFO-ViT), a novel SA mechanism that has linear complexity. The main approach of this work is to eliminate nonlinearity from the original SA. We factorize the matrix multiplication of the SA mechanism without complicated linear approximation. By modifying only a few lines of code from the original SA, the proposed models outperform most transformer-based models on image classification and dense prediction tasks on most capacity regimes.",
      "intriguing_abstract": "Vision transformers have become one of the most important models for computer vision tasks. Although they outperform prior works, they require heavy computational resources on a scale that is quadratic to $N$. This is a major drawback of the traditional self-attention (SA) algorithm. Here, we propose the Unit Force Operated Vision Transformer (UFO-ViT), a novel SA mechanism that has linear complexity. The main approach of this work is to eliminate nonlinearity from the original SA. We factorize the matrix multiplication of the SA mechanism without complicated linear approximation. By modifying only a few lines of code from the original SA, the proposed models outperform most transformer-based models on image classification and dense prediction tasks on most capacity regimes.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/649b706ba282de4eb5a161137f80eb49ed84a0a8.pdf",
      "citation_key": "song20215tk",
      "metadata": {
        "title": "UFO-ViT: High Performance Linear Vision Transformer without Softmax",
        "authors": [
          "Jeonggeun Song"
        ],
        "published_date": "2021",
        "abstract": "Vision transformers have become one of the most important models for computer vision tasks. Although they outperform prior works, they require heavy computational resources on a scale that is quadratic to $N$. This is a major drawback of the traditional self-attention (SA) algorithm. Here, we propose the Unit Force Operated Vision Transformer (UFO-ViT), a novel SA mechanism that has linear complexity. The main approach of this work is to eliminate nonlinearity from the original SA. We factorize the matrix multiplication of the SA mechanism without complicated linear approximation. By modifying only a few lines of code from the original SA, the proposed models outperform most transformer-based models on image classification and dense prediction tasks on most capacity regimes.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/649b706ba282de4eb5a161137f80eb49ed84a0a8.pdf",
        "venue": "arXiv.org",
        "citationCount": 21,
        "score": 5.25,
        "summary": "Vision transformers have become one of the most important models for computer vision tasks. Although they outperform prior works, they require heavy computational resources on a scale that is quadratic to $N$. This is a major drawback of the traditional self-attention (SA) algorithm. Here, we propose the Unit Force Operated Vision Transformer (UFO-ViT), a novel SA mechanism that has linear complexity. The main approach of this work is to eliminate nonlinearity from the original SA. We factorize the matrix multiplication of the SA mechanism without complicated linear approximation. By modifying only a few lines of code from the original SA, the proposed models outperform most transformer-based models on image classification and dense prediction tasks on most capacity regimes.",
        "keywords": []
      },
      "file_name": "649b706ba282de4eb5a161137f80eb49ed84a0a8.pdf"
    },
    {
      "success": true,
      "doc_id": "d9741357ad9732707000ecd42ad99d4c",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/186295f7c79e46c0e4e5f40e094267c09714043d.pdf",
      "citation_key": "xie2021th0",
      "metadata": {
        "title": "So-ViT: Mind Visual Tokens for Vision Transformer",
        "authors": [
          "Jiangtao Xie",
          "Rui Zeng",
          "Qilong Wang",
          "Ziqi Zhou",
          "Peihua Li"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/186295f7c79e46c0e4e5f40e094267c09714043d.pdf",
        "venue": "arXiv.org",
        "citationCount": 20,
        "score": 5.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "186295f7c79e46c0e4e5f40e094267c09714043d.pdf"
    },
    {
      "success": true,
      "doc_id": "76ffe2b3c149ba26f06f1631501cd818",
      "summary": "Sonar image is the main way for underwater vehicles to obtain environmental information. The task of target detection in sonar images can distinguish multi-class targets in real time and accurately locate them, providing perception information for the decision-making system of underwater vehicles. However, there are many challenges in sonar image target detection, such as many kinds of sonar, complex and serious noise interference in images, and less datasets. This paper proposes a sonar image target detection method based on Dual Path Vision Transformer Network (DP-VIT) to accurately detect targets in forward-look sonar and side-scan sonar. DP-ViT increases receptive field by adding multi-scale to patch embedding enhances learning ability of model feature extraction by using Dual Path Transformer Block, then introduces Conv-Attention to reduce model training parameters, and finally uses Generalized Focal Loss to solve the problem of imbalance between positive and negative samples. The experimental results show that the performance of this sonar target detection method is superior to other mainstream methods on both forward-look sonar dataset and side-scan sonar dataset, and it can also maintain good performance in the case of adding noise.",
      "intriguing_abstract": "Sonar image is the main way for underwater vehicles to obtain environmental information. The task of target detection in sonar images can distinguish multi-class targets in real time and accurately locate them, providing perception information for the decision-making system of underwater vehicles. However, there are many challenges in sonar image target detection, such as many kinds of sonar, complex and serious noise interference in images, and less datasets. This paper proposes a sonar image target detection method based on Dual Path Vision Transformer Network (DP-VIT) to accurately detect targets in forward-look sonar and side-scan sonar. DP-ViT increases receptive field by adding multi-scale to patch embedding enhances learning ability of model feature extraction by using Dual Path Transformer Block, then introduces Conv-Attention to reduce model training parameters, and finally uses Generalized Focal Loss to solve the problem of imbalance between positive and negative samples. The experimental results show that the performance of this sonar target detection method is superior to other mainstream methods on both forward-look sonar dataset and side-scan sonar dataset, and it can also maintain good performance in the case of adding noise.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d77288fc7de7b15c200ed75118de702caf841ec3.pdf",
      "citation_key": "sun2022bm5",
      "metadata": {
        "title": "DP-ViT: A Dual-Path Vision Transformer for Real-Time Sonar Target Detection",
        "authors": [
          "Yu-shan Sun",
          "Hao Zheng",
          "Guo-cheng Zhang",
          "Jingfei Ren",
          "Hao Xu",
          "Chaofei Xu"
        ],
        "published_date": "2022",
        "abstract": "Sonar image is the main way for underwater vehicles to obtain environmental information. The task of target detection in sonar images can distinguish multi-class targets in real time and accurately locate them, providing perception information for the decision-making system of underwater vehicles. However, there are many challenges in sonar image target detection, such as many kinds of sonar, complex and serious noise interference in images, and less datasets. This paper proposes a sonar image target detection method based on Dual Path Vision Transformer Network (DP-VIT) to accurately detect targets in forward-look sonar and side-scan sonar. DP-ViT increases receptive field by adding multi-scale to patch embedding enhances learning ability of model feature extraction by using Dual Path Transformer Block, then introduces Conv-Attention to reduce model training parameters, and finally uses Generalized Focal Loss to solve the problem of imbalance between positive and negative samples. The experimental results show that the performance of this sonar target detection method is superior to other mainstream methods on both forward-look sonar dataset and side-scan sonar dataset, and it can also maintain good performance in the case of adding noise.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d77288fc7de7b15c200ed75118de702caf841ec3.pdf",
        "venue": "Remote Sensing",
        "citationCount": 15,
        "score": 5.0,
        "summary": "Sonar image is the main way for underwater vehicles to obtain environmental information. The task of target detection in sonar images can distinguish multi-class targets in real time and accurately locate them, providing perception information for the decision-making system of underwater vehicles. However, there are many challenges in sonar image target detection, such as many kinds of sonar, complex and serious noise interference in images, and less datasets. This paper proposes a sonar image target detection method based on Dual Path Vision Transformer Network (DP-VIT) to accurately detect targets in forward-look sonar and side-scan sonar. DP-ViT increases receptive field by adding multi-scale to patch embedding enhances learning ability of model feature extraction by using Dual Path Transformer Block, then introduces Conv-Attention to reduce model training parameters, and finally uses Generalized Focal Loss to solve the problem of imbalance between positive and negative samples. The experimental results show that the performance of this sonar target detection method is superior to other mainstream methods on both forward-look sonar dataset and side-scan sonar dataset, and it can also maintain good performance in the case of adding noise.",
        "keywords": []
      },
      "file_name": "d77288fc7de7b15c200ed75118de702caf841ec3.pdf"
    },
    {
      "success": true,
      "doc_id": "ecb1dfff9e4e2285facb41b3853236f0",
      "summary": "Recently, inspired by the success of Transformer in natural language processing tasks, a number of works have attempted to apply Transformer-based models to video action recognition. Existing works only use one RGB stream as the input for Transformer. How to use multiple pathways and multiple streams with Transformer for action recognition has not been studied. To address this issue, we present a novel structure namely Two-Pathway Vision Transformer (TP-ViT). Two parallel spatial Transformer encoders are used as two pathways with different framerates and resolutions of the input video. The high-resolution pathway contains more spatial information, while the high-framerate pathway contains more temporal information. The two outputs are fused and fed into a temporal Transformer encoder for action recognition. Furthermore, we also fuse skeleton features into our model to get better results. Our experiments demonstrate that our proposed models achieve the state-of-the-art results on both the coarse-grained dataset Kinetics and the fine-grained dataset FineGym.",
      "intriguing_abstract": "Recently, inspired by the success of Transformer in natural language processing tasks, a number of works have attempted to apply Transformer-based models to video action recognition. Existing works only use one RGB stream as the input for Transformer. How to use multiple pathways and multiple streams with Transformer for action recognition has not been studied. To address this issue, we present a novel structure namely Two-Pathway Vision Transformer (TP-ViT). Two parallel spatial Transformer encoders are used as two pathways with different framerates and resolutions of the input video. The high-resolution pathway contains more spatial information, while the high-framerate pathway contains more temporal information. The two outputs are fused and fed into a temporal Transformer encoder for action recognition. Furthermore, we also fuse skeleton features into our model to get better results. Our experiments demonstrate that our proposed models achieve the state-of-the-art results on both the coarse-grained dataset Kinetics and the fine-grained dataset FineGym.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/2e4dbc3dbd400346be60318ae558a0293e65ba81.pdf",
      "citation_key": "jing2022nkb",
      "metadata": {
        "title": "TP-VIT: A Two-Pathway Vision Transformer for Video Action Recognition",
        "authors": [
          "Yanhao Jing",
          "Feng Wang"
        ],
        "published_date": "2022",
        "abstract": "Recently, inspired by the success of Transformer in natural language processing tasks, a number of works have attempted to apply Transformer-based models to video action recognition. Existing works only use one RGB stream as the input for Transformer. How to use multiple pathways and multiple streams with Transformer for action recognition has not been studied. To address this issue, we present a novel structure namely Two-Pathway Vision Transformer (TP-ViT). Two parallel spatial Transformer encoders are used as two pathways with different framerates and resolutions of the input video. The high-resolution pathway contains more spatial information, while the high-framerate pathway contains more temporal information. The two outputs are fused and fed into a temporal Transformer encoder for action recognition. Furthermore, we also fuse skeleton features into our model to get better results. Our experiments demonstrate that our proposed models achieve the state-of-the-art results on both the coarse-grained dataset Kinetics and the fine-grained dataset FineGym.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2e4dbc3dbd400346be60318ae558a0293e65ba81.pdf",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "citationCount": 14,
        "score": 4.666666666666666,
        "summary": "Recently, inspired by the success of Transformer in natural language processing tasks, a number of works have attempted to apply Transformer-based models to video action recognition. Existing works only use one RGB stream as the input for Transformer. How to use multiple pathways and multiple streams with Transformer for action recognition has not been studied. To address this issue, we present a novel structure namely Two-Pathway Vision Transformer (TP-ViT). Two parallel spatial Transformer encoders are used as two pathways with different framerates and resolutions of the input video. The high-resolution pathway contains more spatial information, while the high-framerate pathway contains more temporal information. The two outputs are fused and fed into a temporal Transformer encoder for action recognition. Furthermore, we also fuse skeleton features into our model to get better results. Our experiments demonstrate that our proposed models achieve the state-of-the-art results on both the coarse-grained dataset Kinetics and the fine-grained dataset FineGym.",
        "keywords": []
      },
      "file_name": "2e4dbc3dbd400346be60318ae558a0293e65ba81.pdf"
    },
    {
      "success": true,
      "doc_id": "c2bd9dbd910f70f9c553456b9c1a003d",
      "summary": "Recently, diffusion models have been proven to perform remarkably well in text-to-image synthesis tasks in a number of studies, immediately presenting new study opportunities for image generation. Googles Imagen follows this research trend and outperforms DALLE2 as the best model for text-to-image generation. However, Imagen merely uses a T5 language model for text processing, which cannot ensure learning the semantic information of the text. Furthermore, the Efficient UNet leveraged by Imagen is not the best choice in image processing. To address these issues, we propose the Swinv2-Imagen, a novel text-to-image diffusion model based on a Hierarchical Visual Transformer and a Scene Graph incorporating a semantic layout. In the proposed model, the feature vectors of entities and relationships are extracted and involved in the diffusion model, effectively improving the quality of generated images. On top of that, we also introduce a Swin-Transformer-based UNet architecture, called Swinv2-Unet, which can address the problems stemming from the CNN convolution operations. Extensive experiments are conducted to evaluate the performance of the proposed model by using three real-world datasets, i.e. MSCOCO, CUB and MM-CelebA-HQ. The experimental results show that the proposed Swinv2-Imagen model outperforms several popular state-of-the-art methods.",
      "intriguing_abstract": "Recently, diffusion models have been proven to perform remarkably well in text-to-image synthesis tasks in a number of studies, immediately presenting new study opportunities for image generation. Googles Imagen follows this research trend and outperforms DALLE2 as the best model for text-to-image generation. However, Imagen merely uses a T5 language model for text processing, which cannot ensure learning the semantic information of the text. Furthermore, the Efficient UNet leveraged by Imagen is not the best choice in image processing. To address these issues, we propose the Swinv2-Imagen, a novel text-to-image diffusion model based on a Hierarchical Visual Transformer and a Scene Graph incorporating a semantic layout. In the proposed model, the feature vectors of entities and relationships are extracted and involved in the diffusion model, effectively improving the quality of generated images. On top of that, we also introduce a Swin-Transformer-based UNet architecture, called Swinv2-Unet, which can address the problems stemming from the CNN convolution operations. Extensive experiments are conducted to evaluate the performance of the proposed model by using three real-world datasets, i.e. MSCOCO, CUB and MM-CelebA-HQ. The experimental results show that the proposed Swinv2-Imagen model outperforms several popular state-of-the-art methods.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/3de95f33c2b4f61a9c0f335b4810a966e209a47a.pdf",
      "citation_key": "li2022spu",
      "metadata": {
        "title": "Swinv2-Imagen: Hierarchical Vision Transformer Diffusion Models for Text-to-Image Generation",
        "authors": [
          "Rui Li",
          "Weihua Li",
          "Yi Yang",
          "Hanyu Wei",
          "Jianhua Jiang",
          "Quan Bai"
        ],
        "published_date": "2022",
        "abstract": "Recently, diffusion models have been proven to perform remarkably well in text-to-image synthesis tasks in a number of studies, immediately presenting new study opportunities for image generation. Googles Imagen follows this research trend and outperforms DALLE2 as the best model for text-to-image generation. However, Imagen merely uses a T5 language model for text processing, which cannot ensure learning the semantic information of the text. Furthermore, the Efficient UNet leveraged by Imagen is not the best choice in image processing. To address these issues, we propose the Swinv2-Imagen, a novel text-to-image diffusion model based on a Hierarchical Visual Transformer and a Scene Graph incorporating a semantic layout. In the proposed model, the feature vectors of entities and relationships are extracted and involved in the diffusion model, effectively improving the quality of generated images. On top of that, we also introduce a Swin-Transformer-based UNet architecture, called Swinv2-Unet, which can address the problems stemming from the CNN convolution operations. Extensive experiments are conducted to evaluate the performance of the proposed model by using three real-world datasets, i.e. MSCOCO, CUB and MM-CelebA-HQ. The experimental results show that the proposed Swinv2-Imagen model outperforms several popular state-of-the-art methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3de95f33c2b4f61a9c0f335b4810a966e209a47a.pdf",
        "venue": "Neural computing & applications (Print)",
        "citationCount": 14,
        "score": 4.666666666666666,
        "summary": "Recently, diffusion models have been proven to perform remarkably well in text-to-image synthesis tasks in a number of studies, immediately presenting new study opportunities for image generation. Googles Imagen follows this research trend and outperforms DALLE2 as the best model for text-to-image generation. However, Imagen merely uses a T5 language model for text processing, which cannot ensure learning the semantic information of the text. Furthermore, the Efficient UNet leveraged by Imagen is not the best choice in image processing. To address these issues, we propose the Swinv2-Imagen, a novel text-to-image diffusion model based on a Hierarchical Visual Transformer and a Scene Graph incorporating a semantic layout. In the proposed model, the feature vectors of entities and relationships are extracted and involved in the diffusion model, effectively improving the quality of generated images. On top of that, we also introduce a Swin-Transformer-based UNet architecture, called Swinv2-Unet, which can address the problems stemming from the CNN convolution operations. Extensive experiments are conducted to evaluate the performance of the proposed model by using three real-world datasets, i.e. MSCOCO, CUB and MM-CelebA-HQ. The experimental results show that the proposed Swinv2-Imagen model outperforms several popular state-of-the-art methods.",
        "keywords": []
      },
      "file_name": "3de95f33c2b4f61a9c0f335b4810a966e209a47a.pdf"
    },
    {
      "success": true,
      "doc_id": "a3f3f009516ca3d43976b48b5a87bf8b",
      "summary": "Transformers have been widely used in numerous vision problems especially for visual recognition and detection. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to construct an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. In addition, we extend it to ViDT+ to support joint-task learning for object detection and instance segmentation. Specifically, we attach an efficient multi-scale feature fusion layer and utilize two more auxiliary training losses, IoU-aware loss and token labeling loss. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and its extended ViDT+ achieves 53.2AP owing to its high scalability for large models. The source code and trained models are available at https://github.com/naver-ai/vidt.",
      "intriguing_abstract": "Transformers have been widely used in numerous vision problems especially for visual recognition and detection. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to construct an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. In addition, we extend it to ViDT+ to support joint-task learning for object detection and instance segmentation. Specifically, we attach an efficient multi-scale feature fusion layer and utilize two more auxiliary training losses, IoU-aware loss and token labeling loss. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and its extended ViDT+ achieves 53.2AP owing to its high scalability for large models. The source code and trained models are available at https://github.com/naver-ai/vidt.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/7817ecb816da8676ae21b401d60c99e706446f06.pdf",
      "citation_key": "song2022y4v",
      "metadata": {
        "title": "An Extendable, Efficient and Effective Transformer-based Object Detector",
        "authors": [
          "Hwanjun Song",
          "Deqing Sun",
          "Sanghyuk Chun",
          "Varun Jampani",
          "Dongyoon Han",
          "Byeongho Heo",
          "Wonjae Kim",
          "Ming-Hsuan Yang"
        ],
        "published_date": "2022",
        "abstract": "Transformers have been widely used in numerous vision problems especially for visual recognition and detection. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to construct an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. In addition, we extend it to ViDT+ to support joint-task learning for object detection and instance segmentation. Specifically, we attach an efficient multi-scale feature fusion layer and utilize two more auxiliary training losses, IoU-aware loss and token labeling loss. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and its extended ViDT+ achieves 53.2AP owing to its high scalability for large models. The source code and trained models are available at https://github.com/naver-ai/vidt.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7817ecb816da8676ae21b401d60c99e706446f06.pdf",
        "venue": "arXiv.org",
        "citationCount": 14,
        "score": 4.666666666666666,
        "summary": "Transformers have been widely used in numerous vision problems especially for visual recognition and detection. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to construct an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. In addition, we extend it to ViDT+ to support joint-task learning for object detection and instance segmentation. Specifically, we attach an efficient multi-scale feature fusion layer and utilize two more auxiliary training losses, IoU-aware loss and token labeling loss. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and its extended ViDT+ achieves 53.2AP owing to its high scalability for large models. The source code and trained models are available at https://github.com/naver-ai/vidt.",
        "keywords": []
      },
      "file_name": "7817ecb816da8676ae21b401d60c99e706446f06.pdf"
    },
    {
      "success": true,
      "doc_id": "792cdf9b7503cdfa4d3bbefd12100df2",
      "summary": "The advancement in deep learning techniques has helped researchers acquire and process multimodal data signals from different healthcare domains. Now, the focus has shifted towards providing end-to-end solutions, i.e., processing these data and developing models that can be directly implemented on edge devices. To achieve this, the researchers try to solve two problems: (I) reduce the complex feature dependencies and (II) reduce the complexity of the deep learning model without compromising accuracy. In this paper, we focus on the later part of reducing the complexity of the model by using the knowledge distillation framework. We have introduced knowledge distillation on the Vision Transformer model to study the MIT-BIH Arrhythmia Database. A tenfold crossvalidation technique was used to validate the model, and we obtained a 99.7% F1 score and 99.3% accuracy. The model was further tested on the Xilinx Alveo U50 FPGA accelerator, and it is found fit for any low-powered wearable device implementation.",
      "intriguing_abstract": "The advancement in deep learning techniques has helped researchers acquire and process multimodal data signals from different healthcare domains. Now, the focus has shifted towards providing end-to-end solutions, i.e., processing these data and developing models that can be directly implemented on edge devices. To achieve this, the researchers try to solve two problems: (I) reduce the complex feature dependencies and (II) reduce the complexity of the deep learning model without compromising accuracy. In this paper, we focus on the later part of reducing the complexity of the model by using the knowledge distillation framework. We have introduced knowledge distillation on the Vision Transformer model to study the MIT-BIH Arrhythmia Database. A tenfold crossvalidation technique was used to validate the model, and we obtained a 99.7% F1 score and 99.3% accuracy. The model was further tested on the Xilinx Alveo U50 FPGA accelerator, and it is found fit for any low-powered wearable device implementation.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/3502b661362b278eebacf1037fc3bb4e21963869.pdf",
      "citation_key": "shukla2022jxz",
      "metadata": {
        "title": "ECG-ViT: A Transformer-Based ECG Classifier for Energy-Constraint Wearable Devices",
        "authors": [
          "Neha Shukla",
          "Anand Pandey",
          "A. P. Shukla",
          "Sanjeev Chandra Neupane"
        ],
        "published_date": "2022",
        "abstract": "The advancement in deep learning techniques has helped researchers acquire and process multimodal data signals from different healthcare domains. Now, the focus has shifted towards providing end-to-end solutions, i.e., processing these data and developing models that can be directly implemented on edge devices. To achieve this, the researchers try to solve two problems: (I) reduce the complex feature dependencies and (II) reduce the complexity of the deep learning model without compromising accuracy. In this paper, we focus on the later part of reducing the complexity of the model by using the knowledge distillation framework. We have introduced knowledge distillation on the Vision Transformer model to study the MIT-BIH Arrhythmia Database. A tenfold crossvalidation technique was used to validate the model, and we obtained a 99.7% F1 score and 99.3% accuracy. The model was further tested on the Xilinx Alveo U50 FPGA accelerator, and it is found fit for any low-powered wearable device implementation.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3502b661362b278eebacf1037fc3bb4e21963869.pdf",
        "venue": "J. Sensors",
        "citationCount": 14,
        "score": 4.666666666666666,
        "summary": "The advancement in deep learning techniques has helped researchers acquire and process multimodal data signals from different healthcare domains. Now, the focus has shifted towards providing end-to-end solutions, i.e., processing these data and developing models that can be directly implemented on edge devices. To achieve this, the researchers try to solve two problems: (I) reduce the complex feature dependencies and (II) reduce the complexity of the deep learning model without compromising accuracy. In this paper, we focus on the later part of reducing the complexity of the model by using the knowledge distillation framework. We have introduced knowledge distillation on the Vision Transformer model to study the MIT-BIH Arrhythmia Database. A tenfold crossvalidation technique was used to validate the model, and we obtained a 99.7% F1 score and 99.3% accuracy. The model was further tested on the Xilinx Alveo U50 FPGA accelerator, and it is found fit for any low-powered wearable device implementation.",
        "keywords": []
      },
      "file_name": "3502b661362b278eebacf1037fc3bb4e21963869.pdf"
    },
    {
      "success": true,
      "doc_id": "a9119d13dd988f943fd55d91ef501e13",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/791d1e306eaa2e87657925ec4f45661baa8da58b.pdf",
      "citation_key": "tran2022bvd",
      "metadata": {
        "title": "Improving Local Features with Relevant Spatial Information by Vision Transformer for Crowd Counting",
        "authors": [
          "Nguyen H. Tran",
          "Ta Duc Huy",
          "S. T. Duong",
          "Nguyen Phan",
          "Dao Huu Hung",
          "Chanh D. Tr. Nguyen",
          "Trung Bui",
          "S. Q. Truong"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/791d1e306eaa2e87657925ec4f45661baa8da58b.pdf",
        "venue": "British Machine Vision Conference",
        "citationCount": 14,
        "score": 4.666666666666666,
        "summary": "",
        "keywords": []
      },
      "file_name": "791d1e306eaa2e87657925ec4f45661baa8da58b.pdf"
    },
    {
      "success": true,
      "doc_id": "781302a9814a55e7212c1eeb91cf85e3",
      "summary": "Modeling in computer vision has long been dominated by convolutional neural networks (CNNs). Recently, in light of the excellent performances of self-attention mech-anism in the language field, transformers tailored for visual data have drawn numerous attention and triumphed CNNs in various vision tasks. These vision transformers heavily rely on large-scale pre-training to achieve competitive accuracy, which not only hinders the freedom of architectural design in downstream tasks like object detection, but also causes learning bias and domain mismatch in the fine-tuning stages. To this end, we aim to get rid of the pre-train & fine-tune paradigm of vision transformer and train transformer based object detector from scratch. Some earlier work in the CNNs era have successfully trained CNNs based detectors without pre-training, unfortunately, their findings do not generalize well when the backbone is switched from CNNs to vision transformer. Instead of proposing a specific vision transformer based detector, in this work, our goal is to reveal the insights of training vision transformer based detectors from scratch. In particular, we expect those insights can help other re-searchers and practitioners, and inspire more interesting research in other fields, such as semantic segmentation, visual-linguistic pre-training, etc. One of the key findings is that both architectural changes and more epochs play critical roles in training vision transformer based detectors from scratch. Experiments on MS COCO datasets demonstrate that vision transformer based detectors trained from scratch can also achieve similar performances to their counterparts with ImageNet pre-training.",
      "intriguing_abstract": "Modeling in computer vision has long been dominated by convolutional neural networks (CNNs). Recently, in light of the excellent performances of self-attention mech-anism in the language field, transformers tailored for visual data have drawn numerous attention and triumphed CNNs in various vision tasks. These vision transformers heavily rely on large-scale pre-training to achieve competitive accuracy, which not only hinders the freedom of architectural design in downstream tasks like object detection, but also causes learning bias and domain mismatch in the fine-tuning stages. To this end, we aim to get rid of the pre-train & fine-tune paradigm of vision transformer and train transformer based object detector from scratch. Some earlier work in the CNNs era have successfully trained CNNs based detectors without pre-training, unfortunately, their findings do not generalize well when the backbone is switched from CNNs to vision transformer. Instead of proposing a specific vision transformer based detector, in this work, our goal is to reveal the insights of training vision transformer based detectors from scratch. In particular, we expect those insights can help other re-searchers and practitioners, and inspire more interesting research in other fields, such as semantic segmentation, visual-linguistic pre-training, etc. One of the key findings is that both architectural changes and more epochs play critical roles in training vision transformer based detectors from scratch. Experiments on MS COCO datasets demonstrate that vision transformer based detectors trained from scratch can also achieve similar performances to their counterparts with ImageNet pre-training.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/1d8ecf2c3a78e5dc4de9e4c41961771bbd0033b3.pdf",
      "citation_key": "hong2022ks6",
      "metadata": {
        "title": "Training Object Detectors from Scratch: An Empirical Study in the Era of Vision Transformer",
        "authors": [
          "Weixiang Hong",
          "Jiangwei Lao",
          "Wang Ren",
          "Jian Wang",
          "Jingdong Chen"
        ],
        "published_date": "2022",
        "abstract": "Modeling in computer vision has long been dominated by convolutional neural networks (CNNs). Recently, in light of the excellent performances of self-attention mech-anism in the language field, transformers tailored for visual data have drawn numerous attention and triumphed CNNs in various vision tasks. These vision transformers heavily rely on large-scale pre-training to achieve competitive accuracy, which not only hinders the freedom of architectural design in downstream tasks like object detection, but also causes learning bias and domain mismatch in the fine-tuning stages. To this end, we aim to get rid of the pre-train & fine-tune paradigm of vision transformer and train transformer based object detector from scratch. Some earlier work in the CNNs era have successfully trained CNNs based detectors without pre-training, unfortunately, their findings do not generalize well when the backbone is switched from CNNs to vision transformer. Instead of proposing a specific vision transformer based detector, in this work, our goal is to reveal the insights of training vision transformer based detectors from scratch. In particular, we expect those insights can help other re-searchers and practitioners, and inspire more interesting research in other fields, such as semantic segmentation, visual-linguistic pre-training, etc. One of the key findings is that both architectural changes and more epochs play critical roles in training vision transformer based detectors from scratch. Experiments on MS COCO datasets demonstrate that vision transformer based detectors trained from scratch can also achieve similar performances to their counterparts with ImageNet pre-training.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1d8ecf2c3a78e5dc4de9e4c41961771bbd0033b3.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 14,
        "score": 4.666666666666666,
        "summary": "Modeling in computer vision has long been dominated by convolutional neural networks (CNNs). Recently, in light of the excellent performances of self-attention mech-anism in the language field, transformers tailored for visual data have drawn numerous attention and triumphed CNNs in various vision tasks. These vision transformers heavily rely on large-scale pre-training to achieve competitive accuracy, which not only hinders the freedom of architectural design in downstream tasks like object detection, but also causes learning bias and domain mismatch in the fine-tuning stages. To this end, we aim to get rid of the pre-train & fine-tune paradigm of vision transformer and train transformer based object detector from scratch. Some earlier work in the CNNs era have successfully trained CNNs based detectors without pre-training, unfortunately, their findings do not generalize well when the backbone is switched from CNNs to vision transformer. Instead of proposing a specific vision transformer based detector, in this work, our goal is to reveal the insights of training vision transformer based detectors from scratch. In particular, we expect those insights can help other re-searchers and practitioners, and inspire more interesting research in other fields, such as semantic segmentation, visual-linguistic pre-training, etc. One of the key findings is that both architectural changes and more epochs play critical roles in training vision transformer based detectors from scratch. Experiments on MS COCO datasets demonstrate that vision transformer based detectors trained from scratch can also achieve similar performances to their counterparts with ImageNet pre-training.",
        "keywords": []
      },
      "file_name": "1d8ecf2c3a78e5dc4de9e4c41961771bbd0033b3.pdf"
    },
    {
      "success": true,
      "doc_id": "52d6a7e77d9c9e22b74cf80d63fb311f",
      "summary": "Due to the various sizes of each object, such as kilometer stones, detection is still a challenge, and it directly impacts the accuracy of these object counts. Transformers have demonstrated impressive results in various natural language processing (NLP) and image processing tasks due to long-range modeling dependencies. This paper aims to propose an exceeding you only look once (YOLO) series with two contributions: (i) We propose to employ a pre-training objective to gain the original visual tokens based on the image patches on road asset images. By utilizing pre-training Vision Transformer (ViT) as a backbone, we immediately fine-tune the model weights on downstream tasks by joining task layers upon the pre-trained encoder. (ii) We apply Feature Pyramid Network (FPN) decoder designs to our deep learning network to learn the importance of different input features instead of simply summing up or concatenating, which may cause feature mismatch and performance degradation. Conclusively, our proposed method (Transformer-Based YOLOX with FPN) learns very general representations of objects. It significantly outperforms other state-of-the-art (SOTA) detectors, including YOLOv5S, YOLOv5M, and YOLOv5L. We boosted it to 61.5% AP on the Thailand highway corpus, surpassing the current best practice (YOLOv5L) by 2.56% AP for the test-dev data set.",
      "intriguing_abstract": "Due to the various sizes of each object, such as kilometer stones, detection is still a challenge, and it directly impacts the accuracy of these object counts. Transformers have demonstrated impressive results in various natural language processing (NLP) and image processing tasks due to long-range modeling dependencies. This paper aims to propose an exceeding you only look once (YOLO) series with two contributions: (i) We propose to employ a pre-training objective to gain the original visual tokens based on the image patches on road asset images. By utilizing pre-training Vision Transformer (ViT) as a backbone, we immediately fine-tune the model weights on downstream tasks by joining task layers upon the pre-trained encoder. (ii) We apply Feature Pyramid Network (FPN) decoder designs to our deep learning network to learn the importance of different input features instead of simply summing up or concatenating, which may cause feature mismatch and performance degradation. Conclusively, our proposed method (Transformer-Based YOLOX with FPN) learns very general representations of objects. It significantly outperforms other state-of-the-art (SOTA) detectors, including YOLOv5S, YOLOv5M, and YOLOv5L. We boosted it to 61.5% AP on the Thailand highway corpus, surpassing the current best practice (YOLOv5L) by 2.56% AP for the test-dev data set.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/e1f7478294fe01ce271cdef9ba93f4c675d92dc9.pdf",
      "citation_key": "panboonyuen2021b4h",
      "metadata": {
        "title": "Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama",
        "authors": [
          "Teerapong Panboonyuen",
          "Sittinun Thongbai",
          "W. Wongweeranimit",
          "P. Santitamnont",
          "Kittiwan Suphan",
          "Chaiyut Charoenphon"
        ],
        "published_date": "2021",
        "abstract": "Due to the various sizes of each object, such as kilometer stones, detection is still a challenge, and it directly impacts the accuracy of these object counts. Transformers have demonstrated impressive results in various natural language processing (NLP) and image processing tasks due to long-range modeling dependencies. This paper aims to propose an exceeding you only look once (YOLO) series with two contributions: (i) We propose to employ a pre-training objective to gain the original visual tokens based on the image patches on road asset images. By utilizing pre-training Vision Transformer (ViT) as a backbone, we immediately fine-tune the model weights on downstream tasks by joining task layers upon the pre-trained encoder. (ii) We apply Feature Pyramid Network (FPN) decoder designs to our deep learning network to learn the importance of different input features instead of simply summing up or concatenating, which may cause feature mismatch and performance degradation. Conclusively, our proposed method (Transformer-Based YOLOX with FPN) learns very general representations of objects. It significantly outperforms other state-of-the-art (SOTA) detectors, including YOLOv5S, YOLOv5M, and YOLOv5L. We boosted it to 61.5% AP on the Thailand highway corpus, surpassing the current best practice (YOLOv5L) by 2.56% AP for the test-dev data set.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e1f7478294fe01ce271cdef9ba93f4c675d92dc9.pdf",
        "venue": "Inf.",
        "citationCount": 18,
        "score": 4.5,
        "summary": "Due to the various sizes of each object, such as kilometer stones, detection is still a challenge, and it directly impacts the accuracy of these object counts. Transformers have demonstrated impressive results in various natural language processing (NLP) and image processing tasks due to long-range modeling dependencies. This paper aims to propose an exceeding you only look once (YOLO) series with two contributions: (i) We propose to employ a pre-training objective to gain the original visual tokens based on the image patches on road asset images. By utilizing pre-training Vision Transformer (ViT) as a backbone, we immediately fine-tune the model weights on downstream tasks by joining task layers upon the pre-trained encoder. (ii) We apply Feature Pyramid Network (FPN) decoder designs to our deep learning network to learn the importance of different input features instead of simply summing up or concatenating, which may cause feature mismatch and performance degradation. Conclusively, our proposed method (Transformer-Based YOLOX with FPN) learns very general representations of objects. It significantly outperforms other state-of-the-art (SOTA) detectors, including YOLOv5S, YOLOv5M, and YOLOv5L. We boosted it to 61.5% AP on the Thailand highway corpus, surpassing the current best practice (YOLOv5L) by 2.56% AP for the test-dev data set.",
        "keywords": []
      },
      "file_name": "e1f7478294fe01ce271cdef9ba93f4c675d92dc9.pdf"
    },
    {
      "success": true,
      "doc_id": "6b8f64d0f77ace5df9f9946cf07b30cb",
      "summary": "Global encoding of visual features in video captioning is important for improving the description accuracy. In this paper, we propose a video captioning method that combines Vision Transformer (ViT) and reinforcement learning. Firstly, Resnet-152 and ResNeXt-101 are used to extract features from videos. Secondly, the encoding block of the ViT network is applied to encode video features. Thirdly, the encoded features are fed into a Long Short-Term Memory (LSTM) network to generate a video content description. Finally, the accuracy of video content description is further improved by fine-tuning reinforcement learning. We conducted experiments on the benchmark dataset MSR-VTT used for video captioning. The results show that compared with the current mainstream methods, the model in this paper has improved by 2.9%, 1.4%, 0.9% and 4.8% under the four evaluation indicators of LEU-4, METEOR, ROUGE-L and CIDEr-D, respectively.",
      "intriguing_abstract": "Global encoding of visual features in video captioning is important for improving the description accuracy. In this paper, we propose a video captioning method that combines Vision Transformer (ViT) and reinforcement learning. Firstly, Resnet-152 and ResNeXt-101 are used to extract features from videos. Secondly, the encoding block of the ViT network is applied to encode video features. Thirdly, the encoded features are fed into a Long Short-Term Memory (LSTM) network to generate a video content description. Finally, the accuracy of video content description is further improved by fine-tuning reinforcement learning. We conducted experiments on the benchmark dataset MSR-VTT used for video captioning. The results show that compared with the current mainstream methods, the model in this paper has improved by 2.9%, 1.4%, 0.9% and 4.8% under the four evaluation indicators of LEU-4, METEOR, ROUGE-L and CIDEr-D, respectively.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/a56f8e42e9efe5290602116b42a247b758052fe4.pdf",
      "citation_key": "zhao2022wi7",
      "metadata": {
        "title": "Video captioning based on vision transformer and reinforcement learning",
        "authors": [
          "Hong Zhao",
          "Zhiwen Chen",
          "Lan Guo",
          "Zeyu Han"
        ],
        "published_date": "2022",
        "abstract": "Global encoding of visual features in video captioning is important for improving the description accuracy. In this paper, we propose a video captioning method that combines Vision Transformer (ViT) and reinforcement learning. Firstly, Resnet-152 and ResNeXt-101 are used to extract features from videos. Secondly, the encoding block of the ViT network is applied to encode video features. Thirdly, the encoded features are fed into a Long Short-Term Memory (LSTM) network to generate a video content description. Finally, the accuracy of video content description is further improved by fine-tuning reinforcement learning. We conducted experiments on the benchmark dataset MSR-VTT used for video captioning. The results show that compared with the current mainstream methods, the model in this paper has improved by 2.9%, 1.4%, 0.9% and 4.8% under the four evaluation indicators of LEU-4, METEOR, ROUGE-L and CIDEr-D, respectively.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a56f8e42e9efe5290602116b42a247b758052fe4.pdf",
        "venue": "PeerJ Computer Science",
        "citationCount": 13,
        "score": 4.333333333333333,
        "summary": "Global encoding of visual features in video captioning is important for improving the description accuracy. In this paper, we propose a video captioning method that combines Vision Transformer (ViT) and reinforcement learning. Firstly, Resnet-152 and ResNeXt-101 are used to extract features from videos. Secondly, the encoding block of the ViT network is applied to encode video features. Thirdly, the encoded features are fed into a Long Short-Term Memory (LSTM) network to generate a video content description. Finally, the accuracy of video content description is further improved by fine-tuning reinforcement learning. We conducted experiments on the benchmark dataset MSR-VTT used for video captioning. The results show that compared with the current mainstream methods, the model in this paper has improved by 2.9%, 1.4%, 0.9% and 4.8% under the four evaluation indicators of LEU-4, METEOR, ROUGE-L and CIDEr-D, respectively.",
        "keywords": []
      },
      "file_name": "a56f8e42e9efe5290602116b42a247b758052fe4.pdf"
    },
    {
      "success": true,
      "doc_id": "0b57358cc3897445e875f4243b166526",
      "summary": "Social relationships refer to the connections that exist between people and indicate how people interact in society. The effective recognition of social relationships is conducive to further understanding human behavioral patterns and thus can be vital for more complex social intelligent systems, such as interactive robots and health self-management systems. The existing works about social relation recognition (SRR) focus on extracting features on different scales but lack a comprehensive mechanism to orchestrate various features which show different degrees of importance. In this paper, we propose a new SRR framework, namely Multi-level Transformer-Based Social Relation Recognition (MT-SRR), for better orchestrating features on different scales. Specifically, a vision transformer (ViT) is firstly employed as a feature extraction module for its advantage in exploiting global features. An intra-relation transformer (Intra-TRM) is then introduced to dynamically fuse the extracted features to generate more rational social relation representations. Next, an inter-relation transformer (Inter-TRM) is adopted to further enhance the social relation representations by attentionally utilizing the logical constraints among relationships. In addition, a new margin related to inter-class similarity and a sample number are added to alleviate the challenges of a data imbalance. Extensive experiments demonstrate that MT-SRR can better fuse features on different scales as well as ameliorate the bad effect caused by a data imbalance. The results on the benchmark datasets show that our proposed model outperforms the state-of-the-art methods with significant improvement.",
      "intriguing_abstract": "Social relationships refer to the connections that exist between people and indicate how people interact in society. The effective recognition of social relationships is conducive to further understanding human behavioral patterns and thus can be vital for more complex social intelligent systems, such as interactive robots and health self-management systems. The existing works about social relation recognition (SRR) focus on extracting features on different scales but lack a comprehensive mechanism to orchestrate various features which show different degrees of importance. In this paper, we propose a new SRR framework, namely Multi-level Transformer-Based Social Relation Recognition (MT-SRR), for better orchestrating features on different scales. Specifically, a vision transformer (ViT) is firstly employed as a feature extraction module for its advantage in exploiting global features. An intra-relation transformer (Intra-TRM) is then introduced to dynamically fuse the extracted features to generate more rational social relation representations. Next, an inter-relation transformer (Inter-TRM) is adopted to further enhance the social relation representations by attentionally utilizing the logical constraints among relationships. In addition, a new margin related to inter-class similarity and a sample number are added to alleviate the challenges of a data imbalance. Extensive experiments demonstrate that MT-SRR can better fuse features on different scales as well as ameliorate the bad effect caused by a data imbalance. The results on the benchmark datasets show that our proposed model outperforms the state-of-the-art methods with significant improvement.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/6eb10790689a4cf239d1ee2a3919dced4be6db8f.pdf",
      "citation_key": "wang2022h3u",
      "metadata": {
        "title": "Multi-Level Transformer-Based Social Relation Recognition",
        "authors": [
          "Yuchen Wang",
          "L. Qing",
          "Zhengyong Wang",
          "Yongqiang Cheng",
          "Yonghong Peng"
        ],
        "published_date": "2022",
        "abstract": "Social relationships refer to the connections that exist between people and indicate how people interact in society. The effective recognition of social relationships is conducive to further understanding human behavioral patterns and thus can be vital for more complex social intelligent systems, such as interactive robots and health self-management systems. The existing works about social relation recognition (SRR) focus on extracting features on different scales but lack a comprehensive mechanism to orchestrate various features which show different degrees of importance. In this paper, we propose a new SRR framework, namely Multi-level Transformer-Based Social Relation Recognition (MT-SRR), for better orchestrating features on different scales. Specifically, a vision transformer (ViT) is firstly employed as a feature extraction module for its advantage in exploiting global features. An intra-relation transformer (Intra-TRM) is then introduced to dynamically fuse the extracted features to generate more rational social relation representations. Next, an inter-relation transformer (Inter-TRM) is adopted to further enhance the social relation representations by attentionally utilizing the logical constraints among relationships. In addition, a new margin related to inter-class similarity and a sample number are added to alleviate the challenges of a data imbalance. Extensive experiments demonstrate that MT-SRR can better fuse features on different scales as well as ameliorate the bad effect caused by a data imbalance. The results on the benchmark datasets show that our proposed model outperforms the state-of-the-art methods with significant improvement.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6eb10790689a4cf239d1ee2a3919dced4be6db8f.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 13,
        "score": 4.333333333333333,
        "summary": "Social relationships refer to the connections that exist between people and indicate how people interact in society. The effective recognition of social relationships is conducive to further understanding human behavioral patterns and thus can be vital for more complex social intelligent systems, such as interactive robots and health self-management systems. The existing works about social relation recognition (SRR) focus on extracting features on different scales but lack a comprehensive mechanism to orchestrate various features which show different degrees of importance. In this paper, we propose a new SRR framework, namely Multi-level Transformer-Based Social Relation Recognition (MT-SRR), for better orchestrating features on different scales. Specifically, a vision transformer (ViT) is firstly employed as a feature extraction module for its advantage in exploiting global features. An intra-relation transformer (Intra-TRM) is then introduced to dynamically fuse the extracted features to generate more rational social relation representations. Next, an inter-relation transformer (Inter-TRM) is adopted to further enhance the social relation representations by attentionally utilizing the logical constraints among relationships. In addition, a new margin related to inter-class similarity and a sample number are added to alleviate the challenges of a data imbalance. Extensive experiments demonstrate that MT-SRR can better fuse features on different scales as well as ameliorate the bad effect caused by a data imbalance. The results on the benchmark datasets show that our proposed model outperforms the state-of-the-art methods with significant improvement.",
        "keywords": []
      },
      "file_name": "6eb10790689a4cf239d1ee2a3919dced4be6db8f.pdf"
    },
    {
      "success": true,
      "doc_id": "c8d16c2b26ab59fc4583b480767f834f",
      "summary": "Recently, Vision Transformers (ViT), with the self-attention (SA) as the de facto ingredients, have demon-strated great potential in the computer vision community. For the sake of trade-off between efficiency and performance, a group of works merely perform SA operation within local patches, whereas the global contextual information is abandoned, which would be indispensable for visual recognition tasks. To solve the issue, the subsequent global-local ViTs take a stab at marrying local SA with global one in parallel or alternative way in the model. Nevertheless, the exhaustively combined local and global context may exist redundancy for various visual data, and the receptive field within each layer is fixed. Alternatively, a more graceful way is that global and local context can adaptively contribute per se to accommodate different visual data. To achieve this goal, we in this paper propose a novel ViT architecture, termed NomMer, which can dynamically Nominate the synergistic global-local context in vision transforMer. By investigating the working pattern of NomMer, we further explore what context information is focused. Beneficial from this dynamic nomination mechanism, without bells and whistles, the NomMer can not only achieve 84.5% Top-1 classification accuracy on ImageNet with only 73M parameters, but also show promising performance on dense prediction tasks, i.e., object detection and semantic segmentation. The code and models are publicly available at https://github.com/TencentYoutuResearch/VisualRecognition-NomMer.",
      "intriguing_abstract": "Recently, Vision Transformers (ViT), with the self-attention (SA) as the de facto ingredients, have demon-strated great potential in the computer vision community. For the sake of trade-off between efficiency and performance, a group of works merely perform SA operation within local patches, whereas the global contextual information is abandoned, which would be indispensable for visual recognition tasks. To solve the issue, the subsequent global-local ViTs take a stab at marrying local SA with global one in parallel or alternative way in the model. Nevertheless, the exhaustively combined local and global context may exist redundancy for various visual data, and the receptive field within each layer is fixed. Alternatively, a more graceful way is that global and local context can adaptively contribute per se to accommodate different visual data. To achieve this goal, we in this paper propose a novel ViT architecture, termed NomMer, which can dynamically Nominate the synergistic global-local context in vision transforMer. By investigating the working pattern of NomMer, we further explore what context information is focused. Beneficial from this dynamic nomination mechanism, without bells and whistles, the NomMer can not only achieve 84.5% Top-1 classification accuracy on ImageNet with only 73M parameters, but also show promising performance on dense prediction tasks, i.e., object detection and semantic segmentation. The code and models are publicly available at https://github.com/TencentYoutuResearch/VisualRecognition-NomMer.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/fc2e7fcdc1bd773f1eb097ae67c8f736108276e3.pdf",
      "citation_key": "liu2021yw0",
      "metadata": {
        "title": "NomMer: Nominate Synergistic Context in Vision Transformer for Visual Recognition",
        "authors": [
          "Hao Liu",
          "Xinghua Jiang",
          "Xin Li",
          "Zhimin Bao",
          "Deqiang Jiang",
          "Bo Ren"
        ],
        "published_date": "2021",
        "abstract": "Recently, Vision Transformers (ViT), with the self-attention (SA) as the de facto ingredients, have demon-strated great potential in the computer vision community. For the sake of trade-off between efficiency and performance, a group of works merely perform SA operation within local patches, whereas the global contextual information is abandoned, which would be indispensable for visual recognition tasks. To solve the issue, the subsequent global-local ViTs take a stab at marrying local SA with global one in parallel or alternative way in the model. Nevertheless, the exhaustively combined local and global context may exist redundancy for various visual data, and the receptive field within each layer is fixed. Alternatively, a more graceful way is that global and local context can adaptively contribute per se to accommodate different visual data. To achieve this goal, we in this paper propose a novel ViT architecture, termed NomMer, which can dynamically Nominate the synergistic global-local context in vision transforMer. By investigating the working pattern of NomMer, we further explore what context information is focused. Beneficial from this dynamic nomination mechanism, without bells and whistles, the NomMer can not only achieve 84.5% Top-1 classification accuracy on ImageNet with only 73M parameters, but also show promising performance on dense prediction tasks, i.e., object detection and semantic segmentation. The code and models are publicly available at https://github.com/TencentYoutuResearch/VisualRecognition-NomMer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/fc2e7fcdc1bd773f1eb097ae67c8f736108276e3.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 17,
        "score": 4.25,
        "summary": "Recently, Vision Transformers (ViT), with the self-attention (SA) as the de facto ingredients, have demon-strated great potential in the computer vision community. For the sake of trade-off between efficiency and performance, a group of works merely perform SA operation within local patches, whereas the global contextual information is abandoned, which would be indispensable for visual recognition tasks. To solve the issue, the subsequent global-local ViTs take a stab at marrying local SA with global one in parallel or alternative way in the model. Nevertheless, the exhaustively combined local and global context may exist redundancy for various visual data, and the receptive field within each layer is fixed. Alternatively, a more graceful way is that global and local context can adaptively contribute per se to accommodate different visual data. To achieve this goal, we in this paper propose a novel ViT architecture, termed NomMer, which can dynamically Nominate the synergistic global-local context in vision transforMer. By investigating the working pattern of NomMer, we further explore what context information is focused. Beneficial from this dynamic nomination mechanism, without bells and whistles, the NomMer can not only achieve 84.5% Top-1 classification accuracy on ImageNet with only 73M parameters, but also show promising performance on dense prediction tasks, i.e., object detection and semantic segmentation. The code and models are publicly available at https://github.com/TencentYoutuResearch/VisualRecognition-NomMer.",
        "keywords": []
      },
      "file_name": "fc2e7fcdc1bd773f1eb097ae67c8f736108276e3.pdf"
    },
    {
      "success": true,
      "doc_id": "c67ccf9e75372edd2edbb8eeae4fe9a2",
      "summary": "Whole Slide Image (WSI) analysis is a powerful method to facilitate the diagnosis of cancer in tissue samples. Automating this diagnosis poses various issues, most notably caused by the immense image resolution and limited annotations. WSIs commonly exhibit resolutions of 100, 000  100, 000 pixels. Annotating cancerous areas in WSIs on the pixel-level is prohibitively labor-intensive and requires a high level of expert knowledge. Multiple instance learning (MIL) alleviates the need for expensive pixel-level annotations. In MIL, learning is performed on slide-level labels, in which a pathologist provides information about whether a slide includes cancerous tissue. Here, we propose Self-ViT-MIL, a novel approach for classifying and localizing cancerous areas based on slide-level annotations, eliminating the need for pixel-wise annotated training data. Self-ViTMIL is pre-trained in a self-supervised setting to learn rich feature representation without relying on any labels. The recent Vision Transformer (ViT) architecture builds the feature extractor of Self-ViT-MIL. For localizing cancerous regions, a MIL aggregator with global attention is utilized. To the best of our knowledge, Self-ViTMIL is the first approach to introduce self-supervised ViTs in MIL-based WSI analysis tasks. We showcase the effectiveness of our approach on the common Camelyon16 dataset. Self-ViT-MIL surpasses existing stateof-the-art MIL-based approaches in terms of accuracy and area under the curve (AUC). Code is available at https://github.com/gokberkgul/self-learning-transformer-mil",
      "intriguing_abstract": "Whole Slide Image (WSI) analysis is a powerful method to facilitate the diagnosis of cancer in tissue samples. Automating this diagnosis poses various issues, most notably caused by the immense image resolution and limited annotations. WSIs commonly exhibit resolutions of 100, 000  100, 000 pixels. Annotating cancerous areas in WSIs on the pixel-level is prohibitively labor-intensive and requires a high level of expert knowledge. Multiple instance learning (MIL) alleviates the need for expensive pixel-level annotations. In MIL, learning is performed on slide-level labels, in which a pathologist provides information about whether a slide includes cancerous tissue. Here, we propose Self-ViT-MIL, a novel approach for classifying and localizing cancerous areas based on slide-level annotations, eliminating the need for pixel-wise annotated training data. Self-ViTMIL is pre-trained in a self-supervised setting to learn rich feature representation without relying on any labels. The recent Vision Transformer (ViT) architecture builds the feature extractor of Self-ViT-MIL. For localizing cancerous regions, a MIL aggregator with global attention is utilized. To the best of our knowledge, Self-ViTMIL is the first approach to introduce self-supervised ViTs in MIL-based WSI analysis tasks. We showcase the effectiveness of our approach on the common Camelyon16 dataset. Self-ViT-MIL surpasses existing stateof-the-art MIL-based approaches in terms of accuracy and area under the curve (AUC). Code is available at https://github.com/gokberkgul/self-learning-transformer-mil",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/16ad38c73f4897e1c58326e3562b74ae1c2fd219.pdf",
      "citation_key": "gul202290q",
      "metadata": {
        "title": "Histopathological image classification based on self-supervised vision transformer and weak labels",
        "authors": [
          "A. Gul",
          "Ozdemir Cetin",
          "Christoph Reich",
          "Nadine Flinner",
          "Tim Prangemeier",
          "H. Koeppl"
        ],
        "published_date": "2022",
        "abstract": "Whole Slide Image (WSI) analysis is a powerful method to facilitate the diagnosis of cancer in tissue samples. Automating this diagnosis poses various issues, most notably caused by the immense image resolution and limited annotations. WSIs commonly exhibit resolutions of 100, 000  100, 000 pixels. Annotating cancerous areas in WSIs on the pixel-level is prohibitively labor-intensive and requires a high level of expert knowledge. Multiple instance learning (MIL) alleviates the need for expensive pixel-level annotations. In MIL, learning is performed on slide-level labels, in which a pathologist provides information about whether a slide includes cancerous tissue. Here, we propose Self-ViT-MIL, a novel approach for classifying and localizing cancerous areas based on slide-level annotations, eliminating the need for pixel-wise annotated training data. Self-ViTMIL is pre-trained in a self-supervised setting to learn rich feature representation without relying on any labels. The recent Vision Transformer (ViT) architecture builds the feature extractor of Self-ViT-MIL. For localizing cancerous regions, a MIL aggregator with global attention is utilized. To the best of our knowledge, Self-ViTMIL is the first approach to introduce self-supervised ViTs in MIL-based WSI analysis tasks. We showcase the effectiveness of our approach on the common Camelyon16 dataset. Self-ViT-MIL surpasses existing stateof-the-art MIL-based approaches in terms of accuracy and area under the curve (AUC). Code is available at https://github.com/gokberkgul/self-learning-transformer-mil",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/16ad38c73f4897e1c58326e3562b74ae1c2fd219.pdf",
        "venue": "Medical Imaging",
        "citationCount": 12,
        "score": 4.0,
        "summary": "Whole Slide Image (WSI) analysis is a powerful method to facilitate the diagnosis of cancer in tissue samples. Automating this diagnosis poses various issues, most notably caused by the immense image resolution and limited annotations. WSIs commonly exhibit resolutions of 100, 000  100, 000 pixels. Annotating cancerous areas in WSIs on the pixel-level is prohibitively labor-intensive and requires a high level of expert knowledge. Multiple instance learning (MIL) alleviates the need for expensive pixel-level annotations. In MIL, learning is performed on slide-level labels, in which a pathologist provides information about whether a slide includes cancerous tissue. Here, we propose Self-ViT-MIL, a novel approach for classifying and localizing cancerous areas based on slide-level annotations, eliminating the need for pixel-wise annotated training data. Self-ViTMIL is pre-trained in a self-supervised setting to learn rich feature representation without relying on any labels. The recent Vision Transformer (ViT) architecture builds the feature extractor of Self-ViT-MIL. For localizing cancerous regions, a MIL aggregator with global attention is utilized. To the best of our knowledge, Self-ViTMIL is the first approach to introduce self-supervised ViTs in MIL-based WSI analysis tasks. We showcase the effectiveness of our approach on the common Camelyon16 dataset. Self-ViT-MIL surpasses existing stateof-the-art MIL-based approaches in terms of accuracy and area under the curve (AUC). Code is available at https://github.com/gokberkgul/self-learning-transformer-mil",
        "keywords": []
      },
      "file_name": "16ad38c73f4897e1c58326e3562b74ae1c2fd219.pdf"
    },
    {
      "success": true,
      "doc_id": "75754b7012340cd65c75607884669799",
      "summary": "Recent advances in vision transformers (ViTs) have achieved great performance in visual recognition tasks. Convolutional neural networks (CNNs) exploit spatial inductive bias to learn visual representations, but these networks are spatially local. ViTs can learn global representations with their self-attention mechanism, but they are usually heavy-weight and unsuitable for mobile devices. In this paper, we propose cross feature attention (XFA) to bring down computation cost for transformers, and combine efficient mobile CNNs to form a novel efficient light-weight CNN-ViT hybrid model, XFormer, which can serve as a general-purpose backbone to learn both global and local representation. Experimental results show that XFormer outperforms numerous CNN and ViT-based models across different tasks and datasets. On ImageNet1K dataset, XFormer achieves top-1 accuracy of 78.5% with 5.5 million parameters, which is 2.2% and 6.3% more accurate than EfficientNet-B0 (CNN-based) and DeiT (ViT-based) for similar number of parameters. Our model also performs well when transferring to object detection and semantic segmentation tasks. On MS COCO dataset, XFormer exceeds MobileNetV2 by 10.5 AP (22.7 ->33.2 AP) in YOLOv3 framework with only 6.3M parameters and 3.8G FLOPs. On Cityscapes dataset, with only a simple all-MLP decoder, XFormer achieves mIoU of 78.5 and FPS of 15.3, surpassing state-of-the-art lightweight segmentation networks.",
      "intriguing_abstract": "Recent advances in vision transformers (ViTs) have achieved great performance in visual recognition tasks. Convolutional neural networks (CNNs) exploit spatial inductive bias to learn visual representations, but these networks are spatially local. ViTs can learn global representations with their self-attention mechanism, but they are usually heavy-weight and unsuitable for mobile devices. In this paper, we propose cross feature attention (XFA) to bring down computation cost for transformers, and combine efficient mobile CNNs to form a novel efficient light-weight CNN-ViT hybrid model, XFormer, which can serve as a general-purpose backbone to learn both global and local representation. Experimental results show that XFormer outperforms numerous CNN and ViT-based models across different tasks and datasets. On ImageNet1K dataset, XFormer achieves top-1 accuracy of 78.5% with 5.5 million parameters, which is 2.2% and 6.3% more accurate than EfficientNet-B0 (CNN-based) and DeiT (ViT-based) for similar number of parameters. Our model also performs well when transferring to object detection and semantic segmentation tasks. On MS COCO dataset, XFormer exceeds MobileNetV2 by 10.5 AP (22.7 ->33.2 AP) in YOLOv3 framework with only 6.3M parameters and 3.8G FLOPs. On Cityscapes dataset, with only a simple all-MLP decoder, XFormer achieves mIoU of 78.5 and FPS of 15.3, surpassing state-of-the-art lightweight segmentation networks.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/371e924dd270a213ee6e8d4104a38875105668df.pdf",
      "citation_key": "zhao2022koc",
      "metadata": {
        "title": "Lightweight Vision Transformer with Cross Feature Attention",
        "authors": [
          "Youpeng Zhao",
          "Huadong Tang",
          "Yingying Jiang",
          "A. Yong",
          "Qiang Wu"
        ],
        "published_date": "2022",
        "abstract": "Recent advances in vision transformers (ViTs) have achieved great performance in visual recognition tasks. Convolutional neural networks (CNNs) exploit spatial inductive bias to learn visual representations, but these networks are spatially local. ViTs can learn global representations with their self-attention mechanism, but they are usually heavy-weight and unsuitable for mobile devices. In this paper, we propose cross feature attention (XFA) to bring down computation cost for transformers, and combine efficient mobile CNNs to form a novel efficient light-weight CNN-ViT hybrid model, XFormer, which can serve as a general-purpose backbone to learn both global and local representation. Experimental results show that XFormer outperforms numerous CNN and ViT-based models across different tasks and datasets. On ImageNet1K dataset, XFormer achieves top-1 accuracy of 78.5% with 5.5 million parameters, which is 2.2% and 6.3% more accurate than EfficientNet-B0 (CNN-based) and DeiT (ViT-based) for similar number of parameters. Our model also performs well when transferring to object detection and semantic segmentation tasks. On MS COCO dataset, XFormer exceeds MobileNetV2 by 10.5 AP (22.7 ->33.2 AP) in YOLOv3 framework with only 6.3M parameters and 3.8G FLOPs. On Cityscapes dataset, with only a simple all-MLP decoder, XFormer achieves mIoU of 78.5 and FPS of 15.3, surpassing state-of-the-art lightweight segmentation networks.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/371e924dd270a213ee6e8d4104a38875105668df.pdf",
        "venue": "arXiv.org",
        "citationCount": 12,
        "score": 4.0,
        "summary": "Recent advances in vision transformers (ViTs) have achieved great performance in visual recognition tasks. Convolutional neural networks (CNNs) exploit spatial inductive bias to learn visual representations, but these networks are spatially local. ViTs can learn global representations with their self-attention mechanism, but they are usually heavy-weight and unsuitable for mobile devices. In this paper, we propose cross feature attention (XFA) to bring down computation cost for transformers, and combine efficient mobile CNNs to form a novel efficient light-weight CNN-ViT hybrid model, XFormer, which can serve as a general-purpose backbone to learn both global and local representation. Experimental results show that XFormer outperforms numerous CNN and ViT-based models across different tasks and datasets. On ImageNet1K dataset, XFormer achieves top-1 accuracy of 78.5% with 5.5 million parameters, which is 2.2% and 6.3% more accurate than EfficientNet-B0 (CNN-based) and DeiT (ViT-based) for similar number of parameters. Our model also performs well when transferring to object detection and semantic segmentation tasks. On MS COCO dataset, XFormer exceeds MobileNetV2 by 10.5 AP (22.7 ->33.2 AP) in YOLOv3 framework with only 6.3M parameters and 3.8G FLOPs. On Cityscapes dataset, with only a simple all-MLP decoder, XFormer achieves mIoU of 78.5 and FPS of 15.3, surpassing state-of-the-art lightweight segmentation networks.",
        "keywords": []
      },
      "file_name": "371e924dd270a213ee6e8d4104a38875105668df.pdf"
    },
    {
      "success": true,
      "doc_id": "609c93a2bd88e9395d408de8d49e35d4",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/0025c4241ffb2cce589dc2dcd82385ff06455542.pdf",
      "citation_key": "yang2022qwh",
      "metadata": {
        "title": "Hierarchical Vision Transformer with Channel Attention for RGB-D Image Segmentation",
        "authors": [
          "Yali Yang",
          "Yuanping Xu",
          "Chaolong Zhang",
          "Zhijie Xu",
          "Jian Huang"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0025c4241ffb2cce589dc2dcd82385ff06455542.pdf",
        "venue": "International Symposium on Signal Processing Systems",
        "citationCount": 12,
        "score": 4.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "0025c4241ffb2cce589dc2dcd82385ff06455542.pdf"
    },
    {
      "success": true,
      "doc_id": "9db83fddbd28b85d4352556a21d8cc1b",
      "summary": "Electroencephalography (EEG) signals have a major impact on how well assistive rehabilitation devices work. These signals have become a common technique in recent studies to investigate human motion functions and behaviors. However, incorporating EEG signals to investigate motor planning or movement intention could benefit all patients who can plan motion but are unable to execute it. In this paper, the movement planning of the lower limb was investigated using EEG signal and bilateral movements were employed, including dorsiflexion and plantar flexion of the right and left ankle joint movements. The proposed system uses Continuous Wavelet Transform (CWT) to generate a timefrequency (TF) map of each EEG signal in the motor cortex and then uses the extracted images as input to a deep learning model for classification. Deep Learning (DL) models are created based on vision transformer architecture (ViT) which is the state-of-the-art of image classification and also the proposed models were compared with residual neural network (ResNet). The proposed technique reveals a significant classification performance for the multiclass problem (<inline-formula> <tex-math notation=\"LaTeX\">$p < 0.0001$ </tex-math></inline-formula>) where the classification accuracy was <inline-formula> <tex-math notation=\"LaTeX\">$97.33~\\pm ~1.86$ </tex-math></inline-formula> % and the F score, recall and precision were <inline-formula> <tex-math notation=\"LaTeX\">$97.32~\\pm ~1.88$ </tex-math></inline-formula> %, <inline-formula> <tex-math notation=\"LaTeX\">$97.30~\\pm ~1.90$ </tex-math></inline-formula> % and <inline-formula> <tex-math notation=\"LaTeX\">$97.36~\\pm ~1.81$ </tex-math></inline-formula> % respectively. These results show that DL is a promising technique that can be applied to investigate the users movements intention from EEG signals and highlight the potential of the proposed model for the development of future brain-machine interface (BMI) for neurorehabilitation purposes.",
      "intriguing_abstract": "Electroencephalography (EEG) signals have a major impact on how well assistive rehabilitation devices work. These signals have become a common technique in recent studies to investigate human motion functions and behaviors. However, incorporating EEG signals to investigate motor planning or movement intention could benefit all patients who can plan motion but are unable to execute it. In this paper, the movement planning of the lower limb was investigated using EEG signal and bilateral movements were employed, including dorsiflexion and plantar flexion of the right and left ankle joint movements. The proposed system uses Continuous Wavelet Transform (CWT) to generate a timefrequency (TF) map of each EEG signal in the motor cortex and then uses the extracted images as input to a deep learning model for classification. Deep Learning (DL) models are created based on vision transformer architecture (ViT) which is the state-of-the-art of image classification and also the proposed models were compared with residual neural network (ResNet). The proposed technique reveals a significant classification performance for the multiclass problem (<inline-formula> <tex-math notation=\"LaTeX\">$p < 0.0001$ </tex-math></inline-formula>) where the classification accuracy was <inline-formula> <tex-math notation=\"LaTeX\">$97.33~\\pm ~1.86$ </tex-math></inline-formula> % and the F score, recall and precision were <inline-formula> <tex-math notation=\"LaTeX\">$97.32~\\pm ~1.88$ </tex-math></inline-formula> %, <inline-formula> <tex-math notation=\"LaTeX\">$97.30~\\pm ~1.90$ </tex-math></inline-formula> % and <inline-formula> <tex-math notation=\"LaTeX\">$97.36~\\pm ~1.81$ </tex-math></inline-formula> % respectively. These results show that DL is a promising technique that can be applied to investigate the users movements intention from EEG signals and highlight the potential of the proposed model for the development of future brain-machine interface (BMI) for neurorehabilitation purposes.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f2b3ece7cb0c88701ae0055236f6d6da39c8156d.pdf",
      "citation_key": "alquraishi2022j3v",
      "metadata": {
        "title": "Decoding the Users Movements Preparation From EEG Signals Using Vision Transformer Architecture",
        "authors": [
          "M. S. Al-Quraishi",
          "I. Elamvazuthi",
          "T. Tang",
          "Muhammad Al-Qurishi",
          "Syed Hasan Adil",
          "Mansoor Ebrahim",
          "A. Borboni"
        ],
        "published_date": "2022",
        "abstract": "Electroencephalography (EEG) signals have a major impact on how well assistive rehabilitation devices work. These signals have become a common technique in recent studies to investigate human motion functions and behaviors. However, incorporating EEG signals to investigate motor planning or movement intention could benefit all patients who can plan motion but are unable to execute it. In this paper, the movement planning of the lower limb was investigated using EEG signal and bilateral movements were employed, including dorsiflexion and plantar flexion of the right and left ankle joint movements. The proposed system uses Continuous Wavelet Transform (CWT) to generate a timefrequency (TF) map of each EEG signal in the motor cortex and then uses the extracted images as input to a deep learning model for classification. Deep Learning (DL) models are created based on vision transformer architecture (ViT) which is the state-of-the-art of image classification and also the proposed models were compared with residual neural network (ResNet). The proposed technique reveals a significant classification performance for the multiclass problem (<inline-formula> <tex-math notation=\"LaTeX\">$p < 0.0001$ </tex-math></inline-formula>) where the classification accuracy was <inline-formula> <tex-math notation=\"LaTeX\">$97.33~\\pm ~1.86$ </tex-math></inline-formula> % and the F score, recall and precision were <inline-formula> <tex-math notation=\"LaTeX\">$97.32~\\pm ~1.88$ </tex-math></inline-formula> %, <inline-formula> <tex-math notation=\"LaTeX\">$97.30~\\pm ~1.90$ </tex-math></inline-formula> % and <inline-formula> <tex-math notation=\"LaTeX\">$97.36~\\pm ~1.81$ </tex-math></inline-formula> % respectively. These results show that DL is a promising technique that can be applied to investigate the users movements intention from EEG signals and highlight the potential of the proposed model for the development of future brain-machine interface (BMI) for neurorehabilitation purposes.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f2b3ece7cb0c88701ae0055236f6d6da39c8156d.pdf",
        "venue": "IEEE Access",
        "citationCount": 12,
        "score": 4.0,
        "summary": "Electroencephalography (EEG) signals have a major impact on how well assistive rehabilitation devices work. These signals have become a common technique in recent studies to investigate human motion functions and behaviors. However, incorporating EEG signals to investigate motor planning or movement intention could benefit all patients who can plan motion but are unable to execute it. In this paper, the movement planning of the lower limb was investigated using EEG signal and bilateral movements were employed, including dorsiflexion and plantar flexion of the right and left ankle joint movements. The proposed system uses Continuous Wavelet Transform (CWT) to generate a timefrequency (TF) map of each EEG signal in the motor cortex and then uses the extracted images as input to a deep learning model for classification. Deep Learning (DL) models are created based on vision transformer architecture (ViT) which is the state-of-the-art of image classification and also the proposed models were compared with residual neural network (ResNet). The proposed technique reveals a significant classification performance for the multiclass problem (<inline-formula> <tex-math notation=\"LaTeX\">$p < 0.0001$ </tex-math></inline-formula>) where the classification accuracy was <inline-formula> <tex-math notation=\"LaTeX\">$97.33~\\pm ~1.86$ </tex-math></inline-formula> % and the F score, recall and precision were <inline-formula> <tex-math notation=\"LaTeX\">$97.32~\\pm ~1.88$ </tex-math></inline-formula> %, <inline-formula> <tex-math notation=\"LaTeX\">$97.30~\\pm ~1.90$ </tex-math></inline-formula> % and <inline-formula> <tex-math notation=\"LaTeX\">$97.36~\\pm ~1.81$ </tex-math></inline-formula> % respectively. These results show that DL is a promising technique that can be applied to investigate the users movements intention from EEG signals and highlight the potential of the proposed model for the development of future brain-machine interface (BMI) for neurorehabilitation purposes.",
        "keywords": []
      },
      "file_name": "f2b3ece7cb0c88701ae0055236f6d6da39c8156d.pdf"
    },
    {
      "success": true,
      "doc_id": "c414b25b2883dfdfef8fa5f59727a1b3",
      "summary": "Due to the success of Bidirectional Encoder Representations from Transformers (BERT) in natural language process (NLP), the multi-head attention transformer has been more and more prevalent in computer-vision researches (CV). However, it still remains a challenge for researchers to put forward complex tasks such as vision detection and semantic segmentation. Although multiple Transformer-Based architectures like DETR and ViT-FRCNN have been proposed to complete object detection task, they inevitably decreases discrimination accuracy and brings down computational efficiency caused by the enormous learning parameters and heavy computational complexity incurred by the traditional self-attention operation. In order to alleviate these issues, we present a novel object detection architecture, named Convolutional vision Transformer-Based Attentive Single Shot MultiBox Detector (CvT-ASSD), that built on the top of Convolutional vision Transormer (CvT) with the efficient Attentive Single Shot MultiBox Detector (ASSD). We provide comprehensive empirical evidence showing that our model CvT-ASSD can leads to good system efficiency and performance while being pretrained on large-scale detection datasets such as PASCAL VOC and MS COCO. Code has been released on public github repository at https://github.com/albert-jin/CvT-ASSD.",
      "intriguing_abstract": "Due to the success of Bidirectional Encoder Representations from Transformers (BERT) in natural language process (NLP), the multi-head attention transformer has been more and more prevalent in computer-vision researches (CV). However, it still remains a challenge for researchers to put forward complex tasks such as vision detection and semantic segmentation. Although multiple Transformer-Based architectures like DETR and ViT-FRCNN have been proposed to complete object detection task, they inevitably decreases discrimination accuracy and brings down computational efficiency caused by the enormous learning parameters and heavy computational complexity incurred by the traditional self-attention operation. In order to alleviate these issues, we present a novel object detection architecture, named Convolutional vision Transformer-Based Attentive Single Shot MultiBox Detector (CvT-ASSD), that built on the top of Convolutional vision Transormer (CvT) with the efficient Attentive Single Shot MultiBox Detector (ASSD). We provide comprehensive empirical evidence showing that our model CvT-ASSD can leads to good system efficiency and performance while being pretrained on large-scale detection datasets such as PASCAL VOC and MS COCO. Code has been released on public github repository at https://github.com/albert-jin/CvT-ASSD.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/1eb4bbaa204890a45da2ca713f34fcc2d763c6e1.pdf",
      "citation_key": "jin2021qdw",
      "metadata": {
        "title": "CvT-ASSD: Convolutional vision-Transformer Based Attentive Single Shot MultiBox Detector",
        "authors": [
          "Weiqiang Jin",
          "Hang Yu",
          "Xiangfeng Luo"
        ],
        "published_date": "2021",
        "abstract": "Due to the success of Bidirectional Encoder Representations from Transformers (BERT) in natural language process (NLP), the multi-head attention transformer has been more and more prevalent in computer-vision researches (CV). However, it still remains a challenge for researchers to put forward complex tasks such as vision detection and semantic segmentation. Although multiple Transformer-Based architectures like DETR and ViT-FRCNN have been proposed to complete object detection task, they inevitably decreases discrimination accuracy and brings down computational efficiency caused by the enormous learning parameters and heavy computational complexity incurred by the traditional self-attention operation. In order to alleviate these issues, we present a novel object detection architecture, named Convolutional vision Transformer-Based Attentive Single Shot MultiBox Detector (CvT-ASSD), that built on the top of Convolutional vision Transormer (CvT) with the efficient Attentive Single Shot MultiBox Detector (ASSD). We provide comprehensive empirical evidence showing that our model CvT-ASSD can leads to good system efficiency and performance while being pretrained on large-scale detection datasets such as PASCAL VOC and MS COCO. Code has been released on public github repository at https://github.com/albert-jin/CvT-ASSD.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1eb4bbaa204890a45da2ca713f34fcc2d763c6e1.pdf",
        "venue": "IEEE International Conference on Tools with Artificial Intelligence",
        "citationCount": 15,
        "score": 3.75,
        "summary": "Due to the success of Bidirectional Encoder Representations from Transformers (BERT) in natural language process (NLP), the multi-head attention transformer has been more and more prevalent in computer-vision researches (CV). However, it still remains a challenge for researchers to put forward complex tasks such as vision detection and semantic segmentation. Although multiple Transformer-Based architectures like DETR and ViT-FRCNN have been proposed to complete object detection task, they inevitably decreases discrimination accuracy and brings down computational efficiency caused by the enormous learning parameters and heavy computational complexity incurred by the traditional self-attention operation. In order to alleviate these issues, we present a novel object detection architecture, named Convolutional vision Transformer-Based Attentive Single Shot MultiBox Detector (CvT-ASSD), that built on the top of Convolutional vision Transormer (CvT) with the efficient Attentive Single Shot MultiBox Detector (ASSD). We provide comprehensive empirical evidence showing that our model CvT-ASSD can leads to good system efficiency and performance while being pretrained on large-scale detection datasets such as PASCAL VOC and MS COCO. Code has been released on public github repository at https://github.com/albert-jin/CvT-ASSD.",
        "keywords": []
      },
      "file_name": "1eb4bbaa204890a45da2ca713f34fcc2d763c6e1.pdf"
    },
    {
      "success": true,
      "doc_id": "70e4a70e02cfea10b0bdb55d1c7660a2",
      "summary": "Person re-identification (re-ID) aims to retrieve images of the same identity from a gallery of person images across cameras and viewpoints. However, most works in person re-ID assume a short-term setting characterized by invariance in appearance. In contrast, a high visual variance can be frequently seen in a long-term setting due to changes in apparel and accessories, which makes the task more challenging. Therefore, learning identity-specific features agnostic of temporally variant features is crucial for robust long-term person Re-ID. To this end, we propose an Attribute De-biased Vision Transformer (AD-ViT) to provide direct supervision to learn identity-specific features. Specifically, we produce attribute labels for person instances and utilize them to guide our model to focus on identity features through gradient reversal. Our experiments on two long-term re-ID datasets - LTCC and NKUP show that the proposed work consistently outperforms current state-of-the-art methods.",
      "intriguing_abstract": "Person re-identification (re-ID) aims to retrieve images of the same identity from a gallery of person images across cameras and viewpoints. However, most works in person re-ID assume a short-term setting characterized by invariance in appearance. In contrast, a high visual variance can be frequently seen in a long-term setting due to changes in apparel and accessories, which makes the task more challenging. Therefore, learning identity-specific features agnostic of temporally variant features is crucial for robust long-term person Re-ID. To this end, we propose an Attribute De-biased Vision Transformer (AD-ViT) to provide direct supervision to learn identity-specific features. Specifically, we produce attribute labels for person instances and utilize them to guide our model to focus on identity features through gradient reversal. Our experiments on two long-term re-ID datasets - LTCC and NKUP show that the proposed work consistently outperforms current state-of-the-art methods.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/1b18750ea0d26575f3e2c18f61e444c2790dbe4c.pdf",
      "citation_key": "lee2022rf1",
      "metadata": {
        "title": "Attribute De-biased Vision Transformer (AD-ViT) for Long-Term Person Re-identification",
        "authors": [
          "K. Lee",
          "Bhavin Jawade",
          "D. Mohan",
          "Srirangaraj Setlur",
          "V. Govindaraju"
        ],
        "published_date": "2022",
        "abstract": "Person re-identification (re-ID) aims to retrieve images of the same identity from a gallery of person images across cameras and viewpoints. However, most works in person re-ID assume a short-term setting characterized by invariance in appearance. In contrast, a high visual variance can be frequently seen in a long-term setting due to changes in apparel and accessories, which makes the task more challenging. Therefore, learning identity-specific features agnostic of temporally variant features is crucial for robust long-term person Re-ID. To this end, we propose an Attribute De-biased Vision Transformer (AD-ViT) to provide direct supervision to learn identity-specific features. Specifically, we produce attribute labels for person instances and utilize them to guide our model to focus on identity features through gradient reversal. Our experiments on two long-term re-ID datasets - LTCC and NKUP show that the proposed work consistently outperforms current state-of-the-art methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1b18750ea0d26575f3e2c18f61e444c2790dbe4c.pdf",
        "venue": "Advanced Video and Signal Based Surveillance",
        "citationCount": 11,
        "score": 3.6666666666666665,
        "summary": "Person re-identification (re-ID) aims to retrieve images of the same identity from a gallery of person images across cameras and viewpoints. However, most works in person re-ID assume a short-term setting characterized by invariance in appearance. In contrast, a high visual variance can be frequently seen in a long-term setting due to changes in apparel and accessories, which makes the task more challenging. Therefore, learning identity-specific features agnostic of temporally variant features is crucial for robust long-term person Re-ID. To this end, we propose an Attribute De-biased Vision Transformer (AD-ViT) to provide direct supervision to learn identity-specific features. Specifically, we produce attribute labels for person instances and utilize them to guide our model to focus on identity features through gradient reversal. Our experiments on two long-term re-ID datasets - LTCC and NKUP show that the proposed work consistently outperforms current state-of-the-art methods.",
        "keywords": []
      },
      "file_name": "1b18750ea0d26575f3e2c18f61e444c2790dbe4c.pdf"
    },
    {
      "success": true,
      "doc_id": "d0bbef141cea1325131273ca93cbf6ba",
      "summary": "Non-Intrusive Load Monitoring (NILM) is an effective energy consumption analysis technology, which just requires voltage and current signals on the user bus. This non-invasive monitoring approach can clarify the working state of multiple loads in the building with fewer sensing devices, thus reducing the cost of energy consumption monitoring. In this paper, an NILM method combining adaptive Recurrence Plot (RP) feature extraction and deep-learning-based image recognition is proposed. Firstly, the time-series signal of current is transformed into a threshold-free RP in phase space to obtain the image features. The Euclidean norm in threshold-free RP is scaled exponentially according to the voltage and current correlation to reflect the working characteristics of different loads adaptively. Afterwards, the obtained adaptive RP features can be mapped into images using the corresponding pixel value. In the load identification stage, an advanced computer vision deep network, Hierarchical Vision Transformer using Shifted Windows (Swin-Transformer), is applied to identify the adaptive RP images. The proposed solution is extensively verified by four real, measured load signal datasets, including industrial and household power situations, covering single-phase and three-phase electrical signals. The numerical results demonstrate that the proposed NILM method based on the adaptive RP can effectively improve the accuracy of load detection.",
      "intriguing_abstract": "Non-Intrusive Load Monitoring (NILM) is an effective energy consumption analysis technology, which just requires voltage and current signals on the user bus. This non-invasive monitoring approach can clarify the working state of multiple loads in the building with fewer sensing devices, thus reducing the cost of energy consumption monitoring. In this paper, an NILM method combining adaptive Recurrence Plot (RP) feature extraction and deep-learning-based image recognition is proposed. Firstly, the time-series signal of current is transformed into a threshold-free RP in phase space to obtain the image features. The Euclidean norm in threshold-free RP is scaled exponentially according to the voltage and current correlation to reflect the working characteristics of different loads adaptively. Afterwards, the obtained adaptive RP features can be mapped into images using the corresponding pixel value. In the load identification stage, an advanced computer vision deep network, Hierarchical Vision Transformer using Shifted Windows (Swin-Transformer), is applied to identify the adaptive RP images. The proposed solution is extensively verified by four real, measured load signal datasets, including industrial and household power situations, covering single-phase and three-phase electrical signals. The numerical results demonstrate that the proposed NILM method based on the adaptive RP can effectively improve the accuracy of load detection.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/08502153c9255399f8ff155e5f75900f121bd2ff.pdf",
      "citation_key": "shi2022evc",
      "metadata": {
        "title": "Non-Intrusive Load Monitoring Based on Swin-Transformer with Adaptive Scaling Recurrence Plot",
        "authors": [
          "Yongtao Shi",
          "Xiaodong Zhao",
          "Fan Zhang",
          "Yaguang Kong"
        ],
        "published_date": "2022",
        "abstract": "Non-Intrusive Load Monitoring (NILM) is an effective energy consumption analysis technology, which just requires voltage and current signals on the user bus. This non-invasive monitoring approach can clarify the working state of multiple loads in the building with fewer sensing devices, thus reducing the cost of energy consumption monitoring. In this paper, an NILM method combining adaptive Recurrence Plot (RP) feature extraction and deep-learning-based image recognition is proposed. Firstly, the time-series signal of current is transformed into a threshold-free RP in phase space to obtain the image features. The Euclidean norm in threshold-free RP is scaled exponentially according to the voltage and current correlation to reflect the working characteristics of different loads adaptively. Afterwards, the obtained adaptive RP features can be mapped into images using the corresponding pixel value. In the load identification stage, an advanced computer vision deep network, Hierarchical Vision Transformer using Shifted Windows (Swin-Transformer), is applied to identify the adaptive RP images. The proposed solution is extensively verified by four real, measured load signal datasets, including industrial and household power situations, covering single-phase and three-phase electrical signals. The numerical results demonstrate that the proposed NILM method based on the adaptive RP can effectively improve the accuracy of load detection.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/08502153c9255399f8ff155e5f75900f121bd2ff.pdf",
        "venue": "Energies",
        "citationCount": 11,
        "score": 3.6666666666666665,
        "summary": "Non-Intrusive Load Monitoring (NILM) is an effective energy consumption analysis technology, which just requires voltage and current signals on the user bus. This non-invasive monitoring approach can clarify the working state of multiple loads in the building with fewer sensing devices, thus reducing the cost of energy consumption monitoring. In this paper, an NILM method combining adaptive Recurrence Plot (RP) feature extraction and deep-learning-based image recognition is proposed. Firstly, the time-series signal of current is transformed into a threshold-free RP in phase space to obtain the image features. The Euclidean norm in threshold-free RP is scaled exponentially according to the voltage and current correlation to reflect the working characteristics of different loads adaptively. Afterwards, the obtained adaptive RP features can be mapped into images using the corresponding pixel value. In the load identification stage, an advanced computer vision deep network, Hierarchical Vision Transformer using Shifted Windows (Swin-Transformer), is applied to identify the adaptive RP images. The proposed solution is extensively verified by four real, measured load signal datasets, including industrial and household power situations, covering single-phase and three-phase electrical signals. The numerical results demonstrate that the proposed NILM method based on the adaptive RP can effectively improve the accuracy of load detection.",
        "keywords": []
      },
      "file_name": "08502153c9255399f8ff155e5f75900f121bd2ff.pdf"
    },
    {
      "success": true,
      "doc_id": "41136dc3cc329f57b368cca911ae28ed",
      "summary": "Objectives Accurate histological typing plays an important role in diagnosing thymoma or thymic carcinoma (TC) and predicting the corresponding prognosis. In this paper, we develop and validate a deep learning-based thymoma typing method for hematoxylin & eosin (H&E)-stained whole slide images (WSIs), which provides useful histopathology information from patients to assist doctors for better diagnosing thymoma or TC. Methods We propose a multi-path cross-scale vision transformer (MC-ViT), which first uses the cross attentive scale-aware transformer (CAST) to classify the pathological information related to thymoma, and then uses such pathological information priors to assist the WSIs transformer (WT) for thymoma typing. To make full use of the multi-scale (10, 20, and 40) information inherent in a WSI, CAST not only employs parallel multi-path to capture different receptive field features from multi-scale WSI inputs, but also introduces the cross-correlation attention module (CAM) to aggregate multi-scale features to achieve cross-scale spatial information complementarity. After that, WT can effectively convert full-scale WSIs into 1D feature matrices with pathological information labels to improve the efficiency and accuracy of thymoma typing. Results We construct a large-scale thymoma histopathology WSI (THW) dataset and annotate corresponding pathological information and thymoma typing labels. The proposed MC-ViT achieves the Top-1 accuracy of 0.939 and 0.951 in pathological information classification and thymoma typing, respectively. Moreover, the quantitative and statistical experiments on the THW dataset also demonstrate that our pipeline performs favorably against the existing classical convolutional neural networks, vision transformers, and deep learning-based medical image classification methods. Conclusion This paper demonstrates that comprehensively utilizing the pathological information contained in multi-scale WSIs is feasible for thymoma typing and achieves clinically acceptable performance. Specifically, the proposed MC-ViT can well predict pathological information classes as well as thymoma types, which show the application potential to the diagnosis of thymoma and TC and may assist doctors in improving diagnosis efficiency and accuracy.",
      "intriguing_abstract": "Objectives Accurate histological typing plays an important role in diagnosing thymoma or thymic carcinoma (TC) and predicting the corresponding prognosis. In this paper, we develop and validate a deep learning-based thymoma typing method for hematoxylin & eosin (H&E)-stained whole slide images (WSIs), which provides useful histopathology information from patients to assist doctors for better diagnosing thymoma or TC. Methods We propose a multi-path cross-scale vision transformer (MC-ViT), which first uses the cross attentive scale-aware transformer (CAST) to classify the pathological information related to thymoma, and then uses such pathological information priors to assist the WSIs transformer (WT) for thymoma typing. To make full use of the multi-scale (10, 20, and 40) information inherent in a WSI, CAST not only employs parallel multi-path to capture different receptive field features from multi-scale WSI inputs, but also introduces the cross-correlation attention module (CAM) to aggregate multi-scale features to achieve cross-scale spatial information complementarity. After that, WT can effectively convert full-scale WSIs into 1D feature matrices with pathological information labels to improve the efficiency and accuracy of thymoma typing. Results We construct a large-scale thymoma histopathology WSI (THW) dataset and annotate corresponding pathological information and thymoma typing labels. The proposed MC-ViT achieves the Top-1 accuracy of 0.939 and 0.951 in pathological information classification and thymoma typing, respectively. Moreover, the quantitative and statistical experiments on the THW dataset also demonstrate that our pipeline performs favorably against the existing classical convolutional neural networks, vision transformers, and deep learning-based medical image classification methods. Conclusion This paper demonstrates that comprehensively utilizing the pathological information contained in multi-scale WSIs is feasible for thymoma typing and achieves clinically acceptable performance. Specifically, the proposed MC-ViT can well predict pathological information classes as well as thymoma types, which show the application potential to the diagnosis of thymoma and TC and may assist doctors in improving diagnosis efficiency and accuracy.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/cb85a7648d2a4b4953abfc7437a33f91dc2f3673.pdf",
      "citation_key": "zhang20223g5",
      "metadata": {
        "title": "MC-ViT: Multi-path cross-scale vision transformer for thymoma histopathology whole slide image typing",
        "authors": [
          "Huaqi Zhang",
          "Huang Chen",
          "Jin Qin",
          "Bei-ning Wang",
          "Guolin Ma",
          "Pengyu Wang",
          "Dingrong Zhong",
          "Jie Liu"
        ],
        "published_date": "2022",
        "abstract": "Objectives Accurate histological typing plays an important role in diagnosing thymoma or thymic carcinoma (TC) and predicting the corresponding prognosis. In this paper, we develop and validate a deep learning-based thymoma typing method for hematoxylin & eosin (H&E)-stained whole slide images (WSIs), which provides useful histopathology information from patients to assist doctors for better diagnosing thymoma or TC. Methods We propose a multi-path cross-scale vision transformer (MC-ViT), which first uses the cross attentive scale-aware transformer (CAST) to classify the pathological information related to thymoma, and then uses such pathological information priors to assist the WSIs transformer (WT) for thymoma typing. To make full use of the multi-scale (10, 20, and 40) information inherent in a WSI, CAST not only employs parallel multi-path to capture different receptive field features from multi-scale WSI inputs, but also introduces the cross-correlation attention module (CAM) to aggregate multi-scale features to achieve cross-scale spatial information complementarity. After that, WT can effectively convert full-scale WSIs into 1D feature matrices with pathological information labels to improve the efficiency and accuracy of thymoma typing. Results We construct a large-scale thymoma histopathology WSI (THW) dataset and annotate corresponding pathological information and thymoma typing labels. The proposed MC-ViT achieves the Top-1 accuracy of 0.939 and 0.951 in pathological information classification and thymoma typing, respectively. Moreover, the quantitative and statistical experiments on the THW dataset also demonstrate that our pipeline performs favorably against the existing classical convolutional neural networks, vision transformers, and deep learning-based medical image classification methods. Conclusion This paper demonstrates that comprehensively utilizing the pathological information contained in multi-scale WSIs is feasible for thymoma typing and achieves clinically acceptable performance. Specifically, the proposed MC-ViT can well predict pathological information classes as well as thymoma types, which show the application potential to the diagnosis of thymoma and TC and may assist doctors in improving diagnosis efficiency and accuracy.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/cb85a7648d2a4b4953abfc7437a33f91dc2f3673.pdf",
        "venue": "Frontiers in Oncology",
        "citationCount": 11,
        "score": 3.6666666666666665,
        "summary": "Objectives Accurate histological typing plays an important role in diagnosing thymoma or thymic carcinoma (TC) and predicting the corresponding prognosis. In this paper, we develop and validate a deep learning-based thymoma typing method for hematoxylin & eosin (H&E)-stained whole slide images (WSIs), which provides useful histopathology information from patients to assist doctors for better diagnosing thymoma or TC. Methods We propose a multi-path cross-scale vision transformer (MC-ViT), which first uses the cross attentive scale-aware transformer (CAST) to classify the pathological information related to thymoma, and then uses such pathological information priors to assist the WSIs transformer (WT) for thymoma typing. To make full use of the multi-scale (10, 20, and 40) information inherent in a WSI, CAST not only employs parallel multi-path to capture different receptive field features from multi-scale WSI inputs, but also introduces the cross-correlation attention module (CAM) to aggregate multi-scale features to achieve cross-scale spatial information complementarity. After that, WT can effectively convert full-scale WSIs into 1D feature matrices with pathological information labels to improve the efficiency and accuracy of thymoma typing. Results We construct a large-scale thymoma histopathology WSI (THW) dataset and annotate corresponding pathological information and thymoma typing labels. The proposed MC-ViT achieves the Top-1 accuracy of 0.939 and 0.951 in pathological information classification and thymoma typing, respectively. Moreover, the quantitative and statistical experiments on the THW dataset also demonstrate that our pipeline performs favorably against the existing classical convolutional neural networks, vision transformers, and deep learning-based medical image classification methods. Conclusion This paper demonstrates that comprehensively utilizing the pathological information contained in multi-scale WSIs is feasible for thymoma typing and achieves clinically acceptable performance. Specifically, the proposed MC-ViT can well predict pathological information classes as well as thymoma types, which show the application potential to the diagnosis of thymoma and TC and may assist doctors in improving diagnosis efficiency and accuracy.",
        "keywords": []
      },
      "file_name": "cb85a7648d2a4b4953abfc7437a33f91dc2f3673.pdf"
    },
    {
      "success": true,
      "doc_id": "dd977d22d4a1a1ae41cba846096646e2",
      "summary": "Rolling bearings are the most crucial components of rotating machinery. Identifying defective bearings in a timely manner may prevent the malfunction of an entire machinery system. The mechanical condition monitoring field has entered the big data phase as a result of the fast advancement of machine parts. When working with large amounts of data, the manual feature extraction approach has the drawback of being inefficient and inaccurate. Data-driven methods like Deep Learning have been successfully used in recent years for mechanical intelligent fault detection. Convolutional neural networks (CNNs) were mostly used in earlier research to detect and identify bearing faults. The CNN model, however, suffers from the drawback of having trouble managing fault-time information, which results in a lack of classification results. In this study, bearing defects have been classified using a state-of-the-art Vision Transformer (ViT). Bearing defects were classified using Case Western Reserve University (CWRU) bearing failure laboratory experimental data. The research took into account 13 distinct kinds of defects under 0-load situations in addition to normal bearing conditions. Using the Short Time Fourier Transform (STFT), the vibration signals were converted into 2D time-frequency images. The 2D time-frequency images are then used as input parameters for the ViT. The model achieved an overall accuracy of 98.8%.",
      "intriguing_abstract": "Rolling bearings are the most crucial components of rotating machinery. Identifying defective bearings in a timely manner may prevent the malfunction of an entire machinery system. The mechanical condition monitoring field has entered the big data phase as a result of the fast advancement of machine parts. When working with large amounts of data, the manual feature extraction approach has the drawback of being inefficient and inaccurate. Data-driven methods like Deep Learning have been successfully used in recent years for mechanical intelligent fault detection. Convolutional neural networks (CNNs) were mostly used in earlier research to detect and identify bearing faults. The CNN model, however, suffers from the drawback of having trouble managing fault-time information, which results in a lack of classification results. In this study, bearing defects have been classified using a state-of-the-art Vision Transformer (ViT). Bearing defects were classified using Case Western Reserve University (CWRU) bearing failure laboratory experimental data. The research took into account 13 distinct kinds of defects under 0-load situations in addition to normal bearing conditions. Using the Short Time Fourier Transform (STFT), the vibration signals were converted into 2D time-frequency images. The 2D time-frequency images are then used as input parameters for the ViT. The model achieved an overall accuracy of 98.8%.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/90f9b2892a437ca32c4ab26c09b8dbb7de8fa034.pdf",
      "citation_key": "zim202282d",
      "metadata": {
        "title": "A Vision Transformer-Based Approach to Bearing Fault Classification via Vibration Signals",
        "authors": [
          "Abid Hasan Zim",
          "Aeyan Ashraf",
          "Aquib Iqbal",
          "Asad Malik",
          "M. Kuribayashi"
        ],
        "published_date": "2022",
        "abstract": "Rolling bearings are the most crucial components of rotating machinery. Identifying defective bearings in a timely manner may prevent the malfunction of an entire machinery system. The mechanical condition monitoring field has entered the big data phase as a result of the fast advancement of machine parts. When working with large amounts of data, the manual feature extraction approach has the drawback of being inefficient and inaccurate. Data-driven methods like Deep Learning have been successfully used in recent years for mechanical intelligent fault detection. Convolutional neural networks (CNNs) were mostly used in earlier research to detect and identify bearing faults. The CNN model, however, suffers from the drawback of having trouble managing fault-time information, which results in a lack of classification results. In this study, bearing defects have been classified using a state-of-the-art Vision Transformer (ViT). Bearing defects were classified using Case Western Reserve University (CWRU) bearing failure laboratory experimental data. The research took into account 13 distinct kinds of defects under 0-load situations in addition to normal bearing conditions. Using the Short Time Fourier Transform (STFT), the vibration signals were converted into 2D time-frequency images. The 2D time-frequency images are then used as input parameters for the ViT. The model achieved an overall accuracy of 98.8%.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/90f9b2892a437ca32c4ab26c09b8dbb7de8fa034.pdf",
        "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
        "citationCount": 11,
        "score": 3.6666666666666665,
        "summary": "Rolling bearings are the most crucial components of rotating machinery. Identifying defective bearings in a timely manner may prevent the malfunction of an entire machinery system. The mechanical condition monitoring field has entered the big data phase as a result of the fast advancement of machine parts. When working with large amounts of data, the manual feature extraction approach has the drawback of being inefficient and inaccurate. Data-driven methods like Deep Learning have been successfully used in recent years for mechanical intelligent fault detection. Convolutional neural networks (CNNs) were mostly used in earlier research to detect and identify bearing faults. The CNN model, however, suffers from the drawback of having trouble managing fault-time information, which results in a lack of classification results. In this study, bearing defects have been classified using a state-of-the-art Vision Transformer (ViT). Bearing defects were classified using Case Western Reserve University (CWRU) bearing failure laboratory experimental data. The research took into account 13 distinct kinds of defects under 0-load situations in addition to normal bearing conditions. Using the Short Time Fourier Transform (STFT), the vibration signals were converted into 2D time-frequency images. The 2D time-frequency images are then used as input parameters for the ViT. The model achieved an overall accuracy of 98.8%.",
        "keywords": []
      },
      "file_name": "90f9b2892a437ca32c4ab26c09b8dbb7de8fa034.pdf"
    },
    {
      "success": true,
      "doc_id": "333ab9250efbf84491cc6dd5870b0b88",
      "summary": "Landslide susceptibility mapping (LSM) is an important decision basis for regional landslide hazard risk management, territorial spatial planning and landslide decision making. The current convolutional neural network (CNN)-based landslide susceptibility mapping models do not adequately take into account the spatial nature of texture features, and vision transformer (ViT)-based LSM models have high requirements for the amount of training data. In this study, we overcome the shortcomings of CNN and ViT by fusing these two deep learning models (bottleneck transformer network (BoTNet) and convolutional vision transformer network (ConViT)), and the fused model was used to predict the probability of landslide occurrence. First, we integrated historical landslide data and landslide evaluation factors and analysed whether there was covariance in the landslide evaluation factors. Then, the testing accuracy and generalisation ability of the CNN, ViT, BoTNet and ConViT models were compared and analysed. Finally, four landslide susceptibility mapping models were used to predict the probability of landslide occurrence in Pingwu County, Sichuan Province, China. Among them, BoTNet and ConViT had the highest accuracy, both at 87.78%, an improvement of 1.11% compared to a single model, while ConViT had the highest F1-socre at 87.64%, an improvement of 1.28% compared to a single model. The results indicate that the fusion model of CNN and ViT has better LSM performance than the single model. Meanwhile, the evaluation results of this study can be used as one of the basic tools for landslide hazard risk quantification and disaster prevention in Pingwu County.",
      "intriguing_abstract": "Landslide susceptibility mapping (LSM) is an important decision basis for regional landslide hazard risk management, territorial spatial planning and landslide decision making. The current convolutional neural network (CNN)-based landslide susceptibility mapping models do not adequately take into account the spatial nature of texture features, and vision transformer (ViT)-based LSM models have high requirements for the amount of training data. In this study, we overcome the shortcomings of CNN and ViT by fusing these two deep learning models (bottleneck transformer network (BoTNet) and convolutional vision transformer network (ConViT)), and the fused model was used to predict the probability of landslide occurrence. First, we integrated historical landslide data and landslide evaluation factors and analysed whether there was covariance in the landslide evaluation factors. Then, the testing accuracy and generalisation ability of the CNN, ViT, BoTNet and ConViT models were compared and analysed. Finally, four landslide susceptibility mapping models were used to predict the probability of landslide occurrence in Pingwu County, Sichuan Province, China. Among them, BoTNet and ConViT had the highest accuracy, both at 87.78%, an improvement of 1.11% compared to a single model, while ConViT had the highest F1-socre at 87.64%, an improvement of 1.28% compared to a single model. The results indicate that the fusion model of CNN and ViT has better LSM performance than the single model. Meanwhile, the evaluation results of this study can be used as one of the basic tools for landslide hazard risk quantification and disaster prevention in Pingwu County.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/7ead0cae4e67f390b2eb0083117ea8ab90c53b47.pdf",
      "citation_key": "bao202239k",
      "metadata": {
        "title": "Landslide Susceptibility Mapping by Fusing Convolutional Neural Networks and Vision Transformer",
        "authors": [
          "Shuai Bao",
          "Jiping Liu",
          "Liang Wang",
          "Milan Konen",
          "Xianghong Che",
          "Shenghua Xu",
          "Peng Li"
        ],
        "published_date": "2022",
        "abstract": "Landslide susceptibility mapping (LSM) is an important decision basis for regional landslide hazard risk management, territorial spatial planning and landslide decision making. The current convolutional neural network (CNN)-based landslide susceptibility mapping models do not adequately take into account the spatial nature of texture features, and vision transformer (ViT)-based LSM models have high requirements for the amount of training data. In this study, we overcome the shortcomings of CNN and ViT by fusing these two deep learning models (bottleneck transformer network (BoTNet) and convolutional vision transformer network (ConViT)), and the fused model was used to predict the probability of landslide occurrence. First, we integrated historical landslide data and landslide evaluation factors and analysed whether there was covariance in the landslide evaluation factors. Then, the testing accuracy and generalisation ability of the CNN, ViT, BoTNet and ConViT models were compared and analysed. Finally, four landslide susceptibility mapping models were used to predict the probability of landslide occurrence in Pingwu County, Sichuan Province, China. Among them, BoTNet and ConViT had the highest accuracy, both at 87.78%, an improvement of 1.11% compared to a single model, while ConViT had the highest F1-socre at 87.64%, an improvement of 1.28% compared to a single model. The results indicate that the fusion model of CNN and ViT has better LSM performance than the single model. Meanwhile, the evaluation results of this study can be used as one of the basic tools for landslide hazard risk quantification and disaster prevention in Pingwu County.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7ead0cae4e67f390b2eb0083117ea8ab90c53b47.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 11,
        "score": 3.6666666666666665,
        "summary": "Landslide susceptibility mapping (LSM) is an important decision basis for regional landslide hazard risk management, territorial spatial planning and landslide decision making. The current convolutional neural network (CNN)-based landslide susceptibility mapping models do not adequately take into account the spatial nature of texture features, and vision transformer (ViT)-based LSM models have high requirements for the amount of training data. In this study, we overcome the shortcomings of CNN and ViT by fusing these two deep learning models (bottleneck transformer network (BoTNet) and convolutional vision transformer network (ConViT)), and the fused model was used to predict the probability of landslide occurrence. First, we integrated historical landslide data and landslide evaluation factors and analysed whether there was covariance in the landslide evaluation factors. Then, the testing accuracy and generalisation ability of the CNN, ViT, BoTNet and ConViT models were compared and analysed. Finally, four landslide susceptibility mapping models were used to predict the probability of landslide occurrence in Pingwu County, Sichuan Province, China. Among them, BoTNet and ConViT had the highest accuracy, both at 87.78%, an improvement of 1.11% compared to a single model, while ConViT had the highest F1-socre at 87.64%, an improvement of 1.28% compared to a single model. The results indicate that the fusion model of CNN and ViT has better LSM performance than the single model. Meanwhile, the evaluation results of this study can be used as one of the basic tools for landslide hazard risk quantification and disaster prevention in Pingwu County.",
        "keywords": []
      },
      "file_name": "7ead0cae4e67f390b2eb0083117ea8ab90c53b47.pdf"
    },
    {
      "success": true,
      "doc_id": "67f01afb90c48f0f84aa315855003958",
      "summary": "Vision transformers (ViTs) are emerging with significantly improved accuracy in computer vision tasks. However, their complex architecture and enormous computation/storage demand impose urgent needs for new hardware accelerator design methodology. This work proposes an FPGA-aware automatic ViT acceleration framework based on the proposed mixed-scheme quantization. To the best of our knowledge, this is the first FPGA-based ViT acceleration framework exploring model quantization. Compared with state-of-the-art ViT quantization work (algorithmic approach only without hardware acceleration), our quantization achieves 0.31% to 1.25% higher Top-1 accuracy under the same bit-width. Compared with the 32-bit floating-point baseline FPGA accelerator, our accelerator achieves around 5.6 improvement on the frame rate (i.e., 56.4 FPS vs. 10.0 FPS) with 0.83% accuracy drop for DeiT-base.",
      "intriguing_abstract": "Vision transformers (ViTs) are emerging with significantly improved accuracy in computer vision tasks. However, their complex architecture and enormous computation/storage demand impose urgent needs for new hardware accelerator design methodology. This work proposes an FPGA-aware automatic ViT acceleration framework based on the proposed mixed-scheme quantization. To the best of our knowledge, this is the first FPGA-based ViT acceleration framework exploring model quantization. Compared with state-of-the-art ViT quantization work (algorithmic approach only without hardware acceleration), our quantization achieves 0.31% to 1.25% higher Top-1 accuracy under the same bit-width. Compared with the 32-bit floating-point baseline FPGA accelerator, our accelerator achieves around 5.6 improvement on the frame rate (i.e., 56.4 FPS vs. 10.0 FPS) with 0.83% accuracy drop for DeiT-base.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/e4864ea54710b4c8f3b2ca9ac39854bdfd0dae28.pdf",
      "citation_key": "sun2022nny",
      "metadata": {
        "title": "FPGA-aware automatic acceleration framework for vision transformer with mixed-scheme quantization: late breaking results",
        "authors": [
          "Mengshu Sun",
          "Z. Li",
          "Alec Lu",
          "Haoyu Ma",
          "Geng Yuan",
          "Yanyue Xie",
          "Hao Tang",
          "Yanyu Li",
          "M. Leeser",
          "Zhangyang Wang",
          "Xue Lin",
          "Zhenman Fang"
        ],
        "published_date": "2022",
        "abstract": "Vision transformers (ViTs) are emerging with significantly improved accuracy in computer vision tasks. However, their complex architecture and enormous computation/storage demand impose urgent needs for new hardware accelerator design methodology. This work proposes an FPGA-aware automatic ViT acceleration framework based on the proposed mixed-scheme quantization. To the best of our knowledge, this is the first FPGA-based ViT acceleration framework exploring model quantization. Compared with state-of-the-art ViT quantization work (algorithmic approach only without hardware acceleration), our quantization achieves 0.31% to 1.25% higher Top-1 accuracy under the same bit-width. Compared with the 32-bit floating-point baseline FPGA accelerator, our accelerator achieves around 5.6 improvement on the frame rate (i.e., 56.4 FPS vs. 10.0 FPS) with 0.83% accuracy drop for DeiT-base.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e4864ea54710b4c8f3b2ca9ac39854bdfd0dae28.pdf",
        "venue": "Design Automation Conference",
        "citationCount": 11,
        "score": 3.6666666666666665,
        "summary": "Vision transformers (ViTs) are emerging with significantly improved accuracy in computer vision tasks. However, their complex architecture and enormous computation/storage demand impose urgent needs for new hardware accelerator design methodology. This work proposes an FPGA-aware automatic ViT acceleration framework based on the proposed mixed-scheme quantization. To the best of our knowledge, this is the first FPGA-based ViT acceleration framework exploring model quantization. Compared with state-of-the-art ViT quantization work (algorithmic approach only without hardware acceleration), our quantization achieves 0.31% to 1.25% higher Top-1 accuracy under the same bit-width. Compared with the 32-bit floating-point baseline FPGA accelerator, our accelerator achieves around 5.6 improvement on the frame rate (i.e., 56.4 FPS vs. 10.0 FPS) with 0.83% accuracy drop for DeiT-base.",
        "keywords": []
      },
      "file_name": "e4864ea54710b4c8f3b2ca9ac39854bdfd0dae28.pdf"
    },
    {
      "success": true,
      "doc_id": "066b48ed07299a01573252b80d1585df",
      "summary": "Supervised object detection methods provide subpar performance when applied to Foreign Object Debris (FOD) detection because FOD could be arbitrary objects according to the Federal Aviation Administration (FAA) specification. Current supervised object detection algorithms require datasets that contain annotated examples of every to-be-detected object. While a large and expensive dataset could be developed to include common FOD examples, it is infeasible to collect all possible FOD examples in the dataset representation because of the open-ended nature of FOD. Limitations of the dataset could cause FOD detection systems driven by those supervised algorithms to miss certain FOD, which can become dangerous to airport operations. To this end, this paper presents a self-supervised FOD localization by learning to predict the runway images, which avoids the enumeration of FOD annotation examples. The localization method utilizes the Vision Transformer (ViT) to improve localization performance. The experiments show that the method successfully detects arbitrary FOD in real-world runway situations. The paper also provides an extension to the localization result to perform classification; a feature that can be useful to downstream tasks. To train the localization, this paper also presents a simple and realistic dataset creation framework that only collects clean runway images. The training and testing data for this method are collected at a local airport using unmanned aircraft systems (UAS). Additionally, the developed dataset is provided for public use and further studies.",
      "intriguing_abstract": "Supervised object detection methods provide subpar performance when applied to Foreign Object Debris (FOD) detection because FOD could be arbitrary objects according to the Federal Aviation Administration (FAA) specification. Current supervised object detection algorithms require datasets that contain annotated examples of every to-be-detected object. While a large and expensive dataset could be developed to include common FOD examples, it is infeasible to collect all possible FOD examples in the dataset representation because of the open-ended nature of FOD. Limitations of the dataset could cause FOD detection systems driven by those supervised algorithms to miss certain FOD, which can become dangerous to airport operations. To this end, this paper presents a self-supervised FOD localization by learning to predict the runway images, which avoids the enumeration of FOD annotation examples. The localization method utilizes the Vision Transformer (ViT) to improve localization performance. The experiments show that the method successfully detects arbitrary FOD in real-world runway situations. The paper also provides an extension to the localization result to perform classification; a feature that can be useful to downstream tasks. To train the localization, this paper also presents a simple and realistic dataset creation framework that only collects clean runway images. The training and testing data for this method are collected at a local airport using unmanned aircraft systems (UAS). Additionally, the developed dataset is provided for public use and further studies.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/fe6887ee065c4f64a0c8d6054d1fc562766452de.pdf",
      "citation_key": "munyer2022pfs",
      "metadata": {
        "title": "Foreign Object Debris Detection for Airport Pavement Images Based on Self-Supervised Localization and Vision Transformer",
        "authors": [
          "Travis J. E. Munyer",
          "D. Brinkman",
          "Xin Zhong",
          "Chenyu Huang",
          "Iason Konstantzos"
        ],
        "published_date": "2022",
        "abstract": "Supervised object detection methods provide subpar performance when applied to Foreign Object Debris (FOD) detection because FOD could be arbitrary objects according to the Federal Aviation Administration (FAA) specification. Current supervised object detection algorithms require datasets that contain annotated examples of every to-be-detected object. While a large and expensive dataset could be developed to include common FOD examples, it is infeasible to collect all possible FOD examples in the dataset representation because of the open-ended nature of FOD. Limitations of the dataset could cause FOD detection systems driven by those supervised algorithms to miss certain FOD, which can become dangerous to airport operations. To this end, this paper presents a self-supervised FOD localization by learning to predict the runway images, which avoids the enumeration of FOD annotation examples. The localization method utilizes the Vision Transformer (ViT) to improve localization performance. The experiments show that the method successfully detects arbitrary FOD in real-world runway situations. The paper also provides an extension to the localization result to perform classification; a feature that can be useful to downstream tasks. To train the localization, this paper also presents a simple and realistic dataset creation framework that only collects clean runway images. The training and testing data for this method are collected at a local airport using unmanned aircraft systems (UAS). Additionally, the developed dataset is provided for public use and further studies.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/fe6887ee065c4f64a0c8d6054d1fc562766452de.pdf",
        "venue": "2022 International Conference on Computational Science and Computational Intelligence (CSCI)",
        "citationCount": 10,
        "score": 3.333333333333333,
        "summary": "Supervised object detection methods provide subpar performance when applied to Foreign Object Debris (FOD) detection because FOD could be arbitrary objects according to the Federal Aviation Administration (FAA) specification. Current supervised object detection algorithms require datasets that contain annotated examples of every to-be-detected object. While a large and expensive dataset could be developed to include common FOD examples, it is infeasible to collect all possible FOD examples in the dataset representation because of the open-ended nature of FOD. Limitations of the dataset could cause FOD detection systems driven by those supervised algorithms to miss certain FOD, which can become dangerous to airport operations. To this end, this paper presents a self-supervised FOD localization by learning to predict the runway images, which avoids the enumeration of FOD annotation examples. The localization method utilizes the Vision Transformer (ViT) to improve localization performance. The experiments show that the method successfully detects arbitrary FOD in real-world runway situations. The paper also provides an extension to the localization result to perform classification; a feature that can be useful to downstream tasks. To train the localization, this paper also presents a simple and realistic dataset creation framework that only collects clean runway images. The training and testing data for this method are collected at a local airport using unmanned aircraft systems (UAS). Additionally, the developed dataset is provided for public use and further studies.",
        "keywords": []
      },
      "file_name": "fe6887ee065c4f64a0c8d6054d1fc562766452de.pdf"
    },
    {
      "success": true,
      "doc_id": "5eb8328a5c894874c03ff61627cb5284",
      "summary": "Rolling bearing is a key component of rotating machines, its working state directly affects the performance and safety of the whole equipment. Deep learning based on big data is a mainstream means of intelligent mechanical fault diagnosis. The key lies in enhancing fault feature and improving diagnosis accuracy. Different from the Convolution Neural Network (CNN) which relies on the convolution layer to extract the image features, the Vision Transformer (VIT) uses the multi-head attention mechanism to establish the relationship among the pixels in an image. In order to improve the accuracy of rolling bearing fault diagnosis, a new fault diagnosis method based on VIT is proposed. The vibration gray texture images to be input are divided into the patches according to the predetermined size and linearly mapped into input sequences, and the global image information is integrated through the self-attention mechanism to realize fault diagnosis. In order to enhance the expressiveness and generalization ability, the pooling layer is introduced into VIT. The tested results show that the fault diagnosis accuracy of VIT on the test set reaches 94.6%, and the corresponding classification indexes top-1 is 84.2% and top-5 is 95.0%. The accuracy of the new Pooling Vision Transformer (PIT) is 3.3% higher than that of the original VIT, which proves that the introduction to pooling layer can improve the image identification performance of VIT.",
      "intriguing_abstract": "Rolling bearing is a key component of rotating machines, its working state directly affects the performance and safety of the whole equipment. Deep learning based on big data is a mainstream means of intelligent mechanical fault diagnosis. The key lies in enhancing fault feature and improving diagnosis accuracy. Different from the Convolution Neural Network (CNN) which relies on the convolution layer to extract the image features, the Vision Transformer (VIT) uses the multi-head attention mechanism to establish the relationship among the pixels in an image. In order to improve the accuracy of rolling bearing fault diagnosis, a new fault diagnosis method based on VIT is proposed. The vibration gray texture images to be input are divided into the patches according to the predetermined size and linearly mapped into input sequences, and the global image information is integrated through the self-attention mechanism to realize fault diagnosis. In order to enhance the expressiveness and generalization ability, the pooling layer is introduced into VIT. The tested results show that the fault diagnosis accuracy of VIT on the test set reaches 94.6%, and the corresponding classification indexes top-1 is 84.2% and top-5 is 95.0%. The accuracy of the new Pooling Vision Transformer (PIT) is 3.3% higher than that of the original VIT, which proves that the introduction to pooling layer can improve the image identification performance of VIT.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/280ea33e67484c442757fe761b75d871a399905d.pdf",
      "citation_key": "fan2022wve",
      "metadata": {
        "title": "New intelligent fault diagnosis approach of rolling bearing based on improved vibration gray texture image and vision transformer",
        "authors": [
          "Hong-wei Fan",
          "Ningge Ma",
          "Xu-hui Zhang",
          "Ceyi Xue",
          "Jia-teng Ma",
          "Yan Yang"
        ],
        "published_date": "2022",
        "abstract": "Rolling bearing is a key component of rotating machines, its working state directly affects the performance and safety of the whole equipment. Deep learning based on big data is a mainstream means of intelligent mechanical fault diagnosis. The key lies in enhancing fault feature and improving diagnosis accuracy. Different from the Convolution Neural Network (CNN) which relies on the convolution layer to extract the image features, the Vision Transformer (VIT) uses the multi-head attention mechanism to establish the relationship among the pixels in an image. In order to improve the accuracy of rolling bearing fault diagnosis, a new fault diagnosis method based on VIT is proposed. The vibration gray texture images to be input are divided into the patches according to the predetermined size and linearly mapped into input sequences, and the global image information is integrated through the self-attention mechanism to realize fault diagnosis. In order to enhance the expressiveness and generalization ability, the pooling layer is introduced into VIT. The tested results show that the fault diagnosis accuracy of VIT on the test set reaches 94.6%, and the corresponding classification indexes top-1 is 84.2% and top-5 is 95.0%. The accuracy of the new Pooling Vision Transformer (PIT) is 3.3% higher than that of the original VIT, which proves that the introduction to pooling layer can improve the image identification performance of VIT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/280ea33e67484c442757fe761b75d871a399905d.pdf",
        "venue": "Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science",
        "citationCount": 10,
        "score": 3.333333333333333,
        "summary": "Rolling bearing is a key component of rotating machines, its working state directly affects the performance and safety of the whole equipment. Deep learning based on big data is a mainstream means of intelligent mechanical fault diagnosis. The key lies in enhancing fault feature and improving diagnosis accuracy. Different from the Convolution Neural Network (CNN) which relies on the convolution layer to extract the image features, the Vision Transformer (VIT) uses the multi-head attention mechanism to establish the relationship among the pixels in an image. In order to improve the accuracy of rolling bearing fault diagnosis, a new fault diagnosis method based on VIT is proposed. The vibration gray texture images to be input are divided into the patches according to the predetermined size and linearly mapped into input sequences, and the global image information is integrated through the self-attention mechanism to realize fault diagnosis. In order to enhance the expressiveness and generalization ability, the pooling layer is introduced into VIT. The tested results show that the fault diagnosis accuracy of VIT on the test set reaches 94.6%, and the corresponding classification indexes top-1 is 84.2% and top-5 is 95.0%. The accuracy of the new Pooling Vision Transformer (PIT) is 3.3% higher than that of the original VIT, which proves that the introduction to pooling layer can improve the image identification performance of VIT.",
        "keywords": []
      },
      "file_name": "280ea33e67484c442757fe761b75d871a399905d.pdf"
    },
    {
      "success": true,
      "doc_id": "863bab0acf675fc898b68bd0fae08242",
      "summary": "Vision Transformers (ViTs) have proven to be effective, in solving 2D image understanding tasks by training over large-scale image datasets; and meanwhile as a somehow separate track, in modeling the 3D visual world too such as voxels or point clouds. However, with the growing hope that transformers can become the\"universal\"modeling tool for heterogeneous data, ViTs for 2D and 3D tasks have so far adopted vastly different architecture designs that are hardly transferable. That invites an (over-)ambitious question: can we close the gap between the 2D and 3D ViT architectures? As a piloting study, this paper demonstrates the appealing promise to understand the 3D visual world, using a standard 2D ViT architecture, with only minimal customization at the input and output levels without redesigning the pipeline. To build a 3D ViT from its 2D sibling, we\"inflate\"the patch embedding and token sequence, accompanied with new positional encoding mechanisms designed to match the 3D data geometry. The resultant\"minimalist\"3D ViT, named Simple3D-Former, performs surprisingly robustly on popular 3D tasks such as object classification, point cloud segmentation and indoor scene detection, compared to highly customized 3D-specific designs. It can hence act as a strong baseline for new 3D ViTs. Moreover, we note that pursing a unified 2D-3D ViT design has practical relevance besides just scientific curiosity. Specifically, we demonstrate that Simple3D-Former naturally enables to exploit the wealth of pre-trained weights from large-scale realistic 2D images (e.g., ImageNet), which can be plugged in to enhancing the 3D task performance\"for free\".",
      "intriguing_abstract": "Vision Transformers (ViTs) have proven to be effective, in solving 2D image understanding tasks by training over large-scale image datasets; and meanwhile as a somehow separate track, in modeling the 3D visual world too such as voxels or point clouds. However, with the growing hope that transformers can become the\"universal\"modeling tool for heterogeneous data, ViTs for 2D and 3D tasks have so far adopted vastly different architecture designs that are hardly transferable. That invites an (over-)ambitious question: can we close the gap between the 2D and 3D ViT architectures? As a piloting study, this paper demonstrates the appealing promise to understand the 3D visual world, using a standard 2D ViT architecture, with only minimal customization at the input and output levels without redesigning the pipeline. To build a 3D ViT from its 2D sibling, we\"inflate\"the patch embedding and token sequence, accompanied with new positional encoding mechanisms designed to match the 3D data geometry. The resultant\"minimalist\"3D ViT, named Simple3D-Former, performs surprisingly robustly on popular 3D tasks such as object classification, point cloud segmentation and indoor scene detection, compared to highly customized 3D-specific designs. It can hence act as a strong baseline for new 3D ViTs. Moreover, we note that pursing a unified 2D-3D ViT design has practical relevance besides just scientific curiosity. Specifically, we demonstrate that Simple3D-Former naturally enables to exploit the wealth of pre-trained weights from large-scale realistic 2D images (e.g., ImageNet), which can be plugged in to enhancing the 3D task performance\"for free\".",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/29f86d6d1eaba6a466c231f6906b18eae4b2b484.pdf",
      "citation_key": "wang2022gq4",
      "metadata": {
        "title": "Can We Solve 3D Vision Tasks Starting from A 2D Vision Transformer?",
        "authors": [
          "Yi Wang",
          "Zhiwen Fan",
          "Tianlong Chen",
          "Hehe Fan",
          "Zhangyang Wang"
        ],
        "published_date": "2022",
        "abstract": "Vision Transformers (ViTs) have proven to be effective, in solving 2D image understanding tasks by training over large-scale image datasets; and meanwhile as a somehow separate track, in modeling the 3D visual world too such as voxels or point clouds. However, with the growing hope that transformers can become the\"universal\"modeling tool for heterogeneous data, ViTs for 2D and 3D tasks have so far adopted vastly different architecture designs that are hardly transferable. That invites an (over-)ambitious question: can we close the gap between the 2D and 3D ViT architectures? As a piloting study, this paper demonstrates the appealing promise to understand the 3D visual world, using a standard 2D ViT architecture, with only minimal customization at the input and output levels without redesigning the pipeline. To build a 3D ViT from its 2D sibling, we\"inflate\"the patch embedding and token sequence, accompanied with new positional encoding mechanisms designed to match the 3D data geometry. The resultant\"minimalist\"3D ViT, named Simple3D-Former, performs surprisingly robustly on popular 3D tasks such as object classification, point cloud segmentation and indoor scene detection, compared to highly customized 3D-specific designs. It can hence act as a strong baseline for new 3D ViTs. Moreover, we note that pursing a unified 2D-3D ViT design has practical relevance besides just scientific curiosity. Specifically, we demonstrate that Simple3D-Former naturally enables to exploit the wealth of pre-trained weights from large-scale realistic 2D images (e.g., ImageNet), which can be plugged in to enhancing the 3D task performance\"for free\".",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/29f86d6d1eaba6a466c231f6906b18eae4b2b484.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 3.333333333333333,
        "summary": "Vision Transformers (ViTs) have proven to be effective, in solving 2D image understanding tasks by training over large-scale image datasets; and meanwhile as a somehow separate track, in modeling the 3D visual world too such as voxels or point clouds. However, with the growing hope that transformers can become the\"universal\"modeling tool for heterogeneous data, ViTs for 2D and 3D tasks have so far adopted vastly different architecture designs that are hardly transferable. That invites an (over-)ambitious question: can we close the gap between the 2D and 3D ViT architectures? As a piloting study, this paper demonstrates the appealing promise to understand the 3D visual world, using a standard 2D ViT architecture, with only minimal customization at the input and output levels without redesigning the pipeline. To build a 3D ViT from its 2D sibling, we\"inflate\"the patch embedding and token sequence, accompanied with new positional encoding mechanisms designed to match the 3D data geometry. The resultant\"minimalist\"3D ViT, named Simple3D-Former, performs surprisingly robustly on popular 3D tasks such as object classification, point cloud segmentation and indoor scene detection, compared to highly customized 3D-specific designs. It can hence act as a strong baseline for new 3D ViTs. Moreover, we note that pursing a unified 2D-3D ViT design has practical relevance besides just scientific curiosity. Specifically, we demonstrate that Simple3D-Former naturally enables to exploit the wealth of pre-trained weights from large-scale realistic 2D images (e.g., ImageNet), which can be plugged in to enhancing the 3D task performance\"for free\".",
        "keywords": []
      },
      "file_name": "29f86d6d1eaba6a466c231f6906b18eae4b2b484.pdf"
    },
    {
      "success": true,
      "doc_id": "1f162c040aef8185020a64ace491eda4",
      "summary": "Recently, deep-learning (DL)-based crack-detection systems have proven to be the method of choice for image processing-based inspection systems. However, human-like generalization remains challenging, owing to a wide variety of factors such as crack type and size. Additionally, because of their localized receptive fields, CNNs have a high false-detection rate and perform poorly when attempting to capture the relevant areas of an image. This study aims to propose a vision-transformer-based crack-detection framework that treats image data as a succession of small patches, to retrieve global contextual information (GCI) through self-attention (SA) methods, and which addresses the CNNs problem of inductive biases, including the locally constrained receptive-fields and translation-invariance. The vision-transformer (ViT) classifier was tested to enhance crack classification, localization, and segmentation performance by blending with a sliding-window and tubularity-flow-field (TuFF) algorithm. Firstly, the ViT framework was trained on a custom dataset consisting of 45K images with 224  224 pixels resolution, and achieved accuracy, precision, recall, and F1 scores of 0.960, 0.971, 0.950, and 0.960, respectively. Secondly, the trained ViT was integrated with the sliding-window (SW) approach, to obtain a crack-localization map from large images. The SW-based ViT classifier was then merged with the TuFF algorithm, to acquire efficient crack-mapping by suppressing the unwanted regions in the last step. The robustness and adaptability of the proposed integrated-architecture were tested on new data acquired under different conditions and which were not utilized during the training and validation of the model. The proposed ViT-architecture performance was evaluated and compared with that of various state-of-the-art (SOTA) deep-learning approaches. The experimental results show that ViT equipped with a sliding-window and the TuFF algorithm can enhance real-world crack classification, localization, and segmentation performance.",
      "intriguing_abstract": "Recently, deep-learning (DL)-based crack-detection systems have proven to be the method of choice for image processing-based inspection systems. However, human-like generalization remains challenging, owing to a wide variety of factors such as crack type and size. Additionally, because of their localized receptive fields, CNNs have a high false-detection rate and perform poorly when attempting to capture the relevant areas of an image. This study aims to propose a vision-transformer-based crack-detection framework that treats image data as a succession of small patches, to retrieve global contextual information (GCI) through self-attention (SA) methods, and which addresses the CNNs problem of inductive biases, including the locally constrained receptive-fields and translation-invariance. The vision-transformer (ViT) classifier was tested to enhance crack classification, localization, and segmentation performance by blending with a sliding-window and tubularity-flow-field (TuFF) algorithm. Firstly, the ViT framework was trained on a custom dataset consisting of 45K images with 224  224 pixels resolution, and achieved accuracy, precision, recall, and F1 scores of 0.960, 0.971, 0.950, and 0.960, respectively. Secondly, the trained ViT was integrated with the sliding-window (SW) approach, to obtain a crack-localization map from large images. The SW-based ViT classifier was then merged with the TuFF algorithm, to acquire efficient crack-mapping by suppressing the unwanted regions in the last step. The robustness and adaptability of the proposed integrated-architecture were tested on new data acquired under different conditions and which were not utilized during the training and validation of the model. The proposed ViT-architecture performance was evaluated and compared with that of various state-of-the-art (SOTA) deep-learning approaches. The experimental results show that ViT equipped with a sliding-window and the TuFF algorithm can enhance real-world crack classification, localization, and segmentation performance.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d8bf4b494d255c6d9b81e9963f29e40831106e3e.pdf",
      "citation_key": "ali2022dux",
      "metadata": {
        "title": "Crack45K: Integration of Vision Transformer with Tubularity Flow Field (TuFF) and Sliding-Window Approach for Crack-Segmentation in Pavement Structures",
        "authors": [
          "Luqman Ali",
          "Hamad Al Jassmi",
          "Wasif Khan",
          "F. Alnajjar"
        ],
        "published_date": "2022",
        "abstract": "Recently, deep-learning (DL)-based crack-detection systems have proven to be the method of choice for image processing-based inspection systems. However, human-like generalization remains challenging, owing to a wide variety of factors such as crack type and size. Additionally, because of their localized receptive fields, CNNs have a high false-detection rate and perform poorly when attempting to capture the relevant areas of an image. This study aims to propose a vision-transformer-based crack-detection framework that treats image data as a succession of small patches, to retrieve global contextual information (GCI) through self-attention (SA) methods, and which addresses the CNNs problem of inductive biases, including the locally constrained receptive-fields and translation-invariance. The vision-transformer (ViT) classifier was tested to enhance crack classification, localization, and segmentation performance by blending with a sliding-window and tubularity-flow-field (TuFF) algorithm. Firstly, the ViT framework was trained on a custom dataset consisting of 45K images with 224  224 pixels resolution, and achieved accuracy, precision, recall, and F1 scores of 0.960, 0.971, 0.950, and 0.960, respectively. Secondly, the trained ViT was integrated with the sliding-window (SW) approach, to obtain a crack-localization map from large images. The SW-based ViT classifier was then merged with the TuFF algorithm, to acquire efficient crack-mapping by suppressing the unwanted regions in the last step. The robustness and adaptability of the proposed integrated-architecture were tested on new data acquired under different conditions and which were not utilized during the training and validation of the model. The proposed ViT-architecture performance was evaluated and compared with that of various state-of-the-art (SOTA) deep-learning approaches. The experimental results show that ViT equipped with a sliding-window and the TuFF algorithm can enhance real-world crack classification, localization, and segmentation performance.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d8bf4b494d255c6d9b81e9963f29e40831106e3e.pdf",
        "venue": "Buildings",
        "citationCount": 10,
        "score": 3.333333333333333,
        "summary": "Recently, deep-learning (DL)-based crack-detection systems have proven to be the method of choice for image processing-based inspection systems. However, human-like generalization remains challenging, owing to a wide variety of factors such as crack type and size. Additionally, because of their localized receptive fields, CNNs have a high false-detection rate and perform poorly when attempting to capture the relevant areas of an image. This study aims to propose a vision-transformer-based crack-detection framework that treats image data as a succession of small patches, to retrieve global contextual information (GCI) through self-attention (SA) methods, and which addresses the CNNs problem of inductive biases, including the locally constrained receptive-fields and translation-invariance. The vision-transformer (ViT) classifier was tested to enhance crack classification, localization, and segmentation performance by blending with a sliding-window and tubularity-flow-field (TuFF) algorithm. Firstly, the ViT framework was trained on a custom dataset consisting of 45K images with 224  224 pixels resolution, and achieved accuracy, precision, recall, and F1 scores of 0.960, 0.971, 0.950, and 0.960, respectively. Secondly, the trained ViT was integrated with the sliding-window (SW) approach, to obtain a crack-localization map from large images. The SW-based ViT classifier was then merged with the TuFF algorithm, to acquire efficient crack-mapping by suppressing the unwanted regions in the last step. The robustness and adaptability of the proposed integrated-architecture were tested on new data acquired under different conditions and which were not utilized during the training and validation of the model. The proposed ViT-architecture performance was evaluated and compared with that of various state-of-the-art (SOTA) deep-learning approaches. The experimental results show that ViT equipped with a sliding-window and the TuFF algorithm can enhance real-world crack classification, localization, and segmentation performance.",
        "keywords": []
      },
      "file_name": "d8bf4b494d255c6d9b81e9963f29e40831106e3e.pdf"
    },
    {
      "success": true,
      "doc_id": "e26d8bd9eb47e8a8d816840f985ad20f",
      "summary": "Detecting plant diseases is usually difficult without an experts knowledge. In this study we want to propose a new classification model based on deep learning that will be able to classify and identify different plant-leaf diseases with high accuracy that outperforms the state of the art approaches and previous works. Using only training images, CNN can automatically extract features for classification, and achieve high classification performance. We used two datasets in this study, PlantVillage dataset containing 54,303 healthy and unhealthy leaf images divided into 38 categories by species and disease, and Tomato dataset containing 11,000 healthy and unhealthy tomato leaf images with nine diseases to train the models. We propose a deep convolutional neural network architecture, with and without attention mechanism, and we tuned 4 pretrained models that have been trained on large dataset such as MobileNet, VGG-16, VGG-19 and ResNET. We also tuned 2 ViT models, the vit b32 from keras and the base patch 16 from google. Our porposed model obtained an accuracy up to 97.74%. The pretrained models gave an accuracy up to 99.52%. And the ViT models obtained an accuracy up to 99.7%. This study may aid in detecting the plant leaf diseases and improve life conditions to plants which will improve quality of humans life.",
      "intriguing_abstract": "Detecting plant diseases is usually difficult without an experts knowledge. In this study we want to propose a new classification model based on deep learning that will be able to classify and identify different plant-leaf diseases with high accuracy that outperforms the state of the art approaches and previous works. Using only training images, CNN can automatically extract features for classification, and achieve high classification performance. We used two datasets in this study, PlantVillage dataset containing 54,303 healthy and unhealthy leaf images divided into 38 categories by species and disease, and Tomato dataset containing 11,000 healthy and unhealthy tomato leaf images with nine diseases to train the models. We propose a deep convolutional neural network architecture, with and without attention mechanism, and we tuned 4 pretrained models that have been trained on large dataset such as MobileNet, VGG-16, VGG-19 and ResNET. We also tuned 2 ViT models, the vit b32 from keras and the base patch 16 from google. Our porposed model obtained an accuracy up to 97.74%. The pretrained models gave an accuracy up to 99.52%. And the ViT models obtained an accuracy up to 99.7%. This study may aid in detecting the plant leaf diseases and improve life conditions to plants which will improve quality of humans life.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/abf037290e859a241a5af2c5adf9c08767971683.pdf",
      "citation_key": "chougui2022mpo",
      "metadata": {
        "title": "Plant-Leaf Diseases Classification using CNN, CBAM and Vision Transformer",
        "authors": [
          "Abdeldjalil Chougui",
          "Achraf Moussaoui",
          "A. Moussaoui"
        ],
        "published_date": "2022",
        "abstract": "Detecting plant diseases is usually difficult without an experts knowledge. In this study we want to propose a new classification model based on deep learning that will be able to classify and identify different plant-leaf diseases with high accuracy that outperforms the state of the art approaches and previous works. Using only training images, CNN can automatically extract features for classification, and achieve high classification performance. We used two datasets in this study, PlantVillage dataset containing 54,303 healthy and unhealthy leaf images divided into 38 categories by species and disease, and Tomato dataset containing 11,000 healthy and unhealthy tomato leaf images with nine diseases to train the models. We propose a deep convolutional neural network architecture, with and without attention mechanism, and we tuned 4 pretrained models that have been trained on large dataset such as MobileNet, VGG-16, VGG-19 and ResNET. We also tuned 2 ViT models, the vit b32 from keras and the base patch 16 from google. Our porposed model obtained an accuracy up to 97.74%. The pretrained models gave an accuracy up to 99.52%. And the ViT models obtained an accuracy up to 99.7%. This study may aid in detecting the plant leaf diseases and improve life conditions to plants which will improve quality of humans life.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/abf037290e859a241a5af2c5adf9c08767971683.pdf",
        "venue": "International Symposium on Information and Automation",
        "citationCount": 10,
        "score": 3.333333333333333,
        "summary": "Detecting plant diseases is usually difficult without an experts knowledge. In this study we want to propose a new classification model based on deep learning that will be able to classify and identify different plant-leaf diseases with high accuracy that outperforms the state of the art approaches and previous works. Using only training images, CNN can automatically extract features for classification, and achieve high classification performance. We used two datasets in this study, PlantVillage dataset containing 54,303 healthy and unhealthy leaf images divided into 38 categories by species and disease, and Tomato dataset containing 11,000 healthy and unhealthy tomato leaf images with nine diseases to train the models. We propose a deep convolutional neural network architecture, with and without attention mechanism, and we tuned 4 pretrained models that have been trained on large dataset such as MobileNet, VGG-16, VGG-19 and ResNET. We also tuned 2 ViT models, the vit b32 from keras and the base patch 16 from google. Our porposed model obtained an accuracy up to 97.74%. The pretrained models gave an accuracy up to 99.52%. And the ViT models obtained an accuracy up to 99.7%. This study may aid in detecting the plant leaf diseases and improve life conditions to plants which will improve quality of humans life.",
        "keywords": []
      },
      "file_name": "abf037290e859a241a5af2c5adf9c08767971683.pdf"
    },
    {
      "success": true,
      "doc_id": "7d64de145356e9c27c25a268bf10c954",
      "summary": "Viral diseases are major causes leading to the poor yields of cassava, which is the second-largest source of food carbohydrates in Africa. As symptoms of these diseases can usually be identified by inspecting cassava leafs, visual diagnosis of cassava leaf diseases is of significant importance in food security and agriculture development. Considering the shortage of qualified agricultural experts, automatic approaches for the image-based detection of cassava leaf diseases are in great demand. In this paper, on the basis of Vision Transformer, we propose a deep learning method to identify the type of viral disease in a cassava leaf image. The image dataset of cassava leaves is provided by the Makerere Artificial Intelligence Lab in a Kaggle competition, consisting of 4 subtypes of diseases and healthy cassava leaves. Our results show that Vision-Transformer-based model can effectively achieve an excellent performance regarding the classification of cassava leaf diseases. After applying the K-Fold cross validation technique, our model reaches a categorization accuracy 0.9002 on the private test set. This score ranks top 3% in the leaderboard, and can get a silver medal prize in the Kaggle competition. Our method can be applied for the identification of diseased plants, and potentially prevent the irreparable damage of crops.",
      "intriguing_abstract": "Viral diseases are major causes leading to the poor yields of cassava, which is the second-largest source of food carbohydrates in Africa. As symptoms of these diseases can usually be identified by inspecting cassava leafs, visual diagnosis of cassava leaf diseases is of significant importance in food security and agriculture development. Considering the shortage of qualified agricultural experts, automatic approaches for the image-based detection of cassava leaf diseases are in great demand. In this paper, on the basis of Vision Transformer, we propose a deep learning method to identify the type of viral disease in a cassava leaf image. The image dataset of cassava leaves is provided by the Makerere Artificial Intelligence Lab in a Kaggle competition, consisting of 4 subtypes of diseases and healthy cassava leaves. Our results show that Vision-Transformer-based model can effectively achieve an excellent performance regarding the classification of cassava leaf diseases. After applying the K-Fold cross validation technique, our model reaches a categorization accuracy 0.9002 on the private test set. This score ranks top 3% in the leaderboard, and can get a silver medal prize in the Kaggle competition. Our method can be applied for the identification of diseased plants, and potentially prevent the irreparable damage of crops.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/dd46070ce18f55a5714e53a096c8219d6934d188.pdf",
      "citation_key": "zhuang2021hqu",
      "metadata": {
        "title": "Deep-Learning-Based Diagnosis of Cassava Leaf Diseases Using Vision Transformer",
        "authors": [
          "Li Zhuang"
        ],
        "published_date": "2021",
        "abstract": "Viral diseases are major causes leading to the poor yields of cassava, which is the second-largest source of food carbohydrates in Africa. As symptoms of these diseases can usually be identified by inspecting cassava leafs, visual diagnosis of cassava leaf diseases is of significant importance in food security and agriculture development. Considering the shortage of qualified agricultural experts, automatic approaches for the image-based detection of cassava leaf diseases are in great demand. In this paper, on the basis of Vision Transformer, we propose a deep learning method to identify the type of viral disease in a cassava leaf image. The image dataset of cassava leaves is provided by the Makerere Artificial Intelligence Lab in a Kaggle competition, consisting of 4 subtypes of diseases and healthy cassava leaves. Our results show that Vision-Transformer-based model can effectively achieve an excellent performance regarding the classification of cassava leaf diseases. After applying the K-Fold cross validation technique, our model reaches a categorization accuracy 0.9002 on the private test set. This score ranks top 3% in the leaderboard, and can get a silver medal prize in the Kaggle competition. Our method can be applied for the identification of diseased plants, and potentially prevent the irreparable damage of crops.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/dd46070ce18f55a5714e53a096c8219d6934d188.pdf",
        "venue": "Artificial Intelligence and Cloud Computing Conference",
        "citationCount": 13,
        "score": 3.25,
        "summary": "Viral diseases are major causes leading to the poor yields of cassava, which is the second-largest source of food carbohydrates in Africa. As symptoms of these diseases can usually be identified by inspecting cassava leafs, visual diagnosis of cassava leaf diseases is of significant importance in food security and agriculture development. Considering the shortage of qualified agricultural experts, automatic approaches for the image-based detection of cassava leaf diseases are in great demand. In this paper, on the basis of Vision Transformer, we propose a deep learning method to identify the type of viral disease in a cassava leaf image. The image dataset of cassava leaves is provided by the Makerere Artificial Intelligence Lab in a Kaggle competition, consisting of 4 subtypes of diseases and healthy cassava leaves. Our results show that Vision-Transformer-based model can effectively achieve an excellent performance regarding the classification of cassava leaf diseases. After applying the K-Fold cross validation technique, our model reaches a categorization accuracy 0.9002 on the private test set. This score ranks top 3% in the leaderboard, and can get a silver medal prize in the Kaggle competition. Our method can be applied for the identification of diseased plants, and potentially prevent the irreparable damage of crops.",
        "keywords": []
      },
      "file_name": "dd46070ce18f55a5714e53a096c8219d6934d188.pdf"
    },
    {
      "success": true,
      "doc_id": "ae24946be25d0b20efa39fe3ca192465",
      "summary": "Hyperspectral image classification (HSIC) is a task assigning the correct label to each pixel. It is a hot topic in the remote sensing field, which has been processed in several deep learning methods. Recently, there are some works that apply Vision Transformer (ViT) methods to the HSIC task, but the performance is not as good as some CNN-structured methods, considering that Vision Transformer uses attention to capture global information but ignores local characteristics. In this paper, a multi-stage Vision Transformer model referring to the feature extraction structure of CNN is proposed, and the result shows the realizability and reliability. Besides, experiments show that the modified ViT structure needs more samples for training. An innovative data augmentation method is used to generate extended samples with virtual yet reliable labels. The generated samples are combined with the original ones as the stacked samples, which are used for the following feature extraction process. Experiments explain the optimization of the multi-stage Vision Transformer structure with stacked samples in the accuracy term compared with other methods.",
      "intriguing_abstract": "Hyperspectral image classification (HSIC) is a task assigning the correct label to each pixel. It is a hot topic in the remote sensing field, which has been processed in several deep learning methods. Recently, there are some works that apply Vision Transformer (ViT) methods to the HSIC task, but the performance is not as good as some CNN-structured methods, considering that Vision Transformer uses attention to capture global information but ignores local characteristics. In this paper, a multi-stage Vision Transformer model referring to the feature extraction structure of CNN is proposed, and the result shows the realizability and reliability. Besides, experiments show that the modified ViT structure needs more samples for training. An innovative data augmentation method is used to generate extended samples with virtual yet reliable labels. The generated samples are combined with the original ones as the stacked samples, which are used for the following feature extraction process. Experiments explain the optimization of the multi-stage Vision Transformer structure with stacked samples in the accuracy term compared with other methods.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/829926d9943c99dd64eea5b4b96541eca1e5e91d.pdf",
      "citation_key": "chen2021d1q",
      "metadata": {
        "title": "Hyperspectral Image Classification Based on Multi-stage Vision Transformer with Stacked Samples",
        "authors": [
          "Xiaoyue Chen",
          "Sei-ichiro Kamata",
          "Weilian Zhou"
        ],
        "published_date": "2021",
        "abstract": "Hyperspectral image classification (HSIC) is a task assigning the correct label to each pixel. It is a hot topic in the remote sensing field, which has been processed in several deep learning methods. Recently, there are some works that apply Vision Transformer (ViT) methods to the HSIC task, but the performance is not as good as some CNN-structured methods, considering that Vision Transformer uses attention to capture global information but ignores local characteristics. In this paper, a multi-stage Vision Transformer model referring to the feature extraction structure of CNN is proposed, and the result shows the realizability and reliability. Besides, experiments show that the modified ViT structure needs more samples for training. An innovative data augmentation method is used to generate extended samples with virtual yet reliable labels. The generated samples are combined with the original ones as the stacked samples, which are used for the following feature extraction process. Experiments explain the optimization of the multi-stage Vision Transformer structure with stacked samples in the accuracy term compared with other methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/829926d9943c99dd64eea5b4b96541eca1e5e91d.pdf",
        "venue": "IEEE Region 10 Conference",
        "citationCount": 12,
        "score": 3.0,
        "summary": "Hyperspectral image classification (HSIC) is a task assigning the correct label to each pixel. It is a hot topic in the remote sensing field, which has been processed in several deep learning methods. Recently, there are some works that apply Vision Transformer (ViT) methods to the HSIC task, but the performance is not as good as some CNN-structured methods, considering that Vision Transformer uses attention to capture global information but ignores local characteristics. In this paper, a multi-stage Vision Transformer model referring to the feature extraction structure of CNN is proposed, and the result shows the realizability and reliability. Besides, experiments show that the modified ViT structure needs more samples for training. An innovative data augmentation method is used to generate extended samples with virtual yet reliable labels. The generated samples are combined with the original ones as the stacked samples, which are used for the following feature extraction process. Experiments explain the optimization of the multi-stage Vision Transformer structure with stacked samples in the accuracy term compared with other methods.",
        "keywords": []
      },
      "file_name": "829926d9943c99dd64eea5b4b96541eca1e5e91d.pdf"
    },
    {
      "success": true,
      "doc_id": "0d2a4d2323556e348834c07a20d64898",
      "summary": "We propose a novel hybrid Mamba-Transformer backbone, MambaVision, specifically tailored for vision applications. Our core contribution includes redesigning the Mamba formulation to enhance its capability for efficient modeling of visual features. Through a comprehensive ablation study, we demonstrate the feasibility of integrating Vision Transformers (ViT) with Mamba. Our results show that equipping the Mamba architecture with self-attention blocks in the final layers greatly improves its capacity to capture longrange spatial dependencies. Based on these findings, we introduce a family of MambaVision models with a hierarchical architecture to meet various design criteria. For classification on the ImageNet-1K dataset, MambaVision variants achieve state-of-the-art (SOTA) performance in terms of both Top-1 accuracy and throughput. In downstream tasks such as object detection, instance segmentation, and semantic segmentation on MS COCO and ADE20K datasets, MambaVision outperforms comparably sized backbones while demonstrating favorable performance. Code: https://github.com/NVlabs/MambaVision",
      "intriguing_abstract": "We propose a novel hybrid Mamba-Transformer backbone, MambaVision, specifically tailored for vision applications. Our core contribution includes redesigning the Mamba formulation to enhance its capability for efficient modeling of visual features. Through a comprehensive ablation study, we demonstrate the feasibility of integrating Vision Transformers (ViT) with Mamba. Our results show that equipping the Mamba architecture with self-attention blocks in the final layers greatly improves its capacity to capture longrange spatial dependencies. Based on these findings, we introduce a family of MambaVision models with a hierarchical architecture to meet various design criteria. For classification on the ImageNet-1K dataset, MambaVision variants achieve state-of-the-art (SOTA) performance in terms of both Top-1 accuracy and throughput. In downstream tasks such as object detection, instance segmentation, and semantic segmentation on MS COCO and ADE20K datasets, MambaVision outperforms comparably sized backbones while demonstrating favorable performance. Code: https://github.com/NVlabs/MambaVision",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/e8dceb26166721014b8ecbd11fd212739c18d315.pdf",
      "citation_key": "hatamizadeh2024xr6",
      "metadata": {
        "title": "MambaVision: A Hybrid Mamba-Transformer Vision Backbone",
        "authors": [
          "Ali Hatamizadeh",
          "Jan Kautz"
        ],
        "published_date": "2024",
        "abstract": "We propose a novel hybrid Mamba-Transformer backbone, MambaVision, specifically tailored for vision applications. Our core contribution includes redesigning the Mamba formulation to enhance its capability for efficient modeling of visual features. Through a comprehensive ablation study, we demonstrate the feasibility of integrating Vision Transformers (ViT) with Mamba. Our results show that equipping the Mamba architecture with self-attention blocks in the final layers greatly improves its capacity to capture longrange spatial dependencies. Based on these findings, we introduce a family of MambaVision models with a hierarchical architecture to meet various design criteria. For classification on the ImageNet-1K dataset, MambaVision variants achieve state-of-the-art (SOTA) performance in terms of both Top-1 accuracy and throughput. In downstream tasks such as object detection, instance segmentation, and semantic segmentation on MS COCO and ADE20K datasets, MambaVision outperforms comparably sized backbones while demonstrating favorable performance. Code: https://github.com/NVlabs/MambaVision",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e8dceb26166721014b8ecbd11fd212739c18d315.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 143,
        "score": 143.0,
        "summary": "We propose a novel hybrid Mamba-Transformer backbone, MambaVision, specifically tailored for vision applications. Our core contribution includes redesigning the Mamba formulation to enhance its capability for efficient modeling of visual features. Through a comprehensive ablation study, we demonstrate the feasibility of integrating Vision Transformers (ViT) with Mamba. Our results show that equipping the Mamba architecture with self-attention blocks in the final layers greatly improves its capacity to capture longrange spatial dependencies. Based on these findings, we introduce a family of MambaVision models with a hierarchical architecture to meet various design criteria. For classification on the ImageNet-1K dataset, MambaVision variants achieve state-of-the-art (SOTA) performance in terms of both Top-1 accuracy and throughput. In downstream tasks such as object detection, instance segmentation, and semantic segmentation on MS COCO and ADE20K datasets, MambaVision outperforms comparably sized backbones while demonstrating favorable performance. Code: https://github.com/NVlabs/MambaVision",
        "keywords": []
      },
      "file_name": "e8dceb26166721014b8ecbd11fd212739c18d315.pdf"
    },
    {
      "success": true,
      "doc_id": "1165f855dd3fe401fe853b86660a82da",
      "summary": "Modern hierarchical vision transformers have added several vision-specific components in the pursuit of supervised classification performance. While these components lead to effective accuracies and attractive FLOP counts, the added complexity actually makes these transformers slower than their vanilla ViT counterparts. In this paper, we argue that this additional bulk is unnecessary. By pretraining with a strong visual pretext task (MAE), we can strip out all the bells-and-whistles from a state-of-the-art multi-stage vision transformer without losing accuracy. In the process, we create Hiera, an extremely simple hierarchical vision transformer that is more accurate than previous models while being significantly faster both at inference and during training. We evaluate Hiera on a variety of tasks for image and video recognition. Our code and models are available at https://github.com/facebookresearch/hiera.",
      "intriguing_abstract": "Modern hierarchical vision transformers have added several vision-specific components in the pursuit of supervised classification performance. While these components lead to effective accuracies and attractive FLOP counts, the added complexity actually makes these transformers slower than their vanilla ViT counterparts. In this paper, we argue that this additional bulk is unnecessary. By pretraining with a strong visual pretext task (MAE), we can strip out all the bells-and-whistles from a state-of-the-art multi-stage vision transformer without losing accuracy. In the process, we create Hiera, an extremely simple hierarchical vision transformer that is more accurate than previous models while being significantly faster both at inference and during training. We evaluate Hiera on a variety of tasks for image and video recognition. Our code and models are available at https://github.com/facebookresearch/hiera.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/e06b703146c46a6455fd0c33077de1bea5fdd877.pdf",
      "citation_key": "ryali202339q",
      "metadata": {
        "title": "Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
        "authors": [
          "Chaitanya K. Ryali",
          "Yuan-Ting Hu",
          "Daniel Bolya",
          "Chen Wei",
          "Haoqi Fan",
          "Po-Yao (Bernie) Huang",
          "Vaibhav Aggarwal",
          "Arkabandhu Chowdhury",
          "Omid Poursaeed",
          "Judy Hoffman",
          "J. Malik",
          "Yanghao Li",
          "Christoph Feichtenhofer"
        ],
        "published_date": "2023",
        "abstract": "Modern hierarchical vision transformers have added several vision-specific components in the pursuit of supervised classification performance. While these components lead to effective accuracies and attractive FLOP counts, the added complexity actually makes these transformers slower than their vanilla ViT counterparts. In this paper, we argue that this additional bulk is unnecessary. By pretraining with a strong visual pretext task (MAE), we can strip out all the bells-and-whistles from a state-of-the-art multi-stage vision transformer without losing accuracy. In the process, we create Hiera, an extremely simple hierarchical vision transformer that is more accurate than previous models while being significantly faster both at inference and during training. We evaluate Hiera on a variety of tasks for image and video recognition. Our code and models are available at https://github.com/facebookresearch/hiera.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e06b703146c46a6455fd0c33077de1bea5fdd877.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 244,
        "score": 122.0,
        "summary": "Modern hierarchical vision transformers have added several vision-specific components in the pursuit of supervised classification performance. While these components lead to effective accuracies and attractive FLOP counts, the added complexity actually makes these transformers slower than their vanilla ViT counterparts. In this paper, we argue that this additional bulk is unnecessary. By pretraining with a strong visual pretext task (MAE), we can strip out all the bells-and-whistles from a state-of-the-art multi-stage vision transformer without losing accuracy. In the process, we create Hiera, an extremely simple hierarchical vision transformer that is more accurate than previous models while being significantly faster both at inference and during training. We evaluate Hiera on a variety of tasks for image and video recognition. Our code and models are available at https://github.com/facebookresearch/hiera.",
        "keywords": []
      },
      "file_name": "e06b703146c46a6455fd0c33077de1bea5fdd877.pdf"
    },
    {
      "success": true,
      "doc_id": "4847c07a0ecd0b76d3c10f387f5d0d72",
      "summary": "The recent success of attention mechanism-driven deep models, like vision transformer (ViT) as one of the most representatives, has intrigued a wave of advanced research to explore their adaptation to broader domains. However, current transformer-based approaches in the remote sensing (RS) community pay more attention to single-modality data, which might lose expandability in making full use of the ever-growing multimodal Earth observation data. To this end, we propose a novel multimodal deep learning framework by extending conventional ViT with minimal modifications, aiming at the task of land use and land cover (LULC) classification. Unlike common stems that adopt either linear patch projection or deep regional embedder, our approach processes multimodal RS image patches with parallel branches of position-shared ViTs extended with separable convolution modules, which offers an economical solution to leverage both spatial and modality-specific channel information. Furthermore, to promote information exchange across heterogeneous modalities, their tokenized embeddings are then fused through a cross-modality attention (CMA) module by exploiting pixel-level spatial correlation in RS scenes. Both of these modifications significantly improve the discriminative ability of classification tokens in each modality and thus further performance increase can be finally attained by a full token-based decision-level fusion module. We conduct extensive experiments on two multimodal RS benchmark datasets, i.e., the Houston2013 dataset containing hyperspectral (HS) and light detection and ranging (LiDAR) data, and Berlin dataset with HS and synthetic aperture radar (SAR) data, to demonstrate that our extended vision transformer (ExViT) outperforms concurrent competitors based on transformer or convolutional neural network (CNN) backbones, in addition to several competitive machine-learning-based models. The source codes and investigated datasets of this work will be made publicly available at https://github.com/jingyao16/ExViT.",
      "intriguing_abstract": "The recent success of attention mechanism-driven deep models, like vision transformer (ViT) as one of the most representatives, has intrigued a wave of advanced research to explore their adaptation to broader domains. However, current transformer-based approaches in the remote sensing (RS) community pay more attention to single-modality data, which might lose expandability in making full use of the ever-growing multimodal Earth observation data. To this end, we propose a novel multimodal deep learning framework by extending conventional ViT with minimal modifications, aiming at the task of land use and land cover (LULC) classification. Unlike common stems that adopt either linear patch projection or deep regional embedder, our approach processes multimodal RS image patches with parallel branches of position-shared ViTs extended with separable convolution modules, which offers an economical solution to leverage both spatial and modality-specific channel information. Furthermore, to promote information exchange across heterogeneous modalities, their tokenized embeddings are then fused through a cross-modality attention (CMA) module by exploiting pixel-level spatial correlation in RS scenes. Both of these modifications significantly improve the discriminative ability of classification tokens in each modality and thus further performance increase can be finally attained by a full token-based decision-level fusion module. We conduct extensive experiments on two multimodal RS benchmark datasets, i.e., the Houston2013 dataset containing hyperspectral (HS) and light detection and ranging (LiDAR) data, and Berlin dataset with HS and synthetic aperture radar (SAR) data, to demonstrate that our extended vision transformer (ExViT) outperforms concurrent competitors based on transformer or convolutional neural network (CNN) backbones, in addition to several competitive machine-learning-based models. The source codes and investigated datasets of this work will be made publicly available at https://github.com/jingyao16/ExViT.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/3af375031a3e23b7daf2f1ed14b5b61147996ca0.pdf",
      "citation_key": "yao2023sax",
      "metadata": {
        "title": "Extended Vision Transformer (ExViT) for Land Use and Land Cover Classification: A Multimodal Deep Learning Framework",
        "authors": [
          "Jing Yao",
          "Bing Zhang",
          "Chenyu Li",
          "D. Hong",
          "J. Chanussot"
        ],
        "published_date": "2023",
        "abstract": "The recent success of attention mechanism-driven deep models, like vision transformer (ViT) as one of the most representatives, has intrigued a wave of advanced research to explore their adaptation to broader domains. However, current transformer-based approaches in the remote sensing (RS) community pay more attention to single-modality data, which might lose expandability in making full use of the ever-growing multimodal Earth observation data. To this end, we propose a novel multimodal deep learning framework by extending conventional ViT with minimal modifications, aiming at the task of land use and land cover (LULC) classification. Unlike common stems that adopt either linear patch projection or deep regional embedder, our approach processes multimodal RS image patches with parallel branches of position-shared ViTs extended with separable convolution modules, which offers an economical solution to leverage both spatial and modality-specific channel information. Furthermore, to promote information exchange across heterogeneous modalities, their tokenized embeddings are then fused through a cross-modality attention (CMA) module by exploiting pixel-level spatial correlation in RS scenes. Both of these modifications significantly improve the discriminative ability of classification tokens in each modality and thus further performance increase can be finally attained by a full token-based decision-level fusion module. We conduct extensive experiments on two multimodal RS benchmark datasets, i.e., the Houston2013 dataset containing hyperspectral (HS) and light detection and ranging (LiDAR) data, and Berlin dataset with HS and synthetic aperture radar (SAR) data, to demonstrate that our extended vision transformer (ExViT) outperforms concurrent competitors based on transformer or convolutional neural network (CNN) backbones, in addition to several competitive machine-learning-based models. The source codes and investigated datasets of this work will be made publicly available at https://github.com/jingyao16/ExViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3af375031a3e23b7daf2f1ed14b5b61147996ca0.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 209,
        "score": 104.5,
        "summary": "The recent success of attention mechanism-driven deep models, like vision transformer (ViT) as one of the most representatives, has intrigued a wave of advanced research to explore their adaptation to broader domains. However, current transformer-based approaches in the remote sensing (RS) community pay more attention to single-modality data, which might lose expandability in making full use of the ever-growing multimodal Earth observation data. To this end, we propose a novel multimodal deep learning framework by extending conventional ViT with minimal modifications, aiming at the task of land use and land cover (LULC) classification. Unlike common stems that adopt either linear patch projection or deep regional embedder, our approach processes multimodal RS image patches with parallel branches of position-shared ViTs extended with separable convolution modules, which offers an economical solution to leverage both spatial and modality-specific channel information. Furthermore, to promote information exchange across heterogeneous modalities, their tokenized embeddings are then fused through a cross-modality attention (CMA) module by exploiting pixel-level spatial correlation in RS scenes. Both of these modifications significantly improve the discriminative ability of classification tokens in each modality and thus further performance increase can be finally attained by a full token-based decision-level fusion module. We conduct extensive experiments on two multimodal RS benchmark datasets, i.e., the Houston2013 dataset containing hyperspectral (HS) and light detection and ranging (LiDAR) data, and Berlin dataset with HS and synthetic aperture radar (SAR) data, to demonstrate that our extended vision transformer (ExViT) outperforms concurrent competitors based on transformer or convolutional neural network (CNN) backbones, in addition to several competitive machine-learning-based models. The source codes and investigated datasets of this work will be made publicly available at https://github.com/jingyao16/ExViT.",
        "keywords": []
      },
      "file_name": "3af375031a3e23b7daf2f1ed14b5b61147996ca0.pdf"
    },
    {
      "success": true,
      "doc_id": "5ee10a20a45f56bd7e9ba4eaa3c1dd42",
      "summary": "Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several specific subfields, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research.",
      "intriguing_abstract": "Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several specific subfields, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d203076c28587895aa344d088b2788dbab5e82a1.pdf",
      "citation_key": "li2023287",
      "metadata": {
        "title": "Transformer-Based Visual Segmentation: A Survey",
        "authors": [
          "Xiangtai Li",
          "Henghui Ding",
          "Wenwei Zhang",
          "Haobo Yuan",
          "Jiangmiao Pang",
          "Guangliang Cheng",
          "Kai Chen",
          "Ziwei Liu",
          "Chen Change Loy"
        ],
        "published_date": "2023",
        "abstract": "Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several specific subfields, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d203076c28587895aa344d088b2788dbab5e82a1.pdf",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citationCount": 194,
        "score": 97.0,
        "summary": "Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several specific subfields, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research.",
        "keywords": []
      },
      "file_name": "d203076c28587895aa344d088b2788dbab5e82a1.pdf"
    },
    {
      "success": true,
      "doc_id": "6e0d8fd72bd1a138bd552112a00071d1",
      "summary": "Recently, vision transformer (ViT)-based deep learning (DL) models have achieved remarkable performance gains in hyperspectral image classification (HSIC) due to their abilities to model long-range dependencies and extract global spatial features. However, ViT is built with a stack of Transformer blocks and faces the challenge of learning a large number of parameters when processing hyperspectral data. Besides, the inherent modeling of global correlation in Transformer ignores the effective representation of local spatial and spectral features. To address these issues, we propose a lightweight ViT network known as groupwise separable convolutional ViT (GSC-ViT). First, a groupwise separable convolution (GSC) module, which is a combination of grouped pointwise convolution (GPWC) and group convolution, is designed to significantly decrease the number of convolutional kernel parameters, and effectively capture local spectralspatial information in hyperspectral image (HSI). Second, a groupwise separable multihead self-attention (GSSA) module is employed to substitute the conventional multihead self-attention (MSA) in ViT, in which the groupwise self-attention (GSA) provides local spatial feature extraction, and the pointwise self-attention (PWSA) provides global spatial feature extraction. Third, a simple pointwise layer with enhanced skip connection mechanism is employed to substitute the multilayer perceptron (MLP) layer in all Transformer blocks of ViT, so as to eliminate unnecessary nonlinear transformations and facilitate the fusion of features derived from GSC and GSSA modules. Extensive experiments on four benchmark hyperspectral datasets reveal that our GSC-ViT can achieve surprising classification performance with relatively few training samples as compared with some existing HSIC approaches. The source code is available at https://github.com/flyzzie/TGRS-GSC-VIT.",
      "intriguing_abstract": "Recently, vision transformer (ViT)-based deep learning (DL) models have achieved remarkable performance gains in hyperspectral image classification (HSIC) due to their abilities to model long-range dependencies and extract global spatial features. However, ViT is built with a stack of Transformer blocks and faces the challenge of learning a large number of parameters when processing hyperspectral data. Besides, the inherent modeling of global correlation in Transformer ignores the effective representation of local spatial and spectral features. To address these issues, we propose a lightweight ViT network known as groupwise separable convolutional ViT (GSC-ViT). First, a groupwise separable convolution (GSC) module, which is a combination of grouped pointwise convolution (GPWC) and group convolution, is designed to significantly decrease the number of convolutional kernel parameters, and effectively capture local spectralspatial information in hyperspectral image (HSI). Second, a groupwise separable multihead self-attention (GSSA) module is employed to substitute the conventional multihead self-attention (MSA) in ViT, in which the groupwise self-attention (GSA) provides local spatial feature extraction, and the pointwise self-attention (PWSA) provides global spatial feature extraction. Third, a simple pointwise layer with enhanced skip connection mechanism is employed to substitute the multilayer perceptron (MLP) layer in all Transformer blocks of ViT, so as to eliminate unnecessary nonlinear transformations and facilitate the fusion of features derived from GSC and GSSA modules. Extensive experiments on four benchmark hyperspectral datasets reveal that our GSC-ViT can achieve surprising classification performance with relatively few training samples as compared with some existing HSIC approaches. The source code is available at https://github.com/flyzzie/TGRS-GSC-VIT.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f3d0278649454f80ba52c966a979499ee33e26c2.pdf",
      "citation_key": "zhao2024671",
      "metadata": {
        "title": "Hyperspectral Image Classification Using Groupwise Separable Convolutional Vision Transformer Network",
        "authors": [
          "Zhuoyi Zhao",
          "Xiang Xu",
          "Shutao Li",
          "A. Plaza"
        ],
        "published_date": "2024",
        "abstract": "Recently, vision transformer (ViT)-based deep learning (DL) models have achieved remarkable performance gains in hyperspectral image classification (HSIC) due to their abilities to model long-range dependencies and extract global spatial features. However, ViT is built with a stack of Transformer blocks and faces the challenge of learning a large number of parameters when processing hyperspectral data. Besides, the inherent modeling of global correlation in Transformer ignores the effective representation of local spatial and spectral features. To address these issues, we propose a lightweight ViT network known as groupwise separable convolutional ViT (GSC-ViT). First, a groupwise separable convolution (GSC) module, which is a combination of grouped pointwise convolution (GPWC) and group convolution, is designed to significantly decrease the number of convolutional kernel parameters, and effectively capture local spectralspatial information in hyperspectral image (HSI). Second, a groupwise separable multihead self-attention (GSSA) module is employed to substitute the conventional multihead self-attention (MSA) in ViT, in which the groupwise self-attention (GSA) provides local spatial feature extraction, and the pointwise self-attention (PWSA) provides global spatial feature extraction. Third, a simple pointwise layer with enhanced skip connection mechanism is employed to substitute the multilayer perceptron (MLP) layer in all Transformer blocks of ViT, so as to eliminate unnecessary nonlinear transformations and facilitate the fusion of features derived from GSC and GSSA modules. Extensive experiments on four benchmark hyperspectral datasets reveal that our GSC-ViT can achieve surprising classification performance with relatively few training samples as compared with some existing HSIC approaches. The source code is available at https://github.com/flyzzie/TGRS-GSC-VIT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f3d0278649454f80ba52c966a979499ee33e26c2.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 90,
        "score": 90.0,
        "summary": "Recently, vision transformer (ViT)-based deep learning (DL) models have achieved remarkable performance gains in hyperspectral image classification (HSIC) due to their abilities to model long-range dependencies and extract global spatial features. However, ViT is built with a stack of Transformer blocks and faces the challenge of learning a large number of parameters when processing hyperspectral data. Besides, the inherent modeling of global correlation in Transformer ignores the effective representation of local spatial and spectral features. To address these issues, we propose a lightweight ViT network known as groupwise separable convolutional ViT (GSC-ViT). First, a groupwise separable convolution (GSC) module, which is a combination of grouped pointwise convolution (GPWC) and group convolution, is designed to significantly decrease the number of convolutional kernel parameters, and effectively capture local spectralspatial information in hyperspectral image (HSI). Second, a groupwise separable multihead self-attention (GSSA) module is employed to substitute the conventional multihead self-attention (MSA) in ViT, in which the groupwise self-attention (GSA) provides local spatial feature extraction, and the pointwise self-attention (PWSA) provides global spatial feature extraction. Third, a simple pointwise layer with enhanced skip connection mechanism is employed to substitute the multilayer perceptron (MLP) layer in all Transformer blocks of ViT, so as to eliminate unnecessary nonlinear transformations and facilitate the fusion of features derived from GSC and GSSA modules. Extensive experiments on four benchmark hyperspectral datasets reveal that our GSC-ViT can achieve surprising classification performance with relatively few training samples as compared with some existing HSIC approaches. The source code is available at https://github.com/flyzzie/TGRS-GSC-VIT.",
        "keywords": []
      },
      "file_name": "f3d0278649454f80ba52c966a979499ee33e26c2.pdf"
    },
    {
      "success": true,
      "doc_id": "ea6fdc8b47eb22db6b109ca9eaf43860",
      "summary": "The ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before processing them with computer vision models has not yet been successfully challenged. However, models such as the Vision Transformer (ViT) offer flexible sequence-based modeling, and hence varying input sequence lengths. We take advantage of this with NaViT (Native Resolution ViT) which uses sequence packing during training to process inputs of arbitrary resolutions and aspect ratios. Alongside flexible model usage, we demonstrate improved training efficiency for large-scale supervised and contrastive image-text pretraining. NaViT can be efficiently transferred to standard tasks such as image and video classification, object detection, and semantic segmentation and leads to improved results on robustness and fairness benchmarks. At inference time, the input resolution flexibility can be used to smoothly navigate the test-time cost-performance trade-off. We believe that NaViT marks a departure from the standard, CNN-designed, input and modelling pipeline used by most computer vision models, and represents a promising direction for ViTs.",
      "intriguing_abstract": "The ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before processing them with computer vision models has not yet been successfully challenged. However, models such as the Vision Transformer (ViT) offer flexible sequence-based modeling, and hence varying input sequence lengths. We take advantage of this with NaViT (Native Resolution ViT) which uses sequence packing during training to process inputs of arbitrary resolutions and aspect ratios. Alongside flexible model usage, we demonstrate improved training efficiency for large-scale supervised and contrastive image-text pretraining. NaViT can be efficiently transferred to standard tasks such as image and video classification, object detection, and semantic segmentation and leads to improved results on robustness and fairness benchmarks. At inference time, the input resolution flexibility can be used to smoothly navigate the test-time cost-performance trade-off. We believe that NaViT marks a departure from the standard, CNN-designed, input and modelling pipeline used by most computer vision models, and represents a promising direction for ViTs.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/918617dbc02fa4df1999599bcf967acd2ea84d71.pdf",
      "citation_key": "dehghani2023u7e",
      "metadata": {
        "title": "Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution",
        "authors": [
          "Mostafa Dehghani",
          "Basil Mustafa",
          "J. Djolonga",
          "J. Heek",
          "Matthias Minderer",
          "Mathilde Caron",
          "A. Steiner",
          "J. Puigcerver",
          "Robert Geirhos",
          "Ibrahim M. Alabdulmohsin",
          "Avital Oliver",
          "Piotr Padlewski",
          "A. Gritsenko",
          "Mario Luvci'c",
          "N. Houlsby"
        ],
        "published_date": "2023",
        "abstract": "The ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before processing them with computer vision models has not yet been successfully challenged. However, models such as the Vision Transformer (ViT) offer flexible sequence-based modeling, and hence varying input sequence lengths. We take advantage of this with NaViT (Native Resolution ViT) which uses sequence packing during training to process inputs of arbitrary resolutions and aspect ratios. Alongside flexible model usage, we demonstrate improved training efficiency for large-scale supervised and contrastive image-text pretraining. NaViT can be efficiently transferred to standard tasks such as image and video classification, object detection, and semantic segmentation and leads to improved results on robustness and fairness benchmarks. At inference time, the input resolution flexibility can be used to smoothly navigate the test-time cost-performance trade-off. We believe that NaViT marks a departure from the standard, CNN-designed, input and modelling pipeline used by most computer vision models, and represents a promising direction for ViTs.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/918617dbc02fa4df1999599bcf967acd2ea84d71.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 147,
        "score": 73.5,
        "summary": "The ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before processing them with computer vision models has not yet been successfully challenged. However, models such as the Vision Transformer (ViT) offer flexible sequence-based modeling, and hence varying input sequence lengths. We take advantage of this with NaViT (Native Resolution ViT) which uses sequence packing during training to process inputs of arbitrary resolutions and aspect ratios. Alongside flexible model usage, we demonstrate improved training efficiency for large-scale supervised and contrastive image-text pretraining. NaViT can be efficiently transferred to standard tasks such as image and video classification, object detection, and semantic segmentation and leads to improved results on robustness and fairness benchmarks. At inference time, the input resolution flexibility can be used to smoothly navigate the test-time cost-performance trade-off. We believe that NaViT marks a departure from the standard, CNN-designed, input and modelling pipeline used by most computer vision models, and represents a promising direction for ViTs.",
        "keywords": []
      },
      "file_name": "918617dbc02fa4df1999599bcf967acd2ea84d71.pdf"
    },
    {
      "success": true,
      "doc_id": "95b3dbacd392ebfe5388440357725fa6",
      "summary": "Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis. This paper introduces Vision-RWKV (VRWKV), a model adapted from the RWKV model used in the NLP field with necessary modifications for vision tasks. Similar to the Vision Transformer (ViT), our model is designed to efficiently handle sparse inputs and demonstrate robust global processing capabilities, while also scaling up effectively, accommodating both large-scale parameters and extensive datasets. Its distinctive advantage lies in its reduced spatial aggregation complexity, which renders it exceptionally adept at processing high-resolution images seamlessly, eliminating the necessity for windowing operations. Our evaluations demonstrate that VRWKV surpasses ViT's performance in image classification and has significantly faster speeds and lower memory usage processing high-resolution inputs. In dense prediction tasks, it outperforms window-based models, maintaining comparable speeds. These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks. Code is released at https://github.com/OpenGVLab/Vision-RWKV.",
      "intriguing_abstract": "Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis. This paper introduces Vision-RWKV (VRWKV), a model adapted from the RWKV model used in the NLP field with necessary modifications for vision tasks. Similar to the Vision Transformer (ViT), our model is designed to efficiently handle sparse inputs and demonstrate robust global processing capabilities, while also scaling up effectively, accommodating both large-scale parameters and extensive datasets. Its distinctive advantage lies in its reduced spatial aggregation complexity, which renders it exceptionally adept at processing high-resolution images seamlessly, eliminating the necessity for windowing operations. Our evaluations demonstrate that VRWKV surpasses ViT's performance in image classification and has significantly faster speeds and lower memory usage processing high-resolution inputs. In dense prediction tasks, it outperforms window-based models, maintaining comparable speeds. These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks. Code is released at https://github.com/OpenGVLab/Vision-RWKV.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/51f38bd957fa863022feb5878fa1ba3bea6657cf.pdf",
      "citation_key": "duan2024q7h",
      "metadata": {
        "title": "Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures",
        "authors": [
          "Yuchen Duan",
          "Weiyun Wang",
          "Zhe Chen",
          "Xizhou Zhu",
          "Lewei Lu",
          "Tong Lu",
          "Yu Qiao",
          "Hongsheng Li",
          "Jifeng Dai",
          "Wenhai Wang"
        ],
        "published_date": "2024",
        "abstract": "Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis. This paper introduces Vision-RWKV (VRWKV), a model adapted from the RWKV model used in the NLP field with necessary modifications for vision tasks. Similar to the Vision Transformer (ViT), our model is designed to efficiently handle sparse inputs and demonstrate robust global processing capabilities, while also scaling up effectively, accommodating both large-scale parameters and extensive datasets. Its distinctive advantage lies in its reduced spatial aggregation complexity, which renders it exceptionally adept at processing high-resolution images seamlessly, eliminating the necessity for windowing operations. Our evaluations demonstrate that VRWKV surpasses ViT's performance in image classification and has significantly faster speeds and lower memory usage processing high-resolution inputs. In dense prediction tasks, it outperforms window-based models, maintaining comparable speeds. These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks. Code is released at https://github.com/OpenGVLab/Vision-RWKV.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/51f38bd957fa863022feb5878fa1ba3bea6657cf.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 66,
        "score": 66.0,
        "summary": "Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis. This paper introduces Vision-RWKV (VRWKV), a model adapted from the RWKV model used in the NLP field with necessary modifications for vision tasks. Similar to the Vision Transformer (ViT), our model is designed to efficiently handle sparse inputs and demonstrate robust global processing capabilities, while also scaling up effectively, accommodating both large-scale parameters and extensive datasets. Its distinctive advantage lies in its reduced spatial aggregation complexity, which renders it exceptionally adept at processing high-resolution images seamlessly, eliminating the necessity for windowing operations. Our evaluations demonstrate that VRWKV surpasses ViT's performance in image classification and has significantly faster speeds and lower memory usage processing high-resolution inputs. In dense prediction tasks, it outperforms window-based models, maintaining comparable speeds. These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks. Code is released at https://github.com/OpenGVLab/Vision-RWKV.",
        "keywords": []
      },
      "file_name": "51f38bd957fa863022feb5878fa1ba3bea6657cf.pdf"
    },
    {
      "success": true,
      "doc_id": "56778d590deeac9478354a1ec0b793b2",
      "summary": "Invading pests and diseases always degrade the quality and quantity of plants. Early and accurate identification of plant diseases is critical for plant health and growth. This work proposes a smartphone-based solution using a Vision Transformer (ViT) model for identifying healthy plants and unhealthy plants with diseases. The collected dataset of tomato leaves was used to collectively train Vision Transformer and Inception V3-based deep learning (DL) models to differentiate healthy and diseased plants. These models detected 10 different tomato disease classes from the dataset containing 10,010 images. The performance of the two DL models was compared. This work also presents a smartphone-based application (Android App) using a ViT-based model, which works on the basis of the self-attention mechanism and yielded a better performance (90.99% testing) than Inception V3 in our experimentation. The proposed ViT-SmartAgri is promising and can be implemented on a colossal scale for smart agriculture, thus inspiring future work in this area.",
      "intriguing_abstract": "Invading pests and diseases always degrade the quality and quantity of plants. Early and accurate identification of plant diseases is critical for plant health and growth. This work proposes a smartphone-based solution using a Vision Transformer (ViT) model for identifying healthy plants and unhealthy plants with diseases. The collected dataset of tomato leaves was used to collectively train Vision Transformer and Inception V3-based deep learning (DL) models to differentiate healthy and diseased plants. These models detected 10 different tomato disease classes from the dataset containing 10,010 images. The performance of the two DL models was compared. This work also presents a smartphone-based application (Android App) using a ViT-based model, which works on the basis of the self-attention mechanism and yielded a better performance (90.99% testing) than Inception V3 in our experimentation. The proposed ViT-SmartAgri is promising and can be implemented on a colossal scale for smart agriculture, thus inspiring future work in this area.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/1f389f54324790bfad6fc40ac4e56428757ea92b.pdf",
      "citation_key": "barman2024q21",
      "metadata": {
        "title": "ViT-SmartAgri: Vision Transformer and Smartphone-Based Plant Disease Detection for Smart Agriculture",
        "authors": [
          "Utpal Barman",
          "Parismita Sarma",
          "Mirzanur Rahman",
          "Vaskar Deka",
          "Swati Lahkar",
          "Vaishali Sharma",
          "Manob Saikia"
        ],
        "published_date": "2024",
        "abstract": "Invading pests and diseases always degrade the quality and quantity of plants. Early and accurate identification of plant diseases is critical for plant health and growth. This work proposes a smartphone-based solution using a Vision Transformer (ViT) model for identifying healthy plants and unhealthy plants with diseases. The collected dataset of tomato leaves was used to collectively train Vision Transformer and Inception V3-based deep learning (DL) models to differentiate healthy and diseased plants. These models detected 10 different tomato disease classes from the dataset containing 10,010 images. The performance of the two DL models was compared. This work also presents a smartphone-based application (Android App) using a ViT-based model, which works on the basis of the self-attention mechanism and yielded a better performance (90.99% testing) than Inception V3 in our experimentation. The proposed ViT-SmartAgri is promising and can be implemented on a colossal scale for smart agriculture, thus inspiring future work in this area.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1f389f54324790bfad6fc40ac4e56428757ea92b.pdf",
        "venue": "Agronomy",
        "citationCount": 60,
        "score": 60.0,
        "summary": "Invading pests and diseases always degrade the quality and quantity of plants. Early and accurate identification of plant diseases is critical for plant health and growth. This work proposes a smartphone-based solution using a Vision Transformer (ViT) model for identifying healthy plants and unhealthy plants with diseases. The collected dataset of tomato leaves was used to collectively train Vision Transformer and Inception V3-based deep learning (DL) models to differentiate healthy and diseased plants. These models detected 10 different tomato disease classes from the dataset containing 10,010 images. The performance of the two DL models was compared. This work also presents a smartphone-based application (Android App) using a ViT-based model, which works on the basis of the self-attention mechanism and yielded a better performance (90.99% testing) than Inception V3 in our experimentation. The proposed ViT-SmartAgri is promising and can be implemented on a colossal scale for smart agriculture, thus inspiring future work in this area.",
        "keywords": []
      },
      "file_name": "1f389f54324790bfad6fc40ac4e56428757ea92b.pdf"
    },
    {
      "success": true,
      "doc_id": "18498fd31f0a9e8eb84115f6f2c9fc47",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/05236fa766fc1a38a9eb895e77075fb65be8c258.pdf",
      "citation_key": "jamil20230ll",
      "metadata": {
        "title": "An efficient and robust Phonocardiography (PCG)-based Valvular Heart Diseases (VHD) detection framework using Vision Transformer (ViT)",
        "authors": [
          "Sonain Jamil",
          "Anisha Roy"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/05236fa766fc1a38a9eb895e77075fb65be8c258.pdf",
        "venue": "Comput. Biol. Medicine",
        "citationCount": 84,
        "score": 42.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "05236fa766fc1a38a9eb895e77075fb65be8c258.pdf"
    },
    {
      "success": true,
      "doc_id": "2a92ff05fe980b572ca359960e8cb58c",
      "summary": "Skin cancer is one of the most frequently occurring cancers worldwide, and early detection is crucial for effective treatment. Dermatologists often face challenges such as heavy data demands, potential human errors, and strict time limits, which can negatively affect diagnostic outcomes. Deep learningbased diagnostic systems offer quick, accurate testing and enhanced research capabilities, providing significant support to dermatologists. In this study, we enhanced the Swin Transformer architecture by implementing the hybrid shifted window-based multi-head self-attention (HSW-MSA) in place of the conventional shifted window-based multi-head self-attention (SW-MSA). This adjustment enables the model to more efficiently process areas of skin cancer overlap, capture finer details, and manage long-range dependencies, while maintaining memory usage and computational efficiency during training. Additionally, the study replaces the standard multi-layer perceptron (MLP) in the Swin Transformer with a SwiGLU-based MLP, an upgraded version of the gated linear unit (GLU) module, to achieve higher accuracy, faster training speeds, and better parameter efficiency. The modified Swin model-base was evaluated using the publicly accessible ISIC 2019 skin dataset with eight classes and was compared against popular convolutional neural networks (CNNs) and cutting-edge vision transformer (ViT) models. In an exhaustive assessment on the unseen test dataset, the proposed Swin-Base model demonstrated exceptional performance, achieving an accuracy of 89.36%, a recall of 85.13%, a precision of 88.22%, and an F1-score of 86.65%, surpassing all previously reported research and deep learning models documented in the literature.",
      "intriguing_abstract": "Skin cancer is one of the most frequently occurring cancers worldwide, and early detection is crucial for effective treatment. Dermatologists often face challenges such as heavy data demands, potential human errors, and strict time limits, which can negatively affect diagnostic outcomes. Deep learningbased diagnostic systems offer quick, accurate testing and enhanced research capabilities, providing significant support to dermatologists. In this study, we enhanced the Swin Transformer architecture by implementing the hybrid shifted window-based multi-head self-attention (HSW-MSA) in place of the conventional shifted window-based multi-head self-attention (SW-MSA). This adjustment enables the model to more efficiently process areas of skin cancer overlap, capture finer details, and manage long-range dependencies, while maintaining memory usage and computational efficiency during training. Additionally, the study replaces the standard multi-layer perceptron (MLP) in the Swin Transformer with a SwiGLU-based MLP, an upgraded version of the gated linear unit (GLU) module, to achieve higher accuracy, faster training speeds, and better parameter efficiency. The modified Swin model-base was evaluated using the publicly accessible ISIC 2019 skin dataset with eight classes and was compared against popular convolutional neural networks (CNNs) and cutting-edge vision transformer (ViT) models. In an exhaustive assessment on the unseen test dataset, the proposed Swin-Base model demonstrated exceptional performance, achieving an accuracy of 89.36%, a recall of 85.13%, a precision of 88.22%, and an F1-score of 86.65%, surpassing all previously reported research and deep learning models documented in the literature.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/0eec6c36da426f78b7091ba7ae8602e129742d30.pdf",
      "citation_key": "paal2024eg1",
      "metadata": {
        "title": "Enhancing Skin Cancer Diagnosis Using Swin Transformer with Hybrid Shifted Window-Based Multi-head Self-attention and SwiGLU-Based MLP",
        "authors": [
          "Ishak Paal",
          "Melek Alaftekin",
          "F. Zengul"
        ],
        "published_date": "2024",
        "abstract": "Skin cancer is one of the most frequently occurring cancers worldwide, and early detection is crucial for effective treatment. Dermatologists often face challenges such as heavy data demands, potential human errors, and strict time limits, which can negatively affect diagnostic outcomes. Deep learningbased diagnostic systems offer quick, accurate testing and enhanced research capabilities, providing significant support to dermatologists. In this study, we enhanced the Swin Transformer architecture by implementing the hybrid shifted window-based multi-head self-attention (HSW-MSA) in place of the conventional shifted window-based multi-head self-attention (SW-MSA). This adjustment enables the model to more efficiently process areas of skin cancer overlap, capture finer details, and manage long-range dependencies, while maintaining memory usage and computational efficiency during training. Additionally, the study replaces the standard multi-layer perceptron (MLP) in the Swin Transformer with a SwiGLU-based MLP, an upgraded version of the gated linear unit (GLU) module, to achieve higher accuracy, faster training speeds, and better parameter efficiency. The modified Swin model-base was evaluated using the publicly accessible ISIC 2019 skin dataset with eight classes and was compared against popular convolutional neural networks (CNNs) and cutting-edge vision transformer (ViT) models. In an exhaustive assessment on the unseen test dataset, the proposed Swin-Base model demonstrated exceptional performance, achieving an accuracy of 89.36%, a recall of 85.13%, a precision of 88.22%, and an F1-score of 86.65%, surpassing all previously reported research and deep learning models documented in the literature.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0eec6c36da426f78b7091ba7ae8602e129742d30.pdf",
        "venue": "Journal of imaging informatics in medicine",
        "citationCount": 42,
        "score": 42.0,
        "summary": "Skin cancer is one of the most frequently occurring cancers worldwide, and early detection is crucial for effective treatment. Dermatologists often face challenges such as heavy data demands, potential human errors, and strict time limits, which can negatively affect diagnostic outcomes. Deep learningbased diagnostic systems offer quick, accurate testing and enhanced research capabilities, providing significant support to dermatologists. In this study, we enhanced the Swin Transformer architecture by implementing the hybrid shifted window-based multi-head self-attention (HSW-MSA) in place of the conventional shifted window-based multi-head self-attention (SW-MSA). This adjustment enables the model to more efficiently process areas of skin cancer overlap, capture finer details, and manage long-range dependencies, while maintaining memory usage and computational efficiency during training. Additionally, the study replaces the standard multi-layer perceptron (MLP) in the Swin Transformer with a SwiGLU-based MLP, an upgraded version of the gated linear unit (GLU) module, to achieve higher accuracy, faster training speeds, and better parameter efficiency. The modified Swin model-base was evaluated using the publicly accessible ISIC 2019 skin dataset with eight classes and was compared against popular convolutional neural networks (CNNs) and cutting-edge vision transformer (ViT) models. In an exhaustive assessment on the unseen test dataset, the proposed Swin-Base model demonstrated exceptional performance, achieving an accuracy of 89.36%, a recall of 85.13%, a precision of 88.22%, and an F1-score of 86.65%, surpassing all previously reported research and deep learning models documented in the literature.",
        "keywords": []
      },
      "file_name": "0eec6c36da426f78b7091ba7ae8602e129742d30.pdf"
    },
    {
      "success": true,
      "doc_id": "7bd5b9bc9f7652096b56a0eb0adfac8e",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/689bc24f71f8f22784534c764d59baa93a62c2e0.pdf",
      "citation_key": "zhang2023k43",
      "metadata": {
        "title": "HiViT: A Simpler and More Efficient Design of Hierarchical Vision Transformer",
        "authors": [
          "Xiaosong Zhang",
          "Yunjie Tian",
          "Lingxi Xie",
          "Wei Huang",
          "Qi Dai",
          "Qixiang Ye",
          "Qi Tian"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/689bc24f71f8f22784534c764d59baa93a62c2e0.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 80,
        "score": 40.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "689bc24f71f8f22784534c764d59baa93a62c2e0.pdf"
    },
    {
      "success": true,
      "doc_id": "1f9fd30a5ef8ba02b2328da5cac85b92",
      "summary": "Skin cancer is a significant health concern worldwide, and early and accurate diagnosis plays a crucial role in improving patient outcomes. In recent years, deep learning models have shown remarkable success in various computer vision tasks, including image classification. In this research study, we introduce an approach for skin cancer classification using vision transformer, a state-of-the-art deep learning architecture that has demonstrated exceptional performance in diverse image analysis tasks. The study utilizes the HAM10000 dataset; a publicly available dataset comprising 10,015 skin lesion images classified into two categories: benign (6705 images) and malignant (3310 images). This dataset consists of high-resolution images captured using dermatoscopes and carefully annotated by expert dermatologists. Preprocessing techniques, such as normalization and augmentation, are applied to enhance the robustness and generalization of the model. The vision transformer architecture is adapted to the skin cancer classification task. The model leverages the self-attention mechanism to capture intricate spatial dependencies and long-range dependencies within the images, enabling it to effectively learn relevant features for accurate classification. Segment Anything Model (SAM) is employed to segment the cancerous areas from the images; achieving an IOU of 96.01% and Dice coefficient of 98.14% and then various pretrained models are used for classification using vision transformer architecture. Extensive experiments and evaluations are conducted to assess the performance of our approach. The results demonstrate the superiority of the vision transformer model over traditional deep learning architectures in skin cancer classification in general with some exceptions. Upon experimenting on six different models, ViT-Google, ViT-MAE, ViT-ResNet50, ViT-VAN, ViT-BEiT, and ViT-DiT, we found out that the ML approach achieves 96.15% accuracy using Google's ViT patch-32 model with a low false negative ratio on the test dataset, showcasing its potential as an effective tool for aiding dermatologists in the diagnosis of skin cancer.",
      "intriguing_abstract": "Skin cancer is a significant health concern worldwide, and early and accurate diagnosis plays a crucial role in improving patient outcomes. In recent years, deep learning models have shown remarkable success in various computer vision tasks, including image classification. In this research study, we introduce an approach for skin cancer classification using vision transformer, a state-of-the-art deep learning architecture that has demonstrated exceptional performance in diverse image analysis tasks. The study utilizes the HAM10000 dataset; a publicly available dataset comprising 10,015 skin lesion images classified into two categories: benign (6705 images) and malignant (3310 images). This dataset consists of high-resolution images captured using dermatoscopes and carefully annotated by expert dermatologists. Preprocessing techniques, such as normalization and augmentation, are applied to enhance the robustness and generalization of the model. The vision transformer architecture is adapted to the skin cancer classification task. The model leverages the self-attention mechanism to capture intricate spatial dependencies and long-range dependencies within the images, enabling it to effectively learn relevant features for accurate classification. Segment Anything Model (SAM) is employed to segment the cancerous areas from the images; achieving an IOU of 96.01% and Dice coefficient of 98.14% and then various pretrained models are used for classification using vision transformer architecture. Extensive experiments and evaluations are conducted to assess the performance of our approach. The results demonstrate the superiority of the vision transformer model over traditional deep learning architectures in skin cancer classification in general with some exceptions. Upon experimenting on six different models, ViT-Google, ViT-MAE, ViT-ResNet50, ViT-VAN, ViT-BEiT, and ViT-DiT, we found out that the ML approach achieves 96.15% accuracy using Google's ViT patch-32 model with a low false negative ratio on the test dataset, showcasing its potential as an effective tool for aiding dermatologists in the diagnosis of skin cancer.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/afb7ff7096cd45de02de96b5937c3416ebca0cb0.pdf",
      "citation_key": "himel2024u0i",
      "metadata": {
        "title": "Skin Cancer Segmentation and Classification Using Vision Transformer for Automatic Analysis in Dermatoscopy-Based Noninvasive Digital System",
        "authors": [
          "Galib Muhammad Shahriar Himel",
          "Md. Masudul Islam",
          "Kh Abdullah Al-Aff",
          "Shams Ibne Karim",
          "Md. Kabir Uddin Sikder"
        ],
        "published_date": "2024",
        "abstract": "Skin cancer is a significant health concern worldwide, and early and accurate diagnosis plays a crucial role in improving patient outcomes. In recent years, deep learning models have shown remarkable success in various computer vision tasks, including image classification. In this research study, we introduce an approach for skin cancer classification using vision transformer, a state-of-the-art deep learning architecture that has demonstrated exceptional performance in diverse image analysis tasks. The study utilizes the HAM10000 dataset; a publicly available dataset comprising 10,015 skin lesion images classified into two categories: benign (6705 images) and malignant (3310 images). This dataset consists of high-resolution images captured using dermatoscopes and carefully annotated by expert dermatologists. Preprocessing techniques, such as normalization and augmentation, are applied to enhance the robustness and generalization of the model. The vision transformer architecture is adapted to the skin cancer classification task. The model leverages the self-attention mechanism to capture intricate spatial dependencies and long-range dependencies within the images, enabling it to effectively learn relevant features for accurate classification. Segment Anything Model (SAM) is employed to segment the cancerous areas from the images; achieving an IOU of 96.01% and Dice coefficient of 98.14% and then various pretrained models are used for classification using vision transformer architecture. Extensive experiments and evaluations are conducted to assess the performance of our approach. The results demonstrate the superiority of the vision transformer model over traditional deep learning architectures in skin cancer classification in general with some exceptions. Upon experimenting on six different models, ViT-Google, ViT-MAE, ViT-ResNet50, ViT-VAN, ViT-BEiT, and ViT-DiT, we found out that the ML approach achieves 96.15% accuracy using Google's ViT patch-32 model with a low false negative ratio on the test dataset, showcasing its potential as an effective tool for aiding dermatologists in the diagnosis of skin cancer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/afb7ff7096cd45de02de96b5937c3416ebca0cb0.pdf",
        "venue": "International Journal of Biomedical Imaging",
        "citationCount": 39,
        "score": 39.0,
        "summary": "Skin cancer is a significant health concern worldwide, and early and accurate diagnosis plays a crucial role in improving patient outcomes. In recent years, deep learning models have shown remarkable success in various computer vision tasks, including image classification. In this research study, we introduce an approach for skin cancer classification using vision transformer, a state-of-the-art deep learning architecture that has demonstrated exceptional performance in diverse image analysis tasks. The study utilizes the HAM10000 dataset; a publicly available dataset comprising 10,015 skin lesion images classified into two categories: benign (6705 images) and malignant (3310 images). This dataset consists of high-resolution images captured using dermatoscopes and carefully annotated by expert dermatologists. Preprocessing techniques, such as normalization and augmentation, are applied to enhance the robustness and generalization of the model. The vision transformer architecture is adapted to the skin cancer classification task. The model leverages the self-attention mechanism to capture intricate spatial dependencies and long-range dependencies within the images, enabling it to effectively learn relevant features for accurate classification. Segment Anything Model (SAM) is employed to segment the cancerous areas from the images; achieving an IOU of 96.01% and Dice coefficient of 98.14% and then various pretrained models are used for classification using vision transformer architecture. Extensive experiments and evaluations are conducted to assess the performance of our approach. The results demonstrate the superiority of the vision transformer model over traditional deep learning architectures in skin cancer classification in general with some exceptions. Upon experimenting on six different models, ViT-Google, ViT-MAE, ViT-ResNet50, ViT-VAN, ViT-BEiT, and ViT-DiT, we found out that the ML approach achieves 96.15% accuracy using Google's ViT patch-32 model with a low false negative ratio on the test dataset, showcasing its potential as an effective tool for aiding dermatologists in the diagnosis of skin cancer.",
        "keywords": []
      },
      "file_name": "afb7ff7096cd45de02de96b5937c3416ebca0cb0.pdf"
    },
    {
      "success": true,
      "doc_id": "331a13801a4ee73ed1d34f5c534db924",
      "summary": "Recently, vision transformers (ViTs) have been investigated in fine-grained visual recognition (FGVC) and are now considered state of the art. However, most ViT-based works ignore the different learning performances of the heads in the multi-head self-attention (MHSA) mechanism and its layers. To address these issues, in this paper, we propose a novel internal ensemble learning transformer (IELT) for FGVC. The proposed IELT involves three main modules: multi-head voting (MHV) module, cross-layer refinement (CLR) module, and dynamic selection (DS) module. To solve the problem of the inconsistent performances of multiple heads, we propose the MHV module, which considers all of the heads in each layer as weak learners and votes for tokens of discriminative regions as cross-layer feature based on the attention maps and spatial relationships. To effectively mine the cross-layer feature and suppress the noise, the CLR module is proposed, where the refined feature is extracted and the assist logits operation is developed for the final prediction. In addition, a newly designed DS module adjusts the token selection number at each layer by weighting their contributions of the refined feature. In this way, the idea of ensemble learning is combined with the ViT to improve fine-grained feature representation. The experiments demonstrate that our method achieves competitive results compared with the state of the art on five popular FGVC datasets.",
      "intriguing_abstract": "Recently, vision transformers (ViTs) have been investigated in fine-grained visual recognition (FGVC) and are now considered state of the art. However, most ViT-based works ignore the different learning performances of the heads in the multi-head self-attention (MHSA) mechanism and its layers. To address these issues, in this paper, we propose a novel internal ensemble learning transformer (IELT) for FGVC. The proposed IELT involves three main modules: multi-head voting (MHV) module, cross-layer refinement (CLR) module, and dynamic selection (DS) module. To solve the problem of the inconsistent performances of multiple heads, we propose the MHV module, which considers all of the heads in each layer as weak learners and votes for tokens of discriminative regions as cross-layer feature based on the attention maps and spatial relationships. To effectively mine the cross-layer feature and suppress the noise, the CLR module is proposed, where the refined feature is extracted and the assist logits operation is developed for the final prediction. In addition, a newly designed DS module adjusts the token selection number at each layer by weighting their contributions of the refined feature. In this way, the idea of ensemble learning is combined with the ViT to improve fine-grained feature representation. The experiments demonstrate that our method achieves competitive results compared with the state of the art on five popular FGVC datasets.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/595adb75ddeb90760c79e89b76d99e55079e0708.pdf",
      "citation_key": "xu20235cu",
      "metadata": {
        "title": "Fine-Grained Visual Classification via Internal Ensemble Learning Transformer",
        "authors": [
          "Qin Xu",
          "Jiahui Wang",
          "Bo Jiang",
          "Bin Luo"
        ],
        "published_date": "2023",
        "abstract": "Recently, vision transformers (ViTs) have been investigated in fine-grained visual recognition (FGVC) and are now considered state of the art. However, most ViT-based works ignore the different learning performances of the heads in the multi-head self-attention (MHSA) mechanism and its layers. To address these issues, in this paper, we propose a novel internal ensemble learning transformer (IELT) for FGVC. The proposed IELT involves three main modules: multi-head voting (MHV) module, cross-layer refinement (CLR) module, and dynamic selection (DS) module. To solve the problem of the inconsistent performances of multiple heads, we propose the MHV module, which considers all of the heads in each layer as weak learners and votes for tokens of discriminative regions as cross-layer feature based on the attention maps and spatial relationships. To effectively mine the cross-layer feature and suppress the noise, the CLR module is proposed, where the refined feature is extracted and the assist logits operation is developed for the final prediction. In addition, a newly designed DS module adjusts the token selection number at each layer by weighting their contributions of the refined feature. In this way, the idea of ensemble learning is combined with the ViT to improve fine-grained feature representation. The experiments demonstrate that our method achieves competitive results compared with the state of the art on five popular FGVC datasets.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/595adb75ddeb90760c79e89b76d99e55079e0708.pdf",
        "venue": "IEEE transactions on multimedia",
        "citationCount": 76,
        "score": 38.0,
        "summary": "Recently, vision transformers (ViTs) have been investigated in fine-grained visual recognition (FGVC) and are now considered state of the art. However, most ViT-based works ignore the different learning performances of the heads in the multi-head self-attention (MHSA) mechanism and its layers. To address these issues, in this paper, we propose a novel internal ensemble learning transformer (IELT) for FGVC. The proposed IELT involves three main modules: multi-head voting (MHV) module, cross-layer refinement (CLR) module, and dynamic selection (DS) module. To solve the problem of the inconsistent performances of multiple heads, we propose the MHV module, which considers all of the heads in each layer as weak learners and votes for tokens of discriminative regions as cross-layer feature based on the attention maps and spatial relationships. To effectively mine the cross-layer feature and suppress the noise, the CLR module is proposed, where the refined feature is extracted and the assist logits operation is developed for the final prediction. In addition, a newly designed DS module adjusts the token selection number at each layer by weighting their contributions of the refined feature. In this way, the idea of ensemble learning is combined with the ViT to improve fine-grained feature representation. The experiments demonstrate that our method achieves competitive results compared with the state of the art on five popular FGVC datasets.",
        "keywords": []
      },
      "file_name": "595adb75ddeb90760c79e89b76d99e55079e0708.pdf"
    },
    {
      "success": true,
      "doc_id": "31abc0e540ca74326f8ff4425f459437",
      "summary": "Haze superimposes a veil over remote sensing images, which severely limits the extraction of valuable military information. To this end, we present a novel trinity model to restore realistic surface information by integrating the merits of both prior- and deep learning-based strategies. Concretely, the critical insight of our Trinity-Net is to investigate how to incorporate prior information into convolutional neural networks (CNNs) and Swin Transformer for reasonable estimation of haze parameters. Then, haze-free images are obtained by reconstructing the remote sensing image formation model. Although Swin Transformer has shown tremendous potential in the dehazing task, which typically results in ambiguous details, we devise a gradient guidance module that naturally inherits structure priors of gradient maps, guiding the deep model to generate visually pleasing details. In light of the generality of image formation parameters, we successfully promote Trinity-Net to natural image dehazing and underwater image enhancement tasks. Notably, the acquisition of large-scale remote sensing hazy images and natural hazy images in military scenes is not feasible in practice. To bridge this gap, we construct a remote sensing image dehazing benchmark (RSID) and a natural image dehazing benchmark (NID), including 1000 real-world hazy images with corresponding ground-truth images. To our knowledge, this is the first exploration to develop dehazing benchmarks in the military field, alleviating the dilemma of data scarcity. Extensive experiments on three vision tasks illustrate the superiority of our Trinity-Net against multiple state-of-the-art methods. The datasets and code are available at https://github.com/chi-kaichen/Trinity-Net.",
      "intriguing_abstract": "Haze superimposes a veil over remote sensing images, which severely limits the extraction of valuable military information. To this end, we present a novel trinity model to restore realistic surface information by integrating the merits of both prior- and deep learning-based strategies. Concretely, the critical insight of our Trinity-Net is to investigate how to incorporate prior information into convolutional neural networks (CNNs) and Swin Transformer for reasonable estimation of haze parameters. Then, haze-free images are obtained by reconstructing the remote sensing image formation model. Although Swin Transformer has shown tremendous potential in the dehazing task, which typically results in ambiguous details, we devise a gradient guidance module that naturally inherits structure priors of gradient maps, guiding the deep model to generate visually pleasing details. In light of the generality of image formation parameters, we successfully promote Trinity-Net to natural image dehazing and underwater image enhancement tasks. Notably, the acquisition of large-scale remote sensing hazy images and natural hazy images in military scenes is not feasible in practice. To bridge this gap, we construct a remote sensing image dehazing benchmark (RSID) and a natural image dehazing benchmark (NID), including 1000 real-world hazy images with corresponding ground-truth images. To our knowledge, this is the first exploration to develop dehazing benchmarks in the military field, alleviating the dilemma of data scarcity. Extensive experiments on three vision tasks illustrate the superiority of our Trinity-Net against multiple state-of-the-art methods. The datasets and code are available at https://github.com/chi-kaichen/Trinity-Net.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/de20c6805b83a2f83ed75784920e91b913d888bb.pdf",
      "citation_key": "chi202331y",
      "metadata": {
        "title": "Trinity-Net: Gradient-Guided Swin Transformer-Based Remote Sensing Image Dehazing and Beyond",
        "authors": [
          "Kaichen Chi",
          "Yuan Yuan",
          "Qi Wang"
        ],
        "published_date": "2023",
        "abstract": "Haze superimposes a veil over remote sensing images, which severely limits the extraction of valuable military information. To this end, we present a novel trinity model to restore realistic surface information by integrating the merits of both prior- and deep learning-based strategies. Concretely, the critical insight of our Trinity-Net is to investigate how to incorporate prior information into convolutional neural networks (CNNs) and Swin Transformer for reasonable estimation of haze parameters. Then, haze-free images are obtained by reconstructing the remote sensing image formation model. Although Swin Transformer has shown tremendous potential in the dehazing task, which typically results in ambiguous details, we devise a gradient guidance module that naturally inherits structure priors of gradient maps, guiding the deep model to generate visually pleasing details. In light of the generality of image formation parameters, we successfully promote Trinity-Net to natural image dehazing and underwater image enhancement tasks. Notably, the acquisition of large-scale remote sensing hazy images and natural hazy images in military scenes is not feasible in practice. To bridge this gap, we construct a remote sensing image dehazing benchmark (RSID) and a natural image dehazing benchmark (NID), including 1000 real-world hazy images with corresponding ground-truth images. To our knowledge, this is the first exploration to develop dehazing benchmarks in the military field, alleviating the dilemma of data scarcity. Extensive experiments on three vision tasks illustrate the superiority of our Trinity-Net against multiple state-of-the-art methods. The datasets and code are available at https://github.com/chi-kaichen/Trinity-Net.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/de20c6805b83a2f83ed75784920e91b913d888bb.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 75,
        "score": 37.5,
        "summary": "Haze superimposes a veil over remote sensing images, which severely limits the extraction of valuable military information. To this end, we present a novel trinity model to restore realistic surface information by integrating the merits of both prior- and deep learning-based strategies. Concretely, the critical insight of our Trinity-Net is to investigate how to incorporate prior information into convolutional neural networks (CNNs) and Swin Transformer for reasonable estimation of haze parameters. Then, haze-free images are obtained by reconstructing the remote sensing image formation model. Although Swin Transformer has shown tremendous potential in the dehazing task, which typically results in ambiguous details, we devise a gradient guidance module that naturally inherits structure priors of gradient maps, guiding the deep model to generate visually pleasing details. In light of the generality of image formation parameters, we successfully promote Trinity-Net to natural image dehazing and underwater image enhancement tasks. Notably, the acquisition of large-scale remote sensing hazy images and natural hazy images in military scenes is not feasible in practice. To bridge this gap, we construct a remote sensing image dehazing benchmark (RSID) and a natural image dehazing benchmark (NID), including 1000 real-world hazy images with corresponding ground-truth images. To our knowledge, this is the first exploration to develop dehazing benchmarks in the military field, alleviating the dilemma of data scarcity. Extensive experiments on three vision tasks illustrate the superiority of our Trinity-Net against multiple state-of-the-art methods. The datasets and code are available at https://github.com/chi-kaichen/Trinity-Net.",
        "keywords": []
      },
      "file_name": "de20c6805b83a2f83ed75784920e91b913d888bb.pdf"
    },
    {
      "success": true,
      "doc_id": "a997bae6a91953874bfaa3e8075b93ac",
      "summary": "Vision transformers have been applied successfully for image recognition tasks. There have been either multi-headed self-attention based (ViT [12], DelT [54]) simi-lar to the original work in textual models or more re-cently based on spectral layers (Fnet [29], GFNet [46], AFNO [15]). We hypothesize that spectral layers cap-ture high-frequency information such as lines and edges, while attention layers capture token interactions. We inves-tigate this hypothesis through this work and observe that indeed mixing spectral and multi-headed attention layers provides a better transformer architecture. We thus pro-pose the novel Spectformer architecture for vision trans-formers that has initial spectral and deeper multi-headed attention layers. We believe that the resulting representation allows the transformer to capture the feature repre-sentation appropriately and it yields improved performance over other transformer representations. For instance, it im-proves the top-1 accuracy by 2% on ImageNet compared to both GFNet-H and LiT. SpectFormer-H-S reaches 84.25% top-1 accuracy on ImageNet-1 K (state of the art for small version). Further, Spectformer-H-L achieves 85.7% which is the state of the art for the comparable base version of the transformers. We further validated the SpectFormer per-formance in other scenarios such as transfer learning on standard datasets such as ClFAR-10, ClFAR-100, Oxford-lIlT-flower, and Standford Car datasets. We then investigate its use in downstream tasks such as object detection and instance segmentation on the MS-COCO dataset and ob-serve that Spectformer shows consistent performance that is comparable to the best backbones and can be further optimized and improved. The source code is available on this website https://github.com/badripatro/SpectFormers.",
      "intriguing_abstract": "Vision transformers have been applied successfully for image recognition tasks. There have been either multi-headed self-attention based (ViT [12], DelT [54]) simi-lar to the original work in textual models or more re-cently based on spectral layers (Fnet [29], GFNet [46], AFNO [15]). We hypothesize that spectral layers cap-ture high-frequency information such as lines and edges, while attention layers capture token interactions. We inves-tigate this hypothesis through this work and observe that indeed mixing spectral and multi-headed attention layers provides a better transformer architecture. We thus pro-pose the novel Spectformer architecture for vision trans-formers that has initial spectral and deeper multi-headed attention layers. We believe that the resulting representation allows the transformer to capture the feature repre-sentation appropriately and it yields improved performance over other transformer representations. For instance, it im-proves the top-1 accuracy by 2% on ImageNet compared to both GFNet-H and LiT. SpectFormer-H-S reaches 84.25% top-1 accuracy on ImageNet-1 K (state of the art for small version). Further, Spectformer-H-L achieves 85.7% which is the state of the art for the comparable base version of the transformers. We further validated the SpectFormer per-formance in other scenarios such as transfer learning on standard datasets such as ClFAR-10, ClFAR-100, Oxford-lIlT-flower, and Standford Car datasets. We then investigate its use in downstream tasks such as object detection and instance segmentation on the MS-COCO dataset and ob-serve that Spectformer shows consistent performance that is comparable to the best backbones and can be further optimized and improved. The source code is available on this website https://github.com/badripatro/SpectFormers.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/c57467e652f3f9131b3e7e40c23059abe395f01d.pdf",
      "citation_key": "patro202303d",
      "metadata": {
        "title": "SpectFormer: Frequency and Attention is what you need in a Vision Transformer",
        "authors": [
          "Badri N. Patro",
          "Vinay P. Namboodiri",
          "Vijay Srinivas Agneeswaran"
        ],
        "published_date": "2023",
        "abstract": "Vision transformers have been applied successfully for image recognition tasks. There have been either multi-headed self-attention based (ViT [12], DelT [54]) simi-lar to the original work in textual models or more re-cently based on spectral layers (Fnet [29], GFNet [46], AFNO [15]). We hypothesize that spectral layers cap-ture high-frequency information such as lines and edges, while attention layers capture token interactions. We inves-tigate this hypothesis through this work and observe that indeed mixing spectral and multi-headed attention layers provides a better transformer architecture. We thus pro-pose the novel Spectformer architecture for vision trans-formers that has initial spectral and deeper multi-headed attention layers. We believe that the resulting representation allows the transformer to capture the feature repre-sentation appropriately and it yields improved performance over other transformer representations. For instance, it im-proves the top-1 accuracy by 2% on ImageNet compared to both GFNet-H and LiT. SpectFormer-H-S reaches 84.25% top-1 accuracy on ImageNet-1 K (state of the art for small version). Further, Spectformer-H-L achieves 85.7% which is the state of the art for the comparable base version of the transformers. We further validated the SpectFormer per-formance in other scenarios such as transfer learning on standard datasets such as ClFAR-10, ClFAR-100, Oxford-lIlT-flower, and Standford Car datasets. We then investigate its use in downstream tasks such as object detection and instance segmentation on the MS-COCO dataset and ob-serve that Spectformer shows consistent performance that is comparable to the best backbones and can be further optimized and improved. The source code is available on this website https://github.com/badripatro/SpectFormers.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c57467e652f3f9131b3e7e40c23059abe395f01d.pdf",
        "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
        "citationCount": 70,
        "score": 35.0,
        "summary": "Vision transformers have been applied successfully for image recognition tasks. There have been either multi-headed self-attention based (ViT [12], DelT [54]) simi-lar to the original work in textual models or more re-cently based on spectral layers (Fnet [29], GFNet [46], AFNO [15]). We hypothesize that spectral layers cap-ture high-frequency information such as lines and edges, while attention layers capture token interactions. We inves-tigate this hypothesis through this work and observe that indeed mixing spectral and multi-headed attention layers provides a better transformer architecture. We thus pro-pose the novel Spectformer architecture for vision trans-formers that has initial spectral and deeper multi-headed attention layers. We believe that the resulting representation allows the transformer to capture the feature repre-sentation appropriately and it yields improved performance over other transformer representations. For instance, it im-proves the top-1 accuracy by 2% on ImageNet compared to both GFNet-H and LiT. SpectFormer-H-S reaches 84.25% top-1 accuracy on ImageNet-1 K (state of the art for small version). Further, Spectformer-H-L achieves 85.7% which is the state of the art for the comparable base version of the transformers. We further validated the SpectFormer per-formance in other scenarios such as transfer learning on standard datasets such as ClFAR-10, ClFAR-100, Oxford-lIlT-flower, and Standford Car datasets. We then investigate its use in downstream tasks such as object detection and instance segmentation on the MS-COCO dataset and ob-serve that Spectformer shows consistent performance that is comparable to the best backbones and can be further optimized and improved. The source code is available on this website https://github.com/badripatro/SpectFormers.",
        "keywords": []
      },
      "file_name": "c57467e652f3f9131b3e7e40c23059abe395f01d.pdf"
    },
    {
      "success": true,
      "doc_id": "0d8b7b0593b6b20d1f23e1019f68134b",
      "summary": "Self-attention mechanism has been a key factor in the recent progress of Vision Transformer (ViT), which enables adaptive feature extraction from global contexts. However, existing self-attention methods either adopt sparse global attention or window attention to reduce the computation complexity, which may compromise the local feature learning or subject to some handcrafted designs. In contrast, local attention, which restricts the receptive field of each query to its own neighboring pixels, enjoys the benefits of both convolution and self-attention, namely local inductive bias and dynamic feature selection. Nevertheless, current local attention modules either use inefficient Im2Col function or rely on specific CUDA kernels that are hard to generalize to devices without CUDA support. In this paper, we propose a novel local attention module, Slide Attention, which leverages common convolution operations to achieve high efficiency, flexibility and generalizability. Specifically, we first re-interpret the column-based Im2Col function from a new row-based perspective and use Depthwise Convolution as an efficient substitution. On this basis, we propose a deformed shifting module based on the re-parameterization technique, which further relaxes the fixed key/value positions to deformed features in the local region. In this way, our module realizes the local attention paradigm in both efficient and flexible manner. Extensive experiments show that our slide attention module is applicable to a variety of advanced Vision Transformer models and compatible with various hardware devices, and achieves consistently improved performances on comprehensive benchmarks.",
      "intriguing_abstract": "Self-attention mechanism has been a key factor in the recent progress of Vision Transformer (ViT), which enables adaptive feature extraction from global contexts. However, existing self-attention methods either adopt sparse global attention or window attention to reduce the computation complexity, which may compromise the local feature learning or subject to some handcrafted designs. In contrast, local attention, which restricts the receptive field of each query to its own neighboring pixels, enjoys the benefits of both convolution and self-attention, namely local inductive bias and dynamic feature selection. Nevertheless, current local attention modules either use inefficient Im2Col function or rely on specific CUDA kernels that are hard to generalize to devices without CUDA support. In this paper, we propose a novel local attention module, Slide Attention, which leverages common convolution operations to achieve high efficiency, flexibility and generalizability. Specifically, we first re-interpret the column-based Im2Col function from a new row-based perspective and use Depthwise Convolution as an efficient substitution. On this basis, we propose a deformed shifting module based on the re-parameterization technique, which further relaxes the fixed key/value positions to deformed features in the local region. In this way, our module realizes the local attention paradigm in both efficient and flexible manner. Extensive experiments show that our slide attention module is applicable to a variety of advanced Vision Transformer models and compatible with various hardware devices, and achieves consistently improved performances on comprehensive benchmarks.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/53e5db85e2a7442f20670be2ae25019fcf9d27a2.pdf",
      "citation_key": "pan2023hry",
      "metadata": {
        "title": "Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention",
        "authors": [
          "Xuran Pan",
          "Tianzhu Ye",
          "Zhuofan Xia",
          "S. Song",
          "Gao Huang"
        ],
        "published_date": "2023",
        "abstract": "Self-attention mechanism has been a key factor in the recent progress of Vision Transformer (ViT), which enables adaptive feature extraction from global contexts. However, existing self-attention methods either adopt sparse global attention or window attention to reduce the computation complexity, which may compromise the local feature learning or subject to some handcrafted designs. In contrast, local attention, which restricts the receptive field of each query to its own neighboring pixels, enjoys the benefits of both convolution and self-attention, namely local inductive bias and dynamic feature selection. Nevertheless, current local attention modules either use inefficient Im2Col function or rely on specific CUDA kernels that are hard to generalize to devices without CUDA support. In this paper, we propose a novel local attention module, Slide Attention, which leverages common convolution operations to achieve high efficiency, flexibility and generalizability. Specifically, we first re-interpret the column-based Im2Col function from a new row-based perspective and use Depthwise Convolution as an efficient substitution. On this basis, we propose a deformed shifting module based on the re-parameterization technique, which further relaxes the fixed key/value positions to deformed features in the local region. In this way, our module realizes the local attention paradigm in both efficient and flexible manner. Extensive experiments show that our slide attention module is applicable to a variety of advanced Vision Transformer models and compatible with various hardware devices, and achieves consistently improved performances on comprehensive benchmarks.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/53e5db85e2a7442f20670be2ae25019fcf9d27a2.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 69,
        "score": 34.5,
        "summary": "Self-attention mechanism has been a key factor in the recent progress of Vision Transformer (ViT), which enables adaptive feature extraction from global contexts. However, existing self-attention methods either adopt sparse global attention or window attention to reduce the computation complexity, which may compromise the local feature learning or subject to some handcrafted designs. In contrast, local attention, which restricts the receptive field of each query to its own neighboring pixels, enjoys the benefits of both convolution and self-attention, namely local inductive bias and dynamic feature selection. Nevertheless, current local attention modules either use inefficient Im2Col function or rely on specific CUDA kernels that are hard to generalize to devices without CUDA support. In this paper, we propose a novel local attention module, Slide Attention, which leverages common convolution operations to achieve high efficiency, flexibility and generalizability. Specifically, we first re-interpret the column-based Im2Col function from a new row-based perspective and use Depthwise Convolution as an efficient substitution. On this basis, we propose a deformed shifting module based on the re-parameterization technique, which further relaxes the fixed key/value positions to deformed features in the local region. In this way, our module realizes the local attention paradigm in both efficient and flexible manner. Extensive experiments show that our slide attention module is applicable to a variety of advanced Vision Transformer models and compatible with various hardware devices, and achieves consistently improved performances on comprehensive benchmarks.",
        "keywords": []
      },
      "file_name": "53e5db85e2a7442f20670be2ae25019fcf9d27a2.pdf"
    },
    {
      "success": true,
      "doc_id": "5e0e9fdad715c72a0101b00058d31f1d",
      "summary": "Medical image segmentation is increasingly reliant on deep learning techniques, yet the promising performance often come with high annotation costs. This paper introduces Weak-Mamba-UNet, an innovative weakly-supervised learning (WSL) framework that leverages the capabilities of Convolutional Neural Network (CNN), Vision Transformer (ViT), and the cutting-edge Visual Mamba (VMamba) architecture for medical image segmentation, especially when dealing with scribble-based annotations. The proposed WSL strategy incorporates three distinct architecture but same symmetrical encoder-decoder networks: a CNN-based UNet for detailed local feature extraction, a Swin Transformer-based SwinUNet for comprehensive global context understanding, and a VMamba-based Mamba-UNet for efficient long-range dependency modeling. The key concept of this framework is a collaborative and cross-supervisory mechanism that employs pseudo labels to facilitate iterative learning and refinement across the networks. The effectiveness of Weak-Mamba-UNet is validated on a publicly available MRI cardiac segmentation dataset with processed scribble annotations, where it surpasses the performance of a similar WSL framework utilizing only UNet or SwinUNet. This highlights its potential in scenarios with sparse or imprecise annotations. The source code is made publicly accessible.",
      "intriguing_abstract": "Medical image segmentation is increasingly reliant on deep learning techniques, yet the promising performance often come with high annotation costs. This paper introduces Weak-Mamba-UNet, an innovative weakly-supervised learning (WSL) framework that leverages the capabilities of Convolutional Neural Network (CNN), Vision Transformer (ViT), and the cutting-edge Visual Mamba (VMamba) architecture for medical image segmentation, especially when dealing with scribble-based annotations. The proposed WSL strategy incorporates three distinct architecture but same symmetrical encoder-decoder networks: a CNN-based UNet for detailed local feature extraction, a Swin Transformer-based SwinUNet for comprehensive global context understanding, and a VMamba-based Mamba-UNet for efficient long-range dependency modeling. The key concept of this framework is a collaborative and cross-supervisory mechanism that employs pseudo labels to facilitate iterative learning and refinement across the networks. The effectiveness of Weak-Mamba-UNet is validated on a publicly available MRI cardiac segmentation dataset with processed scribble annotations, where it surpasses the performance of a similar WSL framework utilizing only UNet or SwinUNet. This highlights its potential in scenarios with sparse or imprecise annotations. The source code is made publicly accessible.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/0682771fd5f611bce2a536bf83587532469a83df.pdf",
      "citation_key": "wang2024mrk",
      "metadata": {
        "title": "Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for Scribble-based Medical Image Segmentation",
        "authors": [
          "Ziyang Wang",
          "Chao Ma"
        ],
        "published_date": "2024",
        "abstract": "Medical image segmentation is increasingly reliant on deep learning techniques, yet the promising performance often come with high annotation costs. This paper introduces Weak-Mamba-UNet, an innovative weakly-supervised learning (WSL) framework that leverages the capabilities of Convolutional Neural Network (CNN), Vision Transformer (ViT), and the cutting-edge Visual Mamba (VMamba) architecture for medical image segmentation, especially when dealing with scribble-based annotations. The proposed WSL strategy incorporates three distinct architecture but same symmetrical encoder-decoder networks: a CNN-based UNet for detailed local feature extraction, a Swin Transformer-based SwinUNet for comprehensive global context understanding, and a VMamba-based Mamba-UNet for efficient long-range dependency modeling. The key concept of this framework is a collaborative and cross-supervisory mechanism that employs pseudo labels to facilitate iterative learning and refinement across the networks. The effectiveness of Weak-Mamba-UNet is validated on a publicly available MRI cardiac segmentation dataset with processed scribble annotations, where it surpasses the performance of a similar WSL framework utilizing only UNet or SwinUNet. This highlights its potential in scenarios with sparse or imprecise annotations. The source code is made publicly accessible.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0682771fd5f611bce2a536bf83587532469a83df.pdf",
        "venue": "arXiv.org",
        "citationCount": 32,
        "score": 32.0,
        "summary": "Medical image segmentation is increasingly reliant on deep learning techniques, yet the promising performance often come with high annotation costs. This paper introduces Weak-Mamba-UNet, an innovative weakly-supervised learning (WSL) framework that leverages the capabilities of Convolutional Neural Network (CNN), Vision Transformer (ViT), and the cutting-edge Visual Mamba (VMamba) architecture for medical image segmentation, especially when dealing with scribble-based annotations. The proposed WSL strategy incorporates three distinct architecture but same symmetrical encoder-decoder networks: a CNN-based UNet for detailed local feature extraction, a Swin Transformer-based SwinUNet for comprehensive global context understanding, and a VMamba-based Mamba-UNet for efficient long-range dependency modeling. The key concept of this framework is a collaborative and cross-supervisory mechanism that employs pseudo labels to facilitate iterative learning and refinement across the networks. The effectiveness of Weak-Mamba-UNet is validated on a publicly available MRI cardiac segmentation dataset with processed scribble annotations, where it surpasses the performance of a similar WSL framework utilizing only UNet or SwinUNet. This highlights its potential in scenarios with sparse or imprecise annotations. The source code is made publicly accessible.",
        "keywords": []
      },
      "file_name": "0682771fd5f611bce2a536bf83587532469a83df.pdf"
    },
    {
      "success": true,
      "doc_id": "0dff541b2575f0433dfd0187243dd83a",
      "summary": "This paper proposes a novel approach for extracting deep features and classifying diseased plant leaves. The agriculture industry is negatively impacted by plant diseases causing crop and economic loss. Accurate and timely diagnosis is crucial for managing and controlling plant diseases, as traditional methods can be costly and time-consuming. Deep learning-based tools effectively detect plant diseases depending on the qualitative of extracted features. In this regard, a hybrid model for plant disease classification based on a Transfer Learning-based model followed by a vision transformer (TLMViT) is proposed. TLMViT has four stages: 1) data acquisition, where the PlantVillage and wheat datasets are used to train and evaluate the proposed model, 2) image augmentation to increase the number of training samples and overcome the overfitting issue, 3) leaf feature extraction by two consecutive phases: initial features extraction by using pre-trained based model and deep features extraction by using ViT model, and 4) classification by using MLP classifier. TLMViT is experimented with using five pre-trained-based models followed by ViT individually. TLMViT performs accurately in plant disease classification, obtaining 98.81% and 99.86% validation accuracy for VGG19 followed by the ViT model on PlantVillage and wheat datasets respectively. Moreover, TLMViT is compared with pre-trained-based architecture. The comparison result illustrates that TLMViT achieved an enhancement of 1.11% and 1.099% in validation accuracy, 2.576% and 2.92% in validation loss compared with the transfer learning-based model for PlantVillage and wheat datasets respectively. Thereby proposed model proves the efficiency of using ViT for extracting deep features from the leaf.",
      "intriguing_abstract": "This paper proposes a novel approach for extracting deep features and classifying diseased plant leaves. The agriculture industry is negatively impacted by plant diseases causing crop and economic loss. Accurate and timely diagnosis is crucial for managing and controlling plant diseases, as traditional methods can be costly and time-consuming. Deep learning-based tools effectively detect plant diseases depending on the qualitative of extracted features. In this regard, a hybrid model for plant disease classification based on a Transfer Learning-based model followed by a vision transformer (TLMViT) is proposed. TLMViT has four stages: 1) data acquisition, where the PlantVillage and wheat datasets are used to train and evaluate the proposed model, 2) image augmentation to increase the number of training samples and overcome the overfitting issue, 3) leaf feature extraction by two consecutive phases: initial features extraction by using pre-trained based model and deep features extraction by using ViT model, and 4) classification by using MLP classifier. TLMViT is experimented with using five pre-trained-based models followed by ViT individually. TLMViT performs accurately in plant disease classification, obtaining 98.81% and 99.86% validation accuracy for VGG19 followed by the ViT model on PlantVillage and wheat datasets respectively. Moreover, TLMViT is compared with pre-trained-based architecture. The comparison result illustrates that TLMViT achieved an enhancement of 1.11% and 1.099% in validation accuracy, 2.576% and 2.92% in validation loss compared with the transfer learning-based model for PlantVillage and wheat datasets respectively. Thereby proposed model proves the efficiency of using ViT for extracting deep features from the leaf.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/a4ef2c7c321dea04d8ea91ec1ce0710ebce4f848.pdf",
      "citation_key": "tabbakh2023ao7",
      "metadata": {
        "title": "A Deep Features Extraction Model Based on the Transfer Learning Model and Vision Transformer TLMViT for Plant Disease Classification",
        "authors": [
          "A. Tabbakh",
          "Soubhagya Sankar Barpanda"
        ],
        "published_date": "2023",
        "abstract": "This paper proposes a novel approach for extracting deep features and classifying diseased plant leaves. The agriculture industry is negatively impacted by plant diseases causing crop and economic loss. Accurate and timely diagnosis is crucial for managing and controlling plant diseases, as traditional methods can be costly and time-consuming. Deep learning-based tools effectively detect plant diseases depending on the qualitative of extracted features. In this regard, a hybrid model for plant disease classification based on a Transfer Learning-based model followed by a vision transformer (TLMViT) is proposed. TLMViT has four stages: 1) data acquisition, where the PlantVillage and wheat datasets are used to train and evaluate the proposed model, 2) image augmentation to increase the number of training samples and overcome the overfitting issue, 3) leaf feature extraction by two consecutive phases: initial features extraction by using pre-trained based model and deep features extraction by using ViT model, and 4) classification by using MLP classifier. TLMViT is experimented with using five pre-trained-based models followed by ViT individually. TLMViT performs accurately in plant disease classification, obtaining 98.81% and 99.86% validation accuracy for VGG19 followed by the ViT model on PlantVillage and wheat datasets respectively. Moreover, TLMViT is compared with pre-trained-based architecture. The comparison result illustrates that TLMViT achieved an enhancement of 1.11% and 1.099% in validation accuracy, 2.576% and 2.92% in validation loss compared with the transfer learning-based model for PlantVillage and wheat datasets respectively. Thereby proposed model proves the efficiency of using ViT for extracting deep features from the leaf.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a4ef2c7c321dea04d8ea91ec1ce0710ebce4f848.pdf",
        "venue": "IEEE Access",
        "citationCount": 64,
        "score": 32.0,
        "summary": "This paper proposes a novel approach for extracting deep features and classifying diseased plant leaves. The agriculture industry is negatively impacted by plant diseases causing crop and economic loss. Accurate and timely diagnosis is crucial for managing and controlling plant diseases, as traditional methods can be costly and time-consuming. Deep learning-based tools effectively detect plant diseases depending on the qualitative of extracted features. In this regard, a hybrid model for plant disease classification based on a Transfer Learning-based model followed by a vision transformer (TLMViT) is proposed. TLMViT has four stages: 1) data acquisition, where the PlantVillage and wheat datasets are used to train and evaluate the proposed model, 2) image augmentation to increase the number of training samples and overcome the overfitting issue, 3) leaf feature extraction by two consecutive phases: initial features extraction by using pre-trained based model and deep features extraction by using ViT model, and 4) classification by using MLP classifier. TLMViT is experimented with using five pre-trained-based models followed by ViT individually. TLMViT performs accurately in plant disease classification, obtaining 98.81% and 99.86% validation accuracy for VGG19 followed by the ViT model on PlantVillage and wheat datasets respectively. Moreover, TLMViT is compared with pre-trained-based architecture. The comparison result illustrates that TLMViT achieved an enhancement of 1.11% and 1.099% in validation accuracy, 2.576% and 2.92% in validation loss compared with the transfer learning-based model for PlantVillage and wheat datasets respectively. Thereby proposed model proves the efficiency of using ViT for extracting deep features from the leaf.",
        "keywords": []
      },
      "file_name": "a4ef2c7c321dea04d8ea91ec1ce0710ebce4f848.pdf"
    },
    {
      "success": true,
      "doc_id": "a7a5db950beb27f362bcc656ff55306e",
      "summary": "The current advancement towards retinal disease detection mainly focused on distinct feature extraction using either a convolutional neural network (CNN) or a transformer-based end-to-end deep learning (DL) model. The individual end-to-end DL models are capable of only processing texture or shape-based information for performing detection tasks. However, extraction of only texture- or shape-based features does not provide the model robustness needed to classify different types of retinal diseases. Therefore, concerning these two features, this paper developed a fusion model called Conv-ViT to detect retinal diseases from foveal cut optical coherence tomography (OCT) images. The transfer learning-based CNN models, such as Inception-V3 and ResNet-50, are utilized to process texture information by calculating the correlation of the nearby pixel. Additionally, the vision transformer model is fused to process shape-based features by determining the correlation between long-distance pixels. The hybridization of these three models results in shape-based texture feature learning during the classification of retinal diseases into its four classes, including choroidal neovascularization (CNV), diabetic macular edema (DME), DRUSEN, and NORMAL. The weighted average classification accuracy, precision, recall, and F1 score of the model are found to be approximately 94%. The results indicate that the fusion of both texture and shape features assisted the proposed Conv-ViT model to outperform the state-of-the-art retinal disease classification models.",
      "intriguing_abstract": "The current advancement towards retinal disease detection mainly focused on distinct feature extraction using either a convolutional neural network (CNN) or a transformer-based end-to-end deep learning (DL) model. The individual end-to-end DL models are capable of only processing texture or shape-based information for performing detection tasks. However, extraction of only texture- or shape-based features does not provide the model robustness needed to classify different types of retinal diseases. Therefore, concerning these two features, this paper developed a fusion model called Conv-ViT to detect retinal diseases from foveal cut optical coherence tomography (OCT) images. The transfer learning-based CNN models, such as Inception-V3 and ResNet-50, are utilized to process texture information by calculating the correlation of the nearby pixel. Additionally, the vision transformer model is fused to process shape-based features by determining the correlation between long-distance pixels. The hybridization of these three models results in shape-based texture feature learning during the classification of retinal diseases into its four classes, including choroidal neovascularization (CNV), diabetic macular edema (DME), DRUSEN, and NORMAL. The weighted average classification accuracy, precision, recall, and F1 score of the model are found to be approximately 94%. The results indicate that the fusion of both texture and shape features assisted the proposed Conv-ViT model to outperform the state-of-the-art retinal disease classification models.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/243a056d1acb153f70e39cc80a10e7d211a4312f.pdf",
      "citation_key": "dutta2023aet",
      "metadata": {
        "title": "Conv-ViT: A Convolution and Vision Transformer-Based Hybrid Feature Extraction Method for Retinal Disease Detection",
        "authors": [
          "Pramit Dutta",
          "Khaleda Akhter Sathi",
          "Md.Azad Hossain",
          "M. Ali",
          "Akber Dewan"
        ],
        "published_date": "2023",
        "abstract": "The current advancement towards retinal disease detection mainly focused on distinct feature extraction using either a convolutional neural network (CNN) or a transformer-based end-to-end deep learning (DL) model. The individual end-to-end DL models are capable of only processing texture or shape-based information for performing detection tasks. However, extraction of only texture- or shape-based features does not provide the model robustness needed to classify different types of retinal diseases. Therefore, concerning these two features, this paper developed a fusion model called Conv-ViT to detect retinal diseases from foveal cut optical coherence tomography (OCT) images. The transfer learning-based CNN models, such as Inception-V3 and ResNet-50, are utilized to process texture information by calculating the correlation of the nearby pixel. Additionally, the vision transformer model is fused to process shape-based features by determining the correlation between long-distance pixels. The hybridization of these three models results in shape-based texture feature learning during the classification of retinal diseases into its four classes, including choroidal neovascularization (CNV), diabetic macular edema (DME), DRUSEN, and NORMAL. The weighted average classification accuracy, precision, recall, and F1 score of the model are found to be approximately 94%. The results indicate that the fusion of both texture and shape features assisted the proposed Conv-ViT model to outperform the state-of-the-art retinal disease classification models.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/243a056d1acb153f70e39cc80a10e7d211a4312f.pdf",
        "venue": "Journal of Imaging",
        "citationCount": 61,
        "score": 30.5,
        "summary": "The current advancement towards retinal disease detection mainly focused on distinct feature extraction using either a convolutional neural network (CNN) or a transformer-based end-to-end deep learning (DL) model. The individual end-to-end DL models are capable of only processing texture or shape-based information for performing detection tasks. However, extraction of only texture- or shape-based features does not provide the model robustness needed to classify different types of retinal diseases. Therefore, concerning these two features, this paper developed a fusion model called Conv-ViT to detect retinal diseases from foveal cut optical coherence tomography (OCT) images. The transfer learning-based CNN models, such as Inception-V3 and ResNet-50, are utilized to process texture information by calculating the correlation of the nearby pixel. Additionally, the vision transformer model is fused to process shape-based features by determining the correlation between long-distance pixels. The hybridization of these three models results in shape-based texture feature learning during the classification of retinal diseases into its four classes, including choroidal neovascularization (CNV), diabetic macular edema (DME), DRUSEN, and NORMAL. The weighted average classification accuracy, precision, recall, and F1 score of the model are found to be approximately 94%. The results indicate that the fusion of both texture and shape features assisted the proposed Conv-ViT model to outperform the state-of-the-art retinal disease classification models.",
        "keywords": []
      },
      "file_name": "243a056d1acb153f70e39cc80a10e7d211a4312f.pdf"
    },
    {
      "success": true,
      "doc_id": "ebc6283b3af8eb8983d0fb3c582f98a8",
      "summary": "Determining dense feature points on fingerprints used in constructing deep fixed-length representations for accurate matching, particularly at the pixel level, is of significant interest. To explore the interpretability of fingerprint matching, we propose a multi-stage interpretable fingerprint matching network, namely Interpretable Fixed-length Representation for Fingerprint Matching via Vision Transformer (IFViT), which consists of two primary modules. The first module, an interpretable dense registration module, establishes a Vision Transformer (ViT)-based Siamese Network to capture long-range dependencies and the global context in fingerprint pairs. It provides interpretable dense pixel-wise correspondences of feature points for fingerprint alignment and enhances the interpretability in the subsequent matching stage. The second module takes into account both local and global representations of the aligned fingerprint pair to achieve an interpretable fixed-length representation extraction and matching. It employs the ViTs trained in the first module with the additional fully connected layer and retrains them to simultaneously produce the discriminative fixed-length representation and interpretable dense pixel-wise correspondences of feature points. Extensive experimental results on diverse publicly available fingerprint databases demonstrate that the proposed framework not only exhibits superior performance on dense registration and matching but also significantly promotes the interpretability in deep fixed-length representations-based fingerprint matching.",
      "intriguing_abstract": "Determining dense feature points on fingerprints used in constructing deep fixed-length representations for accurate matching, particularly at the pixel level, is of significant interest. To explore the interpretability of fingerprint matching, we propose a multi-stage interpretable fingerprint matching network, namely Interpretable Fixed-length Representation for Fingerprint Matching via Vision Transformer (IFViT), which consists of two primary modules. The first module, an interpretable dense registration module, establishes a Vision Transformer (ViT)-based Siamese Network to capture long-range dependencies and the global context in fingerprint pairs. It provides interpretable dense pixel-wise correspondences of feature points for fingerprint alignment and enhances the interpretability in the subsequent matching stage. The second module takes into account both local and global representations of the aligned fingerprint pair to achieve an interpretable fixed-length representation extraction and matching. It employs the ViTs trained in the first module with the additional fully connected layer and retrains them to simultaneously produce the discriminative fixed-length representation and interpretable dense pixel-wise correspondences of feature points. Extensive experimental results on diverse publicly available fingerprint databases demonstrate that the proposed framework not only exhibits superior performance on dense registration and matching but also significantly promotes the interpretability in deep fixed-length representations-based fingerprint matching.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d8ab87176444f8b0747972310431c647a87de2df.pdf",
      "citation_key": "qiu2024eh4",
      "metadata": {
        "title": "IFViT: Interpretable Fixed-Length Representation for Fingerprint Matching via Vision Transformer",
        "authors": [
          "Yuhang Qiu",
          "Honghui Chen",
          "Xingbo Dong",
          "Zheng Lin",
          "Iman Yi Liao",
          "Massimo Tistarelli",
          "Zhe Jin"
        ],
        "published_date": "2024",
        "abstract": "Determining dense feature points on fingerprints used in constructing deep fixed-length representations for accurate matching, particularly at the pixel level, is of significant interest. To explore the interpretability of fingerprint matching, we propose a multi-stage interpretable fingerprint matching network, namely Interpretable Fixed-length Representation for Fingerprint Matching via Vision Transformer (IFViT), which consists of two primary modules. The first module, an interpretable dense registration module, establishes a Vision Transformer (ViT)-based Siamese Network to capture long-range dependencies and the global context in fingerprint pairs. It provides interpretable dense pixel-wise correspondences of feature points for fingerprint alignment and enhances the interpretability in the subsequent matching stage. The second module takes into account both local and global representations of the aligned fingerprint pair to achieve an interpretable fixed-length representation extraction and matching. It employs the ViTs trained in the first module with the additional fully connected layer and retrains them to simultaneously produce the discriminative fixed-length representation and interpretable dense pixel-wise correspondences of feature points. Extensive experimental results on diverse publicly available fingerprint databases demonstrate that the proposed framework not only exhibits superior performance on dense registration and matching but also significantly promotes the interpretability in deep fixed-length representations-based fingerprint matching.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d8ab87176444f8b0747972310431c647a87de2df.pdf",
        "venue": "IEEE Transactions on Information Forensics and Security",
        "citationCount": 30,
        "score": 30.0,
        "summary": "Determining dense feature points on fingerprints used in constructing deep fixed-length representations for accurate matching, particularly at the pixel level, is of significant interest. To explore the interpretability of fingerprint matching, we propose a multi-stage interpretable fingerprint matching network, namely Interpretable Fixed-length Representation for Fingerprint Matching via Vision Transformer (IFViT), which consists of two primary modules. The first module, an interpretable dense registration module, establishes a Vision Transformer (ViT)-based Siamese Network to capture long-range dependencies and the global context in fingerprint pairs. It provides interpretable dense pixel-wise correspondences of feature points for fingerprint alignment and enhances the interpretability in the subsequent matching stage. The second module takes into account both local and global representations of the aligned fingerprint pair to achieve an interpretable fixed-length representation extraction and matching. It employs the ViTs trained in the first module with the additional fully connected layer and retrains them to simultaneously produce the discriminative fixed-length representation and interpretable dense pixel-wise correspondences of feature points. Extensive experimental results on diverse publicly available fingerprint databases demonstrate that the proposed framework not only exhibits superior performance on dense registration and matching but also significantly promotes the interpretability in deep fixed-length representations-based fingerprint matching.",
        "keywords": []
      },
      "file_name": "d8ab87176444f8b0747972310431c647a87de2df.pdf"
    },
    {
      "success": true,
      "doc_id": "bed96680462f30e1c11a95bf15196c82",
      "summary": "Due to the constraints of agricultural computing resources and the diversity of plant diseases, it is challenging to achieve the desired accuracy rate while keeping the network lightweight. In this paper, we proposed a computationally efficient deep learning architecture based on the mobile vision transformer (MobileViT) for real-time detection of plant diseases, which we called plant-based MobileViT (PMVT). Our proposed model was designed to be highly accurate and low-cost, making it suitable for deployment on mobile devices with limited resources. Specifically, we replaced the convolution block in MobileViT with an inverted residual structure that employs a 77 convolution kernel to effectively model long-distance dependencies between different leaves in plant disease images. Furthermore, inspired by the concept of multi-level attention in computer vision tasks, we integrated a convolutional block attention module (CBAM) into the standard ViT encoder. This integration allows the network to effectively avoid irrelevant information and focus on essential features. The PMVT network achieves reduced parameter counts compared to alternative networks on various mobile devices while maintaining high accuracy across different vision tasks. Extensive experiments on multiple agricultural datasets, including wheat, coffee, and rice, demonstrate that the proposed method outperforms the current best lightweight and heavyweight models. On the wheat dataset, PMVT achieves the highest accuracy of 93.6% using approximately 0.98 million (M) parameters. This accuracy is 1.6% higher than that of MobileNetV3. Under the same parameters, PMVT achieved an accuracy of 85.4% on the coffee dataset, surpassing SqueezeNet by 2.3%. Furthermore, out method achieved an accuracy of 93.1% on the rice dataset, surpassing MobileNetV3 by 3.4%. Additionally, we developed a plant disease diagnosis app and successfully used the trained PMVT model to identify plant disease in different scenarios.",
      "intriguing_abstract": "Due to the constraints of agricultural computing resources and the diversity of plant diseases, it is challenging to achieve the desired accuracy rate while keeping the network lightweight. In this paper, we proposed a computationally efficient deep learning architecture based on the mobile vision transformer (MobileViT) for real-time detection of plant diseases, which we called plant-based MobileViT (PMVT). Our proposed model was designed to be highly accurate and low-cost, making it suitable for deployment on mobile devices with limited resources. Specifically, we replaced the convolution block in MobileViT with an inverted residual structure that employs a 77 convolution kernel to effectively model long-distance dependencies between different leaves in plant disease images. Furthermore, inspired by the concept of multi-level attention in computer vision tasks, we integrated a convolutional block attention module (CBAM) into the standard ViT encoder. This integration allows the network to effectively avoid irrelevant information and focus on essential features. The PMVT network achieves reduced parameter counts compared to alternative networks on various mobile devices while maintaining high accuracy across different vision tasks. Extensive experiments on multiple agricultural datasets, including wheat, coffee, and rice, demonstrate that the proposed method outperforms the current best lightweight and heavyweight models. On the wheat dataset, PMVT achieves the highest accuracy of 93.6% using approximately 0.98 million (M) parameters. This accuracy is 1.6% higher than that of MobileNetV3. Under the same parameters, PMVT achieved an accuracy of 85.4% on the coffee dataset, surpassing SqueezeNet by 2.3%. Furthermore, out method achieved an accuracy of 93.1% on the rice dataset, surpassing MobileNetV3 by 3.4%. Additionally, we developed a plant disease diagnosis app and successfully used the trained PMVT model to identify plant disease in different scenarios.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/a1e17a09b0df676f133c2175366ab7fcd0381ac1.pdf",
      "citation_key": "li2023nnd",
      "metadata": {
        "title": "PMVT: a lightweight vision transformer for plant disease identification on mobile devices",
        "authors": [
          "Guoqiang Li",
          "Yuchao Wang",
          "Qing Zhao",
          "Peiyan Yuan",
          "Baofang Chang"
        ],
        "published_date": "2023",
        "abstract": "Due to the constraints of agricultural computing resources and the diversity of plant diseases, it is challenging to achieve the desired accuracy rate while keeping the network lightweight. In this paper, we proposed a computationally efficient deep learning architecture based on the mobile vision transformer (MobileViT) for real-time detection of plant diseases, which we called plant-based MobileViT (PMVT). Our proposed model was designed to be highly accurate and low-cost, making it suitable for deployment on mobile devices with limited resources. Specifically, we replaced the convolution block in MobileViT with an inverted residual structure that employs a 77 convolution kernel to effectively model long-distance dependencies between different leaves in plant disease images. Furthermore, inspired by the concept of multi-level attention in computer vision tasks, we integrated a convolutional block attention module (CBAM) into the standard ViT encoder. This integration allows the network to effectively avoid irrelevant information and focus on essential features. The PMVT network achieves reduced parameter counts compared to alternative networks on various mobile devices while maintaining high accuracy across different vision tasks. Extensive experiments on multiple agricultural datasets, including wheat, coffee, and rice, demonstrate that the proposed method outperforms the current best lightweight and heavyweight models. On the wheat dataset, PMVT achieves the highest accuracy of 93.6% using approximately 0.98 million (M) parameters. This accuracy is 1.6% higher than that of MobileNetV3. Under the same parameters, PMVT achieved an accuracy of 85.4% on the coffee dataset, surpassing SqueezeNet by 2.3%. Furthermore, out method achieved an accuracy of 93.1% on the rice dataset, surpassing MobileNetV3 by 3.4%. Additionally, we developed a plant disease diagnosis app and successfully used the trained PMVT model to identify plant disease in different scenarios.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a1e17a09b0df676f133c2175366ab7fcd0381ac1.pdf",
        "venue": "Frontiers in Plant Science",
        "citationCount": 60,
        "score": 30.0,
        "summary": "Due to the constraints of agricultural computing resources and the diversity of plant diseases, it is challenging to achieve the desired accuracy rate while keeping the network lightweight. In this paper, we proposed a computationally efficient deep learning architecture based on the mobile vision transformer (MobileViT) for real-time detection of plant diseases, which we called plant-based MobileViT (PMVT). Our proposed model was designed to be highly accurate and low-cost, making it suitable for deployment on mobile devices with limited resources. Specifically, we replaced the convolution block in MobileViT with an inverted residual structure that employs a 77 convolution kernel to effectively model long-distance dependencies between different leaves in plant disease images. Furthermore, inspired by the concept of multi-level attention in computer vision tasks, we integrated a convolutional block attention module (CBAM) into the standard ViT encoder. This integration allows the network to effectively avoid irrelevant information and focus on essential features. The PMVT network achieves reduced parameter counts compared to alternative networks on various mobile devices while maintaining high accuracy across different vision tasks. Extensive experiments on multiple agricultural datasets, including wheat, coffee, and rice, demonstrate that the proposed method outperforms the current best lightweight and heavyweight models. On the wheat dataset, PMVT achieves the highest accuracy of 93.6% using approximately 0.98 million (M) parameters. This accuracy is 1.6% higher than that of MobileNetV3. Under the same parameters, PMVT achieved an accuracy of 85.4% on the coffee dataset, surpassing SqueezeNet by 2.3%. Furthermore, out method achieved an accuracy of 93.1% on the rice dataset, surpassing MobileNetV3 by 3.4%. Additionally, we developed a plant disease diagnosis app and successfully used the trained PMVT model to identify plant disease in different scenarios.",
        "keywords": []
      },
      "file_name": "a1e17a09b0df676f133c2175366ab7fcd0381ac1.pdf"
    },
    {
      "success": true,
      "doc_id": "5ac7b9eebc96c3f75eafcd3366c0b6e1",
      "summary": "Cross-view geo-localization of satellite and unmanned aerial vehicles (UAVs) imagery has attracted extensive attention due to its tremendous potential for global navigation satellite system (GNSS) denied navigation. However, inadequate feature representation across different views coupled with positional shifts and distance-scale uncertainty are key challenges. Most of the existing research mainly focused on extracting comprehensive and fine-grained information, yet effective feature representation and alignment should be imposed equal importance. In this article, we propose an innovative transformer-based pipeline TransFG for robust cross-view image matching, which incorporates feature aggregation (FA) and gradient guidance (GG) module. TransFG synergically takes advantage of FA and GG, achieving an effective balance in feature representation and alignment. Specifically, the proposed FA module implicitly learns salient features and dynamically aggregates contextual features from the vision transformer (ViT). The proposed GG module uses the gradient information of local features to further enhance the cross-view feature representation and aligns specific instances across different views. Extensive experiments demonstrate that our pipeline outperforms existing methods in cross-view geo-localization. It achieves an impressive improvement in R@1 and AP than the state-of-the-art (SOTA) methods. The code has been released at https://github.com/happyboy1234/TransFG.",
      "intriguing_abstract": "Cross-view geo-localization of satellite and unmanned aerial vehicles (UAVs) imagery has attracted extensive attention due to its tremendous potential for global navigation satellite system (GNSS) denied navigation. However, inadequate feature representation across different views coupled with positional shifts and distance-scale uncertainty are key challenges. Most of the existing research mainly focused on extracting comprehensive and fine-grained information, yet effective feature representation and alignment should be imposed equal importance. In this article, we propose an innovative transformer-based pipeline TransFG for robust cross-view image matching, which incorporates feature aggregation (FA) and gradient guidance (GG) module. TransFG synergically takes advantage of FA and GG, achieving an effective balance in feature representation and alignment. Specifically, the proposed FA module implicitly learns salient features and dynamically aggregates contextual features from the vision transformer (ViT). The proposed GG module uses the gradient information of local features to further enhance the cross-view feature representation and aligns specific instances across different views. Extensive experiments demonstrate that our pipeline outperforms existing methods in cross-view geo-localization. It achieves an impressive improvement in R@1 and AP than the state-of-the-art (SOTA) methods. The code has been released at https://github.com/happyboy1234/TransFG.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/77b625e6dec2732b96ca8cf57d7a123497873e1d.pdf",
      "citation_key": "zhao20243f3",
      "metadata": {
        "title": "TransFG: A Cross-View Geo-Localization of Satellite and UAVs Imagery Pipeline Using Transformer-Based Feature Aggregation and Gradient Guidance",
        "authors": [
          "Hu Zhao",
          "Keyan Ren",
          "Tianyi Yue",
          "Chun Zhang",
          "Shuai Yuan"
        ],
        "published_date": "2024",
        "abstract": "Cross-view geo-localization of satellite and unmanned aerial vehicles (UAVs) imagery has attracted extensive attention due to its tremendous potential for global navigation satellite system (GNSS) denied navigation. However, inadequate feature representation across different views coupled with positional shifts and distance-scale uncertainty are key challenges. Most of the existing research mainly focused on extracting comprehensive and fine-grained information, yet effective feature representation and alignment should be imposed equal importance. In this article, we propose an innovative transformer-based pipeline TransFG for robust cross-view image matching, which incorporates feature aggregation (FA) and gradient guidance (GG) module. TransFG synergically takes advantage of FA and GG, achieving an effective balance in feature representation and alignment. Specifically, the proposed FA module implicitly learns salient features and dynamically aggregates contextual features from the vision transformer (ViT). The proposed GG module uses the gradient information of local features to further enhance the cross-view feature representation and aligns specific instances across different views. Extensive experiments demonstrate that our pipeline outperforms existing methods in cross-view geo-localization. It achieves an impressive improvement in R@1 and AP than the state-of-the-art (SOTA) methods. The code has been released at https://github.com/happyboy1234/TransFG.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/77b625e6dec2732b96ca8cf57d7a123497873e1d.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 29,
        "score": 29.0,
        "summary": "Cross-view geo-localization of satellite and unmanned aerial vehicles (UAVs) imagery has attracted extensive attention due to its tremendous potential for global navigation satellite system (GNSS) denied navigation. However, inadequate feature representation across different views coupled with positional shifts and distance-scale uncertainty are key challenges. Most of the existing research mainly focused on extracting comprehensive and fine-grained information, yet effective feature representation and alignment should be imposed equal importance. In this article, we propose an innovative transformer-based pipeline TransFG for robust cross-view image matching, which incorporates feature aggregation (FA) and gradient guidance (GG) module. TransFG synergically takes advantage of FA and GG, achieving an effective balance in feature representation and alignment. Specifically, the proposed FA module implicitly learns salient features and dynamically aggregates contextual features from the vision transformer (ViT). The proposed GG module uses the gradient information of local features to further enhance the cross-view feature representation and aligns specific instances across different views. Extensive experiments demonstrate that our pipeline outperforms existing methods in cross-view geo-localization. It achieves an impressive improvement in R@1 and AP than the state-of-the-art (SOTA) methods. The code has been released at https://github.com/happyboy1234/TransFG.",
        "keywords": []
      },
      "file_name": "77b625e6dec2732b96ca8cf57d7a123497873e1d.pdf"
    },
    {
      "success": true,
      "doc_id": "c495bb06f76e8259f08752e98196372a",
      "summary": "Vision Transformers (ViTs) are exceptional at vision tasks. However, when applied to remote sensing images (RSIs), existing methods often necessitate extensive modifications of ViTs to rival convolutional neural networks (CNNs). This requirement significantly impedes the application of ViTs in geosciences, particularly for researchers who lack the time for comprehensive model redesign. To address this issue, we introduce the concept of quantitative regularization (QR), designed to enhance the performance of ViTs in RSI classification. QR represents an effective algorithm that adeptly manages domain discrepancies in RSIs and can be integrated with any ViTs in transfer learning. We evaluated the effectiveness of QR using three ViT architectures: vanilla ViT, SwinViT and NextViT, on four datasets: AID30, NWPU45, AFGR50 and UCM21. The results reveal that our NextViT model surpasses 39 other advanced methods published in the past 3years, maintaining robust performance even with a limited number of training samples. We also discovered that our ViT and SwinViT achieve significantly higher accuracy and robustness compared to other methods using the same backbone. Our findings confirm that ViTs can be as effective as CNNs for RSI classification, regardless of the dataset size. Our approach exclusively employs opensource ViTs and easily accessible training strategies. Consequently, we believe that our method can significantly lower the barriers for geoscience researchers intending to use ViT for RSI applications.",
      "intriguing_abstract": "Vision Transformers (ViTs) are exceptional at vision tasks. However, when applied to remote sensing images (RSIs), existing methods often necessitate extensive modifications of ViTs to rival convolutional neural networks (CNNs). This requirement significantly impedes the application of ViTs in geosciences, particularly for researchers who lack the time for comprehensive model redesign. To address this issue, we introduce the concept of quantitative regularization (QR), designed to enhance the performance of ViTs in RSI classification. QR represents an effective algorithm that adeptly manages domain discrepancies in RSIs and can be integrated with any ViTs in transfer learning. We evaluated the effectiveness of QR using three ViT architectures: vanilla ViT, SwinViT and NextViT, on four datasets: AID30, NWPU45, AFGR50 and UCM21. The results reveal that our NextViT model surpasses 39 other advanced methods published in the past 3years, maintaining robust performance even with a limited number of training samples. We also discovered that our ViT and SwinViT achieve significantly higher accuracy and robustness compared to other methods using the same backbone. Our findings confirm that ViTs can be as effective as CNNs for RSI classification, regardless of the dataset size. Our approach exclusively employs opensource ViTs and easily accessible training strategies. Consequently, we believe that our method can significantly lower the barriers for geoscience researchers intending to use ViT for RSI applications.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/e91934d66d9133d854ff0a4cafbe7966584bbf97.pdf",
      "citation_key": "song2024fx9",
      "metadata": {
        "title": "Quantitative regularization in robust vision transformer for remote sensing image classification",
        "authors": [
          "Huaxiang Song",
          "Yuxuan Yuan",
          "Zhiwei Ouyang",
          "Yu Yang",
          "Hui Xiang"
        ],
        "published_date": "2024",
        "abstract": "Vision Transformers (ViTs) are exceptional at vision tasks. However, when applied to remote sensing images (RSIs), existing methods often necessitate extensive modifications of ViTs to rival convolutional neural networks (CNNs). This requirement significantly impedes the application of ViTs in geosciences, particularly for researchers who lack the time for comprehensive model redesign. To address this issue, we introduce the concept of quantitative regularization (QR), designed to enhance the performance of ViTs in RSI classification. QR represents an effective algorithm that adeptly manages domain discrepancies in RSIs and can be integrated with any ViTs in transfer learning. We evaluated the effectiveness of QR using three ViT architectures: vanilla ViT, SwinViT and NextViT, on four datasets: AID30, NWPU45, AFGR50 and UCM21. The results reveal that our NextViT model surpasses 39 other advanced methods published in the past 3years, maintaining robust performance even with a limited number of training samples. We also discovered that our ViT and SwinViT achieve significantly higher accuracy and robustness compared to other methods using the same backbone. Our findings confirm that ViTs can be as effective as CNNs for RSI classification, regardless of the dataset size. Our approach exclusively employs opensource ViTs and easily accessible training strategies. Consequently, we believe that our method can significantly lower the barriers for geoscience researchers intending to use ViT for RSI applications.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e91934d66d9133d854ff0a4cafbe7966584bbf97.pdf",
        "venue": "Photogrammetric Record",
        "citationCount": 28,
        "score": 28.0,
        "summary": "Vision Transformers (ViTs) are exceptional at vision tasks. However, when applied to remote sensing images (RSIs), existing methods often necessitate extensive modifications of ViTs to rival convolutional neural networks (CNNs). This requirement significantly impedes the application of ViTs in geosciences, particularly for researchers who lack the time for comprehensive model redesign. To address this issue, we introduce the concept of quantitative regularization (QR), designed to enhance the performance of ViTs in RSI classification. QR represents an effective algorithm that adeptly manages domain discrepancies in RSIs and can be integrated with any ViTs in transfer learning. We evaluated the effectiveness of QR using three ViT architectures: vanilla ViT, SwinViT and NextViT, on four datasets: AID30, NWPU45, AFGR50 and UCM21. The results reveal that our NextViT model surpasses 39 other advanced methods published in the past 3years, maintaining robust performance even with a limited number of training samples. We also discovered that our ViT and SwinViT achieve significantly higher accuracy and robustness compared to other methods using the same backbone. Our findings confirm that ViTs can be as effective as CNNs for RSI classification, regardless of the dataset size. Our approach exclusively employs opensource ViTs and easily accessible training strategies. Consequently, we believe that our method can significantly lower the barriers for geoscience researchers intending to use ViT for RSI applications.",
        "keywords": []
      },
      "file_name": "e91934d66d9133d854ff0a4cafbe7966584bbf97.pdf"
    },
    {
      "success": true,
      "doc_id": "64591130ec3bb438e06ec7a9a35c9438",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis: Swin Unet3D: a three-dimensional medical image segmentation network combining vision transformer and convolution \\cite{cai2023hji}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of accurate and efficient automatic semantic segmentation of brain tumors in three-dimensional (3D) magnetic resonance imaging (MRI) scans.\n    *   **Importance & Challenge:** Accurate 3D segmentation of brain tumors is critical for clinical treatment, improving preoperative planning and surgical success rates. Existing 3D Convolutional Neural Networks (CNNs) struggle with learning long-distance dependent information due to limited receptive fields (small convolutional kernels). Vision Transformers (ViTs), while good at long-distance dependencies, suffer from a large number of parameters and cannot effectively learn local dependency information in early layers, especially with insufficient data, which is crucial for detailed image segmentation.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds upon the success of U-Net architectures and the recent advancements in Vision Transformers (ViT, Swin Transformer). It positions itself as a hybrid model, combining the strengths of both CNNs and Transformers.\n    *   **Limitations of Previous Solutions:**\n        *   **Pure CNNs (e.g., 3D-Unet, V-Net):** Limited receptive fields, making them inefficient at capturing long-range dependencies essential for accurate tissue structure segmentation.\n        *   **Pure ViTs (e.g., Swin-Unet for 2D):** High computational complexity and a large number of parameters. More critically, they struggle to learn low-level local detail information, particularly in early layers and with limited medical imaging data.\n        *   **Existing Hybrid Models (e.g., TransUnet, TransBTS, UnetR, SwinBTS):** While combining CNN and Transformer, many still suffer from a large number of model parameters and high computational time complexity.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes Swin Unet3D, a 3D medical image segmentation network that represents voxel segmentation as a sequence-to-sequence prediction. Its core innovation lies in a parallel feature extraction sub-module that combines Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) within each layer of the U-Net-like encoder-decoder structure.\n    *   **Novelty/Difference:**\n        *   **Parallel CNN-ViT Structure:** Unlike sequential or backbone-based hybrid models, Swin Unet3D integrates CNN (Conv Block3D) and ViT (Swin Transformer Block3D) in parallel at each stage of the encoder and decoder. This allows the model to simultaneously learn both global (long-distance) and local (short-distance) dependency information throughout the network.\n        *   **3D Swin Transformer Adaptation:** It introduces Swin Transformer Block3D, a 3D adaptation of the Swin Transformer's windowed and shifted-window multi-head self-attention (SW-MSA3D) mechanism, specifically designed for 3D medical images. This module efficiently captures long-range dependencies in 3D.\n        *   **Efficient Local Feature Learning:** The Conv Block3D uses depth-wise separable convolutions to efficiently learn local dependencies and detailed information, mitigating the high computational cost of standard 3D convolutions.\n        *   **Feature Fusion:** The model performs feature fusion of the outputs from the parallel Swin Block3D and Conv Block3D modules, notably using multiplication rather than addition for convergence, inspired by VAN \\cite{cai2023hji}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Development of **Swin Transformer Block3D**, a module capable of extracting 3D features using a 3D windowed and shifted-window multi-head self-attention mechanism.\n        *   Introduction of a **parallel ViT and CNN structure** (Swin Block3D and Conv Block3D) within a U-Net framework, enabling simultaneous learning of long-distance and short-distance dependencies.\n        *   Implementation of **feature fusion via multiplication** between the outputs of the parallel CNN and ViT branches.\n    *   **System Design/Architectural Innovations:** The Swin Unet3D architecture integrates Patch Merging3D and Patch Expanding3D modules for efficient downsampling and upsampling, respectively, maintaining the U-Net's skip connections for information flow.\n    *   **Theoretical Insights/Analysis:** The ablation experiments demonstrate that the ViT and Convolution structures are complementary in intensive prediction tasks like image segmentation, effectively compensating for each other's shortcomings.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The model was evaluated on two public brain tumor segmentation challenge datasets.\n    *   **Key Performance Metrics:** Dice coefficients were used to measure segmentation accuracy for different tumor channels (Enhancing Tumor (ET), Tumor Core (TC), Whole Tumor (WT)).\n    *   **Comparison Results:**\n        *   On the **Brats2021 validation dataset**, Swin Unet3D achieved Dice coefficients of 0.840 (ET), 0.874 (TC), and 0.911 (WT).\n        *   On the **Brats2018 validation dataset**, it achieved Dice coefficients of 0.716 (ET), 0.761 (TC), and 0.874 (WT).\n        *   The paper claims the model achieves a \"good balance in terms of segmentation accuracy and the number of model parameters,\" suggesting competitive performance while managing model complexity.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** While the paper highlights achieving a \"better balance\" between parameters and accuracy, it implicitly suggests that there might still be a trade-off compared to models optimized solely for one aspect. The effectiveness of the parallel structure in mitigating ViT's data insufficiency problem for local dependencies is a key assumption.\n    *   **Scope of Applicability:** The model is specifically designed for voxel segmentation of 3D medical images, with validation focused on brain tumor segmentation in MRI. Its direct applicability to other 3D medical imaging tasks or 2D segmentation would require further evaluation or adaptation.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** Swin Unet3D significantly advances the state-of-the-art in 3D medical image segmentation by effectively addressing the inherent limitations of both pure CNNs (lack of long-range dependency) and pure ViTs (lack of local detail, high parameters) through its novel parallel architecture.\n    *   **Potential Impact on Future Research:** This work provides a strong foundation for future research in hybrid deep learning architectures for medical image analysis. The concept of parallel CNN-ViT feature extraction and efficient 3D Transformer adaptation could inspire new designs for other 3D medical imaging tasks, potentially leading to more robust, accurate, and parameter-efficient models for clinical applications. The emphasis on balancing accuracy and model parameters is crucial for practical deployment.",
      "intriguing_abstract": "Accurate 3D brain tumor segmentation from MRI is paramount for clinical decision-making, yet current deep learning models face a dilemma: Convolutional Neural Networks (CNNs) excel at local features but struggle with long-range dependencies, while Vision Transformers (ViTs) capture global context but often miss fine local details and demand extensive data. We introduce **Swin Unet3D**, a novel 3D medical image segmentation network that ingeniously resolves this paradox. Our architecture integrates parallel 3D Swin Transformer Blocks and efficient 3D Convolutional Blocks within each U-Net stage, enabling simultaneous learning of both global and local dependencies. The Swin Transformer Block3D efficiently captures long-range contextual information via 3D windowed self-attention, while depth-wise separable convolutions ensure robust local feature extraction. A unique multiplicative feature fusion strategy further enhances information flow. Evaluated on challenging Brats datasets, Swin Unet3D achieves superior Dice coefficients (e.g., 0.911 for Whole Tumor on Brats2021) while maintaining a balanced parameter count. This hybrid approach significantly advances 3D semantic segmentation, offering a robust, accurate, and efficient solution critical for improving preoperative planning and surgical outcomes.",
      "keywords": [
        "Swin Unet3D",
        "3D medical image segmentation",
        "brain tumor segmentation",
        "hybrid CNN-Vision Transformer",
        "parallel feature extraction",
        "Swin Transformer Block3D",
        "3D multi-head self-attention",
        "long-range dependencies",
        "local detail learning",
        "feature fusion (multiplication)",
        "Magnetic Resonance Imaging (MRI)",
        "model parameter efficiency",
        "Dice coefficient"
      ],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/e37539f5c943a92ef56b49b7fa067bd976e418d4.pdf",
      "citation_key": "cai2023hji",
      "metadata": {
        "title": "Swin Unet3D: a three-dimensional medical image segmentation network combining vision transformer and convolution",
        "authors": [
          "Yimin Cai",
          "Yuqing Long",
          "Zhenggong Han",
          "Mingkun Liu",
          "Yuchen Zheng",
          "Wei Yang",
          "Liming Chen"
        ],
        "published_date": "2023",
        "abstract": "Background Semantic segmentation of brain tumors plays a critical role in clinical treatment, especially for three-dimensional (3D) magnetic resonance imaging, which is often used in clinical practice. Automatic segmentation of the 3D structure of brain tumors can quickly help physicians understand the properties of tumors, such as the shape and size, thus improving the efficiency of preoperative planning and the odds of successful surgery. In past decades, 3D convolutional neural networks (CNNs) have dominated automatic segmentation methods for 3D medical images, and these network structures have achieved good results. However, to reduce the number of neural network parameters, practitioners ensure that the size of convolutional kernels in 3D convolutional operations generally does not exceed $$7 \\times 7 \\times 7$$ 7  7  7 , which also leads to CNNs showing limitations in learning long-distance dependent information. Vision Transformer (ViT) is very good at learning long-distance dependent information in images, but it suffers from the problems of many parameters. Whats worse, the ViT cannot learn local dependency information in the previous layers under the condition of insufficient data. However, in the image segmentation task, being able to learn this local dependency information in the previous layers makes a big impact on the performance of the model. Methods This paper proposes the Swin Unet3D model, which represents voxel segmentation on medical images as a sequence-to-sequence prediction. The feature extraction sub-module in the model is designed as a parallel structure of Convolution and ViT so that all layers of the model are able to adequately learn both global and local dependency information in the image. Results On the validation dataset of Brats2021, our proposed model achieves dice coefficients of 0.840, 0.874, and 0.911 on the ET channel, TC channel, and WT channel, respectively. On the validation dataset of Brats2018, our model achieves dice coefficients of 0.716, 0.761, and 0.874 on the corresponding channels, respectively. Conclusion We propose a new segmentation model that combines the advantages of Vision Transformer and Convolution and achieves a better balance between the number of model parameters and segmentation accuracy. The code can be found at https://github.com/1152545264/SwinUnet3D .",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e37539f5c943a92ef56b49b7fa067bd976e418d4.pdf",
        "venue": "BMC Medical Informatics and Decision Making",
        "citationCount": 55,
        "score": 27.5,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis: Swin Unet3D: a three-dimensional medical image segmentation network combining vision transformer and convolution \\cite{cai2023hji}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of accurate and efficient automatic semantic segmentation of brain tumors in three-dimensional (3D) magnetic resonance imaging (MRI) scans.\n    *   **Importance & Challenge:** Accurate 3D segmentation of brain tumors is critical for clinical treatment, improving preoperative planning and surgical success rates. Existing 3D Convolutional Neural Networks (CNNs) struggle with learning long-distance dependent information due to limited receptive fields (small convolutional kernels). Vision Transformers (ViTs), while good at long-distance dependencies, suffer from a large number of parameters and cannot effectively learn local dependency information in early layers, especially with insufficient data, which is crucial for detailed image segmentation.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds upon the success of U-Net architectures and the recent advancements in Vision Transformers (ViT, Swin Transformer). It positions itself as a hybrid model, combining the strengths of both CNNs and Transformers.\n    *   **Limitations of Previous Solutions:**\n        *   **Pure CNNs (e.g., 3D-Unet, V-Net):** Limited receptive fields, making them inefficient at capturing long-range dependencies essential for accurate tissue structure segmentation.\n        *   **Pure ViTs (e.g., Swin-Unet for 2D):** High computational complexity and a large number of parameters. More critically, they struggle to learn low-level local detail information, particularly in early layers and with limited medical imaging data.\n        *   **Existing Hybrid Models (e.g., TransUnet, TransBTS, UnetR, SwinBTS):** While combining CNN and Transformer, many still suffer from a large number of model parameters and high computational time complexity.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes Swin Unet3D, a 3D medical image segmentation network that represents voxel segmentation as a sequence-to-sequence prediction. Its core innovation lies in a parallel feature extraction sub-module that combines Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) within each layer of the U-Net-like encoder-decoder structure.\n    *   **Novelty/Difference:**\n        *   **Parallel CNN-ViT Structure:** Unlike sequential or backbone-based hybrid models, Swin Unet3D integrates CNN (Conv Block3D) and ViT (Swin Transformer Block3D) in parallel at each stage of the encoder and decoder. This allows the model to simultaneously learn both global (long-distance) and local (short-distance) dependency information throughout the network.\n        *   **3D Swin Transformer Adaptation:** It introduces Swin Transformer Block3D, a 3D adaptation of the Swin Transformer's windowed and shifted-window multi-head self-attention (SW-MSA3D) mechanism, specifically designed for 3D medical images. This module efficiently captures long-range dependencies in 3D.\n        *   **Efficient Local Feature Learning:** The Conv Block3D uses depth-wise separable convolutions to efficiently learn local dependencies and detailed information, mitigating the high computational cost of standard 3D convolutions.\n        *   **Feature Fusion:** The model performs feature fusion of the outputs from the parallel Swin Block3D and Conv Block3D modules, notably using multiplication rather than addition for convergence, inspired by VAN \\cite{cai2023hji}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Development of **Swin Transformer Block3D**, a module capable of extracting 3D features using a 3D windowed and shifted-window multi-head self-attention mechanism.\n        *   Introduction of a **parallel ViT and CNN structure** (Swin Block3D and Conv Block3D) within a U-Net framework, enabling simultaneous learning of long-distance and short-distance dependencies.\n        *   Implementation of **feature fusion via multiplication** between the outputs of the parallel CNN and ViT branches.\n    *   **System Design/Architectural Innovations:** The Swin Unet3D architecture integrates Patch Merging3D and Patch Expanding3D modules for efficient downsampling and upsampling, respectively, maintaining the U-Net's skip connections for information flow.\n    *   **Theoretical Insights/Analysis:** The ablation experiments demonstrate that the ViT and Convolution structures are complementary in intensive prediction tasks like image segmentation, effectively compensating for each other's shortcomings.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The model was evaluated on two public brain tumor segmentation challenge datasets.\n    *   **Key Performance Metrics:** Dice coefficients were used to measure segmentation accuracy for different tumor channels (Enhancing Tumor (ET), Tumor Core (TC), Whole Tumor (WT)).\n    *   **Comparison Results:**\n        *   On the **Brats2021 validation dataset**, Swin Unet3D achieved Dice coefficients of 0.840 (ET), 0.874 (TC), and 0.911 (WT).\n        *   On the **Brats2018 validation dataset**, it achieved Dice coefficients of 0.716 (ET), 0.761 (TC), and 0.874 (WT).\n        *   The paper claims the model achieves a \"good balance in terms of segmentation accuracy and the number of model parameters,\" suggesting competitive performance while managing model complexity.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** While the paper highlights achieving a \"better balance\" between parameters and accuracy, it implicitly suggests that there might still be a trade-off compared to models optimized solely for one aspect. The effectiveness of the parallel structure in mitigating ViT's data insufficiency problem for local dependencies is a key assumption.\n    *   **Scope of Applicability:** The model is specifically designed for voxel segmentation of 3D medical images, with validation focused on brain tumor segmentation in MRI. Its direct applicability to other 3D medical imaging tasks or 2D segmentation would require further evaluation or adaptation.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** Swin Unet3D significantly advances the state-of-the-art in 3D medical image segmentation by effectively addressing the inherent limitations of both pure CNNs (lack of long-range dependency) and pure ViTs (lack of local detail, high parameters) through its novel parallel architecture.\n    *   **Potential Impact on Future Research:** This work provides a strong foundation for future research in hybrid deep learning architectures for medical image analysis. The concept of parallel CNN-ViT feature extraction and efficient 3D Transformer adaptation could inspire new designs for other 3D medical imaging tasks, potentially leading to more robust, accurate, and parameter-efficient models for clinical applications. The emphasis on balancing accuracy and model parameters is crucial for practical deployment.",
        "keywords": [
          "Swin Unet3D",
          "3D medical image segmentation",
          "brain tumor segmentation",
          "hybrid CNN-Vision Transformer",
          "parallel feature extraction",
          "Swin Transformer Block3D",
          "3D multi-head self-attention",
          "long-range dependencies",
          "local detail learning",
          "feature fusion (multiplication)",
          "Magnetic Resonance Imaging (MRI)",
          "model parameter efficiency",
          "Dice coefficient"
        ],
        "paper_type": "the paper type is **technical**.\n\nhere's why:\n\n*   **title:** \"swin unet3d: a three-dimensional medical image segmentation network combining vision transformer and convolution\" explicitly names a *new network* (swin unet3d) and describes its core components (vision transformer and convolution). this is a strong indicator of a technical paper presenting a new system or method.\n*   **abstract/introduction:**\n    *   it identifies a technical problem: limitations of 3d cnns in learning long-distance dependencies for medical image segmentation.\n    *   it introduces a technical solution/approach: leveraging vision transformers (vit) which are good at learning long-distance dependencies, implying the development of a new method that combines these ideas.\n    *   the language focuses on network structures, parameters, and learning capabilities, which are characteristic of technical discussions about algorithms and systems.\n\nthe content clearly points to the development and presentation of a new method/system for medical image segmentation."
      },
      "file_name": "e37539f5c943a92ef56b49b7fa067bd976e418d4.pdf"
    },
    {
      "success": true,
      "doc_id": "02db52aa67b85af1c57c8f75bfe1b678",
      "summary": "In humancomputer interaction systems, speech emotion recognition (SER) plays a crucial role because it enables computers to understand and react to users emotions. In the past, SER has significantly emphasised acoustic properties extracted from speech signals. The use of visual signals for enhancing SER performance, however, has been made possible by recent developments in deep learning and computer vision. This work utilizes a lightweight Vision Transformer (ViT) model to propose a novel method for improving speech emotion recognition. We leverage the ViT models capabilities to capture spatial dependencies and high-level features in images which are adequate indicators of emotional states from mel spectrogram input fed into the model. To determine the efficiency of our proposed approach, we conduct a comprehensive experiment on two benchmark speech emotion datasets, the Toronto English Speech Set (TESS) and the Berlin Emotional Database (EMODB). The results of our extensive experiment demonstrate a considerable improvement in speech emotion recognition accuracy attesting to its generalizability as it achieved 98%, 91%, and 93% (TESS-EMODB) accuracy respectively on the datasets. The outcomes of the comparative experiment show that the non-overlapping patch-based feature extraction method substantially improves the discipline of speech emotion recognition. Our research indicates the potential for integrating vision transformer models into SER systems, opening up fresh opportunities for real-world applications requiring accurate emotion recognition from speech compared with other state-of-the-art techniques.",
      "intriguing_abstract": "In humancomputer interaction systems, speech emotion recognition (SER) plays a crucial role because it enables computers to understand and react to users emotions. In the past, SER has significantly emphasised acoustic properties extracted from speech signals. The use of visual signals for enhancing SER performance, however, has been made possible by recent developments in deep learning and computer vision. This work utilizes a lightweight Vision Transformer (ViT) model to propose a novel method for improving speech emotion recognition. We leverage the ViT models capabilities to capture spatial dependencies and high-level features in images which are adequate indicators of emotional states from mel spectrogram input fed into the model. To determine the efficiency of our proposed approach, we conduct a comprehensive experiment on two benchmark speech emotion datasets, the Toronto English Speech Set (TESS) and the Berlin Emotional Database (EMODB). The results of our extensive experiment demonstrate a considerable improvement in speech emotion recognition accuracy attesting to its generalizability as it achieved 98%, 91%, and 93% (TESS-EMODB) accuracy respectively on the datasets. The outcomes of the comparative experiment show that the non-overlapping patch-based feature extraction method substantially improves the discipline of speech emotion recognition. Our research indicates the potential for integrating vision transformer models into SER systems, opening up fresh opportunities for real-world applications requiring accurate emotion recognition from speech compared with other state-of-the-art techniques.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/8fc9a7f23c5cf57e17790ffb99b6acb11693c673.pdf",
      "citation_key": "akinpelu2024d4m",
      "metadata": {
        "title": "An enhanced speech emotion recognition using vision transformer",
        "authors": [
          "S. Akinpelu",
          "Serestina Viriri",
          "A. Adegun"
        ],
        "published_date": "2024",
        "abstract": "In humancomputer interaction systems, speech emotion recognition (SER) plays a crucial role because it enables computers to understand and react to users emotions. In the past, SER has significantly emphasised acoustic properties extracted from speech signals. The use of visual signals for enhancing SER performance, however, has been made possible by recent developments in deep learning and computer vision. This work utilizes a lightweight Vision Transformer (ViT) model to propose a novel method for improving speech emotion recognition. We leverage the ViT models capabilities to capture spatial dependencies and high-level features in images which are adequate indicators of emotional states from mel spectrogram input fed into the model. To determine the efficiency of our proposed approach, we conduct a comprehensive experiment on two benchmark speech emotion datasets, the Toronto English Speech Set (TESS) and the Berlin Emotional Database (EMODB). The results of our extensive experiment demonstrate a considerable improvement in speech emotion recognition accuracy attesting to its generalizability as it achieved 98%, 91%, and 93% (TESS-EMODB) accuracy respectively on the datasets. The outcomes of the comparative experiment show that the non-overlapping patch-based feature extraction method substantially improves the discipline of speech emotion recognition. Our research indicates the potential for integrating vision transformer models into SER systems, opening up fresh opportunities for real-world applications requiring accurate emotion recognition from speech compared with other state-of-the-art techniques.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/8fc9a7f23c5cf57e17790ffb99b6acb11693c673.pdf",
        "venue": "Scientific Reports",
        "citationCount": 26,
        "score": 26.0,
        "summary": "In humancomputer interaction systems, speech emotion recognition (SER) plays a crucial role because it enables computers to understand and react to users emotions. In the past, SER has significantly emphasised acoustic properties extracted from speech signals. The use of visual signals for enhancing SER performance, however, has been made possible by recent developments in deep learning and computer vision. This work utilizes a lightweight Vision Transformer (ViT) model to propose a novel method for improving speech emotion recognition. We leverage the ViT models capabilities to capture spatial dependencies and high-level features in images which are adequate indicators of emotional states from mel spectrogram input fed into the model. To determine the efficiency of our proposed approach, we conduct a comprehensive experiment on two benchmark speech emotion datasets, the Toronto English Speech Set (TESS) and the Berlin Emotional Database (EMODB). The results of our extensive experiment demonstrate a considerable improvement in speech emotion recognition accuracy attesting to its generalizability as it achieved 98%, 91%, and 93% (TESS-EMODB) accuracy respectively on the datasets. The outcomes of the comparative experiment show that the non-overlapping patch-based feature extraction method substantially improves the discipline of speech emotion recognition. Our research indicates the potential for integrating vision transformer models into SER systems, opening up fresh opportunities for real-world applications requiring accurate emotion recognition from speech compared with other state-of-the-art techniques.",
        "keywords": []
      },
      "file_name": "8fc9a7f23c5cf57e17790ffb99b6acb11693c673.pdf"
    },
    {
      "success": true,
      "doc_id": "29b8e7534d3763a89e1cf702b22d105f",
      "summary": "Breast cancer remains a leading cause of death among women, highlighting the urgent need for effective detection methods. In recent years, AI-based techniques, including computer vision, machine learning, and deep learning, have gained significant popularity in the field of medical imaging. The healthcare industry has witnessed remarkable progress due to these AI techniques, particularly in the early detection of cancer, which can greatly impact patient outcomes and survival rates. This research introduces a new approach to identifying breast cancer by combining two advanced computer technologies: EfficientNetV2 and vision transformer, using a specific dataset called BreakHis. EfficientNetV2 is praised for its quick processing and efficient use of resources, making it an excellent tool for initially identify important information in the data. We utilize three variants of EfficientNetV2 (small, medium and large) in order to discern the crucial features. Afterwards, this information is processed by a transformer, a type of model excellent at classifying or sorting data, to determine if breast cancer is present. Our experiments show that this method, especially when using the largest version of EfficientNetV2 paired with the vision transformer, is highly effective in accurately identifying breast cancer. It reached an impressive accuracy of nearly 99.83% when deciding between two possible categories and 98.10% when distinguishing among eight categories. These promising results depict that combining these two technologies could be a powerful way to improve breast cancer detection accuracy.",
      "intriguing_abstract": "Breast cancer remains a leading cause of death among women, highlighting the urgent need for effective detection methods. In recent years, AI-based techniques, including computer vision, machine learning, and deep learning, have gained significant popularity in the field of medical imaging. The healthcare industry has witnessed remarkable progress due to these AI techniques, particularly in the early detection of cancer, which can greatly impact patient outcomes and survival rates. This research introduces a new approach to identifying breast cancer by combining two advanced computer technologies: EfficientNetV2 and vision transformer, using a specific dataset called BreakHis. EfficientNetV2 is praised for its quick processing and efficient use of resources, making it an excellent tool for initially identify important information in the data. We utilize three variants of EfficientNetV2 (small, medium and large) in order to discern the crucial features. Afterwards, this information is processed by a transformer, a type of model excellent at classifying or sorting data, to determine if breast cancer is present. Our experiments show that this method, especially when using the largest version of EfficientNetV2 paired with the vision transformer, is highly effective in accurately identifying breast cancer. It reached an impressive accuracy of nearly 99.83% when deciding between two possible categories and 98.10% when distinguishing among eight categories. These promising results depict that combining these two technologies could be a powerful way to improve breast cancer detection accuracy.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/7ddbb9723420cb474c58d4b6b6834d5ebe112049.pdf",
      "citation_key": "hayat2024e4f",
      "metadata": {
        "title": "Hybrid Deep Learning EfficientNetV2 and Vision Transformer (EffNetV2-ViT) Model for Breast Cancer Histopathological Image Classification",
        "authors": [
          "Mansoor Hayat",
          "Nouman Ahmad",
          "Anam Nasir",
          "Zeeshan Ahmad Tariq"
        ],
        "published_date": "2024",
        "abstract": "Breast cancer remains a leading cause of death among women, highlighting the urgent need for effective detection methods. In recent years, AI-based techniques, including computer vision, machine learning, and deep learning, have gained significant popularity in the field of medical imaging. The healthcare industry has witnessed remarkable progress due to these AI techniques, particularly in the early detection of cancer, which can greatly impact patient outcomes and survival rates. This research introduces a new approach to identifying breast cancer by combining two advanced computer technologies: EfficientNetV2 and vision transformer, using a specific dataset called BreakHis. EfficientNetV2 is praised for its quick processing and efficient use of resources, making it an excellent tool for initially identify important information in the data. We utilize three variants of EfficientNetV2 (small, medium and large) in order to discern the crucial features. Afterwards, this information is processed by a transformer, a type of model excellent at classifying or sorting data, to determine if breast cancer is present. Our experiments show that this method, especially when using the largest version of EfficientNetV2 paired with the vision transformer, is highly effective in accurately identifying breast cancer. It reached an impressive accuracy of nearly 99.83% when deciding between two possible categories and 98.10% when distinguishing among eight categories. These promising results depict that combining these two technologies could be a powerful way to improve breast cancer detection accuracy.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7ddbb9723420cb474c58d4b6b6834d5ebe112049.pdf",
        "venue": "IEEE Access",
        "citationCount": 25,
        "score": 25.0,
        "summary": "Breast cancer remains a leading cause of death among women, highlighting the urgent need for effective detection methods. In recent years, AI-based techniques, including computer vision, machine learning, and deep learning, have gained significant popularity in the field of medical imaging. The healthcare industry has witnessed remarkable progress due to these AI techniques, particularly in the early detection of cancer, which can greatly impact patient outcomes and survival rates. This research introduces a new approach to identifying breast cancer by combining two advanced computer technologies: EfficientNetV2 and vision transformer, using a specific dataset called BreakHis. EfficientNetV2 is praised for its quick processing and efficient use of resources, making it an excellent tool for initially identify important information in the data. We utilize three variants of EfficientNetV2 (small, medium and large) in order to discern the crucial features. Afterwards, this information is processed by a transformer, a type of model excellent at classifying or sorting data, to determine if breast cancer is present. Our experiments show that this method, especially when using the largest version of EfficientNetV2 paired with the vision transformer, is highly effective in accurately identifying breast cancer. It reached an impressive accuracy of nearly 99.83% when deciding between two possible categories and 98.10% when distinguishing among eight categories. These promising results depict that combining these two technologies could be a powerful way to improve breast cancer detection accuracy.",
        "keywords": []
      },
      "file_name": "7ddbb9723420cb474c58d4b6b6834d5ebe112049.pdf"
    },
    {
      "success": true,
      "doc_id": "30d9b856f06ff33a128909ea248515e6",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/c7c0c84fa505995cd888a8fbaa9def21cb84cca3.pdf",
      "citation_key": "li2024g3z",
      "metadata": {
        "title": "Learning Adaptive and View-Invariant Vision Transformer for Real-Time UAV Tracking",
        "authors": [
          "Yongxin Li",
          "Mengyuan Liu",
          "You Wu",
          "Xucheng Wang",
          "Xiangyang Yang",
          "Shuiwang Li"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c7c0c84fa505995cd888a8fbaa9def21cb84cca3.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 25,
        "score": 25.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "c7c0c84fa505995cd888a8fbaa9def21cb84cca3.pdf"
    },
    {
      "success": true,
      "doc_id": "b4932c564ad423f6809da3d013b8624d",
      "summary": "Skin cancer, particularly melanoma, has been recognized as one of the most lethal forms of cancer. Detecting and diagnosing skin lesions accurately can be challenging due to the striking similarities between the various types of skin lesions, such as melanoma and nevi, especially when examining the color images of the skin. However, early diagnosis plays a crucial role in saving lives and reducing the burden on medical resources. Consequently, the development of a robust autonomous system for skin cancer classification becomes imperative. Convolutional neural networks (CNNs) have been widely employed over the past decade to automate cancer diagnosis. Nonetheless, the emergence of the Vision Transformer (ViT) has recently gained a considerable level of popularity in the field and has emerged as a competitive alternative to CNNs. In light of this, the present study proposed an alternative method based on the off-the-shelf ViT for identifying various skin cancer diseases. To evaluate its performance, the proposed method was compared with 11 CNN-based transfer learning methods that have been known to outperform other deep learning techniques that are currently in use. Furthermore, this study addresses the issue of class imbalance within the dataset, a common challenge in skin cancer classification. In addressing this concern, the proposed study leverages the vision transformer and the CNN-based transfer learning models to classify seven distinct types of skin cancers. Through our investigation, we have found that the employment of pre-trained vision transformers achieved an impressive accuracy of 92.14%, surpassing CNN-based transfer learning models across several evaluation metrics for skin cancer diagnosis.",
      "intriguing_abstract": "Skin cancer, particularly melanoma, has been recognized as one of the most lethal forms of cancer. Detecting and diagnosing skin lesions accurately can be challenging due to the striking similarities between the various types of skin lesions, such as melanoma and nevi, especially when examining the color images of the skin. However, early diagnosis plays a crucial role in saving lives and reducing the burden on medical resources. Consequently, the development of a robust autonomous system for skin cancer classification becomes imperative. Convolutional neural networks (CNNs) have been widely employed over the past decade to automate cancer diagnosis. Nonetheless, the emergence of the Vision Transformer (ViT) has recently gained a considerable level of popularity in the field and has emerged as a competitive alternative to CNNs. In light of this, the present study proposed an alternative method based on the off-the-shelf ViT for identifying various skin cancer diseases. To evaluate its performance, the proposed method was compared with 11 CNN-based transfer learning methods that have been known to outperform other deep learning techniques that are currently in use. Furthermore, this study addresses the issue of class imbalance within the dataset, a common challenge in skin cancer classification. In addressing this concern, the proposed study leverages the vision transformer and the CNN-based transfer learning models to classify seven distinct types of skin cancers. Through our investigation, we have found that the employment of pre-trained vision transformers achieved an impressive accuracy of 92.14%, surpassing CNN-based transfer learning models across several evaluation metrics for skin cancer diagnosis.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/1af8c50b4c7329fb2273aa7b755b0f9e8223e2d2.pdf",
      "citation_key": "arshed2023zen",
      "metadata": {
        "title": "Multi-Class Skin Cancer Classification Using Vision Transformer Networks and Convolutional Neural Network-Based Pre-Trained Models",
        "authors": [
          "Muhammad Asad Arshed",
          "Shahzad Mumtaz",
          "Muhammad Ibrahim",
          "Saeed Ahmed",
          "Muhammad Tahir",
          "Muhammad Shafi"
        ],
        "published_date": "2023",
        "abstract": "Skin cancer, particularly melanoma, has been recognized as one of the most lethal forms of cancer. Detecting and diagnosing skin lesions accurately can be challenging due to the striking similarities between the various types of skin lesions, such as melanoma and nevi, especially when examining the color images of the skin. However, early diagnosis plays a crucial role in saving lives and reducing the burden on medical resources. Consequently, the development of a robust autonomous system for skin cancer classification becomes imperative. Convolutional neural networks (CNNs) have been widely employed over the past decade to automate cancer diagnosis. Nonetheless, the emergence of the Vision Transformer (ViT) has recently gained a considerable level of popularity in the field and has emerged as a competitive alternative to CNNs. In light of this, the present study proposed an alternative method based on the off-the-shelf ViT for identifying various skin cancer diseases. To evaluate its performance, the proposed method was compared with 11 CNN-based transfer learning methods that have been known to outperform other deep learning techniques that are currently in use. Furthermore, this study addresses the issue of class imbalance within the dataset, a common challenge in skin cancer classification. In addressing this concern, the proposed study leverages the vision transformer and the CNN-based transfer learning models to classify seven distinct types of skin cancers. Through our investigation, we have found that the employment of pre-trained vision transformers achieved an impressive accuracy of 92.14%, surpassing CNN-based transfer learning models across several evaluation metrics for skin cancer diagnosis.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1af8c50b4c7329fb2273aa7b755b0f9e8223e2d2.pdf",
        "venue": "Inf.",
        "citationCount": 50,
        "score": 25.0,
        "summary": "Skin cancer, particularly melanoma, has been recognized as one of the most lethal forms of cancer. Detecting and diagnosing skin lesions accurately can be challenging due to the striking similarities between the various types of skin lesions, such as melanoma and nevi, especially when examining the color images of the skin. However, early diagnosis plays a crucial role in saving lives and reducing the burden on medical resources. Consequently, the development of a robust autonomous system for skin cancer classification becomes imperative. Convolutional neural networks (CNNs) have been widely employed over the past decade to automate cancer diagnosis. Nonetheless, the emergence of the Vision Transformer (ViT) has recently gained a considerable level of popularity in the field and has emerged as a competitive alternative to CNNs. In light of this, the present study proposed an alternative method based on the off-the-shelf ViT for identifying various skin cancer diseases. To evaluate its performance, the proposed method was compared with 11 CNN-based transfer learning methods that have been known to outperform other deep learning techniques that are currently in use. Furthermore, this study addresses the issue of class imbalance within the dataset, a common challenge in skin cancer classification. In addressing this concern, the proposed study leverages the vision transformer and the CNN-based transfer learning models to classify seven distinct types of skin cancers. Through our investigation, we have found that the employment of pre-trained vision transformers achieved an impressive accuracy of 92.14%, surpassing CNN-based transfer learning models across several evaluation metrics for skin cancer diagnosis.",
        "keywords": []
      },
      "file_name": "1af8c50b4c7329fb2273aa7b755b0f9e8223e2d2.pdf"
    },
    {
      "success": true,
      "doc_id": "0e6444573aceda7af6ff4d66e7fd6c18",
      "summary": "Existing tunnel detection methods include crack and waterleakage segmentation networks. However, if the automated detection algorithm cannot process all defect cases, manual detection is required to eliminate potential risks. The existing intelligent detection methods lack a universal method that can accurately segment all types of defects, particularly when multiple defects are superimposed. To address this issue, a defect segmentation model is proposed based on Vision Transformer (ViT), which is completely different from the network structure of a convolutional neural network. The model proposes an adapter and a decoding head to improve the training effect of the transformer encoder, allowing it to be fitted to smallscale datasets. In postprocessing, a method is proposed to quantify the threat level for the defects, with the aim of outputting qualitative results that simulate human observation. The model showed impressive results on a realworld dataset containing 11,781 defect images collected from a real subway tunnel. The visualizing results proved that this method is effective and has uniform criteria for single, multiple, and comprehensive defects. Moreover, the tests proved that the proposed model has a significant advantage in the case of multipledefect superposition, and it achieved 93.77%, 88.36%, and 92.93% for mean accuracy (Acc), mean intersection over union, and mean F1score, respectively. With similar training parameters, the Acc of the proposed method is improved by more than 10% over the DeepLabv3+, Mask Rconvolutional neural network, and UPerNetR50 models and by more than 5% over the Swin Transformer and ViTAdapter. This study implemented a general method that can process all defect cases and output the threat evaluation results, thereby making more intelligent tunnel detection.",
      "intriguing_abstract": "Existing tunnel detection methods include crack and waterleakage segmentation networks. However, if the automated detection algorithm cannot process all defect cases, manual detection is required to eliminate potential risks. The existing intelligent detection methods lack a universal method that can accurately segment all types of defects, particularly when multiple defects are superimposed. To address this issue, a defect segmentation model is proposed based on Vision Transformer (ViT), which is completely different from the network structure of a convolutional neural network. The model proposes an adapter and a decoding head to improve the training effect of the transformer encoder, allowing it to be fitted to smallscale datasets. In postprocessing, a method is proposed to quantify the threat level for the defects, with the aim of outputting qualitative results that simulate human observation. The model showed impressive results on a realworld dataset containing 11,781 defect images collected from a real subway tunnel. The visualizing results proved that this method is effective and has uniform criteria for single, multiple, and comprehensive defects. Moreover, the tests proved that the proposed model has a significant advantage in the case of multipledefect superposition, and it achieved 93.77%, 88.36%, and 92.93% for mean accuracy (Acc), mean intersection over union, and mean F1score, respectively. With similar training parameters, the Acc of the proposed method is improved by more than 10% over the DeepLabv3+, Mask Rconvolutional neural network, and UPerNetR50 models and by more than 5% over the Swin Transformer and ViTAdapter. This study implemented a general method that can process all defect cases and output the threat evaluation results, thereby making more intelligent tunnel detection.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/bca0adcba0c26c5a34b05795c5bfbc1948bf2fae.pdf",
      "citation_key": "qin20242eu",
      "metadata": {
        "title": "Image segmentation using Vision Transformer for tunnel defect assessment",
        "authors": [
          "S. Qin",
          "Taiyue Qi",
          "Tang Deng",
          "Xiaodong Huang"
        ],
        "published_date": "2024",
        "abstract": "Existing tunnel detection methods include crack and waterleakage segmentation networks. However, if the automated detection algorithm cannot process all defect cases, manual detection is required to eliminate potential risks. The existing intelligent detection methods lack a universal method that can accurately segment all types of defects, particularly when multiple defects are superimposed. To address this issue, a defect segmentation model is proposed based on Vision Transformer (ViT), which is completely different from the network structure of a convolutional neural network. The model proposes an adapter and a decoding head to improve the training effect of the transformer encoder, allowing it to be fitted to smallscale datasets. In postprocessing, a method is proposed to quantify the threat level for the defects, with the aim of outputting qualitative results that simulate human observation. The model showed impressive results on a realworld dataset containing 11,781 defect images collected from a real subway tunnel. The visualizing results proved that this method is effective and has uniform criteria for single, multiple, and comprehensive defects. Moreover, the tests proved that the proposed model has a significant advantage in the case of multipledefect superposition, and it achieved 93.77%, 88.36%, and 92.93% for mean accuracy (Acc), mean intersection over union, and mean F1score, respectively. With similar training parameters, the Acc of the proposed method is improved by more than 10% over the DeepLabv3+, Mask Rconvolutional neural network, and UPerNetR50 models and by more than 5% over the Swin Transformer and ViTAdapter. This study implemented a general method that can process all defect cases and output the threat evaluation results, thereby making more intelligent tunnel detection.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/bca0adcba0c26c5a34b05795c5bfbc1948bf2fae.pdf",
        "venue": "Comput. Aided Civ. Infrastructure Eng.",
        "citationCount": 25,
        "score": 25.0,
        "summary": "Existing tunnel detection methods include crack and waterleakage segmentation networks. However, if the automated detection algorithm cannot process all defect cases, manual detection is required to eliminate potential risks. The existing intelligent detection methods lack a universal method that can accurately segment all types of defects, particularly when multiple defects are superimposed. To address this issue, a defect segmentation model is proposed based on Vision Transformer (ViT), which is completely different from the network structure of a convolutional neural network. The model proposes an adapter and a decoding head to improve the training effect of the transformer encoder, allowing it to be fitted to smallscale datasets. In postprocessing, a method is proposed to quantify the threat level for the defects, with the aim of outputting qualitative results that simulate human observation. The model showed impressive results on a realworld dataset containing 11,781 defect images collected from a real subway tunnel. The visualizing results proved that this method is effective and has uniform criteria for single, multiple, and comprehensive defects. Moreover, the tests proved that the proposed model has a significant advantage in the case of multipledefect superposition, and it achieved 93.77%, 88.36%, and 92.93% for mean accuracy (Acc), mean intersection over union, and mean F1score, respectively. With similar training parameters, the Acc of the proposed method is improved by more than 10% over the DeepLabv3+, Mask Rconvolutional neural network, and UPerNetR50 models and by more than 5% over the Swin Transformer and ViTAdapter. This study implemented a general method that can process all defect cases and output the threat evaluation results, thereby making more intelligent tunnel detection.",
        "keywords": []
      },
      "file_name": "bca0adcba0c26c5a34b05795c5bfbc1948bf2fae.pdf"
    },
    {
      "success": true,
      "doc_id": "5b19b5bd5eaf3d709f981749e4908152",
      "summary": "Plant leaf classification involves identifying and categorizing plant species based on leaf characteristics, such as patterns, shapes, textures, and veins. In recent years, research has been conducted to improve the accuracy of plant classification using machine learning techniques. This involves training models on large datasets of plant images and using them to identify different plant species. However, these models are limited by their reliance on large amounts of training data, which can be difficult to obtain for many plant species. To overcome this challenge, this paper proposes a Plant-CNN-ViT ensemble model that combines the strengths of four pre-trained models: Vision Transformer, ResNet-50, DenseNet-201, and Xception. Vision Transformer utilizes self-attention to capture dependencies and focus on important leaf features. ResNet-50 introduces residual connections, aiding in efficient training and hierarchical feature extraction. DenseNet-201 employs dense connections, facilitating information flow and capturing intricate leaf patterns. Xception uses separable convolutions, reducing the computational cost while capturing fine-grained details in leaf images. The proposed Plant-CNN-ViT was evaluated on four plant leaf datasets and achieved remarkable accuracy of 100.00%, 100.00%, 100.00%, and 99.83% on the Flavia dataset, Folio Leaf dataset, Swedish Leaf dataset, and MalayaKew Leaf dataset, respectively.",
      "intriguing_abstract": "Plant leaf classification involves identifying and categorizing plant species based on leaf characteristics, such as patterns, shapes, textures, and veins. In recent years, research has been conducted to improve the accuracy of plant classification using machine learning techniques. This involves training models on large datasets of plant images and using them to identify different plant species. However, these models are limited by their reliance on large amounts of training data, which can be difficult to obtain for many plant species. To overcome this challenge, this paper proposes a Plant-CNN-ViT ensemble model that combines the strengths of four pre-trained models: Vision Transformer, ResNet-50, DenseNet-201, and Xception. Vision Transformer utilizes self-attention to capture dependencies and focus on important leaf features. ResNet-50 introduces residual connections, aiding in efficient training and hierarchical feature extraction. DenseNet-201 employs dense connections, facilitating information flow and capturing intricate leaf patterns. Xception uses separable convolutions, reducing the computational cost while capturing fine-grained details in leaf images. The proposed Plant-CNN-ViT was evaluated on four plant leaf datasets and achieved remarkable accuracy of 100.00%, 100.00%, 100.00%, and 99.83% on the Flavia dataset, Folio Leaf dataset, Swedish Leaf dataset, and MalayaKew Leaf dataset, respectively.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/c4895869637f73154d608cdd817234b0dbcd3508.pdf",
      "citation_key": "lee2023iwc",
      "metadata": {
        "title": "Plant-CNN-ViT: Plant Classification with Ensemble of Convolutional Neural Networks and Vision Transformer",
        "authors": [
          "C. Lee",
          "K. Lim",
          "Yu Xuan Song",
          "Ali Alqahtani"
        ],
        "published_date": "2023",
        "abstract": "Plant leaf classification involves identifying and categorizing plant species based on leaf characteristics, such as patterns, shapes, textures, and veins. In recent years, research has been conducted to improve the accuracy of plant classification using machine learning techniques. This involves training models on large datasets of plant images and using them to identify different plant species. However, these models are limited by their reliance on large amounts of training data, which can be difficult to obtain for many plant species. To overcome this challenge, this paper proposes a Plant-CNN-ViT ensemble model that combines the strengths of four pre-trained models: Vision Transformer, ResNet-50, DenseNet-201, and Xception. Vision Transformer utilizes self-attention to capture dependencies and focus on important leaf features. ResNet-50 introduces residual connections, aiding in efficient training and hierarchical feature extraction. DenseNet-201 employs dense connections, facilitating information flow and capturing intricate leaf patterns. Xception uses separable convolutions, reducing the computational cost while capturing fine-grained details in leaf images. The proposed Plant-CNN-ViT was evaluated on four plant leaf datasets and achieved remarkable accuracy of 100.00%, 100.00%, 100.00%, and 99.83% on the Flavia dataset, Folio Leaf dataset, Swedish Leaf dataset, and MalayaKew Leaf dataset, respectively.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c4895869637f73154d608cdd817234b0dbcd3508.pdf",
        "venue": "Plants",
        "citationCount": 48,
        "score": 24.0,
        "summary": "Plant leaf classification involves identifying and categorizing plant species based on leaf characteristics, such as patterns, shapes, textures, and veins. In recent years, research has been conducted to improve the accuracy of plant classification using machine learning techniques. This involves training models on large datasets of plant images and using them to identify different plant species. However, these models are limited by their reliance on large amounts of training data, which can be difficult to obtain for many plant species. To overcome this challenge, this paper proposes a Plant-CNN-ViT ensemble model that combines the strengths of four pre-trained models: Vision Transformer, ResNet-50, DenseNet-201, and Xception. Vision Transformer utilizes self-attention to capture dependencies and focus on important leaf features. ResNet-50 introduces residual connections, aiding in efficient training and hierarchical feature extraction. DenseNet-201 employs dense connections, facilitating information flow and capturing intricate leaf patterns. Xception uses separable convolutions, reducing the computational cost while capturing fine-grained details in leaf images. The proposed Plant-CNN-ViT was evaluated on four plant leaf datasets and achieved remarkable accuracy of 100.00%, 100.00%, 100.00%, and 99.83% on the Flavia dataset, Folio Leaf dataset, Swedish Leaf dataset, and MalayaKew Leaf dataset, respectively.",
        "keywords": []
      },
      "file_name": "c4895869637f73154d608cdd817234b0dbcd3508.pdf"
    },
    {
      "success": true,
      "doc_id": "8d89f8591ec88aeae710048d18aa2542",
      "summary": "Nowadays, inspired by the great success of Transformers in Natural Language Processing, many applications of Vision Transformers (ViTs) have been investigated in the field of medical image analysis including breast ultrasound (BUS) image segmentation and classification. In this paper, we propose an efficient multi-task framework to segment and classify tumors in BUS images using hybrid convolutional neural networks (CNNs)-ViTs architecture and Multi-Perceptron (MLP)-Mixer. The proposed method uses a two-encoder architecture with EfficientNetV2 backbone and an adapted ViT encoder to extract tumor regions in BUS images. The self-attention (SA) mechanism in the Transformer encoder allows capturing a wide range of high-level and complex features while the EfficientNetV2 encoder preserves local information in image. To fusion the extracted features, a Channel Attention Fusion (CAF) module is introduced. The CAF module selectively emphasizes important features from both encoders, improving the integration of high-level and local information. The resulting feature maps are reconstructed to obtain the segmentation maps using a decoder. Then, our method classifies the segmented tumor regions into benign and malignant using a simple and efficient classifier based on MLP-Mixer, that is applied for the first time, to the best of our knowledge, for the task of lesion classification in BUS images. Experimental results illustrate the outperformance of our framework compared to recent works for the task of segmentation by producing 83.42% in terms of Dice coefficient as well as for the classification with 86% in terms of accuracy.",
      "intriguing_abstract": "Nowadays, inspired by the great success of Transformers in Natural Language Processing, many applications of Vision Transformers (ViTs) have been investigated in the field of medical image analysis including breast ultrasound (BUS) image segmentation and classification. In this paper, we propose an efficient multi-task framework to segment and classify tumors in BUS images using hybrid convolutional neural networks (CNNs)-ViTs architecture and Multi-Perceptron (MLP)-Mixer. The proposed method uses a two-encoder architecture with EfficientNetV2 backbone and an adapted ViT encoder to extract tumor regions in BUS images. The self-attention (SA) mechanism in the Transformer encoder allows capturing a wide range of high-level and complex features while the EfficientNetV2 encoder preserves local information in image. To fusion the extracted features, a Channel Attention Fusion (CAF) module is introduced. The CAF module selectively emphasizes important features from both encoders, improving the integration of high-level and local information. The resulting feature maps are reconstructed to obtain the segmentation maps using a decoder. Then, our method classifies the segmented tumor regions into benign and malignant using a simple and efficient classifier based on MLP-Mixer, that is applied for the first time, to the best of our knowledge, for the task of lesion classification in BUS images. Experimental results illustrate the outperformance of our framework compared to recent works for the task of segmentation by producing 83.42% in terms of Dice coefficient as well as for the classification with 86% in terms of accuracy.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/64811427a4427588bb049a6a254446ddd2cafacc.pdf",
      "citation_key": "tagnamas20246ug",
      "metadata": {
        "title": "Multi-task approach based on combined CNN-transformer for efficient segmentation and classification of breast tumors in ultrasound images",
        "authors": [
          "Jaouad Tagnamas",
          "Hiba Ramadan",
          "Ali Yahyaouy",
          "H. Tairi"
        ],
        "published_date": "2024",
        "abstract": "Nowadays, inspired by the great success of Transformers in Natural Language Processing, many applications of Vision Transformers (ViTs) have been investigated in the field of medical image analysis including breast ultrasound (BUS) image segmentation and classification. In this paper, we propose an efficient multi-task framework to segment and classify tumors in BUS images using hybrid convolutional neural networks (CNNs)-ViTs architecture and Multi-Perceptron (MLP)-Mixer. The proposed method uses a two-encoder architecture with EfficientNetV2 backbone and an adapted ViT encoder to extract tumor regions in BUS images. The self-attention (SA) mechanism in the Transformer encoder allows capturing a wide range of high-level and complex features while the EfficientNetV2 encoder preserves local information in image. To fusion the extracted features, a Channel Attention Fusion (CAF) module is introduced. The CAF module selectively emphasizes important features from both encoders, improving the integration of high-level and local information. The resulting feature maps are reconstructed to obtain the segmentation maps using a decoder. Then, our method classifies the segmented tumor regions into benign and malignant using a simple and efficient classifier based on MLP-Mixer, that is applied for the first time, to the best of our knowledge, for the task of lesion classification in BUS images. Experimental results illustrate the outperformance of our framework compared to recent works for the task of segmentation by producing 83.42% in terms of Dice coefficient as well as for the classification with 86% in terms of accuracy.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/64811427a4427588bb049a6a254446ddd2cafacc.pdf",
        "venue": "Visual Computing for Industry, Biomedicine, and Art",
        "citationCount": 21,
        "score": 21.0,
        "summary": "Nowadays, inspired by the great success of Transformers in Natural Language Processing, many applications of Vision Transformers (ViTs) have been investigated in the field of medical image analysis including breast ultrasound (BUS) image segmentation and classification. In this paper, we propose an efficient multi-task framework to segment and classify tumors in BUS images using hybrid convolutional neural networks (CNNs)-ViTs architecture and Multi-Perceptron (MLP)-Mixer. The proposed method uses a two-encoder architecture with EfficientNetV2 backbone and an adapted ViT encoder to extract tumor regions in BUS images. The self-attention (SA) mechanism in the Transformer encoder allows capturing a wide range of high-level and complex features while the EfficientNetV2 encoder preserves local information in image. To fusion the extracted features, a Channel Attention Fusion (CAF) module is introduced. The CAF module selectively emphasizes important features from both encoders, improving the integration of high-level and local information. The resulting feature maps are reconstructed to obtain the segmentation maps using a decoder. Then, our method classifies the segmented tumor regions into benign and malignant using a simple and efficient classifier based on MLP-Mixer, that is applied for the first time, to the best of our knowledge, for the task of lesion classification in BUS images. Experimental results illustrate the outperformance of our framework compared to recent works for the task of segmentation by producing 83.42% in terms of Dice coefficient as well as for the classification with 86% in terms of accuracy.",
        "keywords": []
      },
      "file_name": "64811427a4427588bb049a6a254446ddd2cafacc.pdf"
    },
    {
      "success": true,
      "doc_id": "8d35e2f3adf990f80dfd14f1c3305d09",
      "summary": "While discriminative correlation filters (DCF)-based trackers prevail in UAV tracking for their favorable efficiency, lightweight convolutional neural network (CNN)-based trackers using filter pruning have also demonstrated remarkable efficiency and precision. However, the use of pure vision transformer models (ViTs) for UAV tracking remains unexplored, which is a surprising finding given that ViTs have been shown to produce better performance and greater efficiency than CNNs in image classification. In this paper, we propose an efficient ViT-based tracking framework, Aba-ViTrack, for UAV tracking. In our framework, feature learning and template-search coupling are integrated into an efficient one-stream ViT to avoid an extra heavy relation modeling module. The proposed Aba-ViT exploits an adaptive and background-aware token computation method to reduce inference time. This approach adaptively discards tokens based on learned halting probabilities, which a priori are higher for background tokens than target ones. Extensive experiments on six UAV tracking benchmarks demonstrate that the proposed Aba-ViTrack achieves state-of-the-art performance in UAV tracking. Code is available at https://github.com/xyyang317/Aba-ViTrack.",
      "intriguing_abstract": "While discriminative correlation filters (DCF)-based trackers prevail in UAV tracking for their favorable efficiency, lightweight convolutional neural network (CNN)-based trackers using filter pruning have also demonstrated remarkable efficiency and precision. However, the use of pure vision transformer models (ViTs) for UAV tracking remains unexplored, which is a surprising finding given that ViTs have been shown to produce better performance and greater efficiency than CNNs in image classification. In this paper, we propose an efficient ViT-based tracking framework, Aba-ViTrack, for UAV tracking. In our framework, feature learning and template-search coupling are integrated into an efficient one-stream ViT to avoid an extra heavy relation modeling module. The proposed Aba-ViT exploits an adaptive and background-aware token computation method to reduce inference time. This approach adaptively discards tokens based on learned halting probabilities, which a priori are higher for background tokens than target ones. Extensive experiments on six UAV tracking benchmarks demonstrate that the proposed Aba-ViTrack achieves state-of-the-art performance in UAV tracking. Code is available at https://github.com/xyyang317/Aba-ViTrack.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/7a0f92968c63613730b0c06b3bc3bd40d2666571.pdf",
      "citation_key": "li2023jft",
      "metadata": {
        "title": "Adaptive and Background-Aware Vision Transformer for Real-Time UAV Tracking",
        "authors": [
          "Shuiwang Li",
          "Yangxiang Yang",
          "Dan Zeng",
          "Xucheng Wang"
        ],
        "published_date": "2023",
        "abstract": "While discriminative correlation filters (DCF)-based trackers prevail in UAV tracking for their favorable efficiency, lightweight convolutional neural network (CNN)-based trackers using filter pruning have also demonstrated remarkable efficiency and precision. However, the use of pure vision transformer models (ViTs) for UAV tracking remains unexplored, which is a surprising finding given that ViTs have been shown to produce better performance and greater efficiency than CNNs in image classification. In this paper, we propose an efficient ViT-based tracking framework, Aba-ViTrack, for UAV tracking. In our framework, feature learning and template-search coupling are integrated into an efficient one-stream ViT to avoid an extra heavy relation modeling module. The proposed Aba-ViT exploits an adaptive and background-aware token computation method to reduce inference time. This approach adaptively discards tokens based on learned halting probabilities, which a priori are higher for background tokens than target ones. Extensive experiments on six UAV tracking benchmarks demonstrate that the proposed Aba-ViTrack achieves state-of-the-art performance in UAV tracking. Code is available at https://github.com/xyyang317/Aba-ViTrack.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7a0f92968c63613730b0c06b3bc3bd40d2666571.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 39,
        "score": 19.5,
        "summary": "While discriminative correlation filters (DCF)-based trackers prevail in UAV tracking for their favorable efficiency, lightweight convolutional neural network (CNN)-based trackers using filter pruning have also demonstrated remarkable efficiency and precision. However, the use of pure vision transformer models (ViTs) for UAV tracking remains unexplored, which is a surprising finding given that ViTs have been shown to produce better performance and greater efficiency than CNNs in image classification. In this paper, we propose an efficient ViT-based tracking framework, Aba-ViTrack, for UAV tracking. In our framework, feature learning and template-search coupling are integrated into an efficient one-stream ViT to avoid an extra heavy relation modeling module. The proposed Aba-ViT exploits an adaptive and background-aware token computation method to reduce inference time. This approach adaptively discards tokens based on learned halting probabilities, which a priori are higher for background tokens than target ones. Extensive experiments on six UAV tracking benchmarks demonstrate that the proposed Aba-ViTrack achieves state-of-the-art performance in UAV tracking. Code is available at https://github.com/xyyang317/Aba-ViTrack.",
        "keywords": []
      },
      "file_name": "7a0f92968c63613730b0c06b3bc3bd40d2666571.pdf"
    },
    {
      "success": true,
      "doc_id": "a7382d96200f0ad7900467016d73e4a5",
      "summary": "Simple Summary Transformer models, originally successful in natural language processing, have found application in computer vision, demonstrating promising results in tasks related to cancer image analysis. Despite being one of the prevalent and swiftly spreading cancers globally, there is a pressing need for accurate automated analysis methods for oral cancer. This need is particularly critical for high-risk populations residing in low- and middle-income countries. In this study, we evaluated the performance of the Vision Transformer (ViT) and the Swin Transformer in the classification of mobile-based oral cancer images we collected from high-risk populations. The results showed that the Swin Transformer model achieved higher accuracy than the ViT model, and both transformer models work better than the conventional convolution model VGG19. Abstract Oral cancer, a pervasive and rapidly growing malignant disease, poses a significant global health concern. Early and accurate diagnosis is pivotal for improving patient outcomes. Automatic diagnosis methods based on artificial intelligence have shown promising results in the oral cancer field, but the accuracy still needs to be improved for realistic diagnostic scenarios. Vision Transformers (ViT) have outperformed learning CNN models recently in many computer vision benchmark tasks. This study explores the effectiveness of the Vision Transformer and the Swin Transformer, two cutting-edge variants of the transformer architecture, for the mobile-based oral cancer image classification application. The pre-trained Swin transformer model achieved 88.7% accuracy in the binary classification task, outperforming the ViT model by 2.3%, while the conventional convolutional network model VGG19 and ResNet50 achieved 85.2% and 84.5% accuracy. Our experiments demonstrate that these transformer-based architectures outperform traditional convolutional neural networks in terms of oral cancer image classification, and underscore the potential of the ViT and the Swin Transformer in advancing the state of the art in oral cancer image analysis.",
      "intriguing_abstract": "Simple Summary Transformer models, originally successful in natural language processing, have found application in computer vision, demonstrating promising results in tasks related to cancer image analysis. Despite being one of the prevalent and swiftly spreading cancers globally, there is a pressing need for accurate automated analysis methods for oral cancer. This need is particularly critical for high-risk populations residing in low- and middle-income countries. In this study, we evaluated the performance of the Vision Transformer (ViT) and the Swin Transformer in the classification of mobile-based oral cancer images we collected from high-risk populations. The results showed that the Swin Transformer model achieved higher accuracy than the ViT model, and both transformer models work better than the conventional convolution model VGG19. Abstract Oral cancer, a pervasive and rapidly growing malignant disease, poses a significant global health concern. Early and accurate diagnosis is pivotal for improving patient outcomes. Automatic diagnosis methods based on artificial intelligence have shown promising results in the oral cancer field, but the accuracy still needs to be improved for realistic diagnostic scenarios. Vision Transformers (ViT) have outperformed learning CNN models recently in many computer vision benchmark tasks. This study explores the effectiveness of the Vision Transformer and the Swin Transformer, two cutting-edge variants of the transformer architecture, for the mobile-based oral cancer image classification application. The pre-trained Swin transformer model achieved 88.7% accuracy in the binary classification task, outperforming the ViT model by 2.3%, while the conventional convolutional network model VGG19 and ResNet50 achieved 85.2% and 84.5% accuracy. Our experiments demonstrate that these transformer-based architectures outperform traditional convolutional neural networks in terms of oral cancer image classification, and underscore the potential of the ViT and the Swin Transformer in advancing the state of the art in oral cancer image analysis.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/136b0ef7bd7f642a285b1d61ddf7abfc7ddc026f.pdf",
      "citation_key": "song2024c99",
      "metadata": {
        "title": "Classification of Mobile-Based Oral Cancer Images Using the Vision Transformer and the Swin Transformer",
        "authors": [
          "Bofan Song",
          "D. Kc",
          "Rubin Yuchan Yang",
          "Shaobai Li",
          "Chicheng Zhang",
          "Rongguang Liang"
        ],
        "published_date": "2024",
        "abstract": "Simple Summary Transformer models, originally successful in natural language processing, have found application in computer vision, demonstrating promising results in tasks related to cancer image analysis. Despite being one of the prevalent and swiftly spreading cancers globally, there is a pressing need for accurate automated analysis methods for oral cancer. This need is particularly critical for high-risk populations residing in low- and middle-income countries. In this study, we evaluated the performance of the Vision Transformer (ViT) and the Swin Transformer in the classification of mobile-based oral cancer images we collected from high-risk populations. The results showed that the Swin Transformer model achieved higher accuracy than the ViT model, and both transformer models work better than the conventional convolution model VGG19. Abstract Oral cancer, a pervasive and rapidly growing malignant disease, poses a significant global health concern. Early and accurate diagnosis is pivotal for improving patient outcomes. Automatic diagnosis methods based on artificial intelligence have shown promising results in the oral cancer field, but the accuracy still needs to be improved for realistic diagnostic scenarios. Vision Transformers (ViT) have outperformed learning CNN models recently in many computer vision benchmark tasks. This study explores the effectiveness of the Vision Transformer and the Swin Transformer, two cutting-edge variants of the transformer architecture, for the mobile-based oral cancer image classification application. The pre-trained Swin transformer model achieved 88.7% accuracy in the binary classification task, outperforming the ViT model by 2.3%, while the conventional convolutional network model VGG19 and ResNet50 achieved 85.2% and 84.5% accuracy. Our experiments demonstrate that these transformer-based architectures outperform traditional convolutional neural networks in terms of oral cancer image classification, and underscore the potential of the ViT and the Swin Transformer in advancing the state of the art in oral cancer image analysis.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/136b0ef7bd7f642a285b1d61ddf7abfc7ddc026f.pdf",
        "venue": "Cancers",
        "citationCount": 19,
        "score": 19.0,
        "summary": "Simple Summary Transformer models, originally successful in natural language processing, have found application in computer vision, demonstrating promising results in tasks related to cancer image analysis. Despite being one of the prevalent and swiftly spreading cancers globally, there is a pressing need for accurate automated analysis methods for oral cancer. This need is particularly critical for high-risk populations residing in low- and middle-income countries. In this study, we evaluated the performance of the Vision Transformer (ViT) and the Swin Transformer in the classification of mobile-based oral cancer images we collected from high-risk populations. The results showed that the Swin Transformer model achieved higher accuracy than the ViT model, and both transformer models work better than the conventional convolution model VGG19. Abstract Oral cancer, a pervasive and rapidly growing malignant disease, poses a significant global health concern. Early and accurate diagnosis is pivotal for improving patient outcomes. Automatic diagnosis methods based on artificial intelligence have shown promising results in the oral cancer field, but the accuracy still needs to be improved for realistic diagnostic scenarios. Vision Transformers (ViT) have outperformed learning CNN models recently in many computer vision benchmark tasks. This study explores the effectiveness of the Vision Transformer and the Swin Transformer, two cutting-edge variants of the transformer architecture, for the mobile-based oral cancer image classification application. The pre-trained Swin transformer model achieved 88.7% accuracy in the binary classification task, outperforming the ViT model by 2.3%, while the conventional convolutional network model VGG19 and ResNet50 achieved 85.2% and 84.5% accuracy. Our experiments demonstrate that these transformer-based architectures outperform traditional convolutional neural networks in terms of oral cancer image classification, and underscore the potential of the ViT and the Swin Transformer in advancing the state of the art in oral cancer image analysis.",
        "keywords": []
      },
      "file_name": "136b0ef7bd7f642a285b1d61ddf7abfc7ddc026f.pdf"
    },
    {
      "success": true,
      "doc_id": "2f42c78abb7d426cb1a46f36aa824819",
      "summary": "Skin tumors, especially melanoma, which is highly aggressive and progresses quickly to other sites, are an issue in various parts of the world. Nevertheless, the one and only way to save lives is to detect it at its initial stages. This study explores the application of advanced deep learning models for classifying benign and malignant melanoma using dermoscopic images. The aim of the study is to enhance the accuracy and efficiency of melanoma diagnosis with the ConvNeXt, Vision Transformer (ViT) Base-16, and Swin Transformer V2 Small (Swin V2 S) deep learning models. The ConvNeXt model, which integrates principles of both convolutional neural networks and transformers, demonstrated superior performance, with balanced precision and recall metrics. The dataset, sourced from Kaggle, comprises 13,900 uniformly sized images, preprocessed to standardize the inputs for the models. Experimental results revealed that ConvNeXt achieved the highest diagnostic accuracy among the tested models. Experimental results revealed that ConvNeXt achieved an accuracy of 91.5%, with balanced precision and recall rates of 90.45% and 92.8% for benign cases, and 92.61% and 90.2% for malignant cases, respectively. The F1-scores for ConvNeXt were 91.61% for benign cases and 91.39% for malignant cases. This research points out the potential of hybrid deep learning architectures in medical image analysis, particularly for early melanoma detection.",
      "intriguing_abstract": "Skin tumors, especially melanoma, which is highly aggressive and progresses quickly to other sites, are an issue in various parts of the world. Nevertheless, the one and only way to save lives is to detect it at its initial stages. This study explores the application of advanced deep learning models for classifying benign and malignant melanoma using dermoscopic images. The aim of the study is to enhance the accuracy and efficiency of melanoma diagnosis with the ConvNeXt, Vision Transformer (ViT) Base-16, and Swin Transformer V2 Small (Swin V2 S) deep learning models. The ConvNeXt model, which integrates principles of both convolutional neural networks and transformers, demonstrated superior performance, with balanced precision and recall metrics. The dataset, sourced from Kaggle, comprises 13,900 uniformly sized images, preprocessed to standardize the inputs for the models. Experimental results revealed that ConvNeXt achieved the highest diagnostic accuracy among the tested models. Experimental results revealed that ConvNeXt achieved an accuracy of 91.5%, with balanced precision and recall rates of 90.45% and 92.8% for benign cases, and 92.61% and 90.2% for malignant cases, respectively. The F1-scores for ConvNeXt were 91.61% for benign cases and 91.39% for malignant cases. This research points out the potential of hybrid deep learning architectures in medical image analysis, particularly for early melanoma detection.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/cf439db0e071f19305ea1755aa108acdde73ed99.pdf",
      "citation_key": "aksoy20240c0",
      "metadata": {
        "title": "Enhancing Melanoma Diagnosis with Advanced Deep Learning Models Focusing on Vision Transformer, Swin Transformer, and ConvNeXt",
        "authors": [
          "Serra Aksoy",
          "P. Demirciolu",
          "I. Bogrekci"
        ],
        "published_date": "2024",
        "abstract": "Skin tumors, especially melanoma, which is highly aggressive and progresses quickly to other sites, are an issue in various parts of the world. Nevertheless, the one and only way to save lives is to detect it at its initial stages. This study explores the application of advanced deep learning models for classifying benign and malignant melanoma using dermoscopic images. The aim of the study is to enhance the accuracy and efficiency of melanoma diagnosis with the ConvNeXt, Vision Transformer (ViT) Base-16, and Swin Transformer V2 Small (Swin V2 S) deep learning models. The ConvNeXt model, which integrates principles of both convolutional neural networks and transformers, demonstrated superior performance, with balanced precision and recall metrics. The dataset, sourced from Kaggle, comprises 13,900 uniformly sized images, preprocessed to standardize the inputs for the models. Experimental results revealed that ConvNeXt achieved the highest diagnostic accuracy among the tested models. Experimental results revealed that ConvNeXt achieved an accuracy of 91.5%, with balanced precision and recall rates of 90.45% and 92.8% for benign cases, and 92.61% and 90.2% for malignant cases, respectively. The F1-scores for ConvNeXt were 91.61% for benign cases and 91.39% for malignant cases. This research points out the potential of hybrid deep learning architectures in medical image analysis, particularly for early melanoma detection.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/cf439db0e071f19305ea1755aa108acdde73ed99.pdf",
        "venue": "Dermatopathology",
        "citationCount": 19,
        "score": 19.0,
        "summary": "Skin tumors, especially melanoma, which is highly aggressive and progresses quickly to other sites, are an issue in various parts of the world. Nevertheless, the one and only way to save lives is to detect it at its initial stages. This study explores the application of advanced deep learning models for classifying benign and malignant melanoma using dermoscopic images. The aim of the study is to enhance the accuracy and efficiency of melanoma diagnosis with the ConvNeXt, Vision Transformer (ViT) Base-16, and Swin Transformer V2 Small (Swin V2 S) deep learning models. The ConvNeXt model, which integrates principles of both convolutional neural networks and transformers, demonstrated superior performance, with balanced precision and recall metrics. The dataset, sourced from Kaggle, comprises 13,900 uniformly sized images, preprocessed to standardize the inputs for the models. Experimental results revealed that ConvNeXt achieved the highest diagnostic accuracy among the tested models. Experimental results revealed that ConvNeXt achieved an accuracy of 91.5%, with balanced precision and recall rates of 90.45% and 92.8% for benign cases, and 92.61% and 90.2% for malignant cases, respectively. The F1-scores for ConvNeXt were 91.61% for benign cases and 91.39% for malignant cases. This research points out the potential of hybrid deep learning architectures in medical image analysis, particularly for early melanoma detection.",
        "keywords": []
      },
      "file_name": "cf439db0e071f19305ea1755aa108acdde73ed99.pdf"
    },
    {
      "success": true,
      "doc_id": "c896fee1c7c9cfe1451d56a866d1fdb0",
      "summary": "Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.",
      "intriguing_abstract": "Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/ffc855594cad345ea5a1cce2ee27095bec767bc8.pdf",
      "citation_key": "leem2024j4t",
      "metadata": {
        "title": "Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention",
        "authors": [
          "Saebom Leem",
          "Hyunseok Seo"
        ],
        "published_date": "2024",
        "abstract": "Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ffc855594cad345ea5a1cce2ee27095bec767bc8.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 19,
        "score": 19.0,
        "summary": "Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.",
        "keywords": []
      },
      "file_name": "ffc855594cad345ea5a1cce2ee27095bec767bc8.pdf"
    },
    {
      "success": true,
      "doc_id": "f887bd17c60edd961362047acf5fdf7e",
      "summary": "Zero-shot learning (ZSL) recognizes the unseen classes by conducting visual-semantic interactions to transfer se-mantic knowledge from seen classes to unseen ones, sup-ported by semantic information (e.g., attributes). However, existing ZSL methods simply extract visual features using a pre-trained network backbone (i.e., CNN or ViT), which fail to learn matched visual-semantic correspondences for rep-resenting semantic-related visual features as lacking of the guidance of semantic information, resulting in undesirable visual-semantic interactions. To tackle this issue, we pro-pose a progressive semantic-guided vision transformer for zero-shot learning (dubbed ZSLViT). ZSLViT mainly consid-ers two properties in the whole network: i) discover the semantic-related visual representations explicitly, and ii) discard the semantic-unrelated visual information. Specif-ically, we first introduce semantic-embedded token learning to improve the visual-semantic correspondences via semantic enhancement and discover the semantic-related visual tokens explicitly with semantic-guided token attention. Then, we fuse low semantic-visual correspondence visual tokens to discard the semantic-unrelated visual in-formation for visual enhancement. These two operations are integrated into various encoders to progressively learn semantic-related visual representations for accurate visual-semantic interactions in ZSL. The extensive experiments show that our ZSLViT achieves significant performance gains on three popular benchmark datasets, i.e., CUB, SUN, and AWA2.",
      "intriguing_abstract": "Zero-shot learning (ZSL) recognizes the unseen classes by conducting visual-semantic interactions to transfer se-mantic knowledge from seen classes to unseen ones, sup-ported by semantic information (e.g., attributes). However, existing ZSL methods simply extract visual features using a pre-trained network backbone (i.e., CNN or ViT), which fail to learn matched visual-semantic correspondences for rep-resenting semantic-related visual features as lacking of the guidance of semantic information, resulting in undesirable visual-semantic interactions. To tackle this issue, we pro-pose a progressive semantic-guided vision transformer for zero-shot learning (dubbed ZSLViT). ZSLViT mainly consid-ers two properties in the whole network: i) discover the semantic-related visual representations explicitly, and ii) discard the semantic-unrelated visual information. Specif-ically, we first introduce semantic-embedded token learning to improve the visual-semantic correspondences via semantic enhancement and discover the semantic-related visual tokens explicitly with semantic-guided token attention. Then, we fuse low semantic-visual correspondence visual tokens to discard the semantic-unrelated visual in-formation for visual enhancement. These two operations are integrated into various encoders to progressively learn semantic-related visual representations for accurate visual-semantic interactions in ZSL. The extensive experiments show that our ZSLViT achieves significant performance gains on three popular benchmark datasets, i.e., CUB, SUN, and AWA2.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/838d7862215df504dde41496cbe6ee711a12ae9f.pdf",
      "citation_key": "chen2024asi",
      "metadata": {
        "title": "Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning",
        "authors": [
          "Shiming Chen",
          "W. Hou",
          "Salman H. Khan",
          "F. Khan"
        ],
        "published_date": "2024",
        "abstract": "Zero-shot learning (ZSL) recognizes the unseen classes by conducting visual-semantic interactions to transfer se-mantic knowledge from seen classes to unseen ones, sup-ported by semantic information (e.g., attributes). However, existing ZSL methods simply extract visual features using a pre-trained network backbone (i.e., CNN or ViT), which fail to learn matched visual-semantic correspondences for rep-resenting semantic-related visual features as lacking of the guidance of semantic information, resulting in undesirable visual-semantic interactions. To tackle this issue, we pro-pose a progressive semantic-guided vision transformer for zero-shot learning (dubbed ZSLViT). ZSLViT mainly consid-ers two properties in the whole network: i) discover the semantic-related visual representations explicitly, and ii) discard the semantic-unrelated visual information. Specif-ically, we first introduce semantic-embedded token learning to improve the visual-semantic correspondences via semantic enhancement and discover the semantic-related visual tokens explicitly with semantic-guided token attention. Then, we fuse low semantic-visual correspondence visual tokens to discard the semantic-unrelated visual in-formation for visual enhancement. These two operations are integrated into various encoders to progressively learn semantic-related visual representations for accurate visual-semantic interactions in ZSL. The extensive experiments show that our ZSLViT achieves significant performance gains on three popular benchmark datasets, i.e., CUB, SUN, and AWA2.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/838d7862215df504dde41496cbe6ee711a12ae9f.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 19,
        "score": 19.0,
        "summary": "Zero-shot learning (ZSL) recognizes the unseen classes by conducting visual-semantic interactions to transfer se-mantic knowledge from seen classes to unseen ones, sup-ported by semantic information (e.g., attributes). However, existing ZSL methods simply extract visual features using a pre-trained network backbone (i.e., CNN or ViT), which fail to learn matched visual-semantic correspondences for rep-resenting semantic-related visual features as lacking of the guidance of semantic information, resulting in undesirable visual-semantic interactions. To tackle this issue, we pro-pose a progressive semantic-guided vision transformer for zero-shot learning (dubbed ZSLViT). ZSLViT mainly consid-ers two properties in the whole network: i) discover the semantic-related visual representations explicitly, and ii) discard the semantic-unrelated visual information. Specif-ically, we first introduce semantic-embedded token learning to improve the visual-semantic correspondences via semantic enhancement and discover the semantic-related visual tokens explicitly with semantic-guided token attention. Then, we fuse low semantic-visual correspondence visual tokens to discard the semantic-unrelated visual in-formation for visual enhancement. These two operations are integrated into various encoders to progressively learn semantic-related visual representations for accurate visual-semantic interactions in ZSL. The extensive experiments show that our ZSLViT achieves significant performance gains on three popular benchmark datasets, i.e., CUB, SUN, and AWA2.",
        "keywords": []
      },
      "file_name": "838d7862215df504dde41496cbe6ee711a12ae9f.pdf"
    },
    {
      "success": true,
      "doc_id": "87830d747f596cda396b01c3b5782c72",
      "summary": "Precise crop yield prediction provides valuable information for agricultural planning and decision-making processes. However, timely predicting crop yields remains challenging as crop growth is sensitive to growing season weather variation and climate change. In this work, we develop a deep learning-based solution, namely Multi-Modal Spatial-Temporal Vision Transformer (MMST-ViT), for predicting crop yields at the county level across the United States, by considering the effects of short-term meteorological variations during the growing season and the long-term climate change on crops. Specifically, our MMST-ViT consists of a Multi-Modal Transformer, a Spatial Transformer, and a Temporal Transformer. The Multi-Modal Transformer leverages both visual remote sensing data and short-term meteorological data for modeling the effect of growing season weather variations on crop growth. The Spatial Transformer learns the high-resolution spatial dependency among counties for accurate agricultural tracking. The Temporal Transformer captures the long-range temporal dependency for learning the impact of long-term climate change on crops. Meanwhile, we also devise a novel multi-modal contrastive learning technique to pre-train our model without extensive human supervision. Hence, our MMST-ViT captures the impacts of both short-term weather variations and long-term climate change on crops by leveraging both satellite images and meteorological data. We have conducted extensive experiments on over 200 counties in the United States, with the experimental results exhibiting that our MMST-ViT outperforms its counterparts under three performance metrics of interest. Our dataset and code are available at https://github.com/fudong03/MMST-ViT.",
      "intriguing_abstract": "Precise crop yield prediction provides valuable information for agricultural planning and decision-making processes. However, timely predicting crop yields remains challenging as crop growth is sensitive to growing season weather variation and climate change. In this work, we develop a deep learning-based solution, namely Multi-Modal Spatial-Temporal Vision Transformer (MMST-ViT), for predicting crop yields at the county level across the United States, by considering the effects of short-term meteorological variations during the growing season and the long-term climate change on crops. Specifically, our MMST-ViT consists of a Multi-Modal Transformer, a Spatial Transformer, and a Temporal Transformer. The Multi-Modal Transformer leverages both visual remote sensing data and short-term meteorological data for modeling the effect of growing season weather variations on crop growth. The Spatial Transformer learns the high-resolution spatial dependency among counties for accurate agricultural tracking. The Temporal Transformer captures the long-range temporal dependency for learning the impact of long-term climate change on crops. Meanwhile, we also devise a novel multi-modal contrastive learning technique to pre-train our model without extensive human supervision. Hence, our MMST-ViT captures the impacts of both short-term weather variations and long-term climate change on crops by leveraging both satellite images and meteorological data. We have conducted extensive experiments on over 200 counties in the United States, with the experimental results exhibiting that our MMST-ViT outperforms its counterparts under three performance metrics of interest. Our dataset and code are available at https://github.com/fudong03/MMST-ViT.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/9a35a0aac04f4f712f38ea9bbfd2f0b0108242f5.pdf",
      "citation_key": "lin202343q",
      "metadata": {
        "title": "MMST-ViT: Climate Change-aware Crop Yield Prediction via Multi-Modal Spatial-Temporal Vision Transformer",
        "authors": [
          "Fudong Lin",
          "Summer Crawford",
          "Kaleb Guillot",
          "Yihe Zhang",
          "Yan Chen",
          "Xu Yuan",
          "Li Chen",
          "Shelby Williams",
          "Robert Minvielle",
          "Xiangming Xiao",
          "Drew Gholson",
          "Nicolas Ashwell",
          "Tri Setiyono",
          "B. Tubana",
          "Lu Peng",
          "Magdy A. Bayoumi",
          "N. Tzeng"
        ],
        "published_date": "2023",
        "abstract": "Precise crop yield prediction provides valuable information for agricultural planning and decision-making processes. However, timely predicting crop yields remains challenging as crop growth is sensitive to growing season weather variation and climate change. In this work, we develop a deep learning-based solution, namely Multi-Modal Spatial-Temporal Vision Transformer (MMST-ViT), for predicting crop yields at the county level across the United States, by considering the effects of short-term meteorological variations during the growing season and the long-term climate change on crops. Specifically, our MMST-ViT consists of a Multi-Modal Transformer, a Spatial Transformer, and a Temporal Transformer. The Multi-Modal Transformer leverages both visual remote sensing data and short-term meteorological data for modeling the effect of growing season weather variations on crop growth. The Spatial Transformer learns the high-resolution spatial dependency among counties for accurate agricultural tracking. The Temporal Transformer captures the long-range temporal dependency for learning the impact of long-term climate change on crops. Meanwhile, we also devise a novel multi-modal contrastive learning technique to pre-train our model without extensive human supervision. Hence, our MMST-ViT captures the impacts of both short-term weather variations and long-term climate change on crops by leveraging both satellite images and meteorological data. We have conducted extensive experiments on over 200 counties in the United States, with the experimental results exhibiting that our MMST-ViT outperforms its counterparts under three performance metrics of interest. Our dataset and code are available at https://github.com/fudong03/MMST-ViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9a35a0aac04f4f712f38ea9bbfd2f0b0108242f5.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 37,
        "score": 18.5,
        "summary": "Precise crop yield prediction provides valuable information for agricultural planning and decision-making processes. However, timely predicting crop yields remains challenging as crop growth is sensitive to growing season weather variation and climate change. In this work, we develop a deep learning-based solution, namely Multi-Modal Spatial-Temporal Vision Transformer (MMST-ViT), for predicting crop yields at the county level across the United States, by considering the effects of short-term meteorological variations during the growing season and the long-term climate change on crops. Specifically, our MMST-ViT consists of a Multi-Modal Transformer, a Spatial Transformer, and a Temporal Transformer. The Multi-Modal Transformer leverages both visual remote sensing data and short-term meteorological data for modeling the effect of growing season weather variations on crop growth. The Spatial Transformer learns the high-resolution spatial dependency among counties for accurate agricultural tracking. The Temporal Transformer captures the long-range temporal dependency for learning the impact of long-term climate change on crops. Meanwhile, we also devise a novel multi-modal contrastive learning technique to pre-train our model without extensive human supervision. Hence, our MMST-ViT captures the impacts of both short-term weather variations and long-term climate change on crops by leveraging both satellite images and meteorological data. We have conducted extensive experiments on over 200 counties in the United States, with the experimental results exhibiting that our MMST-ViT outperforms its counterparts under three performance metrics of interest. Our dataset and code are available at https://github.com/fudong03/MMST-ViT.",
        "keywords": []
      },
      "file_name": "9a35a0aac04f4f712f38ea9bbfd2f0b0108242f5.pdf"
    },
    {
      "success": true,
      "doc_id": "2b7b185a2d8fc263e5e8309a120c8988",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f6bf7787115affe22c410eb5b2606269912d59a0.pdf",
      "citation_key": "ghahremani202491m",
      "metadata": {
        "title": "H-ViT: A Hierarchical Vision Transformer for Deformable Image Registration",
        "authors": [
          "Morteza Ghahremani",
          "Mohammad Khateri",
          "Bailiang Jian",
          "B. Wiestler",
          "Ehsan Adeli",
          "Christian Wachinger"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f6bf7787115affe22c410eb5b2606269912d59a0.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 18,
        "score": 18.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "f6bf7787115affe22c410eb5b2606269912d59a0.pdf"
    },
    {
      "success": true,
      "doc_id": "b80ea029031b9dbfa4dca1c0e060586d",
      "summary": "This paper proposes a simple, yet effective framework, called GiT, simultaneously applicable for various vision tasks only with a vanilla ViT. Motivated by the universality of the Multi-layer Transformer architecture (e.g, GPT) widely used in large language models (LLMs), we seek to broaden its scope to serve as a powerful vision foundation model (VFM). However, unlike language modeling, visual tasks typically require specific modules, such as bounding box heads for detection and pixel decoders for segmentation, greatly hindering the application of powerful multi-layer transformers in the vision domain. To solve this, we design a universal language interface that empowers the successful auto-regressive decoding to adeptly unify various visual tasks, from image-level understanding (e.g., captioning), over sparse perception (e.g., detection), to dense prediction (e.g., segmentation). Based on the above designs, the entire model is composed solely of a ViT, without any specific additions, offering a remarkable architectural simplification. GiT is a multi-task visual model, jointly trained across five representative benchmarks without task-specific fine-tuning. Interestingly, our GiT builds a new benchmark in generalist performance, and fosters mutual enhancement across tasks, leading to significant improvements compared to isolated training. This reflects a similar impact observed in LLMs. Further enriching training with 27 datasets, GiT achieves strong zero-shot results over various tasks. Due to its simple design, this paradigm holds promise for narrowing the architectural gap between vision and language. Code and models will be available at \\url{https://github.com/Haiyang-W/GiT}.",
      "intriguing_abstract": "This paper proposes a simple, yet effective framework, called GiT, simultaneously applicable for various vision tasks only with a vanilla ViT. Motivated by the universality of the Multi-layer Transformer architecture (e.g, GPT) widely used in large language models (LLMs), we seek to broaden its scope to serve as a powerful vision foundation model (VFM). However, unlike language modeling, visual tasks typically require specific modules, such as bounding box heads for detection and pixel decoders for segmentation, greatly hindering the application of powerful multi-layer transformers in the vision domain. To solve this, we design a universal language interface that empowers the successful auto-regressive decoding to adeptly unify various visual tasks, from image-level understanding (e.g., captioning), over sparse perception (e.g., detection), to dense prediction (e.g., segmentation). Based on the above designs, the entire model is composed solely of a ViT, without any specific additions, offering a remarkable architectural simplification. GiT is a multi-task visual model, jointly trained across five representative benchmarks without task-specific fine-tuning. Interestingly, our GiT builds a new benchmark in generalist performance, and fosters mutual enhancement across tasks, leading to significant improvements compared to isolated training. This reflects a similar impact observed in LLMs. Further enriching training with 27 datasets, GiT achieves strong zero-shot results over various tasks. Due to its simple design, this paradigm holds promise for narrowing the architectural gap between vision and language. Code and models will be available at \\url{https://github.com/Haiyang-W/GiT}.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/69e56df0ae079c83607bb48e68f8db39a4177cd0.pdf",
      "citation_key": "wang20249qa",
      "metadata": {
        "title": "GiT: Towards Generalist Vision Transformer through Universal Language Interface",
        "authors": [
          "Haiyang Wang",
          "Hao Tang",
          "Li Jiang",
          "Shaoshuai Shi",
          "Muhammad Ferjad Naeem",
          "Hongsheng Li",
          "B. Schiele",
          "Liwei Wang"
        ],
        "published_date": "2024",
        "abstract": "This paper proposes a simple, yet effective framework, called GiT, simultaneously applicable for various vision tasks only with a vanilla ViT. Motivated by the universality of the Multi-layer Transformer architecture (e.g, GPT) widely used in large language models (LLMs), we seek to broaden its scope to serve as a powerful vision foundation model (VFM). However, unlike language modeling, visual tasks typically require specific modules, such as bounding box heads for detection and pixel decoders for segmentation, greatly hindering the application of powerful multi-layer transformers in the vision domain. To solve this, we design a universal language interface that empowers the successful auto-regressive decoding to adeptly unify various visual tasks, from image-level understanding (e.g., captioning), over sparse perception (e.g., detection), to dense prediction (e.g., segmentation). Based on the above designs, the entire model is composed solely of a ViT, without any specific additions, offering a remarkable architectural simplification. GiT is a multi-task visual model, jointly trained across five representative benchmarks without task-specific fine-tuning. Interestingly, our GiT builds a new benchmark in generalist performance, and fosters mutual enhancement across tasks, leading to significant improvements compared to isolated training. This reflects a similar impact observed in LLMs. Further enriching training with 27 datasets, GiT achieves strong zero-shot results over various tasks. Due to its simple design, this paradigm holds promise for narrowing the architectural gap between vision and language. Code and models will be available at \\url{https://github.com/Haiyang-W/GiT}.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/69e56df0ae079c83607bb48e68f8db39a4177cd0.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 18,
        "score": 18.0,
        "summary": "This paper proposes a simple, yet effective framework, called GiT, simultaneously applicable for various vision tasks only with a vanilla ViT. Motivated by the universality of the Multi-layer Transformer architecture (e.g, GPT) widely used in large language models (LLMs), we seek to broaden its scope to serve as a powerful vision foundation model (VFM). However, unlike language modeling, visual tasks typically require specific modules, such as bounding box heads for detection and pixel decoders for segmentation, greatly hindering the application of powerful multi-layer transformers in the vision domain. To solve this, we design a universal language interface that empowers the successful auto-regressive decoding to adeptly unify various visual tasks, from image-level understanding (e.g., captioning), over sparse perception (e.g., detection), to dense prediction (e.g., segmentation). Based on the above designs, the entire model is composed solely of a ViT, without any specific additions, offering a remarkable architectural simplification. GiT is a multi-task visual model, jointly trained across five representative benchmarks without task-specific fine-tuning. Interestingly, our GiT builds a new benchmark in generalist performance, and fosters mutual enhancement across tasks, leading to significant improvements compared to isolated training. This reflects a similar impact observed in LLMs. Further enriching training with 27 datasets, GiT achieves strong zero-shot results over various tasks. Due to its simple design, this paradigm holds promise for narrowing the architectural gap between vision and language. Code and models will be available at \\url{https://github.com/Haiyang-W/GiT}.",
        "keywords": []
      },
      "file_name": "69e56df0ae079c83607bb48e68f8db39a4177cd0.pdf"
    },
    {
      "success": true,
      "doc_id": "403c3e6e01cdd1a3690b466306b6fa3e",
      "summary": "Inspections of concrete bridges across the United States represent a significant commitment of resources, given their biannual mandate for many structures. With a notable number of aging bridges, there is an imperative need to enhance the efficiency of these inspections. This study harnessed the power of computer vision to streamline the inspection process. Our experiment examined the efficacy of a state-of-the-art Visual Transformer (ViT) model combined with distinct image enhancement detector algorithms. We benchmarked against a deep learning Convolutional Neural Network (CNN) model. These models were applied to over 20,000 high-quality images from the Concrete Images for Classification dataset. Traditional crack detection methods often fall short due to their heavy reliance on time and resources. This research pioneers bridge inspection by integrating ViT with diverse image enhancement detectors, significantly improving concrete crack detection accuracy. Notably, a custom-built CNN achieves over 99% accuracy with substantially lower training time than ViT, making it an efficient solution for enhancing safety and resource conservation in infrastructure management. These advancements enhance safety by enabling reliable detection and timely maintenance, but they also align with Industry 4.0 objectives, automating manual inspections, reducing costs, and advancing technological integration in public infrastructure management.",
      "intriguing_abstract": "Inspections of concrete bridges across the United States represent a significant commitment of resources, given their biannual mandate for many structures. With a notable number of aging bridges, there is an imperative need to enhance the efficiency of these inspections. This study harnessed the power of computer vision to streamline the inspection process. Our experiment examined the efficacy of a state-of-the-art Visual Transformer (ViT) model combined with distinct image enhancement detector algorithms. We benchmarked against a deep learning Convolutional Neural Network (CNN) model. These models were applied to over 20,000 high-quality images from the Concrete Images for Classification dataset. Traditional crack detection methods often fall short due to their heavy reliance on time and resources. This research pioneers bridge inspection by integrating ViT with diverse image enhancement detectors, significantly improving concrete crack detection accuracy. Notably, a custom-built CNN achieves over 99% accuracy with substantially lower training time than ViT, making it an efficient solution for enhancing safety and resource conservation in infrastructure management. These advancements enhance safety by enabling reliable detection and timely maintenance, but they also align with Industry 4.0 objectives, automating manual inspections, reducing costs, and advancing technological integration in public infrastructure management.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/1528bab641830ea279bdf53ff9e7d11d2b7e5028.pdf",
      "citation_key": "shahin2024g0q",
      "metadata": {
        "title": "Improving the Concrete Crack Detection Process via a Hybrid Visual Transformer Algorithm",
        "authors": [
          "Mohammad Shahin",
          "F. F. Chen",
          "Mazdak Maghanaki",
          "Aliakbar Hosseinzadeh",
          "Neda Zand",
          "Hamid Khodadadi Koodiani"
        ],
        "published_date": "2024",
        "abstract": "Inspections of concrete bridges across the United States represent a significant commitment of resources, given their biannual mandate for many structures. With a notable number of aging bridges, there is an imperative need to enhance the efficiency of these inspections. This study harnessed the power of computer vision to streamline the inspection process. Our experiment examined the efficacy of a state-of-the-art Visual Transformer (ViT) model combined with distinct image enhancement detector algorithms. We benchmarked against a deep learning Convolutional Neural Network (CNN) model. These models were applied to over 20,000 high-quality images from the Concrete Images for Classification dataset. Traditional crack detection methods often fall short due to their heavy reliance on time and resources. This research pioneers bridge inspection by integrating ViT with diverse image enhancement detectors, significantly improving concrete crack detection accuracy. Notably, a custom-built CNN achieves over 99% accuracy with substantially lower training time than ViT, making it an efficient solution for enhancing safety and resource conservation in infrastructure management. These advancements enhance safety by enabling reliable detection and timely maintenance, but they also align with Industry 4.0 objectives, automating manual inspections, reducing costs, and advancing technological integration in public infrastructure management.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1528bab641830ea279bdf53ff9e7d11d2b7e5028.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 18,
        "score": 18.0,
        "summary": "Inspections of concrete bridges across the United States represent a significant commitment of resources, given their biannual mandate for many structures. With a notable number of aging bridges, there is an imperative need to enhance the efficiency of these inspections. This study harnessed the power of computer vision to streamline the inspection process. Our experiment examined the efficacy of a state-of-the-art Visual Transformer (ViT) model combined with distinct image enhancement detector algorithms. We benchmarked against a deep learning Convolutional Neural Network (CNN) model. These models were applied to over 20,000 high-quality images from the Concrete Images for Classification dataset. Traditional crack detection methods often fall short due to their heavy reliance on time and resources. This research pioneers bridge inspection by integrating ViT with diverse image enhancement detectors, significantly improving concrete crack detection accuracy. Notably, a custom-built CNN achieves over 99% accuracy with substantially lower training time than ViT, making it an efficient solution for enhancing safety and resource conservation in infrastructure management. These advancements enhance safety by enabling reliable detection and timely maintenance, but they also align with Industry 4.0 objectives, automating manual inspections, reducing costs, and advancing technological integration in public infrastructure management.",
        "keywords": []
      },
      "file_name": "1528bab641830ea279bdf53ff9e7d11d2b7e5028.pdf"
    },
    {
      "success": true,
      "doc_id": "83a50d0e21ad5638693bed788e94e458",
      "summary": "This paper explores the properties of the plain Vision Transformer (ViT) for Weakly-supervised Semantic Segmentation (WSSS). The class activation map (CAM) is of critical importance for understanding a classification network and launching WSSS. We observe that different attention heads of ViT focus on different image areas. Thus a novel weight-based method is proposed to end-to-end estimate the importance of attention heads, while the self-attention maps are adaptively fused for high-quality CAM results that tend to have more complete objects. Besides, we propose a ViT-based gradient clipping decoder for online retraining with the CAM results to complete the WSSS task. We name this plain Transformer-based Weakly-supervised learning framework WeakTr. It achieves the state-of-the-art WSSS performance on standard benchmarks, i.e., 78.4% mIoU on the val set of PASCAL VOC 2012 and 50.3% mIoU on the val set of COCO 2014. Code is available at https://github.com/hustvl/WeakTr.",
      "intriguing_abstract": "This paper explores the properties of the plain Vision Transformer (ViT) for Weakly-supervised Semantic Segmentation (WSSS). The class activation map (CAM) is of critical importance for understanding a classification network and launching WSSS. We observe that different attention heads of ViT focus on different image areas. Thus a novel weight-based method is proposed to end-to-end estimate the importance of attention heads, while the self-attention maps are adaptively fused for high-quality CAM results that tend to have more complete objects. Besides, we propose a ViT-based gradient clipping decoder for online retraining with the CAM results to complete the WSSS task. We name this plain Transformer-based Weakly-supervised learning framework WeakTr. It achieves the state-of-the-art WSSS performance on standard benchmarks, i.e., 78.4% mIoU on the val set of PASCAL VOC 2012 and 50.3% mIoU on the val set of COCO 2014. Code is available at https://github.com/hustvl/WeakTr.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/b43bb480caad36ab6fd667570275d42fe9050175.pdf",
      "citation_key": "zhu2023dpi",
      "metadata": {
        "title": "WeakTr: Exploring Plain Vision Transformer for Weakly-supervised Semantic Segmentation",
        "authors": [
          "Liang Zhu",
          "Yingyue Li",
          "Jiemin Fang",
          "Yang Liu",
          "Hao Xin",
          "Wenyu Liu",
          "Xinggang Wang"
        ],
        "published_date": "2023",
        "abstract": "This paper explores the properties of the plain Vision Transformer (ViT) for Weakly-supervised Semantic Segmentation (WSSS). The class activation map (CAM) is of critical importance for understanding a classification network and launching WSSS. We observe that different attention heads of ViT focus on different image areas. Thus a novel weight-based method is proposed to end-to-end estimate the importance of attention heads, while the self-attention maps are adaptively fused for high-quality CAM results that tend to have more complete objects. Besides, we propose a ViT-based gradient clipping decoder for online retraining with the CAM results to complete the WSSS task. We name this plain Transformer-based Weakly-supervised learning framework WeakTr. It achieves the state-of-the-art WSSS performance on standard benchmarks, i.e., 78.4% mIoU on the val set of PASCAL VOC 2012 and 50.3% mIoU on the val set of COCO 2014. Code is available at https://github.com/hustvl/WeakTr.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b43bb480caad36ab6fd667570275d42fe9050175.pdf",
        "venue": "arXiv.org",
        "citationCount": 36,
        "score": 18.0,
        "summary": "This paper explores the properties of the plain Vision Transformer (ViT) for Weakly-supervised Semantic Segmentation (WSSS). The class activation map (CAM) is of critical importance for understanding a classification network and launching WSSS. We observe that different attention heads of ViT focus on different image areas. Thus a novel weight-based method is proposed to end-to-end estimate the importance of attention heads, while the self-attention maps are adaptively fused for high-quality CAM results that tend to have more complete objects. Besides, we propose a ViT-based gradient clipping decoder for online retraining with the CAM results to complete the WSSS task. We name this plain Transformer-based Weakly-supervised learning framework WeakTr. It achieves the state-of-the-art WSSS performance on standard benchmarks, i.e., 78.4% mIoU on the val set of PASCAL VOC 2012 and 50.3% mIoU on the val set of COCO 2014. Code is available at https://github.com/hustvl/WeakTr.",
        "keywords": []
      },
      "file_name": "b43bb480caad36ab6fd667570275d42fe9050175.pdf"
    },
    {
      "success": true,
      "doc_id": "9874b64675e4e66fe26038df71457548",
      "summary": "Recently, vision transformer (ViT) based multimodal learning methods have been proposed to improve the robustness of face anti-spoofing (FAS) systems. However, there are still no works to explore the fundamental natures (e.g., modality-aware inputs, suitable multimodal pre-training, and efficient finetuning) in vanilla ViT for multimodal FAS. In this paper, we investigate three key factors (i.e., inputs, pre-training, and finetuning) in ViT for multimodal FAS with RGB, Infrared (IR), and Depth. First, in terms of the ViT inputs, we find that leveraging local feature descriptors (such as histograms of oriented gradients) benefits the ViT on IR modality but not RGB or Depth modalities. Second, in consideration of the task (FAS vs. generic object classification) and modality (multimodal vs. unimodal) gaps, ImageNet pre-trained models might be sub-optimal for the multimodal FAS task. Finally, in observation of the inefficiency on direct finetuning the whole or partial ViT, we design an adaptive multimodal adapter (AMA), which can efficiently aggregate local multimodal features while freezing majority of ViT parameters. To bridge these gaps, we propose the modality-asymmetric masked autoencoder (M$$^{2}$$\n \n \n 2\n \n A$$^{2}$$\n \n \n 2\n \n E) for multimodal FAS self-supervised pre-training without costly annotated labels. Compared with the previous modality-symmetric autoencoder, the proposed M$$^{2}$$\n \n \n 2\n \n A$$^{2}$$\n \n \n 2\n \n E is able to learn more intrinsic task-aware representation and compatible with modality-agnostic (e.g., unimodal, bimodal, and trimodal) downstream settings. Extensive experiments with both unimodal (RGB, Depth, IR) and multimodal (RGB+Depth, RGB+IR, Depth+IR, RGB+Depth+IR) settings conducted on multimodal FAS benchmarks demonstrate the superior performance of the proposed methods. One highlight is that the proposed method is robust under various missing-modality cases where previous multimodal FAS models suffer serious performance drops. We hope these findings and solutions can facilitate the future research for ViT-based multimodal FAS.",
      "intriguing_abstract": "Recently, vision transformer (ViT) based multimodal learning methods have been proposed to improve the robustness of face anti-spoofing (FAS) systems. However, there are still no works to explore the fundamental natures (e.g., modality-aware inputs, suitable multimodal pre-training, and efficient finetuning) in vanilla ViT for multimodal FAS. In this paper, we investigate three key factors (i.e., inputs, pre-training, and finetuning) in ViT for multimodal FAS with RGB, Infrared (IR), and Depth. First, in terms of the ViT inputs, we find that leveraging local feature descriptors (such as histograms of oriented gradients) benefits the ViT on IR modality but not RGB or Depth modalities. Second, in consideration of the task (FAS vs. generic object classification) and modality (multimodal vs. unimodal) gaps, ImageNet pre-trained models might be sub-optimal for the multimodal FAS task. Finally, in observation of the inefficiency on direct finetuning the whole or partial ViT, we design an adaptive multimodal adapter (AMA), which can efficiently aggregate local multimodal features while freezing majority of ViT parameters. To bridge these gaps, we propose the modality-asymmetric masked autoencoder (M$$^{2}$$\n \n \n 2\n \n A$$^{2}$$\n \n \n 2\n \n E) for multimodal FAS self-supervised pre-training without costly annotated labels. Compared with the previous modality-symmetric autoencoder, the proposed M$$^{2}$$\n \n \n 2\n \n A$$^{2}$$\n \n \n 2\n \n E is able to learn more intrinsic task-aware representation and compatible with modality-agnostic (e.g., unimodal, bimodal, and trimodal) downstream settings. Extensive experiments with both unimodal (RGB, Depth, IR) and multimodal (RGB+Depth, RGB+IR, Depth+IR, RGB+Depth+IR) settings conducted on multimodal FAS benchmarks demonstrate the superior performance of the proposed methods. One highlight is that the proposed method is robust under various missing-modality cases where previous multimodal FAS models suffer serious performance drops. We hope these findings and solutions can facilitate the future research for ViT-based multimodal FAS.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/1970ace992d742bdf098de08a82817b05ef87477.pdf",
      "citation_key": "yu2023l1g",
      "metadata": {
        "title": "Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing",
        "authors": [
          "Zitong Yu",
          "Rizhao Cai",
          "Yawen Cui",
          "Xin Liu",
          "Yongjian Hu",
          "A. Kot"
        ],
        "published_date": "2023",
        "abstract": "Recently, vision transformer (ViT) based multimodal learning methods have been proposed to improve the robustness of face anti-spoofing (FAS) systems. However, there are still no works to explore the fundamental natures (e.g., modality-aware inputs, suitable multimodal pre-training, and efficient finetuning) in vanilla ViT for multimodal FAS. In this paper, we investigate three key factors (i.e., inputs, pre-training, and finetuning) in ViT for multimodal FAS with RGB, Infrared (IR), and Depth. First, in terms of the ViT inputs, we find that leveraging local feature descriptors (such as histograms of oriented gradients) benefits the ViT on IR modality but not RGB or Depth modalities. Second, in consideration of the task (FAS vs. generic object classification) and modality (multimodal vs. unimodal) gaps, ImageNet pre-trained models might be sub-optimal for the multimodal FAS task. Finally, in observation of the inefficiency on direct finetuning the whole or partial ViT, we design an adaptive multimodal adapter (AMA), which can efficiently aggregate local multimodal features while freezing majority of ViT parameters. To bridge these gaps, we propose the modality-asymmetric masked autoencoder (M$$^{2}$$\n \n \n 2\n \n A$$^{2}$$\n \n \n 2\n \n E) for multimodal FAS self-supervised pre-training without costly annotated labels. Compared with the previous modality-symmetric autoencoder, the proposed M$$^{2}$$\n \n \n 2\n \n A$$^{2}$$\n \n \n 2\n \n E is able to learn more intrinsic task-aware representation and compatible with modality-agnostic (e.g., unimodal, bimodal, and trimodal) downstream settings. Extensive experiments with both unimodal (RGB, Depth, IR) and multimodal (RGB+Depth, RGB+IR, Depth+IR, RGB+Depth+IR) settings conducted on multimodal FAS benchmarks demonstrate the superior performance of the proposed methods. One highlight is that the proposed method is robust under various missing-modality cases where previous multimodal FAS models suffer serious performance drops. We hope these findings and solutions can facilitate the future research for ViT-based multimodal FAS.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1970ace992d742bdf098de08a82817b05ef87477.pdf",
        "venue": "International Journal of Computer Vision",
        "citationCount": 35,
        "score": 17.5,
        "summary": "Recently, vision transformer (ViT) based multimodal learning methods have been proposed to improve the robustness of face anti-spoofing (FAS) systems. However, there are still no works to explore the fundamental natures (e.g., modality-aware inputs, suitable multimodal pre-training, and efficient finetuning) in vanilla ViT for multimodal FAS. In this paper, we investigate three key factors (i.e., inputs, pre-training, and finetuning) in ViT for multimodal FAS with RGB, Infrared (IR), and Depth. First, in terms of the ViT inputs, we find that leveraging local feature descriptors (such as histograms of oriented gradients) benefits the ViT on IR modality but not RGB or Depth modalities. Second, in consideration of the task (FAS vs. generic object classification) and modality (multimodal vs. unimodal) gaps, ImageNet pre-trained models might be sub-optimal for the multimodal FAS task. Finally, in observation of the inefficiency on direct finetuning the whole or partial ViT, we design an adaptive multimodal adapter (AMA), which can efficiently aggregate local multimodal features while freezing majority of ViT parameters. To bridge these gaps, we propose the modality-asymmetric masked autoencoder (M$$^{2}$$\n \n \n 2\n \n A$$^{2}$$\n \n \n 2\n \n E) for multimodal FAS self-supervised pre-training without costly annotated labels. Compared with the previous modality-symmetric autoencoder, the proposed M$$^{2}$$\n \n \n 2\n \n A$$^{2}$$\n \n \n 2\n \n E is able to learn more intrinsic task-aware representation and compatible with modality-agnostic (e.g., unimodal, bimodal, and trimodal) downstream settings. Extensive experiments with both unimodal (RGB, Depth, IR) and multimodal (RGB+Depth, RGB+IR, Depth+IR, RGB+Depth+IR) settings conducted on multimodal FAS benchmarks demonstrate the superior performance of the proposed methods. One highlight is that the proposed method is robust under various missing-modality cases where previous multimodal FAS models suffer serious performance drops. We hope these findings and solutions can facilitate the future research for ViT-based multimodal FAS.",
        "keywords": []
      },
      "file_name": "1970ace992d742bdf098de08a82817b05ef87477.pdf"
    },
    {
      "success": true,
      "doc_id": "5ee8a2c270fa4a165f9f1e2994a24d39",
      "summary": "Here's a focused summary of the paper by Ko et al. \\cite{ko2024eax} for a literature review:\n\n### Technical Paper Analysis: Optimization of Vision Transformer-based Detection of Lung Diseases from Chest X-ray Images \\cite{ko2024eax}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: While Vision Transformers (ViTs) have significantly advanced lung disease prediction from chest X-ray (CXR) images, there is a lack of systematic research comparing the effectiveness of different optimization methods when training these ViT-based models.\n    *   **Importance and Challenge**: Optimizers play a critical role in the training efficiency and final performance (accuracy, robustness) of deep learning models. Their performance is often model-dependent. Identifying optimal optimization strategies for ViT-based models in medical image analysis, especially with potentially imbalanced datasets common in clinical settings, is crucial for maximizing diagnostic accuracy and reliability.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon the success of deep learning, particularly Convolutional Neural Networks (CNNs) and more recently ViTs, in medical image analysis. ViTs, with their self-attention mechanisms, have shown superior performance over CNNs in various computer vision tasks, including COVID-19 detection from CXRs.\n    *   **Limitations of Previous Solutions**: Previous studies have shown that optimizer performance is highly model-dependent, with different optimizers (e.g., RMSProp, Adam, AdaGrad) yielding varying results for CNNs. While some optimizers (RAdam, Adam with cosine decay, NovoGrad, AdaBelief) have been applied to ViTs, a comprehensive, systematic comparison of a broad range of optimization methods specifically for ViT models in chest X-ray image classification for lung diseases was lacking.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The study systematically evaluates and compares six prominent optimization methods (Adam, AdamW, NAdam, RAdam, SGDW, and Momentum) across three different Vision Transformer architectures (ViT, FastViT, and CrossViT) for multi-class classification of lung diseases from chest X-ray images.\n    *   **Novelty/Difference**: The primary innovation lies in its comprehensive, comparative analysis of optimizers for ViT-based models in a specific medical imaging domain. It investigates performance not only across different ViT variants but also under conditions of both balanced and imbalanced class distributions within the dataset, providing practical guidance for model development.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: While not introducing new optimizers or ViT architectures, the paper provides a novel empirical comparison and characterization of existing optimizers' performance within the context of ViT-based lung disease detection.\n    *   **System Design/Architectural Innovations**: The study systematically tests combinations of three ViT architectures (ViT-B/16, FastViT, CrossViT) with six optimizers, evaluating their performance across different learning rates and dataset balancing scenarios.\n    *   **Theoretical Insights/Analysis**: The findings suggest that Adam-based optimizers generally outperform traditional SGD-based methods (SGDW, Momentum) for ViT models in this domain, likely due to their adaptive momentum algorithms. It also highlights the specific strengths of RAdam for balanced datasets and NAdam (especially with FastViT) for imbalanced datasets, linking these to their underlying mechanisms (e.g., RAdam's variance rectification, NAdam's Nesterov accelerated gradient).\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Training of ViT, FastViT, and CrossViT models using six optimizers (Adam, AdamW, NAdam, RAdam, SGDW, Momentum) at three learning rates (10^-4, 10^-5, 10^-6).\n        *   Evaluation on a publicly available chest X-ray dataset of 19,003 images, comprising normal cases and six lung diseases (COVID-19, Viral Pneumonia, Bacterial Pneumonia, MERS, SARS, Tuberculosis).\n        *   Performance assessed on both a 4-class balanced subset (Normal, COVID-19, Viral Pneumonia, Bacterial Pneumonia) and the full 7-class imbalanced dataset.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Metrics**: Accuracy, F1-score, Precision, Recall.\n        *   **ViT (balanced 4-class dataset)**: RAdam achieved the highest accuracy (95.87% at LR 10^-5). Adam achieved the highest F1-score (94.71% at LR 10^-5). Adam-based optimizers consistently outperformed SGDW and Momentum.\n        *   **ViT (imbalanced 7-class dataset)**: RAdam showed the highest accuracy (96.61% at LR 10^-5) and F1-score (96.62%).\n        *   **FastViT (imbalanced 7-class dataset)**: NAdam achieved the best overall performance with 97.63% accuracy and 97.64% F1-score (at LR 10^-4), demonstrating robustness against sample imbalance.\n        *   **CrossViT (imbalanced 7-class dataset)**: AdamW achieved the best performance with 96.95% accuracy and 96.94% F1-score (at LR 10^-5).\n        *   **Class-specific prediction**: Normal and Tuberculosis classes were generally better predicted than others.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The models struggled with detecting small-sized classes (e.g., MERS, SARS) due to limited sample sizes. The study exclusively focused on transformer models (ViT, FastViT, CrossViT) and a specific set of optimizers.\n    *   **Scope of Applicability**: The findings are directly applicable to ViT-based models for lung disease detection from chest X-ray images. The insights into optimizer performance under balanced/imbalanced data conditions can inform model development in similar medical imaging classification tasks.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This study provides crucial empirical evidence and practical recommendations for selecting optimal optimizers for ViT-based models in medical image analysis, a previously under-researched area. It demonstrates that specific optimizer-model combinations (e.g., FastViT with NAdam for imbalanced data) can achieve very high diagnostic accuracies (up to 97.63%).\n    *   **Potential Impact on Future Research**: The results offer a foundational understanding for developing more robust and accurate ViT-based diagnostic tools. Future research can leverage these findings to explore other optimizers, hybrid models, or different medical imaging modalities, and to address the challenge of small-sample-sized classes through techniques like advanced data augmentation or few-shot learning, building upon the identified optimal optimization strategies \\cite{ko2024eax}.",
      "intriguing_abstract": "Unleashing the full potential of Vision Transformers (ViTs) for critical medical diagnostics hinges on optimal training strategies. While ViTs have revolutionized lung disease detection from chest X-ray (CXR) images, the systematic impact of diverse optimization methods on their performance, particularly with prevalent imbalanced clinical datasets, remains largely unexplored. This paper presents an unprecedented, comprehensive empirical analysis comparing six prominent optimizers (Adam, AdamW, NAdam, RAdam, SGDW, Momentum) across three distinct ViT architectures (ViT, FastViT, CrossViT) for multi-class lung disease classification.\n\nOur rigorous evaluation, conducted on both balanced and imbalanced CXR datasets, reveals pivotal insights. We demonstrate that adaptive Adam-based optimizers generally outperform traditional SGD variants. Crucially, specific pairings yield superior results: RAdam excels on balanced data (95.87% accuracy), while NAdam, particularly with FastViT, achieves remarkable robustness and accuracy (97.63%) on challenging imbalanced datasets. These findings provide critical, data-driven guidance for developing highly accurate and reliable ViT-based diagnostic tools. This work significantly advances the state-of-the-art in medical image analysis, offering a foundational roadmap for optimizing deep learning models in real-world clinical applications.",
      "keywords": [
        "Vision Transformers (ViTs)",
        "Optimizers",
        "Lung disease detection",
        "Chest X-ray images (CXR)",
        "Medical image analysis",
        "Systematic comparative analysis",
        "Adam-based optimizers",
        "RAdam",
        "NAdam",
        "Imbalanced datasets",
        "Diagnostic accuracy",
        "Optimal optimization strategies",
        "FastViT"
      ],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/fec86abbb29b559c1eaff31428f5b59f8070bb67.pdf",
      "citation_key": "ko2024eax",
      "metadata": {
        "title": "Optimization of vision transformer-based detection of lung diseases from chest X-ray images",
        "authors": [
          "Jinsol Ko",
          "Soyeon Park",
          "H. G. Woo"
        ],
        "published_date": "2024",
        "abstract": "Background Recent advances in Vision Transformer (ViT)-based deep learning have significantly improved the accuracy of lung disease prediction from chest X-ray images. However, limited research exists on comparing the effectiveness of different optimizers for lung disease prediction within ViT models. This study aims to systematically evaluate and compare the performance of various optimization methods for ViT-based models in predicting lung diseases from chest X-ray images. Methods This study utilized a chest X-ray image dataset comprising 19,003 images containing both normal cases and six lung diseases: COVID-19, Viral Pneumonia, Bacterial Pneumonia, Middle East Respiratory Syndrome (MERS), Severe Acute Respiratory Syndrome (SARS), and Tuberculosis. Each ViT model (ViT, FastViT, and CrossViT) was individually trained with each optimization method (Adam, AdamW, NAdam, RAdam, SGDW, and Momentum) to assess their performance in lung disease prediction. Results When tested with ViT on the dataset with balanced-sample sized classes, RAdam demonstrated superior accuracy compared to other optimizers, achieving 95.87%. In the dataset with imbalanced sample size, FastViT with NAdam achieved the best performance with an accuracy of 97.63%. Conclusions We provide comprehensive optimization strategies for developing ViT-based model architectures, which can enhance the performance of these models for lung disease prediction from chest X-ray images. Supplementary Information The online version contains supplementary material available at 10.1186/s12911-024-02591-3.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/fec86abbb29b559c1eaff31428f5b59f8070bb67.pdf",
        "venue": "BMC Medical Informatics Decis. Mak.",
        "citationCount": 17,
        "score": 17.0,
        "summary": "Here's a focused summary of the paper by Ko et al. \\cite{ko2024eax} for a literature review:\n\n### Technical Paper Analysis: Optimization of Vision Transformer-based Detection of Lung Diseases from Chest X-ray Images \\cite{ko2024eax}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: While Vision Transformers (ViTs) have significantly advanced lung disease prediction from chest X-ray (CXR) images, there is a lack of systematic research comparing the effectiveness of different optimization methods when training these ViT-based models.\n    *   **Importance and Challenge**: Optimizers play a critical role in the training efficiency and final performance (accuracy, robustness) of deep learning models. Their performance is often model-dependent. Identifying optimal optimization strategies for ViT-based models in medical image analysis, especially with potentially imbalanced datasets common in clinical settings, is crucial for maximizing diagnostic accuracy and reliability.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon the success of deep learning, particularly Convolutional Neural Networks (CNNs) and more recently ViTs, in medical image analysis. ViTs, with their self-attention mechanisms, have shown superior performance over CNNs in various computer vision tasks, including COVID-19 detection from CXRs.\n    *   **Limitations of Previous Solutions**: Previous studies have shown that optimizer performance is highly model-dependent, with different optimizers (e.g., RMSProp, Adam, AdaGrad) yielding varying results for CNNs. While some optimizers (RAdam, Adam with cosine decay, NovoGrad, AdaBelief) have been applied to ViTs, a comprehensive, systematic comparison of a broad range of optimization methods specifically for ViT models in chest X-ray image classification for lung diseases was lacking.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The study systematically evaluates and compares six prominent optimization methods (Adam, AdamW, NAdam, RAdam, SGDW, and Momentum) across three different Vision Transformer architectures (ViT, FastViT, and CrossViT) for multi-class classification of lung diseases from chest X-ray images.\n    *   **Novelty/Difference**: The primary innovation lies in its comprehensive, comparative analysis of optimizers for ViT-based models in a specific medical imaging domain. It investigates performance not only across different ViT variants but also under conditions of both balanced and imbalanced class distributions within the dataset, providing practical guidance for model development.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: While not introducing new optimizers or ViT architectures, the paper provides a novel empirical comparison and characterization of existing optimizers' performance within the context of ViT-based lung disease detection.\n    *   **System Design/Architectural Innovations**: The study systematically tests combinations of three ViT architectures (ViT-B/16, FastViT, CrossViT) with six optimizers, evaluating their performance across different learning rates and dataset balancing scenarios.\n    *   **Theoretical Insights/Analysis**: The findings suggest that Adam-based optimizers generally outperform traditional SGD-based methods (SGDW, Momentum) for ViT models in this domain, likely due to their adaptive momentum algorithms. It also highlights the specific strengths of RAdam for balanced datasets and NAdam (especially with FastViT) for imbalanced datasets, linking these to their underlying mechanisms (e.g., RAdam's variance rectification, NAdam's Nesterov accelerated gradient).\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Training of ViT, FastViT, and CrossViT models using six optimizers (Adam, AdamW, NAdam, RAdam, SGDW, Momentum) at three learning rates (10^-4, 10^-5, 10^-6).\n        *   Evaluation on a publicly available chest X-ray dataset of 19,003 images, comprising normal cases and six lung diseases (COVID-19, Viral Pneumonia, Bacterial Pneumonia, MERS, SARS, Tuberculosis).\n        *   Performance assessed on both a 4-class balanced subset (Normal, COVID-19, Viral Pneumonia, Bacterial Pneumonia) and the full 7-class imbalanced dataset.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Metrics**: Accuracy, F1-score, Precision, Recall.\n        *   **ViT (balanced 4-class dataset)**: RAdam achieved the highest accuracy (95.87% at LR 10^-5). Adam achieved the highest F1-score (94.71% at LR 10^-5). Adam-based optimizers consistently outperformed SGDW and Momentum.\n        *   **ViT (imbalanced 7-class dataset)**: RAdam showed the highest accuracy (96.61% at LR 10^-5) and F1-score (96.62%).\n        *   **FastViT (imbalanced 7-class dataset)**: NAdam achieved the best overall performance with 97.63% accuracy and 97.64% F1-score (at LR 10^-4), demonstrating robustness against sample imbalance.\n        *   **CrossViT (imbalanced 7-class dataset)**: AdamW achieved the best performance with 96.95% accuracy and 96.94% F1-score (at LR 10^-5).\n        *   **Class-specific prediction**: Normal and Tuberculosis classes were generally better predicted than others.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The models struggled with detecting small-sized classes (e.g., MERS, SARS) due to limited sample sizes. The study exclusively focused on transformer models (ViT, FastViT, CrossViT) and a specific set of optimizers.\n    *   **Scope of Applicability**: The findings are directly applicable to ViT-based models for lung disease detection from chest X-ray images. The insights into optimizer performance under balanced/imbalanced data conditions can inform model development in similar medical imaging classification tasks.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This study provides crucial empirical evidence and practical recommendations for selecting optimal optimizers for ViT-based models in medical image analysis, a previously under-researched area. It demonstrates that specific optimizer-model combinations (e.g., FastViT with NAdam for imbalanced data) can achieve very high diagnostic accuracies (up to 97.63%).\n    *   **Potential Impact on Future Research**: The results offer a foundational understanding for developing more robust and accurate ViT-based diagnostic tools. Future research can leverage these findings to explore other optimizers, hybrid models, or different medical imaging modalities, and to address the challenge of small-sample-sized classes through techniques like advanced data augmentation or few-shot learning, building upon the identified optimal optimization strategies \\cite{ko2024eax}.",
        "keywords": [
          "Vision Transformers (ViTs)",
          "Optimizers",
          "Lung disease detection",
          "Chest X-ray images (CXR)",
          "Medical image analysis",
          "Systematic comparative analysis",
          "Adam-based optimizers",
          "RAdam",
          "NAdam",
          "Imbalanced datasets",
          "Diagnostic accuracy",
          "Optimal optimization strategies",
          "FastViT"
        ],
        "paper_type": "this paper is an **empirical** study.\n\nhere's why:\n\n*   **abstract mentions:** \"this study aims to systematically evaluate and compare the performance of various optimization methods...\", \"this study utilized a chest x-ray image dataset...\", \"results when tested with vit on the dataset...\", \"radam demonstrated superior accuracy...\", \"fastvit with nadam achieved the best performance...\". these phrases directly indicate a data-driven study with experimental methodology and quantitative findings.\n*   **introduction discusses:** the problem of enhancing deep learning model accuracy, the use of specific models (vits, fastvit, crossvit), and the need for evaluation, all leading to the experimental setup described.\n*   the core of the paper is an experiment comparing the performance of different optimizers on a specific task (lung disease detection) using a defined dataset and models, which is the hallmark of an empirical study."
      },
      "file_name": "fec86abbb29b559c1eaff31428f5b59f8070bb67.pdf"
    },
    {
      "success": true,
      "doc_id": "35a2ece5491aea9be74ea7b05550ac2b",
      "summary": "Existing industrial image anomaly detection techniques predominantly utilize codecs based on convolutional neural networks (CNNs). However, traditional convolutional autoencoders are limited to local features, struggling to assimilate global feature information. CNNs generalizability enables the reconstruction of certain anomalous regions. This is particularly evident when normal and abnormal regions, despite having similar pixel values, contain different semantic information, leading to ineffective anomaly detection. Furthermore, collecting abnormal image samples during actual industrial production poses challenges, often resulting in data imbalance. To mitigate these issues, this study proposes an unsupervised anomaly detection model employing the Vision Transformer (ViT) architecture, incorporating a Transformer structure to understand the global context between image blocks, thereby extracting a superior representation of feature information. It integrates a memory module to catalog normal sample features, both to counteract anomaly reconstruction issues and bolster feature representation, and additionally introduces a coordinate attention (CA) mechanism to intensify focus on image features at both spatial and channel dimensions, minimizing feature information loss and thereby enabling more precise anomaly identification and localization. Experiments conducted on two public datasets, MVTec AD and BeanTech AD, substantiate the methods effectiveness, demonstrating an approximate 20% improvement in average AUROC% at the image level over traditional convolutional encoders.",
      "intriguing_abstract": "Existing industrial image anomaly detection techniques predominantly utilize codecs based on convolutional neural networks (CNNs). However, traditional convolutional autoencoders are limited to local features, struggling to assimilate global feature information. CNNs generalizability enables the reconstruction of certain anomalous regions. This is particularly evident when normal and abnormal regions, despite having similar pixel values, contain different semantic information, leading to ineffective anomaly detection. Furthermore, collecting abnormal image samples during actual industrial production poses challenges, often resulting in data imbalance. To mitigate these issues, this study proposes an unsupervised anomaly detection model employing the Vision Transformer (ViT) architecture, incorporating a Transformer structure to understand the global context between image blocks, thereby extracting a superior representation of feature information. It integrates a memory module to catalog normal sample features, both to counteract anomaly reconstruction issues and bolster feature representation, and additionally introduces a coordinate attention (CA) mechanism to intensify focus on image features at both spatial and channel dimensions, minimizing feature information loss and thereby enabling more precise anomaly identification and localization. Experiments conducted on two public datasets, MVTec AD and BeanTech AD, substantiate the methods effectiveness, demonstrating an approximate 20% improvement in average AUROC% at the image level over traditional convolutional encoders.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/cb8b0eba078098000f004d7e0f97a33189261f30.pdf",
      "citation_key": "yang2024w08",
      "metadata": {
        "title": "An Unsupervised Method for Industrial Image Anomaly Detection with Vision Transformer-Based Autoencoder",
        "authors": [
          "Qiying Yang",
          "Rongzuo Guo"
        ],
        "published_date": "2024",
        "abstract": "Existing industrial image anomaly detection techniques predominantly utilize codecs based on convolutional neural networks (CNNs). However, traditional convolutional autoencoders are limited to local features, struggling to assimilate global feature information. CNNs generalizability enables the reconstruction of certain anomalous regions. This is particularly evident when normal and abnormal regions, despite having similar pixel values, contain different semantic information, leading to ineffective anomaly detection. Furthermore, collecting abnormal image samples during actual industrial production poses challenges, often resulting in data imbalance. To mitigate these issues, this study proposes an unsupervised anomaly detection model employing the Vision Transformer (ViT) architecture, incorporating a Transformer structure to understand the global context between image blocks, thereby extracting a superior representation of feature information. It integrates a memory module to catalog normal sample features, both to counteract anomaly reconstruction issues and bolster feature representation, and additionally introduces a coordinate attention (CA) mechanism to intensify focus on image features at both spatial and channel dimensions, minimizing feature information loss and thereby enabling more precise anomaly identification and localization. Experiments conducted on two public datasets, MVTec AD and BeanTech AD, substantiate the methods effectiveness, demonstrating an approximate 20% improvement in average AUROC% at the image level over traditional convolutional encoders.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/cb8b0eba078098000f004d7e0f97a33189261f30.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 17,
        "score": 17.0,
        "summary": "Existing industrial image anomaly detection techniques predominantly utilize codecs based on convolutional neural networks (CNNs). However, traditional convolutional autoencoders are limited to local features, struggling to assimilate global feature information. CNNs generalizability enables the reconstruction of certain anomalous regions. This is particularly evident when normal and abnormal regions, despite having similar pixel values, contain different semantic information, leading to ineffective anomaly detection. Furthermore, collecting abnormal image samples during actual industrial production poses challenges, often resulting in data imbalance. To mitigate these issues, this study proposes an unsupervised anomaly detection model employing the Vision Transformer (ViT) architecture, incorporating a Transformer structure to understand the global context between image blocks, thereby extracting a superior representation of feature information. It integrates a memory module to catalog normal sample features, both to counteract anomaly reconstruction issues and bolster feature representation, and additionally introduces a coordinate attention (CA) mechanism to intensify focus on image features at both spatial and channel dimensions, minimizing feature information loss and thereby enabling more precise anomaly identification and localization. Experiments conducted on two public datasets, MVTec AD and BeanTech AD, substantiate the methods effectiveness, demonstrating an approximate 20% improvement in average AUROC% at the image level over traditional convolutional encoders.",
        "keywords": []
      },
      "file_name": "cb8b0eba078098000f004d7e0f97a33189261f30.pdf"
    },
    {
      "success": true,
      "doc_id": "6786bb7f62c21fc494f1fbee59b86a5f",
      "summary": "Diabetic Retinopathy (DR) is a result of prolonged diabetes with poor blood sugar management. It causes vision problems and blindness due to the deformation of the human retina. Recently, DR has become a crucial medical problem that affects the health and life of people. Diagnosis of DR can be done manually by ophthalmologists, but this is cumbersome and time consuming especially in the current overloaded physicians environment. The early detection and prevention of DR, a severe complication of diabetes that can lead to blindness, require an automatic, accurate, and personalized machine learning-based method. Various deep learning algorithms, particularly convolutional neural networks (CNNs), have been investigated for detecting different stages of DR. Recently, transformers have proved their capabilities in natural language processing. Vision transformers (ViTs) are extensions of these models to capture long-range dependencies in images, which achieved better results than CNN models. However, ViT always needs huge datasets to learn properly, and this condition reduced its applicability in DR domain. Recently, a new real-world and large fundus image dataset called fine-grained annotated diabetic retinopathy (FGADR) has been released which supported the application of ViT in DR diagnosis domain. The literature has not explored FGADR to optimize ViT models. In this paper, we propose a novel ViT based deep learning pipeline for detecting the severity stages of DR based on fundus photography-based retina images. The model has been built using FGADR dataset. The model has been optimized using a new optimizer called AdamW to detect the global context of images. Because FGADR is an imbalanced dataset, we combine several techniques for handling this issue including the usage of F1-score as the optimization metric, data augmentation, class weights, label smoothing, and focal loss. Extensive experiments have been conducted to explore the role of ViT with different data balancing techniques to detect DR. In addition, the proposed model has been compared with the state-of-the-art CNN algorithms such as ResNet50, Incep-tionV3, and VGG19. The adopted model was able to capture the crucial features of retinal images to understand DR severity better. It achieved superior results compared to other CNN and baseline ViT models (i.e., 0.825, 0.825, 0.826, 0.964, 0.825, 0.825, and 0.956 for F1-score, accuracy, balanced accuracy, AUC, precision, recall, specificity, respectively). The results of the proposed ViT model were quite encouraging to be applied in real medical environment for assisting physicians to make accurate, personalized, and timely decisions.",
      "intriguing_abstract": "Diabetic Retinopathy (DR) is a result of prolonged diabetes with poor blood sugar management. It causes vision problems and blindness due to the deformation of the human retina. Recently, DR has become a crucial medical problem that affects the health and life of people. Diagnosis of DR can be done manually by ophthalmologists, but this is cumbersome and time consuming especially in the current overloaded physicians environment. The early detection and prevention of DR, a severe complication of diabetes that can lead to blindness, require an automatic, accurate, and personalized machine learning-based method. Various deep learning algorithms, particularly convolutional neural networks (CNNs), have been investigated for detecting different stages of DR. Recently, transformers have proved their capabilities in natural language processing. Vision transformers (ViTs) are extensions of these models to capture long-range dependencies in images, which achieved better results than CNN models. However, ViT always needs huge datasets to learn properly, and this condition reduced its applicability in DR domain. Recently, a new real-world and large fundus image dataset called fine-grained annotated diabetic retinopathy (FGADR) has been released which supported the application of ViT in DR diagnosis domain. The literature has not explored FGADR to optimize ViT models. In this paper, we propose a novel ViT based deep learning pipeline for detecting the severity stages of DR based on fundus photography-based retina images. The model has been built using FGADR dataset. The model has been optimized using a new optimizer called AdamW to detect the global context of images. Because FGADR is an imbalanced dataset, we combine several techniques for handling this issue including the usage of F1-score as the optimization metric, data augmentation, class weights, label smoothing, and focal loss. Extensive experiments have been conducted to explore the role of ViT with different data balancing techniques to detect DR. In addition, the proposed model has been compared with the state-of-the-art CNN algorithms such as ResNet50, Incep-tionV3, and VGG19. The adopted model was able to capture the crucial features of retinal images to understand DR severity better. It achieved superior results compared to other CNN and baseline ViT models (i.e., 0.825, 0.825, 0.826, 0.964, 0.825, 0.825, and 0.956 for F1-score, accuracy, balanced accuracy, AUC, precision, recall, specificity, respectively). The results of the proposed ViT model were quite encouraging to be applied in real medical environment for assisting physicians to make accurate, personalized, and timely decisions.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/9b4d81736637392adabe688b6a698cec58f9ce57.pdf",
      "citation_key": "nazih20238nf",
      "metadata": {
        "title": "Vision Transformer Model for Predicting the Severity of Diabetic Retinopathy in Fundus Photography-Based Retina Images",
        "authors": [
          "Waleed Nazih",
          "Ahmad O. Aseeri",
          "Osama Youssef Atallah",
          "Shaker El-Sappagh"
        ],
        "published_date": "2023",
        "abstract": "Diabetic Retinopathy (DR) is a result of prolonged diabetes with poor blood sugar management. It causes vision problems and blindness due to the deformation of the human retina. Recently, DR has become a crucial medical problem that affects the health and life of people. Diagnosis of DR can be done manually by ophthalmologists, but this is cumbersome and time consuming especially in the current overloaded physicians environment. The early detection and prevention of DR, a severe complication of diabetes that can lead to blindness, require an automatic, accurate, and personalized machine learning-based method. Various deep learning algorithms, particularly convolutional neural networks (CNNs), have been investigated for detecting different stages of DR. Recently, transformers have proved their capabilities in natural language processing. Vision transformers (ViTs) are extensions of these models to capture long-range dependencies in images, which achieved better results than CNN models. However, ViT always needs huge datasets to learn properly, and this condition reduced its applicability in DR domain. Recently, a new real-world and large fundus image dataset called fine-grained annotated diabetic retinopathy (FGADR) has been released which supported the application of ViT in DR diagnosis domain. The literature has not explored FGADR to optimize ViT models. In this paper, we propose a novel ViT based deep learning pipeline for detecting the severity stages of DR based on fundus photography-based retina images. The model has been built using FGADR dataset. The model has been optimized using a new optimizer called AdamW to detect the global context of images. Because FGADR is an imbalanced dataset, we combine several techniques for handling this issue including the usage of F1-score as the optimization metric, data augmentation, class weights, label smoothing, and focal loss. Extensive experiments have been conducted to explore the role of ViT with different data balancing techniques to detect DR. In addition, the proposed model has been compared with the state-of-the-art CNN algorithms such as ResNet50, Incep-tionV3, and VGG19. The adopted model was able to capture the crucial features of retinal images to understand DR severity better. It achieved superior results compared to other CNN and baseline ViT models (i.e., 0.825, 0.825, 0.826, 0.964, 0.825, 0.825, and 0.956 for F1-score, accuracy, balanced accuracy, AUC, precision, recall, specificity, respectively). The results of the proposed ViT model were quite encouraging to be applied in real medical environment for assisting physicians to make accurate, personalized, and timely decisions.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9b4d81736637392adabe688b6a698cec58f9ce57.pdf",
        "venue": "IEEE Access",
        "citationCount": 34,
        "score": 17.0,
        "summary": "Diabetic Retinopathy (DR) is a result of prolonged diabetes with poor blood sugar management. It causes vision problems and blindness due to the deformation of the human retina. Recently, DR has become a crucial medical problem that affects the health and life of people. Diagnosis of DR can be done manually by ophthalmologists, but this is cumbersome and time consuming especially in the current overloaded physicians environment. The early detection and prevention of DR, a severe complication of diabetes that can lead to blindness, require an automatic, accurate, and personalized machine learning-based method. Various deep learning algorithms, particularly convolutional neural networks (CNNs), have been investigated for detecting different stages of DR. Recently, transformers have proved their capabilities in natural language processing. Vision transformers (ViTs) are extensions of these models to capture long-range dependencies in images, which achieved better results than CNN models. However, ViT always needs huge datasets to learn properly, and this condition reduced its applicability in DR domain. Recently, a new real-world and large fundus image dataset called fine-grained annotated diabetic retinopathy (FGADR) has been released which supported the application of ViT in DR diagnosis domain. The literature has not explored FGADR to optimize ViT models. In this paper, we propose a novel ViT based deep learning pipeline for detecting the severity stages of DR based on fundus photography-based retina images. The model has been built using FGADR dataset. The model has been optimized using a new optimizer called AdamW to detect the global context of images. Because FGADR is an imbalanced dataset, we combine several techniques for handling this issue including the usage of F1-score as the optimization metric, data augmentation, class weights, label smoothing, and focal loss. Extensive experiments have been conducted to explore the role of ViT with different data balancing techniques to detect DR. In addition, the proposed model has been compared with the state-of-the-art CNN algorithms such as ResNet50, Incep-tionV3, and VGG19. The adopted model was able to capture the crucial features of retinal images to understand DR severity better. It achieved superior results compared to other CNN and baseline ViT models (i.e., 0.825, 0.825, 0.826, 0.964, 0.825, 0.825, and 0.956 for F1-score, accuracy, balanced accuracy, AUC, precision, recall, specificity, respectively). The results of the proposed ViT model were quite encouraging to be applied in real medical environment for assisting physicians to make accurate, personalized, and timely decisions.",
        "keywords": []
      },
      "file_name": "9b4d81736637392adabe688b6a698cec58f9ce57.pdf"
    },
    {
      "success": true,
      "doc_id": "a2e263ed6b01750fcea3ea00feb21e49",
      "summary": "Transformers have shown superior performance on various vision tasks. Their large receptive field endows Transformer models with higher representation power than their CNN counterparts. Nevertheless, simply enlarging the receptive field also raises several concerns. On the one hand, using dense attention in ViT leads to excessive memory and computational cost, and features can be influenced by irrelevant parts that are beyond the region of interests. On the other hand, the handcrafted attention adopted in PVT or Swin Transformer is data agnostic and may limit the ability to model long-range relations. To solve this dilemma, we propose a novel deformable multi-head attention module, where the positions of key and value pairs in self-attention are adaptively allocated in a data-dependent way. This flexible scheme enables the proposed deformable attention to dynamically focus on relevant regions while maintains the representation power of global attention. On this basis, we present Deformable Attention Transformer (DAT), a general vision backbone efficient and effective for visual recognition. We further build an enhanced version DAT++. Extensive experiments show that our DAT++ achieves state-of-the-art results on various visual recognition benchmarks, with 85.9% ImageNet accuracy, 54.5 and 47.0 MS-COCO instance segmentation mAP, and 51.5 ADE20K semantic segmentation mIoU.",
      "intriguing_abstract": "Transformers have shown superior performance on various vision tasks. Their large receptive field endows Transformer models with higher representation power than their CNN counterparts. Nevertheless, simply enlarging the receptive field also raises several concerns. On the one hand, using dense attention in ViT leads to excessive memory and computational cost, and features can be influenced by irrelevant parts that are beyond the region of interests. On the other hand, the handcrafted attention adopted in PVT or Swin Transformer is data agnostic and may limit the ability to model long-range relations. To solve this dilemma, we propose a novel deformable multi-head attention module, where the positions of key and value pairs in self-attention are adaptively allocated in a data-dependent way. This flexible scheme enables the proposed deformable attention to dynamically focus on relevant regions while maintains the representation power of global attention. On this basis, we present Deformable Attention Transformer (DAT), a general vision backbone efficient and effective for visual recognition. We further build an enhanced version DAT++. Extensive experiments show that our DAT++ achieves state-of-the-art results on various visual recognition benchmarks, with 85.9% ImageNet accuracy, 54.5 and 47.0 MS-COCO instance segmentation mAP, and 51.5 ADE20K semantic segmentation mIoU.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/981970d0f586761e7cdd978670c6a8f46990f514.pdf",
      "citation_key": "xia2023bp7",
      "metadata": {
        "title": "DAT++: Spatially Dynamic Vision Transformer with Deformable Attention",
        "authors": [
          "Zhuofan Xia",
          "Xuran Pan",
          "Shiji Song",
          "Li Erran Li",
          "Gao Huang"
        ],
        "published_date": "2023",
        "abstract": "Transformers have shown superior performance on various vision tasks. Their large receptive field endows Transformer models with higher representation power than their CNN counterparts. Nevertheless, simply enlarging the receptive field also raises several concerns. On the one hand, using dense attention in ViT leads to excessive memory and computational cost, and features can be influenced by irrelevant parts that are beyond the region of interests. On the other hand, the handcrafted attention adopted in PVT or Swin Transformer is data agnostic and may limit the ability to model long-range relations. To solve this dilemma, we propose a novel deformable multi-head attention module, where the positions of key and value pairs in self-attention are adaptively allocated in a data-dependent way. This flexible scheme enables the proposed deformable attention to dynamically focus on relevant regions while maintains the representation power of global attention. On this basis, we present Deformable Attention Transformer (DAT), a general vision backbone efficient and effective for visual recognition. We further build an enhanced version DAT++. Extensive experiments show that our DAT++ achieves state-of-the-art results on various visual recognition benchmarks, with 85.9% ImageNet accuracy, 54.5 and 47.0 MS-COCO instance segmentation mAP, and 51.5 ADE20K semantic segmentation mIoU.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/981970d0f586761e7cdd978670c6a8f46990f514.pdf",
        "venue": "arXiv.org",
        "citationCount": 34,
        "score": 17.0,
        "summary": "Transformers have shown superior performance on various vision tasks. Their large receptive field endows Transformer models with higher representation power than their CNN counterparts. Nevertheless, simply enlarging the receptive field also raises several concerns. On the one hand, using dense attention in ViT leads to excessive memory and computational cost, and features can be influenced by irrelevant parts that are beyond the region of interests. On the other hand, the handcrafted attention adopted in PVT or Swin Transformer is data agnostic and may limit the ability to model long-range relations. To solve this dilemma, we propose a novel deformable multi-head attention module, where the positions of key and value pairs in self-attention are adaptively allocated in a data-dependent way. This flexible scheme enables the proposed deformable attention to dynamically focus on relevant regions while maintains the representation power of global attention. On this basis, we present Deformable Attention Transformer (DAT), a general vision backbone efficient and effective for visual recognition. We further build an enhanced version DAT++. Extensive experiments show that our DAT++ achieves state-of-the-art results on various visual recognition benchmarks, with 85.9% ImageNet accuracy, 54.5 and 47.0 MS-COCO instance segmentation mAP, and 51.5 ADE20K semantic segmentation mIoU.",
        "keywords": []
      },
      "file_name": "981970d0f586761e7cdd978670c6a8f46990f514.pdf"
    },
    {
      "success": true,
      "doc_id": "d59c17edc477b16de397b34daed935d2",
      "summary": "Vision Transformer models, such as ViT, Swin Transformer, and Transformer-in-Transformer, have recently gained significant traction in computer vision tasks due to their ability to capture the global relation between features which leads to superior performance. However, they are compute-heavy and difficult to deploy in resource-constrained edge devices. Existing hardware accelerators, including those for the closely-related BERT transformer models, do not target highly resource-constrained environments. In this paper, we address this gap and propose ViTA - a configurable hardware accelerator for inference of vision transformer models, targeting resource-constrained edge computing devices and avoiding repeated off-chip memory accesses. We employ a head-level pipeline and inter-layer MLP optimizations, and can support several commonly used vision transformer models with changes solely in our control logic. We achieve nearly 90% hardware utilization efficiency on most vision transformer models, report a power of 0.88W when synthesised with a clock of 150 MHz, and get reasonable frame rates - all of which makes ViTA suitable for edge applications.",
      "intriguing_abstract": "Vision Transformer models, such as ViT, Swin Transformer, and Transformer-in-Transformer, have recently gained significant traction in computer vision tasks due to their ability to capture the global relation between features which leads to superior performance. However, they are compute-heavy and difficult to deploy in resource-constrained edge devices. Existing hardware accelerators, including those for the closely-related BERT transformer models, do not target highly resource-constrained environments. In this paper, we address this gap and propose ViTA - a configurable hardware accelerator for inference of vision transformer models, targeting resource-constrained edge computing devices and avoiding repeated off-chip memory accesses. We employ a head-level pipeline and inter-layer MLP optimizations, and can support several commonly used vision transformer models with changes solely in our control logic. We achieve nearly 90% hardware utilization efficiency on most vision transformer models, report a power of 0.88W when synthesised with a clock of 150 MHz, and get reasonable frame rates - all of which makes ViTA suitable for edge applications.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/6bdafb965e94c5240db2c30f20c37c4b4dd0e451.pdf",
      "citation_key": "nag2023cfn",
      "metadata": {
        "title": "ViTA: A Vision Transformer Inference Accelerator for Edge Applications",
        "authors": [
          "Shashank Nag",
          "G. Datta",
          "Souvik Kundu",
          "N. Chandrachoodan",
          "P. Beerel"
        ],
        "published_date": "2023",
        "abstract": "Vision Transformer models, such as ViT, Swin Transformer, and Transformer-in-Transformer, have recently gained significant traction in computer vision tasks due to their ability to capture the global relation between features which leads to superior performance. However, they are compute-heavy and difficult to deploy in resource-constrained edge devices. Existing hardware accelerators, including those for the closely-related BERT transformer models, do not target highly resource-constrained environments. In this paper, we address this gap and propose ViTA - a configurable hardware accelerator for inference of vision transformer models, targeting resource-constrained edge computing devices and avoiding repeated off-chip memory accesses. We employ a head-level pipeline and inter-layer MLP optimizations, and can support several commonly used vision transformer models with changes solely in our control logic. We achieve nearly 90% hardware utilization efficiency on most vision transformer models, report a power of 0.88W when synthesised with a clock of 150 MHz, and get reasonable frame rates - all of which makes ViTA suitable for edge applications.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6bdafb965e94c5240db2c30f20c37c4b4dd0e451.pdf",
        "venue": "International Symposium on Circuits and Systems",
        "citationCount": 34,
        "score": 17.0,
        "summary": "Vision Transformer models, such as ViT, Swin Transformer, and Transformer-in-Transformer, have recently gained significant traction in computer vision tasks due to their ability to capture the global relation between features which leads to superior performance. However, they are compute-heavy and difficult to deploy in resource-constrained edge devices. Existing hardware accelerators, including those for the closely-related BERT transformer models, do not target highly resource-constrained environments. In this paper, we address this gap and propose ViTA - a configurable hardware accelerator for inference of vision transformer models, targeting resource-constrained edge computing devices and avoiding repeated off-chip memory accesses. We employ a head-level pipeline and inter-layer MLP optimizations, and can support several commonly used vision transformer models with changes solely in our control logic. We achieve nearly 90% hardware utilization efficiency on most vision transformer models, report a power of 0.88W when synthesised with a clock of 150 MHz, and get reasonable frame rates - all of which makes ViTA suitable for edge applications.",
        "keywords": []
      },
      "file_name": "6bdafb965e94c5240db2c30f20c37c4b4dd0e451.pdf"
    },
    {
      "success": true,
      "doc_id": "0fd3001414210022496e3bb96f3d4d82",
      "summary": "The field of algorithmic trading, driven by deep learning methodologies, has garnered substantial attention in recent times. Within this domain, transformers, convolutional neural networks, and patch embedding-based techniques have emerged as popular choices within the computer vision community. Here, inspired by the latest cutting-edge computer vision methodologies and the existing work showing the capability of image-like conversion for time-series datasets, we apply more advanced transformer-based and patch-based approaches for predicting asset prices and directional price movements. The employed transformer models include Vision Transformer (ViT), Data Efficient Image Transformers (DeiT), and Swin. We use ConvMixer for a patch embedding-based convolutional neural network architecture without a transformer. Our tested transformer-based and patch-based methodologies aim to predict asset prices and directional movements using historical price data by leveraging the inherent image-like properties within the historical time-series dataset. Before the implementation of attention-based architectures, the historical time series price dataset is transformed into two-dimensional images. This transformation is facilitated through the incorporation of various common technical financial indicators, each contributing to the data for a fixed number of consecutive days. Consequently, a diverse set of two-dimensional images is constructed, reflecting various dimensions of the dataset. Subsequently, the original images depicting market valleys and peaks are annotated with labels such as Hold, Buy, or Sell. According to the experiments, trained attention-based models consistently outperform the baseline convolutional architectures, particularly when applied to a subset of frequently traded Exchange-Traded Funds (ETFs). This better performance of attention-based architectures, especially ViT, is evident in terms of both accuracy and other financial evaluation metrics, particularly during extended testing and holding periods. These findings underscore the potential of transformer-based approaches to enhance predictive capabilities in asset price and directional forecasting. Our code and processed datasets are available at https://github.com/seferlab/price_transformer.",
      "intriguing_abstract": "The field of algorithmic trading, driven by deep learning methodologies, has garnered substantial attention in recent times. Within this domain, transformers, convolutional neural networks, and patch embedding-based techniques have emerged as popular choices within the computer vision community. Here, inspired by the latest cutting-edge computer vision methodologies and the existing work showing the capability of image-like conversion for time-series datasets, we apply more advanced transformer-based and patch-based approaches for predicting asset prices and directional price movements. The employed transformer models include Vision Transformer (ViT), Data Efficient Image Transformers (DeiT), and Swin. We use ConvMixer for a patch embedding-based convolutional neural network architecture without a transformer. Our tested transformer-based and patch-based methodologies aim to predict asset prices and directional movements using historical price data by leveraging the inherent image-like properties within the historical time-series dataset. Before the implementation of attention-based architectures, the historical time series price dataset is transformed into two-dimensional images. This transformation is facilitated through the incorporation of various common technical financial indicators, each contributing to the data for a fixed number of consecutive days. Consequently, a diverse set of two-dimensional images is constructed, reflecting various dimensions of the dataset. Subsequently, the original images depicting market valleys and peaks are annotated with labels such as Hold, Buy, or Sell. According to the experiments, trained attention-based models consistently outperform the baseline convolutional architectures, particularly when applied to a subset of frequently traded Exchange-Traded Funds (ETFs). This better performance of attention-based architectures, especially ViT, is evident in terms of both accuracy and other financial evaluation metrics, particularly during extended testing and holding periods. These findings underscore the potential of transformer-based approaches to enhance predictive capabilities in asset price and directional forecasting. Our code and processed datasets are available at https://github.com/seferlab/price_transformer.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/a3d1cebf99262cc20d22863b9540769b49a15ede.pdf",
      "citation_key": "gezici20246lf",
      "metadata": {
        "title": "Deep Transformer-Based Asset Price and Direction Prediction",
        "authors": [
          "Abdul Haluk Batur Gezici",
          "Emre Sefer"
        ],
        "published_date": "2024",
        "abstract": "The field of algorithmic trading, driven by deep learning methodologies, has garnered substantial attention in recent times. Within this domain, transformers, convolutional neural networks, and patch embedding-based techniques have emerged as popular choices within the computer vision community. Here, inspired by the latest cutting-edge computer vision methodologies and the existing work showing the capability of image-like conversion for time-series datasets, we apply more advanced transformer-based and patch-based approaches for predicting asset prices and directional price movements. The employed transformer models include Vision Transformer (ViT), Data Efficient Image Transformers (DeiT), and Swin. We use ConvMixer for a patch embedding-based convolutional neural network architecture without a transformer. Our tested transformer-based and patch-based methodologies aim to predict asset prices and directional movements using historical price data by leveraging the inherent image-like properties within the historical time-series dataset. Before the implementation of attention-based architectures, the historical time series price dataset is transformed into two-dimensional images. This transformation is facilitated through the incorporation of various common technical financial indicators, each contributing to the data for a fixed number of consecutive days. Consequently, a diverse set of two-dimensional images is constructed, reflecting various dimensions of the dataset. Subsequently, the original images depicting market valleys and peaks are annotated with labels such as Hold, Buy, or Sell. According to the experiments, trained attention-based models consistently outperform the baseline convolutional architectures, particularly when applied to a subset of frequently traded Exchange-Traded Funds (ETFs). This better performance of attention-based architectures, especially ViT, is evident in terms of both accuracy and other financial evaluation metrics, particularly during extended testing and holding periods. These findings underscore the potential of transformer-based approaches to enhance predictive capabilities in asset price and directional forecasting. Our code and processed datasets are available at https://github.com/seferlab/price_transformer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a3d1cebf99262cc20d22863b9540769b49a15ede.pdf",
        "venue": "IEEE Access",
        "citationCount": 16,
        "score": 16.0,
        "summary": "The field of algorithmic trading, driven by deep learning methodologies, has garnered substantial attention in recent times. Within this domain, transformers, convolutional neural networks, and patch embedding-based techniques have emerged as popular choices within the computer vision community. Here, inspired by the latest cutting-edge computer vision methodologies and the existing work showing the capability of image-like conversion for time-series datasets, we apply more advanced transformer-based and patch-based approaches for predicting asset prices and directional price movements. The employed transformer models include Vision Transformer (ViT), Data Efficient Image Transformers (DeiT), and Swin. We use ConvMixer for a patch embedding-based convolutional neural network architecture without a transformer. Our tested transformer-based and patch-based methodologies aim to predict asset prices and directional movements using historical price data by leveraging the inherent image-like properties within the historical time-series dataset. Before the implementation of attention-based architectures, the historical time series price dataset is transformed into two-dimensional images. This transformation is facilitated through the incorporation of various common technical financial indicators, each contributing to the data for a fixed number of consecutive days. Consequently, a diverse set of two-dimensional images is constructed, reflecting various dimensions of the dataset. Subsequently, the original images depicting market valleys and peaks are annotated with labels such as Hold, Buy, or Sell. According to the experiments, trained attention-based models consistently outperform the baseline convolutional architectures, particularly when applied to a subset of frequently traded Exchange-Traded Funds (ETFs). This better performance of attention-based architectures, especially ViT, is evident in terms of both accuracy and other financial evaluation metrics, particularly during extended testing and holding periods. These findings underscore the potential of transformer-based approaches to enhance predictive capabilities in asset price and directional forecasting. Our code and processed datasets are available at https://github.com/seferlab/price_transformer.",
        "keywords": []
      },
      "file_name": "a3d1cebf99262cc20d22863b9540769b49a15ede.pdf"
    },
    {
      "success": true,
      "doc_id": "ee9c081e47a7522458bbddd54c181501",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f05176e2d3f7a6e95b60f0e96fd9bf20e8b335d2.pdf",
      "citation_key": "ghazouani202342t",
      "metadata": {
        "title": "Efficient brain tumor segmentation using Swin transformer and enhanced local self-attention",
        "authors": [
          "Fethi Ghazouani",
          "Pierre Vera",
          "Su Ruan"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f05176e2d3f7a6e95b60f0e96fd9bf20e8b335d2.pdf",
        "venue": "International Journal of Computer Assisted Radiology and Surgery",
        "citationCount": 30,
        "score": 15.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "f05176e2d3f7a6e95b60f0e96fd9bf20e8b335d2.pdf"
    },
    {
      "success": true,
      "doc_id": "1981dcfd88e4cb14b3e057fd63dab19d",
      "summary": "Remote sensing image classification (RSIC) is a classical and fundamental task in the intelligent interpretation of remote sensing imagery, which can provide unique labeling information for each acquired remote sensing image. Thanks to the potent global context information extraction ability of the multi-head self-attention (MSA) mechanism, visual transformer (ViT)-based architectures have shown excellent capability in natural scene image classification. However, in order to achieve powerful RSIC performance, it is insufficient to capture global spatial information alone. Specifically, for fine-grained target recognition tasks with high inter-class similarity, discriminative and effective local feature representations are key to correct classification. In addition, due to the lack of inductive biases, the powerful global spatial context representation capability of ViT requires lengthy training procedures and large-scale pre-training data volume. To solve the above problems, a hybrid architecture of convolution neural network (CNN) and ViT is proposed to improve the RSIC ability, called P2FEViT, which integrates plug-and-play CNN features with ViT. In this paper, the feature representation capabilities of CNN and ViT applying for RSIC are first analyzed. Second, aiming to integrate the advantages of CNN and ViT, a novel approach embedding CNN features into the ViT architecture is proposed, which can make the model synchronously capture and fuse global context and local multimodal information to further improve the classification capability of ViT. Third, based on the hybrid structure, only a simple cross-entropy loss is employed for model training. The model can also have rapid and comfortable convergence with relatively less training data than the original ViT. Finally, extensive experiments are conducted on the public and challenging remote sensing scene classification dataset of NWPU-RESISC45 (NWPU-R45) and the self-built fine-grained target classification dataset called BIT-AFGR50. The experimental results demonstrate that the proposed P2FEViT can effectively improve the feature description capability and obtain outstanding image classification performance, while significantly reducing the high dependence of ViT on large-scale pre-training data volume and accelerating the convergence speed. The code and self-built dataset will be released at our webpages.",
      "intriguing_abstract": "Remote sensing image classification (RSIC) is a classical and fundamental task in the intelligent interpretation of remote sensing imagery, which can provide unique labeling information for each acquired remote sensing image. Thanks to the potent global context information extraction ability of the multi-head self-attention (MSA) mechanism, visual transformer (ViT)-based architectures have shown excellent capability in natural scene image classification. However, in order to achieve powerful RSIC performance, it is insufficient to capture global spatial information alone. Specifically, for fine-grained target recognition tasks with high inter-class similarity, discriminative and effective local feature representations are key to correct classification. In addition, due to the lack of inductive biases, the powerful global spatial context representation capability of ViT requires lengthy training procedures and large-scale pre-training data volume. To solve the above problems, a hybrid architecture of convolution neural network (CNN) and ViT is proposed to improve the RSIC ability, called P2FEViT, which integrates plug-and-play CNN features with ViT. In this paper, the feature representation capabilities of CNN and ViT applying for RSIC are first analyzed. Second, aiming to integrate the advantages of CNN and ViT, a novel approach embedding CNN features into the ViT architecture is proposed, which can make the model synchronously capture and fuse global context and local multimodal information to further improve the classification capability of ViT. Third, based on the hybrid structure, only a simple cross-entropy loss is employed for model training. The model can also have rapid and comfortable convergence with relatively less training data than the original ViT. Finally, extensive experiments are conducted on the public and challenging remote sensing scene classification dataset of NWPU-RESISC45 (NWPU-R45) and the self-built fine-grained target classification dataset called BIT-AFGR50. The experimental results demonstrate that the proposed P2FEViT can effectively improve the feature description capability and obtain outstanding image classification performance, while significantly reducing the high dependence of ViT on large-scale pre-training data volume and accelerating the convergence speed. The code and self-built dataset will be released at our webpages.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/442b5ec3aad4b099e71d6203a62eb7ec7519544c.pdf",
      "citation_key": "wang202338i",
      "metadata": {
        "title": "P2FEViT: Plug-and-Play CNN Feature Embedded Hybrid Vision Transformer for Remote Sensing Image Classification",
        "authors": [
          "Guanqun Wang",
          "He Chen",
          "Liang Chen",
          "Yin Zhuang",
          "Shanghang Zhang",
          "T. Zhang",
          "Hao Dong",
          "Peng Gao"
        ],
        "published_date": "2023",
        "abstract": "Remote sensing image classification (RSIC) is a classical and fundamental task in the intelligent interpretation of remote sensing imagery, which can provide unique labeling information for each acquired remote sensing image. Thanks to the potent global context information extraction ability of the multi-head self-attention (MSA) mechanism, visual transformer (ViT)-based architectures have shown excellent capability in natural scene image classification. However, in order to achieve powerful RSIC performance, it is insufficient to capture global spatial information alone. Specifically, for fine-grained target recognition tasks with high inter-class similarity, discriminative and effective local feature representations are key to correct classification. In addition, due to the lack of inductive biases, the powerful global spatial context representation capability of ViT requires lengthy training procedures and large-scale pre-training data volume. To solve the above problems, a hybrid architecture of convolution neural network (CNN) and ViT is proposed to improve the RSIC ability, called P2FEViT, which integrates plug-and-play CNN features with ViT. In this paper, the feature representation capabilities of CNN and ViT applying for RSIC are first analyzed. Second, aiming to integrate the advantages of CNN and ViT, a novel approach embedding CNN features into the ViT architecture is proposed, which can make the model synchronously capture and fuse global context and local multimodal information to further improve the classification capability of ViT. Third, based on the hybrid structure, only a simple cross-entropy loss is employed for model training. The model can also have rapid and comfortable convergence with relatively less training data than the original ViT. Finally, extensive experiments are conducted on the public and challenging remote sensing scene classification dataset of NWPU-RESISC45 (NWPU-R45) and the self-built fine-grained target classification dataset called BIT-AFGR50. The experimental results demonstrate that the proposed P2FEViT can effectively improve the feature description capability and obtain outstanding image classification performance, while significantly reducing the high dependence of ViT on large-scale pre-training data volume and accelerating the convergence speed. The code and self-built dataset will be released at our webpages.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/442b5ec3aad4b099e71d6203a62eb7ec7519544c.pdf",
        "venue": "Remote Sensing",
        "citationCount": 30,
        "score": 15.0,
        "summary": "Remote sensing image classification (RSIC) is a classical and fundamental task in the intelligent interpretation of remote sensing imagery, which can provide unique labeling information for each acquired remote sensing image. Thanks to the potent global context information extraction ability of the multi-head self-attention (MSA) mechanism, visual transformer (ViT)-based architectures have shown excellent capability in natural scene image classification. However, in order to achieve powerful RSIC performance, it is insufficient to capture global spatial information alone. Specifically, for fine-grained target recognition tasks with high inter-class similarity, discriminative and effective local feature representations are key to correct classification. In addition, due to the lack of inductive biases, the powerful global spatial context representation capability of ViT requires lengthy training procedures and large-scale pre-training data volume. To solve the above problems, a hybrid architecture of convolution neural network (CNN) and ViT is proposed to improve the RSIC ability, called P2FEViT, which integrates plug-and-play CNN features with ViT. In this paper, the feature representation capabilities of CNN and ViT applying for RSIC are first analyzed. Second, aiming to integrate the advantages of CNN and ViT, a novel approach embedding CNN features into the ViT architecture is proposed, which can make the model synchronously capture and fuse global context and local multimodal information to further improve the classification capability of ViT. Third, based on the hybrid structure, only a simple cross-entropy loss is employed for model training. The model can also have rapid and comfortable convergence with relatively less training data than the original ViT. Finally, extensive experiments are conducted on the public and challenging remote sensing scene classification dataset of NWPU-RESISC45 (NWPU-R45) and the self-built fine-grained target classification dataset called BIT-AFGR50. The experimental results demonstrate that the proposed P2FEViT can effectively improve the feature description capability and obtain outstanding image classification performance, while significantly reducing the high dependence of ViT on large-scale pre-training data volume and accelerating the convergence speed. The code and self-built dataset will be released at our webpages.",
        "keywords": []
      },
      "file_name": "442b5ec3aad4b099e71d6203a62eb7ec7519544c.pdf"
    },
    {
      "success": true,
      "doc_id": "590f7424257b73424763a23ea7795c53",
      "summary": "In this letter, we propose a deep learning-based method for the direction of arrival (DOA) estimation in the low signal-to-noise ratio (SNR) scenario. Specifically, the DOA estimation is modeled as a multi-label classification task, and a novel dual class token Vision Transformer (DCT-ViT) is designed to fit it. Different from the classical ViT architecture with a single class token, the DCT-ViT includes two class tokens which are located at the beginning and end of the latent vector sequence, respectively. This architecture enables enhanced information mining and feature extraction from the array signal data in order to improve the accuracy of DOA estimation. Furthermore, a single DCT-ViT model can accommodate different source numbers by leveraging a training dataset with different numbers of sources. Simulation results illustrate that our proposed method outperforms existing methods in the low SNR scenario, including classical model-based and other deep learning-based methods.",
      "intriguing_abstract": "In this letter, we propose a deep learning-based method for the direction of arrival (DOA) estimation in the low signal-to-noise ratio (SNR) scenario. Specifically, the DOA estimation is modeled as a multi-label classification task, and a novel dual class token Vision Transformer (DCT-ViT) is designed to fit it. Different from the classical ViT architecture with a single class token, the DCT-ViT includes two class tokens which are located at the beginning and end of the latent vector sequence, respectively. This architecture enables enhanced information mining and feature extraction from the array signal data in order to improve the accuracy of DOA estimation. Furthermore, a single DCT-ViT model can accommodate different source numbers by leveraging a training dataset with different numbers of sources. Simulation results illustrate that our proposed method outperforms existing methods in the low SNR scenario, including classical model-based and other deep learning-based methods.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/635675452852e838644516e1eeefd1aaa8c8ac07.pdf",
      "citation_key": "guo2024tr7",
      "metadata": {
        "title": "Dual Class Token Vision Transformer for Direction of Arrival Estimation in Low SNR",
        "authors": [
          "Yu Guo",
          "Zhi Zhang",
          "Yuzhen Huang"
        ],
        "published_date": "2024",
        "abstract": "In this letter, we propose a deep learning-based method for the direction of arrival (DOA) estimation in the low signal-to-noise ratio (SNR) scenario. Specifically, the DOA estimation is modeled as a multi-label classification task, and a novel dual class token Vision Transformer (DCT-ViT) is designed to fit it. Different from the classical ViT architecture with a single class token, the DCT-ViT includes two class tokens which are located at the beginning and end of the latent vector sequence, respectively. This architecture enables enhanced information mining and feature extraction from the array signal data in order to improve the accuracy of DOA estimation. Furthermore, a single DCT-ViT model can accommodate different source numbers by leveraging a training dataset with different numbers of sources. Simulation results illustrate that our proposed method outperforms existing methods in the low SNR scenario, including classical model-based and other deep learning-based methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/635675452852e838644516e1eeefd1aaa8c8ac07.pdf",
        "venue": "IEEE Signal Processing Letters",
        "citationCount": 15,
        "score": 15.0,
        "summary": "In this letter, we propose a deep learning-based method for the direction of arrival (DOA) estimation in the low signal-to-noise ratio (SNR) scenario. Specifically, the DOA estimation is modeled as a multi-label classification task, and a novel dual class token Vision Transformer (DCT-ViT) is designed to fit it. Different from the classical ViT architecture with a single class token, the DCT-ViT includes two class tokens which are located at the beginning and end of the latent vector sequence, respectively. This architecture enables enhanced information mining and feature extraction from the array signal data in order to improve the accuracy of DOA estimation. Furthermore, a single DCT-ViT model can accommodate different source numbers by leveraging a training dataset with different numbers of sources. Simulation results illustrate that our proposed method outperforms existing methods in the low SNR scenario, including classical model-based and other deep learning-based methods.",
        "keywords": []
      },
      "file_name": "635675452852e838644516e1eeefd1aaa8c8ac07.pdf"
    },
    {
      "success": true,
      "doc_id": "19e301eab70046ed25317b5a5a71737d",
      "summary": "Automated classification of gastrointestinal endoscope images can help reduce the workload of doctors and improve the accuracy of diagnoses. The rapidly developed vision Transformer, represented by Swin Transformer, has become an impressive technique for medical image classification. However, Swin Transformer cannot capture the long-range dependency well in complex gastrointestinal endoscopy images. As a result, it fails to represent features of some widely-spread targets in digestive tract images, such as normal-z-line and esophagitis, effectively. To solve this problem, we propose a novel vision Transformer model based on hybrid shifted windows for digestive tract image classification, which can obtain both short-range and long-range dependency concurrently. Extensive experiments demonstrate the superiority of our method to the state-of-the-art methods with a classification accuracy of 95.42% on the Kvasir v2 dataset and a classification accuracy of 86.81% on the HyperKvasir dataset.",
      "intriguing_abstract": "Automated classification of gastrointestinal endoscope images can help reduce the workload of doctors and improve the accuracy of diagnoses. The rapidly developed vision Transformer, represented by Swin Transformer, has become an impressive technique for medical image classification. However, Swin Transformer cannot capture the long-range dependency well in complex gastrointestinal endoscopy images. As a result, it fails to represent features of some widely-spread targets in digestive tract images, such as normal-z-line and esophagitis, effectively. To solve this problem, we propose a novel vision Transformer model based on hybrid shifted windows for digestive tract image classification, which can obtain both short-range and long-range dependency concurrently. Extensive experiments demonstrate the superiority of our method to the state-of-the-art methods with a classification accuracy of 95.42% on the Kvasir v2 dataset and a classification accuracy of 86.81% on the HyperKvasir dataset.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d2fce7480111d66a74caa801a236f71ab021c42c.pdf",
      "citation_key": "wang2023ski",
      "metadata": {
        "title": "Vision Transformer With Hybrid Shifted Windows for Gastrointestinal Endoscopy Image Classification",
        "authors": [
          "Wei Wang",
          "Xin Yang",
          "Jinhui Tang"
        ],
        "published_date": "2023",
        "abstract": "Automated classification of gastrointestinal endoscope images can help reduce the workload of doctors and improve the accuracy of diagnoses. The rapidly developed vision Transformer, represented by Swin Transformer, has become an impressive technique for medical image classification. However, Swin Transformer cannot capture the long-range dependency well in complex gastrointestinal endoscopy images. As a result, it fails to represent features of some widely-spread targets in digestive tract images, such as normal-z-line and esophagitis, effectively. To solve this problem, we propose a novel vision Transformer model based on hybrid shifted windows for digestive tract image classification, which can obtain both short-range and long-range dependency concurrently. Extensive experiments demonstrate the superiority of our method to the state-of-the-art methods with a classification accuracy of 95.42% on the Kvasir v2 dataset and a classification accuracy of 86.81% on the HyperKvasir dataset.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d2fce7480111d66a74caa801a236f71ab021c42c.pdf",
        "venue": "IEEE transactions on circuits and systems for video technology (Print)",
        "citationCount": 30,
        "score": 15.0,
        "summary": "Automated classification of gastrointestinal endoscope images can help reduce the workload of doctors and improve the accuracy of diagnoses. The rapidly developed vision Transformer, represented by Swin Transformer, has become an impressive technique for medical image classification. However, Swin Transformer cannot capture the long-range dependency well in complex gastrointestinal endoscopy images. As a result, it fails to represent features of some widely-spread targets in digestive tract images, such as normal-z-line and esophagitis, effectively. To solve this problem, we propose a novel vision Transformer model based on hybrid shifted windows for digestive tract image classification, which can obtain both short-range and long-range dependency concurrently. Extensive experiments demonstrate the superiority of our method to the state-of-the-art methods with a classification accuracy of 95.42% on the Kvasir v2 dataset and a classification accuracy of 86.81% on the HyperKvasir dataset.",
        "keywords": []
      },
      "file_name": "d2fce7480111d66a74caa801a236f71ab021c42c.pdf"
    },
    {
      "success": true,
      "doc_id": "d8ad65dc4f2869177b606867115cdab0",
      "summary": "The main challenge of scene classification is to understand the semantic context information of high-resolution remote sensing images. Although vision transformer (ViT)-based methods have been explored to boost the long-range dependencies of high-resolution remote sensing images, the connectivity between neighboring windows is still limited. Meanwhile, ViT-based methods commonly contain a large number of parameters, resulting in a huge computational consumption. In this paper, a novel lightweight dual-branch swin transformer (LDBST) method for remote sensing scene classification is proposed, and the discriminative ability of scene features is increased through combining a ViT branch and convolutional neural network (CNN) branch. First, based on the hierarchical swin transformer model, LDBST divides the input features of each stage into two parts, which are then separately fed into the two branches. For the ViT branch, a dual multilayer perceptron structure with a depthwise convolutional layer, termed Conv-MLP, is integrated into the branch to boost the connections with neighboring windows. Then, a simple-structured CNN branch with maximum pooling preserves the strong features of the scene feature map. Specifically, the CNN branch lightens the LDBST, by avoiding complex multi-head attention and multilayer perceptron computations. To obtain better feature representation, LDBST was pretrained on the large-scale remote scene classification images of the MLRSN and RSD46-WHU datasets. These two pretrained weights were fine-tuned on target scene classification datasets. The experimental results showed that the proposed LDBST method was more effective than some other advanced remote sensing scene classification methods.",
      "intriguing_abstract": "The main challenge of scene classification is to understand the semantic context information of high-resolution remote sensing images. Although vision transformer (ViT)-based methods have been explored to boost the long-range dependencies of high-resolution remote sensing images, the connectivity between neighboring windows is still limited. Meanwhile, ViT-based methods commonly contain a large number of parameters, resulting in a huge computational consumption. In this paper, a novel lightweight dual-branch swin transformer (LDBST) method for remote sensing scene classification is proposed, and the discriminative ability of scene features is increased through combining a ViT branch and convolutional neural network (CNN) branch. First, based on the hierarchical swin transformer model, LDBST divides the input features of each stage into two parts, which are then separately fed into the two branches. For the ViT branch, a dual multilayer perceptron structure with a depthwise convolutional layer, termed Conv-MLP, is integrated into the branch to boost the connections with neighboring windows. Then, a simple-structured CNN branch with maximum pooling preserves the strong features of the scene feature map. Specifically, the CNN branch lightens the LDBST, by avoiding complex multi-head attention and multilayer perceptron computations. To obtain better feature representation, LDBST was pretrained on the large-scale remote scene classification images of the MLRSN and RSD46-WHU datasets. These two pretrained weights were fine-tuned on target scene classification datasets. The experimental results showed that the proposed LDBST method was more effective than some other advanced remote sensing scene classification methods.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/5135a8f690c66c3b64928227443c4f9378bd20e1.pdf",
      "citation_key": "zheng202325h",
      "metadata": {
        "title": "A Lightweight Dual-Branch Swin Transformer for Remote Sensing Scene Classification",
        "authors": [
          "Fujian Zheng",
          "Shuai Lin",
          "Wei Zhou",
          "Hong Huang"
        ],
        "published_date": "2023",
        "abstract": "The main challenge of scene classification is to understand the semantic context information of high-resolution remote sensing images. Although vision transformer (ViT)-based methods have been explored to boost the long-range dependencies of high-resolution remote sensing images, the connectivity between neighboring windows is still limited. Meanwhile, ViT-based methods commonly contain a large number of parameters, resulting in a huge computational consumption. In this paper, a novel lightweight dual-branch swin transformer (LDBST) method for remote sensing scene classification is proposed, and the discriminative ability of scene features is increased through combining a ViT branch and convolutional neural network (CNN) branch. First, based on the hierarchical swin transformer model, LDBST divides the input features of each stage into two parts, which are then separately fed into the two branches. For the ViT branch, a dual multilayer perceptron structure with a depthwise convolutional layer, termed Conv-MLP, is integrated into the branch to boost the connections with neighboring windows. Then, a simple-structured CNN branch with maximum pooling preserves the strong features of the scene feature map. Specifically, the CNN branch lightens the LDBST, by avoiding complex multi-head attention and multilayer perceptron computations. To obtain better feature representation, LDBST was pretrained on the large-scale remote scene classification images of the MLRSN and RSD46-WHU datasets. These two pretrained weights were fine-tuned on target scene classification datasets. The experimental results showed that the proposed LDBST method was more effective than some other advanced remote sensing scene classification methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/5135a8f690c66c3b64928227443c4f9378bd20e1.pdf",
        "venue": "Remote Sensing",
        "citationCount": 29,
        "score": 14.5,
        "summary": "The main challenge of scene classification is to understand the semantic context information of high-resolution remote sensing images. Although vision transformer (ViT)-based methods have been explored to boost the long-range dependencies of high-resolution remote sensing images, the connectivity between neighboring windows is still limited. Meanwhile, ViT-based methods commonly contain a large number of parameters, resulting in a huge computational consumption. In this paper, a novel lightweight dual-branch swin transformer (LDBST) method for remote sensing scene classification is proposed, and the discriminative ability of scene features is increased through combining a ViT branch and convolutional neural network (CNN) branch. First, based on the hierarchical swin transformer model, LDBST divides the input features of each stage into two parts, which are then separately fed into the two branches. For the ViT branch, a dual multilayer perceptron structure with a depthwise convolutional layer, termed Conv-MLP, is integrated into the branch to boost the connections with neighboring windows. Then, a simple-structured CNN branch with maximum pooling preserves the strong features of the scene feature map. Specifically, the CNN branch lightens the LDBST, by avoiding complex multi-head attention and multilayer perceptron computations. To obtain better feature representation, LDBST was pretrained on the large-scale remote scene classification images of the MLRSN and RSD46-WHU datasets. These two pretrained weights were fine-tuned on target scene classification datasets. The experimental results showed that the proposed LDBST method was more effective than some other advanced remote sensing scene classification methods.",
        "keywords": []
      },
      "file_name": "5135a8f690c66c3b64928227443c4f9378bd20e1.pdf"
    },
    {
      "success": true,
      "doc_id": "e29dde6362cbc4fe902f2871c003f6a7",
      "summary": "Gait recognition, the task of identifying an individual based on their unique walking style, can be difficult because walking styles can be influenced by external factors such as clothing, viewing angle, and carrying conditions. To address these challenges, this paper proposes a multi-model gait recognition system that integrates Convolutional Neural Networks (CNNs) and Vision Transformer. The first step in the process is to obtain a gait energy image, which is achieved by applying an averaging technique to a gait cycle. The gait energy image is then fed into three different models, DenseNet-201, VGG-16, and a Vision Transformer. These models are pre-trained and fine-tuned to encode the salient gait features that are specific to an individuals walking style. Each model provides prediction scores for the classes based on the encoded features, and these scores are then summed and averaged to produce the final class label. The performance of this multi-model gait recognition system was evaluated on three datasets, CASIA-B, OU-ISIR dataset D, and OU-ISIR Large Population dataset. The experimental results showed substantial improvement compared to existing methods on all three datasets. The integration of CNNs and ViT allows the system to learn both the pre-defined and distinct features, providing a robust solution for gait recognition even under the influence of covariates.",
      "intriguing_abstract": "Gait recognition, the task of identifying an individual based on their unique walking style, can be difficult because walking styles can be influenced by external factors such as clothing, viewing angle, and carrying conditions. To address these challenges, this paper proposes a multi-model gait recognition system that integrates Convolutional Neural Networks (CNNs) and Vision Transformer. The first step in the process is to obtain a gait energy image, which is achieved by applying an averaging technique to a gait cycle. The gait energy image is then fed into three different models, DenseNet-201, VGG-16, and a Vision Transformer. These models are pre-trained and fine-tuned to encode the salient gait features that are specific to an individuals walking style. Each model provides prediction scores for the classes based on the encoded features, and these scores are then summed and averaged to produce the final class label. The performance of this multi-model gait recognition system was evaluated on three datasets, CASIA-B, OU-ISIR dataset D, and OU-ISIR Large Population dataset. The experimental results showed substantial improvement compared to existing methods on all three datasets. The integration of CNNs and ViT allows the system to learn both the pre-defined and distinct features, providing a robust solution for gait recognition even under the influence of covariates.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/77eea367f79e69995948699d806683c7731a60b1.pdf",
      "citation_key": "mogan2023ywz",
      "metadata": {
        "title": "Gait-CNN-ViT: Multi-Model Gait Recognition with Convolutional Neural Networks and Vision Transformer",
        "authors": [
          "Jashila Nair Mogan",
          "C. Lee",
          "K. Lim",
          "M. Ali",
          "Ali Alqahtani"
        ],
        "published_date": "2023",
        "abstract": "Gait recognition, the task of identifying an individual based on their unique walking style, can be difficult because walking styles can be influenced by external factors such as clothing, viewing angle, and carrying conditions. To address these challenges, this paper proposes a multi-model gait recognition system that integrates Convolutional Neural Networks (CNNs) and Vision Transformer. The first step in the process is to obtain a gait energy image, which is achieved by applying an averaging technique to a gait cycle. The gait energy image is then fed into three different models, DenseNet-201, VGG-16, and a Vision Transformer. These models are pre-trained and fine-tuned to encode the salient gait features that are specific to an individuals walking style. Each model provides prediction scores for the classes based on the encoded features, and these scores are then summed and averaged to produce the final class label. The performance of this multi-model gait recognition system was evaluated on three datasets, CASIA-B, OU-ISIR dataset D, and OU-ISIR Large Population dataset. The experimental results showed substantial improvement compared to existing methods on all three datasets. The integration of CNNs and ViT allows the system to learn both the pre-defined and distinct features, providing a robust solution for gait recognition even under the influence of covariates.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/77eea367f79e69995948699d806683c7731a60b1.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 29,
        "score": 14.5,
        "summary": "Gait recognition, the task of identifying an individual based on their unique walking style, can be difficult because walking styles can be influenced by external factors such as clothing, viewing angle, and carrying conditions. To address these challenges, this paper proposes a multi-model gait recognition system that integrates Convolutional Neural Networks (CNNs) and Vision Transformer. The first step in the process is to obtain a gait energy image, which is achieved by applying an averaging technique to a gait cycle. The gait energy image is then fed into three different models, DenseNet-201, VGG-16, and a Vision Transformer. These models are pre-trained and fine-tuned to encode the salient gait features that are specific to an individuals walking style. Each model provides prediction scores for the classes based on the encoded features, and these scores are then summed and averaged to produce the final class label. The performance of this multi-model gait recognition system was evaluated on three datasets, CASIA-B, OU-ISIR dataset D, and OU-ISIR Large Population dataset. The experimental results showed substantial improvement compared to existing methods on all three datasets. The integration of CNNs and ViT allows the system to learn both the pre-defined and distinct features, providing a robust solution for gait recognition even under the influence of covariates.",
        "keywords": []
      },
      "file_name": "77eea367f79e69995948699d806683c7731a60b1.pdf"
    },
    {
      "success": true,
      "doc_id": "252ed245dbf53ee9cdaa5dc2b0e8e588",
      "summary": "Recently, transformer architectures have shown superior performance compared to their CNN counterparts in many computer vision tasks. The self-attention mechanism enables transformer networks to connect visual dependencies over short as well as long distances, thus generating a large, sometimes even a global receptive field. In this paper, we propose our Parallel Local-Global Vision Transformer (PLG-ViT), a general backbone model that fuses local window self-attention with global self-attention. By merging these local and global features, short- and long-range spatial interactions can be effectively and efficiently represented without the need for costly computational operations such as shifted windows. In a comprehensive evaluation, we demonstrate that our PLG-ViT outperforms CNN-based as well as state-of-the-art transformer-based architectures in image classification and in complex downstream tasks such as object detection, instance segmentation, and semantic segmentation. In particular, our PLG-ViT models outperformed similarly sized networks like ConvNeXt and Swin Transformer, achieving Top-1 accuracy values of 83.4%, 84.0%, and 84.5% on ImageNet-1K with 27M, 52M, and 91M parameters, respectively.",
      "intriguing_abstract": "Recently, transformer architectures have shown superior performance compared to their CNN counterparts in many computer vision tasks. The self-attention mechanism enables transformer networks to connect visual dependencies over short as well as long distances, thus generating a large, sometimes even a global receptive field. In this paper, we propose our Parallel Local-Global Vision Transformer (PLG-ViT), a general backbone model that fuses local window self-attention with global self-attention. By merging these local and global features, short- and long-range spatial interactions can be effectively and efficiently represented without the need for costly computational operations such as shifted windows. In a comprehensive evaluation, we demonstrate that our PLG-ViT outperforms CNN-based as well as state-of-the-art transformer-based architectures in image classification and in complex downstream tasks such as object detection, instance segmentation, and semantic segmentation. In particular, our PLG-ViT models outperformed similarly sized networks like ConvNeXt and Swin Transformer, achieving Top-1 accuracy values of 83.4%, 84.0%, and 84.5% on ImageNet-1K with 27M, 52M, and 91M parameters, respectively.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/861f670073679ba05990f3bc6d119b13ab62aca7.pdf",
      "citation_key": "ebert202377v",
      "metadata": {
        "title": "PLG-ViT: Vision Transformer with Parallel Local and Global Self-Attention",
        "authors": [
          "Nikolas Ebert",
          "D. Stricker",
          "Oliver Wasenmller"
        ],
        "published_date": "2023",
        "abstract": "Recently, transformer architectures have shown superior performance compared to their CNN counterparts in many computer vision tasks. The self-attention mechanism enables transformer networks to connect visual dependencies over short as well as long distances, thus generating a large, sometimes even a global receptive field. In this paper, we propose our Parallel Local-Global Vision Transformer (PLG-ViT), a general backbone model that fuses local window self-attention with global self-attention. By merging these local and global features, short- and long-range spatial interactions can be effectively and efficiently represented without the need for costly computational operations such as shifted windows. In a comprehensive evaluation, we demonstrate that our PLG-ViT outperforms CNN-based as well as state-of-the-art transformer-based architectures in image classification and in complex downstream tasks such as object detection, instance segmentation, and semantic segmentation. In particular, our PLG-ViT models outperformed similarly sized networks like ConvNeXt and Swin Transformer, achieving Top-1 accuracy values of 83.4%, 84.0%, and 84.5% on ImageNet-1K with 27M, 52M, and 91M parameters, respectively.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/861f670073679ba05990f3bc6d119b13ab62aca7.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 28,
        "score": 14.0,
        "summary": "Recently, transformer architectures have shown superior performance compared to their CNN counterparts in many computer vision tasks. The self-attention mechanism enables transformer networks to connect visual dependencies over short as well as long distances, thus generating a large, sometimes even a global receptive field. In this paper, we propose our Parallel Local-Global Vision Transformer (PLG-ViT), a general backbone model that fuses local window self-attention with global self-attention. By merging these local and global features, short- and long-range spatial interactions can be effectively and efficiently represented without the need for costly computational operations such as shifted windows. In a comprehensive evaluation, we demonstrate that our PLG-ViT outperforms CNN-based as well as state-of-the-art transformer-based architectures in image classification and in complex downstream tasks such as object detection, instance segmentation, and semantic segmentation. In particular, our PLG-ViT models outperformed similarly sized networks like ConvNeXt and Swin Transformer, achieving Top-1 accuracy values of 83.4%, 84.0%, and 84.5% on ImageNet-1K with 27M, 52M, and 91M parameters, respectively.",
        "keywords": []
      },
      "file_name": "861f670073679ba05990f3bc6d119b13ab62aca7.pdf"
    },
    {
      "success": true,
      "doc_id": "ff0c6230140f8c6c522ffe00c7427926",
      "summary": "Swin-Transformer has demonstrated remarkable success in computer vision by leveraging its hierarchical feature representation based on Transformer. In speech signals, emotional information is distributed across different scales of speech features, e. g., word, phrase, and utterance. Drawing above inspiration, this paper presents a hierarchical speech Transformer with shifted windows to aggregate multi-scale emotion features for speech emotion recognition (SER), called Speech Swin-Transformer. Specifically, we first divide the speech spectrogram into segment-level patches in the time domain, composed of multiple frame patches. These segment-level patches are then encoded using a stack of Swin blocks, in which a local window Transformer is utilized to explore local inter-frame emotional information across frame patches of each segment patch. After that, we also design a shifted window Transformer to compensate for patch correlations near the boundaries of segment patches. Finally, we employ a patch merging operation to aggregate segment-level emotional features for hierarchical speech representation by expanding the receptive field of Transformer from frame-level to segment-level. Experimental results demonstrate that our proposed Speech Swin-Transformer outperforms the state-of-the-art methods.",
      "intriguing_abstract": "Swin-Transformer has demonstrated remarkable success in computer vision by leveraging its hierarchical feature representation based on Transformer. In speech signals, emotional information is distributed across different scales of speech features, e. g., word, phrase, and utterance. Drawing above inspiration, this paper presents a hierarchical speech Transformer with shifted windows to aggregate multi-scale emotion features for speech emotion recognition (SER), called Speech Swin-Transformer. Specifically, we first divide the speech spectrogram into segment-level patches in the time domain, composed of multiple frame patches. These segment-level patches are then encoded using a stack of Swin blocks, in which a local window Transformer is utilized to explore local inter-frame emotional information across frame patches of each segment patch. After that, we also design a shifted window Transformer to compensate for patch correlations near the boundaries of segment patches. Finally, we employ a patch merging operation to aggregate segment-level emotional features for hierarchical speech representation by expanding the receptive field of Transformer from frame-level to segment-level. Experimental results demonstrate that our proposed Speech Swin-Transformer outperforms the state-of-the-art methods.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f93f718bb2d3f5e4354e174e54e71c5596f56ea3.pdf",
      "citation_key": "wang20245bq",
      "metadata": {
        "title": "Speech Swin-Transformer: Exploring a Hierarchical Transformer with Shifted Windows for Speech Emotion Recognition",
        "authors": [
          "Yong Wang",
          "Cheng Lu",
          "Hailun Lian",
          "Yan Zhao",
          "Bjorn Schuller",
          "Yuan Zong",
          "Wenming Zheng"
        ],
        "published_date": "2024",
        "abstract": "Swin-Transformer has demonstrated remarkable success in computer vision by leveraging its hierarchical feature representation based on Transformer. In speech signals, emotional information is distributed across different scales of speech features, e. g., word, phrase, and utterance. Drawing above inspiration, this paper presents a hierarchical speech Transformer with shifted windows to aggregate multi-scale emotion features for speech emotion recognition (SER), called Speech Swin-Transformer. Specifically, we first divide the speech spectrogram into segment-level patches in the time domain, composed of multiple frame patches. These segment-level patches are then encoded using a stack of Swin blocks, in which a local window Transformer is utilized to explore local inter-frame emotional information across frame patches of each segment patch. After that, we also design a shifted window Transformer to compensate for patch correlations near the boundaries of segment patches. Finally, we employ a patch merging operation to aggregate segment-level emotional features for hierarchical speech representation by expanding the receptive field of Transformer from frame-level to segment-level. Experimental results demonstrate that our proposed Speech Swin-Transformer outperforms the state-of-the-art methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f93f718bb2d3f5e4354e174e54e71c5596f56ea3.pdf",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "citationCount": 14,
        "score": 14.0,
        "summary": "Swin-Transformer has demonstrated remarkable success in computer vision by leveraging its hierarchical feature representation based on Transformer. In speech signals, emotional information is distributed across different scales of speech features, e. g., word, phrase, and utterance. Drawing above inspiration, this paper presents a hierarchical speech Transformer with shifted windows to aggregate multi-scale emotion features for speech emotion recognition (SER), called Speech Swin-Transformer. Specifically, we first divide the speech spectrogram into segment-level patches in the time domain, composed of multiple frame patches. These segment-level patches are then encoded using a stack of Swin blocks, in which a local window Transformer is utilized to explore local inter-frame emotional information across frame patches of each segment patch. After that, we also design a shifted window Transformer to compensate for patch correlations near the boundaries of segment patches. Finally, we employ a patch merging operation to aggregate segment-level emotional features for hierarchical speech representation by expanding the receptive field of Transformer from frame-level to segment-level. Experimental results demonstrate that our proposed Speech Swin-Transformer outperforms the state-of-the-art methods.",
        "keywords": []
      },
      "file_name": "f93f718bb2d3f5e4354e174e54e71c5596f56ea3.pdf"
    },
    {
      "success": true,
      "doc_id": "8353287967e940108d9b252c94e7e7f1",
      "summary": "The classification of galaxy morphology is among the most active fields in astronomical research today. With the development of artificial intelligence technology, deep learning is a useful tool in the classification of black the morphology of galaxies and significant progress has been made in this domain. However, there is still some room for improvement in terms of classification accuracy, automation, and related issues. Convolutional vision Transformer (CvT) is an improved version of the Vision Transformer (ViT) model. It improves the performance of the ViT model by introducing a convolutional neural network (CNN). This study explores the performance of the CvT model in the area of galaxy morphology classification. In this work, the CvT model was applied, for the first time, in a black five-class classification task of galaxy morphology. We black added different types and degrees of noise to the original galaxy images to verify that the CvT model achieves good classification performance, even in galaxy images with low signal-to-noise ratios (S/Ns). Then, we also validated the classification performance of the CvT model for galaxy images at different redshifts based on the low-redshift dataset GZ2 and the high-redshift dataset Galaxy Zoo CANDELS. In addition, we black visualized and analyzed the classification results of the CvT model based on the t-distributed stochastic black neighborhood -embedding (t-SNE) algorithm. We find that (1) compared with other black five-class classification models of galaxy morphology based on CNN models, the average accuracy, precision, recall, and F1\\_score evaluation metrics of the CvT classification model are all higher than 98, which is an improvement of at least 1 compared with those based on CNNs; (2) the classification black visualization results show that different categories of galaxies are separated from each other in multi-dimensional space. The application of the CvT model to the classification study of galaxy morphology is a novel undertaking that carries important implications for future studies.",
      "intriguing_abstract": "The classification of galaxy morphology is among the most active fields in astronomical research today. With the development of artificial intelligence technology, deep learning is a useful tool in the classification of black the morphology of galaxies and significant progress has been made in this domain. However, there is still some room for improvement in terms of classification accuracy, automation, and related issues. Convolutional vision Transformer (CvT) is an improved version of the Vision Transformer (ViT) model. It improves the performance of the ViT model by introducing a convolutional neural network (CNN). This study explores the performance of the CvT model in the area of galaxy morphology classification. In this work, the CvT model was applied, for the first time, in a black five-class classification task of galaxy morphology. We black added different types and degrees of noise to the original galaxy images to verify that the CvT model achieves good classification performance, even in galaxy images with low signal-to-noise ratios (S/Ns). Then, we also validated the classification performance of the CvT model for galaxy images at different redshifts based on the low-redshift dataset GZ2 and the high-redshift dataset Galaxy Zoo CANDELS. In addition, we black visualized and analyzed the classification results of the CvT model based on the t-distributed stochastic black neighborhood -embedding (t-SNE) algorithm. We find that (1) compared with other black five-class classification models of galaxy morphology based on CNN models, the average accuracy, precision, recall, and F1\\_score evaluation metrics of the CvT classification model are all higher than 98, which is an improvement of at least 1 compared with those based on CNNs; (2) the classification black visualization results show that different categories of galaxies are separated from each other in multi-dimensional space. The application of the CvT model to the classification study of galaxy morphology is a novel undertaking that carries important implications for future studies.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/c5c9005aae80795e241de18b595c2d01393808f8.pdf",
      "citation_key": "cao20241ng",
      "metadata": {
        "title": "Galaxy morphology classification based on Convolutional vision Transformer (CvT)",
        "authors": [
          "Jie Cao",
          "Tingting Xu",
          "Yu-he Deng",
          "Linhua Deng",
          "Ming-cun Yang",
          "Zhi-jing Liu",
          "Weihong Zhou"
        ],
        "published_date": "2024",
        "abstract": "The classification of galaxy morphology is among the most active fields in astronomical research today. With the development of artificial intelligence technology, deep learning is a useful tool in the classification of black the morphology of galaxies and significant progress has been made in this domain. However, there is still some room for improvement in terms of classification accuracy, automation, and related issues. Convolutional vision Transformer (CvT) is an improved version of the Vision Transformer (ViT) model. It improves the performance of the ViT model by introducing a convolutional neural network (CNN). This study explores the performance of the CvT model in the area of galaxy morphology classification. In this work, the CvT model was applied, for the first time, in a black five-class classification task of galaxy morphology. We black added different types and degrees of noise to the original galaxy images to verify that the CvT model achieves good classification performance, even in galaxy images with low signal-to-noise ratios (S/Ns). Then, we also validated the classification performance of the CvT model for galaxy images at different redshifts based on the low-redshift dataset GZ2 and the high-redshift dataset Galaxy Zoo CANDELS. In addition, we black visualized and analyzed the classification results of the CvT model based on the t-distributed stochastic black neighborhood -embedding (t-SNE) algorithm. We find that (1) compared with other black five-class classification models of galaxy morphology based on CNN models, the average accuracy, precision, recall, and F1\\_score evaluation metrics of the CvT classification model are all higher than 98, which is an improvement of at least 1 compared with those based on CNNs; (2) the classification black visualization results show that different categories of galaxies are separated from each other in multi-dimensional space. The application of the CvT model to the classification study of galaxy morphology is a novel undertaking that carries important implications for future studies.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c5c9005aae80795e241de18b595c2d01393808f8.pdf",
        "venue": "Astronomy &amp; Astrophysics",
        "citationCount": 14,
        "score": 14.0,
        "summary": "The classification of galaxy morphology is among the most active fields in astronomical research today. With the development of artificial intelligence technology, deep learning is a useful tool in the classification of black the morphology of galaxies and significant progress has been made in this domain. However, there is still some room for improvement in terms of classification accuracy, automation, and related issues. Convolutional vision Transformer (CvT) is an improved version of the Vision Transformer (ViT) model. It improves the performance of the ViT model by introducing a convolutional neural network (CNN). This study explores the performance of the CvT model in the area of galaxy morphology classification. In this work, the CvT model was applied, for the first time, in a black five-class classification task of galaxy morphology. We black added different types and degrees of noise to the original galaxy images to verify that the CvT model achieves good classification performance, even in galaxy images with low signal-to-noise ratios (S/Ns). Then, we also validated the classification performance of the CvT model for galaxy images at different redshifts based on the low-redshift dataset GZ2 and the high-redshift dataset Galaxy Zoo CANDELS. In addition, we black visualized and analyzed the classification results of the CvT model based on the t-distributed stochastic black neighborhood -embedding (t-SNE) algorithm. We find that (1) compared with other black five-class classification models of galaxy morphology based on CNN models, the average accuracy, precision, recall, and F1\\_score evaluation metrics of the CvT classification model are all higher than 98, which is an improvement of at least 1 compared with those based on CNNs; (2) the classification black visualization results show that different categories of galaxies are separated from each other in multi-dimensional space. The application of the CvT model to the classification study of galaxy morphology is a novel undertaking that carries important implications for future studies.",
        "keywords": []
      },
      "file_name": "c5c9005aae80795e241de18b595c2d01393808f8.pdf"
    },
    {
      "success": true,
      "doc_id": "8d10f3a79e6d459aee002a5e361c1615",
      "summary": "Computer-aided diagnosis systems based on deep learning algorithms have shown potential applications in rapid diagnosis of diabetic retinopathy (DR). Due to the superior performance of Transformer over convolutional neural networks (CNN) on natural images, we attempted to develop a new model to classify referable DR based on a limited number of large-size retinal images by using Transformer. Vision Transformer (ViT) with Masked Autoencoders (MAE) was applied in this study to improve the classification performance of referable DR. We collected over 100,000 publicly fundus retinal images larger than 224224, and then pre-trained ViT on these retinal images using MAE. The pre-trained ViT was applied to classify referable DR, the performance was also compared with that of ViT pre-trained using ImageNet. The improvement in model classification performance by pre-training with over 100,000 retinal images using MAE is superior to that pre-trained with ImageNet. The accuracy, area under curve (AUC), highest sensitivity and highest specificity of the present model are 93.42%, 0.9853, 0.973 and 0.9539, respectively. This study shows that MAE can provide more flexibility to the input image and substantially reduce the number of images required. Meanwhile, the pretraining dataset scale in this study is much smaller than ImageNet, and the pre-trained weights from ImageNet are not required also.",
      "intriguing_abstract": "Computer-aided diagnosis systems based on deep learning algorithms have shown potential applications in rapid diagnosis of diabetic retinopathy (DR). Due to the superior performance of Transformer over convolutional neural networks (CNN) on natural images, we attempted to develop a new model to classify referable DR based on a limited number of large-size retinal images by using Transformer. Vision Transformer (ViT) with Masked Autoencoders (MAE) was applied in this study to improve the classification performance of referable DR. We collected over 100,000 publicly fundus retinal images larger than 224224, and then pre-trained ViT on these retinal images using MAE. The pre-trained ViT was applied to classify referable DR, the performance was also compared with that of ViT pre-trained using ImageNet. The improvement in model classification performance by pre-training with over 100,000 retinal images using MAE is superior to that pre-trained with ImageNet. The accuracy, area under curve (AUC), highest sensitivity and highest specificity of the present model are 93.42%, 0.9853, 0.973 and 0.9539, respectively. This study shows that MAE can provide more flexibility to the input image and substantially reduce the number of images required. Meanwhile, the pretraining dataset scale in this study is much smaller than ImageNet, and the pre-trained weights from ImageNet are not required also.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/14c42c0f2c94e0a1f4aa820886080263f9922047.pdf",
      "citation_key": "yang2024in8",
      "metadata": {
        "title": "Vision transformer with masked autoencoders for referable diabetic retinopathy classification based on large-size retina image",
        "authors": [
          "Yaoming Yang",
          "Zhili Cai",
          "Shuxia Qiu",
          "Peng Xu"
        ],
        "published_date": "2024",
        "abstract": "Computer-aided diagnosis systems based on deep learning algorithms have shown potential applications in rapid diagnosis of diabetic retinopathy (DR). Due to the superior performance of Transformer over convolutional neural networks (CNN) on natural images, we attempted to develop a new model to classify referable DR based on a limited number of large-size retinal images by using Transformer. Vision Transformer (ViT) with Masked Autoencoders (MAE) was applied in this study to improve the classification performance of referable DR. We collected over 100,000 publicly fundus retinal images larger than 224224, and then pre-trained ViT on these retinal images using MAE. The pre-trained ViT was applied to classify referable DR, the performance was also compared with that of ViT pre-trained using ImageNet. The improvement in model classification performance by pre-training with over 100,000 retinal images using MAE is superior to that pre-trained with ImageNet. The accuracy, area under curve (AUC), highest sensitivity and highest specificity of the present model are 93.42%, 0.9853, 0.973 and 0.9539, respectively. This study shows that MAE can provide more flexibility to the input image and substantially reduce the number of images required. Meanwhile, the pretraining dataset scale in this study is much smaller than ImageNet, and the pre-trained weights from ImageNet are not required also.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/14c42c0f2c94e0a1f4aa820886080263f9922047.pdf",
        "venue": "PLoS ONE",
        "citationCount": 14,
        "score": 14.0,
        "summary": "Computer-aided diagnosis systems based on deep learning algorithms have shown potential applications in rapid diagnosis of diabetic retinopathy (DR). Due to the superior performance of Transformer over convolutional neural networks (CNN) on natural images, we attempted to develop a new model to classify referable DR based on a limited number of large-size retinal images by using Transformer. Vision Transformer (ViT) with Masked Autoencoders (MAE) was applied in this study to improve the classification performance of referable DR. We collected over 100,000 publicly fundus retinal images larger than 224224, and then pre-trained ViT on these retinal images using MAE. The pre-trained ViT was applied to classify referable DR, the performance was also compared with that of ViT pre-trained using ImageNet. The improvement in model classification performance by pre-training with over 100,000 retinal images using MAE is superior to that pre-trained with ImageNet. The accuracy, area under curve (AUC), highest sensitivity and highest specificity of the present model are 93.42%, 0.9853, 0.973 and 0.9539, respectively. This study shows that MAE can provide more flexibility to the input image and substantially reduce the number of images required. Meanwhile, the pretraining dataset scale in this study is much smaller than ImageNet, and the pre-trained weights from ImageNet are not required also.",
        "keywords": []
      },
      "file_name": "14c42c0f2c94e0a1f4aa820886080263f9922047.pdf"
    },
    {
      "success": true,
      "doc_id": "ad6ee6aadb7ac0f5b5aeb6da823c19b3",
      "summary": "The rapid advancement of medical imaging technologies requires the development of advanced, automated, and interpretable diagnostic tools for clinical decision-making. Although convolutional neural networks (CNNs) have shown significant promise in medical image analysis, they have limitations in capturing the global context and lack interpretability, thereby hindering their clinical adoption. This study presents EFFResNet-ViT, a novel hybrid deep learning (DL) model designed to address these challenges by combining EfficientNet-B0 and ResNet-50 CNN backbones with a vision transformer (ViT) module. The proposed architecture employs a feature fusion strategy to integrate the local feature extraction strengths of CNNs with the global dependency modeling capabilities of transformers. The extracted features are further refined through a post-transformer CNN and a global average pooling layer to enhance the classification performance. To improve interpretability, EFFResNet-ViT incorporates Grad-CAM visualization techniques to highlight regions contributing to classification decisions and employs t-distributed stochastic neighbor embedding for feature space analysis, providing insights into class separability. The proposed model was evaluated on two benchmark datasets: brain tumor (BT) CE-MRI for BT classification and a retinal image dataset for ophthalmological diagnosis. EFFResNet-ViT achieved state-of-the-art performance, with accuracies of 99.31% and 92.54% on the BT CE-MRI and retinal datasets, respectively. Comparative analyses demonstrate the superior classification performance and interpretability of EFFResNet-ViT over existing ViT and CNN-based hybrid models. The explainable design of EFFResNet-ViT addresses the critical need for transparency in artificial intelligence-driven medical diagnostics, facilitating its potential integration into clinical workflows to improve decision-making and patient outcomes.",
      "intriguing_abstract": "The rapid advancement of medical imaging technologies requires the development of advanced, automated, and interpretable diagnostic tools for clinical decision-making. Although convolutional neural networks (CNNs) have shown significant promise in medical image analysis, they have limitations in capturing the global context and lack interpretability, thereby hindering their clinical adoption. This study presents EFFResNet-ViT, a novel hybrid deep learning (DL) model designed to address these challenges by combining EfficientNet-B0 and ResNet-50 CNN backbones with a vision transformer (ViT) module. The proposed architecture employs a feature fusion strategy to integrate the local feature extraction strengths of CNNs with the global dependency modeling capabilities of transformers. The extracted features are further refined through a post-transformer CNN and a global average pooling layer to enhance the classification performance. To improve interpretability, EFFResNet-ViT incorporates Grad-CAM visualization techniques to highlight regions contributing to classification decisions and employs t-distributed stochastic neighbor embedding for feature space analysis, providing insights into class separability. The proposed model was evaluated on two benchmark datasets: brain tumor (BT) CE-MRI for BT classification and a retinal image dataset for ophthalmological diagnosis. EFFResNet-ViT achieved state-of-the-art performance, with accuracies of 99.31% and 92.54% on the BT CE-MRI and retinal datasets, respectively. Comparative analyses demonstrate the superior classification performance and interpretability of EFFResNet-ViT over existing ViT and CNN-based hybrid models. The explainable design of EFFResNet-ViT addresses the critical need for transparency in artificial intelligence-driven medical diagnostics, facilitating its potential integration into clinical workflows to improve decision-making and patient outcomes.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/9998291d71f4e8ddf59f4b016b19df1f848eeed1.pdf",
      "citation_key": "hussain2025qoe",
      "metadata": {
        "title": "EFFResNet-ViT: A Fusion-Based Convolutional and Vision Transformer Model for Explainable Medical Image Classification",
        "authors": [
          "Tahir Hussain",
          "Hayaru Shouno",
          "Abid Hussain",
          "Dostdar Hussain",
          "Muhammad Ismail",
          "Tatheer Hussain Mir",
          "Fang Rong Hsu",
          "Taukir Alam",
          "Shabnur Anonna Akhy"
        ],
        "published_date": "2025",
        "abstract": "The rapid advancement of medical imaging technologies requires the development of advanced, automated, and interpretable diagnostic tools for clinical decision-making. Although convolutional neural networks (CNNs) have shown significant promise in medical image analysis, they have limitations in capturing the global context and lack interpretability, thereby hindering their clinical adoption. This study presents EFFResNet-ViT, a novel hybrid deep learning (DL) model designed to address these challenges by combining EfficientNet-B0 and ResNet-50 CNN backbones with a vision transformer (ViT) module. The proposed architecture employs a feature fusion strategy to integrate the local feature extraction strengths of CNNs with the global dependency modeling capabilities of transformers. The extracted features are further refined through a post-transformer CNN and a global average pooling layer to enhance the classification performance. To improve interpretability, EFFResNet-ViT incorporates Grad-CAM visualization techniques to highlight regions contributing to classification decisions and employs t-distributed stochastic neighbor embedding for feature space analysis, providing insights into class separability. The proposed model was evaluated on two benchmark datasets: brain tumor (BT) CE-MRI for BT classification and a retinal image dataset for ophthalmological diagnosis. EFFResNet-ViT achieved state-of-the-art performance, with accuracies of 99.31% and 92.54% on the BT CE-MRI and retinal datasets, respectively. Comparative analyses demonstrate the superior classification performance and interpretability of EFFResNet-ViT over existing ViT and CNN-based hybrid models. The explainable design of EFFResNet-ViT addresses the critical need for transparency in artificial intelligence-driven medical diagnostics, facilitating its potential integration into clinical workflows to improve decision-making and patient outcomes.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9998291d71f4e8ddf59f4b016b19df1f848eeed1.pdf",
        "venue": "IEEE Access",
        "citationCount": 14,
        "score": 14.0,
        "summary": "The rapid advancement of medical imaging technologies requires the development of advanced, automated, and interpretable diagnostic tools for clinical decision-making. Although convolutional neural networks (CNNs) have shown significant promise in medical image analysis, they have limitations in capturing the global context and lack interpretability, thereby hindering their clinical adoption. This study presents EFFResNet-ViT, a novel hybrid deep learning (DL) model designed to address these challenges by combining EfficientNet-B0 and ResNet-50 CNN backbones with a vision transformer (ViT) module. The proposed architecture employs a feature fusion strategy to integrate the local feature extraction strengths of CNNs with the global dependency modeling capabilities of transformers. The extracted features are further refined through a post-transformer CNN and a global average pooling layer to enhance the classification performance. To improve interpretability, EFFResNet-ViT incorporates Grad-CAM visualization techniques to highlight regions contributing to classification decisions and employs t-distributed stochastic neighbor embedding for feature space analysis, providing insights into class separability. The proposed model was evaluated on two benchmark datasets: brain tumor (BT) CE-MRI for BT classification and a retinal image dataset for ophthalmological diagnosis. EFFResNet-ViT achieved state-of-the-art performance, with accuracies of 99.31% and 92.54% on the BT CE-MRI and retinal datasets, respectively. Comparative analyses demonstrate the superior classification performance and interpretability of EFFResNet-ViT over existing ViT and CNN-based hybrid models. The explainable design of EFFResNet-ViT addresses the critical need for transparency in artificial intelligence-driven medical diagnostics, facilitating its potential integration into clinical workflows to improve decision-making and patient outcomes.",
        "keywords": []
      },
      "file_name": "9998291d71f4e8ddf59f4b016b19df1f848eeed1.pdf"
    },
    {
      "success": true,
      "doc_id": "1564f2ab813941d00d00b1c96e422bd5",
      "summary": "Monocular depth estimation plays a critical role in various computer vision and robotics applications such as localization, mapping, and 3D object detection. Recently, learning-based algorithms achieve huge success in depth estimation by training models with a large amount of data in a supervised manner. However, it is challenging to acquire dense ground truth depth labels for supervised training, and the unsupervised depth estimation using monocular sequences emerges as a promising alternative. Unfortunately, most studies on unsupervised depth estimation explore loss functions or occlusion masks, and there is little change in model architecture in that ConvNet-based encoder-decoder structure becomes a de-facto standard for depth estimation. In this paper, we employ a convolution-free Swin Transformer as an image feature extractor so that the network can capture both local geometric features and global semantic features for depth estimation. Also, we propose a Densely Cascaded Multi-scale Network (DCMNet) that connects every feature map directly with another from different scales via a top-down cascade pathway. This densely cascaded connectivity reinforces the interconnection between decoding layers and produces high-quality multi-scale depth outputs. The experiments on two different datasets, KITTI and Make3D, demonstrate that our proposed method outperforms existing state-of-the-art unsupervised algorithms.",
      "intriguing_abstract": "Monocular depth estimation plays a critical role in various computer vision and robotics applications such as localization, mapping, and 3D object detection. Recently, learning-based algorithms achieve huge success in depth estimation by training models with a large amount of data in a supervised manner. However, it is challenging to acquire dense ground truth depth labels for supervised training, and the unsupervised depth estimation using monocular sequences emerges as a promising alternative. Unfortunately, most studies on unsupervised depth estimation explore loss functions or occlusion masks, and there is little change in model architecture in that ConvNet-based encoder-decoder structure becomes a de-facto standard for depth estimation. In this paper, we employ a convolution-free Swin Transformer as an image feature extractor so that the network can capture both local geometric features and global semantic features for depth estimation. Also, we propose a Densely Cascaded Multi-scale Network (DCMNet) that connects every feature map directly with another from different scales via a top-down cascade pathway. This densely cascaded connectivity reinforces the interconnection between decoding layers and produces high-quality multi-scale depth outputs. The experiments on two different datasets, KITTI and Make3D, demonstrate that our proposed method outperforms existing state-of-the-art unsupervised algorithms.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/9fa44f616e796f187d97b9cc324bd839bc21ad81.pdf",
      "citation_key": "shim2023z7g",
      "metadata": {
        "title": "SwinDepth: Unsupervised Depth Estimation using Monocular Sequences via Swin Transformer and Densely Cascaded Network",
        "authors": [
          "D. Shim",
          "H. J. Kim"
        ],
        "published_date": "2023",
        "abstract": "Monocular depth estimation plays a critical role in various computer vision and robotics applications such as localization, mapping, and 3D object detection. Recently, learning-based algorithms achieve huge success in depth estimation by training models with a large amount of data in a supervised manner. However, it is challenging to acquire dense ground truth depth labels for supervised training, and the unsupervised depth estimation using monocular sequences emerges as a promising alternative. Unfortunately, most studies on unsupervised depth estimation explore loss functions or occlusion masks, and there is little change in model architecture in that ConvNet-based encoder-decoder structure becomes a de-facto standard for depth estimation. In this paper, we employ a convolution-free Swin Transformer as an image feature extractor so that the network can capture both local geometric features and global semantic features for depth estimation. Also, we propose a Densely Cascaded Multi-scale Network (DCMNet) that connects every feature map directly with another from different scales via a top-down cascade pathway. This densely cascaded connectivity reinforces the interconnection between decoding layers and produces high-quality multi-scale depth outputs. The experiments on two different datasets, KITTI and Make3D, demonstrate that our proposed method outperforms existing state-of-the-art unsupervised algorithms.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9fa44f616e796f187d97b9cc324bd839bc21ad81.pdf",
        "venue": "IEEE International Conference on Robotics and Automation",
        "citationCount": 27,
        "score": 13.5,
        "summary": "Monocular depth estimation plays a critical role in various computer vision and robotics applications such as localization, mapping, and 3D object detection. Recently, learning-based algorithms achieve huge success in depth estimation by training models with a large amount of data in a supervised manner. However, it is challenging to acquire dense ground truth depth labels for supervised training, and the unsupervised depth estimation using monocular sequences emerges as a promising alternative. Unfortunately, most studies on unsupervised depth estimation explore loss functions or occlusion masks, and there is little change in model architecture in that ConvNet-based encoder-decoder structure becomes a de-facto standard for depth estimation. In this paper, we employ a convolution-free Swin Transformer as an image feature extractor so that the network can capture both local geometric features and global semantic features for depth estimation. Also, we propose a Densely Cascaded Multi-scale Network (DCMNet) that connects every feature map directly with another from different scales via a top-down cascade pathway. This densely cascaded connectivity reinforces the interconnection between decoding layers and produces high-quality multi-scale depth outputs. The experiments on two different datasets, KITTI and Make3D, demonstrate that our proposed method outperforms existing state-of-the-art unsupervised algorithms.",
        "keywords": []
      },
      "file_name": "9fa44f616e796f187d97b9cc324bd839bc21ad81.pdf"
    },
    {
      "success": true,
      "doc_id": "e6008f3f5087ace5b4ad572e4c67c1b4",
      "summary": "INTRODUCTION\nIn this study, we harnessed three cutting-edge algorithms' capabilities to refine the elbow fracture prediction process through X-ray image analysis. Employing the YOLOv8 (You only look once) algorithm, we first identified Regions of Interest (ROI) within the X-ray images, significantly augmenting fracture prediction accuracy.\n\n\nMETHODS\nSubsequently, we integrated and compared the ResNet, the SeResNet (Squeeze-and-Excitation Residual Network) ViT (Vision Transformer) algorithms to refine our predictive capabilities. Furthermore, to ensure optimal precision, we implemented a series of meticulous refinements. This included recalibrating ROI regions to enable finer-grained identification of diagnostically significant areas within the X-ray images. Additionally, advanced image enhancement techniques were applied to optimize the X-ray images' visual quality and structural clarity.\n\n\nRESULTS\nThese methodological enhancements synergistically contributed to a substantial improvement in the overall accuracy of our fracture predictions. The dataset utilized for training, testing & validation, and comprehensive evaluation exclusively comprised elbow X-ray images, where predicting the fracture with three algorithms: Resnet50; accuracy 0.97, precision 1, recall 0.95, SeResnet50; accuracy 0.97, precision 1, recall 0.95 & ViTB- 16 with high accuracy of 0.99, precision same as the other two algorithms, with a recall of 0.95.\n\n\nCONCLUSION\nThis approach has the potential to increase the precision of diagnoses, lessen the burden of radiologists, easily integrate into current medical imaging systems, and assist clinical decision-making, all of which could lead to better patient care and health outcomes overall.",
      "intriguing_abstract": "INTRODUCTION\nIn this study, we harnessed three cutting-edge algorithms' capabilities to refine the elbow fracture prediction process through X-ray image analysis. Employing the YOLOv8 (You only look once) algorithm, we first identified Regions of Interest (ROI) within the X-ray images, significantly augmenting fracture prediction accuracy.\n\n\nMETHODS\nSubsequently, we integrated and compared the ResNet, the SeResNet (Squeeze-and-Excitation Residual Network) ViT (Vision Transformer) algorithms to refine our predictive capabilities. Furthermore, to ensure optimal precision, we implemented a series of meticulous refinements. This included recalibrating ROI regions to enable finer-grained identification of diagnostically significant areas within the X-ray images. Additionally, advanced image enhancement techniques were applied to optimize the X-ray images' visual quality and structural clarity.\n\n\nRESULTS\nThese methodological enhancements synergistically contributed to a substantial improvement in the overall accuracy of our fracture predictions. The dataset utilized for training, testing & validation, and comprehensive evaluation exclusively comprised elbow X-ray images, where predicting the fracture with three algorithms: Resnet50; accuracy 0.97, precision 1, recall 0.95, SeResnet50; accuracy 0.97, precision 1, recall 0.95 & ViTB- 16 with high accuracy of 0.99, precision same as the other two algorithms, with a recall of 0.95.\n\n\nCONCLUSION\nThis approach has the potential to increase the precision of diagnoses, lessen the burden of radiologists, easily integrate into current medical imaging systems, and assist clinical decision-making, all of which could lead to better patient care and health outcomes overall.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d62e6ba83d3da5b75e1d428f20f8199e3754bd5d.pdf",
      "citation_key": "alam2024t09",
      "metadata": {
        "title": "An Integrated Approach using YOLOv8 and ResNet, SeResNet & Vision Transformer (ViT) Algorithms based on ROI Fracture Prediction in X-ray Images of the Elbow.",
        "authors": [
          "Taukir Alam",
          "Wei-Cheng Yeh",
          "Fang Rong Hsu",
          "W. Shia",
          "Robert Singh",
          "Taimoor Hassan",
          "Wenru Lin",
          "Hong-Ye Yang",
          "Tahir Hussain"
        ],
        "published_date": "2024",
        "abstract": "INTRODUCTION\nIn this study, we harnessed three cutting-edge algorithms' capabilities to refine the elbow fracture prediction process through X-ray image analysis. Employing the YOLOv8 (You only look once) algorithm, we first identified Regions of Interest (ROI) within the X-ray images, significantly augmenting fracture prediction accuracy.\n\n\nMETHODS\nSubsequently, we integrated and compared the ResNet, the SeResNet (Squeeze-and-Excitation Residual Network) ViT (Vision Transformer) algorithms to refine our predictive capabilities. Furthermore, to ensure optimal precision, we implemented a series of meticulous refinements. This included recalibrating ROI regions to enable finer-grained identification of diagnostically significant areas within the X-ray images. Additionally, advanced image enhancement techniques were applied to optimize the X-ray images' visual quality and structural clarity.\n\n\nRESULTS\nThese methodological enhancements synergistically contributed to a substantial improvement in the overall accuracy of our fracture predictions. The dataset utilized for training, testing & validation, and comprehensive evaluation exclusively comprised elbow X-ray images, where predicting the fracture with three algorithms: Resnet50; accuracy 0.97, precision 1, recall 0.95, SeResnet50; accuracy 0.97, precision 1, recall 0.95 & ViTB- 16 with high accuracy of 0.99, precision same as the other two algorithms, with a recall of 0.95.\n\n\nCONCLUSION\nThis approach has the potential to increase the precision of diagnoses, lessen the burden of radiologists, easily integrate into current medical imaging systems, and assist clinical decision-making, all of which could lead to better patient care and health outcomes overall.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d62e6ba83d3da5b75e1d428f20f8199e3754bd5d.pdf",
        "venue": "Current medical imaging",
        "citationCount": 13,
        "score": 13.0,
        "summary": "INTRODUCTION\nIn this study, we harnessed three cutting-edge algorithms' capabilities to refine the elbow fracture prediction process through X-ray image analysis. Employing the YOLOv8 (You only look once) algorithm, we first identified Regions of Interest (ROI) within the X-ray images, significantly augmenting fracture prediction accuracy.\n\n\nMETHODS\nSubsequently, we integrated and compared the ResNet, the SeResNet (Squeeze-and-Excitation Residual Network) ViT (Vision Transformer) algorithms to refine our predictive capabilities. Furthermore, to ensure optimal precision, we implemented a series of meticulous refinements. This included recalibrating ROI regions to enable finer-grained identification of diagnostically significant areas within the X-ray images. Additionally, advanced image enhancement techniques were applied to optimize the X-ray images' visual quality and structural clarity.\n\n\nRESULTS\nThese methodological enhancements synergistically contributed to a substantial improvement in the overall accuracy of our fracture predictions. The dataset utilized for training, testing & validation, and comprehensive evaluation exclusively comprised elbow X-ray images, where predicting the fracture with three algorithms: Resnet50; accuracy 0.97, precision 1, recall 0.95, SeResnet50; accuracy 0.97, precision 1, recall 0.95 & ViTB- 16 with high accuracy of 0.99, precision same as the other two algorithms, with a recall of 0.95.\n\n\nCONCLUSION\nThis approach has the potential to increase the precision of diagnoses, lessen the burden of radiologists, easily integrate into current medical imaging systems, and assist clinical decision-making, all of which could lead to better patient care and health outcomes overall.",
        "keywords": []
      },
      "file_name": "d62e6ba83d3da5b75e1d428f20f8199e3754bd5d.pdf"
    },
    {
      "success": true,
      "doc_id": "7338e106dcdd91f61309b62ae5ee0ec9",
      "summary": "The existing image semantic segmentation models have low accuracy in detecting tiny targets or multi-targets at overlapping regions. This work proposes a hybrid vision transformer with unified-perceptual-parsing network (ViT-UperNet) for medical image segmentation. A self-attention mechanism is embedded in a vision transformer to extract multi-level features. The image features are extracted hierarchically from low to high dimensions using 4 groups of Transformer blocks with different numbers. Then, it uses a unified-perceptual-parsing network based on a feature pyramid network (FPN) and a pyramid pooling module (PPM) for the fusion of multi-scale contextual features and semantic segmentation. FPN can naturally use hierarchical features, and generate strong semantic information on all scales. PPM can better use the global prior knowledge to understand complex scenes, and extract features with global context information to improve segmentation results. In the training process, a scalable self-supervised learner named masked autoencoder is used for pre-training, which strengthens the visual representation ability and improves the efficiency of the feature learning. Experiments are conducted on cardiac magnetic resonance image segmentation where the left and right atrium and ventricle are selected for segmentation. The pixels accuracy is 93.85%, the Dice coefficient is 92.61% and Hausdorff distance is 11.16, which are improved compared with the other methods. The results show the superiority of Vit-UperNet in medical images segmentation, especially for the low-recognition and serious-occlusion targets.",
      "intriguing_abstract": "The existing image semantic segmentation models have low accuracy in detecting tiny targets or multi-targets at overlapping regions. This work proposes a hybrid vision transformer with unified-perceptual-parsing network (ViT-UperNet) for medical image segmentation. A self-attention mechanism is embedded in a vision transformer to extract multi-level features. The image features are extracted hierarchically from low to high dimensions using 4 groups of Transformer blocks with different numbers. Then, it uses a unified-perceptual-parsing network based on a feature pyramid network (FPN) and a pyramid pooling module (PPM) for the fusion of multi-scale contextual features and semantic segmentation. FPN can naturally use hierarchical features, and generate strong semantic information on all scales. PPM can better use the global prior knowledge to understand complex scenes, and extract features with global context information to improve segmentation results. In the training process, a scalable self-supervised learner named masked autoencoder is used for pre-training, which strengthens the visual representation ability and improves the efficiency of the feature learning. Experiments are conducted on cardiac magnetic resonance image segmentation where the left and right atrium and ventricle are selected for segmentation. The pixels accuracy is 93.85%, the Dice coefficient is 92.61% and Hausdorff distance is 11.16, which are improved compared with the other methods. The results show the superiority of Vit-UperNet in medical images segmentation, especially for the low-recognition and serious-occlusion targets.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d1faaa1d7d312dd5867683ce60519979de6b3349.pdf",
      "citation_key": "yang2024tti",
      "metadata": {
        "title": "ViT-UperNet: a hybrid vision transformer with unified-perceptual-parsing network for medical image segmentation",
        "authors": [
          "Ruiping Yang",
          "Liu Kun",
          "Shaohua Xu",
          "Yin Jian",
          "Zhang Zhen"
        ],
        "published_date": "2024",
        "abstract": "The existing image semantic segmentation models have low accuracy in detecting tiny targets or multi-targets at overlapping regions. This work proposes a hybrid vision transformer with unified-perceptual-parsing network (ViT-UperNet) for medical image segmentation. A self-attention mechanism is embedded in a vision transformer to extract multi-level features. The image features are extracted hierarchically from low to high dimensions using 4 groups of Transformer blocks with different numbers. Then, it uses a unified-perceptual-parsing network based on a feature pyramid network (FPN) and a pyramid pooling module (PPM) for the fusion of multi-scale contextual features and semantic segmentation. FPN can naturally use hierarchical features, and generate strong semantic information on all scales. PPM can better use the global prior knowledge to understand complex scenes, and extract features with global context information to improve segmentation results. In the training process, a scalable self-supervised learner named masked autoencoder is used for pre-training, which strengthens the visual representation ability and improves the efficiency of the feature learning. Experiments are conducted on cardiac magnetic resonance image segmentation where the left and right atrium and ventricle are selected for segmentation. The pixels accuracy is 93.85%, the Dice coefficient is 92.61% and Hausdorff distance is 11.16, which are improved compared with the other methods. The results show the superiority of Vit-UperNet in medical images segmentation, especially for the low-recognition and serious-occlusion targets.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d1faaa1d7d312dd5867683ce60519979de6b3349.pdf",
        "venue": "Complex &amp; Intelligent Systems",
        "citationCount": 13,
        "score": 13.0,
        "summary": "The existing image semantic segmentation models have low accuracy in detecting tiny targets or multi-targets at overlapping regions. This work proposes a hybrid vision transformer with unified-perceptual-parsing network (ViT-UperNet) for medical image segmentation. A self-attention mechanism is embedded in a vision transformer to extract multi-level features. The image features are extracted hierarchically from low to high dimensions using 4 groups of Transformer blocks with different numbers. Then, it uses a unified-perceptual-parsing network based on a feature pyramid network (FPN) and a pyramid pooling module (PPM) for the fusion of multi-scale contextual features and semantic segmentation. FPN can naturally use hierarchical features, and generate strong semantic information on all scales. PPM can better use the global prior knowledge to understand complex scenes, and extract features with global context information to improve segmentation results. In the training process, a scalable self-supervised learner named masked autoencoder is used for pre-training, which strengthens the visual representation ability and improves the efficiency of the feature learning. Experiments are conducted on cardiac magnetic resonance image segmentation where the left and right atrium and ventricle are selected for segmentation. The pixels accuracy is 93.85%, the Dice coefficient is 92.61% and Hausdorff distance is 11.16, which are improved compared with the other methods. The results show the superiority of Vit-UperNet in medical images segmentation, especially for the low-recognition and serious-occlusion targets.",
        "keywords": []
      },
      "file_name": "d1faaa1d7d312dd5867683ce60519979de6b3349.pdf"
    },
    {
      "success": true,
      "doc_id": "b1f18f0a977065e773c4067ae1a79abc",
      "summary": "Introduction The precise identification of retinal disorders is of utmost importance in the prevention of both temporary and permanent visual impairment. Prior research has yielded encouraging results in the classification of retinal images pertaining to a specific retinal condition. In clinical practice, it is not uncommon for a single patient to present with multiple retinal disorders concurrently. Hence, the task of classifying retinal images into multiple labels remains a significant obstacle for existing methodologies, but its successful accomplishment would yield valuable insights into a diverse array of situations simultaneously. Methods This study presents a novel vision transformer architecture called retinal ViT, which incorporates the self-attention mechanism into the field of medical image analysis. To note that this study supposed to prove that the transformer-based models can achieve competitive performance comparing with the CNN-based models, hence the convolutional modules have been eliminated from the proposed model. The suggested model concludes with a multi-label classifier that utilizes a feed-forward network architecture. This classifier consists of two layers and employs a sigmoid activation function. Results and discussion The experimental findings provide evidence of the improved performance exhibited by the suggested model when compared to state-of-the-art approaches such as ResNet, VGG, DenseNet, and MobileNet, on the publicly available dataset ODIR-2019, and the proposed approach has outperformed the state-of-the-art algorithms in terms of Kappa, F1 score, AUC, and AVG.",
      "intriguing_abstract": "Introduction The precise identification of retinal disorders is of utmost importance in the prevention of both temporary and permanent visual impairment. Prior research has yielded encouraging results in the classification of retinal images pertaining to a specific retinal condition. In clinical practice, it is not uncommon for a single patient to present with multiple retinal disorders concurrently. Hence, the task of classifying retinal images into multiple labels remains a significant obstacle for existing methodologies, but its successful accomplishment would yield valuable insights into a diverse array of situations simultaneously. Methods This study presents a novel vision transformer architecture called retinal ViT, which incorporates the self-attention mechanism into the field of medical image analysis. To note that this study supposed to prove that the transformer-based models can achieve competitive performance comparing with the CNN-based models, hence the convolutional modules have been eliminated from the proposed model. The suggested model concludes with a multi-label classifier that utilizes a feed-forward network architecture. This classifier consists of two layers and employs a sigmoid activation function. Results and discussion The experimental findings provide evidence of the improved performance exhibited by the suggested model when compared to state-of-the-art approaches such as ResNet, VGG, DenseNet, and MobileNet, on the publicly available dataset ODIR-2019, and the proposed approach has outperformed the state-of-the-art algorithms in terms of Kappa, F1 score, AUC, and AVG.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d68aedc3a957c25e5f19438ec4b64fb53b48422c.pdf",
      "citation_key": "wang20245hx",
      "metadata": {
        "title": "Multi-label classification of retinal disease via a novel vision transformer model",
        "authors": [
          "Dong Wang",
          "Jian Lian",
          "Wanzhen Jiao"
        ],
        "published_date": "2024",
        "abstract": "Introduction The precise identification of retinal disorders is of utmost importance in the prevention of both temporary and permanent visual impairment. Prior research has yielded encouraging results in the classification of retinal images pertaining to a specific retinal condition. In clinical practice, it is not uncommon for a single patient to present with multiple retinal disorders concurrently. Hence, the task of classifying retinal images into multiple labels remains a significant obstacle for existing methodologies, but its successful accomplishment would yield valuable insights into a diverse array of situations simultaneously. Methods This study presents a novel vision transformer architecture called retinal ViT, which incorporates the self-attention mechanism into the field of medical image analysis. To note that this study supposed to prove that the transformer-based models can achieve competitive performance comparing with the CNN-based models, hence the convolutional modules have been eliminated from the proposed model. The suggested model concludes with a multi-label classifier that utilizes a feed-forward network architecture. This classifier consists of two layers and employs a sigmoid activation function. Results and discussion The experimental findings provide evidence of the improved performance exhibited by the suggested model when compared to state-of-the-art approaches such as ResNet, VGG, DenseNet, and MobileNet, on the publicly available dataset ODIR-2019, and the proposed approach has outperformed the state-of-the-art algorithms in terms of Kappa, F1 score, AUC, and AVG.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d68aedc3a957c25e5f19438ec4b64fb53b48422c.pdf",
        "venue": "Frontiers in Neuroscience",
        "citationCount": 13,
        "score": 13.0,
        "summary": "Introduction The precise identification of retinal disorders is of utmost importance in the prevention of both temporary and permanent visual impairment. Prior research has yielded encouraging results in the classification of retinal images pertaining to a specific retinal condition. In clinical practice, it is not uncommon for a single patient to present with multiple retinal disorders concurrently. Hence, the task of classifying retinal images into multiple labels remains a significant obstacle for existing methodologies, but its successful accomplishment would yield valuable insights into a diverse array of situations simultaneously. Methods This study presents a novel vision transformer architecture called retinal ViT, which incorporates the self-attention mechanism into the field of medical image analysis. To note that this study supposed to prove that the transformer-based models can achieve competitive performance comparing with the CNN-based models, hence the convolutional modules have been eliminated from the proposed model. The suggested model concludes with a multi-label classifier that utilizes a feed-forward network architecture. This classifier consists of two layers and employs a sigmoid activation function. Results and discussion The experimental findings provide evidence of the improved performance exhibited by the suggested model when compared to state-of-the-art approaches such as ResNet, VGG, DenseNet, and MobileNet, on the publicly available dataset ODIR-2019, and the proposed approach has outperformed the state-of-the-art algorithms in terms of Kappa, F1 score, AUC, and AVG.",
        "keywords": []
      },
      "file_name": "d68aedc3a957c25e5f19438ec4b64fb53b48422c.pdf"
    },
    {
      "success": true,
      "doc_id": "69ce339710b531ce41831a73d7e013c6",
      "summary": "PurposeVision transformers (ViT) detectors excel in processing natural images. However, when processing remote sensing images (RSIs), ViT methods generally exhibit inferior accuracy compared to approaches based on convolutional neural networks (CNNs). Recently, researchers have proposed various structural optimization strategies to enhance the performance of ViT detectors, but the progress has been insignificant. We contend that the frequent scarcity of RSI samples is the primary cause of this problem, and model modifications alone cannot solve it.Design/methodology/approachTo address this, we introduce a faster RCNN-based approach, termed QAGA-Net, which significantly enhances the performance of ViT detectors in RSI recognition. Initially, we propose a novel quantitative augmentation learning (QAL) strategy to address the sparse data distribution in RSIs. This strategy is integrated as the QAL module, a plug-and-play component active exclusively during the models training phase. Subsequently, we enhanced the feature pyramid network (FPN) by introducing two efficient modules: a global attention (GA) module to model long-range feature dependencies and enhance multi-scale information fusion, and an efficient pooling (EP) module to optimize the models capability to understand both high and low frequency information. Importantly, QAGA-Net has a compact model size and achieves a balance between computational efficiency and accuracy.FindingsWe verified the performance of QAGA-Net by using two different efficient ViT models as the detectors backbone. Extensive experiments on the NWPU-10 and DIOR20 datasets demonstrate that QAGA-Net achieves superior accuracy compared to 23 other ViT or CNN methods in the literature. Specifically, QAGA-Net shows an increase in mAP by 2.1% or 2.6% on the challenging DIOR20 dataset when compared to the top-ranked CNN or ViT detectors, respectively.Originality/valueThis paper highlights the impact of sparse data distribution on ViT detection performance. To address this, we introduce a fundamentally data-driven approach: the QAL module. Additionally, we introduced two efficient modules to enhance the performance of FPN. More importantly, our strategy has the potential to collaborate with other ViT detectors, as the proposed method does not require any structural modifications to the ViT backbone.",
      "intriguing_abstract": "PurposeVision transformers (ViT) detectors excel in processing natural images. However, when processing remote sensing images (RSIs), ViT methods generally exhibit inferior accuracy compared to approaches based on convolutional neural networks (CNNs). Recently, researchers have proposed various structural optimization strategies to enhance the performance of ViT detectors, but the progress has been insignificant. We contend that the frequent scarcity of RSI samples is the primary cause of this problem, and model modifications alone cannot solve it.Design/methodology/approachTo address this, we introduce a faster RCNN-based approach, termed QAGA-Net, which significantly enhances the performance of ViT detectors in RSI recognition. Initially, we propose a novel quantitative augmentation learning (QAL) strategy to address the sparse data distribution in RSIs. This strategy is integrated as the QAL module, a plug-and-play component active exclusively during the models training phase. Subsequently, we enhanced the feature pyramid network (FPN) by introducing two efficient modules: a global attention (GA) module to model long-range feature dependencies and enhance multi-scale information fusion, and an efficient pooling (EP) module to optimize the models capability to understand both high and low frequency information. Importantly, QAGA-Net has a compact model size and achieves a balance between computational efficiency and accuracy.FindingsWe verified the performance of QAGA-Net by using two different efficient ViT models as the detectors backbone. Extensive experiments on the NWPU-10 and DIOR20 datasets demonstrate that QAGA-Net achieves superior accuracy compared to 23 other ViT or CNN methods in the literature. Specifically, QAGA-Net shows an increase in mAP by 2.1% or 2.6% on the challenging DIOR20 dataset when compared to the top-ranked CNN or ViT detectors, respectively.Originality/valueThis paper highlights the impact of sparse data distribution on ViT detection performance. To address this, we introduce a fundamentally data-driven approach: the QAL module. Additionally, we introduced two efficient modules to enhance the performance of FPN. More importantly, our strategy has the potential to collaborate with other ViT detectors, as the proposed method does not require any structural modifications to the ViT backbone.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/bb5e6bbd4ad8c613a0bac3db23940c03ce94bec9.pdf",
      "citation_key": "song202479c",
      "metadata": {
        "title": "QAGA-Net: enhanced vision transformer-based object detection for remote sensing images",
        "authors": [
          "Huaxiang Song",
          "Hanjun Xia",
          "Wenhui Wang",
          "Yang Zhou",
          "Wanbo Liu",
          "Qun Liu",
          "Jinling Liu"
        ],
        "published_date": "2024",
        "abstract": "PurposeVision transformers (ViT) detectors excel in processing natural images. However, when processing remote sensing images (RSIs), ViT methods generally exhibit inferior accuracy compared to approaches based on convolutional neural networks (CNNs). Recently, researchers have proposed various structural optimization strategies to enhance the performance of ViT detectors, but the progress has been insignificant. We contend that the frequent scarcity of RSI samples is the primary cause of this problem, and model modifications alone cannot solve it.Design/methodology/approachTo address this, we introduce a faster RCNN-based approach, termed QAGA-Net, which significantly enhances the performance of ViT detectors in RSI recognition. Initially, we propose a novel quantitative augmentation learning (QAL) strategy to address the sparse data distribution in RSIs. This strategy is integrated as the QAL module, a plug-and-play component active exclusively during the models training phase. Subsequently, we enhanced the feature pyramid network (FPN) by introducing two efficient modules: a global attention (GA) module to model long-range feature dependencies and enhance multi-scale information fusion, and an efficient pooling (EP) module to optimize the models capability to understand both high and low frequency information. Importantly, QAGA-Net has a compact model size and achieves a balance between computational efficiency and accuracy.FindingsWe verified the performance of QAGA-Net by using two different efficient ViT models as the detectors backbone. Extensive experiments on the NWPU-10 and DIOR20 datasets demonstrate that QAGA-Net achieves superior accuracy compared to 23 other ViT or CNN methods in the literature. Specifically, QAGA-Net shows an increase in mAP by 2.1% or 2.6% on the challenging DIOR20 dataset when compared to the top-ranked CNN or ViT detectors, respectively.Originality/valueThis paper highlights the impact of sparse data distribution on ViT detection performance. To address this, we introduce a fundamentally data-driven approach: the QAL module. Additionally, we introduced two efficient modules to enhance the performance of FPN. More importantly, our strategy has the potential to collaborate with other ViT detectors, as the proposed method does not require any structural modifications to the ViT backbone.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/bb5e6bbd4ad8c613a0bac3db23940c03ce94bec9.pdf",
        "venue": "International Journal of Intelligent Computing and Cybernetics",
        "citationCount": 13,
        "score": 13.0,
        "summary": "PurposeVision transformers (ViT) detectors excel in processing natural images. However, when processing remote sensing images (RSIs), ViT methods generally exhibit inferior accuracy compared to approaches based on convolutional neural networks (CNNs). Recently, researchers have proposed various structural optimization strategies to enhance the performance of ViT detectors, but the progress has been insignificant. We contend that the frequent scarcity of RSI samples is the primary cause of this problem, and model modifications alone cannot solve it.Design/methodology/approachTo address this, we introduce a faster RCNN-based approach, termed QAGA-Net, which significantly enhances the performance of ViT detectors in RSI recognition. Initially, we propose a novel quantitative augmentation learning (QAL) strategy to address the sparse data distribution in RSIs. This strategy is integrated as the QAL module, a plug-and-play component active exclusively during the models training phase. Subsequently, we enhanced the feature pyramid network (FPN) by introducing two efficient modules: a global attention (GA) module to model long-range feature dependencies and enhance multi-scale information fusion, and an efficient pooling (EP) module to optimize the models capability to understand both high and low frequency information. Importantly, QAGA-Net has a compact model size and achieves a balance between computational efficiency and accuracy.FindingsWe verified the performance of QAGA-Net by using two different efficient ViT models as the detectors backbone. Extensive experiments on the NWPU-10 and DIOR20 datasets demonstrate that QAGA-Net achieves superior accuracy compared to 23 other ViT or CNN methods in the literature. Specifically, QAGA-Net shows an increase in mAP by 2.1% or 2.6% on the challenging DIOR20 dataset when compared to the top-ranked CNN or ViT detectors, respectively.Originality/valueThis paper highlights the impact of sparse data distribution on ViT detection performance. To address this, we introduce a fundamentally data-driven approach: the QAL module. Additionally, we introduced two efficient modules to enhance the performance of FPN. More importantly, our strategy has the potential to collaborate with other ViT detectors, as the proposed method does not require any structural modifications to the ViT backbone.",
        "keywords": []
      },
      "file_name": "bb5e6bbd4ad8c613a0bac3db23940c03ce94bec9.pdf"
    },
    {
      "success": true,
      "doc_id": "6a2af6b1e1df3f6827af3e0cad51b60f",
      "summary": "Vision Transformer (ViT) has drawn the attention of many researchers in computer vision due to its superior performance in many computer vision tasks. However, there is limited research based on ViT models in finger vein recognition. This may be because the excellent performance of the ViT models relies on the abundance of training data, but finger vein databases are typically small. In this study, we focus on this question and proposed a model for finger vein recognition, referred to as FV-ViT. With only rigorous regularization added in the MLP head, called regMLP, instead of changing architecture in the ViT backbone, the proposed FV-ViT shows outstanding performance compared to other state-of-the-art works: 0.042% EER for FV-USM and 1.033% EER for SDUMLA-HMT. In addition, we also compare the baseline FV-ViT model with the corresponding ViT model trained with pretrained weights: 0.068% EER from non-pretrained FV-ViT base versus 0.116% EER from pretrained for FV-USM, 1.258% EER from non-pretrained FV-ViT base versus 1.022% EER from pretrained for SDUMLA-HMT. This means that the ViT models can be trained from scratch on finger vein databases and achieve comparable performance when compared to the pretrained model.",
      "intriguing_abstract": "Vision Transformer (ViT) has drawn the attention of many researchers in computer vision due to its superior performance in many computer vision tasks. However, there is limited research based on ViT models in finger vein recognition. This may be because the excellent performance of the ViT models relies on the abundance of training data, but finger vein databases are typically small. In this study, we focus on this question and proposed a model for finger vein recognition, referred to as FV-ViT. With only rigorous regularization added in the MLP head, called regMLP, instead of changing architecture in the ViT backbone, the proposed FV-ViT shows outstanding performance compared to other state-of-the-art works: 0.042% EER for FV-USM and 1.033% EER for SDUMLA-HMT. In addition, we also compare the baseline FV-ViT model with the corresponding ViT model trained with pretrained weights: 0.068% EER from non-pretrained FV-ViT base versus 0.116% EER from pretrained for FV-USM, 1.258% EER from non-pretrained FV-ViT base versus 1.022% EER from pretrained for SDUMLA-HMT. This means that the ViT models can be trained from scratch on finger vein databases and achieve comparable performance when compared to the pretrained model.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/55156532cb9c20fdcaed9ead238f7a2cbaab2527.pdf",
      "citation_key": "li2023lvd",
      "metadata": {
        "title": "FV-ViT: Vision Transformer for Finger Vein Recognition",
        "authors": [
          "Xiaoye Li",
          "Bin-Bin Zhang"
        ],
        "published_date": "2023",
        "abstract": "Vision Transformer (ViT) has drawn the attention of many researchers in computer vision due to its superior performance in many computer vision tasks. However, there is limited research based on ViT models in finger vein recognition. This may be because the excellent performance of the ViT models relies on the abundance of training data, but finger vein databases are typically small. In this study, we focus on this question and proposed a model for finger vein recognition, referred to as FV-ViT. With only rigorous regularization added in the MLP head, called regMLP, instead of changing architecture in the ViT backbone, the proposed FV-ViT shows outstanding performance compared to other state-of-the-art works: 0.042% EER for FV-USM and 1.033% EER for SDUMLA-HMT. In addition, we also compare the baseline FV-ViT model with the corresponding ViT model trained with pretrained weights: 0.068% EER from non-pretrained FV-ViT base versus 0.116% EER from pretrained for FV-USM, 1.258% EER from non-pretrained FV-ViT base versus 1.022% EER from pretrained for SDUMLA-HMT. This means that the ViT models can be trained from scratch on finger vein databases and achieve comparable performance when compared to the pretrained model.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/55156532cb9c20fdcaed9ead238f7a2cbaab2527.pdf",
        "venue": "IEEE Access",
        "citationCount": 25,
        "score": 12.5,
        "summary": "Vision Transformer (ViT) has drawn the attention of many researchers in computer vision due to its superior performance in many computer vision tasks. However, there is limited research based on ViT models in finger vein recognition. This may be because the excellent performance of the ViT models relies on the abundance of training data, but finger vein databases are typically small. In this study, we focus on this question and proposed a model for finger vein recognition, referred to as FV-ViT. With only rigorous regularization added in the MLP head, called regMLP, instead of changing architecture in the ViT backbone, the proposed FV-ViT shows outstanding performance compared to other state-of-the-art works: 0.042% EER for FV-USM and 1.033% EER for SDUMLA-HMT. In addition, we also compare the baseline FV-ViT model with the corresponding ViT model trained with pretrained weights: 0.068% EER from non-pretrained FV-ViT base versus 0.116% EER from pretrained for FV-USM, 1.258% EER from non-pretrained FV-ViT base versus 1.022% EER from pretrained for SDUMLA-HMT. This means that the ViT models can be trained from scratch on finger vein databases and achieve comparable performance when compared to the pretrained model.",
        "keywords": []
      },
      "file_name": "55156532cb9c20fdcaed9ead238f7a2cbaab2527.pdf"
    },
    {
      "success": true,
      "doc_id": "ded1b4278ebd645d61148c6d2e5161e3",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/ae7c5ae7d377189c5a5f33a60b33c6cc5ca56124.pdf",
      "citation_key": "ma2023vhi",
      "metadata": {
        "title": "IML-ViT: Image Manipulation Localization by Vision Transformer",
        "authors": [
          "Xiaochen Ma",
          "Bo Du",
          "Xianggen Liu",
          "Ahmed Y. Al Hammadi",
          "Jizhe Zhou"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ae7c5ae7d377189c5a5f33a60b33c6cc5ca56124.pdf",
        "venue": "arXiv.org",
        "citationCount": 25,
        "score": 12.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "ae7c5ae7d377189c5a5f33a60b33c6cc5ca56124.pdf"
    },
    {
      "success": true,
      "doc_id": "a181f788377267eb2c138343a358a3c1",
      "summary": "Human activity recognition has a wide range of applications in various fields, such as video surveillance, virtual reality and humancomputer intelligent interaction. It has emerged as a significant research area in computer vision. GCN (Graph Convolutional networks) have recently been widely used in these fields and have made great performance. However, there are still some challenges including over-smoothing problem caused by stack graph convolutions and deficient semantics correlation to capture the large movements between time sequences. Vision Transformer (ViT) is utilized in many 2D and 3D image fields and has surprised results. In our work, we propose a novel human activity recognition method based on ViT (HAR-ViT). We integrate enhanced AGCL (eAGCL) in 2s-AGCN to ViT to make it process spatio-temporal data (3D skeleton) and make full use of spatial features. The position encoder module orders the non-sequenced information while the transformer encoder efficiently compresses sequence data features to enhance calculation speed. Human activity recognition is accomplished through multi-layer perceptron (MLP) classifier. Experimental results demonstrate that the proposed method achieves SOTA performance on three extensively used datasets, NTU RGB+D 60, NTU RGB+D 120 and Kinetics-Skeleton 400.",
      "intriguing_abstract": "Human activity recognition has a wide range of applications in various fields, such as video surveillance, virtual reality and humancomputer intelligent interaction. It has emerged as a significant research area in computer vision. GCN (Graph Convolutional networks) have recently been widely used in these fields and have made great performance. However, there are still some challenges including over-smoothing problem caused by stack graph convolutions and deficient semantics correlation to capture the large movements between time sequences. Vision Transformer (ViT) is utilized in many 2D and 3D image fields and has surprised results. In our work, we propose a novel human activity recognition method based on ViT (HAR-ViT). We integrate enhanced AGCL (eAGCL) in 2s-AGCN to ViT to make it process spatio-temporal data (3D skeleton) and make full use of spatial features. The position encoder module orders the non-sequenced information while the transformer encoder efficiently compresses sequence data features to enhance calculation speed. Human activity recognition is accomplished through multi-layer perceptron (MLP) classifier. Experimental results demonstrate that the proposed method achieves SOTA performance on three extensively used datasets, NTU RGB+D 60, NTU RGB+D 120 and Kinetics-Skeleton 400.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/3b427c8d3258968b9ac5eaf846d99ac027de9a76.pdf",
      "citation_key": "han202416k",
      "metadata": {
        "title": "A human activity recognition method based on Vision Transformer",
        "authors": [
          "Huiyan Han",
          "H. Zeng",
          "Liqun Kuang",
          "Xie Han",
          "Hongxin Xue"
        ],
        "published_date": "2024",
        "abstract": "Human activity recognition has a wide range of applications in various fields, such as video surveillance, virtual reality and humancomputer intelligent interaction. It has emerged as a significant research area in computer vision. GCN (Graph Convolutional networks) have recently been widely used in these fields and have made great performance. However, there are still some challenges including over-smoothing problem caused by stack graph convolutions and deficient semantics correlation to capture the large movements between time sequences. Vision Transformer (ViT) is utilized in many 2D and 3D image fields and has surprised results. In our work, we propose a novel human activity recognition method based on ViT (HAR-ViT). We integrate enhanced AGCL (eAGCL) in 2s-AGCN to ViT to make it process spatio-temporal data (3D skeleton) and make full use of spatial features. The position encoder module orders the non-sequenced information while the transformer encoder efficiently compresses sequence data features to enhance calculation speed. Human activity recognition is accomplished through multi-layer perceptron (MLP) classifier. Experimental results demonstrate that the proposed method achieves SOTA performance on three extensively used datasets, NTU RGB+D 60, NTU RGB+D 120 and Kinetics-Skeleton 400.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3b427c8d3258968b9ac5eaf846d99ac027de9a76.pdf",
        "venue": "Scientific Reports",
        "citationCount": 12,
        "score": 12.0,
        "summary": "Human activity recognition has a wide range of applications in various fields, such as video surveillance, virtual reality and humancomputer intelligent interaction. It has emerged as a significant research area in computer vision. GCN (Graph Convolutional networks) have recently been widely used in these fields and have made great performance. However, there are still some challenges including over-smoothing problem caused by stack graph convolutions and deficient semantics correlation to capture the large movements between time sequences. Vision Transformer (ViT) is utilized in many 2D and 3D image fields and has surprised results. In our work, we propose a novel human activity recognition method based on ViT (HAR-ViT). We integrate enhanced AGCL (eAGCL) in 2s-AGCN to ViT to make it process spatio-temporal data (3D skeleton) and make full use of spatial features. The position encoder module orders the non-sequenced information while the transformer encoder efficiently compresses sequence data features to enhance calculation speed. Human activity recognition is accomplished through multi-layer perceptron (MLP) classifier. Experimental results demonstrate that the proposed method achieves SOTA performance on three extensively used datasets, NTU RGB+D 60, NTU RGB+D 120 and Kinetics-Skeleton 400.",
        "keywords": []
      },
      "file_name": "3b427c8d3258968b9ac5eaf846d99ac027de9a76.pdf"
    },
    {
      "success": true,
      "doc_id": "f33a976de098ac46249c0c2a4c4fbbb2",
      "summary": "White blood cells (WBCs) are crucial components of the immune system that play a vital role in defending the body against infections and diseases. The identification of WBCs subtypes is useful in the detection of various diseases, such as infections, leukemia, and other hematological malignancies. The manual screening of blood films is time-consuming and subjective, leading to inconsistencies and errors. Convolutional neural networks (CNN)-based models can automate such classification processes, but are incapable of capturing long-range dependencies and global context. This paper proposes an explainable Vision Transformer (ViT) model for automatic WBCs detection from blood films. The proposed model uses a self-attention mechanism to extract features from input images. Our proposed model was trained and validated on a public dataset of 16,633 samples containing five different types of WBCs. As a result of experiments on the classification of five different types of WBCs, our model achieved an accuracy of 99.40%. Moreover, the models examination of misclassified test samples revealed a correlation between incorrect predictions and the presence or absence of granules in the cell samples. To validate this observation, we divided the dataset into two classes, Granulocytes and Agranulocytes, and conducted a secondary training process. The resulting ViT model, trained for binary classification, achieved impressive performance metrics during the test phase, including an accuracy of 99.70%, recall of 99.54%, precision of 99.32%, and F-1 score of 99.43%. To ensure the reliability of the ViT models, we employed the Score-CAM algorithm to visualize the pixel areas on which the model focuses during its predictions. Our proposed method is suitable for clinical use due to its explainable structure as well as its superior performance compared to similar studies in the literature. The classification and localization of WBCs with this model can facilitate the detection and reporting process for the pathologist.",
      "intriguing_abstract": "White blood cells (WBCs) are crucial components of the immune system that play a vital role in defending the body against infections and diseases. The identification of WBCs subtypes is useful in the detection of various diseases, such as infections, leukemia, and other hematological malignancies. The manual screening of blood films is time-consuming and subjective, leading to inconsistencies and errors. Convolutional neural networks (CNN)-based models can automate such classification processes, but are incapable of capturing long-range dependencies and global context. This paper proposes an explainable Vision Transformer (ViT) model for automatic WBCs detection from blood films. The proposed model uses a self-attention mechanism to extract features from input images. Our proposed model was trained and validated on a public dataset of 16,633 samples containing five different types of WBCs. As a result of experiments on the classification of five different types of WBCs, our model achieved an accuracy of 99.40%. Moreover, the models examination of misclassified test samples revealed a correlation between incorrect predictions and the presence or absence of granules in the cell samples. To validate this observation, we divided the dataset into two classes, Granulocytes and Agranulocytes, and conducted a secondary training process. The resulting ViT model, trained for binary classification, achieved impressive performance metrics during the test phase, including an accuracy of 99.70%, recall of 99.54%, precision of 99.32%, and F-1 score of 99.43%. To ensure the reliability of the ViT models, we employed the Score-CAM algorithm to visualize the pixel areas on which the model focuses during its predictions. Our proposed method is suitable for clinical use due to its explainable structure as well as its superior performance compared to similar studies in the literature. The classification and localization of WBCs with this model can facilitate the detection and reporting process for the pathologist.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/10e9943b3a974ac6175ffec3228e670ec9d2cc18.pdf",
      "citation_key": "katar202352u",
      "metadata": {
        "title": "An Explainable Vision Transformer Model Based White Blood Cells Classification and Localization",
        "authors": [
          "Ouzhan Katar",
          "Ozal Yildirim"
        ],
        "published_date": "2023",
        "abstract": "White blood cells (WBCs) are crucial components of the immune system that play a vital role in defending the body against infections and diseases. The identification of WBCs subtypes is useful in the detection of various diseases, such as infections, leukemia, and other hematological malignancies. The manual screening of blood films is time-consuming and subjective, leading to inconsistencies and errors. Convolutional neural networks (CNN)-based models can automate such classification processes, but are incapable of capturing long-range dependencies and global context. This paper proposes an explainable Vision Transformer (ViT) model for automatic WBCs detection from blood films. The proposed model uses a self-attention mechanism to extract features from input images. Our proposed model was trained and validated on a public dataset of 16,633 samples containing five different types of WBCs. As a result of experiments on the classification of five different types of WBCs, our model achieved an accuracy of 99.40%. Moreover, the models examination of misclassified test samples revealed a correlation between incorrect predictions and the presence or absence of granules in the cell samples. To validate this observation, we divided the dataset into two classes, Granulocytes and Agranulocytes, and conducted a secondary training process. The resulting ViT model, trained for binary classification, achieved impressive performance metrics during the test phase, including an accuracy of 99.70%, recall of 99.54%, precision of 99.32%, and F-1 score of 99.43%. To ensure the reliability of the ViT models, we employed the Score-CAM algorithm to visualize the pixel areas on which the model focuses during its predictions. Our proposed method is suitable for clinical use due to its explainable structure as well as its superior performance compared to similar studies in the literature. The classification and localization of WBCs with this model can facilitate the detection and reporting process for the pathologist.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/10e9943b3a974ac6175ffec3228e670ec9d2cc18.pdf",
        "venue": "Diagnostics",
        "citationCount": 24,
        "score": 12.0,
        "summary": "White blood cells (WBCs) are crucial components of the immune system that play a vital role in defending the body against infections and diseases. The identification of WBCs subtypes is useful in the detection of various diseases, such as infections, leukemia, and other hematological malignancies. The manual screening of blood films is time-consuming and subjective, leading to inconsistencies and errors. Convolutional neural networks (CNN)-based models can automate such classification processes, but are incapable of capturing long-range dependencies and global context. This paper proposes an explainable Vision Transformer (ViT) model for automatic WBCs detection from blood films. The proposed model uses a self-attention mechanism to extract features from input images. Our proposed model was trained and validated on a public dataset of 16,633 samples containing five different types of WBCs. As a result of experiments on the classification of five different types of WBCs, our model achieved an accuracy of 99.40%. Moreover, the models examination of misclassified test samples revealed a correlation between incorrect predictions and the presence or absence of granules in the cell samples. To validate this observation, we divided the dataset into two classes, Granulocytes and Agranulocytes, and conducted a secondary training process. The resulting ViT model, trained for binary classification, achieved impressive performance metrics during the test phase, including an accuracy of 99.70%, recall of 99.54%, precision of 99.32%, and F-1 score of 99.43%. To ensure the reliability of the ViT models, we employed the Score-CAM algorithm to visualize the pixel areas on which the model focuses during its predictions. Our proposed method is suitable for clinical use due to its explainable structure as well as its superior performance compared to similar studies in the literature. The classification and localization of WBCs with this model can facilitate the detection and reporting process for the pathologist.",
        "keywords": []
      },
      "file_name": "10e9943b3a974ac6175ffec3228e670ec9d2cc18.pdf"
    },
    {
      "success": true,
      "doc_id": "61d3030e84e5b2a81009d10238aa9695",
      "summary": "Plant disease detection is a critical task in agriculture, essential for ensuring crop health and productivity. Traditional methods in this context are often labor-intensive and prone to errors, highlighting the need for automated solutions. While computer vision-based solutions have been successfully deployed in recent years for plant disease identification and localization tasks, these often operate independently, leading to suboptimal performance. It is essential to develop an integrated solution combining these two tasks for improved efficiency and accuracy. This research proposes the innovative Plant Disease Localization and Classification model based on Vision Transformer (PDLC-ViT), which integrates co-scale, co-attention, and cross-attention mechanisms and a ViT, within a Multi-Task Learning (MTL) framework. The model was trained and evaluated on the Plant Village dataset. Key hyperparameters, including learning rate, batch size, dropout ratio, and regularization factor, were optimized through a thorough grid search. Early stopping based on validation loss was employed to prevent overfitting. The PDLC-ViT model demonstrated significant improvements in plant disease localization and classification tasks. The integration of co-scale, co-attention, and cross-attention mechanisms allowed the model to capture multi-scale dependencies and enhance feature learning, leading to superior performance compared to existing models. The PDLC-ViT model evaluated on two public datasets achieved an accuracy of 99.97%, a Mean Average Precision (MAP) of 99.18%, and a Mean Average Recall (MAR) of 99.11%. These results underscore the model's exceptional precision and recall, highlighting its robustness and reliability in detecting and classifying plant diseases. The PDLC-ViT model sets a new benchmark in plant disease detection, offering a reliable and advanced tool for agricultural applications. Its ability to integrate localization and classification tasks within an MTL framework promotes timely and accurate disease management, contributing to sustainable agriculture and food security.",
      "intriguing_abstract": "Plant disease detection is a critical task in agriculture, essential for ensuring crop health and productivity. Traditional methods in this context are often labor-intensive and prone to errors, highlighting the need for automated solutions. While computer vision-based solutions have been successfully deployed in recent years for plant disease identification and localization tasks, these often operate independently, leading to suboptimal performance. It is essential to develop an integrated solution combining these two tasks for improved efficiency and accuracy. This research proposes the innovative Plant Disease Localization and Classification model based on Vision Transformer (PDLC-ViT), which integrates co-scale, co-attention, and cross-attention mechanisms and a ViT, within a Multi-Task Learning (MTL) framework. The model was trained and evaluated on the Plant Village dataset. Key hyperparameters, including learning rate, batch size, dropout ratio, and regularization factor, were optimized through a thorough grid search. Early stopping based on validation loss was employed to prevent overfitting. The PDLC-ViT model demonstrated significant improvements in plant disease localization and classification tasks. The integration of co-scale, co-attention, and cross-attention mechanisms allowed the model to capture multi-scale dependencies and enhance feature learning, leading to superior performance compared to existing models. The PDLC-ViT model evaluated on two public datasets achieved an accuracy of 99.97%, a Mean Average Precision (MAP) of 99.18%, and a Mean Average Recall (MAR) of 99.11%. These results underscore the model's exceptional precision and recall, highlighting its robustness and reliability in detecting and classifying plant diseases. The PDLC-ViT model sets a new benchmark in plant disease detection, offering a reliable and advanced tool for agricultural applications. Its ability to integrate localization and classification tasks within an MTL framework promotes timely and accurate disease management, contributing to sustainable agriculture and food security.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/1783c85f8b951dc11c9dcb4929bfcae2d52ecb63.pdf",
      "citation_key": "hemalatha2024a14",
      "metadata": {
        "title": "A Multitask Learning-Based Vision Transformer for Plant Disease Localization and Classification",
        "authors": [
          "S. Hemalatha",
          "Jayachandiran Jai Jaganath Babu"
        ],
        "published_date": "2024",
        "abstract": "Plant disease detection is a critical task in agriculture, essential for ensuring crop health and productivity. Traditional methods in this context are often labor-intensive and prone to errors, highlighting the need for automated solutions. While computer vision-based solutions have been successfully deployed in recent years for plant disease identification and localization tasks, these often operate independently, leading to suboptimal performance. It is essential to develop an integrated solution combining these two tasks for improved efficiency and accuracy. This research proposes the innovative Plant Disease Localization and Classification model based on Vision Transformer (PDLC-ViT), which integrates co-scale, co-attention, and cross-attention mechanisms and a ViT, within a Multi-Task Learning (MTL) framework. The model was trained and evaluated on the Plant Village dataset. Key hyperparameters, including learning rate, batch size, dropout ratio, and regularization factor, were optimized through a thorough grid search. Early stopping based on validation loss was employed to prevent overfitting. The PDLC-ViT model demonstrated significant improvements in plant disease localization and classification tasks. The integration of co-scale, co-attention, and cross-attention mechanisms allowed the model to capture multi-scale dependencies and enhance feature learning, leading to superior performance compared to existing models. The PDLC-ViT model evaluated on two public datasets achieved an accuracy of 99.97%, a Mean Average Precision (MAP) of 99.18%, and a Mean Average Recall (MAR) of 99.11%. These results underscore the model's exceptional precision and recall, highlighting its robustness and reliability in detecting and classifying plant diseases. The PDLC-ViT model sets a new benchmark in plant disease detection, offering a reliable and advanced tool for agricultural applications. Its ability to integrate localization and classification tasks within an MTL framework promotes timely and accurate disease management, contributing to sustainable agriculture and food security.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1783c85f8b951dc11c9dcb4929bfcae2d52ecb63.pdf",
        "venue": "International Journal of Computational Intelligence Systems",
        "citationCount": 12,
        "score": 12.0,
        "summary": "Plant disease detection is a critical task in agriculture, essential for ensuring crop health and productivity. Traditional methods in this context are often labor-intensive and prone to errors, highlighting the need for automated solutions. While computer vision-based solutions have been successfully deployed in recent years for plant disease identification and localization tasks, these often operate independently, leading to suboptimal performance. It is essential to develop an integrated solution combining these two tasks for improved efficiency and accuracy. This research proposes the innovative Plant Disease Localization and Classification model based on Vision Transformer (PDLC-ViT), which integrates co-scale, co-attention, and cross-attention mechanisms and a ViT, within a Multi-Task Learning (MTL) framework. The model was trained and evaluated on the Plant Village dataset. Key hyperparameters, including learning rate, batch size, dropout ratio, and regularization factor, were optimized through a thorough grid search. Early stopping based on validation loss was employed to prevent overfitting. The PDLC-ViT model demonstrated significant improvements in plant disease localization and classification tasks. The integration of co-scale, co-attention, and cross-attention mechanisms allowed the model to capture multi-scale dependencies and enhance feature learning, leading to superior performance compared to existing models. The PDLC-ViT model evaluated on two public datasets achieved an accuracy of 99.97%, a Mean Average Precision (MAP) of 99.18%, and a Mean Average Recall (MAR) of 99.11%. These results underscore the model's exceptional precision and recall, highlighting its robustness and reliability in detecting and classifying plant diseases. The PDLC-ViT model sets a new benchmark in plant disease detection, offering a reliable and advanced tool for agricultural applications. Its ability to integrate localization and classification tasks within an MTL framework promotes timely and accurate disease management, contributing to sustainable agriculture and food security.",
        "keywords": []
      },
      "file_name": "1783c85f8b951dc11c9dcb4929bfcae2d52ecb63.pdf"
    },
    {
      "success": true,
      "doc_id": "b39877dc6133c00e10eb0e6b1a21c0c3",
      "summary": "We present ProtoViT, a method for interpretable image classification combining deep learning and case-based reasoning. This method classifies an image by comparing it to a set of learned prototypes, providing explanations of the form ``this looks like that.''In our model, a prototype consists of \\textit{parts}, which can deform over irregular geometries to create a better comparison between images. Unlike existing models that rely on Convolutional Neural Network (CNN) backbones and spatially rigid prototypes, our model integrates Vision Transformer (ViT) backbones into prototype based models, while offering spatially deformed prototypes that not only accommodate geometric variations of objects but also provide coherent and clear prototypical feature representations with an adaptive number of prototypical parts. Our experiments show that our model can generally achieve higher performance than the existing prototype based models. Our comprehensive analyses ensure that the prototypes are consistent and the interpretations are faithful.",
      "intriguing_abstract": "We present ProtoViT, a method for interpretable image classification combining deep learning and case-based reasoning. This method classifies an image by comparing it to a set of learned prototypes, providing explanations of the form ``this looks like that.''In our model, a prototype consists of \\textit{parts}, which can deform over irregular geometries to create a better comparison between images. Unlike existing models that rely on Convolutional Neural Network (CNN) backbones and spatially rigid prototypes, our model integrates Vision Transformer (ViT) backbones into prototype based models, while offering spatially deformed prototypes that not only accommodate geometric variations of objects but also provide coherent and clear prototypical feature representations with an adaptive number of prototypical parts. Our experiments show that our model can generally achieve higher performance than the existing prototype based models. Our comprehensive analyses ensure that the prototypes are consistent and the interpretations are faithful.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/42bd7d3ca2b66eca5609fe77078fe8951ab9fd5c.pdf",
      "citation_key": "ma2024uan",
      "metadata": {
        "title": "Interpretable Image Classification with Adaptive Prototype-based Vision Transformers",
        "authors": [
          "Chiyu Ma",
          "Jon Donnelly",
          "Wenjun Liu",
          "Soroush Vosoughi",
          "Cynthia Rudin",
          "Chaofan Chen"
        ],
        "published_date": "2024",
        "abstract": "We present ProtoViT, a method for interpretable image classification combining deep learning and case-based reasoning. This method classifies an image by comparing it to a set of learned prototypes, providing explanations of the form ``this looks like that.''In our model, a prototype consists of \\textit{parts}, which can deform over irregular geometries to create a better comparison between images. Unlike existing models that rely on Convolutional Neural Network (CNN) backbones and spatially rigid prototypes, our model integrates Vision Transformer (ViT) backbones into prototype based models, while offering spatially deformed prototypes that not only accommodate geometric variations of objects but also provide coherent and clear prototypical feature representations with an adaptive number of prototypical parts. Our experiments show that our model can generally achieve higher performance than the existing prototype based models. Our comprehensive analyses ensure that the prototypes are consistent and the interpretations are faithful.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/42bd7d3ca2b66eca5609fe77078fe8951ab9fd5c.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 12,
        "score": 12.0,
        "summary": "We present ProtoViT, a method for interpretable image classification combining deep learning and case-based reasoning. This method classifies an image by comparing it to a set of learned prototypes, providing explanations of the form ``this looks like that.''In our model, a prototype consists of \\textit{parts}, which can deform over irregular geometries to create a better comparison between images. Unlike existing models that rely on Convolutional Neural Network (CNN) backbones and spatially rigid prototypes, our model integrates Vision Transformer (ViT) backbones into prototype based models, while offering spatially deformed prototypes that not only accommodate geometric variations of objects but also provide coherent and clear prototypical feature representations with an adaptive number of prototypical parts. Our experiments show that our model can generally achieve higher performance than the existing prototype based models. Our comprehensive analyses ensure that the prototypes are consistent and the interpretations are faithful.",
        "keywords": []
      },
      "file_name": "42bd7d3ca2b66eca5609fe77078fe8951ab9fd5c.pdf"
    },
    {
      "success": true,
      "doc_id": "e7ace2c9d7686ffbee30e0375c080fc5",
      "summary": "Sleep posture has a crucial impact on the incidence and severity of obstructive sleep apnea (OSA). Therefore, the surveillance and recognition of sleep postures could facilitate the assessment of OSA. The existing contact-based systems might interfere with sleeping, while camera-based systems introduce privacy concerns. Radar-based systems might overcome these challenges, especially when individuals are covered with blankets. The aim of this research is to develop a nonobstructive multiple ultra-wideband radar sleep posture recognition system based on machine learning models. We evaluated three single-radar configurations (top, side, and head), three dual-radar configurations (top + side, top + head, and side + head), and one tri-radar configuration (top + side + head), in addition to machine learning models, including CNN-based networks (ResNet50, DenseNet121, and EfficientNetV2) and vision transformer-based networks (traditional vision transformer and Swin Transformer V2). Thirty participants (n = 30) were invited to perform four recumbent postures (supine, left side-lying, right side-lying, and prone). Data from eighteen participants were randomly chosen for model training, another six participants data (n = 6) for model validation, and the remaining six participants data (n = 6) for model testing. The Swin Transformer with side and head radar configuration achieved the highest prediction accuracy (0.808). Future research may consider the application of the synthetic aperture radar technique.",
      "intriguing_abstract": "Sleep posture has a crucial impact on the incidence and severity of obstructive sleep apnea (OSA). Therefore, the surveillance and recognition of sleep postures could facilitate the assessment of OSA. The existing contact-based systems might interfere with sleeping, while camera-based systems introduce privacy concerns. Radar-based systems might overcome these challenges, especially when individuals are covered with blankets. The aim of this research is to develop a nonobstructive multiple ultra-wideband radar sleep posture recognition system based on machine learning models. We evaluated three single-radar configurations (top, side, and head), three dual-radar configurations (top + side, top + head, and side + head), and one tri-radar configuration (top + side + head), in addition to machine learning models, including CNN-based networks (ResNet50, DenseNet121, and EfficientNetV2) and vision transformer-based networks (traditional vision transformer and Swin Transformer V2). Thirty participants (n = 30) were invited to perform four recumbent postures (supine, left side-lying, right side-lying, and prone). Data from eighteen participants were randomly chosen for model training, another six participants data (n = 6) for model validation, and the remaining six participants data (n = 6) for model testing. The Swin Transformer with side and head radar configuration achieved the highest prediction accuracy (0.808). Future research may consider the application of the synthetic aperture radar technique.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/7aeadf7b9d839bdd9dbfbff5239f71792ff0c04c.pdf",
      "citation_key": "lai20238ck",
      "metadata": {
        "title": "Vision Transformers (ViT) for Blanket-Penetrating Sleep Posture Recognition Using a Triple Ultra-Wideband (UWB) Radar System",
        "authors": [
          "D. K. Lai",
          "Zi-Han Yu",
          "Tommy Yau-Nam Leung",
          "Hyo-Jung Lim",
          "Andy Yiu-Chau Tam",
          "Bryan Pak-Hei So",
          "Ye-Jiao Mao",
          "D. Cheung",
          "D. Wong",
          "C. Cheung"
        ],
        "published_date": "2023",
        "abstract": "Sleep posture has a crucial impact on the incidence and severity of obstructive sleep apnea (OSA). Therefore, the surveillance and recognition of sleep postures could facilitate the assessment of OSA. The existing contact-based systems might interfere with sleeping, while camera-based systems introduce privacy concerns. Radar-based systems might overcome these challenges, especially when individuals are covered with blankets. The aim of this research is to develop a nonobstructive multiple ultra-wideband radar sleep posture recognition system based on machine learning models. We evaluated three single-radar configurations (top, side, and head), three dual-radar configurations (top + side, top + head, and side + head), and one tri-radar configuration (top + side + head), in addition to machine learning models, including CNN-based networks (ResNet50, DenseNet121, and EfficientNetV2) and vision transformer-based networks (traditional vision transformer and Swin Transformer V2). Thirty participants (n = 30) were invited to perform four recumbent postures (supine, left side-lying, right side-lying, and prone). Data from eighteen participants were randomly chosen for model training, another six participants data (n = 6) for model validation, and the remaining six participants data (n = 6) for model testing. The Swin Transformer with side and head radar configuration achieved the highest prediction accuracy (0.808). Future research may consider the application of the synthetic aperture radar technique.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7aeadf7b9d839bdd9dbfbff5239f71792ff0c04c.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 22,
        "score": 11.0,
        "summary": "Sleep posture has a crucial impact on the incidence and severity of obstructive sleep apnea (OSA). Therefore, the surveillance and recognition of sleep postures could facilitate the assessment of OSA. The existing contact-based systems might interfere with sleeping, while camera-based systems introduce privacy concerns. Radar-based systems might overcome these challenges, especially when individuals are covered with blankets. The aim of this research is to develop a nonobstructive multiple ultra-wideband radar sleep posture recognition system based on machine learning models. We evaluated three single-radar configurations (top, side, and head), three dual-radar configurations (top + side, top + head, and side + head), and one tri-radar configuration (top + side + head), in addition to machine learning models, including CNN-based networks (ResNet50, DenseNet121, and EfficientNetV2) and vision transformer-based networks (traditional vision transformer and Swin Transformer V2). Thirty participants (n = 30) were invited to perform four recumbent postures (supine, left side-lying, right side-lying, and prone). Data from eighteen participants were randomly chosen for model training, another six participants data (n = 6) for model validation, and the remaining six participants data (n = 6) for model testing. The Swin Transformer with side and head radar configuration achieved the highest prediction accuracy (0.808). Future research may consider the application of the synthetic aperture radar technique.",
        "keywords": []
      },
      "file_name": "7aeadf7b9d839bdd9dbfbff5239f71792ff0c04c.pdf"
    },
    {
      "success": true,
      "doc_id": "40901310c33460eed87ae41a55d477ef",
      "summary": "In recent years, the rapid advancement of deepfake technology has revolutionized content creation, lowering forgery costs while elevating quality. However, this progress brings forth pressing concerns such as infringements on individual rights, national security threats, and risks to public safety. To counter these challenges, various detection methodologies have emerged, with Vision Transformer (ViT)-based approaches showcasing superior performance in generality and efficiency. This survey presents a timely overview of ViT-based deepfake detection models, categorized into standalone, sequential, and parallel architectures. Furthermore, it succinctly delineates the structure and characteristics of each model. By analyzing existing research and addressing future directions, this survey aims to equip researchers with a nuanced understanding of ViT's pivotal role in deepfake detection, serving as a valuable reference for both academic and practical pursuits in this domain.",
      "intriguing_abstract": "In recent years, the rapid advancement of deepfake technology has revolutionized content creation, lowering forgery costs while elevating quality. However, this progress brings forth pressing concerns such as infringements on individual rights, national security threats, and risks to public safety. To counter these challenges, various detection methodologies have emerged, with Vision Transformer (ViT)-based approaches showcasing superior performance in generality and efficiency. This survey presents a timely overview of ViT-based deepfake detection models, categorized into standalone, sequential, and parallel architectures. Furthermore, it succinctly delineates the structure and characteristics of each model. By analyzing existing research and addressing future directions, this survey aims to equip researchers with a nuanced understanding of ViT's pivotal role in deepfake detection, serving as a valuable reference for both academic and practical pursuits in this domain.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/b2becca9911c155bf97656df8e5079ca76767ab9.pdf",
      "citation_key": "wang2024luv",
      "metadata": {
        "title": "A Timely Survey on Vision Transformer for Deepfake Detection",
        "authors": [
          "Zhikan Wang",
          "Zhongyao Cheng",
          "Jiajie Xiong",
          "Xun Xu",
          "Tianrui Li",
          "B. Veeravalli",
          "Xulei Yang"
        ],
        "published_date": "2024",
        "abstract": "In recent years, the rapid advancement of deepfake technology has revolutionized content creation, lowering forgery costs while elevating quality. However, this progress brings forth pressing concerns such as infringements on individual rights, national security threats, and risks to public safety. To counter these challenges, various detection methodologies have emerged, with Vision Transformer (ViT)-based approaches showcasing superior performance in generality and efficiency. This survey presents a timely overview of ViT-based deepfake detection models, categorized into standalone, sequential, and parallel architectures. Furthermore, it succinctly delineates the structure and characteristics of each model. By analyzing existing research and addressing future directions, this survey aims to equip researchers with a nuanced understanding of ViT's pivotal role in deepfake detection, serving as a valuable reference for both academic and practical pursuits in this domain.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b2becca9911c155bf97656df8e5079ca76767ab9.pdf",
        "venue": "arXiv.org",
        "citationCount": 11,
        "score": 11.0,
        "summary": "In recent years, the rapid advancement of deepfake technology has revolutionized content creation, lowering forgery costs while elevating quality. However, this progress brings forth pressing concerns such as infringements on individual rights, national security threats, and risks to public safety. To counter these challenges, various detection methodologies have emerged, with Vision Transformer (ViT)-based approaches showcasing superior performance in generality and efficiency. This survey presents a timely overview of ViT-based deepfake detection models, categorized into standalone, sequential, and parallel architectures. Furthermore, it succinctly delineates the structure and characteristics of each model. By analyzing existing research and addressing future directions, this survey aims to equip researchers with a nuanced understanding of ViT's pivotal role in deepfake detection, serving as a valuable reference for both academic and practical pursuits in this domain.",
        "keywords": []
      },
      "file_name": "b2becca9911c155bf97656df8e5079ca76767ab9.pdf"
    },
    {
      "success": true,
      "doc_id": "ff6d9e18d8504758de51fe074ddc3c09",
      "summary": "In panorama understanding, the widely used equirectangular projection (ERP) entails boundary discontinuity and spatial distortion. It severely deteriorates the conventional CNNs and vision Transformers on panoramas. In this paper, we propose a simple yet effective architecture named PanoSwin to learn panorama representations with ERP. To deal with the challenges brought by equirectangular projection, we explore a pano-style shift windowing scheme and novel pitch attention to address the boundary discontinuity and the spatial distortion, respectively. Besides, based on spherical distance and Cartesian coordinates, we adapt absolute positional embeddings and relative positional biases for panoramas to enhance panoramic geometry information. Realizing that planar image understanding might share some common knowledge with panorama understanding, we devise a novel two-stage learning framework to facilitate knowledge transfer from the planar images to panoramas. We conduct experiments against the state-of-the-art on various panoramic tasks, i.e., panoramic object detection, panoramic classification, and panoramic layout estimation. The experimental results demonstrate the effectiveness of PanoSwin in panorama understanding.",
      "intriguing_abstract": "In panorama understanding, the widely used equirectangular projection (ERP) entails boundary discontinuity and spatial distortion. It severely deteriorates the conventional CNNs and vision Transformers on panoramas. In this paper, we propose a simple yet effective architecture named PanoSwin to learn panorama representations with ERP. To deal with the challenges brought by equirectangular projection, we explore a pano-style shift windowing scheme and novel pitch attention to address the boundary discontinuity and the spatial distortion, respectively. Besides, based on spherical distance and Cartesian coordinates, we adapt absolute positional embeddings and relative positional biases for panoramas to enhance panoramic geometry information. Realizing that planar image understanding might share some common knowledge with panorama understanding, we devise a novel two-stage learning framework to facilitate knowledge transfer from the planar images to panoramas. We conduct experiments against the state-of-the-art on various panoramic tasks, i.e., panoramic object detection, panoramic classification, and panoramic layout estimation. The experimental results demonstrate the effectiveness of PanoSwin in panorama understanding.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/25a69bdd7f87ecc287b4e9a64eb4d6d562371add.pdf",
      "citation_key": "ling2023x36",
      "metadata": {
        "title": "PanoSwin: a Pano-style Swin Transformer for Panorama Understanding",
        "authors": [
          "Zhixin Ling",
          "Zhen Xing",
          "Xiangdong Zhou",
          "Manliang Cao",
          "G. Zhou"
        ],
        "published_date": "2023",
        "abstract": "In panorama understanding, the widely used equirectangular projection (ERP) entails boundary discontinuity and spatial distortion. It severely deteriorates the conventional CNNs and vision Transformers on panoramas. In this paper, we propose a simple yet effective architecture named PanoSwin to learn panorama representations with ERP. To deal with the challenges brought by equirectangular projection, we explore a pano-style shift windowing scheme and novel pitch attention to address the boundary discontinuity and the spatial distortion, respectively. Besides, based on spherical distance and Cartesian coordinates, we adapt absolute positional embeddings and relative positional biases for panoramas to enhance panoramic geometry information. Realizing that planar image understanding might share some common knowledge with panorama understanding, we devise a novel two-stage learning framework to facilitate knowledge transfer from the planar images to panoramas. We conduct experiments against the state-of-the-art on various panoramic tasks, i.e., panoramic object detection, panoramic classification, and panoramic layout estimation. The experimental results demonstrate the effectiveness of PanoSwin in panorama understanding.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/25a69bdd7f87ecc287b4e9a64eb4d6d562371add.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 20,
        "score": 10.0,
        "summary": "In panorama understanding, the widely used equirectangular projection (ERP) entails boundary discontinuity and spatial distortion. It severely deteriorates the conventional CNNs and vision Transformers on panoramas. In this paper, we propose a simple yet effective architecture named PanoSwin to learn panorama representations with ERP. To deal with the challenges brought by equirectangular projection, we explore a pano-style shift windowing scheme and novel pitch attention to address the boundary discontinuity and the spatial distortion, respectively. Besides, based on spherical distance and Cartesian coordinates, we adapt absolute positional embeddings and relative positional biases for panoramas to enhance panoramic geometry information. Realizing that planar image understanding might share some common knowledge with panorama understanding, we devise a novel two-stage learning framework to facilitate knowledge transfer from the planar images to panoramas. We conduct experiments against the state-of-the-art on various panoramic tasks, i.e., panoramic object detection, panoramic classification, and panoramic layout estimation. The experimental results demonstrate the effectiveness of PanoSwin in panorama understanding.",
        "keywords": []
      },
      "file_name": "25a69bdd7f87ecc287b4e9a64eb4d6d562371add.pdf"
    },
    {
      "success": true,
      "doc_id": "ec3e8587f8c04f469722a20eecc40368",
      "summary": "Artificial intelligence (AI) and computer vision technologies have gained significant prominence in the field of education. These technologies enable the detection and analysis of students classroom behaviors, providing valuable insights for assessing individual concentration levels. However, the accuracy of target detection methods based on Convolutional Neural Networks (CNNs) can be compromised in classrooms with multiple targets and varying scales, as convolutional operations may result in the loss of location information. In contrast, transformers, which leverage attention mechanisms, have the capability to learn global features and mitigate the information loss caused by convolutional operations. In this paper, we propose a students classroom behavior detection system that combines deformable DETR with a Swin Transformer and light-weight Feature Pyramid Network (FPN). By employing a feature pyramid structure, the system can effectively process multi-scale feature maps extracted by the Swin Transformer, thereby improving the detection accuracy for targets of different sizes and scales. Moreover, the integration of the CARAFE lightweight operator into the FPN structure enhances the networks detection accuracy. To validate the effectiveness of our approach, extensive experiments are conducted on a real dataset of students classroom behavior. The experimental results demonstrate a significant 6.1% improvement in detection accuracy compared to state-of-the-art methods. These findings highlight the superiority of our proposed network in accurately detecting and analyzing students classroom behaviors. Overall, this research contributes to the field of education by addressing the limitations of CNN-based target detection methods and leveraging the capabilities of transformers to improve accuracy. The proposed system showcases the benefits of integrating deformable DETR, Swin Transformer, and the lightweight FPN in the context of students classroom behavior detection. The experimental results provide compelling evidence of the systems effectiveness and its potential to enhance classroom monitoring and assessment practices.",
      "intriguing_abstract": "Artificial intelligence (AI) and computer vision technologies have gained significant prominence in the field of education. These technologies enable the detection and analysis of students classroom behaviors, providing valuable insights for assessing individual concentration levels. However, the accuracy of target detection methods based on Convolutional Neural Networks (CNNs) can be compromised in classrooms with multiple targets and varying scales, as convolutional operations may result in the loss of location information. In contrast, transformers, which leverage attention mechanisms, have the capability to learn global features and mitigate the information loss caused by convolutional operations. In this paper, we propose a students classroom behavior detection system that combines deformable DETR with a Swin Transformer and light-weight Feature Pyramid Network (FPN). By employing a feature pyramid structure, the system can effectively process multi-scale feature maps extracted by the Swin Transformer, thereby improving the detection accuracy for targets of different sizes and scales. Moreover, the integration of the CARAFE lightweight operator into the FPN structure enhances the networks detection accuracy. To validate the effectiveness of our approach, extensive experiments are conducted on a real dataset of students classroom behavior. The experimental results demonstrate a significant 6.1% improvement in detection accuracy compared to state-of-the-art methods. These findings highlight the superiority of our proposed network in accurately detecting and analyzing students classroom behaviors. Overall, this research contributes to the field of education by addressing the limitations of CNN-based target detection methods and leveraging the capabilities of transformers to improve accuracy. The proposed system showcases the benefits of integrating deformable DETR, Swin Transformer, and the lightweight FPN in the context of students classroom behavior detection. The experimental results provide compelling evidence of the systems effectiveness and its potential to enhance classroom monitoring and assessment practices.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/50e997b23a534a6fbfd32d63990fa80373ec7c6b.pdf",
      "citation_key": "wang2023bfo",
      "metadata": {
        "title": "Students' Classroom Behavior Detection System Incorporating Deformable DETR with Swin Transformer and Light-Weight Feature Pyramid Network",
        "authors": [
          "Zhifeng Wang",
          "Jialong Yao",
          "Chunyan Zeng",
          "Longlong Li",
          "Cheng Tan"
        ],
        "published_date": "2023",
        "abstract": "Artificial intelligence (AI) and computer vision technologies have gained significant prominence in the field of education. These technologies enable the detection and analysis of students classroom behaviors, providing valuable insights for assessing individual concentration levels. However, the accuracy of target detection methods based on Convolutional Neural Networks (CNNs) can be compromised in classrooms with multiple targets and varying scales, as convolutional operations may result in the loss of location information. In contrast, transformers, which leverage attention mechanisms, have the capability to learn global features and mitigate the information loss caused by convolutional operations. In this paper, we propose a students classroom behavior detection system that combines deformable DETR with a Swin Transformer and light-weight Feature Pyramid Network (FPN). By employing a feature pyramid structure, the system can effectively process multi-scale feature maps extracted by the Swin Transformer, thereby improving the detection accuracy for targets of different sizes and scales. Moreover, the integration of the CARAFE lightweight operator into the FPN structure enhances the networks detection accuracy. To validate the effectiveness of our approach, extensive experiments are conducted on a real dataset of students classroom behavior. The experimental results demonstrate a significant 6.1% improvement in detection accuracy compared to state-of-the-art methods. These findings highlight the superiority of our proposed network in accurately detecting and analyzing students classroom behaviors. Overall, this research contributes to the field of education by addressing the limitations of CNN-based target detection methods and leveraging the capabilities of transformers to improve accuracy. The proposed system showcases the benefits of integrating deformable DETR, Swin Transformer, and the lightweight FPN in the context of students classroom behavior detection. The experimental results provide compelling evidence of the systems effectiveness and its potential to enhance classroom monitoring and assessment practices.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/50e997b23a534a6fbfd32d63990fa80373ec7c6b.pdf",
        "venue": "Syst.",
        "citationCount": 19,
        "score": 9.5,
        "summary": "Artificial intelligence (AI) and computer vision technologies have gained significant prominence in the field of education. These technologies enable the detection and analysis of students classroom behaviors, providing valuable insights for assessing individual concentration levels. However, the accuracy of target detection methods based on Convolutional Neural Networks (CNNs) can be compromised in classrooms with multiple targets and varying scales, as convolutional operations may result in the loss of location information. In contrast, transformers, which leverage attention mechanisms, have the capability to learn global features and mitigate the information loss caused by convolutional operations. In this paper, we propose a students classroom behavior detection system that combines deformable DETR with a Swin Transformer and light-weight Feature Pyramid Network (FPN). By employing a feature pyramid structure, the system can effectively process multi-scale feature maps extracted by the Swin Transformer, thereby improving the detection accuracy for targets of different sizes and scales. Moreover, the integration of the CARAFE lightweight operator into the FPN structure enhances the networks detection accuracy. To validate the effectiveness of our approach, extensive experiments are conducted on a real dataset of students classroom behavior. The experimental results demonstrate a significant 6.1% improvement in detection accuracy compared to state-of-the-art methods. These findings highlight the superiority of our proposed network in accurately detecting and analyzing students classroom behaviors. Overall, this research contributes to the field of education by addressing the limitations of CNN-based target detection methods and leveraging the capabilities of transformers to improve accuracy. The proposed system showcases the benefits of integrating deformable DETR, Swin Transformer, and the lightweight FPN in the context of students classroom behavior detection. The experimental results provide compelling evidence of the systems effectiveness and its potential to enhance classroom monitoring and assessment practices.",
        "keywords": []
      },
      "file_name": "50e997b23a534a6fbfd32d63990fa80373ec7c6b.pdf"
    },
    {
      "success": true,
      "doc_id": "a9cd95190bf1951fedde76aab34c29af",
      "summary": "The recently proposed Vision transformers (ViTs) have shown\nvery impressive empirical performance in various computer vision tasks,\nand they are viewed as an important type of foundation model. However, ViTs are typically constructed with large-scale sizes, which then\nseverely hinder their potential deployment in many practical resources constrained applications. \nTo mitigate this challenging problem, structured pruning is a promising solution to compress model size and enable\npractical efficiency. However, unlike its current popularity for CNNs and\nRNNs, structured pruning for ViT models is little explored.\nIn this paper, we propose GOHSP, a unified framework of Graph and\nOptimization-based Structured Pruning for ViT models. We first develop\na graph-based ranking for measuring the importance of attention heads,\nand the extracted importance information is further integrated to an\noptimization-based procedure to impose the heterogeneous structured\nsparsity patterns on the ViT models. Experimental results show that\nour proposed GOHSP demonstrates excellent compression performance.\nOn CIFAR-10 dataset, our approach can bring 40% parameters reduction\nwith no accuracy loss for ViT-Small model. On ImageNet dataset, with\n30% and 35% sparsity ratio for DeiT-Tiny and DeiT-Small models, our\napproach achieves 1.65% and 0.76% accuracy increase over the existing\nstructured pruning methods, respectively.",
      "intriguing_abstract": "The recently proposed Vision transformers (ViTs) have shown\nvery impressive empirical performance in various computer vision tasks,\nand they are viewed as an important type of foundation model. However, ViTs are typically constructed with large-scale sizes, which then\nseverely hinder their potential deployment in many practical resources constrained applications. \nTo mitigate this challenging problem, structured pruning is a promising solution to compress model size and enable\npractical efficiency. However, unlike its current popularity for CNNs and\nRNNs, structured pruning for ViT models is little explored.\nIn this paper, we propose GOHSP, a unified framework of Graph and\nOptimization-based Structured Pruning for ViT models. We first develop\na graph-based ranking for measuring the importance of attention heads,\nand the extracted importance information is further integrated to an\noptimization-based procedure to impose the heterogeneous structured\nsparsity patterns on the ViT models. Experimental results show that\nour proposed GOHSP demonstrates excellent compression performance.\nOn CIFAR-10 dataset, our approach can bring 40% parameters reduction\nwith no accuracy loss for ViT-Small model. On ImageNet dataset, with\n30% and 35% sparsity ratio for DeiT-Tiny and DeiT-Small models, our\napproach achieves 1.65% and 0.76% accuracy increase over the existing\nstructured pruning methods, respectively.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/3ea79430455304c782572dfb6ca3e5230b0351de.pdf",
      "citation_key": "yin2023029",
      "metadata": {
        "title": "GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous Structured Pruning for Vision Transformer",
        "authors": [
          "Miao Yin",
          "Burak Uzkent",
          "Yilin Shen",
          "Hongxia Jin",
          "Bo Yuan"
        ],
        "published_date": "2023",
        "abstract": "The recently proposed Vision transformers (ViTs) have shown\nvery impressive empirical performance in various computer vision tasks,\nand they are viewed as an important type of foundation model. However, ViTs are typically constructed with large-scale sizes, which then\nseverely hinder their potential deployment in many practical resources constrained applications. \nTo mitigate this challenging problem, structured pruning is a promising solution to compress model size and enable\npractical efficiency. However, unlike its current popularity for CNNs and\nRNNs, structured pruning for ViT models is little explored.\nIn this paper, we propose GOHSP, a unified framework of Graph and\nOptimization-based Structured Pruning for ViT models. We first develop\na graph-based ranking for measuring the importance of attention heads,\nand the extracted importance information is further integrated to an\noptimization-based procedure to impose the heterogeneous structured\nsparsity patterns on the ViT models. Experimental results show that\nour proposed GOHSP demonstrates excellent compression performance.\nOn CIFAR-10 dataset, our approach can bring 40% parameters reduction\nwith no accuracy loss for ViT-Small model. On ImageNet dataset, with\n30% and 35% sparsity ratio for DeiT-Tiny and DeiT-Small models, our\napproach achieves 1.65% and 0.76% accuracy increase over the existing\nstructured pruning methods, respectively.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3ea79430455304c782572dfb6ca3e5230b0351de.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 19,
        "score": 9.5,
        "summary": "The recently proposed Vision transformers (ViTs) have shown\nvery impressive empirical performance in various computer vision tasks,\nand they are viewed as an important type of foundation model. However, ViTs are typically constructed with large-scale sizes, which then\nseverely hinder their potential deployment in many practical resources constrained applications. \nTo mitigate this challenging problem, structured pruning is a promising solution to compress model size and enable\npractical efficiency. However, unlike its current popularity for CNNs and\nRNNs, structured pruning for ViT models is little explored.\nIn this paper, we propose GOHSP, a unified framework of Graph and\nOptimization-based Structured Pruning for ViT models. We first develop\na graph-based ranking for measuring the importance of attention heads,\nand the extracted importance information is further integrated to an\noptimization-based procedure to impose the heterogeneous structured\nsparsity patterns on the ViT models. Experimental results show that\nour proposed GOHSP demonstrates excellent compression performance.\nOn CIFAR-10 dataset, our approach can bring 40% parameters reduction\nwith no accuracy loss for ViT-Small model. On ImageNet dataset, with\n30% and 35% sparsity ratio for DeiT-Tiny and DeiT-Small models, our\napproach achieves 1.65% and 0.76% accuracy increase over the existing\nstructured pruning methods, respectively.",
        "keywords": []
      },
      "file_name": "3ea79430455304c782572dfb6ca3e5230b0351de.pdf"
    },
    {
      "success": true,
      "doc_id": "1e05f77e5582d658d0751267cba79c9d",
      "summary": "Transformer-based models have reshaped image captioning but grapple with issues like caption accuracy, particularly for complex visuals. Addressing these shortcomings is essential. Motivated by existing challenges, the Vision Transformer (ViT) as encoder and Generative Pretrained Transformer 2 (GPT-2) as decoder have been employed to enhance caption quality, utilizing the Seq2Seq framework and training on Flickr8k. This work introduced a novel ViT-GPT-2 image captioning model, evaluating it against benchmarks including Flickr8k. The model excels with BLEU-4 at 39.76 and METEOR at 52.30, bridging visual-textual gaps effectively. This research advances image captioning, offering practitioners an improved model for content indexing, accessibility, and human-computer interaction. ViT- GPT-2s success underscores bridging semantic gaps in image captioning, with future work exploring diverse datasets and fine-tuning techniques for enhanced performance.",
      "intriguing_abstract": "Transformer-based models have reshaped image captioning but grapple with issues like caption accuracy, particularly for complex visuals. Addressing these shortcomings is essential. Motivated by existing challenges, the Vision Transformer (ViT) as encoder and Generative Pretrained Transformer 2 (GPT-2) as decoder have been employed to enhance caption quality, utilizing the Seq2Seq framework and training on Flickr8k. This work introduced a novel ViT-GPT-2 image captioning model, evaluating it against benchmarks including Flickr8k. The model excels with BLEU-4 at 39.76 and METEOR at 52.30, bridging visual-textual gaps effectively. This research advances image captioning, offering practitioners an improved model for content indexing, accessibility, and human-computer interaction. ViT- GPT-2s success underscores bridging semantic gaps in image captioning, with future work exploring diverse datasets and fine-tuning techniques for enhanced performance.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/0284fa05879f1609bf459f15a4d40e22355cd5ae.pdf",
      "citation_key": "mishra2024fbz",
      "metadata": {
        "title": "Image Caption Generation using Vision Transformer and GPT Architecture",
        "authors": [
          "Swapneel Mishra",
          "Saumya Seth",
          "Shrishti Jain",
          "Vasudev Pant",
          "Jolly Parikh",
          "Rachna Jain",
          "Sardar M. N. Islam"
        ],
        "published_date": "2024",
        "abstract": "Transformer-based models have reshaped image captioning but grapple with issues like caption accuracy, particularly for complex visuals. Addressing these shortcomings is essential. Motivated by existing challenges, the Vision Transformer (ViT) as encoder and Generative Pretrained Transformer 2 (GPT-2) as decoder have been employed to enhance caption quality, utilizing the Seq2Seq framework and training on Flickr8k. This work introduced a novel ViT-GPT-2 image captioning model, evaluating it against benchmarks including Flickr8k. The model excels with BLEU-4 at 39.76 and METEOR at 52.30, bridging visual-textual gaps effectively. This research advances image captioning, offering practitioners an improved model for content indexing, accessibility, and human-computer interaction. ViT- GPT-2s success underscores bridging semantic gaps in image captioning, with future work exploring diverse datasets and fine-tuning techniques for enhanced performance.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0284fa05879f1609bf459f15a4d40e22355cd5ae.pdf",
        "venue": "2024 2nd International Conference on Advancement in Computation & Computer Technologies (InCACCT)",
        "citationCount": 9,
        "score": 9.0,
        "summary": "Transformer-based models have reshaped image captioning but grapple with issues like caption accuracy, particularly for complex visuals. Addressing these shortcomings is essential. Motivated by existing challenges, the Vision Transformer (ViT) as encoder and Generative Pretrained Transformer 2 (GPT-2) as decoder have been employed to enhance caption quality, utilizing the Seq2Seq framework and training on Flickr8k. This work introduced a novel ViT-GPT-2 image captioning model, evaluating it against benchmarks including Flickr8k. The model excels with BLEU-4 at 39.76 and METEOR at 52.30, bridging visual-textual gaps effectively. This research advances image captioning, offering practitioners an improved model for content indexing, accessibility, and human-computer interaction. ViT- GPT-2s success underscores bridging semantic gaps in image captioning, with future work exploring diverse datasets and fine-tuning techniques for enhanced performance.",
        "keywords": []
      },
      "file_name": "0284fa05879f1609bf459f15a4d40e22355cd5ae.pdf"
    },
    {
      "success": true,
      "doc_id": "40c84c7818b114e656e748987557e276",
      "summary": "Intrigued by the inherent ability of the human visual system to identify salient regions in complex scenes, attention mechanisms have been seamlessly integrated into various Computer Vision (CV) tasks. Building upon this paradigm, Vision Transformer (ViT) networks exploit attention mechanisms for improved efficiency. This review navigates the landscape of redesigned attention mechanisms within ViTs, aiming to enhance their performance. This paper provides a comprehensive exploration of techniques and insights for designing attention mechanisms, systematically reviewing recent literature in the field of CV. This survey begins with an introduction to the theoretical foundations and fundamental concepts underlying attention mechanisms. We then present a systematic taxonomy of various attention mechanisms within ViTs, employing redesigned approaches. A multi-perspective categorization is proposed based on their application, objectives, and the type of attention applied. The analysis includes an exploration of the novelty, strengths, weaknesses, and an in-depth evaluation of the different proposed strategies. This culminates in the development of taxonomies that highlight key properties and contributions. Finally, we gather the reviewed studies along with their available open-source implementations at our \\href{https://github.com/mindflow-institue/Awesome-Attention-Mechanism-in-Medical-Imaging}{GitHub}\\footnote{\\url{https://github.com/xmindflow/Awesome-Attention-Mechanism-in-Medical-Imaging}}. We aim to regularly update it with the most recent relevant papers.",
      "intriguing_abstract": "Intrigued by the inherent ability of the human visual system to identify salient regions in complex scenes, attention mechanisms have been seamlessly integrated into various Computer Vision (CV) tasks. Building upon this paradigm, Vision Transformer (ViT) networks exploit attention mechanisms for improved efficiency. This review navigates the landscape of redesigned attention mechanisms within ViTs, aiming to enhance their performance. This paper provides a comprehensive exploration of techniques and insights for designing attention mechanisms, systematically reviewing recent literature in the field of CV. This survey begins with an introduction to the theoretical foundations and fundamental concepts underlying attention mechanisms. We then present a systematic taxonomy of various attention mechanisms within ViTs, employing redesigned approaches. A multi-perspective categorization is proposed based on their application, objectives, and the type of attention applied. The analysis includes an exploration of the novelty, strengths, weaknesses, and an in-depth evaluation of the different proposed strategies. This culminates in the development of taxonomies that highlight key properties and contributions. Finally, we gather the reviewed studies along with their available open-source implementations at our \\href{https://github.com/mindflow-institue/Awesome-Attention-Mechanism-in-Medical-Imaging}{GitHub}\\footnote{\\url{https://github.com/xmindflow/Awesome-Attention-Mechanism-in-Medical-Imaging}}. We aim to regularly update it with the most recent relevant papers.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/714e21409b8c4f7788ac8c93795249a4e45e51ce.pdf",
      "citation_key": "heidari2024d9k",
      "metadata": {
        "title": "Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights",
        "authors": [
          "Moein Heidari",
          "Reza Azad",
          "Sina Ghorbani Kolahi",
          "Ren'e Arimond",
          "Leon Niggemeier",
          "Alaa Sulaiman",
          "Afshin Bozorgpour",
          "Ehsan Khodapanah Aghdam",
          "A. Kazerouni",
          "I. Hacihaliloglu",
          "D. Merhof"
        ],
        "published_date": "2024",
        "abstract": "Intrigued by the inherent ability of the human visual system to identify salient regions in complex scenes, attention mechanisms have been seamlessly integrated into various Computer Vision (CV) tasks. Building upon this paradigm, Vision Transformer (ViT) networks exploit attention mechanisms for improved efficiency. This review navigates the landscape of redesigned attention mechanisms within ViTs, aiming to enhance their performance. This paper provides a comprehensive exploration of techniques and insights for designing attention mechanisms, systematically reviewing recent literature in the field of CV. This survey begins with an introduction to the theoretical foundations and fundamental concepts underlying attention mechanisms. We then present a systematic taxonomy of various attention mechanisms within ViTs, employing redesigned approaches. A multi-perspective categorization is proposed based on their application, objectives, and the type of attention applied. The analysis includes an exploration of the novelty, strengths, weaknesses, and an in-depth evaluation of the different proposed strategies. This culminates in the development of taxonomies that highlight key properties and contributions. Finally, we gather the reviewed studies along with their available open-source implementations at our \\href{https://github.com/mindflow-institue/Awesome-Attention-Mechanism-in-Medical-Imaging}{GitHub}\\footnote{\\url{https://github.com/xmindflow/Awesome-Attention-Mechanism-in-Medical-Imaging}}. We aim to regularly update it with the most recent relevant papers.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/714e21409b8c4f7788ac8c93795249a4e45e51ce.pdf",
        "venue": "arXiv.org",
        "citationCount": 9,
        "score": 9.0,
        "summary": "Intrigued by the inherent ability of the human visual system to identify salient regions in complex scenes, attention mechanisms have been seamlessly integrated into various Computer Vision (CV) tasks. Building upon this paradigm, Vision Transformer (ViT) networks exploit attention mechanisms for improved efficiency. This review navigates the landscape of redesigned attention mechanisms within ViTs, aiming to enhance their performance. This paper provides a comprehensive exploration of techniques and insights for designing attention mechanisms, systematically reviewing recent literature in the field of CV. This survey begins with an introduction to the theoretical foundations and fundamental concepts underlying attention mechanisms. We then present a systematic taxonomy of various attention mechanisms within ViTs, employing redesigned approaches. A multi-perspective categorization is proposed based on their application, objectives, and the type of attention applied. The analysis includes an exploration of the novelty, strengths, weaknesses, and an in-depth evaluation of the different proposed strategies. This culminates in the development of taxonomies that highlight key properties and contributions. Finally, we gather the reviewed studies along with their available open-source implementations at our \\href{https://github.com/mindflow-institue/Awesome-Attention-Mechanism-in-Medical-Imaging}{GitHub}\\footnote{\\url{https://github.com/xmindflow/Awesome-Attention-Mechanism-in-Medical-Imaging}}. We aim to regularly update it with the most recent relevant papers.",
        "keywords": []
      },
      "file_name": "714e21409b8c4f7788ac8c93795249a4e45e51ce.pdf"
    },
    {
      "success": true,
      "doc_id": "f09559ccf509217f46aff30878cd1766",
      "summary": "Robotic grasping techniques have been widely studied in recent years. However, it is always a challenging problem for robots to grasp in cluttered scenes. In this issue, objects are placed close to each other, and there is no space around for the robot to place the gripper, making it difficult to find a suitable grasping position. To solve this problem, this article proposes to use the combination of pushing and grasping (PG) actions to help grasp pose detection and robot grasping. We propose a pushinggrasping combined grasping network (GN), PG method based on transformer and convolution (PGTC). For the pushing action, we propose a vision transformer (ViT)-based object position prediction network pushing transformer network (PTNet), which can well capture the global and temporal features and can better predict the position of objects after pushing. To perform the grasping detection, we propose a cross dense fusion network (CDFNet), which can make full use of the RGB image and depth image, and fuse and refine them several times. Compared with previous networks, CDFNet is able to detect the optimal grasping position more accurately. Finally, we use the network for both simulation and actual UR3 robot grasping experiments and achieve SOTA performance. Video and dataset are available at https://youtu.be/Q58YE-Cc250.",
      "intriguing_abstract": "Robotic grasping techniques have been widely studied in recent years. However, it is always a challenging problem for robots to grasp in cluttered scenes. In this issue, objects are placed close to each other, and there is no space around for the robot to place the gripper, making it difficult to find a suitable grasping position. To solve this problem, this article proposes to use the combination of pushing and grasping (PG) actions to help grasp pose detection and robot grasping. We propose a pushinggrasping combined grasping network (GN), PG method based on transformer and convolution (PGTC). For the pushing action, we propose a vision transformer (ViT)-based object position prediction network pushing transformer network (PTNet), which can well capture the global and temporal features and can better predict the position of objects after pushing. To perform the grasping detection, we propose a cross dense fusion network (CDFNet), which can make full use of the RGB image and depth image, and fuse and refine them several times. Compared with previous networks, CDFNet is able to detect the optimal grasping position more accurately. Finally, we use the network for both simulation and actual UR3 robot grasping experiments and achieve SOTA performance. Video and dataset are available at https://youtu.be/Q58YE-Cc250.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/2ef224f7a1f545eb8bfdf61c7b849f2676fe6a61.pdf",
      "citation_key": "yu2023fqo",
      "metadata": {
        "title": "A Novel Robotic Pushing and Grasping Method Based on Vision Transformer and Convolution",
        "authors": [
          "Sheng Yu",
          "Dihua Zhai",
          "Yuanqing Xia"
        ],
        "published_date": "2023",
        "abstract": "Robotic grasping techniques have been widely studied in recent years. However, it is always a challenging problem for robots to grasp in cluttered scenes. In this issue, objects are placed close to each other, and there is no space around for the robot to place the gripper, making it difficult to find a suitable grasping position. To solve this problem, this article proposes to use the combination of pushing and grasping (PG) actions to help grasp pose detection and robot grasping. We propose a pushinggrasping combined grasping network (GN), PG method based on transformer and convolution (PGTC). For the pushing action, we propose a vision transformer (ViT)-based object position prediction network pushing transformer network (PTNet), which can well capture the global and temporal features and can better predict the position of objects after pushing. To perform the grasping detection, we propose a cross dense fusion network (CDFNet), which can make full use of the RGB image and depth image, and fuse and refine them several times. Compared with previous networks, CDFNet is able to detect the optimal grasping position more accurately. Finally, we use the network for both simulation and actual UR3 robot grasping experiments and achieve SOTA performance. Video and dataset are available at https://youtu.be/Q58YE-Cc250.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2ef224f7a1f545eb8bfdf61c7b849f2676fe6a61.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 18,
        "score": 9.0,
        "summary": "Robotic grasping techniques have been widely studied in recent years. However, it is always a challenging problem for robots to grasp in cluttered scenes. In this issue, objects are placed close to each other, and there is no space around for the robot to place the gripper, making it difficult to find a suitable grasping position. To solve this problem, this article proposes to use the combination of pushing and grasping (PG) actions to help grasp pose detection and robot grasping. We propose a pushinggrasping combined grasping network (GN), PG method based on transformer and convolution (PGTC). For the pushing action, we propose a vision transformer (ViT)-based object position prediction network pushing transformer network (PTNet), which can well capture the global and temporal features and can better predict the position of objects after pushing. To perform the grasping detection, we propose a cross dense fusion network (CDFNet), which can make full use of the RGB image and depth image, and fuse and refine them several times. Compared with previous networks, CDFNet is able to detect the optimal grasping position more accurately. Finally, we use the network for both simulation and actual UR3 robot grasping experiments and achieve SOTA performance. Video and dataset are available at https://youtu.be/Q58YE-Cc250.",
        "keywords": []
      },
      "file_name": "2ef224f7a1f545eb8bfdf61c7b849f2676fe6a61.pdf"
    },
    {
      "success": true,
      "doc_id": "d4de0e2b7ee7e7a71d2ea216a0ebb6d9",
      "summary": "The recently proposed data augmentation TransMix employs attention labels to help visual transformers (ViT) achieve better robustness and performance. However, TransMix is deficient in two aspects: 1) The image cropping method of TransMix may not be suitable for ViTs. 2) At the early stage of training, the model produces unreliable attention maps. TransMix uses unreliable attention maps to compute mixed attention labels that can affect the model. To address the aforementioned issues, we propose MaskMix and Progressive Attention Labeling (PAL) in image and label space, respectively. In detail, from the perspective of image space, we design MaskMix, which mixes two images based on a patch-like grid mask. In particular, the size of each mask patch is adjustable and is a multiple of the image patch size, which ensures each image patch comes from only one image and contains more global contents. From the perspective of label space, we design PAL, which utilizes a progressive factor to dynamically re-weight the attention weights of the mixed attention label. Finally, we combine MaskMix and Progressive Attention Labeling as our new data augmentation method, named MixPro. The experimental results show that our method can improve various ViT-based models at scales on ImageNet classification (73.8\\% top-1 accuracy based on DeiT-T for 300 epochs). After being pre-trained with MixPro on ImageNet, the ViT-based models also demonstrate better transferability to semantic segmentation, object detection, and instance segmentation. Furthermore, compared to TransMix, MixPro also shows stronger robustness on several benchmarks. The code is available at https://github.com/fistyee/MixPro.",
      "intriguing_abstract": "The recently proposed data augmentation TransMix employs attention labels to help visual transformers (ViT) achieve better robustness and performance. However, TransMix is deficient in two aspects: 1) The image cropping method of TransMix may not be suitable for ViTs. 2) At the early stage of training, the model produces unreliable attention maps. TransMix uses unreliable attention maps to compute mixed attention labels that can affect the model. To address the aforementioned issues, we propose MaskMix and Progressive Attention Labeling (PAL) in image and label space, respectively. In detail, from the perspective of image space, we design MaskMix, which mixes two images based on a patch-like grid mask. In particular, the size of each mask patch is adjustable and is a multiple of the image patch size, which ensures each image patch comes from only one image and contains more global contents. From the perspective of label space, we design PAL, which utilizes a progressive factor to dynamically re-weight the attention weights of the mixed attention label. Finally, we combine MaskMix and Progressive Attention Labeling as our new data augmentation method, named MixPro. The experimental results show that our method can improve various ViT-based models at scales on ImageNet classification (73.8\\% top-1 accuracy based on DeiT-T for 300 epochs). After being pre-trained with MixPro on ImageNet, the ViT-based models also demonstrate better transferability to semantic segmentation, object detection, and instance segmentation. Furthermore, compared to TransMix, MixPro also shows stronger robustness on several benchmarks. The code is available at https://github.com/fistyee/MixPro.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/bc99c855d52ba3d432c428fb4096b3a22c04f8bf.pdf",
      "citation_key": "zhao2023pau",
      "metadata": {
        "title": "MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer",
        "authors": [
          "Qihao Zhao",
          "Yangyu Huang",
          "Wei Hu",
          "Fan Zhang",
          "J. Liu"
        ],
        "published_date": "2023",
        "abstract": "The recently proposed data augmentation TransMix employs attention labels to help visual transformers (ViT) achieve better robustness and performance. However, TransMix is deficient in two aspects: 1) The image cropping method of TransMix may not be suitable for ViTs. 2) At the early stage of training, the model produces unreliable attention maps. TransMix uses unreliable attention maps to compute mixed attention labels that can affect the model. To address the aforementioned issues, we propose MaskMix and Progressive Attention Labeling (PAL) in image and label space, respectively. In detail, from the perspective of image space, we design MaskMix, which mixes two images based on a patch-like grid mask. In particular, the size of each mask patch is adjustable and is a multiple of the image patch size, which ensures each image patch comes from only one image and contains more global contents. From the perspective of label space, we design PAL, which utilizes a progressive factor to dynamically re-weight the attention weights of the mixed attention label. Finally, we combine MaskMix and Progressive Attention Labeling as our new data augmentation method, named MixPro. The experimental results show that our method can improve various ViT-based models at scales on ImageNet classification (73.8\\% top-1 accuracy based on DeiT-T for 300 epochs). After being pre-trained with MixPro on ImageNet, the ViT-based models also demonstrate better transferability to semantic segmentation, object detection, and instance segmentation. Furthermore, compared to TransMix, MixPro also shows stronger robustness on several benchmarks. The code is available at https://github.com/fistyee/MixPro.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/bc99c855d52ba3d432c428fb4096b3a22c04f8bf.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 18,
        "score": 9.0,
        "summary": "The recently proposed data augmentation TransMix employs attention labels to help visual transformers (ViT) achieve better robustness and performance. However, TransMix is deficient in two aspects: 1) The image cropping method of TransMix may not be suitable for ViTs. 2) At the early stage of training, the model produces unreliable attention maps. TransMix uses unreliable attention maps to compute mixed attention labels that can affect the model. To address the aforementioned issues, we propose MaskMix and Progressive Attention Labeling (PAL) in image and label space, respectively. In detail, from the perspective of image space, we design MaskMix, which mixes two images based on a patch-like grid mask. In particular, the size of each mask patch is adjustable and is a multiple of the image patch size, which ensures each image patch comes from only one image and contains more global contents. From the perspective of label space, we design PAL, which utilizes a progressive factor to dynamically re-weight the attention weights of the mixed attention label. Finally, we combine MaskMix and Progressive Attention Labeling as our new data augmentation method, named MixPro. The experimental results show that our method can improve various ViT-based models at scales on ImageNet classification (73.8\\% top-1 accuracy based on DeiT-T for 300 epochs). After being pre-trained with MixPro on ImageNet, the ViT-based models also demonstrate better transferability to semantic segmentation, object detection, and instance segmentation. Furthermore, compared to TransMix, MixPro also shows stronger robustness on several benchmarks. The code is available at https://github.com/fistyee/MixPro.",
        "keywords": []
      },
      "file_name": "bc99c855d52ba3d432c428fb4096b3a22c04f8bf.pdf"
    },
    {
      "success": true,
      "doc_id": "bee5bde19256b79ea2ea1e2fb830b47e",
      "summary": "Monocular 3D object detection is challenging due to the lack of accurate depth information. Some methods estimate the pixel-wise depth maps from off-the-shelf depth estimators and then use them as an additional input to augment the RGB images. Depth-based methods attempt to convert estimated depth maps to pseudo-LiDAR and then use LiDAR-based object detectors or focus on the perspective of image and depth fusion learning. However, they demonstrate limited performance and efficiency as a result of depth inaccuracy and complex fusion mode with convolutions. Different from these approaches, our proposed depth-guided vision transformer with a normalizing flows (NF-DVT) network uses normalizing flows to build priors in depth maps to achieve more accurate depth information. Then we develop a novel Swin-Transformer-based backbone with a fusion module to process RGB image patches and depth map patches with two separate branches and fuse them using cross-attention to exchange information with each other. Furthermore, with the help of pixel-wise relative depth values in depth maps, we develop new relative position embeddings in the cross-attention mechanism to capture more accurate sequence ordering of input tokens. Our method is the first Swin-Transformer-based backbone architecture for monocular 3D object detection. The experimental results on the KITTI and the challenging Waymo Open datasets show the effectiveness of our proposed method and superior performance over previous counterparts.",
      "intriguing_abstract": "Monocular 3D object detection is challenging due to the lack of accurate depth information. Some methods estimate the pixel-wise depth maps from off-the-shelf depth estimators and then use them as an additional input to augment the RGB images. Depth-based methods attempt to convert estimated depth maps to pseudo-LiDAR and then use LiDAR-based object detectors or focus on the perspective of image and depth fusion learning. However, they demonstrate limited performance and efficiency as a result of depth inaccuracy and complex fusion mode with convolutions. Different from these approaches, our proposed depth-guided vision transformer with a normalizing flows (NF-DVT) network uses normalizing flows to build priors in depth maps to achieve more accurate depth information. Then we develop a novel Swin-Transformer-based backbone with a fusion module to process RGB image patches and depth map patches with two separate branches and fuse them using cross-attention to exchange information with each other. Furthermore, with the help of pixel-wise relative depth values in depth maps, we develop new relative position embeddings in the cross-attention mechanism to capture more accurate sequence ordering of input tokens. Our method is the first Swin-Transformer-based backbone architecture for monocular 3D object detection. The experimental results on the KITTI and the challenging Waymo Open datasets show the effectiveness of our proposed method and superior performance over previous counterparts.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/3dee43cea71d5988a72a914121f3455106f89cc7.pdf",
      "citation_key": "pan20249k5",
      "metadata": {
        "title": "Depth-Guided Vision Transformer With Normalizing Flows for Monocular 3D Object Detection",
        "authors": [
          "C. Pan",
          "Junran Peng",
          "Zhaoxiang Zhang"
        ],
        "published_date": "2024",
        "abstract": "Monocular 3D object detection is challenging due to the lack of accurate depth information. Some methods estimate the pixel-wise depth maps from off-the-shelf depth estimators and then use them as an additional input to augment the RGB images. Depth-based methods attempt to convert estimated depth maps to pseudo-LiDAR and then use LiDAR-based object detectors or focus on the perspective of image and depth fusion learning. However, they demonstrate limited performance and efficiency as a result of depth inaccuracy and complex fusion mode with convolutions. Different from these approaches, our proposed depth-guided vision transformer with a normalizing flows (NF-DVT) network uses normalizing flows to build priors in depth maps to achieve more accurate depth information. Then we develop a novel Swin-Transformer-based backbone with a fusion module to process RGB image patches and depth map patches with two separate branches and fuse them using cross-attention to exchange information with each other. Furthermore, with the help of pixel-wise relative depth values in depth maps, we develop new relative position embeddings in the cross-attention mechanism to capture more accurate sequence ordering of input tokens. Our method is the first Swin-Transformer-based backbone architecture for monocular 3D object detection. The experimental results on the KITTI and the challenging Waymo Open datasets show the effectiveness of our proposed method and superior performance over previous counterparts.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3dee43cea71d5988a72a914121f3455106f89cc7.pdf",
        "venue": "IEEE/CAA Journal of Automatica Sinica",
        "citationCount": 9,
        "score": 9.0,
        "summary": "Monocular 3D object detection is challenging due to the lack of accurate depth information. Some methods estimate the pixel-wise depth maps from off-the-shelf depth estimators and then use them as an additional input to augment the RGB images. Depth-based methods attempt to convert estimated depth maps to pseudo-LiDAR and then use LiDAR-based object detectors or focus on the perspective of image and depth fusion learning. However, they demonstrate limited performance and efficiency as a result of depth inaccuracy and complex fusion mode with convolutions. Different from these approaches, our proposed depth-guided vision transformer with a normalizing flows (NF-DVT) network uses normalizing flows to build priors in depth maps to achieve more accurate depth information. Then we develop a novel Swin-Transformer-based backbone with a fusion module to process RGB image patches and depth map patches with two separate branches and fuse them using cross-attention to exchange information with each other. Furthermore, with the help of pixel-wise relative depth values in depth maps, we develop new relative position embeddings in the cross-attention mechanism to capture more accurate sequence ordering of input tokens. Our method is the first Swin-Transformer-based backbone architecture for monocular 3D object detection. The experimental results on the KITTI and the challenging Waymo Open datasets show the effectiveness of our proposed method and superior performance over previous counterparts.",
        "keywords": []
      },
      "file_name": "3dee43cea71d5988a72a914121f3455106f89cc7.pdf"
    },
    {
      "success": true,
      "doc_id": "684d2abf3ade0e58b297a1d3d7af63bb",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of achieving high accuracy in radar-based Human Activity Recognition (HAR) while simultaneously ensuring the network is lightweight and has low latency for embedded applications \\cite{huan202345b}.\n    *   **Importance and Challenge:** Radar-based HAR offers non-contact, privacy-protected, and lighting-robust solutions for applications like intelligent healthcare and smart homes. However, complex deep neural networks, while accurate, are computationally burdensome for embedded systems. Existing lightweight solutions often compromise recognition accuracy \\cite{huan202345b}.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:** Previous HAR methods include traditional classification techniques (e.g., MLP, PCA, SVM) using manually extracted micro-Doppler features, and deep learning (DL) techniques (e.g., CNNs, RNNs, Transformers, hybrid networks) that automatically extract features \\cite{huan202345b}.\n    *   **Limitations of Previous Solutions:** Traditional methods are limited by prior knowledge and task intricacy. DL methods, especially Transformers (ViT), often have a large number of parameters, making them challenging for embedded applications. Lightweight CNNs reduce parameters but can lead to a decline in recognition accuracy by missing details \\cite{huan202345b}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes a Lightweight Hybrid Vision Transformer (LH-ViT) network for radar-based HAR \\cite{huan202345b}. This network combines efficient convolution operations with the self-attention mechanism of ViT.\n    *   **Novelty/Difference:**\n        *   It employs a Feature Pyramid architecture for multi-scale feature extraction from micro-Doppler maps.\n        *   Feature enhancement is performed by stacked Radar-ViT modules, which incorporate fold and unfold operations to significantly reduce the computational load of the attention mechanism.\n        *   The traditional convolution operator is replaced by an efficient RES-SE block, which combines a residual learning framework with a Squeeze-and-Excitation (SE) network and uses depthwise separable convolutions \\cite{huan202345b}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Development of a novel Lightweight Hybrid Vision Transformer (LH-ViT) that integrates a pyramid-structured feature extraction network with a stacked Radar-ViT feature enhancement network, enhancing representational power through spatial attention in the micro-Doppler feature hierarchy \\cite{huan202345b}.\n        *   Design of an efficient RES-SE block that replaces traditional convolution. This block uses depthwise separable convolutions within a residual learning framework and incorporates a lightweight SE module for adaptive channel weight adjustment, reducing computational overhead \\cite{huan202345b}.\n        *   Introduction of Radar-ViT, a lightweight design of ViT for embedded applications. It simplifies the class token module to a point-wise convolution and uses fold and unfold operations to reduce the computational demands of the multi-head attention block, effectively capturing global micro-Doppler features \\cite{huan202345b}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Experiments were performed on two human activity datasets: a public C-band radar dataset (5 activities: walking, sitting, standing up, bending, drinking) and a self-established 79 GHz mmWave radar dataset (5 activities: walking, running, standing up after squatting, bending, turning) \\cite{huan202345b}.\n    *   **Key Performance Metrics & Comparison Results:** The experiments demonstrated the LH-ViT method's advantages in terms of expressiveness and computing efficiency compared to traditional methods \\cite{huan202345b}. The paper claims the method achieves efficient HAR at different Doppler scales and significantly reduces parameter count while maintaining accuracy compared to conventional ViT \\cite{huan202345b}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The primary focus is on radar-based HAR using micro-Doppler signals. The method's effectiveness is demonstrated on specific human activities and radar bands (C-band and 79 GHz mmWave). The paper implies that the \"lightweight\" aspect is relative to larger ViT models and deep learning networks, but specific trade-offs in extreme resource-constrained environments are not detailed in the provided text.\n    *   **Scope of Applicability:** The proposed LH-ViT is designed for embedded applications requiring efficient and accurate human activity recognition using radar data \\cite{huan202345b}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** The LH-ViT advances the technical state-of-the-art by effectively addressing the critical trade-off between HAR accuracy and network lightweightness for radar-based systems \\cite{huan202345b}. It demonstrates how to leverage the strengths of both convolutional networks (for local feature extraction and efficiency) and Vision Transformers (for global feature capture via self-attention) in a highly optimized, hybrid architecture.\n    *   **Potential Impact:** This work has the potential to enable the deployment of high-performance radar-based HAR systems in resource-constrained embedded environments, expanding applications in intelligent healthcare, smart homes, and security where privacy and robustness are paramount \\cite{huan202345b}.",
      "intriguing_abstract": "Achieving accurate Human Activity Recognition (HAR) from radar signals in resource-constrained embedded systems remains a critical challenge, often forcing a trade-off between recognition accuracy and computational efficiency. This paper introduces the Lightweight Hybrid Vision Transformer (LH-ViT), a novel network architecture designed to overcome this dilemma. LH-ViT ingeniously combines a multi-scale Feature Pyramid for robust micro-Doppler feature extraction with stacked Radar-ViT modules. Our innovative Radar-ViT significantly reduces the computational burden of the self-attention mechanism through fold and unfold operations, while an efficient RES-SE block leverages depthwise separable convolutions and residual learning for further optimization. Extensive experiments on both C-band and mmWave radar datasets demonstrate that LH-ViT achieves state-of-the-art HAR accuracy with dramatically fewer parameters compared to conventional Vision Transformers. This breakthrough enables the deployment of high-performance, privacy-preserving radar-based HAR in intelligent healthcare, smart homes, and security applications, paving the way for ubiquitous, efficient, and robust activity monitoring.",
      "keywords": [
        "Radar-based Human Activity Recognition (HAR)",
        "Lightweight Hybrid Vision Transformer (LH-ViT)",
        "Embedded applications",
        "Micro-Doppler features",
        "Feature Pyramid architecture",
        "Radar-ViT module",
        "Efficient RES-SE block",
        "Depthwise separable convolutions",
        "Self-attention mechanism",
        "Multi-scale feature extraction",
        "High accuracy",
        "Low latency",
        "Reduced parameter count",
        "Computational efficiency"
      ],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/1871c0bc27bd5cd5f2f6c3d6cedeb4b8c5a89499.pdf",
      "citation_key": "huan202345b",
      "metadata": {
        "title": "A lightweight hybrid vision transformer network for radar-based human activity recognition",
        "authors": [
          "Sha Huan",
          "Zhaoyue Wang",
          "Xiaoqiang Wang",
          "Limei Wu",
          "Xiaoxuan Yang",
          "Hongming Huang",
          "Gan E Dai"
        ],
        "published_date": "2023",
        "abstract": "Radar-based human activity recognition (HAR) offers a non-contact technique with privacy protection and lighting robustness for many advanced applications. Complex deep neural networks demonstrate significant performance advantages when classifying the radar micro-Doppler signals that have unique correspondences with human behavior. However, in embedded applications, the demand for lightweight and low latency poses challenges to the radar-based HAR network construction. In this paper, an efficient network based on a lightweight hybrid Vision Transformer (LH-ViT) is proposed to address the HAR accuracy and network lightweight simultaneously. This network combines the efficient convolution operations with the strength of the self-attention mechanism in ViT. Feature Pyramid architecture is applied for the multi-scale feature extraction for the micro-Doppler map. Feature enhancement is executed by the stacked Radar-ViT subsequently, in which the fold and unfold operations are added to lower the computational load of the attention mechanism. The convolution operator in the LH-ViT is replaced by the RES-SE block, an efficient structure that combines the residual learning framework with the Squeeze-and-Excitation network. Experiments based on two human activity datasets indicate our methods advantages in terms of expressiveness and computing efficiency over traditional methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1871c0bc27bd5cd5f2f6c3d6cedeb4b8c5a89499.pdf",
        "venue": "Scientific Reports",
        "citationCount": 17,
        "score": 8.5,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of achieving high accuracy in radar-based Human Activity Recognition (HAR) while simultaneously ensuring the network is lightweight and has low latency for embedded applications \\cite{huan202345b}.\n    *   **Importance and Challenge:** Radar-based HAR offers non-contact, privacy-protected, and lighting-robust solutions for applications like intelligent healthcare and smart homes. However, complex deep neural networks, while accurate, are computationally burdensome for embedded systems. Existing lightweight solutions often compromise recognition accuracy \\cite{huan202345b}.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:** Previous HAR methods include traditional classification techniques (e.g., MLP, PCA, SVM) using manually extracted micro-Doppler features, and deep learning (DL) techniques (e.g., CNNs, RNNs, Transformers, hybrid networks) that automatically extract features \\cite{huan202345b}.\n    *   **Limitations of Previous Solutions:** Traditional methods are limited by prior knowledge and task intricacy. DL methods, especially Transformers (ViT), often have a large number of parameters, making them challenging for embedded applications. Lightweight CNNs reduce parameters but can lead to a decline in recognition accuracy by missing details \\cite{huan202345b}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes a Lightweight Hybrid Vision Transformer (LH-ViT) network for radar-based HAR \\cite{huan202345b}. This network combines efficient convolution operations with the self-attention mechanism of ViT.\n    *   **Novelty/Difference:**\n        *   It employs a Feature Pyramid architecture for multi-scale feature extraction from micro-Doppler maps.\n        *   Feature enhancement is performed by stacked Radar-ViT modules, which incorporate fold and unfold operations to significantly reduce the computational load of the attention mechanism.\n        *   The traditional convolution operator is replaced by an efficient RES-SE block, which combines a residual learning framework with a Squeeze-and-Excitation (SE) network and uses depthwise separable convolutions \\cite{huan202345b}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Development of a novel Lightweight Hybrid Vision Transformer (LH-ViT) that integrates a pyramid-structured feature extraction network with a stacked Radar-ViT feature enhancement network, enhancing representational power through spatial attention in the micro-Doppler feature hierarchy \\cite{huan202345b}.\n        *   Design of an efficient RES-SE block that replaces traditional convolution. This block uses depthwise separable convolutions within a residual learning framework and incorporates a lightweight SE module for adaptive channel weight adjustment, reducing computational overhead \\cite{huan202345b}.\n        *   Introduction of Radar-ViT, a lightweight design of ViT for embedded applications. It simplifies the class token module to a point-wise convolution and uses fold and unfold operations to reduce the computational demands of the multi-head attention block, effectively capturing global micro-Doppler features \\cite{huan202345b}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Experiments were performed on two human activity datasets: a public C-band radar dataset (5 activities: walking, sitting, standing up, bending, drinking) and a self-established 79 GHz mmWave radar dataset (5 activities: walking, running, standing up after squatting, bending, turning) \\cite{huan202345b}.\n    *   **Key Performance Metrics & Comparison Results:** The experiments demonstrated the LH-ViT method's advantages in terms of expressiveness and computing efficiency compared to traditional methods \\cite{huan202345b}. The paper claims the method achieves efficient HAR at different Doppler scales and significantly reduces parameter count while maintaining accuracy compared to conventional ViT \\cite{huan202345b}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The primary focus is on radar-based HAR using micro-Doppler signals. The method's effectiveness is demonstrated on specific human activities and radar bands (C-band and 79 GHz mmWave). The paper implies that the \"lightweight\" aspect is relative to larger ViT models and deep learning networks, but specific trade-offs in extreme resource-constrained environments are not detailed in the provided text.\n    *   **Scope of Applicability:** The proposed LH-ViT is designed for embedded applications requiring efficient and accurate human activity recognition using radar data \\cite{huan202345b}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** The LH-ViT advances the technical state-of-the-art by effectively addressing the critical trade-off between HAR accuracy and network lightweightness for radar-based systems \\cite{huan202345b}. It demonstrates how to leverage the strengths of both convolutional networks (for local feature extraction and efficiency) and Vision Transformers (for global feature capture via self-attention) in a highly optimized, hybrid architecture.\n    *   **Potential Impact:** This work has the potential to enable the deployment of high-performance radar-based HAR systems in resource-constrained embedded environments, expanding applications in intelligent healthcare, smart homes, and security where privacy and robustness are paramount \\cite{huan202345b}.",
        "keywords": [
          "Radar-based Human Activity Recognition (HAR)",
          "Lightweight Hybrid Vision Transformer (LH-ViT)",
          "Embedded applications",
          "Micro-Doppler features",
          "Feature Pyramid architecture",
          "Radar-ViT module",
          "Efficient RES-SE block",
          "Depthwise separable convolutions",
          "Self-attention mechanism",
          "Multi-scale feature extraction",
          "High accuracy",
          "Low latency",
          "Reduced parameter count",
          "Computational efficiency"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the **title** \"a lightweight hybrid vision transformer network for radar-based human activity recognition\" immediately indicates the development of a new system/method.\n*   the **abstract** mentions \"a lightweight hybrid vision transformer network\" and discusses challenges in embedded applications that this network aims to address.\n*   the **introduction** (which includes the end of the abstract in the provided text) extensively describes the proposed network: \"this network combines...\", \"feature pyramid architecture is applied...\", \"feature enhancement is executed by the stacked radar-vit...\", \"fold and unfold operations are added to lower the computational load...\", \"the convolution operator in the lh-vit is replaced by the res-se block...\".\n*   it also states, \"experiments based on two human activity datasets indicate our methods advantages...\", which confirms the evaluation of the *proposed method*.\n\nthese elements strongly align with the criteria for a **technical** paper: \"presents new methods, algorithms, or systems\" and discusses a \"proposed solution.\"\n\n**classification: technical**"
      },
      "file_name": "1871c0bc27bd5cd5f2f6c3d6cedeb4b8c5a89499.pdf"
    },
    {
      "success": true,
      "doc_id": "4d7e75fe07c9bdb1392c42392ed00e0a",
      "summary": "In recent studies, convolutional neural networks (CNNs) are mostly used as dynamic techniques for visualization-based malware classification and detection. Though vision transformer (ViT) proved its efficiency in image classification, a few of the earlier studies developed a ViT-based malware classifier. This paper proposes a butterfly construction-based vision transformer (B_ViT) model for visualization-based malware classification and detection. B_ViT has four phases: (1) image partitioning and patches embeddings; (2) local attention; (3) global attention; and (4) training and malware classification. B_ViT is an enhanced ViT architecture that supports the parallel processing of image patches and captures local and global spatial representations of malware images. B_ViT is a transfer learning-based model that uses a pre-trained ViT model on the ImageNet dataset to initialize the training parameters of transformers. Four B_ViT variants are experimented and evaluated on grayscale malware images collected from MalImg, Microsoft BIG datasets or converted from portable executable imports. The experiments show that B_ViT variants outperform the Input Enhanced vision transformer (IEViT) and ViT variants, achieving an accuracy equal to 99.49% and 99.99% for malware classification and detection respectively. The experiments also show that B_ViT is time effective for malware classification and detection where the average speed-up of B_ViT variants over IEViT and ViT variants are equal to 2.42 and 1.81 respectively. The analysis proves the efficiency of texture-based malware detection as well as the resilience of B_ViT to polymorphic obfuscation. Finally, the proposed B_ViT-based malware classifier outperforms the CNN-based malware classification methods in well.",
      "intriguing_abstract": "In recent studies, convolutional neural networks (CNNs) are mostly used as dynamic techniques for visualization-based malware classification and detection. Though vision transformer (ViT) proved its efficiency in image classification, a few of the earlier studies developed a ViT-based malware classifier. This paper proposes a butterfly construction-based vision transformer (B_ViT) model for visualization-based malware classification and detection. B_ViT has four phases: (1) image partitioning and patches embeddings; (2) local attention; (3) global attention; and (4) training and malware classification. B_ViT is an enhanced ViT architecture that supports the parallel processing of image patches and captures local and global spatial representations of malware images. B_ViT is a transfer learning-based model that uses a pre-trained ViT model on the ImageNet dataset to initialize the training parameters of transformers. Four B_ViT variants are experimented and evaluated on grayscale malware images collected from MalImg, Microsoft BIG datasets or converted from portable executable imports. The experiments show that B_ViT variants outperform the Input Enhanced vision transformer (IEViT) and ViT variants, achieving an accuracy equal to 99.49% and 99.99% for malware classification and detection respectively. The experiments also show that B_ViT is time effective for malware classification and detection where the average speed-up of B_ViT variants over IEViT and ViT variants are equal to 2.42 and 1.81 respectively. The analysis proves the efficiency of texture-based malware detection as well as the resilience of B_ViT to polymorphic obfuscation. Finally, the proposed B_ViT-based malware classifier outperforms the CNN-based malware classification methods in well.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/c064efa0419b75ba131ec8470ed80f01e1a80f64.pdf",
      "citation_key": "belal2023x1u",
      "metadata": {
        "title": "Global-Local Attention-Based Butterfly Vision Transformer for Visualization-Based Malware Classification",
        "authors": [
          "Mohamad Mulham Belal",
          "Dr. Divya Meena Sundaram"
        ],
        "published_date": "2023",
        "abstract": "In recent studies, convolutional neural networks (CNNs) are mostly used as dynamic techniques for visualization-based malware classification and detection. Though vision transformer (ViT) proved its efficiency in image classification, a few of the earlier studies developed a ViT-based malware classifier. This paper proposes a butterfly construction-based vision transformer (B_ViT) model for visualization-based malware classification and detection. B_ViT has four phases: (1) image partitioning and patches embeddings; (2) local attention; (3) global attention; and (4) training and malware classification. B_ViT is an enhanced ViT architecture that supports the parallel processing of image patches and captures local and global spatial representations of malware images. B_ViT is a transfer learning-based model that uses a pre-trained ViT model on the ImageNet dataset to initialize the training parameters of transformers. Four B_ViT variants are experimented and evaluated on grayscale malware images collected from MalImg, Microsoft BIG datasets or converted from portable executable imports. The experiments show that B_ViT variants outperform the Input Enhanced vision transformer (IEViT) and ViT variants, achieving an accuracy equal to 99.49% and 99.99% for malware classification and detection respectively. The experiments also show that B_ViT is time effective for malware classification and detection where the average speed-up of B_ViT variants over IEViT and ViT variants are equal to 2.42 and 1.81 respectively. The analysis proves the efficiency of texture-based malware detection as well as the resilience of B_ViT to polymorphic obfuscation. Finally, the proposed B_ViT-based malware classifier outperforms the CNN-based malware classification methods in well.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c064efa0419b75ba131ec8470ed80f01e1a80f64.pdf",
        "venue": "IEEE Access",
        "citationCount": 17,
        "score": 8.5,
        "summary": "In recent studies, convolutional neural networks (CNNs) are mostly used as dynamic techniques for visualization-based malware classification and detection. Though vision transformer (ViT) proved its efficiency in image classification, a few of the earlier studies developed a ViT-based malware classifier. This paper proposes a butterfly construction-based vision transformer (B_ViT) model for visualization-based malware classification and detection. B_ViT has four phases: (1) image partitioning and patches embeddings; (2) local attention; (3) global attention; and (4) training and malware classification. B_ViT is an enhanced ViT architecture that supports the parallel processing of image patches and captures local and global spatial representations of malware images. B_ViT is a transfer learning-based model that uses a pre-trained ViT model on the ImageNet dataset to initialize the training parameters of transformers. Four B_ViT variants are experimented and evaluated on grayscale malware images collected from MalImg, Microsoft BIG datasets or converted from portable executable imports. The experiments show that B_ViT variants outperform the Input Enhanced vision transformer (IEViT) and ViT variants, achieving an accuracy equal to 99.49% and 99.99% for malware classification and detection respectively. The experiments also show that B_ViT is time effective for malware classification and detection where the average speed-up of B_ViT variants over IEViT and ViT variants are equal to 2.42 and 1.81 respectively. The analysis proves the efficiency of texture-based malware detection as well as the resilience of B_ViT to polymorphic obfuscation. Finally, the proposed B_ViT-based malware classifier outperforms the CNN-based malware classification methods in well.",
        "keywords": []
      },
      "file_name": "c064efa0419b75ba131ec8470ed80f01e1a80f64.pdf"
    },
    {
      "success": true,
      "doc_id": "a590f30de2d7e13c23432e75f0eee256",
      "summary": "Vision transformers (ViTs) quantization offers a promising prospect to facilitate deploying large pre-trained networks on resource-limited devices. Fully-binarized ViTs (Bi-ViT) that pushes the quantization of ViTs to its limit remain largely unexplored and a very challenging task yet, due to their unacceptable performance. Through extensive empirical analyses, we identify the severe drop in ViT binarization is caused by attention distortion in self-attention, which technically stems from the gradient vanishing and ranking disorder. To address these issues, we first introduce a learnable scaling factor to reactivate the vanished gradients and illustrate its effectiveness through theoretical and experimental analyses. We then propose a ranking-aware distillation method to rectify the disordered ranking in a teacher-student framework. Bi-ViT achieves significant improvements over popular DeiT and Swin backbones in terms of Top-1 accuracy and FLOPs. For example, with DeiT-Tiny and Swin-Tiny, our method significantly outperforms baselines by 22.1% and 21.4% respectively, while 61.5x and 56.1x theoretical acceleration in terms of FLOPs compared with real-valued counterparts on ImageNet. Our codes and models are attached on https://github.com/YanjingLi0202/Bi-ViT/ .",
      "intriguing_abstract": "Vision transformers (ViTs) quantization offers a promising prospect to facilitate deploying large pre-trained networks on resource-limited devices. Fully-binarized ViTs (Bi-ViT) that pushes the quantization of ViTs to its limit remain largely unexplored and a very challenging task yet, due to their unacceptable performance. Through extensive empirical analyses, we identify the severe drop in ViT binarization is caused by attention distortion in self-attention, which technically stems from the gradient vanishing and ranking disorder. To address these issues, we first introduce a learnable scaling factor to reactivate the vanished gradients and illustrate its effectiveness through theoretical and experimental analyses. We then propose a ranking-aware distillation method to rectify the disordered ranking in a teacher-student framework. Bi-ViT achieves significant improvements over popular DeiT and Swin backbones in terms of Top-1 accuracy and FLOPs. For example, with DeiT-Tiny and Swin-Tiny, our method significantly outperforms baselines by 22.1% and 21.4% respectively, while 61.5x and 56.1x theoretical acceleration in terms of FLOPs compared with real-valued counterparts on ImageNet. Our codes and models are attached on https://github.com/YanjingLi0202/Bi-ViT/ .",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/b48a85980deb5f1baa64d862b9f0e4e62124e4de.pdf",
      "citation_key": "li20238ti",
      "metadata": {
        "title": "Bi-ViT: Pushing the Limit of Vision Transformer Quantization",
        "authors": [
          "Yanjing Li",
          "Sheng Xu",
          "Mingbao Lin",
          "Xianbin Cao",
          "Chuanjian Liu",
          "Xiao Sun",
          "Baochang Zhang"
        ],
        "published_date": "2023",
        "abstract": "Vision transformers (ViTs) quantization offers a promising prospect to facilitate deploying large pre-trained networks on resource-limited devices. Fully-binarized ViTs (Bi-ViT) that pushes the quantization of ViTs to its limit remain largely unexplored and a very challenging task yet, due to their unacceptable performance. Through extensive empirical analyses, we identify the severe drop in ViT binarization is caused by attention distortion in self-attention, which technically stems from the gradient vanishing and ranking disorder. To address these issues, we first introduce a learnable scaling factor to reactivate the vanished gradients and illustrate its effectiveness through theoretical and experimental analyses. We then propose a ranking-aware distillation method to rectify the disordered ranking in a teacher-student framework. Bi-ViT achieves significant improvements over popular DeiT and Swin backbones in terms of Top-1 accuracy and FLOPs. For example, with DeiT-Tiny and Swin-Tiny, our method significantly outperforms baselines by 22.1% and 21.4% respectively, while 61.5x and 56.1x theoretical acceleration in terms of FLOPs compared with real-valued counterparts on ImageNet. Our codes and models are attached on https://github.com/YanjingLi0202/Bi-ViT/ .",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b48a85980deb5f1baa64d862b9f0e4e62124e4de.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 16,
        "score": 8.0,
        "summary": "Vision transformers (ViTs) quantization offers a promising prospect to facilitate deploying large pre-trained networks on resource-limited devices. Fully-binarized ViTs (Bi-ViT) that pushes the quantization of ViTs to its limit remain largely unexplored and a very challenging task yet, due to their unacceptable performance. Through extensive empirical analyses, we identify the severe drop in ViT binarization is caused by attention distortion in self-attention, which technically stems from the gradient vanishing and ranking disorder. To address these issues, we first introduce a learnable scaling factor to reactivate the vanished gradients and illustrate its effectiveness through theoretical and experimental analyses. We then propose a ranking-aware distillation method to rectify the disordered ranking in a teacher-student framework. Bi-ViT achieves significant improvements over popular DeiT and Swin backbones in terms of Top-1 accuracy and FLOPs. For example, with DeiT-Tiny and Swin-Tiny, our method significantly outperforms baselines by 22.1% and 21.4% respectively, while 61.5x and 56.1x theoretical acceleration in terms of FLOPs compared with real-valued counterparts on ImageNet. Our codes and models are attached on https://github.com/YanjingLi0202/Bi-ViT/ .",
        "keywords": []
      },
      "file_name": "b48a85980deb5f1baa64d862b9f0e4e62124e4de.pdf"
    },
    {
      "success": true,
      "doc_id": "8fae7cacebd681d9c1a6b6aff970aa92",
      "summary": "In recent years, the ViT model has been widely used in the field of computer vision, especially for image classification tasks. This paper summarizes the application of ViT in image classification tasks, first introduces the image classification imple- mentation process and the basic architecture of the ViT model, then analyzes and summarizes the image classification methods, including traditional image classification methods, CNN-based image classification methods, and ViT-based image classification methods, and provides a comparative analysis of CNN and ViT. Subsequently, this paper outlines the application prospects of ViT in image classification and its future development and also outlines some shortcomings of ViT and its solutions.",
      "intriguing_abstract": "In recent years, the ViT model has been widely used in the field of computer vision, especially for image classification tasks. This paper summarizes the application of ViT in image classification tasks, first introduces the image classification imple- mentation process and the basic architecture of the ViT model, then analyzes and summarizes the image classification methods, including traditional image classification methods, CNN-based image classification methods, and ViT-based image classification methods, and provides a comparative analysis of CNN and ViT. Subsequently, this paper outlines the application prospects of ViT in image classification and its future development and also outlines some shortcomings of ViT and its solutions.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/8ec10ffe0c1fc8f6a92d541f0e002e78080b564a.pdf",
      "citation_key": "huo2023e5h",
      "metadata": {
        "title": "Vision Transformer (ViT)-based Applications in Image Classification",
        "authors": [
          "Yingzi Huo",
          "Kai Jin",
          "Jiahong Cai",
          "Huixuan Xiong",
          "Jiacheng Pang"
        ],
        "published_date": "2023",
        "abstract": "In recent years, the ViT model has been widely used in the field of computer vision, especially for image classification tasks. This paper summarizes the application of ViT in image classification tasks, first introduces the image classification imple- mentation process and the basic architecture of the ViT model, then analyzes and summarizes the image classification methods, including traditional image classification methods, CNN-based image classification methods, and ViT-based image classification methods, and provides a comparative analysis of CNN and ViT. Subsequently, this paper outlines the application prospects of ViT in image classification and its future development and also outlines some shortcomings of ViT and its solutions.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/8ec10ffe0c1fc8f6a92d541f0e002e78080b564a.pdf",
        "venue": "2023 IEEE 9th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)",
        "citationCount": 16,
        "score": 8.0,
        "summary": "In recent years, the ViT model has been widely used in the field of computer vision, especially for image classification tasks. This paper summarizes the application of ViT in image classification tasks, first introduces the image classification imple- mentation process and the basic architecture of the ViT model, then analyzes and summarizes the image classification methods, including traditional image classification methods, CNN-based image classification methods, and ViT-based image classification methods, and provides a comparative analysis of CNN and ViT. Subsequently, this paper outlines the application prospects of ViT in image classification and its future development and also outlines some shortcomings of ViT and its solutions.",
        "keywords": []
      },
      "file_name": "8ec10ffe0c1fc8f6a92d541f0e002e78080b564a.pdf"
    },
    {
      "success": true,
      "doc_id": "ab354382fd7a90a6767a109463515618",
      "summary": "Generalized zero-shot learning (GZSL) is a technique to train a deep learning model to identify unseen classes using the image attribute. In this paper, we put forth a new GZSL technique exploiting Vision Transformer (ViT) to maximize the attribute-related information contained in the image feature. In ViT, the entire image region is processed without the degradation of the image resolution and the local image information is preserved in patch features. To fully enjoy the benefits of ViT, we exploit patch features as well as the CLS feature in the extraction of the attribute-related image feature. In particular, we propose a novel attention-based module, called attribute attention module (AAM), to aggregate the attribute-related information in the patch features. From extensive experiments on benchmark datasets, we demonstrate that the proposed technique outperforms the state-of-the-art GZSL approaches by a large margin.",
      "intriguing_abstract": "Generalized zero-shot learning (GZSL) is a technique to train a deep learning model to identify unseen classes using the image attribute. In this paper, we put forth a new GZSL technique exploiting Vision Transformer (ViT) to maximize the attribute-related information contained in the image feature. In ViT, the entire image region is processed without the degradation of the image resolution and the local image information is preserved in patch features. To fully enjoy the benefits of ViT, we exploit patch features as well as the CLS feature in the extraction of the attribute-related image feature. In particular, we propose a novel attention-based module, called attribute attention module (AAM), to aggregate the attribute-related information in the patch features. From extensive experiments on benchmark datasets, we demonstrate that the proposed technique outperforms the state-of-the-art GZSL approaches by a large margin.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/769ba82ab77fc46db4a594dc64576286d02cf1d7.pdf",
      "citation_key": "kim2023cvz",
      "metadata": {
        "title": "Vision Transformer-Based Feature Extraction for Generalized Zero-Shot Learning",
        "authors": [
          "Jiseob Kim",
          "Kyuhong Shim",
          "Junhan Kim",
          "B. Shim"
        ],
        "published_date": "2023",
        "abstract": "Generalized zero-shot learning (GZSL) is a technique to train a deep learning model to identify unseen classes using the image attribute. In this paper, we put forth a new GZSL technique exploiting Vision Transformer (ViT) to maximize the attribute-related information contained in the image feature. In ViT, the entire image region is processed without the degradation of the image resolution and the local image information is preserved in patch features. To fully enjoy the benefits of ViT, we exploit patch features as well as the CLS feature in the extraction of the attribute-related image feature. In particular, we propose a novel attention-based module, called attribute attention module (AAM), to aggregate the attribute-related information in the patch features. From extensive experiments on benchmark datasets, we demonstrate that the proposed technique outperforms the state-of-the-art GZSL approaches by a large margin.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/769ba82ab77fc46db4a594dc64576286d02cf1d7.pdf",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "citationCount": 16,
        "score": 8.0,
        "summary": "Generalized zero-shot learning (GZSL) is a technique to train a deep learning model to identify unseen classes using the image attribute. In this paper, we put forth a new GZSL technique exploiting Vision Transformer (ViT) to maximize the attribute-related information contained in the image feature. In ViT, the entire image region is processed without the degradation of the image resolution and the local image information is preserved in patch features. To fully enjoy the benefits of ViT, we exploit patch features as well as the CLS feature in the extraction of the attribute-related image feature. In particular, we propose a novel attention-based module, called attribute attention module (AAM), to aggregate the attribute-related information in the patch features. From extensive experiments on benchmark datasets, we demonstrate that the proposed technique outperforms the state-of-the-art GZSL approaches by a large margin.",
        "keywords": []
      },
      "file_name": "769ba82ab77fc46db4a594dc64576286d02cf1d7.pdf"
    },
    {
      "success": true,
      "doc_id": "0d4781619edcede9a8094bac7f9a1aa4",
      "summary": "Cardiac magnetic resonance imaging analysis has been a useful tool in screening patients for heart disease. Early, timely and accurate diagnosis of diseases of the heart series is the key to effective treatment. MRI provides important material for the diagnosis of cardiac diseases. The rise of deep learning has transformed computer-aided diagnostic systems, especially in the field of medical imaging. Existing work on cardiac structure segmentation models based on MRI imaging mainly relies on convolutional neural networks (CNNs), which lack model diversity and limit the prediction performance. This paper introduces Visual Transformer with Feature Recombination and Feature Distillation(ViT-FRD), a novel learning pipeline that combines a visual transformer (ViT) and a CNN through knowledge refinement. The training procedure allows the student model, i.e., ViT, to learn from the teacher model, i.e., CNN, by optimizing distillation losses. Meanwhile, ViT-FRD provides two performance boosters to increase the efficacy and efficiency of training. The proposed method is validated on two cardiac MRI image datasets. The findings demonstrate that ViT-FRD achieves SOTA and outperforms the widely used baseline model.",
      "intriguing_abstract": "Cardiac magnetic resonance imaging analysis has been a useful tool in screening patients for heart disease. Early, timely and accurate diagnosis of diseases of the heart series is the key to effective treatment. MRI provides important material for the diagnosis of cardiac diseases. The rise of deep learning has transformed computer-aided diagnostic systems, especially in the field of medical imaging. Existing work on cardiac structure segmentation models based on MRI imaging mainly relies on convolutional neural networks (CNNs), which lack model diversity and limit the prediction performance. This paper introduces Visual Transformer with Feature Recombination and Feature Distillation(ViT-FRD), a novel learning pipeline that combines a visual transformer (ViT) and a CNN through knowledge refinement. The training procedure allows the student model, i.e., ViT, to learn from the teacher model, i.e., CNN, by optimizing distillation losses. Meanwhile, ViT-FRD provides two performance boosters to increase the efficacy and efficiency of training. The proposed method is validated on two cardiac MRI image datasets. The findings demonstrate that ViT-FRD achieves SOTA and outperforms the widely used baseline model.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d1255f6e7e7e14b253fe2245e842d0b45f5e715e.pdf",
      "citation_key": "fan2023whi",
      "metadata": {
        "title": "ViT-FRD: A Vision Transformer Model for Cardiac MRI Image Segmentation Based on Feature Recombination Distillation",
        "authors": [
          "Chunyu Fan",
          "Q. Su",
          "Zhifeng Xiao",
          "Haoran Su",
          "Aijie Hou",
          "Bo Luan"
        ],
        "published_date": "2023",
        "abstract": "Cardiac magnetic resonance imaging analysis has been a useful tool in screening patients for heart disease. Early, timely and accurate diagnosis of diseases of the heart series is the key to effective treatment. MRI provides important material for the diagnosis of cardiac diseases. The rise of deep learning has transformed computer-aided diagnostic systems, especially in the field of medical imaging. Existing work on cardiac structure segmentation models based on MRI imaging mainly relies on convolutional neural networks (CNNs), which lack model diversity and limit the prediction performance. This paper introduces Visual Transformer with Feature Recombination and Feature Distillation(ViT-FRD), a novel learning pipeline that combines a visual transformer (ViT) and a CNN through knowledge refinement. The training procedure allows the student model, i.e., ViT, to learn from the teacher model, i.e., CNN, by optimizing distillation losses. Meanwhile, ViT-FRD provides two performance boosters to increase the efficacy and efficiency of training. The proposed method is validated on two cardiac MRI image datasets. The findings demonstrate that ViT-FRD achieves SOTA and outperforms the widely used baseline model.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d1255f6e7e7e14b253fe2245e842d0b45f5e715e.pdf",
        "venue": "IEEE Access",
        "citationCount": 15,
        "score": 7.5,
        "summary": "Cardiac magnetic resonance imaging analysis has been a useful tool in screening patients for heart disease. Early, timely and accurate diagnosis of diseases of the heart series is the key to effective treatment. MRI provides important material for the diagnosis of cardiac diseases. The rise of deep learning has transformed computer-aided diagnostic systems, especially in the field of medical imaging. Existing work on cardiac structure segmentation models based on MRI imaging mainly relies on convolutional neural networks (CNNs), which lack model diversity and limit the prediction performance. This paper introduces Visual Transformer with Feature Recombination and Feature Distillation(ViT-FRD), a novel learning pipeline that combines a visual transformer (ViT) and a CNN through knowledge refinement. The training procedure allows the student model, i.e., ViT, to learn from the teacher model, i.e., CNN, by optimizing distillation losses. Meanwhile, ViT-FRD provides two performance boosters to increase the efficacy and efficiency of training. The proposed method is validated on two cardiac MRI image datasets. The findings demonstrate that ViT-FRD achieves SOTA and outperforms the widely used baseline model.",
        "keywords": []
      },
      "file_name": "d1255f6e7e7e14b253fe2245e842d0b45f5e715e.pdf"
    },
    {
      "success": true,
      "doc_id": "b603e26a0d6936c01f497f628aac482e",
      "summary": "Here's a focused summary of the paper \\cite{zhao2023rle} for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Accurate and efficient ship detection in Synthetic Aperture Radar (SAR) images, particularly under complex backgrounds.\n    *   **Importance and Challenge**: SAR images are crucial for various applications (e.g., marine observation, military reconnaissance). However, ship detection in SAR images is challenging due to:\n        *   Background clutter (e.g., small islands, sea structures causing false alarms).\n        *   Shape deformation and pose variations of ships.\n        *   Scale changes of targets.\n        *   Dense distribution of ships (e.g., in docks) leading to overlapping targets and low accuracy.\n        *   Existing deep learning methods often struggle with detection accuracy and computational speed in real-world complex scenarios.\n\n2.  **Related Work & Positioning**\n    *   **Traditional Methods (e.g., CFAR)**: Rely on artificial feature selection, leading to poor robustness and generalization, and require high contrast, making them unsuitable for complex environments.\n    *   **Two-stage Deep Learning (e.g., R-CNN series)**: Achieve high detection accuracy but suffer from low detection efficiency.\n    *   **One-stage Deep Learning (e.g., YOLO, SSD)**: Offer faster detection but are prone to false and missing detections compared to two-stage methods.\n    *   **YOLO's Limitations for SAR Ship Detection**: Poor recognition of small targets, inaccurate positioning, and lack of global information capture.\n    *   **Swin Transformer (ST) Limitations**: Limited contextual encoding capability and high parameter count, often requiring large training datasets.\n    *   **Positioning**: \\cite{zhao2023rle} aims to overcome the limitations of existing methods by integrating the strengths of Swin Transformer and YOLO (specifically YOLOX) with attention mechanisms to improve accuracy and speed in complex SAR environments.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Method**: Proposes ST-YOLOA, a novel Swin-transformer-based YOLO model with an attention mechanism. It builds upon YOLOX and Swin Transformer.\n    *   **STCNet Backbone Network**:\n        *   Integrates the Swin Transformer network architecture and Coordinate Attention (CA) model.\n        *   Enhances feature extraction performance and captures global information, addressing insufficient feature extraction from strong scattering in SAR images.\n        *   Adopts a layered architecture with Patch Embedding, Swin Transformer Blocks, and a CA-PatchMerging layer.\n    *   **Enhanced PANet for Feature Fusion (Neck)**:\n        *   Constructs a feature pyramid network using PANet with a residual structure to deeply fuse high-level and low-level features, increasing global feature extraction capability.\n        *   Introduces SE (Squeeze-and-Excitation) and CBAM (Convolutional Block Attention Module) attention mechanisms to enhance focus on target information.\n        *   Proposes a novel up/down-sampling method using binary trilinear interpolation up-sampling to cope with local interference and semantic information loss, maintaining original feature map data.\n    *   **Decoupled Detection Head**:\n        *   Separates classification and regression tasks to improve convergence speed and detection accuracy.\n    *   **EIOU Loss Function**: Utilizes EIOU as the localization loss function to accelerate convergence, improve model performance, and cope with sample imbalance.\n\n4.  **Key Technical Contributions**\n    *   **Novel Backbone (STCNet)**: A new backbone network that effectively combines Swin Transformer's global modeling capabilities with Coordinate Attention for robust feature extraction in SAR images, particularly under strong scattering.\n    *   **Enhanced Feature Pyramid Network**: An improved PANet architecture incorporating residual structures and SE/CBAM attention mechanisms for profound multi-scale feature fusion, coupled with binary trilinear interpolation for superior up-sampling.\n    *   **Optimized Detection Head and Loss**: Integration of a decoupled detection head and EIOU loss function to boost convergence speed, detection accuracy, and generalization ability, especially for imbalanced datasets.\n\n5.  **Experimental Validation**\n    *   **Datasets**: Constructed three SAR ship detection datasets:\n        *   Norm Test Set (NTS)\n        *   Complex Test Set (CTS)\n        *   Merged Test Set (MTS)\n    *   **Key Performance Metrics**: Accuracy (percentage) and real-time detection speed (FPS).\n    *   **Comparison Results**:\n        *   ST-YOLOA achieved accuracies of 97.37% on NTS, 75.69% on CTS, and 88.50% on MTS.\n        *   Demonstrated superior performance compared to other state-of-the-art methods.\n        *   Specifically, it achieved 4.83% higher accuracy than YOLOX on the challenging CTS.\n        *   Achieved real-time detection with a speed of 21.4 FPS.\n\n6.  **Limitations & Scope**\n    *   The paper primarily focuses on SAR ship detection, implying its direct applicability is within this specific domain.\n    *   While the paper addresses the challenges of complex backgrounds and dense targets, it does not explicitly state any inherent technical limitations of the ST-YOLOA model itself, beyond the general challenges of SAR imaging.\n    *   The Swin Transformer's need for large training data is mentioned as a general drawback, which might implicitly apply to ST-YOLOA, though the paper doesn't highlight it as a specific limitation of their method.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: Significantly advances SAR ship detection capabilities, particularly in complex environments, by achieving higher accuracy and maintaining real-time performance.\n    *   **Robustness in Challenging Scenarios**: Provides a more robust solution for detecting ships amidst background clutter, pose variations, scale changes, and overlapping targets.\n    *   **Potential Impact**: Offers a practical and efficient model for real-world applications in both civil and military fields, such as marine observation, catastrophe monitoring, and military reconnaissance, where accurate and fast SAR ship detection is critical.",
      "intriguing_abstract": "Accurate and efficient ship detection in Synthetic Aperture Radar (SAR) images is crucial but challenging due to complex backgrounds, diverse ship characteristics, and dense distributions. Current deep learning methods often struggle with balancing detection accuracy and real-time speed, especially for small or overlapping targets amidst clutter.\n\nThis paper introduces **ST-YOLOA**, a novel Swin-transformer-based YOLO model with an integrated attention mechanism, addressing these limitations. Our core innovations include **STCNet**, a backbone network fusing Swin Transformer and Coordinate Attention for robust global feature extraction in strong scattering. We further propose an **enhanced PANet** incorporating residual structures, SE/CBAM attention, and binary trilinear interpolation for superior multi-scale fusion and resilience to local interference. A **decoupled detection head** and **EIOU loss function** further boost convergence and precision.\n\nExtensive experiments on challenging SAR datasets demonstrate ST-YOLOA's superior performance, achieving state-of-the-art accuracy (e.g., 97.37% on NTS, 75.69% on CTS, a 4.83% improvement over YOLOX) while maintaining real-time detection speeds (21.4 FPS). This robust and efficient framework significantly advances SAR ship detection, offering critical capabilities for real-world civil and military applications.",
      "keywords": [
        "SAR ship detection",
        "complex backgrounds",
        "ST-YOLOA",
        "Swin Transformer",
        "YOLO",
        "attention mechanisms",
        "STCNet backbone",
        "enhanced PANet",
        "binary trilinear interpolation",
        "decoupled detection head",
        "EIOU loss function",
        "multi-scale feature fusion",
        "real-time detection",
        "high accuracy",
        "robustness"
      ],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/5572237909914e23758115be6b8d7f99a8bd51dc.pdf",
      "citation_key": "zhao2023rle",
      "metadata": {
        "title": "ST-YOLOA: a Swin-transformer-based YOLO model with an attention mechanism for SAR ship detection under complex background",
        "authors": [
          "Kai Zhao",
          "Ruitao Lu",
          "Siyu Wang",
          "Xiaogang Yang",
          "Qingge Li",
          "Jiwei Fan"
        ],
        "published_date": "2023",
        "abstract": "A synthetic aperture radar (SAR) image is crucial for ship detection in computer vision. Due to the background clutter, pose variations, and scale changes, it is a challenge to construct a SAR ship detection model with low false-alarm rates and high accuracy. Therefore, this paper proposes a novel SAR ship detection model called ST-YOLOA. First, the Swin Transformer network architecture and coordinate attention (CA) model are embedded in the STCNet backbone network to enhance the feature extraction performance and capture global information. Second, we used the PANet path aggregation network with a residual structure to construct the feature pyramid to increase global feature extraction capability. Next, to cope with the local interference and semantic information loss problems, a novel up/down-sampling method is proposed. Finally, the decoupled detection head is used to achieve the predicted output of the target position and the boundary box to improve convergence speed and detection accuracy. To demonstrate the efficiency of the proposed method, we have constructed three SAR ship detection datasets: a norm test set (NTS), a complex test set (CTS), and a merged test set (MTS). The experimental results show that our ST-YOLOA achieved an accuracy of 97.37%, 75.69%, and 88.50% on the three datasets, respectively, superior to the effects of other state-of-the-art methods. Our ST-YOLOA performs favorably in complex scenarios, and the accuracy is 4.83% higher than YOLOX on the CTS. Moreover, ST-YOLOA achieves real-time detection with a speed of 21.4 FPS.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/5572237909914e23758115be6b8d7f99a8bd51dc.pdf",
        "venue": "Frontiers in Neurorobotics",
        "citationCount": 15,
        "score": 7.5,
        "summary": "Here's a focused summary of the paper \\cite{zhao2023rle} for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Accurate and efficient ship detection in Synthetic Aperture Radar (SAR) images, particularly under complex backgrounds.\n    *   **Importance and Challenge**: SAR images are crucial for various applications (e.g., marine observation, military reconnaissance). However, ship detection in SAR images is challenging due to:\n        *   Background clutter (e.g., small islands, sea structures causing false alarms).\n        *   Shape deformation and pose variations of ships.\n        *   Scale changes of targets.\n        *   Dense distribution of ships (e.g., in docks) leading to overlapping targets and low accuracy.\n        *   Existing deep learning methods often struggle with detection accuracy and computational speed in real-world complex scenarios.\n\n2.  **Related Work & Positioning**\n    *   **Traditional Methods (e.g., CFAR)**: Rely on artificial feature selection, leading to poor robustness and generalization, and require high contrast, making them unsuitable for complex environments.\n    *   **Two-stage Deep Learning (e.g., R-CNN series)**: Achieve high detection accuracy but suffer from low detection efficiency.\n    *   **One-stage Deep Learning (e.g., YOLO, SSD)**: Offer faster detection but are prone to false and missing detections compared to two-stage methods.\n    *   **YOLO's Limitations for SAR Ship Detection**: Poor recognition of small targets, inaccurate positioning, and lack of global information capture.\n    *   **Swin Transformer (ST) Limitations**: Limited contextual encoding capability and high parameter count, often requiring large training datasets.\n    *   **Positioning**: \\cite{zhao2023rle} aims to overcome the limitations of existing methods by integrating the strengths of Swin Transformer and YOLO (specifically YOLOX) with attention mechanisms to improve accuracy and speed in complex SAR environments.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Method**: Proposes ST-YOLOA, a novel Swin-transformer-based YOLO model with an attention mechanism. It builds upon YOLOX and Swin Transformer.\n    *   **STCNet Backbone Network**:\n        *   Integrates the Swin Transformer network architecture and Coordinate Attention (CA) model.\n        *   Enhances feature extraction performance and captures global information, addressing insufficient feature extraction from strong scattering in SAR images.\n        *   Adopts a layered architecture with Patch Embedding, Swin Transformer Blocks, and a CA-PatchMerging layer.\n    *   **Enhanced PANet for Feature Fusion (Neck)**:\n        *   Constructs a feature pyramid network using PANet with a residual structure to deeply fuse high-level and low-level features, increasing global feature extraction capability.\n        *   Introduces SE (Squeeze-and-Excitation) and CBAM (Convolutional Block Attention Module) attention mechanisms to enhance focus on target information.\n        *   Proposes a novel up/down-sampling method using binary trilinear interpolation up-sampling to cope with local interference and semantic information loss, maintaining original feature map data.\n    *   **Decoupled Detection Head**:\n        *   Separates classification and regression tasks to improve convergence speed and detection accuracy.\n    *   **EIOU Loss Function**: Utilizes EIOU as the localization loss function to accelerate convergence, improve model performance, and cope with sample imbalance.\n\n4.  **Key Technical Contributions**\n    *   **Novel Backbone (STCNet)**: A new backbone network that effectively combines Swin Transformer's global modeling capabilities with Coordinate Attention for robust feature extraction in SAR images, particularly under strong scattering.\n    *   **Enhanced Feature Pyramid Network**: An improved PANet architecture incorporating residual structures and SE/CBAM attention mechanisms for profound multi-scale feature fusion, coupled with binary trilinear interpolation for superior up-sampling.\n    *   **Optimized Detection Head and Loss**: Integration of a decoupled detection head and EIOU loss function to boost convergence speed, detection accuracy, and generalization ability, especially for imbalanced datasets.\n\n5.  **Experimental Validation**\n    *   **Datasets**: Constructed three SAR ship detection datasets:\n        *   Norm Test Set (NTS)\n        *   Complex Test Set (CTS)\n        *   Merged Test Set (MTS)\n    *   **Key Performance Metrics**: Accuracy (percentage) and real-time detection speed (FPS).\n    *   **Comparison Results**:\n        *   ST-YOLOA achieved accuracies of 97.37% on NTS, 75.69% on CTS, and 88.50% on MTS.\n        *   Demonstrated superior performance compared to other state-of-the-art methods.\n        *   Specifically, it achieved 4.83% higher accuracy than YOLOX on the challenging CTS.\n        *   Achieved real-time detection with a speed of 21.4 FPS.\n\n6.  **Limitations & Scope**\n    *   The paper primarily focuses on SAR ship detection, implying its direct applicability is within this specific domain.\n    *   While the paper addresses the challenges of complex backgrounds and dense targets, it does not explicitly state any inherent technical limitations of the ST-YOLOA model itself, beyond the general challenges of SAR imaging.\n    *   The Swin Transformer's need for large training data is mentioned as a general drawback, which might implicitly apply to ST-YOLOA, though the paper doesn't highlight it as a specific limitation of their method.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: Significantly advances SAR ship detection capabilities, particularly in complex environments, by achieving higher accuracy and maintaining real-time performance.\n    *   **Robustness in Challenging Scenarios**: Provides a more robust solution for detecting ships amidst background clutter, pose variations, scale changes, and overlapping targets.\n    *   **Potential Impact**: Offers a practical and efficient model for real-world applications in both civil and military fields, such as marine observation, catastrophe monitoring, and military reconnaissance, where accurate and fast SAR ship detection is critical.",
        "keywords": [
          "SAR ship detection",
          "complex backgrounds",
          "ST-YOLOA",
          "Swin Transformer",
          "YOLO",
          "attention mechanisms",
          "STCNet backbone",
          "enhanced PANet",
          "binary trilinear interpolation",
          "decoupled detection head",
          "EIOU loss function",
          "multi-scale feature fusion",
          "real-time detection",
          "high accuracy",
          "robustness"
        ],
        "paper_type": "based on the provided information:\n\n1.  **title:** \"st-yoloa: a swin-transformer-based yolo model with an attention mechanism for sar ship detection under complex background\"\n    *   this title clearly indicates the development and presentation of a *new model* (\"st-yoloa\") which is a specific *method* or *system* (swin-transformer-based yolo model with an attention mechanism).\n\n2.  **abstract (metadata):** \"typeoriginal research\"\n    *   \"original research\" typically signifies the presentation of new findings, methods, or systems, aligning well with a technical paper.\n\n3.  **introduction:**\n    *   it identifies a specific technical problem: \"ship detection in sar images remains a challenge due to shape deformation, pose variation, and background clutter.\"\n    *   it discusses existing \"traditional ship detection methods\" and their limitations, setting the stage for a proposed solution.\n\nthese points strongly align with the criteria for a **technical** paper, which \"presents new methods, algorithms, or systems\" and discusses a \"technical problem, proposed solution.\"\n\n**classification:** technical"
      },
      "file_name": "5572237909914e23758115be6b8d7f99a8bd51dc.pdf"
    },
    {
      "success": true,
      "doc_id": "11aec4a82ae585ee03fe311df7aa3b71",
      "summary": "In this work, we seek to learn multiple mainstream vision tasks concurrently using a unified network, which is storage-efficient as numerous networks with task-shared parameters can be implanted into a single consolidated network. Our framework, vision transformer (ViT)-MVT, built on a plain and nonhierarchical ViT, incorporates numerous visual tasks into a modest supernet and optimizes them jointly across various dataset domains. For the design of ViT-MVT, we augment the ViT with a multihead self-attention (MHSE) to offer complementary cues in the channel and spatial dimension, as well as a local perception unit (LPU) and locality feed-forward network (locality FFN) for information exchange in the local region, thus endowing ViT-MVT with the ability to effectively optimize multiple tasks. Besides, we construct a search space comprising potential architectures with a broad spectrum of model sizes to offer various optimum candidates for diverse tasks. After that, we design a layer-adaptive sharing technique that automatically determines whether each layer of the transformer block is shared or not for all tasks, enabling ViT-MVT to obtain task-shared parameters for a reduction of storage and task-specific parameters to learn task-related features such that boosting performance. Finally, we introduce a joint-task evolutionary search algorithm to discover an optimal backbone for all tasks under total model size constraint, which challenges the conventional wisdom that visual tasks are typically supplied with backbone networks developed for image classification. Extensive experiments reveal that ViT-MVT delivers exceptional performances for multiple visual tasks over state-of-the-art methods while necessitating considerably fewer total storage costs. We further demonstrate that once ViT-MVT has been trained, ViT-MVT is capable of incremental learning when generalized to new tasks while retaining identical performances for trained tasks. The code is available at https://github.com/XT-1997/vitmvt.",
      "intriguing_abstract": "In this work, we seek to learn multiple mainstream vision tasks concurrently using a unified network, which is storage-efficient as numerous networks with task-shared parameters can be implanted into a single consolidated network. Our framework, vision transformer (ViT)-MVT, built on a plain and nonhierarchical ViT, incorporates numerous visual tasks into a modest supernet and optimizes them jointly across various dataset domains. For the design of ViT-MVT, we augment the ViT with a multihead self-attention (MHSE) to offer complementary cues in the channel and spatial dimension, as well as a local perception unit (LPU) and locality feed-forward network (locality FFN) for information exchange in the local region, thus endowing ViT-MVT with the ability to effectively optimize multiple tasks. Besides, we construct a search space comprising potential architectures with a broad spectrum of model sizes to offer various optimum candidates for diverse tasks. After that, we design a layer-adaptive sharing technique that automatically determines whether each layer of the transformer block is shared or not for all tasks, enabling ViT-MVT to obtain task-shared parameters for a reduction of storage and task-specific parameters to learn task-related features such that boosting performance. Finally, we introduce a joint-task evolutionary search algorithm to discover an optimal backbone for all tasks under total model size constraint, which challenges the conventional wisdom that visual tasks are typically supplied with backbone networks developed for image classification. Extensive experiments reveal that ViT-MVT delivers exceptional performances for multiple visual tasks over state-of-the-art methods while necessitating considerably fewer total storage costs. We further demonstrate that once ViT-MVT has been trained, ViT-MVT is capable of incremental learning when generalized to new tasks while retaining identical performances for trained tasks. The code is available at https://github.com/XT-1997/vitmvt.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/21c37fd6d00d6c727ec10f0c871c1ddf6e4f7ed9.pdf",
      "citation_key": "xie20234ve",
      "metadata": {
        "title": "ViT-MVT: A Unified Vision Transformer Network for Multiple Vision Tasks",
        "authors": [
          "Tao Xie",
          "Kun Dai",
          "Zhiqiang Jiang",
          "Ruifeng Li",
          "Shouren Mao",
          "Ke Wang",
          "Lijun Zhao"
        ],
        "published_date": "2023",
        "abstract": "In this work, we seek to learn multiple mainstream vision tasks concurrently using a unified network, which is storage-efficient as numerous networks with task-shared parameters can be implanted into a single consolidated network. Our framework, vision transformer (ViT)-MVT, built on a plain and nonhierarchical ViT, incorporates numerous visual tasks into a modest supernet and optimizes them jointly across various dataset domains. For the design of ViT-MVT, we augment the ViT with a multihead self-attention (MHSE) to offer complementary cues in the channel and spatial dimension, as well as a local perception unit (LPU) and locality feed-forward network (locality FFN) for information exchange in the local region, thus endowing ViT-MVT with the ability to effectively optimize multiple tasks. Besides, we construct a search space comprising potential architectures with a broad spectrum of model sizes to offer various optimum candidates for diverse tasks. After that, we design a layer-adaptive sharing technique that automatically determines whether each layer of the transformer block is shared or not for all tasks, enabling ViT-MVT to obtain task-shared parameters for a reduction of storage and task-specific parameters to learn task-related features such that boosting performance. Finally, we introduce a joint-task evolutionary search algorithm to discover an optimal backbone for all tasks under total model size constraint, which challenges the conventional wisdom that visual tasks are typically supplied with backbone networks developed for image classification. Extensive experiments reveal that ViT-MVT delivers exceptional performances for multiple visual tasks over state-of-the-art methods while necessitating considerably fewer total storage costs. We further demonstrate that once ViT-MVT has been trained, ViT-MVT is capable of incremental learning when generalized to new tasks while retaining identical performances for trained tasks. The code is available at https://github.com/XT-1997/vitmvt.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/21c37fd6d00d6c727ec10f0c871c1ddf6e4f7ed9.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 15,
        "score": 7.5,
        "summary": "In this work, we seek to learn multiple mainstream vision tasks concurrently using a unified network, which is storage-efficient as numerous networks with task-shared parameters can be implanted into a single consolidated network. Our framework, vision transformer (ViT)-MVT, built on a plain and nonhierarchical ViT, incorporates numerous visual tasks into a modest supernet and optimizes them jointly across various dataset domains. For the design of ViT-MVT, we augment the ViT with a multihead self-attention (MHSE) to offer complementary cues in the channel and spatial dimension, as well as a local perception unit (LPU) and locality feed-forward network (locality FFN) for information exchange in the local region, thus endowing ViT-MVT with the ability to effectively optimize multiple tasks. Besides, we construct a search space comprising potential architectures with a broad spectrum of model sizes to offer various optimum candidates for diverse tasks. After that, we design a layer-adaptive sharing technique that automatically determines whether each layer of the transformer block is shared or not for all tasks, enabling ViT-MVT to obtain task-shared parameters for a reduction of storage and task-specific parameters to learn task-related features such that boosting performance. Finally, we introduce a joint-task evolutionary search algorithm to discover an optimal backbone for all tasks under total model size constraint, which challenges the conventional wisdom that visual tasks are typically supplied with backbone networks developed for image classification. Extensive experiments reveal that ViT-MVT delivers exceptional performances for multiple visual tasks over state-of-the-art methods while necessitating considerably fewer total storage costs. We further demonstrate that once ViT-MVT has been trained, ViT-MVT is capable of incremental learning when generalized to new tasks while retaining identical performances for trained tasks. The code is available at https://github.com/XT-1997/vitmvt.",
        "keywords": []
      },
      "file_name": "21c37fd6d00d6c727ec10f0c871c1ddf6e4f7ed9.pdf"
    },
    {
      "success": true,
      "doc_id": "d8cfab4b7ae389ae0b40d4d3d7a48a35",
      "summary": "BACKGROUND\nRadiotherapy (RT) combined with cetuximab is the standard treatment for patients with inoperable head and neck cancers. Segmentation of head and neck (H&N) tumors is a prerequisite for radiotherapy planning but a time-consuming process. In recent years, deep convolutional neural networks (DCNN) have become the de facto standard for automated image segmentation. However, due to the expensive computational cost associated with enlarging the field of view in DCNNs, their ability to model long-range dependency is still limited, and this can result in sub-optimal segmentation performance for objects with background context spanning over long distances. On the other hand, Transformer models have demonstrated excellent capabilities in capturing such long-range information in several semantic segmentation tasks performed on medical images.\n\n\nPURPOSE\nDespite the impressive representation capacity of vision transformer models, current vision transformer-based segmentation models still suffer from inconsistent and incorrect dense predictions when fed with multi-modal input data. We suspect that the power of their self-attention mechanism may be limited in extracting the complementary information that exists in multi-modal data. To this end, we propose a novel segmentation model, debuted, Cross-modal Swin Transformer (SwinCross), with cross-modal attention (CMA) module to incorporate cross-modal feature extraction at multiple resolutions.\n\n\nMETHODS\nWe propose a novel architecture for cross-modal 3D semantic segmentation with two main components: (1) a cross-modal 3D Swin Transformer for integrating information from multiple modalities (PET and CT), and (2) a cross-modal shifted window attention block for learning complementary information from the modalities. To evaluate the efficacy of our approach, we conducted experiments and ablation studies on the HECKTOR 2021 challenge dataset. We compared our method against nnU-Net (the backbone of the top-5 methods in HECKTOR 2021) and other state-of-the-art transformer-based models, including UNETR and Swin UNETR. The experiments employed a five-fold cross-validation setup using PET and CT images.\n\n\nRESULTS\nEmpirical evidence demonstrates that our proposed method consistently outperforms the comparative techniques. This success can be attributed to the CMA module's capacity to enhance inter-modality feature representations between PET and CT during head-and-neck tumor segmentation. Notably, SwinCross consistently surpasses Swin UNETR across all five folds, showcasing its proficiency in learning multi-modal feature representations at varying resolutions through the cross-modal attention modules.\n\n\nCONCLUSIONS\nWe introduced a cross-modal Swin Transformer for automating the delineation of head and neck tumors in PET and CT images. Our model incorporates a cross-modality attention module, enabling the exchange of features between modalities at multiple resolutions. The experimental results establish the superiority of our method in capturing improved inter-modality correlations between PET and CT for head-and-neck tumor segmentation. Furthermore, the proposed methodology holds applicability to other semantic segmentation tasks involving different imaging modalities like SPECT/CT or PET/MRI. Code:https://github.com/yli192/SwinCross_CrossModalSwinTransformer_for_Medical_Image_Segmentation.",
      "intriguing_abstract": "BACKGROUND\nRadiotherapy (RT) combined with cetuximab is the standard treatment for patients with inoperable head and neck cancers. Segmentation of head and neck (H&N) tumors is a prerequisite for radiotherapy planning but a time-consuming process. In recent years, deep convolutional neural networks (DCNN) have become the de facto standard for automated image segmentation. However, due to the expensive computational cost associated with enlarging the field of view in DCNNs, their ability to model long-range dependency is still limited, and this can result in sub-optimal segmentation performance for objects with background context spanning over long distances. On the other hand, Transformer models have demonstrated excellent capabilities in capturing such long-range information in several semantic segmentation tasks performed on medical images.\n\n\nPURPOSE\nDespite the impressive representation capacity of vision transformer models, current vision transformer-based segmentation models still suffer from inconsistent and incorrect dense predictions when fed with multi-modal input data. We suspect that the power of their self-attention mechanism may be limited in extracting the complementary information that exists in multi-modal data. To this end, we propose a novel segmentation model, debuted, Cross-modal Swin Transformer (SwinCross), with cross-modal attention (CMA) module to incorporate cross-modal feature extraction at multiple resolutions.\n\n\nMETHODS\nWe propose a novel architecture for cross-modal 3D semantic segmentation with two main components: (1) a cross-modal 3D Swin Transformer for integrating information from multiple modalities (PET and CT), and (2) a cross-modal shifted window attention block for learning complementary information from the modalities. To evaluate the efficacy of our approach, we conducted experiments and ablation studies on the HECKTOR 2021 challenge dataset. We compared our method against nnU-Net (the backbone of the top-5 methods in HECKTOR 2021) and other state-of-the-art transformer-based models, including UNETR and Swin UNETR. The experiments employed a five-fold cross-validation setup using PET and CT images.\n\n\nRESULTS\nEmpirical evidence demonstrates that our proposed method consistently outperforms the comparative techniques. This success can be attributed to the CMA module's capacity to enhance inter-modality feature representations between PET and CT during head-and-neck tumor segmentation. Notably, SwinCross consistently surpasses Swin UNETR across all five folds, showcasing its proficiency in learning multi-modal feature representations at varying resolutions through the cross-modal attention modules.\n\n\nCONCLUSIONS\nWe introduced a cross-modal Swin Transformer for automating the delineation of head and neck tumors in PET and CT images. Our model incorporates a cross-modality attention module, enabling the exchange of features between modalities at multiple resolutions. The experimental results establish the superiority of our method in capturing improved inter-modality correlations between PET and CT for head-and-neck tumor segmentation. Furthermore, the proposed methodology holds applicability to other semantic segmentation tasks involving different imaging modalities like SPECT/CT or PET/MRI. Code:https://github.com/yli192/SwinCross_CrossModalSwinTransformer_for_Medical_Image_Segmentation.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/e38e70580acb204c05096de8da90b7ab1d4bdb6b.pdf",
      "citation_key": "li20233lv",
      "metadata": {
        "title": "SwinCross: Cross-modal Swin Transformer for Head-and-Neck Tumor Segmentation in PET/CT Images",
        "authors": [
          "Gary Y. Li",
          "Junyu Chen",
          "Se-In Jang",
          "Kuang Gong",
          "Quanzheng Li"
        ],
        "published_date": "2023",
        "abstract": "BACKGROUND\nRadiotherapy (RT) combined with cetuximab is the standard treatment for patients with inoperable head and neck cancers. Segmentation of head and neck (H&N) tumors is a prerequisite for radiotherapy planning but a time-consuming process. In recent years, deep convolutional neural networks (DCNN) have become the de facto standard for automated image segmentation. However, due to the expensive computational cost associated with enlarging the field of view in DCNNs, their ability to model long-range dependency is still limited, and this can result in sub-optimal segmentation performance for objects with background context spanning over long distances. On the other hand, Transformer models have demonstrated excellent capabilities in capturing such long-range information in several semantic segmentation tasks performed on medical images.\n\n\nPURPOSE\nDespite the impressive representation capacity of vision transformer models, current vision transformer-based segmentation models still suffer from inconsistent and incorrect dense predictions when fed with multi-modal input data. We suspect that the power of their self-attention mechanism may be limited in extracting the complementary information that exists in multi-modal data. To this end, we propose a novel segmentation model, debuted, Cross-modal Swin Transformer (SwinCross), with cross-modal attention (CMA) module to incorporate cross-modal feature extraction at multiple resolutions.\n\n\nMETHODS\nWe propose a novel architecture for cross-modal 3D semantic segmentation with two main components: (1) a cross-modal 3D Swin Transformer for integrating information from multiple modalities (PET and CT), and (2) a cross-modal shifted window attention block for learning complementary information from the modalities. To evaluate the efficacy of our approach, we conducted experiments and ablation studies on the HECKTOR 2021 challenge dataset. We compared our method against nnU-Net (the backbone of the top-5 methods in HECKTOR 2021) and other state-of-the-art transformer-based models, including UNETR and Swin UNETR. The experiments employed a five-fold cross-validation setup using PET and CT images.\n\n\nRESULTS\nEmpirical evidence demonstrates that our proposed method consistently outperforms the comparative techniques. This success can be attributed to the CMA module's capacity to enhance inter-modality feature representations between PET and CT during head-and-neck tumor segmentation. Notably, SwinCross consistently surpasses Swin UNETR across all five folds, showcasing its proficiency in learning multi-modal feature representations at varying resolutions through the cross-modal attention modules.\n\n\nCONCLUSIONS\nWe introduced a cross-modal Swin Transformer for automating the delineation of head and neck tumors in PET and CT images. Our model incorporates a cross-modality attention module, enabling the exchange of features between modalities at multiple resolutions. The experimental results establish the superiority of our method in capturing improved inter-modality correlations between PET and CT for head-and-neck tumor segmentation. Furthermore, the proposed methodology holds applicability to other semantic segmentation tasks involving different imaging modalities like SPECT/CT or PET/MRI. Code:https://github.com/yli192/SwinCross_CrossModalSwinTransformer_for_Medical_Image_Segmentation.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e38e70580acb204c05096de8da90b7ab1d4bdb6b.pdf",
        "venue": "Medical Physics (Lancaster)",
        "citationCount": 15,
        "score": 7.5,
        "summary": "BACKGROUND\nRadiotherapy (RT) combined with cetuximab is the standard treatment for patients with inoperable head and neck cancers. Segmentation of head and neck (H&N) tumors is a prerequisite for radiotherapy planning but a time-consuming process. In recent years, deep convolutional neural networks (DCNN) have become the de facto standard for automated image segmentation. However, due to the expensive computational cost associated with enlarging the field of view in DCNNs, their ability to model long-range dependency is still limited, and this can result in sub-optimal segmentation performance for objects with background context spanning over long distances. On the other hand, Transformer models have demonstrated excellent capabilities in capturing such long-range information in several semantic segmentation tasks performed on medical images.\n\n\nPURPOSE\nDespite the impressive representation capacity of vision transformer models, current vision transformer-based segmentation models still suffer from inconsistent and incorrect dense predictions when fed with multi-modal input data. We suspect that the power of their self-attention mechanism may be limited in extracting the complementary information that exists in multi-modal data. To this end, we propose a novel segmentation model, debuted, Cross-modal Swin Transformer (SwinCross), with cross-modal attention (CMA) module to incorporate cross-modal feature extraction at multiple resolutions.\n\n\nMETHODS\nWe propose a novel architecture for cross-modal 3D semantic segmentation with two main components: (1) a cross-modal 3D Swin Transformer for integrating information from multiple modalities (PET and CT), and (2) a cross-modal shifted window attention block for learning complementary information from the modalities. To evaluate the efficacy of our approach, we conducted experiments and ablation studies on the HECKTOR 2021 challenge dataset. We compared our method against nnU-Net (the backbone of the top-5 methods in HECKTOR 2021) and other state-of-the-art transformer-based models, including UNETR and Swin UNETR. The experiments employed a five-fold cross-validation setup using PET and CT images.\n\n\nRESULTS\nEmpirical evidence demonstrates that our proposed method consistently outperforms the comparative techniques. This success can be attributed to the CMA module's capacity to enhance inter-modality feature representations between PET and CT during head-and-neck tumor segmentation. Notably, SwinCross consistently surpasses Swin UNETR across all five folds, showcasing its proficiency in learning multi-modal feature representations at varying resolutions through the cross-modal attention modules.\n\n\nCONCLUSIONS\nWe introduced a cross-modal Swin Transformer for automating the delineation of head and neck tumors in PET and CT images. Our model incorporates a cross-modality attention module, enabling the exchange of features between modalities at multiple resolutions. The experimental results establish the superiority of our method in capturing improved inter-modality correlations between PET and CT for head-and-neck tumor segmentation. Furthermore, the proposed methodology holds applicability to other semantic segmentation tasks involving different imaging modalities like SPECT/CT or PET/MRI. Code:https://github.com/yli192/SwinCross_CrossModalSwinTransformer_for_Medical_Image_Segmentation.",
        "keywords": []
      },
      "file_name": "e38e70580acb204c05096de8da90b7ab1d4bdb6b.pdf"
    },
    {
      "success": true,
      "doc_id": "79419ae48b7c95f7eb13975c06332925",
      "summary": "Advanced image tampering techniques are increasingly challenging the trustworthiness of multimedia, leading to the development of Image Manipulation Localization (IML). But what makes a good IML model? The answer lies in the way to capture artifacts. Exploiting artifacts requires the model to extract non-semantic discrepancies between manipulated and authentic regions, necessitating explicit comparisons between the two areas. With the self-attention mechanism, naturally, the Transformer should be a better candidate to capture artifacts. However, due to limited datasets, there is currently no pure ViT-based approach for IML to serve as a benchmark, and CNNs dominate the entire task. Nevertheless, CNNs suffer from weak long-range and non-semantic modeling. To bridge this gap, based on the fact that artifacts are sensitive to image resolution, amplified under multi-scale features, and massive at the manipulation border, we formulate the answer to the former question as building a ViT with high-resolution capacity, multi-scale feature extraction capability, and manipulation edge supervision that could converge with a small amount of data. We term this simple but effective ViT paradigm IML-ViT, which has significant potential to become a new benchmark for IML. Extensive experiments on three different mainstream protocols verified our model outperforms the state-of-the-art manipulation localization methods. Code and models are available at https://github.com/SunnyHaze/IML-ViT.",
      "intriguing_abstract": "Advanced image tampering techniques are increasingly challenging the trustworthiness of multimedia, leading to the development of Image Manipulation Localization (IML). But what makes a good IML model? The answer lies in the way to capture artifacts. Exploiting artifacts requires the model to extract non-semantic discrepancies between manipulated and authentic regions, necessitating explicit comparisons between the two areas. With the self-attention mechanism, naturally, the Transformer should be a better candidate to capture artifacts. However, due to limited datasets, there is currently no pure ViT-based approach for IML to serve as a benchmark, and CNNs dominate the entire task. Nevertheless, CNNs suffer from weak long-range and non-semantic modeling. To bridge this gap, based on the fact that artifacts are sensitive to image resolution, amplified under multi-scale features, and massive at the manipulation border, we formulate the answer to the former question as building a ViT with high-resolution capacity, multi-scale feature extraction capability, and manipulation edge supervision that could converge with a small amount of data. We term this simple but effective ViT paradigm IML-ViT, which has significant potential to become a new benchmark for IML. Extensive experiments on three different mainstream protocols verified our model outperforms the state-of-the-art manipulation localization methods. Code and models are available at https://github.com/SunnyHaze/IML-ViT.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/a246677a3d68e8cd7a63d24639a5cf3fc3b9f56e.pdf",
      "citation_key": "ma2023qek",
      "metadata": {
        "title": "IML-ViT: Benchmarking Image Manipulation Localization by Vision Transformer",
        "authors": [
          "Xiaochen Ma",
          "Bo Du",
          "Zhuohang Jiang",
          "Xia Du",
          "Ahmed Y. Al Hammadi",
          "Jizhe Zhou"
        ],
        "published_date": "2023",
        "abstract": "Advanced image tampering techniques are increasingly challenging the trustworthiness of multimedia, leading to the development of Image Manipulation Localization (IML). But what makes a good IML model? The answer lies in the way to capture artifacts. Exploiting artifacts requires the model to extract non-semantic discrepancies between manipulated and authentic regions, necessitating explicit comparisons between the two areas. With the self-attention mechanism, naturally, the Transformer should be a better candidate to capture artifacts. However, due to limited datasets, there is currently no pure ViT-based approach for IML to serve as a benchmark, and CNNs dominate the entire task. Nevertheless, CNNs suffer from weak long-range and non-semantic modeling. To bridge this gap, based on the fact that artifacts are sensitive to image resolution, amplified under multi-scale features, and massive at the manipulation border, we formulate the answer to the former question as building a ViT with high-resolution capacity, multi-scale feature extraction capability, and manipulation edge supervision that could converge with a small amount of data. We term this simple but effective ViT paradigm IML-ViT, which has significant potential to become a new benchmark for IML. Extensive experiments on three different mainstream protocols verified our model outperforms the state-of-the-art manipulation localization methods. Code and models are available at https://github.com/SunnyHaze/IML-ViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a246677a3d68e8cd7a63d24639a5cf3fc3b9f56e.pdf",
        "venue": "",
        "citationCount": 15,
        "score": 7.5,
        "summary": "Advanced image tampering techniques are increasingly challenging the trustworthiness of multimedia, leading to the development of Image Manipulation Localization (IML). But what makes a good IML model? The answer lies in the way to capture artifacts. Exploiting artifacts requires the model to extract non-semantic discrepancies between manipulated and authentic regions, necessitating explicit comparisons between the two areas. With the self-attention mechanism, naturally, the Transformer should be a better candidate to capture artifacts. However, due to limited datasets, there is currently no pure ViT-based approach for IML to serve as a benchmark, and CNNs dominate the entire task. Nevertheless, CNNs suffer from weak long-range and non-semantic modeling. To bridge this gap, based on the fact that artifacts are sensitive to image resolution, amplified under multi-scale features, and massive at the manipulation border, we formulate the answer to the former question as building a ViT with high-resolution capacity, multi-scale feature extraction capability, and manipulation edge supervision that could converge with a small amount of data. We term this simple but effective ViT paradigm IML-ViT, which has significant potential to become a new benchmark for IML. Extensive experiments on three different mainstream protocols verified our model outperforms the state-of-the-art manipulation localization methods. Code and models are available at https://github.com/SunnyHaze/IML-ViT.",
        "keywords": []
      },
      "file_name": "a246677a3d68e8cd7a63d24639a5cf3fc3b9f56e.pdf"
    },
    {
      "success": true,
      "doc_id": "420995f3eeead475de2fac8e6a2e20a6",
      "summary": "Breast cancer is the most prevalent type of disease among women. It has become one of the foremost causes of death among women globally. Early detection plays a significant role in administering personalized treatment and improving patient outcomes. Mammography procedures are often used to detect early-stage cancer cells. This traditional method of mammography while valuable has limitations in its potential for false positives and negatives, patient discomfort, and radiation exposure. Therefore, there is a probe for more accurate techniques required in detecting breast cancer, leading to exploring the potential of machine learning in the classification of diagnostic images due to its efficiency and accuracy. This study conducted a comparative analysis of pre-trained CNNs (ResNet50 and VGG16) and vision transformers (ViT-base and SWIN transformer) with the inclusion of ViT-base trained from scratch model architectures to effectively classify mammographic breast cancer images into benign and malignant cases. The SWIN transformer exhibits superior performance with 99.9% accuracy and a precision of 99.8%. These findings demonstrate the efficiency of deep learning to accurately classify mammographic breast cancer images for the diagnosis of breast cancer, leading to improvements in patient outcomes.",
      "intriguing_abstract": "Breast cancer is the most prevalent type of disease among women. It has become one of the foremost causes of death among women globally. Early detection plays a significant role in administering personalized treatment and improving patient outcomes. Mammography procedures are often used to detect early-stage cancer cells. This traditional method of mammography while valuable has limitations in its potential for false positives and negatives, patient discomfort, and radiation exposure. Therefore, there is a probe for more accurate techniques required in detecting breast cancer, leading to exploring the potential of machine learning in the classification of diagnostic images due to its efficiency and accuracy. This study conducted a comparative analysis of pre-trained CNNs (ResNet50 and VGG16) and vision transformers (ViT-base and SWIN transformer) with the inclusion of ViT-base trained from scratch model architectures to effectively classify mammographic breast cancer images into benign and malignant cases. The SWIN transformer exhibits superior performance with 99.9% accuracy and a precision of 99.8%. These findings demonstrate the efficiency of deep learning to accurately classify mammographic breast cancer images for the diagnosis of breast cancer, leading to improvements in patient outcomes.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/1397d1bcfeae68078756fdbc3ed7e62ef51a1e32.pdf",
      "citation_key": "tanimola20246cv",
      "metadata": {
        "title": "Breast Cancer Classification Using Fine-Tuned SWIN Transformer Model on Mammographic Images",
        "authors": [
          "Oluwatosin Tanimola",
          "Olamilekan Shobayo",
          "O. Popoola",
          "O. Okoyeigbo"
        ],
        "published_date": "2024",
        "abstract": "Breast cancer is the most prevalent type of disease among women. It has become one of the foremost causes of death among women globally. Early detection plays a significant role in administering personalized treatment and improving patient outcomes. Mammography procedures are often used to detect early-stage cancer cells. This traditional method of mammography while valuable has limitations in its potential for false positives and negatives, patient discomfort, and radiation exposure. Therefore, there is a probe for more accurate techniques required in detecting breast cancer, leading to exploring the potential of machine learning in the classification of diagnostic images due to its efficiency and accuracy. This study conducted a comparative analysis of pre-trained CNNs (ResNet50 and VGG16) and vision transformers (ViT-base and SWIN transformer) with the inclusion of ViT-base trained from scratch model architectures to effectively classify mammographic breast cancer images into benign and malignant cases. The SWIN transformer exhibits superior performance with 99.9% accuracy and a precision of 99.8%. These findings demonstrate the efficiency of deep learning to accurately classify mammographic breast cancer images for the diagnosis of breast cancer, leading to improvements in patient outcomes.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1397d1bcfeae68078756fdbc3ed7e62ef51a1e32.pdf",
        "venue": "Analytics",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Breast cancer is the most prevalent type of disease among women. It has become one of the foremost causes of death among women globally. Early detection plays a significant role in administering personalized treatment and improving patient outcomes. Mammography procedures are often used to detect early-stage cancer cells. This traditional method of mammography while valuable has limitations in its potential for false positives and negatives, patient discomfort, and radiation exposure. Therefore, there is a probe for more accurate techniques required in detecting breast cancer, leading to exploring the potential of machine learning in the classification of diagnostic images due to its efficiency and accuracy. This study conducted a comparative analysis of pre-trained CNNs (ResNet50 and VGG16) and vision transformers (ViT-base and SWIN transformer) with the inclusion of ViT-base trained from scratch model architectures to effectively classify mammographic breast cancer images into benign and malignant cases. The SWIN transformer exhibits superior performance with 99.9% accuracy and a precision of 99.8%. These findings demonstrate the efficiency of deep learning to accurately classify mammographic breast cancer images for the diagnosis of breast cancer, leading to improvements in patient outcomes.",
        "keywords": []
      },
      "file_name": "1397d1bcfeae68078756fdbc3ed7e62ef51a1e32.pdf"
    },
    {
      "success": true,
      "doc_id": "ecd7e012612355769d217b8823c9d74f",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/52239e2338cd9f0e0e690203af8f0f1260ef7abf.pdf",
      "citation_key": "chen2023xxw",
      "metadata": {
        "title": "Swin-Fusion: Swin-Transformer with Feature Fusion for Human Action Recognition",
        "authors": [
          "Tiansheng Chen",
          "L. Mo"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/52239e2338cd9f0e0e690203af8f0f1260ef7abf.pdf",
        "venue": "Neural Processing Letters",
        "citationCount": 14,
        "score": 7.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "52239e2338cd9f0e0e690203af8f0f1260ef7abf.pdf"
    },
    {
      "success": true,
      "doc_id": "4e0e672c2ed348f70d7efaab276a2273",
      "summary": "Vision transformers (ViTs) have demonstrated remarkable performance across various visual tasks. However, ViT models suffer from substantial computational and memory requirements, making it challenging to deploy them on resource-constrained platforms. Quantization is a popular approach for reducing model size, but most studies mainly focus on equal bit-width quantization for the entire network, resulting in sub-optimal solutions. While there are few works on mixed precision quantization (MPQ) for ViTs, they typically rely on search space-based methods or employ mixed precision arbitrarily. In this paper, we introduce LRP-QViT, an explainability-based method for assigning mixed-precision bit allocations to different layers based on their importance during classification. Specifically, to measure the contribution score of each layer in predicting the target class, we employ the Layer-wise Relevance Propagation (LRP) method. LRP assigns local relevance at the output layer and propagates it through all layers, distributing the relevance until it reaches the input layers. These relevance scores serve as indicators for computing the layer contribution score. Additionally, we have introduced a clipped channel-wise quantization aimed at eliminating outliers from post-LayerNorm activations to alleviate severe inter-channel variations. To validate and assess our approach, we employ LRP-QViT across ViT, DeiT, and Swin transformer models on various datasets. Our experimental findings demonstrate that both our fixed-bit and mixed-bit post-training quantization methods surpass existing models in the context of 4-bit and 6-bit quantization.",
      "intriguing_abstract": "Vision transformers (ViTs) have demonstrated remarkable performance across various visual tasks. However, ViT models suffer from substantial computational and memory requirements, making it challenging to deploy them on resource-constrained platforms. Quantization is a popular approach for reducing model size, but most studies mainly focus on equal bit-width quantization for the entire network, resulting in sub-optimal solutions. While there are few works on mixed precision quantization (MPQ) for ViTs, they typically rely on search space-based methods or employ mixed precision arbitrarily. In this paper, we introduce LRP-QViT, an explainability-based method for assigning mixed-precision bit allocations to different layers based on their importance during classification. Specifically, to measure the contribution score of each layer in predicting the target class, we employ the Layer-wise Relevance Propagation (LRP) method. LRP assigns local relevance at the output layer and propagates it through all layers, distributing the relevance until it reaches the input layers. These relevance scores serve as indicators for computing the layer contribution score. Additionally, we have introduced a clipped channel-wise quantization aimed at eliminating outliers from post-LayerNorm activations to alleviate severe inter-channel variations. To validate and assess our approach, we employ LRP-QViT across ViT, DeiT, and Swin transformer models on various datasets. Our experimental findings demonstrate that both our fixed-bit and mixed-bit post-training quantization methods surpass existing models in the context of 4-bit and 6-bit quantization.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f996d5ee3b8ad3c60510862a92fd72c6a41777e0.pdf",
      "citation_key": "ranjan20243bn",
      "metadata": {
        "title": "LRP-QViT: Mixed-Precision Vision Transformer Quantization via Layer-wise Relevance Propagation",
        "authors": [
          "Navin Ranjan",
          "Andreas E. Savakis"
        ],
        "published_date": "2024",
        "abstract": "Vision transformers (ViTs) have demonstrated remarkable performance across various visual tasks. However, ViT models suffer from substantial computational and memory requirements, making it challenging to deploy them on resource-constrained platforms. Quantization is a popular approach for reducing model size, but most studies mainly focus on equal bit-width quantization for the entire network, resulting in sub-optimal solutions. While there are few works on mixed precision quantization (MPQ) for ViTs, they typically rely on search space-based methods or employ mixed precision arbitrarily. In this paper, we introduce LRP-QViT, an explainability-based method for assigning mixed-precision bit allocations to different layers based on their importance during classification. Specifically, to measure the contribution score of each layer in predicting the target class, we employ the Layer-wise Relevance Propagation (LRP) method. LRP assigns local relevance at the output layer and propagates it through all layers, distributing the relevance until it reaches the input layers. These relevance scores serve as indicators for computing the layer contribution score. Additionally, we have introduced a clipped channel-wise quantization aimed at eliminating outliers from post-LayerNorm activations to alleviate severe inter-channel variations. To validate and assess our approach, we employ LRP-QViT across ViT, DeiT, and Swin transformer models on various datasets. Our experimental findings demonstrate that both our fixed-bit and mixed-bit post-training quantization methods surpass existing models in the context of 4-bit and 6-bit quantization.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f996d5ee3b8ad3c60510862a92fd72c6a41777e0.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Vision transformers (ViTs) have demonstrated remarkable performance across various visual tasks. However, ViT models suffer from substantial computational and memory requirements, making it challenging to deploy them on resource-constrained platforms. Quantization is a popular approach for reducing model size, but most studies mainly focus on equal bit-width quantization for the entire network, resulting in sub-optimal solutions. While there are few works on mixed precision quantization (MPQ) for ViTs, they typically rely on search space-based methods or employ mixed precision arbitrarily. In this paper, we introduce LRP-QViT, an explainability-based method for assigning mixed-precision bit allocations to different layers based on their importance during classification. Specifically, to measure the contribution score of each layer in predicting the target class, we employ the Layer-wise Relevance Propagation (LRP) method. LRP assigns local relevance at the output layer and propagates it through all layers, distributing the relevance until it reaches the input layers. These relevance scores serve as indicators for computing the layer contribution score. Additionally, we have introduced a clipped channel-wise quantization aimed at eliminating outliers from post-LayerNorm activations to alleviate severe inter-channel variations. To validate and assess our approach, we employ LRP-QViT across ViT, DeiT, and Swin transformer models on various datasets. Our experimental findings demonstrate that both our fixed-bit and mixed-bit post-training quantization methods surpass existing models in the context of 4-bit and 6-bit quantization.",
        "keywords": []
      },
      "file_name": "f996d5ee3b8ad3c60510862a92fd72c6a41777e0.pdf"
    },
    {
      "success": true,
      "doc_id": "a629c34315f6b9e3efa2c35e1e2c5aef",
      "summary": "Transformers have made remarkable contributions to natural language processing (NLP) and many other fields. Recently, transformer-based models have achieved state-of-the-art (SOTA) performance on computer vision tasks compared with traditional convolutional neural networks (CNNs). Unfortunately, existing CNN accelerators cannot efficiently support transformer due to the high computational overhead and redundant data accesses associated with the KQV matrix operations in the transformer models. If the recently-developed NLP transformer accelerators are applied to the vision transformer (ViT) models, their efficiency would decrease due to three challenges. 1) Redundant data storage and access still exist in ViT data flow scheduling. 2) For matrix transposition in transformer models, the previous transpose-operation schemes lack flexibility, resulting in extra area overhead. 3) The sparse acceleration schemes for NLP in prior transformer accelerators cannot efficiently accelerate ViT with relatively fewer tokens. To overcome these challenges, we propose <inline-formula> <tex-math notation=\"LaTeX\">$P^{3}$ </tex-math></inline-formula> ViT, a computing-in-memory (CIM)-based architecture, to efficiently accelerate ViT, achieving high utilization on data flow scheduling. There are three key contributions: 1) P3ViT architecture supports three ping-pong pipeline scheduling modes, involving inter-core parallel and intra-core ping-pong pipeline mode (IEP-IAP3), inter-core pipeline and parallel mode (IEP2), and full parallel mode, to eliminate redundant memory accesses. 2) A two-way ping-pong CIM macro is proposed, which can be configured to regular calculation mode and transpose calculation mode to adapt to both <inline-formula> <tex-math notation=\"LaTeX\">$\\text{Q}\\times \\text{K}^{\\mathrm {T}}$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$\\text{A}\\times \\text{V}$ </tex-math></inline-formula> tasks. 3) P3ViT also runs a small prediction network. It prunes redundant tokens to be a standard number hierarchically and dynamically, enabling high-throughput and high-utilization attention computation. Measurements show that P3ViT achieves <inline-formula> <tex-math notation=\"LaTeX\">$1.13\\times $ </tex-math></inline-formula> higher energy efficiency than the state-of-the-art transformer accelerator and achieves <inline-formula> <tex-math notation=\"LaTeX\">$30.8\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$14.6\\times $ </tex-math></inline-formula> speedup compared to CPU and GPU.",
      "intriguing_abstract": "Transformers have made remarkable contributions to natural language processing (NLP) and many other fields. Recently, transformer-based models have achieved state-of-the-art (SOTA) performance on computer vision tasks compared with traditional convolutional neural networks (CNNs). Unfortunately, existing CNN accelerators cannot efficiently support transformer due to the high computational overhead and redundant data accesses associated with the KQV matrix operations in the transformer models. If the recently-developed NLP transformer accelerators are applied to the vision transformer (ViT) models, their efficiency would decrease due to three challenges. 1) Redundant data storage and access still exist in ViT data flow scheduling. 2) For matrix transposition in transformer models, the previous transpose-operation schemes lack flexibility, resulting in extra area overhead. 3) The sparse acceleration schemes for NLP in prior transformer accelerators cannot efficiently accelerate ViT with relatively fewer tokens. To overcome these challenges, we propose <inline-formula> <tex-math notation=\"LaTeX\">$P^{3}$ </tex-math></inline-formula> ViT, a computing-in-memory (CIM)-based architecture, to efficiently accelerate ViT, achieving high utilization on data flow scheduling. There are three key contributions: 1) P3ViT architecture supports three ping-pong pipeline scheduling modes, involving inter-core parallel and intra-core ping-pong pipeline mode (IEP-IAP3), inter-core pipeline and parallel mode (IEP2), and full parallel mode, to eliminate redundant memory accesses. 2) A two-way ping-pong CIM macro is proposed, which can be configured to regular calculation mode and transpose calculation mode to adapt to both <inline-formula> <tex-math notation=\"LaTeX\">$\\text{Q}\\times \\text{K}^{\\mathrm {T}}$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$\\text{A}\\times \\text{V}$ </tex-math></inline-formula> tasks. 3) P3ViT also runs a small prediction network. It prunes redundant tokens to be a standard number hierarchically and dynamically, enabling high-throughput and high-utilization attention computation. Measurements show that P3ViT achieves <inline-formula> <tex-math notation=\"LaTeX\">$1.13\\times $ </tex-math></inline-formula> higher energy efficiency than the state-of-the-art transformer accelerator and achieves <inline-formula> <tex-math notation=\"LaTeX\">$30.8\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$14.6\\times $ </tex-math></inline-formula> speedup compared to CPU and GPU.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f1b6f564e7cba8121df0eef0d656673e3bd18621.pdf",
      "citation_key": "fu20232q3",
      "metadata": {
        "title": "P3 ViT: A CIM-Based High-Utilization Architecture With Dynamic Pruning and Two-Way Ping-Pong Macro for Vision Transformer",
        "authors": [
          "Xiangqu Fu",
          "Qirui Ren",
          "Hao Wu",
          "Feibin Xiang",
          "Q. Luo",
          "Jinshan Yue",
          "Yong Chen",
          "Feng Zhang"
        ],
        "published_date": "2023",
        "abstract": "Transformers have made remarkable contributions to natural language processing (NLP) and many other fields. Recently, transformer-based models have achieved state-of-the-art (SOTA) performance on computer vision tasks compared with traditional convolutional neural networks (CNNs). Unfortunately, existing CNN accelerators cannot efficiently support transformer due to the high computational overhead and redundant data accesses associated with the KQV matrix operations in the transformer models. If the recently-developed NLP transformer accelerators are applied to the vision transformer (ViT) models, their efficiency would decrease due to three challenges. 1) Redundant data storage and access still exist in ViT data flow scheduling. 2) For matrix transposition in transformer models, the previous transpose-operation schemes lack flexibility, resulting in extra area overhead. 3) The sparse acceleration schemes for NLP in prior transformer accelerators cannot efficiently accelerate ViT with relatively fewer tokens. To overcome these challenges, we propose <inline-formula> <tex-math notation=\"LaTeX\">$P^{3}$ </tex-math></inline-formula> ViT, a computing-in-memory (CIM)-based architecture, to efficiently accelerate ViT, achieving high utilization on data flow scheduling. There are three key contributions: 1) P3ViT architecture supports three ping-pong pipeline scheduling modes, involving inter-core parallel and intra-core ping-pong pipeline mode (IEP-IAP3), inter-core pipeline and parallel mode (IEP2), and full parallel mode, to eliminate redundant memory accesses. 2) A two-way ping-pong CIM macro is proposed, which can be configured to regular calculation mode and transpose calculation mode to adapt to both <inline-formula> <tex-math notation=\"LaTeX\">$\\text{Q}\\times \\text{K}^{\\mathrm {T}}$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$\\text{A}\\times \\text{V}$ </tex-math></inline-formula> tasks. 3) P3ViT also runs a small prediction network. It prunes redundant tokens to be a standard number hierarchically and dynamically, enabling high-throughput and high-utilization attention computation. Measurements show that P3ViT achieves <inline-formula> <tex-math notation=\"LaTeX\">$1.13\\times $ </tex-math></inline-formula> higher energy efficiency than the state-of-the-art transformer accelerator and achieves <inline-formula> <tex-math notation=\"LaTeX\">$30.8\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$14.6\\times $ </tex-math></inline-formula> speedup compared to CPU and GPU.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f1b6f564e7cba8121df0eef0d656673e3bd18621.pdf",
        "venue": "IEEE Transactions on Circuits and Systems Part 1: Regular Papers",
        "citationCount": 13,
        "score": 6.5,
        "summary": "Transformers have made remarkable contributions to natural language processing (NLP) and many other fields. Recently, transformer-based models have achieved state-of-the-art (SOTA) performance on computer vision tasks compared with traditional convolutional neural networks (CNNs). Unfortunately, existing CNN accelerators cannot efficiently support transformer due to the high computational overhead and redundant data accesses associated with the KQV matrix operations in the transformer models. If the recently-developed NLP transformer accelerators are applied to the vision transformer (ViT) models, their efficiency would decrease due to three challenges. 1) Redundant data storage and access still exist in ViT data flow scheduling. 2) For matrix transposition in transformer models, the previous transpose-operation schemes lack flexibility, resulting in extra area overhead. 3) The sparse acceleration schemes for NLP in prior transformer accelerators cannot efficiently accelerate ViT with relatively fewer tokens. To overcome these challenges, we propose <inline-formula> <tex-math notation=\"LaTeX\">$P^{3}$ </tex-math></inline-formula> ViT, a computing-in-memory (CIM)-based architecture, to efficiently accelerate ViT, achieving high utilization on data flow scheduling. There are three key contributions: 1) P3ViT architecture supports three ping-pong pipeline scheduling modes, involving inter-core parallel and intra-core ping-pong pipeline mode (IEP-IAP3), inter-core pipeline and parallel mode (IEP2), and full parallel mode, to eliminate redundant memory accesses. 2) A two-way ping-pong CIM macro is proposed, which can be configured to regular calculation mode and transpose calculation mode to adapt to both <inline-formula> <tex-math notation=\"LaTeX\">$\\text{Q}\\times \\text{K}^{\\mathrm {T}}$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$\\text{A}\\times \\text{V}$ </tex-math></inline-formula> tasks. 3) P3ViT also runs a small prediction network. It prunes redundant tokens to be a standard number hierarchically and dynamically, enabling high-throughput and high-utilization attention computation. Measurements show that P3ViT achieves <inline-formula> <tex-math notation=\"LaTeX\">$1.13\\times $ </tex-math></inline-formula> higher energy efficiency than the state-of-the-art transformer accelerator and achieves <inline-formula> <tex-math notation=\"LaTeX\">$30.8\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$14.6\\times $ </tex-math></inline-formula> speedup compared to CPU and GPU.",
        "keywords": []
      },
      "file_name": "f1b6f564e7cba8121df0eef0d656673e3bd18621.pdf"
    },
    {
      "success": true,
      "doc_id": "36108d0f7881c76784c19ff0e41cfbeb",
      "summary": "Recently Transformer models is new direction in the computer vision field, which is based on self multihead attention mechanism. Compared with the convolutional neural network, this Transformer uses the self-attention mechanism to capture global contextual information and extract more strong features by learning the association relationship between different features, which has achieved good results in many vision tasks. In face-based age estimation, some facial patches that contain rich age-specific information are critical in the age estimation task. The present study proposed an attention-based convolution (ABC) age estimation framework, called improved Swin Transformer with ABC, in which two separate regions were implemented, namely ABC and Swin Transformer. ABC extracted facial patches containing rich age-specific information using a shallow convolutional network and a multiheaded attention mechanism. Subsequently, the features obtained by ABC were spliced with the flattened image in the Swin Transformer, which were then input to the Swin Transformer to predict the age of the image. The ABC framework spliced the important regions that contained rich age-specific information into the original image, which could fully mobilize the long-dependency of the Swin Transformer, that is, extracting stronger features by learning the dependency relationship between different features. ABC also introduced loss of diversity to guide the training of self-attention mechanism, reducing overlap between patches so that the diverse and important patches were discovered. Through extensive experiments, this study showed that the proposed framework outperformed several state-of-the-art methods on age estimation benchmark datasets.",
      "intriguing_abstract": "Recently Transformer models is new direction in the computer vision field, which is based on self multihead attention mechanism. Compared with the convolutional neural network, this Transformer uses the self-attention mechanism to capture global contextual information and extract more strong features by learning the association relationship between different features, which has achieved good results in many vision tasks. In face-based age estimation, some facial patches that contain rich age-specific information are critical in the age estimation task. The present study proposed an attention-based convolution (ABC) age estimation framework, called improved Swin Transformer with ABC, in which two separate regions were implemented, namely ABC and Swin Transformer. ABC extracted facial patches containing rich age-specific information using a shallow convolutional network and a multiheaded attention mechanism. Subsequently, the features obtained by ABC were spliced with the flattened image in the Swin Transformer, which were then input to the Swin Transformer to predict the age of the image. The ABC framework spliced the important regions that contained rich age-specific information into the original image, which could fully mobilize the long-dependency of the Swin Transformer, that is, extracting stronger features by learning the dependency relationship between different features. ABC also introduced loss of diversity to guide the training of self-attention mechanism, reducing overlap between patches so that the diverse and important patches were discovered. Through extensive experiments, this study showed that the proposed framework outperformed several state-of-the-art methods on age estimation benchmark datasets.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/12106888ec74baf48d1e9fb64e8932c9ce509e84.pdf",
      "citation_key": "shi20235zy",
      "metadata": {
        "title": "Face-based age estimation using improved Swin Transformer with attention-based convolution",
        "authors": [
          "Chaojun Shi",
          "Shiwei Zhao",
          "Kecheng Zhang",
          "Yibo Wang",
          "Longping Liang"
        ],
        "published_date": "2023",
        "abstract": "Recently Transformer models is new direction in the computer vision field, which is based on self multihead attention mechanism. Compared with the convolutional neural network, this Transformer uses the self-attention mechanism to capture global contextual information and extract more strong features by learning the association relationship between different features, which has achieved good results in many vision tasks. In face-based age estimation, some facial patches that contain rich age-specific information are critical in the age estimation task. The present study proposed an attention-based convolution (ABC) age estimation framework, called improved Swin Transformer with ABC, in which two separate regions were implemented, namely ABC and Swin Transformer. ABC extracted facial patches containing rich age-specific information using a shallow convolutional network and a multiheaded attention mechanism. Subsequently, the features obtained by ABC were spliced with the flattened image in the Swin Transformer, which were then input to the Swin Transformer to predict the age of the image. The ABC framework spliced the important regions that contained rich age-specific information into the original image, which could fully mobilize the long-dependency of the Swin Transformer, that is, extracting stronger features by learning the dependency relationship between different features. ABC also introduced loss of diversity to guide the training of self-attention mechanism, reducing overlap between patches so that the diverse and important patches were discovered. Through extensive experiments, this study showed that the proposed framework outperformed several state-of-the-art methods on age estimation benchmark datasets.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/12106888ec74baf48d1e9fb64e8932c9ce509e84.pdf",
        "venue": "Frontiers in Neuroscience",
        "citationCount": 13,
        "score": 6.5,
        "summary": "Recently Transformer models is new direction in the computer vision field, which is based on self multihead attention mechanism. Compared with the convolutional neural network, this Transformer uses the self-attention mechanism to capture global contextual information and extract more strong features by learning the association relationship between different features, which has achieved good results in many vision tasks. In face-based age estimation, some facial patches that contain rich age-specific information are critical in the age estimation task. The present study proposed an attention-based convolution (ABC) age estimation framework, called improved Swin Transformer with ABC, in which two separate regions were implemented, namely ABC and Swin Transformer. ABC extracted facial patches containing rich age-specific information using a shallow convolutional network and a multiheaded attention mechanism. Subsequently, the features obtained by ABC were spliced with the flattened image in the Swin Transformer, which were then input to the Swin Transformer to predict the age of the image. The ABC framework spliced the important regions that contained rich age-specific information into the original image, which could fully mobilize the long-dependency of the Swin Transformer, that is, extracting stronger features by learning the dependency relationship between different features. ABC also introduced loss of diversity to guide the training of self-attention mechanism, reducing overlap between patches so that the diverse and important patches were discovered. Through extensive experiments, this study showed that the proposed framework outperformed several state-of-the-art methods on age estimation benchmark datasets.",
        "keywords": []
      },
      "file_name": "12106888ec74baf48d1e9fb64e8932c9ce509e84.pdf"
    },
    {
      "success": true,
      "doc_id": "f5f8c413984fdeae47b7d37acfcda8e1",
      "summary": "Deepfakes have raised significant concerns due to their potential to spread false information and compromise the integrity of digital media. Current deepfake detection models often struggle to generalize across a diverse range of deepfake generation techniques and video content. In this work, we propose a Generative Convolutional Vision Transformer (GenConViT) for deepfake video detection. Our model combines ConvNeXt and Swin Transformer models for feature extraction, and it utilizes an Autoencoder and Variational Autoencoder to learn from latent data distributions. By learning from the visual artifacts and latent data distribution, GenConViT achieves an improved performance in detecting a wide range of deepfake videos. The model is trained and evaluated on DFDC, FF++, TM, DeepfakeTIMIT, and Celeb-DF (v2) datasets. The proposed GenConViT model demonstrates strong performance in deepfake video detection, achieving high accuracy across the tested datasets. While our model shows promising results in deepfake video detection by leveraging visual and latent features, we demonstrate that further work is needed to improve its generalizability when encountering out-of-distribution data. Our model provides an effective solution for identifying a wide range of fake videos while preserving the integrity of media.",
      "intriguing_abstract": "Deepfakes have raised significant concerns due to their potential to spread false information and compromise the integrity of digital media. Current deepfake detection models often struggle to generalize across a diverse range of deepfake generation techniques and video content. In this work, we propose a Generative Convolutional Vision Transformer (GenConViT) for deepfake video detection. Our model combines ConvNeXt and Swin Transformer models for feature extraction, and it utilizes an Autoencoder and Variational Autoencoder to learn from latent data distributions. By learning from the visual artifacts and latent data distribution, GenConViT achieves an improved performance in detecting a wide range of deepfake videos. The model is trained and evaluated on DFDC, FF++, TM, DeepfakeTIMIT, and Celeb-DF (v2) datasets. The proposed GenConViT model demonstrates strong performance in deepfake video detection, achieving high accuracy across the tested datasets. While our model shows promising results in deepfake video detection by leveraging visual and latent features, we demonstrate that further work is needed to improve its generalizability when encountering out-of-distribution data. Our model provides an effective solution for identifying a wide range of fake videos while preserving the integrity of media.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/3798e7f16fe69c29307a9bab4f0f4d779478afc5.pdf",
      "citation_key": "deressa2023lrl",
      "metadata": {
        "title": "GenConViT: Deepfake Video Detection Using Generative Convolutional Vision Transformer",
        "authors": [
          "Deressa Wodajo Deressa",
          "Hannes Mareen",
          "Peter Lambert",
          "Solomon Atnafu",
          "Z. Akhtar",
          "Glenn Van Wallendael"
        ],
        "published_date": "2023",
        "abstract": "Deepfakes have raised significant concerns due to their potential to spread false information and compromise the integrity of digital media. Current deepfake detection models often struggle to generalize across a diverse range of deepfake generation techniques and video content. In this work, we propose a Generative Convolutional Vision Transformer (GenConViT) for deepfake video detection. Our model combines ConvNeXt and Swin Transformer models for feature extraction, and it utilizes an Autoencoder and Variational Autoencoder to learn from latent data distributions. By learning from the visual artifacts and latent data distribution, GenConViT achieves an improved performance in detecting a wide range of deepfake videos. The model is trained and evaluated on DFDC, FF++, TM, DeepfakeTIMIT, and Celeb-DF (v2) datasets. The proposed GenConViT model demonstrates strong performance in deepfake video detection, achieving high accuracy across the tested datasets. While our model shows promising results in deepfake video detection by leveraging visual and latent features, we demonstrate that further work is needed to improve its generalizability when encountering out-of-distribution data. Our model provides an effective solution for identifying a wide range of fake videos while preserving the integrity of media.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3798e7f16fe69c29307a9bab4f0f4d779478afc5.pdf",
        "venue": "Applied Sciences",
        "citationCount": 13,
        "score": 6.5,
        "summary": "Deepfakes have raised significant concerns due to their potential to spread false information and compromise the integrity of digital media. Current deepfake detection models often struggle to generalize across a diverse range of deepfake generation techniques and video content. In this work, we propose a Generative Convolutional Vision Transformer (GenConViT) for deepfake video detection. Our model combines ConvNeXt and Swin Transformer models for feature extraction, and it utilizes an Autoencoder and Variational Autoencoder to learn from latent data distributions. By learning from the visual artifacts and latent data distribution, GenConViT achieves an improved performance in detecting a wide range of deepfake videos. The model is trained and evaluated on DFDC, FF++, TM, DeepfakeTIMIT, and Celeb-DF (v2) datasets. The proposed GenConViT model demonstrates strong performance in deepfake video detection, achieving high accuracy across the tested datasets. While our model shows promising results in deepfake video detection by leveraging visual and latent features, we demonstrate that further work is needed to improve its generalizability when encountering out-of-distribution data. Our model provides an effective solution for identifying a wide range of fake videos while preserving the integrity of media.",
        "keywords": []
      },
      "file_name": "3798e7f16fe69c29307a9bab4f0f4d779478afc5.pdf"
    },
    {
      "success": true,
      "doc_id": "7e49fa13b733a9223969282c41ef00ba",
      "summary": "Vision Transformers (ViTs) have emerged as a promising approach for visual recognition tasks, revolutionizing the field by leveraging the power of transformer-based architectures. Among the various ViT models, Swin Transformers have gained considerable attention due to their hierarchical design and ability to capture both local and global visual features effectively. This paper evaluates the performance of Swin ViT model using gradient accumulation optimization (GAO) technique. We investigate the impact of gradient accumulation optimization technique on the model's accuracy and training time. Our experiments show that applying the GAO technique leads to a significant decrease in the accuracy of the Swin ViT model, compared to the standard Swin Transformer model. Moreover, we detect a significant increase in the training time of the Swin ViT model when GAO model is applied. These findings suggest that applying the GAO technique may not be suitable for the Swin ViT model, and concern should be undertaken when using GAO technique for other transformer-based models.",
      "intriguing_abstract": "Vision Transformers (ViTs) have emerged as a promising approach for visual recognition tasks, revolutionizing the field by leveraging the power of transformer-based architectures. Among the various ViT models, Swin Transformers have gained considerable attention due to their hierarchical design and ability to capture both local and global visual features effectively. This paper evaluates the performance of Swin ViT model using gradient accumulation optimization (GAO) technique. We investigate the impact of gradient accumulation optimization technique on the model's accuracy and training time. Our experiments show that applying the GAO technique leads to a significant decrease in the accuracy of the Swin ViT model, compared to the standard Swin Transformer model. Moreover, we detect a significant increase in the training time of the Swin ViT model when GAO model is applied. These findings suggest that applying the GAO technique may not be suitable for the Swin ViT model, and concern should be undertaken when using GAO technique for other transformer-based models.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/29a0077d198418bab2ea4d78d04a892ede860d68.pdf",
      "citation_key": "aburass2023qpf",
      "metadata": {
        "title": "Performance Evaluation of Swin Vision Transformer Model using Gradient Accumulation Optimization Technique",
        "authors": [
          "Sanad Aburass",
          "O. Dorgham"
        ],
        "published_date": "2023",
        "abstract": "Vision Transformers (ViTs) have emerged as a promising approach for visual recognition tasks, revolutionizing the field by leveraging the power of transformer-based architectures. Among the various ViT models, Swin Transformers have gained considerable attention due to their hierarchical design and ability to capture both local and global visual features effectively. This paper evaluates the performance of Swin ViT model using gradient accumulation optimization (GAO) technique. We investigate the impact of gradient accumulation optimization technique on the model's accuracy and training time. Our experiments show that applying the GAO technique leads to a significant decrease in the accuracy of the Swin ViT model, compared to the standard Swin Transformer model. Moreover, we detect a significant increase in the training time of the Swin ViT model when GAO model is applied. These findings suggest that applying the GAO technique may not be suitable for the Swin ViT model, and concern should be undertaken when using GAO technique for other transformer-based models.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/29a0077d198418bab2ea4d78d04a892ede860d68.pdf",
        "venue": "arXiv.org",
        "citationCount": 12,
        "score": 6.0,
        "summary": "Vision Transformers (ViTs) have emerged as a promising approach for visual recognition tasks, revolutionizing the field by leveraging the power of transformer-based architectures. Among the various ViT models, Swin Transformers have gained considerable attention due to their hierarchical design and ability to capture both local and global visual features effectively. This paper evaluates the performance of Swin ViT model using gradient accumulation optimization (GAO) technique. We investigate the impact of gradient accumulation optimization technique on the model's accuracy and training time. Our experiments show that applying the GAO technique leads to a significant decrease in the accuracy of the Swin ViT model, compared to the standard Swin Transformer model. Moreover, we detect a significant increase in the training time of the Swin ViT model when GAO model is applied. These findings suggest that applying the GAO technique may not be suitable for the Swin ViT model, and concern should be undertaken when using GAO technique for other transformer-based models.",
        "keywords": []
      },
      "file_name": "29a0077d198418bab2ea4d78d04a892ede860d68.pdf"
    },
    {
      "success": true,
      "doc_id": "d48a5953e1a6787fcc4a60b11525959b",
      "summary": "Transformers have emerged as a groundbreaking architecture in the field of computer vision, offering a compelling alternative to traditional convolutional neural networks (CNNs) by enabling the modeling of long-range dependencies and global context through self-attention mechanisms. Originally developed for natural language processing, transformers have now been successfully adapted for a wide range of vision tasks, leading to significant improvements in performance and generalization. This survey provides a comprehensive overview of the fundamental principles of transformer architectures, highlighting the core mechanisms such as self-attention, multi-head attention, and positional encoding that distinguish them from CNNs. We delve into the theoretical adaptations required to apply transformers to visual data, including image tokenization and the integration of positional embeddings. A detailed analysis of key transformer-based vision architectures such as ViT, DeiT, Swin Transformer, PVT, Twins, and CrossViT are presented, alongside their practical applications in image classification, object detection, video understanding, medical imaging, and cross-modal tasks. The paper further compares the performance of vision transformers with CNNs, examining their respective strengths, limitations, and the emergence of hybrid models. Finally, current challenges in deploying ViTs, such as computational cost, data efficiency, and interpretability, and explore recent advancements and future research directions including efficient architectures, self-supervised learning, and multimodal integration are discussed.",
      "intriguing_abstract": "Transformers have emerged as a groundbreaking architecture in the field of computer vision, offering a compelling alternative to traditional convolutional neural networks (CNNs) by enabling the modeling of long-range dependencies and global context through self-attention mechanisms. Originally developed for natural language processing, transformers have now been successfully adapted for a wide range of vision tasks, leading to significant improvements in performance and generalization. This survey provides a comprehensive overview of the fundamental principles of transformer architectures, highlighting the core mechanisms such as self-attention, multi-head attention, and positional encoding that distinguish them from CNNs. We delve into the theoretical adaptations required to apply transformers to visual data, including image tokenization and the integration of positional embeddings. A detailed analysis of key transformer-based vision architectures such as ViT, DeiT, Swin Transformer, PVT, Twins, and CrossViT are presented, alongside their practical applications in image classification, object detection, video understanding, medical imaging, and cross-modal tasks. The paper further compares the performance of vision transformers with CNNs, examining their respective strengths, limitations, and the emergence of hybrid models. Finally, current challenges in deploying ViTs, such as computational cost, data efficiency, and interpretability, and explore recent advancements and future research directions including efficient architectures, self-supervised learning, and multimodal integration are discussed.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/ac9cc0c28838a037e77f4e19433de170f47b3de9.pdf",
      "citation_key": "hassija2025wq3",
      "metadata": {
        "title": "Transformers for Vision: A Survey on Innovative Methods for Computer Vision",
        "authors": [
          "Vikas Hassija",
          "Balamurugan Palanisamy",
          "Arpita Chatterjee",
          "Arpita Mandal",
          "Debanshi Chakraborty",
          "Amit Pandey",
          "G. Chalapathi",
          "Dhruv Kumar"
        ],
        "published_date": "2025",
        "abstract": "Transformers have emerged as a groundbreaking architecture in the field of computer vision, offering a compelling alternative to traditional convolutional neural networks (CNNs) by enabling the modeling of long-range dependencies and global context through self-attention mechanisms. Originally developed for natural language processing, transformers have now been successfully adapted for a wide range of vision tasks, leading to significant improvements in performance and generalization. This survey provides a comprehensive overview of the fundamental principles of transformer architectures, highlighting the core mechanisms such as self-attention, multi-head attention, and positional encoding that distinguish them from CNNs. We delve into the theoretical adaptations required to apply transformers to visual data, including image tokenization and the integration of positional embeddings. A detailed analysis of key transformer-based vision architectures such as ViT, DeiT, Swin Transformer, PVT, Twins, and CrossViT are presented, alongside their practical applications in image classification, object detection, video understanding, medical imaging, and cross-modal tasks. The paper further compares the performance of vision transformers with CNNs, examining their respective strengths, limitations, and the emergence of hybrid models. Finally, current challenges in deploying ViTs, such as computational cost, data efficiency, and interpretability, and explore recent advancements and future research directions including efficient architectures, self-supervised learning, and multimodal integration are discussed.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ac9cc0c28838a037e77f4e19433de170f47b3de9.pdf",
        "venue": "IEEE Access",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Transformers have emerged as a groundbreaking architecture in the field of computer vision, offering a compelling alternative to traditional convolutional neural networks (CNNs) by enabling the modeling of long-range dependencies and global context through self-attention mechanisms. Originally developed for natural language processing, transformers have now been successfully adapted for a wide range of vision tasks, leading to significant improvements in performance and generalization. This survey provides a comprehensive overview of the fundamental principles of transformer architectures, highlighting the core mechanisms such as self-attention, multi-head attention, and positional encoding that distinguish them from CNNs. We delve into the theoretical adaptations required to apply transformers to visual data, including image tokenization and the integration of positional embeddings. A detailed analysis of key transformer-based vision architectures such as ViT, DeiT, Swin Transformer, PVT, Twins, and CrossViT are presented, alongside their practical applications in image classification, object detection, video understanding, medical imaging, and cross-modal tasks. The paper further compares the performance of vision transformers with CNNs, examining their respective strengths, limitations, and the emergence of hybrid models. Finally, current challenges in deploying ViTs, such as computational cost, data efficiency, and interpretability, and explore recent advancements and future research directions including efficient architectures, self-supervised learning, and multimodal integration are discussed.",
        "keywords": []
      },
      "file_name": "ac9cc0c28838a037e77f4e19433de170f47b3de9.pdf"
    },
    {
      "success": true,
      "doc_id": "52e669a78c4d8c98c65ef963ccd536b4",
      "summary": "Structural magnetic resonance imaging (sMRI) is widely used in the clinical diagnosis of diseases due to its advantages: high-definition and noninvasive visualization. Therefore, computer-aided diagnosis based on sMRI images is broadly applied in classifying Alzheimers disease (AD). Due to the excellent performance of the Transformer in computer vision, the Vision Transformer (ViT) has been employed for AD classification in recent years. The ViT relies on access to large datasets, while the sample size of brain imaging datasets is relatively insufficient. Moreover, the preprocessing procedures of brain sMRI images are complex and labor-intensive. To overcome the limitations mentioned above, we propose the Resizer Swin Transformer (RST), a deep-learning model that can extract information from brain sMRI images that are only briefly processed to achieve multi-scale and cross-channel features. In addition, we pre-trained our RST on a natural image dataset and obtained better performance. We achieved 99.59% and 94.01% average accuracy on the ADNI and AIBL datasets, respectively. Importantly, the RST has a sensitivity of 99.59%, a specificity of 99.58%, and a precision of 99.83% on the ADNI dataset, which are better than or comparable to state-of-the-art approaches. The experimental results prove that RST can achieve better classification performance in AD prediction compared with CNN-based and Transformer models.",
      "intriguing_abstract": "Structural magnetic resonance imaging (sMRI) is widely used in the clinical diagnosis of diseases due to its advantages: high-definition and noninvasive visualization. Therefore, computer-aided diagnosis based on sMRI images is broadly applied in classifying Alzheimers disease (AD). Due to the excellent performance of the Transformer in computer vision, the Vision Transformer (ViT) has been employed for AD classification in recent years. The ViT relies on access to large datasets, while the sample size of brain imaging datasets is relatively insufficient. Moreover, the preprocessing procedures of brain sMRI images are complex and labor-intensive. To overcome the limitations mentioned above, we propose the Resizer Swin Transformer (RST), a deep-learning model that can extract information from brain sMRI images that are only briefly processed to achieve multi-scale and cross-channel features. In addition, we pre-trained our RST on a natural image dataset and obtained better performance. We achieved 99.59% and 94.01% average accuracy on the ADNI and AIBL datasets, respectively. Importantly, the RST has a sensitivity of 99.59%, a specificity of 99.58%, and a precision of 99.83% on the ADNI dataset, which are better than or comparable to state-of-the-art approaches. The experimental results prove that RST can achieve better classification performance in AD prediction compared with CNN-based and Transformer models.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/c4357abf10ff937e4ad62df4289fbbf74f114725.pdf",
      "citation_key": "huang20238er",
      "metadata": {
        "title": "Resizer Swin Transformer-Based Classification Using sMRI for Alzheimers Disease",
        "authors": [
          "Yihang Huang",
          "Wan Li"
        ],
        "published_date": "2023",
        "abstract": "Structural magnetic resonance imaging (sMRI) is widely used in the clinical diagnosis of diseases due to its advantages: high-definition and noninvasive visualization. Therefore, computer-aided diagnosis based on sMRI images is broadly applied in classifying Alzheimers disease (AD). Due to the excellent performance of the Transformer in computer vision, the Vision Transformer (ViT) has been employed for AD classification in recent years. The ViT relies on access to large datasets, while the sample size of brain imaging datasets is relatively insufficient. Moreover, the preprocessing procedures of brain sMRI images are complex and labor-intensive. To overcome the limitations mentioned above, we propose the Resizer Swin Transformer (RST), a deep-learning model that can extract information from brain sMRI images that are only briefly processed to achieve multi-scale and cross-channel features. In addition, we pre-trained our RST on a natural image dataset and obtained better performance. We achieved 99.59% and 94.01% average accuracy on the ADNI and AIBL datasets, respectively. Importantly, the RST has a sensitivity of 99.59%, a specificity of 99.58%, and a precision of 99.83% on the ADNI dataset, which are better than or comparable to state-of-the-art approaches. The experimental results prove that RST can achieve better classification performance in AD prediction compared with CNN-based and Transformer models.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c4357abf10ff937e4ad62df4289fbbf74f114725.pdf",
        "venue": "Applied Sciences",
        "citationCount": 11,
        "score": 5.5,
        "summary": "Structural magnetic resonance imaging (sMRI) is widely used in the clinical diagnosis of diseases due to its advantages: high-definition and noninvasive visualization. Therefore, computer-aided diagnosis based on sMRI images is broadly applied in classifying Alzheimers disease (AD). Due to the excellent performance of the Transformer in computer vision, the Vision Transformer (ViT) has been employed for AD classification in recent years. The ViT relies on access to large datasets, while the sample size of brain imaging datasets is relatively insufficient. Moreover, the preprocessing procedures of brain sMRI images are complex and labor-intensive. To overcome the limitations mentioned above, we propose the Resizer Swin Transformer (RST), a deep-learning model that can extract information from brain sMRI images that are only briefly processed to achieve multi-scale and cross-channel features. In addition, we pre-trained our RST on a natural image dataset and obtained better performance. We achieved 99.59% and 94.01% average accuracy on the ADNI and AIBL datasets, respectively. Importantly, the RST has a sensitivity of 99.59%, a specificity of 99.58%, and a precision of 99.83% on the ADNI dataset, which are better than or comparable to state-of-the-art approaches. The experimental results prove that RST can achieve better classification performance in AD prediction compared with CNN-based and Transformer models.",
        "keywords": []
      },
      "file_name": "c4357abf10ff937e4ad62df4289fbbf74f114725.pdf"
    },
    {
      "success": true,
      "doc_id": "a98b74e0657f187a8c9abfe560b4b327",
      "summary": "Legged robots, particularly quadrupeds, offer promising navigation capabilities, especially in scenarios requiring traversal over diverse terrains and obstacle avoidance. This paper addresses the challenge of enabling legged robots to navigate complex environments effectively through the integration of data-driven path-planning methods. We propose an approach that utilizes differentiable planners, allowing the learning of end-to-end global plans via a neural network for commanding quadruped robots. The approach leverages 2D maps and obstacle specifications as inputs to generate a global path. To enhance the functionality of the developed neural network-based path planner, we use Vision Transformers (ViT) for map preprocessing, to enable the effective handling of larger maps. Experimental evaluations on two real robotic quadrupeds (Boston Dynamics Spot and Unitree Gol) demonstrate the effectiveness and versatility of the proposed approach in generating reliable path plans.",
      "intriguing_abstract": "Legged robots, particularly quadrupeds, offer promising navigation capabilities, especially in scenarios requiring traversal over diverse terrains and obstacle avoidance. This paper addresses the challenge of enabling legged robots to navigate complex environments effectively through the integration of data-driven path-planning methods. We propose an approach that utilizes differentiable planners, allowing the learning of end-to-end global plans via a neural network for commanding quadruped robots. The approach leverages 2D maps and obstacle specifications as inputs to generate a global path. To enhance the functionality of the developed neural network-based path planner, we use Vision Transformers (ViT) for map preprocessing, to enable the effective handling of larger maps. Experimental evaluations on two real robotic quadrupeds (Boston Dynamics Spot and Unitree Gol) demonstrate the effectiveness and versatility of the proposed approach in generating reliable path plans.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/0b41c18d0397e14ddacee4143db74a05d774434d.pdf",
      "citation_key": "liu20230kl",
      "metadata": {
        "title": "ViT-A*: Legged Robot Path Planning using Vision Transformer A*",
        "authors": [
          "Jianwei Liu",
          "Shirui Lyu",
          "Denis Hadjivelichkov",
          "Valerio Modugno",
          "D. Kanoulas"
        ],
        "published_date": "2023",
        "abstract": "Legged robots, particularly quadrupeds, offer promising navigation capabilities, especially in scenarios requiring traversal over diverse terrains and obstacle avoidance. This paper addresses the challenge of enabling legged robots to navigate complex environments effectively through the integration of data-driven path-planning methods. We propose an approach that utilizes differentiable planners, allowing the learning of end-to-end global plans via a neural network for commanding quadruped robots. The approach leverages 2D maps and obstacle specifications as inputs to generate a global path. To enhance the functionality of the developed neural network-based path planner, we use Vision Transformers (ViT) for map preprocessing, to enable the effective handling of larger maps. Experimental evaluations on two real robotic quadrupeds (Boston Dynamics Spot and Unitree Gol) demonstrate the effectiveness and versatility of the proposed approach in generating reliable path plans.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0b41c18d0397e14ddacee4143db74a05d774434d.pdf",
        "venue": "IEEE-RAS International Conference on Humanoid Robots",
        "citationCount": 11,
        "score": 5.5,
        "summary": "Legged robots, particularly quadrupeds, offer promising navigation capabilities, especially in scenarios requiring traversal over diverse terrains and obstacle avoidance. This paper addresses the challenge of enabling legged robots to navigate complex environments effectively through the integration of data-driven path-planning methods. We propose an approach that utilizes differentiable planners, allowing the learning of end-to-end global plans via a neural network for commanding quadruped robots. The approach leverages 2D maps and obstacle specifications as inputs to generate a global path. To enhance the functionality of the developed neural network-based path planner, we use Vision Transformers (ViT) for map preprocessing, to enable the effective handling of larger maps. Experimental evaluations on two real robotic quadrupeds (Boston Dynamics Spot and Unitree Gol) demonstrate the effectiveness and versatility of the proposed approach in generating reliable path plans.",
        "keywords": []
      },
      "file_name": "0b41c18d0397e14ddacee4143db74a05d774434d.pdf"
    },
    {
      "success": true,
      "doc_id": "2eb5fc6d815da0fc27cc44b9649c811e",
      "summary": "Pose estimation plays a crucial role in recognizing and analyzing the postures, actions, and movements of humans and animals using computer vision and machine learning techniques. However, bird pose estimation encounters specific challenges, including bird diversity, posture variation, and the fine granularity of posture. To overcome these challenges, we propose VHR-BirdPose, a method that combines Vision Transformer (ViT) and Deep High-Resolution Network (HRNet) with an attention mechanism. VHR-BirdPose effectively extracts features using Vision Transformers self-attention mechanism, which captures global dependencies in the images and allows for better capturing of pose details and changes. The attention mechanism is employed to enhance the focus on bird keypoints, improving the accuracy of pose estimation. By combining HRNet with Vision Transformer, our model can extract multi-scale features while maintaining high-resolution details and incorporating richer semantic information through the attention mechanism. This integration of HRNet and Vision Transformer leverages the advantages of both models, resulting in accurate and robust bird pose estimation. We conducted extensive experiments on the Animal Kingdom dataset to evaluate the performance of VHR-BirdPose. The results demonstrate that our proposed method achieves state-of-the-art performance in bird pose estimation. VHR-BirdPose based on bird images is of great significance for the advancement of bird behaviors, ecological understanding, and the protection of bird populations.",
      "intriguing_abstract": "Pose estimation plays a crucial role in recognizing and analyzing the postures, actions, and movements of humans and animals using computer vision and machine learning techniques. However, bird pose estimation encounters specific challenges, including bird diversity, posture variation, and the fine granularity of posture. To overcome these challenges, we propose VHR-BirdPose, a method that combines Vision Transformer (ViT) and Deep High-Resolution Network (HRNet) with an attention mechanism. VHR-BirdPose effectively extracts features using Vision Transformers self-attention mechanism, which captures global dependencies in the images and allows for better capturing of pose details and changes. The attention mechanism is employed to enhance the focus on bird keypoints, improving the accuracy of pose estimation. By combining HRNet with Vision Transformer, our model can extract multi-scale features while maintaining high-resolution details and incorporating richer semantic information through the attention mechanism. This integration of HRNet and Vision Transformer leverages the advantages of both models, resulting in accurate and robust bird pose estimation. We conducted extensive experiments on the Animal Kingdom dataset to evaluate the performance of VHR-BirdPose. The results demonstrate that our proposed method achieves state-of-the-art performance in bird pose estimation. VHR-BirdPose based on bird images is of great significance for the advancement of bird behaviors, ecological understanding, and the protection of bird populations.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f9e1a8754e77a4b5f240d11e9e81e2563a319b89.pdf",
      "citation_key": "he20238sy",
      "metadata": {
        "title": "VHR-BirdPose: Vision Transformer-Based HRNet for Bird Pose Estimation with Attention Mechanism",
        "authors": [
          "Ru He",
          "Xiaomin Wang",
          "Huazhen Chen",
          "Chang Liu"
        ],
        "published_date": "2023",
        "abstract": "Pose estimation plays a crucial role in recognizing and analyzing the postures, actions, and movements of humans and animals using computer vision and machine learning techniques. However, bird pose estimation encounters specific challenges, including bird diversity, posture variation, and the fine granularity of posture. To overcome these challenges, we propose VHR-BirdPose, a method that combines Vision Transformer (ViT) and Deep High-Resolution Network (HRNet) with an attention mechanism. VHR-BirdPose effectively extracts features using Vision Transformers self-attention mechanism, which captures global dependencies in the images and allows for better capturing of pose details and changes. The attention mechanism is employed to enhance the focus on bird keypoints, improving the accuracy of pose estimation. By combining HRNet with Vision Transformer, our model can extract multi-scale features while maintaining high-resolution details and incorporating richer semantic information through the attention mechanism. This integration of HRNet and Vision Transformer leverages the advantages of both models, resulting in accurate and robust bird pose estimation. We conducted extensive experiments on the Animal Kingdom dataset to evaluate the performance of VHR-BirdPose. The results demonstrate that our proposed method achieves state-of-the-art performance in bird pose estimation. VHR-BirdPose based on bird images is of great significance for the advancement of bird behaviors, ecological understanding, and the protection of bird populations.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f9e1a8754e77a4b5f240d11e9e81e2563a319b89.pdf",
        "venue": "Electronics",
        "citationCount": 11,
        "score": 5.5,
        "summary": "Pose estimation plays a crucial role in recognizing and analyzing the postures, actions, and movements of humans and animals using computer vision and machine learning techniques. However, bird pose estimation encounters specific challenges, including bird diversity, posture variation, and the fine granularity of posture. To overcome these challenges, we propose VHR-BirdPose, a method that combines Vision Transformer (ViT) and Deep High-Resolution Network (HRNet) with an attention mechanism. VHR-BirdPose effectively extracts features using Vision Transformers self-attention mechanism, which captures global dependencies in the images and allows for better capturing of pose details and changes. The attention mechanism is employed to enhance the focus on bird keypoints, improving the accuracy of pose estimation. By combining HRNet with Vision Transformer, our model can extract multi-scale features while maintaining high-resolution details and incorporating richer semantic information through the attention mechanism. This integration of HRNet and Vision Transformer leverages the advantages of both models, resulting in accurate and robust bird pose estimation. We conducted extensive experiments on the Animal Kingdom dataset to evaluate the performance of VHR-BirdPose. The results demonstrate that our proposed method achieves state-of-the-art performance in bird pose estimation. VHR-BirdPose based on bird images is of great significance for the advancement of bird behaviors, ecological understanding, and the protection of bird populations.",
        "keywords": []
      },
      "file_name": "f9e1a8754e77a4b5f240d11e9e81e2563a319b89.pdf"
    },
    {
      "success": true,
      "doc_id": "310ed4a1d807f7be254a5ff19a6fe3e4",
      "summary": "Animal tracking and feeding monitoring is crucial for automatic individual cow welfare measurement and naturally becomes a prerequisite for autonomous livestock farming systems. The deformable body posture and irregular movement of cows under complex farming environments make tracking of individual animals in a herd very challenging. To tackle the above challenge, a deep learning network-based approach, namely, YOLOv5s-CA+DeepSORT-ViT, is proposed in this article. In our proposed approach, coordinate attention (CA)-integrated YOLOv5 was developed to capture spatial location information to improve the face detection performance for overlapping regions. Then the vision transformer (ViT) was embedded in the reidentification (reID) network Deep Simple Online and Real-time Tracking (DeepSORT) to enhance feature matching and tracking accuracy. The comparative results of the multicow complex dataset constructed from a commercial farm show that the ID F1 score (IDF1) and multitarget tracking accuracy (MOTA) of the proposed YOLOv5s-CA+DeepSORT-ViT are 88.5% and 84.4%, respectively. Meanwhile, the ID switching (ID Sw.) times and the processing time are reduced by 50% and 20% compared to the YOLOv5s+DeepSORT model. Experimental results also showed that the overall cow tracking performance of our proposed approach outperformed the other baselines (e.g. SORT, ByteTrack, BoT-SORT, and DeepSORT).",
      "intriguing_abstract": "Animal tracking and feeding monitoring is crucial for automatic individual cow welfare measurement and naturally becomes a prerequisite for autonomous livestock farming systems. The deformable body posture and irregular movement of cows under complex farming environments make tracking of individual animals in a herd very challenging. To tackle the above challenge, a deep learning network-based approach, namely, YOLOv5s-CA+DeepSORT-ViT, is proposed in this article. In our proposed approach, coordinate attention (CA)-integrated YOLOv5 was developed to capture spatial location information to improve the face detection performance for overlapping regions. Then the vision transformer (ViT) was embedded in the reidentification (reID) network Deep Simple Online and Real-time Tracking (DeepSORT) to enhance feature matching and tracking accuracy. The comparative results of the multicow complex dataset constructed from a commercial farm show that the ID F1 score (IDF1) and multitarget tracking accuracy (MOTA) of the proposed YOLOv5s-CA+DeepSORT-ViT are 88.5% and 84.4%, respectively. Meanwhile, the ID switching (ID Sw.) times and the processing time are reduced by 50% and 20% compared to the YOLOv5s+DeepSORT model. Experimental results also showed that the overall cow tracking performance of our proposed approach outperformed the other baselines (e.g. SORT, ByteTrack, BoT-SORT, and DeepSORT).",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/4b9568a9798c527779a1f7479ffb3ba373640e3e.pdf",
      "citation_key": "guo2023dpo",
      "metadata": {
        "title": "Vision-Based Cow Tracking and Feeding Monitoring for Autonomous Livestock Farming: The YOLOv5s-CA+DeepSORT-Vision Transformer",
        "authors": [
          "Yangyang Guo",
          "Wenhao Hong",
          "Jiaxin Wu",
          "Xiaoping Huang",
          "Yongliang Qiao",
          "He Kong"
        ],
        "published_date": "2023",
        "abstract": "Animal tracking and feeding monitoring is crucial for automatic individual cow welfare measurement and naturally becomes a prerequisite for autonomous livestock farming systems. The deformable body posture and irregular movement of cows under complex farming environments make tracking of individual animals in a herd very challenging. To tackle the above challenge, a deep learning network-based approach, namely, YOLOv5s-CA+DeepSORT-ViT, is proposed in this article. In our proposed approach, coordinate attention (CA)-integrated YOLOv5 was developed to capture spatial location information to improve the face detection performance for overlapping regions. Then the vision transformer (ViT) was embedded in the reidentification (reID) network Deep Simple Online and Real-time Tracking (DeepSORT) to enhance feature matching and tracking accuracy. The comparative results of the multicow complex dataset constructed from a commercial farm show that the ID F1 score (IDF1) and multitarget tracking accuracy (MOTA) of the proposed YOLOv5s-CA+DeepSORT-ViT are 88.5% and 84.4%, respectively. Meanwhile, the ID switching (ID Sw.) times and the processing time are reduced by 50% and 20% compared to the YOLOv5s+DeepSORT model. Experimental results also showed that the overall cow tracking performance of our proposed approach outperformed the other baselines (e.g. SORT, ByteTrack, BoT-SORT, and DeepSORT).",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/4b9568a9798c527779a1f7479ffb3ba373640e3e.pdf",
        "venue": "IEEE robotics & automation magazine",
        "citationCount": 11,
        "score": 5.5,
        "summary": "Animal tracking and feeding monitoring is crucial for automatic individual cow welfare measurement and naturally becomes a prerequisite for autonomous livestock farming systems. The deformable body posture and irregular movement of cows under complex farming environments make tracking of individual animals in a herd very challenging. To tackle the above challenge, a deep learning network-based approach, namely, YOLOv5s-CA+DeepSORT-ViT, is proposed in this article. In our proposed approach, coordinate attention (CA)-integrated YOLOv5 was developed to capture spatial location information to improve the face detection performance for overlapping regions. Then the vision transformer (ViT) was embedded in the reidentification (reID) network Deep Simple Online and Real-time Tracking (DeepSORT) to enhance feature matching and tracking accuracy. The comparative results of the multicow complex dataset constructed from a commercial farm show that the ID F1 score (IDF1) and multitarget tracking accuracy (MOTA) of the proposed YOLOv5s-CA+DeepSORT-ViT are 88.5% and 84.4%, respectively. Meanwhile, the ID switching (ID Sw.) times and the processing time are reduced by 50% and 20% compared to the YOLOv5s+DeepSORT model. Experimental results also showed that the overall cow tracking performance of our proposed approach outperformed the other baselines (e.g. SORT, ByteTrack, BoT-SORT, and DeepSORT).",
        "keywords": []
      },
      "file_name": "4b9568a9798c527779a1f7479ffb3ba373640e3e.pdf"
    },
    {
      "success": true,
      "doc_id": "10dca6a1c09fbcf974551f321a56f0b9",
      "summary": "Blind hyperspectral unmixing (HU) involves identifying pixel spectra as distinct materials (endmembers) and simultaneously determining their proportions (abundances) at each pixel. In this paper, we present Swin-HU, a novel method based on the Swin Transformer, designed to efficiently tackle blind HU. This method addresses the limitations of existing techniques, such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViT), in capturing global spatial information and spectral sequence attributes. Swin-HU employs Window Multi-head Self-Attention (W-MSA) and Shifted Window Multi-head Self-Attention (SW-MSA) mechanisms to extract global spatial priors while maintaining linear computational complexity. We evaluate Swin-HU against six other unmixing methods on both synthetic and real datasets, demonstrating its superior performance in endmember extraction and abundance estimation. The source code is available at https://github.com/wangyunjeff/Swin-HU.",
      "intriguing_abstract": "Blind hyperspectral unmixing (HU) involves identifying pixel spectra as distinct materials (endmembers) and simultaneously determining their proportions (abundances) at each pixel. In this paper, we present Swin-HU, a novel method based on the Swin Transformer, designed to efficiently tackle blind HU. This method addresses the limitations of existing techniques, such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViT), in capturing global spatial information and spectral sequence attributes. Swin-HU employs Window Multi-head Self-Attention (W-MSA) and Shifted Window Multi-head Self-Attention (SW-MSA) mechanisms to extract global spatial priors while maintaining linear computational complexity. We evaluate Swin-HU against six other unmixing methods on both synthetic and real datasets, demonstrating its superior performance in endmember extraction and abundance estimation. The source code is available at https://github.com/wangyunjeff/Swin-HU.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/34ca004eda791c8c4191e0ebe65cc4405a116b08.pdf",
      "citation_key": "wang2023j6b",
      "metadata": {
        "title": "Efficient Blind Hyperspectral Unmixing with Non-Local Spatial Information Based on Swin Transformer",
        "authors": [
          "Yun Wang",
          "Shuai Shi",
          "Jie Chen"
        ],
        "published_date": "2023",
        "abstract": "Blind hyperspectral unmixing (HU) involves identifying pixel spectra as distinct materials (endmembers) and simultaneously determining their proportions (abundances) at each pixel. In this paper, we present Swin-HU, a novel method based on the Swin Transformer, designed to efficiently tackle blind HU. This method addresses the limitations of existing techniques, such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViT), in capturing global spatial information and spectral sequence attributes. Swin-HU employs Window Multi-head Self-Attention (W-MSA) and Shifted Window Multi-head Self-Attention (SW-MSA) mechanisms to extract global spatial priors while maintaining linear computational complexity. We evaluate Swin-HU against six other unmixing methods on both synthetic and real datasets, demonstrating its superior performance in endmember extraction and abundance estimation. The source code is available at https://github.com/wangyunjeff/Swin-HU.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/34ca004eda791c8c4191e0ebe65cc4405a116b08.pdf",
        "venue": "IEEE International Geoscience and Remote Sensing Symposium",
        "citationCount": 9,
        "score": 4.5,
        "summary": "Blind hyperspectral unmixing (HU) involves identifying pixel spectra as distinct materials (endmembers) and simultaneously determining their proportions (abundances) at each pixel. In this paper, we present Swin-HU, a novel method based on the Swin Transformer, designed to efficiently tackle blind HU. This method addresses the limitations of existing techniques, such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViT), in capturing global spatial information and spectral sequence attributes. Swin-HU employs Window Multi-head Self-Attention (W-MSA) and Shifted Window Multi-head Self-Attention (SW-MSA) mechanisms to extract global spatial priors while maintaining linear computational complexity. We evaluate Swin-HU against six other unmixing methods on both synthetic and real datasets, demonstrating its superior performance in endmember extraction and abundance estimation. The source code is available at https://github.com/wangyunjeff/Swin-HU.",
        "keywords": []
      },
      "file_name": "34ca004eda791c8c4191e0ebe65cc4405a116b08.pdf"
    },
    {
      "success": true,
      "doc_id": "90c7dbfa585fc97c3e30d4613e741e69",
      "summary": "The introduction of robust backbones, such as Vision Transformers, has improved the performance of object tracking algorithms in recent years. However, these state-of-the-art trackers are computationally expensive since they have a large number of model parameters and rely on specialized hardware (e.g., GPU) for faster inference. On the other hand, recent lightweight trackers are fast but are less accurate, especially on large-scale datasets. We propose a lightweight, accurate, and fast tracking algorithm using Mobile Vision Transformers (MobileViT) as the backbone for the first time. We also present a novel approach of fusing the template and search region representations in the MobileViT backbone, thereby generating superior feature encoding for target localization. The experimental results show that our MobileViT-based Tracker, MVT, surpasses the performance of recent lightweight trackers on the large-scale datasets GOT10k and TrackingNet, and with a high inference speed. In addition, our method outperforms the popular DiMP-50 tracker despite having 4.7 times fewer model parameters and running at 2.8 times its speed on a GPU. The tracker code and models are available at https://github.com/goutamyg/MVT",
      "intriguing_abstract": "The introduction of robust backbones, such as Vision Transformers, has improved the performance of object tracking algorithms in recent years. However, these state-of-the-art trackers are computationally expensive since they have a large number of model parameters and rely on specialized hardware (e.g., GPU) for faster inference. On the other hand, recent lightweight trackers are fast but are less accurate, especially on large-scale datasets. We propose a lightweight, accurate, and fast tracking algorithm using Mobile Vision Transformers (MobileViT) as the backbone for the first time. We also present a novel approach of fusing the template and search region representations in the MobileViT backbone, thereby generating superior feature encoding for target localization. The experimental results show that our MobileViT-based Tracker, MVT, surpasses the performance of recent lightweight trackers on the large-scale datasets GOT10k and TrackingNet, and with a high inference speed. In addition, our method outperforms the popular DiMP-50 tracker despite having 4.7 times fewer model parameters and running at 2.8 times its speed on a GPU. The tracker code and models are available at https://github.com/goutamyg/MVT",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/409b43b8cd8a2ba69f93e80c2bacc0126238b550.pdf",
      "citation_key": "gopal20237ol",
      "metadata": {
        "title": "Mobile Vision Transformer-based Visual Object Tracking",
        "authors": [
          "Goutam Yelluru Gopal",
          "Maria A. Amer"
        ],
        "published_date": "2023",
        "abstract": "The introduction of robust backbones, such as Vision Transformers, has improved the performance of object tracking algorithms in recent years. However, these state-of-the-art trackers are computationally expensive since they have a large number of model parameters and rely on specialized hardware (e.g., GPU) for faster inference. On the other hand, recent lightweight trackers are fast but are less accurate, especially on large-scale datasets. We propose a lightweight, accurate, and fast tracking algorithm using Mobile Vision Transformers (MobileViT) as the backbone for the first time. We also present a novel approach of fusing the template and search region representations in the MobileViT backbone, thereby generating superior feature encoding for target localization. The experimental results show that our MobileViT-based Tracker, MVT, surpasses the performance of recent lightweight trackers on the large-scale datasets GOT10k and TrackingNet, and with a high inference speed. In addition, our method outperforms the popular DiMP-50 tracker despite having 4.7 times fewer model parameters and running at 2.8 times its speed on a GPU. The tracker code and models are available at https://github.com/goutamyg/MVT",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/409b43b8cd8a2ba69f93e80c2bacc0126238b550.pdf",
        "venue": "British Machine Vision Conference",
        "citationCount": 8,
        "score": 4.0,
        "summary": "The introduction of robust backbones, such as Vision Transformers, has improved the performance of object tracking algorithms in recent years. However, these state-of-the-art trackers are computationally expensive since they have a large number of model parameters and rely on specialized hardware (e.g., GPU) for faster inference. On the other hand, recent lightweight trackers are fast but are less accurate, especially on large-scale datasets. We propose a lightweight, accurate, and fast tracking algorithm using Mobile Vision Transformers (MobileViT) as the backbone for the first time. We also present a novel approach of fusing the template and search region representations in the MobileViT backbone, thereby generating superior feature encoding for target localization. The experimental results show that our MobileViT-based Tracker, MVT, surpasses the performance of recent lightweight trackers on the large-scale datasets GOT10k and TrackingNet, and with a high inference speed. In addition, our method outperforms the popular DiMP-50 tracker despite having 4.7 times fewer model parameters and running at 2.8 times its speed on a GPU. The tracker code and models are available at https://github.com/goutamyg/MVT",
        "keywords": []
      },
      "file_name": "409b43b8cd8a2ba69f93e80c2bacc0126238b550.pdf"
    },
    {
      "success": true,
      "doc_id": "3e2097e63d66484619ff4aa96284dcbc",
      "summary": "Since introduced, Swin Transformer has achieved remarkable results in the field of computer vision, it has sparked the need for dedicated hardware accelerators, specifically catering to edge computing demands. For the advantages of flexibility, low power consumption, FPGAs have been widely employed to accelerate the inference of convolutional neural networks (CNNs) and show potential in Transformer-based models. Unlike CNNs, which mainly involve multiply and accumulate (MAC) operations, Transformer involve non-linear computations such as Layer Normalization (LN), Softmax, and GELU. These nonlinear computations do pose challenges for accelerator design. In this paper, to propose an efficient FPGA-based hardware accelerator for Swin Transformer, we focused on using different strategies to deal with these nonlinear calculations and efficiently handling MAC computations to achieve the best acceleration results. We replaced LN with BN, Given that Batch Normalization (BN) can be fused with linear layers during inference to optimize inference efficiency. The modified Swin-T, Swin-S, and Swin-B respectively achieved Top-1 accuracy rates of 80.7%, 82.7%, and 82.8% in ImageNet. Furthermore, We employed strategies for approximate computation to design hardware-friendly architectures for Softmax and GELU computations. We also designed an efficient Matrix Multiplication Unit to handle all linear computations in Swin Transformer. As a conclude, compared with CPU (AMD Ryzen 5700X), our accelerator achieved 1.76x, 1.66x, and 1.25x speedup and achieved 20.45x, 18.60x, and 14.63x energy efficiency (FPS/power consumption) improvement on Swin-T, Swin-S, and Swin-B models, respectively. Compared to GPU (Nvidia RTX 2080 Ti), we achieved 5.05x, 4.42x, and 3.00x energy efficiency improvement respectively. As far as we know, the accelerator we proposed is the fastest FPGA-based accelerator for Swin Transformer.",
      "intriguing_abstract": "Since introduced, Swin Transformer has achieved remarkable results in the field of computer vision, it has sparked the need for dedicated hardware accelerators, specifically catering to edge computing demands. For the advantages of flexibility, low power consumption, FPGAs have been widely employed to accelerate the inference of convolutional neural networks (CNNs) and show potential in Transformer-based models. Unlike CNNs, which mainly involve multiply and accumulate (MAC) operations, Transformer involve non-linear computations such as Layer Normalization (LN), Softmax, and GELU. These nonlinear computations do pose challenges for accelerator design. In this paper, to propose an efficient FPGA-based hardware accelerator for Swin Transformer, we focused on using different strategies to deal with these nonlinear calculations and efficiently handling MAC computations to achieve the best acceleration results. We replaced LN with BN, Given that Batch Normalization (BN) can be fused with linear layers during inference to optimize inference efficiency. The modified Swin-T, Swin-S, and Swin-B respectively achieved Top-1 accuracy rates of 80.7%, 82.7%, and 82.8% in ImageNet. Furthermore, We employed strategies for approximate computation to design hardware-friendly architectures for Softmax and GELU computations. We also designed an efficient Matrix Multiplication Unit to handle all linear computations in Swin Transformer. As a conclude, compared with CPU (AMD Ryzen 5700X), our accelerator achieved 1.76x, 1.66x, and 1.25x speedup and achieved 20.45x, 18.60x, and 14.63x energy efficiency (FPS/power consumption) improvement on Swin-T, Swin-S, and Swin-B models, respectively. Compared to GPU (Nvidia RTX 2080 Ti), we achieved 5.05x, 4.42x, and 3.00x energy efficiency improvement respectively. As far as we know, the accelerator we proposed is the fastest FPGA-based accelerator for Swin Transformer.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/dc5d707a8f9efaf32d2319ec2ca8d2619ef2926e.pdf",
      "citation_key": "liu2023awp",
      "metadata": {
        "title": "An Efficient FPGA-Based Accelerator for Swin Transformer",
        "authors": [
          "Zhiyang Liu",
          "Pengyu Yin",
          "Zhenhua Ren"
        ],
        "published_date": "2023",
        "abstract": "Since introduced, Swin Transformer has achieved remarkable results in the field of computer vision, it has sparked the need for dedicated hardware accelerators, specifically catering to edge computing demands. For the advantages of flexibility, low power consumption, FPGAs have been widely employed to accelerate the inference of convolutional neural networks (CNNs) and show potential in Transformer-based models. Unlike CNNs, which mainly involve multiply and accumulate (MAC) operations, Transformer involve non-linear computations such as Layer Normalization (LN), Softmax, and GELU. These nonlinear computations do pose challenges for accelerator design. In this paper, to propose an efficient FPGA-based hardware accelerator for Swin Transformer, we focused on using different strategies to deal with these nonlinear calculations and efficiently handling MAC computations to achieve the best acceleration results. We replaced LN with BN, Given that Batch Normalization (BN) can be fused with linear layers during inference to optimize inference efficiency. The modified Swin-T, Swin-S, and Swin-B respectively achieved Top-1 accuracy rates of 80.7%, 82.7%, and 82.8% in ImageNet. Furthermore, We employed strategies for approximate computation to design hardware-friendly architectures for Softmax and GELU computations. We also designed an efficient Matrix Multiplication Unit to handle all linear computations in Swin Transformer. As a conclude, compared with CPU (AMD Ryzen 5700X), our accelerator achieved 1.76x, 1.66x, and 1.25x speedup and achieved 20.45x, 18.60x, and 14.63x energy efficiency (FPS/power consumption) improvement on Swin-T, Swin-S, and Swin-B models, respectively. Compared to GPU (Nvidia RTX 2080 Ti), we achieved 5.05x, 4.42x, and 3.00x energy efficiency improvement respectively. As far as we know, the accelerator we proposed is the fastest FPGA-based accelerator for Swin Transformer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/dc5d707a8f9efaf32d2319ec2ca8d2619ef2926e.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 3.0,
        "summary": "Since introduced, Swin Transformer has achieved remarkable results in the field of computer vision, it has sparked the need for dedicated hardware accelerators, specifically catering to edge computing demands. For the advantages of flexibility, low power consumption, FPGAs have been widely employed to accelerate the inference of convolutional neural networks (CNNs) and show potential in Transformer-based models. Unlike CNNs, which mainly involve multiply and accumulate (MAC) operations, Transformer involve non-linear computations such as Layer Normalization (LN), Softmax, and GELU. These nonlinear computations do pose challenges for accelerator design. In this paper, to propose an efficient FPGA-based hardware accelerator for Swin Transformer, we focused on using different strategies to deal with these nonlinear calculations and efficiently handling MAC computations to achieve the best acceleration results. We replaced LN with BN, Given that Batch Normalization (BN) can be fused with linear layers during inference to optimize inference efficiency. The modified Swin-T, Swin-S, and Swin-B respectively achieved Top-1 accuracy rates of 80.7%, 82.7%, and 82.8% in ImageNet. Furthermore, We employed strategies for approximate computation to design hardware-friendly architectures for Softmax and GELU computations. We also designed an efficient Matrix Multiplication Unit to handle all linear computations in Swin Transformer. As a conclude, compared with CPU (AMD Ryzen 5700X), our accelerator achieved 1.76x, 1.66x, and 1.25x speedup and achieved 20.45x, 18.60x, and 14.63x energy efficiency (FPS/power consumption) improvement on Swin-T, Swin-S, and Swin-B models, respectively. Compared to GPU (Nvidia RTX 2080 Ti), we achieved 5.05x, 4.42x, and 3.00x energy efficiency improvement respectively. As far as we know, the accelerator we proposed is the fastest FPGA-based accelerator for Swin Transformer.",
        "keywords": []
      },
      "file_name": "dc5d707a8f9efaf32d2319ec2ca8d2619ef2926e.pdf"
    },
    {
      "success": true,
      "doc_id": "b2741ee51d469edfb4e6fdd7cf6910ba",
      "summary": "Transformer, an attention-based encoder-decoder architecture, has not only revolutionized the field of natural language processing (NLP), but has also done some pioneering work in the field of computer vision (CV). Compared to convolutional neural networks (CNNs), the Vision Transformer (ViT) relies on excellent modeling capabilities to achieve very good performance on several benchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the self-attention mechanism in natural language processing, where word embeddings are replaced with patch embeddings. This paper reviews the derivatives of ViT and the cross-applications of ViT with other fields.",
      "intriguing_abstract": "Transformer, an attention-based encoder-decoder architecture, has not only revolutionized the field of natural language processing (NLP), but has also done some pioneering work in the field of computer vision (CV). Compared to convolutional neural networks (CNNs), the Vision Transformer (ViT) relies on excellent modeling capabilities to achieve very good performance on several benchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the self-attention mechanism in natural language processing, where word embeddings are replaced with patch embeddings. This paper reviews the derivatives of ViT and the cross-applications of ViT with other fields.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/9017053fb240d0870779b9658082488b392e7cde.pdf",
      "citation_key": "fu20228zq",
      "metadata": {
        "title": "Vision Transformer: Vit and its Derivatives",
        "authors": [
          "Zujun Fu"
        ],
        "published_date": "2022",
        "abstract": "Transformer, an attention-based encoder-decoder architecture, has not only revolutionized the field of natural language processing (NLP), but has also done some pioneering work in the field of computer vision (CV). Compared to convolutional neural networks (CNNs), the Vision Transformer (ViT) relies on excellent modeling capabilities to achieve very good performance on several benchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the self-attention mechanism in natural language processing, where word embeddings are replaced with patch embeddings. This paper reviews the derivatives of ViT and the cross-applications of ViT with other fields.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9017053fb240d0870779b9658082488b392e7cde.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 2.6666666666666665,
        "summary": "Transformer, an attention-based encoder-decoder architecture, has not only revolutionized the field of natural language processing (NLP), but has also done some pioneering work in the field of computer vision (CV). Compared to convolutional neural networks (CNNs), the Vision Transformer (ViT) relies on excellent modeling capabilities to achieve very good performance on several benchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the self-attention mechanism in natural language processing, where word embeddings are replaced with patch embeddings. This paper reviews the derivatives of ViT and the cross-applications of ViT with other fields.",
        "keywords": []
      },
      "file_name": "9017053fb240d0870779b9658082488b392e7cde.pdf"
    },
    {
      "success": true,
      "doc_id": "4ef471fdcfd8c9d1ddb449c8f243b28f",
      "summary": "The fast proliferation of the coronavirus around the globe has put several countries' healthcare systems in danger of collapsing. As a result, locating and separating COVID-19-positive patients is a critical task. Deep Learning approaches were used in several computer-aided automated systems that utilized chest computed tomography (CT-scan) or X-ray images to create diagnostic tools. However, current Convolutional Neural Network (CNN) based approaches cannot capture the global context because of inherent image-specific inductive bias. These techniques also require large and labeled datasets to train the algorithm, but not many labeled COVID-19 datasets exist publicly. To mitigate the problem, we have developed a self-attention-based Vision Transformer (ViT) architecture using CT-scan. The proposed ViT model achieves an accuracy of 98.39% on the popular SARS-CoV-2 datasets, outperforming the existing state-of-the-art CNN-based models by 1%. We also provide the characteristics of CT scan images of the COVID-19-affected patients and an error analysis of the model's outcome. Our findings show that the proposed ViT-based model can be an alternative option for medical professionals for effective COVID-19 screening. The implementation details of the proposed model can be accessed at https://github.com/Pranabiitp/ViT.",
      "intriguing_abstract": "The fast proliferation of the coronavirus around the globe has put several countries' healthcare systems in danger of collapsing. As a result, locating and separating COVID-19-positive patients is a critical task. Deep Learning approaches were used in several computer-aided automated systems that utilized chest computed tomography (CT-scan) or X-ray images to create diagnostic tools. However, current Convolutional Neural Network (CNN) based approaches cannot capture the global context because of inherent image-specific inductive bias. These techniques also require large and labeled datasets to train the algorithm, but not many labeled COVID-19 datasets exist publicly. To mitigate the problem, we have developed a self-attention-based Vision Transformer (ViT) architecture using CT-scan. The proposed ViT model achieves an accuracy of 98.39% on the popular SARS-CoV-2 datasets, outperforming the existing state-of-the-art CNN-based models by 1%. We also provide the characteristics of CT scan images of the COVID-19-affected patients and an error analysis of the model's outcome. Our findings show that the proposed ViT-based model can be an alternative option for medical professionals for effective COVID-19 screening. The implementation details of the proposed model can be accessed at https://github.com/Pranabiitp/ViT.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f27221a15f4c3c8af7ea0ada5bab25831ffe21ea.pdf",
      "citation_key": "sahoo20223yl",
      "metadata": {
        "title": "Vision Transformer Based COVID-19 Detection Using Chest CT-scan images",
        "authors": [
          "P. Sahoo",
          "S. Saha",
          "S. Mondal",
          "Suraj Gowda"
        ],
        "published_date": "2022",
        "abstract": "The fast proliferation of the coronavirus around the globe has put several countries' healthcare systems in danger of collapsing. As a result, locating and separating COVID-19-positive patients is a critical task. Deep Learning approaches were used in several computer-aided automated systems that utilized chest computed tomography (CT-scan) or X-ray images to create diagnostic tools. However, current Convolutional Neural Network (CNN) based approaches cannot capture the global context because of inherent image-specific inductive bias. These techniques also require large and labeled datasets to train the algorithm, but not many labeled COVID-19 datasets exist publicly. To mitigate the problem, we have developed a self-attention-based Vision Transformer (ViT) architecture using CT-scan. The proposed ViT model achieves an accuracy of 98.39% on the popular SARS-CoV-2 datasets, outperforming the existing state-of-the-art CNN-based models by 1%. We also provide the characteristics of CT scan images of the COVID-19-affected patients and an error analysis of the model's outcome. Our findings show that the proposed ViT-based model can be an alternative option for medical professionals for effective COVID-19 screening. The implementation details of the proposed model can be accessed at https://github.com/Pranabiitp/ViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f27221a15f4c3c8af7ea0ada5bab25831ffe21ea.pdf",
        "venue": "2022 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)",
        "citationCount": 8,
        "score": 2.6666666666666665,
        "summary": "The fast proliferation of the coronavirus around the globe has put several countries' healthcare systems in danger of collapsing. As a result, locating and separating COVID-19-positive patients is a critical task. Deep Learning approaches were used in several computer-aided automated systems that utilized chest computed tomography (CT-scan) or X-ray images to create diagnostic tools. However, current Convolutional Neural Network (CNN) based approaches cannot capture the global context because of inherent image-specific inductive bias. These techniques also require large and labeled datasets to train the algorithm, but not many labeled COVID-19 datasets exist publicly. To mitigate the problem, we have developed a self-attention-based Vision Transformer (ViT) architecture using CT-scan. The proposed ViT model achieves an accuracy of 98.39% on the popular SARS-CoV-2 datasets, outperforming the existing state-of-the-art CNN-based models by 1%. We also provide the characteristics of CT scan images of the COVID-19-affected patients and an error analysis of the model's outcome. Our findings show that the proposed ViT-based model can be an alternative option for medical professionals for effective COVID-19 screening. The implementation details of the proposed model can be accessed at https://github.com/Pranabiitp/ViT.",
        "keywords": []
      },
      "file_name": "f27221a15f4c3c8af7ea0ada5bab25831ffe21ea.pdf"
    },
    {
      "success": true,
      "doc_id": "94edf8ff75bdc45b46b8e72f6e6720f4",
      "summary": "Vision-Language (VL) models have gained significant research focus, enabling remarkable advances in multimodal reasoning. These architectures typically comprise a vision encoder, a Large Language Model (LLM), and a projection module that aligns visual features with the LLM's representation space. Despite their success, a critical limitation persists: the vision encoding process remains decoupled from user queries, often in the form of image-related questions. Consequently, the resulting visual features may not be optimally attuned to the query-specific elements of the image. To address this, we introduce QA-ViT, a Question Aware Vision Transformer approach for multimodal reasoning, which embeds question awareness directly within the vision encoder. This integration results in dynamic visual features focusing on relevant image aspects to the posed question. QA-ViT is model-agnostic and can be incorporated efficiently into any VL architecture. Extensive experiments demonstrate the effectiveness of applying our method to various multimodal architectures, leading to consistent improvement across diverse tasks and showcasing its potential for enhancing visual and scene-text understanding.",
      "intriguing_abstract": "Vision-Language (VL) models have gained significant research focus, enabling remarkable advances in multimodal reasoning. These architectures typically comprise a vision encoder, a Large Language Model (LLM), and a projection module that aligns visual features with the LLM's representation space. Despite their success, a critical limitation persists: the vision encoding process remains decoupled from user queries, often in the form of image-related questions. Consequently, the resulting visual features may not be optimally attuned to the query-specific elements of the image. To address this, we introduce QA-ViT, a Question Aware Vision Transformer approach for multimodal reasoning, which embeds question awareness directly within the vision encoder. This integration results in dynamic visual features focusing on relevant image aspects to the posed question. QA-ViT is model-agnostic and can be incorporated efficiently into any VL architecture. Extensive experiments demonstrate the effectiveness of applying our method to various multimodal architectures, leading to consistent improvement across diverse tasks and showcasing its potential for enhancing visual and scene-text understanding.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/2d421d94bc9eed935870088a6f3218244e36dc97.pdf",
      "citation_key": "ganz20249zr",
      "metadata": {
        "title": "Question Aware Vision Transformer for Multimodal Reasoning",
        "authors": [
          "Roy Ganz",
          "Yair Kittenplon",
          "Aviad Aberdam",
          "Elad Ben Avraham",
          "Oren Nuriel",
          "Shai Mazor",
          "Ron Litman"
        ],
        "published_date": "2024",
        "abstract": "Vision-Language (VL) models have gained significant research focus, enabling remarkable advances in multimodal reasoning. These architectures typically comprise a vision encoder, a Large Language Model (LLM), and a projection module that aligns visual features with the LLM's representation space. Despite their success, a critical limitation persists: the vision encoding process remains decoupled from user queries, often in the form of image-related questions. Consequently, the resulting visual features may not be optimally attuned to the query-specific elements of the image. To address this, we introduce QA-ViT, a Question Aware Vision Transformer approach for multimodal reasoning, which embeds question awareness directly within the vision encoder. This integration results in dynamic visual features focusing on relevant image aspects to the posed question. QA-ViT is model-agnostic and can be incorporated efficiently into any VL architecture. Extensive experiments demonstrate the effectiveness of applying our method to various multimodal architectures, leading to consistent improvement across diverse tasks and showcasing its potential for enhancing visual and scene-text understanding.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2d421d94bc9eed935870088a6f3218244e36dc97.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 28,
        "score": 28.0,
        "summary": "Vision-Language (VL) models have gained significant research focus, enabling remarkable advances in multimodal reasoning. These architectures typically comprise a vision encoder, a Large Language Model (LLM), and a projection module that aligns visual features with the LLM's representation space. Despite their success, a critical limitation persists: the vision encoding process remains decoupled from user queries, often in the form of image-related questions. Consequently, the resulting visual features may not be optimally attuned to the query-specific elements of the image. To address this, we introduce QA-ViT, a Question Aware Vision Transformer approach for multimodal reasoning, which embeds question awareness directly within the vision encoder. This integration results in dynamic visual features focusing on relevant image aspects to the posed question. QA-ViT is model-agnostic and can be incorporated efficiently into any VL architecture. Extensive experiments demonstrate the effectiveness of applying our method to various multimodal architectures, leading to consistent improvement across diverse tasks and showcasing its potential for enhancing visual and scene-text understanding.",
        "keywords": []
      },
      "file_name": "2d421d94bc9eed935870088a6f3218244e36dc97.pdf"
    },
    {
      "success": true,
      "doc_id": "d5d65df30fc53bc34afba8d86cad36a6",
      "summary": "Sugar cane is an important agricultural product that provides 75% of the world's sugar production. As with all plant species, any disease affecting sugarcane can significantly impact yields and planning. Diagnosing diseases in sugarcane leaves using traditional methods is slow, inefficient, and often lacking in accuracy. This study presents a deep learning-based approach for accurate diagnosis of diseases in sugarcane leaves. Specifically, training and evaluation were conducted on the publicly available Sugarcane Leaf Dataset using leading ViT (Vision Transformer) architectures such as DeiT3-Small and DeiT-Tiny. This dataset includes 11 different disease classes and a total of 6748 images. Additionally, these models were compared with popular CNN models. The findings of the study show that there is no direct relationship between model complexity, depth, and accuracy for the 11-class sugarcane dataset. Among the 12 models tested, the DeiT3-Small model showed the highest performance with 93.79% accuracy, 91.27% precision, and 90.96% F1-score. These results highlight that rapid, accurate, and automatic disease diagnosis systems developed using deep learning techniques can significantly improve sugarcane disease management and contribute to increased yields.",
      "intriguing_abstract": "Sugar cane is an important agricultural product that provides 75% of the world's sugar production. As with all plant species, any disease affecting sugarcane can significantly impact yields and planning. Diagnosing diseases in sugarcane leaves using traditional methods is slow, inefficient, and often lacking in accuracy. This study presents a deep learning-based approach for accurate diagnosis of diseases in sugarcane leaves. Specifically, training and evaluation were conducted on the publicly available Sugarcane Leaf Dataset using leading ViT (Vision Transformer) architectures such as DeiT3-Small and DeiT-Tiny. This dataset includes 11 different disease classes and a total of 6748 images. Additionally, these models were compared with popular CNN models. The findings of the study show that there is no direct relationship between model complexity, depth, and accuracy for the 11-class sugarcane dataset. Among the 12 models tested, the DeiT3-Small model showed the highest performance with 93.79% accuracy, 91.27% precision, and 90.96% F1-score. These results highlight that rapid, accurate, and automatic disease diagnosis systems developed using deep learning techniques can significantly improve sugarcane disease management and contribute to increased yields.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/be1aabb6460d49905575da88d564864da9f80417.pdf",
      "citation_key": "paal2024no4",
      "metadata": {
        "title": "Data-Efficient Vision Transformer Models for Robust Classification of Sugarcane",
        "authors": [
          "Ishak Paal",
          "Ismail Kunduracioglu"
        ],
        "published_date": "2024",
        "abstract": "Sugar cane is an important agricultural product that provides 75% of the world's sugar production. As with all plant species, any disease affecting sugarcane can significantly impact yields and planning. Diagnosing diseases in sugarcane leaves using traditional methods is slow, inefficient, and often lacking in accuracy. This study presents a deep learning-based approach for accurate diagnosis of diseases in sugarcane leaves. Specifically, training and evaluation were conducted on the publicly available Sugarcane Leaf Dataset using leading ViT (Vision Transformer) architectures such as DeiT3-Small and DeiT-Tiny. This dataset includes 11 different disease classes and a total of 6748 images. Additionally, these models were compared with popular CNN models. The findings of the study show that there is no direct relationship between model complexity, depth, and accuracy for the 11-class sugarcane dataset. Among the 12 models tested, the DeiT3-Small model showed the highest performance with 93.79% accuracy, 91.27% precision, and 90.96% F1-score. These results highlight that rapid, accurate, and automatic disease diagnosis systems developed using deep learning techniques can significantly improve sugarcane disease management and contribute to increased yields.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/be1aabb6460d49905575da88d564864da9f80417.pdf",
        "venue": "Journal of Soft Computing and Decision Analytics",
        "citationCount": 19,
        "score": 19.0,
        "summary": "Sugar cane is an important agricultural product that provides 75% of the world's sugar production. As with all plant species, any disease affecting sugarcane can significantly impact yields and planning. Diagnosing diseases in sugarcane leaves using traditional methods is slow, inefficient, and often lacking in accuracy. This study presents a deep learning-based approach for accurate diagnosis of diseases in sugarcane leaves. Specifically, training and evaluation were conducted on the publicly available Sugarcane Leaf Dataset using leading ViT (Vision Transformer) architectures such as DeiT3-Small and DeiT-Tiny. This dataset includes 11 different disease classes and a total of 6748 images. Additionally, these models were compared with popular CNN models. The findings of the study show that there is no direct relationship between model complexity, depth, and accuracy for the 11-class sugarcane dataset. Among the 12 models tested, the DeiT3-Small model showed the highest performance with 93.79% accuracy, 91.27% precision, and 90.96% F1-score. These results highlight that rapid, accurate, and automatic disease diagnosis systems developed using deep learning techniques can significantly improve sugarcane disease management and contribute to increased yields.",
        "keywords": []
      },
      "file_name": "be1aabb6460d49905575da88d564864da9f80417.pdf"
    },
    {
      "success": true,
      "doc_id": "ca66c4c66dd69ebe89708e7c3c8ab3ab",
      "summary": "Breast cancer detection is considered a challenging task for the average experienced radiologist due to the variation of the lesions size and shape, especially with the existence of high fibro-glandular tissues. The revolution of deep learning and computer vision contributes recently in introducing systems that can provide an automated diagnosis for breast cancer that can act as a second opinion for doctors/radiologists. The most of previously proposed deep learning-based Computer-Aided Diagnosis (CAD) systems mainly utilized Convolutional Neural Networks (CNN) that focuses on local features. Recently, vision transformers (ViT) have shown great potential in image classification tasks due to its ability in learning the local and global spatial features. This paper proposes a fully automated CAD framework based on YOLOv4 network and ViT transformers for mass detection and classification of Contrast Enhanced Spectral Mammography (CESM) images. CESM is an evolution type of Full Field Digital Mammography (FFDM) images that provides enhanced visualization for breast tissues. Different experiments were conducted to evaluate the proposed framework on two different datasets that are INbreast and CDD-CESM that provides both FFDM and CESM images. The model achieved at mass detection a mean Average Precision (mAP) score of 98.69%, 81.52%, and 71.65% and mass classification accuracy of 95.65%, 97.61%, and 80% for INbreast, CE-CESM, and DM-CESM, respectively. The proposed framework showed competitive results regarding the state-of-the-art models in INbreast. It outperformed the previous work in the literature in terms of the F1-score by almost 5% for mass detection in CESM. Moreover, the experiments showed that the CESM could provide more morphological features that can be more informative, especially with the highly dense breast tissues.",
      "intriguing_abstract": "Breast cancer detection is considered a challenging task for the average experienced radiologist due to the variation of the lesions size and shape, especially with the existence of high fibro-glandular tissues. The revolution of deep learning and computer vision contributes recently in introducing systems that can provide an automated diagnosis for breast cancer that can act as a second opinion for doctors/radiologists. The most of previously proposed deep learning-based Computer-Aided Diagnosis (CAD) systems mainly utilized Convolutional Neural Networks (CNN) that focuses on local features. Recently, vision transformers (ViT) have shown great potential in image classification tasks due to its ability in learning the local and global spatial features. This paper proposes a fully automated CAD framework based on YOLOv4 network and ViT transformers for mass detection and classification of Contrast Enhanced Spectral Mammography (CESM) images. CESM is an evolution type of Full Field Digital Mammography (FFDM) images that provides enhanced visualization for breast tissues. Different experiments were conducted to evaluate the proposed framework on two different datasets that are INbreast and CDD-CESM that provides both FFDM and CESM images. The model achieved at mass detection a mean Average Precision (mAP) score of 98.69%, 81.52%, and 71.65% and mass classification accuracy of 95.65%, 97.61%, and 80% for INbreast, CE-CESM, and DM-CESM, respectively. The proposed framework showed competitive results regarding the state-of-the-art models in INbreast. It outperformed the previous work in the literature in terms of the F1-score by almost 5% for mass detection in CESM. Moreover, the experiments showed that the CESM could provide more morphological features that can be more informative, especially with the highly dense breast tissues.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/0d7d27fbd8193acf8db032441fd22945d26e9952.pdf",
      "citation_key": "hassan20243qi",
      "metadata": {
        "title": "YOLO-based CAD framework with ViT transformer for breast mass detection and classification in CESM and FFDM images",
        "authors": [
          "Nada M. Hassan",
          "Safwat Hamad",
          "Khaled Mahar"
        ],
        "published_date": "2024",
        "abstract": "Breast cancer detection is considered a challenging task for the average experienced radiologist due to the variation of the lesions size and shape, especially with the existence of high fibro-glandular tissues. The revolution of deep learning and computer vision contributes recently in introducing systems that can provide an automated diagnosis for breast cancer that can act as a second opinion for doctors/radiologists. The most of previously proposed deep learning-based Computer-Aided Diagnosis (CAD) systems mainly utilized Convolutional Neural Networks (CNN) that focuses on local features. Recently, vision transformers (ViT) have shown great potential in image classification tasks due to its ability in learning the local and global spatial features. This paper proposes a fully automated CAD framework based on YOLOv4 network and ViT transformers for mass detection and classification of Contrast Enhanced Spectral Mammography (CESM) images. CESM is an evolution type of Full Field Digital Mammography (FFDM) images that provides enhanced visualization for breast tissues. Different experiments were conducted to evaluate the proposed framework on two different datasets that are INbreast and CDD-CESM that provides both FFDM and CESM images. The model achieved at mass detection a mean Average Precision (mAP) score of 98.69%, 81.52%, and 71.65% and mass classification accuracy of 95.65%, 97.61%, and 80% for INbreast, CE-CESM, and DM-CESM, respectively. The proposed framework showed competitive results regarding the state-of-the-art models in INbreast. It outperformed the previous work in the literature in terms of the F1-score by almost 5% for mass detection in CESM. Moreover, the experiments showed that the CESM could provide more morphological features that can be more informative, especially with the highly dense breast tissues.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0d7d27fbd8193acf8db032441fd22945d26e9952.pdf",
        "venue": "Neural computing & applications (Print)",
        "citationCount": 18,
        "score": 18.0,
        "summary": "Breast cancer detection is considered a challenging task for the average experienced radiologist due to the variation of the lesions size and shape, especially with the existence of high fibro-glandular tissues. The revolution of deep learning and computer vision contributes recently in introducing systems that can provide an automated diagnosis for breast cancer that can act as a second opinion for doctors/radiologists. The most of previously proposed deep learning-based Computer-Aided Diagnosis (CAD) systems mainly utilized Convolutional Neural Networks (CNN) that focuses on local features. Recently, vision transformers (ViT) have shown great potential in image classification tasks due to its ability in learning the local and global spatial features. This paper proposes a fully automated CAD framework based on YOLOv4 network and ViT transformers for mass detection and classification of Contrast Enhanced Spectral Mammography (CESM) images. CESM is an evolution type of Full Field Digital Mammography (FFDM) images that provides enhanced visualization for breast tissues. Different experiments were conducted to evaluate the proposed framework on two different datasets that are INbreast and CDD-CESM that provides both FFDM and CESM images. The model achieved at mass detection a mean Average Precision (mAP) score of 98.69%, 81.52%, and 71.65% and mass classification accuracy of 95.65%, 97.61%, and 80% for INbreast, CE-CESM, and DM-CESM, respectively. The proposed framework showed competitive results regarding the state-of-the-art models in INbreast. It outperformed the previous work in the literature in terms of the F1-score by almost 5% for mass detection in CESM. Moreover, the experiments showed that the CESM could provide more morphological features that can be more informative, especially with the highly dense breast tissues.",
        "keywords": []
      },
      "file_name": "0d7d27fbd8193acf8db032441fd22945d26e9952.pdf"
    },
    {
      "success": true,
      "doc_id": "18f83b43172ab62e30c58941e2046ffa",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/0af7ccb9f6cf2edb35674dc97d0ec5d165f8433b.pdf",
      "citation_key": "k2024wyx",
      "metadata": {
        "title": "A Deep Learning-Based Approach for Cervical Cancer Classification Using 3D CNN and Vision Transformer.",
        "authors": [
          "Abinaya K",
          "S. B"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0af7ccb9f6cf2edb35674dc97d0ec5d165f8433b.pdf",
        "venue": "Journal of imaging informatics in medicine",
        "citationCount": 16,
        "score": 16.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "0af7ccb9f6cf2edb35674dc97d0ec5d165f8433b.pdf"
    },
    {
      "success": true,
      "doc_id": "a2fdbfe77ca77344e0ded200f009025d",
      "summary": "Unsupervised vision clustering, a cornerstone in computer vision, has been studied for decades, yielding signif-icant outcomes across numerous vision tasks. However, these algorithms involve substantial computational demands when confronted with vast amounts of unlabeled data. Conversely, quantum computing holds promise in expediting unsupervised algorithms when handling large-scale databases. In this study, we introduce QClusformer, a pioneering Transformer-based frame-work leveraging quantum machines to tackle unsupervised vision clustering challenges. Specifically, we design the Transformer architecture, including the self-attention module and transformer blocks, from a quantum perspective to enable execution on quan-tum hardware. In addition, we present QClusformer, a variant based on the Transformer architecture, tailored for unsupervised vision clustering tasks. By integrating these elements into an end-to-end framework, QClusformer consistently outperforms previous methods running on classical computers. Empirical evaluations across diverse benchmarks, including MS-Celeb-1M and DeepFashion, underscore the superior performance of QClusformer compared to state-of-the-art methods.",
      "intriguing_abstract": "Unsupervised vision clustering, a cornerstone in computer vision, has been studied for decades, yielding signif-icant outcomes across numerous vision tasks. However, these algorithms involve substantial computational demands when confronted with vast amounts of unlabeled data. Conversely, quantum computing holds promise in expediting unsupervised algorithms when handling large-scale databases. In this study, we introduce QClusformer, a pioneering Transformer-based frame-work leveraging quantum machines to tackle unsupervised vision clustering challenges. Specifically, we design the Transformer architecture, including the self-attention module and transformer blocks, from a quantum perspective to enable execution on quan-tum hardware. In addition, we present QClusformer, a variant based on the Transformer architecture, tailored for unsupervised vision clustering tasks. By integrating these elements into an end-to-end framework, QClusformer consistently outperforms previous methods running on classical computers. Empirical evaluations across diverse benchmarks, including MS-Celeb-1M and DeepFashion, underscore the superior performance of QClusformer compared to state-of-the-art methods.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f9efe4918b9a20c2e8084d7a5ec624b87ec1f211.pdf",
      "citation_key": "nguyen2024id9",
      "metadata": {
        "title": "QClusformer: A Quantum Transformer-based Framework for Unsupervised Visual Clustering",
        "authors": [
          "Xuan-Bac Nguyen",
          "Hoang-Quan Nguyen",
          "Samuel Yen-Chi Chen",
          "S. U. Khan",
          "Hugh Churchill",
          "Khoa Luu"
        ],
        "published_date": "2024",
        "abstract": "Unsupervised vision clustering, a cornerstone in computer vision, has been studied for decades, yielding signif-icant outcomes across numerous vision tasks. However, these algorithms involve substantial computational demands when confronted with vast amounts of unlabeled data. Conversely, quantum computing holds promise in expediting unsupervised algorithms when handling large-scale databases. In this study, we introduce QClusformer, a pioneering Transformer-based frame-work leveraging quantum machines to tackle unsupervised vision clustering challenges. Specifically, we design the Transformer architecture, including the self-attention module and transformer blocks, from a quantum perspective to enable execution on quan-tum hardware. In addition, we present QClusformer, a variant based on the Transformer architecture, tailored for unsupervised vision clustering tasks. By integrating these elements into an end-to-end framework, QClusformer consistently outperforms previous methods running on classical computers. Empirical evaluations across diverse benchmarks, including MS-Celeb-1M and DeepFashion, underscore the superior performance of QClusformer compared to state-of-the-art methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f9efe4918b9a20c2e8084d7a5ec624b87ec1f211.pdf",
        "venue": "International Conference on Quantum Computing and Engineering",
        "citationCount": 16,
        "score": 16.0,
        "summary": "Unsupervised vision clustering, a cornerstone in computer vision, has been studied for decades, yielding signif-icant outcomes across numerous vision tasks. However, these algorithms involve substantial computational demands when confronted with vast amounts of unlabeled data. Conversely, quantum computing holds promise in expediting unsupervised algorithms when handling large-scale databases. In this study, we introduce QClusformer, a pioneering Transformer-based frame-work leveraging quantum machines to tackle unsupervised vision clustering challenges. Specifically, we design the Transformer architecture, including the self-attention module and transformer blocks, from a quantum perspective to enable execution on quan-tum hardware. In addition, we present QClusformer, a variant based on the Transformer architecture, tailored for unsupervised vision clustering tasks. By integrating these elements into an end-to-end framework, QClusformer consistently outperforms previous methods running on classical computers. Empirical evaluations across diverse benchmarks, including MS-Celeb-1M and DeepFashion, underscore the superior performance of QClusformer compared to state-of-the-art methods.",
        "keywords": []
      },
      "file_name": "f9efe4918b9a20c2e8084d7a5ec624b87ec1f211.pdf"
    },
    {
      "success": true,
      "doc_id": "2f2a9b366062955a7a43ae33304910ba",
      "summary": "Cervical cancer (CCa) is the fourth most prevalent and common cancer affecting women worldwide, with increasing incidence and mortality rates. Hence, early detection of CCa plays a crucial role in improving outcomes. Non-invasive imaging procedures with good diagnostic performance are desirable and have the potential to lessen the degree of intervention associated with the gold standard, biopsy. Recently, artificial intelligence-based diagnostic models such as Vision Transformers (ViT) have shown promising performance in image classification tasks, rivaling or surpassing traditional convolutional neural networks (CNNs). This paper studies the effect of applying a ViT to predict CCa using different image benchmark datasets. A newly developed approach (ViT-PSO-SVM) was presented for boosting the results of the ViT based on integrating the ViT with particle swarm optimization (PSO), and support vector machine (SVM). First, the proposed framework extracts features from the Vision Transformer. Then, PSO is used to reduce the complexity of extracted features and optimize feature representation. Finally, a softmax classification layer is replaced with an SVM classification model to precisely predict CCa. The models are evaluated using two benchmark cervical cell image datasets, namely SipakMed and Herlev, with different classification scenarios: two, three, and five classes. The proposed approach achieved 99.112% accuracy and 99.113% F1-score for SipakMed with two classes and achieved 97.778% accuracy and 97.805% F1-score for Herlev with two classes outperforming other Vision Transformers, CNN models, and pre-trained models. Finally, GradCAM is used as an explainable artificial intelligence (XAI) tool to visualize and understand the regions of a given image that are important for a models prediction. The obtained experimental results demonstrate the feasibility and efficacy of the developed ViT-PSO-SVM approach and hold the promise of providing a robust, reliable, accurate, and non-invasive diagnostic tool that will lead to improved healthcare outcomes worldwide.",
      "intriguing_abstract": "Cervical cancer (CCa) is the fourth most prevalent and common cancer affecting women worldwide, with increasing incidence and mortality rates. Hence, early detection of CCa plays a crucial role in improving outcomes. Non-invasive imaging procedures with good diagnostic performance are desirable and have the potential to lessen the degree of intervention associated with the gold standard, biopsy. Recently, artificial intelligence-based diagnostic models such as Vision Transformers (ViT) have shown promising performance in image classification tasks, rivaling or surpassing traditional convolutional neural networks (CNNs). This paper studies the effect of applying a ViT to predict CCa using different image benchmark datasets. A newly developed approach (ViT-PSO-SVM) was presented for boosting the results of the ViT based on integrating the ViT with particle swarm optimization (PSO), and support vector machine (SVM). First, the proposed framework extracts features from the Vision Transformer. Then, PSO is used to reduce the complexity of extracted features and optimize feature representation. Finally, a softmax classification layer is replaced with an SVM classification model to precisely predict CCa. The models are evaluated using two benchmark cervical cell image datasets, namely SipakMed and Herlev, with different classification scenarios: two, three, and five classes. The proposed approach achieved 99.112% accuracy and 99.113% F1-score for SipakMed with two classes and achieved 97.778% accuracy and 97.805% F1-score for Herlev with two classes outperforming other Vision Transformers, CNN models, and pre-trained models. Finally, GradCAM is used as an explainable artificial intelligence (XAI) tool to visualize and understand the regions of a given image that are important for a models prediction. The obtained experimental results demonstrate the feasibility and efficacy of the developed ViT-PSO-SVM approach and hold the promise of providing a robust, reliable, accurate, and non-invasive diagnostic tool that will lead to improved healthcare outcomes worldwide.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f62cb393ab51505bdd22823e74b7cf28f4801e1c.pdf",
      "citation_key": "almohimeed2024jq1",
      "metadata": {
        "title": "ViT-PSO-SVM: Cervical Cancer Predication Based on Integrating Vision Transformer with Particle Swarm Optimization and Support Vector Machine",
        "authors": [
          "Abdulaziz Almohimeed",
          "Mohamed Shehata",
          "Nora El-Rashidy",
          "Sherif Mostafa",
          "Amira Samy Talaat",
          "Hager Saleh"
        ],
        "published_date": "2024",
        "abstract": "Cervical cancer (CCa) is the fourth most prevalent and common cancer affecting women worldwide, with increasing incidence and mortality rates. Hence, early detection of CCa plays a crucial role in improving outcomes. Non-invasive imaging procedures with good diagnostic performance are desirable and have the potential to lessen the degree of intervention associated with the gold standard, biopsy. Recently, artificial intelligence-based diagnostic models such as Vision Transformers (ViT) have shown promising performance in image classification tasks, rivaling or surpassing traditional convolutional neural networks (CNNs). This paper studies the effect of applying a ViT to predict CCa using different image benchmark datasets. A newly developed approach (ViT-PSO-SVM) was presented for boosting the results of the ViT based on integrating the ViT with particle swarm optimization (PSO), and support vector machine (SVM). First, the proposed framework extracts features from the Vision Transformer. Then, PSO is used to reduce the complexity of extracted features and optimize feature representation. Finally, a softmax classification layer is replaced with an SVM classification model to precisely predict CCa. The models are evaluated using two benchmark cervical cell image datasets, namely SipakMed and Herlev, with different classification scenarios: two, three, and five classes. The proposed approach achieved 99.112% accuracy and 99.113% F1-score for SipakMed with two classes and achieved 97.778% accuracy and 97.805% F1-score for Herlev with two classes outperforming other Vision Transformers, CNN models, and pre-trained models. Finally, GradCAM is used as an explainable artificial intelligence (XAI) tool to visualize and understand the regions of a given image that are important for a models prediction. The obtained experimental results demonstrate the feasibility and efficacy of the developed ViT-PSO-SVM approach and hold the promise of providing a robust, reliable, accurate, and non-invasive diagnostic tool that will lead to improved healthcare outcomes worldwide.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f62cb393ab51505bdd22823e74b7cf28f4801e1c.pdf",
        "venue": "Bioengineering",
        "citationCount": 14,
        "score": 14.0,
        "summary": "Cervical cancer (CCa) is the fourth most prevalent and common cancer affecting women worldwide, with increasing incidence and mortality rates. Hence, early detection of CCa plays a crucial role in improving outcomes. Non-invasive imaging procedures with good diagnostic performance are desirable and have the potential to lessen the degree of intervention associated with the gold standard, biopsy. Recently, artificial intelligence-based diagnostic models such as Vision Transformers (ViT) have shown promising performance in image classification tasks, rivaling or surpassing traditional convolutional neural networks (CNNs). This paper studies the effect of applying a ViT to predict CCa using different image benchmark datasets. A newly developed approach (ViT-PSO-SVM) was presented for boosting the results of the ViT based on integrating the ViT with particle swarm optimization (PSO), and support vector machine (SVM). First, the proposed framework extracts features from the Vision Transformer. Then, PSO is used to reduce the complexity of extracted features and optimize feature representation. Finally, a softmax classification layer is replaced with an SVM classification model to precisely predict CCa. The models are evaluated using two benchmark cervical cell image datasets, namely SipakMed and Herlev, with different classification scenarios: two, three, and five classes. The proposed approach achieved 99.112% accuracy and 99.113% F1-score for SipakMed with two classes and achieved 97.778% accuracy and 97.805% F1-score for Herlev with two classes outperforming other Vision Transformers, CNN models, and pre-trained models. Finally, GradCAM is used as an explainable artificial intelligence (XAI) tool to visualize and understand the regions of a given image that are important for a models prediction. The obtained experimental results demonstrate the feasibility and efficacy of the developed ViT-PSO-SVM approach and hold the promise of providing a robust, reliable, accurate, and non-invasive diagnostic tool that will lead to improved healthcare outcomes worldwide.",
        "keywords": []
      },
      "file_name": "f62cb393ab51505bdd22823e74b7cf28f4801e1c.pdf"
    },
    {
      "success": true,
      "doc_id": "c7ed000a11d0f14290814200bbf0fbd8",
      "summary": "Camouflaged object detection (COD) and salient object detection (SOD) are two distinct yet closely-related computer vision tasks widely studied during the past decades. Though sharing the same purpose of segmenting an image into binary foreground and background regions, their distinction lies in the fact that COD focuses on concealed objects hidden in the image, while SOD concentrates on the most prominent objects in the image. Building universal segmentation models is currently a hot topic in the community. Previous works achieved good performance on certain task by stacking various hand-designed modules and multi-scale features. However, these careful task-specific designs also make them lose their potential as general-purpose architectures. Therefore, we hope to build general architectures that can be applied to both tasks. In this work, we propose a simple yet effective network (SENet) based on vision Transformer (ViT), by employing a simple design of an asymmetric ViT-based encoder-decoder structure, we yield competitive results on both tasks, exhibiting greater versatility than meticulously crafted ones. To enhance the performance of universal architectures on both tasks, we propose some general methods targeting some common difficulties of the two tasks. First, we use image reconstruction as an auxiliary task during training to increase the difficulty of training, forcing the network to have a better perception of the image as a whole to help with segmentation tasks. In addition, we propose a local information capture module (LICM) to make up for the limitations of the patch-level attention mechanism in pixel-level COD and SOD tasks and a dynamic weighted loss (DW loss) to solve the problem that small target samples are more difficult to locate and segment in both tasks. Finally, we also conduct a preliminary exploration of joint training, trying to use one model to complete two tasks simultaneously. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method. The code is available at https://github.com/linuxsino/SENet.",
      "intriguing_abstract": "Camouflaged object detection (COD) and salient object detection (SOD) are two distinct yet closely-related computer vision tasks widely studied during the past decades. Though sharing the same purpose of segmenting an image into binary foreground and background regions, their distinction lies in the fact that COD focuses on concealed objects hidden in the image, while SOD concentrates on the most prominent objects in the image. Building universal segmentation models is currently a hot topic in the community. Previous works achieved good performance on certain task by stacking various hand-designed modules and multi-scale features. However, these careful task-specific designs also make them lose their potential as general-purpose architectures. Therefore, we hope to build general architectures that can be applied to both tasks. In this work, we propose a simple yet effective network (SENet) based on vision Transformer (ViT), by employing a simple design of an asymmetric ViT-based encoder-decoder structure, we yield competitive results on both tasks, exhibiting greater versatility than meticulously crafted ones. To enhance the performance of universal architectures on both tasks, we propose some general methods targeting some common difficulties of the two tasks. First, we use image reconstruction as an auxiliary task during training to increase the difficulty of training, forcing the network to have a better perception of the image as a whole to help with segmentation tasks. In addition, we propose a local information capture module (LICM) to make up for the limitations of the patch-level attention mechanism in pixel-level COD and SOD tasks and a dynamic weighted loss (DW loss) to solve the problem that small target samples are more difficult to locate and segment in both tasks. Finally, we also conduct a preliminary exploration of joint training, trying to use one model to complete two tasks simultaneously. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method. The code is available at https://github.com/linuxsino/SENet.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/4702a22a3c2da1284a88d5e608d38cd106d66736.pdf",
      "citation_key": "hao202488z",
      "metadata": {
        "title": "A Simple Yet Effective Network Based on Vision Transformer for Camouflaged Object and Salient Object Detection",
        "authors": [
          "Chao Hao",
          "Zitong Yu",
          "Xin Liu",
          "Jun Xu",
          "Huanjing Yue",
          "Jingyu Yang"
        ],
        "published_date": "2024",
        "abstract": "Camouflaged object detection (COD) and salient object detection (SOD) are two distinct yet closely-related computer vision tasks widely studied during the past decades. Though sharing the same purpose of segmenting an image into binary foreground and background regions, their distinction lies in the fact that COD focuses on concealed objects hidden in the image, while SOD concentrates on the most prominent objects in the image. Building universal segmentation models is currently a hot topic in the community. Previous works achieved good performance on certain task by stacking various hand-designed modules and multi-scale features. However, these careful task-specific designs also make them lose their potential as general-purpose architectures. Therefore, we hope to build general architectures that can be applied to both tasks. In this work, we propose a simple yet effective network (SENet) based on vision Transformer (ViT), by employing a simple design of an asymmetric ViT-based encoder-decoder structure, we yield competitive results on both tasks, exhibiting greater versatility than meticulously crafted ones. To enhance the performance of universal architectures on both tasks, we propose some general methods targeting some common difficulties of the two tasks. First, we use image reconstruction as an auxiliary task during training to increase the difficulty of training, forcing the network to have a better perception of the image as a whole to help with segmentation tasks. In addition, we propose a local information capture module (LICM) to make up for the limitations of the patch-level attention mechanism in pixel-level COD and SOD tasks and a dynamic weighted loss (DW loss) to solve the problem that small target samples are more difficult to locate and segment in both tasks. Finally, we also conduct a preliminary exploration of joint training, trying to use one model to complete two tasks simultaneously. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method. The code is available at https://github.com/linuxsino/SENet.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/4702a22a3c2da1284a88d5e608d38cd106d66736.pdf",
        "venue": "IEEE Transactions on Image Processing",
        "citationCount": 14,
        "score": 14.0,
        "summary": "Camouflaged object detection (COD) and salient object detection (SOD) are two distinct yet closely-related computer vision tasks widely studied during the past decades. Though sharing the same purpose of segmenting an image into binary foreground and background regions, their distinction lies in the fact that COD focuses on concealed objects hidden in the image, while SOD concentrates on the most prominent objects in the image. Building universal segmentation models is currently a hot topic in the community. Previous works achieved good performance on certain task by stacking various hand-designed modules and multi-scale features. However, these careful task-specific designs also make them lose their potential as general-purpose architectures. Therefore, we hope to build general architectures that can be applied to both tasks. In this work, we propose a simple yet effective network (SENet) based on vision Transformer (ViT), by employing a simple design of an asymmetric ViT-based encoder-decoder structure, we yield competitive results on both tasks, exhibiting greater versatility than meticulously crafted ones. To enhance the performance of universal architectures on both tasks, we propose some general methods targeting some common difficulties of the two tasks. First, we use image reconstruction as an auxiliary task during training to increase the difficulty of training, forcing the network to have a better perception of the image as a whole to help with segmentation tasks. In addition, we propose a local information capture module (LICM) to make up for the limitations of the patch-level attention mechanism in pixel-level COD and SOD tasks and a dynamic weighted loss (DW loss) to solve the problem that small target samples are more difficult to locate and segment in both tasks. Finally, we also conduct a preliminary exploration of joint training, trying to use one model to complete two tasks simultaneously. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method. The code is available at https://github.com/linuxsino/SENet.",
        "keywords": []
      },
      "file_name": "4702a22a3c2da1284a88d5e608d38cd106d66736.pdf"
    },
    {
      "success": true,
      "doc_id": "f7cc754262fdd8583200f252a62bcff7",
      "summary": "In recent years, the field of industrial visual anomaly detection (VAD) has attracted significant attention in the context of advanced smart manufacturing systems. However, several limitations remain unresolved in existing approaches. While these methods can achieve satisfactory performance when training separate models for different categories, their scalability and performance suffer when faced with the challenge of simultaneous training for multiple categories. Reconstruction-based methods generally suffer from the identical mapping problem. To address these limitations, this study introduces the partial semantic aggregation vision transformer (PSA-VT), a scalable framework for industrial visual anomaly detection (VAD) that enables simultaneous multicategory anomaly detection using a single model. Our proposed PSA-VT framework adopts a hybrid design strategy. First, a pretrained convolutional neural network (CNN) is employed to extract multiscale discriminative local representation. Subsequently, the PSA-VT is introduced to perform representation reconstruction through long-range global semantic aggregation. Finally, the anomalous properties can be estimated by evaluating the reconstruction error of the representations. We conducted extensive experiments using the Mvtec AD industrial anomaly detection dataset, as well as the semantic anomaly detection datasets. The experimental results demonstrate that our method achieves state-of-the-art (SOTA) performance by capturing high-level semantics. Notably, PSA-VT surpasses other methods for the one-model-15-category anomaly detection tasks on the Mvtec AD dataset. Furthermore, we applied incremental learning techniques to enable the rapid deployment of PSA-VT in a real industrial scenario.",
      "intriguing_abstract": "In recent years, the field of industrial visual anomaly detection (VAD) has attracted significant attention in the context of advanced smart manufacturing systems. However, several limitations remain unresolved in existing approaches. While these methods can achieve satisfactory performance when training separate models for different categories, their scalability and performance suffer when faced with the challenge of simultaneous training for multiple categories. Reconstruction-based methods generally suffer from the identical mapping problem. To address these limitations, this study introduces the partial semantic aggregation vision transformer (PSA-VT), a scalable framework for industrial visual anomaly detection (VAD) that enables simultaneous multicategory anomaly detection using a single model. Our proposed PSA-VT framework adopts a hybrid design strategy. First, a pretrained convolutional neural network (CNN) is employed to extract multiscale discriminative local representation. Subsequently, the PSA-VT is introduced to perform representation reconstruction through long-range global semantic aggregation. Finally, the anomalous properties can be estimated by evaluating the reconstruction error of the representations. We conducted extensive experiments using the Mvtec AD industrial anomaly detection dataset, as well as the semantic anomaly detection datasets. The experimental results demonstrate that our method achieves state-of-the-art (SOTA) performance by capturing high-level semantics. Notably, PSA-VT surpasses other methods for the one-model-15-category anomaly detection tasks on the Mvtec AD dataset. Furthermore, we applied incremental learning techniques to enable the rapid deployment of PSA-VT in a real industrial scenario.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/9fcea59a7076064f5ac3949177307c1637473ffd.pdf",
      "citation_key": "yao20244li",
      "metadata": {
        "title": "Scalable Industrial Visual Anomaly Detection With Partial Semantics Aggregation Vision Transformer",
        "authors": [
          "Haiming Yao",
          "Wei Luo",
          "Jianan Lou",
          "Wen-yong Yu",
          "Xiaotian Zhang",
          "Zhenfeng Qiang",
          "Hui Shi"
        ],
        "published_date": "2024",
        "abstract": "In recent years, the field of industrial visual anomaly detection (VAD) has attracted significant attention in the context of advanced smart manufacturing systems. However, several limitations remain unresolved in existing approaches. While these methods can achieve satisfactory performance when training separate models for different categories, their scalability and performance suffer when faced with the challenge of simultaneous training for multiple categories. Reconstruction-based methods generally suffer from the identical mapping problem. To address these limitations, this study introduces the partial semantic aggregation vision transformer (PSA-VT), a scalable framework for industrial visual anomaly detection (VAD) that enables simultaneous multicategory anomaly detection using a single model. Our proposed PSA-VT framework adopts a hybrid design strategy. First, a pretrained convolutional neural network (CNN) is employed to extract multiscale discriminative local representation. Subsequently, the PSA-VT is introduced to perform representation reconstruction through long-range global semantic aggregation. Finally, the anomalous properties can be estimated by evaluating the reconstruction error of the representations. We conducted extensive experiments using the Mvtec AD industrial anomaly detection dataset, as well as the semantic anomaly detection datasets. The experimental results demonstrate that our method achieves state-of-the-art (SOTA) performance by capturing high-level semantics. Notably, PSA-VT surpasses other methods for the one-model-15-category anomaly detection tasks on the Mvtec AD dataset. Furthermore, we applied incremental learning techniques to enable the rapid deployment of PSA-VT in a real industrial scenario.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9fcea59a7076064f5ac3949177307c1637473ffd.pdf",
        "venue": "IEEE Transactions on Instrumentation and Measurement",
        "citationCount": 14,
        "score": 14.0,
        "summary": "In recent years, the field of industrial visual anomaly detection (VAD) has attracted significant attention in the context of advanced smart manufacturing systems. However, several limitations remain unresolved in existing approaches. While these methods can achieve satisfactory performance when training separate models for different categories, their scalability and performance suffer when faced with the challenge of simultaneous training for multiple categories. Reconstruction-based methods generally suffer from the identical mapping problem. To address these limitations, this study introduces the partial semantic aggregation vision transformer (PSA-VT), a scalable framework for industrial visual anomaly detection (VAD) that enables simultaneous multicategory anomaly detection using a single model. Our proposed PSA-VT framework adopts a hybrid design strategy. First, a pretrained convolutional neural network (CNN) is employed to extract multiscale discriminative local representation. Subsequently, the PSA-VT is introduced to perform representation reconstruction through long-range global semantic aggregation. Finally, the anomalous properties can be estimated by evaluating the reconstruction error of the representations. We conducted extensive experiments using the Mvtec AD industrial anomaly detection dataset, as well as the semantic anomaly detection datasets. The experimental results demonstrate that our method achieves state-of-the-art (SOTA) performance by capturing high-level semantics. Notably, PSA-VT surpasses other methods for the one-model-15-category anomaly detection tasks on the Mvtec AD dataset. Furthermore, we applied incremental learning techniques to enable the rapid deployment of PSA-VT in a real industrial scenario.",
        "keywords": []
      },
      "file_name": "9fcea59a7076064f5ac3949177307c1637473ffd.pdf"
    },
    {
      "success": true,
      "doc_id": "b142ca9c4d4c17b30d69eaf4434da92c",
      "summary": "While vision transformers (ViTs) have shown consistent progress in computer vision, deploying them for real-time decision-making scenarios (<1 ms) is challenging. Current computing platforms like CPUs, GPUs, or FPGA-based solutions struggle to meet this deterministic low-latency real-time requirement, even with quantized ViT models. Some approaches use pruning or sparsity to reduce the model size and latency, but this often results in accuracy loss. To address the aforementioned constraints, in this work, we propose EQ-ViT, an end-to-end acceleration framework with the novel algorithm and architecture co-design features to enable the real-time ViT acceleration on the AMD Versal adaptive compute acceleration platform (ACAP). The contributions are four-fold. First, we perform in-depth kernel-level performance profiling and analysis and explain the bottlenecks for the existing acceleration solutions on GPU, FPGA, and ACAP. Second, on the hardware level, we introduce a new spatial and heterogeneous accelerator architecture, the EQ-ViT architecture. This architecture leverages the heterogeneous features of ACAP, where both FPGA and artificial intelligence engines (AIEs) coexist on the same system-on-chip (SoC). Third, On the algorithm level, we create a comprehensive quantization-aware training strategy, the EQ-ViT algorithm. This strategy concurrently quantizes both the weights and activations into 8-bit integers, aiming to improve the accuracy rather than compromise it during quantization. Notably, the method also quantizes nonlinear functions for efficient hardware implementation. Fourth, we design the EQ-ViT automation framework to implement the EQ-ViT architecture for four different ViT applications on the AMD Versal ACAP VCK190 board, achieving accuracy improvement with 2.4%, and average speedups of 315.0, 3.39, 3.38, 14.92, 59.5, and $13.1\\times $ over computing solutions of Intel Xeon 8375C vCPU, Nvidia A10G, A100, Jetson AGX Orin GPUs, AMD ZCU102, and U250 FPGAs. The energy efficiency gains are 62.2, 15.33, 12.82, 13.31, 13.5, and $21.9\\times $ .",
      "intriguing_abstract": "While vision transformers (ViTs) have shown consistent progress in computer vision, deploying them for real-time decision-making scenarios (<1 ms) is challenging. Current computing platforms like CPUs, GPUs, or FPGA-based solutions struggle to meet this deterministic low-latency real-time requirement, even with quantized ViT models. Some approaches use pruning or sparsity to reduce the model size and latency, but this often results in accuracy loss. To address the aforementioned constraints, in this work, we propose EQ-ViT, an end-to-end acceleration framework with the novel algorithm and architecture co-design features to enable the real-time ViT acceleration on the AMD Versal adaptive compute acceleration platform (ACAP). The contributions are four-fold. First, we perform in-depth kernel-level performance profiling and analysis and explain the bottlenecks for the existing acceleration solutions on GPU, FPGA, and ACAP. Second, on the hardware level, we introduce a new spatial and heterogeneous accelerator architecture, the EQ-ViT architecture. This architecture leverages the heterogeneous features of ACAP, where both FPGA and artificial intelligence engines (AIEs) coexist on the same system-on-chip (SoC). Third, On the algorithm level, we create a comprehensive quantization-aware training strategy, the EQ-ViT algorithm. This strategy concurrently quantizes both the weights and activations into 8-bit integers, aiming to improve the accuracy rather than compromise it during quantization. Notably, the method also quantizes nonlinear functions for efficient hardware implementation. Fourth, we design the EQ-ViT automation framework to implement the EQ-ViT architecture for four different ViT applications on the AMD Versal ACAP VCK190 board, achieving accuracy improvement with 2.4%, and average speedups of 315.0, 3.39, 3.38, 14.92, 59.5, and $13.1\\times $ over computing solutions of Intel Xeon 8375C vCPU, Nvidia A10G, A100, Jetson AGX Orin GPUs, AMD ZCU102, and U250 FPGAs. The energy efficiency gains are 62.2, 15.33, 12.82, 13.31, 13.5, and $21.9\\times $ .",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/1ec9b653475287e95fdaef2f5247f82a8376c56c.pdf",
      "citation_key": "dong20245zz",
      "metadata": {
        "title": "EQ-ViT: Algorithm-Hardware Co-Design for End-to-End Acceleration of Real-Time Vision Transformer Inference on Versal ACAP Architecture",
        "authors": [
          "Peiyan Dong",
          "Jinming Zhuang",
          "Zhuoping Yang",
          "Shixin Ji",
          "Yanyu Li",
          "Dongkuan Xu",
          "Heng Huang",
          "Jingtong Hu",
          "Alex K. Jones",
          "Yiyu Shi",
          "Yanzhi Wang",
          "Peipei Zhou"
        ],
        "published_date": "2024",
        "abstract": "While vision transformers (ViTs) have shown consistent progress in computer vision, deploying them for real-time decision-making scenarios (<1 ms) is challenging. Current computing platforms like CPUs, GPUs, or FPGA-based solutions struggle to meet this deterministic low-latency real-time requirement, even with quantized ViT models. Some approaches use pruning or sparsity to reduce the model size and latency, but this often results in accuracy loss. To address the aforementioned constraints, in this work, we propose EQ-ViT, an end-to-end acceleration framework with the novel algorithm and architecture co-design features to enable the real-time ViT acceleration on the AMD Versal adaptive compute acceleration platform (ACAP). The contributions are four-fold. First, we perform in-depth kernel-level performance profiling and analysis and explain the bottlenecks for the existing acceleration solutions on GPU, FPGA, and ACAP. Second, on the hardware level, we introduce a new spatial and heterogeneous accelerator architecture, the EQ-ViT architecture. This architecture leverages the heterogeneous features of ACAP, where both FPGA and artificial intelligence engines (AIEs) coexist on the same system-on-chip (SoC). Third, On the algorithm level, we create a comprehensive quantization-aware training strategy, the EQ-ViT algorithm. This strategy concurrently quantizes both the weights and activations into 8-bit integers, aiming to improve the accuracy rather than compromise it during quantization. Notably, the method also quantizes nonlinear functions for efficient hardware implementation. Fourth, we design the EQ-ViT automation framework to implement the EQ-ViT architecture for four different ViT applications on the AMD Versal ACAP VCK190 board, achieving accuracy improvement with 2.4%, and average speedups of 315.0, 3.39, 3.38, 14.92, 59.5, and $13.1\\times $ over computing solutions of Intel Xeon 8375C vCPU, Nvidia A10G, A100, Jetson AGX Orin GPUs, AMD ZCU102, and U250 FPGAs. The energy efficiency gains are 62.2, 15.33, 12.82, 13.31, 13.5, and $21.9\\times $ .",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1ec9b653475287e95fdaef2f5247f82a8376c56c.pdf",
        "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
        "citationCount": 13,
        "score": 13.0,
        "summary": "While vision transformers (ViTs) have shown consistent progress in computer vision, deploying them for real-time decision-making scenarios (<1 ms) is challenging. Current computing platforms like CPUs, GPUs, or FPGA-based solutions struggle to meet this deterministic low-latency real-time requirement, even with quantized ViT models. Some approaches use pruning or sparsity to reduce the model size and latency, but this often results in accuracy loss. To address the aforementioned constraints, in this work, we propose EQ-ViT, an end-to-end acceleration framework with the novel algorithm and architecture co-design features to enable the real-time ViT acceleration on the AMD Versal adaptive compute acceleration platform (ACAP). The contributions are four-fold. First, we perform in-depth kernel-level performance profiling and analysis and explain the bottlenecks for the existing acceleration solutions on GPU, FPGA, and ACAP. Second, on the hardware level, we introduce a new spatial and heterogeneous accelerator architecture, the EQ-ViT architecture. This architecture leverages the heterogeneous features of ACAP, where both FPGA and artificial intelligence engines (AIEs) coexist on the same system-on-chip (SoC). Third, On the algorithm level, we create a comprehensive quantization-aware training strategy, the EQ-ViT algorithm. This strategy concurrently quantizes both the weights and activations into 8-bit integers, aiming to improve the accuracy rather than compromise it during quantization. Notably, the method also quantizes nonlinear functions for efficient hardware implementation. Fourth, we design the EQ-ViT automation framework to implement the EQ-ViT architecture for four different ViT applications on the AMD Versal ACAP VCK190 board, achieving accuracy improvement with 2.4%, and average speedups of 315.0, 3.39, 3.38, 14.92, 59.5, and $13.1\\times $ over computing solutions of Intel Xeon 8375C vCPU, Nvidia A10G, A100, Jetson AGX Orin GPUs, AMD ZCU102, and U250 FPGAs. The energy efficiency gains are 62.2, 15.33, 12.82, 13.31, 13.5, and $21.9\\times $ .",
        "keywords": []
      },
      "file_name": "1ec9b653475287e95fdaef2f5247f82a8376c56c.pdf"
    },
    {
      "success": true,
      "doc_id": "3df0be6659ba08056bcaa36597bc8287",
      "summary": "Face morphing attacks have posed severe threats to Face Recognition Systems (FRS), which are operated in border control and passport issuance use cases. Correspondingly, morphing attack detection algorithms (MAD) are needed to defend against such attacks. MAD approaches must be robust enough to handle unknown attacks in an open-set scenario where attacks can originate from various morphing generation algorithms, post-processing and the diversity of printers/scanners. The problem of generalization is further pronounced when the detection has to be made on a single suspected image. In this paper, we propose a generalized single-image-based MAD (S-MAD) algorithm by learning the encoding from Vision Transformer (ViT) architecture. Compared to CNN-based architectures, ViT model has the advantage on integrating local and global information and hence can be suitable to detect the morphing traces widely distributed among the face region. Extensive experiments are carried out on face morphing datasets generated using publicly available FRGC face datasets. Several state-of-the-art (SOTA) MAD algorithms, including representative ones that have been publicly evaluated, have been selected and benchmarked with our ViT-based approach. Obtained results demonstrate the improved detection performance of the proposed S-MAD method on inter-dataset testing (when different data is used for training and testing) and comparable performance on intra-dataset testing (when the same data is used for training and testing) experimental protocol.",
      "intriguing_abstract": "Face morphing attacks have posed severe threats to Face Recognition Systems (FRS), which are operated in border control and passport issuance use cases. Correspondingly, morphing attack detection algorithms (MAD) are needed to defend against such attacks. MAD approaches must be robust enough to handle unknown attacks in an open-set scenario where attacks can originate from various morphing generation algorithms, post-processing and the diversity of printers/scanners. The problem of generalization is further pronounced when the detection has to be made on a single suspected image. In this paper, we propose a generalized single-image-based MAD (S-MAD) algorithm by learning the encoding from Vision Transformer (ViT) architecture. Compared to CNN-based architectures, ViT model has the advantage on integrating local and global information and hence can be suitable to detect the morphing traces widely distributed among the face region. Extensive experiments are carried out on face morphing datasets generated using publicly available FRGC face datasets. Several state-of-the-art (SOTA) MAD algorithms, including representative ones that have been publicly evaluated, have been selected and benchmarked with our ViT-based approach. Obtained results demonstrate the improved detection performance of the proposed S-MAD method on inter-dataset testing (when different data is used for training and testing) and comparable performance on intra-dataset testing (when the same data is used for training and testing) experimental protocol.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/2b48f5f0769bf41b5a3025ace73417d19de65cf1.pdf",
      "citation_key": "zhang2024jha",
      "metadata": {
        "title": "Generalized Single-Image-Based Morphing Attack Detection Using Deep Representations from Vision Transformer",
        "authors": [
          "Haoyu Zhang",
          "Raghavendra Ramachandra",
          "Kiran B. Raja",
          "Christoph Busch"
        ],
        "published_date": "2024",
        "abstract": "Face morphing attacks have posed severe threats to Face Recognition Systems (FRS), which are operated in border control and passport issuance use cases. Correspondingly, morphing attack detection algorithms (MAD) are needed to defend against such attacks. MAD approaches must be robust enough to handle unknown attacks in an open-set scenario where attacks can originate from various morphing generation algorithms, post-processing and the diversity of printers/scanners. The problem of generalization is further pronounced when the detection has to be made on a single suspected image. In this paper, we propose a generalized single-image-based MAD (S-MAD) algorithm by learning the encoding from Vision Transformer (ViT) architecture. Compared to CNN-based architectures, ViT model has the advantage on integrating local and global information and hence can be suitable to detect the morphing traces widely distributed among the face region. Extensive experiments are carried out on face morphing datasets generated using publicly available FRGC face datasets. Several state-of-the-art (SOTA) MAD algorithms, including representative ones that have been publicly evaluated, have been selected and benchmarked with our ViT-based approach. Obtained results demonstrate the improved detection performance of the proposed S-MAD method on inter-dataset testing (when different data is used for training and testing) and comparable performance on intra-dataset testing (when the same data is used for training and testing) experimental protocol.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2b48f5f0769bf41b5a3025ace73417d19de65cf1.pdf",
        "venue": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Face morphing attacks have posed severe threats to Face Recognition Systems (FRS), which are operated in border control and passport issuance use cases. Correspondingly, morphing attack detection algorithms (MAD) are needed to defend against such attacks. MAD approaches must be robust enough to handle unknown attacks in an open-set scenario where attacks can originate from various morphing generation algorithms, post-processing and the diversity of printers/scanners. The problem of generalization is further pronounced when the detection has to be made on a single suspected image. In this paper, we propose a generalized single-image-based MAD (S-MAD) algorithm by learning the encoding from Vision Transformer (ViT) architecture. Compared to CNN-based architectures, ViT model has the advantage on integrating local and global information and hence can be suitable to detect the morphing traces widely distributed among the face region. Extensive experiments are carried out on face morphing datasets generated using publicly available FRGC face datasets. Several state-of-the-art (SOTA) MAD algorithms, including representative ones that have been publicly evaluated, have been selected and benchmarked with our ViT-based approach. Obtained results demonstrate the improved detection performance of the proposed S-MAD method on inter-dataset testing (when different data is used for training and testing) and comparable performance on intra-dataset testing (when the same data is used for training and testing) experimental protocol.",
        "keywords": []
      },
      "file_name": "2b48f5f0769bf41b5a3025ace73417d19de65cf1.pdf"
    },
    {
      "success": true,
      "doc_id": "8ba3878057f4349a0dd8d6f9d53fd368",
      "summary": "Facial beauty analysis is a crucial subject in human culture among researchers across various applications. Recent studies have utilized multidisciplinary approaches to examine the relationship between facial traits, age, emotions, and other factors. Facial beauty prediction is a significant visual recognition challenge that evaluates facial attractiveness for human perception. This task demands considerable effort due to the novelty of the field and the limited resources available, including a small database for facial beauty prediction. In this context, a deep learning method has recently shown remarkable capabilities in predicting facial beauty. Additionally, vision Transformers have recently been introduced as novel deep learning approaches and have shown strong performance in various applications. The key issue is that the vision transformer performs significantly worse than ResNet when trained on a small ImageNet database. In this paper, we propose to address the challenges of predicting facial beauty by utilizing vision transformers instead of relying on feature extraction based on Convolutional Neural Networks, which are commonly used in traditional methods. Moreover, we define and optimize a set of hyperparameters according to the SCUT-FBP5500 benchmark dataset. The model achieves a Pearson coefficient of 0.9534. Experimental results indicated that using this proposed network leads to better predicting facial beauty closer to human evaluation than conventional technology that provides facial beauty assessment.",
      "intriguing_abstract": "Facial beauty analysis is a crucial subject in human culture among researchers across various applications. Recent studies have utilized multidisciplinary approaches to examine the relationship between facial traits, age, emotions, and other factors. Facial beauty prediction is a significant visual recognition challenge that evaluates facial attractiveness for human perception. This task demands considerable effort due to the novelty of the field and the limited resources available, including a small database for facial beauty prediction. In this context, a deep learning method has recently shown remarkable capabilities in predicting facial beauty. Additionally, vision Transformers have recently been introduced as novel deep learning approaches and have shown strong performance in various applications. The key issue is that the vision transformer performs significantly worse than ResNet when trained on a small ImageNet database. In this paper, we propose to address the challenges of predicting facial beauty by utilizing vision transformers instead of relying on feature extraction based on Convolutional Neural Networks, which are commonly used in traditional methods. Moreover, we define and optimize a set of hyperparameters according to the SCUT-FBP5500 benchmark dataset. The model achieves a Pearson coefficient of 0.9534. Experimental results indicated that using this proposed network leads to better predicting facial beauty closer to human evaluation than conventional technology that provides facial beauty assessment.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/bfb310434204d99fa5c420ac272966c5ae1c1bd2.pdf",
      "citation_key": "boukhari2024gbb",
      "metadata": {
        "title": "Facial Beauty Prediction Based on Vision Transformer",
        "authors": [
          "D. E. Boukhari"
        ],
        "published_date": "2024",
        "abstract": "Facial beauty analysis is a crucial subject in human culture among researchers across various applications. Recent studies have utilized multidisciplinary approaches to examine the relationship between facial traits, age, emotions, and other factors. Facial beauty prediction is a significant visual recognition challenge that evaluates facial attractiveness for human perception. This task demands considerable effort due to the novelty of the field and the limited resources available, including a small database for facial beauty prediction. In this context, a deep learning method has recently shown remarkable capabilities in predicting facial beauty. Additionally, vision Transformers have recently been introduced as novel deep learning approaches and have shown strong performance in various applications. The key issue is that the vision transformer performs significantly worse than ResNet when trained on a small ImageNet database. In this paper, we propose to address the challenges of predicting facial beauty by utilizing vision transformers instead of relying on feature extraction based on Convolutional Neural Networks, which are commonly used in traditional methods. Moreover, we define and optimize a set of hyperparameters according to the SCUT-FBP5500 benchmark dataset. The model achieves a Pearson coefficient of 0.9534. Experimental results indicated that using this proposed network leads to better predicting facial beauty closer to human evaluation than conventional technology that provides facial beauty assessment.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/bfb310434204d99fa5c420ac272966c5ae1c1bd2.pdf",
        "venue": "International Journal of Electrical and Electronic Engineering &amp; Telecommunications",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Facial beauty analysis is a crucial subject in human culture among researchers across various applications. Recent studies have utilized multidisciplinary approaches to examine the relationship between facial traits, age, emotions, and other factors. Facial beauty prediction is a significant visual recognition challenge that evaluates facial attractiveness for human perception. This task demands considerable effort due to the novelty of the field and the limited resources available, including a small database for facial beauty prediction. In this context, a deep learning method has recently shown remarkable capabilities in predicting facial beauty. Additionally, vision Transformers have recently been introduced as novel deep learning approaches and have shown strong performance in various applications. The key issue is that the vision transformer performs significantly worse than ResNet when trained on a small ImageNet database. In this paper, we propose to address the challenges of predicting facial beauty by utilizing vision transformers instead of relying on feature extraction based on Convolutional Neural Networks, which are commonly used in traditional methods. Moreover, we define and optimize a set of hyperparameters according to the SCUT-FBP5500 benchmark dataset. The model achieves a Pearson coefficient of 0.9534. Experimental results indicated that using this proposed network leads to better predicting facial beauty closer to human evaluation than conventional technology that provides facial beauty assessment.",
        "keywords": []
      },
      "file_name": "bfb310434204d99fa5c420ac272966c5ae1c1bd2.pdf"
    },
    {
      "success": true,
      "doc_id": "0afb7856428852364b517440a1f831e1",
      "summary": "Existing Vision Transformer (ViT)based object detection methods for remote sensing images (RSIs) face significant challenges due to the scarcity of RSI samples and the overreliance on enhancement strategies originally developed for natural images. This often leads to inconsistent data distributions between training and testing subsets, resulting in degraded model performance. In this study, we introduce an optimized data distribution learning (ODDL) strategy and develop an object detection framework based on the Faster RCNN architecture, named ODDLNet. The ODDL strategy begins with an optimized augmentation (OA) technique, overcoming the limitations of conventional data augmentation methods. Next, we propose an optimized mosaic algorithm (OMA), improving upon the shortcomings of traditional Mosaic augmentation techniques. Additionally, we introduce a feature fusion regularization (FFR) method, addressing the inherent limitations of classic feature pyramid networks. These innovations are integrated into three modular, plugandplay componentsnamely, the OA, OMA, and FFR modulesensuring that the ODDL strategy can be seamlessly incorporated into existing detection frameworks without requiring significant modifications. To evaluate the effectiveness of the proposed ODDLNet, we develop two variants based on different ViT architectures: the Next ViT (NViT) small model and the Swin Transformer (SwinT) tiny model, both used as detection backbones. Experimental results on the NWPU10, DIOR20, MAR20, and GLHBridge datasets demonstrate that both variants of ODDLNet achieve impressive accuracy, surpassing 23 stateoftheart methods introduced since 2023. Specifically, ODDLNetNViT attained accuracies of 78.3% on the challenging DIOR20 dataset and 61.4% on the GLHBridge dataset. Notably, this represents a substantial improvement of approximately 23% over the Faster RCNNResNet50 baseline on the DIOR20 dataset. In conclusion, this study demonstrates that ViTs are well suited for highaccuracy object detection in RSIs. Furthermore, it provides a straightforward solution for building ViTbased detectors, offering a practical approach that requires little model modification.",
      "intriguing_abstract": "Existing Vision Transformer (ViT)based object detection methods for remote sensing images (RSIs) face significant challenges due to the scarcity of RSI samples and the overreliance on enhancement strategies originally developed for natural images. This often leads to inconsistent data distributions between training and testing subsets, resulting in degraded model performance. In this study, we introduce an optimized data distribution learning (ODDL) strategy and develop an object detection framework based on the Faster RCNN architecture, named ODDLNet. The ODDL strategy begins with an optimized augmentation (OA) technique, overcoming the limitations of conventional data augmentation methods. Next, we propose an optimized mosaic algorithm (OMA), improving upon the shortcomings of traditional Mosaic augmentation techniques. Additionally, we introduce a feature fusion regularization (FFR) method, addressing the inherent limitations of classic feature pyramid networks. These innovations are integrated into three modular, plugandplay componentsnamely, the OA, OMA, and FFR modulesensuring that the ODDL strategy can be seamlessly incorporated into existing detection frameworks without requiring significant modifications. To evaluate the effectiveness of the proposed ODDLNet, we develop two variants based on different ViT architectures: the Next ViT (NViT) small model and the Swin Transformer (SwinT) tiny model, both used as detection backbones. Experimental results on the NWPU10, DIOR20, MAR20, and GLHBridge datasets demonstrate that both variants of ODDLNet achieve impressive accuracy, surpassing 23 stateoftheart methods introduced since 2023. Specifically, ODDLNetNViT attained accuracies of 78.3% on the challenging DIOR20 dataset and 61.4% on the GLHBridge dataset. Notably, this represents a substantial improvement of approximately 23% over the Faster RCNNResNet50 baseline on the DIOR20 dataset. In conclusion, this study demonstrates that ViTs are well suited for highaccuracy object detection in RSIs. Furthermore, it provides a straightforward solution for building ViTbased detectors, offering a practical approach that requires little model modification.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/bbe5dfbecfd1bed7556b9c8269b0d363faa24973.pdf",
      "citation_key": "song2025idg",
      "metadata": {
        "title": "Optimized Data Distribution Learning for Enhancing Vision TransformerBased Object Detection in Remote Sensing Images",
        "authors": [
          "Huaxiang Song",
          "Junping Xie",
          "Yunyang Wang",
          "Lihua Fu",
          "Yang Zhou",
          "Xing Zhou"
        ],
        "published_date": "2025",
        "abstract": "Existing Vision Transformer (ViT)based object detection methods for remote sensing images (RSIs) face significant challenges due to the scarcity of RSI samples and the overreliance on enhancement strategies originally developed for natural images. This often leads to inconsistent data distributions between training and testing subsets, resulting in degraded model performance. In this study, we introduce an optimized data distribution learning (ODDL) strategy and develop an object detection framework based on the Faster RCNN architecture, named ODDLNet. The ODDL strategy begins with an optimized augmentation (OA) technique, overcoming the limitations of conventional data augmentation methods. Next, we propose an optimized mosaic algorithm (OMA), improving upon the shortcomings of traditional Mosaic augmentation techniques. Additionally, we introduce a feature fusion regularization (FFR) method, addressing the inherent limitations of classic feature pyramid networks. These innovations are integrated into three modular, plugandplay componentsnamely, the OA, OMA, and FFR modulesensuring that the ODDL strategy can be seamlessly incorporated into existing detection frameworks without requiring significant modifications. To evaluate the effectiveness of the proposed ODDLNet, we develop two variants based on different ViT architectures: the Next ViT (NViT) small model and the Swin Transformer (SwinT) tiny model, both used as detection backbones. Experimental results on the NWPU10, DIOR20, MAR20, and GLHBridge datasets demonstrate that both variants of ODDLNet achieve impressive accuracy, surpassing 23 stateoftheart methods introduced since 2023. Specifically, ODDLNetNViT attained accuracies of 78.3% on the challenging DIOR20 dataset and 61.4% on the GLHBridge dataset. Notably, this represents a substantial improvement of approximately 23% over the Faster RCNNResNet50 baseline on the DIOR20 dataset. In conclusion, this study demonstrates that ViTs are well suited for highaccuracy object detection in RSIs. Furthermore, it provides a straightforward solution for building ViTbased detectors, offering a practical approach that requires little model modification.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/bbe5dfbecfd1bed7556b9c8269b0d363faa24973.pdf",
        "venue": "Photogrammetric Record",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Existing Vision Transformer (ViT)based object detection methods for remote sensing images (RSIs) face significant challenges due to the scarcity of RSI samples and the overreliance on enhancement strategies originally developed for natural images. This often leads to inconsistent data distributions between training and testing subsets, resulting in degraded model performance. In this study, we introduce an optimized data distribution learning (ODDL) strategy and develop an object detection framework based on the Faster RCNN architecture, named ODDLNet. The ODDL strategy begins with an optimized augmentation (OA) technique, overcoming the limitations of conventional data augmentation methods. Next, we propose an optimized mosaic algorithm (OMA), improving upon the shortcomings of traditional Mosaic augmentation techniques. Additionally, we introduce a feature fusion regularization (FFR) method, addressing the inherent limitations of classic feature pyramid networks. These innovations are integrated into three modular, plugandplay componentsnamely, the OA, OMA, and FFR modulesensuring that the ODDL strategy can be seamlessly incorporated into existing detection frameworks without requiring significant modifications. To evaluate the effectiveness of the proposed ODDLNet, we develop two variants based on different ViT architectures: the Next ViT (NViT) small model and the Swin Transformer (SwinT) tiny model, both used as detection backbones. Experimental results on the NWPU10, DIOR20, MAR20, and GLHBridge datasets demonstrate that both variants of ODDLNet achieve impressive accuracy, surpassing 23 stateoftheart methods introduced since 2023. Specifically, ODDLNetNViT attained accuracies of 78.3% on the challenging DIOR20 dataset and 61.4% on the GLHBridge dataset. Notably, this represents a substantial improvement of approximately 23% over the Faster RCNNResNet50 baseline on the DIOR20 dataset. In conclusion, this study demonstrates that ViTs are well suited for highaccuracy object detection in RSIs. Furthermore, it provides a straightforward solution for building ViTbased detectors, offering a practical approach that requires little model modification.",
        "keywords": []
      },
      "file_name": "bbe5dfbecfd1bed7556b9c8269b0d363faa24973.pdf"
    },
    {
      "success": true,
      "doc_id": "3cc5ebee57bf95b98d9c19ca8ee2d693",
      "summary": "As WiFi technology becomes more widespread and integrated, precise location tracking within complex indoor environments is gaining significance in contemporary public spaces.Nevertheless, challenges persist in indoor scenarios, including issues such as noise and multi-path effects stemming from the degradation of the Received Signal Strength Indicator (RSSI). Additionally, obstacles causing signal shading contribute to a decrease in the accuracy of indoor location tracking. Within this paper, we introduce an innovative indoor localization algorithm termed Vision Transformer Indoor Localization (VTIL). This algorithmleverages RSSI and Vision-Transformer (ViT) technologies to enhance indoor positioning accuracy. Initially, the RSSI fingerprints undergo normalization by scaling them to the maximum and minimum values. To mitigate the impact of noise and irrelevant features, the Principal Component Analysis (PCA) algorithmis then employed for effective feature extraction.Secondly, the RSSI fingerprint library is converted into an RSSI gray image library. Then the RSSI gray image is divided into several small blocks, and the position-coding input is performed in the form of a sequence. The ViT model divides the weight ratio of each block in the RSSI image to alleviate the impact of multi-path effects. According to the experimental results using public datasets, our approach achieves a noteworthy 37.26% reduction in the average distance estimation error when compared to existing indoor fingerprint localization algorithms.",
      "intriguing_abstract": "As WiFi technology becomes more widespread and integrated, precise location tracking within complex indoor environments is gaining significance in contemporary public spaces.Nevertheless, challenges persist in indoor scenarios, including issues such as noise and multi-path effects stemming from the degradation of the Received Signal Strength Indicator (RSSI). Additionally, obstacles causing signal shading contribute to a decrease in the accuracy of indoor location tracking. Within this paper, we introduce an innovative indoor localization algorithm termed Vision Transformer Indoor Localization (VTIL). This algorithmleverages RSSI and Vision-Transformer (ViT) technologies to enhance indoor positioning accuracy. Initially, the RSSI fingerprints undergo normalization by scaling them to the maximum and minimum values. To mitigate the impact of noise and irrelevant features, the Principal Component Analysis (PCA) algorithmis then employed for effective feature extraction.Secondly, the RSSI fingerprint library is converted into an RSSI gray image library. Then the RSSI gray image is divided into several small blocks, and the position-coding input is performed in the form of a sequence. The ViT model divides the weight ratio of each block in the RSSI image to alleviate the impact of multi-path effects. According to the experimental results using public datasets, our approach achieves a noteworthy 37.26% reduction in the average distance estimation error when compared to existing indoor fingerprint localization algorithms.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/be28edb064e2050e0852ff376434ee39f7c4b0a3.pdf",
      "citation_key": "zhou2024tps",
      "metadata": {
        "title": "VTIL: A multi-layer indoor location algorithm for RSSI images based on vision transformer",
        "authors": [
          "Heng Zhou",
          "Jingmin Yang",
          "Shanghui Deng",
          "Wenjie Zhang"
        ],
        "published_date": "2024",
        "abstract": "As WiFi technology becomes more widespread and integrated, precise location tracking within complex indoor environments is gaining significance in contemporary public spaces.Nevertheless, challenges persist in indoor scenarios, including issues such as noise and multi-path effects stemming from the degradation of the Received Signal Strength Indicator (RSSI). Additionally, obstacles causing signal shading contribute to a decrease in the accuracy of indoor location tracking. Within this paper, we introduce an innovative indoor localization algorithm termed Vision Transformer Indoor Localization (VTIL). This algorithmleverages RSSI and Vision-Transformer (ViT) technologies to enhance indoor positioning accuracy. Initially, the RSSI fingerprints undergo normalization by scaling them to the maximum and minimum values. To mitigate the impact of noise and irrelevant features, the Principal Component Analysis (PCA) algorithmis then employed for effective feature extraction.Secondly, the RSSI fingerprint library is converted into an RSSI gray image library. Then the RSSI gray image is divided into several small blocks, and the position-coding input is performed in the form of a sequence. The ViT model divides the weight ratio of each block in the RSSI image to alleviate the impact of multi-path effects. According to the experimental results using public datasets, our approach achieves a noteworthy 37.26% reduction in the average distance estimation error when compared to existing indoor fingerprint localization algorithms.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/be28edb064e2050e0852ff376434ee39f7c4b0a3.pdf",
        "venue": "Engineering Research Express",
        "citationCount": 10,
        "score": 10.0,
        "summary": "As WiFi technology becomes more widespread and integrated, precise location tracking within complex indoor environments is gaining significance in contemporary public spaces.Nevertheless, challenges persist in indoor scenarios, including issues such as noise and multi-path effects stemming from the degradation of the Received Signal Strength Indicator (RSSI). Additionally, obstacles causing signal shading contribute to a decrease in the accuracy of indoor location tracking. Within this paper, we introduce an innovative indoor localization algorithm termed Vision Transformer Indoor Localization (VTIL). This algorithmleverages RSSI and Vision-Transformer (ViT) technologies to enhance indoor positioning accuracy. Initially, the RSSI fingerprints undergo normalization by scaling them to the maximum and minimum values. To mitigate the impact of noise and irrelevant features, the Principal Component Analysis (PCA) algorithmis then employed for effective feature extraction.Secondly, the RSSI fingerprint library is converted into an RSSI gray image library. Then the RSSI gray image is divided into several small blocks, and the position-coding input is performed in the form of a sequence. The ViT model divides the weight ratio of each block in the RSSI image to alleviate the impact of multi-path effects. According to the experimental results using public datasets, our approach achieves a noteworthy 37.26% reduction in the average distance estimation error when compared to existing indoor fingerprint localization algorithms.",
        "keywords": []
      },
      "file_name": "be28edb064e2050e0852ff376434ee39f7c4b0a3.pdf"
    },
    {
      "success": true,
      "doc_id": "cedfe441fd24c10f4b32cef8d032c84f",
      "summary": "Background: This study evaluates the performance of a vision transformer (ViT) model, ViT-b16, in classifying ischemic stroke cases from Moroccan MRI scans and compares it to the Visual Geometry Group 16 (VGG-16) model used in a prior study. Methods: A dataset of 342 MRI scans, categorized into Normal and Stroke classes, underwent preprocessing using TensorFlows tf.data API. Results: The ViT-b16 model was trained and evaluated, yielding an impressive accuracy of 97.59%, surpassing the VGG-16 models 90% accuracy. Conclusions: This research highlights the ViT-b16 models superior classification capabilities for ischemic stroke diagnosis, contributing to the field of medical image analysis. By showcasing the efficacy of advanced deep learning architectures, particularly in the context of Moroccan MRI scans, this study underscores the potential for real-world clinical applications. Ultimately, our findings emphasize the importance of further exploration into AI-based diagnostic tools for improving healthcare outcomes.",
      "intriguing_abstract": "Background: This study evaluates the performance of a vision transformer (ViT) model, ViT-b16, in classifying ischemic stroke cases from Moroccan MRI scans and compares it to the Visual Geometry Group 16 (VGG-16) model used in a prior study. Methods: A dataset of 342 MRI scans, categorized into Normal and Stroke classes, underwent preprocessing using TensorFlows tf.data API. Results: The ViT-b16 model was trained and evaluated, yielding an impressive accuracy of 97.59%, surpassing the VGG-16 models 90% accuracy. Conclusions: This research highlights the ViT-b16 models superior classification capabilities for ischemic stroke diagnosis, contributing to the field of medical image analysis. By showcasing the efficacy of advanced deep learning architectures, particularly in the context of Moroccan MRI scans, this study underscores the potential for real-world clinical applications. Ultimately, our findings emphasize the importance of further exploration into AI-based diagnostic tools for improving healthcare outcomes.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/e25a0b06079966b8e43f8e1f2455913266cb7426.pdf",
      "citation_key": "abbaoui20244wy",
      "metadata": {
        "title": "Automated Ischemic Stroke Classification from MRI Scans: Using a Vision Transformer Approach",
        "authors": [
          "Wafae Abbaoui",
          "Sara Retal",
          "Soumia Ziti",
          "Brahim El Bhiri"
        ],
        "published_date": "2024",
        "abstract": "Background: This study evaluates the performance of a vision transformer (ViT) model, ViT-b16, in classifying ischemic stroke cases from Moroccan MRI scans and compares it to the Visual Geometry Group 16 (VGG-16) model used in a prior study. Methods: A dataset of 342 MRI scans, categorized into Normal and Stroke classes, underwent preprocessing using TensorFlows tf.data API. Results: The ViT-b16 model was trained and evaluated, yielding an impressive accuracy of 97.59%, surpassing the VGG-16 models 90% accuracy. Conclusions: This research highlights the ViT-b16 models superior classification capabilities for ischemic stroke diagnosis, contributing to the field of medical image analysis. By showcasing the efficacy of advanced deep learning architectures, particularly in the context of Moroccan MRI scans, this study underscores the potential for real-world clinical applications. Ultimately, our findings emphasize the importance of further exploration into AI-based diagnostic tools for improving healthcare outcomes.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e25a0b06079966b8e43f8e1f2455913266cb7426.pdf",
        "venue": "Journal of Clinical Medicine",
        "citationCount": 10,
        "score": 10.0,
        "summary": "Background: This study evaluates the performance of a vision transformer (ViT) model, ViT-b16, in classifying ischemic stroke cases from Moroccan MRI scans and compares it to the Visual Geometry Group 16 (VGG-16) model used in a prior study. Methods: A dataset of 342 MRI scans, categorized into Normal and Stroke classes, underwent preprocessing using TensorFlows tf.data API. Results: The ViT-b16 model was trained and evaluated, yielding an impressive accuracy of 97.59%, surpassing the VGG-16 models 90% accuracy. Conclusions: This research highlights the ViT-b16 models superior classification capabilities for ischemic stroke diagnosis, contributing to the field of medical image analysis. By showcasing the efficacy of advanced deep learning architectures, particularly in the context of Moroccan MRI scans, this study underscores the potential for real-world clinical applications. Ultimately, our findings emphasize the importance of further exploration into AI-based diagnostic tools for improving healthcare outcomes.",
        "keywords": []
      },
      "file_name": "e25a0b06079966b8e43f8e1f2455913266cb7426.pdf"
    },
    {
      "success": true,
      "doc_id": "51a640e35c5365e527dcfd217efa818a",
      "summary": "Empowered by transformer-based models, visual tracking has advanced significantly. However, the slow speed of current trackers limits their applicability on devices with constrained computational resources. To address this challenge, we introduce ABTrack, an adaptive computation framework that adaptively bypassing transformer blocks for efficient visual tracking. The rationale behind ABTrack is rooted in the observation that semantic features or relations do not uniformly impact the tracking task across all abstraction levels. Instead, this impact varies based on the characteristics of the target and the scene it occupies. Consequently, disregarding insignificant semantic features or relations at certain abstraction levels may not significantly affect the tracking accuracy. We propose a Bypass Decision Module (BDM) to determine if a transformer block should be bypassed, which adaptively simplifies the architecture of ViTs and thus speeds up the inference process. To counteract the time cost incurred by the BDMs and further enhance the efficiency of ViTs, we introduce a novel ViT pruning method to reduce the dimension of the latent representation of tokens in each transformer block. Extensive experiments on multiple tracking benchmarks validate the effectiveness and generality of the proposed method and show that it achieves state-of-the-art performance. Code is released at: https://github.com/xyyang317/ABTrack.",
      "intriguing_abstract": "Empowered by transformer-based models, visual tracking has advanced significantly. However, the slow speed of current trackers limits their applicability on devices with constrained computational resources. To address this challenge, we introduce ABTrack, an adaptive computation framework that adaptively bypassing transformer blocks for efficient visual tracking. The rationale behind ABTrack is rooted in the observation that semantic features or relations do not uniformly impact the tracking task across all abstraction levels. Instead, this impact varies based on the characteristics of the target and the scene it occupies. Consequently, disregarding insignificant semantic features or relations at certain abstraction levels may not significantly affect the tracking accuracy. We propose a Bypass Decision Module (BDM) to determine if a transformer block should be bypassed, which adaptively simplifies the architecture of ViTs and thus speeds up the inference process. To counteract the time cost incurred by the BDMs and further enhance the efficiency of ViTs, we introduce a novel ViT pruning method to reduce the dimension of the latent representation of tokens in each transformer block. Extensive experiments on multiple tracking benchmarks validate the effectiveness and generality of the proposed method and show that it achieves state-of-the-art performance. Code is released at: https://github.com/xyyang317/ABTrack.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/ecd9598308161557d6ac35b3e4d32770489e811d.pdf",
      "citation_key": "yang2024nyx",
      "metadata": {
        "title": "Adaptively Bypassing Vision Transformer Blocks for Efficient Visual Tracking",
        "authors": [
          "Xiangyang Yang",
          "Dan Zeng",
          "Xucheng Wang",
          "You Wu",
          "Hengzhou Ye",
          "Qijun Zhao",
          "Shuiwang Li"
        ],
        "published_date": "2024",
        "abstract": "Empowered by transformer-based models, visual tracking has advanced significantly. However, the slow speed of current trackers limits their applicability on devices with constrained computational resources. To address this challenge, we introduce ABTrack, an adaptive computation framework that adaptively bypassing transformer blocks for efficient visual tracking. The rationale behind ABTrack is rooted in the observation that semantic features or relations do not uniformly impact the tracking task across all abstraction levels. Instead, this impact varies based on the characteristics of the target and the scene it occupies. Consequently, disregarding insignificant semantic features or relations at certain abstraction levels may not significantly affect the tracking accuracy. We propose a Bypass Decision Module (BDM) to determine if a transformer block should be bypassed, which adaptively simplifies the architecture of ViTs and thus speeds up the inference process. To counteract the time cost incurred by the BDMs and further enhance the efficiency of ViTs, we introduce a novel ViT pruning method to reduce the dimension of the latent representation of tokens in each transformer block. Extensive experiments on multiple tracking benchmarks validate the effectiveness and generality of the proposed method and show that it achieves state-of-the-art performance. Code is released at: https://github.com/xyyang317/ABTrack.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ecd9598308161557d6ac35b3e4d32770489e811d.pdf",
        "venue": "Pattern Recognition",
        "citationCount": 10,
        "score": 10.0,
        "summary": "Empowered by transformer-based models, visual tracking has advanced significantly. However, the slow speed of current trackers limits their applicability on devices with constrained computational resources. To address this challenge, we introduce ABTrack, an adaptive computation framework that adaptively bypassing transformer blocks for efficient visual tracking. The rationale behind ABTrack is rooted in the observation that semantic features or relations do not uniformly impact the tracking task across all abstraction levels. Instead, this impact varies based on the characteristics of the target and the scene it occupies. Consequently, disregarding insignificant semantic features or relations at certain abstraction levels may not significantly affect the tracking accuracy. We propose a Bypass Decision Module (BDM) to determine if a transformer block should be bypassed, which adaptively simplifies the architecture of ViTs and thus speeds up the inference process. To counteract the time cost incurred by the BDMs and further enhance the efficiency of ViTs, we introduce a novel ViT pruning method to reduce the dimension of the latent representation of tokens in each transformer block. Extensive experiments on multiple tracking benchmarks validate the effectiveness and generality of the proposed method and show that it achieves state-of-the-art performance. Code is released at: https://github.com/xyyang317/ABTrack.",
        "keywords": []
      },
      "file_name": "ecd9598308161557d6ac35b3e4d32770489e811d.pdf"
    },
    {
      "success": true,
      "doc_id": "599c1e360f9cbf80a1b137aae70b4358",
      "summary": "This article presented a novel significant wave height (SWH) estimation method, SWHFormer, which incorporates the Vision Transformer (ViT) to estimate SWH from X-band nautical radar images. Unlike traditional convolutional neural networks (CNNs), the ViT model treats the input as a sequence, capitalizing on its attention mechanism to capture long-range dependencies, resulting in superior performance in capturing the complex patterns present in sea wave dynamics. The radar data undergo an image denoising routine, followed by patching, flattening, and embedding processes to form a sequence fed into the transformer encoding module. The outputs from the encoder are then aggregated to derive the final regression result, i.e., SWH estimation. To evaluate the performance of SWHFormer, the dataset collected by a Decca radar aboard a free-navigating vessel is analyzed, and both buoy and model-based data are used as ground truth. In this study, two traditional linear fitting methods, i.e., ensemble empirical mode decomposition (EEMD) and variational mode decomposition (VMD)-based approaches, and a recent deep learning algorithm, convolutional gated recurrent unit (CGRU) network, are exploited for comparison with SWHFormer. It is found that the root mean square error (RMSE) of the estimated results using the proposed SWHFormer is decreased from 0.29, 0.26, and 0.18 m to 0.16 m after the temporal moving average, respectively, compared with the above three methods, when the buoy-measured SWH is served as ground truth. Besides, it is decreased from 0.30, 0.28, and 0.16 m to 0.14 m, respectively, when the model-based SWH is used as reference.",
      "intriguing_abstract": "This article presented a novel significant wave height (SWH) estimation method, SWHFormer, which incorporates the Vision Transformer (ViT) to estimate SWH from X-band nautical radar images. Unlike traditional convolutional neural networks (CNNs), the ViT model treats the input as a sequence, capitalizing on its attention mechanism to capture long-range dependencies, resulting in superior performance in capturing the complex patterns present in sea wave dynamics. The radar data undergo an image denoising routine, followed by patching, flattening, and embedding processes to form a sequence fed into the transformer encoding module. The outputs from the encoder are then aggregated to derive the final regression result, i.e., SWH estimation. To evaluate the performance of SWHFormer, the dataset collected by a Decca radar aboard a free-navigating vessel is analyzed, and both buoy and model-based data are used as ground truth. In this study, two traditional linear fitting methods, i.e., ensemble empirical mode decomposition (EEMD) and variational mode decomposition (VMD)-based approaches, and a recent deep learning algorithm, convolutional gated recurrent unit (CGRU) network, are exploited for comparison with SWHFormer. It is found that the root mean square error (RMSE) of the estimated results using the proposed SWHFormer is decreased from 0.29, 0.26, and 0.18 m to 0.16 m after the temporal moving average, respectively, compared with the above three methods, when the buoy-measured SWH is served as ground truth. Besides, it is decreased from 0.30, 0.28, and 0.16 m to 0.14 m, respectively, when the model-based SWH is used as reference.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/7dc4b2930870e66caa7ff23b5d447283a6171452.pdf",
      "citation_key": "yang20241kf",
      "metadata": {
        "title": "SWHFormer: A Vision Transformer for Significant Wave Height Estimation From Nautical Radar Images",
        "authors": [
          "Zhiding Yang",
          "Weimin Huang"
        ],
        "published_date": "2024",
        "abstract": "This article presented a novel significant wave height (SWH) estimation method, SWHFormer, which incorporates the Vision Transformer (ViT) to estimate SWH from X-band nautical radar images. Unlike traditional convolutional neural networks (CNNs), the ViT model treats the input as a sequence, capitalizing on its attention mechanism to capture long-range dependencies, resulting in superior performance in capturing the complex patterns present in sea wave dynamics. The radar data undergo an image denoising routine, followed by patching, flattening, and embedding processes to form a sequence fed into the transformer encoding module. The outputs from the encoder are then aggregated to derive the final regression result, i.e., SWH estimation. To evaluate the performance of SWHFormer, the dataset collected by a Decca radar aboard a free-navigating vessel is analyzed, and both buoy and model-based data are used as ground truth. In this study, two traditional linear fitting methods, i.e., ensemble empirical mode decomposition (EEMD) and variational mode decomposition (VMD)-based approaches, and a recent deep learning algorithm, convolutional gated recurrent unit (CGRU) network, are exploited for comparison with SWHFormer. It is found that the root mean square error (RMSE) of the estimated results using the proposed SWHFormer is decreased from 0.29, 0.26, and 0.18 m to 0.16 m after the temporal moving average, respectively, compared with the above three methods, when the buoy-measured SWH is served as ground truth. Besides, it is decreased from 0.30, 0.28, and 0.16 m to 0.14 m, respectively, when the model-based SWH is used as reference.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7dc4b2930870e66caa7ff23b5d447283a6171452.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 10,
        "score": 10.0,
        "summary": "This article presented a novel significant wave height (SWH) estimation method, SWHFormer, which incorporates the Vision Transformer (ViT) to estimate SWH from X-band nautical radar images. Unlike traditional convolutional neural networks (CNNs), the ViT model treats the input as a sequence, capitalizing on its attention mechanism to capture long-range dependencies, resulting in superior performance in capturing the complex patterns present in sea wave dynamics. The radar data undergo an image denoising routine, followed by patching, flattening, and embedding processes to form a sequence fed into the transformer encoding module. The outputs from the encoder are then aggregated to derive the final regression result, i.e., SWH estimation. To evaluate the performance of SWHFormer, the dataset collected by a Decca radar aboard a free-navigating vessel is analyzed, and both buoy and model-based data are used as ground truth. In this study, two traditional linear fitting methods, i.e., ensemble empirical mode decomposition (EEMD) and variational mode decomposition (VMD)-based approaches, and a recent deep learning algorithm, convolutional gated recurrent unit (CGRU) network, are exploited for comparison with SWHFormer. It is found that the root mean square error (RMSE) of the estimated results using the proposed SWHFormer is decreased from 0.29, 0.26, and 0.18 m to 0.16 m after the temporal moving average, respectively, compared with the above three methods, when the buoy-measured SWH is served as ground truth. Besides, it is decreased from 0.30, 0.28, and 0.16 m to 0.14 m, respectively, when the model-based SWH is used as reference.",
        "keywords": []
      },
      "file_name": "7dc4b2930870e66caa7ff23b5d447283a6171452.pdf"
    },
    {
      "success": true,
      "doc_id": "237fa87b6f6982f8f8cabcbdad7f05d9",
      "summary": "The Vision Transformer (ViT) excels in accuracy when handling high-resolution images, yet it confronts the challenge of significant spatial redundancy, leading to increased computational and memory requirements. To address this, we present the Localization and Focus Vision Transformer (LF-ViT). This model operates by strategically curtailing computational demands without impinging on performance. In the Localization phase, a reduced-resolution image is processed; if a definitive prediction remains elusive, our pioneering Neighborhood Global Class Attention (NGCA) mechanism is triggered, effectively identifying and spotlighting class-discriminative regions based on initial findings. Subsequently, in the Focus phase, this designated region is used from the original image to enhance recognition. Uniquely, LF-ViT employs consistent parameters across both phases, ensuring seamless end-to-end optimization. Our empirical tests affirm LF-ViT's prowess: it remarkably decreases Deit-S's FLOPs by 63% and concurrently amplifies throughput twofold. Code of this project is at https://github.com/edgeai1/LF-ViT.git.",
      "intriguing_abstract": "The Vision Transformer (ViT) excels in accuracy when handling high-resolution images, yet it confronts the challenge of significant spatial redundancy, leading to increased computational and memory requirements. To address this, we present the Localization and Focus Vision Transformer (LF-ViT). This model operates by strategically curtailing computational demands without impinging on performance. In the Localization phase, a reduced-resolution image is processed; if a definitive prediction remains elusive, our pioneering Neighborhood Global Class Attention (NGCA) mechanism is triggered, effectively identifying and spotlighting class-discriminative regions based on initial findings. Subsequently, in the Focus phase, this designated region is used from the original image to enhance recognition. Uniquely, LF-ViT employs consistent parameters across both phases, ensuring seamless end-to-end optimization. Our empirical tests affirm LF-ViT's prowess: it remarkably decreases Deit-S's FLOPs by 63% and concurrently amplifies throughput twofold. Code of this project is at https://github.com/edgeai1/LF-ViT.git.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/b1f0ac0b5e4044251dcdcce78e0046feb43cc7c7.pdf",
      "citation_key": "hu202434n",
      "metadata": {
        "title": "LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition",
        "authors": [
          "Youbing Hu",
          "Yun Cheng",
          "Anqi Lu",
          "Zhiqiang Cao",
          "Dawei Wei",
          "Jie Liu",
          "Zhijun Li"
        ],
        "published_date": "2024",
        "abstract": "The Vision Transformer (ViT) excels in accuracy when handling high-resolution images, yet it confronts the challenge of significant spatial redundancy, leading to increased computational and memory requirements. To address this, we present the Localization and Focus Vision Transformer (LF-ViT). This model operates by strategically curtailing computational demands without impinging on performance. In the Localization phase, a reduced-resolution image is processed; if a definitive prediction remains elusive, our pioneering Neighborhood Global Class Attention (NGCA) mechanism is triggered, effectively identifying and spotlighting class-discriminative regions based on initial findings. Subsequently, in the Focus phase, this designated region is used from the original image to enhance recognition. Uniquely, LF-ViT employs consistent parameters across both phases, ensuring seamless end-to-end optimization. Our empirical tests affirm LF-ViT's prowess: it remarkably decreases Deit-S's FLOPs by 63% and concurrently amplifies throughput twofold. Code of this project is at https://github.com/edgeai1/LF-ViT.git.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b1f0ac0b5e4044251dcdcce78e0046feb43cc7c7.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 9,
        "score": 9.0,
        "summary": "The Vision Transformer (ViT) excels in accuracy when handling high-resolution images, yet it confronts the challenge of significant spatial redundancy, leading to increased computational and memory requirements. To address this, we present the Localization and Focus Vision Transformer (LF-ViT). This model operates by strategically curtailing computational demands without impinging on performance. In the Localization phase, a reduced-resolution image is processed; if a definitive prediction remains elusive, our pioneering Neighborhood Global Class Attention (NGCA) mechanism is triggered, effectively identifying and spotlighting class-discriminative regions based on initial findings. Subsequently, in the Focus phase, this designated region is used from the original image to enhance recognition. Uniquely, LF-ViT employs consistent parameters across both phases, ensuring seamless end-to-end optimization. Our empirical tests affirm LF-ViT's prowess: it remarkably decreases Deit-S's FLOPs by 63% and concurrently amplifies throughput twofold. Code of this project is at https://github.com/edgeai1/LF-ViT.git.",
        "keywords": []
      },
      "file_name": "b1f0ac0b5e4044251dcdcce78e0046feb43cc7c7.pdf"
    },
    {
      "success": true,
      "doc_id": "e6b34003c85e86764f390a856b05661a",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/903c8a82f5539b3e482efcb23ff59819f04f2c0f.pdf",
      "citation_key": "yang20244dq",
      "metadata": {
        "title": "Vision transformer-based visual language understanding of the construction process",
        "authors": [
          "Bin Yang",
          "Binghan Zhang",
          "Yilong Han",
          "Boda Liu",
          "Jiniming Hu",
          "Yiming Jin"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/903c8a82f5539b3e482efcb23ff59819f04f2c0f.pdf",
        "venue": "Alexandria Engineering Journal",
        "citationCount": 9,
        "score": 9.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "903c8a82f5539b3e482efcb23ff59819f04f2c0f.pdf"
    },
    {
      "success": true,
      "doc_id": "5a416435431664337661d6887ba066d3",
      "summary": "Face recognition systems are increasingly used in biometric security for convenience and effectiveness. However, they remain vulnerable to spoofing attacks, where attackers use photos, videos, or masks to impersonate legitimate users. This research addresses these vulnerabilities by exploring the Vision Transformer (ViT) architecture, fine-tuned with the DINO framework utilizing CelebA-Spoof, CASIA SURF, and a proprietary dataset. The DINO framework facilitates self-supervised learning, enabling the model to learn distinguishing features from unlabeled data. We compared the performance of the proposed fine-tuned ViT model using the DINO framework against traditional models, including CNN Model EfficientNet b2, EfficientNet b2 (Noisy Student), and Mobile ViT on the face anti-spoofing task. Numerous tests on standard datasets show that the ViT model performs better than other models in terms of accuracy and resistance to different spoofing methods. Our models superior performance, particularly in APCER (1.6%), the most critical metric in this domain, underscores its improved ability to detect spoofing relative to other models. Additionally, we collected our own dataset from a biometric application to validate our findings further. This study highlights the superior performance of transformer-based architecture in identifying complex spoofing cues, leading to significant advancements in biometric security.",
      "intriguing_abstract": "Face recognition systems are increasingly used in biometric security for convenience and effectiveness. However, they remain vulnerable to spoofing attacks, where attackers use photos, videos, or masks to impersonate legitimate users. This research addresses these vulnerabilities by exploring the Vision Transformer (ViT) architecture, fine-tuned with the DINO framework utilizing CelebA-Spoof, CASIA SURF, and a proprietary dataset. The DINO framework facilitates self-supervised learning, enabling the model to learn distinguishing features from unlabeled data. We compared the performance of the proposed fine-tuned ViT model using the DINO framework against traditional models, including CNN Model EfficientNet b2, EfficientNet b2 (Noisy Student), and Mobile ViT on the face anti-spoofing task. Numerous tests on standard datasets show that the ViT model performs better than other models in terms of accuracy and resistance to different spoofing methods. Our models superior performance, particularly in APCER (1.6%), the most critical metric in this domain, underscores its improved ability to detect spoofing relative to other models. Additionally, we collected our own dataset from a biometric application to validate our findings further. This study highlights the superior performance of transformer-based architecture in identifying complex spoofing cues, leading to significant advancements in biometric security.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/1e95bb5827dc784547a46058793c15effd74dccc.pdf",
      "citation_key": "keresh20249rl",
      "metadata": {
        "title": "Liveness Detection in Computer Vision: Transformer-Based Self-Supervised Learning for Face Anti-Spoofing",
        "authors": [
          "Arman Keresh",
          "Pakizar Shamoi"
        ],
        "published_date": "2024",
        "abstract": "Face recognition systems are increasingly used in biometric security for convenience and effectiveness. However, they remain vulnerable to spoofing attacks, where attackers use photos, videos, or masks to impersonate legitimate users. This research addresses these vulnerabilities by exploring the Vision Transformer (ViT) architecture, fine-tuned with the DINO framework utilizing CelebA-Spoof, CASIA SURF, and a proprietary dataset. The DINO framework facilitates self-supervised learning, enabling the model to learn distinguishing features from unlabeled data. We compared the performance of the proposed fine-tuned ViT model using the DINO framework against traditional models, including CNN Model EfficientNet b2, EfficientNet b2 (Noisy Student), and Mobile ViT on the face anti-spoofing task. Numerous tests on standard datasets show that the ViT model performs better than other models in terms of accuracy and resistance to different spoofing methods. Our models superior performance, particularly in APCER (1.6%), the most critical metric in this domain, underscores its improved ability to detect spoofing relative to other models. Additionally, we collected our own dataset from a biometric application to validate our findings further. This study highlights the superior performance of transformer-based architecture in identifying complex spoofing cues, leading to significant advancements in biometric security.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1e95bb5827dc784547a46058793c15effd74dccc.pdf",
        "venue": "IEEE Access",
        "citationCount": 9,
        "score": 9.0,
        "summary": "Face recognition systems are increasingly used in biometric security for convenience and effectiveness. However, they remain vulnerable to spoofing attacks, where attackers use photos, videos, or masks to impersonate legitimate users. This research addresses these vulnerabilities by exploring the Vision Transformer (ViT) architecture, fine-tuned with the DINO framework utilizing CelebA-Spoof, CASIA SURF, and a proprietary dataset. The DINO framework facilitates self-supervised learning, enabling the model to learn distinguishing features from unlabeled data. We compared the performance of the proposed fine-tuned ViT model using the DINO framework against traditional models, including CNN Model EfficientNet b2, EfficientNet b2 (Noisy Student), and Mobile ViT on the face anti-spoofing task. Numerous tests on standard datasets show that the ViT model performs better than other models in terms of accuracy and resistance to different spoofing methods. Our models superior performance, particularly in APCER (1.6%), the most critical metric in this domain, underscores its improved ability to detect spoofing relative to other models. Additionally, we collected our own dataset from a biometric application to validate our findings further. This study highlights the superior performance of transformer-based architecture in identifying complex spoofing cues, leading to significant advancements in biometric security.",
        "keywords": []
      },
      "file_name": "1e95bb5827dc784547a46058793c15effd74dccc.pdf"
    },
    {
      "success": true,
      "doc_id": "a6faf52f521c353b8be1bfb719d204e6",
      "summary": "Cone-beam computed tomography (CBCT) is a crucial component of adaptive radiation therapy; however, it frequently encounters challenges such as artifacts and noise, significantly constraining its clinical utility. While CycleGAN is a widely employed method for CT image synthesis, it has notable limitations regarding the inadequate capture of global features. To tackle these challenges, we introduce a refined unsupervised learning model called improved vision transformer CycleGAN (IViT-CycleGAN). Firstly, we integrate a U-net framework that builds upon ViT. Next, we augment the feed-forward neural network by incorporating deep convolutional networks. Lastly, we enhance the stability of the model training process by introducing gradient penalty and integrating an additional loss term into the generator loss. The experiment demonstrates from multiple perspectives that our model-generated synthesizing CT(sCT) has significant advantages compared to other unsupervised learning models, thereby validating the clinical applicability and robustness of our model. In future clinical practice, our model has the potential to assist clinical practitioners in formulating precise radiotherapy plans.",
      "intriguing_abstract": "Cone-beam computed tomography (CBCT) is a crucial component of adaptive radiation therapy; however, it frequently encounters challenges such as artifacts and noise, significantly constraining its clinical utility. While CycleGAN is a widely employed method for CT image synthesis, it has notable limitations regarding the inadequate capture of global features. To tackle these challenges, we introduce a refined unsupervised learning model called improved vision transformer CycleGAN (IViT-CycleGAN). Firstly, we integrate a U-net framework that builds upon ViT. Next, we augment the feed-forward neural network by incorporating deep convolutional networks. Lastly, we enhance the stability of the model training process by introducing gradient penalty and integrating an additional loss term into the generator loss. The experiment demonstrates from multiple perspectives that our model-generated synthesizing CT(sCT) has significant advantages compared to other unsupervised learning models, thereby validating the clinical applicability and robustness of our model. In future clinical practice, our model has the potential to assist clinical practitioners in formulating precise radiotherapy plans.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/2104eea2b01b7a4b6ba2d256769328ac4919afad.pdf",
      "citation_key": "hu20247km",
      "metadata": {
        "title": "Synthetic CT generation based on CBCT using improved vision transformer CycleGAN",
        "authors": [
          "Yuxin Hu",
          "Han Zhou",
          "Ning Cao",
          "Can Li",
          "Can Hu"
        ],
        "published_date": "2024",
        "abstract": "Cone-beam computed tomography (CBCT) is a crucial component of adaptive radiation therapy; however, it frequently encounters challenges such as artifacts and noise, significantly constraining its clinical utility. While CycleGAN is a widely employed method for CT image synthesis, it has notable limitations regarding the inadequate capture of global features. To tackle these challenges, we introduce a refined unsupervised learning model called improved vision transformer CycleGAN (IViT-CycleGAN). Firstly, we integrate a U-net framework that builds upon ViT. Next, we augment the feed-forward neural network by incorporating deep convolutional networks. Lastly, we enhance the stability of the model training process by introducing gradient penalty and integrating an additional loss term into the generator loss. The experiment demonstrates from multiple perspectives that our model-generated synthesizing CT(sCT) has significant advantages compared to other unsupervised learning models, thereby validating the clinical applicability and robustness of our model. In future clinical practice, our model has the potential to assist clinical practitioners in formulating precise radiotherapy plans.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2104eea2b01b7a4b6ba2d256769328ac4919afad.pdf",
        "venue": "Scientific Reports",
        "citationCount": 9,
        "score": 9.0,
        "summary": "Cone-beam computed tomography (CBCT) is a crucial component of adaptive radiation therapy; however, it frequently encounters challenges such as artifacts and noise, significantly constraining its clinical utility. While CycleGAN is a widely employed method for CT image synthesis, it has notable limitations regarding the inadequate capture of global features. To tackle these challenges, we introduce a refined unsupervised learning model called improved vision transformer CycleGAN (IViT-CycleGAN). Firstly, we integrate a U-net framework that builds upon ViT. Next, we augment the feed-forward neural network by incorporating deep convolutional networks. Lastly, we enhance the stability of the model training process by introducing gradient penalty and integrating an additional loss term into the generator loss. The experiment demonstrates from multiple perspectives that our model-generated synthesizing CT(sCT) has significant advantages compared to other unsupervised learning models, thereby validating the clinical applicability and robustness of our model. In future clinical practice, our model has the potential to assist clinical practitioners in formulating precise radiotherapy plans.",
        "keywords": []
      },
      "file_name": "2104eea2b01b7a4b6ba2d256769328ac4919afad.pdf"
    },
    {
      "success": true,
      "doc_id": "3c4b4a7b9d8a16128b87e57aec583037",
      "summary": "Lung and colon cancer (LCC) is a dominant life-threatening disease that needs timely attention and precise diagnosis for efficient treatment. The conventional diagnostic techniques for LCC regularly encounter constraints in terms of efficiency and accuracy, thus causing challenges in primary recognition and treatment. Early diagnosis of the disease can immensely reduce the probability of death. In medical practice, the histopathological study of the tissue samples generally uses a classical model. Still, the automated devices that exploit artificial intelligence (AI) techniques produce efficient results in disease diagnosis. In histopathology, both machine learning (ML) and deep learning (DL) approaches can be deployed owing to their latent ability in analyzing and predicting physically accurate molecular phenotypes and microsatellite uncertainty. In this background, this study presents a novel technique called Lung and Colon Cancer using a Swin Transformer with an Ensemble Model on the Histopathological Images (LCCST-EMHI). The proposed LCCST-EMHI method focuses on designing a DL model for the diagnosis and classification of the LCC using histopathological images (HI). In order to achieve this, the LCCST-EMHI model utilizes the bilateral filtering (BF) technique to get rid of the noise. Further, the Swin Transformer (ST) model is also employed for the purpose of feature extraction. For the LCC detection and classification process, an ensemble deep learning classifier is used with three techniques: bidirectional long short-term memory with multi-head attention (BiLSTM-MHA), Double Deep Q-Network (DDQN), and sparse stacked autoencoder (SSAE). Eventually, the hyperparameter selection of the three DL models can be implemented utilizing the walrus optimization algorithm (WaOA) method. In order to illustrate the promising performance of the LCCST-EMHI approach, an extensive range of simulation analyses was conducted on a benchmark dataset. The experimentation results demonstrated the promising performance of the LCCST-EMHI approach over other recent methods.",
      "intriguing_abstract": "Lung and colon cancer (LCC) is a dominant life-threatening disease that needs timely attention and precise diagnosis for efficient treatment. The conventional diagnostic techniques for LCC regularly encounter constraints in terms of efficiency and accuracy, thus causing challenges in primary recognition and treatment. Early diagnosis of the disease can immensely reduce the probability of death. In medical practice, the histopathological study of the tissue samples generally uses a classical model. Still, the automated devices that exploit artificial intelligence (AI) techniques produce efficient results in disease diagnosis. In histopathology, both machine learning (ML) and deep learning (DL) approaches can be deployed owing to their latent ability in analyzing and predicting physically accurate molecular phenotypes and microsatellite uncertainty. In this background, this study presents a novel technique called Lung and Colon Cancer using a Swin Transformer with an Ensemble Model on the Histopathological Images (LCCST-EMHI). The proposed LCCST-EMHI method focuses on designing a DL model for the diagnosis and classification of the LCC using histopathological images (HI). In order to achieve this, the LCCST-EMHI model utilizes the bilateral filtering (BF) technique to get rid of the noise. Further, the Swin Transformer (ST) model is also employed for the purpose of feature extraction. For the LCC detection and classification process, an ensemble deep learning classifier is used with three techniques: bidirectional long short-term memory with multi-head attention (BiLSTM-MHA), Double Deep Q-Network (DDQN), and sparse stacked autoencoder (SSAE). Eventually, the hyperparameter selection of the three DL models can be implemented utilizing the walrus optimization algorithm (WaOA) method. In order to illustrate the promising performance of the LCCST-EMHI approach, an extensive range of simulation analyses was conducted on a benchmark dataset. The experimentation results demonstrated the promising performance of the LCCST-EMHI approach over other recent methods.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/8972ebf4f9ec47ee60c3ed0effd520daff495d5a.pdf",
      "citation_key": "alsulami2024ffb",
      "metadata": {
        "title": "Identification of Anomalies in Lung and Colon Cancer Using Computer Vision-Based Swin Transformer with Ensemble Model on Histopathological Images",
        "authors": [
          "Abdulkream A Alsulami",
          "Aishah Albarakati",
          "A. A. Al-Ghamdi",
          "Mahmoud Ragab"
        ],
        "published_date": "2024",
        "abstract": "Lung and colon cancer (LCC) is a dominant life-threatening disease that needs timely attention and precise diagnosis for efficient treatment. The conventional diagnostic techniques for LCC regularly encounter constraints in terms of efficiency and accuracy, thus causing challenges in primary recognition and treatment. Early diagnosis of the disease can immensely reduce the probability of death. In medical practice, the histopathological study of the tissue samples generally uses a classical model. Still, the automated devices that exploit artificial intelligence (AI) techniques produce efficient results in disease diagnosis. In histopathology, both machine learning (ML) and deep learning (DL) approaches can be deployed owing to their latent ability in analyzing and predicting physically accurate molecular phenotypes and microsatellite uncertainty. In this background, this study presents a novel technique called Lung and Colon Cancer using a Swin Transformer with an Ensemble Model on the Histopathological Images (LCCST-EMHI). The proposed LCCST-EMHI method focuses on designing a DL model for the diagnosis and classification of the LCC using histopathological images (HI). In order to achieve this, the LCCST-EMHI model utilizes the bilateral filtering (BF) technique to get rid of the noise. Further, the Swin Transformer (ST) model is also employed for the purpose of feature extraction. For the LCC detection and classification process, an ensemble deep learning classifier is used with three techniques: bidirectional long short-term memory with multi-head attention (BiLSTM-MHA), Double Deep Q-Network (DDQN), and sparse stacked autoencoder (SSAE). Eventually, the hyperparameter selection of the three DL models can be implemented utilizing the walrus optimization algorithm (WaOA) method. In order to illustrate the promising performance of the LCCST-EMHI approach, an extensive range of simulation analyses was conducted on a benchmark dataset. The experimentation results demonstrated the promising performance of the LCCST-EMHI approach over other recent methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/8972ebf4f9ec47ee60c3ed0effd520daff495d5a.pdf",
        "venue": "Bioengineering",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Lung and colon cancer (LCC) is a dominant life-threatening disease that needs timely attention and precise diagnosis for efficient treatment. The conventional diagnostic techniques for LCC regularly encounter constraints in terms of efficiency and accuracy, thus causing challenges in primary recognition and treatment. Early diagnosis of the disease can immensely reduce the probability of death. In medical practice, the histopathological study of the tissue samples generally uses a classical model. Still, the automated devices that exploit artificial intelligence (AI) techniques produce efficient results in disease diagnosis. In histopathology, both machine learning (ML) and deep learning (DL) approaches can be deployed owing to their latent ability in analyzing and predicting physically accurate molecular phenotypes and microsatellite uncertainty. In this background, this study presents a novel technique called Lung and Colon Cancer using a Swin Transformer with an Ensemble Model on the Histopathological Images (LCCST-EMHI). The proposed LCCST-EMHI method focuses on designing a DL model for the diagnosis and classification of the LCC using histopathological images (HI). In order to achieve this, the LCCST-EMHI model utilizes the bilateral filtering (BF) technique to get rid of the noise. Further, the Swin Transformer (ST) model is also employed for the purpose of feature extraction. For the LCC detection and classification process, an ensemble deep learning classifier is used with three techniques: bidirectional long short-term memory with multi-head attention (BiLSTM-MHA), Double Deep Q-Network (DDQN), and sparse stacked autoencoder (SSAE). Eventually, the hyperparameter selection of the three DL models can be implemented utilizing the walrus optimization algorithm (WaOA) method. In order to illustrate the promising performance of the LCCST-EMHI approach, an extensive range of simulation analyses was conducted on a benchmark dataset. The experimentation results demonstrated the promising performance of the LCCST-EMHI approach over other recent methods.",
        "keywords": []
      },
      "file_name": "8972ebf4f9ec47ee60c3ed0effd520daff495d5a.pdf"
    },
    {
      "success": true,
      "doc_id": "ea8a9721db71a67d48d4df194010e68b",
      "summary": "Traditional traffic classification methods struggle to identify emerging network traffic due to the need for model retraining, which hampers the real-time response of deployed edge devices. Moreover, emerging network traffic samples are often scarce, and traditional methods often treat a session as a single image, thereby overlooking essential structural features. These factors can result in poor generalization ability of the trained model. To overcome these challenges, we propose ConViTML (Convolutional Vision Transformer-based Meta-Learning), a real-time end-to-end network traffic classification framework that employs meta-learning to avoid model retraining. We propose a novel feature extraction network, Convolutional Visual Transformer (ConViT), merging Convolutional Neural Network (CNN) and Visual Transformer (ViT). ConViT can directly extract low-dimensional discriminative features containing basic and structural features of the session, which is vital for improving detection accuracy and accelerating convergence in a data-scarce environment. Furthermore, we employ a Packet-based Relation Network (PRN) to analyze the matching degree of support samples and query samples. Therefore, accurate classification in novel traffic identification tasks can be achieved with just a few labeled samples, eliminating extensive data collection and labeling operations. Finally, we replace various feature extractors and compare our approach with the classic meta-learning framework Relation Network (RelationNet). Extensive experimental results demonstrate that ConViTML outperforms others with various performance indicators.",
      "intriguing_abstract": "Traditional traffic classification methods struggle to identify emerging network traffic due to the need for model retraining, which hampers the real-time response of deployed edge devices. Moreover, emerging network traffic samples are often scarce, and traditional methods often treat a session as a single image, thereby overlooking essential structural features. These factors can result in poor generalization ability of the trained model. To overcome these challenges, we propose ConViTML (Convolutional Vision Transformer-based Meta-Learning), a real-time end-to-end network traffic classification framework that employs meta-learning to avoid model retraining. We propose a novel feature extraction network, Convolutional Visual Transformer (ConViT), merging Convolutional Neural Network (CNN) and Visual Transformer (ViT). ConViT can directly extract low-dimensional discriminative features containing basic and structural features of the session, which is vital for improving detection accuracy and accelerating convergence in a data-scarce environment. Furthermore, we employ a Packet-based Relation Network (PRN) to analyze the matching degree of support samples and query samples. Therefore, accurate classification in novel traffic identification tasks can be achieved with just a few labeled samples, eliminating extensive data collection and labeling operations. Finally, we replace various feature extractors and compare our approach with the classic meta-learning framework Relation Network (RelationNet). Extensive experimental results demonstrate that ConViTML outperforms others with various performance indicators.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/0fde8e3287ac04dcf210b73898c8ceb697ad489f.pdf",
      "citation_key": "yang2024wxl",
      "metadata": {
        "title": "ConViTML: A Convolutional Vision Transformer-Based Meta-Learning Framework for Real-Time Edge Network Traffic Classification",
        "authors": [
          "Lu Yang",
          "Songtao Guo",
          "Defang Liu",
          "Yue Zeng",
          "Xianlong Jiao",
          "Yuhao Zhou"
        ],
        "published_date": "2024",
        "abstract": "Traditional traffic classification methods struggle to identify emerging network traffic due to the need for model retraining, which hampers the real-time response of deployed edge devices. Moreover, emerging network traffic samples are often scarce, and traditional methods often treat a session as a single image, thereby overlooking essential structural features. These factors can result in poor generalization ability of the trained model. To overcome these challenges, we propose ConViTML (Convolutional Vision Transformer-based Meta-Learning), a real-time end-to-end network traffic classification framework that employs meta-learning to avoid model retraining. We propose a novel feature extraction network, Convolutional Visual Transformer (ConViT), merging Convolutional Neural Network (CNN) and Visual Transformer (ViT). ConViT can directly extract low-dimensional discriminative features containing basic and structural features of the session, which is vital for improving detection accuracy and accelerating convergence in a data-scarce environment. Furthermore, we employ a Packet-based Relation Network (PRN) to analyze the matching degree of support samples and query samples. Therefore, accurate classification in novel traffic identification tasks can be achieved with just a few labeled samples, eliminating extensive data collection and labeling operations. Finally, we replace various feature extractors and compare our approach with the classic meta-learning framework Relation Network (RelationNet). Extensive experimental results demonstrate that ConViTML outperforms others with various performance indicators.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0fde8e3287ac04dcf210b73898c8ceb697ad489f.pdf",
        "venue": "IEEE Transactions on Network and Service Management",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Traditional traffic classification methods struggle to identify emerging network traffic due to the need for model retraining, which hampers the real-time response of deployed edge devices. Moreover, emerging network traffic samples are often scarce, and traditional methods often treat a session as a single image, thereby overlooking essential structural features. These factors can result in poor generalization ability of the trained model. To overcome these challenges, we propose ConViTML (Convolutional Vision Transformer-based Meta-Learning), a real-time end-to-end network traffic classification framework that employs meta-learning to avoid model retraining. We propose a novel feature extraction network, Convolutional Visual Transformer (ConViT), merging Convolutional Neural Network (CNN) and Visual Transformer (ViT). ConViT can directly extract low-dimensional discriminative features containing basic and structural features of the session, which is vital for improving detection accuracy and accelerating convergence in a data-scarce environment. Furthermore, we employ a Packet-based Relation Network (PRN) to analyze the matching degree of support samples and query samples. Therefore, accurate classification in novel traffic identification tasks can be achieved with just a few labeled samples, eliminating extensive data collection and labeling operations. Finally, we replace various feature extractors and compare our approach with the classic meta-learning framework Relation Network (RelationNet). Extensive experimental results demonstrate that ConViTML outperforms others with various performance indicators.",
        "keywords": []
      },
      "file_name": "0fde8e3287ac04dcf210b73898c8ceb697ad489f.pdf"
    },
    {
      "success": true,
      "doc_id": "98f15cdf49d27a9f650ad7cac8e7d53f",
      "summary": "Here's a focused summary of the paper \\cite{p2024nbn} for a literature review:\n\n### Focused Summary for Literature Review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of accurately and efficiently classifying groundnut crop pests (specifically Thrips, Aphids, Armyworm, and Wireworm) using image-based methods.\n    *   **Importance and Challenge:** Groundnut crops are vital for food security and economic stability, but pest infestations significantly reduce yields and quality. Traditional pest identification methods are labor-intensive, time-consuming, often inaccurate, and difficult to scale across large farming landscapes. Existing Convolutional Neural Network (CNN) based solutions, while an improvement, suffer from limitations like translation invariance, locality sensitivity, and a lack of global understanding in images, leading to potential misdiagnosis in dynamic agricultural environments.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds upon advancements in computer vision and deep learning for agricultural applications, particularly image processing and recognition for crop diagnosis and pest identification. It acknowledges the success of CNNs in this domain (e.g., achieving 99.58% accuracy for groundnut diseases with JIUUs, 99.73% with ResNet-93, and 99.1% with other CNNs).\n    *   **Limitations of Previous Solutions:**\n        *   **Traditional Methods:** Relied on human expertise, labor-intensive, time-consuming, and prone to inaccuracies, especially when scaled.\n        *   **Conventional Image Processing:** Often accompanied by manually crafted features, struggled with unpredictable natural environments (varying light, shadows, inconsistent backgrounds), making them resource-intensive and less effective.\n        *   **CNNs:** While powerful, they primarily focus on local features and can suffer from issues like translation invariance and locality sensitivity, potentially missing global contextual information crucial for distinguishing subtle pest differences. This limits their generalizability and robustness in diverse agricultural settings.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes **MU^t\\**, an enhanced Vision Transformer (ViT) model, for groundnut pest classification. The approach involves:\n        *   **Data Pre-processing:** Resizing images to 224x224 pixels and normalization.\n        *   **Extensive Data Augmentation:** Employing random horizontal flips, rotations (up to 10 degrees), zooms (up to 10%), and brightness adjustments (up to 20%) to increase dataset diversity and model robustness.\n        *   **Transfer Learning with ViT:** Utilizing a pre-trained ViT model (adapted from PyTorch, likely pre-trained on ImageNet) and fine-tuning its final classification layer to match the four specific groundnut pest classes.\n        *   **Training:** Using Cross-Entropy Loss and the Adam optimizer with a learning rate of 0.0001 over 50 epochs with a batch size of 32.\n    *   **Novelty/Difference:** The core novelty lies in effectively leveraging the **Vision Transformer (ViT)** architecture, which is inherently designed to capture global dependencies and contextual information across an entire image, unlike CNNs that process local features hierarchically. This global understanding, combined with a robust data augmentation strategy and transfer learning, allows MU^t\\ to overcome the limitations of CNNs in distinguishing subtle pest characteristics and adapting to varied environmental conditions. The paper specifically highlights the ViT's ability to capture long-range dependencies, which is a key differentiator from traditional CNNs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method:** The **MU^t\\ model**, an optimized Vision Transformer architecture specifically tailored for groundnut pest classification.\n    *   **Enhanced ViT Architecture:** The adaptation and fine-tuning of a pre-trained ViT model, particularly its final layer, to precisely classify four groundnut pest types.\n    *   **Robust Data Augmentation Strategy:** Implementation of a comprehensive data augmentation pipeline (flips, rotations, zooms, brightness adjustments) to significantly enhance the diversity and robustness of the training dataset, crucial for real-world applicability.\n    *   **Effective Transfer Learning Application:** Demonstrating the efficacy of transfer learning from a large general image dataset (ImageNet) to a specialized agricultural task with a relatively smaller dataset, leading to high accuracy and efficiency.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The MU^t\\ model was trained, validated, and tested on the **GW435 dataset**, which comprises 7495 images of groundnut pests categorized into four classes: Thrips (2798 images), Aphids (965 images), Armyworm (875 images), and Wireworm (957 images). The dataset was split into 80% for training, 10% for validation, and 10% for testing.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   The MU^t\\ model achieved an exceptional **accuracy of 99.95%**.\n        *   All other key metrics also reported **99.95%**: Precision, Recall, and F1-Score.\n        *   **Comparison with other methods** (from literature review):\n            *   MU^t\\ (ViT): **99.95%**\n            *   ResNet-93 \\cite{p2024nbn}: 99.73%\n            *   CNN \\cite{p2024nbn}: 98.68%\n            *   JIUU \\cite{p2024nbn}: 99.58%\n            *   SVM \\cite{p2024nbn}: 99%\n            *   R-UNet \\cite{p2024nbn}: 99%\n            *   CNN \\cite{p2024nbn}: 99.1%\n        *   The MU^t\\ model **significantly outperformed** all compared existing models, demonstrating its superior robustness and effectiveness.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The study primarily focuses on four specific groundnut pests.\n        *   While achieving high accuracy, the paper acknowledges that real-time applicability and optimal accuracy challenges can persist in highly dynamic environments.\n        *   Future work mentions investigating robustness against adversarial attacks, implying this is a current limitation or an area needing further exploration.\n        *   The dataset, while substantial, is specific to groundnut pests, and generalizability to other crops or a wider range of pests would require further validation.\n    *   **Scope of Applicability:** The model is specifically developed for image-based classification of groundnut pests. Its immediate applicability is in groundnut farming for early and accurate pest detection to facilitate timely management.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** The MU^t\\ model significantly advances the technical state-of-the-art in agricultural pest classification by demonstrating that Vision Transformers, when appropriately enhanced and fine-tuned with robust data augmentation, can achieve near-perfect accuracy (99.95%) for specific crop pest identification, outperforming traditional CNN-based methods. This highlights the superior capability of ViTs in capturing global image context for fine-grained classification tasks in complex natural settings.\n    *   **Potential Impact on Future Research:**\n        *   Encourages further exploration of ViT architectures and their hybridizations with other deep learning models for diverse agricultural challenges.\n        *   Promotes research into integrating such high-accuracy models with real-time detection systems using edge computing for practical, on-field deployment.\n        *   Motivates studies on model robustness against adversarial attacks and expanding datasets to cover a broader spectrum of pests and environmental conditions to enhance generalizability.\n        *   Contributes to the broader goal of sustainable agriculture by enabling more precise and timely pest management, thereby reducing crop losses and enhancing global food security.",
      "intriguing_abstract": "Groundnut crops, vital for global food security, face devastating yield losses from pests like Thrips, Aphids, Armyworm, and Wireworm. Traditional identification methods are labor-intensive and prone to inaccuracy, while existing Convolutional Neural Networks (CNNs) often struggle with locality sensitivity and a lack of global contextual understanding, risking misdiagnosis in dynamic agricultural environments. This paper introduces **MU^t\\**, a novel and highly effective Vision Transformer (ViT) model, specifically engineered to overcome these limitations for groundnut pest classification.\n\nUnlike CNNs, MU^t\\ leverages the inherent ability of ViTs to capture long-range dependencies and global image context across an entire image. Through an optimized architecture, robust data augmentation, and transfer learning, MU^t\\ achieves an unprecedented 99.95% accuracy, precision, recall, and F1-score on the GW435 dataset, significantly outperforming state-of-the-art CNN-based approaches. This breakthrough in image-based pest identification offers a powerful, scalable solution, poised to revolutionize agricultural pest management, enhance crop yields, and bolster sustainable farming practices worldwide.",
      "keywords": [
        "Groundnut pest classification",
        "Vision Transformer (ViT)",
        "MU^t\\ model",
        "Image-based pest identification",
        "Deep learning",
        "Data augmentation",
        "Transfer learning",
        "Global contextual information",
        "Convolutional Neural Network (CNN) limitations",
        "99.95% accuracy",
        "Agricultural applications",
        "Sustainable agriculture",
        "GW435 dataset"
      ],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/9a4718faa07a32cf1dce745062181d3342e9b054.pdf",
      "citation_key": "p2024nbn",
      "metadata": {
        "title": "GNViT- An enhanced image-based groundnut pest classification using Vision Transformer (ViT) model",
        "authors": [
          "Venkatasaichandrakanth P",
          "I. M"
        ],
        "published_date": "2024",
        "abstract": "Crop losses caused by diseases and pests present substantial challenges to global agriculture, with groundnut crops particularly vulnerable to their detrimental effects. This study introduces the Groundnut Vision Transformer (GNViT) model, a novel approach that harnesses a pre-trained Vision Transformer (ViT) on the ImageNet dataset. The primary goal is to detect and classify various pests affecting groundnut crops. Rigorous training and evaluation were conducted using a comprehensive dataset from IP102, encompassing pests such as Thrips, Aphids, Armyworms, and Wireworms. The GNViT models effectiveness was assessed using reliability metrics, including the F1-score, recall, and overall accuracy. Data augmentation with GNViT resulted in a significant increase in training accuracy, achieving 99.52%. Comparative analysis highlighted the GNViT models superior performance, particularly in accuracy, compared to state-of-the-art methodologies. These findings underscore the potential of deep learning models, such as GNViT, in providing reliable pest classification solutions for groundnut crops. The deployment of advanced technological solutions brings us closer to the overarching goal of reducing crop losses and enhancing global food security for the growing population.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9a4718faa07a32cf1dce745062181d3342e9b054.pdf",
        "venue": "PLoS ONE",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Here's a focused summary of the paper \\cite{p2024nbn} for a literature review:\n\n### Focused Summary for Literature Review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of accurately and efficiently classifying groundnut crop pests (specifically Thrips, Aphids, Armyworm, and Wireworm) using image-based methods.\n    *   **Importance and Challenge:** Groundnut crops are vital for food security and economic stability, but pest infestations significantly reduce yields and quality. Traditional pest identification methods are labor-intensive, time-consuming, often inaccurate, and difficult to scale across large farming landscapes. Existing Convolutional Neural Network (CNN) based solutions, while an improvement, suffer from limitations like translation invariance, locality sensitivity, and a lack of global understanding in images, leading to potential misdiagnosis in dynamic agricultural environments.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds upon advancements in computer vision and deep learning for agricultural applications, particularly image processing and recognition for crop diagnosis and pest identification. It acknowledges the success of CNNs in this domain (e.g., achieving 99.58% accuracy for groundnut diseases with JIUUs, 99.73% with ResNet-93, and 99.1% with other CNNs).\n    *   **Limitations of Previous Solutions:**\n        *   **Traditional Methods:** Relied on human expertise, labor-intensive, time-consuming, and prone to inaccuracies, especially when scaled.\n        *   **Conventional Image Processing:** Often accompanied by manually crafted features, struggled with unpredictable natural environments (varying light, shadows, inconsistent backgrounds), making them resource-intensive and less effective.\n        *   **CNNs:** While powerful, they primarily focus on local features and can suffer from issues like translation invariance and locality sensitivity, potentially missing global contextual information crucial for distinguishing subtle pest differences. This limits their generalizability and robustness in diverse agricultural settings.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes **MU^t\\**, an enhanced Vision Transformer (ViT) model, for groundnut pest classification. The approach involves:\n        *   **Data Pre-processing:** Resizing images to 224x224 pixels and normalization.\n        *   **Extensive Data Augmentation:** Employing random horizontal flips, rotations (up to 10 degrees), zooms (up to 10%), and brightness adjustments (up to 20%) to increase dataset diversity and model robustness.\n        *   **Transfer Learning with ViT:** Utilizing a pre-trained ViT model (adapted from PyTorch, likely pre-trained on ImageNet) and fine-tuning its final classification layer to match the four specific groundnut pest classes.\n        *   **Training:** Using Cross-Entropy Loss and the Adam optimizer with a learning rate of 0.0001 over 50 epochs with a batch size of 32.\n    *   **Novelty/Difference:** The core novelty lies in effectively leveraging the **Vision Transformer (ViT)** architecture, which is inherently designed to capture global dependencies and contextual information across an entire image, unlike CNNs that process local features hierarchically. This global understanding, combined with a robust data augmentation strategy and transfer learning, allows MU^t\\ to overcome the limitations of CNNs in distinguishing subtle pest characteristics and adapting to varied environmental conditions. The paper specifically highlights the ViT's ability to capture long-range dependencies, which is a key differentiator from traditional CNNs.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method:** The **MU^t\\ model**, an optimized Vision Transformer architecture specifically tailored for groundnut pest classification.\n    *   **Enhanced ViT Architecture:** The adaptation and fine-tuning of a pre-trained ViT model, particularly its final layer, to precisely classify four groundnut pest types.\n    *   **Robust Data Augmentation Strategy:** Implementation of a comprehensive data augmentation pipeline (flips, rotations, zooms, brightness adjustments) to significantly enhance the diversity and robustness of the training dataset, crucial for real-world applicability.\n    *   **Effective Transfer Learning Application:** Demonstrating the efficacy of transfer learning from a large general image dataset (ImageNet) to a specialized agricultural task with a relatively smaller dataset, leading to high accuracy and efficiency.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The MU^t\\ model was trained, validated, and tested on the **GW435 dataset**, which comprises 7495 images of groundnut pests categorized into four classes: Thrips (2798 images), Aphids (965 images), Armyworm (875 images), and Wireworm (957 images). The dataset was split into 80% for training, 10% for validation, and 10% for testing.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   The MU^t\\ model achieved an exceptional **accuracy of 99.95%**.\n        *   All other key metrics also reported **99.95%**: Precision, Recall, and F1-Score.\n        *   **Comparison with other methods** (from literature review):\n            *   MU^t\\ (ViT): **99.95%**\n            *   ResNet-93 \\cite{p2024nbn}: 99.73%\n            *   CNN \\cite{p2024nbn}: 98.68%\n            *   JIUU \\cite{p2024nbn}: 99.58%\n            *   SVM \\cite{p2024nbn}: 99%\n            *   R-UNet \\cite{p2024nbn}: 99%\n            *   CNN \\cite{p2024nbn}: 99.1%\n        *   The MU^t\\ model **significantly outperformed** all compared existing models, demonstrating its superior robustness and effectiveness.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The study primarily focuses on four specific groundnut pests.\n        *   While achieving high accuracy, the paper acknowledges that real-time applicability and optimal accuracy challenges can persist in highly dynamic environments.\n        *   Future work mentions investigating robustness against adversarial attacks, implying this is a current limitation or an area needing further exploration.\n        *   The dataset, while substantial, is specific to groundnut pests, and generalizability to other crops or a wider range of pests would require further validation.\n    *   **Scope of Applicability:** The model is specifically developed for image-based classification of groundnut pests. Its immediate applicability is in groundnut farming for early and accurate pest detection to facilitate timely management.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** The MU^t\\ model significantly advances the technical state-of-the-art in agricultural pest classification by demonstrating that Vision Transformers, when appropriately enhanced and fine-tuned with robust data augmentation, can achieve near-perfect accuracy (99.95%) for specific crop pest identification, outperforming traditional CNN-based methods. This highlights the superior capability of ViTs in capturing global image context for fine-grained classification tasks in complex natural settings.\n    *   **Potential Impact on Future Research:**\n        *   Encourages further exploration of ViT architectures and their hybridizations with other deep learning models for diverse agricultural challenges.\n        *   Promotes research into integrating such high-accuracy models with real-time detection systems using edge computing for practical, on-field deployment.\n        *   Motivates studies on model robustness against adversarial attacks and expanding datasets to cover a broader spectrum of pests and environmental conditions to enhance generalizability.\n        *   Contributes to the broader goal of sustainable agriculture by enabling more precise and timely pest management, thereby reducing crop losses and enhancing global food security.",
        "keywords": [
          "Groundnut pest classification",
          "Vision Transformer (ViT)",
          "MU^t\\ model",
          "Image-based pest identification",
          "Deep learning",
          "Data augmentation",
          "Transfer learning",
          "Global contextual information",
          "Convolutional Neural Network (CNN) limitations",
          "99.95% accuracy",
          "Agricultural applications",
          "Sustainable agriculture",
          "GW435 dataset"
        ],
        "paper_type": "the garbled text, once deciphered, reveals the core content.\n\n**deciphered abstract snippets:**\n*   \"this paper proposes an enhanced image-based groundnut pest classification using vision transformer (vit) model.\"\n*   \"the proposed gnvit model effectively classifies groundnut pests.\"\n*   \"the findings demonstrate the effectiveness and reliability of the gnvit model.\"\n\n**deciphered introduction snippets:**\n*   \"the development of advanced technological solutions is crucial for addressing the global food security challenge.\"\n*   \"this paper proposes an enhanced image-based groundnut pest classification using vision transformer (vit) model.\"\n*   \"however, recent trends indicate a deceleration in agricultural production growth, raising concerns due to escalating challenges like climate change, population boom, and urban migration.\" (setting up the problem)\n\n**reasoning:**\nthe abstract and introduction repeatedly use phrases like \"proposes an enhanced... model\" and discuss the \"proposed gnvit model.\" they identify a technical problem (pest classification for groundnuts to address food security) and present a new solution (the gnvit model based on vision transformer). while \"findings\" are mentioned, they are in the context of demonstrating the effectiveness of the *proposed model*, which is typical for validating a new technical contribution.\n\nthis aligns perfectly with the criteria for a **technical** paper:\n*   **abstract mentions:** \"propose\", \"develop\", \"present\", \"algorithm\", \"method\" (implied by \"model\")\n*   **introduction discusses:** \"technical problem\", \"proposed solution\"\n\ntherefore, this paper is a **technical** paper.\n\n**classification:** technical"
      },
      "file_name": "9a4718faa07a32cf1dce745062181d3342e9b054.pdf"
    },
    {
      "success": true,
      "doc_id": "f2fcf1415d72e8a2633d453e6c3a8264",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/6e97c1ba023afc87c1b99881f631af8146230d96.pdf",
      "citation_key": "wu2024tsm",
      "metadata": {
        "title": "A new ECT image reconstruction algorithm based on Vision transformer (ViT)",
        "authors": [
          "Xinhao Wu",
          "Sirui Xu",
          "Ming-Yu Gao",
          "Yan-Dong Liu",
          "Shiwei Liu",
          "Hua Yan",
          "Yan Wang"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6e97c1ba023afc87c1b99881f631af8146230d96.pdf",
        "venue": "Flow Measurement and Instrumentation",
        "citationCount": 8,
        "score": 8.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "6e97c1ba023afc87c1b99881f631af8146230d96.pdf"
    },
    {
      "success": true,
      "doc_id": "18646bb4d57921bf0d7e633285e0724a",
      "summary": "Swin Transformer achieves greater efficiency than Vision Transformer by utilizing local self-attention and shifted windows. However, existing hardware accelerators designed for Transformer have not been optimized for the unique computation flow and data reuse property in Swin Transformer, resulting in lower hardware utilization and extra memory accesses. To address this issue, we develop SWAT, an efficient Swin Transformer Accelerator based on FPGA. Firstly, to eliminate the redundant computations in shifted windows, a novel tiling strategy is employed, which helps the developed multiplier array to fully utilize the sparsity. Additionally, we deploy a dynamic pipeline interleaving dataflow, which not only reduces the processing latency but also maximizes data reuse, thereby decreasing access to memories. Furthermore, customized quantization strategies and approximate calculations for non-linear calculations are adopted to simplify the hardware complexity with negligible network accuracy loss. We implement SWAT on the Xilinx Alveo U50 platform and evaluate it with Swin-T on the ImageNet dataset. The proposed architecture can achieve improvements of $2.02 \\times \\sim 3.11 \\times$ in power efficiency compared to existing Transformer accelerators on FPGAs.",
      "intriguing_abstract": "Swin Transformer achieves greater efficiency than Vision Transformer by utilizing local self-attention and shifted windows. However, existing hardware accelerators designed for Transformer have not been optimized for the unique computation flow and data reuse property in Swin Transformer, resulting in lower hardware utilization and extra memory accesses. To address this issue, we develop SWAT, an efficient Swin Transformer Accelerator based on FPGA. Firstly, to eliminate the redundant computations in shifted windows, a novel tiling strategy is employed, which helps the developed multiplier array to fully utilize the sparsity. Additionally, we deploy a dynamic pipeline interleaving dataflow, which not only reduces the processing latency but also maximizes data reuse, thereby decreasing access to memories. Furthermore, customized quantization strategies and approximate calculations for non-linear calculations are adopted to simplify the hardware complexity with negligible network accuracy loss. We implement SWAT on the Xilinx Alveo U50 platform and evaluate it with Swin-T on the ImageNet dataset. The proposed architecture can achieve improvements of $2.02 \\times \\sim 3.11 \\times$ in power efficiency compared to existing Transformer accelerators on FPGAs.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/1d7183d481ae5a396743dde39984f1f0c8f47edf.pdf",
      "citation_key": "dong2024bm2",
      "metadata": {
        "title": "SWAT: An Efficient Swin Transformer Accelerator Based on FPGA",
        "authors": [
          "Qiwei Dong",
          "Xiaoru Xie",
          "Zhongfeng Wang"
        ],
        "published_date": "2024",
        "abstract": "Swin Transformer achieves greater efficiency than Vision Transformer by utilizing local self-attention and shifted windows. However, existing hardware accelerators designed for Transformer have not been optimized for the unique computation flow and data reuse property in Swin Transformer, resulting in lower hardware utilization and extra memory accesses. To address this issue, we develop SWAT, an efficient Swin Transformer Accelerator based on FPGA. Firstly, to eliminate the redundant computations in shifted windows, a novel tiling strategy is employed, which helps the developed multiplier array to fully utilize the sparsity. Additionally, we deploy a dynamic pipeline interleaving dataflow, which not only reduces the processing latency but also maximizes data reuse, thereby decreasing access to memories. Furthermore, customized quantization strategies and approximate calculations for non-linear calculations are adopted to simplify the hardware complexity with negligible network accuracy loss. We implement SWAT on the Xilinx Alveo U50 platform and evaluate it with Swin-T on the ImageNet dataset. The proposed architecture can achieve improvements of $2.02 \\times \\sim 3.11 \\times$ in power efficiency compared to existing Transformer accelerators on FPGAs.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1d7183d481ae5a396743dde39984f1f0c8f47edf.pdf",
        "venue": "Asia and South Pacific Design Automation Conference",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Swin Transformer achieves greater efficiency than Vision Transformer by utilizing local self-attention and shifted windows. However, existing hardware accelerators designed for Transformer have not been optimized for the unique computation flow and data reuse property in Swin Transformer, resulting in lower hardware utilization and extra memory accesses. To address this issue, we develop SWAT, an efficient Swin Transformer Accelerator based on FPGA. Firstly, to eliminate the redundant computations in shifted windows, a novel tiling strategy is employed, which helps the developed multiplier array to fully utilize the sparsity. Additionally, we deploy a dynamic pipeline interleaving dataflow, which not only reduces the processing latency but also maximizes data reuse, thereby decreasing access to memories. Furthermore, customized quantization strategies and approximate calculations for non-linear calculations are adopted to simplify the hardware complexity with negligible network accuracy loss. We implement SWAT on the Xilinx Alveo U50 platform and evaluate it with Swin-T on the ImageNet dataset. The proposed architecture can achieve improvements of $2.02 \\times \\sim 3.11 \\times$ in power efficiency compared to existing Transformer accelerators on FPGAs.",
        "keywords": []
      },
      "file_name": "1d7183d481ae5a396743dde39984f1f0c8f47edf.pdf"
    },
    {
      "success": true,
      "doc_id": "fb1b30a075d78dae041c8552facbfbbf",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/9fbf36da9ee60d56a7675042bb0b24798a3b966d.pdf",
      "citation_key": "swapno2025y2b",
      "metadata": {
        "title": "ViT-SENet-Tom: machine learning-based novel hybrid squeeze-excitation network and vision transformer framework for tomato fruits classification",
        "authors": [
          "S. M. M. Swapno",
          "S. N. Nobel",
          "Md Babul Islam",
          "Pronaya Bhattacharya",
          "Ebrahim A. Mattar"
        ],
        "published_date": "2025",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9fbf36da9ee60d56a7675042bb0b24798a3b966d.pdf",
        "venue": "Neural computing & applications (Print)",
        "citationCount": 8,
        "score": 8.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "9fbf36da9ee60d56a7675042bb0b24798a3b966d.pdf"
    },
    {
      "success": true,
      "doc_id": "8da14a5102bdd0160731df2a3447d28e",
      "summary": "The vision transformer (ViT) with global self-attention exhibits quadratic computational complexity that depends on the image size. To address this issue, window-based self-attention ViT limits attention area to a specific window, thereby mitigating the computational complexity. However, it cannot effectively capture the relationships between windows. The Swin Transformer, a representative window-based self-attention ViT, introduces shifted-window multi-head self-attention (SW-MSA) to capture the cross-window information. However, SW-MSA groups tokens that are close to each other in the image into one window and thus cannot capture relationships between distant tokens. Therefore, this paper introduces a feature-space window attention transformer (FSwin Transformer) that includes distant but similar tokens in one window. The proposed FSwin Transformer clusters similar tokens based on the feature space and conducts self-attention within the cluster. Thus, this approach helps understand the global context of the image by compensating for interactions between long-distance tokens, which cannot be captured when windows are set based on the image space. In addition, we incorporate a feature-space refinement method with channel and spatial attention to emphasize key parts and suppress non-essential parts. The refined feature map improves the representation power of the model, resulting in improved classification performance. Consequently, in classification tasks for ImageNet-1K, FSwin Transformer outperforms existing Transformer-based backbones, including the Swin Transformer.",
      "intriguing_abstract": "The vision transformer (ViT) with global self-attention exhibits quadratic computational complexity that depends on the image size. To address this issue, window-based self-attention ViT limits attention area to a specific window, thereby mitigating the computational complexity. However, it cannot effectively capture the relationships between windows. The Swin Transformer, a representative window-based self-attention ViT, introduces shifted-window multi-head self-attention (SW-MSA) to capture the cross-window information. However, SW-MSA groups tokens that are close to each other in the image into one window and thus cannot capture relationships between distant tokens. Therefore, this paper introduces a feature-space window attention transformer (FSwin Transformer) that includes distant but similar tokens in one window. The proposed FSwin Transformer clusters similar tokens based on the feature space and conducts self-attention within the cluster. Thus, this approach helps understand the global context of the image by compensating for interactions between long-distance tokens, which cannot be captured when windows are set based on the image space. In addition, we incorporate a feature-space refinement method with channel and spatial attention to emphasize key parts and suppress non-essential parts. The refined feature map improves the representation power of the model, resulting in improved classification performance. Consequently, in classification tasks for ImageNet-1K, FSwin Transformer outperforms existing Transformer-based backbones, including the Swin Transformer.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d80166681f3344a1946b8bfc623f4679d979ee10.pdf",
      "citation_key": "yoo2024u1f",
      "metadata": {
        "title": "FSwin Transformer: Feature-Space Window Attention Vision Transformer for Image Classification",
        "authors": [
          "Dayeon Yoo",
          "Jeesu Kim",
          "Jinwoo Yoo"
        ],
        "published_date": "2024",
        "abstract": "The vision transformer (ViT) with global self-attention exhibits quadratic computational complexity that depends on the image size. To address this issue, window-based self-attention ViT limits attention area to a specific window, thereby mitigating the computational complexity. However, it cannot effectively capture the relationships between windows. The Swin Transformer, a representative window-based self-attention ViT, introduces shifted-window multi-head self-attention (SW-MSA) to capture the cross-window information. However, SW-MSA groups tokens that are close to each other in the image into one window and thus cannot capture relationships between distant tokens. Therefore, this paper introduces a feature-space window attention transformer (FSwin Transformer) that includes distant but similar tokens in one window. The proposed FSwin Transformer clusters similar tokens based on the feature space and conducts self-attention within the cluster. Thus, this approach helps understand the global context of the image by compensating for interactions between long-distance tokens, which cannot be captured when windows are set based on the image space. In addition, we incorporate a feature-space refinement method with channel and spatial attention to emphasize key parts and suppress non-essential parts. The refined feature map improves the representation power of the model, resulting in improved classification performance. Consequently, in classification tasks for ImageNet-1K, FSwin Transformer outperforms existing Transformer-based backbones, including the Swin Transformer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d80166681f3344a1946b8bfc623f4679d979ee10.pdf",
        "venue": "IEEE Access",
        "citationCount": 8,
        "score": 8.0,
        "summary": "The vision transformer (ViT) with global self-attention exhibits quadratic computational complexity that depends on the image size. To address this issue, window-based self-attention ViT limits attention area to a specific window, thereby mitigating the computational complexity. However, it cannot effectively capture the relationships between windows. The Swin Transformer, a representative window-based self-attention ViT, introduces shifted-window multi-head self-attention (SW-MSA) to capture the cross-window information. However, SW-MSA groups tokens that are close to each other in the image into one window and thus cannot capture relationships between distant tokens. Therefore, this paper introduces a feature-space window attention transformer (FSwin Transformer) that includes distant but similar tokens in one window. The proposed FSwin Transformer clusters similar tokens based on the feature space and conducts self-attention within the cluster. Thus, this approach helps understand the global context of the image by compensating for interactions between long-distance tokens, which cannot be captured when windows are set based on the image space. In addition, we incorporate a feature-space refinement method with channel and spatial attention to emphasize key parts and suppress non-essential parts. The refined feature map improves the representation power of the model, resulting in improved classification performance. Consequently, in classification tasks for ImageNet-1K, FSwin Transformer outperforms existing Transformer-based backbones, including the Swin Transformer.",
        "keywords": []
      },
      "file_name": "d80166681f3344a1946b8bfc623f4679d979ee10.pdf"
    },
    {
      "success": true,
      "doc_id": "da27cbb7d33f98b75ff5709faf0ad492",
      "summary": "Network intrusion detection technology has always been an indispensable protection mechanism for industrial network security. The rise of new forms of network attacks has resulted in a heightened demand for these technologies. Nevertheless, the current models effectiveness is subpar. We propose a new Deformable Vision Transformer (DE-VIT) method to address this issue. DE-VIT introduces a new deformable attention mechanism module, where the positions of key-value pairs in the attention mechanism are selected in a data-dependent manner, allowing it to focus on relevant areas, capture more informative features, and avoid excessive memory and computational costs. In addition to using deformable convolutions instead of regular convolutions in embedding layers to enhance the receptive field of patches, a sliding window mechanism is also employed to utilize edge information fully. In Parallel, we use a layered focal loss function to improve classification performance and address data imbalance issues. In summary, DE-VIT reduces computational complexity and achieves better results. We conduct experimental simulations on the public intrusion detection datasets, and the accuracy of the enhanced intrusion detection model surpasses that of the Deep Belief Network with Improved Kernel-Based Extreme Learning (DBN-KELM). It reaches 99.5% and 97.5% on the CIC IDS2017 and UNSW-NB15 datasets, exhibiting an increase of 8.5% and 9.1%, respectively.",
      "intriguing_abstract": "Network intrusion detection technology has always been an indispensable protection mechanism for industrial network security. The rise of new forms of network attacks has resulted in a heightened demand for these technologies. Nevertheless, the current models effectiveness is subpar. We propose a new Deformable Vision Transformer (DE-VIT) method to address this issue. DE-VIT introduces a new deformable attention mechanism module, where the positions of key-value pairs in the attention mechanism are selected in a data-dependent manner, allowing it to focus on relevant areas, capture more informative features, and avoid excessive memory and computational costs. In addition to using deformable convolutions instead of regular convolutions in embedding layers to enhance the receptive field of patches, a sliding window mechanism is also employed to utilize edge information fully. In Parallel, we use a layered focal loss function to improve classification performance and address data imbalance issues. In summary, DE-VIT reduces computational complexity and achieves better results. We conduct experimental simulations on the public intrusion detection datasets, and the accuracy of the enhanced intrusion detection model surpasses that of the Deep Belief Network with Improved Kernel-Based Extreme Learning (DBN-KELM). It reaches 99.5% and 97.5% on the CIC IDS2017 and UNSW-NB15 datasets, exhibiting an increase of 8.5% and 9.1%, respectively.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/9285996627124b945ec601a763f6ff884bac3281.pdf",
      "citation_key": "he2024m6j",
      "metadata": {
        "title": "Network Intrusion Detection Based on Feature Image and Deformable Vision Transformer Classification",
        "authors": [
          "Kan He",
          "Wei Zhang",
          "Xuejun Zong",
          "Lian Lian"
        ],
        "published_date": "2024",
        "abstract": "Network intrusion detection technology has always been an indispensable protection mechanism for industrial network security. The rise of new forms of network attacks has resulted in a heightened demand for these technologies. Nevertheless, the current models effectiveness is subpar. We propose a new Deformable Vision Transformer (DE-VIT) method to address this issue. DE-VIT introduces a new deformable attention mechanism module, where the positions of key-value pairs in the attention mechanism are selected in a data-dependent manner, allowing it to focus on relevant areas, capture more informative features, and avoid excessive memory and computational costs. In addition to using deformable convolutions instead of regular convolutions in embedding layers to enhance the receptive field of patches, a sliding window mechanism is also employed to utilize edge information fully. In Parallel, we use a layered focal loss function to improve classification performance and address data imbalance issues. In summary, DE-VIT reduces computational complexity and achieves better results. We conduct experimental simulations on the public intrusion detection datasets, and the accuracy of the enhanced intrusion detection model surpasses that of the Deep Belief Network with Improved Kernel-Based Extreme Learning (DBN-KELM). It reaches 99.5% and 97.5% on the CIC IDS2017 and UNSW-NB15 datasets, exhibiting an increase of 8.5% and 9.1%, respectively.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9285996627124b945ec601a763f6ff884bac3281.pdf",
        "venue": "IEEE Access",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Network intrusion detection technology has always been an indispensable protection mechanism for industrial network security. The rise of new forms of network attacks has resulted in a heightened demand for these technologies. Nevertheless, the current models effectiveness is subpar. We propose a new Deformable Vision Transformer (DE-VIT) method to address this issue. DE-VIT introduces a new deformable attention mechanism module, where the positions of key-value pairs in the attention mechanism are selected in a data-dependent manner, allowing it to focus on relevant areas, capture more informative features, and avoid excessive memory and computational costs. In addition to using deformable convolutions instead of regular convolutions in embedding layers to enhance the receptive field of patches, a sliding window mechanism is also employed to utilize edge information fully. In Parallel, we use a layered focal loss function to improve classification performance and address data imbalance issues. In summary, DE-VIT reduces computational complexity and achieves better results. We conduct experimental simulations on the public intrusion detection datasets, and the accuracy of the enhanced intrusion detection model surpasses that of the Deep Belief Network with Improved Kernel-Based Extreme Learning (DBN-KELM). It reaches 99.5% and 97.5% on the CIC IDS2017 and UNSW-NB15 datasets, exhibiting an increase of 8.5% and 9.1%, respectively.",
        "keywords": []
      },
      "file_name": "9285996627124b945ec601a763f6ff884bac3281.pdf"
    },
    {
      "success": true,
      "doc_id": "5e13e5676eb37209fdbedc99f0f6af54",
      "summary": "In order to improve the accuracy of bearing fault diagnosis under a small sample, variable load, and noise conditions, a new fault diagnosis method based on an image information fusion and Vision Transformer (ViT) transfer learning model is proposed in this paper. Firstly, the method applies continuous wavelet transform (CWT), Gramian angular summation field (GASF), and Gramian angular difference field (GADF) to the time series data, and generates three grayscale images. Then, the generated three grayscale images are merged into an information fusion image (IFI) using image processing techniques. Finally, the obtained IFIs are fed into the advanced ViT model and trained based on transfer learning. In order to verify the effectiveness and superiority of the proposed method, the rolling bearing dataset from Case Western Reserve University (CWRU) is used to carry out experimental studies under different working conditions. Experimental results show that the method proposed in this paper is superior to other traditional methods in terms of accuracy, and the effect of ViT model based on transfer learning (TLViT) training is better than that of the Resnet50 model based on transfer learning training (TLResnet50) under variable loads and small sample conditions. In addition, the experimental results also prove that the IFI with multiple image information has better anti-noise ability than the single information image. Therefore, the method proposed in this paper can improve the accuracy of bearing fault diagnosis under small sample, variable load and noise conditions, and provide a new method for bearing fault diagnosis.",
      "intriguing_abstract": "In order to improve the accuracy of bearing fault diagnosis under a small sample, variable load, and noise conditions, a new fault diagnosis method based on an image information fusion and Vision Transformer (ViT) transfer learning model is proposed in this paper. Firstly, the method applies continuous wavelet transform (CWT), Gramian angular summation field (GASF), and Gramian angular difference field (GADF) to the time series data, and generates three grayscale images. Then, the generated three grayscale images are merged into an information fusion image (IFI) using image processing techniques. Finally, the obtained IFIs are fed into the advanced ViT model and trained based on transfer learning. In order to verify the effectiveness and superiority of the proposed method, the rolling bearing dataset from Case Western Reserve University (CWRU) is used to carry out experimental studies under different working conditions. Experimental results show that the method proposed in this paper is superior to other traditional methods in terms of accuracy, and the effect of ViT model based on transfer learning (TLViT) training is better than that of the Resnet50 model based on transfer learning training (TLResnet50) under variable loads and small sample conditions. In addition, the experimental results also prove that the IFI with multiple image information has better anti-noise ability than the single information image. Therefore, the method proposed in this paper can improve the accuracy of bearing fault diagnosis under small sample, variable load and noise conditions, and provide a new method for bearing fault diagnosis.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/3f002ea3ede9d97f1fe29b16691491219c1e626d.pdf",
      "citation_key": "zhang202489a",
      "metadata": {
        "title": "Bearing Fault Diagnosis Based on Image Information Fusion and Vision Transformer Transfer Learning Model",
        "authors": [
          "Zichen Zhang",
          "Jing Li",
          "C. Cai",
          "Jianhua Ren",
          "Y. Xue"
        ],
        "published_date": "2024",
        "abstract": "In order to improve the accuracy of bearing fault diagnosis under a small sample, variable load, and noise conditions, a new fault diagnosis method based on an image information fusion and Vision Transformer (ViT) transfer learning model is proposed in this paper. Firstly, the method applies continuous wavelet transform (CWT), Gramian angular summation field (GASF), and Gramian angular difference field (GADF) to the time series data, and generates three grayscale images. Then, the generated three grayscale images are merged into an information fusion image (IFI) using image processing techniques. Finally, the obtained IFIs are fed into the advanced ViT model and trained based on transfer learning. In order to verify the effectiveness and superiority of the proposed method, the rolling bearing dataset from Case Western Reserve University (CWRU) is used to carry out experimental studies under different working conditions. Experimental results show that the method proposed in this paper is superior to other traditional methods in terms of accuracy, and the effect of ViT model based on transfer learning (TLViT) training is better than that of the Resnet50 model based on transfer learning training (TLResnet50) under variable loads and small sample conditions. In addition, the experimental results also prove that the IFI with multiple image information has better anti-noise ability than the single information image. Therefore, the method proposed in this paper can improve the accuracy of bearing fault diagnosis under small sample, variable load and noise conditions, and provide a new method for bearing fault diagnosis.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3f002ea3ede9d97f1fe29b16691491219c1e626d.pdf",
        "venue": "Applied Sciences",
        "citationCount": 8,
        "score": 8.0,
        "summary": "In order to improve the accuracy of bearing fault diagnosis under a small sample, variable load, and noise conditions, a new fault diagnosis method based on an image information fusion and Vision Transformer (ViT) transfer learning model is proposed in this paper. Firstly, the method applies continuous wavelet transform (CWT), Gramian angular summation field (GASF), and Gramian angular difference field (GADF) to the time series data, and generates three grayscale images. Then, the generated three grayscale images are merged into an information fusion image (IFI) using image processing techniques. Finally, the obtained IFIs are fed into the advanced ViT model and trained based on transfer learning. In order to verify the effectiveness and superiority of the proposed method, the rolling bearing dataset from Case Western Reserve University (CWRU) is used to carry out experimental studies under different working conditions. Experimental results show that the method proposed in this paper is superior to other traditional methods in terms of accuracy, and the effect of ViT model based on transfer learning (TLViT) training is better than that of the Resnet50 model based on transfer learning training (TLResnet50) under variable loads and small sample conditions. In addition, the experimental results also prove that the IFI with multiple image information has better anti-noise ability than the single information image. Therefore, the method proposed in this paper can improve the accuracy of bearing fault diagnosis under small sample, variable load and noise conditions, and provide a new method for bearing fault diagnosis.",
        "keywords": []
      },
      "file_name": "3f002ea3ede9d97f1fe29b16691491219c1e626d.pdf"
    },
    {
      "success": true,
      "doc_id": "42399174284d8964f516800509399b5a",
      "summary": "The Vision Transformer (ViT) models have demonstrated excellent performance in computer vision tasks, but a large amount of computation and memory access for massive matrix multiplications lead to degraded hardware performance compared to convolutional neural network (CNN). In this paper, we propose a ViT accelerator with a novel Weight-Loop dataflow and its computing unit, for efficient matrix multiplication computation. By data partitioning and rearrangement, the number of memory accesses and the number of registers are greatly reduced, and the adder trees are eliminated. A computation pipeline with the proposed dataflow scheduling method is constructed to maintain a high utilization rate through zero bubble switching. Moreover, a novel accurate dual INT8 multiply-accumulate (DI8MAC) method for DSP optimization is introduced to eliminate the additional correction circuits by weight encoding. Verified in the Xilinx XCZU9EG FPGA, the proposed ViT accelerator achieves the lowest inference latencies of 3.91 ms and 13.98 ms for ViT-S and ViT-B, respectively. The throughput of the accelerator can reach up to 2330.2 GOPs with an energy efficiency of 109 GOPs/W, showing a significant improvement compared to the state-of-the-art works.",
      "intriguing_abstract": "The Vision Transformer (ViT) models have demonstrated excellent performance in computer vision tasks, but a large amount of computation and memory access for massive matrix multiplications lead to degraded hardware performance compared to convolutional neural network (CNN). In this paper, we propose a ViT accelerator with a novel Weight-Loop dataflow and its computing unit, for efficient matrix multiplication computation. By data partitioning and rearrangement, the number of memory accesses and the number of registers are greatly reduced, and the adder trees are eliminated. A computation pipeline with the proposed dataflow scheduling method is constructed to maintain a high utilization rate through zero bubble switching. Moreover, a novel accurate dual INT8 multiply-accumulate (DI8MAC) method for DSP optimization is introduced to eliminate the additional correction circuits by weight encoding. Verified in the Xilinx XCZU9EG FPGA, the proposed ViT accelerator achieves the lowest inference latencies of 3.91 ms and 13.98 ms for ViT-S and ViT-B, respectively. The throughput of the accelerator can reach up to 2330.2 GOPs with an energy efficiency of 109 GOPs/W, showing a significant improvement compared to the state-of-the-art works.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/9121dcd10df00e5cc51dc94400e0325e0ae47bb9.pdf",
      "citation_key": "zhang2024pd6",
      "metadata": {
        "title": "A 109-GOPs/W FPGA-Based Vision Transformer Accelerator With Weight-Loop Dataflow Featuring Data Reusing and Resource Saving",
        "authors": [
          "Yueqi Zhang",
          "Lichen Feng",
          "Hongwei Shan",
          "Zhangming Zhu"
        ],
        "published_date": "2024",
        "abstract": "The Vision Transformer (ViT) models have demonstrated excellent performance in computer vision tasks, but a large amount of computation and memory access for massive matrix multiplications lead to degraded hardware performance compared to convolutional neural network (CNN). In this paper, we propose a ViT accelerator with a novel Weight-Loop dataflow and its computing unit, for efficient matrix multiplication computation. By data partitioning and rearrangement, the number of memory accesses and the number of registers are greatly reduced, and the adder trees are eliminated. A computation pipeline with the proposed dataflow scheduling method is constructed to maintain a high utilization rate through zero bubble switching. Moreover, a novel accurate dual INT8 multiply-accumulate (DI8MAC) method for DSP optimization is introduced to eliminate the additional correction circuits by weight encoding. Verified in the Xilinx XCZU9EG FPGA, the proposed ViT accelerator achieves the lowest inference latencies of 3.91 ms and 13.98 ms for ViT-S and ViT-B, respectively. The throughput of the accelerator can reach up to 2330.2 GOPs with an energy efficiency of 109 GOPs/W, showing a significant improvement compared to the state-of-the-art works.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9121dcd10df00e5cc51dc94400e0325e0ae47bb9.pdf",
        "venue": "IEEE transactions on circuits and systems for video technology (Print)",
        "citationCount": 8,
        "score": 8.0,
        "summary": "The Vision Transformer (ViT) models have demonstrated excellent performance in computer vision tasks, but a large amount of computation and memory access for massive matrix multiplications lead to degraded hardware performance compared to convolutional neural network (CNN). In this paper, we propose a ViT accelerator with a novel Weight-Loop dataflow and its computing unit, for efficient matrix multiplication computation. By data partitioning and rearrangement, the number of memory accesses and the number of registers are greatly reduced, and the adder trees are eliminated. A computation pipeline with the proposed dataflow scheduling method is constructed to maintain a high utilization rate through zero bubble switching. Moreover, a novel accurate dual INT8 multiply-accumulate (DI8MAC) method for DSP optimization is introduced to eliminate the additional correction circuits by weight encoding. Verified in the Xilinx XCZU9EG FPGA, the proposed ViT accelerator achieves the lowest inference latencies of 3.91 ms and 13.98 ms for ViT-S and ViT-B, respectively. The throughput of the accelerator can reach up to 2330.2 GOPs with an energy efficiency of 109 GOPs/W, showing a significant improvement compared to the state-of-the-art works.",
        "keywords": []
      },
      "file_name": "9121dcd10df00e5cc51dc94400e0325e0ae47bb9.pdf"
    },
    {
      "success": true,
      "doc_id": "5d062a263ec894b9a05cd3c09ad8bd2f",
      "summary": "Vehicle classification technology is one of the foundations in the field of automatic driving. With the development of deep learning technology, visual transformer structures based on attention mechanisms can represent global information quickly and effectively. However, due to direct image segmentation, local feature details and information will be lost. To solve this problem, we propose an improved vision transformer vehicle classification network (IND-ViT). Specifically, we first design a CNN-In D branch module to extract local features before image segmentation to make up for the loss of detail information in the vision transformer. Then, in order to solve the problem of misdetection caused by the large similarity of some vehicles, we propose a sparse attention module, which can screen out the discernible regions in the image and further improve the detailed feature representation ability of the model. Finally, this paper uses the contrast loss function to further increase the intra-class consistency and inter-class difference of classification features and improve the accuracy of vehicle classification recognition. Experimental results show that the accuracy of the proposed model on the datasets of vehicle classification BIT-Vehicles, CIFAR-10, Oxford Flower-102, and Caltech-101 is higher than that of the original vision transformer model. Respectively, it increased by 1.3%, 1.21%, 7.54%, and 3.60%; at the same time, it also met a certain real-time requirement to achieve a balance of accuracy and real time.",
      "intriguing_abstract": "Vehicle classification technology is one of the foundations in the field of automatic driving. With the development of deep learning technology, visual transformer structures based on attention mechanisms can represent global information quickly and effectively. However, due to direct image segmentation, local feature details and information will be lost. To solve this problem, we propose an improved vision transformer vehicle classification network (IND-ViT). Specifically, we first design a CNN-In D branch module to extract local features before image segmentation to make up for the loss of detail information in the vision transformer. Then, in order to solve the problem of misdetection caused by the large similarity of some vehicles, we propose a sparse attention module, which can screen out the discernible regions in the image and further improve the detailed feature representation ability of the model. Finally, this paper uses the contrast loss function to further increase the intra-class consistency and inter-class difference of classification features and improve the accuracy of vehicle classification recognition. Experimental results show that the accuracy of the proposed model on the datasets of vehicle classification BIT-Vehicles, CIFAR-10, Oxford Flower-102, and Caltech-101 is higher than that of the original vision transformer model. Respectively, it increased by 1.3%, 1.21%, 7.54%, and 3.60%; at the same time, it also met a certain real-time requirement to achieve a balance of accuracy and real time.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/08606a6a8b447909e714be2c3160074fdf1b91ad.pdf",
      "citation_key": "dong20242ow",
      "metadata": {
        "title": "Vehicle Classification Algorithm Based on Improved Vision Transformer",
        "authors": [
          "Xinlong Dong",
          "Peicheng Shi",
          "Yueyue Tang",
          "Li Yang",
          "Aixi Yang",
          "Taonian Liang"
        ],
        "published_date": "2024",
        "abstract": "Vehicle classification technology is one of the foundations in the field of automatic driving. With the development of deep learning technology, visual transformer structures based on attention mechanisms can represent global information quickly and effectively. However, due to direct image segmentation, local feature details and information will be lost. To solve this problem, we propose an improved vision transformer vehicle classification network (IND-ViT). Specifically, we first design a CNN-In D branch module to extract local features before image segmentation to make up for the loss of detail information in the vision transformer. Then, in order to solve the problem of misdetection caused by the large similarity of some vehicles, we propose a sparse attention module, which can screen out the discernible regions in the image and further improve the detailed feature representation ability of the model. Finally, this paper uses the contrast loss function to further increase the intra-class consistency and inter-class difference of classification features and improve the accuracy of vehicle classification recognition. Experimental results show that the accuracy of the proposed model on the datasets of vehicle classification BIT-Vehicles, CIFAR-10, Oxford Flower-102, and Caltech-101 is higher than that of the original vision transformer model. Respectively, it increased by 1.3%, 1.21%, 7.54%, and 3.60%; at the same time, it also met a certain real-time requirement to achieve a balance of accuracy and real time.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/08606a6a8b447909e714be2c3160074fdf1b91ad.pdf",
        "venue": "World Electric Vehicle Journal",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Vehicle classification technology is one of the foundations in the field of automatic driving. With the development of deep learning technology, visual transformer structures based on attention mechanisms can represent global information quickly and effectively. However, due to direct image segmentation, local feature details and information will be lost. To solve this problem, we propose an improved vision transformer vehicle classification network (IND-ViT). Specifically, we first design a CNN-In D branch module to extract local features before image segmentation to make up for the loss of detail information in the vision transformer. Then, in order to solve the problem of misdetection caused by the large similarity of some vehicles, we propose a sparse attention module, which can screen out the discernible regions in the image and further improve the detailed feature representation ability of the model. Finally, this paper uses the contrast loss function to further increase the intra-class consistency and inter-class difference of classification features and improve the accuracy of vehicle classification recognition. Experimental results show that the accuracy of the proposed model on the datasets of vehicle classification BIT-Vehicles, CIFAR-10, Oxford Flower-102, and Caltech-101 is higher than that of the original vision transformer model. Respectively, it increased by 1.3%, 1.21%, 7.54%, and 3.60%; at the same time, it also met a certain real-time requirement to achieve a balance of accuracy and real time.",
        "keywords": []
      },
      "file_name": "08606a6a8b447909e714be2c3160074fdf1b91ad.pdf"
    },
    {
      "success": true,
      "doc_id": "2f69d61572ebdf3869dfc8400dacb370",
      "summary": "Since clutter encountered in ground-penetrating radar (GPR) systems deteriorates the performance of target detection algorithms, clutter removal is an active research area in the GPR community. In this letter, instead of convolutional neural network (CNN) architectures used in the recently proposed deep-learning-based clutter removal methods, we introduce declutter vision transformers (DC-ViTs) to remove the clutter. Transformer encoders in DC-ViT provide an alternative to CNNs which has limitations to capture long-range dependencies due to its local operations. In addition, the implementation of a convolutional layer instead of multilayer perceptron (MLP) in the transformer encoder increases the capturing ability of local dependencies. While deep features are extracted with blocks consisting of transformer encoders arranged sequentially, losses during information flow are reduced using dense connections between these blocks. Our proposed DC-ViT was compared with low-rank and sparse methods such as robust principle component analysis (RPCA), robust nonnegative matrix factorization (RNMF), and CNN-based deep networks such as convolutional autoencoder (CAE) and CR-NET. In comparisons made with the hybrid dataset, DC-ViT is 2.5% better in peak signal-to-noise ratio (PSNR) results than its closest competitor. As a result of the tests, we conducted using our experimental GPR data, and the proposed model provided an improvement of up to 20%, compared with its closest competitor in terms of signal-to-clutter ratio (SCR).",
      "intriguing_abstract": "Since clutter encountered in ground-penetrating radar (GPR) systems deteriorates the performance of target detection algorithms, clutter removal is an active research area in the GPR community. In this letter, instead of convolutional neural network (CNN) architectures used in the recently proposed deep-learning-based clutter removal methods, we introduce declutter vision transformers (DC-ViTs) to remove the clutter. Transformer encoders in DC-ViT provide an alternative to CNNs which has limitations to capture long-range dependencies due to its local operations. In addition, the implementation of a convolutional layer instead of multilayer perceptron (MLP) in the transformer encoder increases the capturing ability of local dependencies. While deep features are extracted with blocks consisting of transformer encoders arranged sequentially, losses during information flow are reduced using dense connections between these blocks. Our proposed DC-ViT was compared with low-rank and sparse methods such as robust principle component analysis (RPCA), robust nonnegative matrix factorization (RNMF), and CNN-based deep networks such as convolutional autoencoder (CAE) and CR-NET. In comparisons made with the hybrid dataset, DC-ViT is 2.5% better in peak signal-to-noise ratio (PSNR) results than its closest competitor. As a result of the tests, we conducted using our experimental GPR data, and the proposed model provided an improvement of up to 20%, compared with its closest competitor in terms of signal-to-clutter ratio (SCR).",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/0222269c963f0902cc9eae6768a3c5948531488b.pdf",
      "citation_key": "kayacan2024yy7",
      "metadata": {
        "title": "A Vision-Transformer-Based Approach to Clutter Removal in GPR: DC-ViT",
        "authors": [
          "Yavuz Emre Kayacan",
          "I. Erer"
        ],
        "published_date": "2024",
        "abstract": "Since clutter encountered in ground-penetrating radar (GPR) systems deteriorates the performance of target detection algorithms, clutter removal is an active research area in the GPR community. In this letter, instead of convolutional neural network (CNN) architectures used in the recently proposed deep-learning-based clutter removal methods, we introduce declutter vision transformers (DC-ViTs) to remove the clutter. Transformer encoders in DC-ViT provide an alternative to CNNs which has limitations to capture long-range dependencies due to its local operations. In addition, the implementation of a convolutional layer instead of multilayer perceptron (MLP) in the transformer encoder increases the capturing ability of local dependencies. While deep features are extracted with blocks consisting of transformer encoders arranged sequentially, losses during information flow are reduced using dense connections between these blocks. Our proposed DC-ViT was compared with low-rank and sparse methods such as robust principle component analysis (RPCA), robust nonnegative matrix factorization (RNMF), and CNN-based deep networks such as convolutional autoencoder (CAE) and CR-NET. In comparisons made with the hybrid dataset, DC-ViT is 2.5% better in peak signal-to-noise ratio (PSNR) results than its closest competitor. As a result of the tests, we conducted using our experimental GPR data, and the proposed model provided an improvement of up to 20%, compared with its closest competitor in terms of signal-to-clutter ratio (SCR).",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0222269c963f0902cc9eae6768a3c5948531488b.pdf",
        "venue": "IEEE Geoscience and Remote Sensing Letters",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Since clutter encountered in ground-penetrating radar (GPR) systems deteriorates the performance of target detection algorithms, clutter removal is an active research area in the GPR community. In this letter, instead of convolutional neural network (CNN) architectures used in the recently proposed deep-learning-based clutter removal methods, we introduce declutter vision transformers (DC-ViTs) to remove the clutter. Transformer encoders in DC-ViT provide an alternative to CNNs which has limitations to capture long-range dependencies due to its local operations. In addition, the implementation of a convolutional layer instead of multilayer perceptron (MLP) in the transformer encoder increases the capturing ability of local dependencies. While deep features are extracted with blocks consisting of transformer encoders arranged sequentially, losses during information flow are reduced using dense connections between these blocks. Our proposed DC-ViT was compared with low-rank and sparse methods such as robust principle component analysis (RPCA), robust nonnegative matrix factorization (RNMF), and CNN-based deep networks such as convolutional autoencoder (CAE) and CR-NET. In comparisons made with the hybrid dataset, DC-ViT is 2.5% better in peak signal-to-noise ratio (PSNR) results than its closest competitor. As a result of the tests, we conducted using our experimental GPR data, and the proposed model provided an improvement of up to 20%, compared with its closest competitor in terms of signal-to-clutter ratio (SCR).",
        "keywords": []
      },
      "file_name": "0222269c963f0902cc9eae6768a3c5948531488b.pdf"
    },
    {
      "success": true,
      "doc_id": "f65560eddf777198943f3d353eaf1097",
      "summary": "The real-time classification of fish feeding behavior plays a crucial role in aquaculture, which is closely related to feeding cost and environmental preservation. In this paper, a Fish Feeding Intensity classification model based on the improved Vision Transformer (CFFI-Vit) is proposed, which is capable of quantifying the feeding behaviors of rainbow trout (Oncorhynchus mykiss) into three intensities: strong, moderate, and weak. The process is outlined as follows: firstly, we obtained 2685 raw feeding images of rainbow trout from recorded videos and classified them into three categories: strong, moderate, and weak. Secondly, the number of transformer encoder blocks in the internal structure of the ViT was reduced from 12 to 4, which can greatly reduce the computational load of the model, facilitating its deployment on mobile devices. And finally, a residual module was added to the head of the ViT, enhancing the models ability to extract features. The proposed CFFI-Vit has a computational load of 5.81 G (Giga) Floating Point Operations per Second (FLOPs). Compared to the original ViT model, it reduces computational demands by 65.54% and improves classification accuracy on the validation set by 5.4 percentage points. On the test set, the model achieves precision, recall, and F1 score of 93.47%, 93.44%, and 93.42%, respectively. Additionally, compared to state-of-the-art models such as ResNet34, MobileNetv2, VGG16, and GoogLeNet, the CFFI-Vit models classification accuracy is higher by 6.87, 8.43, 7.03, and 5.65 percentage points, respectively. Therefore, the proposed CFFI-Vit can achieve higher classification accuracy while significantly reducing computational demands. This provides a foundation for deploying lightweight deep network models on edge devices with limited hardware capabilities.",
      "intriguing_abstract": "The real-time classification of fish feeding behavior plays a crucial role in aquaculture, which is closely related to feeding cost and environmental preservation. In this paper, a Fish Feeding Intensity classification model based on the improved Vision Transformer (CFFI-Vit) is proposed, which is capable of quantifying the feeding behaviors of rainbow trout (Oncorhynchus mykiss) into three intensities: strong, moderate, and weak. The process is outlined as follows: firstly, we obtained 2685 raw feeding images of rainbow trout from recorded videos and classified them into three categories: strong, moderate, and weak. Secondly, the number of transformer encoder blocks in the internal structure of the ViT was reduced from 12 to 4, which can greatly reduce the computational load of the model, facilitating its deployment on mobile devices. And finally, a residual module was added to the head of the ViT, enhancing the models ability to extract features. The proposed CFFI-Vit has a computational load of 5.81 G (Giga) Floating Point Operations per Second (FLOPs). Compared to the original ViT model, it reduces computational demands by 65.54% and improves classification accuracy on the validation set by 5.4 percentage points. On the test set, the model achieves precision, recall, and F1 score of 93.47%, 93.44%, and 93.42%, respectively. Additionally, compared to state-of-the-art models such as ResNet34, MobileNetv2, VGG16, and GoogLeNet, the CFFI-Vit models classification accuracy is higher by 6.87, 8.43, 7.03, and 5.65 percentage points, respectively. Therefore, the proposed CFFI-Vit can achieve higher classification accuracy while significantly reducing computational demands. This provides a foundation for deploying lightweight deep network models on edge devices with limited hardware capabilities.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/8776fd7934dc48df4663dadf30c6da665d84fb19.pdf",
      "citation_key": "liu20248jh",
      "metadata": {
        "title": "CFFI-Vit: Enhanced Vision Transformer for the Accurate Classification of Fish Feeding Intensity in Aquaculture",
        "authors": [
          "Jintao Liu",
          "Alfredo Toln Becerra",
          "Jos Fernando Bienvenido-Barcena",
          "Xinting Yang",
          "Zhenxi Zhao",
          "Chao Zhou"
        ],
        "published_date": "2024",
        "abstract": "The real-time classification of fish feeding behavior plays a crucial role in aquaculture, which is closely related to feeding cost and environmental preservation. In this paper, a Fish Feeding Intensity classification model based on the improved Vision Transformer (CFFI-Vit) is proposed, which is capable of quantifying the feeding behaviors of rainbow trout (Oncorhynchus mykiss) into three intensities: strong, moderate, and weak. The process is outlined as follows: firstly, we obtained 2685 raw feeding images of rainbow trout from recorded videos and classified them into three categories: strong, moderate, and weak. Secondly, the number of transformer encoder blocks in the internal structure of the ViT was reduced from 12 to 4, which can greatly reduce the computational load of the model, facilitating its deployment on mobile devices. And finally, a residual module was added to the head of the ViT, enhancing the models ability to extract features. The proposed CFFI-Vit has a computational load of 5.81 G (Giga) Floating Point Operations per Second (FLOPs). Compared to the original ViT model, it reduces computational demands by 65.54% and improves classification accuracy on the validation set by 5.4 percentage points. On the test set, the model achieves precision, recall, and F1 score of 93.47%, 93.44%, and 93.42%, respectively. Additionally, compared to state-of-the-art models such as ResNet34, MobileNetv2, VGG16, and GoogLeNet, the CFFI-Vit models classification accuracy is higher by 6.87, 8.43, 7.03, and 5.65 percentage points, respectively. Therefore, the proposed CFFI-Vit can achieve higher classification accuracy while significantly reducing computational demands. This provides a foundation for deploying lightweight deep network models on edge devices with limited hardware capabilities.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/8776fd7934dc48df4663dadf30c6da665d84fb19.pdf",
        "venue": "Journal of Marine Science and Engineering",
        "citationCount": 7,
        "score": 7.0,
        "summary": "The real-time classification of fish feeding behavior plays a crucial role in aquaculture, which is closely related to feeding cost and environmental preservation. In this paper, a Fish Feeding Intensity classification model based on the improved Vision Transformer (CFFI-Vit) is proposed, which is capable of quantifying the feeding behaviors of rainbow trout (Oncorhynchus mykiss) into three intensities: strong, moderate, and weak. The process is outlined as follows: firstly, we obtained 2685 raw feeding images of rainbow trout from recorded videos and classified them into three categories: strong, moderate, and weak. Secondly, the number of transformer encoder blocks in the internal structure of the ViT was reduced from 12 to 4, which can greatly reduce the computational load of the model, facilitating its deployment on mobile devices. And finally, a residual module was added to the head of the ViT, enhancing the models ability to extract features. The proposed CFFI-Vit has a computational load of 5.81 G (Giga) Floating Point Operations per Second (FLOPs). Compared to the original ViT model, it reduces computational demands by 65.54% and improves classification accuracy on the validation set by 5.4 percentage points. On the test set, the model achieves precision, recall, and F1 score of 93.47%, 93.44%, and 93.42%, respectively. Additionally, compared to state-of-the-art models such as ResNet34, MobileNetv2, VGG16, and GoogLeNet, the CFFI-Vit models classification accuracy is higher by 6.87, 8.43, 7.03, and 5.65 percentage points, respectively. Therefore, the proposed CFFI-Vit can achieve higher classification accuracy while significantly reducing computational demands. This provides a foundation for deploying lightweight deep network models on edge devices with limited hardware capabilities.",
        "keywords": []
      },
      "file_name": "8776fd7934dc48df4663dadf30c6da665d84fb19.pdf"
    },
    {
      "success": true,
      "doc_id": "a4109f820c3c6936e64d7de3183d3c6f",
      "summary": "Vision transformers (ViTs) have excelled in computer vision (CV) tasks but are memory-consuming and computation-intensive, challenging their deployment on resource-constrained devices. To tackle this limitation, prior works have explored ViT-tailored quantization algorithms but retained floating-point scaling factors, which yield nonnegligible requantization overhead, limiting ViTs hardware efficiency and motivating more hardware-friendly solutions. To this end, we propose P2-ViT, the first power-of-two (PoT) posttraining quantization (PTQ) and acceleration framework to accelerate fully quantized ViTs. Specifically, as for quantization, we explore a dedicated quantization scheme to effectively quantize ViTs with PoT scaling factors, thus minimizing the requantization overhead. Furthermore, we propose coarse-to-fine automatic mixed-precision quantization to enable better accuracy-efficiency tradeoffs. In terms of hardware, we develop a dedicated chunk-based accelerator featuring multiple tailored subprocessors to individually handle ViTs different types of operations, alleviating reconfigurable overhead. In addition, we design a tailored row-stationary dataflow to seize the pipeline processing opportunity introduced by our PoT scaling factors, thereby enhancing throughput. Extensive experiments consistently validate P2-ViTs effectiveness. Particularly, we offer comparable or even superior quantization performance with PoT scaling factors when compared with the counterpart with floating-point scaling factors. Besides, we achieve up to <inline-formula> <tex-math notation=\"LaTeX\">$10.1\\times $ </tex-math></inline-formula> speedup and <inline-formula> <tex-math notation=\"LaTeX\">$36.8\\times $ </tex-math></inline-formula> energy saving over GPUs Turing Tensor Cores, and up to <inline-formula> <tex-math notation=\"LaTeX\">$1.84\\times $ </tex-math></inline-formula> higher computation utilization efficiency against SOTA quantization-based ViT accelerators. Codes are available at <uri>https://github.com/shihuihong214/P2-ViT</uri>.",
      "intriguing_abstract": "Vision transformers (ViTs) have excelled in computer vision (CV) tasks but are memory-consuming and computation-intensive, challenging their deployment on resource-constrained devices. To tackle this limitation, prior works have explored ViT-tailored quantization algorithms but retained floating-point scaling factors, which yield nonnegligible requantization overhead, limiting ViTs hardware efficiency and motivating more hardware-friendly solutions. To this end, we propose P2-ViT, the first power-of-two (PoT) posttraining quantization (PTQ) and acceleration framework to accelerate fully quantized ViTs. Specifically, as for quantization, we explore a dedicated quantization scheme to effectively quantize ViTs with PoT scaling factors, thus minimizing the requantization overhead. Furthermore, we propose coarse-to-fine automatic mixed-precision quantization to enable better accuracy-efficiency tradeoffs. In terms of hardware, we develop a dedicated chunk-based accelerator featuring multiple tailored subprocessors to individually handle ViTs different types of operations, alleviating reconfigurable overhead. In addition, we design a tailored row-stationary dataflow to seize the pipeline processing opportunity introduced by our PoT scaling factors, thereby enhancing throughput. Extensive experiments consistently validate P2-ViTs effectiveness. Particularly, we offer comparable or even superior quantization performance with PoT scaling factors when compared with the counterpart with floating-point scaling factors. Besides, we achieve up to <inline-formula> <tex-math notation=\"LaTeX\">$10.1\\times $ </tex-math></inline-formula> speedup and <inline-formula> <tex-math notation=\"LaTeX\">$36.8\\times $ </tex-math></inline-formula> energy saving over GPUs Turing Tensor Cores, and up to <inline-formula> <tex-math notation=\"LaTeX\">$1.84\\times $ </tex-math></inline-formula> higher computation utilization efficiency against SOTA quantization-based ViT accelerators. Codes are available at <uri>https://github.com/shihuihong214/P2-ViT</uri>.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/cc24f933b343b6a9701088cf6ae1dbf3299c0c9e.pdf",
      "citation_key": "shi2024r44",
      "metadata": {
        "title": "P2-ViT: Power-of-Two Post-Training Quantization and Acceleration for Fully Quantized Vision Transformer",
        "authors": [
          "Huihong Shi",
          "Xin Cheng",
          "Wendong Mao",
          "Zhongfeng Wang"
        ],
        "published_date": "2024",
        "abstract": "Vision transformers (ViTs) have excelled in computer vision (CV) tasks but are memory-consuming and computation-intensive, challenging their deployment on resource-constrained devices. To tackle this limitation, prior works have explored ViT-tailored quantization algorithms but retained floating-point scaling factors, which yield nonnegligible requantization overhead, limiting ViTs hardware efficiency and motivating more hardware-friendly solutions. To this end, we propose P2-ViT, the first power-of-two (PoT) posttraining quantization (PTQ) and acceleration framework to accelerate fully quantized ViTs. Specifically, as for quantization, we explore a dedicated quantization scheme to effectively quantize ViTs with PoT scaling factors, thus minimizing the requantization overhead. Furthermore, we propose coarse-to-fine automatic mixed-precision quantization to enable better accuracy-efficiency tradeoffs. In terms of hardware, we develop a dedicated chunk-based accelerator featuring multiple tailored subprocessors to individually handle ViTs different types of operations, alleviating reconfigurable overhead. In addition, we design a tailored row-stationary dataflow to seize the pipeline processing opportunity introduced by our PoT scaling factors, thereby enhancing throughput. Extensive experiments consistently validate P2-ViTs effectiveness. Particularly, we offer comparable or even superior quantization performance with PoT scaling factors when compared with the counterpart with floating-point scaling factors. Besides, we achieve up to <inline-formula> <tex-math notation=\"LaTeX\">$10.1\\times $ </tex-math></inline-formula> speedup and <inline-formula> <tex-math notation=\"LaTeX\">$36.8\\times $ </tex-math></inline-formula> energy saving over GPUs Turing Tensor Cores, and up to <inline-formula> <tex-math notation=\"LaTeX\">$1.84\\times $ </tex-math></inline-formula> higher computation utilization efficiency against SOTA quantization-based ViT accelerators. Codes are available at <uri>https://github.com/shihuihong214/P2-ViT</uri>.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/cc24f933b343b6a9701088cf6ae1dbf3299c0c9e.pdf",
        "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Vision transformers (ViTs) have excelled in computer vision (CV) tasks but are memory-consuming and computation-intensive, challenging their deployment on resource-constrained devices. To tackle this limitation, prior works have explored ViT-tailored quantization algorithms but retained floating-point scaling factors, which yield nonnegligible requantization overhead, limiting ViTs hardware efficiency and motivating more hardware-friendly solutions. To this end, we propose P2-ViT, the first power-of-two (PoT) posttraining quantization (PTQ) and acceleration framework to accelerate fully quantized ViTs. Specifically, as for quantization, we explore a dedicated quantization scheme to effectively quantize ViTs with PoT scaling factors, thus minimizing the requantization overhead. Furthermore, we propose coarse-to-fine automatic mixed-precision quantization to enable better accuracy-efficiency tradeoffs. In terms of hardware, we develop a dedicated chunk-based accelerator featuring multiple tailored subprocessors to individually handle ViTs different types of operations, alleviating reconfigurable overhead. In addition, we design a tailored row-stationary dataflow to seize the pipeline processing opportunity introduced by our PoT scaling factors, thereby enhancing throughput. Extensive experiments consistently validate P2-ViTs effectiveness. Particularly, we offer comparable or even superior quantization performance with PoT scaling factors when compared with the counterpart with floating-point scaling factors. Besides, we achieve up to <inline-formula> <tex-math notation=\"LaTeX\">$10.1\\times $ </tex-math></inline-formula> speedup and <inline-formula> <tex-math notation=\"LaTeX\">$36.8\\times $ </tex-math></inline-formula> energy saving over GPUs Turing Tensor Cores, and up to <inline-formula> <tex-math notation=\"LaTeX\">$1.84\\times $ </tex-math></inline-formula> higher computation utilization efficiency against SOTA quantization-based ViT accelerators. Codes are available at <uri>https://github.com/shihuihong214/P2-ViT</uri>.",
        "keywords": []
      },
      "file_name": "cc24f933b343b6a9701088cf6ae1dbf3299c0c9e.pdf"
    },
    {
      "success": true,
      "doc_id": "9967a2f816f8cb33af058838a63a5bd0",
      "summary": "Vision transformer (ViT) provides new ideas for polarization synthetic aperture radar (PolSAR) image classification due to its advantages in learning global-spatial information. However, the lack of local-spatial information within samples and correlation information among samples, as well as the complexity of network structure, limit the application of ViT in practice. In addition, dual-frequency PolSAR data provide rich information, but there are fewer related studies compared to single-frequency classification algorithms. In this article, we adopt ViT as the basic framework, and propose a novel model based on mixed patch interaction for dual-frequency PolSAR image adaptive fusion classification (PolSAR-MPIformer). First, a mixed patch interaction (MPI) module is designed for the feature extraction, which replaces the high-complexity self-attention in ViT with patch interaction intra- and intersample. Besides the global-spatial information learning within samples by ViT, the MPI module adds the learning of local-spatial information within samples and correlation information among samples, thereby obtaining more discriminative features through a low-complexity network. Subsequently, a dual-frequency adaptive fusion (DAF) module is constructed as the classifier of PolSAR-MPIformer. On the one hand, the attention mechanism is utilized in DAF to reduce the impact of speckle noise while preserving details. On the other hand, the DAF evaluates the classification confidence of each band and assigns different weights accordingly, which achieves reasonable utilization of the complementarity between dual-frequency data and improves classification accuracy. Experiments on four real dual-frequency PolSAR datasets substantiate the superiority of the proposed PolSAR-MPIformer over other state-of-the-art algorithms.",
      "intriguing_abstract": "Vision transformer (ViT) provides new ideas for polarization synthetic aperture radar (PolSAR) image classification due to its advantages in learning global-spatial information. However, the lack of local-spatial information within samples and correlation information among samples, as well as the complexity of network structure, limit the application of ViT in practice. In addition, dual-frequency PolSAR data provide rich information, but there are fewer related studies compared to single-frequency classification algorithms. In this article, we adopt ViT as the basic framework, and propose a novel model based on mixed patch interaction for dual-frequency PolSAR image adaptive fusion classification (PolSAR-MPIformer). First, a mixed patch interaction (MPI) module is designed for the feature extraction, which replaces the high-complexity self-attention in ViT with patch interaction intra- and intersample. Besides the global-spatial information learning within samples by ViT, the MPI module adds the learning of local-spatial information within samples and correlation information among samples, thereby obtaining more discriminative features through a low-complexity network. Subsequently, a dual-frequency adaptive fusion (DAF) module is constructed as the classifier of PolSAR-MPIformer. On the one hand, the attention mechanism is utilized in DAF to reduce the impact of speckle noise while preserving details. On the other hand, the DAF evaluates the classification confidence of each band and assigns different weights accordingly, which achieves reasonable utilization of the complementarity between dual-frequency data and improves classification accuracy. Experiments on four real dual-frequency PolSAR datasets substantiate the superiority of the proposed PolSAR-MPIformer over other state-of-the-art algorithms.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/88589b0b2d2d8caa09d8ca94414343455ae87d7c.pdf",
      "citation_key": "xin2024ljt",
      "metadata": {
        "title": "PolSAR-MPIformer: A Vision Transformer Based on Mixed Patch Interaction for Dual-Frequency PolSAR Image Adaptive Fusion Classification",
        "authors": [
          "Xinyue Xin",
          "Ming Li",
          "Yan Wu",
          "Xiang Li",
          "Peng Zhang",
          "Dazhi Xu"
        ],
        "published_date": "2024",
        "abstract": "Vision transformer (ViT) provides new ideas for polarization synthetic aperture radar (PolSAR) image classification due to its advantages in learning global-spatial information. However, the lack of local-spatial information within samples and correlation information among samples, as well as the complexity of network structure, limit the application of ViT in practice. In addition, dual-frequency PolSAR data provide rich information, but there are fewer related studies compared to single-frequency classification algorithms. In this article, we adopt ViT as the basic framework, and propose a novel model based on mixed patch interaction for dual-frequency PolSAR image adaptive fusion classification (PolSAR-MPIformer). First, a mixed patch interaction (MPI) module is designed for the feature extraction, which replaces the high-complexity self-attention in ViT with patch interaction intra- and intersample. Besides the global-spatial information learning within samples by ViT, the MPI module adds the learning of local-spatial information within samples and correlation information among samples, thereby obtaining more discriminative features through a low-complexity network. Subsequently, a dual-frequency adaptive fusion (DAF) module is constructed as the classifier of PolSAR-MPIformer. On the one hand, the attention mechanism is utilized in DAF to reduce the impact of speckle noise while preserving details. On the other hand, the DAF evaluates the classification confidence of each band and assigns different weights accordingly, which achieves reasonable utilization of the complementarity between dual-frequency data and improves classification accuracy. Experiments on four real dual-frequency PolSAR datasets substantiate the superiority of the proposed PolSAR-MPIformer over other state-of-the-art algorithms.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/88589b0b2d2d8caa09d8ca94414343455ae87d7c.pdf",
        "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Vision transformer (ViT) provides new ideas for polarization synthetic aperture radar (PolSAR) image classification due to its advantages in learning global-spatial information. However, the lack of local-spatial information within samples and correlation information among samples, as well as the complexity of network structure, limit the application of ViT in practice. In addition, dual-frequency PolSAR data provide rich information, but there are fewer related studies compared to single-frequency classification algorithms. In this article, we adopt ViT as the basic framework, and propose a novel model based on mixed patch interaction for dual-frequency PolSAR image adaptive fusion classification (PolSAR-MPIformer). First, a mixed patch interaction (MPI) module is designed for the feature extraction, which replaces the high-complexity self-attention in ViT with patch interaction intra- and intersample. Besides the global-spatial information learning within samples by ViT, the MPI module adds the learning of local-spatial information within samples and correlation information among samples, thereby obtaining more discriminative features through a low-complexity network. Subsequently, a dual-frequency adaptive fusion (DAF) module is constructed as the classifier of PolSAR-MPIformer. On the one hand, the attention mechanism is utilized in DAF to reduce the impact of speckle noise while preserving details. On the other hand, the DAF evaluates the classification confidence of each band and assigns different weights accordingly, which achieves reasonable utilization of the complementarity between dual-frequency data and improves classification accuracy. Experiments on four real dual-frequency PolSAR datasets substantiate the superiority of the proposed PolSAR-MPIformer over other state-of-the-art algorithms.",
        "keywords": []
      },
      "file_name": "88589b0b2d2d8caa09d8ca94414343455ae87d7c.pdf"
    },
    {
      "success": true,
      "doc_id": "8357de37b0250c1cec7eb33499b633e4",
      "summary": "Dam is an essential structure in hydraulic engineering, and its surface cracks pose significant threats to its integrity, impermeability, and durability. Automated crack detection methods based on computer vision offer substantial advantages over manual approaches with regard to efficiency, objectivity and precision. However, current methods face challenges such as misidentification, discontinuity, and loss of details when analyzing real-world dam crack images. These images often exhibit characteristics such as low contrast, complex backgrounds, and diverse crack morphologies. To address the above challenges, this paper presents a pure Vision Transformer (ViT)-based dam crack segmentation network (DCST-net). The DCST-net utilizes an improved Swin Transformer (SwinT) block as the fundamental block for enhancing the long-range dependencies within a SegNet-like encoderdecoder structure. Additionally, we employ a weighted attention block to facilitate side fusion between the symmetric pair of encoder and decoder in each stage to sharpen the edge of crack. To demonstrate the superior performance of our proposed method, six semantic segmentation models have been trained and tested on both a self-built dam crack dataset and two publicly available datasets. Comparison results indicate that our proposed model outperforms the mainstream methods in terms of visualization and most evaluation metrics, highlighting its potential for practical application in dam safety inspection and maintenance.",
      "intriguing_abstract": "Dam is an essential structure in hydraulic engineering, and its surface cracks pose significant threats to its integrity, impermeability, and durability. Automated crack detection methods based on computer vision offer substantial advantages over manual approaches with regard to efficiency, objectivity and precision. However, current methods face challenges such as misidentification, discontinuity, and loss of details when analyzing real-world dam crack images. These images often exhibit characteristics such as low contrast, complex backgrounds, and diverse crack morphologies. To address the above challenges, this paper presents a pure Vision Transformer (ViT)-based dam crack segmentation network (DCST-net). The DCST-net utilizes an improved Swin Transformer (SwinT) block as the fundamental block for enhancing the long-range dependencies within a SegNet-like encoderdecoder structure. Additionally, we employ a weighted attention block to facilitate side fusion between the symmetric pair of encoder and decoder in each stage to sharpen the edge of crack. To demonstrate the superior performance of our proposed method, six semantic segmentation models have been trained and tested on both a self-built dam crack dataset and two publicly available datasets. Comparison results indicate that our proposed model outperforms the mainstream methods in terms of visualization and most evaluation metrics, highlighting its potential for practical application in dam safety inspection and maintenance.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d629289cc2f3efbe374a66d57690dd84f19d59ec.pdf",
      "citation_key": "zhou2024qty",
      "metadata": {
        "title": "Vison Transformer-Based Automatic Crack Detection on Dam Surface",
        "authors": [
          "Jian Zhou",
          "Guochuan Zhao",
          "Yonglong Li"
        ],
        "published_date": "2024",
        "abstract": "Dam is an essential structure in hydraulic engineering, and its surface cracks pose significant threats to its integrity, impermeability, and durability. Automated crack detection methods based on computer vision offer substantial advantages over manual approaches with regard to efficiency, objectivity and precision. However, current methods face challenges such as misidentification, discontinuity, and loss of details when analyzing real-world dam crack images. These images often exhibit characteristics such as low contrast, complex backgrounds, and diverse crack morphologies. To address the above challenges, this paper presents a pure Vision Transformer (ViT)-based dam crack segmentation network (DCST-net). The DCST-net utilizes an improved Swin Transformer (SwinT) block as the fundamental block for enhancing the long-range dependencies within a SegNet-like encoderdecoder structure. Additionally, we employ a weighted attention block to facilitate side fusion between the symmetric pair of encoder and decoder in each stage to sharpen the edge of crack. To demonstrate the superior performance of our proposed method, six semantic segmentation models have been trained and tested on both a self-built dam crack dataset and two publicly available datasets. Comparison results indicate that our proposed model outperforms the mainstream methods in terms of visualization and most evaluation metrics, highlighting its potential for practical application in dam safety inspection and maintenance.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d629289cc2f3efbe374a66d57690dd84f19d59ec.pdf",
        "venue": "Water",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Dam is an essential structure in hydraulic engineering, and its surface cracks pose significant threats to its integrity, impermeability, and durability. Automated crack detection methods based on computer vision offer substantial advantages over manual approaches with regard to efficiency, objectivity and precision. However, current methods face challenges such as misidentification, discontinuity, and loss of details when analyzing real-world dam crack images. These images often exhibit characteristics such as low contrast, complex backgrounds, and diverse crack morphologies. To address the above challenges, this paper presents a pure Vision Transformer (ViT)-based dam crack segmentation network (DCST-net). The DCST-net utilizes an improved Swin Transformer (SwinT) block as the fundamental block for enhancing the long-range dependencies within a SegNet-like encoderdecoder structure. Additionally, we employ a weighted attention block to facilitate side fusion between the symmetric pair of encoder and decoder in each stage to sharpen the edge of crack. To demonstrate the superior performance of our proposed method, six semantic segmentation models have been trained and tested on both a self-built dam crack dataset and two publicly available datasets. Comparison results indicate that our proposed model outperforms the mainstream methods in terms of visualization and most evaluation metrics, highlighting its potential for practical application in dam safety inspection and maintenance.",
        "keywords": []
      },
      "file_name": "d629289cc2f3efbe374a66d57690dd84f19d59ec.pdf"
    },
    {
      "success": true,
      "doc_id": "074d2991a1a976696852b0e09a4bf0cb",
      "summary": "Breast cancer is one of the most significant health threats to women worldwide. This disease manifests through abnormal proliferation of cells and the formation of tumors in breast tissue. Definitive breast cancer diagnosis is usually determined by analyzing tissue samples obtained from biopsies and reviewing them by pathologists. However, this method is highly dependent on the knowledge and experience of pathologists and may lead to errors due to the subjective nature of human interpretation and the high volume of cases. This study presents a multi-scale hybrid model based on Vision Transformer and residual networks for breast cancer detection in histopathological images, abbreviated as RI-ViT. In this approach, local features are extracted through a combination of residual stages and multi-scale learning, while global features are obtained using the attention mechanism in transformers. This combination enables simultaneous extraction of both local and global features from histopathological images, effectively improving the models performance in detecting complex cases. We have used an imbalanced and publicly available dataset called BreakHis to evaluate the performance of the RI-ViT model. The experimental results of the proposed model show that it achieves accuracies of 99.75%, 98.80%, 98.01%, and 97.53% at magnifications of 40X, 100X, 200X, and 400X, respectively. The RI-ViT model can also perform well in an magnification-independent mode. Results show that, regardless of the magnification level, it achieves an accuracy of 99.37%, demonstrating its superiority over other state-of-the-art models.",
      "intriguing_abstract": "Breast cancer is one of the most significant health threats to women worldwide. This disease manifests through abnormal proliferation of cells and the formation of tumors in breast tissue. Definitive breast cancer diagnosis is usually determined by analyzing tissue samples obtained from biopsies and reviewing them by pathologists. However, this method is highly dependent on the knowledge and experience of pathologists and may lead to errors due to the subjective nature of human interpretation and the high volume of cases. This study presents a multi-scale hybrid model based on Vision Transformer and residual networks for breast cancer detection in histopathological images, abbreviated as RI-ViT. In this approach, local features are extracted through a combination of residual stages and multi-scale learning, while global features are obtained using the attention mechanism in transformers. This combination enables simultaneous extraction of both local and global features from histopathological images, effectively improving the models performance in detecting complex cases. We have used an imbalanced and publicly available dataset called BreakHis to evaluate the performance of the RI-ViT model. The experimental results of the proposed model show that it achieves accuracies of 99.75%, 98.80%, 98.01%, and 97.53% at magnifications of 40X, 100X, 200X, and 400X, respectively. The RI-ViT model can also perform well in an magnification-independent mode. Results show that, regardless of the magnification level, it achieves an accuracy of 99.37%, demonstrating its superiority over other state-of-the-art models.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/70211e2b04fcb4413edfd43e75b9f10e8d130171.pdf",
      "citation_key": "monjezi2024tdt",
      "metadata": {
        "title": "RI-ViT: A Multi-Scale Hybrid Method Based on Vision Transformer for Breast Cancer Detection in Histopathological Images",
        "authors": [
          "Ehsan Monjezi",
          "G. Akbarizadeh",
          "Karim Ansari-Asl"
        ],
        "published_date": "2024",
        "abstract": "Breast cancer is one of the most significant health threats to women worldwide. This disease manifests through abnormal proliferation of cells and the formation of tumors in breast tissue. Definitive breast cancer diagnosis is usually determined by analyzing tissue samples obtained from biopsies and reviewing them by pathologists. However, this method is highly dependent on the knowledge and experience of pathologists and may lead to errors due to the subjective nature of human interpretation and the high volume of cases. This study presents a multi-scale hybrid model based on Vision Transformer and residual networks for breast cancer detection in histopathological images, abbreviated as RI-ViT. In this approach, local features are extracted through a combination of residual stages and multi-scale learning, while global features are obtained using the attention mechanism in transformers. This combination enables simultaneous extraction of both local and global features from histopathological images, effectively improving the models performance in detecting complex cases. We have used an imbalanced and publicly available dataset called BreakHis to evaluate the performance of the RI-ViT model. The experimental results of the proposed model show that it achieves accuracies of 99.75%, 98.80%, 98.01%, and 97.53% at magnifications of 40X, 100X, 200X, and 400X, respectively. The RI-ViT model can also perform well in an magnification-independent mode. Results show that, regardless of the magnification level, it achieves an accuracy of 99.37%, demonstrating its superiority over other state-of-the-art models.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/70211e2b04fcb4413edfd43e75b9f10e8d130171.pdf",
        "venue": "IEEE Access",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Breast cancer is one of the most significant health threats to women worldwide. This disease manifests through abnormal proliferation of cells and the formation of tumors in breast tissue. Definitive breast cancer diagnosis is usually determined by analyzing tissue samples obtained from biopsies and reviewing them by pathologists. However, this method is highly dependent on the knowledge and experience of pathologists and may lead to errors due to the subjective nature of human interpretation and the high volume of cases. This study presents a multi-scale hybrid model based on Vision Transformer and residual networks for breast cancer detection in histopathological images, abbreviated as RI-ViT. In this approach, local features are extracted through a combination of residual stages and multi-scale learning, while global features are obtained using the attention mechanism in transformers. This combination enables simultaneous extraction of both local and global features from histopathological images, effectively improving the models performance in detecting complex cases. We have used an imbalanced and publicly available dataset called BreakHis to evaluate the performance of the RI-ViT model. The experimental results of the proposed model show that it achieves accuracies of 99.75%, 98.80%, 98.01%, and 97.53% at magnifications of 40X, 100X, 200X, and 400X, respectively. The RI-ViT model can also perform well in an magnification-independent mode. Results show that, regardless of the magnification level, it achieves an accuracy of 99.37%, demonstrating its superiority over other state-of-the-art models.",
        "keywords": []
      },
      "file_name": "70211e2b04fcb4413edfd43e75b9f10e8d130171.pdf"
    },
    {
      "success": true,
      "doc_id": "6a2bf67ccaf5678cb1fbc5a51fefdbb0",
      "summary": "This study proposes an advanced plant disease classification framework leveraging the Attention Score-Based Multi-Vision Transformer (Multi-ViT) model. The framework introduces a novel attention mechanism to dynamically prioritize relevant features from multiple leaf images, overcoming the limitations of single-leaf-based diagnoses. Building on the Vision Transformer (ViT) architecture, the Multi-ViT model aggregates diverse feature representations by combining outputs from multiple ViTs, each capturing unique visual patterns. This approach allows for a holistic analysis of spatially distributed symptoms, crucial for accurately diagnosing diseases in trees. Extensive experiments conducted on apple, grape, and tomato leaf disease datasets demonstrate the models superior performance, achieving over 99% accuracy and significantly improving F1 scores compared to traditional methods such as ResNet, VGG, and MobileNet. These findings underscore the effectiveness of the proposed model for precise and reliable plant disease classification.",
      "intriguing_abstract": "This study proposes an advanced plant disease classification framework leveraging the Attention Score-Based Multi-Vision Transformer (Multi-ViT) model. The framework introduces a novel attention mechanism to dynamically prioritize relevant features from multiple leaf images, overcoming the limitations of single-leaf-based diagnoses. Building on the Vision Transformer (ViT) architecture, the Multi-ViT model aggregates diverse feature representations by combining outputs from multiple ViTs, each capturing unique visual patterns. This approach allows for a holistic analysis of spatially distributed symptoms, crucial for accurately diagnosing diseases in trees. Extensive experiments conducted on apple, grape, and tomato leaf disease datasets demonstrate the models superior performance, achieving over 99% accuracy and significantly improving F1 scores compared to traditional methods such as ResNet, VGG, and MobileNet. These findings underscore the effectiveness of the proposed model for precise and reliable plant disease classification.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/cc817ba09f1c3c94bae73576463bcaf57c70261a.pdf",
      "citation_key": "baek2025h8e",
      "metadata": {
        "title": "Attention Score-Based Multi-Vision Transformer Technique for Plant Disease Classification",
        "authors": [
          "Eu-tteum Baek"
        ],
        "published_date": "2025",
        "abstract": "This study proposes an advanced plant disease classification framework leveraging the Attention Score-Based Multi-Vision Transformer (Multi-ViT) model. The framework introduces a novel attention mechanism to dynamically prioritize relevant features from multiple leaf images, overcoming the limitations of single-leaf-based diagnoses. Building on the Vision Transformer (ViT) architecture, the Multi-ViT model aggregates diverse feature representations by combining outputs from multiple ViTs, each capturing unique visual patterns. This approach allows for a holistic analysis of spatially distributed symptoms, crucial for accurately diagnosing diseases in trees. Extensive experiments conducted on apple, grape, and tomato leaf disease datasets demonstrate the models superior performance, achieving over 99% accuracy and significantly improving F1 scores compared to traditional methods such as ResNet, VGG, and MobileNet. These findings underscore the effectiveness of the proposed model for precise and reliable plant disease classification.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/cc817ba09f1c3c94bae73576463bcaf57c70261a.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 6,
        "score": 6.0,
        "summary": "This study proposes an advanced plant disease classification framework leveraging the Attention Score-Based Multi-Vision Transformer (Multi-ViT) model. The framework introduces a novel attention mechanism to dynamically prioritize relevant features from multiple leaf images, overcoming the limitations of single-leaf-based diagnoses. Building on the Vision Transformer (ViT) architecture, the Multi-ViT model aggregates diverse feature representations by combining outputs from multiple ViTs, each capturing unique visual patterns. This approach allows for a holistic analysis of spatially distributed symptoms, crucial for accurately diagnosing diseases in trees. Extensive experiments conducted on apple, grape, and tomato leaf disease datasets demonstrate the models superior performance, achieving over 99% accuracy and significantly improving F1 scores compared to traditional methods such as ResNet, VGG, and MobileNet. These findings underscore the effectiveness of the proposed model for precise and reliable plant disease classification.",
        "keywords": []
      },
      "file_name": "cc817ba09f1c3c94bae73576463bcaf57c70261a.pdf"
    },
    {
      "success": true,
      "doc_id": "e58dbd920772af3306fff142f4cd9ff5",
      "summary": "BACKGROUND AND PURPOSE: Cervical spinal cord compression, defined as spinal cord deformity and severe narrowing of the spinal canal in the cervical region, can lead to severe clinical consequences, including intractable pain, sensory disturbance, paralysis, and even death, and may require emergent intervention to prevent negative outcomes. Despite the critical nature of cord compression, no automated tool is available to alert clinical radiologists to the presence of such findings. This study aims to demonstrate the ability of a vision transformer (ViT) model for the accurate detection of cervical cord compression. MATERIALS AND METHODS: A clinically diverse cohort of 142 cervical spine MRIs was identified, 34% of which were normal or had mild stenosis, 31% with moderate stenosis, and 35% with cord compression. Utilizing gradient-echo images, slices were labeled as no cord compression/mild stenosis, moderate stenosis, or severe stenosis/cord compression. Segmentation of the spinal canal was performed and confirmed by neuroradiology faculty. A pretrained ViT model was fine-tuned to predict section-level severity by using a train:validation:test split of 60:20:20. Each examination was assigned an overall severity based on the highest level of section severity, with an examination labeled as positive for cord compression if 1 section was predicted in the severe category. Additionally, 2 convolutional neural network (CNN) models (ResNet50, DenseNet121) were tested in the same manner. RESULTS: The ViT model outperformed both CNN models at the section level, achieving section-level accuracy of 82%, compared with 72% and 78% for ResNet and DenseNet121, respectively. ViT patient-level classification achieved accuracy of 93%, sensitivity of 0.90, positive predictive value of 0.90, specificity of 0.95, and negative predictive value of 0.95. Receiver operating characteristic area under the curve was greater for ViT than either CNN. CONCLUSIONS: This classification approach using a ViT model and rules-based classification accurately detects the presence of cervical spinal cord compression at the patient level. In this study, the ViT model outperformed both conventional CNN approaches at the section and patient levels. If implemented into the clinical setting, such a tool may streamline neuroradiology workflow, improving efficiency and consistency.",
      "intriguing_abstract": "BACKGROUND AND PURPOSE: Cervical spinal cord compression, defined as spinal cord deformity and severe narrowing of the spinal canal in the cervical region, can lead to severe clinical consequences, including intractable pain, sensory disturbance, paralysis, and even death, and may require emergent intervention to prevent negative outcomes. Despite the critical nature of cord compression, no automated tool is available to alert clinical radiologists to the presence of such findings. This study aims to demonstrate the ability of a vision transformer (ViT) model for the accurate detection of cervical cord compression. MATERIALS AND METHODS: A clinically diverse cohort of 142 cervical spine MRIs was identified, 34% of which were normal or had mild stenosis, 31% with moderate stenosis, and 35% with cord compression. Utilizing gradient-echo images, slices were labeled as no cord compression/mild stenosis, moderate stenosis, or severe stenosis/cord compression. Segmentation of the spinal canal was performed and confirmed by neuroradiology faculty. A pretrained ViT model was fine-tuned to predict section-level severity by using a train:validation:test split of 60:20:20. Each examination was assigned an overall severity based on the highest level of section severity, with an examination labeled as positive for cord compression if 1 section was predicted in the severe category. Additionally, 2 convolutional neural network (CNN) models (ResNet50, DenseNet121) were tested in the same manner. RESULTS: The ViT model outperformed both CNN models at the section level, achieving section-level accuracy of 82%, compared with 72% and 78% for ResNet and DenseNet121, respectively. ViT patient-level classification achieved accuracy of 93%, sensitivity of 0.90, positive predictive value of 0.90, specificity of 0.95, and negative predictive value of 0.95. Receiver operating characteristic area under the curve was greater for ViT than either CNN. CONCLUSIONS: This classification approach using a ViT model and rules-based classification accurately detects the presence of cervical spinal cord compression at the patient level. In this study, the ViT model outperformed both conventional CNN approaches at the section and patient levels. If implemented into the clinical setting, such a tool may streamline neuroradiology workflow, improving efficiency and consistency.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/77e4ea46e7248408844f62fa3613aa246e35fdf0.pdf",
      "citation_key": "payne2024u8l",
      "metadata": {
        "title": "Automated Detection of Cervical Spinal Stenosis and Cord Compression via Vision Transformer and Rules-Based Classification",
        "authors": [
          "David L. Payne",
          "Xuan Xu",
          "Farshid Faraji",
          "Kevin John",
          "K. F. Pradas",
          "Vahni Vishala Bernard",
          "Lev Bangiyev",
          "Prateek Prasanna"
        ],
        "published_date": "2024",
        "abstract": "BACKGROUND AND PURPOSE: Cervical spinal cord compression, defined as spinal cord deformity and severe narrowing of the spinal canal in the cervical region, can lead to severe clinical consequences, including intractable pain, sensory disturbance, paralysis, and even death, and may require emergent intervention to prevent negative outcomes. Despite the critical nature of cord compression, no automated tool is available to alert clinical radiologists to the presence of such findings. This study aims to demonstrate the ability of a vision transformer (ViT) model for the accurate detection of cervical cord compression. MATERIALS AND METHODS: A clinically diverse cohort of 142 cervical spine MRIs was identified, 34% of which were normal or had mild stenosis, 31% with moderate stenosis, and 35% with cord compression. Utilizing gradient-echo images, slices were labeled as no cord compression/mild stenosis, moderate stenosis, or severe stenosis/cord compression. Segmentation of the spinal canal was performed and confirmed by neuroradiology faculty. A pretrained ViT model was fine-tuned to predict section-level severity by using a train:validation:test split of 60:20:20. Each examination was assigned an overall severity based on the highest level of section severity, with an examination labeled as positive for cord compression if 1 section was predicted in the severe category. Additionally, 2 convolutional neural network (CNN) models (ResNet50, DenseNet121) were tested in the same manner. RESULTS: The ViT model outperformed both CNN models at the section level, achieving section-level accuracy of 82%, compared with 72% and 78% for ResNet and DenseNet121, respectively. ViT patient-level classification achieved accuracy of 93%, sensitivity of 0.90, positive predictive value of 0.90, specificity of 0.95, and negative predictive value of 0.95. Receiver operating characteristic area under the curve was greater for ViT than either CNN. CONCLUSIONS: This classification approach using a ViT model and rules-based classification accurately detects the presence of cervical spinal cord compression at the patient level. In this study, the ViT model outperformed both conventional CNN approaches at the section and patient levels. If implemented into the clinical setting, such a tool may streamline neuroradiology workflow, improving efficiency and consistency.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/77e4ea46e7248408844f62fa3613aa246e35fdf0.pdf",
        "venue": "American Journal of Neuroradiology",
        "citationCount": 6,
        "score": 6.0,
        "summary": "BACKGROUND AND PURPOSE: Cervical spinal cord compression, defined as spinal cord deformity and severe narrowing of the spinal canal in the cervical region, can lead to severe clinical consequences, including intractable pain, sensory disturbance, paralysis, and even death, and may require emergent intervention to prevent negative outcomes. Despite the critical nature of cord compression, no automated tool is available to alert clinical radiologists to the presence of such findings. This study aims to demonstrate the ability of a vision transformer (ViT) model for the accurate detection of cervical cord compression. MATERIALS AND METHODS: A clinically diverse cohort of 142 cervical spine MRIs was identified, 34% of which were normal or had mild stenosis, 31% with moderate stenosis, and 35% with cord compression. Utilizing gradient-echo images, slices were labeled as no cord compression/mild stenosis, moderate stenosis, or severe stenosis/cord compression. Segmentation of the spinal canal was performed and confirmed by neuroradiology faculty. A pretrained ViT model was fine-tuned to predict section-level severity by using a train:validation:test split of 60:20:20. Each examination was assigned an overall severity based on the highest level of section severity, with an examination labeled as positive for cord compression if 1 section was predicted in the severe category. Additionally, 2 convolutional neural network (CNN) models (ResNet50, DenseNet121) were tested in the same manner. RESULTS: The ViT model outperformed both CNN models at the section level, achieving section-level accuracy of 82%, compared with 72% and 78% for ResNet and DenseNet121, respectively. ViT patient-level classification achieved accuracy of 93%, sensitivity of 0.90, positive predictive value of 0.90, specificity of 0.95, and negative predictive value of 0.95. Receiver operating characteristic area under the curve was greater for ViT than either CNN. CONCLUSIONS: This classification approach using a ViT model and rules-based classification accurately detects the presence of cervical spinal cord compression at the patient level. In this study, the ViT model outperformed both conventional CNN approaches at the section and patient levels. If implemented into the clinical setting, such a tool may streamline neuroradiology workflow, improving efficiency and consistency.",
        "keywords": []
      },
      "file_name": "77e4ea46e7248408844f62fa3613aa246e35fdf0.pdf"
    },
    {
      "success": true,
      "doc_id": "7770a401d6973cfbfa6350ee5df241d1",
      "summary": "Abstract Epileptic seizures are unpredictable events caused by abnormal discharges of a patients brain cells. Extensive research has been conducted to develop seizure prediction algorithms based on long-term continuous electroencephalogram (EEG) signals. This paper describes a patient-specific seizure prediction method that can serve as a basis for the design of lightweight, wearable and effective seizure-prediction devices. We aim to achieve two objectives using this method. The first aim is to extract robust feature representations from multichannel EEG signals, and the second aim is to reduce the number of channels used for prediction by selecting an optimal set of channels from multichannel EEG signals while ensuring good prediction performance. We design a seizure-prediction algorithm based on a vision transformer (ViT) model. The algorithm selects channels that play a key role in seizure prediction from 22 channels of EEG signals. First, we perform a time-frequency analysis of processed time-series signals to obtain EEG spectrograms. We then segment the spectrograms of multiple channels into many non-overlapping patches of the same size, which are input into the channel selection layer of the proposed model, named Sel-JPM-ViT, enabling it to select channels. Application of the Sel-JPM-ViT model to the Boston Childrens HospitalMassachusetts Institute of Technology scalp EEG dataset yields results using only three to six channels of EEG signals that are slightly better that the results obtained using 22 channels of EEG signals. Overall, the Sel-JPM-ViT model exhibits an average classification accuracy of 93.65%, an average sensitivity of 94.70% and an average specificity of 92.78%.",
      "intriguing_abstract": "Abstract Epileptic seizures are unpredictable events caused by abnormal discharges of a patients brain cells. Extensive research has been conducted to develop seizure prediction algorithms based on long-term continuous electroencephalogram (EEG) signals. This paper describes a patient-specific seizure prediction method that can serve as a basis for the design of lightweight, wearable and effective seizure-prediction devices. We aim to achieve two objectives using this method. The first aim is to extract robust feature representations from multichannel EEG signals, and the second aim is to reduce the number of channels used for prediction by selecting an optimal set of channels from multichannel EEG signals while ensuring good prediction performance. We design a seizure-prediction algorithm based on a vision transformer (ViT) model. The algorithm selects channels that play a key role in seizure prediction from 22 channels of EEG signals. First, we perform a time-frequency analysis of processed time-series signals to obtain EEG spectrograms. We then segment the spectrograms of multiple channels into many non-overlapping patches of the same size, which are input into the channel selection layer of the proposed model, named Sel-JPM-ViT, enabling it to select channels. Application of the Sel-JPM-ViT model to the Boston Childrens HospitalMassachusetts Institute of Technology scalp EEG dataset yields results using only three to six channels of EEG signals that are slightly better that the results obtained using 22 channels of EEG signals. Overall, the Sel-JPM-ViT model exhibits an average classification accuracy of 93.65%, an average sensitivity of 94.70% and an average specificity of 92.78%.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/271c6a87213b8b2a26ce3e1a6f20b07cc2f82b81.pdf",
      "citation_key": "qi2024rzy",
      "metadata": {
        "title": "Seizure prediction based on improved vision transformer model for EEG channel optimization",
        "authors": [
          "Nan Qi",
          "Yan Piao",
          "Hao Zhang",
          "Qi Wang",
          "Yue Wang"
        ],
        "published_date": "2024",
        "abstract": "Abstract Epileptic seizures are unpredictable events caused by abnormal discharges of a patients brain cells. Extensive research has been conducted to develop seizure prediction algorithms based on long-term continuous electroencephalogram (EEG) signals. This paper describes a patient-specific seizure prediction method that can serve as a basis for the design of lightweight, wearable and effective seizure-prediction devices. We aim to achieve two objectives using this method. The first aim is to extract robust feature representations from multichannel EEG signals, and the second aim is to reduce the number of channels used for prediction by selecting an optimal set of channels from multichannel EEG signals while ensuring good prediction performance. We design a seizure-prediction algorithm based on a vision transformer (ViT) model. The algorithm selects channels that play a key role in seizure prediction from 22 channels of EEG signals. First, we perform a time-frequency analysis of processed time-series signals to obtain EEG spectrograms. We then segment the spectrograms of multiple channels into many non-overlapping patches of the same size, which are input into the channel selection layer of the proposed model, named Sel-JPM-ViT, enabling it to select channels. Application of the Sel-JPM-ViT model to the Boston Childrens HospitalMassachusetts Institute of Technology scalp EEG dataset yields results using only three to six channels of EEG signals that are slightly better that the results obtained using 22 channels of EEG signals. Overall, the Sel-JPM-ViT model exhibits an average classification accuracy of 93.65%, an average sensitivity of 94.70% and an average specificity of 92.78%.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/271c6a87213b8b2a26ce3e1a6f20b07cc2f82b81.pdf",
        "venue": "Computer Methods in Biomechanics and Biomedical Engineering",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Abstract Epileptic seizures are unpredictable events caused by abnormal discharges of a patients brain cells. Extensive research has been conducted to develop seizure prediction algorithms based on long-term continuous electroencephalogram (EEG) signals. This paper describes a patient-specific seizure prediction method that can serve as a basis for the design of lightweight, wearable and effective seizure-prediction devices. We aim to achieve two objectives using this method. The first aim is to extract robust feature representations from multichannel EEG signals, and the second aim is to reduce the number of channels used for prediction by selecting an optimal set of channels from multichannel EEG signals while ensuring good prediction performance. We design a seizure-prediction algorithm based on a vision transformer (ViT) model. The algorithm selects channels that play a key role in seizure prediction from 22 channels of EEG signals. First, we perform a time-frequency analysis of processed time-series signals to obtain EEG spectrograms. We then segment the spectrograms of multiple channels into many non-overlapping patches of the same size, which are input into the channel selection layer of the proposed model, named Sel-JPM-ViT, enabling it to select channels. Application of the Sel-JPM-ViT model to the Boston Childrens HospitalMassachusetts Institute of Technology scalp EEG dataset yields results using only three to six channels of EEG signals that are slightly better that the results obtained using 22 channels of EEG signals. Overall, the Sel-JPM-ViT model exhibits an average classification accuracy of 93.65%, an average sensitivity of 94.70% and an average specificity of 92.78%.",
        "keywords": []
      },
      "file_name": "271c6a87213b8b2a26ce3e1a6f20b07cc2f82b81.pdf"
    },
    {
      "success": true,
      "doc_id": "cfa7c1d3c2a23766f60d74b4b7dd4b93",
      "summary": "Mobile eye tracking captures egocentric vision and is well-suited for naturalistic studies. However, its data is noisy, especially when acquired outdoor with multiple participants over several sessions. Area of interest analysis on moving targets is difficult because A) camera and objects move nonlinearly and may disappear/reappear from the scene; and B) off-the-shelf analysis tools are limited to linearly moving objects. As a result, researchers resort to time-consuming manual annotation, which limits the use of mobile eye tracking in naturalistic studies. We introduce a method based on a fine-tuned Vision Transformer (ViT) model for classifying frames with overlaying gaze markers. After fine-tuning a model on a manually labelled training set made of 1.98% (=7845 frames) of our entire data for three epochs, our model reached 99.34% accuracy as evaluated on hold-out data. We used the method to quantify participants dwell time on a tablet during the outdoor user test of a mobile augmented reality application for biodiversity education. We discuss the benefits and limitations of our approach and its potential to be applied to other contexts.",
      "intriguing_abstract": "Mobile eye tracking captures egocentric vision and is well-suited for naturalistic studies. However, its data is noisy, especially when acquired outdoor with multiple participants over several sessions. Area of interest analysis on moving targets is difficult because A) camera and objects move nonlinearly and may disappear/reappear from the scene; and B) off-the-shelf analysis tools are limited to linearly moving objects. As a result, researchers resort to time-consuming manual annotation, which limits the use of mobile eye tracking in naturalistic studies. We introduce a method based on a fine-tuned Vision Transformer (ViT) model for classifying frames with overlaying gaze markers. After fine-tuning a model on a manually labelled training set made of 1.98% (=7845 frames) of our entire data for three epochs, our model reached 99.34% accuracy as evaluated on hold-out data. We used the method to quantify participants dwell time on a tablet during the outdoor user test of a mobile augmented reality application for biodiversity education. We discuss the benefits and limitations of our approach and its potential to be applied to other contexts.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/05d15576d88f9384738908f98716f91bdb5dbc78.pdf",
      "citation_key": "mercier2024063",
      "metadata": {
        "title": "Quantifying Dwell Time With Location-based Augmented Reality: Dynamic AOI Analysis on Mobile Eye Tracking Data With Vision Transformer",
        "authors": [
          "J. Mercier",
          "O. Ertz",
          "E. Bocher"
        ],
        "published_date": "2024",
        "abstract": "Mobile eye tracking captures egocentric vision and is well-suited for naturalistic studies. However, its data is noisy, especially when acquired outdoor with multiple participants over several sessions. Area of interest analysis on moving targets is difficult because A) camera and objects move nonlinearly and may disappear/reappear from the scene; and B) off-the-shelf analysis tools are limited to linearly moving objects. As a result, researchers resort to time-consuming manual annotation, which limits the use of mobile eye tracking in naturalistic studies. We introduce a method based on a fine-tuned Vision Transformer (ViT) model for classifying frames with overlaying gaze markers. After fine-tuning a model on a manually labelled training set made of 1.98% (=7845 frames) of our entire data for three epochs, our model reached 99.34% accuracy as evaluated on hold-out data. We used the method to quantify participants dwell time on a tablet during the outdoor user test of a mobile augmented reality application for biodiversity education. We discuss the benefits and limitations of our approach and its potential to be applied to other contexts.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/05d15576d88f9384738908f98716f91bdb5dbc78.pdf",
        "venue": "Journal of Eye Movement Research",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Mobile eye tracking captures egocentric vision and is well-suited for naturalistic studies. However, its data is noisy, especially when acquired outdoor with multiple participants over several sessions. Area of interest analysis on moving targets is difficult because A) camera and objects move nonlinearly and may disappear/reappear from the scene; and B) off-the-shelf analysis tools are limited to linearly moving objects. As a result, researchers resort to time-consuming manual annotation, which limits the use of mobile eye tracking in naturalistic studies. We introduce a method based on a fine-tuned Vision Transformer (ViT) model for classifying frames with overlaying gaze markers. After fine-tuning a model on a manually labelled training set made of 1.98% (=7845 frames) of our entire data for three epochs, our model reached 99.34% accuracy as evaluated on hold-out data. We used the method to quantify participants dwell time on a tablet during the outdoor user test of a mobile augmented reality application for biodiversity education. We discuss the benefits and limitations of our approach and its potential to be applied to other contexts.",
        "keywords": []
      },
      "file_name": "05d15576d88f9384738908f98716f91bdb5dbc78.pdf"
    },
    {
      "success": true,
      "doc_id": "6cf1c2ea6651e0e42937b003d1ea5ee2",
      "summary": "Polyp detection is a challenging task in the diagnosis of Colorectal Cancer (CRC), and it demands clinical expertise due to the diverse nature of polyps. The recent years have witnessed the development of automated polyp detection systems to assist the experts in early diagnosis, considerably reducing the time consumption and diagnostic errors. In automated CRC diagnosis, polyp segmentation is an important step which is carried out with deep learning segmentation models. Recently, Vision Transformers (ViT) are slowly replacing these models due to their ability to capture long range dependencies among image patches. However, the existing ViTs for polyp do not harness the inherent self-attention abilities and incorporate complex attention mechanisms. This paper presents Polyp-Vision Transformer (Polyp-ViT), a novel Transformer model based on the conventional Transformer architecture, which is enhanced with adaptive mechanisms for feature extraction and positional embedding. Polyp-ViT is tested on the Kvasir-seg and CVC-Clinic DB Datasets achieving segmentation accuracies of 0.98910.01 and 0.98750.71 respectively, outperforming state-of-the-art models. Polyp-ViT is a prospective tool for polyp segmentation which can be adapted to other medical image segmentation tasks as well due to its ability to generalize well.",
      "intriguing_abstract": "Polyp detection is a challenging task in the diagnosis of Colorectal Cancer (CRC), and it demands clinical expertise due to the diverse nature of polyps. The recent years have witnessed the development of automated polyp detection systems to assist the experts in early diagnosis, considerably reducing the time consumption and diagnostic errors. In automated CRC diagnosis, polyp segmentation is an important step which is carried out with deep learning segmentation models. Recently, Vision Transformers (ViT) are slowly replacing these models due to their ability to capture long range dependencies among image patches. However, the existing ViTs for polyp do not harness the inherent self-attention abilities and incorporate complex attention mechanisms. This paper presents Polyp-Vision Transformer (Polyp-ViT), a novel Transformer model based on the conventional Transformer architecture, which is enhanced with adaptive mechanisms for feature extraction and positional embedding. Polyp-ViT is tested on the Kvasir-seg and CVC-Clinic DB Datasets achieving segmentation accuracies of 0.98910.01 and 0.98750.71 respectively, outperforming state-of-the-art models. Polyp-ViT is a prospective tool for polyp segmentation which can be adapted to other medical image segmentation tasks as well due to its ability to generalize well.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/6feea99a1a6c876d6f4f714b463da9c95998dee7.pdf",
      "citation_key": "sikkandar2024p0d",
      "metadata": {
        "title": "Utilizing adaptive deformable convolution and position embedding for colon polyp segmentation with a visual transformer",
        "authors": [
          "Mohamed Yacin Sikkandar",
          "S. Sundaram",
          "Ahmad Alassaf",
          "Ibrahim AlMohimeed",
          "Khalid Alhussaini",
          "Adham Aleid",
          "S. Alolayan",
          "P. Ramkumar",
          "Meshal Khalaf Almutairi",
          "S. Begum"
        ],
        "published_date": "2024",
        "abstract": "Polyp detection is a challenging task in the diagnosis of Colorectal Cancer (CRC), and it demands clinical expertise due to the diverse nature of polyps. The recent years have witnessed the development of automated polyp detection systems to assist the experts in early diagnosis, considerably reducing the time consumption and diagnostic errors. In automated CRC diagnosis, polyp segmentation is an important step which is carried out with deep learning segmentation models. Recently, Vision Transformers (ViT) are slowly replacing these models due to their ability to capture long range dependencies among image patches. However, the existing ViTs for polyp do not harness the inherent self-attention abilities and incorporate complex attention mechanisms. This paper presents Polyp-Vision Transformer (Polyp-ViT), a novel Transformer model based on the conventional Transformer architecture, which is enhanced with adaptive mechanisms for feature extraction and positional embedding. Polyp-ViT is tested on the Kvasir-seg and CVC-Clinic DB Datasets achieving segmentation accuracies of 0.98910.01 and 0.98750.71 respectively, outperforming state-of-the-art models. Polyp-ViT is a prospective tool for polyp segmentation which can be adapted to other medical image segmentation tasks as well due to its ability to generalize well.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6feea99a1a6c876d6f4f714b463da9c95998dee7.pdf",
        "venue": "Scientific Reports",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Polyp detection is a challenging task in the diagnosis of Colorectal Cancer (CRC), and it demands clinical expertise due to the diverse nature of polyps. The recent years have witnessed the development of automated polyp detection systems to assist the experts in early diagnosis, considerably reducing the time consumption and diagnostic errors. In automated CRC diagnosis, polyp segmentation is an important step which is carried out with deep learning segmentation models. Recently, Vision Transformers (ViT) are slowly replacing these models due to their ability to capture long range dependencies among image patches. However, the existing ViTs for polyp do not harness the inherent self-attention abilities and incorporate complex attention mechanisms. This paper presents Polyp-Vision Transformer (Polyp-ViT), a novel Transformer model based on the conventional Transformer architecture, which is enhanced with adaptive mechanisms for feature extraction and positional embedding. Polyp-ViT is tested on the Kvasir-seg and CVC-Clinic DB Datasets achieving segmentation accuracies of 0.98910.01 and 0.98750.71 respectively, outperforming state-of-the-art models. Polyp-ViT is a prospective tool for polyp segmentation which can be adapted to other medical image segmentation tasks as well due to its ability to generalize well.",
        "keywords": []
      },
      "file_name": "6feea99a1a6c876d6f4f714b463da9c95998dee7.pdf"
    },
    {
      "success": true,
      "doc_id": "d08126e1f8a1b60fa1d11a6e917d7329",
      "summary": "Image super-resolution (SR) stands as a pivotal process in the domains of image processing and computer vision, finding diverse applications in film, television, photography, surveillance, medical imaging, and remote sensing. In the context of remote sensing images (RSIs), the inherent challenge arises from low spatial resolution caused by factors such as sensor noise, orbit height, and weather conditions, necessitating SR reconstruction. An evident limitation of prevailing methods lies in their dependence on idealized fixed degradation models, which fail to capture the intricate degradation processes unique to remote sensing scenes. In response to these constraints, this article introduces an innovative blind image super-resolution reconstruction method tailored for remote sensing images. The proposed approach integrates convolution with a transformer and incorporates an amplitude-phase learning module (ALM) to comprehensively capture local and long-range dependencies while enhancing frequency information. The iterative optimization strategy refines texture information by carefully balancing structural and detail elements. Key contributions include a holistic approach to remote sensing image SR, ALM integration for precise feature representation, and the introduction of a patch-based frequency loss mechanism for evaluating frequency-domain features. Rigorous experiments demonstrate that compared with other state-of-the-art (SOTA) methods, the proposed algorithm delivers SR results with exceptional visual perception quality across three distinct remote sensing datasets.",
      "intriguing_abstract": "Image super-resolution (SR) stands as a pivotal process in the domains of image processing and computer vision, finding diverse applications in film, television, photography, surveillance, medical imaging, and remote sensing. In the context of remote sensing images (RSIs), the inherent challenge arises from low spatial resolution caused by factors such as sensor noise, orbit height, and weather conditions, necessitating SR reconstruction. An evident limitation of prevailing methods lies in their dependence on idealized fixed degradation models, which fail to capture the intricate degradation processes unique to remote sensing scenes. In response to these constraints, this article introduces an innovative blind image super-resolution reconstruction method tailored for remote sensing images. The proposed approach integrates convolution with a transformer and incorporates an amplitude-phase learning module (ALM) to comprehensively capture local and long-range dependencies while enhancing frequency information. The iterative optimization strategy refines texture information by carefully balancing structural and detail elements. Key contributions include a holistic approach to remote sensing image SR, ALM integration for precise feature representation, and the introduction of a patch-based frequency loss mechanism for evaluating frequency-domain features. Rigorous experiments demonstrate that compared with other state-of-the-art (SOTA) methods, the proposed algorithm delivers SR results with exceptional visual perception quality across three distinct remote sensing datasets.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/c05744f690ab9db007012a63c3c5c3ca48201c66.pdf",
      "citation_key": "hou2024e4y",
      "metadata": {
        "title": "CSwT-SR: Conv-Swin Transformer for Blind Remote Sensing Image Super-Resolution With Amplitude-Phase Learning and Structural Detail Alternating Learning",
        "authors": [
          "Mingyang Hou",
          "Zhiyong Huang",
          "Zhi Yu",
          "Yan Yan",
          "Yunlan Zhao",
          "Xiao Han"
        ],
        "published_date": "2024",
        "abstract": "Image super-resolution (SR) stands as a pivotal process in the domains of image processing and computer vision, finding diverse applications in film, television, photography, surveillance, medical imaging, and remote sensing. In the context of remote sensing images (RSIs), the inherent challenge arises from low spatial resolution caused by factors such as sensor noise, orbit height, and weather conditions, necessitating SR reconstruction. An evident limitation of prevailing methods lies in their dependence on idealized fixed degradation models, which fail to capture the intricate degradation processes unique to remote sensing scenes. In response to these constraints, this article introduces an innovative blind image super-resolution reconstruction method tailored for remote sensing images. The proposed approach integrates convolution with a transformer and incorporates an amplitude-phase learning module (ALM) to comprehensively capture local and long-range dependencies while enhancing frequency information. The iterative optimization strategy refines texture information by carefully balancing structural and detail elements. Key contributions include a holistic approach to remote sensing image SR, ALM integration for precise feature representation, and the introduction of a patch-based frequency loss mechanism for evaluating frequency-domain features. Rigorous experiments demonstrate that compared with other state-of-the-art (SOTA) methods, the proposed algorithm delivers SR results with exceptional visual perception quality across three distinct remote sensing datasets.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c05744f690ab9db007012a63c3c5c3ca48201c66.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Image super-resolution (SR) stands as a pivotal process in the domains of image processing and computer vision, finding diverse applications in film, television, photography, surveillance, medical imaging, and remote sensing. In the context of remote sensing images (RSIs), the inherent challenge arises from low spatial resolution caused by factors such as sensor noise, orbit height, and weather conditions, necessitating SR reconstruction. An evident limitation of prevailing methods lies in their dependence on idealized fixed degradation models, which fail to capture the intricate degradation processes unique to remote sensing scenes. In response to these constraints, this article introduces an innovative blind image super-resolution reconstruction method tailored for remote sensing images. The proposed approach integrates convolution with a transformer and incorporates an amplitude-phase learning module (ALM) to comprehensively capture local and long-range dependencies while enhancing frequency information. The iterative optimization strategy refines texture information by carefully balancing structural and detail elements. Key contributions include a holistic approach to remote sensing image SR, ALM integration for precise feature representation, and the introduction of a patch-based frequency loss mechanism for evaluating frequency-domain features. Rigorous experiments demonstrate that compared with other state-of-the-art (SOTA) methods, the proposed algorithm delivers SR results with exceptional visual perception quality across three distinct remote sensing datasets.",
        "keywords": []
      },
      "file_name": "c05744f690ab9db007012a63c3c5c3ca48201c66.pdf"
    },
    {
      "success": true,
      "doc_id": "e7953ba088a8968441bbc47991214fdc",
      "summary": "Background: Food image recognition, a crucial step in computational gastronomy, has diverse applications across nutritional platforms. Convolutional neural networks (CNNs) are widely used for this task due to their ability to capture hierarchical features. However, they struggle with long-range dependencies and global feature extraction, which are vital in distinguishing visually similar foods or images where the context of the whole dish is crucial, thus necessitating transformer architecture. Objectives: This research explores the capabilities of the CNNs and transformers to build a robust classification model that can handle both short- and long-range dependencies with global features to accurately classify food images and enhance food image recognition for better nutritional analysis. Methods: Our approach, which combines CNNs and Vision Transformers (ViTs), begins with the RestNet50 backbone model. This model is responsible for local feature extraction from the input image. The resulting feature map is then passed to the ViT encoder block, which handles further global feature extraction and classification using multi-head attention and fully connected layers with pre-trained weights. Results: Our experiments on five diverse datasets have confirmed a superior performance compared to the current state-of-the-art methods, and our combined dataset leveraging complementary features showed enhanced generalizability and robust performance in addressing global food diversity. We used explainable techniques like grad-CAM and LIME to understand how the models made their decisions, thereby enhancing the users trust in the proposed system. This model has been integrated into a mobile application for food recognition and nutrition analysis, offering features like an intelligent diet-tracking system. Conclusion: This research paves the way for practical applications in personalized nutrition and healthcare, showcasing the extensive potential of AI in nutritional sciences across various dietary platforms.",
      "intriguing_abstract": "Background: Food image recognition, a crucial step in computational gastronomy, has diverse applications across nutritional platforms. Convolutional neural networks (CNNs) are widely used for this task due to their ability to capture hierarchical features. However, they struggle with long-range dependencies and global feature extraction, which are vital in distinguishing visually similar foods or images where the context of the whole dish is crucial, thus necessitating transformer architecture. Objectives: This research explores the capabilities of the CNNs and transformers to build a robust classification model that can handle both short- and long-range dependencies with global features to accurately classify food images and enhance food image recognition for better nutritional analysis. Methods: Our approach, which combines CNNs and Vision Transformers (ViTs), begins with the RestNet50 backbone model. This model is responsible for local feature extraction from the input image. The resulting feature map is then passed to the ViT encoder block, which handles further global feature extraction and classification using multi-head attention and fully connected layers with pre-trained weights. Results: Our experiments on five diverse datasets have confirmed a superior performance compared to the current state-of-the-art methods, and our combined dataset leveraging complementary features showed enhanced generalizability and robust performance in addressing global food diversity. We used explainable techniques like grad-CAM and LIME to understand how the models made their decisions, thereby enhancing the users trust in the proposed system. This model has been integrated into a mobile application for food recognition and nutrition analysis, offering features like an intelligent diet-tracking system. Conclusion: This research paves the way for practical applications in personalized nutrition and healthcare, showcasing the extensive potential of AI in nutritional sciences across various dietary platforms.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/cebc29015a2827aa8a98ec39c90c93bca2f14848.pdf",
      "citation_key": "nfor2025o20",
      "metadata": {
        "title": "An Explainable CNN and Vision Transformer-Based Approach for Real-Time Food Recognition",
        "authors": [
          "Kintoh Allen Nfor",
          "Tagne Poupi Theodore Armand",
          "Kenesbaeva Periyzat Ismaylovna",
          "Moon-Il Joo",
          "Hee-Cheol Kim"
        ],
        "published_date": "2025",
        "abstract": "Background: Food image recognition, a crucial step in computational gastronomy, has diverse applications across nutritional platforms. Convolutional neural networks (CNNs) are widely used for this task due to their ability to capture hierarchical features. However, they struggle with long-range dependencies and global feature extraction, which are vital in distinguishing visually similar foods or images where the context of the whole dish is crucial, thus necessitating transformer architecture. Objectives: This research explores the capabilities of the CNNs and transformers to build a robust classification model that can handle both short- and long-range dependencies with global features to accurately classify food images and enhance food image recognition for better nutritional analysis. Methods: Our approach, which combines CNNs and Vision Transformers (ViTs), begins with the RestNet50 backbone model. This model is responsible for local feature extraction from the input image. The resulting feature map is then passed to the ViT encoder block, which handles further global feature extraction and classification using multi-head attention and fully connected layers with pre-trained weights. Results: Our experiments on five diverse datasets have confirmed a superior performance compared to the current state-of-the-art methods, and our combined dataset leveraging complementary features showed enhanced generalizability and robust performance in addressing global food diversity. We used explainable techniques like grad-CAM and LIME to understand how the models made their decisions, thereby enhancing the users trust in the proposed system. This model has been integrated into a mobile application for food recognition and nutrition analysis, offering features like an intelligent diet-tracking system. Conclusion: This research paves the way for practical applications in personalized nutrition and healthcare, showcasing the extensive potential of AI in nutritional sciences across various dietary platforms.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/cebc29015a2827aa8a98ec39c90c93bca2f14848.pdf",
        "venue": "Nutrients",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Background: Food image recognition, a crucial step in computational gastronomy, has diverse applications across nutritional platforms. Convolutional neural networks (CNNs) are widely used for this task due to their ability to capture hierarchical features. However, they struggle with long-range dependencies and global feature extraction, which are vital in distinguishing visually similar foods or images where the context of the whole dish is crucial, thus necessitating transformer architecture. Objectives: This research explores the capabilities of the CNNs and transformers to build a robust classification model that can handle both short- and long-range dependencies with global features to accurately classify food images and enhance food image recognition for better nutritional analysis. Methods: Our approach, which combines CNNs and Vision Transformers (ViTs), begins with the RestNet50 backbone model. This model is responsible for local feature extraction from the input image. The resulting feature map is then passed to the ViT encoder block, which handles further global feature extraction and classification using multi-head attention and fully connected layers with pre-trained weights. Results: Our experiments on five diverse datasets have confirmed a superior performance compared to the current state-of-the-art methods, and our combined dataset leveraging complementary features showed enhanced generalizability and robust performance in addressing global food diversity. We used explainable techniques like grad-CAM and LIME to understand how the models made their decisions, thereby enhancing the users trust in the proposed system. This model has been integrated into a mobile application for food recognition and nutrition analysis, offering features like an intelligent diet-tracking system. Conclusion: This research paves the way for practical applications in personalized nutrition and healthcare, showcasing the extensive potential of AI in nutritional sciences across various dietary platforms.",
        "keywords": []
      },
      "file_name": "cebc29015a2827aa8a98ec39c90c93bca2f14848.pdf"
    },
    {
      "success": true,
      "doc_id": "e66a90b877f717d05b837b70b54a5585",
      "summary": "Visual inspection of the workplace and timely reminders of unsafe behaviors (e.g, not wearing a helmet) are particularly significant for avoiding injuries to workers on the construction site. Video surveillance systems generate large amounts of non-structure image data on site for this purpose; however, they require real-time recognition automation solutions based on computer vision. Although various deep-learning-based models have recently provided new ideas for identifying helmets in traffic monitoring, few solutions suitable for industry applications have been discussed due to the complex scenarios of construction sites. In this paper, a fast and robust network based on a mutilscale Swin Transformer is proposed for safety helmet detection (FRSHNet) at construction sites, which contains the following contributions. Firstly, MAE-NAS with the variant of MobileNetV3s MobBlock as a basic block is applied to implement feature extraction. Simultaneously, a multiscale Swin Transformer module is utilized to obtain the spatial and contexture relationships in the multiscale features. Subsequently, in order to meet the scheme requirements of real-time helmet detection, efficient RepGFPN are adopted to integrate refined multiscale features to form a pyramid structure. Extensive experiments were conducted on the publicly available Pictor-v3 and SHWD datasets. The experimental results show that FRSHNet consistently provided a favorable performance, outperforming the existing state-of-the-art models.",
      "intriguing_abstract": "Visual inspection of the workplace and timely reminders of unsafe behaviors (e.g, not wearing a helmet) are particularly significant for avoiding injuries to workers on the construction site. Video surveillance systems generate large amounts of non-structure image data on site for this purpose; however, they require real-time recognition automation solutions based on computer vision. Although various deep-learning-based models have recently provided new ideas for identifying helmets in traffic monitoring, few solutions suitable for industry applications have been discussed due to the complex scenarios of construction sites. In this paper, a fast and robust network based on a mutilscale Swin Transformer is proposed for safety helmet detection (FRSHNet) at construction sites, which contains the following contributions. Firstly, MAE-NAS with the variant of MobileNetV3s MobBlock as a basic block is applied to implement feature extraction. Simultaneously, a multiscale Swin Transformer module is utilized to obtain the spatial and contexture relationships in the multiscale features. Subsequently, in order to meet the scheme requirements of real-time helmet detection, efficient RepGFPN are adopted to integrate refined multiscale features to form a pyramid structure. Extensive experiments were conducted on the publicly available Pictor-v3 and SHWD datasets. The experimental results show that FRSHNet consistently provided a favorable performance, outperforming the existing state-of-the-art models.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/630c9993b6aaf812e1aa3dae8e243af131abd3d0.pdf",
      "citation_key": "xiang2024tww",
      "metadata": {
        "title": "A Fast and Robust Safety Helmet Network Based on a Mutilscale Swin Transformer",
        "authors": [
          "Changcheng Xiang",
          "Duofen Yin",
          "Fei Song",
          "Zaixue Yu",
          "Xu Jian",
          "Huaming Gong"
        ],
        "published_date": "2024",
        "abstract": "Visual inspection of the workplace and timely reminders of unsafe behaviors (e.g, not wearing a helmet) are particularly significant for avoiding injuries to workers on the construction site. Video surveillance systems generate large amounts of non-structure image data on site for this purpose; however, they require real-time recognition automation solutions based on computer vision. Although various deep-learning-based models have recently provided new ideas for identifying helmets in traffic monitoring, few solutions suitable for industry applications have been discussed due to the complex scenarios of construction sites. In this paper, a fast and robust network based on a mutilscale Swin Transformer is proposed for safety helmet detection (FRSHNet) at construction sites, which contains the following contributions. Firstly, MAE-NAS with the variant of MobileNetV3s MobBlock as a basic block is applied to implement feature extraction. Simultaneously, a multiscale Swin Transformer module is utilized to obtain the spatial and contexture relationships in the multiscale features. Subsequently, in order to meet the scheme requirements of real-time helmet detection, efficient RepGFPN are adopted to integrate refined multiscale features to form a pyramid structure. Extensive experiments were conducted on the publicly available Pictor-v3 and SHWD datasets. The experimental results show that FRSHNet consistently provided a favorable performance, outperforming the existing state-of-the-art models.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/630c9993b6aaf812e1aa3dae8e243af131abd3d0.pdf",
        "venue": "Buildings",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Visual inspection of the workplace and timely reminders of unsafe behaviors (e.g, not wearing a helmet) are particularly significant for avoiding injuries to workers on the construction site. Video surveillance systems generate large amounts of non-structure image data on site for this purpose; however, they require real-time recognition automation solutions based on computer vision. Although various deep-learning-based models have recently provided new ideas for identifying helmets in traffic monitoring, few solutions suitable for industry applications have been discussed due to the complex scenarios of construction sites. In this paper, a fast and robust network based on a mutilscale Swin Transformer is proposed for safety helmet detection (FRSHNet) at construction sites, which contains the following contributions. Firstly, MAE-NAS with the variant of MobileNetV3s MobBlock as a basic block is applied to implement feature extraction. Simultaneously, a multiscale Swin Transformer module is utilized to obtain the spatial and contexture relationships in the multiscale features. Subsequently, in order to meet the scheme requirements of real-time helmet detection, efficient RepGFPN are adopted to integrate refined multiscale features to form a pyramid structure. Extensive experiments were conducted on the publicly available Pictor-v3 and SHWD datasets. The experimental results show that FRSHNet consistently provided a favorable performance, outperforming the existing state-of-the-art models.",
        "keywords": []
      },
      "file_name": "630c9993b6aaf812e1aa3dae8e243af131abd3d0.pdf"
    },
    {
      "success": true,
      "doc_id": "5661baf1830ef57d2cb0f05834191e97",
      "summary": "Facial expression recognition has wide application prospects in many occasions. Due to the complexity and variability of facial expressions, facial expression recognition has become a very challenging research topic. This paper proposes a Vision Transformer expression recognition method based on hybrid local attention (HLA-ViT). The network adopts a dual-stream structure. One stream extracts the hybrid local features and the other stream extracts the global contextual features. These two streams constitute a globallocal fusion attention. The hybrid local attention module is proposed to enhance the networks robustness to face occlusion and head pose variations. The convolutional neural network is combined with the hybrid local attention module to obtain feature maps with local prominent information. Robust features are then captured by the ViT from the global perspective of the visual sequence context. Finally, the decision-level fusion mechanism fuses the expression features with local prominent information, adding complementary information to enhance the networks recognition performance and robustness against interference factors such as occlusion and head posture changes in natural scenes. Extensive experiments demonstrate that our HLA-ViT network achieves an excellent performance with 90.45% on RAF-DB, 90.13% on FERPlus, and 65.07% on AffectNet.",
      "intriguing_abstract": "Facial expression recognition has wide application prospects in many occasions. Due to the complexity and variability of facial expressions, facial expression recognition has become a very challenging research topic. This paper proposes a Vision Transformer expression recognition method based on hybrid local attention (HLA-ViT). The network adopts a dual-stream structure. One stream extracts the hybrid local features and the other stream extracts the global contextual features. These two streams constitute a globallocal fusion attention. The hybrid local attention module is proposed to enhance the networks robustness to face occlusion and head pose variations. The convolutional neural network is combined with the hybrid local attention module to obtain feature maps with local prominent information. Robust features are then captured by the ViT from the global perspective of the visual sequence context. Finally, the decision-level fusion mechanism fuses the expression features with local prominent information, adding complementary information to enhance the networks recognition performance and robustness against interference factors such as occlusion and head posture changes in natural scenes. Extensive experiments demonstrate that our HLA-ViT network achieves an excellent performance with 90.45% on RAF-DB, 90.13% on FERPlus, and 65.07% on AffectNet.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d14a1677e416bd7ac6fbe01672cd3152fc1f983d.pdf",
      "citation_key": "tian20242kr",
      "metadata": {
        "title": "Facial Expression Recognition Based on Vision Transformer with Hybrid Local Attention",
        "authors": [
          "Yuan Tian",
          "Jingxuan Zhu",
          "Huang Yao",
          "Di Chen"
        ],
        "published_date": "2024",
        "abstract": "Facial expression recognition has wide application prospects in many occasions. Due to the complexity and variability of facial expressions, facial expression recognition has become a very challenging research topic. This paper proposes a Vision Transformer expression recognition method based on hybrid local attention (HLA-ViT). The network adopts a dual-stream structure. One stream extracts the hybrid local features and the other stream extracts the global contextual features. These two streams constitute a globallocal fusion attention. The hybrid local attention module is proposed to enhance the networks robustness to face occlusion and head pose variations. The convolutional neural network is combined with the hybrid local attention module to obtain feature maps with local prominent information. Robust features are then captured by the ViT from the global perspective of the visual sequence context. Finally, the decision-level fusion mechanism fuses the expression features with local prominent information, adding complementary information to enhance the networks recognition performance and robustness against interference factors such as occlusion and head posture changes in natural scenes. Extensive experiments demonstrate that our HLA-ViT network achieves an excellent performance with 90.45% on RAF-DB, 90.13% on FERPlus, and 65.07% on AffectNet.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d14a1677e416bd7ac6fbe01672cd3152fc1f983d.pdf",
        "venue": "Applied Sciences",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Facial expression recognition has wide application prospects in many occasions. Due to the complexity and variability of facial expressions, facial expression recognition has become a very challenging research topic. This paper proposes a Vision Transformer expression recognition method based on hybrid local attention (HLA-ViT). The network adopts a dual-stream structure. One stream extracts the hybrid local features and the other stream extracts the global contextual features. These two streams constitute a globallocal fusion attention. The hybrid local attention module is proposed to enhance the networks robustness to face occlusion and head pose variations. The convolutional neural network is combined with the hybrid local attention module to obtain feature maps with local prominent information. Robust features are then captured by the ViT from the global perspective of the visual sequence context. Finally, the decision-level fusion mechanism fuses the expression features with local prominent information, adding complementary information to enhance the networks recognition performance and robustness against interference factors such as occlusion and head posture changes in natural scenes. Extensive experiments demonstrate that our HLA-ViT network achieves an excellent performance with 90.45% on RAF-DB, 90.13% on FERPlus, and 65.07% on AffectNet.",
        "keywords": []
      },
      "file_name": "d14a1677e416bd7ac6fbe01672cd3152fc1f983d.pdf"
    },
    {
      "success": true,
      "doc_id": "ab0ac0145116c0458996fc9f6bcd75f1",
      "summary": "High resolution remote sensing imagery plays a crucial role in monitoring coastal wetlands. Coastal wetland landscapes exhibit diverse features, ranging from fragmented patches to expansive areas. Mainstream convolutional neural networks cannot effectively analyze spatial relationships among consecutive image elements. This limitation impedes their performance in accurately classifying coastal wetlands. In order to tackle the above issues, we propose a Vision Transformer based UNet (ViT-UNet) model. This model extracts wetland features from high resolution remote sensing images by sensing and optimizing multiscale features. To establish global dependencies, the Vision Transformer (ViT) is introduced to replace the convolutional layer in the UNet encoder. Simultaneously, the model incorporates a convolutional block attention module and a multiple hierarchies attention module to restore attentional features and reduce feature loss. In addition, a skip connection is added to the single-skip structure of the original UNet model. This connection simultaneously links the output of the entire transformer and internal attention features to the corresponding decoder level. This enhancement aims to furnish the decoder with comprehensive global information guidance. Finally, all the extracted feature information is fused using Bilinear Polymerization Pooling (BPP). The BPP assists the network in obtaining a more comprehensive and detailed feature representation. Experimental results on the Gaofen-1 dataset demonstrate that the proposed ViT-UNet method achieves a Precision score of 93.50$\\%$, outperforming the original UNet model by 4.10$\\%$. Compared with other state-of-the-art networks, ViT-UNet performs more accurately and finer in the extraction of wetland information in the Yellow River Delta.",
      "intriguing_abstract": "High resolution remote sensing imagery plays a crucial role in monitoring coastal wetlands. Coastal wetland landscapes exhibit diverse features, ranging from fragmented patches to expansive areas. Mainstream convolutional neural networks cannot effectively analyze spatial relationships among consecutive image elements. This limitation impedes their performance in accurately classifying coastal wetlands. In order to tackle the above issues, we propose a Vision Transformer based UNet (ViT-UNet) model. This model extracts wetland features from high resolution remote sensing images by sensing and optimizing multiscale features. To establish global dependencies, the Vision Transformer (ViT) is introduced to replace the convolutional layer in the UNet encoder. Simultaneously, the model incorporates a convolutional block attention module and a multiple hierarchies attention module to restore attentional features and reduce feature loss. In addition, a skip connection is added to the single-skip structure of the original UNet model. This connection simultaneously links the output of the entire transformer and internal attention features to the corresponding decoder level. This enhancement aims to furnish the decoder with comprehensive global information guidance. Finally, all the extracted feature information is fused using Bilinear Polymerization Pooling (BPP). The BPP assists the network in obtaining a more comprehensive and detailed feature representation. Experimental results on the Gaofen-1 dataset demonstrate that the proposed ViT-UNet method achieves a Precision score of 93.50$\\%$, outperforming the original UNet model by 4.10$\\%$. Compared with other state-of-the-art networks, ViT-UNet performs more accurately and finer in the extraction of wetland information in the Yellow River Delta.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/8ed42c0d60eb4c91e99ee184a17719f7c3ce3f3f.pdf",
      "citation_key": "zhou2024r66",
      "metadata": {
        "title": "ViT-UNet: A Vision Transformer Based UNet Model for Coastal Wetland Classification Based on High Spatial Resolution Imagery",
        "authors": [
          "Nan Zhou",
          "Mingming Xu",
          "Biaoqun Shen",
          "Ke Hou",
          "Shanwei Liu",
          "Hui Sheng",
          "Yanfen Liu",
          "Jianhua Wan"
        ],
        "published_date": "2024",
        "abstract": "High resolution remote sensing imagery plays a crucial role in monitoring coastal wetlands. Coastal wetland landscapes exhibit diverse features, ranging from fragmented patches to expansive areas. Mainstream convolutional neural networks cannot effectively analyze spatial relationships among consecutive image elements. This limitation impedes their performance in accurately classifying coastal wetlands. In order to tackle the above issues, we propose a Vision Transformer based UNet (ViT-UNet) model. This model extracts wetland features from high resolution remote sensing images by sensing and optimizing multiscale features. To establish global dependencies, the Vision Transformer (ViT) is introduced to replace the convolutional layer in the UNet encoder. Simultaneously, the model incorporates a convolutional block attention module and a multiple hierarchies attention module to restore attentional features and reduce feature loss. In addition, a skip connection is added to the single-skip structure of the original UNet model. This connection simultaneously links the output of the entire transformer and internal attention features to the corresponding decoder level. This enhancement aims to furnish the decoder with comprehensive global information guidance. Finally, all the extracted feature information is fused using Bilinear Polymerization Pooling (BPP). The BPP assists the network in obtaining a more comprehensive and detailed feature representation. Experimental results on the Gaofen-1 dataset demonstrate that the proposed ViT-UNet method achieves a Precision score of 93.50$\\%$, outperforming the original UNet model by 4.10$\\%$. Compared with other state-of-the-art networks, ViT-UNet performs more accurately and finer in the extraction of wetland information in the Yellow River Delta.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/8ed42c0d60eb4c91e99ee184a17719f7c3ce3f3f.pdf",
        "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
        "citationCount": 5,
        "score": 5.0,
        "summary": "High resolution remote sensing imagery plays a crucial role in monitoring coastal wetlands. Coastal wetland landscapes exhibit diverse features, ranging from fragmented patches to expansive areas. Mainstream convolutional neural networks cannot effectively analyze spatial relationships among consecutive image elements. This limitation impedes their performance in accurately classifying coastal wetlands. In order to tackle the above issues, we propose a Vision Transformer based UNet (ViT-UNet) model. This model extracts wetland features from high resolution remote sensing images by sensing and optimizing multiscale features. To establish global dependencies, the Vision Transformer (ViT) is introduced to replace the convolutional layer in the UNet encoder. Simultaneously, the model incorporates a convolutional block attention module and a multiple hierarchies attention module to restore attentional features and reduce feature loss. In addition, a skip connection is added to the single-skip structure of the original UNet model. This connection simultaneously links the output of the entire transformer and internal attention features to the corresponding decoder level. This enhancement aims to furnish the decoder with comprehensive global information guidance. Finally, all the extracted feature information is fused using Bilinear Polymerization Pooling (BPP). The BPP assists the network in obtaining a more comprehensive and detailed feature representation. Experimental results on the Gaofen-1 dataset demonstrate that the proposed ViT-UNet method achieves a Precision score of 93.50$\\%$, outperforming the original UNet model by 4.10$\\%$. Compared with other state-of-the-art networks, ViT-UNet performs more accurately and finer in the extraction of wetland information in the Yellow River Delta.",
        "keywords": []
      },
      "file_name": "8ed42c0d60eb4c91e99ee184a17719f7c3ce3f3f.pdf"
    },
    {
      "success": true,
      "doc_id": "132832922a01fad6cffabfa6ffed24cb",
      "summary": "In this research, we developed a two-stage deep learning (DL) model using Vision Transformer (ViT) to detect COVID-19 and assess its severity from thoracic CT images. In the first stage, we utilized a pre-trained ViT model (ViT_B/32) and a custom CNN model to classify CT images as COVID-19 or non-COVID-19. The ViT model achieved superior performance with a fivefold cross-validated accuracy of 99.7%, compared to the custom CNNs 98%. In the second stage, we employed a ViT-based U-Net model (Vision Transformer for Biomedical Image Segmentation, VITBIS) to segment lung and infection regions in COVID-19 positive CT images, determining the infection severity. This model uses transformers with attention mechanisms in both the encoder and decoder. The lung segmentation network achieved an Intersection Over Union (IOU) of 95.8% and a sensitivity of 99.67%, while the lesion segmentation network attained an IOU of 94% and a sensitivity of 98.3%.",
      "intriguing_abstract": "In this research, we developed a two-stage deep learning (DL) model using Vision Transformer (ViT) to detect COVID-19 and assess its severity from thoracic CT images. In the first stage, we utilized a pre-trained ViT model (ViT_B/32) and a custom CNN model to classify CT images as COVID-19 or non-COVID-19. The ViT model achieved superior performance with a fivefold cross-validated accuracy of 99.7%, compared to the custom CNNs 98%. In the second stage, we employed a ViT-based U-Net model (Vision Transformer for Biomedical Image Segmentation, VITBIS) to segment lung and infection regions in COVID-19 positive CT images, determining the infection severity. This model uses transformers with attention mechanisms in both the encoder and decoder. The lung segmentation network achieved an Intersection Over Union (IOU) of 95.8% and a sensitivity of 99.67%, while the lesion segmentation network attained an IOU of 94% and a sensitivity of 98.3%.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/c4cf58e104fc8d783dfd0bcf3ccd65f8e4028d7f.pdf",
      "citation_key": "taye20244db",
      "metadata": {
        "title": "Thoracic computed tomography (CT) image-based identification and severity classification of COVID-19 cases using vision transformer (ViT)",
        "authors": [
          "Gizatie Desalegn Taye",
          "Zewdie Habtie Sisay",
          "Genet Worku Gebeyhu",
          "Fisha Haileslassie Kidus"
        ],
        "published_date": "2024",
        "abstract": "In this research, we developed a two-stage deep learning (DL) model using Vision Transformer (ViT) to detect COVID-19 and assess its severity from thoracic CT images. In the first stage, we utilized a pre-trained ViT model (ViT_B/32) and a custom CNN model to classify CT images as COVID-19 or non-COVID-19. The ViT model achieved superior performance with a fivefold cross-validated accuracy of 99.7%, compared to the custom CNNs 98%. In the second stage, we employed a ViT-based U-Net model (Vision Transformer for Biomedical Image Segmentation, VITBIS) to segment lung and infection regions in COVID-19 positive CT images, determining the infection severity. This model uses transformers with attention mechanisms in both the encoder and decoder. The lung segmentation network achieved an Intersection Over Union (IOU) of 95.8% and a sensitivity of 99.67%, while the lesion segmentation network attained an IOU of 94% and a sensitivity of 98.3%.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c4cf58e104fc8d783dfd0bcf3ccd65f8e4028d7f.pdf",
        "venue": "Discover Applied Sciences",
        "citationCount": 5,
        "score": 5.0,
        "summary": "In this research, we developed a two-stage deep learning (DL) model using Vision Transformer (ViT) to detect COVID-19 and assess its severity from thoracic CT images. In the first stage, we utilized a pre-trained ViT model (ViT_B/32) and a custom CNN model to classify CT images as COVID-19 or non-COVID-19. The ViT model achieved superior performance with a fivefold cross-validated accuracy of 99.7%, compared to the custom CNNs 98%. In the second stage, we employed a ViT-based U-Net model (Vision Transformer for Biomedical Image Segmentation, VITBIS) to segment lung and infection regions in COVID-19 positive CT images, determining the infection severity. This model uses transformers with attention mechanisms in both the encoder and decoder. The lung segmentation network achieved an Intersection Over Union (IOU) of 95.8% and a sensitivity of 99.67%, while the lesion segmentation network attained an IOU of 94% and a sensitivity of 98.3%.",
        "keywords": []
      },
      "file_name": "c4cf58e104fc8d783dfd0bcf3ccd65f8e4028d7f.pdf"
    },
    {
      "success": true,
      "doc_id": "40c85801d9cabdab6c1b3ad6c29627da",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **CITATION**: \\cite{alohali2024xwz}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: The paper addresses the challenge of accurately and reliably classifying cervical cells in Pap smear images to facilitate early detection of cervical cancer \\cite{alohali2024xwz}.\n    *   **Importance and challenge**: Cervical cancer is a significant health concern with increasing incidence and mortality rates, making early and accurate diagnosis crucial for improving patient outcomes. Manual diagnosis is prone to misdiagnosis due to inter/intra-observer variability, and is both costly and time-consuming. Existing deep learning models, particularly Convolutional Neural Networks (CNNs), often focus on local patterns, limiting their ability to capture the global context of medical images, while some prior vision transformer applications showed lower performance \\cite{alohali2024xwz}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches**: This work builds upon advancements in computer vision, specifically leveraging the Swin Transformer, which has shown promise in image classification, and comparing against various pre-trained CNN models \\cite{alohali2024xwz}.\n    *   **Limitations of previous solutions**: Previous studies primarily applied deep learning models and pre-trained CNNs (e.g., DenseNet, VGG, ResNet, AlexNet, MobileNet) for cervical cancer classification. While these models have contributed, they often struggle with capturing both local and global contextual information effectively, and some vision transformer models previously applied recorded lower performance \\cite{alohali2024xwz}. The paper aims to overcome these limitations by integrating a robust feature extractor with intelligent feature selection and a powerful classifier.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method**: The paper proposes a novel approach called **Swin-GA-RF** \\cite{alohali2024xwz}. This method combines:\n        *   **Swin Transformer**: Used for hierarchical feature extraction, capturing both local and global contextual information from Pap smear images through shifted windows and self-attention mechanisms.\n        *   **Genetic Algorithm (GA)**: Employed for feature selection to identify and optimize the most informative feature set extracted by the Swin Transformer, thereby reducing feature complexity.\n        *   **Random Forest (RF) Classifier**: Replaces the traditional softmax layer of the Swin Transformer to perform the final classification, enhancing overall performance and robustness.\n    *   **Novelty**:\n        *   **Hybrid Architecture**: The innovative combination of a Swin Transformer, GA-based feature selection, and a Random Forest classifier specifically tailored for cervical cancer classification \\cite{alohali2024xwz}.\n        *   **Optimized Feature Set**: The use of GA to intelligently select the most relevant features from the Swin Transformer's output, which is crucial for handling high-dimensional medical image data.\n        *   **Enhanced Classification**: Replacing the standard softmax with a Random Forest classifier to improve the discriminative power and generalization of the model.\n        *   **Data Augmentation**: Applied to the SIPaKMeD1 dataset to increase image diversity and quantity, further improving model generalization \\cite{alohali2024xwz}.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques**:\n        *   Introduction of the Swin-GA-RF approach, a novel hybrid model for cervical cell classification \\cite{alohali2024xwz}.\n        *   A methodology that effectively leverages the Swin Transformer's ability to capture both local and global contextual information from medical images \\cite{alohali2024xwz}.\n        *   The application of Genetic Algorithms to reduce feature complexity and select the most significant features extracted by the Swin Transformer \\cite{alohali2024xwz}.\n        *   Enhancement of classification performance by replacing the conventional softmax classifier with a Random Forest \\cite{alohali2024xwz}.\n    *   **System design or architectural innovations**: A modular design integrating a state-of-the-art vision transformer with metaheuristic optimization and ensemble learning for robust medical image analysis.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted**:\n        *   The Swin-GA-RF model was comprehensively compared against other Swin Transformers and several pre-trained CNN models (DenseNet121, VGG16, ResNet18, AlexNet, MobileNet) \\cite{alohali2024xwz}.\n        *   Evaluations were performed using two different optimizers: Adam and SGD \\cite{alohali2024xwz}.\n        *   The model's performance was assessed in both binary (normal vs. abnormal) and five-class classification scenarios on the publicly available SIPaKMeD1 cervical cancer image dataset \\cite{alohali2024xwz}.\n    *   **Key performance metrics and comparison results**:\n        *   Swin-GA-RF consistently outperformed other Swin transformers and pre-trained CNN models across all tested scenarios \\cite{alohali2024xwz}.\n        *   The highest performance was achieved when utilizing the Adam optimizer \\cite{alohali2024xwz}.\n        *   **Binary Classification (with Adam optimizer)**: Achieved an accuracy of 99.012%, precision of 99.015%, recall of 99.012%, and F1-score of 99.011% \\cite{alohali2024xwz}.\n        *   **Five-class Classification (with Adam optimizer)**: Achieved an accuracy of 98.808%, precision of 98.812%, recall of 98.808%, and F1-score of 98.808% \\cite{alohali2024xwz}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions**: The paper does not explicitly detail the specific parameters or implementation specifics of the Genetic Algorithm, which could influence reproducibility and generalizability. The comparison is limited to a specific set of pre-trained CNNs and Swin Transformers.\n    *   **Scope of applicability**: The research is specifically focused on cervical cancer classification using Pap smear images from the SIPaKMeD1 dataset. While highly effective for this domain, its direct applicability to other medical imaging modalities or different cancer types would require further investigation and validation \\cite{alohali2024xwz}.\n\n7.  **Technical Significance**\n    *   **Advances the technical state-of-the-art**: The Swin-GA-RF model significantly advances the state-of-the-art in cervical cancer classification by achieving superior accuracy, precision, recall, and F1-scores compared to existing deep learning and vision transformer approaches \\cite{alohali2024xwz}. The hybrid approach demonstrates a powerful paradigm for medical image analysis.\n    *   **Potential impact on future research**: This work highlights the effectiveness of combining advanced vision transformers with metaheuristic optimization for feature selection and robust ensemble classifiers. It provides a strong foundation for future research in developing highly accurate and reliable AI-powered diagnostic tools for various medical imaging applications, potentially leading to improved early diagnosis and screening programs for cervical cancer and other diseases \\cite{alohali2024xwz}.",
      "intriguing_abstract": "Early and accurate detection of cervical cancer is paramount for improving patient outcomes, yet current diagnostic methods often suffer from subjectivity and the limitations of traditional deep learning models in capturing comprehensive image context. We introduce **Swin-GA-RF**, a novel hybrid framework designed to revolutionize cervical cell classification from Pap smear images. This pioneering approach leverages the **Swin Transformer** for its unparalleled ability to extract hierarchical, multi-scale features, capturing both local intricacies and global contextual information. To overcome feature redundancy and enhance discriminative power, a **Genetic Algorithm (GA)** intelligently optimizes feature selection, identifying the most salient representations. Finally, a robust **Random Forest (RF)** classifier replaces the conventional softmax layer, significantly boosting classification accuracy and generalization. Evaluated on the challenging SIPaKMeD1 dataset, Swin-GA-RF achieved remarkable state-of-the-art performance, with 99.012% accuracy for binary classification and 98.808% for five-class classification. This work establishes a powerful paradigm for **medical image analysis**, offering a highly accurate and reliable **AI-powered diagnostic tool** that promises to enhance early cervical cancer screening and diagnosis, ultimately saving lives.",
      "keywords": [
        "Swin-GA-RF",
        "cervical cancer classification",
        "Pap smear images",
        "Swin Transformer",
        "Genetic Algorithm",
        "Random Forest classifier",
        "hybrid architecture",
        "feature extraction and selection",
        "medical image analysis",
        "vision transformers",
        "deep learning",
        "high accuracy",
        "early detection"
      ],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/72e23cdc3accca1f09e2e19446bc475368c912d0.pdf",
      "citation_key": "alohali2024xwz",
      "metadata": {
        "title": "Swin-GA-RF: genetic algorithm-based Swin Transformer and random forest for enhancing cervical cancer classification",
        "authors": [
          "Manal Abdullah Alohali",
          "Nora El-Rashidy",
          "Saad Alaklabi",
          "H. Elmannai",
          "Saleh Alharbi",
          "Hager Saleh"
        ],
        "published_date": "2024",
        "abstract": "Cervical cancer is a prevalent and concerning disease affecting women, with increasing incidence and mortality rates. Early detection plays a crucial role in improving outcomes. Recent advancements in computer vision, particularly the Swin transformer, have shown promising performance in image classification tasks, rivaling or surpassing traditional convolutional neural networks (CNNs). The Swin transformer adopts a hierarchical and efficient approach using shifted windows, enabling the capture of both local and global contextual information in images. In this paper, we propose a novel approach called Swin-GA-RF to enhance the classification performance of cervical cells in Pap smear images. Swin-GA-RF combines the strengths of the Swin transformer, genetic algorithm (GA) feature selection, and the replacement of the softmax layer with a random forest classifier. Our methodology involves extracting feature representations from the Swin transformer, utilizing GA to identify the optimal feature set, and employing random forest as the classification model. Additionally, data augmentation techniques are applied to augment the diversity and quantity of the SIPaKMeD1 cervical cancer image dataset. We compare the performance of the Swin-GA-RF Transformer with pre-trained CNN models using two classes and five classes of cervical cancer classification, employing both Adam and SGD optimizers. The experimental results demonstrate that Swin-GA-RF outperforms other Swin transformers and pre-trained CNN models. When utilizing the Adam optimizer, Swin-GA-RF achieves the highest performance in both binary and five-class classification tasks. Specifically, for binary classification, it achieves an accuracy, precision, recall, and F1-score of 99.012, 99.015, 99.012, and 99.011, respectively. In the five-class classification, it achieves an accuracy, precision, recall, and F1-score of 98.808, 98.812, 98.808, and 98.808, respectively. These results underscore the effectiveness of the Swin-GA-RF approach in cervical cancer classification, demonstrating its potential as a valuable tool for early diagnosis and screening programs.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/72e23cdc3accca1f09e2e19446bc475368c912d0.pdf",
        "venue": "Frontiers in Oncology",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **CITATION**: \\cite{alohali2024xwz}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: The paper addresses the challenge of accurately and reliably classifying cervical cells in Pap smear images to facilitate early detection of cervical cancer \\cite{alohali2024xwz}.\n    *   **Importance and challenge**: Cervical cancer is a significant health concern with increasing incidence and mortality rates, making early and accurate diagnosis crucial for improving patient outcomes. Manual diagnosis is prone to misdiagnosis due to inter/intra-observer variability, and is both costly and time-consuming. Existing deep learning models, particularly Convolutional Neural Networks (CNNs), often focus on local patterns, limiting their ability to capture the global context of medical images, while some prior vision transformer applications showed lower performance \\cite{alohali2024xwz}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches**: This work builds upon advancements in computer vision, specifically leveraging the Swin Transformer, which has shown promise in image classification, and comparing against various pre-trained CNN models \\cite{alohali2024xwz}.\n    *   **Limitations of previous solutions**: Previous studies primarily applied deep learning models and pre-trained CNNs (e.g., DenseNet, VGG, ResNet, AlexNet, MobileNet) for cervical cancer classification. While these models have contributed, they often struggle with capturing both local and global contextual information effectively, and some vision transformer models previously applied recorded lower performance \\cite{alohali2024xwz}. The paper aims to overcome these limitations by integrating a robust feature extractor with intelligent feature selection and a powerful classifier.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method**: The paper proposes a novel approach called **Swin-GA-RF** \\cite{alohali2024xwz}. This method combines:\n        *   **Swin Transformer**: Used for hierarchical feature extraction, capturing both local and global contextual information from Pap smear images through shifted windows and self-attention mechanisms.\n        *   **Genetic Algorithm (GA)**: Employed for feature selection to identify and optimize the most informative feature set extracted by the Swin Transformer, thereby reducing feature complexity.\n        *   **Random Forest (RF) Classifier**: Replaces the traditional softmax layer of the Swin Transformer to perform the final classification, enhancing overall performance and robustness.\n    *   **Novelty**:\n        *   **Hybrid Architecture**: The innovative combination of a Swin Transformer, GA-based feature selection, and a Random Forest classifier specifically tailored for cervical cancer classification \\cite{alohali2024xwz}.\n        *   **Optimized Feature Set**: The use of GA to intelligently select the most relevant features from the Swin Transformer's output, which is crucial for handling high-dimensional medical image data.\n        *   **Enhanced Classification**: Replacing the standard softmax with a Random Forest classifier to improve the discriminative power and generalization of the model.\n        *   **Data Augmentation**: Applied to the SIPaKMeD1 dataset to increase image diversity and quantity, further improving model generalization \\cite{alohali2024xwz}.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques**:\n        *   Introduction of the Swin-GA-RF approach, a novel hybrid model for cervical cell classification \\cite{alohali2024xwz}.\n        *   A methodology that effectively leverages the Swin Transformer's ability to capture both local and global contextual information from medical images \\cite{alohali2024xwz}.\n        *   The application of Genetic Algorithms to reduce feature complexity and select the most significant features extracted by the Swin Transformer \\cite{alohali2024xwz}.\n        *   Enhancement of classification performance by replacing the conventional softmax classifier with a Random Forest \\cite{alohali2024xwz}.\n    *   **System design or architectural innovations**: A modular design integrating a state-of-the-art vision transformer with metaheuristic optimization and ensemble learning for robust medical image analysis.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted**:\n        *   The Swin-GA-RF model was comprehensively compared against other Swin Transformers and several pre-trained CNN models (DenseNet121, VGG16, ResNet18, AlexNet, MobileNet) \\cite{alohali2024xwz}.\n        *   Evaluations were performed using two different optimizers: Adam and SGD \\cite{alohali2024xwz}.\n        *   The model's performance was assessed in both binary (normal vs. abnormal) and five-class classification scenarios on the publicly available SIPaKMeD1 cervical cancer image dataset \\cite{alohali2024xwz}.\n    *   **Key performance metrics and comparison results**:\n        *   Swin-GA-RF consistently outperformed other Swin transformers and pre-trained CNN models across all tested scenarios \\cite{alohali2024xwz}.\n        *   The highest performance was achieved when utilizing the Adam optimizer \\cite{alohali2024xwz}.\n        *   **Binary Classification (with Adam optimizer)**: Achieved an accuracy of 99.012%, precision of 99.015%, recall of 99.012%, and F1-score of 99.011% \\cite{alohali2024xwz}.\n        *   **Five-class Classification (with Adam optimizer)**: Achieved an accuracy of 98.808%, precision of 98.812%, recall of 98.808%, and F1-score of 98.808% \\cite{alohali2024xwz}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions**: The paper does not explicitly detail the specific parameters or implementation specifics of the Genetic Algorithm, which could influence reproducibility and generalizability. The comparison is limited to a specific set of pre-trained CNNs and Swin Transformers.\n    *   **Scope of applicability**: The research is specifically focused on cervical cancer classification using Pap smear images from the SIPaKMeD1 dataset. While highly effective for this domain, its direct applicability to other medical imaging modalities or different cancer types would require further investigation and validation \\cite{alohali2024xwz}.\n\n7.  **Technical Significance**\n    *   **Advances the technical state-of-the-art**: The Swin-GA-RF model significantly advances the state-of-the-art in cervical cancer classification by achieving superior accuracy, precision, recall, and F1-scores compared to existing deep learning and vision transformer approaches \\cite{alohali2024xwz}. The hybrid approach demonstrates a powerful paradigm for medical image analysis.\n    *   **Potential impact on future research**: This work highlights the effectiveness of combining advanced vision transformers with metaheuristic optimization for feature selection and robust ensemble classifiers. It provides a strong foundation for future research in developing highly accurate and reliable AI-powered diagnostic tools for various medical imaging applications, potentially leading to improved early diagnosis and screening programs for cervical cancer and other diseases \\cite{alohali2024xwz}.",
        "keywords": [
          "Swin-GA-RF",
          "cervical cancer classification",
          "Pap smear images",
          "Swin Transformer",
          "Genetic Algorithm",
          "Random Forest classifier",
          "hybrid architecture",
          "feature extraction and selection",
          "medical image analysis",
          "vision transformers",
          "deep learning",
          "high accuracy",
          "early detection"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the **title** \"swin-ga-rf: genetic algorithm-based swin transformer and random forest for enhancing cervical cancer classification\" clearly indicates the proposal of a new system/method (swin-ga-rf) that combines specific techniques (genetic algorithm, swin transformer, random forest).\n*   the **abstract** directly names this new system/method.\n*   the **introduction** discusses a significant health problem (cervical cancer), highlights the limitations of current screening techniques (\"manual diagnosis is prone to misdiagnosis,\" \"costly,\" \"time-consuming\"), and explicitly states \"there is an urgent need to develop an advanced and reliable model capable of providing accurate decision-making in cervical cancer diagnosis.\" this sets up a technical problem that the paper aims to solve with its proposed method.\n\nthese elements strongly align with the criteria for a **technical** paper, which presents new methods, algorithms, or systems to address a technical problem.\n\n**classification: technical**"
      },
      "file_name": "72e23cdc3accca1f09e2e19446bc475368c912d0.pdf"
    },
    {
      "success": true,
      "doc_id": "19545a913c1dbfeb60787b81537169b3",
      "summary": "Hydrocolloids are widely used in meat products as common food additives. However, research has indicated that excessive consumption of these hydrocolloids may have potential health implications. Currently, consumers mainly rely on sensory evaluation to identify hydrocolloid adulteration in meat products. Although many studies on quantitative detection of hydrocolloids have been conducted by biochemical methods in laboratory environments, there is currently a lack of effective tools for consumers and regulators to obtain real-time and reliable information on hydrocolloid adulteration. To address this challenge, a smartphone-based computer vision method was developed to quantitatively detect carrageenan adulteration in beef in this work. Specifically, Swin Transformer models, along with pre-training and fine-tuning techniques, were used to successfully automate the classification of beef into nine different levels of carrageenan adulteration, ranging from 0% to 20%. Among the tested models, Swin-Tiny (Swin-T) achieved the highest trade-off performance, with a Top-1 accuracy of 0.997, a detection speed of 3.2ms, and a model size of 103.45 Mb. Compared to computer vision, the electrochemical impedance spectroscopy achieved a lower accuracy of 0.792 and required a constant temperature environment and a waiting time of around 30 min for data stabilization. In addition, Swin-T model was also capable of distinguishing between different types of hydrocolloids with a Top-1 accuracy of 0.975. This study provides consumers and regulators with a valuable tool to obtain real-time quantitative information about meat adulteration anytime, anywhere. PRACTICAL APPLICATION: This research provides a practical solution for regulators and consumers to non-destructively and quantitatively detect the content and type of hydrocolloids in beef in real-time using smartphones. This innovation has the potential to significantly reduce the costs associated with meat quality testing, such as the use of chemical reagents and expensive instruments.",
      "intriguing_abstract": "Hydrocolloids are widely used in meat products as common food additives. However, research has indicated that excessive consumption of these hydrocolloids may have potential health implications. Currently, consumers mainly rely on sensory evaluation to identify hydrocolloid adulteration in meat products. Although many studies on quantitative detection of hydrocolloids have been conducted by biochemical methods in laboratory environments, there is currently a lack of effective tools for consumers and regulators to obtain real-time and reliable information on hydrocolloid adulteration. To address this challenge, a smartphone-based computer vision method was developed to quantitatively detect carrageenan adulteration in beef in this work. Specifically, Swin Transformer models, along with pre-training and fine-tuning techniques, were used to successfully automate the classification of beef into nine different levels of carrageenan adulteration, ranging from 0% to 20%. Among the tested models, Swin-Tiny (Swin-T) achieved the highest trade-off performance, with a Top-1 accuracy of 0.997, a detection speed of 3.2ms, and a model size of 103.45 Mb. Compared to computer vision, the electrochemical impedance spectroscopy achieved a lower accuracy of 0.792 and required a constant temperature environment and a waiting time of around 30 min for data stabilization. In addition, Swin-T model was also capable of distinguishing between different types of hydrocolloids with a Top-1 accuracy of 0.975. This study provides consumers and regulators with a valuable tool to obtain real-time quantitative information about meat adulteration anytime, anywhere. PRACTICAL APPLICATION: This research provides a practical solution for regulators and consumers to non-destructively and quantitatively detect the content and type of hydrocolloids in beef in real-time using smartphones. This innovation has the potential to significantly reduce the costs associated with meat quality testing, such as the use of chemical reagents and expensive instruments.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/7b6d64097d16219c043df64e4576bd7d87656073.pdf",
      "citation_key": "gao20246ks",
      "metadata": {
        "title": "Real-time quantitative detection of hydrocolloid adulteration in meat based on Swin Transformer and smartphone.",
        "authors": [
          "Zhenchang Gao",
          "Shanshan Chen",
          "Jinxian Huang",
          "H. Cai"
        ],
        "published_date": "2024",
        "abstract": "Hydrocolloids are widely used in meat products as common food additives. However, research has indicated that excessive consumption of these hydrocolloids may have potential health implications. Currently, consumers mainly rely on sensory evaluation to identify hydrocolloid adulteration in meat products. Although many studies on quantitative detection of hydrocolloids have been conducted by biochemical methods in laboratory environments, there is currently a lack of effective tools for consumers and regulators to obtain real-time and reliable information on hydrocolloid adulteration. To address this challenge, a smartphone-based computer vision method was developed to quantitatively detect carrageenan adulteration in beef in this work. Specifically, Swin Transformer models, along with pre-training and fine-tuning techniques, were used to successfully automate the classification of beef into nine different levels of carrageenan adulteration, ranging from 0% to 20%. Among the tested models, Swin-Tiny (Swin-T) achieved the highest trade-off performance, with a Top-1 accuracy of 0.997, a detection speed of 3.2ms, and a model size of 103.45 Mb. Compared to computer vision, the electrochemical impedance spectroscopy achieved a lower accuracy of 0.792 and required a constant temperature environment and a waiting time of around 30 min for data stabilization. In addition, Swin-T model was also capable of distinguishing between different types of hydrocolloids with a Top-1 accuracy of 0.975. This study provides consumers and regulators with a valuable tool to obtain real-time quantitative information about meat adulteration anytime, anywhere. PRACTICAL APPLICATION: This research provides a practical solution for regulators and consumers to non-destructively and quantitatively detect the content and type of hydrocolloids in beef in real-time using smartphones. This innovation has the potential to significantly reduce the costs associated with meat quality testing, such as the use of chemical reagents and expensive instruments.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7b6d64097d16219c043df64e4576bd7d87656073.pdf",
        "venue": "Journal of Food Science",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Hydrocolloids are widely used in meat products as common food additives. However, research has indicated that excessive consumption of these hydrocolloids may have potential health implications. Currently, consumers mainly rely on sensory evaluation to identify hydrocolloid adulteration in meat products. Although many studies on quantitative detection of hydrocolloids have been conducted by biochemical methods in laboratory environments, there is currently a lack of effective tools for consumers and regulators to obtain real-time and reliable information on hydrocolloid adulteration. To address this challenge, a smartphone-based computer vision method was developed to quantitatively detect carrageenan adulteration in beef in this work. Specifically, Swin Transformer models, along with pre-training and fine-tuning techniques, were used to successfully automate the classification of beef into nine different levels of carrageenan adulteration, ranging from 0% to 20%. Among the tested models, Swin-Tiny (Swin-T) achieved the highest trade-off performance, with a Top-1 accuracy of 0.997, a detection speed of 3.2ms, and a model size of 103.45 Mb. Compared to computer vision, the electrochemical impedance spectroscopy achieved a lower accuracy of 0.792 and required a constant temperature environment and a waiting time of around 30 min for data stabilization. In addition, Swin-T model was also capable of distinguishing between different types of hydrocolloids with a Top-1 accuracy of 0.975. This study provides consumers and regulators with a valuable tool to obtain real-time quantitative information about meat adulteration anytime, anywhere. PRACTICAL APPLICATION: This research provides a practical solution for regulators and consumers to non-destructively and quantitatively detect the content and type of hydrocolloids in beef in real-time using smartphones. This innovation has the potential to significantly reduce the costs associated with meat quality testing, such as the use of chemical reagents and expensive instruments.",
        "keywords": []
      },
      "file_name": "7b6d64097d16219c043df64e4576bd7d87656073.pdf"
    },
    {
      "success": true,
      "doc_id": "ff33db70eca9eb0cdfbc34d1dc416876",
      "summary": "The advancement of autonomous driving heavily relies on the ability to accurate lane lines detection. As deep learning and computer vision technologies evolve, a variety of deep learning-based methods for lane line detection have been proposed by researchers in the field. However, owing to the simple appearance of lane lines and the lack of distinctive features, it is easy for other objects with similar local appearances to interfere with the process of detecting lane lines. The precision of lane line detection is limited by the unpredictable quantity and diversity of lane lines. To address the aforementioned challenges, we propose a novel deep learning approach for lane line detection. This method leverages the Swin Transformer in conjunction with LaneNet (called ST-LaneNet). The experience results showed that the true positive detection rate can reach 97.53% for easy lanes and 96.83% for difficult lanes (such as scenes with severe occlusion and extreme lighting conditions), which can better accomplish the objective of detecting lane lines. In 1000 detection samples, the average detection accuracy can reach 97.83%, the average inference time per image can reach 17.8ms, and the average number of frames per second can reach 64.8Hz. The programming scripts and associated models for this project can be accessed openly at the following GitHub repository: https://github.com/Duane711/Lane-line-detection-ST-LaneNet .",
      "intriguing_abstract": "The advancement of autonomous driving heavily relies on the ability to accurate lane lines detection. As deep learning and computer vision technologies evolve, a variety of deep learning-based methods for lane line detection have been proposed by researchers in the field. However, owing to the simple appearance of lane lines and the lack of distinctive features, it is easy for other objects with similar local appearances to interfere with the process of detecting lane lines. The precision of lane line detection is limited by the unpredictable quantity and diversity of lane lines. To address the aforementioned challenges, we propose a novel deep learning approach for lane line detection. This method leverages the Swin Transformer in conjunction with LaneNet (called ST-LaneNet). The experience results showed that the true positive detection rate can reach 97.53% for easy lanes and 96.83% for difficult lanes (such as scenes with severe occlusion and extreme lighting conditions), which can better accomplish the objective of detecting lane lines. In 1000 detection samples, the average detection accuracy can reach 97.83%, the average inference time per image can reach 17.8ms, and the average number of frames per second can reach 64.8Hz. The programming scripts and associated models for this project can be accessed openly at the following GitHub repository: https://github.com/Duane711/Lane-line-detection-ST-LaneNet .",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/0820d2ac35cd55fd60f53c14460cca78fa996f1e.pdf",
      "citation_key": "du2024s3t",
      "metadata": {
        "title": "ST-LaneNet: Lane Line Detection Method Based on Swin Transformer and LaneNet",
        "authors": [
          "Yufeng Du",
          "Rongyun Zhang",
          "Peicheng Shi",
          "Linfeng Zhao",
          "Bin Zhang",
          "Yaming Liu"
        ],
        "published_date": "2024",
        "abstract": "The advancement of autonomous driving heavily relies on the ability to accurate lane lines detection. As deep learning and computer vision technologies evolve, a variety of deep learning-based methods for lane line detection have been proposed by researchers in the field. However, owing to the simple appearance of lane lines and the lack of distinctive features, it is easy for other objects with similar local appearances to interfere with the process of detecting lane lines. The precision of lane line detection is limited by the unpredictable quantity and diversity of lane lines. To address the aforementioned challenges, we propose a novel deep learning approach for lane line detection. This method leverages the Swin Transformer in conjunction with LaneNet (called ST-LaneNet). The experience results showed that the true positive detection rate can reach 97.53% for easy lanes and 96.83% for difficult lanes (such as scenes with severe occlusion and extreme lighting conditions), which can better accomplish the objective of detecting lane lines. In 1000 detection samples, the average detection accuracy can reach 97.83%, the average inference time per image can reach 17.8ms, and the average number of frames per second can reach 64.8Hz. The programming scripts and associated models for this project can be accessed openly at the following GitHub repository: https://github.com/Duane711/Lane-line-detection-ST-LaneNet .",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0820d2ac35cd55fd60f53c14460cca78fa996f1e.pdf",
        "venue": "Chinese Journal of Mechanical Engineering",
        "citationCount": 5,
        "score": 5.0,
        "summary": "The advancement of autonomous driving heavily relies on the ability to accurate lane lines detection. As deep learning and computer vision technologies evolve, a variety of deep learning-based methods for lane line detection have been proposed by researchers in the field. However, owing to the simple appearance of lane lines and the lack of distinctive features, it is easy for other objects with similar local appearances to interfere with the process of detecting lane lines. The precision of lane line detection is limited by the unpredictable quantity and diversity of lane lines. To address the aforementioned challenges, we propose a novel deep learning approach for lane line detection. This method leverages the Swin Transformer in conjunction with LaneNet (called ST-LaneNet). The experience results showed that the true positive detection rate can reach 97.53% for easy lanes and 96.83% for difficult lanes (such as scenes with severe occlusion and extreme lighting conditions), which can better accomplish the objective of detecting lane lines. In 1000 detection samples, the average detection accuracy can reach 97.83%, the average inference time per image can reach 17.8ms, and the average number of frames per second can reach 64.8Hz. The programming scripts and associated models for this project can be accessed openly at the following GitHub repository: https://github.com/Duane711/Lane-line-detection-ST-LaneNet .",
        "keywords": []
      },
      "file_name": "0820d2ac35cd55fd60f53c14460cca78fa996f1e.pdf"
    },
    {
      "success": true,
      "doc_id": "2c679b38c70b6c5f7a3ae75e0f603ef3",
      "summary": "This study proposes CurrencyNet, a new way to classify Indian rupee notes using Vision Transformer (ViT) deep architecture. CurrencyNet takes advantage of ViT's feature to manage spatial connections in images. Different optimizers, induding Adadelta, Gradient Descent, AdaGrad, RMS Prop, Adamax, Momentum, Adaptive Moment Estimation (Adam), and Nesterov Momentum, are explored to see how they affect CurrencyNet's performance. Testing results show that CurrencyNet gets 97.76 % accuracy, using both the Vision Transformer and the Adam optimizer together. Proposed CurrencyNet is compared with well-known deep architectures, such as VGG16, VGG18, Inception, Xception, ResNet, and MobileNet. The outcomes show that CurrencyNet is the best way to sort Indian rupee notes, making it the only choice.",
      "intriguing_abstract": "This study proposes CurrencyNet, a new way to classify Indian rupee notes using Vision Transformer (ViT) deep architecture. CurrencyNet takes advantage of ViT's feature to manage spatial connections in images. Different optimizers, induding Adadelta, Gradient Descent, AdaGrad, RMS Prop, Adamax, Momentum, Adaptive Moment Estimation (Adam), and Nesterov Momentum, are explored to see how they affect CurrencyNet's performance. Testing results show that CurrencyNet gets 97.76 % accuracy, using both the Vision Transformer and the Adam optimizer together. Proposed CurrencyNet is compared with well-known deep architectures, such as VGG16, VGG18, Inception, Xception, ResNet, and MobileNet. The outcomes show that CurrencyNet is the best way to sort Indian rupee notes, making it the only choice.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/b02144ef4ed94df78544959bc97eddef4580dd95.pdf",
      "citation_key": "tiwari2024jm9",
      "metadata": {
        "title": "CurrencyNet: A Vision Transformer-Based Approach for Indian Currency Note Classification with Optimizer Exploration",
        "authors": [
          "R. Tiwari",
          "Himani Maheshwari",
          "Vinay Gautam",
          "Neema Gupta",
          "N. Trivedi",
          "A. Agarwal"
        ],
        "published_date": "2024",
        "abstract": "This study proposes CurrencyNet, a new way to classify Indian rupee notes using Vision Transformer (ViT) deep architecture. CurrencyNet takes advantage of ViT's feature to manage spatial connections in images. Different optimizers, induding Adadelta, Gradient Descent, AdaGrad, RMS Prop, Adamax, Momentum, Adaptive Moment Estimation (Adam), and Nesterov Momentum, are explored to see how they affect CurrencyNet's performance. Testing results show that CurrencyNet gets 97.76 % accuracy, using both the Vision Transformer and the Adam optimizer together. Proposed CurrencyNet is compared with well-known deep architectures, such as VGG16, VGG18, Inception, Xception, ResNet, and MobileNet. The outcomes show that CurrencyNet is the best way to sort Indian rupee notes, making it the only choice.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b02144ef4ed94df78544959bc97eddef4580dd95.pdf",
        "venue": "2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT)",
        "citationCount": 5,
        "score": 5.0,
        "summary": "This study proposes CurrencyNet, a new way to classify Indian rupee notes using Vision Transformer (ViT) deep architecture. CurrencyNet takes advantage of ViT's feature to manage spatial connections in images. Different optimizers, induding Adadelta, Gradient Descent, AdaGrad, RMS Prop, Adamax, Momentum, Adaptive Moment Estimation (Adam), and Nesterov Momentum, are explored to see how they affect CurrencyNet's performance. Testing results show that CurrencyNet gets 97.76 % accuracy, using both the Vision Transformer and the Adam optimizer together. Proposed CurrencyNet is compared with well-known deep architectures, such as VGG16, VGG18, Inception, Xception, ResNet, and MobileNet. The outcomes show that CurrencyNet is the best way to sort Indian rupee notes, making it the only choice.",
        "keywords": []
      },
      "file_name": "b02144ef4ed94df78544959bc97eddef4580dd95.pdf"
    },
    {
      "success": true,
      "doc_id": "fccfeba71556f7e0e3d5d90151c8f0d2",
      "summary": "With the rapid expansion and ubiquitous presence of the Internet of Things (IoT), the proliferation of IoT devices has reached unprecedented levels, heightening concerns about IoT security. Intrusion detection based on deep learning has become a crucial approach for safeguarding IoT ecosystems. However, challenges remain in IoT intrusion detection research, including inadequate feature representation at the classifier level and poor correlation among extracted traffic features, leading to diminished classification accuracy. To address these issues, we propose a novel transformer-based IoT intrusion detection model, MBConv-ViT (MobileNet Convolution and Vision Transformer), which enhances the correlation of extracted features by fusing local and global features. By leveraging the high correlation of traffic flow, our model can identify subtle differences in IoT traffic flow, thereby achieving precise classification of attack traffic. Experiments based on the open datasets TON-IoT and Bot-IoT demonstrate that the accuracy of the MBConv-ViT model, respectively, 97.14% and 99.99%, is more effective than several existing typical models.",
      "intriguing_abstract": "With the rapid expansion and ubiquitous presence of the Internet of Things (IoT), the proliferation of IoT devices has reached unprecedented levels, heightening concerns about IoT security. Intrusion detection based on deep learning has become a crucial approach for safeguarding IoT ecosystems. However, challenges remain in IoT intrusion detection research, including inadequate feature representation at the classifier level and poor correlation among extracted traffic features, leading to diminished classification accuracy. To address these issues, we propose a novel transformer-based IoT intrusion detection model, MBConv-ViT (MobileNet Convolution and Vision Transformer), which enhances the correlation of extracted features by fusing local and global features. By leveraging the high correlation of traffic flow, our model can identify subtle differences in IoT traffic flow, thereby achieving precise classification of attack traffic. Experiments based on the open datasets TON-IoT and Bot-IoT demonstrate that the accuracy of the MBConv-ViT model, respectively, 97.14% and 99.99%, is more effective than several existing typical models.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/b6488fded5fb0728d7c3bd04f3afa3fba68d0450.pdf",
      "citation_key": "du20248pd",
      "metadata": {
        "title": "A Deep Learning-Based Intrusion Detection Model Integrating Convolutional Neural Network and Vision Transformer for Network Traffic Attack in the Internet of Things",
        "authors": [
          "Chunlai Du",
          "Yanhui Guo",
          "Yuhang Zhang"
        ],
        "published_date": "2024",
        "abstract": "With the rapid expansion and ubiquitous presence of the Internet of Things (IoT), the proliferation of IoT devices has reached unprecedented levels, heightening concerns about IoT security. Intrusion detection based on deep learning has become a crucial approach for safeguarding IoT ecosystems. However, challenges remain in IoT intrusion detection research, including inadequate feature representation at the classifier level and poor correlation among extracted traffic features, leading to diminished classification accuracy. To address these issues, we propose a novel transformer-based IoT intrusion detection model, MBConv-ViT (MobileNet Convolution and Vision Transformer), which enhances the correlation of extracted features by fusing local and global features. By leveraging the high correlation of traffic flow, our model can identify subtle differences in IoT traffic flow, thereby achieving precise classification of attack traffic. Experiments based on the open datasets TON-IoT and Bot-IoT demonstrate that the accuracy of the MBConv-ViT model, respectively, 97.14% and 99.99%, is more effective than several existing typical models.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b6488fded5fb0728d7c3bd04f3afa3fba68d0450.pdf",
        "venue": "Electronics",
        "citationCount": 5,
        "score": 5.0,
        "summary": "With the rapid expansion and ubiquitous presence of the Internet of Things (IoT), the proliferation of IoT devices has reached unprecedented levels, heightening concerns about IoT security. Intrusion detection based on deep learning has become a crucial approach for safeguarding IoT ecosystems. However, challenges remain in IoT intrusion detection research, including inadequate feature representation at the classifier level and poor correlation among extracted traffic features, leading to diminished classification accuracy. To address these issues, we propose a novel transformer-based IoT intrusion detection model, MBConv-ViT (MobileNet Convolution and Vision Transformer), which enhances the correlation of extracted features by fusing local and global features. By leveraging the high correlation of traffic flow, our model can identify subtle differences in IoT traffic flow, thereby achieving precise classification of attack traffic. Experiments based on the open datasets TON-IoT and Bot-IoT demonstrate that the accuracy of the MBConv-ViT model, respectively, 97.14% and 99.99%, is more effective than several existing typical models.",
        "keywords": []
      },
      "file_name": "b6488fded5fb0728d7c3bd04f3afa3fba68d0450.pdf"
    },
    {
      "success": true,
      "doc_id": "731dcdd4f8ab08378a8820c5667e8a80",
      "summary": "Background Gleason grading remains the gold standard for prostate cancer histological classification and prognosis, yet its subjectivity leads to grade variability between pathologists, potentially impacting clinical decision-making. Artificial intelligence (AI), particularly self-supervised vision transformer (ViT) architecture, can enhance diagnostic accuracy and consistency for prostate cancer. We trained and validated a generalised AI-driven system for diagnosing prostate cancer using diverse datasets from tissue microarray (TMA) core and whole slide images (WSIs) with Hematoxylin and Eosin staining. Methods We analysed eight prostate cancer datasets, which included 12,711 histological images from 3,648 patients, incorporating TMA core images and WSIs. Patches were extracted with 512 x 512 pixels size at 10x magnification from histological images with their corresponding mask annotations from segmentation data. The Macenko method was used to normalise colours for consistency across diverse images. Subsequently, we trained a multi-resolution (5x, 10x, 20x, and 40x) binary classifier to identify benign and malignant tissue. We then implemented a multi-class classifier for Gleason patterns (GP) sub-categorisation from malignant tissue. Finally, the models were externally validated on 11,132 histology images from 2,176 patients to determine the International Society of Urological Pathology (ISUP) grade. Models were assessed using various classification metrics, and the agreement between the model's predictions and the ground truth was quantified using the quadratic weighted Cohen's Kappa (k) score. Results Our multi-resolution binary classifier demonstrated robust performance in distinguishing malignant from benign tissue with k scores of 0.967 on internal validation. The model achieved k scores ranging from 0.876 to 0.995 across four unseen testing datasets. The multi-class classifier also performed well in distinguishing GP3, GP4, and GPs with an overall k score of 0.841. This model was further tested across four datasets, obtaining k scores ranging from 0.774 to 0.888. Attention maps generated by both classifiers revealed the clinical features of GPs. The models' performance was compared against an independent pathologist's annotation on an external dataset, achieving a k score of 0.752 for four classes. Conclusion Our self-supervised ViT-based model demonstrates high utility in diagnosing and grading prostate cancer using histological images. The models exhibit robust performance in categorising benign and malignant tissues, further differentiating malignant tissue based on the aggressiveness of cancer. Attention maps exposed high coherence with expert-confirmed pathological features. External validation reflects the robustness and generalizability of the models across different datasets, highlighting their clinical applicability for diagnosing and grading prostate cancer as a tool in digital pathology.",
      "intriguing_abstract": "Background Gleason grading remains the gold standard for prostate cancer histological classification and prognosis, yet its subjectivity leads to grade variability between pathologists, potentially impacting clinical decision-making. Artificial intelligence (AI), particularly self-supervised vision transformer (ViT) architecture, can enhance diagnostic accuracy and consistency for prostate cancer. We trained and validated a generalised AI-driven system for diagnosing prostate cancer using diverse datasets from tissue microarray (TMA) core and whole slide images (WSIs) with Hematoxylin and Eosin staining. Methods We analysed eight prostate cancer datasets, which included 12,711 histological images from 3,648 patients, incorporating TMA core images and WSIs. Patches were extracted with 512 x 512 pixels size at 10x magnification from histological images with their corresponding mask annotations from segmentation data. The Macenko method was used to normalise colours for consistency across diverse images. Subsequently, we trained a multi-resolution (5x, 10x, 20x, and 40x) binary classifier to identify benign and malignant tissue. We then implemented a multi-class classifier for Gleason patterns (GP) sub-categorisation from malignant tissue. Finally, the models were externally validated on 11,132 histology images from 2,176 patients to determine the International Society of Urological Pathology (ISUP) grade. Models were assessed using various classification metrics, and the agreement between the model's predictions and the ground truth was quantified using the quadratic weighted Cohen's Kappa (k) score. Results Our multi-resolution binary classifier demonstrated robust performance in distinguishing malignant from benign tissue with k scores of 0.967 on internal validation. The model achieved k scores ranging from 0.876 to 0.995 across four unseen testing datasets. The multi-class classifier also performed well in distinguishing GP3, GP4, and GPs with an overall k score of 0.841. This model was further tested across four datasets, obtaining k scores ranging from 0.774 to 0.888. Attention maps generated by both classifiers revealed the clinical features of GPs. The models' performance was compared against an independent pathologist's annotation on an external dataset, achieving a k score of 0.752 for four classes. Conclusion Our self-supervised ViT-based model demonstrates high utility in diagnosing and grading prostate cancer using histological images. The models exhibit robust performance in categorising benign and malignant tissues, further differentiating malignant tissue based on the aggressiveness of cancer. Attention maps exposed high coherence with expert-confirmed pathological features. External validation reflects the robustness and generalizability of the models across different datasets, highlighting their clinical applicability for diagnosing and grading prostate cancer as a tool in digital pathology.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/24eba2f5345bc8279bb63f085e75cc26b37d0b1e.pdf",
      "citation_key": "chaurasia2024tri",
      "metadata": {
        "title": "A generalised vision transformer-based self-supervised model for diagnosing and grading prostate cancer using histological images",
        "authors": [
          "A. Chaurasia",
          "H. C. Harris",
          "P. W. Toohey",
          "A. W. Hewitt"
        ],
        "published_date": "2024",
        "abstract": "Background Gleason grading remains the gold standard for prostate cancer histological classification and prognosis, yet its subjectivity leads to grade variability between pathologists, potentially impacting clinical decision-making. Artificial intelligence (AI), particularly self-supervised vision transformer (ViT) architecture, can enhance diagnostic accuracy and consistency for prostate cancer. We trained and validated a generalised AI-driven system for diagnosing prostate cancer using diverse datasets from tissue microarray (TMA) core and whole slide images (WSIs) with Hematoxylin and Eosin staining. Methods We analysed eight prostate cancer datasets, which included 12,711 histological images from 3,648 patients, incorporating TMA core images and WSIs. Patches were extracted with 512 x 512 pixels size at 10x magnification from histological images with their corresponding mask annotations from segmentation data. The Macenko method was used to normalise colours for consistency across diverse images. Subsequently, we trained a multi-resolution (5x, 10x, 20x, and 40x) binary classifier to identify benign and malignant tissue. We then implemented a multi-class classifier for Gleason patterns (GP) sub-categorisation from malignant tissue. Finally, the models were externally validated on 11,132 histology images from 2,176 patients to determine the International Society of Urological Pathology (ISUP) grade. Models were assessed using various classification metrics, and the agreement between the model's predictions and the ground truth was quantified using the quadratic weighted Cohen's Kappa (k) score. Results Our multi-resolution binary classifier demonstrated robust performance in distinguishing malignant from benign tissue with k scores of 0.967 on internal validation. The model achieved k scores ranging from 0.876 to 0.995 across four unseen testing datasets. The multi-class classifier also performed well in distinguishing GP3, GP4, and GPs with an overall k score of 0.841. This model was further tested across four datasets, obtaining k scores ranging from 0.774 to 0.888. Attention maps generated by both classifiers revealed the clinical features of GPs. The models' performance was compared against an independent pathologist's annotation on an external dataset, achieving a k score of 0.752 for four classes. Conclusion Our self-supervised ViT-based model demonstrates high utility in diagnosing and grading prostate cancer using histological images. The models exhibit robust performance in categorising benign and malignant tissues, further differentiating malignant tissue based on the aggressiveness of cancer. Attention maps exposed high coherence with expert-confirmed pathological features. External validation reflects the robustness and generalizability of the models across different datasets, highlighting their clinical applicability for diagnosing and grading prostate cancer as a tool in digital pathology.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/24eba2f5345bc8279bb63f085e75cc26b37d0b1e.pdf",
        "venue": "medRxiv",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Background Gleason grading remains the gold standard for prostate cancer histological classification and prognosis, yet its subjectivity leads to grade variability between pathologists, potentially impacting clinical decision-making. Artificial intelligence (AI), particularly self-supervised vision transformer (ViT) architecture, can enhance diagnostic accuracy and consistency for prostate cancer. We trained and validated a generalised AI-driven system for diagnosing prostate cancer using diverse datasets from tissue microarray (TMA) core and whole slide images (WSIs) with Hematoxylin and Eosin staining. Methods We analysed eight prostate cancer datasets, which included 12,711 histological images from 3,648 patients, incorporating TMA core images and WSIs. Patches were extracted with 512 x 512 pixels size at 10x magnification from histological images with their corresponding mask annotations from segmentation data. The Macenko method was used to normalise colours for consistency across diverse images. Subsequently, we trained a multi-resolution (5x, 10x, 20x, and 40x) binary classifier to identify benign and malignant tissue. We then implemented a multi-class classifier for Gleason patterns (GP) sub-categorisation from malignant tissue. Finally, the models were externally validated on 11,132 histology images from 2,176 patients to determine the International Society of Urological Pathology (ISUP) grade. Models were assessed using various classification metrics, and the agreement between the model's predictions and the ground truth was quantified using the quadratic weighted Cohen's Kappa (k) score. Results Our multi-resolution binary classifier demonstrated robust performance in distinguishing malignant from benign tissue with k scores of 0.967 on internal validation. The model achieved k scores ranging from 0.876 to 0.995 across four unseen testing datasets. The multi-class classifier also performed well in distinguishing GP3, GP4, and GPs with an overall k score of 0.841. This model was further tested across four datasets, obtaining k scores ranging from 0.774 to 0.888. Attention maps generated by both classifiers revealed the clinical features of GPs. The models' performance was compared against an independent pathologist's annotation on an external dataset, achieving a k score of 0.752 for four classes. Conclusion Our self-supervised ViT-based model demonstrates high utility in diagnosing and grading prostate cancer using histological images. The models exhibit robust performance in categorising benign and malignant tissues, further differentiating malignant tissue based on the aggressiveness of cancer. Attention maps exposed high coherence with expert-confirmed pathological features. External validation reflects the robustness and generalizability of the models across different datasets, highlighting their clinical applicability for diagnosing and grading prostate cancer as a tool in digital pathology.",
        "keywords": []
      },
      "file_name": "24eba2f5345bc8279bb63f085e75cc26b37d0b1e.pdf"
    },
    {
      "success": true,
      "doc_id": "fb693aac6796af333e16f428fd2cd06b",
      "summary": "Deep learning has proven very promising for interpreting MRI in brain tumor diagnosis. However, deep learning models suffer from a scarcity of brain MRI datasets for effective training. Self-supervised learning (SSL) models provide data-efficient and remarkable solutions to limited dataset problems. Therefore, this paper introduces a generative SSL model for brain tumor classification in two stages. The first stage is designed to pre-train a Residual Vision Transformer (ResViT) model for MRI synthesis as a pretext task. The second stage includes fine-tuning a ResViT-based classifier model as a downstream task. Accordingly, we aim to leverage local features via CNN and global features via ViT, employing a hybrid CNN-transformer architecture for ResViT in pretext and downstream tasks. Moreover, synthetic MRI images are utilized to balance the training set. The proposed model performs on public BraTs 2023, Figshare, and Kaggle datasets. Furthermore, we compare the proposed model with various deep learning models, including A-UNet, ResNet-9, pix2pix, pGAN for MRI synthesis, and ConvNeXtTiny, ResNet101, DenseNet12, Residual CNN, ViT for classification. According to the results, the proposed model pretraining on the MRI dataset is superior compared to the pretraining on the ImageNet dataset. Overall, the proposed model attains the highest accuracy, achieving 90.56% on the BraTs dataset with T1 sequence, 98.53% on the Figshare, and 98.47% on the Kaggle brain tumor datasets. As a result, the proposed model demonstrates a robust, effective, and successful approach to handling insufficient dataset challenges in MRI analysis by incorporating SSL, fine-tuning, data augmentation, and combining CNN and ViT.",
      "intriguing_abstract": "Deep learning has proven very promising for interpreting MRI in brain tumor diagnosis. However, deep learning models suffer from a scarcity of brain MRI datasets for effective training. Self-supervised learning (SSL) models provide data-efficient and remarkable solutions to limited dataset problems. Therefore, this paper introduces a generative SSL model for brain tumor classification in two stages. The first stage is designed to pre-train a Residual Vision Transformer (ResViT) model for MRI synthesis as a pretext task. The second stage includes fine-tuning a ResViT-based classifier model as a downstream task. Accordingly, we aim to leverage local features via CNN and global features via ViT, employing a hybrid CNN-transformer architecture for ResViT in pretext and downstream tasks. Moreover, synthetic MRI images are utilized to balance the training set. The proposed model performs on public BraTs 2023, Figshare, and Kaggle datasets. Furthermore, we compare the proposed model with various deep learning models, including A-UNet, ResNet-9, pix2pix, pGAN for MRI synthesis, and ConvNeXtTiny, ResNet101, DenseNet12, Residual CNN, ViT for classification. According to the results, the proposed model pretraining on the MRI dataset is superior compared to the pretraining on the ImageNet dataset. Overall, the proposed model attains the highest accuracy, achieving 90.56% on the BraTs dataset with T1 sequence, 98.53% on the Figshare, and 98.47% on the Kaggle brain tumor datasets. As a result, the proposed model demonstrates a robust, effective, and successful approach to handling insufficient dataset challenges in MRI analysis by incorporating SSL, fine-tuning, data augmentation, and combining CNN and ViT.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/8ce6872b26f88e52aeed57f6f7528dee30b5f7b2.pdf",
      "citation_key": "karagz2024ukp",
      "metadata": {
        "title": "Residual Vision Transformer (ResViT) Based Self-Supervised Learning Model for Brain Tumor Classification",
        "authors": [
          "Meryem Altin Karagz",
          "zkan U. Nalbantoglu",
          "Geoffrey C. Fox"
        ],
        "published_date": "2024",
        "abstract": "Deep learning has proven very promising for interpreting MRI in brain tumor diagnosis. However, deep learning models suffer from a scarcity of brain MRI datasets for effective training. Self-supervised learning (SSL) models provide data-efficient and remarkable solutions to limited dataset problems. Therefore, this paper introduces a generative SSL model for brain tumor classification in two stages. The first stage is designed to pre-train a Residual Vision Transformer (ResViT) model for MRI synthesis as a pretext task. The second stage includes fine-tuning a ResViT-based classifier model as a downstream task. Accordingly, we aim to leverage local features via CNN and global features via ViT, employing a hybrid CNN-transformer architecture for ResViT in pretext and downstream tasks. Moreover, synthetic MRI images are utilized to balance the training set. The proposed model performs on public BraTs 2023, Figshare, and Kaggle datasets. Furthermore, we compare the proposed model with various deep learning models, including A-UNet, ResNet-9, pix2pix, pGAN for MRI synthesis, and ConvNeXtTiny, ResNet101, DenseNet12, Residual CNN, ViT for classification. According to the results, the proposed model pretraining on the MRI dataset is superior compared to the pretraining on the ImageNet dataset. Overall, the proposed model attains the highest accuracy, achieving 90.56% on the BraTs dataset with T1 sequence, 98.53% on the Figshare, and 98.47% on the Kaggle brain tumor datasets. As a result, the proposed model demonstrates a robust, effective, and successful approach to handling insufficient dataset challenges in MRI analysis by incorporating SSL, fine-tuning, data augmentation, and combining CNN and ViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/8ce6872b26f88e52aeed57f6f7528dee30b5f7b2.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Deep learning has proven very promising for interpreting MRI in brain tumor diagnosis. However, deep learning models suffer from a scarcity of brain MRI datasets for effective training. Self-supervised learning (SSL) models provide data-efficient and remarkable solutions to limited dataset problems. Therefore, this paper introduces a generative SSL model for brain tumor classification in two stages. The first stage is designed to pre-train a Residual Vision Transformer (ResViT) model for MRI synthesis as a pretext task. The second stage includes fine-tuning a ResViT-based classifier model as a downstream task. Accordingly, we aim to leverage local features via CNN and global features via ViT, employing a hybrid CNN-transformer architecture for ResViT in pretext and downstream tasks. Moreover, synthetic MRI images are utilized to balance the training set. The proposed model performs on public BraTs 2023, Figshare, and Kaggle datasets. Furthermore, we compare the proposed model with various deep learning models, including A-UNet, ResNet-9, pix2pix, pGAN for MRI synthesis, and ConvNeXtTiny, ResNet101, DenseNet12, Residual CNN, ViT for classification. According to the results, the proposed model pretraining on the MRI dataset is superior compared to the pretraining on the ImageNet dataset. Overall, the proposed model attains the highest accuracy, achieving 90.56% on the BraTs dataset with T1 sequence, 98.53% on the Figshare, and 98.47% on the Kaggle brain tumor datasets. As a result, the proposed model demonstrates a robust, effective, and successful approach to handling insufficient dataset challenges in MRI analysis by incorporating SSL, fine-tuning, data augmentation, and combining CNN and ViT.",
        "keywords": []
      },
      "file_name": "8ce6872b26f88e52aeed57f6f7528dee30b5f7b2.pdf"
    },
    {
      "success": true,
      "doc_id": "43cc0c919da59bb1cfee85fe0a84d7fc",
      "summary": "Polysomnography (PSG) is crucial for diagnosing sleep disorders, but manual scoring of PSG is time-consuming and subjective, leading to high variability. While machine-learning models have improved PSG scoring, their clinical use is hindered by the black-box nature. In this study, we present SleepXViT, an automatic sleep staging system using Vision Transformer (ViT) that provides intuitive, consistent explanations by mimicking human visual scoring. Tested on KISSa PSG image dataset from 7745 patients across four hospitalsSleepXViT achieved a Macro F1 score of 81.94%, outperforming baseline models and showing robust performances on public datasets SHHS1 and SHHS2. Furthermore, SleepXViT offers well-calibrated confidence scores, enabling expert review for low-confidence predictions, alongside high-resolution heatmaps highlighting essential features and relevance scores for adjacent epochs influence on sleep stage predictions. Together, these explanations reinforce the scoring consistency of SleepXViT, making it both reliable and interpretable, thereby facilitating the synergy between the AI model and human scorers in clinical settings.",
      "intriguing_abstract": "Polysomnography (PSG) is crucial for diagnosing sleep disorders, but manual scoring of PSG is time-consuming and subjective, leading to high variability. While machine-learning models have improved PSG scoring, their clinical use is hindered by the black-box nature. In this study, we present SleepXViT, an automatic sleep staging system using Vision Transformer (ViT) that provides intuitive, consistent explanations by mimicking human visual scoring. Tested on KISSa PSG image dataset from 7745 patients across four hospitalsSleepXViT achieved a Macro F1 score of 81.94%, outperforming baseline models and showing robust performances on public datasets SHHS1 and SHHS2. Furthermore, SleepXViT offers well-calibrated confidence scores, enabling expert review for low-confidence predictions, alongside high-resolution heatmaps highlighting essential features and relevance scores for adjacent epochs influence on sleep stage predictions. Together, these explanations reinforce the scoring consistency of SleepXViT, making it both reliable and interpretable, thereby facilitating the synergy between the AI model and human scorers in clinical settings.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/dcf77b8e23bbc56c5c8f76ddc5d3cf68d0a051be.pdf",
      "citation_key": "lee2025r01",
      "metadata": {
        "title": "Explainable vision transformer for automatic visual sleep staging on multimodal PSG signals",
        "authors": [
          "Hyojin Lee",
          "You Rim Choi",
          "Hyun Kyung Lee",
          "Jaemin Jeong",
          "Joopyo Hong",
          "Hyun-Woo Shin",
          "Hyung-Sin Kim"
        ],
        "published_date": "2025",
        "abstract": "Polysomnography (PSG) is crucial for diagnosing sleep disorders, but manual scoring of PSG is time-consuming and subjective, leading to high variability. While machine-learning models have improved PSG scoring, their clinical use is hindered by the black-box nature. In this study, we present SleepXViT, an automatic sleep staging system using Vision Transformer (ViT) that provides intuitive, consistent explanations by mimicking human visual scoring. Tested on KISSa PSG image dataset from 7745 patients across four hospitalsSleepXViT achieved a Macro F1 score of 81.94%, outperforming baseline models and showing robust performances on public datasets SHHS1 and SHHS2. Furthermore, SleepXViT offers well-calibrated confidence scores, enabling expert review for low-confidence predictions, alongside high-resolution heatmaps highlighting essential features and relevance scores for adjacent epochs influence on sleep stage predictions. Together, these explanations reinforce the scoring consistency of SleepXViT, making it both reliable and interpretable, thereby facilitating the synergy between the AI model and human scorers in clinical settings.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/dcf77b8e23bbc56c5c8f76ddc5d3cf68d0a051be.pdf",
        "venue": "npj Digit. Medicine",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Polysomnography (PSG) is crucial for diagnosing sleep disorders, but manual scoring of PSG is time-consuming and subjective, leading to high variability. While machine-learning models have improved PSG scoring, their clinical use is hindered by the black-box nature. In this study, we present SleepXViT, an automatic sleep staging system using Vision Transformer (ViT) that provides intuitive, consistent explanations by mimicking human visual scoring. Tested on KISSa PSG image dataset from 7745 patients across four hospitalsSleepXViT achieved a Macro F1 score of 81.94%, outperforming baseline models and showing robust performances on public datasets SHHS1 and SHHS2. Furthermore, SleepXViT offers well-calibrated confidence scores, enabling expert review for low-confidence predictions, alongside high-resolution heatmaps highlighting essential features and relevance scores for adjacent epochs influence on sleep stage predictions. Together, these explanations reinforce the scoring consistency of SleepXViT, making it both reliable and interpretable, thereby facilitating the synergy between the AI model and human scorers in clinical settings.",
        "keywords": []
      },
      "file_name": "dcf77b8e23bbc56c5c8f76ddc5d3cf68d0a051be.pdf"
    },
    {
      "success": true,
      "doc_id": "eef35961d7518ca13a83ec7b23cbf290",
      "summary": "Assessing the quality of agricultural products holds vital significance in enhancing production efficiency and market viability. The adoption of artificial intelligence (AI) has notably surged for this purpose, employing deep learning and machine learning techniques to process and classify agricultural product images, adhering to defined standards. This study focuses on the lemon dataset, encompassing good and bad quality classes, initiate by augmenting data through rescaling, random zoom, flip, and rotation methods. Subsequently, employing eight diverse deep learning approaches and two transformer methods for classification, the study culminated in the ViT method achieving an unprecedented 99.84% accuracy, 99.95% recall, and 99.66% precision, marking the highest accuracy documented. These findings strongly advocate for the efficacy of the ViT method in successfully classifying lemon quality, spotlighting its potential impact on agricultural quality assessment.",
      "intriguing_abstract": "Assessing the quality of agricultural products holds vital significance in enhancing production efficiency and market viability. The adoption of artificial intelligence (AI) has notably surged for this purpose, employing deep learning and machine learning techniques to process and classify agricultural product images, adhering to defined standards. This study focuses on the lemon dataset, encompassing good and bad quality classes, initiate by augmenting data through rescaling, random zoom, flip, and rotation methods. Subsequently, employing eight diverse deep learning approaches and two transformer methods for classification, the study culminated in the ViT method achieving an unprecedented 99.84% accuracy, 99.95% recall, and 99.66% precision, marking the highest accuracy documented. These findings strongly advocate for the efficacy of the ViT method in successfully classifying lemon quality, spotlighting its potential impact on agricultural quality assessment.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f792dcf878e5bf8e7fb5aa31d231474d43462f4a.pdf",
      "citation_key": "dmen2024cb9",
      "metadata": {
        "title": "Performance of vision transformer and swin transformer models for lemon quality classification in fruit juice factories",
        "authors": [
          "Sezer Dmen",
          "Esra Kavalc Ylmaz",
          "Kemal Adem",
          "Erdin Avaroglu"
        ],
        "published_date": "2024",
        "abstract": "Assessing the quality of agricultural products holds vital significance in enhancing production efficiency and market viability. The adoption of artificial intelligence (AI) has notably surged for this purpose, employing deep learning and machine learning techniques to process and classify agricultural product images, adhering to defined standards. This study focuses on the lemon dataset, encompassing good and bad quality classes, initiate by augmenting data through rescaling, random zoom, flip, and rotation methods. Subsequently, employing eight diverse deep learning approaches and two transformer methods for classification, the study culminated in the ViT method achieving an unprecedented 99.84% accuracy, 99.95% recall, and 99.66% precision, marking the highest accuracy documented. These findings strongly advocate for the efficacy of the ViT method in successfully classifying lemon quality, spotlighting its potential impact on agricultural quality assessment.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f792dcf878e5bf8e7fb5aa31d231474d43462f4a.pdf",
        "venue": "European Food Research and Technology",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Assessing the quality of agricultural products holds vital significance in enhancing production efficiency and market viability. The adoption of artificial intelligence (AI) has notably surged for this purpose, employing deep learning and machine learning techniques to process and classify agricultural product images, adhering to defined standards. This study focuses on the lemon dataset, encompassing good and bad quality classes, initiate by augmenting data through rescaling, random zoom, flip, and rotation methods. Subsequently, employing eight diverse deep learning approaches and two transformer methods for classification, the study culminated in the ViT method achieving an unprecedented 99.84% accuracy, 99.95% recall, and 99.66% precision, marking the highest accuracy documented. These findings strongly advocate for the efficacy of the ViT method in successfully classifying lemon quality, spotlighting its potential impact on agricultural quality assessment.",
        "keywords": []
      },
      "file_name": "f792dcf878e5bf8e7fb5aa31d231474d43462f4a.pdf"
    },
    {
      "success": true,
      "doc_id": "2daa4a1a45d36d662d9ca9696e595b79",
      "summary": "Recently, the transformer-based model e.g., the vision transformer (ViT) has been extensively used in computer vision tasks. The superior performance of the ViT leads to the requirement of an enormous dataset and the complexity of calculating self-attention between patches is quadratic in nature. To acknowledge these two concerns, this paper proposes a novel shifted patch tokenization swin transformer (SPT-Swin) for the image classification task. The shifted patch tokenization (SPT) compensates for the data deficiency by increasing the data samples based on spatial information of the image patches while the swin transformer provides linear computational complexity by calculating self-attention between the shifted window based patches. For model validation, the SPT-Swin framework is trained on popular benchmark image datasets such as ImageNet-1K, CIFAR-10 and CIFAR-100, and the classification accuracies are found 89.45%, 95.67% and 92.95% respectively. Moreover, the comparative analysis of the proposed model with the existing state-of-the-art models shows that the classification performances are improved by 7.05%, 4.14%, and 8.30% for the ImageNet-1K, CIFAR-10 and CIFAR-100 datasets respectively. Therefore, our proposed SPT-based data augmentation technique with the core swin transformer model could be a data-efficient linear complex-able model for future computer vision tasks.",
      "intriguing_abstract": "Recently, the transformer-based model e.g., the vision transformer (ViT) has been extensively used in computer vision tasks. The superior performance of the ViT leads to the requirement of an enormous dataset and the complexity of calculating self-attention between patches is quadratic in nature. To acknowledge these two concerns, this paper proposes a novel shifted patch tokenization swin transformer (SPT-Swin) for the image classification task. The shifted patch tokenization (SPT) compensates for the data deficiency by increasing the data samples based on spatial information of the image patches while the swin transformer provides linear computational complexity by calculating self-attention between the shifted window based patches. For model validation, the SPT-Swin framework is trained on popular benchmark image datasets such as ImageNet-1K, CIFAR-10 and CIFAR-100, and the classification accuracies are found 89.45%, 95.67% and 92.95% respectively. Moreover, the comparative analysis of the proposed model with the existing state-of-the-art models shows that the classification performances are improved by 7.05%, 4.14%, and 8.30% for the ImageNet-1K, CIFAR-10 and CIFAR-100 datasets respectively. Therefore, our proposed SPT-based data augmentation technique with the core swin transformer model could be a data-efficient linear complex-able model for future computer vision tasks.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/310f5543603bef94d42366878a14161db1bf45de.pdf",
      "citation_key": "ferdous2024f89",
      "metadata": {
        "title": "SPT-Swin: A Shifted Patch Tokenization Swin Transformer for Image Classification",
        "authors": [
          "Gazi Jannatul Ferdous",
          "Khaleda Akhter Sathi",
          "Md. Azad Hossain",
          "M. Ali Akber Dewan"
        ],
        "published_date": "2024",
        "abstract": "Recently, the transformer-based model e.g., the vision transformer (ViT) has been extensively used in computer vision tasks. The superior performance of the ViT leads to the requirement of an enormous dataset and the complexity of calculating self-attention between patches is quadratic in nature. To acknowledge these two concerns, this paper proposes a novel shifted patch tokenization swin transformer (SPT-Swin) for the image classification task. The shifted patch tokenization (SPT) compensates for the data deficiency by increasing the data samples based on spatial information of the image patches while the swin transformer provides linear computational complexity by calculating self-attention between the shifted window based patches. For model validation, the SPT-Swin framework is trained on popular benchmark image datasets such as ImageNet-1K, CIFAR-10 and CIFAR-100, and the classification accuracies are found 89.45%, 95.67% and 92.95% respectively. Moreover, the comparative analysis of the proposed model with the existing state-of-the-art models shows that the classification performances are improved by 7.05%, 4.14%, and 8.30% for the ImageNet-1K, CIFAR-10 and CIFAR-100 datasets respectively. Therefore, our proposed SPT-based data augmentation technique with the core swin transformer model could be a data-efficient linear complex-able model for future computer vision tasks.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/310f5543603bef94d42366878a14161db1bf45de.pdf",
        "venue": "IEEE Access",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Recently, the transformer-based model e.g., the vision transformer (ViT) has been extensively used in computer vision tasks. The superior performance of the ViT leads to the requirement of an enormous dataset and the complexity of calculating self-attention between patches is quadratic in nature. To acknowledge these two concerns, this paper proposes a novel shifted patch tokenization swin transformer (SPT-Swin) for the image classification task. The shifted patch tokenization (SPT) compensates for the data deficiency by increasing the data samples based on spatial information of the image patches while the swin transformer provides linear computational complexity by calculating self-attention between the shifted window based patches. For model validation, the SPT-Swin framework is trained on popular benchmark image datasets such as ImageNet-1K, CIFAR-10 and CIFAR-100, and the classification accuracies are found 89.45%, 95.67% and 92.95% respectively. Moreover, the comparative analysis of the proposed model with the existing state-of-the-art models shows that the classification performances are improved by 7.05%, 4.14%, and 8.30% for the ImageNet-1K, CIFAR-10 and CIFAR-100 datasets respectively. Therefore, our proposed SPT-based data augmentation technique with the core swin transformer model could be a data-efficient linear complex-able model for future computer vision tasks.",
        "keywords": []
      },
      "file_name": "310f5543603bef94d42366878a14161db1bf45de.pdf"
    },
    {
      "success": true,
      "doc_id": "7287ed84227a43054e613f781360bf64",
      "summary": "The re-identification (ReID) of objects in images is a widely studied topic in computer vision, with significant relevance to various applications. The ReID of players in broadcast videos of team sports is the focus of this study. We specifically focus on identifying the same player in images taken at any given moment during a game from various camera angles. This work varies from other person ReID apps since the same team wears very similar clothes, there are few samples for each identification, and image resolutions are low. One of the hardest parts of object ReID is robust feature representation extraction. Despite the great success of current convolutional neural network-based (CNN) methods, most studies only consider learning representations from images, neglecting long-range dependency. Transformer-based model studies are increasing and yielding encouraging results. Transformers still have trouble extracting features from small objects and visual cues. To address these issues, we enhanced the Swin Transformer with the levering of CNNs. We created a regional feature extraction Swin Transformer (RFES) backbone to increase local feature extraction and small-scale object feature extraction. We also use three loss functions to handle imbalanced data and highlight challenging situations. Re-ranking with k-reciprocal encoding was used in this study's retrieval phase, and its assessment findings were provided. Finally, we conducted experiments on the Market-1501 and SoccerNet-v3 ReID datasets. Experimental results show that the proposed re-ID method reaches rank-1 accuracy of 96.2% with mAP: 89.1 and rank-1 accuracy of 84.1% with mAP: 86.7 on the Market-1501 and SoccerNet-v3 datasets, respectively, outperforming the state-of-the-art approaches.",
      "intriguing_abstract": "The re-identification (ReID) of objects in images is a widely studied topic in computer vision, with significant relevance to various applications. The ReID of players in broadcast videos of team sports is the focus of this study. We specifically focus on identifying the same player in images taken at any given moment during a game from various camera angles. This work varies from other person ReID apps since the same team wears very similar clothes, there are few samples for each identification, and image resolutions are low. One of the hardest parts of object ReID is robust feature representation extraction. Despite the great success of current convolutional neural network-based (CNN) methods, most studies only consider learning representations from images, neglecting long-range dependency. Transformer-based model studies are increasing and yielding encouraging results. Transformers still have trouble extracting features from small objects and visual cues. To address these issues, we enhanced the Swin Transformer with the levering of CNNs. We created a regional feature extraction Swin Transformer (RFES) backbone to increase local feature extraction and small-scale object feature extraction. We also use three loss functions to handle imbalanced data and highlight challenging situations. Re-ranking with k-reciprocal encoding was used in this study's retrieval phase, and its assessment findings were provided. Finally, we conducted experiments on the Market-1501 and SoccerNet-v3 ReID datasets. Experimental results show that the proposed re-ID method reaches rank-1 accuracy of 96.2% with mAP: 89.1 and rank-1 accuracy of 84.1% with mAP: 86.7 on the Market-1501 and SoccerNet-v3 datasets, respectively, outperforming the state-of-the-art approaches.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f9f524944dd9d1fe4b8956f9a822927b2eec29ad.pdf",
      "citation_key": "akan2024izq",
      "metadata": {
        "title": "An enhanced Swin Transformer for soccer player reidentification",
        "authors": [
          "Sara Akan",
          "Songl Varli",
          "Mohammad Alfrad Nobel Bhuiyan"
        ],
        "published_date": "2024",
        "abstract": "The re-identification (ReID) of objects in images is a widely studied topic in computer vision, with significant relevance to various applications. The ReID of players in broadcast videos of team sports is the focus of this study. We specifically focus on identifying the same player in images taken at any given moment during a game from various camera angles. This work varies from other person ReID apps since the same team wears very similar clothes, there are few samples for each identification, and image resolutions are low. One of the hardest parts of object ReID is robust feature representation extraction. Despite the great success of current convolutional neural network-based (CNN) methods, most studies only consider learning representations from images, neglecting long-range dependency. Transformer-based model studies are increasing and yielding encouraging results. Transformers still have trouble extracting features from small objects and visual cues. To address these issues, we enhanced the Swin Transformer with the levering of CNNs. We created a regional feature extraction Swin Transformer (RFES) backbone to increase local feature extraction and small-scale object feature extraction. We also use three loss functions to handle imbalanced data and highlight challenging situations. Re-ranking with k-reciprocal encoding was used in this study's retrieval phase, and its assessment findings were provided. Finally, we conducted experiments on the Market-1501 and SoccerNet-v3 ReID datasets. Experimental results show that the proposed re-ID method reaches rank-1 accuracy of 96.2% with mAP: 89.1 and rank-1 accuracy of 84.1% with mAP: 86.7 on the Market-1501 and SoccerNet-v3 datasets, respectively, outperforming the state-of-the-art approaches.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f9f524944dd9d1fe4b8956f9a822927b2eec29ad.pdf",
        "venue": "Scientific Reports",
        "citationCount": 4,
        "score": 4.0,
        "summary": "The re-identification (ReID) of objects in images is a widely studied topic in computer vision, with significant relevance to various applications. The ReID of players in broadcast videos of team sports is the focus of this study. We specifically focus on identifying the same player in images taken at any given moment during a game from various camera angles. This work varies from other person ReID apps since the same team wears very similar clothes, there are few samples for each identification, and image resolutions are low. One of the hardest parts of object ReID is robust feature representation extraction. Despite the great success of current convolutional neural network-based (CNN) methods, most studies only consider learning representations from images, neglecting long-range dependency. Transformer-based model studies are increasing and yielding encouraging results. Transformers still have trouble extracting features from small objects and visual cues. To address these issues, we enhanced the Swin Transformer with the levering of CNNs. We created a regional feature extraction Swin Transformer (RFES) backbone to increase local feature extraction and small-scale object feature extraction. We also use three loss functions to handle imbalanced data and highlight challenging situations. Re-ranking with k-reciprocal encoding was used in this study's retrieval phase, and its assessment findings were provided. Finally, we conducted experiments on the Market-1501 and SoccerNet-v3 ReID datasets. Experimental results show that the proposed re-ID method reaches rank-1 accuracy of 96.2% with mAP: 89.1 and rank-1 accuracy of 84.1% with mAP: 86.7 on the Market-1501 and SoccerNet-v3 datasets, respectively, outperforming the state-of-the-art approaches.",
        "keywords": []
      },
      "file_name": "f9f524944dd9d1fe4b8956f9a822927b2eec29ad.pdf"
    },
    {
      "success": true,
      "doc_id": "d01e20fcc8174cb2694b78e29e359067",
      "summary": "Automated assessment of tomato crop maturity is vital for improving agricultural productivity and reducing food waste. Traditionally, farmers have relied on visual inspection and manual assessment to predict tomato maturity, which is prone to human error and time-consuming. Computer vision and deep learning automate this process by analysing visual characteristics, enabling data-driven harvest decisions, optimising quality, and reducing waste for sustainable and efficient agriculture. This research demonstrates deep learning models accurately classifying tomato maturity stages using computer vision techniques, utilising a novel dataset of 4,353 tomato images. The Vision Transformer (ViT) model exhibited superior performance in classifying tomatoes into three ripeness categories (immature, mature, and partially mature), achieving a remarkable testing accuracy of 98.67% and the Convolution neural network (CNN) models, including EfficientNetB1, EfficientNetB5, EfficientNetB7, InceptionV3, ResNet50, and VGG16, achieved testing accuracies of 88.52%, 89.84%, 91.16%, 90.94%, 93.15%, and 92.27%, respectively, when tested with unseen data. ViT significantly surpassed the performance of CNN models. This research highlights the potential for deploying ViT in agricultural environments to monitor tomato maturity stages and packaging facilities smartly. Transformer-based systems could substantially reduce food waste and improve producer profits and productivity by optimising fruit harvest time and sorting decisions.",
      "intriguing_abstract": "Automated assessment of tomato crop maturity is vital for improving agricultural productivity and reducing food waste. Traditionally, farmers have relied on visual inspection and manual assessment to predict tomato maturity, which is prone to human error and time-consuming. Computer vision and deep learning automate this process by analysing visual characteristics, enabling data-driven harvest decisions, optimising quality, and reducing waste for sustainable and efficient agriculture. This research demonstrates deep learning models accurately classifying tomato maturity stages using computer vision techniques, utilising a novel dataset of 4,353 tomato images. The Vision Transformer (ViT) model exhibited superior performance in classifying tomatoes into three ripeness categories (immature, mature, and partially mature), achieving a remarkable testing accuracy of 98.67% and the Convolution neural network (CNN) models, including EfficientNetB1, EfficientNetB5, EfficientNetB7, InceptionV3, ResNet50, and VGG16, achieved testing accuracies of 88.52%, 89.84%, 91.16%, 90.94%, 93.15%, and 92.27%, respectively, when tested with unseen data. ViT significantly surpassed the performance of CNN models. This research highlights the potential for deploying ViT in agricultural environments to monitor tomato maturity stages and packaging facilities smartly. Transformer-based systems could substantially reduce food waste and improve producer profits and productivity by optimising fruit harvest time and sorting decisions.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/6ec48cb5fa3d149c9cc8a29373e64b63fe257913.pdf",
      "citation_key": "nahak20242mv",
      "metadata": {
        "title": "Tomato maturity stage prediction based on vision transformer and deep convolution neural networks",
        "authors": [
          "Pradeep Nahak",
          "D. K. Pratihar",
          "A. K. Deb"
        ],
        "published_date": "2024",
        "abstract": "Automated assessment of tomato crop maturity is vital for improving agricultural productivity and reducing food waste. Traditionally, farmers have relied on visual inspection and manual assessment to predict tomato maturity, which is prone to human error and time-consuming. Computer vision and deep learning automate this process by analysing visual characteristics, enabling data-driven harvest decisions, optimising quality, and reducing waste for sustainable and efficient agriculture. This research demonstrates deep learning models accurately classifying tomato maturity stages using computer vision techniques, utilising a novel dataset of 4,353 tomato images. The Vision Transformer (ViT) model exhibited superior performance in classifying tomatoes into three ripeness categories (immature, mature, and partially mature), achieving a remarkable testing accuracy of 98.67% and the Convolution neural network (CNN) models, including EfficientNetB1, EfficientNetB5, EfficientNetB7, InceptionV3, ResNet50, and VGG16, achieved testing accuracies of 88.52%, 89.84%, 91.16%, 90.94%, 93.15%, and 92.27%, respectively, when tested with unseen data. ViT significantly surpassed the performance of CNN models. This research highlights the potential for deploying ViT in agricultural environments to monitor tomato maturity stages and packaging facilities smartly. Transformer-based systems could substantially reduce food waste and improve producer profits and productivity by optimising fruit harvest time and sorting decisions.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6ec48cb5fa3d149c9cc8a29373e64b63fe257913.pdf",
        "venue": "International Journal of Hybrid Intelligent Systems",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Automated assessment of tomato crop maturity is vital for improving agricultural productivity and reducing food waste. Traditionally, farmers have relied on visual inspection and manual assessment to predict tomato maturity, which is prone to human error and time-consuming. Computer vision and deep learning automate this process by analysing visual characteristics, enabling data-driven harvest decisions, optimising quality, and reducing waste for sustainable and efficient agriculture. This research demonstrates deep learning models accurately classifying tomato maturity stages using computer vision techniques, utilising a novel dataset of 4,353 tomato images. The Vision Transformer (ViT) model exhibited superior performance in classifying tomatoes into three ripeness categories (immature, mature, and partially mature), achieving a remarkable testing accuracy of 98.67% and the Convolution neural network (CNN) models, including EfficientNetB1, EfficientNetB5, EfficientNetB7, InceptionV3, ResNet50, and VGG16, achieved testing accuracies of 88.52%, 89.84%, 91.16%, 90.94%, 93.15%, and 92.27%, respectively, when tested with unseen data. ViT significantly surpassed the performance of CNN models. This research highlights the potential for deploying ViT in agricultural environments to monitor tomato maturity stages and packaging facilities smartly. Transformer-based systems could substantially reduce food waste and improve producer profits and productivity by optimising fruit harvest time and sorting decisions.",
        "keywords": []
      },
      "file_name": "6ec48cb5fa3d149c9cc8a29373e64b63fe257913.pdf"
    },
    {
      "success": true,
      "doc_id": "fd7027c73952a3eb383644be8514e5d9",
      "summary": "In the current clinical healthcare environment, there is often a challenge where there is a wealth of unlabeled medical images, but a shortage of labeled images specific to particular medical cases. This limitation restricts the improvement of training effectiveness for deep learning models. This paper, starting from self-supervised learning and drawing inspiration from multi-task learning, explores a Vision Transformer-based self-supervised auxiliary task using MAE for medical image classification. This model establishes both a self-supervised auxiliary task and a supervised primary task, using a shared-weight VIT (Vision Transformer) encoder and synchronously updating network parameters. It effectively leverages both unlabeled and labeled medical images, resulting in promising outcomes. The model is alternately tested on two different medical image classification primary task datasets using three different types of self-supervised auxiliary task sets, and in most cases, it outperforms pure VIT models trained using both supervised and self-supervised learning methods under similar conditions.",
      "intriguing_abstract": "In the current clinical healthcare environment, there is often a challenge where there is a wealth of unlabeled medical images, but a shortage of labeled images specific to particular medical cases. This limitation restricts the improvement of training effectiveness for deep learning models. This paper, starting from self-supervised learning and drawing inspiration from multi-task learning, explores a Vision Transformer-based self-supervised auxiliary task using MAE for medical image classification. This model establishes both a self-supervised auxiliary task and a supervised primary task, using a shared-weight VIT (Vision Transformer) encoder and synchronously updating network parameters. It effectively leverages both unlabeled and labeled medical images, resulting in promising outcomes. The model is alternately tested on two different medical image classification primary task datasets using three different types of self-supervised auxiliary task sets, and in most cases, it outperforms pure VIT models trained using both supervised and self-supervised learning methods under similar conditions.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/3c7e07304bab6a860dbbe4ff36a4d87010036d2a.pdf",
      "citation_key": "han2024f96",
      "metadata": {
        "title": "MAT-VIT:A Vision Transformer with MAE-Based Self-Supervised Auxiliary Task for Medical Image Classification",
        "authors": [
          "Yufei Han",
          "Haoyuan Chen",
          "Linwei Yao",
          "Kuan Li",
          "Jianping Yin"
        ],
        "published_date": "2024",
        "abstract": "In the current clinical healthcare environment, there is often a challenge where there is a wealth of unlabeled medical images, but a shortage of labeled images specific to particular medical cases. This limitation restricts the improvement of training effectiveness for deep learning models. This paper, starting from self-supervised learning and drawing inspiration from multi-task learning, explores a Vision Transformer-based self-supervised auxiliary task using MAE for medical image classification. This model establishes both a self-supervised auxiliary task and a supervised primary task, using a shared-weight VIT (Vision Transformer) encoder and synchronously updating network parameters. It effectively leverages both unlabeled and labeled medical images, resulting in promising outcomes. The model is alternately tested on two different medical image classification primary task datasets using three different types of self-supervised auxiliary task sets, and in most cases, it outperforms pure VIT models trained using both supervised and self-supervised learning methods under similar conditions.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3c7e07304bab6a860dbbe4ff36a4d87010036d2a.pdf",
        "venue": "International Conference on Computer Supported Cooperative Work in Design",
        "citationCount": 4,
        "score": 4.0,
        "summary": "In the current clinical healthcare environment, there is often a challenge where there is a wealth of unlabeled medical images, but a shortage of labeled images specific to particular medical cases. This limitation restricts the improvement of training effectiveness for deep learning models. This paper, starting from self-supervised learning and drawing inspiration from multi-task learning, explores a Vision Transformer-based self-supervised auxiliary task using MAE for medical image classification. This model establishes both a self-supervised auxiliary task and a supervised primary task, using a shared-weight VIT (Vision Transformer) encoder and synchronously updating network parameters. It effectively leverages both unlabeled and labeled medical images, resulting in promising outcomes. The model is alternately tested on two different medical image classification primary task datasets using three different types of self-supervised auxiliary task sets, and in most cases, it outperforms pure VIT models trained using both supervised and self-supervised learning methods under similar conditions.",
        "keywords": []
      },
      "file_name": "3c7e07304bab6a860dbbe4ff36a4d87010036d2a.pdf"
    },
    {
      "success": true,
      "doc_id": "8316956ed0a11f82f416016c8d53c741",
      "summary": "Pulmonary cancer is one of the most common and deadliest cancers worldwide, and the detection of benign and malignant nodules in the lungs can be an important aid in the early diagnosis of lung cancer. Existing convolutional neural networks inherit their limitations by extracting global contextual information, and in most cases prove to be less efficient in obtaining satisfactory results. Transformer-based deep learning methods have obtained good performance in different computer vision tasks, and this study attempts to introduce them into the task of computed tomography (CT) image classification of lung nodules. However, the problems of sample scarcity and difficulty of local feature extraction in this field. To this end, we are inspired by Swin Transformer to propose a model named BiCFormer for the task of classifying and diagnosing CT scan images of lung nodules. Specifically, first we introduce a multi-layer discriminator generative adversarial network module for data augmentation to assist the model in extracting features more accurately. Second, unlike the encoder of traditional Transformer, we divide the encoder part of BiCFormer into two parts: bi-level coordinate (BiC) and fast-partial-window (FPW). The BiC module has a part similar to the traditional channel attention mechanism is able to enhance the performance of the model, and is more able to enhance the representation of attention object features by aggregating features along two spatial directions. The BiC module also has a dynamic sparse attention mechanism that filters out irrelevant key-value pairs in rough regions, allowing the model to focus more on features of interest. The FPW module is mainly used to reduce computational redundancy and minimize feature loss. We conducted extensive experiments on the LIDC-IDRI dataset. The experimental results show that our model achieves an accuracy of 97.4% compared to other studies using this dataset for lung nodule classification, making it an effective and competitive method.",
      "intriguing_abstract": "Pulmonary cancer is one of the most common and deadliest cancers worldwide, and the detection of benign and malignant nodules in the lungs can be an important aid in the early diagnosis of lung cancer. Existing convolutional neural networks inherit their limitations by extracting global contextual information, and in most cases prove to be less efficient in obtaining satisfactory results. Transformer-based deep learning methods have obtained good performance in different computer vision tasks, and this study attempts to introduce them into the task of computed tomography (CT) image classification of lung nodules. However, the problems of sample scarcity and difficulty of local feature extraction in this field. To this end, we are inspired by Swin Transformer to propose a model named BiCFormer for the task of classifying and diagnosing CT scan images of lung nodules. Specifically, first we introduce a multi-layer discriminator generative adversarial network module for data augmentation to assist the model in extracting features more accurately. Second, unlike the encoder of traditional Transformer, we divide the encoder part of BiCFormer into two parts: bi-level coordinate (BiC) and fast-partial-window (FPW). The BiC module has a part similar to the traditional channel attention mechanism is able to enhance the performance of the model, and is more able to enhance the representation of attention object features by aggregating features along two spatial directions. The BiC module also has a dynamic sparse attention mechanism that filters out irrelevant key-value pairs in rough regions, allowing the model to focus more on features of interest. The FPW module is mainly used to reduce computational redundancy and minimize feature loss. We conducted extensive experiments on the LIDC-IDRI dataset. The experimental results show that our model achieves an accuracy of 97.4% compared to other studies using this dataset for lung nodule classification, making it an effective and competitive method.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/819ae728828d50f56f234e35832b1222de081bfc.pdf",
      "citation_key": "zhao2024p8o",
      "metadata": {
        "title": "BiCFormer: Swin Transformer based model for classification of benign and malignant pulmonary nodules",
        "authors": [
          "Xiaoping Zhao",
          "Jingjing Xu",
          "Zhichen Lin",
          "Xingan Xue"
        ],
        "published_date": "2024",
        "abstract": "Pulmonary cancer is one of the most common and deadliest cancers worldwide, and the detection of benign and malignant nodules in the lungs can be an important aid in the early diagnosis of lung cancer. Existing convolutional neural networks inherit their limitations by extracting global contextual information, and in most cases prove to be less efficient in obtaining satisfactory results. Transformer-based deep learning methods have obtained good performance in different computer vision tasks, and this study attempts to introduce them into the task of computed tomography (CT) image classification of lung nodules. However, the problems of sample scarcity and difficulty of local feature extraction in this field. To this end, we are inspired by Swin Transformer to propose a model named BiCFormer for the task of classifying and diagnosing CT scan images of lung nodules. Specifically, first we introduce a multi-layer discriminator generative adversarial network module for data augmentation to assist the model in extracting features more accurately. Second, unlike the encoder of traditional Transformer, we divide the encoder part of BiCFormer into two parts: bi-level coordinate (BiC) and fast-partial-window (FPW). The BiC module has a part similar to the traditional channel attention mechanism is able to enhance the performance of the model, and is more able to enhance the representation of attention object features by aggregating features along two spatial directions. The BiC module also has a dynamic sparse attention mechanism that filters out irrelevant key-value pairs in rough regions, allowing the model to focus more on features of interest. The FPW module is mainly used to reduce computational redundancy and minimize feature loss. We conducted extensive experiments on the LIDC-IDRI dataset. The experimental results show that our model achieves an accuracy of 97.4% compared to other studies using this dataset for lung nodule classification, making it an effective and competitive method.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/819ae728828d50f56f234e35832b1222de081bfc.pdf",
        "venue": "Measurement science and technology",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Pulmonary cancer is one of the most common and deadliest cancers worldwide, and the detection of benign and malignant nodules in the lungs can be an important aid in the early diagnosis of lung cancer. Existing convolutional neural networks inherit their limitations by extracting global contextual information, and in most cases prove to be less efficient in obtaining satisfactory results. Transformer-based deep learning methods have obtained good performance in different computer vision tasks, and this study attempts to introduce them into the task of computed tomography (CT) image classification of lung nodules. However, the problems of sample scarcity and difficulty of local feature extraction in this field. To this end, we are inspired by Swin Transformer to propose a model named BiCFormer for the task of classifying and diagnosing CT scan images of lung nodules. Specifically, first we introduce a multi-layer discriminator generative adversarial network module for data augmentation to assist the model in extracting features more accurately. Second, unlike the encoder of traditional Transformer, we divide the encoder part of BiCFormer into two parts: bi-level coordinate (BiC) and fast-partial-window (FPW). The BiC module has a part similar to the traditional channel attention mechanism is able to enhance the performance of the model, and is more able to enhance the representation of attention object features by aggregating features along two spatial directions. The BiC module also has a dynamic sparse attention mechanism that filters out irrelevant key-value pairs in rough regions, allowing the model to focus more on features of interest. The FPW module is mainly used to reduce computational redundancy and minimize feature loss. We conducted extensive experiments on the LIDC-IDRI dataset. The experimental results show that our model achieves an accuracy of 97.4% compared to other studies using this dataset for lung nodule classification, making it an effective and competitive method.",
        "keywords": []
      },
      "file_name": "819ae728828d50f56f234e35832b1222de081bfc.pdf"
    },
    {
      "success": true,
      "doc_id": "6253d4f7ce77ce3a06a4564d5fa2767d",
      "summary": "Depth estimation from monocular vision sensor is a fundamental problem in scene perception with wide industrial applications. Previous works tend to predict the scene depth based on high-level features obtained by convolutional neural networks (CNNs) or rely on encoderdecoder frameworks of Transformers. However, they achieved less satisfactory results, especially around object contours. In this article, we propose a Transformer-based contour-aware depth estimation module to recover the scene depth with the aid of the enhanced perception of object contours. Besides, we develop a cascaded multiscale fusion module to aggregate multilevel features, where we combine the global context with local information and refine the depth map to a higher resolution from coarse to fine. Finally, we model depth estimation as a classification problem and discretize the depth value in an adaptive way to further improve the performance of our network. Extensive experiments have been conducted on mainstream public datasets (KITTI and NYUv2) to demonstrate the effectiveness of our network, where our network exhibits superior performance against other state-of-the-art methods.",
      "intriguing_abstract": "Depth estimation from monocular vision sensor is a fundamental problem in scene perception with wide industrial applications. Previous works tend to predict the scene depth based on high-level features obtained by convolutional neural networks (CNNs) or rely on encoderdecoder frameworks of Transformers. However, they achieved less satisfactory results, especially around object contours. In this article, we propose a Transformer-based contour-aware depth estimation module to recover the scene depth with the aid of the enhanced perception of object contours. Besides, we develop a cascaded multiscale fusion module to aggregate multilevel features, where we combine the global context with local information and refine the depth map to a higher resolution from coarse to fine. Finally, we model depth estimation as a classification problem and discretize the depth value in an adaptive way to further improve the performance of our network. Extensive experiments have been conducted on mainstream public datasets (KITTI and NYUv2) to demonstrate the effectiveness of our network, where our network exhibits superior performance against other state-of-the-art methods.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/6a9247fe471690218ef04cea5f6f4b59b3b50747.pdf",
      "citation_key": "li2024qva",
      "metadata": {
        "title": "A Contour-Aware Monocular Depth Estimation Network Using Swin Transformer and Cascaded Multiscale Fusion",
        "authors": [
          "Tao Li",
          "Yi Zhang"
        ],
        "published_date": "2024",
        "abstract": "Depth estimation from monocular vision sensor is a fundamental problem in scene perception with wide industrial applications. Previous works tend to predict the scene depth based on high-level features obtained by convolutional neural networks (CNNs) or rely on encoderdecoder frameworks of Transformers. However, they achieved less satisfactory results, especially around object contours. In this article, we propose a Transformer-based contour-aware depth estimation module to recover the scene depth with the aid of the enhanced perception of object contours. Besides, we develop a cascaded multiscale fusion module to aggregate multilevel features, where we combine the global context with local information and refine the depth map to a higher resolution from coarse to fine. Finally, we model depth estimation as a classification problem and discretize the depth value in an adaptive way to further improve the performance of our network. Extensive experiments have been conducted on mainstream public datasets (KITTI and NYUv2) to demonstrate the effectiveness of our network, where our network exhibits superior performance against other state-of-the-art methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6a9247fe471690218ef04cea5f6f4b59b3b50747.pdf",
        "venue": "IEEE Sensors Journal",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Depth estimation from monocular vision sensor is a fundamental problem in scene perception with wide industrial applications. Previous works tend to predict the scene depth based on high-level features obtained by convolutional neural networks (CNNs) or rely on encoderdecoder frameworks of Transformers. However, they achieved less satisfactory results, especially around object contours. In this article, we propose a Transformer-based contour-aware depth estimation module to recover the scene depth with the aid of the enhanced perception of object contours. Besides, we develop a cascaded multiscale fusion module to aggregate multilevel features, where we combine the global context with local information and refine the depth map to a higher resolution from coarse to fine. Finally, we model depth estimation as a classification problem and discretize the depth value in an adaptive way to further improve the performance of our network. Extensive experiments have been conducted on mainstream public datasets (KITTI and NYUv2) to demonstrate the effectiveness of our network, where our network exhibits superior performance against other state-of-the-art methods.",
        "keywords": []
      },
      "file_name": "6a9247fe471690218ef04cea5f6f4b59b3b50747.pdf"
    },
    {
      "success": true,
      "doc_id": "7745980fe6a417d23f95e00ab5b17693",
      "summary": "Self-attention and transformers have been widely used in deep learning. Recent efforts have been devoted to incorporating transformer blocks into different neural architectures, including those with convolutions, leading to various visual transformers for computer vision tasks. In this paper, we propose a novel and compact transformer block, Transformer with Learnable Token Merging (LTM), or LTM-Transformer. LTM-Transformer performs token merging in a learnable scheme. LTM-Transformer is compatible with many popular and compact transformer networks, and it reduces the FLOPs and the inference time of the visual transformers while maintaining or even improving the prediction accuracy. In the experiments, we replace all the transformer blocks in popular visual transformers, including MobileViT, EfficientViT, ViT, and Swin, with LTM-Transformer blocks, leading to LTM-Transformer networks with different backbones. The LTM-Transformer is motivated by reduction of Information Bottleneck, and a novel and separable variational upper bound for the IB loss is derived. The architecture of the mask module in our LTM blocks which generates the token merging mask is designed to reduce the derived upper bound for the IB loss. Extensive results on computer vision tasks evidence that LTM-Transformer renders compact and efficient visual transformers with comparable or much better prediction accuracy than the original visual transformers.",
      "intriguing_abstract": "Self-attention and transformers have been widely used in deep learning. Recent efforts have been devoted to incorporating transformer blocks into different neural architectures, including those with convolutions, leading to various visual transformers for computer vision tasks. In this paper, we propose a novel and compact transformer block, Transformer with Learnable Token Merging (LTM), or LTM-Transformer. LTM-Transformer performs token merging in a learnable scheme. LTM-Transformer is compatible with many popular and compact transformer networks, and it reduces the FLOPs and the inference time of the visual transformers while maintaining or even improving the prediction accuracy. In the experiments, we replace all the transformer blocks in popular visual transformers, including MobileViT, EfficientViT, ViT, and Swin, with LTM-Transformer blocks, leading to LTM-Transformer networks with different backbones. The LTM-Transformer is motivated by reduction of Information Bottleneck, and a novel and separable variational upper bound for the IB loss is derived. The architecture of the mask module in our LTM blocks which generates the token merging mask is designed to reduce the derived upper bound for the IB loss. Extensive results on computer vision tasks evidence that LTM-Transformer renders compact and efficient visual transformers with comparable or much better prediction accuracy than the original visual transformers.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f462bb00b8c4379c4a4699b66a19ce10da530b08.pdf",
      "citation_key": "wang2024ueo",
      "metadata": {
        "title": "Efficient Visual Transformer by Learnable Token Merging",
        "authors": [
          "Yancheng Wang",
          "Yingzhen Yang"
        ],
        "published_date": "2024",
        "abstract": "Self-attention and transformers have been widely used in deep learning. Recent efforts have been devoted to incorporating transformer blocks into different neural architectures, including those with convolutions, leading to various visual transformers for computer vision tasks. In this paper, we propose a novel and compact transformer block, Transformer with Learnable Token Merging (LTM), or LTM-Transformer. LTM-Transformer performs token merging in a learnable scheme. LTM-Transformer is compatible with many popular and compact transformer networks, and it reduces the FLOPs and the inference time of the visual transformers while maintaining or even improving the prediction accuracy. In the experiments, we replace all the transformer blocks in popular visual transformers, including MobileViT, EfficientViT, ViT, and Swin, with LTM-Transformer blocks, leading to LTM-Transformer networks with different backbones. The LTM-Transformer is motivated by reduction of Information Bottleneck, and a novel and separable variational upper bound for the IB loss is derived. The architecture of the mask module in our LTM blocks which generates the token merging mask is designed to reduce the derived upper bound for the IB loss. Extensive results on computer vision tasks evidence that LTM-Transformer renders compact and efficient visual transformers with comparable or much better prediction accuracy than the original visual transformers.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f462bb00b8c4379c4a4699b66a19ce10da530b08.pdf",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Self-attention and transformers have been widely used in deep learning. Recent efforts have been devoted to incorporating transformer blocks into different neural architectures, including those with convolutions, leading to various visual transformers for computer vision tasks. In this paper, we propose a novel and compact transformer block, Transformer with Learnable Token Merging (LTM), or LTM-Transformer. LTM-Transformer performs token merging in a learnable scheme. LTM-Transformer is compatible with many popular and compact transformer networks, and it reduces the FLOPs and the inference time of the visual transformers while maintaining or even improving the prediction accuracy. In the experiments, we replace all the transformer blocks in popular visual transformers, including MobileViT, EfficientViT, ViT, and Swin, with LTM-Transformer blocks, leading to LTM-Transformer networks with different backbones. The LTM-Transformer is motivated by reduction of Information Bottleneck, and a novel and separable variational upper bound for the IB loss is derived. The architecture of the mask module in our LTM blocks which generates the token merging mask is designed to reduce the derived upper bound for the IB loss. Extensive results on computer vision tasks evidence that LTM-Transformer renders compact and efficient visual transformers with comparable or much better prediction accuracy than the original visual transformers.",
        "keywords": []
      },
      "file_name": "f462bb00b8c4379c4a4699b66a19ce10da530b08.pdf"
    },
    {
      "success": true,
      "doc_id": "d24cbdb2be508271209834f493c01598",
      "summary": "This study addresses the fine-grained segmentation of defects in bridge pavements, which is crucial for the maintenance and structural safety of bridges. Although bridge pavements pose distinctive challenges owing to their unique characteristics and varied defect types, previous studies have primarily focused on the detection of slender cracks. To fill this research gap, we developed a novel end-to-end hybrid method that dynamically combines the vision transformer (ViT) and level set theory to handle the complex geometry of bridge pavement defects. The novelty of the proposed method lies in the configuration of two parallel decoders. These decoders, operating under a unified objective function, share weights and perform simultaneous optimization, thereby facilitating a holistic end-to-end training process. Furthermore, we compiled two new bridge pavement defect datasets, namely BdridgeDefX and BdridgeDef20, which offer broader applicability for practical defect detection. The results of a rigorous experimental validation on four datasets demonstrated the proposed methods capability of generating accurate defect boundaries and delivering state-of-the-art performance.",
      "intriguing_abstract": "This study addresses the fine-grained segmentation of defects in bridge pavements, which is crucial for the maintenance and structural safety of bridges. Although bridge pavements pose distinctive challenges owing to their unique characteristics and varied defect types, previous studies have primarily focused on the detection of slender cracks. To fill this research gap, we developed a novel end-to-end hybrid method that dynamically combines the vision transformer (ViT) and level set theory to handle the complex geometry of bridge pavement defects. The novelty of the proposed method lies in the configuration of two parallel decoders. These decoders, operating under a unified objective function, share weights and perform simultaneous optimization, thereby facilitating a holistic end-to-end training process. Furthermore, we compiled two new bridge pavement defect datasets, namely BdridgeDefX and BdridgeDef20, which offer broader applicability for practical defect detection. The results of a rigorous experimental validation on four datasets demonstrated the proposed methods capability of generating accurate defect boundaries and delivering state-of-the-art performance.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/52fafbbf20a37ffe863f1fa1d44fde6b4fc3334e.pdf",
      "citation_key": "qi2024f5d",
      "metadata": {
        "title": "A Vision-Transformer-Based Convex Variational Network for Bridge Pavement Defect Segmentation",
        "authors": [
          "Haochen Qi",
          "Xiangwei Kong",
          "Zhibo Jin",
          "Jiqiang Zhang",
          "Zinan Wang"
        ],
        "published_date": "2024",
        "abstract": "This study addresses the fine-grained segmentation of defects in bridge pavements, which is crucial for the maintenance and structural safety of bridges. Although bridge pavements pose distinctive challenges owing to their unique characteristics and varied defect types, previous studies have primarily focused on the detection of slender cracks. To fill this research gap, we developed a novel end-to-end hybrid method that dynamically combines the vision transformer (ViT) and level set theory to handle the complex geometry of bridge pavement defects. The novelty of the proposed method lies in the configuration of two parallel decoders. These decoders, operating under a unified objective function, share weights and perform simultaneous optimization, thereby facilitating a holistic end-to-end training process. Furthermore, we compiled two new bridge pavement defect datasets, namely BdridgeDefX and BdridgeDef20, which offer broader applicability for practical defect detection. The results of a rigorous experimental validation on four datasets demonstrated the proposed methods capability of generating accurate defect boundaries and delivering state-of-the-art performance.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/52fafbbf20a37ffe863f1fa1d44fde6b4fc3334e.pdf",
        "venue": "IEEE transactions on intelligent transportation systems (Print)",
        "citationCount": 4,
        "score": 4.0,
        "summary": "This study addresses the fine-grained segmentation of defects in bridge pavements, which is crucial for the maintenance and structural safety of bridges. Although bridge pavements pose distinctive challenges owing to their unique characteristics and varied defect types, previous studies have primarily focused on the detection of slender cracks. To fill this research gap, we developed a novel end-to-end hybrid method that dynamically combines the vision transformer (ViT) and level set theory to handle the complex geometry of bridge pavement defects. The novelty of the proposed method lies in the configuration of two parallel decoders. These decoders, operating under a unified objective function, share weights and perform simultaneous optimization, thereby facilitating a holistic end-to-end training process. Furthermore, we compiled two new bridge pavement defect datasets, namely BdridgeDefX and BdridgeDef20, which offer broader applicability for practical defect detection. The results of a rigorous experimental validation on four datasets demonstrated the proposed methods capability of generating accurate defect boundaries and delivering state-of-the-art performance.",
        "keywords": []
      },
      "file_name": "52fafbbf20a37ffe863f1fa1d44fde6b4fc3334e.pdf"
    },
    {
      "success": true,
      "doc_id": "0db10ad8b673dc2fa374a91e649b965c",
      "summary": "BACKGROUND\nBladder prolapse is a common clinical disorder of pelvic floor dysfunction in women, and early diagnosis and treatment can help them recover. Pelvic magnetic resonance imaging (MRI) is one of the most important methods used by physicians to diagnose bladder prolapse; however, it is highly subjective and largely dependent on the clinical experience of physicians. The application of computer-aided diagnostic techniques to achieve a graded diagnosis of bladder prolapse can help improve its accuracy and shorten the learning curve.\n\n\nPURPOSE\nThe purpose of this study is to combine convolutional neural network (CNN) and vision transformer (ViT) for grading bladder prolapse in place of traditional neural networks, and to incorporate attention mechanisms into mobile vision transformer (MobileViT) for assisting in the grading of bladder prolapse.\n\n\nMETHODS\nThis study focuses on the grading of bladder prolapse in pelvic organs using a combination of a CNN and a ViT. First, this study used MobileNetV2 to extract the local features of the images. Next, a ViT was used to extract the global features by modeling the non-local dependencies at a distance. Finally, a channel attention module (i.e., squeeze-and-excitation network) was used to improve the feature extraction network and enhance its feature representation capability. The final grading of the degree of bladder prolapse was thus achieved.\n\n\nRESULTS\nUsing pelvic MRI images provided by a Huzhou Maternal and Child Health Care Hospital, this study used the proposed method to grade patients with bladder prolapse. The accuracy, Kappa value, sensitivity, specificity, precision, and area under the curve of our method were 86.34%, 78.27%, 83.75%, 95.43%, 85.70%, and 95.05%, respectively. In comparison with other CNN models, the proposed method performed better.\n\n\nCONCLUSIONS\nThus, the model based on attention mechanisms exhibits better classification performance than existing methods for grading bladder prolapse in pelvic organs, and it can effectively assist physicians in achieving a more accurate bladder prolapse diagnosis.",
      "intriguing_abstract": "BACKGROUND\nBladder prolapse is a common clinical disorder of pelvic floor dysfunction in women, and early diagnosis and treatment can help them recover. Pelvic magnetic resonance imaging (MRI) is one of the most important methods used by physicians to diagnose bladder prolapse; however, it is highly subjective and largely dependent on the clinical experience of physicians. The application of computer-aided diagnostic techniques to achieve a graded diagnosis of bladder prolapse can help improve its accuracy and shorten the learning curve.\n\n\nPURPOSE\nThe purpose of this study is to combine convolutional neural network (CNN) and vision transformer (ViT) for grading bladder prolapse in place of traditional neural networks, and to incorporate attention mechanisms into mobile vision transformer (MobileViT) for assisting in the grading of bladder prolapse.\n\n\nMETHODS\nThis study focuses on the grading of bladder prolapse in pelvic organs using a combination of a CNN and a ViT. First, this study used MobileNetV2 to extract the local features of the images. Next, a ViT was used to extract the global features by modeling the non-local dependencies at a distance. Finally, a channel attention module (i.e., squeeze-and-excitation network) was used to improve the feature extraction network and enhance its feature representation capability. The final grading of the degree of bladder prolapse was thus achieved.\n\n\nRESULTS\nUsing pelvic MRI images provided by a Huzhou Maternal and Child Health Care Hospital, this study used the proposed method to grade patients with bladder prolapse. The accuracy, Kappa value, sensitivity, specificity, precision, and area under the curve of our method were 86.34%, 78.27%, 83.75%, 95.43%, 85.70%, and 95.05%, respectively. In comparison with other CNN models, the proposed method performed better.\n\n\nCONCLUSIONS\nThus, the model based on attention mechanisms exhibits better classification performance than existing methods for grading bladder prolapse in pelvic organs, and it can effectively assist physicians in achieving a more accurate bladder prolapse diagnosis.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/b4be4f2ecb8711755f75dcb09a57d8c9722ac349.pdf",
      "citation_key": "zhu2024l2i",
      "metadata": {
        "title": "Squeeze-and-excitation-attention-based mobile vision transformer for grading recognition of bladder prolapse in pelvic MRI images.",
        "authors": [
          "Shaojun Zhu",
          "Guotao Chen",
          "Hongguang Chen",
          "Ying Lu",
          "Maonian Wu",
          "Bo Zheng",
          "Dongquan Liu",
          "Cheng Qian",
          "Yun Chen"
        ],
        "published_date": "2024",
        "abstract": "BACKGROUND\nBladder prolapse is a common clinical disorder of pelvic floor dysfunction in women, and early diagnosis and treatment can help them recover. Pelvic magnetic resonance imaging (MRI) is one of the most important methods used by physicians to diagnose bladder prolapse; however, it is highly subjective and largely dependent on the clinical experience of physicians. The application of computer-aided diagnostic techniques to achieve a graded diagnosis of bladder prolapse can help improve its accuracy and shorten the learning curve.\n\n\nPURPOSE\nThe purpose of this study is to combine convolutional neural network (CNN) and vision transformer (ViT) for grading bladder prolapse in place of traditional neural networks, and to incorporate attention mechanisms into mobile vision transformer (MobileViT) for assisting in the grading of bladder prolapse.\n\n\nMETHODS\nThis study focuses on the grading of bladder prolapse in pelvic organs using a combination of a CNN and a ViT. First, this study used MobileNetV2 to extract the local features of the images. Next, a ViT was used to extract the global features by modeling the non-local dependencies at a distance. Finally, a channel attention module (i.e., squeeze-and-excitation network) was used to improve the feature extraction network and enhance its feature representation capability. The final grading of the degree of bladder prolapse was thus achieved.\n\n\nRESULTS\nUsing pelvic MRI images provided by a Huzhou Maternal and Child Health Care Hospital, this study used the proposed method to grade patients with bladder prolapse. The accuracy, Kappa value, sensitivity, specificity, precision, and area under the curve of our method were 86.34%, 78.27%, 83.75%, 95.43%, 85.70%, and 95.05%, respectively. In comparison with other CNN models, the proposed method performed better.\n\n\nCONCLUSIONS\nThus, the model based on attention mechanisms exhibits better classification performance than existing methods for grading bladder prolapse in pelvic organs, and it can effectively assist physicians in achieving a more accurate bladder prolapse diagnosis.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b4be4f2ecb8711755f75dcb09a57d8c9722ac349.pdf",
        "venue": "Medical Physics (Lancaster)",
        "citationCount": 4,
        "score": 4.0,
        "summary": "BACKGROUND\nBladder prolapse is a common clinical disorder of pelvic floor dysfunction in women, and early diagnosis and treatment can help them recover. Pelvic magnetic resonance imaging (MRI) is one of the most important methods used by physicians to diagnose bladder prolapse; however, it is highly subjective and largely dependent on the clinical experience of physicians. The application of computer-aided diagnostic techniques to achieve a graded diagnosis of bladder prolapse can help improve its accuracy and shorten the learning curve.\n\n\nPURPOSE\nThe purpose of this study is to combine convolutional neural network (CNN) and vision transformer (ViT) for grading bladder prolapse in place of traditional neural networks, and to incorporate attention mechanisms into mobile vision transformer (MobileViT) for assisting in the grading of bladder prolapse.\n\n\nMETHODS\nThis study focuses on the grading of bladder prolapse in pelvic organs using a combination of a CNN and a ViT. First, this study used MobileNetV2 to extract the local features of the images. Next, a ViT was used to extract the global features by modeling the non-local dependencies at a distance. Finally, a channel attention module (i.e., squeeze-and-excitation network) was used to improve the feature extraction network and enhance its feature representation capability. The final grading of the degree of bladder prolapse was thus achieved.\n\n\nRESULTS\nUsing pelvic MRI images provided by a Huzhou Maternal and Child Health Care Hospital, this study used the proposed method to grade patients with bladder prolapse. The accuracy, Kappa value, sensitivity, specificity, precision, and area under the curve of our method were 86.34%, 78.27%, 83.75%, 95.43%, 85.70%, and 95.05%, respectively. In comparison with other CNN models, the proposed method performed better.\n\n\nCONCLUSIONS\nThus, the model based on attention mechanisms exhibits better classification performance than existing methods for grading bladder prolapse in pelvic organs, and it can effectively assist physicians in achieving a more accurate bladder prolapse diagnosis.",
        "keywords": []
      },
      "file_name": "b4be4f2ecb8711755f75dcb09a57d8c9722ac349.pdf"
    },
    {
      "success": true,
      "doc_id": "90bae789c3cbd2affa595cce355f9fa9",
      "summary": "Acute Lymphoblastic Leukemia (ALL) stands as the most prevalent form of cancer among children and its diagnosis predominantly involves microscopic blood evaluations of bone marrow. A quick and accurate diagnosis is crucial for effective treatment and better survival rates. The intricate challenge emerges from categorizing leukemia into distinct sub-types aligned with WHO standards. This task deviates from binary classification due to the striking similarity of inter-class features, leading to misclassification. In response, a ViT-CNN ensemble model was introduced in this study to aid in the automated diagnosis of ALL. The proposed ensemble architecture seamlessly integrated Vision Transformer (ViT) with Convolutional Neural Network (CNN) to achieve accurate classification of leukemia sub-types. The ViT-CNN ensemble model orchestrated the extraction of cell image features through two distinct pathways, culminating in enhanced classification outcomes. Leveraging a publicly accessible dataset comprising blood cell images adhering to WHO standards, this study empirically showcased the efficacy of the approach. The proposed ViT-ResNet101v2 ensemble model achieved an exceptional overall accuracy of 99.39%, outperforming numerous prior methods on this dataset. This achievement represents a significant advancement compared to the existing research on leukemia. The proposed methodology adeptly discriminated between leukemia sub-types, thus serving as an efficacious computer-aided diagnostic tool for ALL.",
      "intriguing_abstract": "Acute Lymphoblastic Leukemia (ALL) stands as the most prevalent form of cancer among children and its diagnosis predominantly involves microscopic blood evaluations of bone marrow. A quick and accurate diagnosis is crucial for effective treatment and better survival rates. The intricate challenge emerges from categorizing leukemia into distinct sub-types aligned with WHO standards. This task deviates from binary classification due to the striking similarity of inter-class features, leading to misclassification. In response, a ViT-CNN ensemble model was introduced in this study to aid in the automated diagnosis of ALL. The proposed ensemble architecture seamlessly integrated Vision Transformer (ViT) with Convolutional Neural Network (CNN) to achieve accurate classification of leukemia sub-types. The ViT-CNN ensemble model orchestrated the extraction of cell image features through two distinct pathways, culminating in enhanced classification outcomes. Leveraging a publicly accessible dataset comprising blood cell images adhering to WHO standards, this study empirically showcased the efficacy of the approach. The proposed ViT-ResNet101v2 ensemble model achieved an exceptional overall accuracy of 99.39%, outperforming numerous prior methods on this dataset. This achievement represents a significant advancement compared to the existing research on leukemia. The proposed methodology adeptly discriminated between leukemia sub-types, thus serving as an efficacious computer-aided diagnostic tool for ALL.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/7492734c76036143baf574d6602bd45a348c416f.pdf",
      "citation_key": "roy2024r9y",
      "metadata": {
        "title": "A Cutting-Edge Ensemble of Vision Transformer and ResNet101v2 Based Transfer Learning for the Precise Classification of Leukemia Sub-types from Peripheral Blood Smear Images",
        "authors": [
          "Barsha Roy",
          "Md. Farukuzzaman Faruk",
          "Md Nazmul Islam",
          "Azmain Yakin Srizon",
          "S. M. Hasan",
          "Md. Al Mamun",
          "Md. Rakib Hossain",
          "Md. Faruk Hossain"
        ],
        "published_date": "2024",
        "abstract": "Acute Lymphoblastic Leukemia (ALL) stands as the most prevalent form of cancer among children and its diagnosis predominantly involves microscopic blood evaluations of bone marrow. A quick and accurate diagnosis is crucial for effective treatment and better survival rates. The intricate challenge emerges from categorizing leukemia into distinct sub-types aligned with WHO standards. This task deviates from binary classification due to the striking similarity of inter-class features, leading to misclassification. In response, a ViT-CNN ensemble model was introduced in this study to aid in the automated diagnosis of ALL. The proposed ensemble architecture seamlessly integrated Vision Transformer (ViT) with Convolutional Neural Network (CNN) to achieve accurate classification of leukemia sub-types. The ViT-CNN ensemble model orchestrated the extraction of cell image features through two distinct pathways, culminating in enhanced classification outcomes. Leveraging a publicly accessible dataset comprising blood cell images adhering to WHO standards, this study empirically showcased the efficacy of the approach. The proposed ViT-ResNet101v2 ensemble model achieved an exceptional overall accuracy of 99.39%, outperforming numerous prior methods on this dataset. This achievement represents a significant advancement compared to the existing research on leukemia. The proposed methodology adeptly discriminated between leukemia sub-types, thus serving as an efficacious computer-aided diagnostic tool for ALL.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7492734c76036143baf574d6602bd45a348c416f.pdf",
        "venue": "International Conference on Electrical Engineering and Information Communication Technology",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Acute Lymphoblastic Leukemia (ALL) stands as the most prevalent form of cancer among children and its diagnosis predominantly involves microscopic blood evaluations of bone marrow. A quick and accurate diagnosis is crucial for effective treatment and better survival rates. The intricate challenge emerges from categorizing leukemia into distinct sub-types aligned with WHO standards. This task deviates from binary classification due to the striking similarity of inter-class features, leading to misclassification. In response, a ViT-CNN ensemble model was introduced in this study to aid in the automated diagnosis of ALL. The proposed ensemble architecture seamlessly integrated Vision Transformer (ViT) with Convolutional Neural Network (CNN) to achieve accurate classification of leukemia sub-types. The ViT-CNN ensemble model orchestrated the extraction of cell image features through two distinct pathways, culminating in enhanced classification outcomes. Leveraging a publicly accessible dataset comprising blood cell images adhering to WHO standards, this study empirically showcased the efficacy of the approach. The proposed ViT-ResNet101v2 ensemble model achieved an exceptional overall accuracy of 99.39%, outperforming numerous prior methods on this dataset. This achievement represents a significant advancement compared to the existing research on leukemia. The proposed methodology adeptly discriminated between leukemia sub-types, thus serving as an efficacious computer-aided diagnostic tool for ALL.",
        "keywords": []
      },
      "file_name": "7492734c76036143baf574d6602bd45a348c416f.pdf"
    },
    {
      "success": true,
      "doc_id": "4b0356a72935961fddb6a2a88bead133",
      "summary": "Serous cavity effusion is a prevalent pathological condition encountered in clinical settings. Fluid samples obtained from these effusions are vital for diagnostic and therapeutic purposes. Traditionally, cytological examination of smears is a common method for diagnosing serous cavity effusion, renowned for its convenience. However, this technique presents limitations that can compromise its efficiency and diagnostic accuracy. This study aims to overcome these challenges and introduce an improved method for the precise detection of malignant cells in serous cavity effusions. We have developed a transformer-based classification framework, specifically employing the vision transformer (ViT) model, to fulfill this objective. Our research involved collecting smear images and corresponding cytological reports from 161 patients who underwent serous cavity drainage. We meticulously annotated 4836 patches from these images, identifying regions with and without malignant cells, thus creating a unique dataset for smear image classification. The findings of our study reveal that deep learning models, particularly the ViT model, exhibit remarkable accuracy in classifying patches as malignant or non-malignant. The ViT model achieved an impressive area under the receiver operating characteristic curve (AUROC) of 0.99, surpassing the performance of the convolutional neural network (CNN) model, which recorded an AUROC of 0.86. Additionally, we validated our models using an external cohort of 127 patients. The ViT model sustained its high-level screening performance, achieving an AUROC of 0.98 at the patient level, compared to the CNN models AUROC of 0.84. The visualization of our ViT models confirmed their capability to precisely identify regions containing malignant cells in multiple serous cavity effusion smear images. In summary, our study demonstrates the potential of deep learning models, particularly the ViT model, in automating the screening process for serous cavity effusions. These models offer significant assistance to cytologists in enhancing diagnostic accuracy and efficiency. The ViT model stands out for its advanced self-attention mechanism, making it exceptionally suitable for tasks that necessitate detailed analysis of small, sparsely distributed targets like cellular clusters in serous cavity effusions.",
      "intriguing_abstract": "Serous cavity effusion is a prevalent pathological condition encountered in clinical settings. Fluid samples obtained from these effusions are vital for diagnostic and therapeutic purposes. Traditionally, cytological examination of smears is a common method for diagnosing serous cavity effusion, renowned for its convenience. However, this technique presents limitations that can compromise its efficiency and diagnostic accuracy. This study aims to overcome these challenges and introduce an improved method for the precise detection of malignant cells in serous cavity effusions. We have developed a transformer-based classification framework, specifically employing the vision transformer (ViT) model, to fulfill this objective. Our research involved collecting smear images and corresponding cytological reports from 161 patients who underwent serous cavity drainage. We meticulously annotated 4836 patches from these images, identifying regions with and without malignant cells, thus creating a unique dataset for smear image classification. The findings of our study reveal that deep learning models, particularly the ViT model, exhibit remarkable accuracy in classifying patches as malignant or non-malignant. The ViT model achieved an impressive area under the receiver operating characteristic curve (AUROC) of 0.99, surpassing the performance of the convolutional neural network (CNN) model, which recorded an AUROC of 0.86. Additionally, we validated our models using an external cohort of 127 patients. The ViT model sustained its high-level screening performance, achieving an AUROC of 0.98 at the patient level, compared to the CNN models AUROC of 0.84. The visualization of our ViT models confirmed their capability to precisely identify regions containing malignant cells in multiple serous cavity effusion smear images. In summary, our study demonstrates the potential of deep learning models, particularly the ViT model, in automating the screening process for serous cavity effusions. These models offer significant assistance to cytologists in enhancing diagnostic accuracy and efficiency. The ViT model stands out for its advanced self-attention mechanism, making it exceptionally suitable for tasks that necessitate detailed analysis of small, sparsely distributed targets like cellular clusters in serous cavity effusions.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/eacb2581b480cf4a80bc601c8ea657651b41c0fc.pdf",
      "citation_key": "wang2024w4u",
      "metadata": {
        "title": "Multiple serous cavity effusion screening based on smear images using vision transformer",
        "authors": [
          "Chunbao Wang",
          "Xiangyu Wang",
          "Zeyu Gao",
          "Caihong Ran",
          "Chen Li",
          "Caixia Ding"
        ],
        "published_date": "2024",
        "abstract": "Serous cavity effusion is a prevalent pathological condition encountered in clinical settings. Fluid samples obtained from these effusions are vital for diagnostic and therapeutic purposes. Traditionally, cytological examination of smears is a common method for diagnosing serous cavity effusion, renowned for its convenience. However, this technique presents limitations that can compromise its efficiency and diagnostic accuracy. This study aims to overcome these challenges and introduce an improved method for the precise detection of malignant cells in serous cavity effusions. We have developed a transformer-based classification framework, specifically employing the vision transformer (ViT) model, to fulfill this objective. Our research involved collecting smear images and corresponding cytological reports from 161 patients who underwent serous cavity drainage. We meticulously annotated 4836 patches from these images, identifying regions with and without malignant cells, thus creating a unique dataset for smear image classification. The findings of our study reveal that deep learning models, particularly the ViT model, exhibit remarkable accuracy in classifying patches as malignant or non-malignant. The ViT model achieved an impressive area under the receiver operating characteristic curve (AUROC) of 0.99, surpassing the performance of the convolutional neural network (CNN) model, which recorded an AUROC of 0.86. Additionally, we validated our models using an external cohort of 127 patients. The ViT model sustained its high-level screening performance, achieving an AUROC of 0.98 at the patient level, compared to the CNN models AUROC of 0.84. The visualization of our ViT models confirmed their capability to precisely identify regions containing malignant cells in multiple serous cavity effusion smear images. In summary, our study demonstrates the potential of deep learning models, particularly the ViT model, in automating the screening process for serous cavity effusions. These models offer significant assistance to cytologists in enhancing diagnostic accuracy and efficiency. The ViT model stands out for its advanced self-attention mechanism, making it exceptionally suitable for tasks that necessitate detailed analysis of small, sparsely distributed targets like cellular clusters in serous cavity effusions.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/eacb2581b480cf4a80bc601c8ea657651b41c0fc.pdf",
        "venue": "Scientific Reports",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Serous cavity effusion is a prevalent pathological condition encountered in clinical settings. Fluid samples obtained from these effusions are vital for diagnostic and therapeutic purposes. Traditionally, cytological examination of smears is a common method for diagnosing serous cavity effusion, renowned for its convenience. However, this technique presents limitations that can compromise its efficiency and diagnostic accuracy. This study aims to overcome these challenges and introduce an improved method for the precise detection of malignant cells in serous cavity effusions. We have developed a transformer-based classification framework, specifically employing the vision transformer (ViT) model, to fulfill this objective. Our research involved collecting smear images and corresponding cytological reports from 161 patients who underwent serous cavity drainage. We meticulously annotated 4836 patches from these images, identifying regions with and without malignant cells, thus creating a unique dataset for smear image classification. The findings of our study reveal that deep learning models, particularly the ViT model, exhibit remarkable accuracy in classifying patches as malignant or non-malignant. The ViT model achieved an impressive area under the receiver operating characteristic curve (AUROC) of 0.99, surpassing the performance of the convolutional neural network (CNN) model, which recorded an AUROC of 0.86. Additionally, we validated our models using an external cohort of 127 patients. The ViT model sustained its high-level screening performance, achieving an AUROC of 0.98 at the patient level, compared to the CNN models AUROC of 0.84. The visualization of our ViT models confirmed their capability to precisely identify regions containing malignant cells in multiple serous cavity effusion smear images. In summary, our study demonstrates the potential of deep learning models, particularly the ViT model, in automating the screening process for serous cavity effusions. These models offer significant assistance to cytologists in enhancing diagnostic accuracy and efficiency. The ViT model stands out for its advanced self-attention mechanism, making it exceptionally suitable for tasks that necessitate detailed analysis of small, sparsely distributed targets like cellular clusters in serous cavity effusions.",
        "keywords": []
      },
      "file_name": "eacb2581b480cf4a80bc601c8ea657651b41c0fc.pdf"
    },
    {
      "success": true,
      "doc_id": "1eb390a7c42ccb66c8c3c1ef29346102",
      "summary": "In this paper, we propose a deep learning (DL)-based task-driven spectrum prediction framework, named DeepSPred. The DeepSPred comprises a feature encoder and a task predictor, where the encoder extracts spectrum usage pattern features, and the predictor configures different networks according to the task requirements to predict future spectrum. Based on the DeepSPred, we first propose a novel 3D spectrum prediction method combining a flow processing strategy with 3D vision Transformer (ViT, i.e., Swin) and a pyramid to serve possible applications such as spectrum monitoring task, named 3D-SwinSTB. 3D-SwinSTB unique 3D Patch Merging ViT-to-3D ViT Patch Expanding and pyramid designs help the model accurately learn the potential correlation of the evolution of the spectrogram over time. Then, we propose a novel spectrum occupancy rate (SOR) method by redesigning a predictor consisting exclusively of 3D convolutional and linear layers to serve possible applications such as dynamic spectrum access (DSA) task, named 3D-SwinLinear. Unlike the 3D-SwinSTB output spectrogram, 3D-SwinLinear projects the spectrogram directly as the SOR. Finally, we employ transfer learning (TL) to ensure the applicability of our two methods to diverse spectrum services. The results show that our 3D-SwinSTB outperforms recent benchmarks by more than 5%, while our 3D-SwinLinear achieves a 90% accuracy, with a performance improvement exceeding 10%.",
      "intriguing_abstract": "In this paper, we propose a deep learning (DL)-based task-driven spectrum prediction framework, named DeepSPred. The DeepSPred comprises a feature encoder and a task predictor, where the encoder extracts spectrum usage pattern features, and the predictor configures different networks according to the task requirements to predict future spectrum. Based on the DeepSPred, we first propose a novel 3D spectrum prediction method combining a flow processing strategy with 3D vision Transformer (ViT, i.e., Swin) and a pyramid to serve possible applications such as spectrum monitoring task, named 3D-SwinSTB. 3D-SwinSTB unique 3D Patch Merging ViT-to-3D ViT Patch Expanding and pyramid designs help the model accurately learn the potential correlation of the evolution of the spectrogram over time. Then, we propose a novel spectrum occupancy rate (SOR) method by redesigning a predictor consisting exclusively of 3D convolutional and linear layers to serve possible applications such as dynamic spectrum access (DSA) task, named 3D-SwinLinear. Unlike the 3D-SwinSTB output spectrogram, 3D-SwinLinear projects the spectrogram directly as the SOR. Finally, we employ transfer learning (TL) to ensure the applicability of our two methods to diverse spectrum services. The results show that our 3D-SwinSTB outperforms recent benchmarks by more than 5%, while our 3D-SwinLinear achieves a 90% accuracy, with a performance improvement exceeding 10%.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/da7d671f61e53601bdfc760d336cc98e5d9c516b.pdf",
      "citation_key": "pan202424q",
      "metadata": {
        "title": "Spectrum Prediction With Deep 3D Pyramid Vision Transformer Learning",
        "authors": [
          "Guangliang Pan",
          "Qihui Wu",
          "Bo Zhou",
          "Jie Li",
          "Wei Wang",
          "Guoru Ding",
          "David K. Y. Yau"
        ],
        "published_date": "2024",
        "abstract": "In this paper, we propose a deep learning (DL)-based task-driven spectrum prediction framework, named DeepSPred. The DeepSPred comprises a feature encoder and a task predictor, where the encoder extracts spectrum usage pattern features, and the predictor configures different networks according to the task requirements to predict future spectrum. Based on the DeepSPred, we first propose a novel 3D spectrum prediction method combining a flow processing strategy with 3D vision Transformer (ViT, i.e., Swin) and a pyramid to serve possible applications such as spectrum monitoring task, named 3D-SwinSTB. 3D-SwinSTB unique 3D Patch Merging ViT-to-3D ViT Patch Expanding and pyramid designs help the model accurately learn the potential correlation of the evolution of the spectrogram over time. Then, we propose a novel spectrum occupancy rate (SOR) method by redesigning a predictor consisting exclusively of 3D convolutional and linear layers to serve possible applications such as dynamic spectrum access (DSA) task, named 3D-SwinLinear. Unlike the 3D-SwinSTB output spectrogram, 3D-SwinLinear projects the spectrogram directly as the SOR. Finally, we employ transfer learning (TL) to ensure the applicability of our two methods to diverse spectrum services. The results show that our 3D-SwinSTB outperforms recent benchmarks by more than 5%, while our 3D-SwinLinear achieves a 90% accuracy, with a performance improvement exceeding 10%.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/da7d671f61e53601bdfc760d336cc98e5d9c516b.pdf",
        "venue": "IEEE Transactions on Wireless Communications",
        "citationCount": 4,
        "score": 4.0,
        "summary": "In this paper, we propose a deep learning (DL)-based task-driven spectrum prediction framework, named DeepSPred. The DeepSPred comprises a feature encoder and a task predictor, where the encoder extracts spectrum usage pattern features, and the predictor configures different networks according to the task requirements to predict future spectrum. Based on the DeepSPred, we first propose a novel 3D spectrum prediction method combining a flow processing strategy with 3D vision Transformer (ViT, i.e., Swin) and a pyramid to serve possible applications such as spectrum monitoring task, named 3D-SwinSTB. 3D-SwinSTB unique 3D Patch Merging ViT-to-3D ViT Patch Expanding and pyramid designs help the model accurately learn the potential correlation of the evolution of the spectrogram over time. Then, we propose a novel spectrum occupancy rate (SOR) method by redesigning a predictor consisting exclusively of 3D convolutional and linear layers to serve possible applications such as dynamic spectrum access (DSA) task, named 3D-SwinLinear. Unlike the 3D-SwinSTB output spectrogram, 3D-SwinLinear projects the spectrogram directly as the SOR. Finally, we employ transfer learning (TL) to ensure the applicability of our two methods to diverse spectrum services. The results show that our 3D-SwinSTB outperforms recent benchmarks by more than 5%, while our 3D-SwinLinear achieves a 90% accuracy, with a performance improvement exceeding 10%.",
        "keywords": []
      },
      "file_name": "da7d671f61e53601bdfc760d336cc98e5d9c516b.pdf"
    },
    {
      "success": true,
      "doc_id": "95908ec85fe4e59597d68bad617c4a1e",
      "summary": "The classification of mixed gases is one of the major functions of the electronic nose. To address the challenges associated with complex feature construction and inadequate feature extraction in gas classification, we propose a classification model for gas mixtures based on the vision transformer (ViT). The whole-process signals of the sensor array are taken as input signals in the proposed classification model, and self-attention mechanism is employed for the fusion of global information and adaptive feature extraction to make full use of the dependence of responses at different stages of the whole-process signals to improve the models classification accuracy. Our model exhibited a remarkable accuracy (96.66%) using a dataset containing acetone, methanol, ammonia, and their binary mixtures. In comparison, experiments conducted by support vector machine and a one-dimensional deep convolutional neural network model demonstrated classification accuracy of 90.56% and 92.75%, respectively. Experimental results indicate that the ViT gas classification model can be effectively combined with multi-channel time series data from the sensor array using the self-attention mechanism, thereby improving the accuracy of mixed gases classification. This advancement can be expected to become a standard method for classifying mixed gases.",
      "intriguing_abstract": "The classification of mixed gases is one of the major functions of the electronic nose. To address the challenges associated with complex feature construction and inadequate feature extraction in gas classification, we propose a classification model for gas mixtures based on the vision transformer (ViT). The whole-process signals of the sensor array are taken as input signals in the proposed classification model, and self-attention mechanism is employed for the fusion of global information and adaptive feature extraction to make full use of the dependence of responses at different stages of the whole-process signals to improve the models classification accuracy. Our model exhibited a remarkable accuracy (96.66%) using a dataset containing acetone, methanol, ammonia, and their binary mixtures. In comparison, experiments conducted by support vector machine and a one-dimensional deep convolutional neural network model demonstrated classification accuracy of 90.56% and 92.75%, respectively. Experimental results indicate that the ViT gas classification model can be effectively combined with multi-channel time series data from the sensor array using the self-attention mechanism, thereby improving the accuracy of mixed gases classification. This advancement can be expected to become a standard method for classifying mixed gases.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/2bb98b30fb9aae37858248cd5ac3221ebcf7eeba.pdf",
      "citation_key": "du2024lml",
      "metadata": {
        "title": "Vision transformer-based electronic nose for enhanced mixed gases classification",
        "authors": [
          "Haiying Du",
          "Jie Shen",
          "Jing Wang",
          "Qingyu Li",
          "Long Zhao",
          "Wanmin He",
          "Xianrong Li"
        ],
        "published_date": "2024",
        "abstract": "The classification of mixed gases is one of the major functions of the electronic nose. To address the challenges associated with complex feature construction and inadequate feature extraction in gas classification, we propose a classification model for gas mixtures based on the vision transformer (ViT). The whole-process signals of the sensor array are taken as input signals in the proposed classification model, and self-attention mechanism is employed for the fusion of global information and adaptive feature extraction to make full use of the dependence of responses at different stages of the whole-process signals to improve the models classification accuracy. Our model exhibited a remarkable accuracy (96.66%) using a dataset containing acetone, methanol, ammonia, and their binary mixtures. In comparison, experiments conducted by support vector machine and a one-dimensional deep convolutional neural network model demonstrated classification accuracy of 90.56% and 92.75%, respectively. Experimental results indicate that the ViT gas classification model can be effectively combined with multi-channel time series data from the sensor array using the self-attention mechanism, thereby improving the accuracy of mixed gases classification. This advancement can be expected to become a standard method for classifying mixed gases.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2bb98b30fb9aae37858248cd5ac3221ebcf7eeba.pdf",
        "venue": "Measurement science and technology",
        "citationCount": 4,
        "score": 4.0,
        "summary": "The classification of mixed gases is one of the major functions of the electronic nose. To address the challenges associated with complex feature construction and inadequate feature extraction in gas classification, we propose a classification model for gas mixtures based on the vision transformer (ViT). The whole-process signals of the sensor array are taken as input signals in the proposed classification model, and self-attention mechanism is employed for the fusion of global information and adaptive feature extraction to make full use of the dependence of responses at different stages of the whole-process signals to improve the models classification accuracy. Our model exhibited a remarkable accuracy (96.66%) using a dataset containing acetone, methanol, ammonia, and their binary mixtures. In comparison, experiments conducted by support vector machine and a one-dimensional deep convolutional neural network model demonstrated classification accuracy of 90.56% and 92.75%, respectively. Experimental results indicate that the ViT gas classification model can be effectively combined with multi-channel time series data from the sensor array using the self-attention mechanism, thereby improving the accuracy of mixed gases classification. This advancement can be expected to become a standard method for classifying mixed gases.",
        "keywords": []
      },
      "file_name": "2bb98b30fb9aae37858248cd5ac3221ebcf7eeba.pdf"
    },
    {
      "success": true,
      "doc_id": "15f979475cd1a042967e0cbc91d02390",
      "summary": "The increase in both the frequency and magnitude of natural disasters, coupled with recent advancements in artificial intelligence, has introduced prospects for investigating the potential of new technologies to facilitate disaster response processes. Preliminary Damage Assessment (PDA), a labor-intensive procedure necessitating manual examination of residential structures to ascertain post-disaster damage severity, stands to significantly benefit from the integration of computer vision-based classification algorithms, promising efficiency gains and heightened accuracy. Our paper proposes a Vision Transformer (ViT)-based model for classifying damage severity, achieving an accuracy rate of 95%. Notably, our model, trained on a repository of over 18,000 ground-level images of homes with damage severity annotated by damage assessment professionals during the 20202022 California wildfires, represents a novel application of ViT technology within this domain. Furthermore, we have open sourced this datasetthe first of its kind and scaleto be used by the research community. Additionally, we have developed a publicly accessible web application prototype built on this classification algorithm, which we have demonstrated to disaster management practitioners and received feedback on. Hence, our contribution to the literature encompasses the provision of a novel imagery dataset, an applied framework from field professionals, and a damage severity classification model with high accuracy.",
      "intriguing_abstract": "The increase in both the frequency and magnitude of natural disasters, coupled with recent advancements in artificial intelligence, has introduced prospects for investigating the potential of new technologies to facilitate disaster response processes. Preliminary Damage Assessment (PDA), a labor-intensive procedure necessitating manual examination of residential structures to ascertain post-disaster damage severity, stands to significantly benefit from the integration of computer vision-based classification algorithms, promising efficiency gains and heightened accuracy. Our paper proposes a Vision Transformer (ViT)-based model for classifying damage severity, achieving an accuracy rate of 95%. Notably, our model, trained on a repository of over 18,000 ground-level images of homes with damage severity annotated by damage assessment professionals during the 20202022 California wildfires, represents a novel application of ViT technology within this domain. Furthermore, we have open sourced this datasetthe first of its kind and scaleto be used by the research community. Additionally, we have developed a publicly accessible web application prototype built on this classification algorithm, which we have demonstrated to disaster management practitioners and received feedback on. Hence, our contribution to the literature encompasses the provision of a novel imagery dataset, an applied framework from field professionals, and a damage severity classification model with high accuracy.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/90e9d0a35bcf7e8251c4c9601a224bd95db14dd9.pdf",
      "citation_key": "luo202432g",
      "metadata": {
        "title": "Building a Vision Transformer-Based Damage Severity Classifier with Ground-Level Imagery of Homes Affected by California Wildfires",
        "authors": [
          "Kevin Luo",
          "Ie-bin Lian"
        ],
        "published_date": "2024",
        "abstract": "The increase in both the frequency and magnitude of natural disasters, coupled with recent advancements in artificial intelligence, has introduced prospects for investigating the potential of new technologies to facilitate disaster response processes. Preliminary Damage Assessment (PDA), a labor-intensive procedure necessitating manual examination of residential structures to ascertain post-disaster damage severity, stands to significantly benefit from the integration of computer vision-based classification algorithms, promising efficiency gains and heightened accuracy. Our paper proposes a Vision Transformer (ViT)-based model for classifying damage severity, achieving an accuracy rate of 95%. Notably, our model, trained on a repository of over 18,000 ground-level images of homes with damage severity annotated by damage assessment professionals during the 20202022 California wildfires, represents a novel application of ViT technology within this domain. Furthermore, we have open sourced this datasetthe first of its kind and scaleto be used by the research community. Additionally, we have developed a publicly accessible web application prototype built on this classification algorithm, which we have demonstrated to disaster management practitioners and received feedback on. Hence, our contribution to the literature encompasses the provision of a novel imagery dataset, an applied framework from field professionals, and a damage severity classification model with high accuracy.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/90e9d0a35bcf7e8251c4c9601a224bd95db14dd9.pdf",
        "venue": "Fire",
        "citationCount": 4,
        "score": 4.0,
        "summary": "The increase in both the frequency and magnitude of natural disasters, coupled with recent advancements in artificial intelligence, has introduced prospects for investigating the potential of new technologies to facilitate disaster response processes. Preliminary Damage Assessment (PDA), a labor-intensive procedure necessitating manual examination of residential structures to ascertain post-disaster damage severity, stands to significantly benefit from the integration of computer vision-based classification algorithms, promising efficiency gains and heightened accuracy. Our paper proposes a Vision Transformer (ViT)-based model for classifying damage severity, achieving an accuracy rate of 95%. Notably, our model, trained on a repository of over 18,000 ground-level images of homes with damage severity annotated by damage assessment professionals during the 20202022 California wildfires, represents a novel application of ViT technology within this domain. Furthermore, we have open sourced this datasetthe first of its kind and scaleto be used by the research community. Additionally, we have developed a publicly accessible web application prototype built on this classification algorithm, which we have demonstrated to disaster management practitioners and received feedback on. Hence, our contribution to the literature encompasses the provision of a novel imagery dataset, an applied framework from field professionals, and a damage severity classification model with high accuracy.",
        "keywords": []
      },
      "file_name": "90e9d0a35bcf7e8251c4c9601a224bd95db14dd9.pdf"
    },
    {
      "success": true,
      "doc_id": "e9fe1f45c8c03ececec0b777f2569f72",
      "summary": "With the rapid development of intelligent transportation systems and growing emphasis on driver safety, real-time detection of driver drowsiness has become a critical area of research. This study presents a robust and scalable driver drowsiness detection framework that integrates a Swin Transformer-based deep learning model with a diffusion model for image denoising. While conventional convolutional neural networks (CNNs) are effective in standard vision tasks, they often suffer performance degradation in real-world driving scenarios due to noise, poor lighting, motion blur, and adversarial attacks. To address these challenges, the proposed model focuses on eye-state detection, specifically, prolonged eye closure, as a primary indicator of driver disengagement and fatigue. Our system introduces a novel preprocessing stage using a denoising diffusion model built on a U-Net encoder-decoder architecture, effectively mitigating the impact of Gaussian noise and adversarial perturbations. Additionally, we incorporate adversarial training with Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks, demonstrating significant improvements in classification accuracy and resilience. Evaluations are conducted on two benchmark datasets, Eye-Blink and Closed Eyes in the Wild (CEW), under both clean and noisy conditions. Comparative experiments show that the proposed system outperforms several state-of-the-art models, including ViT, ResNet50V2, InceptionV3, MobileNet, DenseNet169, and VGG19, in terms of accuracy (up to 99.82%), PSNR (up to 41.61 dB), and SSIM (up to 0.984), while maintaining competitive inference times suitable for practical deployment. Moreover, a detailed sensitivity analysis of data augmentation strategies reveals that techniques such as rotation and horizontal flip substantially enhance the models generalization across variable visual inputs. The system also demonstrates improved robustness under real-world black-box scenarios and adversarial conditions. While this study primarily targets static image datasets, preliminary evaluations on dynamic video frames suggest potential for real-time monitoring applications. Overall, this research delivers a high-performing driver monitoring system capable of real-time drowsiness detection, even under adverse visual conditions. It lays a strong foundation for future extensions, including temporal modeling, real-time deployment, and multimodal integration (e.g., combining visual input with physiological signals such as EEG and heart rate) to further enhance driver safety and awareness in smart vehicles.",
      "intriguing_abstract": "With the rapid development of intelligent transportation systems and growing emphasis on driver safety, real-time detection of driver drowsiness has become a critical area of research. This study presents a robust and scalable driver drowsiness detection framework that integrates a Swin Transformer-based deep learning model with a diffusion model for image denoising. While conventional convolutional neural networks (CNNs) are effective in standard vision tasks, they often suffer performance degradation in real-world driving scenarios due to noise, poor lighting, motion blur, and adversarial attacks. To address these challenges, the proposed model focuses on eye-state detection, specifically, prolonged eye closure, as a primary indicator of driver disengagement and fatigue. Our system introduces a novel preprocessing stage using a denoising diffusion model built on a U-Net encoder-decoder architecture, effectively mitigating the impact of Gaussian noise and adversarial perturbations. Additionally, we incorporate adversarial training with Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks, demonstrating significant improvements in classification accuracy and resilience. Evaluations are conducted on two benchmark datasets, Eye-Blink and Closed Eyes in the Wild (CEW), under both clean and noisy conditions. Comparative experiments show that the proposed system outperforms several state-of-the-art models, including ViT, ResNet50V2, InceptionV3, MobileNet, DenseNet169, and VGG19, in terms of accuracy (up to 99.82%), PSNR (up to 41.61 dB), and SSIM (up to 0.984), while maintaining competitive inference times suitable for practical deployment. Moreover, a detailed sensitivity analysis of data augmentation strategies reveals that techniques such as rotation and horizontal flip substantially enhance the models generalization across variable visual inputs. The system also demonstrates improved robustness under real-world black-box scenarios and adversarial conditions. While this study primarily targets static image datasets, preliminary evaluations on dynamic video frames suggest potential for real-time monitoring applications. Overall, this research delivers a high-performing driver monitoring system capable of real-time drowsiness detection, even under adverse visual conditions. It lays a strong foundation for future extensions, including temporal modeling, real-time deployment, and multimodal integration (e.g., combining visual input with physiological signals such as EEG and heart rate) to further enhance driver safety and awareness in smart vehicles.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/5ce1ebe051a64969caeb879c44c45320a5ca8011.pdf",
      "citation_key": "elnabi2025psy",
      "metadata": {
        "title": "Driver Drowsiness Detection Using Swin Transformer and Diffusion Models for Robust Image Denoising",
        "authors": [
          "Samy Abd El-Nabi",
          "Ahmed F. Ibrahim",
          "El-Sayed M. El-Rabaie",
          "Osama F. Hassan",
          "N. Soliman",
          "K. Ramadan",
          "W. El-shafai"
        ],
        "published_date": "2025",
        "abstract": "With the rapid development of intelligent transportation systems and growing emphasis on driver safety, real-time detection of driver drowsiness has become a critical area of research. This study presents a robust and scalable driver drowsiness detection framework that integrates a Swin Transformer-based deep learning model with a diffusion model for image denoising. While conventional convolutional neural networks (CNNs) are effective in standard vision tasks, they often suffer performance degradation in real-world driving scenarios due to noise, poor lighting, motion blur, and adversarial attacks. To address these challenges, the proposed model focuses on eye-state detection, specifically, prolonged eye closure, as a primary indicator of driver disengagement and fatigue. Our system introduces a novel preprocessing stage using a denoising diffusion model built on a U-Net encoder-decoder architecture, effectively mitigating the impact of Gaussian noise and adversarial perturbations. Additionally, we incorporate adversarial training with Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks, demonstrating significant improvements in classification accuracy and resilience. Evaluations are conducted on two benchmark datasets, Eye-Blink and Closed Eyes in the Wild (CEW), under both clean and noisy conditions. Comparative experiments show that the proposed system outperforms several state-of-the-art models, including ViT, ResNet50V2, InceptionV3, MobileNet, DenseNet169, and VGG19, in terms of accuracy (up to 99.82%), PSNR (up to 41.61 dB), and SSIM (up to 0.984), while maintaining competitive inference times suitable for practical deployment. Moreover, a detailed sensitivity analysis of data augmentation strategies reveals that techniques such as rotation and horizontal flip substantially enhance the models generalization across variable visual inputs. The system also demonstrates improved robustness under real-world black-box scenarios and adversarial conditions. While this study primarily targets static image datasets, preliminary evaluations on dynamic video frames suggest potential for real-time monitoring applications. Overall, this research delivers a high-performing driver monitoring system capable of real-time drowsiness detection, even under adverse visual conditions. It lays a strong foundation for future extensions, including temporal modeling, real-time deployment, and multimodal integration (e.g., combining visual input with physiological signals such as EEG and heart rate) to further enhance driver safety and awareness in smart vehicles.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/5ce1ebe051a64969caeb879c44c45320a5ca8011.pdf",
        "venue": "IEEE Access",
        "citationCount": 4,
        "score": 4.0,
        "summary": "With the rapid development of intelligent transportation systems and growing emphasis on driver safety, real-time detection of driver drowsiness has become a critical area of research. This study presents a robust and scalable driver drowsiness detection framework that integrates a Swin Transformer-based deep learning model with a diffusion model for image denoising. While conventional convolutional neural networks (CNNs) are effective in standard vision tasks, they often suffer performance degradation in real-world driving scenarios due to noise, poor lighting, motion blur, and adversarial attacks. To address these challenges, the proposed model focuses on eye-state detection, specifically, prolonged eye closure, as a primary indicator of driver disengagement and fatigue. Our system introduces a novel preprocessing stage using a denoising diffusion model built on a U-Net encoder-decoder architecture, effectively mitigating the impact of Gaussian noise and adversarial perturbations. Additionally, we incorporate adversarial training with Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks, demonstrating significant improvements in classification accuracy and resilience. Evaluations are conducted on two benchmark datasets, Eye-Blink and Closed Eyes in the Wild (CEW), under both clean and noisy conditions. Comparative experiments show that the proposed system outperforms several state-of-the-art models, including ViT, ResNet50V2, InceptionV3, MobileNet, DenseNet169, and VGG19, in terms of accuracy (up to 99.82%), PSNR (up to 41.61 dB), and SSIM (up to 0.984), while maintaining competitive inference times suitable for practical deployment. Moreover, a detailed sensitivity analysis of data augmentation strategies reveals that techniques such as rotation and horizontal flip substantially enhance the models generalization across variable visual inputs. The system also demonstrates improved robustness under real-world black-box scenarios and adversarial conditions. While this study primarily targets static image datasets, preliminary evaluations on dynamic video frames suggest potential for real-time monitoring applications. Overall, this research delivers a high-performing driver monitoring system capable of real-time drowsiness detection, even under adverse visual conditions. It lays a strong foundation for future extensions, including temporal modeling, real-time deployment, and multimodal integration (e.g., combining visual input with physiological signals such as EEG and heart rate) to further enhance driver safety and awareness in smart vehicles.",
        "keywords": []
      },
      "file_name": "5ce1ebe051a64969caeb879c44c45320a5ca8011.pdf"
    },
    {
      "success": true,
      "doc_id": "cbf9fa277f0df9bfa68b641f2e8fdab5",
      "summary": "Bananas, renowned for their delightful flavor, exceptional nutritional value, and digestibility, are among the most widely consumed fruits globally. The advent of advanced image processing, computer vision, and deep learning (DL) techniques has revolutionized agricultural diagnostics, offering innovative and automated solutions for detecting and classifying fruit varieties. Despite significant progress in DL, the accurate classification of banana varieties remains challenging, particularly due to the difficulty in identifying subtle features at early developmental stages. To address these challenges, this study presents a novel hybrid framework that integrates the Vision Transformer (ViT) model for global semantic feature representation with the robust classification capabilities of Support Vector Machines. The proposed framework was rigorously evaluated on two datasets: the four-class BananaImageBD and the six-class BananaSet. To mitigate data imbalance issues, a robust evaluation strategy was employed, resulting in a remarkable classification accuracy rate (CAR) of 99.86%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\pm\\:$$\\end{document}0.099 for BananaSet and 99.70%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\pm\\:$$\\end{document}0.17 for BananaImageBD, surpassing traditional methods by a margin of 1.77%. The ViT model, leveraging self-supervised and semi-supervised learning mechanisms, demonstrated exceptional promise in extracting nuanced features critical for agricultural applications. By combining ViT features with cutting-edge machine learning classifiers, the proposed system establishes a new benchmark in precision and reliability for the automated detection and classification of banana varieties. These findings underscore the potential of hybrid DL frameworks in advancing agricultural diagnostics and pave the way for future innovations in the domain.",
      "intriguing_abstract": "Bananas, renowned for their delightful flavor, exceptional nutritional value, and digestibility, are among the most widely consumed fruits globally. The advent of advanced image processing, computer vision, and deep learning (DL) techniques has revolutionized agricultural diagnostics, offering innovative and automated solutions for detecting and classifying fruit varieties. Despite significant progress in DL, the accurate classification of banana varieties remains challenging, particularly due to the difficulty in identifying subtle features at early developmental stages. To address these challenges, this study presents a novel hybrid framework that integrates the Vision Transformer (ViT) model for global semantic feature representation with the robust classification capabilities of Support Vector Machines. The proposed framework was rigorously evaluated on two datasets: the four-class BananaImageBD and the six-class BananaSet. To mitigate data imbalance issues, a robust evaluation strategy was employed, resulting in a remarkable classification accuracy rate (CAR) of 99.86%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\pm\\:$$\\end{document}0.099 for BananaSet and 99.70%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\pm\\:$$\\end{document}0.17 for BananaImageBD, surpassing traditional methods by a margin of 1.77%. The ViT model, leveraging self-supervised and semi-supervised learning mechanisms, demonstrated exceptional promise in extracting nuanced features critical for agricultural applications. By combining ViT features with cutting-edge machine learning classifiers, the proposed system establishes a new benchmark in precision and reliability for the automated detection and classification of banana varieties. These findings underscore the potential of hybrid DL frameworks in advancing agricultural diagnostics and pave the way for future innovations in the domain.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/3bba1b3376e43a39bf4f1bf4eab558758a37ea2e.pdf",
      "citation_key": "ergn2025r6s",
      "metadata": {
        "title": "High precision banana variety identification using vision transformer based feature extraction and support vector machine",
        "authors": [
          "Ebru Ergn"
        ],
        "published_date": "2025",
        "abstract": "Bananas, renowned for their delightful flavor, exceptional nutritional value, and digestibility, are among the most widely consumed fruits globally. The advent of advanced image processing, computer vision, and deep learning (DL) techniques has revolutionized agricultural diagnostics, offering innovative and automated solutions for detecting and classifying fruit varieties. Despite significant progress in DL, the accurate classification of banana varieties remains challenging, particularly due to the difficulty in identifying subtle features at early developmental stages. To address these challenges, this study presents a novel hybrid framework that integrates the Vision Transformer (ViT) model for global semantic feature representation with the robust classification capabilities of Support Vector Machines. The proposed framework was rigorously evaluated on two datasets: the four-class BananaImageBD and the six-class BananaSet. To mitigate data imbalance issues, a robust evaluation strategy was employed, resulting in a remarkable classification accuracy rate (CAR) of 99.86%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\pm\\:$$\\end{document}0.099 for BananaSet and 99.70%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\pm\\:$$\\end{document}0.17 for BananaImageBD, surpassing traditional methods by a margin of 1.77%. The ViT model, leveraging self-supervised and semi-supervised learning mechanisms, demonstrated exceptional promise in extracting nuanced features critical for agricultural applications. By combining ViT features with cutting-edge machine learning classifiers, the proposed system establishes a new benchmark in precision and reliability for the automated detection and classification of banana varieties. These findings underscore the potential of hybrid DL frameworks in advancing agricultural diagnostics and pave the way for future innovations in the domain.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3bba1b3376e43a39bf4f1bf4eab558758a37ea2e.pdf",
        "venue": "Scientific Reports",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Bananas, renowned for their delightful flavor, exceptional nutritional value, and digestibility, are among the most widely consumed fruits globally. The advent of advanced image processing, computer vision, and deep learning (DL) techniques has revolutionized agricultural diagnostics, offering innovative and automated solutions for detecting and classifying fruit varieties. Despite significant progress in DL, the accurate classification of banana varieties remains challenging, particularly due to the difficulty in identifying subtle features at early developmental stages. To address these challenges, this study presents a novel hybrid framework that integrates the Vision Transformer (ViT) model for global semantic feature representation with the robust classification capabilities of Support Vector Machines. The proposed framework was rigorously evaluated on two datasets: the four-class BananaImageBD and the six-class BananaSet. To mitigate data imbalance issues, a robust evaluation strategy was employed, resulting in a remarkable classification accuracy rate (CAR) of 99.86%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\pm\\:$$\\end{document}0.099 for BananaSet and 99.70%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\pm\\:$$\\end{document}0.17 for BananaImageBD, surpassing traditional methods by a margin of 1.77%. The ViT model, leveraging self-supervised and semi-supervised learning mechanisms, demonstrated exceptional promise in extracting nuanced features critical for agricultural applications. By combining ViT features with cutting-edge machine learning classifiers, the proposed system establishes a new benchmark in precision and reliability for the automated detection and classification of banana varieties. These findings underscore the potential of hybrid DL frameworks in advancing agricultural diagnostics and pave the way for future innovations in the domain.",
        "keywords": []
      },
      "file_name": "3bba1b3376e43a39bf4f1bf4eab558758a37ea2e.pdf"
    },
    {
      "success": true,
      "doc_id": "9c9ed2fe1f9d8ff0626d1c9f92425825",
      "summary": "In the evolving landscape of 6G networks, semantic communications are poised to revolutionize data transmission by prioritizing the transmission of semantic meaning over raw data accuracy. This paper presents a Vision Transformer (ViT)-based semantic communication framework that has been deliberately designed to achieve high semantic similarity during image transmission while simultaneously minimizing the demand for bandwidth. By equipping ViT as the encoder-decoder framework, the proposed architecture can proficiently encode images into a high semantic content at the transmitter and precisely reconstruct the images, considering real-world fading and noise consideration at the receiver. Building on the attention mechanisms inherent to ViTs, our model outperforms Convolution Neural Network (CNNs) and Generative Adversarial Networks (GANs) tailored for generating such images. The architecture based on the proposed ViT network achieves the Peak Signal-to-noise Ratio (PSNR) of 38 dB, which is higher than other Deep Learning (DL) approaches in maintaining semantic similarity across different communication environments. These findings establish our ViT-based approach as a significant breakthrough in semantic communications.",
      "intriguing_abstract": "In the evolving landscape of 6G networks, semantic communications are poised to revolutionize data transmission by prioritizing the transmission of semantic meaning over raw data accuracy. This paper presents a Vision Transformer (ViT)-based semantic communication framework that has been deliberately designed to achieve high semantic similarity during image transmission while simultaneously minimizing the demand for bandwidth. By equipping ViT as the encoder-decoder framework, the proposed architecture can proficiently encode images into a high semantic content at the transmitter and precisely reconstruct the images, considering real-world fading and noise consideration at the receiver. Building on the attention mechanisms inherent to ViTs, our model outperforms Convolution Neural Network (CNNs) and Generative Adversarial Networks (GANs) tailored for generating such images. The architecture based on the proposed ViT network achieves the Peak Signal-to-noise Ratio (PSNR) of 38 dB, which is higher than other Deep Learning (DL) approaches in maintaining semantic similarity across different communication environments. These findings establish our ViT-based approach as a significant breakthrough in semantic communications.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/3b11c8774bbeddd6722d65455bf9bb729a63c1cb.pdf",
      "citation_key": "mohsin2025gup",
      "metadata": {
        "title": "Vision Transformer Based Semantic Communications for Next Generation Wireless Networks",
        "authors": [
          "Muhammad Ahmed Mohsin",
          "Muhammad Jazib",
          "Zeeshan Alam",
          "Muhammad Farhan Khan",
          "Muhammad Saad",
          "Muhammad Ali Jamshed"
        ],
        "published_date": "2025",
        "abstract": "In the evolving landscape of 6G networks, semantic communications are poised to revolutionize data transmission by prioritizing the transmission of semantic meaning over raw data accuracy. This paper presents a Vision Transformer (ViT)-based semantic communication framework that has been deliberately designed to achieve high semantic similarity during image transmission while simultaneously minimizing the demand for bandwidth. By equipping ViT as the encoder-decoder framework, the proposed architecture can proficiently encode images into a high semantic content at the transmitter and precisely reconstruct the images, considering real-world fading and noise consideration at the receiver. Building on the attention mechanisms inherent to ViTs, our model outperforms Convolution Neural Network (CNNs) and Generative Adversarial Networks (GANs) tailored for generating such images. The architecture based on the proposed ViT network achieves the Peak Signal-to-noise Ratio (PSNR) of 38 dB, which is higher than other Deep Learning (DL) approaches in maintaining semantic similarity across different communication environments. These findings establish our ViT-based approach as a significant breakthrough in semantic communications.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3b11c8774bbeddd6722d65455bf9bb729a63c1cb.pdf",
        "venue": "2025 IEEE International Conference on Communications Workshops (ICC Workshops)",
        "citationCount": 4,
        "score": 4.0,
        "summary": "In the evolving landscape of 6G networks, semantic communications are poised to revolutionize data transmission by prioritizing the transmission of semantic meaning over raw data accuracy. This paper presents a Vision Transformer (ViT)-based semantic communication framework that has been deliberately designed to achieve high semantic similarity during image transmission while simultaneously minimizing the demand for bandwidth. By equipping ViT as the encoder-decoder framework, the proposed architecture can proficiently encode images into a high semantic content at the transmitter and precisely reconstruct the images, considering real-world fading and noise consideration at the receiver. Building on the attention mechanisms inherent to ViTs, our model outperforms Convolution Neural Network (CNNs) and Generative Adversarial Networks (GANs) tailored for generating such images. The architecture based on the proposed ViT network achieves the Peak Signal-to-noise Ratio (PSNR) of 38 dB, which is higher than other Deep Learning (DL) approaches in maintaining semantic similarity across different communication environments. These findings establish our ViT-based approach as a significant breakthrough in semantic communications.",
        "keywords": []
      },
      "file_name": "3b11c8774bbeddd6722d65455bf9bb729a63c1cb.pdf"
    },
    {
      "success": true,
      "doc_id": "bde5fabd4306c7394a9fdc4d3c727e76",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/04ed3b0f4e21d9f4884bf4db98edf83a34d005a9.pdf",
      "citation_key": "marcos2024oo2",
      "metadata": {
        "title": "Pure Vision Transformer (CT-ViT) with Noise2Neighbors Interpolation for Low-Dose CT Image Denoising.",
        "authors": [
          "Luella Marcos",
          "Paul S. Babyn",
          "J. Alirezaie"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/04ed3b0f4e21d9f4884bf4db98edf83a34d005a9.pdf",
        "venue": "Journal of imaging informatics in medicine",
        "citationCount": 4,
        "score": 4.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "04ed3b0f4e21d9f4884bf4db98edf83a34d005a9.pdf"
    },
    {
      "success": true,
      "doc_id": "12ace11ba6e02f92503653a54f7e6a6e",
      "summary": "ABSTRACT In order to achieve real-time classification and detection of various chicken parts, this study introduces an optimized Swin-Transformer method for the classification and detection of multiple chicken parts. It initially leverages the Transformers self-attention structure to capture more comprehensive high-level visual semantic information from chicken part images. The image enhancement technique was applied to the image in the preprocessing stage to enhance the feature information of the image, and the migration learning method was used to train and optimize the Swin-Transformer model on the enhanced chicken parts dataset for classification and detection of chicken parts. Furthermore, this model was compared to four commonly used models in object target detection tasks: YOLOV3-Darknet53, YOLOV3-MobileNetv3, SSD-MobileNetv3, and SSD-VGG16. The results indicated that the Swin-Transformer model outperforms these models with a higher mAP value by 1.62%, 2.13%, 5.26%, and 4.48%, accompanied by a reduction in detection time by 16.18ms, 5.08ms, 9.38ms, and 23.48ms, respectively. The method of this study fulfills the production line requirements while exhibiting superior performance and greater robustness compared to existing conventional methods. GRAPHICAL ABSTRACT",
      "intriguing_abstract": "ABSTRACT In order to achieve real-time classification and detection of various chicken parts, this study introduces an optimized Swin-Transformer method for the classification and detection of multiple chicken parts. It initially leverages the Transformers self-attention structure to capture more comprehensive high-level visual semantic information from chicken part images. The image enhancement technique was applied to the image in the preprocessing stage to enhance the feature information of the image, and the migration learning method was used to train and optimize the Swin-Transformer model on the enhanced chicken parts dataset for classification and detection of chicken parts. Furthermore, this model was compared to four commonly used models in object target detection tasks: YOLOV3-Darknet53, YOLOV3-MobileNetv3, SSD-MobileNetv3, and SSD-VGG16. The results indicated that the Swin-Transformer model outperforms these models with a higher mAP value by 1.62%, 2.13%, 5.26%, and 4.48%, accompanied by a reduction in detection time by 16.18ms, 5.08ms, 9.38ms, and 23.48ms, respectively. The method of this study fulfills the production line requirements while exhibiting superior performance and greater robustness compared to existing conventional methods. GRAPHICAL ABSTRACT",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/23ce9c2814d6567efec884b7043977cefcb7602e.pdf",
      "citation_key": "peng2024kal",
      "metadata": {
        "title": "Computer vision classification detection of chicken parts based on optimized Swin-Transformer",
        "authors": [
          "Xianhui Peng",
          "Chenchen Xu",
          "Peng Zhang",
          "Dandan Fu",
          "Yan Chen",
          "Zhigang Hu"
        ],
        "published_date": "2024",
        "abstract": "ABSTRACT In order to achieve real-time classification and detection of various chicken parts, this study introduces an optimized Swin-Transformer method for the classification and detection of multiple chicken parts. It initially leverages the Transformers self-attention structure to capture more comprehensive high-level visual semantic information from chicken part images. The image enhancement technique was applied to the image in the preprocessing stage to enhance the feature information of the image, and the migration learning method was used to train and optimize the Swin-Transformer model on the enhanced chicken parts dataset for classification and detection of chicken parts. Furthermore, this model was compared to four commonly used models in object target detection tasks: YOLOV3-Darknet53, YOLOV3-MobileNetv3, SSD-MobileNetv3, and SSD-VGG16. The results indicated that the Swin-Transformer model outperforms these models with a higher mAP value by 1.62%, 2.13%, 5.26%, and 4.48%, accompanied by a reduction in detection time by 16.18ms, 5.08ms, 9.38ms, and 23.48ms, respectively. The method of this study fulfills the production line requirements while exhibiting superior performance and greater robustness compared to existing conventional methods. GRAPHICAL ABSTRACT",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/23ce9c2814d6567efec884b7043977cefcb7602e.pdf",
        "venue": "CyTA - Journal of Food",
        "citationCount": 3,
        "score": 3.0,
        "summary": "ABSTRACT In order to achieve real-time classification and detection of various chicken parts, this study introduces an optimized Swin-Transformer method for the classification and detection of multiple chicken parts. It initially leverages the Transformers self-attention structure to capture more comprehensive high-level visual semantic information from chicken part images. The image enhancement technique was applied to the image in the preprocessing stage to enhance the feature information of the image, and the migration learning method was used to train and optimize the Swin-Transformer model on the enhanced chicken parts dataset for classification and detection of chicken parts. Furthermore, this model was compared to four commonly used models in object target detection tasks: YOLOV3-Darknet53, YOLOV3-MobileNetv3, SSD-MobileNetv3, and SSD-VGG16. The results indicated that the Swin-Transformer model outperforms these models with a higher mAP value by 1.62%, 2.13%, 5.26%, and 4.48%, accompanied by a reduction in detection time by 16.18ms, 5.08ms, 9.38ms, and 23.48ms, respectively. The method of this study fulfills the production line requirements while exhibiting superior performance and greater robustness compared to existing conventional methods. GRAPHICAL ABSTRACT",
        "keywords": []
      },
      "file_name": "23ce9c2814d6567efec884b7043977cefcb7602e.pdf"
    },
    {
      "success": true,
      "doc_id": "660685e9c8e845c27ec22a7871fc4454",
      "summary": "The development of autonomous vehicles has grown significantly recently due to the promise of improving safety and productivity in cities and industries. The scene perception module has benefited from the latest advances in computer vision and deep learning techniques, allowing the creation of more accurate and efficient models. This study develops and evaluates semantic segmentation models based on a bilateral architecture to enhance the detection of traversable areas for autonomous vehicles on unstructured routes, particularly in datasets where the distinction between the traversable area and the surrounding ground is minimal. The proposed hybrid models combine Convolutional Neural Networks (CNNs), Vision Transformer (ViT), and Multilayer Perceptron (MLP) techniques, achieving a balance between precision and computational efficiency. The results demonstrate that these models outperform the base architectures in prediction accuracy, capturing distant details more effectively while maintaining real-time operational capabilities.",
      "intriguing_abstract": "The development of autonomous vehicles has grown significantly recently due to the promise of improving safety and productivity in cities and industries. The scene perception module has benefited from the latest advances in computer vision and deep learning techniques, allowing the creation of more accurate and efficient models. This study develops and evaluates semantic segmentation models based on a bilateral architecture to enhance the detection of traversable areas for autonomous vehicles on unstructured routes, particularly in datasets where the distinction between the traversable area and the surrounding ground is minimal. The proposed hybrid models combine Convolutional Neural Networks (CNNs), Vision Transformer (ViT), and Multilayer Perceptron (MLP) techniques, achieving a balance between precision and computational efficiency. The results demonstrate that these models outperform the base architectures in prediction accuracy, capturing distant details more effectively while maintaining real-time operational capabilities.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/5b22bdc6aedf13d812509dd0f768353eb1469a79.pdf",
      "citation_key": "urrea20245k4",
      "metadata": {
        "title": "Enhancing Autonomous Visual Perception in Challenging Environments: Bilateral Models with Vision Transformer and Multilayer Perceptron for Traversable Area Detection",
        "authors": [
          "Claudio Urrea",
          "Maximiliano Vlez"
        ],
        "published_date": "2024",
        "abstract": "The development of autonomous vehicles has grown significantly recently due to the promise of improving safety and productivity in cities and industries. The scene perception module has benefited from the latest advances in computer vision and deep learning techniques, allowing the creation of more accurate and efficient models. This study develops and evaluates semantic segmentation models based on a bilateral architecture to enhance the detection of traversable areas for autonomous vehicles on unstructured routes, particularly in datasets where the distinction between the traversable area and the surrounding ground is minimal. The proposed hybrid models combine Convolutional Neural Networks (CNNs), Vision Transformer (ViT), and Multilayer Perceptron (MLP) techniques, achieving a balance between precision and computational efficiency. The results demonstrate that these models outperform the base architectures in prediction accuracy, capturing distant details more effectively while maintaining real-time operational capabilities.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/5b22bdc6aedf13d812509dd0f768353eb1469a79.pdf",
        "venue": "Technologies",
        "citationCount": 3,
        "score": 3.0,
        "summary": "The development of autonomous vehicles has grown significantly recently due to the promise of improving safety and productivity in cities and industries. The scene perception module has benefited from the latest advances in computer vision and deep learning techniques, allowing the creation of more accurate and efficient models. This study develops and evaluates semantic segmentation models based on a bilateral architecture to enhance the detection of traversable areas for autonomous vehicles on unstructured routes, particularly in datasets where the distinction between the traversable area and the surrounding ground is minimal. The proposed hybrid models combine Convolutional Neural Networks (CNNs), Vision Transformer (ViT), and Multilayer Perceptron (MLP) techniques, achieving a balance between precision and computational efficiency. The results demonstrate that these models outperform the base architectures in prediction accuracy, capturing distant details more effectively while maintaining real-time operational capabilities.",
        "keywords": []
      },
      "file_name": "5b22bdc6aedf13d812509dd0f768353eb1469a79.pdf"
    },
    {
      "success": true,
      "doc_id": "1cc5141d80a82d234c5a975e29d737e6",
      "summary": "This research introduces BAE-ViT, a specialized vision transformer model developed for bone age estimation (BAE). This model is designed to efficiently merge image and sex data, a capability not present in traditional convolutional neural networks (CNNs). BAE-ViT employs a novel data fusion method to facilitate detailed interactions between visual and non-visual data by tokenizing non-visual information and concatenating all tokens (visual or non-visual) as the input to the model. The model underwent training on a large-scale dataset from the 2017 RSNA Pediatric Bone Age Machine Learning Challenge, where it exhibited commendable performance, particularly excelling in handling image distortions compared to existing models. The effectiveness of BAE-ViT was further affirmed through statistical analysis, demonstrating a strong correlation with the actual ground-truth labels. This study contributes to the field by showcasing the potential of vision transformers as a viable option for integrating multimodal data in medical imaging applications, specifically emphasizing their capacity to incorporate non-visual elements like sex information into the framework. This tokenization method not only demonstrates superior performance in this specific task but also offers a versatile framework for integrating multimodal data in medical imaging applications.",
      "intriguing_abstract": "This research introduces BAE-ViT, a specialized vision transformer model developed for bone age estimation (BAE). This model is designed to efficiently merge image and sex data, a capability not present in traditional convolutional neural networks (CNNs). BAE-ViT employs a novel data fusion method to facilitate detailed interactions between visual and non-visual data by tokenizing non-visual information and concatenating all tokens (visual or non-visual) as the input to the model. The model underwent training on a large-scale dataset from the 2017 RSNA Pediatric Bone Age Machine Learning Challenge, where it exhibited commendable performance, particularly excelling in handling image distortions compared to existing models. The effectiveness of BAE-ViT was further affirmed through statistical analysis, demonstrating a strong correlation with the actual ground-truth labels. This study contributes to the field by showcasing the potential of vision transformers as a viable option for integrating multimodal data in medical imaging applications, specifically emphasizing their capacity to incorporate non-visual elements like sex information into the framework. This tokenization method not only demonstrates superior performance in this specific task but also offers a versatile framework for integrating multimodal data in medical imaging applications.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/d2fd0dc314023ebca1342f1379ba6d79c7ded84a.pdf",
      "citation_key": "zhang2024b7v",
      "metadata": {
        "title": "BAE-ViT: An Efficient Multimodal Vision Transformer for Bone Age Estimation",
        "authors": [
          "Jinnian Zhang",
          "Weijie Chen",
          "Tanmayee Joshi",
          "Xiaomin Zhang",
          "Po-Ling Loh",
          "Varun Jog",
          "Richard J. Bruce",
          "John W. Garrett",
          "Alan B McMillan"
        ],
        "published_date": "2024",
        "abstract": "This research introduces BAE-ViT, a specialized vision transformer model developed for bone age estimation (BAE). This model is designed to efficiently merge image and sex data, a capability not present in traditional convolutional neural networks (CNNs). BAE-ViT employs a novel data fusion method to facilitate detailed interactions between visual and non-visual data by tokenizing non-visual information and concatenating all tokens (visual or non-visual) as the input to the model. The model underwent training on a large-scale dataset from the 2017 RSNA Pediatric Bone Age Machine Learning Challenge, where it exhibited commendable performance, particularly excelling in handling image distortions compared to existing models. The effectiveness of BAE-ViT was further affirmed through statistical analysis, demonstrating a strong correlation with the actual ground-truth labels. This study contributes to the field by showcasing the potential of vision transformers as a viable option for integrating multimodal data in medical imaging applications, specifically emphasizing their capacity to incorporate non-visual elements like sex information into the framework. This tokenization method not only demonstrates superior performance in this specific task but also offers a versatile framework for integrating multimodal data in medical imaging applications.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d2fd0dc314023ebca1342f1379ba6d79c7ded84a.pdf",
        "venue": "Tomography",
        "citationCount": 3,
        "score": 3.0,
        "summary": "This research introduces BAE-ViT, a specialized vision transformer model developed for bone age estimation (BAE). This model is designed to efficiently merge image and sex data, a capability not present in traditional convolutional neural networks (CNNs). BAE-ViT employs a novel data fusion method to facilitate detailed interactions between visual and non-visual data by tokenizing non-visual information and concatenating all tokens (visual or non-visual) as the input to the model. The model underwent training on a large-scale dataset from the 2017 RSNA Pediatric Bone Age Machine Learning Challenge, where it exhibited commendable performance, particularly excelling in handling image distortions compared to existing models. The effectiveness of BAE-ViT was further affirmed through statistical analysis, demonstrating a strong correlation with the actual ground-truth labels. This study contributes to the field by showcasing the potential of vision transformers as a viable option for integrating multimodal data in medical imaging applications, specifically emphasizing their capacity to incorporate non-visual elements like sex information into the framework. This tokenization method not only demonstrates superior performance in this specific task but also offers a versatile framework for integrating multimodal data in medical imaging applications.",
        "keywords": []
      },
      "file_name": "d2fd0dc314023ebca1342f1379ba6d79c7ded84a.pdf"
    },
    {
      "success": true,
      "doc_id": "22a98534db7320dbbd085039911cd53b",
      "summary": "Operational weather forecasting system relies on computationally expensive physics-based models. Recently, transformer based models have shown remarkable potential in weather forecasting achieving state-of-the-art results. However, transformers are discrete and physics-agnostic models which limit their ability to learn the continuous spatio-temporal features of the dynamical weather system. We address this issue with STC-ViT, a Spatio-Temporal Continuous Vision Transformer for weather forecasting. STC-ViT incorporates the continuous time Neural ODE layers with multi-head attention mechanism to learn the continuous weather evolution over time. The attention mechanism is encoded as a differentiable function in the transformer architecture to model the complex weather dynamics. Further, we define a customised physics informed loss for STC-ViT which penalize the model's predictions for deviating away from physical laws. We evaluate STC-ViT against operational Numerical Weather Prediction (NWP) model and several deep learning based weather forecasting models. STC-ViT, trained on 1.5-degree 6-hourly data, demonstrates computational efficiency and competitive performance compared to state-of-the-art data-driven models trained on higher-resolution data for global forecasting.",
      "intriguing_abstract": "Operational weather forecasting system relies on computationally expensive physics-based models. Recently, transformer based models have shown remarkable potential in weather forecasting achieving state-of-the-art results. However, transformers are discrete and physics-agnostic models which limit their ability to learn the continuous spatio-temporal features of the dynamical weather system. We address this issue with STC-ViT, a Spatio-Temporal Continuous Vision Transformer for weather forecasting. STC-ViT incorporates the continuous time Neural ODE layers with multi-head attention mechanism to learn the continuous weather evolution over time. The attention mechanism is encoded as a differentiable function in the transformer architecture to model the complex weather dynamics. Further, we define a customised physics informed loss for STC-ViT which penalize the model's predictions for deviating away from physical laws. We evaluate STC-ViT against operational Numerical Weather Prediction (NWP) model and several deep learning based weather forecasting models. STC-ViT, trained on 1.5-degree 6-hourly data, demonstrates computational efficiency and competitive performance compared to state-of-the-art data-driven models trained on higher-resolution data for global forecasting.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/1c96dc4ad4dfa3bf7c6ca9cf0c6197e7ac00272b.pdf",
      "citation_key": "saleem20249yl",
      "metadata": {
        "title": "STC-ViT: Spatio Temporal Continuous Vision Transformer for Weather Forecasting",
        "authors": [
          "Hira Saleem",
          "Flora Salim",
          "Cormac Purcell"
        ],
        "published_date": "2024",
        "abstract": "Operational weather forecasting system relies on computationally expensive physics-based models. Recently, transformer based models have shown remarkable potential in weather forecasting achieving state-of-the-art results. However, transformers are discrete and physics-agnostic models which limit their ability to learn the continuous spatio-temporal features of the dynamical weather system. We address this issue with STC-ViT, a Spatio-Temporal Continuous Vision Transformer for weather forecasting. STC-ViT incorporates the continuous time Neural ODE layers with multi-head attention mechanism to learn the continuous weather evolution over time. The attention mechanism is encoded as a differentiable function in the transformer architecture to model the complex weather dynamics. Further, we define a customised physics informed loss for STC-ViT which penalize the model's predictions for deviating away from physical laws. We evaluate STC-ViT against operational Numerical Weather Prediction (NWP) model and several deep learning based weather forecasting models. STC-ViT, trained on 1.5-degree 6-hourly data, demonstrates computational efficiency and competitive performance compared to state-of-the-art data-driven models trained on higher-resolution data for global forecasting.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1c96dc4ad4dfa3bf7c6ca9cf0c6197e7ac00272b.pdf",
        "venue": "",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Operational weather forecasting system relies on computationally expensive physics-based models. Recently, transformer based models have shown remarkable potential in weather forecasting achieving state-of-the-art results. However, transformers are discrete and physics-agnostic models which limit their ability to learn the continuous spatio-temporal features of the dynamical weather system. We address this issue with STC-ViT, a Spatio-Temporal Continuous Vision Transformer for weather forecasting. STC-ViT incorporates the continuous time Neural ODE layers with multi-head attention mechanism to learn the continuous weather evolution over time. The attention mechanism is encoded as a differentiable function in the transformer architecture to model the complex weather dynamics. Further, we define a customised physics informed loss for STC-ViT which penalize the model's predictions for deviating away from physical laws. We evaluate STC-ViT against operational Numerical Weather Prediction (NWP) model and several deep learning based weather forecasting models. STC-ViT, trained on 1.5-degree 6-hourly data, demonstrates computational efficiency and competitive performance compared to state-of-the-art data-driven models trained on higher-resolution data for global forecasting.",
        "keywords": []
      },
      "file_name": "1c96dc4ad4dfa3bf7c6ca9cf0c6197e7ac00272b.pdf"
    },
    {
      "success": true,
      "doc_id": "1200e3e3bc9c602fc1571baf3ebcbf41",
      "summary": "Perception plays a vital role in autonomous driving as it serves as a prerequisite for downstream planning and decision tasks. Existing research has mainly focused on developing vehicle-side perception models using a single type of sensors. However, relying solely on one type of on-board sensors to perceive the surrounding environment leads to perceptual deficiencies owing to inherent characteristics and sensor sparsity. To address this bottleneck, we propose ViT-FuseNet, a novel vehicle-infrastructure cooperative perception framework that utilizes a Vision Transformer to fuse feature maps extracted from LiDAR and camera data. The key component is a multimodal fusion module designed based on a cross-attention mechanism. ViT-FuseNet has two distinct advantages: i) it incorporates roadside LiDAR point clouds as additional inputs to enhance the 3D object detection capability of the vehicle; and ii) for the effective fusion of data from two different modal sensors, we employ a cross-attention mechanism for feature fusion, rather than directly merging camera features with point clouds at the raw data level. Extensive experiments are conducted using the DAIR-V2X Dataset to demonstrate the effectiveness of the proposed method. Compared with advanced cooperative perception methods, our method achieves a 6.17% improvement in 3D-mAP (IoU=0.5) and an 8.72% improvement in 3D-mAP (IoU=0.7). Moreover, the framework achieves the highest 3D-mAP (IoU=0.5) in all three object categories of benchmarks for single-vehicle perception.",
      "intriguing_abstract": "Perception plays a vital role in autonomous driving as it serves as a prerequisite for downstream planning and decision tasks. Existing research has mainly focused on developing vehicle-side perception models using a single type of sensors. However, relying solely on one type of on-board sensors to perceive the surrounding environment leads to perceptual deficiencies owing to inherent characteristics and sensor sparsity. To address this bottleneck, we propose ViT-FuseNet, a novel vehicle-infrastructure cooperative perception framework that utilizes a Vision Transformer to fuse feature maps extracted from LiDAR and camera data. The key component is a multimodal fusion module designed based on a cross-attention mechanism. ViT-FuseNet has two distinct advantages: i) it incorporates roadside LiDAR point clouds as additional inputs to enhance the 3D object detection capability of the vehicle; and ii) for the effective fusion of data from two different modal sensors, we employ a cross-attention mechanism for feature fusion, rather than directly merging camera features with point clouds at the raw data level. Extensive experiments are conducted using the DAIR-V2X Dataset to demonstrate the effectiveness of the proposed method. Compared with advanced cooperative perception methods, our method achieves a 6.17% improvement in 3D-mAP (IoU=0.5) and an 8.72% improvement in 3D-mAP (IoU=0.7). Moreover, the framework achieves the highest 3D-mAP (IoU=0.5) in all three object categories of benchmarks for single-vehicle perception.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/c35c316feba84334a4b08feb5281a5f83db0b8b2.pdf",
      "citation_key": "zhou2024toe",
      "metadata": {
        "title": "ViT-FuseNet: Multimodal Fusion of Vision Transformer for Vehicle-Infrastructure Cooperative Perception",
        "authors": [
          "Yang Zhou",
          "Cai Yang",
          "Ping Wang",
          "Chao Wang",
          "Xinhong Wang",
          "Nguyen Ngoc Van"
        ],
        "published_date": "2024",
        "abstract": "Perception plays a vital role in autonomous driving as it serves as a prerequisite for downstream planning and decision tasks. Existing research has mainly focused on developing vehicle-side perception models using a single type of sensors. However, relying solely on one type of on-board sensors to perceive the surrounding environment leads to perceptual deficiencies owing to inherent characteristics and sensor sparsity. To address this bottleneck, we propose ViT-FuseNet, a novel vehicle-infrastructure cooperative perception framework that utilizes a Vision Transformer to fuse feature maps extracted from LiDAR and camera data. The key component is a multimodal fusion module designed based on a cross-attention mechanism. ViT-FuseNet has two distinct advantages: i) it incorporates roadside LiDAR point clouds as additional inputs to enhance the 3D object detection capability of the vehicle; and ii) for the effective fusion of data from two different modal sensors, we employ a cross-attention mechanism for feature fusion, rather than directly merging camera features with point clouds at the raw data level. Extensive experiments are conducted using the DAIR-V2X Dataset to demonstrate the effectiveness of the proposed method. Compared with advanced cooperative perception methods, our method achieves a 6.17% improvement in 3D-mAP (IoU=0.5) and an 8.72% improvement in 3D-mAP (IoU=0.7). Moreover, the framework achieves the highest 3D-mAP (IoU=0.5) in all three object categories of benchmarks for single-vehicle perception.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c35c316feba84334a4b08feb5281a5f83db0b8b2.pdf",
        "venue": "IEEE Access",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Perception plays a vital role in autonomous driving as it serves as a prerequisite for downstream planning and decision tasks. Existing research has mainly focused on developing vehicle-side perception models using a single type of sensors. However, relying solely on one type of on-board sensors to perceive the surrounding environment leads to perceptual deficiencies owing to inherent characteristics and sensor sparsity. To address this bottleneck, we propose ViT-FuseNet, a novel vehicle-infrastructure cooperative perception framework that utilizes a Vision Transformer to fuse feature maps extracted from LiDAR and camera data. The key component is a multimodal fusion module designed based on a cross-attention mechanism. ViT-FuseNet has two distinct advantages: i) it incorporates roadside LiDAR point clouds as additional inputs to enhance the 3D object detection capability of the vehicle; and ii) for the effective fusion of data from two different modal sensors, we employ a cross-attention mechanism for feature fusion, rather than directly merging camera features with point clouds at the raw data level. Extensive experiments are conducted using the DAIR-V2X Dataset to demonstrate the effectiveness of the proposed method. Compared with advanced cooperative perception methods, our method achieves a 6.17% improvement in 3D-mAP (IoU=0.5) and an 8.72% improvement in 3D-mAP (IoU=0.7). Moreover, the framework achieves the highest 3D-mAP (IoU=0.5) in all three object categories of benchmarks for single-vehicle perception.",
        "keywords": []
      },
      "file_name": "c35c316feba84334a4b08feb5281a5f83db0b8b2.pdf"
    },
    {
      "success": true,
      "doc_id": "aefd0d87ef9420267df8be58f4c3f540",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/c8174af99bc92d96935683beccc4161c65a8aa46.pdf",
      "citation_key": "lijin2024mhk",
      "metadata": {
        "title": "PolySegNet: improving polyp segmentation through swin transformer and vision transformer fusion.",
        "authors": [
          "P. Lijin",
          "M. Ullah",
          "Anuja Vats",
          "F. A. Cheikh",
          "G. Santhosh Kumar",
          "Madhu S. Nair"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c8174af99bc92d96935683beccc4161c65a8aa46.pdf",
        "venue": "Biomedical Engineering Letters",
        "citationCount": 3,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "c8174af99bc92d96935683beccc4161c65a8aa46.pdf"
    },
    {
      "success": true,
      "doc_id": "00ed1a7122db3ee70adfaa2c828a318b",
      "summary": "Background Disease diagnosis in chest X-ray images has predominantly relied on convolutional neural networks (CNNs). However, Vision Transformer (ViT) offers several advantages over CNNs, as it excels at capturing long-term dependencies, exploring correlations, and extracting features with richer semantic information. Methods We adapted ViT for chest X-ray image analysis by making the following three key improvements: (I) employing a sliding window approach in the image sequence feature extraction module to divide the input image into blocks to identify small and difficult-to-detect lesion areas; (II) introducing an attention region selection module in the encoder layer of the ViT model to enhance the models ability to focus on relevant regions; and (III) constructing a parallel patient metadata feature extraction network on top of the image feature extraction network to integrate multi-modal input data, enabling the model to synergistically learn and expand image-semantic information. Results The experimental results showed the effectiveness of our proposed model, which had an average area under the curve value of 0.831 in diagnosing 14 common chest diseases. The metadata feature network module effectively integrated patient metadata, further enhancing the models accuracy in diagnosis. Our ViT-based model had a sensitivity of 0.863, a specificity of 0.821, and an accuracy of 0.834 in diagnosing these common chest diseases. Conclusions Our model has good general applicability and shows promise in chest X-ray image analysis, effectively integrating patient metadata and enhancing diagnostic capabilities.",
      "intriguing_abstract": "Background Disease diagnosis in chest X-ray images has predominantly relied on convolutional neural networks (CNNs). However, Vision Transformer (ViT) offers several advantages over CNNs, as it excels at capturing long-term dependencies, exploring correlations, and extracting features with richer semantic information. Methods We adapted ViT for chest X-ray image analysis by making the following three key improvements: (I) employing a sliding window approach in the image sequence feature extraction module to divide the input image into blocks to identify small and difficult-to-detect lesion areas; (II) introducing an attention region selection module in the encoder layer of the ViT model to enhance the models ability to focus on relevant regions; and (III) constructing a parallel patient metadata feature extraction network on top of the image feature extraction network to integrate multi-modal input data, enabling the model to synergistically learn and expand image-semantic information. Results The experimental results showed the effectiveness of our proposed model, which had an average area under the curve value of 0.831 in diagnosing 14 common chest diseases. The metadata feature network module effectively integrated patient metadata, further enhancing the models accuracy in diagnosis. Our ViT-based model had a sensitivity of 0.863, a specificity of 0.821, and an accuracy of 0.834 in diagnosing these common chest diseases. Conclusions Our model has good general applicability and shows promise in chest X-ray image analysis, effectively integrating patient metadata and enhancing diagnostic capabilities.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/05548c4b3db8be40fac236dfa8e21882fe9ce9de.pdf",
      "citation_key": "huang2024htf",
      "metadata": {
        "title": "Research and implementation of multi-disease diagnosis on chest X-ray based on vision transformer",
        "authors": [
          "Lan Huang",
          "Jiong Ma",
          "Hui Yang",
          "Yan Wang"
        ],
        "published_date": "2024",
        "abstract": "Background Disease diagnosis in chest X-ray images has predominantly relied on convolutional neural networks (CNNs). However, Vision Transformer (ViT) offers several advantages over CNNs, as it excels at capturing long-term dependencies, exploring correlations, and extracting features with richer semantic information. Methods We adapted ViT for chest X-ray image analysis by making the following three key improvements: (I) employing a sliding window approach in the image sequence feature extraction module to divide the input image into blocks to identify small and difficult-to-detect lesion areas; (II) introducing an attention region selection module in the encoder layer of the ViT model to enhance the models ability to focus on relevant regions; and (III) constructing a parallel patient metadata feature extraction network on top of the image feature extraction network to integrate multi-modal input data, enabling the model to synergistically learn and expand image-semantic information. Results The experimental results showed the effectiveness of our proposed model, which had an average area under the curve value of 0.831 in diagnosing 14 common chest diseases. The metadata feature network module effectively integrated patient metadata, further enhancing the models accuracy in diagnosis. Our ViT-based model had a sensitivity of 0.863, a specificity of 0.821, and an accuracy of 0.834 in diagnosing these common chest diseases. Conclusions Our model has good general applicability and shows promise in chest X-ray image analysis, effectively integrating patient metadata and enhancing diagnostic capabilities.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/05548c4b3db8be40fac236dfa8e21882fe9ce9de.pdf",
        "venue": "Quantitative Imaging in Medicine and Surgery",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Background Disease diagnosis in chest X-ray images has predominantly relied on convolutional neural networks (CNNs). However, Vision Transformer (ViT) offers several advantages over CNNs, as it excels at capturing long-term dependencies, exploring correlations, and extracting features with richer semantic information. Methods We adapted ViT for chest X-ray image analysis by making the following three key improvements: (I) employing a sliding window approach in the image sequence feature extraction module to divide the input image into blocks to identify small and difficult-to-detect lesion areas; (II) introducing an attention region selection module in the encoder layer of the ViT model to enhance the models ability to focus on relevant regions; and (III) constructing a parallel patient metadata feature extraction network on top of the image feature extraction network to integrate multi-modal input data, enabling the model to synergistically learn and expand image-semantic information. Results The experimental results showed the effectiveness of our proposed model, which had an average area under the curve value of 0.831 in diagnosing 14 common chest diseases. The metadata feature network module effectively integrated patient metadata, further enhancing the models accuracy in diagnosis. Our ViT-based model had a sensitivity of 0.863, a specificity of 0.821, and an accuracy of 0.834 in diagnosing these common chest diseases. Conclusions Our model has good general applicability and shows promise in chest X-ray image analysis, effectively integrating patient metadata and enhancing diagnostic capabilities.",
        "keywords": []
      },
      "file_name": "05548c4b3db8be40fac236dfa8e21882fe9ce9de.pdf"
    },
    {
      "success": true,
      "doc_id": "b36c7ec0617f05c0501b472ec4166d78",
      "summary": "BACKGROUND\nLymph node metastasis (LNM) plays a crucial role in the management of lung cancer; however, the ability of chest computed tomography (CT) imaging to detect LNM status is limited.\n\n\nPURPOSE\nThis study aimed to develop and validate a vision transformer-based deep transfer learning nomogram for predicting LNM in lung adenocarcinoma patients using preoperative unenhanced chest CT imaging.\n\n\nMETHODS\nThis study included 528 patients with lung adenocarcinoma who were randomly divided into training and validation cohorts at a 7:3 ratio. The pretrained vision transformer (ViT) was utilized to extract deep transfer learning (DTL) feature, and logistic regression was employed to construct a ViT-based DTL model. Subsequently, the model was compared with six classical convolutional neural network (CNN) models. Finally, the ViT-based DTL signature was combined with independent clinical predictors to construct a ViT-based deep transfer learning nomogram (DTLN).\n\n\nRESULTS\nThe ViT-based DTL model showed good performance, with an area under the curve (AUC) of 0.821 (95% CI, 0.775-0.867) in the training cohort and 0.825 (95% CI, 0.758-0.891) in the validation cohort. The ViT-based DTL model demonstrated comparable performance to classical CNN models in predicting LNM, and the ViT-based DTL signature was then used to construct ViT-based DTLN with independent clinical predictors such as tumor maximum diameter, location, and density. The DTLN achieved the best predictive performance, with AUCs of 0.865 (95% CI, 0.827-0.903) and 0.894 (95% CI, 0845-0942), respectively, surpassing both the clinical factor model and the ViT-based DTL model (p<0.001).\n\n\nCONCLUSION\nThis study developed a new DTL model based on ViT to predict LNM status in lung adenocarcinoma patients and revealed that the performance of the ViT-based DTL model was comparable to that of classical CNN models, confirming that ViT was viable for deep learning tasks involving medical images. The ViT-based DTLN performed exceptionally well and can assist clinicians and radiologists in making accurate judgments and formulating appropriate treatment plans.",
      "intriguing_abstract": "BACKGROUND\nLymph node metastasis (LNM) plays a crucial role in the management of lung cancer; however, the ability of chest computed tomography (CT) imaging to detect LNM status is limited.\n\n\nPURPOSE\nThis study aimed to develop and validate a vision transformer-based deep transfer learning nomogram for predicting LNM in lung adenocarcinoma patients using preoperative unenhanced chest CT imaging.\n\n\nMETHODS\nThis study included 528 patients with lung adenocarcinoma who were randomly divided into training and validation cohorts at a 7:3 ratio. The pretrained vision transformer (ViT) was utilized to extract deep transfer learning (DTL) feature, and logistic regression was employed to construct a ViT-based DTL model. Subsequently, the model was compared with six classical convolutional neural network (CNN) models. Finally, the ViT-based DTL signature was combined with independent clinical predictors to construct a ViT-based deep transfer learning nomogram (DTLN).\n\n\nRESULTS\nThe ViT-based DTL model showed good performance, with an area under the curve (AUC) of 0.821 (95% CI, 0.775-0.867) in the training cohort and 0.825 (95% CI, 0.758-0.891) in the validation cohort. The ViT-based DTL model demonstrated comparable performance to classical CNN models in predicting LNM, and the ViT-based DTL signature was then used to construct ViT-based DTLN with independent clinical predictors such as tumor maximum diameter, location, and density. The DTLN achieved the best predictive performance, with AUCs of 0.865 (95% CI, 0.827-0.903) and 0.894 (95% CI, 0845-0942), respectively, surpassing both the clinical factor model and the ViT-based DTL model (p<0.001).\n\n\nCONCLUSION\nThis study developed a new DTL model based on ViT to predict LNM status in lung adenocarcinoma patients and revealed that the performance of the ViT-based DTL model was comparable to that of classical CNN models, confirming that ViT was viable for deep learning tasks involving medical images. The ViT-based DTLN performed exceptionally well and can assist clinicians and radiologists in making accurate judgments and formulating appropriate treatment plans.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/bd7a4a9e020b594ebcacb886ddae8731a2885209.pdf",
      "citation_key": "chen2024cha",
      "metadata": {
        "title": "A vision transformer-based deep transfer learning nomogram for predicting lymph node metastasis in lung adenocarcinoma.",
        "authors": [
          "Chuanyu Chen",
          "Yi Luo",
          "Qiuyang Hou",
          "Jun Qiu",
          "Shuya Yuan",
          "Kexue Deng"
        ],
        "published_date": "2024",
        "abstract": "BACKGROUND\nLymph node metastasis (LNM) plays a crucial role in the management of lung cancer; however, the ability of chest computed tomography (CT) imaging to detect LNM status is limited.\n\n\nPURPOSE\nThis study aimed to develop and validate a vision transformer-based deep transfer learning nomogram for predicting LNM in lung adenocarcinoma patients using preoperative unenhanced chest CT imaging.\n\n\nMETHODS\nThis study included 528 patients with lung adenocarcinoma who were randomly divided into training and validation cohorts at a 7:3 ratio. The pretrained vision transformer (ViT) was utilized to extract deep transfer learning (DTL) feature, and logistic regression was employed to construct a ViT-based DTL model. Subsequently, the model was compared with six classical convolutional neural network (CNN) models. Finally, the ViT-based DTL signature was combined with independent clinical predictors to construct a ViT-based deep transfer learning nomogram (DTLN).\n\n\nRESULTS\nThe ViT-based DTL model showed good performance, with an area under the curve (AUC) of 0.821 (95% CI, 0.775-0.867) in the training cohort and 0.825 (95% CI, 0.758-0.891) in the validation cohort. The ViT-based DTL model demonstrated comparable performance to classical CNN models in predicting LNM, and the ViT-based DTL signature was then used to construct ViT-based DTLN with independent clinical predictors such as tumor maximum diameter, location, and density. The DTLN achieved the best predictive performance, with AUCs of 0.865 (95% CI, 0.827-0.903) and 0.894 (95% CI, 0845-0942), respectively, surpassing both the clinical factor model and the ViT-based DTL model (p<0.001).\n\n\nCONCLUSION\nThis study developed a new DTL model based on ViT to predict LNM status in lung adenocarcinoma patients and revealed that the performance of the ViT-based DTL model was comparable to that of classical CNN models, confirming that ViT was viable for deep learning tasks involving medical images. The ViT-based DTLN performed exceptionally well and can assist clinicians and radiologists in making accurate judgments and formulating appropriate treatment plans.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/bd7a4a9e020b594ebcacb886ddae8731a2885209.pdf",
        "venue": "Medical Physics (Lancaster)",
        "citationCount": 3,
        "score": 3.0,
        "summary": "BACKGROUND\nLymph node metastasis (LNM) plays a crucial role in the management of lung cancer; however, the ability of chest computed tomography (CT) imaging to detect LNM status is limited.\n\n\nPURPOSE\nThis study aimed to develop and validate a vision transformer-based deep transfer learning nomogram for predicting LNM in lung adenocarcinoma patients using preoperative unenhanced chest CT imaging.\n\n\nMETHODS\nThis study included 528 patients with lung adenocarcinoma who were randomly divided into training and validation cohorts at a 7:3 ratio. The pretrained vision transformer (ViT) was utilized to extract deep transfer learning (DTL) feature, and logistic regression was employed to construct a ViT-based DTL model. Subsequently, the model was compared with six classical convolutional neural network (CNN) models. Finally, the ViT-based DTL signature was combined with independent clinical predictors to construct a ViT-based deep transfer learning nomogram (DTLN).\n\n\nRESULTS\nThe ViT-based DTL model showed good performance, with an area under the curve (AUC) of 0.821 (95% CI, 0.775-0.867) in the training cohort and 0.825 (95% CI, 0.758-0.891) in the validation cohort. The ViT-based DTL model demonstrated comparable performance to classical CNN models in predicting LNM, and the ViT-based DTL signature was then used to construct ViT-based DTLN with independent clinical predictors such as tumor maximum diameter, location, and density. The DTLN achieved the best predictive performance, with AUCs of 0.865 (95% CI, 0.827-0.903) and 0.894 (95% CI, 0845-0942), respectively, surpassing both the clinical factor model and the ViT-based DTL model (p<0.001).\n\n\nCONCLUSION\nThis study developed a new DTL model based on ViT to predict LNM status in lung adenocarcinoma patients and revealed that the performance of the ViT-based DTL model was comparable to that of classical CNN models, confirming that ViT was viable for deep learning tasks involving medical images. The ViT-based DTLN performed exceptionally well and can assist clinicians and radiologists in making accurate judgments and formulating appropriate treatment plans.",
        "keywords": []
      },
      "file_name": "bd7a4a9e020b594ebcacb886ddae8731a2885209.pdf"
    },
    {
      "success": true,
      "doc_id": "b8c136f2de9cc2f42bf948e827739356",
      "summary": "Generative Adversarial Networks (GANs) have enabled the creation of photo-realistic images from random noise. GAN based technologies however, led to the dissemination of synthetic images, often containing inappropriate and miss leading content, on social media. Detecting such manipulated images is crucial, yet challenging. The issue is compounded by the fact that GAN-generated images can be indistinguishable from authentic ones, rendering traditional forgery detection techniques ineffective. Deepfake images further exacerbate this problem, posing threats to news integrity, legal proceedings, and societal security. To address these challenges, we harness the potential of Vision Transformer (ViT) in conjunction with Convolutional Autoencoders (CAE) to craft innovative Framework for image analysis and deepfake detection. We introduce two distinct models, each offering unique insights into image processing. The proposed models yield excellent accuracy rate of approximately 87%, reaffirming the robustness and consistency of the proposed approach and enhanced performance compared to state of the art.",
      "intriguing_abstract": "Generative Adversarial Networks (GANs) have enabled the creation of photo-realistic images from random noise. GAN based technologies however, led to the dissemination of synthetic images, often containing inappropriate and miss leading content, on social media. Detecting such manipulated images is crucial, yet challenging. The issue is compounded by the fact that GAN-generated images can be indistinguishable from authentic ones, rendering traditional forgery detection techniques ineffective. Deepfake images further exacerbate this problem, posing threats to news integrity, legal proceedings, and societal security. To address these challenges, we harness the potential of Vision Transformer (ViT) in conjunction with Convolutional Autoencoders (CAE) to craft innovative Framework for image analysis and deepfake detection. We introduce two distinct models, each offering unique insights into image processing. The proposed models yield excellent accuracy rate of approximately 87%, reaffirming the robustness and consistency of the proposed approach and enhanced performance compared to state of the art.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/a7df70e86f049a86b1c555f9a399d3540f466be7.pdf",
      "citation_key": "shahin2024o1c",
      "metadata": {
        "title": "A Novel Framework based on a Hybrid Vision Transformer and Deep Neural Network for Deepfake Detection",
        "authors": [
          "Mohammed Shahin",
          "Mohamed Deriche"
        ],
        "published_date": "2024",
        "abstract": "Generative Adversarial Networks (GANs) have enabled the creation of photo-realistic images from random noise. GAN based technologies however, led to the dissemination of synthetic images, often containing inappropriate and miss leading content, on social media. Detecting such manipulated images is crucial, yet challenging. The issue is compounded by the fact that GAN-generated images can be indistinguishable from authentic ones, rendering traditional forgery detection techniques ineffective. Deepfake images further exacerbate this problem, posing threats to news integrity, legal proceedings, and societal security. To address these challenges, we harness the potential of Vision Transformer (ViT) in conjunction with Convolutional Autoencoders (CAE) to craft innovative Framework for image analysis and deepfake detection. We introduce two distinct models, each offering unique insights into image processing. The proposed models yield excellent accuracy rate of approximately 87%, reaffirming the robustness and consistency of the proposed approach and enhanced performance compared to state of the art.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a7df70e86f049a86b1c555f9a399d3540f466be7.pdf",
        "venue": "International Multi-Conference on Systems, Signals & Devices",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Generative Adversarial Networks (GANs) have enabled the creation of photo-realistic images from random noise. GAN based technologies however, led to the dissemination of synthetic images, often containing inappropriate and miss leading content, on social media. Detecting such manipulated images is crucial, yet challenging. The issue is compounded by the fact that GAN-generated images can be indistinguishable from authentic ones, rendering traditional forgery detection techniques ineffective. Deepfake images further exacerbate this problem, posing threats to news integrity, legal proceedings, and societal security. To address these challenges, we harness the potential of Vision Transformer (ViT) in conjunction with Convolutional Autoencoders (CAE) to craft innovative Framework for image analysis and deepfake detection. We introduce two distinct models, each offering unique insights into image processing. The proposed models yield excellent accuracy rate of approximately 87%, reaffirming the robustness and consistency of the proposed approach and enhanced performance compared to state of the art.",
        "keywords": []
      },
      "file_name": "a7df70e86f049a86b1c555f9a399d3540f466be7.pdf"
    },
    {
      "success": true,
      "doc_id": "d7e2c406cfe823404a2b81cce2981737",
      "summary": "Over the past few years, the COVID19 virus has had a significant impact on the physical and mental health of people around the world. Therefore, in order to effectively distinguish COVID19 patients, many deep learning efforts have used chest medical images to detect COVID19. As with model accuracy, interpretability is also important in the work related to human health. This work introduces an interpretable vision transformer that uses the prototype method for the detection of positive patients with COVID19. The model can learn the prototype features of each category based on the structural characteristics of ViT. The predictions of the model are obtained by comparing all the features of the prototype in the designed prototype block. The proposed model was applied to two chest Xray datasets and one chest CT dataset, achieving classification performance of 99.3%, 96.8%, and 98.5% respectively. Moreover, the prototype method can significantly improve the interpretability of the model. The decisions of the model can be interpreted based on prototype parts. In the prototype block, the entire inference process of the model can be shown and the predictions of the model can be demonstrated to be meaningful through the visualization of the prototypefeatures.",
      "intriguing_abstract": "Over the past few years, the COVID19 virus has had a significant impact on the physical and mental health of people around the world. Therefore, in order to effectively distinguish COVID19 patients, many deep learning efforts have used chest medical images to detect COVID19. As with model accuracy, interpretability is also important in the work related to human health. This work introduces an interpretable vision transformer that uses the prototype method for the detection of positive patients with COVID19. The model can learn the prototype features of each category based on the structural characteristics of ViT. The predictions of the model are obtained by comparing all the features of the prototype in the designed prototype block. The proposed model was applied to two chest Xray datasets and one chest CT dataset, achieving classification performance of 99.3%, 96.8%, and 98.5% respectively. Moreover, the prototype method can significantly improve the interpretability of the model. The decisions of the model can be interpreted based on prototype parts. In the prototype block, the entire inference process of the model can be shown and the predictions of the model can be demonstrated to be meaningful through the visualization of the prototypefeatures.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/e4b0c4a1d7ee87ad666214172f329831ade4c25d.pdf",
      "citation_key": "xu2024wux",
      "metadata": {
        "title": "Interpretable vision transformer based on prototype parts for COVID-19 detection",
        "authors": [
          "Yang Xu",
          "Zuqiang Meng"
        ],
        "published_date": "2024",
        "abstract": "Over the past few years, the COVID19 virus has had a significant impact on the physical and mental health of people around the world. Therefore, in order to effectively distinguish COVID19 patients, many deep learning efforts have used chest medical images to detect COVID19. As with model accuracy, interpretability is also important in the work related to human health. This work introduces an interpretable vision transformer that uses the prototype method for the detection of positive patients with COVID19. The model can learn the prototype features of each category based on the structural characteristics of ViT. The predictions of the model are obtained by comparing all the features of the prototype in the designed prototype block. The proposed model was applied to two chest Xray datasets and one chest CT dataset, achieving classification performance of 99.3%, 96.8%, and 98.5% respectively. Moreover, the prototype method can significantly improve the interpretability of the model. The decisions of the model can be interpreted based on prototype parts. In the prototype block, the entire inference process of the model can be shown and the predictions of the model can be demonstrated to be meaningful through the visualization of the prototypefeatures.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e4b0c4a1d7ee87ad666214172f329831ade4c25d.pdf",
        "venue": "IET Image Processing",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Over the past few years, the COVID19 virus has had a significant impact on the physical and mental health of people around the world. Therefore, in order to effectively distinguish COVID19 patients, many deep learning efforts have used chest medical images to detect COVID19. As with model accuracy, interpretability is also important in the work related to human health. This work introduces an interpretable vision transformer that uses the prototype method for the detection of positive patients with COVID19. The model can learn the prototype features of each category based on the structural characteristics of ViT. The predictions of the model are obtained by comparing all the features of the prototype in the designed prototype block. The proposed model was applied to two chest Xray datasets and one chest CT dataset, achieving classification performance of 99.3%, 96.8%, and 98.5% respectively. Moreover, the prototype method can significantly improve the interpretability of the model. The decisions of the model can be interpreted based on prototype parts. In the prototype block, the entire inference process of the model can be shown and the predictions of the model can be demonstrated to be meaningful through the visualization of the prototypefeatures.",
        "keywords": []
      },
      "file_name": "e4b0c4a1d7ee87ad666214172f329831ade4c25d.pdf"
    },
    {
      "success": true,
      "doc_id": "80698081832c4d425eea8f23fce3decb",
      "summary": "Semantic communications provide significant performance gains over traditional communications by transmitting task-relevant semantic features through wireless channels. However, most existing studies rely on end-to-end (E2E) training of neural-type encoders and decoders to ensure effective transmission of these semantic features. To enable semantic communications without relying on E2E training, this article presents a vision transformer (ViT)-based semantic communication system with importance-aware quantization (IAQ) for wireless image transmission. The core idea of the presented system is to leverage the attention scores of a pretrained ViT model to quantify the importance levels of image patches. Based on this idea, our IAQ framework assigns different quantization bits to image patches based on their importance levels. This is achieved by formulating a weighted quantization error minimization problem, where the weight is set to be an increasing function of the attention score. Then, an optimal incremental allocation method and a low-complexity water-filling method are devised to solve the formulated problem. Our framework is further extended for realistic digital communication systems by modifying the bit allocation problem and the corresponding allocation methods based on an equivalent binary symmetric channel (BSC) model. Simulations on single-view image classification, multiview image classification, and single-object detection tasks demonstrate that our IAQ framework outperforms conventional image compression methods under both error-free and realistic communication scenarios.",
      "intriguing_abstract": "Semantic communications provide significant performance gains over traditional communications by transmitting task-relevant semantic features through wireless channels. However, most existing studies rely on end-to-end (E2E) training of neural-type encoders and decoders to ensure effective transmission of these semantic features. To enable semantic communications without relying on E2E training, this article presents a vision transformer (ViT)-based semantic communication system with importance-aware quantization (IAQ) for wireless image transmission. The core idea of the presented system is to leverage the attention scores of a pretrained ViT model to quantify the importance levels of image patches. Based on this idea, our IAQ framework assigns different quantization bits to image patches based on their importance levels. This is achieved by formulating a weighted quantization error minimization problem, where the weight is set to be an increasing function of the attention score. Then, an optimal incremental allocation method and a low-complexity water-filling method are devised to solve the formulated problem. Our framework is further extended for realistic digital communication systems by modifying the bit allocation problem and the corresponding allocation methods based on an equivalent binary symmetric channel (BSC) model. Simulations on single-view image classification, multiview image classification, and single-object detection tasks demonstrate that our IAQ framework outperforms conventional image compression methods under both error-free and realistic communication scenarios.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/6604a900b9a7404a447b2167892a947012a9ffb8.pdf",
      "citation_key": "park2024d7y",
      "metadata": {
        "title": "Vision Transformer-Based Semantic Communications With Importance-Aware Quantization",
        "authors": [
          "Joohyuk Park",
          "Yong-Nam Oh",
          "Yongjune Kim",
          "Yo-Seb Jeon"
        ],
        "published_date": "2024",
        "abstract": "Semantic communications provide significant performance gains over traditional communications by transmitting task-relevant semantic features through wireless channels. However, most existing studies rely on end-to-end (E2E) training of neural-type encoders and decoders to ensure effective transmission of these semantic features. To enable semantic communications without relying on E2E training, this article presents a vision transformer (ViT)-based semantic communication system with importance-aware quantization (IAQ) for wireless image transmission. The core idea of the presented system is to leverage the attention scores of a pretrained ViT model to quantify the importance levels of image patches. Based on this idea, our IAQ framework assigns different quantization bits to image patches based on their importance levels. This is achieved by formulating a weighted quantization error minimization problem, where the weight is set to be an increasing function of the attention score. Then, an optimal incremental allocation method and a low-complexity water-filling method are devised to solve the formulated problem. Our framework is further extended for realistic digital communication systems by modifying the bit allocation problem and the corresponding allocation methods based on an equivalent binary symmetric channel (BSC) model. Simulations on single-view image classification, multiview image classification, and single-object detection tasks demonstrate that our IAQ framework outperforms conventional image compression methods under both error-free and realistic communication scenarios.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6604a900b9a7404a447b2167892a947012a9ffb8.pdf",
        "venue": "IEEE Internet of Things Journal",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Semantic communications provide significant performance gains over traditional communications by transmitting task-relevant semantic features through wireless channels. However, most existing studies rely on end-to-end (E2E) training of neural-type encoders and decoders to ensure effective transmission of these semantic features. To enable semantic communications without relying on E2E training, this article presents a vision transformer (ViT)-based semantic communication system with importance-aware quantization (IAQ) for wireless image transmission. The core idea of the presented system is to leverage the attention scores of a pretrained ViT model to quantify the importance levels of image patches. Based on this idea, our IAQ framework assigns different quantization bits to image patches based on their importance levels. This is achieved by formulating a weighted quantization error minimization problem, where the weight is set to be an increasing function of the attention score. Then, an optimal incremental allocation method and a low-complexity water-filling method are devised to solve the formulated problem. Our framework is further extended for realistic digital communication systems by modifying the bit allocation problem and the corresponding allocation methods based on an equivalent binary symmetric channel (BSC) model. Simulations on single-view image classification, multiview image classification, and single-object detection tasks demonstrate that our IAQ framework outperforms conventional image compression methods under both error-free and realistic communication scenarios.",
        "keywords": []
      },
      "file_name": "6604a900b9a7404a447b2167892a947012a9ffb8.pdf"
    },
    {
      "success": true,
      "doc_id": "4b0e9b39ad3995a4ecfaa0b1d74ee363",
      "summary": "Source camera identification has emerged as a vital solution to unlock incidents involving critical cases like terrorism, violence, and other criminal activities. The ability to trace the origin of an image/video can aid law enforcement agencies in gathering evidence and constructing the timeline of events. Moreover, identifying the owner of a certain device narrows down the area of search in a criminal investigation where smartphone devices are involved. This paper proposes a new pixel-based method for source camera identification, integrating Pixel Difference Convolution (PDC) with a Vision Transformer network (ViT), and named PDC-ViT. While the PDC acts as the backbone for feature extraction by exploiting Angular PDC (APDC) and Radial PDC (RPDC). These techniques enhance the capability to capture subtle variations in pixel information, which are crucial for distinguishing between different source cameras. The second part of the methodology focuses on classification, which is based on a Vision Transformer network. Unlike traditional methods that utilize image patches directly for training the classification network, the proposed approach uniquely inputs PDC features into the Vision Transformer network. To demonstrate the effectiveness of the PDC-ViT approach, it has been assessed on five different datasets, which include various image contents and video scenes. The method has also been compared with state-of-the-art source camera identification methods. Experimental results demonstrate the effectiveness and superiority of the proposed system in terms of accuracy and robustness when compared to its competitors. For example, our proposed PDC-ViT has achieved an accuracy of 94.30%, 84%, 94.22% and 92.29% using the Vision dataset, Daxing dataset, Socrates dataset and QUFVD dataset, respectively.",
      "intriguing_abstract": "Source camera identification has emerged as a vital solution to unlock incidents involving critical cases like terrorism, violence, and other criminal activities. The ability to trace the origin of an image/video can aid law enforcement agencies in gathering evidence and constructing the timeline of events. Moreover, identifying the owner of a certain device narrows down the area of search in a criminal investigation where smartphone devices are involved. This paper proposes a new pixel-based method for source camera identification, integrating Pixel Difference Convolution (PDC) with a Vision Transformer network (ViT), and named PDC-ViT. While the PDC acts as the backbone for feature extraction by exploiting Angular PDC (APDC) and Radial PDC (RPDC). These techniques enhance the capability to capture subtle variations in pixel information, which are crucial for distinguishing between different source cameras. The second part of the methodology focuses on classification, which is based on a Vision Transformer network. Unlike traditional methods that utilize image patches directly for training the classification network, the proposed approach uniquely inputs PDC features into the Vision Transformer network. To demonstrate the effectiveness of the PDC-ViT approach, it has been assessed on five different datasets, which include various image contents and video scenes. The method has also been compared with state-of-the-art source camera identification methods. Experimental results demonstrate the effectiveness and superiority of the proposed system in terms of accuracy and robustness when compared to its competitors. For example, our proposed PDC-ViT has achieved an accuracy of 94.30%, 84%, 94.22% and 92.29% using the Vision dataset, Daxing dataset, Socrates dataset and QUFVD dataset, respectively.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/9814001811c4924171787de52e01cc31446e2f97.pdf",
      "citation_key": "elharrouss20252ng",
      "metadata": {
        "title": "PDC-ViT : Source Camera Identification using Pixel Difference Convolution and Vision Transformer",
        "authors": [
          "O. Elharrouss",
          "Y. Akbari",
          "Noor Almaadeed",
          "S. Al-maadeed",
          "F. Khelifi",
          "Ahmed Bouridane"
        ],
        "published_date": "2025",
        "abstract": "Source camera identification has emerged as a vital solution to unlock incidents involving critical cases like terrorism, violence, and other criminal activities. The ability to trace the origin of an image/video can aid law enforcement agencies in gathering evidence and constructing the timeline of events. Moreover, identifying the owner of a certain device narrows down the area of search in a criminal investigation where smartphone devices are involved. This paper proposes a new pixel-based method for source camera identification, integrating Pixel Difference Convolution (PDC) with a Vision Transformer network (ViT), and named PDC-ViT. While the PDC acts as the backbone for feature extraction by exploiting Angular PDC (APDC) and Radial PDC (RPDC). These techniques enhance the capability to capture subtle variations in pixel information, which are crucial for distinguishing between different source cameras. The second part of the methodology focuses on classification, which is based on a Vision Transformer network. Unlike traditional methods that utilize image patches directly for training the classification network, the proposed approach uniquely inputs PDC features into the Vision Transformer network. To demonstrate the effectiveness of the PDC-ViT approach, it has been assessed on five different datasets, which include various image contents and video scenes. The method has also been compared with state-of-the-art source camera identification methods. Experimental results demonstrate the effectiveness and superiority of the proposed system in terms of accuracy and robustness when compared to its competitors. For example, our proposed PDC-ViT has achieved an accuracy of 94.30%, 84%, 94.22% and 92.29% using the Vision dataset, Daxing dataset, Socrates dataset and QUFVD dataset, respectively.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9814001811c4924171787de52e01cc31446e2f97.pdf",
        "venue": "Neural computing & applications (Print)",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Source camera identification has emerged as a vital solution to unlock incidents involving critical cases like terrorism, violence, and other criminal activities. The ability to trace the origin of an image/video can aid law enforcement agencies in gathering evidence and constructing the timeline of events. Moreover, identifying the owner of a certain device narrows down the area of search in a criminal investigation where smartphone devices are involved. This paper proposes a new pixel-based method for source camera identification, integrating Pixel Difference Convolution (PDC) with a Vision Transformer network (ViT), and named PDC-ViT. While the PDC acts as the backbone for feature extraction by exploiting Angular PDC (APDC) and Radial PDC (RPDC). These techniques enhance the capability to capture subtle variations in pixel information, which are crucial for distinguishing between different source cameras. The second part of the methodology focuses on classification, which is based on a Vision Transformer network. Unlike traditional methods that utilize image patches directly for training the classification network, the proposed approach uniquely inputs PDC features into the Vision Transformer network. To demonstrate the effectiveness of the PDC-ViT approach, it has been assessed on five different datasets, which include various image contents and video scenes. The method has also been compared with state-of-the-art source camera identification methods. Experimental results demonstrate the effectiveness and superiority of the proposed system in terms of accuracy and robustness when compared to its competitors. For example, our proposed PDC-ViT has achieved an accuracy of 94.30%, 84%, 94.22% and 92.29% using the Vision dataset, Daxing dataset, Socrates dataset and QUFVD dataset, respectively.",
        "keywords": []
      },
      "file_name": "9814001811c4924171787de52e01cc31446e2f97.pdf"
    },
    {
      "success": true,
      "doc_id": "f5ebd25a8a3e834803693a67151c7317",
      "summary": "Blind image inpainting, the task of detecting corrupted regions with diverse patterns within an image and then generating plausible content for the corrupted regions, remains a both challenging and practical problem in computer vision. In this paper, we propose a novel model InViT for blind image inpainting, which leverages a combination of a pre-trained Generative Adversarial Network (GAN) and a learnable Vision Transformer (ViT). The proposed InViT mainly consists of two phases, the mask prediction phase and the image inpainting phase. Benefiting from the learned latent feature space from the full training data through GAN inversion, a pre-trained StyleGAN is able to provide reliable cues of corrupted regions for mask prediction. By further incorporating the predicted mask into the image inpainting phase, we design a vision Transformer with the mask-aware self-attention mechanism to capture long-range dependencies between pixels during content reconstruction. Besides, we propose a Prompt-augment Contextual Aggregation module to strengthen the reasonableness of generated content for the corrupted regions. Extensive experiments on several benchmark datasets for blind image inpainting demonstrate that our InViT model achieves state-of-the-art performance compared to existing methods in terms of both quantitative metrics and qualitative visual quality.",
      "intriguing_abstract": "Blind image inpainting, the task of detecting corrupted regions with diverse patterns within an image and then generating plausible content for the corrupted regions, remains a both challenging and practical problem in computer vision. In this paper, we propose a novel model InViT for blind image inpainting, which leverages a combination of a pre-trained Generative Adversarial Network (GAN) and a learnable Vision Transformer (ViT). The proposed InViT mainly consists of two phases, the mask prediction phase and the image inpainting phase. Benefiting from the learned latent feature space from the full training data through GAN inversion, a pre-trained StyleGAN is able to provide reliable cues of corrupted regions for mask prediction. By further incorporating the predicted mask into the image inpainting phase, we design a vision Transformer with the mask-aware self-attention mechanism to capture long-range dependencies between pixels during content reconstruction. Besides, we propose a Prompt-augment Contextual Aggregation module to strengthen the reasonableness of generated content for the corrupted regions. Extensive experiments on several benchmark datasets for blind image inpainting demonstrate that our InViT model achieves state-of-the-art performance compared to existing methods in terms of both quantitative metrics and qualitative visual quality.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/ab10aacab1a2672a154034c589dd0aa801912272.pdf",
      "citation_key": "du2024i6n",
      "metadata": {
        "title": "InViT: GAN Inversion-Based Vision Transformer for Blind Image Inpainting",
        "authors": [
          "Yongqiang Du",
          "Haoran Liu",
          "Shengjie He",
          "Songnan Chen"
        ],
        "published_date": "2024",
        "abstract": "Blind image inpainting, the task of detecting corrupted regions with diverse patterns within an image and then generating plausible content for the corrupted regions, remains a both challenging and practical problem in computer vision. In this paper, we propose a novel model InViT for blind image inpainting, which leverages a combination of a pre-trained Generative Adversarial Network (GAN) and a learnable Vision Transformer (ViT). The proposed InViT mainly consists of two phases, the mask prediction phase and the image inpainting phase. Benefiting from the learned latent feature space from the full training data through GAN inversion, a pre-trained StyleGAN is able to provide reliable cues of corrupted regions for mask prediction. By further incorporating the predicted mask into the image inpainting phase, we design a vision Transformer with the mask-aware self-attention mechanism to capture long-range dependencies between pixels during content reconstruction. Besides, we propose a Prompt-augment Contextual Aggregation module to strengthen the reasonableness of generated content for the corrupted regions. Extensive experiments on several benchmark datasets for blind image inpainting demonstrate that our InViT model achieves state-of-the-art performance compared to existing methods in terms of both quantitative metrics and qualitative visual quality.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ab10aacab1a2672a154034c589dd0aa801912272.pdf",
        "venue": "IEEE Access",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Blind image inpainting, the task of detecting corrupted regions with diverse patterns within an image and then generating plausible content for the corrupted regions, remains a both challenging and practical problem in computer vision. In this paper, we propose a novel model InViT for blind image inpainting, which leverages a combination of a pre-trained Generative Adversarial Network (GAN) and a learnable Vision Transformer (ViT). The proposed InViT mainly consists of two phases, the mask prediction phase and the image inpainting phase. Benefiting from the learned latent feature space from the full training data through GAN inversion, a pre-trained StyleGAN is able to provide reliable cues of corrupted regions for mask prediction. By further incorporating the predicted mask into the image inpainting phase, we design a vision Transformer with the mask-aware self-attention mechanism to capture long-range dependencies between pixels during content reconstruction. Besides, we propose a Prompt-augment Contextual Aggregation module to strengthen the reasonableness of generated content for the corrupted regions. Extensive experiments on several benchmark datasets for blind image inpainting demonstrate that our InViT model achieves state-of-the-art performance compared to existing methods in terms of both quantitative metrics and qualitative visual quality.",
        "keywords": []
      },
      "file_name": "ab10aacab1a2672a154034c589dd0aa801912272.pdf"
    },
    {
      "success": true,
      "doc_id": "43cad0f4d0bca4821b7bcd56229b4a90",
      "summary": "Robotic grippers are receiving increasing attention in various industries as essential components of robots for interacting and manipulating objects. While significant progress has been made in the past, conventional rigid grippers still have limitations in handling irregular objects and can damage fragile objects. We have shown that soft grippers offer deformability to adapt to a variety of object shapes and maximize object protection. At the same time, dynamic vision sensors (e.g., event-based cameras) are capable of capturing small changes in brightness and streaming them asynchronously as events, unlike RGB cameras, which do not perform well in low-light and fast-moving environments. In this paper, a dynamic-vision-based algorithm is proposed to measure the force applied to the gripper. In particular, we first set up a DVXplorer Lite series event camera to capture twenty-five sets of event data. Second, motivated by the impressive performance of the Vision Transformer (ViT) algorithm in dense image prediction tasks, we propose a new approach that demonstrates the potential for force estimation and meets the requirements of real-world scenarios. We extensively evaluate the proposed algorithm on a wide range of scenarios and settings, and show that it consistently outperforms recent approaches.",
      "intriguing_abstract": "Robotic grippers are receiving increasing attention in various industries as essential components of robots for interacting and manipulating objects. While significant progress has been made in the past, conventional rigid grippers still have limitations in handling irregular objects and can damage fragile objects. We have shown that soft grippers offer deformability to adapt to a variety of object shapes and maximize object protection. At the same time, dynamic vision sensors (e.g., event-based cameras) are capable of capturing small changes in brightness and streaming them asynchronously as events, unlike RGB cameras, which do not perform well in low-light and fast-moving environments. In this paper, a dynamic-vision-based algorithm is proposed to measure the force applied to the gripper. In particular, we first set up a DVXplorer Lite series event camera to capture twenty-five sets of event data. Second, motivated by the impressive performance of the Vision Transformer (ViT) algorithm in dense image prediction tasks, we propose a new approach that demonstrates the potential for force estimation and meets the requirements of real-world scenarios. We extensively evaluate the proposed algorithm on a wide range of scenarios and settings, and show that it consistently outperforms recent approaches.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/325367f93439652efaa4bc6b50115bbb7371704e.pdf",
      "citation_key": "guo2024o8u",
      "metadata": {
        "title": "Force-EvT: A Closer Look at Robotic Gripper Force Measurement with Event-Based Vision Transformer",
        "authors": [
          "Qianyu Guo",
          "Ziqing Yu",
          "Jiaming Fu",
          "Yawen Lu",
          "Yahya H. Zweiri",
          "Dongming Gan"
        ],
        "published_date": "2024",
        "abstract": "Robotic grippers are receiving increasing attention in various industries as essential components of robots for interacting and manipulating objects. While significant progress has been made in the past, conventional rigid grippers still have limitations in handling irregular objects and can damage fragile objects. We have shown that soft grippers offer deformability to adapt to a variety of object shapes and maximize object protection. At the same time, dynamic vision sensors (e.g., event-based cameras) are capable of capturing small changes in brightness and streaming them asynchronously as events, unlike RGB cameras, which do not perform well in low-light and fast-moving environments. In this paper, a dynamic-vision-based algorithm is proposed to measure the force applied to the gripper. In particular, we first set up a DVXplorer Lite series event camera to capture twenty-five sets of event data. Second, motivated by the impressive performance of the Vision Transformer (ViT) algorithm in dense image prediction tasks, we propose a new approach that demonstrates the potential for force estimation and meets the requirements of real-world scenarios. We extensively evaluate the proposed algorithm on a wide range of scenarios and settings, and show that it consistently outperforms recent approaches.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/325367f93439652efaa4bc6b50115bbb7371704e.pdf",
        "venue": "2024 6th International Conference on Reconfigurable Mechanisms and Robots (ReMAR)",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Robotic grippers are receiving increasing attention in various industries as essential components of robots for interacting and manipulating objects. While significant progress has been made in the past, conventional rigid grippers still have limitations in handling irregular objects and can damage fragile objects. We have shown that soft grippers offer deformability to adapt to a variety of object shapes and maximize object protection. At the same time, dynamic vision sensors (e.g., event-based cameras) are capable of capturing small changes in brightness and streaming them asynchronously as events, unlike RGB cameras, which do not perform well in low-light and fast-moving environments. In this paper, a dynamic-vision-based algorithm is proposed to measure the force applied to the gripper. In particular, we first set up a DVXplorer Lite series event camera to capture twenty-five sets of event data. Second, motivated by the impressive performance of the Vision Transformer (ViT) algorithm in dense image prediction tasks, we propose a new approach that demonstrates the potential for force estimation and meets the requirements of real-world scenarios. We extensively evaluate the proposed algorithm on a wide range of scenarios and settings, and show that it consistently outperforms recent approaches.",
        "keywords": []
      },
      "file_name": "325367f93439652efaa4bc6b50115bbb7371704e.pdf"
    },
    {
      "success": true,
      "doc_id": "4921a7aa880edfaa3960eb65cb991e48",
      "summary": "This paper introduces a novel action recognition model named ViT-Shift, which combines the Time Shift Module (TSM) with the Vision Transformer (ViT) architecture. Traditional video action recognition tasks face significant computational challenges, requiring substantial computing resources. However, our model successfully addresses this issue by incorporating the TSM, achieving outstanding performance while significantly reducing computational costs. Our approach is based on the latest Transformer self-attention mechanism, applied to video sequence processing instead of traditional convolutional methods. To preserve the core architecture of ViT and transfer its excellent performance in image recognition to video action recognition, we strategically introduce the TSM only before the multi-head attention layer of ViT. This design allows us to simulate temporal interactions using channel shifts, effectively reducing computational complexity. We carefully design the position and shift parameters of the TSM to maximize the models performance. Experimental results demonstrate that ViT-Shift achieves remarkable results on two standard action recognition datasets. With ImageNet-21K pretraining, we achieve an accuracy of 77.55% on the Kinetics-400 dataset and 93.07% on the UCF-101 dataset.",
      "intriguing_abstract": "This paper introduces a novel action recognition model named ViT-Shift, which combines the Time Shift Module (TSM) with the Vision Transformer (ViT) architecture. Traditional video action recognition tasks face significant computational challenges, requiring substantial computing resources. However, our model successfully addresses this issue by incorporating the TSM, achieving outstanding performance while significantly reducing computational costs. Our approach is based on the latest Transformer self-attention mechanism, applied to video sequence processing instead of traditional convolutional methods. To preserve the core architecture of ViT and transfer its excellent performance in image recognition to video action recognition, we strategically introduce the TSM only before the multi-head attention layer of ViT. This design allows us to simulate temporal interactions using channel shifts, effectively reducing computational complexity. We carefully design the position and shift parameters of the TSM to maximize the models performance. Experimental results demonstrate that ViT-Shift achieves remarkable results on two standard action recognition datasets. With ImageNet-21K pretraining, we achieve an accuracy of 77.55% on the Kinetics-400 dataset and 93.07% on the UCF-101 dataset.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/3c6980902883f03c37332d34ead343e1229062b3.pdf",
      "citation_key": "zhang2024g0m",
      "metadata": {
        "title": "Temporal Shift Module-Based Vision Transformer Network for Action Recognition",
        "authors": [
          "Kunpeng Zhang",
          "Mengyan Lyu",
          "Xinxin Guo",
          "Liye Zhang",
          "Cong Liu"
        ],
        "published_date": "2024",
        "abstract": "This paper introduces a novel action recognition model named ViT-Shift, which combines the Time Shift Module (TSM) with the Vision Transformer (ViT) architecture. Traditional video action recognition tasks face significant computational challenges, requiring substantial computing resources. However, our model successfully addresses this issue by incorporating the TSM, achieving outstanding performance while significantly reducing computational costs. Our approach is based on the latest Transformer self-attention mechanism, applied to video sequence processing instead of traditional convolutional methods. To preserve the core architecture of ViT and transfer its excellent performance in image recognition to video action recognition, we strategically introduce the TSM only before the multi-head attention layer of ViT. This design allows us to simulate temporal interactions using channel shifts, effectively reducing computational complexity. We carefully design the position and shift parameters of the TSM to maximize the models performance. Experimental results demonstrate that ViT-Shift achieves remarkable results on two standard action recognition datasets. With ImageNet-21K pretraining, we achieve an accuracy of 77.55% on the Kinetics-400 dataset and 93.07% on the UCF-101 dataset.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3c6980902883f03c37332d34ead343e1229062b3.pdf",
        "venue": "IEEE Access",
        "citationCount": 3,
        "score": 3.0,
        "summary": "This paper introduces a novel action recognition model named ViT-Shift, which combines the Time Shift Module (TSM) with the Vision Transformer (ViT) architecture. Traditional video action recognition tasks face significant computational challenges, requiring substantial computing resources. However, our model successfully addresses this issue by incorporating the TSM, achieving outstanding performance while significantly reducing computational costs. Our approach is based on the latest Transformer self-attention mechanism, applied to video sequence processing instead of traditional convolutional methods. To preserve the core architecture of ViT and transfer its excellent performance in image recognition to video action recognition, we strategically introduce the TSM only before the multi-head attention layer of ViT. This design allows us to simulate temporal interactions using channel shifts, effectively reducing computational complexity. We carefully design the position and shift parameters of the TSM to maximize the models performance. Experimental results demonstrate that ViT-Shift achieves remarkable results on two standard action recognition datasets. With ImageNet-21K pretraining, we achieve an accuracy of 77.55% on the Kinetics-400 dataset and 93.07% on the UCF-101 dataset.",
        "keywords": []
      },
      "file_name": "3c6980902883f03c37332d34ead343e1229062b3.pdf"
    },
    {
      "success": true,
      "doc_id": "949465efb1a3bad2ae94789acdf5b539",
      "summary": "In this article, we propose a novel approach to addressing the challenge of unmanned aerial vehicle (UAV) detection and identification by combining object detection and image classification techniques. Diverging from prevailing research that predominantly relies on convolutional neural networks (CNNs), we introduce the radio frequency drone vision transformer (RFDroneViT), a novel two-stage vision transformer (ViT) consisting of two distinct sub-ViT. The first stage employs the designed end-to-end object detection with transformer (DETR) to detect the UAV, while the second stage utilizes ViT-B/16 to identify the detected UAV. Based on RFDroneViT, we propose several detailed optimization algorithms, culminating in the development of a cost-effective system capable of UAV detection and identification. We evaluate the systems performance through rigorous experimentation. Our custom DETR model achieves the state-of-the-art performance in drone signal detection with an average precision (AP) of 74.3 and demonstrates an 8.7 improvement in small target AP (APS) for the signal detection task compared to the original DETR. Additionally, the classification model can achieve 98.7% Top-1. The experimental results demonstrated that our proposed system achieves great performance on drone detection and identification.",
      "intriguing_abstract": "In this article, we propose a novel approach to addressing the challenge of unmanned aerial vehicle (UAV) detection and identification by combining object detection and image classification techniques. Diverging from prevailing research that predominantly relies on convolutional neural networks (CNNs), we introduce the radio frequency drone vision transformer (RFDroneViT), a novel two-stage vision transformer (ViT) consisting of two distinct sub-ViT. The first stage employs the designed end-to-end object detection with transformer (DETR) to detect the UAV, while the second stage utilizes ViT-B/16 to identify the detected UAV. Based on RFDroneViT, we propose several detailed optimization algorithms, culminating in the development of a cost-effective system capable of UAV detection and identification. We evaluate the systems performance through rigorous experimentation. Our custom DETR model achieves the state-of-the-art performance in drone signal detection with an average precision (AP) of 74.3 and demonstrates an 8.7 improvement in small target AP (APS) for the signal detection task compared to the original DETR. Additionally, the classification model can achieve 98.7% Top-1. The experimental results demonstrated that our proposed system achieves great performance on drone detection and identification.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/f2b1b0fb57cccaac51b44477726d510570c4c799.pdf",
      "citation_key": "xu2025tku",
      "metadata": {
        "title": "A Radio Frequency Sensor-Based UAV Detection and Identification System Using Improved Vision Transformer-Based Model",
        "authors": [
          "Lu Xu",
          "Rui Shi",
          "Yijia Zhang"
        ],
        "published_date": "2025",
        "abstract": "In this article, we propose a novel approach to addressing the challenge of unmanned aerial vehicle (UAV) detection and identification by combining object detection and image classification techniques. Diverging from prevailing research that predominantly relies on convolutional neural networks (CNNs), we introduce the radio frequency drone vision transformer (RFDroneViT), a novel two-stage vision transformer (ViT) consisting of two distinct sub-ViT. The first stage employs the designed end-to-end object detection with transformer (DETR) to detect the UAV, while the second stage utilizes ViT-B/16 to identify the detected UAV. Based on RFDroneViT, we propose several detailed optimization algorithms, culminating in the development of a cost-effective system capable of UAV detection and identification. We evaluate the systems performance through rigorous experimentation. Our custom DETR model achieves the state-of-the-art performance in drone signal detection with an average precision (AP) of 74.3 and demonstrates an 8.7 improvement in small target AP (APS) for the signal detection task compared to the original DETR. Additionally, the classification model can achieve 98.7% Top-1. The experimental results demonstrated that our proposed system achieves great performance on drone detection and identification.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f2b1b0fb57cccaac51b44477726d510570c4c799.pdf",
        "venue": "IEEE Sensors Journal",
        "citationCount": 3,
        "score": 3.0,
        "summary": "In this article, we propose a novel approach to addressing the challenge of unmanned aerial vehicle (UAV) detection and identification by combining object detection and image classification techniques. Diverging from prevailing research that predominantly relies on convolutional neural networks (CNNs), we introduce the radio frequency drone vision transformer (RFDroneViT), a novel two-stage vision transformer (ViT) consisting of two distinct sub-ViT. The first stage employs the designed end-to-end object detection with transformer (DETR) to detect the UAV, while the second stage utilizes ViT-B/16 to identify the detected UAV. Based on RFDroneViT, we propose several detailed optimization algorithms, culminating in the development of a cost-effective system capable of UAV detection and identification. We evaluate the systems performance through rigorous experimentation. Our custom DETR model achieves the state-of-the-art performance in drone signal detection with an average precision (AP) of 74.3 and demonstrates an 8.7 improvement in small target AP (APS) for the signal detection task compared to the original DETR. Additionally, the classification model can achieve 98.7% Top-1. The experimental results demonstrated that our proposed system achieves great performance on drone detection and identification.",
        "keywords": []
      },
      "file_name": "f2b1b0fb57cccaac51b44477726d510570c4c799.pdf"
    },
    {
      "success": true,
      "doc_id": "0b5374d6de74edb864a6a687d9be592e",
      "summary": "Intelligent Transportation Systems (ITS) utilize sensors, cameras, and big data analysis to monitor real-time traffic conditions, aiming to improve traffic efficiency and safety. Accurate vehicle recognition is crucial in this process, and Vehicle Logo Recognition (VLR) stands as a key method. VLR enables effective management and monitoring by distinguishing vehicles on the road. Convolutional Neural Networks (CNNs) have made impressive strides in VLR research. However, achieving higher performance demands significant time and computational resources for training. Recently, the rise of Transformer models has brought new opportunities to VLR. Swin Transformer, with its efficient computation and global feature modeling capabilities, outperforms CNNs under challenging conditions. In this paper, we implement real-time VLR using Swin Transformer and fine-tune it for optimal performance. Extensive experiments conducted on three public vehicle logo datasets (HFUT-VL1, XMU, CTGU-VLD) demonstrate impressive top accuracy results of 99.28%, 100%, and 99.17%, respectively. Additionally, the use of a transfer learning strategy enables our method to be on par with state-of-the-art VLR methods. These findings affirm the superiority of our approach over existing methods. Future research can explore and optimize the application of the Swin Transformer in other vehicle vision recognition tasks to drive advancements in ITS.",
      "intriguing_abstract": "Intelligent Transportation Systems (ITS) utilize sensors, cameras, and big data analysis to monitor real-time traffic conditions, aiming to improve traffic efficiency and safety. Accurate vehicle recognition is crucial in this process, and Vehicle Logo Recognition (VLR) stands as a key method. VLR enables effective management and monitoring by distinguishing vehicles on the road. Convolutional Neural Networks (CNNs) have made impressive strides in VLR research. However, achieving higher performance demands significant time and computational resources for training. Recently, the rise of Transformer models has brought new opportunities to VLR. Swin Transformer, with its efficient computation and global feature modeling capabilities, outperforms CNNs under challenging conditions. In this paper, we implement real-time VLR using Swin Transformer and fine-tune it for optimal performance. Extensive experiments conducted on three public vehicle logo datasets (HFUT-VL1, XMU, CTGU-VLD) demonstrate impressive top accuracy results of 99.28%, 100%, and 99.17%, respectively. Additionally, the use of a transfer learning strategy enables our method to be on par with state-of-the-art VLR methods. These findings affirm the superiority of our approach over existing methods. Future research can explore and optimize the application of the Swin Transformer in other vehicle vision recognition tasks to drive advancements in ITS.",
      "keywords": [],
      "file_path": "paper_data/A_survey_on_Visual_Transformer/2456506ed87faa667a0c2b8af4028a5a86a49650.pdf",
      "citation_key": "li2024m4t",
      "metadata": {
        "title": "A New Method for Vehicle Logo Recognition Based on Swin Transformer",
        "authors": [
          "Yang Li",
          "Doudou Zhang",
          "Jianli Xiao"
        ],
        "published_date": "2024",
        "abstract": "Intelligent Transportation Systems (ITS) utilize sensors, cameras, and big data analysis to monitor real-time traffic conditions, aiming to improve traffic efficiency and safety. Accurate vehicle recognition is crucial in this process, and Vehicle Logo Recognition (VLR) stands as a key method. VLR enables effective management and monitoring by distinguishing vehicles on the road. Convolutional Neural Networks (CNNs) have made impressive strides in VLR research. However, achieving higher performance demands significant time and computational resources for training. Recently, the rise of Transformer models has brought new opportunities to VLR. Swin Transformer, with its efficient computation and global feature modeling capabilities, outperforms CNNs under challenging conditions. In this paper, we implement real-time VLR using Swin Transformer and fine-tune it for optimal performance. Extensive experiments conducted on three public vehicle logo datasets (HFUT-VL1, XMU, CTGU-VLD) demonstrate impressive top accuracy results of 99.28%, 100%, and 99.17%, respectively. Additionally, the use of a transfer learning strategy enables our method to be on par with state-of-the-art VLR methods. These findings affirm the superiority of our approach over existing methods. Future research can explore and optimize the application of the Swin Transformer in other vehicle vision recognition tasks to drive advancements in ITS.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2456506ed87faa667a0c2b8af4028a5a86a49650.pdf",
        "venue": "arXiv.org",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Intelligent Transportation Systems (ITS) utilize sensors, cameras, and big data analysis to monitor real-time traffic conditions, aiming to improve traffic efficiency and safety. Accurate vehicle recognition is crucial in this process, and Vehicle Logo Recognition (VLR) stands as a key method. VLR enables effective management and monitoring by distinguishing vehicles on the road. Convolutional Neural Networks (CNNs) have made impressive strides in VLR research. However, achieving higher performance demands significant time and computational resources for training. Recently, the rise of Transformer models has brought new opportunities to VLR. Swin Transformer, with its efficient computation and global feature modeling capabilities, outperforms CNNs under challenging conditions. In this paper, we implement real-time VLR using Swin Transformer and fine-tune it for optimal performance. Extensive experiments conducted on three public vehicle logo datasets (HFUT-VL1, XMU, CTGU-VLD) demonstrate impressive top accuracy results of 99.28%, 100%, and 99.17%, respectively. Additionally, the use of a transfer learning strategy enables our method to be on par with state-of-the-art VLR methods. These findings affirm the superiority of our approach over existing methods. Future research can explore and optimize the application of the Swin Transformer in other vehicle vision recognition tasks to drive advancements in ITS.",
        "keywords": []
      },
      "file_name": "2456506ed87faa667a0c2b8af4028a5a86a49650.pdf"
    }
  ]
}