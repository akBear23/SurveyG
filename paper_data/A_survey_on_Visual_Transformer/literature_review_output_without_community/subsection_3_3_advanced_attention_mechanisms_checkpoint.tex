\subsection*{Advanced Attention Mechanisms}

The foundational self-attention mechanism in Vision Transformers, while powerful for capturing global dependencies, inherently suffers from quadratic computational complexity with respect to the number of tokens, posing significant challenges for high-resolution images or deeper architectures. Furthermore, vanilla self-attention often lacks explicit spatial inductive biases, which can sometimes lead to redundancy or "attention collapse" in very deep models, limiting their expressive power. To address these limitations, researchers have developed various advanced attention mechanisms aimed at improving efficiency, flexibility, and expressiveness.

One critical challenge in scaling Vision Transformers to greater depths is the phenomenon of "attention collapse." As models become deeper, the attention maps across different layers can become increasingly similar, hindering the model's ability to learn diverse and effective representations. \textcite{zhou202105h} introduced DeepViT to tackle this issue, observing that in deeper ViTs, self-attention mechanisms often fail to learn distinct concepts, leading to performance saturation. To mitigate this, they proposed Re-attention, a simple yet effective method designed to re-generate attention maps and increase their diversity across layers. This innovation allows for the training of significantly deeper ViT models with consistent performance improvements, enhancing the model's overall expressiveness by ensuring that attention layers continue to contribute unique feature transformations.

Beyond improving expressiveness in deep models, a major thrust in advanced attention research focuses on enhancing efficiency and spatial awareness. The global nature of standard self-attention means every token attends to every other token, which can be computationally expensive and may lead to attention being drawn to irrelevant regions. To make attention more adaptive and efficient, Deformable Attention was introduced, allowing the attention mechanism to sample features at adaptive, learned offsets \textcite{xia2022qga}. Unlike fixed-grid or sparse attention patterns, this data-dependent sampling enables the model to dynamically focus on relevant regions and capture more informative features. This flexible scheme significantly reduces computational overhead by concentrating attention on salient parts of the input, while simultaneously enhancing the model's ability to adapt to varying object shapes and scales, thereby improving spatial awareness and fine-grained detail capture.

This concept of focusing attention on relevant tokens to reduce computation is also explored in approaches like Focal Attention. Such mechanisms typically employ hierarchical attention or explicit token selection strategies to prioritize important visual information. By selectively attending to a subset of tokens or regions, these methods aim to mitigate the quadratic complexity of global attention, making ViTs more scalable for high-resolution inputs. These innovations collectively enhance the model's ability to capture fine-grained details and adapt to varying object shapes and scales, leading to more robust and accurate visual understanding.

In summary, advanced attention mechanisms represent a crucial evolutionary step for Vision Transformers, moving beyond the limitations of vanilla self-attention. By addressing issues like attention collapse through techniques such as Re-attention \textcite{zhou202105h} and improving efficiency and spatial adaptability with data-dependent sampling as seen in Deformable Attention \textcite{xia2022qga}, these refinements enable the development of more powerful, flexible, and scalable ViT architectures. Future directions will likely continue to explore more sophisticated ways to balance global context modeling with local detail, further optimizing computational efficiency, and integrating stronger inductive biases into attention mechanisms for even more robust visual understanding across diverse tasks.