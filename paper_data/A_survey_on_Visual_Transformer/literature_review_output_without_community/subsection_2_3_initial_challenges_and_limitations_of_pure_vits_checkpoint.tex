\subsection*{Initial Challenges and Limitations of Pure ViTs}

Despite the groundbreaking success of the Vision Transformer (ViT) in demonstrating that a pure Transformer architecture could achieve state-of-the-art performance in image classification, the initial iterations of these models presented several significant challenges and inherent limitations \cite{ICLR2019}. These drawbacks, which became apparent despite their impressive performance on large-scale datasets, necessitated extensive subsequent research into their optimization and refinement.

A primary limitation of the original pure ViT architecture, as introduced by \cite{ICLR2019}, was its substantial data requirement for pre-training. Unlike Convolutional Neural Networks (CNNs) that inherently possess inductive biases such as locality and translation equivariance, pure ViTs lack these built-in priors. Consequently, they must learn these fundamental visual properties from scratch, demanding massive datasets like JFT-300M to achieve competitive performance, making them less accessible for researchers without access to such vast resources. This data hunger was a critical bottleneck, prompting early efforts to make ViTs more data-efficient, such as through knowledge distillation \cite{Touvron2021} or refined tokenization strategies \cite{Yuan2021}.

Another significant challenge stemmed from the computational complexity of the global self-attention mechanism, which is central to pure ViTs. For an input image of $H \times W$ resolution, if flattened into $N$ patches, the self-attention operation scales quadratically with the number of patches, $O(N^2)$, or equivalently, $O((HW)^2)$ with respect to the image resolution. This quadratic complexity made pure ViTs computationally prohibitive for high-resolution images and impractical for tasks requiring fine-grained, pixel-level understanding, such as dense prediction. This limitation spurred the development of hierarchical ViTs that employed localized attention mechanisms, like the shifted window attention in Swin Transformer \cite{ICLR2021} or pyramid structures in PVT \cite{ICCV2021}, to reduce computational cost and enable multi-scale feature extraction.

Beyond data and computational demands, pure ViTs also exhibited architectural limitations that hindered their scalability and applicability. For instance, deeper pure ViT models were found to suffer from an "attention collapse" issue \cite{zhou202105h}. As the network depth increased, the attention maps across layers tended to become increasingly similar, leading to redundant feature learning and performance saturation rather than improvement. To address this, \cite{zhou202105h} proposed Re-attention to diversify attention maps and enable consistent performance gains in deeper ViTs.

Furthermore, the lack of inherent hierarchical feature representation and strong locality inductive biases in pure ViTs made them less naturally suited for dense prediction tasks like object detection and semantic segmentation, which traditionally benefit from multi-scale feature pyramids and local context modeling. While studies like \cite{li2022raj} demonstrated that plain ViT backbones could be adapted for object detection, they often required significant modifications, such as building simple feature pyramids from single-scale feature maps and incorporating cross-window propagation blocks, to achieve competitive results. This highlighted that the original pure ViT design was not intrinsically optimized for these tasks without architectural augmentations.

In summary, the initial pure ViT architectures, despite their paradigm-shifting performance, were characterized by their substantial data requirements, quadratic computational complexity, and a lack of built-in inductive biases that are naturally present in CNNs. These factors collectively made them less efficient, harder to train effectively on smaller datasets, and less versatile for a broad range of computer vision tasks without significant modifications. These inherent limitations became a fertile ground for subsequent research, driving the evolution towards more efficient, data-agnostic, and task-adaptable Vision Transformer variants.