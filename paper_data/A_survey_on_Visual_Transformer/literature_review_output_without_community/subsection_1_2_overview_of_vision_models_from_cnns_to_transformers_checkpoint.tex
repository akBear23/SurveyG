\subsection{Overview of Vision Models: From CNNs to Transformers}

The trajectory of computer vision research has been marked by a significant paradigm shift, evolving from the long-standing dominance of Convolutional Neural Networks (CNNs) to the recent ascendance of Transformer architectures. For decades, CNNs served as the foundational backbone for visual recognition tasks, owing their success to inherent inductive biases that align well with the hierarchical and local nature of visual data \cite{han2020yk0}. Key among these biases are local receptive fields, which allow neurons to process only a small, localized region of the input; weight sharing, which enables the detection of features regardless of their position; and spatial pooling, which progressively reduces spatial dimensions while retaining essential information. Architectures like AlexNet \cite{alexnet_2012}, VGG \cite{vgg_2014}, ResNet \cite{resnet_2016}, and Inception \cite{inception_2015} exemplified this era, demonstrating remarkable capabilities in tasks ranging from image classification to object detection by building increasingly complex and abstract representations through stacked convolutional layers. Their architectural evolution often involved increasing depth, introducing residual connections to mitigate vanishing gradients, and designing more intricate modules to enhance representational power, all while retaining the core principle of localized feature extraction.

Despite their profound success and continuous advancements, CNNs inherently faced limitations, particularly in effectively modeling global contextual information across an entire image. Their reliance on local operations meant that capturing long-range dependencies required very deep networks, which could be computationally intensive and sometimes struggled to integrate information from widely separated regions efficiently. This inductive bias towards locality, while beneficial for many tasks, could also constrain their ability to understand broader semantic relationships or complex spatial layouts without extensive architectural modifications \cite{han2020yk0, zhou2021rtn}.

A pivotal moment arrived with the introduction of the Transformer architecture \cite{attention_is_all_you_need_2017}, originally conceived for natural language processing (NLP). Its core innovation, the self-attention mechanism, revolutionized sequence modeling by enabling models to weigh the importance of different parts of an input sequence, fostering a more global understanding of relationships without relying on recurrent or convolutional operations. This mechanism allowed for parallel processing of inputs and offered an unprecedented ability to capture long-range dependencies directly. The profound success of Transformers in NLP inspired researchers to explore their applicability to other domains, including computer vision \cite{heidari2024d9k}.

This exploration culminated in the introduction of the Vision Transformer (ViT) \cite{image_is_worth_16x16_words_2021}. The ViT architecture ingeniously adapted the standard Transformer encoder for image recognition by treating images as sequences of non-overlapping patches. Each patch was linearly embedded into a token, and positional embeddings were added to retain spatial information. These visual tokens were then fed into a Transformer encoder, leveraging its global self-attention mechanism to capture relationships between patches across the entire image. This marked a significant paradigm shift, demonstrating that models could achieve competitive performance in image recognition without relying on convolutional layers, instead directly leveraging the Transformer's ability to capture global dependencies. This approach fundamentally altered how visual features were extracted and processed, moving away from predefined spatial hierarchies and towards a more flexible, data-driven understanding of global context \cite{han2020yk0}.

The initial success of ViTs, however, was accompanied by practical challenges. Pure ViT models often required significantly larger datasets for pre-training compared to CNNs to achieve comparable performance, largely due to their reduced inherent inductive biases \cite{image_is_worth_16x16_words_2021, zhou2021rtn}. Furthermore, the quadratic computational complexity of global self-attention with respect to the number of tokens (and thus image resolution) posed considerable computational costs, limiting their applicability to high-resolution images or resource-constrained environments. These initial limitations became the primary drivers for an explosion of subsequent research aimed at enhancing the efficiency, data-effectiveness, and architectural flexibility of Vision Transformers.

In conclusion, the transition from CNNs to Transformers represents a fundamental evolution in computer vision, shifting the focus from local, hierarchical feature extraction to global context understanding through attention mechanisms. While CNNs provided a robust foundation, the advent of ViTs offered a new paradigm capable of overcoming some of their inherent limitations, particularly in modeling long-range dependencies. This breakthrough, despite its initial challenges related to data hunger and computational expense, has spurred extensive research into architectural refinements, efficient training methodologies, and hybrid models. These ongoing efforts, which will be explored in detail in the subsequent sections of this review, are continuously solidifying the position of Vision Transformers as versatile and powerful backbones for a wide array of computer vision tasks, fundamentally altering how visual features are extracted and processed.