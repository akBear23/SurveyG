\subsection*{Hierarchical Vision Transformers}

Early Vision Transformers (ViTs) \cite{dosovitskiy2020image} marked a significant paradigm shift in computer vision, yet their direct application to tasks requiring fine-grained spatial understanding, such as dense prediction, or processing of high-resolution images, was hampered by two primary limitations \cite{hassija2025wq3, heidari2024d9k}. Firstly, the global self-attention mechanism exhibited quadratic computational complexity with respect to the number of input tokens (image patches), making it prohibitively expensive for large inputs. Secondly, pure ViTs lacked the inherent multi-scale feature representations and inductive biases (like locality) that Convolutional Neural Networks (CNNs) naturally possess, which are crucial for capturing both global context and local details across various scales. To address these challenges, a critical line of research emerged, focusing on the development of hierarchical Vision Transformers that integrate multi-scale processing and more efficient attention mechanisms, thereby mimicking CNN-like feature pyramids and achieving linear computational complexity.

One of the foundational models in this category is the \textbf{Pyramid Vision Transformer (PVT)} \cite{wang2021pyramid}. PVT introduced a progressive shrinking pyramid structure, akin to feature pyramids in CNNs, by gradually reducing the resolution of feature maps in deeper layers. This hierarchical design allows for the generation of multi-scale features, which are essential for dense prediction tasks. To manage the computational cost, PVT employs a Spatial-Reduction Attention (SRA) module. Unlike global self-attention, SRA reduces the spatial dimension of the key and value matrices before computing attention, effectively lowering the computational complexity from quadratic to linear with respect to image size. This innovation enabled PVT to serve as a robust backbone for tasks like object detection and semantic segmentation, demonstrating the viability of hierarchical Transformers for a broader range of vision applications \cite{hassija2025wq3}.

Building upon the success of hierarchical designs, the \textbf{Swin Transformer} \cite{liu2021ljs} emerged as another seminal work, further solidifying the potential of these architectures. Swin Transformer also adopts a hierarchical structure through progressive patch merging, generating multi-scale feature maps. Its core innovation, however, lies in the "shifted window attention" mechanism. Instead of global attention, Swin restricts self-attention computation to non-overlapping local windows within each stage, drastically reducing computational complexity to linear. To facilitate information exchange between windows and capture global dependencies, the windows are shifted between successive Transformer blocks. This elegant solution allows Swin Transformer to achieve state-of-the-art performance across a wide array of vision tasks, including image classification, object detection, and semantic segmentation, effectively establishing hierarchical Transformers as powerful general-purpose vision backbones. While both PVT and Swin Transformer achieved linear complexity, Swin's shifted window approach provided a more dynamic mechanism for cross-window information flow compared to PVT's static spatial reduction, often leading to stronger performance.

Further advancements in hierarchical ViTs have focused on refining attention mechanisms and simplifying architectures. The fixed window sizes and handcrafted attention patterns in models like Swin Transformer and PVT, while efficient, can sometimes limit their ability to model long-range relations adaptively \cite{xia2023bp7}. Addressing this, the \textbf{Deformable Attention Transformer (DAT++)} \cite{xia2023bp7} proposes a novel deformable multi-head attention module. This module adaptively allocates the positions of key and value pairs in a data-dependent manner, allowing the model to dynamically focus on relevant regions and capture more flexible spatial relationships, similar to deformable convolutions in CNNs. This approach enhances the representation power of global attention while maintaining efficiency. Similarly, the \textbf{Dynamic Window Vision Transformer (DW-ViT)} \cite{ren2022ifo} extends the window-based paradigm by moving "beyond fixation." DW-ViT assigns windows of different sizes to different head groups within multi-head self-attention and dynamically fuses the multi-scale information, overcoming the limitation of fixed single-scale windows in models like Swin Transformer and further improving multi-scale modeling capabilities.

In contrast to increasing architectural complexity, some research has explored simplification. \textbf{Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles} \cite{ryali202339q} proposes a streamlined hierarchical ViT design, arguing that many added complexities can be removed without sacrificing accuracy, provided the model is pre-trained with a robust self-supervised pretext task, such as Masked Autoencoders (MAE) \cite{he2022masked}. By leveraging strong pre-training, Hiera achieves competitive or superior accuracy to more complex hierarchical models while being significantly faster during both inference and training. This work underscores the critical interplay between architectural design and effective pre-training strategies, suggesting that inductive biases from hierarchical processing, combined with powerful self-supervision, can lead to highly efficient and performant architectures without excessive overhead. This also opens a comparative perspective on the fundamental role of attention itself; for instance, the \textbf{ShiftViT} \cite{wang2022da0} even explores replacing attention layers entirely with a zero-parameter shift operation, achieving performance comparable to Swin Transformer, suggesting that the hierarchical structure and feature processing might be as crucial as the attention mechanism itself.

In conclusion, the evolution of hierarchical Vision Transformers has been instrumental in overcoming the initial limitations of pure ViTs, particularly their quadratic complexity and lack of multi-scale representations. Models like PVT and Swin Transformer pioneered efficient multi-scale processing through techniques such as spatial-reduction attention and shifted window attention, respectively, making ViTs suitable for dense prediction tasks and high-resolution images. Subsequent innovations, including deformable attention (DAT++) and dynamic windowing (DW-ViT), have further refined these attention mechanisms for greater flexibility and multi-scale understanding. Concurrently, models like Hiera highlight the power of architectural simplification when coupled with strong self-supervised pre-training. Future research in this domain will likely continue to explore the optimal balance between architectural complexity, computational efficiency, and representational power within hierarchical structures, further optimizing attention mechanisms, and deepening the synergy with advanced self-supervised learning paradigms to unlock even more robust and versatile vision backbones.