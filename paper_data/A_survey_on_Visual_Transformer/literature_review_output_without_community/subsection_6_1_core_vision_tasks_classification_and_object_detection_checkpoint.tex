\subsection*{Core Vision Tasks: Classification and Object Detection}

Vision Transformers (ViTs) have profoundly impacted fundamental computer vision tasks, particularly image classification and object detection, by leveraging their global receptive fields and powerful feature extraction capabilities. This architectural shift has enabled ViTs to capture both broad contextual information and fine-grained details, often leading to new performance benchmarks that frequently surpass traditional Convolutional Neural Networks (CNNs), especially when trained on extensive datasets.

The initial foray of Transformers into vision began with image classification. The seminal Vision Transformer (ViT) by \cite{Dosovitskiy2021} demonstrated that a pure Transformer, by treating image patches as sequential tokens, could effectively classify images. While achieving state-of-the-art results on massive datasets like ImageNet-21K (e.g., ViT-L/16 reaching 88.55\% top-1 accuracy), it initially exhibited a significant dependency on vast training data, often underperforming CNNs on smaller datasets. To mitigate this data-hungry nature, \cite{Touvron2021} introduced Data-efficient Image Transformers (DeiT), employing knowledge distillation to enable ViTs to be trained efficiently on ImageNet-1K, achieving competitive performance (e.g., DeiT-S reaching 83.1\% top-1 accuracy) without requiring massive pre-training. Concurrently, \cite{Yuan2021} proposed Tokens-to-Token ViT (T2T-ViT), which refined the initial tokenization process to better represent local structures, thereby boosting performance on standard datasets from scratch. Further architectural advancements aimed at enhancing the depth and stability of ViTs for classification; \cite{zhou202105h} identified the "attention collapse" issue in deeper ViTs and proposed Re-attention to increase the diversity of attention maps, enabling the training of significantly deeper models with consistent performance gains. More recently, `[ferdous2024f89]` introduced SPT-Swin, a variant that combines shifted patch tokenization with Swin Transformer to address data deficiency and computational complexity, achieving 89.45\% accuracy on ImageNet-1K.

A major breakthrough in efficient pre-training for classification came with \cite{CVPR2022}, which introduced Masked Autoencoders (MAE). This self-supervised learning approach reconstructs masked image patches, proving highly scalable and data-efficient. MAE enables ViTs to learn robust representations from unlabeled data, significantly improving their performance and reducing the need for extensive labeled datasets for classification. For instance, a ViT-Large pre-trained with MAE on ImageNet-1K can achieve 87.8\% top-1 accuracy. Building on this, the field has seen the emergence of "Vision Foundation Models," with \cite{ICLR2023} demonstrating the efficacy of scaling ViTs to over a billion parameters, achieving unprecedented classification performance through massive pre-training. Similarly, \cite{ICLR2023} advanced self-supervised learning with DINOv2, which learns highly robust and generalizable visual features without supervision, often outperforming supervised pre-training in transfer tasks and further enhancing classification accuracy. The influence of ViT principles even extended to CNNs, as shown by \cite{ICLR2023}, where ConvNeXt V2 leveraged MAE pre-training to significantly boost CNN performance, blurring the lines between the two architectures for classification tasks. For practical deployment, `[song2022603]` proposed CP-ViT, a cascade pruning framework that dynamically predicts sparsity in ViT models, reducing FLOPs by over 40\% while maintaining accuracy, crucial for resource-limited devices.

The success of ViTs quickly extended to object detection, a more complex task requiring both accurate object categorization and precise localization. Early pure ViTs struggled with dense prediction tasks due to their fixed-size patch embeddings and lack of inherent hierarchical feature maps, which are crucial for detecting objects at various scales. This limitation was fundamentally addressed by the introduction of end-to-end Transformer detectors. The seminal work on DEtection TRansformer (DETR) by \cite{Carion2020} revolutionized object detection by formulating it as a direct set prediction problem. DETR eliminated hand-designed components like Non-Maximum Suppression (NMS) by using a set of learned object queries and a bipartite matching loss. While DETR demonstrated the power of Transformers for detection, its slow convergence and high computational cost were initial drawbacks. These were largely overcome by Deformable DETR \cite{Zhu2020}, which introduced deformable attention to focus on a small set of key sampling points, significantly accelerating training and improving performance, particularly for small objects.

Following these foundational end-to-end Transformer detectors, architectural innovations focused on integrating hierarchical ViT backbones to better capture multi-scale visual features. The \cite{Liu2021} Swin Transformer, detailed in `[liu2021ljs]`, revolutionized ViT applicability to dense prediction by proposing a hierarchical architecture with shifted window attention. This design limited self-attention computation to non-overlapping local windows while allowing for cross-window connections, effectively generating multi-scale feature maps. Swin Transformers became highly suitable as backbones for object detection frameworks, achieving state-of-the-art results, such as 58.7 box AP and 51.1 mask AP on COCO test-dev with Swin-Large. Similarly, \cite{Wang2021} introduced the Pyramid Vision Transformer (PVT), a pure Transformer-based pyramid structure that progressively reduces feature map resolution, enabling the generation of multi-scale features essential for detecting objects of different sizes. These hierarchical backbones, when integrated with DETR-like heads, led to powerful detectors. For instance, `[wang2023bfo]` combined Deformable DETR with a Swin Transformer and a lightweight Feature Pyramid Network (FPN) to enhance detection accuracy for multi-scale targets, demonstrating a 6.1\% improvement in accuracy on a classroom behavior dataset.

Interestingly, even plain (non-hierarchical) ViT backbones, when properly leveraged, have shown strong performance in object detection. `[li2022raj]` explored the use of plain ViT backbones, pre-trained with MAE, for object detection. Their work, ViTDet, demonstrated that with minimal adaptations like a simple feature pyramid and limited cross-window propagation, these models could achieve competitive results on the COCO dataset, reaching up to 61.3 AP\_box using only ImageNet-1K pre-training. This highlighted that the powerful representations learned by plain ViTs, especially through self-supervised pre-training, could be effectively fine-tuned for localization tasks, challenging the strict necessity of hierarchical backbones for all detection scenarios. The convergence of ViT backbones and end-to-end Transformer detectors is further exemplified by `[song2022y4v]`, which introduced ViDT, an extendable and efficient object detector integrating Vision and Detection Transformers. ViDT reconfigures the Swin Transformer as a standalone detector and employs an efficient Transformer decoder, achieving an excellent AP and latency trade-off on COCO. Furthermore, the advancements in large-scale ViT pre-training, such as those in \cite{ICLR2023} and \cite{ICLR2023}, implicitly provide even more robust and generalizable backbones for object detection, allowing downstream detection models to achieve higher accuracy and better generalization across diverse scenarios.

In conclusion, Vision Transformers have established themselves as formidable architectures for core vision tasks. From their initial success in image classification, overcoming challenges related to data efficiency and architectural depth through innovations like DeiT and MAE, to their subsequent adaptation for object detection via end-to-end Transformer designs (DETR) and hierarchical backbones (Swin Transformer), ViTs have consistently pushed performance boundaries. While initial pure ViTs were data-hungry and lacked inherent inductive biases for dense prediction, the continuous evolution has led to more efficient, robust, and scalable models. The ongoing development of massive "Vision Foundation Models" and sophisticated hybrid architectures, leveraging advanced self-supervised learning, signifies a future where increasingly versatile visual intelligence can be deployed across an even wider spectrum of real-world applications, further solidifying ViTs' foundational strength in capturing both global context and fine-grained details.