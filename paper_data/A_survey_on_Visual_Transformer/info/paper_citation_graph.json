{
  "nodes": [
    {
      "id": "1970ace992d742bdf098de08a82817b05ef87477",
      "title": "Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing",
      "abstract": "Recently, vision transformer (ViT) based multimodal learning methods have been proposed to improve the robustness of face anti-spoofing (FAS) systems. However, there are still no works to explore the fundamental natures (e.g., modality-aware inputs, suitable multimodal pre-training, and efficient finetuning) in vanilla ViT for multimodal FAS. In this paper, we investigate three key factors (i.e., inputs, pre-training, and finetuning) in ViT for multimodal FAS with RGB, Infrared (IR), and Depth. First, in terms of the ViT inputs, we find that leveraging local feature descriptors (such as histograms of oriented gradients) benefits the ViT on IR modality but not RGB or Depth modalities. Second, in consideration of the task (FAS vs. generic object classification) and modality (multimodal vs. unimodal) gaps, ImageNet pre-trained models might be sub-optimal for the multimodal FAS task. Finally, in observation of the inefficiency on direct finetuning the whole or partial ViT, we design an adaptive multimodal adapter (AMA), which can efficiently aggregate local multimodal features while freezing majority of ViT parameters. To bridge these gaps, we propose the modality-asymmetric masked autoencoder (M$$^{2}$$\n \n \n 2\n \n A$$^{2}$$\n \n \n 2\n \n E) for multimodal FAS self-supervised pre-training without costly annotated labels. Compared with the previous modality-symmetric autoencoder, the proposed M$$^{2}$$\n \n \n 2\n \n A$$^{2}$$\n \n \n 2\n \n E is able to learn more intrinsic task-aware representation and compatible with modality-agnostic (e.g., unimodal, bimodal, and trimodal) downstream settings. Extensive experiments with both unimodal (RGB, Depth, IR) and multimodal (RGB+Depth, RGB+IR, Depth+IR, RGB+Depth+IR) settings conducted on multimodal FAS benchmarks demonstrate the superior performance of the proposed methods. One highlight is that the proposed method is robust under various missing-modality cases where previous multimodal FAS models suffer serious performance drops. We hope these findings and solutions can facilitate the future research for ViT-based multimodal FAS.",
      "authors": [
        "Zitong Yu",
        "Rizhao Cai",
        "Yawen Cui",
        "Xin Liu",
        "Yongjian Hu",
        "A. Kot"
      ],
      "year": 2023,
      "citation_count": 35,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1970ace992d742bdf098de08a82817b05ef87477",
      "pdf_link": "",
      "venue": "International Journal of Computer Vision",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5135a8f690c66c3b64928227443c4f9378bd20e1",
      "title": "A Lightweight Dual-Branch Swin Transformer for Remote Sensing Scene Classification",
      "abstract": "The main challenge of scene classification is to understand the semantic context information of high-resolution remote sensing images. Although vision transformer (ViT)-based methods have been explored to boost the long-range dependencies of high-resolution remote sensing images, the connectivity between neighboring windows is still limited. Meanwhile, ViT-based methods commonly contain a large number of parameters, resulting in a huge computational consumption. In this paper, a novel lightweight dual-branch swin transformer (LDBST) method for remote sensing scene classification is proposed, and the discriminative ability of scene features is increased through combining a ViT branch and convolutional neural network (CNN) branch. First, based on the hierarchical swin transformer model, LDBST divides the input features of each stage into two parts, which are then separately fed into the two branches. For the ViT branch, a dual multilayer perceptron structure with a depthwise convolutional layer, termed Conv-MLP, is integrated into the branch to boost the connections with neighboring windows. Then, a simple-structured CNN branch with maximum pooling preserves the strong features of the scene feature map. Specifically, the CNN branch lightens the LDBST, by avoiding complex multi-head attention and multilayer perceptron computations. To obtain better feature representation, LDBST was pretrained on the large-scale remote scene classification images of the MLRSN and RSD46-WHU datasets. These two pretrained weights were fine-tuned on target scene classification datasets. The experimental results showed that the proposed LDBST method was more effective than some other advanced remote sensing scene classification methods.",
      "authors": [
        "Fujian Zheng",
        "Shuai Lin",
        "Wei Zhou",
        "Hong Huang"
      ],
      "year": 2023,
      "citation_count": 29,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5135a8f690c66c3b64928227443c4f9378bd20e1",
      "pdf_link": "",
      "venue": "Remote Sensing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2e4dbc3dbd400346be60318ae558a0293e65ba81",
      "title": "TP-VIT: A Two-Pathway Vision Transformer for Video Action Recognition",
      "abstract": "Recently, inspired by the success of Transformer in natural language processing tasks, a number of works have attempted to apply Transformer-based models to video action recognition. Existing works only use one RGB stream as the input for Transformer. How to use multiple pathways and multiple streams with Transformer for action recognition has not been studied. To address this issue, we present a novel structure namely Two-Pathway Vision Transformer (TP-ViT). Two parallel spatial Transformer encoders are used as two pathways with different framerates and resolutions of the input video. The high-resolution pathway contains more spatial information, while the high-framerate pathway contains more temporal information. The two outputs are fused and fed into a temporal Transformer encoder for action recognition. Furthermore, we also fuse skeleton features into our model to get better results. Our experiments demonstrate that our proposed models achieve the state-of-the-art results on both the coarse-grained dataset Kinetics and the fine-grained dataset FineGym.",
      "authors": [
        "Yanhao Jing",
        "Feng Wang"
      ],
      "year": 2022,
      "citation_count": 14,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2e4dbc3dbd400346be60318ae558a0293e65ba81",
      "pdf_link": "",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "88589b0b2d2d8caa09d8ca94414343455ae87d7c",
      "title": "PolSAR-MPIformer: A Vision Transformer Based on Mixed Patch Interaction for Dual-Frequency PolSAR Image Adaptive Fusion Classification",
      "abstract": "Vision transformer (ViT) provides new ideas for polarization synthetic aperture radar (PolSAR) image classification due to its advantages in learning global-spatial information. However, the lack of local-spatial information within samples and correlation information among samples, as well as the complexity of network structure, limit the application of ViT in practice. In addition, dual-frequency PolSAR data provide rich information, but there are fewer related studies compared to single-frequency classification algorithms. In this article, we adopt ViT as the basic framework, and propose a novel model based on mixed patch interaction for dual-frequency PolSAR image adaptive fusion classification (PolSAR-MPIformer). First, a mixed patch interaction (MPI) module is designed for the feature extraction, which replaces the high-complexity self-attention in ViT with patch interaction intra- and intersample. Besides the global-spatial information learning within samples by ViT, the MPI module adds the learning of local-spatial information within samples and correlation information among samples, thereby obtaining more discriminative features through a low-complexity network. Subsequently, a dual-frequency adaptive fusion (DAF) module is constructed as the classifier of PolSAR-MPIformer. On the one hand, the attention mechanism is utilized in DAF to reduce the impact of speckle noise while preserving details. On the other hand, the DAF evaluates the classification confidence of each band and assigns different weights accordingly, which achieves reasonable utilization of the complementarity between dual-frequency data and improves classification accuracy. Experiments on four real dual-frequency PolSAR datasets substantiate the superiority of the proposed PolSAR-MPIformer over other state-of-the-art algorithms.",
      "authors": [
        "Xinyue Xin",
        "Ming Li",
        "Yan Wu",
        "Xiang Li",
        "Peng Zhang",
        "Dazhi Xu"
      ],
      "year": 2024,
      "citation_count": 7,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/88589b0b2d2d8caa09d8ca94414343455ae87d7c",
      "pdf_link": "",
      "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e37539f5c943a92ef56b49b7fa067bd976e418d4",
      "title": "Swin Unet3D: a three-dimensional medical image segmentation network combining vision transformer and convolution",
      "abstract": "Background Semantic segmentation of brain tumors plays a critical role in clinical treatment, especially for three-dimensional (3D) magnetic resonance imaging, which is often used in clinical practice. Automatic segmentation of the 3D structure of brain tumors can quickly help physicians understand the properties of tumors, such as the shape and size, thus improving the efficiency of preoperative planning and the odds of successful surgery. In past decades, 3D convolutional neural networks (CNNs) have dominated automatic segmentation methods for 3D medical images, and these network structures have achieved good results. However, to reduce the number of neural network parameters, practitioners ensure that the size of convolutional kernels in 3D convolutional operations generally does not exceed $$7 \\times 7 \\times 7$$ 7 × 7 × 7 , which also leads to CNNs showing limitations in learning long-distance dependent information. Vision Transformer (ViT) is very good at learning long-distance dependent information in images, but it suffers from the problems of many parameters. What’s worse, the ViT cannot learn local dependency information in the previous layers under the condition of insufficient data. However, in the image segmentation task, being able to learn this local dependency information in the previous layers makes a big impact on the performance of the model. Methods This paper proposes the Swin Unet3D model, which represents voxel segmentation on medical images as a sequence-to-sequence prediction. The feature extraction sub-module in the model is designed as a parallel structure of Convolution and ViT so that all layers of the model are able to adequately learn both global and local dependency information in the image. Results On the validation dataset of Brats2021, our proposed model achieves dice coefficients of 0.840, 0.874, and 0.911 on the ET channel, TC channel, and WT channel, respectively. On the validation dataset of Brats2018, our model achieves dice coefficients of 0.716, 0.761, and 0.874 on the corresponding channels, respectively. Conclusion We propose a new segmentation model that combines the advantages of Vision Transformer and Convolution and achieves a better balance between the number of model parameters and segmentation accuracy. The code can be found at https://github.com/1152545264/SwinUnet3D .",
      "authors": [
        "Yimin Cai",
        "Yuqing Long",
        "Zhenggong Han",
        "Mingkun Liu",
        "Yuchen Zheng",
        "Wei Yang",
        "Liming Chen"
      ],
      "year": 2023,
      "citation_count": 55,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e37539f5c943a92ef56b49b7fa067bd976e418d4",
      "pdf_link": "",
      "venue": "BMC Medical Informatics and Decision Making",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "9121dcd10df00e5cc51dc94400e0325e0ae47bb9",
      "title": "A 109-GOPs/W FPGA-Based Vision Transformer Accelerator With Weight-Loop Dataflow Featuring Data Reusing and Resource Saving",
      "abstract": "The Vision Transformer (ViT) models have demonstrated excellent performance in computer vision tasks, but a large amount of computation and memory access for massive matrix multiplications lead to degraded hardware performance compared to convolutional neural network (CNN). In this paper, we propose a ViT accelerator with a novel “Weight-Loop” dataflow and its computing unit, for efficient matrix multiplication computation. By data partitioning and rearrangement, the number of memory accesses and the number of registers are greatly reduced, and the adder trees are eliminated. A computation pipeline with the proposed dataflow scheduling method is constructed to maintain a high utilization rate through zero bubble switching. Moreover, a novel accurate dual INT8 multiply-accumulate (DI8MAC) method for DSP optimization is introduced to eliminate the additional correction circuits by weight encoding. Verified in the Xilinx XCZU9EG FPGA, the proposed ViT accelerator achieves the lowest inference latencies of 3.91 ms and 13.98 ms for ViT-S and ViT-B, respectively. The throughput of the accelerator can reach up to 2330.2 GOPs with an energy efficiency of 109 GOPs/W, showing a significant improvement compared to the state-of-the-art works.",
      "authors": [
        "Yueqi Zhang",
        "Lichen Feng",
        "Hongwei Shan",
        "Zhangming Zhu"
      ],
      "year": 2024,
      "citation_count": 8,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9121dcd10df00e5cc51dc94400e0325e0ae47bb9",
      "pdf_link": "",
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "174919e5a4ef95ff66440d56614ad954c6f27df1",
      "title": "ViT-HGR: Vision Transformer-based Hand Gesture Recognition from High Density Surface EMG Signals",
      "abstract": "Recently, there has been a surge of significant interest on application of Deep Learning (DL) models to autonomously perform hand gesture recognition using surface Electromyogram (sEMG) signals. Many of the existing DL models are, however, designed to be applied on sparse sEMG signals. Furthermore, due to the complex structure of these models, typically, we are faced with memory constraint issues, require large training times and a large number of training samples, and; there is the need to resort to data augmentation and/or transfer learning. In this paper, for the first time (to the best of our knowledge), we investigate and design a Vision Transformer (ViT) based architecture to perform hand gesture recognition from High Density (HD-sEMG) signals. Intuitively speaking, we capitalize on the recent breakthrough role of the transformer architecture in tackling different com-plex problems together with its potential for employing more input parallelization via its attention mechanism. The proposed Vision Transformer-based Hand Gesture Recognition (ViT-HGR) framework can overcome the aforementioned training time problems and can accurately classify a large number of hand gestures from scratch without any need for data augmentation and/or transfer learning. The efficiency of the proposed ViT-HGR framework is evaluated using a recently-released HD-sEMG dataset consisting of 65 isometric hand gestures. Our experiments with 64-sample (31.25 ms) window size yield average test accuracy of 84.62 ± 3.07%, where only 78,210 learnable parameters are utilized in the model. The compact structure of the proposed ViT-based ViT-HGR framework (i.e., having significantly reduced number of trainable parameters) shows great potentials for its practical application for prosthetic control.",
      "authors": [
        "Mansooreh Montazerin",
        "Soheil Zabihi",
        "E. Rahimian",
        "Arash Mohammadi",
        "Farnoosh Naderkhani"
      ],
      "year": 2022,
      "citation_count": 30,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/174919e5a4ef95ff66440d56614ad954c6f27df1",
      "pdf_link": "",
      "venue": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5572237909914e23758115be6b8d7f99a8bd51dc",
      "title": "ST-YOLOA: a Swin-transformer-based YOLO model with an attention mechanism for SAR ship detection under complex background",
      "abstract": "A synthetic aperture radar (SAR) image is crucial for ship detection in computer vision. Due to the background clutter, pose variations, and scale changes, it is a challenge to construct a SAR ship detection model with low false-alarm rates and high accuracy. Therefore, this paper proposes a novel SAR ship detection model called ST-YOLOA. First, the Swin Transformer network architecture and coordinate attention (CA) model are embedded in the STCNet backbone network to enhance the feature extraction performance and capture global information. Second, we used the PANet path aggregation network with a residual structure to construct the feature pyramid to increase global feature extraction capability. Next, to cope with the local interference and semantic information loss problems, a novel up/down-sampling method is proposed. Finally, the decoupled detection head is used to achieve the predicted output of the target position and the boundary box to improve convergence speed and detection accuracy. To demonstrate the efficiency of the proposed method, we have constructed three SAR ship detection datasets: a norm test set (NTS), a complex test set (CTS), and a merged test set (MTS). The experimental results show that our ST-YOLOA achieved an accuracy of 97.37%, 75.69%, and 88.50% on the three datasets, respectively, superior to the effects of other state-of-the-art methods. Our ST-YOLOA performs favorably in complex scenarios, and the accuracy is 4.83% higher than YOLOX on the CTS. Moreover, ST-YOLOA achieves real-time detection with a speed of 21.4 FPS.",
      "authors": [
        "Kai Zhao",
        "Ruitao Lu",
        "Siyu Wang",
        "Xiaogang Yang",
        "Qingge Li",
        "Jiwei Fan"
      ],
      "year": 2023,
      "citation_count": 15,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5572237909914e23758115be6b8d7f99a8bd51dc",
      "pdf_link": "",
      "venue": "Frontiers in Neurorobotics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "10e9943b3a974ac6175ffec3228e670ec9d2cc18",
      "title": "An Explainable Vision Transformer Model Based White Blood Cells Classification and Localization",
      "abstract": "White blood cells (WBCs) are crucial components of the immune system that play a vital role in defending the body against infections and diseases. The identification of WBCs subtypes is useful in the detection of various diseases, such as infections, leukemia, and other hematological malignancies. The manual screening of blood films is time-consuming and subjective, leading to inconsistencies and errors. Convolutional neural networks (CNN)-based models can automate such classification processes, but are incapable of capturing long-range dependencies and global context. This paper proposes an explainable Vision Transformer (ViT) model for automatic WBCs detection from blood films. The proposed model uses a self-attention mechanism to extract features from input images. Our proposed model was trained and validated on a public dataset of 16,633 samples containing five different types of WBCs. As a result of experiments on the classification of five different types of WBCs, our model achieved an accuracy of 99.40%. Moreover, the model’s examination of misclassified test samples revealed a correlation between incorrect predictions and the presence or absence of granules in the cell samples. To validate this observation, we divided the dataset into two classes, Granulocytes and Agranulocytes, and conducted a secondary training process. The resulting ViT model, trained for binary classification, achieved impressive performance metrics during the test phase, including an accuracy of 99.70%, recall of 99.54%, precision of 99.32%, and F-1 score of 99.43%. To ensure the reliability of the ViT model’s, we employed the Score-CAM algorithm to visualize the pixel areas on which the model focuses during its predictions. Our proposed method is suitable for clinical use due to its explainable structure as well as its superior performance compared to similar studies in the literature. The classification and localization of WBCs with this model can facilitate the detection and reporting process for the pathologist.",
      "authors": [
        "Oğuzhan Katar",
        "Ozal Yildirim"
      ],
      "year": 2023,
      "citation_count": 24,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/10e9943b3a974ac6175ffec3228e670ec9d2cc18",
      "pdf_link": "",
      "venue": "Diagnostics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e4add4391dfa2a806a50cc1fbe9a9696dac9501f",
      "title": "MPCViT: Searching for Accurate and Efficient MPC-Friendly Vision Transformer with Heterogeneous Attention",
      "abstract": "Secure multi-party computation (MPC) enables computation directly on encrypted data and protects both data and model privacy in deep learning inference. However, existing neural network architectures, including Vision Transformers (ViTs), are not designed or optimized for MPC and incur significant latency overhead. We observe Softmax accounts for the major latency bottleneck due to a high communication complexity, but can be selectively replaced or linearized without compromising the model accuracy. Hence, in this paper, we propose an MPC-friendly ViT, dubbed MPCViT, to enable accurate yet efficient ViT inference in MPC. Based on a systematic latency and accuracy evaluation of the Softmax attention and other attention variants, we propose a heterogeneous attention optimization space. We also develop a simple yet effective MPC-aware neural architecture search algorithm for fast Pareto optimization. To further boost the inference efficiency, we propose MPCViT+, to jointly optimize the Softmax attention and other network components, including GeLU, matrix multiplication, etc. With extensive experiments, we demonstrate that MPCViT achieves 1.9%, 1.3% and 3.6% higher accuracy with 6.2×, 2.9× and 1.9× latency reduction compared with baseline ViT, MPCFormer and THE-X on the Tiny-ImageNet dataset, respectively. MPCViT+ further achieves a better Pareto front compared with MPCViT. The code and models for evaluation are available at https://github.com/PKU-SEC-Lab/mpcvit.",
      "authors": [
        "Wenyuan Zeng",
        "Meng Li",
        "Wenjie Xiong",
        "Tong Tong",
        "Wen-jie Lu",
        "Jin Tan",
        "Runsheng Wang",
        "Ru Huang"
      ],
      "year": 2022,
      "citation_count": 26,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e4add4391dfa2a806a50cc1fbe9a9696dac9501f",
      "pdf_link": "",
      "venue": "IEEE International Conference on Computer Vision",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3c6980902883f03c37332d34ead343e1229062b3",
      "title": "Temporal Shift Module-Based Vision Transformer Network for Action Recognition",
      "abstract": "This paper introduces a novel action recognition model named ViT-Shift, which combines the Time Shift Module (TSM) with the Vision Transformer (ViT) architecture. Traditional video action recognition tasks face significant computational challenges, requiring substantial computing resources. However, our model successfully addresses this issue by incorporating the TSM, achieving outstanding performance while significantly reducing computational costs. Our approach is based on the latest Transformer self-attention mechanism, applied to video sequence processing instead of traditional convolutional methods. To preserve the core architecture of ViT and transfer its excellent performance in image recognition to video action recognition, we strategically introduce the TSM only before the multi-head attention layer of ViT. This design allows us to simulate temporal interactions using channel shifts, effectively reducing computational complexity. We carefully design the position and shift parameters of the TSM to maximize the model’s performance. Experimental results demonstrate that ViT-Shift achieves remarkable results on two standard action recognition datasets. With ImageNet-21K pretraining, we achieve an accuracy of 77.55% on the Kinetics-400 dataset and 93.07% on the UCF-101 dataset.",
      "authors": [
        "Kunpeng Zhang",
        "Mengyan Lyu",
        "Xinxin Guo",
        "Liye Zhang",
        "Cong Liu"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3c6980902883f03c37332d34ead343e1229062b3",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b1f0ac0b5e4044251dcdcce78e0046feb43cc7c7",
      "title": "LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition",
      "abstract": "The Vision Transformer (ViT) excels in accuracy when handling high-resolution images, yet it confronts the challenge of significant spatial redundancy, leading to increased computational and memory requirements. To address this, we present the Localization and Focus Vision Transformer (LF-ViT). This model operates by strategically curtailing computational demands without impinging on performance. In the Localization phase, a reduced-resolution image is processed; if a definitive prediction remains elusive, our pioneering Neighborhood Global Class Attention (NGCA) mechanism is triggered, effectively identifying and spotlighting class-discriminative regions based on initial findings. Subsequently, in the Focus phase, this designated region is used from the original image to enhance recognition. Uniquely, LF-ViT employs consistent parameters across both phases, ensuring seamless end-to-end optimization. Our empirical tests affirm LF-ViT's prowess: it remarkably decreases Deit-S's FLOPs by 63% and concurrently amplifies throughput twofold. Code of this project is at https://github.com/edgeai1/LF-ViT.git.",
      "authors": [
        "Youbing Hu",
        "Yun Cheng",
        "Anqi Lu",
        "Zhiqiang Cao",
        "Dawei Wei",
        "Jie Liu",
        "Zhijun Li"
      ],
      "year": 2024,
      "citation_count": 9,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b1f0ac0b5e4044251dcdcce78e0046feb43cc7c7",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "649b706ba282de4eb5a161137f80eb49ed84a0a8",
      "title": "UFO-ViT: High Performance Linear Vision Transformer without Softmax",
      "abstract": "Vision transformers have become one of the most important models for computer vision tasks. Although they outperform prior works, they require heavy computational resources on a scale that is quadratic to $N$. This is a major drawback of the traditional self-attention (SA) algorithm. Here, we propose the Unit Force Operated Vision Transformer (UFO-ViT), a novel SA mechanism that has linear complexity. The main approach of this work is to eliminate nonlinearity from the original SA. We factorize the matrix multiplication of the SA mechanism without complicated linear approximation. By modifying only a few lines of code from the original SA, the proposed models outperform most transformer-based models on image classification and dense prediction tasks on most capacity regimes.",
      "authors": [
        "Jeonggeun Song"
      ],
      "year": 2021,
      "citation_count": 21,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/649b706ba282de4eb5a161137f80eb49ed84a0a8",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e8dceb26166721014b8ecbd11fd212739c18d315",
      "title": "MambaVision: A Hybrid Mamba-Transformer Vision Backbone",
      "abstract": "We propose a novel hybrid Mamba-Transformer backbone, MambaVision, specifically tailored for vision applications. Our core contribution includes redesigning the Mamba formulation to enhance its capability for efficient modeling of visual features. Through a comprehensive ablation study, we demonstrate the feasibility of integrating Vision Transformers (ViT) with Mamba. Our results show that equipping the Mamba architecture with self-attention blocks in the final layers greatly improves its capacity to capture longrange spatial dependencies. Based on these findings, we introduce a family of MambaVision models with a hierarchical architecture to meet various design criteria. For classification on the ImageNet-1K dataset, MambaVision variants achieve state-of-the-art (SOTA) performance in terms of both Top-1 accuracy and throughput. In downstream tasks such as object detection, instance segmentation, and semantic segmentation on MS COCO and ADE20K datasets, MambaVision outperforms comparably sized backbones while demonstrating favorable performance. Code: https://github.com/NVlabs/MambaVision",
      "authors": [
        "Ali Hatamizadeh",
        "Jan Kautz"
      ],
      "year": 2024,
      "citation_count": 143,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e8dceb26166721014b8ecbd11fd212739c18d315",
      "pdf_link": "",
      "venue": "Computer Vision and Pattern Recognition",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b52844a746dafd8a5051cef49abbbda64a312605",
      "title": "When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism",
      "abstract": "Attention mechanism has been widely believed as the key to success of vision transformers (ViTs), since it provides a flexible and powerful way to model spatial relationships. However, is the attention mechanism truly an indispensable part of ViT? Can it be replaced by some other alternatives? To demystify the role of attention mechanism, we simplify it into an extremely simple case: ZERO FLOP and ZERO parameter. Concretely, we revisit the shift operation. It does not contain any parameter or arithmetic calculation. The only operation is to exchange a small portion of the channels between neighboring features. Based on this simple operation, we construct a new backbone network, namely ShiftViT, where the attention layers in ViT are substituted by shift operations. Surprisingly, ShiftViT works quite well in several mainstream tasks, e.g., classification, detection, and segmentation. The performance is on par with or even better than the strong baseline Swin Transformer. These results suggest that the attention mechanism might not be the vital factor that makes ViT successful. It can be even replaced by a zero-parameter operation. We should pay more attentions to the remaining parts of ViT in the future work. Code is available at github.com/microsoft/SPACH.",
      "authors": [
        "Guangting Wang",
        "Yucheng Zhao",
        "Chuanxin Tang",
        "Chong Luo",
        "Wenjun Zeng"
      ],
      "year": 2022,
      "citation_count": 78,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b52844a746dafd8a5051cef49abbbda64a312605",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1ec9b653475287e95fdaef2f5247f82a8376c56c",
      "title": "EQ-ViT: Algorithm-Hardware Co-Design for End-to-End Acceleration of Real-Time Vision Transformer Inference on Versal ACAP Architecture",
      "abstract": "While vision transformers (ViTs) have shown consistent progress in computer vision, deploying them for real-time decision-making scenarios (<1 ms) is challenging. Current computing platforms like CPUs, GPUs, or FPGA-based solutions struggle to meet this deterministic low-latency real-time requirement, even with quantized ViT models. Some approaches use pruning or sparsity to reduce the model size and latency, but this often results in accuracy loss. To address the aforementioned constraints, in this work, we propose EQ-ViT, an end-to-end acceleration framework with the novel algorithm and architecture co-design features to enable the real-time ViT acceleration on the AMD Versal adaptive compute acceleration platform (ACAP). The contributions are four-fold. First, we perform in-depth kernel-level performance profiling and analysis and explain the bottlenecks for the existing acceleration solutions on GPU, FPGA, and ACAP. Second, on the hardware level, we introduce a new spatial and heterogeneous accelerator architecture, the EQ-ViT architecture. This architecture leverages the heterogeneous features of ACAP, where both FPGA and artificial intelligence engines (AIEs) coexist on the same system-on-chip (SoC). Third, On the algorithm level, we create a comprehensive quantization-aware training strategy, the EQ-ViT algorithm. This strategy concurrently quantizes both the weights and activations into 8-bit integers, aiming to improve the accuracy rather than compromise it during quantization. Notably, the method also quantizes nonlinear functions for efficient hardware implementation. Fourth, we design the EQ-ViT automation framework to implement the EQ-ViT architecture for four different ViT applications on the AMD Versal ACAP VCK190 board, achieving accuracy improvement with 2.4%, and average speedups of 315.0, 3.39, 3.38, 14.92, 59.5, and $13.1\\times $ over computing solutions of Intel Xeon 8375C vCPU, Nvidia A10G, A100, Jetson AGX Orin GPUs, AMD ZCU102, and U250 FPGAs. The energy efficiency gains are 62.2, 15.33, 12.82, 13.31, 13.5, and $21.9\\times $ .",
      "authors": [
        "Peiyan Dong",
        "Jinming Zhuang",
        "Zhuoping Yang",
        "Shixin Ji",
        "Yanyu Li",
        "Dongkuan Xu",
        "Heng Huang",
        "Jingtong Hu",
        "Alex K. Jones",
        "Yiyu Shi",
        "Yanzhi Wang",
        "Peipei Zhou"
      ],
      "year": 2024,
      "citation_count": 13,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1ec9b653475287e95fdaef2f5247f82a8376c56c",
      "pdf_link": "",
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "635675452852e838644516e1eeefd1aaa8c8ac07",
      "title": "Dual Class Token Vision Transformer for Direction of Arrival Estimation in Low SNR",
      "abstract": "In this letter, we propose a deep learning-based method for the direction of arrival (DOA) estimation in the low signal-to-noise ratio (SNR) scenario. Specifically, the DOA estimation is modeled as a multi-label classification task, and a novel dual class token Vision Transformer (DCT-ViT) is designed to fit it. Different from the classical ViT architecture with a single class token, the DCT-ViT includes two class tokens which are located at the beginning and end of the latent vector sequence, respectively. This architecture enables enhanced information mining and feature extraction from the array signal data in order to improve the accuracy of DOA estimation. Furthermore, a single DCT-ViT model can accommodate different source numbers by leveraging a training dataset with different numbers of sources. Simulation results illustrate that our proposed method outperforms existing methods in the low SNR scenario, including classical model-based and other deep learning-based methods.",
      "authors": [
        "Yu Guo",
        "Zhi Zhang",
        "Yuzhen Huang"
      ],
      "year": 2024,
      "citation_count": 15,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/635675452852e838644516e1eeefd1aaa8c8ac07",
      "pdf_link": "",
      "venue": "IEEE Signal Processing Letters",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5b22bdc6aedf13d812509dd0f768353eb1469a79",
      "title": "Enhancing Autonomous Visual Perception in Challenging Environments: Bilateral Models with Vision Transformer and Multilayer Perceptron for Traversable Area Detection",
      "abstract": "The development of autonomous vehicles has grown significantly recently due to the promise of improving safety and productivity in cities and industries. The scene perception module has benefited from the latest advances in computer vision and deep learning techniques, allowing the creation of more accurate and efficient models. This study develops and evaluates semantic segmentation models based on a bilateral architecture to enhance the detection of traversable areas for autonomous vehicles on unstructured routes, particularly in datasets where the distinction between the traversable area and the surrounding ground is minimal. The proposed hybrid models combine Convolutional Neural Networks (CNNs), Vision Transformer (ViT), and Multilayer Perceptron (MLP) techniques, achieving a balance between precision and computational efficiency. The results demonstrate that these models outperform the base architectures in prediction accuracy, capturing distant details more effectively while maintaining real-time operational capabilities.",
      "authors": [
        "Claudio Urrea",
        "Maximiliano Vélez"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5b22bdc6aedf13d812509dd0f768353eb1469a79",
      "pdf_link": "",
      "venue": "Technologies",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "934942934a6a785e2a80daa6421fa79971558b89",
      "title": "BViT: Broad Attention-Based Vision Transformer",
      "abstract": "Recent works have demonstrated that transformer can achieve promising performance in computer vision, by exploiting the relationship among image patches with self-attention. They only consider the attention in a single feature layer, but ignore the complementarity of attention in different layers. In this article, we propose broad attention to improve the performance by incorporating the attention relationship of different layers for vision transformer (ViT), which is called BViT. The broad attention is implemented by broad connection and parameter-free attention. Broad connection of each transformer layer promotes the transmission and integration of information for BViT. Without introducing additional trainable parameters, parameter-free attention jointly focuses on the already available attention information in different layers for extracting useful information and building their relationship. Experiments on image classification tasks demonstrate that BViT delivers superior accuracy of 75.0%/81.6% top-1 accuracy on ImageNet with 5M/22M parameters. Moreover, we transfer BViT to downstream object recognition benchmarks to achieve 98.9% and 89.9% on CIFAR10 and CIFAR100, respectively, that exceed ViT with fewer parameters. For the generalization test, the broad attention in Swin Transformer, T2T-ViT and LVT also brings an improvement of more than 1%. To sum up, broad attention is promising to promote the performance of attention-based models. Code and pretrained models are available at https://github.com/DRL/BViT.",
      "authors": [
        "Nannan Li",
        "Yaran Chen",
        "Weifan Li",
        "Zixiang Ding",
        "Dong Zhao"
      ],
      "year": 2022,
      "citation_count": 27,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/934942934a6a785e2a80daa6421fa79971558b89",
      "pdf_link": "",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1d8ecf2c3a78e5dc4de9e4c41961771bbd0033b3",
      "title": "Training Object Detectors from Scratch: An Empirical Study in the Era of Vision Transformer",
      "abstract": "Modeling in computer vision has long been dominated by convolutional neural networks (CNNs). Recently, in light of the excellent performances of self-attention mech-anism in the language field, transformers tailored for visual data have drawn numerous attention and triumphed CNNs in various vision tasks. These vision transformers heavily rely on large-scale pre-training to achieve competitive accuracy, which not only hinders the freedom of architectural design in downstream tasks like object detection, but also causes learning bias and domain mismatch in the fine-tuning stages. To this end, we aim to get rid of the “pre-train & fine-tune” paradigm of vision transformer and train transformer based object detector from scratch. Some earlier work in the CNNs era have successfully trained CNNs based detectors without pre-training, unfortunately, their findings do not generalize well when the backbone is switched from CNNs to vision transformer. Instead of proposing a specific vision transformer based detector, in this work, our goal is to reveal the insights of training vision transformer based detectors from scratch. In particular, we expect those insights can help other re-searchers and practitioners, and inspire more interesting research in other fields, such as semantic segmentation, visual-linguistic pre-training, etc. One of the key findings is that both architectural changes and more epochs play critical roles in training vision transformer based detectors from scratch. Experiments on MS COCO datasets demonstrate that vision transformer based detectors trained from scratch can also achieve similar performances to their counterparts with ImageNet pre-training.",
      "authors": [
        "Weixiang Hong",
        "Jiangwei Lao",
        "Wang Ren",
        "Jian Wang",
        "Jingdong Chen"
      ],
      "year": 2022,
      "citation_count": 14,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1d8ecf2c3a78e5dc4de9e4c41961771bbd0033b3",
      "pdf_link": "",
      "venue": "Computer Vision and Pattern Recognition",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a4b728dbbf5afdc231afb95ad4e5c2ececdefc48",
      "title": "Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios",
      "abstract": "Due to the complex attention mechanisms and model design, most existing vision Transformers (ViTs) can not perform as efficiently as convolutional neural networks (CNNs) in realistic industrial deployment scenarios, e.g. TensorRT and CoreML. This poses a distinct challenge: Can a visual neural network be designed to infer as fast as CNNs and perform as powerful as ViTs? Recent works have tried to design CNN-Transformer hybrid architectures to address this issue, yet the overall performance of these works is far away from satisfactory. To end these, we propose a next generation vision Transformer for efficient deployment in realistic industrial scenarios, namely Next-ViT, which dominates both CNNs and ViTs from the perspective of latency/accuracy trade-off. In this work, the Next Convolution Block (NCB) and Next Transformer Block (NTB) are respectively developed to capture local and global information with deployment-friendly mechanisms. Then, Next Hybrid Strategy (NHS) is designed to stack NCB and NTB in an efficient hybrid paradigm, which boosts performance in various downstream tasks. Extensive experiments show that Next-ViT significantly outperforms existing CNNs, ViTs and CNN-Transformer hybrid architectures with respect to the latency/accuracy trade-off across various vision tasks. On TensorRT, Next-ViT surpasses ResNet by 5.5 mAP (from 40.4 to 45.9) on COCO detection and 7.7% mIoU (from 38.8% to 46.5%) on ADE20K segmentation under similar latency. Meanwhile, it achieves comparable performance with CSWin, while the inference speed is accelerated by 3.6x. On CoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on COCO detection and 3.5% mIoU (from 45.1% to 48.6%) on ADE20K segmentation under similar latency. Our code and models are made public at: https://github.com/bytedance/Next-ViT",
      "authors": [
        "Jiashi Li",
        "Xin Xia",
        "W. Li",
        "Huixia Li",
        "Xing Wang",
        "Xuefeng Xiao",
        "Rui Wang",
        "Minghang Zheng",
        "Xin Pan"
      ],
      "year": 2022,
      "citation_count": 168,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a4b728dbbf5afdc231afb95ad4e5c2ececdefc48",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3c14992a490cc31a7a38f5fab156c9da40a861d4",
      "title": "SUNet: Swin Transformer UNet for Image Denoising",
      "abstract": "Image restoration is a challenging ill-posed problem which also has been a long-standing issue. In the past few years, the convolution neural networks (CNNs) almost dominated the computer vision and had achieved considerable success in different levels of vision tasks including image restoration. However, recently the Swin Transformer-based model also shows impressive performance, even surpasses the CNN-based methods to become the state-of-the-art on high-level vision tasks. In this paper, we proposed a restoration model called SUNet which uses the Swin Transformer layer as our basic block and then is applied to UNet architecture for image denoising. The source code and pre-trained models are available at https://github.com/FanChiMao/SUNet.",
      "authors": [
        "Chi-Mao Fan",
        "Tsung-Jung Liu",
        "Kuan-Hsien Liu"
      ],
      "year": 2022,
      "citation_count": 139,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3c14992a490cc31a7a38f5fab156c9da40a861d4",
      "pdf_link": "",
      "venue": "International Symposium on Circuits and Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b2becca9911c155bf97656df8e5079ca76767ab9",
      "title": "A Timely Survey on Vision Transformer for Deepfake Detection",
      "abstract": "In recent years, the rapid advancement of deepfake technology has revolutionized content creation, lowering forgery costs while elevating quality. However, this progress brings forth pressing concerns such as infringements on individual rights, national security threats, and risks to public safety. To counter these challenges, various detection methodologies have emerged, with Vision Transformer (ViT)-based approaches showcasing superior performance in generality and efficiency. This survey presents a timely overview of ViT-based deepfake detection models, categorized into standalone, sequential, and parallel architectures. Furthermore, it succinctly delineates the structure and characteristics of each model. By analyzing existing research and addressing future directions, this survey aims to equip researchers with a nuanced understanding of ViT's pivotal role in deepfake detection, serving as a valuable reference for both academic and practical pursuits in this domain.",
      "authors": [
        "Zhikan Wang",
        "Zhongyao Cheng",
        "Jiajie Xiong",
        "Xun Xu",
        "Tianrui Li",
        "B. Veeravalli",
        "Xulei Yang"
      ],
      "year": 2024,
      "citation_count": 11,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b2becca9911c155bf97656df8e5079ca76767ab9",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0d1e4faa1580266a56ad62ac2fdd72ed0c0bbbe9",
      "title": "Temporally Efficient Vision Transformer for Video Instance Segmentation",
      "abstract": "Recently vision transformer has achieved tremendous success on image-level visual recognition tasks. To effectively and efficiently model the crucial temporal information within a video clip, we propose a Temporally Efficient Vision Transformer (TeViT) for video instance segmentation (VIS). Different from previous transformer-based VIS methods, TeViT is nearly convolution-free, which contains a transformer backbone and a query-based video instance segmentation head. In the backbone stage, we propose a nearly parameter-free messenger shift mechanism for early temporal context fusion. In the head stages, we propose a parameter-shared spatiotemporal query interaction mechanism to build the one-to-one correspondence between video instances and queries. Thus, TeViT fully utilizes both frame-level and instance-level temporal context information and obtains strong temporal modeling capacity with negligible extra computational cost. On three widely adopted VIS benchmarks, i.e., YouTube-VIS-2019, YouTube-VIS-2021, and OVIS, TeViT obtains state-of-the-art results and maintains high inference speed, e.g., 46.6 AP with 68.9 FPS on YouTube-VIS-2019. Code is available at https://github.com/hustvl/TeViT.",
      "authors": [
        "Shusheng Yang",
        "Xinggang Wang",
        "Yu Li",
        "Yuxin Fang",
        "Jiemin Fang",
        "Wenyu Liu",
        "Xun Zhao",
        "Ying Shan"
      ],
      "year": 2022,
      "citation_count": 73,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0d1e4faa1580266a56ad62ac2fdd72ed0c0bbbe9",
      "pdf_link": "",
      "venue": "Computer Vision and Pattern Recognition",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "dfdb2894d50e095ce97f994ed6cee38554c4c84f",
      "title": "Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer",
      "abstract": "The large pre-trained vision transformers (ViTs) have demonstrated remarkable performance on various visual tasks, but suffer from expensive computational and memory cost problems when deployed on resource-constrained devices. Among the powerful compression approaches, quantization extremely reduces the computation and memory consumption by low-bit parameters and bit-wise operations. However, low-bit ViTs remain largely unexplored and usually suffer from a significant performance drop compared with the real-valued counterparts. In this work, through extensive empirical analysis, we first identify the bottleneck for severe performance drop comes from the information distortion of the low-bit quantized self-attention map. We then develop an information rectification module (IRM) and a distribution guided distillation (DGD) scheme for fully quantized vision transformers (Q-ViT) to effectively eliminate such distortion, leading to a fully quantized ViTs. We evaluate our methods on popular DeiT and Swin backbones. Extensive experimental results show that our method achieves a much better performance than the prior arts. For example, our Q-ViT can theoretically accelerates the ViT-S by 6.14x and achieves about 80.9% Top-1 accuracy, even surpassing the full-precision counterpart by 1.0% on ImageNet dataset. Our codes and models are attached on https://github.com/YanjingLi0202/Q-ViT",
      "authors": [
        "Yanjing Li",
        "Sheng Xu",
        "Baochang Zhang",
        "Xianbin Cao",
        "Penglei Gao",
        "Guodong Guo"
      ],
      "year": 2022,
      "citation_count": 115,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/dfdb2894d50e095ce97f994ed6cee38554c4c84f",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
      "title": "SwinIR: Image Restoration Using Swin Transformer",
      "abstract": "Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by up to 0.14∼0.45dB, while the total number of parameters can be reduced by up to 67%.",
      "authors": [
        "Jingyun Liang",
        "Jie Cao",
        "Guolei Sun",
        "K. Zhang",
        "L. Gool",
        "R. Timofte"
      ],
      "year": 2021,
      "citation_count": 3340,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/7a9a708ca61c14886aa0dcd6d13dac7879713f5f",
      "pdf_link": "",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3798e7f16fe69c29307a9bab4f0f4d779478afc5",
      "title": "GenConViT: Deepfake Video Detection Using Generative Convolutional Vision Transformer",
      "abstract": "Deepfakes have raised significant concerns due to their potential to spread false information and compromise the integrity of digital media. Current deepfake detection models often struggle to generalize across a diverse range of deepfake generation techniques and video content. In this work, we propose a Generative Convolutional Vision Transformer (GenConViT) for deepfake video detection. Our model combines ConvNeXt and Swin Transformer models for feature extraction, and it utilizes an Autoencoder and Variational Autoencoder to learn from latent data distributions. By learning from the visual artifacts and latent data distribution, GenConViT achieves an improved performance in detecting a wide range of deepfake videos. The model is trained and evaluated on DFDC, FF++, TM, DeepfakeTIMIT, and Celeb-DF (v2) datasets. The proposed GenConViT model demonstrates strong performance in deepfake video detection, achieving high accuracy across the tested datasets. While our model shows promising results in deepfake video detection by leveraging visual and latent features, we demonstrate that further work is needed to improve its generalizability when encountering out-of-distribution data. Our model provides an effective solution for identifying a wide range of fake videos while preserving the integrity of media.",
      "authors": [
        "Deressa Wodajo Deressa",
        "Hannes Mareen",
        "Peter Lambert",
        "Solomon Atnafu",
        "Z. Akhtar",
        "Glenn Van Wallendael"
      ],
      "year": 2023,
      "citation_count": 13,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3798e7f16fe69c29307a9bab4f0f4d779478afc5",
      "pdf_link": "",
      "venue": "Applied Sciences",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e33434a141bb2881a2e60c518a0cda4feed3f19a",
      "title": "A vision transformer for emphysema classification using CT images",
      "abstract": "Objective. Emphysema is characterized by the destruction and permanent enlargement of the alveoli in the lung. According to visual CT appearance, emphysema can be divided into three subtypes: centrilobular emphysema (CLE), panlobular emphysema (PLE), and paraseptal emphysema (PSE). Automating emphysema classification can help precisely determine the patterns of lung destruction and provide a quantitative evaluation. Approach. We propose a vision transformer (ViT) model to classify the emphysema subtypes via CT images. First, large patches (61 × 61) are cropped from CT images which contain the area of normal lung parenchyma, CLE, PLE, and PSE. After resizing, the large patch is divided into small patches and these small patches are converted to a sequence of patch embeddings by flattening and linear embedding. A class embedding is concatenated to the patch embedding, and the positional embedding is added to the resulting embeddings described above. Then, the obtained embedding is fed into the transformer encoder blocks to generate the final representation. Finally, the learnable class embedding is fed to a softmax layer to classify the emphysema. Main results. To overcome the lack of massive data, the transformer encoder blocks (pre-trained on ImageNet) are transferred and fine-tuned in our ViT model. The average accuracy of the pre-trained ViT model achieves 95.95% in our lab’s own dataset, which is higher than that of AlexNet, Inception-V3, MobileNet-V2, ResNet34, and ResNet50. Meanwhile, the pre-trained ViT model outperforms the ViT model without the pre-training. The accuracy of our pre-trained ViT model is higher than or comparable to that by available methods for the public dataset. Significance. The results demonstrated that the proposed ViT model can accurately classify the subtypes of emphysema using CT images. The ViT model can help make an effective computer-aided diagnosis of emphysema, and the ViT method can be extended to other medical applications.",
      "authors": [
        "Yanan Wu",
        "Shouliang Qi",
        "Yu Sun",
        "Shuyue Xia",
        "Yudong Yao",
        "W. Qian"
      ],
      "year": 2021,
      "citation_count": 61,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e33434a141bb2881a2e60c518a0cda4feed3f19a",
      "pdf_link": "",
      "venue": "Physics in Medicine and Biology",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ac9cc0c28838a037e77f4e19433de170f47b3de9",
      "title": "Transformers for Vision: A Survey on Innovative Methods for Computer Vision",
      "abstract": "Transformers have emerged as a groundbreaking architecture in the field of computer vision, offering a compelling alternative to traditional convolutional neural networks (CNNs) by enabling the modeling of long-range dependencies and global context through self-attention mechanisms. Originally developed for natural language processing, transformers have now been successfully adapted for a wide range of vision tasks, leading to significant improvements in performance and generalization. This survey provides a comprehensive overview of the fundamental principles of transformer architectures, highlighting the core mechanisms such as self-attention, multi-head attention, and positional encoding that distinguish them from CNNs. We delve into the theoretical adaptations required to apply transformers to visual data, including image tokenization and the integration of positional embeddings. A detailed analysis of key transformer-based vision architectures such as ViT, DeiT, Swin Transformer, PVT, Twins, and CrossViT are presented, alongside their practical applications in image classification, object detection, video understanding, medical imaging, and cross-modal tasks. The paper further compares the performance of vision transformers with CNNs, examining their respective strengths, limitations, and the emergence of hybrid models. Finally, current challenges in deploying ViTs, such as computational cost, data efficiency, and interpretability, and explore recent advancements and future research directions including efficient architectures, self-supervised learning, and multimodal integration are discussed.",
      "authors": [
        "Vikas Hassija",
        "Balamurugan Palanisamy",
        "Arpita Chatterjee",
        "Arpita Mandal",
        "Debanshi Chakraborty",
        "Amit Pandey",
        "G. Chalapathi",
        "Dhruv Kumar"
      ],
      "year": 2025,
      "citation_count": 6,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ac9cc0c28838a037e77f4e19433de170f47b3de9",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "49030ae220c863e9b72ab380ecc749c9d0f0ad13",
      "title": "UIA-ViT: Unsupervised Inconsistency-Aware Method based on Vision Transformer for Face Forgery Detection",
      "abstract": "Intra-frame inconsistency has been proved to be effective for the generalization of face forgery detection. However, learning to focus on these inconsistency requires extra pixel-level forged location annotations. Acquiring such annotations is non-trivial. Some existing methods generate large-scale synthesized data with location annotations, which is only composed of real images and cannot capture the properties of forgery regions. Others generate forgery location labels by subtracting paired real and fake images, yet such paired data is difficult to collected and the generated label is usually discontinuous. To overcome these limitations, we propose a novel Unsupervised Inconsistency-Aware method based on Vision Transformer, called UIA-ViT, which only makes use of video-level labels and can learn inconsistency-aware feature without pixel-level annotations. Due to the self-attention mechanism, the attention map among patch embeddings naturally represents the consistency relation, making the vision Transformer suitable for the consistency representation learning. Based on vision Transformer, we propose two key components: Unsupervised Patch Consistency Learning (UPCL) and Progressive Consistency Weighted Assemble (PCWA). UPCL is designed for learning the consistency-related representation with progressive optimized pseudo annotations. PCWA enhances the final classification embedding with previous patch embeddings optimized by UPCL to further improve the detection performance. Extensive experiments demonstrate the effectiveness of the proposed method.",
      "authors": [
        "Wanyi Zhuang",
        "Qi Chu",
        "Zhentao Tan",
        "Qiankun Liu",
        "Haojie Yuan",
        "Changtao Miao",
        "Zixiang Luo",
        "Nenghai Yu"
      ],
      "year": 2022,
      "citation_count": 92,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/49030ae220c863e9b72ab380ecc749c9d0f0ad13",
      "pdf_link": "",
      "venue": "European Conference on Computer Vision",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "714e21409b8c4f7788ac8c93795249a4e45e51ce",
      "title": "Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights",
      "abstract": "Intrigued by the inherent ability of the human visual system to identify salient regions in complex scenes, attention mechanisms have been seamlessly integrated into various Computer Vision (CV) tasks. Building upon this paradigm, Vision Transformer (ViT) networks exploit attention mechanisms for improved efficiency. This review navigates the landscape of redesigned attention mechanisms within ViTs, aiming to enhance their performance. This paper provides a comprehensive exploration of techniques and insights for designing attention mechanisms, systematically reviewing recent literature in the field of CV. This survey begins with an introduction to the theoretical foundations and fundamental concepts underlying attention mechanisms. We then present a systematic taxonomy of various attention mechanisms within ViTs, employing redesigned approaches. A multi-perspective categorization is proposed based on their application, objectives, and the type of attention applied. The analysis includes an exploration of the novelty, strengths, weaknesses, and an in-depth evaluation of the different proposed strategies. This culminates in the development of taxonomies that highlight key properties and contributions. Finally, we gather the reviewed studies along with their available open-source implementations at our \\href{https://github.com/mindflow-institue/Awesome-Attention-Mechanism-in-Medical-Imaging}{GitHub}\\footnote{\\url{https://github.com/xmindflow/Awesome-Attention-Mechanism-in-Medical-Imaging}}. We aim to regularly update it with the most recent relevant papers.",
      "authors": [
        "Moein Heidari",
        "Reza Azad",
        "Sina Ghorbani Kolahi",
        "Ren'e Arimond",
        "Leon Niggemeier",
        "Alaa Sulaiman",
        "Afshin Bozorgpour",
        "Ehsan Khodapanah Aghdam",
        "A. Kazerouni",
        "I. Hacihaliloglu",
        "D. Merhof"
      ],
      "year": 2024,
      "citation_count": 9,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/714e21409b8c4f7788ac8c93795249a4e45e51ce",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c57467e652f3f9131b3e7e40c23059abe395f01d",
      "title": "SpectFormer: Frequency and Attention is what you need in a Vision Transformer",
      "abstract": "Vision transformers have been applied successfully for image recognition tasks. There have been either multi-headed self-attention based (ViT [12], DelT [54]) simi-lar to the original work in textual models or more re-cently based on spectral layers (Fnet [29], GFNet [46], AFNO [15]). We hypothesize that spectral layers cap-ture high-frequency information such as lines and edges, while attention layers capture token interactions. We inves-tigate this hypothesis through this work and observe that indeed mixing spectral and multi-headed attention layers provides a better transformer architecture. We thus pro-pose the novel Spectformer architecture for vision trans-formers that has initial spectral and deeper multi-headed attention layers. We believe that the resulting representation allows the transformer to capture the feature repre-sentation appropriately and it yields improved performance over other transformer representations. For instance, it im-proves the top-1 accuracy by 2% on ImageNet compared to both GFNet-H and LiT. SpectFormer-H-S reaches 84.25% top-1 accuracy on ImageNet-1 K (state of the art for small version). Further, Spectformer-H-L achieves 85.7% which is the state of the art for the comparable base version of the transformers. We further validated the SpectFormer per-formance in other scenarios such as transfer learning on standard datasets such as ClFAR-10, ClFAR-100, Oxford-lIlT-flower, and Standford Car datasets. We then investigate its use in downstream tasks such as object detection and instance segmentation on the MS-COCO dataset and ob-serve that Spectformer shows consistent performance that is comparable to the best backbones and can be further optimized and improved. The source code is available on this website https://github.com/badripatro/SpectFormers.",
      "authors": [
        "Badri N. Patro",
        "Vinay P. Namboodiri",
        "Vijay Srinivas Agneeswaran"
      ],
      "year": 2023,
      "citation_count": 70,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c57467e652f3f9131b3e7e40c23059abe395f01d",
      "pdf_link": "",
      "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "dc5d707a8f9efaf32d2319ec2ca8d2619ef2926e",
      "title": "An Efficient FPGA-Based Accelerator for Swin Transformer",
      "abstract": "Since introduced, Swin Transformer has achieved remarkable results in the field of computer vision, it has sparked the need for dedicated hardware accelerators, specifically catering to edge computing demands. For the advantages of flexibility, low power consumption, FPGAs have been widely employed to accelerate the inference of convolutional neural networks (CNNs) and show potential in Transformer-based models. Unlike CNNs, which mainly involve multiply and accumulate (MAC) operations, Transformer involve non-linear computations such as Layer Normalization (LN), Softmax, and GELU. These nonlinear computations do pose challenges for accelerator design. In this paper, to propose an efficient FPGA-based hardware accelerator for Swin Transformer, we focused on using different strategies to deal with these nonlinear calculations and efficiently handling MAC computations to achieve the best acceleration results. We replaced LN with BN, Given that Batch Normalization (BN) can be fused with linear layers during inference to optimize inference efficiency. The modified Swin-T, Swin-S, and Swin-B respectively achieved Top-1 accuracy rates of 80.7%, 82.7%, and 82.8% in ImageNet. Furthermore, We employed strategies for approximate computation to design hardware-friendly architectures for Softmax and GELU computations. We also designed an efficient Matrix Multiplication Unit to handle all linear computations in Swin Transformer. As a conclude, compared with CPU (AMD Ryzen 5700X), our accelerator achieved 1.76x, 1.66x, and 1.25x speedup and achieved 20.45x, 18.60x, and 14.63x energy efficiency (FPS/power consumption) improvement on Swin-T, Swin-S, and Swin-B models, respectively. Compared to GPU (Nvidia RTX 2080 Ti), we achieved 5.05x, 4.42x, and 3.00x energy efficiency improvement respectively. As far as we know, the accelerator we proposed is the fastest FPGA-based accelerator for Swin Transformer.",
      "authors": [
        "Zhiyang Liu",
        "Pengyu Yin",
        "Zhenhua Ren"
      ],
      "year": 2023,
      "citation_count": 6,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/dc5d707a8f9efaf32d2319ec2ca8d2619ef2926e",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "cc24f933b343b6a9701088cf6ae1dbf3299c0c9e",
      "title": "P2-ViT: Power-of-Two Post-Training Quantization and Acceleration for Fully Quantized Vision Transformer",
      "abstract": "Vision transformers (ViTs) have excelled in computer vision (CV) tasks but are memory-consuming and computation-intensive, challenging their deployment on resource-constrained devices. To tackle this limitation, prior works have explored ViT-tailored quantization algorithms but retained floating-point scaling factors, which yield nonnegligible requantization overhead, limiting ViTs’ hardware efficiency and motivating more hardware-friendly solutions. To this end, we propose P2-ViT, the first power-of-two (PoT) posttraining quantization (PTQ) and acceleration framework to accelerate fully quantized ViTs. Specifically, as for quantization, we explore a dedicated quantization scheme to effectively quantize ViTs with PoT scaling factors, thus minimizing the requantization overhead. Furthermore, we propose coarse-to-fine automatic mixed-precision quantization to enable better accuracy-efficiency tradeoffs. In terms of hardware, we develop a dedicated chunk-based accelerator featuring multiple tailored subprocessors to individually handle ViTs’ different types of operations, alleviating reconfigurable overhead. In addition, we design a tailored row-stationary dataflow to seize the pipeline processing opportunity introduced by our PoT scaling factors, thereby enhancing throughput. Extensive experiments consistently validate P2-ViT’s effectiveness. Particularly, we offer comparable or even superior quantization performance with PoT scaling factors when compared with the counterpart with floating-point scaling factors. Besides, we achieve up to <inline-formula> <tex-math notation=\"LaTeX\">$10.1\\times $ </tex-math></inline-formula> speedup and <inline-formula> <tex-math notation=\"LaTeX\">$36.8\\times $ </tex-math></inline-formula> energy saving over GPU’s Turing Tensor Cores, and up to <inline-formula> <tex-math notation=\"LaTeX\">$1.84\\times $ </tex-math></inline-formula> higher computation utilization efficiency against SOTA quantization-based ViT accelerators. Codes are available at <uri>https://github.com/shihuihong214/P2-ViT</uri>.",
      "authors": [
        "Huihong Shi",
        "Xin Cheng",
        "Wendong Mao",
        "Zhongfeng Wang"
      ],
      "year": 2024,
      "citation_count": 7,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/cc24f933b343b6a9701088cf6ae1dbf3299c0c9e",
      "pdf_link": "",
      "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a09cbcaac305884f043810afc4fa4053099b5970",
      "title": "Exploring Plain Vision Transformer Backbones for Object Detection",
      "abstract": "We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors. Code for ViTDet is available in Detectron2.",
      "authors": [
        "Yanghao Li",
        "Hanzi Mao",
        "Ross B. Girshick",
        "Kaiming He"
      ],
      "year": 2022,
      "citation_count": 918,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a09cbcaac305884f043810afc4fa4053099b5970",
      "pdf_link": "",
      "venue": "European Conference on Computer Vision",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0eff37167876356da2163b2e396df2719adf7de9",
      "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
      "abstract": "The recently developed vision transformer (ViT) has achieved promising results on image classification compared to convolutional neural networks. Inspired by this, in this paper, we study how to learn multi-scale feature representations in transformer models for image classification. To this end, we propose a dual-branch transformer to com-bine image patches (i.e., tokens in a transformer) of different sizes to produce stronger image features. Our approach processes small-patch and large-patch tokens with two separate branches of different computational complexity and these tokens are then fused purely by attention multiple times to complement each other. Furthermore, to reduce computation, we develop a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches. Our proposed cross-attention only requires linear time for both computational and memory complexity instead of quadratic time otherwise. Extensive experiments demonstrate that our approach performs better than or on par with several concurrent works on vision transformer, in addition to efficient CNN models. For example, on the ImageNet1K dataset, with some architectural changes, our approach outperforms the recent DeiT by a large margin of 2% with a small to moderate increase in FLOPs and model parameters. Our source codes and models are available at https://github.com/IBM/CrossViT.",
      "authors": [
        "Chun-Fu Chen",
        "Quanfu Fan",
        "Rameswar Panda"
      ],
      "year": 2021,
      "citation_count": 1640,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0eff37167876356da2163b2e396df2719adf7de9",
      "pdf_link": "",
      "venue": "IEEE International Conference on Computer Vision",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "6f4093a7ad5378e8cd3b73a52fbec80b784c107d",
      "title": "Multi-Dimensional Vision Transformer Compression via Dependency Guided Gaussian Process Search",
      "abstract": "Vision transformers (ViT) have recently attracted considerable attentions, but the huge computational cost remains an issue for practical deployment. Previous ViT pruning methods tend to prune the model along one dimension solely, which may suffer from excessive reduction and lead to sub-optimal model quality. In contrast, we advocate a multi-dimensional ViT compression paradigm, and propose to harness the redundancy reduction from attention head, neuron and sequence dimensions jointly. Firstly, we propose a statistical dependence based pruning criterion that is generalizable to different dimensions for identifying the deleterious components. Moreover, we cast the multidimensional ViT compression as an optimization problem, objective of which is to learn an optimal pruning policy across the three dimensions while maximizing the compressed model’s accuracy under a computational budget. The problem is solved by an adapted Gaussian process search with expected improvement. Experimental results show that our method effectively reduces the computational cost of various ViT models. For example, our method reduces 40% FLOPs without top-1 accuracy loss for DeiT and T2T-ViT models on the ImageNet dataset, outperforming previous state-of-the-art ViT pruning methods.",
      "authors": [
        "Zejiang Hou",
        "S. Kung"
      ],
      "year": 2022,
      "citation_count": 17,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/6f4093a7ad5378e8cd3b73a52fbec80b784c107d",
      "pdf_link": "",
      "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3efcd3a4c54694a093886981d59e3cffe0dd7149",
      "title": "SwinGD: A Robust Grape Bunch Detection Model Based on Swin Transformer in Complex Vineyard Environment",
      "abstract": "Accurate recognition of fruits in the orchard is an important step for robot picking in the natural environment, since many CNN models have a low recognition rate when dealing with irregularly shaped and very dense fruits, such as a grape bunch. It is a new trend to use a transformer structure and apply it to a computer vision domain for image processing. This paper provides Swin Transformer and DETR models to achieve grape bunch detection. Additionally, they are compared with traditional CNN models, such as Faster-RCNN, SSD, and YOLO. In addition, the optimal number of stages for a Swin Transformer through experiments is selected. Furthermore, the latest YOLOX model is also used to make a comparison with the Swin Transformer, and the experimental results show that YOLOX has higher accuracy and better detection effect. The above models are trained under red grape datasets collected under natural light. In addition, the dataset is expanded through image data augmentation to achieve a better training effect. After 200 epochs of training, SwinGD obtained an exciting mAP value of 94% when IoU = 0.5. In case of overexposure, overdarkness, and occlusion, SwinGD can recognize more accurately and robustly compared with other models. At the same time, SwinGD still has a better effect when dealing with dense grape bunches. Furthermore, 100 pictures of grapes containing 655 grape bunches are downloaded from Baidu pictures to detect the effect. The Swin Transformer has an accuracy of 91.5%. In order to verify the universality of SwinGD, we conducted a test under green grape images. The experimental results show that SwinGD has a good effect in practical application. The success of SwinGD provides a new solution for precision harvesting in agriculture.",
      "authors": [
        "Jinhai Wang",
        "Zongyin Zhang",
        "Lufeng Luo",
        "Wenbo Zhu",
        "Jianwen Chen",
        "Wen Wang"
      ],
      "year": 2021,
      "citation_count": 49,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3efcd3a4c54694a093886981d59e3cffe0dd7149",
      "pdf_link": "",
      "venue": "Horticulturae",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "cec0cbc2dd6d7975714110632b6bfcb5c1927ec3",
      "title": "Attention Distillation: self-supervised vision transformer students need more guidance",
      "abstract": "Self-supervised learning has been widely applied to train high-quality vision transformers. Unleashing their excellent performance on memory and compute constraint devices is therefore an important research topic. However, how to distill knowledge from one self-supervised ViT to another has not yet been explored. Moreover, the existing self-supervised knowledge distillation (SSKD) methods focus on ConvNet based architectures are suboptimal for ViT knowledge distillation. In this paper, we study knowledge distillation of self-supervised vision transformers (ViT-SSKD). We show that directly distilling information from the crucial attention mechanism from teacher to student can significantly narrow the performance gap between both. In experiments on ImageNet-Subset and ImageNet-1K, we show that our method AttnDistill outperforms existing self-supervised knowledge distillation (SSKD) methods and achieves state-of-the-art k-NN accuracy compared with self-supervised learning (SSL) methods learning from scratch (with the ViT-S model). We are also the first to apply the tiny ViT-T model on self-supervised learning. Moreover, AttnDistill is independent of self-supervised learning algorithms, it can be adapted to ViT based SSL methods to improve the performance in future research. The code is here: https://github.com/wangkai930418/attndistill",
      "authors": [
        "Kai Wang",
        "Fei Yang",
        "Joost van de Weijer"
      ],
      "year": 2022,
      "citation_count": 19,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/cec0cbc2dd6d7975714110632b6bfcb5c1927ec3",
      "pdf_link": "",
      "venue": "British Machine Vision Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d80166681f3344a1946b8bfc623f4679d979ee10",
      "title": "FSwin Transformer: Feature-Space Window Attention Vision Transformer for Image Classification",
      "abstract": "The vision transformer (ViT) with global self-attention exhibits quadratic computational complexity that depends on the image size. To address this issue, window-based self-attention ViT limits attention area to a specific window, thereby mitigating the computational complexity. However, it cannot effectively capture the relationships between windows. The Swin Transformer, a representative window-based self-attention ViT, introduces shifted-window multi-head self-attention (SW-MSA) to capture the cross-window information. However, SW-MSA groups tokens that are close to each other in the image into one window and thus cannot capture relationships between distant tokens. Therefore, this paper introduces a feature-space window attention transformer (FSwin Transformer) that includes distant but similar tokens in one window. The proposed FSwin Transformer clusters similar tokens based on the feature space and conducts self-attention within the cluster. Thus, this approach helps understand the global context of the image by compensating for interactions between long-distance tokens, which cannot be captured when windows are set based on the image space. In addition, we incorporate a feature-space refinement method with channel and spatial attention to emphasize key parts and suppress non-essential parts. The refined feature map improves the representation power of the model, resulting in improved classification performance. Consequently, in classification tasks for ImageNet-1K, FSwin Transformer outperforms existing Transformer-based backbones, including the Swin Transformer.",
      "authors": [
        "Dayeon Yoo",
        "Jeesu Kim",
        "Jinwoo Yoo"
      ],
      "year": 2024,
      "citation_count": 8,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d80166681f3344a1946b8bfc623f4679d979ee10",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c064efa0419b75ba131ec8470ed80f01e1a80f64",
      "title": "Global-Local Attention-Based Butterfly Vision Transformer for Visualization-Based Malware Classification",
      "abstract": "In recent studies, convolutional neural networks (CNNs) are mostly used as dynamic techniques for visualization-based malware classification and detection. Though vision transformer (ViT) proved its efficiency in image classification, a few of the earlier studies developed a ViT-based malware classifier. This paper proposes a butterfly construction-based vision transformer (B_ViT) model for visualization-based malware classification and detection. B_ViT has four phases: (1) image partitioning and patches embeddings; (2) local attention; (3) global attention; and (4) training and malware classification. B_ViT is an enhanced ViT architecture that supports the parallel processing of image patches and captures local and global spatial representations of malware images. B_ViT is a transfer learning-based model that uses a pre-trained ViT model on the ImageNet dataset to initialize the training parameters of transformers. Four B_ViT variants are experimented and evaluated on grayscale malware images collected from MalImg, Microsoft BIG datasets or converted from portable executable imports. The experiments show that B_ViT variants outperform the Input Enhanced vision transformer (IEViT) and ViT variants, achieving an accuracy equal to 99.49% and 99.99% for malware classification and detection respectively. The experiments also show that B_ViT is time effective for malware classification and detection where the average speed-up of B_ViT variants over IEViT and ViT variants are equal to 2.42 and 1.81 respectively. The analysis proves the efficiency of texture-based malware detection as well as the resilience of B_ViT to polymorphic obfuscation. Finally, the proposed B_ViT-based malware classifier outperforms the CNN-based malware classification methods in well.",
      "authors": [
        "Mohamad Mulham Belal",
        "Dr. Divya Meena Sundaram"
      ],
      "year": 2023,
      "citation_count": 17,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c064efa0419b75ba131ec8470ed80f01e1a80f64",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "13f7a106bb3814ad1fab25fd1356e99e91f402d3",
      "title": "Q-ViT: Fully Differentiable Quantization for Vision Transformer",
      "abstract": "In this paper, we propose a fully differentiable quantization method for vision transformer (ViT) named as Q-ViT, in which both of the quantization scales and bit-widths are learnable parameters. Specifically, based on our observation that heads in ViT display different quantization robustness, we leverage head-wise bit-width to squeeze the size of Q-ViT while preserving performance. In addition, we propose a novel technique named switchable scale to resolve the convergence problem in the joint training of quantization scales and bit-widths. In this way, Q-ViT pushes the limits of ViT quantization to 3-bit without heavy performance drop. Moreover, we analyze the quantization robustness of every architecture component of ViT and show that the Multi-head Self-Attention (MSA) and the Gaussian Error Linear Units (GELU) are the key aspects for ViT quantization. This study provides some insights for further research about ViT quantization. Extensive experiments on different ViT models, such as DeiT and Swin Transformer show the effectiveness of our quantization method. In particular, our method outperforms the state-of-the-art uniform quantization method by 1.5% on DeiT-Tiny.",
      "authors": [
        "Zhexin Li",
        "Tong Yang",
        "Peisong Wang",
        "Jian Cheng"
      ],
      "year": 2022,
      "citation_count": 44,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/13f7a106bb3814ad1fab25fd1356e99e91f402d3",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "bb5e6bbd4ad8c613a0bac3db23940c03ce94bec9",
      "title": "QAGA-Net: enhanced vision transformer-based object detection for remote sensing images",
      "abstract": "PurposeVision transformers (ViT) detectors excel in processing natural images. However, when processing remote sensing images (RSIs), ViT methods generally exhibit inferior accuracy compared to approaches based on convolutional neural networks (CNNs). Recently, researchers have proposed various structural optimization strategies to enhance the performance of ViT detectors, but the progress has been insignificant. We contend that the frequent scarcity of RSI samples is the primary cause of this problem, and model modifications alone cannot solve it.Design/methodology/approachTo address this, we introduce a faster RCNN-based approach, termed QAGA-Net, which significantly enhances the performance of ViT detectors in RSI recognition. Initially, we propose a novel quantitative augmentation learning (QAL) strategy to address the sparse data distribution in RSIs. This strategy is integrated as the QAL module, a plug-and-play component active exclusively during the model’s training phase. Subsequently, we enhanced the feature pyramid network (FPN) by introducing two efficient modules: a global attention (GA) module to model long-range feature dependencies and enhance multi-scale information fusion, and an efficient pooling (EP) module to optimize the model’s capability to understand both high and low frequency information. Importantly, QAGA-Net has a compact model size and achieves a balance between computational efficiency and accuracy.FindingsWe verified the performance of QAGA-Net by using two different efficient ViT models as the detector’s backbone. Extensive experiments on the NWPU-10 and DIOR20 datasets demonstrate that QAGA-Net achieves superior accuracy compared to 23 other ViT or CNN methods in the literature. Specifically, QAGA-Net shows an increase in mAP by 2.1% or 2.6% on the challenging DIOR20 dataset when compared to the top-ranked CNN or ViT detectors, respectively.Originality/valueThis paper highlights the impact of sparse data distribution on ViT detection performance. To address this, we introduce a fundamentally data-driven approach: the QAL module. Additionally, we introduced two efficient modules to enhance the performance of FPN. More importantly, our strategy has the potential to collaborate with other ViT detectors, as the proposed method does not require any structural modifications to the ViT backbone.",
      "authors": [
        "Huaxiang Song",
        "Hanjun Xia",
        "Wenhui Wang",
        "Yang Zhou",
        "Wanbo Liu",
        "Qun Liu",
        "Jinling Liu"
      ],
      "year": 2024,
      "citation_count": 13,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/bb5e6bbd4ad8c613a0bac3db23940c03ce94bec9",
      "pdf_link": "",
      "venue": "International Journal of Intelligent Computing and Cybernetics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d2fce7480111d66a74caa801a236f71ab021c42c",
      "title": "Vision Transformer With Hybrid Shifted Windows for Gastrointestinal Endoscopy Image Classification",
      "abstract": "Automated classification of gastrointestinal endoscope images can help reduce the workload of doctors and improve the accuracy of diagnoses. The rapidly developed vision Transformer, represented by Swin Transformer, has become an impressive technique for medical image classification. However, Swin Transformer cannot capture the long-range dependency well in complex gastrointestinal endoscopy images. As a result, it fails to represent features of some widely-spread targets in digestive tract images, such as normal-z-line and esophagitis, effectively. To solve this problem, we propose a novel vision Transformer model based on hybrid shifted windows for digestive tract image classification, which can obtain both short-range and long-range dependency concurrently. Extensive experiments demonstrate the superiority of our method to the state-of-the-art methods with a classification accuracy of 95.42% on the Kvasir v2 dataset and a classification accuracy of 86.81% on the HyperKvasir dataset.",
      "authors": [
        "Wei Wang",
        "Xin Yang",
        "Jinhui Tang"
      ],
      "year": 2023,
      "citation_count": 30,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d2fce7480111d66a74caa801a236f71ab021c42c",
      "pdf_link": "",
      "venue": "IEEE transactions on circuits and systems for video technology (Print)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f3d0278649454f80ba52c966a979499ee33e26c2",
      "title": "Hyperspectral Image Classification Using Groupwise Separable Convolutional Vision Transformer Network",
      "abstract": "Recently, vision transformer (ViT)-based deep learning (DL) models have achieved remarkable performance gains in hyperspectral image classification (HSIC) due to their abilities to model long-range dependencies and extract global spatial features. However, ViT is built with a stack of Transformer blocks and faces the challenge of learning a large number of parameters when processing hyperspectral data. Besides, the inherent modeling of global correlation in Transformer ignores the effective representation of local spatial and spectral features. To address these issues, we propose a lightweight ViT network known as groupwise separable convolutional ViT (GSC-ViT). First, a groupwise separable convolution (GSC) module, which is a combination of grouped pointwise convolution (GPWC) and group convolution, is designed to significantly decrease the number of convolutional kernel parameters, and effectively capture local spectral–spatial information in hyperspectral image (HSI). Second, a groupwise separable multihead self-attention (GSSA) module is employed to substitute the conventional multihead self-attention (MSA) in ViT, in which the groupwise self-attention (GSA) provides local spatial feature extraction, and the pointwise self-attention (PWSA) provides global spatial feature extraction. Third, a simple pointwise layer with enhanced skip connection mechanism is employed to substitute the multilayer perceptron (MLP) layer in all Transformer blocks of ViT, so as to eliminate unnecessary nonlinear transformations and facilitate the fusion of features derived from GSC and GSSA modules. Extensive experiments on four benchmark hyperspectral datasets reveal that our GSC-ViT can achieve surprising classification performance with relatively few training samples as compared with some existing HSIC approaches. The source code is available at https://github.com/flyzzie/TGRS-GSC-VIT.",
      "authors": [
        "Zhuoyi Zhao",
        "Xiang Xu",
        "Shutao Li",
        "A. Plaza"
      ],
      "year": 2024,
      "citation_count": 90,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f3d0278649454f80ba52c966a979499ee33e26c2",
      "pdf_link": "",
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e06b703146c46a6455fd0c33077de1bea5fdd877",
      "title": "Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
      "abstract": "Modern hierarchical vision transformers have added several vision-specific components in the pursuit of supervised classification performance. While these components lead to effective accuracies and attractive FLOP counts, the added complexity actually makes these transformers slower than their vanilla ViT counterparts. In this paper, we argue that this additional bulk is unnecessary. By pretraining with a strong visual pretext task (MAE), we can strip out all the bells-and-whistles from a state-of-the-art multi-stage vision transformer without losing accuracy. In the process, we create Hiera, an extremely simple hierarchical vision transformer that is more accurate than previous models while being significantly faster both at inference and during training. We evaluate Hiera on a variety of tasks for image and video recognition. Our code and models are available at https://github.com/facebookresearch/hiera.",
      "authors": [
        "Chaitanya K. Ryali",
        "Yuan-Ting Hu",
        "Daniel Bolya",
        "Chen Wei",
        "Haoqi Fan",
        "Po-Yao (Bernie) Huang",
        "Vaibhav Aggarwal",
        "Arkabandhu Chowdhury",
        "Omid Poursaeed",
        "Judy Hoffman",
        "J. Malik",
        "Yanghao Li",
        "Christoph Feichtenhofer"
      ],
      "year": 2023,
      "citation_count": 244,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e06b703146c46a6455fd0c33077de1bea5fdd877",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f462bb00b8c4379c4a4699b66a19ce10da530b08",
      "title": "Efficient Visual Transformer by Learnable Token Merging",
      "abstract": "Self-attention and transformers have been widely used in deep learning. Recent efforts have been devoted to incorporating transformer blocks into different neural architectures, including those with convolutions, leading to various visual transformers for computer vision tasks. In this paper, we propose a novel and compact transformer block, Transformer with Learnable Token Merging (LTM), or LTM-Transformer. LTM-Transformer performs token merging in a learnable scheme. LTM-Transformer is compatible with many popular and compact transformer networks, and it reduces the FLOPs and the inference time of the visual transformers while maintaining or even improving the prediction accuracy. In the experiments, we replace all the transformer blocks in popular visual transformers, including MobileViT, EfficientViT, ViT, and Swin, with LTM-Transformer blocks, leading to LTM-Transformer networks with different backbones. The LTM-Transformer is motivated by reduction of Information Bottleneck, and a novel and separable variational upper bound for the IB loss is derived. The architecture of the mask module in our LTM blocks which generates the token merging mask is designed to reduce the derived upper bound for the IB loss. Extensive results on computer vision tasks evidence that LTM-Transformer renders compact and efficient visual transformers with comparable or much better prediction accuracy than the original visual transformers.",
      "authors": [
        "Yancheng Wang",
        "Yingzhen Yang"
      ],
      "year": 2024,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f462bb00b8c4379c4a4699b66a19ce10da530b08",
      "pdf_link": "",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "46880aeca86695ca3117cc04f6bd9edaf088111b",
      "title": "Residual Swin Transformer Channel Attention Network for Image Demosaicing",
      "abstract": "Image demosaicing is problem of interpolating full-resolution color images from raw sensor (color filter array) data. During last decade, deep neural networks have been widely used in image restoration, and in particular, in demosaicing, attaining significant performance improvement. In recent years, vision transformers have been designed and successfully used in various computer vision applications. One of the recent methods of image restoration based on a Swin Transformer (ST), SwinIR, demonstrates state-of-the-art performance with a smaller number of parameters than neural network-based methods. Inspired by the success of SwinIR, we propose in this paper a novel Swin Transformer-based network for image demosaicing, called RSTCANet. To extract image features, RSTCANet stacks several residual Swin Transformer Channel Attention blocks (RSTCAB), introducing the channel attention for each two successive ST blocks. Extensive experiments demonstrate that RSTCANet outperforms state-of-the-art image demosaicing methods, and has a smaller number of parameters. The source code is available at https://github.com/xingwz/RSTCANet.",
      "authors": [
        "W. Xing",
        "K. Egiazarian"
      ],
      "year": 2022,
      "citation_count": 17,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/46880aeca86695ca3117cc04f6bd9edaf088111b",
      "pdf_link": "",
      "venue": "European Workshop on Visual Information Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "fec86abbb29b559c1eaff31428f5b59f8070bb67",
      "title": "Optimization of vision transformer-based detection of lung diseases from chest X-ray images",
      "abstract": "Background Recent advances in Vision Transformer (ViT)-based deep learning have significantly improved the accuracy of lung disease prediction from chest X-ray images. However, limited research exists on comparing the effectiveness of different optimizers for lung disease prediction within ViT models. This study aims to systematically evaluate and compare the performance of various optimization methods for ViT-based models in predicting lung diseases from chest X-ray images. Methods This study utilized a chest X-ray image dataset comprising 19,003 images containing both normal cases and six lung diseases: COVID-19, Viral Pneumonia, Bacterial Pneumonia, Middle East Respiratory Syndrome (MERS), Severe Acute Respiratory Syndrome (SARS), and Tuberculosis. Each ViT model (ViT, FastViT, and CrossViT) was individually trained with each optimization method (Adam, AdamW, NAdam, RAdam, SGDW, and Momentum) to assess their performance in lung disease prediction. Results When tested with ViT on the dataset with balanced-sample sized classes, RAdam demonstrated superior accuracy compared to other optimizers, achieving 95.87%. In the dataset with imbalanced sample size, FastViT with NAdam achieved the best performance with an accuracy of 97.63%. Conclusions We provide comprehensive optimization strategies for developing ViT-based model architectures, which can enhance the performance of these models for lung disease prediction from chest X-ray images. Supplementary Information The online version contains supplementary material available at 10.1186/s12911-024-02591-3.",
      "authors": [
        "Jinsol Ko",
        "Soyeon Park",
        "H. G. Woo"
      ],
      "year": 2024,
      "citation_count": 17,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/fec86abbb29b559c1eaff31428f5b59f8070bb67",
      "pdf_link": "",
      "venue": "BMC Medical Informatics Decis. Mak.",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "03384825d373aabe67c4288ef1eae4d1cf89dc00",
      "title": "ViA: A Novel Vision-Transformer Accelerator Based on FPGA",
      "abstract": "Since Google proposed Transformer in 2017, it has made significant natural language processing (NLP) development. However, the increasing cost is a large amount of calculation and parameters. Previous researchers designed and proposed some accelerator structures for transformer models in field-programmable gate array (FPGA) to deal with NLP tasks efficiently. Now, the development of Transformer has also affected computer vision (CV) and has rapidly surpassed convolution neural networks (CNNs) in various image tasks. And there are apparent differences between the image data used in CV and the sequence data in NLP. The details in the models contained with transformer units in these two fields are also different. The difference in terms of data brings about the problem of the locality. The difference in the model structure brings about the problem of path dependence, which is not noticed in the existing related accelerator design. Therefore, in this work, we propose the ViA, a novel vision transformer (ViT) accelerator architecture based on FPGA, to execute the transformer application efficiently and avoid the cost of these challenges. By analyzing the data structure in the ViT, we design an appropriate partition strategy to reduce the impact of data locality in the image and improve the efficiency of computation and memory access. Meanwhile, by observing the computing flow of the ViT, we use the half-layer mapping and throughput analysis to reduce the impact of path dependence caused by the shortcut mechanism and fully utilize hardware resources to execute the Transformer efficiently. Based on optimization strategies, we design two reuse processing engines with the internal stream, different from the previous overlap or stream design patterns. In the stage of the experiment, we implement the ViA architecture in Xilinx Alveo U50 FPGA and finally achieved ~5.2 times improvement of energy efficiency compared with NVIDIA Tesla V100, and 4–10 times improvement of performance compared with related accelerators based on FPGA, that obtained nearly 309.6 GOP/s computing performance in the peek.",
      "authors": [
        "Teng Wang",
        "Lei Gong",
        "Chao Wang",
        "Y. Yang",
        "Yingxue Gao",
        "Xuehai Zhou",
        "Huaping Chen"
      ],
      "year": 2022,
      "citation_count": 84,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/03384825d373aabe67c4288ef1eae4d1cf89dc00",
      "pdf_link": "",
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "53e5db85e2a7442f20670be2ae25019fcf9d27a2",
      "title": "Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention",
      "abstract": "Self-attention mechanism has been a key factor in the recent progress of Vision Transformer (ViT), which enables adaptive feature extraction from global contexts. However, existing self-attention methods either adopt sparse global attention or window attention to reduce the computation complexity, which may compromise the local feature learning or subject to some handcrafted designs. In contrast, local attention, which restricts the receptive field of each query to its own neighboring pixels, enjoys the benefits of both convolution and self-attention, namely local inductive bias and dynamic feature selection. Nevertheless, current local attention modules either use inefficient Im2Col function or rely on specific CUDA kernels that are hard to generalize to devices without CUDA support. In this paper, we propose a novel local attention module, Slide Attention, which leverages common convolution operations to achieve high efficiency, flexibility and generalizability. Specifically, we first re-interpret the column-based Im2Col function from a new row-based perspective and use Depthwise Convolution as an efficient substitution. On this basis, we propose a deformed shifting module based on the re-parameterization technique, which further relaxes the fixed key/value positions to deformed features in the local region. In this way, our module realizes the local attention paradigm in both efficient and flexible manner. Extensive experiments show that our slide attention module is applicable to a variety of advanced Vision Transformer models and compatible with various hardware devices, and achieves consistently improved performances on comprehensive benchmarks.",
      "authors": [
        "Xuran Pan",
        "Tianzhu Ye",
        "Zhuofan Xia",
        "S. Song",
        "Gao Huang"
      ],
      "year": 2023,
      "citation_count": 69,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/53e5db85e2a7442f20670be2ae25019fcf9d27a2",
      "pdf_link": "",
      "venue": "Computer Vision and Pattern Recognition",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "60b0f9af990349546f284dea666fbf52ebfa7004",
      "title": "When CNNs Meet Vision Transformer: A Joint Framework for Remote Sensing Scene Classification",
      "abstract": "Scene classification is an indispensable part of remote sensing image interpretation, and various convolutional neural network (CNN)-based methods have been explored to improve classification accuracy. Although they have shown good classification performance on high-resolution remote sensing (HRRS) images, discriminative ability of extracted features is still limited. In this letter, a high-performance joint framework combined CNNs and vision transformer (ViT) (CTNet) is proposed to further boost the discriminative ability of features for HRRS scene classification. The CTNet method contains two modules, including the stream of ViT (T-stream) and the stream of CNNs (C-stream). For the T-stream, flattened image patches are sent into pretrained ViT model to mine semantic features in HRRS images. To complement with T-stream, pretrained CNN is transferred to extract local structural features in the C-stream. Then, semantic features and structural features are concatenated to predict labels of unknown samples. Finally, a joint loss function is developed to optimize the joint model and increase the intraclass aggregation. The highest accuracies on the aerial image dataset (AID) and Northwestern Polytechnical University (NWPU)-RESISC45 datasets obtained by the CTNet method are 97.70% and 95.49%, respectively. The classification results reveal that the proposed method achieves high classification performance compared with other state-of-the-art (SOTA) methods.",
      "authors": [
        "Peifang Deng",
        "Kejie Xu",
        "Hong Huang"
      ],
      "year": 2021,
      "citation_count": 121,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/60b0f9af990349546f284dea666fbf52ebfa7004",
      "pdf_link": "",
      "venue": "IEEE Geoscience and Remote Sensing Letters",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "226fcbe55235d873bedb2fcf5b981bd5ec860d4f",
      "title": "SwinBTS: A Method for 3D Multimodal Brain Tumor Segmentation Using Swin Transformer",
      "abstract": "Brain tumor semantic segmentation is a critical medical image processing work, which aids clinicians in diagnosing patients and determining the extent of lesions. Convolutional neural networks (CNNs) have demonstrated exceptional performance in computer vision tasks in recent years. For 3D medical image tasks, deep convolutional neural networks based on an encoder–decoder structure and skip-connection have been frequently used. However, CNNs have the drawback of being unable to learn global and remote semantic information well. On the other hand, the transformer has recently found success in natural language processing and computer vision as a result of its usage of a self-attention mechanism for global information modeling. For demanding prediction tasks, such as 3D medical picture segmentation, local and global characteristics are critical. We propose SwinBTS, a new 3D medical picture segmentation approach, which combines a transformer, convolutional neural network, and encoder–decoder structure to define the 3D brain tumor semantic segmentation job as a sequence-to-sequence prediction challenge in this research. To extract contextual data, the 3D Swin Transformer is utilized as the network’s encoder and decoder, and convolutional operations are employed for upsampling and downsampling. Finally, we achieve segmentation results using an improved Transformer module that we built for increasing detail feature extraction. Extensive experimental results on the BraTS 2019, BraTS 2020, and BraTS 2021 datasets reveal that SwinBTS outperforms state-of-the-art 3D algorithms for brain tumor segmentation on 3D MRI scanned images.",
      "authors": [
        "Yun Jiang",
        "Yuan Zhang",
        "Xinyi Lin",
        "Jinkun Dong",
        "Tongtong Cheng",
        "Jing Liang"
      ],
      "year": 2022,
      "citation_count": 181,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/226fcbe55235d873bedb2fcf5b981bd5ec860d4f",
      "pdf_link": "",
      "venue": "Brain Science",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a4ef2c7c321dea04d8ea91ec1ce0710ebce4f848",
      "title": "A Deep Features Extraction Model Based on the Transfer Learning Model and Vision Transformer “TLMViT” for Plant Disease Classification",
      "abstract": "This paper proposes a novel approach for extracting deep features and classifying diseased plant leaves. The agriculture industry is negatively impacted by plant diseases causing crop and economic loss. Accurate and timely diagnosis is crucial for managing and controlling plant diseases, as traditional methods can be costly and time-consuming. Deep learning-based tools effectively detect plant diseases depending on the qualitative of extracted features. In this regard, a hybrid model for plant disease classification based on a Transfer Learning-based model followed by a vision transformer (TLMViT) is proposed. TLMViT has four stages: 1) data acquisition, where the PlantVillage and wheat datasets are used to train and evaluate the proposed model, 2) image augmentation to increase the number of training samples and overcome the overfitting issue, 3) leaf feature extraction by two consecutive phases: initial features extraction by using pre-trained based model and deep features extraction by using ViT model, and 4) classification by using MLP classifier. TLMViT is experimented with using five pre-trained-based models followed by ViT individually. TLMViT performs accurately in plant disease classification, obtaining 98.81% and 99.86% validation accuracy for VGG19 followed by the ViT model on PlantVillage and wheat datasets respectively. Moreover, TLMViT is compared with pre-trained-based architecture. The comparison result illustrates that TLMViT achieved an enhancement of 1.11% and 1.099% in validation accuracy, 2.576% and 2.92% in validation loss compared with the transfer learning-based model for PlantVillage and wheat datasets respectively. Thereby proposed model proves the efficiency of using ViT for extracting deep features from the leaf.",
      "authors": [
        "A. Tabbakh",
        "Soubhagya Sankar Barpanda"
      ],
      "year": 2023,
      "citation_count": 64,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a4ef2c7c321dea04d8ea91ec1ce0710ebce4f848",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3ae8c0b646ddce95ffd09da31c02ed6fdb744e90",
      "title": "BOAT: Bilateral Local Attention Vision Transformer",
      "abstract": "Vision Transformers achieved outstanding performance in many computer vision tasks. Early Vision Transformers such as ViT and DeiT adopt global self-attention, which is computationally expensive when the number of patches is large. To improve efficiency, recent Vision Transformers adopt local self-attention mechanisms, where self-attention is computed within local windows. Despite the fact that window-based local self-attention significantly boosts efficiency, it fails to capture the relationships between distant but similar patches in the image plane. To overcome this limitation of image-space local attention, in this paper, we further exploit the locality of patches in the feature space. We group the patches into multiple clusters using their features, and self-attention is computed within every cluster. Such feature-space local attention effectively captures the connections between patches across different local windows but still relevant. We propose a Bilateral lOcal Attention vision Transformer (BOAT), which integrates feature-space local attention with image-space local attention. We further integrate BOAT with both Swin and CSWin models, and extensive experiments on several benchmark datasets demonstrate that our BOAT-CSWin model clearly and consistently outperforms existing state-of-the-art CNN models and vision Transformers.",
      "authors": [
        "Tan Yu",
        "Gangming Zhao",
        "Ping Li",
        "Yizhou Yu"
      ],
      "year": 2022,
      "citation_count": 27,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3ae8c0b646ddce95ffd09da31c02ed6fdb744e90",
      "pdf_link": "",
      "venue": "British Machine Vision Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b48a85980deb5f1baa64d862b9f0e4e62124e4de",
      "title": "Bi-ViT: Pushing the Limit of Vision Transformer Quantization",
      "abstract": "Vision transformers (ViTs) quantization offers a promising prospect to facilitate deploying large pre-trained networks on resource-limited devices. Fully-binarized ViTs (Bi-ViT) that pushes the quantization of ViTs to its limit remain largely unexplored and a very challenging task yet, due to their unacceptable performance. Through extensive empirical analyses, we identify the severe drop in ViT binarization is caused by attention distortion in self-attention, which technically stems from the gradient vanishing and ranking disorder. To address these issues, we first introduce a learnable scaling factor to reactivate the vanished gradients and illustrate its effectiveness through theoretical and experimental analyses. We then propose a ranking-aware distillation method to rectify the disordered ranking in a teacher-student framework. Bi-ViT achieves significant improvements over popular DeiT and Swin backbones in terms of Top-1 accuracy and FLOPs. For example, with DeiT-Tiny and Swin-Tiny, our method significantly outperforms baselines by 22.1% and 21.4% respectively, while 61.5x and 56.1x theoretical acceleration in terms of FLOPs compared with real-valued counterparts on ImageNet. Our codes and models are attached on https://github.com/YanjingLi0202/Bi-ViT/ .",
      "authors": [
        "Yanjing Li",
        "Sheng Xu",
        "Mingbao Lin",
        "Xianbin Cao",
        "Chuanjian Liu",
        "Xiao Sun",
        "Baochang Zhang"
      ],
      "year": 2023,
      "citation_count": 16,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b48a85980deb5f1baa64d862b9f0e4e62124e4de",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "bbe5dfbecfd1bed7556b9c8269b0d363faa24973",
      "title": "Optimized Data Distribution Learning for Enhancing Vision Transformer‐Based Object Detection in Remote Sensing Images",
      "abstract": "Existing Vision Transformer (ViT)‐based object detection methods for remote sensing images (RSIs) face significant challenges due to the scarcity of RSI samples and the over‐reliance on enhancement strategies originally developed for natural images. This often leads to inconsistent data distributions between training and testing subsets, resulting in degraded model performance. In this study, we introduce an optimized data distribution learning (ODDL) strategy and develop an object detection framework based on the Faster R‐CNN architecture, named ODDL‐Net. The ODDL strategy begins with an optimized augmentation (OA) technique, overcoming the limitations of conventional data augmentation methods. Next, we propose an optimized mosaic algorithm (OMA), improving upon the shortcomings of traditional Mosaic augmentation techniques. Additionally, we introduce a feature fusion regularization (FFR) method, addressing the inherent limitations of classic feature pyramid networks. These innovations are integrated into three modular, plug‐and‐play components—namely, the OA, OMA, and FFR modules—ensuring that the ODDL strategy can be seamlessly incorporated into existing detection frameworks without requiring significant modifications. To evaluate the effectiveness of the proposed ODDL‐Net, we develop two variants based on different ViT architectures: the Next ViT (NViT) small model and the Swin Transformer (SwinT) tiny model, both used as detection backbones. Experimental results on the NWPU10, DIOR20, MAR20, and GLH‐Bridge datasets demonstrate that both variants of ODDL‐Net achieve impressive accuracy, surpassing 23 state‐of‐the‐art methods introduced since 2023. Specifically, ODDL‐Net‐NViT attained accuracies of 78.3% on the challenging DIOR20 dataset and 61.4% on the GLH‐Bridge dataset. Notably, this represents a substantial improvement of approximately 23% over the Faster R‐CNN‐ResNet50 baseline on the DIOR20 dataset. In conclusion, this study demonstrates that ViTs are well suited for high‐accuracy object detection in RSIs. Furthermore, it provides a straightforward solution for building ViT‐based detectors, offering a practical approach that requires little model modification.",
      "authors": [
        "Huaxiang Song",
        "Junping Xie",
        "Yunyang Wang",
        "Lihua Fu",
        "Yang Zhou",
        "Xing Zhou"
      ],
      "year": 2025,
      "citation_count": 11,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/bbe5dfbecfd1bed7556b9c8269b0d363faa24973",
      "pdf_link": "",
      "venue": "Photogrammetric Record",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ce79649b126dfe9e3cbeb1ecd64a80708bbd5538",
      "title": "Gait-ViT: Gait Recognition with Vision Transformer",
      "abstract": "Identifying an individual based on their physical/behavioral characteristics is known as biometric recognition. Gait is one of the most reliable biometrics due to its advantages, such as being perceivable at a long distance and difficult to replicate. The existing works mostly leverage Convolutional Neural Networks for gait recognition. The Convolutional Neural Networks perform well in image recognition tasks; however, they lack the attention mechanism to emphasize more on the significant regions of the image. The attention mechanism encodes information in the image patches, which facilitates the model to learn the substantial features in the specific regions. In light of this, this work employs the Vision Transformer (ViT) with an attention mechanism for gait recognition, referred to as Gait-ViT. In the proposed Gait-ViT, the gait energy image is first obtained by averaging the series of images over the gait cycle. The images are then split into patches and transformed into sequences by flattening and patch embedding. Position embedding, along with patch embedding, are applied on the sequence of patches to restore the positional information of the patches. Subsequently, the sequence of vectors is fed to the Transformer encoder to produce the final gait representation. As for the classification, the first element of the sequence is sent to the multi-layer perceptron to predict the class label. The proposed method obtained 99.93% on CASIA-B, 100% on OU-ISIR D and 99.51% on OU-LP, which exhibit the ability of the Vision Transformer model to outperform the state-of-the-art methods.",
      "authors": [
        "Jashila Nair Mogan",
        "C. Lee",
        "K. Lim",
        "K. Anbananthen"
      ],
      "year": 2022,
      "citation_count": 27,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ce79649b126dfe9e3cbeb1ecd64a80708bbd5538",
      "pdf_link": "",
      "venue": "Italian National Conference on Sensors",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "751b71158b7dcd2a7949e72a6ad8fb13657a401c",
      "title": "Visual Saliency Transformer",
      "abstract": "Existing state-of-the-art saliency detection methods heavily rely on CNN-based architectures. Alternatively, we rethink this task from a convolution-free sequence-to-sequence perspective and predict saliency by modeling long-range dependencies, which can not be achieved by convolution. Specifically, we develop a novel unified model based on a pure transformer, namely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient object detection (SOD). It takes image patches as inputs and leverages the transformer to propagate global contexts among image patches. Unlike conventional architectures used in Vision Transformer (ViT), we leverage multi-level token fusion and propose a new token upsampling method under the transformer framework to get high-resolution detection results. We also develop a token-based multi-task decoder to simultaneously perform saliency and boundary detection by introducing task-related tokens and a novel patch-task-attention mechanism. Experimental results show that our model outperforms existing methods on both RGB and RGB-D SOD benchmark datasets. Most importantly, our whole framework not only provides a new perspective for the SOD field but also shows a new paradigm for transformer-based dense prediction models. Code is available at https://github.com/nnizhang/VST.",
      "authors": [
        "Nian Liu",
        "Ni Zhang",
        "Kaiyuan Wan",
        "Junwei Han",
        "Ling Shao"
      ],
      "year": 2021,
      "citation_count": 391,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/751b71158b7dcd2a7949e72a6ad8fb13657a401c",
      "pdf_link": "",
      "venue": "IEEE International Conference on Computer Vision",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "23ce9c2814d6567efec884b7043977cefcb7602e",
      "title": "Computer vision classification detection of chicken parts based on optimized Swin-Transformer",
      "abstract": "ABSTRACT In order to achieve real-time classification and detection of various chicken parts, this study introduces an optimized Swin-Transformer method for the classification and detection of multiple chicken parts. It initially leverages the Transformer’s self-attention structure to capture more comprehensive high-level visual semantic information from chicken part images. The image enhancement technique was applied to the image in the preprocessing stage to enhance the feature information of the image, and the migration learning method was used to train and optimize the Swin-Transformer model on the enhanced chicken parts dataset for classification and detection of chicken parts. Furthermore, this model was compared to four commonly used models in object target detection tasks: YOLOV3-Darknet53, YOLOV3-MobileNetv3, SSD-MobileNetv3, and SSD-VGG16. The results indicated that the Swin-Transformer model outperforms these models with a higher mAP value by 1.62%, 2.13%, 5.26%, and 4.48%, accompanied by a reduction in detection time by 16.18 ms, 5.08 ms, 9.38 ms, and 23.48 ms, respectively. The method of this study fulfills the production line requirements while exhibiting superior performance and greater robustness compared to existing conventional methods. GRAPHICAL ABSTRACT",
      "authors": [
        "Xianhui Peng",
        "Chenchen Xu",
        "Peng Zhang",
        "Dandan Fu",
        "Yan Chen",
        "Zhigang Hu"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/23ce9c2814d6567efec884b7043977cefcb7602e",
      "pdf_link": "",
      "venue": "CyTA - Journal of Food",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "6bdafb965e94c5240db2c30f20c37c4b4dd0e451",
      "title": "ViTA: A Vision Transformer Inference Accelerator for Edge Applications",
      "abstract": "Vision Transformer models, such as ViT, Swin Transformer, and Transformer-in-Transformer, have recently gained significant traction in computer vision tasks due to their ability to capture the global relation between features which leads to superior performance. However, they are compute-heavy and difficult to deploy in resource-constrained edge devices. Existing hardware accelerators, including those for the closely-related BERT transformer models, do not target highly resource-constrained environments. In this paper, we address this gap and propose ViTA - a configurable hardware accelerator for inference of vision transformer models, targeting resource-constrained edge computing devices and avoiding repeated off-chip memory accesses. We employ a head-level pipeline and inter-layer MLP optimizations, and can support several commonly used vision transformer models with changes solely in our control logic. We achieve nearly 90% hardware utilization efficiency on most vision transformer models, report a power of 0.88W when synthesised with a clock of 150 MHz, and get reasonable frame rates - all of which makes ViTA suitable for edge applications.",
      "authors": [
        "Shashank Nag",
        "G. Datta",
        "Souvik Kundu",
        "N. Chandrachoodan",
        "P. Beerel"
      ],
      "year": 2023,
      "citation_count": 34,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/6bdafb965e94c5240db2c30f20c37c4b4dd0e451",
      "pdf_link": "",
      "venue": "International Symposium on Circuits and Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f27c847e2909f30745f4a3528b574f5acfd76ea7",
      "title": "Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization",
      "abstract": "Vision transformers (ViTs) are emerging with significantly improved accuracy in computer vision tasks. However, their complex architecture and enormous computation/storage demand impose urgent needs for new hardware accelerator design methodology. This work proposes an FPGA-aware automatic ViT acceleration framework based on the proposed mixed-scheme quantization. To the best of our knowledge, this is the first FPGA-based ViT acceleration framework exploring model quantization. Compared with state-of-the-art ViT quantization work (algorithmic approach only without hardware acceleration), our quantization achieves 0.47% to 1.36% higher Top-l accuracy under the same bit-width. Compared with the 32-bit floating-point baseline FPGA accelerator, our accelerator achieves around 5.6x improvement on the frame rate (i.e., 56.8 FPS vs. 10.0 FPS) with 0.71% accuracy drop on ImageNet dataset for DeiT-base.",
      "authors": [
        "Z. Li",
        "Mengshu Sun",
        "Alec Lu",
        "Haoyu Ma",
        "Geng Yuan",
        "Yanyue Xie",
        "Hao Tang",
        "Yanyu Li",
        "M. Leeser",
        "Zhangyang Wang",
        "Xue Lin",
        "Zhenman Fang"
      ],
      "year": 2022,
      "citation_count": 59,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f27c847e2909f30745f4a3528b574f5acfd76ea7",
      "pdf_link": "",
      "venue": "International Conference on Field-Programmable Logic and Applications",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0594eaa8dfe580678a2382aaf77ac3582c872a97",
      "title": "TRT-ViT: TensorRT-oriented Vision Transformer",
      "abstract": "We revisit the existing excellent Transformers from the perspective of practical application. Most of them are not even as efficient as the basic ResNets series and deviate from the realistic deployment scenario. It may be due to the current criterion to measure computation efficiency, such as FLOPs or parameters is one-sided, sub-optimal, and hardware-insensitive. Thus, this paper directly treats the TensorRT latency on the specific hardware as an efficiency metric, which provides more comprehensive feedback involving computational capacity, memory cost, and bandwidth. Based on a series of controlled experiments, this work derives four practical guidelines for TensorRT-oriented and deployment-friendly network design, e.g., early CNN and late Transformer at stage-level, early Transformer and late CNN at block-level. Accordingly, a family of TensortRT-oriented Transformers is presented, abbreviated as TRT-ViT. Extensive experiments demonstrate that TRT-ViT significantly outperforms existing ConvNets and vision Transformers with respect to the latency/accuracy trade-off across diverse visual tasks, e.g., image classification, object detection and semantic segmentation. For example, at 82.7% ImageNet-1k top-1 accuracy, TRT-ViT is 2.7$\\times$ faster than CSWin and 2.0$\\times$ faster than Twins. On the MS-COCO object detection task, TRT-ViT achieves comparable performance with Twins, while the inference speed is increased by 2.8$\\times$.",
      "authors": [
        "Xin Xia",
        "Jiashi Li",
        "Jie Wu",
        "Xing Wang",
        "Ming-Yu Wang",
        "Xuefeng Xiao",
        "Minghang Zheng",
        "Rui Wang"
      ],
      "year": 2022,
      "citation_count": 28,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0594eaa8dfe580678a2382aaf77ac3582c872a97",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "442b5ec3aad4b099e71d6203a62eb7ec7519544c",
      "title": "P2FEViT: Plug-and-Play CNN Feature Embedded Hybrid Vision Transformer for Remote Sensing Image Classification",
      "abstract": "Remote sensing image classification (RSIC) is a classical and fundamental task in the intelligent interpretation of remote sensing imagery, which can provide unique labeling information for each acquired remote sensing image. Thanks to the potent global context information extraction ability of the multi-head self-attention (MSA) mechanism, visual transformer (ViT)-based architectures have shown excellent capability in natural scene image classification. However, in order to achieve powerful RSIC performance, it is insufficient to capture global spatial information alone. Specifically, for fine-grained target recognition tasks with high inter-class similarity, discriminative and effective local feature representations are key to correct classification. In addition, due to the lack of inductive biases, the powerful global spatial context representation capability of ViT requires lengthy training procedures and large-scale pre-training data volume. To solve the above problems, a hybrid architecture of convolution neural network (CNN) and ViT is proposed to improve the RSIC ability, called P2FEViT, which integrates plug-and-play CNN features with ViT. In this paper, the feature representation capabilities of CNN and ViT applying for RSIC are first analyzed. Second, aiming to integrate the advantages of CNN and ViT, a novel approach embedding CNN features into the ViT architecture is proposed, which can make the model synchronously capture and fuse global context and local multimodal information to further improve the classification capability of ViT. Third, based on the hybrid structure, only a simple cross-entropy loss is employed for model training. The model can also have rapid and comfortable convergence with relatively less training data than the original ViT. Finally, extensive experiments are conducted on the public and challenging remote sensing scene classification dataset of NWPU-RESISC45 (NWPU-R45) and the self-built fine-grained target classification dataset called BIT-AFGR50. The experimental results demonstrate that the proposed P2FEViT can effectively improve the feature description capability and obtain outstanding image classification performance, while significantly reducing the high dependence of ViT on large-scale pre-training data volume and accelerating the convergence speed. The code and self-built dataset will be released at our webpages.",
      "authors": [
        "Guanqun Wang",
        "He Chen",
        "Liang Chen",
        "Yin Zhuang",
        "Shanghang Zhang",
        "T. Zhang",
        "Hao Dong",
        "Peng Gao"
      ],
      "year": 2023,
      "citation_count": 30,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/442b5ec3aad4b099e71d6203a62eb7ec7519544c",
      "pdf_link": "",
      "venue": "Remote Sensing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "50a260631a28bfed18eccf8ebfc75ff34917518f",
      "title": "Convolutional Bypasses Are Better Vision Transformer Adapters",
      "abstract": "The pretrain-then-finetune paradigm has been widely adopted in computer vision. But as the size of Vision Transformer (ViT) grows exponentially, the full finetuning becomes prohibitive in view of the heavier storage overhead. Motivated by parameter-efficient transfer learning (PETL) on language transformers, recent studies attempt to insert lightweight adaptation modules (e.g., adapter layers or prompt tokens) to pretrained ViT and only finetune these modules while the pretrained weights are frozen. However, these modules were originally proposed to finetune language models and did not take into account the prior knowledge specifically for visual tasks. In this paper, we propose to construct Convolutional Bypasses (Convpass) in ViT as adaptation modules, introducing only a small amount (less than 0.5% of model parameters) of trainable parameters to adapt the large ViT. Different from other PETL methods, Convpass benefits from the hard-coded inductive bias of convolutional layers and thus is more suitable for visual tasks, especially in the low-data regime. Experimental results on VTAB-1K benchmark and few-shot learning datasets show that Convpass outperforms current language-oriented adaptation modules, demonstrating the necessity to tailor vision-oriented adaptation modules for adapting vision models.",
      "authors": [
        "Shibo Jie",
        "Zhi-Hong Deng"
      ],
      "year": 2022,
      "citation_count": 150,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/50a260631a28bfed18eccf8ebfc75ff34917518f",
      "pdf_link": "",
      "venue": "European Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3dee43cea71d5988a72a914121f3455106f89cc7",
      "title": "Depth-Guided Vision Transformer With Normalizing Flows for Monocular 3D Object Detection",
      "abstract": "Monocular 3D object detection is challenging due to the lack of accurate depth information. Some methods estimate the pixel-wise depth maps from off-the-shelf depth estimators and then use them as an additional input to augment the RGB images. Depth-based methods attempt to convert estimated depth maps to pseudo-LiDAR and then use LiDAR-based object detectors or focus on the perspective of image and depth fusion learning. However, they demonstrate limited performance and efficiency as a result of depth inaccuracy and complex fusion mode with convolutions. Different from these approaches, our proposed depth-guided vision transformer with a normalizing flows (NF-DVT) network uses normalizing flows to build priors in depth maps to achieve more accurate depth information. Then we develop a novel Swin-Transformer-based backbone with a fusion module to process RGB image patches and depth map patches with two separate branches and fuse them using cross-attention to exchange information with each other. Furthermore, with the help of pixel-wise relative depth values in depth maps, we develop new relative position embeddings in the cross-attention mechanism to capture more accurate sequence ordering of input tokens. Our method is the first Swin-Transformer-based backbone architecture for monocular 3D object detection. The experimental results on the KITTI and the challenging Waymo Open datasets show the effectiveness of our proposed method and superior performance over previous counterparts.",
      "authors": [
        "C. Pan",
        "Junran Peng",
        "Zhaoxiang Zhang"
      ],
      "year": 2024,
      "citation_count": 9,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3dee43cea71d5988a72a914121f3455106f89cc7",
      "pdf_link": "",
      "venue": "IEEE/CAA Journal of Automatica Sinica",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "595adb75ddeb90760c79e89b76d99e55079e0708",
      "title": "Fine-Grained Visual Classification via Internal Ensemble Learning Transformer",
      "abstract": "Recently, vision transformers (ViTs) have been investigated in fine-grained visual recognition (FGVC) and are now considered state of the art. However, most ViT-based works ignore the different learning performances of the heads in the multi-head self-attention (MHSA) mechanism and its layers. To address these issues, in this paper, we propose a novel internal ensemble learning transformer (IELT) for FGVC. The proposed IELT involves three main modules: multi-head voting (MHV) module, cross-layer refinement (CLR) module, and dynamic selection (DS) module. To solve the problem of the inconsistent performances of multiple heads, we propose the MHV module, which considers all of the heads in each layer as weak learners and votes for tokens of discriminative regions as cross-layer feature based on the attention maps and spatial relationships. To effectively mine the cross-layer feature and suppress the noise, the CLR module is proposed, where the refined feature is extracted and the assist logits operation is developed for the final prediction. In addition, a newly designed DS module adjusts the token selection number at each layer by weighting their contributions of the refined feature. In this way, the idea of ensemble learning is combined with the ViT to improve fine-grained feature representation. The experiments demonstrate that our method achieves competitive results compared with the state of the art on five popular FGVC datasets.",
      "authors": [
        "Qin Xu",
        "Jiahui Wang",
        "Bo Jiang",
        "Bin Luo"
      ],
      "year": 2023,
      "citation_count": 76,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/595adb75ddeb90760c79e89b76d99e55079e0708",
      "pdf_link": "",
      "venue": "IEEE transactions on multimedia",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d203076c28587895aa344d088b2788dbab5e82a1",
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "abstract": "Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several specific subfields, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research.",
      "authors": [
        "Xiangtai Li",
        "Henghui Ding",
        "Wenwei Zhang",
        "Haobo Yuan",
        "Jiangmiao Pang",
        "Guangliang Cheng",
        "Kai Chen",
        "Ziwei Liu",
        "Chen Change Loy"
      ],
      "year": 2023,
      "citation_count": 194,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d203076c28587895aa344d088b2788dbab5e82a1",
      "pdf_link": "",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "401c8d72a9b275e88e6ba159d8d646cfb9f397aa",
      "title": "A free lunch from ViT: adaptive attention multi-scale fusion Transformer for fine-grained visual recognition",
      "abstract": "Learning subtle representation about object parts plays a vital role in fine-grained visual recognition (FGVR) field. The vision transformer (ViT) achieves promising results on computer vision due to its attention mechanism. Nonetheless, with the fixed size of patches in ViT, the class token in deep layer focuses on the global receptive field and cannot generate multi-granularity features for FGVR. To capture region attention without box annotations and compensate for ViT shortcomings in FGVR, we propose a novel method named Adaptive attention multi-scale Fusion Transformer (AFTrans). The Selective Attention Collection Module (SACM) in our approach leverages attention weights in ViT and filters them adaptively to correspond with the relative importance of input patches. The multiple scales (global and local) pipeline is supervised by our weights sharing encoder and can be easily trained end-to-end. Comprehensive experiments demonstrate that AFTrans can achieve SOTA performance on three published fine-grained benchmarks: CUB-200-2011, Stanford Dogs and iNat2017.",
      "authors": [
        "Yuan Zhang",
        "Jian Cao",
        "Ling Zhang",
        "Xiangcheng Liu",
        "Zhiyi Wang",
        "Feng Ling",
        "Weiqian Chen"
      ],
      "year": 2021,
      "citation_count": 53,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/401c8d72a9b275e88e6ba159d8d646cfb9f397aa",
      "pdf_link": "",
      "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ab70c5e1a338cb470ec39c22a4f10e0f19e61edd",
      "title": "CAT: Cross Attention in Vision Transformer",
      "abstract": "Since Transformer has found widespread use in NLP, the potential of Transformer in CV has been realized and has inspired many new approaches. However, the computation required for replacing word tokens with image patches for Transformer after the tokenization of the image is vast(e.g., ViT), which bottlenecks model training and inference. In this paper, we propose a new attention mechanism in Transformer termed Cross Attention, which alternates attention inner the image patch instead of the whole image to capture local information and apply attention between image patches which are divided from single-channel feature maps to capture global information. Both operations have less computation than standard self-attention in Transformer. Based on that, we build a hierarchical network called Cross Attention Transformer(CAT) for other vision tasks. Our model achieves 82.8% on ImageNet-1K, and improves the performance of other methods on COCO and ADE20K, illustrating that our network has the potential to serve as general backbones. The code and models are avalible at https://github.com/linhezheng19/CAT.",
      "authors": [
        "Hezheng Lin",
        "Xingyi Cheng",
        "Xiangyu Wu",
        "Fan Yang",
        "Dong Shen",
        "Zhongyuan Wang",
        "Qing Song",
        "Wei Yuan"
      ],
      "year": 2021,
      "citation_count": 182,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ab70c5e1a338cb470ec39c22a4f10e0f19e61edd",
      "pdf_link": "",
      "venue": "IEEE International Conference on Multimedia and Expo",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "39240f94c9915d9f9959c34b1dc68593894531e6",
      "title": "ConvNets vs. Transformers: Whose Visual Representations are More Transferable?",
      "abstract": "Vision transformers have attracted much attention from computer vision researchers as they are not restricted to the spatial inductive bias of ConvNets. However, although Transformer-based backbones have achieved much progress on ImageNet classification, it is still unclear whether the learned representations are as transferable as or even more transferable than ConvNets’ features. To address this point, we systematically investigate the transfer learning ability of ConvNets and vision transformers in 15 single-task and multi-task performance evaluations. We observe consistent advantages of Transformer-based backbones on 13 downstream tasks (out of 15), including but not limited to fine-grained classification, scene recognition (classification, segmentation and depth estimation), open-domain classification, face recognition, etc. More specifically, we find that two ViT models heavily rely on whole network fine-tuning to achieve performance gains while Swin Transformer does not have such a requirement. Moreover, vision transformers behave more robustly in multi-task learning, i.e., bringing more improvements when managing mutually beneficial tasks and reducing performance losses when tackling irrelevant tasks. We hope our discoveries can facilitate the exploration and exploitation of vision transformers in the future.",
      "authors": [
        "Hong-Yu Zhou",
        "Chi-Ken Lu",
        "Sibei Yang",
        "Yizhou Yu"
      ],
      "year": 2021,
      "citation_count": 55,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/39240f94c9915d9f9959c34b1dc68593894531e6",
      "pdf_link": "",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "42bd7d3ca2b66eca5609fe77078fe8951ab9fd5c",
      "title": "Interpretable Image Classification with Adaptive Prototype-based Vision Transformers",
      "abstract": "We present ProtoViT, a method for interpretable image classification combining deep learning and case-based reasoning. This method classifies an image by comparing it to a set of learned prototypes, providing explanations of the form ``this looks like that.''In our model, a prototype consists of \\textit{parts}, which can deform over irregular geometries to create a better comparison between images. Unlike existing models that rely on Convolutional Neural Network (CNN) backbones and spatially rigid prototypes, our model integrates Vision Transformer (ViT) backbones into prototype based models, while offering spatially deformed prototypes that not only accommodate geometric variations of objects but also provide coherent and clear prototypical feature representations with an adaptive number of prototypical parts. Our experiments show that our model can generally achieve higher performance than the existing prototype based models. Our comprehensive analyses ensure that the prototypes are consistent and the interpretations are faithful.",
      "authors": [
        "Chiyu Ma",
        "Jon Donnelly",
        "Wenjun Liu",
        "Soroush Vosoughi",
        "Cynthia Rudin",
        "Chaofan Chen"
      ],
      "year": 2024,
      "citation_count": 12,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/42bd7d3ca2b66eca5609fe77078fe8951ab9fd5c",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "981970d0f586761e7cdd978670c6a8f46990f514",
      "title": "DAT++: Spatially Dynamic Vision Transformer with Deformable Attention",
      "abstract": "Transformers have shown superior performance on various vision tasks. Their large receptive field endows Transformer models with higher representation power than their CNN counterparts. Nevertheless, simply enlarging the receptive field also raises several concerns. On the one hand, using dense attention in ViT leads to excessive memory and computational cost, and features can be influenced by irrelevant parts that are beyond the region of interests. On the other hand, the handcrafted attention adopted in PVT or Swin Transformer is data agnostic and may limit the ability to model long-range relations. To solve this dilemma, we propose a novel deformable multi-head attention module, where the positions of key and value pairs in self-attention are adaptively allocated in a data-dependent way. This flexible scheme enables the proposed deformable attention to dynamically focus on relevant regions while maintains the representation power of global attention. On this basis, we present Deformable Attention Transformer (DAT), a general vision backbone efficient and effective for visual recognition. We further build an enhanced version DAT++. Extensive experiments show that our DAT++ achieves state-of-the-art results on various visual recognition benchmarks, with 85.9% ImageNet accuracy, 54.5 and 47.0 MS-COCO instance segmentation mAP, and 51.5 ADE20K semantic segmentation mIoU.",
      "authors": [
        "Zhuofan Xia",
        "Xuran Pan",
        "Shiji Song",
        "Li Erran Li",
        "Gao Huang"
      ],
      "year": 2023,
      "citation_count": 34,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/981970d0f586761e7cdd978670c6a8f46990f514",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1b026103e33b4c9eb637bc6f34715e22636b3492",
      "title": "Facial Expression Recognition Based on Squeeze Vision Transformer",
      "abstract": "In recent image classification approaches, a vision transformer (ViT) has shown an excellent performance beyond that of a convolutional neural network. A ViT achieves a high classification for natural images because it properly preserves the global image features. Conversely, a ViT still has many limitations in facial expression recognition (FER), which requires the detection of subtle changes in expression, because it can lose the local features of the image. Therefore, in this paper, we propose Squeeze ViT, a method for reducing the computational complexity by reducing the number of feature dimensions while increasing the FER performance by concurrently combining global and local features. To measure the FER performance of Squeeze ViT, experiments were conducted on lab-controlled FER datasets and a wild FER dataset. Through comparative experiments with previous state-of-the-art approaches, we proved that the proposed method achieves an excellent performance on both types of datasets.",
      "authors": [
        "Sangwon Kim",
        "J. Nam",
        "ByoungChul Ko"
      ],
      "year": 2022,
      "citation_count": 39,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1b026103e33b4c9eb637bc6f34715e22636b3492",
      "pdf_link": "",
      "venue": "Italian National Conference on Sensors",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2fe2f849b94cf08b559226bc9d78adcaef5ef186",
      "title": "AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition",
      "abstract": "Pretraining Vision Transformers (ViTs) has achieved great success in visual recognition. A following scenario is to adapt a ViT to various image and video recognition tasks. The adaptation is challenging because of heavy computation and memory storage. Each model needs an independent and complete finetuning process to adapt to different tasks, which limits its transferability to different visual domains. To address this challenge, we propose an effective adaptation approach for Transformer, namely AdaptFormer, which can adapt the pre-trained ViTs into many different image and video tasks efficiently. It possesses several benefits more appealing than prior arts. Firstly, AdaptFormer introduces lightweight modules that only add less than 2% extra parameters to a ViT, while it is able to increase the ViT's transferability without updating its original pre-trained parameters, significantly outperforming the existing 100\\% fully fine-tuned models on action recognition benchmarks. Secondly, it can be plug-and-play in different Transformers and scalable to many visual tasks. Thirdly, extensive experiments on five image and video datasets show that AdaptFormer largely improves ViTs in the target domains. For example, when updating just 1.5% extra parameters, it achieves about 10% and 19% relative improvement compared to the fully fine-tuned models on Something-Something~v2 and HMDB51, respectively. Code is available at https://github.com/ShoufaChen/AdaptFormer.",
      "authors": [
        "Shoufa Chen",
        "Chongjian Ge",
        "Zhan Tong",
        "Jiangliu Wang",
        "Yibing Song",
        "Jue Wang",
        "Ping Luo"
      ],
      "year": 2022,
      "citation_count": 806,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2fe2f849b94cf08b559226bc9d78adcaef5ef186",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "99fbe810d4194684be03458fdfebacb12d8a5c4e",
      "title": "Self-Ensembling Vision Transformer (SEViT) for Robust Medical Image Classification",
      "abstract": "Vision Transformers (ViT) are competing to replace Convolutional Neural Networks (CNN) for various computer vision tasks in medical imaging such as classification and segmentation. While the vulnerability of CNNs to adversarial attacks is a well-known problem, recent works have shown that ViTs are also susceptible to such attacks and suffer significant performance degradation under attack. The vulnerability of ViTs to carefully engineered adversarial samples raises serious concerns about their safety in clinical settings. In this paper, we propose a novel self-ensembling method to enhance the robustness of ViT in the presence of adversarial attacks. The proposed Self-Ensembling Vision Transformer (SEViT) leverages the fact that feature representations learned by initial blocks of a ViT are relatively unaffected by adversarial perturbations. Learning multiple classifiers based on these intermediate feature representations and combining these predictions with that of the final ViT classifier can provide robustness against adversarial attacks. Measuring the consistency between the various predictions can also help detect adversarial samples. Experiments on two modalities (chest X-ray and fundoscopy) demonstrate the efficacy of SEViT architecture to defend against various adversarial attacks in the gray-box (attacker has full knowledge of the target model, but not the defense mechanism) setting. Code: https://github.com/faresmalik/SEViT",
      "authors": [
        "Faris Almalik",
        "Mohammad Yaqub",
        "Karthik Nandakumar"
      ],
      "year": 2022,
      "citation_count": 35,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/99fbe810d4194684be03458fdfebacb12d8a5c4e",
      "pdf_link": "",
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5553f9508dd1056ecc20c5b1f367e9a07e2c7e81",
      "title": "StyleSwin: Transformer-based GAN for High-resolution Image Generation",
      "abstract": "Despite the tantalizing success in a broad of vision tasks, transformers have not yet demonstrated on-par ability as ConvNets in high-resolution image generative modeling. In this paper, we seek to explore using pure transformers to build a generative adversarial network for high-resolution image synthesis. To this end, we believe that local attention is crucial to strike the balance between computational efficiency and modeling capacity. Hence, the proposed generator adopts Swin transformer in a style-based architecture. To achieve a larger receptive field, we propose double attention which simultaneously leverages the context of the local and the shifted windows, leading to improved generation quality. Moreover, we show that offering the knowledge of the absolute position that has been lost in window-based transformers greatly benefits the generation quality. The proposed StyleSwin is scalable to high resolutions, with both the coarse geometry and fine structures benefit from the strong expressivity of transformers. However, blocking artifacts occur during high-resolution synthesis because performing the local attention in a block-wise manner may break the spatial coherency. To solve this, we empirically investigate various solutions, among which we find that employing a wavelet discriminator to examine the spectral discrepancy effectively suppresses the artifacts. Extensive experiments show the superiority over prior transformer-based GANs, especially on high resolutions, e.g., $1024 \\times$ 1024. The StyleSwin, without complex training strategies, excels over StyleGAN on CelebA-HQ 1024, and achieves on-par performance on FFHQ-1024, proving the promise of using transformers for high-resolution image generation. The code and pretrained models are available at https://github.com/microsoft/StyleSwin.",
      "authors": [
        "Bo Zhang",
        "Shuyang Gu",
        "Bo Zhang",
        "Jianmin Bao",
        "Dong Chen",
        "Fang Wen",
        "Yong Wang",
        "B. Guo"
      ],
      "year": 2021,
      "citation_count": 249,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5553f9508dd1056ecc20c5b1f367e9a07e2c7e81",
      "pdf_link": "",
      "venue": "Computer Vision and Pattern Recognition",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "310f5543603bef94d42366878a14161db1bf45de",
      "title": "SPT-Swin: A Shifted Patch Tokenization Swin Transformer for Image Classification",
      "abstract": "Recently, the transformer-based model e.g., the vision transformer (ViT) has been extensively used in computer vision tasks. The superior performance of the ViT leads to the requirement of an enormous dataset and the complexity of calculating self-attention between patches is quadratic in nature. To acknowledge these two concerns, this paper proposes a novel shifted patch tokenization swin transformer (SPT-Swin) for the image classification task. The shifted patch tokenization (SPT) compensates for the data deficiency by increasing the data samples based on spatial information of the image patches while the swin transformer provides linear computational complexity by calculating self-attention between the shifted window based patches. For model validation, the SPT-Swin framework is trained on popular benchmark image datasets such as ImageNet-1K, CIFAR-10 and CIFAR-100, and the classification accuracies are found 89.45%, 95.67% and 92.95% respectively. Moreover, the comparative analysis of the proposed model with the existing state-of-the-art models shows that the classification performances are improved by 7.05%, 4.14%, and 8.30% for the ImageNet-1K, CIFAR-10 and CIFAR-100 datasets respectively. Therefore, our proposed SPT-based data augmentation technique with the core swin transformer model could be a data-efficient linear complex-able model for future computer vision tasks.",
      "authors": [
        "Gazi Jannatul Ferdous",
        "Khaleda Akhter Sathi",
        "Md. Azad Hossain",
        "M. Ali Akber Dewan"
      ],
      "year": 2024,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/310f5543603bef94d42366878a14161db1bf45de",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d40c77c010c8dbef6142903a02f2a73a85012d5d",
      "title": "A Survey on Vision Transformer",
      "abstract": "Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",
      "authors": [
        "Kai Han",
        "Yunhe Wang",
        "Hanting Chen",
        "Xinghao Chen",
        "Jianyuan Guo",
        "Zhenhua Liu",
        "Yehui Tang",
        "An Xiao",
        "Chunjing Xu",
        "Yixing Xu",
        "Zhaohui Yang",
        "Yiman Zhang",
        "D. Tao"
      ],
      "year": 2020,
      "citation_count": 2562,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d40c77c010c8dbef6142903a02f2a73a85012d5d",
      "pdf_link": "",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "409b43b8cd8a2ba69f93e80c2bacc0126238b550",
      "title": "Mobile Vision Transformer-based Visual Object Tracking",
      "abstract": "The introduction of robust backbones, such as Vision Transformers, has improved the performance of object tracking algorithms in recent years. However, these state-of-the-art trackers are computationally expensive since they have a large number of model parameters and rely on specialized hardware (e.g., GPU) for faster inference. On the other hand, recent lightweight trackers are fast but are less accurate, especially on large-scale datasets. We propose a lightweight, accurate, and fast tracking algorithm using Mobile Vision Transformers (MobileViT) as the backbone for the first time. We also present a novel approach of fusing the template and search region representations in the MobileViT backbone, thereby generating superior feature encoding for target localization. The experimental results show that our MobileViT-based Tracker, MVT, surpasses the performance of recent lightweight trackers on the large-scale datasets GOT10k and TrackingNet, and with a high inference speed. In addition, our method outperforms the popular DiMP-50 tracker despite having 4.7 times fewer model parameters and running at 2.8 times its speed on a GPU. The tracker code and models are available at https://github.com/goutamyg/MVT",
      "authors": [
        "Goutam Yelluru Gopal",
        "Maria A. Amer"
      ],
      "year": 2023,
      "citation_count": 8,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/409b43b8cd8a2ba69f93e80c2bacc0126238b550",
      "pdf_link": "",
      "venue": "British Machine Vision Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
      "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
      "abstract": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer.",
      "authors": [
        "Ze Liu",
        "Yutong Lin",
        "Yue Cao",
        "Han Hu",
        "Yixuan Wei",
        "Zheng Zhang",
        "Stephen Lin",
        "B. Guo"
      ],
      "year": 2021,
      "citation_count": 24050,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c8b25fab5608c3e033d34b4483ec47e68ba109b7",
      "pdf_link": "",
      "venue": "IEEE International Conference on Computer Vision",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "957a3d34303b424fe90a279cf5361253c93ac265",
      "title": "High Resolution SAR Image Classification Using Global-Local Network Structure Based on Vision Transformer and CNN",
      "abstract": "High-resolution (HR) synthetic aperture radar (SAR) image classification is a challenging task for the limitation of its complex semantic scenes and coherent speckles. Convolutional neural networks (CNNs) have been proven the superior local spatial features representation capability for SAR images. However, it is hard to capture global information of images by convolutions. To solve such issues, this letter proposes an end-to-end network named global–local network structure (GLNS) for HR SAR classification. In the GLNS framework, a lightweight CNN and a compact vision transformer (ViT) are designed to learn local and global features, and two types of features are fused in quality to mine complementary information through the fusion net. Then, our research devolves the twofold loss function to reduce the interclass distance of SAR images, which brings more compactness to classification features and less interference of coherent speckles. Experimental results on real HR SAR images indicate that the proposed method has more strong feature extraction capability and noise resistance performance. This method achieves the highest classification accuracy on both datasets compared with other related approaches based on CNN.",
      "authors": [
        "Xingyu Liu",
        "Yuehua Wu",
        "Wenkai Liang",
        "Yi Cao",
        "Ming Li"
      ],
      "year": 2022,
      "citation_count": 40,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/957a3d34303b424fe90a279cf5361253c93ac265",
      "pdf_link": "",
      "venue": "IEEE Geoscience and Remote Sensing Letters",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "17534840dc6016229a577a66f108a1564b8a0131",
      "title": "A deep learning based approach for automated plant disease classification using vision transformer",
      "abstract": "Plant disease can diminish a considerable portion of the agricultural products on each farm. The main goal of this work is to provide visual information for the farmers to enable them to take the necessary preventive measures. A lightweight deep learning approach is proposed based on the Vision Transformer (ViT) for real-time automated plant disease classification. In addition to the ViT, the classical convolutional neural network (CNN) methods and the combination of CNN and ViT have been implemented for the plant disease classification. The models have been trained and evaluated on multiple datasets. Based on the comparison between the obtained results, it is concluded that although attention blocks increase the accuracy, they decelerate the prediction. Combining attention blocks with CNN blocks can compensate for the speed.",
      "authors": [
        "Y. Borhani",
        "Javad Khoramdel",
        "E. Najafi"
      ],
      "year": 2022,
      "citation_count": 165,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/17534840dc6016229a577a66f108a1564b8a0131",
      "pdf_link": "",
      "venue": "Scientific Reports",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "9a4718faa07a32cf1dce745062181d3342e9b054",
      "title": "GNViT- An enhanced image-based groundnut pest classification using Vision Transformer (ViT) model",
      "abstract": "Crop losses caused by diseases and pests present substantial challenges to global agriculture, with groundnut crops particularly vulnerable to their detrimental effects. This study introduces the Groundnut Vision Transformer (GNViT) model, a novel approach that harnesses a pre-trained Vision Transformer (ViT) on the ImageNet dataset. The primary goal is to detect and classify various pests affecting groundnut crops. Rigorous training and evaluation were conducted using a comprehensive dataset from IP102, encompassing pests such as Thrips, Aphids, Armyworms, and Wireworms. The GNViT model’s effectiveness was assessed using reliability metrics, including the F1-score, recall, and overall accuracy. Data augmentation with GNViT resulted in a significant increase in training accuracy, achieving 99.52%. Comparative analysis highlighted the GNViT model’s superior performance, particularly in accuracy, compared to state-of-the-art methodologies. These findings underscore the potential of deep learning models, such as GNViT, in providing reliable pest classification solutions for groundnut crops. The deployment of advanced technological solutions brings us closer to the overarching goal of reducing crop losses and enhancing global food security for the growing population.",
      "authors": [
        "Venkatasaichandrakanth P",
        "I. M"
      ],
      "year": 2024,
      "citation_count": 8,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9a4718faa07a32cf1dce745062181d3342e9b054",
      "pdf_link": "",
      "venue": "PLoS ONE",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ffc855594cad345ea5a1cce2ee27095bec767bc8",
      "title": "Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention",
      "abstract": "Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.",
      "authors": [
        "Saebom Leem",
        "Hyunseok Seo"
      ],
      "year": 2024,
      "citation_count": 19,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ffc855594cad345ea5a1cce2ee27095bec767bc8",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "7aeadf7b9d839bdd9dbfbff5239f71792ff0c04c",
      "title": "Vision Transformers (ViT) for Blanket-Penetrating Sleep Posture Recognition Using a Triple Ultra-Wideband (UWB) Radar System",
      "abstract": "Sleep posture has a crucial impact on the incidence and severity of obstructive sleep apnea (OSA). Therefore, the surveillance and recognition of sleep postures could facilitate the assessment of OSA. The existing contact-based systems might interfere with sleeping, while camera-based systems introduce privacy concerns. Radar-based systems might overcome these challenges, especially when individuals are covered with blankets. The aim of this research is to develop a nonobstructive multiple ultra-wideband radar sleep posture recognition system based on machine learning models. We evaluated three single-radar configurations (top, side, and head), three dual-radar configurations (top + side, top + head, and side + head), and one tri-radar configuration (top + side + head), in addition to machine learning models, including CNN-based networks (ResNet50, DenseNet121, and EfficientNetV2) and vision transformer-based networks (traditional vision transformer and Swin Transformer V2). Thirty participants (n = 30) were invited to perform four recumbent postures (supine, left side-lying, right side-lying, and prone). Data from eighteen participants were randomly chosen for model training, another six participants’ data (n = 6) for model validation, and the remaining six participants’ data (n = 6) for model testing. The Swin Transformer with side and head radar configuration achieved the highest prediction accuracy (0.808). Future research may consider the application of the synthetic aperture radar technique.",
      "authors": [
        "D. K. Lai",
        "Zi-Han Yu",
        "Tommy Yau-Nam Leung",
        "Hyo-Jung Lim",
        "Andy Yiu-Chau Tam",
        "Bryan Pak-Hei So",
        "Ye-Jiao Mao",
        "D. Cheung",
        "D. Wong",
        "C. Cheung"
      ],
      "year": 2023,
      "citation_count": 22,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/7aeadf7b9d839bdd9dbfbff5239f71792ff0c04c",
      "pdf_link": "",
      "venue": "Italian National Conference on Sensors",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "55156532cb9c20fdcaed9ead238f7a2cbaab2527",
      "title": "FV-ViT: Vision Transformer for Finger Vein Recognition",
      "abstract": "Vision Transformer (ViT) has drawn the attention of many researchers in computer vision due to its superior performance in many computer vision tasks. However, there is limited research based on ViT models in finger vein recognition. This may be because the excellent performance of the ViT models relies on the abundance of training data, but finger vein databases are typically small. In this study, we focus on this question and proposed a model for finger vein recognition, referred to as FV-ViT. With only rigorous regularization added in the MLP head, called regMLP, instead of changing architecture in the ViT backbone, the proposed FV-ViT shows outstanding performance compared to other state-of-the-art works: 0.042% EER for FV-USM and 1.033% EER for SDUMLA-HMT. In addition, we also compare the baseline FV-ViT model with the corresponding ViT model trained with pretrained weights: 0.068% EER from non-pretrained FV-ViT base versus 0.116% EER from pretrained for FV-USM, 1.258% EER from non-pretrained FV-ViT base versus 1.022% EER from pretrained for SDUMLA-HMT. This means that the ViT models can be trained from scratch on finger vein databases and achieve comparable performance when compared to the pretrained model.",
      "authors": [
        "Xiaoye Li",
        "Bin-Bin Zhang"
      ],
      "year": 2023,
      "citation_count": 25,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/55156532cb9c20fdcaed9ead238f7a2cbaab2527",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0afbaae7d0bfe98a04a3b7dd1ba69e25f32b24c6",
      "title": "Multi-granularity Feature Extraction Based on Vision Transformer for Tomato Leaf Disease Recognition",
      "abstract": "At present, the task of identifying crop diseases is mainly to simply distinguish the types of different crop diseases. However, the current classifiers cannot solve problems, such as accurate identification of similar disease categories. Compared with convolutional neural network (CNN), the recent vision transformer (VIT) has achieved good results on image tasks. Inspired by this, this paper proposed a multi-granularity feature extraction model based on vision transformer. By combining image block information of different scales, the model can learn image information from different granularities. At the same time, in order to further grasp the important areas, this paper developed a feature selection module. Through experimental comparison, the scheme has an accuracy improvement of nearly 2% compared with other classification models, and the model parameters have not improved much.",
      "authors": [
        "Shupei Wu",
        "Youqiang Sun",
        "He Huang"
      ],
      "year": 2021,
      "citation_count": 27,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0afbaae7d0bfe98a04a3b7dd1ba69e25f32b24c6",
      "pdf_link": "",
      "venue": "2021 3rd International Academic Exchange Conference on Science and Technology Innovation (IAECST)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "72e23cdc3accca1f09e2e19446bc475368c912d0",
      "title": "Swin-GA-RF: genetic algorithm-based Swin Transformer and random forest for enhancing cervical cancer classification",
      "abstract": "Cervical cancer is a prevalent and concerning disease affecting women, with increasing incidence and mortality rates. Early detection plays a crucial role in improving outcomes. Recent advancements in computer vision, particularly the Swin transformer, have shown promising performance in image classification tasks, rivaling or surpassing traditional convolutional neural networks (CNNs). The Swin transformer adopts a hierarchical and efficient approach using shifted windows, enabling the capture of both local and global contextual information in images. In this paper, we propose a novel approach called Swin-GA-RF to enhance the classification performance of cervical cells in Pap smear images. Swin-GA-RF combines the strengths of the Swin transformer, genetic algorithm (GA) feature selection, and the replacement of the softmax layer with a random forest classifier. Our methodology involves extracting feature representations from the Swin transformer, utilizing GA to identify the optimal feature set, and employing random forest as the classification model. Additionally, data augmentation techniques are applied to augment the diversity and quantity of the SIPaKMeD1 cervical cancer image dataset. We compare the performance of the Swin-GA-RF Transformer with pre-trained CNN models using two classes and five classes of cervical cancer classification, employing both Adam and SGD optimizers. The experimental results demonstrate that Swin-GA-RF outperforms other Swin transformers and pre-trained CNN models. When utilizing the Adam optimizer, Swin-GA-RF achieves the highest performance in both binary and five-class classification tasks. Specifically, for binary classification, it achieves an accuracy, precision, recall, and F1-score of 99.012, 99.015, 99.012, and 99.011, respectively. In the five-class classification, it achieves an accuracy, precision, recall, and F1-score of 98.808, 98.812, 98.808, and 98.808, respectively. These results underscore the effectiveness of the Swin-GA-RF approach in cervical cancer classification, demonstrating its potential as a valuable tool for early diagnosis and screening programs.",
      "authors": [
        "Manal Abdullah Alohali",
        "Nora El-Rashidy",
        "Saad Alaklabi",
        "H. Elmannai",
        "Saleh Alharbi",
        "Hager Saleh"
      ],
      "year": 2024,
      "citation_count": 5,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/72e23cdc3accca1f09e2e19446bc475368c912d0",
      "pdf_link": "",
      "venue": "Frontiers in Oncology",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0b8e8760d7dd64c7439019aeb3b6ac55dd5075d4",
      "title": "Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning",
      "abstract": "Multi-scale Vision Transformer (ViT) has emerged as a powerful backbone for computer vision tasks, while the self-attention computation in Transformer scales quadratically w.r.t. the input patch number. Thus, existing solutions commonly employ down-sampling operations (e.g., average pooling) over keys/values to dramatically reduce the computational cost. In this work, we argue that such over-aggressive down-sampling design is not invertible and inevitably causes information dropping especially for high-frequency components in objects (e.g., texture details). Motivated by the wavelet theory, we construct a new Wavelet Vision Transformer (\\textbf{Wave-ViT}) that formulates the invertible down-sampling with wavelet transforms and self-attention learning in a unified way. This proposal enables self-attention learning with lossless down-sampling over keys/values, facilitating the pursuing of a better efficiency-vs-accuracy trade-off. Furthermore, inverse wavelet transforms are leveraged to strengthen self-attention outputs by aggregating local contexts with enlarged receptive field. We validate the superiority of Wave-ViT through extensive experiments over multiple vision tasks (e.g., image recognition, object detection and instance segmentation). Its performances surpass state-of-the-art ViT backbones with comparable FLOPs. Source code is available at \\url{https://github.com/YehLi/ImageNetModel}.",
      "authors": [
        "Ting Yao",
        "Yingwei Pan",
        "Yehao Li",
        "C. Ngo",
        "Tao Mei"
      ],
      "year": 2022,
      "citation_count": 166,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0b8e8760d7dd64c7439019aeb3b6ac55dd5075d4",
      "pdf_link": "",
      "venue": "European Conference on Computer Vision",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e38e70580acb204c05096de8da90b7ab1d4bdb6b",
      "title": "SwinCross: Cross-modal Swin Transformer for Head-and-Neck Tumor Segmentation in PET/CT Images",
      "abstract": "BACKGROUND\nRadiotherapy (RT) combined with cetuximab is the standard treatment for patients with inoperable head and neck cancers. Segmentation of head and neck (H&N) tumors is a prerequisite for radiotherapy planning but a time-consuming process. In recent years, deep convolutional neural networks (DCNN) have become the de facto standard for automated image segmentation. However, due to the expensive computational cost associated with enlarging the field of view in DCNNs, their ability to model long-range dependency is still limited, and this can result in sub-optimal segmentation performance for objects with background context spanning over long distances. On the other hand, Transformer models have demonstrated excellent capabilities in capturing such long-range information in several semantic segmentation tasks performed on medical images.\n\n\nPURPOSE\nDespite the impressive representation capacity of vision transformer models, current vision transformer-based segmentation models still suffer from inconsistent and incorrect dense predictions when fed with multi-modal input data. We suspect that the power of their self-attention mechanism may be limited in extracting the complementary information that exists in multi-modal data. To this end, we propose a novel segmentation model, debuted, Cross-modal Swin Transformer (SwinCross), with cross-modal attention (CMA) module to incorporate cross-modal feature extraction at multiple resolutions.\n\n\nMETHODS\nWe propose a novel architecture for cross-modal 3D semantic segmentation with two main components: (1) a cross-modal 3D Swin Transformer for integrating information from multiple modalities (PET and CT), and (2) a cross-modal shifted window attention block for learning complementary information from the modalities. To evaluate the efficacy of our approach, we conducted experiments and ablation studies on the HECKTOR 2021 challenge dataset. We compared our method against nnU-Net (the backbone of the top-5 methods in HECKTOR 2021) and other state-of-the-art transformer-based models, including UNETR and Swin UNETR. The experiments employed a five-fold cross-validation setup using PET and CT images.\n\n\nRESULTS\nEmpirical evidence demonstrates that our proposed method consistently outperforms the comparative techniques. This success can be attributed to the CMA module's capacity to enhance inter-modality feature representations between PET and CT during head-and-neck tumor segmentation. Notably, SwinCross consistently surpasses Swin UNETR across all five folds, showcasing its proficiency in learning multi-modal feature representations at varying resolutions through the cross-modal attention modules.\n\n\nCONCLUSIONS\nWe introduced a cross-modal Swin Transformer for automating the delineation of head and neck tumors in PET and CT images. Our model incorporates a cross-modality attention module, enabling the exchange of features between modalities at multiple resolutions. The experimental results establish the superiority of our method in capturing improved inter-modality correlations between PET and CT for head-and-neck tumor segmentation. Furthermore, the proposed methodology holds applicability to other semantic segmentation tasks involving different imaging modalities like SPECT/CT or PET/MRI. Code:https://github.com/yli192/SwinCross_CrossModalSwinTransformer_for_Medical_Image_Segmentation.",
      "authors": [
        "Gary Y. Li",
        "Junyu Chen",
        "Se-In Jang",
        "Kuang Gong",
        "Quanzheng Li"
      ],
      "year": 2023,
      "citation_count": 15,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e38e70580acb204c05096de8da90b7ab1d4bdb6b",
      "pdf_link": "",
      "venue": "Medical Physics (Lancaster)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "8ec10ffe0c1fc8f6a92d541f0e002e78080b564a",
      "title": "Vision Transformer (ViT)-based Applications in Image Classification",
      "abstract": "In recent years, the ViT model has been widely used in the field of computer vision, especially for image classification tasks. This paper summarizes the application of ViT in image classification tasks, first introduces the image classification imple- mentation process and the basic architecture of the ViT model, then analyzes and summarizes the image classification methods, including traditional image classification methods, CNN-based image classification methods, and ViT-based image classification methods, and provides a comparative analysis of CNN and ViT. Subsequently, this paper outlines the application prospects of ViT in image classification and its future development and also outlines some shortcomings of ViT and its solutions.",
      "authors": [
        "Yingzi Huo",
        "Kai Jin",
        "Jiahong Cai",
        "Huixuan Xiong",
        "Jiacheng Pang"
      ],
      "year": 2023,
      "citation_count": 16,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/8ec10ffe0c1fc8f6a92d541f0e002e78080b564a",
      "pdf_link": "",
      "venue": "2023 IEEE 9th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "371e924dd270a213ee6e8d4104a38875105668df",
      "title": "Lightweight Vision Transformer with Cross Feature Attention",
      "abstract": "Recent advances in vision transformers (ViTs) have achieved great performance in visual recognition tasks. Convolutional neural networks (CNNs) exploit spatial inductive bias to learn visual representations, but these networks are spatially local. ViTs can learn global representations with their self-attention mechanism, but they are usually heavy-weight and unsuitable for mobile devices. In this paper, we propose cross feature attention (XFA) to bring down computation cost for transformers, and combine efficient mobile CNNs to form a novel efficient light-weight CNN-ViT hybrid model, XFormer, which can serve as a general-purpose backbone to learn both global and local representation. Experimental results show that XFormer outperforms numerous CNN and ViT-based models across different tasks and datasets. On ImageNet1K dataset, XFormer achieves top-1 accuracy of 78.5% with 5.5 million parameters, which is 2.2% and 6.3% more accurate than EfficientNet-B0 (CNN-based) and DeiT (ViT-based) for similar number of parameters. Our model also performs well when transferring to object detection and semantic segmentation tasks. On MS COCO dataset, XFormer exceeds MobileNetV2 by 10.5 AP (22.7 ->33.2 AP) in YOLOv3 framework with only 6.3M parameters and 3.8G FLOPs. On Cityscapes dataset, with only a simple all-MLP decoder, XFormer achieves mIoU of 78.5 and FPS of 15.3, surpassing state-of-the-art lightweight segmentation networks.",
      "authors": [
        "Youpeng Zhao",
        "Huadong Tang",
        "Yingying Jiang",
        "A. Yong",
        "Qiang Wu"
      ],
      "year": 2022,
      "citation_count": 12,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/371e924dd270a213ee6e8d4104a38875105668df",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3af375031a3e23b7daf2f1ed14b5b61147996ca0",
      "title": "Extended Vision Transformer (ExViT) for Land Use and Land Cover Classification: A Multimodal Deep Learning Framework",
      "abstract": "The recent success of attention mechanism-driven deep models, like vision transformer (ViT) as one of the most representatives, has intrigued a wave of advanced research to explore their adaptation to broader domains. However, current transformer-based approaches in the remote sensing (RS) community pay more attention to single-modality data, which might lose expandability in making full use of the ever-growing multimodal Earth observation data. To this end, we propose a novel multimodal deep learning framework by extending conventional ViT with minimal modifications, aiming at the task of land use and land cover (LULC) classification. Unlike common stems that adopt either linear patch projection or deep regional embedder, our approach processes multimodal RS image patches with parallel branches of position-shared ViTs extended with separable convolution modules, which offers an economical solution to leverage both spatial and modality-specific channel information. Furthermore, to promote information exchange across heterogeneous modalities, their tokenized embeddings are then fused through a cross-modality attention (CMA) module by exploiting pixel-level spatial correlation in RS scenes. Both of these modifications significantly improve the discriminative ability of classification tokens in each modality and thus further performance increase can be finally attained by a full token-based decision-level fusion module. We conduct extensive experiments on two multimodal RS benchmark datasets, i.e., the Houston2013 dataset containing hyperspectral (HS) and light detection and ranging (LiDAR) data, and Berlin dataset with HS and synthetic aperture radar (SAR) data, to demonstrate that our extended vision transformer (ExViT) outperforms concurrent competitors based on transformer or convolutional neural network (CNN) backbones, in addition to several competitive machine-learning-based models. The source codes and investigated datasets of this work will be made publicly available at https://github.com/jingyao16/ExViT.",
      "authors": [
        "Jing Yao",
        "Bing Zhang",
        "Chenyu Li",
        "D. Hong",
        "J. Chanussot"
      ],
      "year": 2023,
      "citation_count": 209,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3af375031a3e23b7daf2f1ed14b5b61147996ca0",
      "pdf_link": "",
      "venue": "IEEE Transactions on Geoscience and Remote Sensing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "164e41a60120917d13fb69e183ee3c996b6c9414",
      "title": "Vision Transformer for Small-Size Datasets",
      "abstract": "Recently, the Vision Transformer (ViT), which applied the transformer structure to the image classification task, has outperformed convolutional neural networks. However, the high performance of the ViT results from pre-training using a large-size dataset such as JFT-300M, and its dependence on a large dataset is interpreted as due to low locality inductive bias. This paper proposes Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA), which effectively solve the lack of locality inductive bias and enable it to learn from scratch even on small-size datasets. Moreover, SPT and LSA are generic and effective add-on modules that are easily applicable to various ViTs. Experimental results show that when both SPT and LSA were applied to the ViTs, the performance improved by an average of 2.96% in Tiny-ImageNet, which is a representative small-size dataset. Especially, Swin Transformer achieved an overwhelming performance improvement of 4.08% thanks to the proposed SPT and LSA.",
      "authors": [
        "Seung Hoon Lee",
        "Seunghyun Lee",
        "B. Song"
      ],
      "year": 2021,
      "citation_count": 250,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/164e41a60120917d13fb69e183ee3c996b6c9414",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "9998291d71f4e8ddf59f4b016b19df1f848eeed1",
      "title": "EFFResNet-ViT: A Fusion-Based Convolutional and Vision Transformer Model for Explainable Medical Image Classification",
      "abstract": "The rapid advancement of medical imaging technologies requires the development of advanced, automated, and interpretable diagnostic tools for clinical decision-making. Although convolutional neural networks (CNNs) have shown significant promise in medical image analysis, they have limitations in capturing the global context and lack interpretability, thereby hindering their clinical adoption. This study presents EFFResNet-ViT, a novel hybrid deep learning (DL) model designed to address these challenges by combining EfficientNet-B0 and ResNet-50 CNN backbones with a vision transformer (ViT) module. The proposed architecture employs a feature fusion strategy to integrate the local feature extraction strengths of CNNs with the global dependency modeling capabilities of transformers. The extracted features are further refined through a post-transformer CNN and a global average pooling layer to enhance the classification performance. To improve interpretability, EFFResNet-ViT incorporates Grad-CAM visualization techniques to highlight regions contributing to classification decisions and employs t-distributed stochastic neighbor embedding for feature space analysis, providing insights into class separability. The proposed model was evaluated on two benchmark datasets: brain tumor (BT) CE-MRI for BT classification and a retinal image dataset for ophthalmological diagnosis. EFFResNet-ViT achieved state-of-the-art performance, with accuracies of 99.31% and 92.54% on the BT CE-MRI and retinal datasets, respectively. Comparative analyses demonstrate the superior classification performance and interpretability of EFFResNet-ViT over existing ViT and CNN-based hybrid models. The explainable design of EFFResNet-ViT addresses the critical need for transparency in artificial intelligence-driven medical diagnostics, facilitating its potential integration into clinical workflows to improve decision-making and patient outcomes.",
      "authors": [
        "Tahir Hussain",
        "Hayaru Shouno",
        "Abid Hussain",
        "Dostdar Hussain",
        "Muhammad Ismail",
        "Tatheer Hussain Mir",
        "Fang Rong Hsu",
        "Taukir Alam",
        "Shabnur Anonna Akhy"
      ],
      "year": 2025,
      "citation_count": 14,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9998291d71f4e8ddf59f4b016b19df1f848eeed1",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e",
      "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer",
      "abstract": "Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets",
      "authors": [
        "Sachin Mehta",
        "Mohammad Rastegari"
      ],
      "year": 2021,
      "citation_count": 1482,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/da74a10824193be9d3889ce0d6ed4c6f8ee48b9e",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e91934d66d9133d854ff0a4cafbe7966584bbf97",
      "title": "Quantitative regularization in robust vision transformer for remote sensing image classification",
      "abstract": "Vision Transformers (ViTs) are exceptional at vision tasks. However, when applied to remote sensing images (RSIs), existing methods often necessitate extensive modifications of ViTs to rival convolutional neural networks (CNNs). This requirement significantly impedes the application of ViTs in geosciences, particularly for researchers who lack the time for comprehensive model redesign. To address this issue, we introduce the concept of quantitative regularization (QR), designed to enhance the performance of ViTs in RSI classification. QR represents an effective algorithm that adeptly manages domain discrepancies in RSIs and can be integrated with any ViTs in transfer learning. We evaluated the effectiveness of QR using three ViT architectures: vanilla ViT, Swin‐ViT and Next‐ViT, on four datasets: AID30, NWPU45, AFGR50 and UCM21. The results reveal that our Next‐ViT model surpasses 39 other advanced methods published in the past 3 years, maintaining robust performance even with a limited number of training samples. We also discovered that our ViT and Swin‐ViT achieve significantly higher accuracy and robustness compared to other methods using the same backbone. Our findings confirm that ViTs can be as effective as CNNs for RSI classification, regardless of the dataset size. Our approach exclusively employs open‐source ViTs and easily accessible training strategies. Consequently, we believe that our method can significantly lower the barriers for geoscience researchers intending to use ViT for RSI applications.",
      "authors": [
        "Huaxiang Song",
        "Yuxuan Yuan",
        "Zhiwei Ouyang",
        "Yu Yang",
        "Hui Xiang"
      ],
      "year": 2024,
      "citation_count": 28,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e91934d66d9133d854ff0a4cafbe7966584bbf97",
      "pdf_link": "",
      "venue": "Photogrammetric Record",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a",
      "title": "Unified Visual Transformer Compression",
      "abstract": "Vision transformers (ViTs) have gained popularity recently. Even without customized image operators such as convolutions, ViTs can yield competitive performance when properly trained on massive data. However, the computational overhead of ViTs remains prohibitive, due to stacking multi-head self-attention modules and else. Compared to the vast literature and prevailing success in compressing convolutional neural networks, the study of Vision Transformer compression has also just emerged, and existing works focused on one or two aspects of compression. This paper proposes a unified ViT compression framework that seamlessly assembles three effective techniques: pruning, layer skipping, and knowledge distillation. We formulate a budget-constrained, end-to-end optimization framework, targeting jointly learning model weights, layer-wise pruning ratios/masks, and skip configurations, under a distillation loss. The optimization problem is then solved using the primal-dual algorithm. Experiments are conducted with several ViT variants, e.g. DeiT and T2T-ViT backbones on the ImageNet dataset, and our approach consistently outperforms recent competitors. For example, DeiT-Tiny can be trimmed down to 50\\% of the original FLOPs almost without losing accuracy. Codes are available online:~\\url{https://github.com/VITA-Group/UVC}.",
      "authors": [
        "Shixing Yu",
        "Tianlong Chen",
        "Jiayi Shen",
        "Huan Yuan",
        "Jianchao Tan",
        "Sen Yang",
        "Ji Liu",
        "Zhangyang Wang"
      ],
      "year": 2022,
      "citation_count": 103,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d28fed119d9293af31776205150b3c34f3adc82b",
      "title": "Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality",
      "abstract": "Masked AutoEncoder (MAE) has recently led the trends of visual self-supervision area by an elegant asymmetric encoder-decoder design, which significantly optimizes both the pre-training efficiency and fine-tuning accuracy. Notably, the success of the asymmetric structure relies on the\"global\"property of Vanilla Vision Transformer (ViT), whose self-attention mechanism reasons over arbitrary subset of discrete image patches. However, it is still unclear how the advanced Pyramid-based ViTs (e.g., PVT, Swin) can be adopted in MAE pre-training as they commonly introduce operators within\"local\"windows, making it difficult to handle the random sequence of partial vision tokens. In this paper, we propose Uniform Masking (UM), successfully enabling MAE pre-training for Pyramid-based ViTs with locality (termed\"UM-MAE\"for short). Specifically, UM includes a Uniform Sampling (US) that strictly samples $1$ random patch from each $2 \\times 2$ grid, and a Secondary Masking (SM) which randomly masks a portion of (usually $25\\%$) the already sampled regions as learnable tokens. US preserves equivalent elements across multiple non-overlapped local windows, resulting in the smooth support for popular Pyramid-based ViTs; whilst SM is designed for better transferable visual representations since US reduces the difficulty of pixel recovery pre-task that hinders the semantic learning. We demonstrate that UM-MAE significantly improves the pre-training efficiency (e.g., it speeds up and reduces the GPU memory by $\\sim 2\\times$) of Pyramid-based ViTs, but maintains the competitive fine-tuning performance across downstream tasks. For example using HTC++ detector, the pre-trained Swin-Large backbone self-supervised under UM-MAE only in ImageNet-1K can even outperform the one supervised in ImageNet-22K. The codes are available at https://github.com/implus/UM-MAE.",
      "authors": [
        "Xiang Li",
        "Wenhai Wang",
        "Lingfeng Yang",
        "Jian Yang"
      ],
      "year": 2022,
      "citation_count": 81,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d28fed119d9293af31776205150b3c34f3adc82b",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2dcc1f5f1ba11f99e09f22c53d02aa66cec35c48",
      "title": "Vision Transformer Based Models for Plant Disease Detection and Diagnosis",
      "abstract": "Plant health is one of the most interesting aspects in the natural cycle, it needs to be conserved to keep the life of the organisms. Several plant diseases could be observed at early stages in the leaf level, where immediate interventions should be taken to prevent the progression of the disease. The use of deep learning has dramatically increased recently, owing to its remarkable performance in multiple applications in different research areas. In this study, we focus on the detection of tomato diseases at the leaf stage using recent deep learning architectures. Several deep learning models are put in comparative experiments to achieve a stable and robust classification performance with high precision that outperforms previous SOTA results. Vision Transformers (ViT) models reported the top classification re-sults, with an accuracy of 96.7%, 98.52%, 99.1% and 99.7%. The research funding will help in the early automatic detection of diseases in the leaf plants, thus providing necessary treatments and maintaining the natural cycle.",
      "authors": [
        "Rayene Amina Boukabouya",
        "A. Moussaoui",
        "Mohamed Berrimi"
      ],
      "year": 2022,
      "citation_count": 16,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2dcc1f5f1ba11f99e09f22c53d02aa66cec35c48",
      "pdf_link": "",
      "venue": "International Symposium on Information and Automation",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1ee05cd919590eaba129caa0fda5e850c87b75a5",
      "title": "FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer",
      "abstract": "Network quantization significantly reduces model inference complexity and has been widely used in real-world deployments. However, most existing quantization methods have been developed mainly on Convolutional Neural Networks (CNNs), and suffer severe degradation when applied to fully quantized vision transformers. In this work, we demonstrate that many of these difficulties arise because of serious inter-channel variation in LayerNorm inputs, and present, Power-of-Two Factor (PTF), a systematic method to reduce the performance degradation and inference complexity of fully quantized vision transformers. In addition, observing an extreme non-uniform distribution in attention maps, we propose Log-Int-Softmax (LIS) to sustain that and simplify inference by using 4-bit quantization and the BitShift operator. Comprehensive experiments on various transformer-based architectures and benchmarks show that our Fully Quantized Vision Transformer (FQ-ViT) outperforms previous works while even using lower bit-width on attention maps. For instance, we reach 84.89% top-1 accuracy with ViT-L on ImageNet and 50.8 mAP with Cascade Mask R-CNN (Swin-S) on COCO. To our knowledge, we are the first to achieve lossless accuracy degradation (~1%) on fully quantized vision transformers. The code is available at https://github.com/megvii-research/FQ-ViT.",
      "authors": [
        "Yang Lin",
        "Tianyu Zhang",
        "Peiqin Sun",
        "Zheng Li",
        "Shuchang Zhou"
      ],
      "year": 2021,
      "citation_count": 179,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1ee05cd919590eaba129caa0fda5e850c87b75a5",
      "pdf_link": "",
      "venue": "International Joint Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "96da196d6f8c947db03d13759f030642f8234abf",
      "title": "DeepViT: Towards Deeper Vision Transformer",
      "abstract": "Vision transformers (ViTs) have been successfully applied in image classification tasks recently. In this paper, we show that, unlike convolution neural networks (CNNs)that can be improved by stacking more convolutional layers, the performance of ViTs saturate fast when scaled to be deeper. More specifically, we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper, the attention maps gradually become similar and even much the same after certain layers. In other words, the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs, the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation, we propose a simple yet effective method, named Re-attention, to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably, when training a deep ViT model with 32 transformer blocks, the Top-1 classification accuracy can be improved by 1.6% on ImageNet. Code is publicly available at https://github.com/zhoudaquan/dvit_repo.",
      "authors": [
        "Daquan Zhou",
        "Bingyi Kang",
        "Xiaojie Jin",
        "Linjie Yang",
        "Xiaochen Lian",
        "Qibin Hou",
        "Jiashi Feng"
      ],
      "year": 2021,
      "citation_count": 548,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/96da196d6f8c947db03d13759f030642f8234abf",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "861f670073679ba05990f3bc6d119b13ab62aca7",
      "title": "PLG-ViT: Vision Transformer with Parallel Local and Global Self-Attention",
      "abstract": "Recently, transformer architectures have shown superior performance compared to their CNN counterparts in many computer vision tasks. The self-attention mechanism enables transformer networks to connect visual dependencies over short as well as long distances, thus generating a large, sometimes even a global receptive field. In this paper, we propose our Parallel Local-Global Vision Transformer (PLG-ViT), a general backbone model that fuses local window self-attention with global self-attention. By merging these local and global features, short- and long-range spatial interactions can be effectively and efficiently represented without the need for costly computational operations such as shifted windows. In a comprehensive evaluation, we demonstrate that our PLG-ViT outperforms CNN-based as well as state-of-the-art transformer-based architectures in image classification and in complex downstream tasks such as object detection, instance segmentation, and semantic segmentation. In particular, our PLG-ViT models outperformed similarly sized networks like ConvNeXt and Swin Transformer, achieving Top-1 accuracy values of 83.4%, 84.0%, and 84.5% on ImageNet-1K with 27M, 52M, and 91M parameters, respectively.",
      "authors": [
        "Nikolas Ebert",
        "D. Stricker",
        "Oliver Wasenmüller"
      ],
      "year": 2023,
      "citation_count": 28,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/861f670073679ba05990f3bc6d119b13ab62aca7",
      "pdf_link": "",
      "venue": "Italian National Conference on Sensors",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "44ee4165b8a3811dc7d557f99150ff9e62f3733f",
      "title": "ViT-V-Net: Vision Transformer for Unsupervised Volumetric Medical Image Registration",
      "abstract": "In the last decade, convolutional neural networks (ConvNets) have dominated and achieved state-of-the-art performances in a variety of medical imaging applications. However, the performances of ConvNets are still limited by lacking the understanding of long-range spatial relations in an image. The recently proposed Vision Transformer (ViT) for image classification uses a purely self-attention-based model that learns long-range spatial relations to focus on the relevant parts of an image. Nevertheless, ViT emphasizes the low-resolution features because of the consecutive downsamplings, result in a lack of detailed localization information, making it unsuitable for image registration. Recently, several ViT-based image segmentation methods have been combined with ConvNets to improve the recovery of detailed localization information. Inspired by them, we present ViT-V-Net, which bridges ViT and ConvNet to provide volumetric medical image registration. The experimental results presented here demonstrate that the proposed architecture achieves superior performance to several top-performing registration methods.",
      "authors": [
        "Junyu Chen",
        "Yufan He",
        "E. Frey",
        "Ye Li",
        "Yong Du"
      ],
      "year": 2021,
      "citation_count": 202,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/44ee4165b8a3811dc7d557f99150ff9e62f3733f",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "29f86d6d1eaba6a466c231f6906b18eae4b2b484",
      "title": "Can We Solve 3D Vision Tasks Starting from A 2D Vision Transformer?",
      "abstract": "Vision Transformers (ViTs) have proven to be effective, in solving 2D image understanding tasks by training over large-scale image datasets; and meanwhile as a somehow separate track, in modeling the 3D visual world too such as voxels or point clouds. However, with the growing hope that transformers can become the\"universal\"modeling tool for heterogeneous data, ViTs for 2D and 3D tasks have so far adopted vastly different architecture designs that are hardly transferable. That invites an (over-)ambitious question: can we close the gap between the 2D and 3D ViT architectures? As a piloting study, this paper demonstrates the appealing promise to understand the 3D visual world, using a standard 2D ViT architecture, with only minimal customization at the input and output levels without redesigning the pipeline. To build a 3D ViT from its 2D sibling, we\"inflate\"the patch embedding and token sequence, accompanied with new positional encoding mechanisms designed to match the 3D data geometry. The resultant\"minimalist\"3D ViT, named Simple3D-Former, performs surprisingly robustly on popular 3D tasks such as object classification, point cloud segmentation and indoor scene detection, compared to highly customized 3D-specific designs. It can hence act as a strong baseline for new 3D ViTs. Moreover, we note that pursing a unified 2D-3D ViT design has practical relevance besides just scientific curiosity. Specifically, we demonstrate that Simple3D-Former naturally enables to exploit the wealth of pre-trained weights from large-scale realistic 2D images (e.g., ImageNet), which can be plugged in to enhancing the 3D task performance\"for free\".",
      "authors": [
        "Yi Wang",
        "Zhiwen Fan",
        "Tianlong Chen",
        "Hehe Fan",
        "Zhangyang Wang"
      ],
      "year": 2022,
      "citation_count": 10,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/29f86d6d1eaba6a466c231f6906b18eae4b2b484",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1d7183d481ae5a396743dde39984f1f0c8f47edf",
      "title": "SWAT: An Efficient Swin Transformer Accelerator Based on FPGA",
      "abstract": "Swin Transformer achieves greater efficiency than Vision Transformer by utilizing local self-attention and shifted windows. However, existing hardware accelerators designed for Transformer have not been optimized for the unique computation flow and data reuse property in Swin Transformer, resulting in lower hardware utilization and extra memory accesses. To address this issue, we develop SWAT, an efficient Swin Transformer Accelerator based on FPGA. Firstly, to eliminate the redundant computations in shifted windows, a novel tiling strategy is employed, which helps the developed multiplier array to fully utilize the sparsity. Additionally, we deploy a dynamic pipeline interleaving dataflow, which not only reduces the processing latency but also maximizes data reuse, thereby decreasing access to memories. Furthermore, customized quantization strategies and approximate calculations for non-linear calculations are adopted to simplify the hardware complexity with negligible network accuracy loss. We implement SWAT on the Xilinx Alveo U50 platform and evaluate it with Swin-T on the ImageNet dataset. The proposed architecture can achieve improvements of $2.02 \\times \\sim 3.11 \\times$ in power efficiency compared to existing Transformer accelerators on FPGAs.",
      "authors": [
        "Qiwei Dong",
        "Xiaoru Xie",
        "Zhongfeng Wang"
      ],
      "year": 2024,
      "citation_count": 8,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1d7183d481ae5a396743dde39984f1f0c8f47edf",
      "pdf_link": "",
      "venue": "Asia and South Pacific Design Automation Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "50e997b23a534a6fbfd32d63990fa80373ec7c6b",
      "title": "Students' Classroom Behavior Detection System Incorporating Deformable DETR with Swin Transformer and Light-Weight Feature Pyramid Network",
      "abstract": "Artificial intelligence (AI) and computer vision technologies have gained significant prominence in the field of education. These technologies enable the detection and analysis of students’ classroom behaviors, providing valuable insights for assessing individual concentration levels. However, the accuracy of target detection methods based on Convolutional Neural Networks (CNNs) can be compromised in classrooms with multiple targets and varying scales, as convolutional operations may result in the loss of location information. In contrast, transformers, which leverage attention mechanisms, have the capability to learn global features and mitigate the information loss caused by convolutional operations. In this paper, we propose a students’ classroom behavior detection system that combines deformable DETR with a Swin Transformer and light-weight Feature Pyramid Network (FPN). By employing a feature pyramid structure, the system can effectively process multi-scale feature maps extracted by the Swin Transformer, thereby improving the detection accuracy for targets of different sizes and scales. Moreover, the integration of the CARAFE lightweight operator into the FPN structure enhances the network’s detection accuracy. To validate the effectiveness of our approach, extensive experiments are conducted on a real dataset of students’ classroom behavior. The experimental results demonstrate a significant 6.1% improvement in detection accuracy compared to state-of-the-art methods. These findings highlight the superiority of our proposed network in accurately detecting and analyzing students’ classroom behaviors. Overall, this research contributes to the field of education by addressing the limitations of CNN-based target detection methods and leveraging the capabilities of transformers to improve accuracy. The proposed system showcases the benefits of integrating deformable DETR, Swin Transformer, and the lightweight FPN in the context of students’ classroom behavior detection. The experimental results provide compelling evidence of the system’s effectiveness and its potential to enhance classroom monitoring and assessment practices.",
      "authors": [
        "Zhifeng Wang",
        "Jialong Yao",
        "Chunyan Zeng",
        "Longlong Li",
        "Cheng Tan"
      ],
      "year": 2023,
      "citation_count": 19,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/50e997b23a534a6fbfd32d63990fa80373ec7c6b",
      "pdf_link": "",
      "venue": "Syst.",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f4e32b928d7cc27447e312bdc052aa75888045aa",
      "title": "MITformer: A Multiinstance Vision Transformer for Remote Sensing Scene Classification",
      "abstract": "The latest vision transformer (ViT) has stronger contextual feature representation capability than the existing convolutional neural networks and thus has the potential to depict the remote sensing scenes, which usually have more complicated object distribution and spatial arrangement than ground image scenes. However, recent researches reflect that while ViT learns global features, it also ignores the key local features, which poses a bottleneck for understanding remote sensing scenes. In this letter, we tackle this challenge by proposing a novel multiinstance vision transformer (MITformer). Its originality mainly lies in the classic multiple instance learning (MIL) formulation, where each image patch embedded in ViT is regarded as an instance and each image is regarded as a bag. The benefit of designing ViT under MIL formulation is straightforward, as it helps highlight the feature response of key local regions of remote sensing scenes. Moreover, to enhance the feature propagation of local features, attention-based multilayer perceptron (AMLP) head is embedded at the end of each encoder unit. Finally, to minimize the potential semantic prediction differences between the classic ViT and our MIL head, a semantic consistency loss is designed. Experiments on three remote sensing scene classification benchmarks show that our proposed MITformer outperforms the existing state-of-the-art methods and validate the effectiveness of each component in our MITformer.",
      "authors": [
        "Z. Sha",
        "Jianfeng Li"
      ],
      "year": 2022,
      "citation_count": 34,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f4e32b928d7cc27447e312bdc052aa75888045aa",
      "pdf_link": "",
      "venue": "IEEE Geoscience and Remote Sensing Letters",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "9fb327c55a30b9771a364f45f33f77778756a164",
      "title": "I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference",
      "abstract": "Vision Transformers (ViTs) have achieved state-of-the-art performance on various computer vision applications. However, these models have considerable storage and computational overheads, making their deployment and efficient inference on edge devices challenging. Quantization is a promising approach to reducing model complexity, and the dyadic arithmetic pipeline can allow the quantized models to perform efficient integer-only inference. Unfortunately, dyadic arithmetic is based on the homogeneity condition in convolutional neural networks, which is not applicable to the non-linear components in ViTs, making integer-only inference of ViTs an open issue. In this paper, we propose I-ViT, an integer-only quantization scheme for ViTs, to enable ViTs to perform the entire computational graph of inference with integer arithmetic and bit-shifting, and without any floating-point arithmetic. In I-ViT, linear operations (e.g., MatMul and Dense) follow the integer-only pipeline with dyadic arithmetic, and non-linear operations (e.g., Softmax, GELU, and LayerNorm) are approximated by the proposed light-weight integer-only arithmetic methods. More specifically, I-ViT applies the proposed Shiftmax and ShiftGELU, which are designed to use integer bit-shifting to approximate the corresponding floating-point operations. We evaluate I-ViT on various benchmark models and the results show that integer-only INT8 quantization achieves comparable (or even slightly higher) accuracy to the full-precision (FP) baseline. Furthermore, we utilize TVM for practical hardware deployment on the GPU’s integer arithmetic units, achieving 3.72 ~ 4.11 inference speedup compared to the FP model. Code of both Pytorch and TVM is released at https://github.com/zkkli/I-ViT.",
      "authors": [
        "Zhikai Li",
        "Qingyi Gu"
      ],
      "year": 2022,
      "citation_count": 122,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9fb327c55a30b9771a364f45f33f77778756a164",
      "pdf_link": "",
      "venue": "IEEE International Conference on Computer Vision",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e5cb26148791b57bfd36aa26ce2401e231d01b57",
      "title": "Vision Transformer with Deformable Attention",
      "abstract": "Transformers have recently shown superior performances on various vision tasks. The large, sometimes even global, receptive field endows Transformer models with higher representation power over their CNN counterparts. Nevertheless, simply enlarging receptive field also gives rise to several concerns. On the one hand, using dense attention e.g., in ViT, leads to excessive memory and computational cost, and features can be influenced by irrelevant parts which are beyond the region of interests. On the other hand, the sparse attention adopted in PVT or Swin Transformer is data agnostic and may limit the ability to model long range relations. To mitigate these issues, we propose a novel deformable selfattention module, where the positions of key and value pairs in selfattention are selected in a data-dependent way. This flexible scheme enables the self-attention module to focus on relevant re-gions and capture more informative features. On this basis, we present Deformable Attention Transformer, a general backbone model with deformable attention for both image classification and dense prediction tasks. Extensive experi-ments show that our models achieve consistently improved results on comprehensive benchmarks. Code is available at https://github.com/LeapLabTHU/DAT.",
      "authors": [
        "Zhuofan Xia",
        "Xuran Pan",
        "S. Song",
        "Li Erran Li",
        "Gao Huang"
      ],
      "year": 2022,
      "citation_count": 560,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e5cb26148791b57bfd36aa26ce2401e231d01b57",
      "pdf_link": "",
      "venue": "Computer Vision and Pattern Recognition",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4702a22a3c2da1284a88d5e608d38cd106d66736",
      "title": "A Simple Yet Effective Network Based on Vision Transformer for Camouflaged Object and Salient Object Detection",
      "abstract": "Camouflaged object detection (COD) and salient object detection (SOD) are two distinct yet closely-related computer vision tasks widely studied during the past decades. Though sharing the same purpose of segmenting an image into binary foreground and background regions, their distinction lies in the fact that COD focuses on concealed objects hidden in the image, while SOD concentrates on the most prominent objects in the image. Building universal segmentation models is currently a hot topic in the community. Previous works achieved good performance on certain task by stacking various hand-designed modules and multi-scale features. However, these careful task-specific designs also make them lose their potential as general-purpose architectures. Therefore, we hope to build general architectures that can be applied to both tasks. In this work, we propose a simple yet effective network (SENet) based on vision Transformer (ViT), by employing a simple design of an asymmetric ViT-based encoder-decoder structure, we yield competitive results on both tasks, exhibiting greater versatility than meticulously crafted ones. To enhance the performance of universal architectures on both tasks, we propose some general methods targeting some common difficulties of the two tasks. First, we use image reconstruction as an auxiliary task during training to increase the difficulty of training, forcing the network to have a better perception of the image as a whole to help with segmentation tasks. In addition, we propose a local information capture module (LICM) to make up for the limitations of the patch-level attention mechanism in pixel-level COD and SOD tasks and a dynamic weighted loss (DW loss) to solve the problem that small target samples are more difficult to locate and segment in both tasks. Finally, we also conduct a preliminary exploration of joint training, trying to use one model to complete two tasks simultaneously. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method. The code is available at https://github.com/linuxsino/SENet.",
      "authors": [
        "Chao Hao",
        "Zitong Yu",
        "Xin Liu",
        "Jun Xu",
        "Huanjing Yue",
        "Jingyu Yang"
      ],
      "year": 2024,
      "citation_count": 14,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4702a22a3c2da1284a88d5e608d38cd106d66736",
      "pdf_link": "",
      "venue": "IEEE Transactions on Image Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3ea79430455304c782572dfb6ca3e5230b0351de",
      "title": "GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous Structured Pruning for Vision Transformer",
      "abstract": "The recently proposed Vision transformers (ViTs) have shown\nvery impressive empirical performance in various computer vision tasks,\nand they are viewed as an important type of foundation model. However, ViTs are typically constructed with large-scale sizes, which then\nseverely hinder their potential deployment in many practical resources constrained applications. \nTo mitigate this challenging problem, structured pruning is a promising solution to compress model size and enable\npractical efficiency. However, unlike its current popularity for CNNs and\nRNNs, structured pruning for ViT models is little explored.\nIn this paper, we propose GOHSP, a unified framework of Graph and\nOptimization-based Structured Pruning for ViT models. We first develop\na graph-based ranking for measuring the importance of attention heads,\nand the extracted importance information is further integrated to an\noptimization-based procedure to impose the heterogeneous structured\nsparsity patterns on the ViT models. Experimental results show that\nour proposed GOHSP demonstrates excellent compression performance.\nOn CIFAR-10 dataset, our approach can bring 40% parameters reduction\nwith no accuracy loss for ViT-Small model. On ImageNet dataset, with\n30% and 35% sparsity ratio for DeiT-Tiny and DeiT-Small models, our\napproach achieves 1.65% and 0.76% accuracy increase over the existing\nstructured pruning methods, respectively.",
      "authors": [
        "Miao Yin",
        "Burak Uzkent",
        "Yilin Shen",
        "Hongxia Jin",
        "Bo Yuan"
      ],
      "year": 2023,
      "citation_count": 19,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3ea79430455304c782572dfb6ca3e5230b0351de",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1871c0bc27bd5cd5f2f6c3d6cedeb4b8c5a89499",
      "title": "A lightweight hybrid vision transformer network for radar-based human activity recognition",
      "abstract": "Radar-based human activity recognition (HAR) offers a non-contact technique with privacy protection and lighting robustness for many advanced applications. Complex deep neural networks demonstrate significant performance advantages when classifying the radar micro-Doppler signals that have unique correspondences with human behavior. However, in embedded applications, the demand for lightweight and low latency poses challenges to the radar-based HAR network construction. In this paper, an efficient network based on a lightweight hybrid Vision Transformer (LH-ViT) is proposed to address the HAR accuracy and network lightweight simultaneously. This network combines the efficient convolution operations with the strength of the self-attention mechanism in ViT. Feature Pyramid architecture is applied for the multi-scale feature extraction for the micro-Doppler map. Feature enhancement is executed by the stacked Radar-ViT subsequently, in which the fold and unfold operations are added to lower the computational load of the attention mechanism. The convolution operator in the LH-ViT is replaced by the RES-SE block, an efficient structure that combines the residual learning framework with the Squeeze-and-Excitation network. Experiments based on two human activity datasets indicate our method’s advantages in terms of expressiveness and computing efficiency over traditional methods.",
      "authors": [
        "Sha Huan",
        "Zhaoyue Wang",
        "Xiaoqiang Wang",
        "Limei Wu",
        "Xiaoxuan Yang",
        "Hongming Huang",
        "Gan E Dai"
      ],
      "year": 2023,
      "citation_count": 17,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1871c0bc27bd5cd5f2f6c3d6cedeb4b8c5a89499",
      "pdf_link": "",
      "venue": "Scientific Reports",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "011f59c91bbee6de780d35ebe50fff62087e5b13",
      "title": "Class-Guided Swin Transformer for Semantic Segmentation of Remote Sensing Imagery",
      "abstract": "Semantic segmentation of remote sensing images plays a crucial role in a wide variety of practical applications, including land cover mapping, environmental protection, and economic assessment. In the last decade, convolutional neural network (CNN) is the mainstream deep learning-based method of semantic segmentation. Compared with conventional methods, CNN-based methods learn semantic features automatically, thereby achieving strong representation capability. However, the local receptive field of the convolution operation limits CNN-based methods from capturing long-range dependencies. In contrast, Vision Transformer (ViT) demonstrates its great potential in modeling long-range dependencies and obtains superior results in semantic segmentation. Inspired by this, in this letter, we propose a class-guided Swin Transformer (CG-Swin) for semantic segmentation of remote sensing images. Specifically, we adopt a Transformer-based encoder–decoder structure, which introduces the Swin Transformer backbone as the encoder and designs a class-guided Transformer block to construct the decoder. The experimental results on ISPRS Vaihingen and Potsdam datasets demonstrate the significant breakthrough of the proposed method over ten benchmarks, outperforming both advanced CNN-based and recent Transformer-based approaches.",
      "authors": [
        "Xiaoliang Meng",
        "Yuechi Yang",
        "Libo Wang",
        "Teng Wang",
        "Rui Li",
        "Ce Zhang"
      ],
      "year": 2022,
      "citation_count": 60,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/011f59c91bbee6de780d35ebe50fff62087e5b13",
      "pdf_link": "",
      "venue": "IEEE Geoscience and Remote Sensing Letters",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "572ed945b06818472105bd17cfeb355d4e46c5e5",
      "title": "Intrusion detection: A model based on the improved vision transformer",
      "abstract": "We propose an intrusion detection model based on an improved vision transformer (ViT). More specifically, the model uses an attention mechanism to process data, which overcomes the flaw of the short‐term memory in recurrent neural network (RNN) and the difficulty of learning remote dependency in convolutional neural network. It supports parallelization and has a faster computing speed than RNN. A sliding window mechanism is presented to improve the capability of modeling local features for ViT. The hierarchical focal loss function is used to improve the classification effect, and solve the issue of the data imbalance. The public intrusion detection dataset NSL‐KDD is used for experimental simulations. By experimental simulations, the accuracy is up to 99.68%, the false‐positive rate is 0.22%, and the recall rate is 99.57%, which show that the improved ViT has better accuracy, false positive rate, and recall rate than existing intrusion detection models.",
      "authors": [
        "Yuguang Yang",
        "Hong‐Mei Fu",
        "Shang Gao",
        "Yihua Zhou",
        "Wei-Min shi"
      ],
      "year": 2022,
      "citation_count": 27,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/572ed945b06818472105bd17cfeb355d4e46c5e5",
      "pdf_link": "",
      "venue": "Transactions on Emerging Telecommunications Technologies",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b66e4257aa8856df537f03f6a12341f489eb6500",
      "title": "A Fast Inference Vision Transformer for Automatic Pavement Image Classification and Its Visual Interpretation Method",
      "abstract": "Traditional automatic pavement distress detection methods using convolutional neural networks (CNNs) require a great deal of time and resources for computing and are poor in terms of interpretability. Therefore, inspired by the successful application of Transformer architecture in natural language processing (NLP) tasks, a novel Transformer method called LeViT was introduced for automatic asphalt pavement image classification. LeViT consists of convolutional layers, transformer stages where Multi-layer Perception (MLP) and multi-head self-attention blocks alternate using the residual connection, and two classifier heads. To conduct the proposed methods, three different sources of pavement image datasets and pre-trained weights based on ImageNet were attained. The performance of the proposed model was compared with six state-of-the-art (SOTA) deep learning models. All of them were trained based on transfer learning strategy. Compared to the tested SOTA methods, LeViT has less than 1/8 of the parameters of the original Vision Transformer (ViT) and 1/2 of ResNet and InceptionNet. Experimental results show that after training for 100 epochs with a 16 batch-size, the proposed method acquired 91.56% accuracy, 91.72% precision, 91.56% recall, and 91.45% F1-score in the Chinese asphalt pavement dataset and 99.17% accuracy, 99.19% precision, 99.17% recall, and 99.17% F1-score in the German asphalt pavement dataset, which is the best performance among all the tested SOTA models. Moreover, it shows superiority in inference speed (86 ms/step), which is approximately 25% of the original ViT method and 80% of some prevailing CNN-based models, including DenseNet, VGG, and ResNet. Overall, the proposed method can achieve competitive performance with fewer computation costs. In addition, a visualization method combining Grad-CAM and Attention Rollout was proposed to analyze the classification results and explore what has been learned in every MLP and attention block of LeViT, which improved the interpretability of the proposed pavement image classification model.",
      "authors": [
        "Yihan Chen",
        "Xingyu Gu",
        "Zhen Liu",
        "Jia-Yun Liang"
      ],
      "year": 2022,
      "citation_count": 52,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b66e4257aa8856df537f03f6a12341f489eb6500",
      "pdf_link": "",
      "venue": "Remote Sensing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "35fccd11326e799ebf724f4150acef12a6538953",
      "title": "TransVG++: End-to-End Visual Grounding With Language Conditioned Vision Transformer",
      "abstract": "In this work, we explore neat yet effective Transformer-based frameworks for visual grounding. The previous methods generally address the core problem of visual grounding, i.e., multi-modal fusion and reasoning, with manually-designed mechanisms. Such heuristic designs are not only complicated but also make models easily overfit specific data distributions. To avoid this, we first propose TransVG, which establishes multi-modal correspondences by Transformers and localizes referred regions by directly regressing box coordinates. We empirically show that complicated fusion modules can be replaced by a simple stack of Transformer encoder layers with higher performance. However, the core fusion Transformer in TransVG is stand-alone against uni-modal encoders, and thus should be trained from scratch on limited visual grounding data, which makes it hard to be optimized and leads to sub-optimal performance. To this end, we further introduce TransVG++ to make two-fold improvements. For one thing, we upgrade our framework to a purely Transformer-based one by leveraging Vision Transformer (ViT) for vision feature encoding. For another, we devise Language Conditioned Vision Transformer that removes external fusion modules and reuses the uni-modal ViT for vision-language fusion at the intermediate layers. We conduct extensive experiments on five prevalent datasets, and report a series of state-of-the-art records.",
      "authors": [
        "Jiajun Deng",
        "Zhengyuan Yang",
        "Daqing Liu",
        "Tianlang Chen",
        "Wen-gang Zhou",
        "Yanyong Zhang",
        "Houqiang Li",
        "Wanli Ouyang"
      ],
      "year": 2022,
      "citation_count": 73,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/35fccd11326e799ebf724f4150acef12a6538953",
      "pdf_link": "",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "16fa1a8575ff56781b6b83726906754ed4e5f3a7",
      "title": "ViT-ReT: Vision and Recurrent Transformer Neural Networks for Human Activity Recognition in Videos",
      "abstract": "Human activity recognition is an emerging and important area in computer vision which seeks to determine the activity an individual or group of individuals are performing. The applications of this field ranges from generating highlight videos in sports, to intelligent surveillance and gesture recognition. Most activity recognition systems rely on a combination of convolutional neural networks (CNNs) to perform feature extraction from the data and recurrent neural networks (RNNs) to determine the time dependent nature of the data. This paper proposes and designs two transformer neural networks for human activity recognition: a recurrent transformer (ReT), a specialized neural network used to make predictions on sequences of data, as well as a vision transformer (ViT), a transformer optimized for extracting salient features from images, to improve speed and scalability of activity recognition. We have provided an extensive comparison of the proposed transformer neural networks with the contemporary CNN and RNN-based human activity recognition models in terms of speed and accuracy for four publicly available human action datasets. Experimental results reveal that the proposed ViT-ReT framework attains a speedup of $2\\times $ over the baseline ResNet50-LSTM approach while attaining nearly the same level of accuracy. Furthermore, results show that the proposed ViT-ReT framework attains significant improvements over the state-of-the-art human action recognition methods in terms of both model accuracy and runtime for each of the datasets used in our experiments, thus verifying the suitability of the proposed ViT-ReT framework for human activity recognition in resource-constrained and real-time environments.",
      "authors": [
        "James Wensel",
        "Hayat Ullah",
        "Arslan Munir"
      ],
      "year": 2022,
      "citation_count": 51,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/16fa1a8575ff56781b6b83726906754ed4e5f3a7",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "98e702ef2f64ab2643df9e80b1bd034334142e62",
      "title": "HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling",
      "abstract": "Recently, masked image modeling (MIM) has offered a new methodology of self-supervised pre-training of vision transformers. A key idea of efficient implementation is to discard the masked image patches (or tokens) throughout the target network (encoder), which requires the encoder to be a plain vision transformer (e.g., ViT), albeit hierarchical vision transformers (e.g., Swin Transformer) have potentially better properties in formulating vision inputs. In this paper, we offer a new design of hierarchical vision transformers named HiViT (short for Hierarchical ViT) that enjoys both high efficiency and good performance in MIM. The key is to remove the unnecessary\"local inter-unit operations\", deriving structurally simple hierarchical vision transformers in which mask-units can be serialized like plain vision transformers. For this purpose, we start with Swin Transformer and (i) set the masking unit size to be the token size in the main stage of Swin Transformer, (ii) switch off inter-unit self-attentions before the main stage, and (iii) eliminate all operations after the main stage. Empirical studies demonstrate the advantageous performance of HiViT in terms of fully-supervised, self-supervised, and transfer learning. In particular, in running MAE on ImageNet-1K, HiViT-B reports a +0.6% accuracy gain over ViT-B and a 1.9$\\times$ speed-up over Swin-B, and the performance gain generalizes to downstream tasks of detection and segmentation. Code will be made publicly available.",
      "authors": [
        "Xiaosong Zhang",
        "Yunjie Tian",
        "Wei Huang",
        "Qixiang Ye",
        "Qi Dai",
        "Lingxi Xie",
        "Qi Tian"
      ],
      "year": 2022,
      "citation_count": 33,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/98e702ef2f64ab2643df9e80b1bd034334142e62",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f996d5ee3b8ad3c60510862a92fd72c6a41777e0",
      "title": "LRP-QViT: Mixed-Precision Vision Transformer Quantization via Layer-wise Relevance Propagation",
      "abstract": "Vision transformers (ViTs) have demonstrated remarkable performance across various visual tasks. However, ViT models suffer from substantial computational and memory requirements, making it challenging to deploy them on resource-constrained platforms. Quantization is a popular approach for reducing model size, but most studies mainly focus on equal bit-width quantization for the entire network, resulting in sub-optimal solutions. While there are few works on mixed precision quantization (MPQ) for ViTs, they typically rely on search space-based methods or employ mixed precision arbitrarily. In this paper, we introduce LRP-QViT, an explainability-based method for assigning mixed-precision bit allocations to different layers based on their importance during classification. Specifically, to measure the contribution score of each layer in predicting the target class, we employ the Layer-wise Relevance Propagation (LRP) method. LRP assigns local relevance at the output layer and propagates it through all layers, distributing the relevance until it reaches the input layers. These relevance scores serve as indicators for computing the layer contribution score. Additionally, we have introduced a clipped channel-wise quantization aimed at eliminating outliers from post-LayerNorm activations to alleviate severe inter-channel variations. To validate and assess our approach, we employ LRP-QViT across ViT, DeiT, and Swin transformer models on various datasets. Our experimental findings demonstrate that both our fixed-bit and mixed-bit post-training quantization methods surpass existing models in the context of 4-bit and 6-bit quantization.",
      "authors": [
        "Navin Ranjan",
        "Andreas E. Savakis"
      ],
      "year": 2024,
      "citation_count": 7,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f996d5ee3b8ad3c60510862a92fd72c6a41777e0",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "21c37fd6d00d6c727ec10f0c871c1ddf6e4f7ed9",
      "title": "ViT-MVT: A Unified Vision Transformer Network for Multiple Vision Tasks",
      "abstract": "In this work, we seek to learn multiple mainstream vision tasks concurrently using a unified network, which is storage-efficient as numerous networks with task-shared parameters can be implanted into a single consolidated network. Our framework, vision transformer (ViT)-MVT, built on a plain and nonhierarchical ViT, incorporates numerous visual tasks into a modest supernet and optimizes them jointly across various dataset domains. For the design of ViT-MVT, we augment the ViT with a multihead self-attention (MHSE) to offer complementary cues in the channel and spatial dimension, as well as a local perception unit (LPU) and locality feed-forward network (locality FFN) for information exchange in the local region, thus endowing ViT-MVT with the ability to effectively optimize multiple tasks. Besides, we construct a search space comprising potential architectures with a broad spectrum of model sizes to offer various optimum candidates for diverse tasks. After that, we design a layer-adaptive sharing technique that automatically determines whether each layer of the transformer block is shared or not for all tasks, enabling ViT-MVT to obtain task-shared parameters for a reduction of storage and task-specific parameters to learn task-related features such that boosting performance. Finally, we introduce a joint-task evolutionary search algorithm to discover an optimal backbone for all tasks under total model size constraint, which challenges the conventional wisdom that visual tasks are typically supplied with backbone networks developed for image classification. Extensive experiments reveal that ViT-MVT delivers exceptional performances for multiple visual tasks over state-of-the-art methods while necessitating considerably fewer total storage costs. We further demonstrate that once ViT-MVT has been trained, ViT-MVT is capable of incremental learning when generalized to new tasks while retaining identical performances for trained tasks. The code is available at https://github.com/XT-1997/vitmvt.",
      "authors": [
        "Tao Xie",
        "Kun Dai",
        "Zhiqiang Jiang",
        "Ruifeng Li",
        "Shouren Mao",
        "Ke Wang",
        "Lijun Zhao"
      ],
      "year": 2023,
      "citation_count": 15,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/21c37fd6d00d6c727ec10f0c871c1ddf6e4f7ed9",
      "pdf_link": "",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "136b0ef7bd7f642a285b1d61ddf7abfc7ddc026f",
      "title": "Classification of Mobile-Based Oral Cancer Images Using the Vision Transformer and the Swin Transformer",
      "abstract": "Simple Summary Transformer models, originally successful in natural language processing, have found application in computer vision, demonstrating promising results in tasks related to cancer image analysis. Despite being one of the prevalent and swiftly spreading cancers globally, there is a pressing need for accurate automated analysis methods for oral cancer. This need is particularly critical for high-risk populations residing in low- and middle-income countries. In this study, we evaluated the performance of the Vision Transformer (ViT) and the Swin Transformer in the classification of mobile-based oral cancer images we collected from high-risk populations. The results showed that the Swin Transformer model achieved higher accuracy than the ViT model, and both transformer models work better than the conventional convolution model VGG19. Abstract Oral cancer, a pervasive and rapidly growing malignant disease, poses a significant global health concern. Early and accurate diagnosis is pivotal for improving patient outcomes. Automatic diagnosis methods based on artificial intelligence have shown promising results in the oral cancer field, but the accuracy still needs to be improved for realistic diagnostic scenarios. Vision Transformers (ViT) have outperformed learning CNN models recently in many computer vision benchmark tasks. This study explores the effectiveness of the Vision Transformer and the Swin Transformer, two cutting-edge variants of the transformer architecture, for the mobile-based oral cancer image classification application. The pre-trained Swin transformer model achieved 88.7% accuracy in the binary classification task, outperforming the ViT model by 2.3%, while the conventional convolutional network model VGG19 and ResNet50 achieved 85.2% and 84.5% accuracy. Our experiments demonstrate that these transformer-based architectures outperform traditional convolutional neural networks in terms of oral cancer image classification, and underscore the potential of the ViT and the Swin Transformer in advancing the state of the art in oral cancer image analysis.",
      "authors": [
        "Bofan Song",
        "D. Kc",
        "Rubin Yuchan Yang",
        "Shaobai Li",
        "Chicheng Zhang",
        "Rongguang Liang"
      ],
      "year": 2024,
      "citation_count": 19,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/136b0ef7bd7f642a285b1d61ddf7abfc7ddc026f",
      "pdf_link": "",
      "venue": "Cancers",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ba12a9915553b3b42df17a33afcfd547821d8cc3",
      "title": "Beyond Fixation: Dynamic Window Visual Transformer",
      "abstract": "Recently, a surge of interest in visual transformers is to reduce the computational cost by limiting the calculation of self-attention to a local window. Most current work uses a fixed single-scale window for modeling by default, ignoring the impact of window size on model performance. How-ever, this may limit the modeling potential of these window-based models for multi-scale information. In this paper, we propose a novel method, named Dynamic Window Vision Transformer (DW-ViT). The dynamic window strategy proposed by DW- ViT goes beyond the model that employs a fixed single window setting. To the best of our knowl-edge, we are the first to use dynamic multi-scale windows to explore the upper limit of the effect of window settings on model performance. In DW- ViT, multi-scale information is obtained by assigning windows of different sizes to different head groups of window multi-head self-attention. Then, the information is dynamically fused by assigning different weights to the multi-scale window branches. We con-ducted a detailed performance evaluation on three datasets, ImageNet-1K, ADE20K, and COCO. Compared with re-lated state-of-the-art (SoTA) methods, DW- ViT obtains the best performance. Specifically, compared with the current SoTA Swin Transformers [31], DW-ViT has achieved con-sistent and substantial improvements on all three datasets with similar parameters and computational costs. In addition, DW-ViT exhibits good scalability and can be easily inserted into any window-based visual transformers.11Code release: https://github.com/pzhren/DW-ViT. This work was done when the first author interned at Dark Matter AI..",
      "authors": [
        "Pengzhen Ren",
        "Changlin Li",
        "Guangrun Wang",
        "Yun Xiao",
        "Qing Du Xiaodan Liang Xiaojun Chang"
      ],
      "year": 2022,
      "citation_count": 37,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ba12a9915553b3b42df17a33afcfd547821d8cc3",
      "pdf_link": "",
      "venue": "Computer Vision and Pattern Recognition",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "58fc305734a0d5d849ae69b9233af082d712197e",
      "title": "A Swin Transformer-Based Encoding Booster Integrated in U-Shaped Network for Building Extraction",
      "abstract": "Building extraction is a popular topic in remote sensing image processing. Efficient building extraction algorithms can identify and segment building areas to provide informative data for downstream tasks. Currently, building extraction is mainly achieved by deep convolutional neural networks (CNNs) based on the U-shaped encoder–decoder architecture. However, the local perceptive field of the convolutional operation poses a challenge for CNNs to fully capture the semantic information of large buildings, especially in high-resolution remote sensing images. Considering the recent success of the Transformer in computer vision tasks, in this paper, first we propose a shifted-window (swin) Transformer-based encoding booster. The proposed encoding booster includes a swin Transformer pyramid containing patch merging layers for down-sampling, which enables our encoding booster to extract semantics from multi-level features at different scales. Most importantly, the receptive field is significantly expanded by the global self-attention mechanism of the swin Transformer, allowing the encoding booster to capture the large-scale semantic information effectively and transcend the limitations of CNNs. Furthermore, we integrate the encoding booster in a specially designed U-shaped network through a novel manner, named the Swin Transformer-based Encoding Booster- U-shaped Network (STEB-UNet), to achieve the feature-level fusion of local and large-scale semantics. Remarkably, compared with other Transformer-included networks, the computational complexity and memory requirement of the STEB-UNet are significantly reduced due to the swin design, making the network training much easier. Experimental results show that the STEB-UNet can effectively discriminate and extract buildings of different scales and demonstrate higher accuracy than the state-of-the-art networks on public datasets.",
      "authors": [
        "Xiao Xiao",
        "Wenliang Guo",
        "Rui Chen",
        "Yilong Hui",
        "J. Wang",
        "Hongyu Zhao"
      ],
      "year": 2022,
      "citation_count": 36,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/58fc305734a0d5d849ae69b9233af082d712197e",
      "pdf_link": "",
      "venue": "Remote Sensing",
      "paper_type": "",
      "keywords": []
    }
  ],
  "edges": [
    {
      "source": "5135a8f690c66c3b64928227443c4f9378bd20e1",
      "target": "e91934d66d9133d854ff0a4cafbe7966584bbf97",
      "weight": 0.285327391390227
    },
    {
      "source": "2e4dbc3dbd400346be60318ae558a0293e65ba81",
      "target": "3c6980902883f03c37332d34ead343e1229062b3",
      "weight": 0.46140713260848826
    },
    {
      "source": "174919e5a4ef95ff66440d56614ad954c6f27df1",
      "target": "635675452852e838644516e1eeefd1aaa8c8ac07",
      "weight": 0.24999617831450704
    },
    {
      "source": "649b706ba282de4eb5a161137f80eb49ed84a0a8",
      "target": "e4add4391dfa2a806a50cc1fbe9a9696dac9501f",
      "weight": 0.2522203484522421
    },
    {
      "source": "649b706ba282de4eb5a161137f80eb49ed84a0a8",
      "target": "98e702ef2f64ab2643df9e80b1bd034334142e62",
      "weight": 0.2933582800534256
    },
    {
      "source": "b52844a746dafd8a5051cef49abbbda64a312605",
      "target": "714e21409b8c4f7788ac8c93795249a4e45e51ce",
      "weight": 0.29184471277622537
    },
    {
      "source": "b52844a746dafd8a5051cef49abbbda64a312605",
      "target": "310f5543603bef94d42366878a14161db1bf45de",
      "weight": 0.25152692504343505
    },
    {
      "source": "934942934a6a785e2a80daa6421fa79971558b89",
      "target": "714e21409b8c4f7788ac8c93795249a4e45e51ce",
      "weight": 0.3145891044601017
    },
    {
      "source": "934942934a6a785e2a80daa6421fa79971558b89",
      "target": "21c37fd6d00d6c727ec10f0c871c1ddf6e4f7ed9",
      "weight": 0.3137495905921158
    },
    {
      "source": "1d8ecf2c3a78e5dc4de9e4c41961771bbd0033b3",
      "target": "bb5e6bbd4ad8c613a0bac3db23940c03ce94bec9",
      "weight": 0.34661689682990054
    },
    {
      "source": "a4b728dbbf5afdc231afb95ad4e5c2ececdefc48",
      "target": "bb5e6bbd4ad8c613a0bac3db23940c03ce94bec9",
      "weight": 0.29115697616849
    },
    {
      "source": "a4b728dbbf5afdc231afb95ad4e5c2ececdefc48",
      "target": "e8dceb26166721014b8ecbd11fd212739c18d315",
      "weight": 0.30305319716314677
    },
    {
      "source": "a4b728dbbf5afdc231afb95ad4e5c2ececdefc48",
      "target": "e91934d66d9133d854ff0a4cafbe7966584bbf97",
      "weight": 0.35342938428359894
    },
    {
      "source": "3c14992a490cc31a7a38f5fab156c9da40a861d4",
      "target": "46880aeca86695ca3117cc04f6bd9edaf088111b",
      "weight": 0.43014195820323486
    },
    {
      "source": "0d1e4faa1580266a56ad62ac2fdd72ed0c0bbbe9",
      "target": "d203076c28587895aa344d088b2788dbab5e82a1",
      "weight": 0.06939630655017023
    },
    {
      "source": "dfdb2894d50e095ce97f994ed6cee38554c4c84f",
      "target": "1ec9b653475287e95fdaef2f5247f82a8376c56c",
      "weight": 0.2880835292471667
    },
    {
      "source": "dfdb2894d50e095ce97f994ed6cee38554c4c84f",
      "target": "f996d5ee3b8ad3c60510862a92fd72c6a41777e0",
      "weight": 0.4474501113509747
    },
    {
      "source": "dfdb2894d50e095ce97f994ed6cee38554c4c84f",
      "target": "b48a85980deb5f1baa64d862b9f0e4e62124e4de",
      "weight": 0.4991627774846021
    },
    {
      "source": "3798e7f16fe69c29307a9bab4f0f4d779478afc5",
      "target": "b2becca9911c155bf97656df8e5079ca76767ab9",
      "weight": 0.4315066967459864
    },
    {
      "source": "e33434a141bb2881a2e60c518a0cda4feed3f19a",
      "target": "10e9943b3a974ac6175ffec3228e670ec9d2cc18",
      "weight": 0.26899310263521464
    },
    {
      "source": "e33434a141bb2881a2e60c518a0cda4feed3f19a",
      "target": "b66e4257aa8856df537f03f6a12341f489eb6500",
      "weight": 0.24460533739828666
    },
    {
      "source": "49030ae220c863e9b72ab380ecc749c9d0f0ad13",
      "target": "b2becca9911c155bf97656df8e5079ca76767ab9",
      "weight": 0.24480643468361912
    },
    {
      "source": "c57467e652f3f9131b3e7e40c23059abe395f01d",
      "target": "714e21409b8c4f7788ac8c93795249a4e45e51ce",
      "weight": 0.2610047095175981
    },
    {
      "source": "c57467e652f3f9131b3e7e40c23059abe395f01d",
      "target": "b1f0ac0b5e4044251dcdcce78e0046feb43cc7c7",
      "weight": 0.24056273913914514
    },
    {
      "source": "a09cbcaac305884f043810afc4fa4053099b5970",
      "target": "136b0ef7bd7f642a285b1d61ddf7abfc7ddc026f",
      "weight": 0.2338549925443127
    },
    {
      "source": "a09cbcaac305884f043810afc4fa4053099b5970",
      "target": "f996d5ee3b8ad3c60510862a92fd72c6a41777e0",
      "weight": 0.21736948695725794
    },
    {
      "source": "a09cbcaac305884f043810afc4fa4053099b5970",
      "target": "21c37fd6d00d6c727ec10f0c871c1ddf6e4f7ed9",
      "weight": 0.2882820720059867
    },
    {
      "source": "a09cbcaac305884f043810afc4fa4053099b5970",
      "target": "e06b703146c46a6455fd0c33077de1bea5fdd877",
      "weight": 0.2842930873647942
    },
    {
      "source": "a09cbcaac305884f043810afc4fa4053099b5970",
      "target": "53e5db85e2a7442f20670be2ae25019fcf9d27a2",
      "weight": 0.27407769026216744
    },
    {
      "source": "a09cbcaac305884f043810afc4fa4053099b5970",
      "target": "29f86d6d1eaba6a466c231f6906b18eae4b2b484",
      "weight": 0.268823949853301
    },
    {
      "source": "a09cbcaac305884f043810afc4fa4053099b5970",
      "target": "371e924dd270a213ee6e8d4104a38875105668df",
      "weight": 0.25647859260865197
    },
    {
      "source": "a09cbcaac305884f043810afc4fa4053099b5970",
      "target": "35fccd11326e799ebf724f4150acef12a6538953",
      "weight": 0.22740772139918533
    },
    {
      "source": "a09cbcaac305884f043810afc4fa4053099b5970",
      "target": "d28fed119d9293af31776205150b3c34f3adc82b",
      "weight": 0.37607030009170717
    },
    {
      "source": "0eff37167876356da2163b2e396df2719adf7de9",
      "target": "e8dceb26166721014b8ecbd11fd212739c18d315",
      "weight": 0.24475614487626018
    },
    {
      "source": "0eff37167876356da2163b2e396df2719adf7de9",
      "target": "fec86abbb29b559c1eaff31428f5b59f8070bb67",
      "weight": 0.24800804573587396
    },
    {
      "source": "0eff37167876356da2163b2e396df2719adf7de9",
      "target": "714e21409b8c4f7788ac8c93795249a4e45e51ce",
      "weight": 0.2818497428118607
    },
    {
      "source": "0eff37167876356da2163b2e396df2719adf7de9",
      "target": "9a4718faa07a32cf1dce745062181d3342e9b054",
      "weight": 0.23078382814850096
    },
    {
      "source": "0eff37167876356da2163b2e396df2719adf7de9",
      "target": "3dee43cea71d5988a72a914121f3455106f89cc7",
      "weight": 0.3076139285891044
    },
    {
      "source": "0eff37167876356da2163b2e396df2719adf7de9",
      "target": "ffc855594cad345ea5a1cce2ee27095bec767bc8",
      "weight": 0.28022068818233836
    },
    {
      "source": "0eff37167876356da2163b2e396df2719adf7de9",
      "target": "b1f0ac0b5e4044251dcdcce78e0046feb43cc7c7",
      "weight": 0.2831564363685659
    },
    {
      "source": "0eff37167876356da2163b2e396df2719adf7de9",
      "target": "1871c0bc27bd5cd5f2f6c3d6cedeb4b8c5a89499",
      "weight": 0.2501529918107033
    },
    {
      "source": "0eff37167876356da2163b2e396df2719adf7de9",
      "target": "8ec10ffe0c1fc8f6a92d541f0e002e78080b564a",
      "weight": 0.11049641471608725
    },
    {
      "source": "0eff37167876356da2163b2e396df2719adf7de9",
      "target": "d203076c28587895aa344d088b2788dbab5e82a1",
      "weight": 0.0393949216862149
    },
    {
      "source": "0eff37167876356da2163b2e396df2719adf7de9",
      "target": "c57467e652f3f9131b3e7e40c23059abe395f01d",
      "weight": 0.2742056324501918
    },
    {
      "source": "0eff37167876356da2163b2e396df2719adf7de9",
      "target": "442b5ec3aad4b099e71d6203a62eb7ec7519544c",
      "weight": 0.3172198714922243
    },
    {
      "source": "0eff37167876356da2163b2e396df2719adf7de9",
      "target": "861f670073679ba05990f3bc6d119b13ab62aca7",
      "weight": 0.2958263527532825
    },
    {
      "source": "6f4093a7ad5378e8cd3b73a52fbec80b784c107d",
      "target": "9fb327c55a30b9771a364f45f33f77778756a164",
      "weight": 0.2593730748646722
    },
    {
      "source": "3efcd3a4c54694a093886981d59e3cffe0dd7149",
      "target": "5572237909914e23758115be6b8d7f99a8bd51dc",
      "weight": 0.259576110044371
    },
    {
      "source": "13f7a106bb3814ad1fab25fd1356e99e91f402d3",
      "target": "b48a85980deb5f1baa64d862b9f0e4e62124e4de",
      "weight": 0.4101059591673194
    },
    {
      "source": "13f7a106bb3814ad1fab25fd1356e99e91f402d3",
      "target": "e4add4391dfa2a806a50cc1fbe9a9696dac9501f",
      "weight": 0.23610294498970202
    },
    {
      "source": "13f7a106bb3814ad1fab25fd1356e99e91f402d3",
      "target": "9fb327c55a30b9771a364f45f33f77778756a164",
      "weight": 0.36197777347649684
    },
    {
      "source": "bb5e6bbd4ad8c613a0bac3db23940c03ce94bec9",
      "target": "bbe5dfbecfd1bed7556b9c8269b0d363faa24973",
      "weight": 0.47838797387460563
    },
    {
      "source": "d2fce7480111d66a74caa801a236f71ab021c42c",
      "target": "9121dcd10df00e5cc51dc94400e0325e0ae47bb9",
      "weight": 0.21712105508636426
    },
    {
      "source": "03384825d373aabe67c4288ef1eae4d1cf89dc00",
      "target": "9121dcd10df00e5cc51dc94400e0325e0ae47bb9",
      "weight": 0.3887710380485438
    },
    {
      "source": "03384825d373aabe67c4288ef1eae4d1cf89dc00",
      "target": "1ec9b653475287e95fdaef2f5247f82a8376c56c",
      "weight": 0.3636788669685544
    },
    {
      "source": "03384825d373aabe67c4288ef1eae4d1cf89dc00",
      "target": "1d7183d481ae5a396743dde39984f1f0c8f47edf",
      "weight": 0.23962815846623592
    },
    {
      "source": "03384825d373aabe67c4288ef1eae4d1cf89dc00",
      "target": "dc5d707a8f9efaf32d2319ec2ca8d2619ef2926e",
      "weight": 0.4117377104326374
    },
    {
      "source": "53e5db85e2a7442f20670be2ae25019fcf9d27a2",
      "target": "714e21409b8c4f7788ac8c93795249a4e45e51ce",
      "weight": 0.32693844885051987
    },
    {
      "source": "53e5db85e2a7442f20670be2ae25019fcf9d27a2",
      "target": "21c37fd6d00d6c727ec10f0c871c1ddf6e4f7ed9",
      "weight": 0.2692488024868138
    },
    {
      "source": "53e5db85e2a7442f20670be2ae25019fcf9d27a2",
      "target": "981970d0f586761e7cdd978670c6a8f46990f514",
      "weight": 0.4044406519891216
    },
    {
      "source": "53e5db85e2a7442f20670be2ae25019fcf9d27a2",
      "target": "d80166681f3344a1946b8bfc623f4679d979ee10",
      "weight": 0.36647351988316396
    },
    {
      "source": "60b0f9af990349546f284dea666fbf52ebfa7004",
      "target": "5135a8f690c66c3b64928227443c4f9378bd20e1",
      "weight": 0.4213142764099653
    },
    {
      "source": "60b0f9af990349546f284dea666fbf52ebfa7004",
      "target": "442b5ec3aad4b099e71d6203a62eb7ec7519544c",
      "weight": 0.40241495214569095
    },
    {
      "source": "60b0f9af990349546f284dea666fbf52ebfa7004",
      "target": "011f59c91bbee6de780d35ebe50fff62087e5b13",
      "weight": 0.3380609771585147
    },
    {
      "source": "226fcbe55235d873bedb2fcf5b981bd5ec860d4f",
      "target": "e37539f5c943a92ef56b49b7fa067bd976e418d4",
      "weight": 0.5500512399804289
    },
    {
      "source": "a4ef2c7c321dea04d8ea91ec1ce0710ebce4f848",
      "target": "c064efa0419b75ba131ec8470ed80f01e1a80f64",
      "weight": 0.27663821632176616
    },
    {
      "source": "3ae8c0b646ddce95ffd09da31c02ed6fdb744e90",
      "target": "d80166681f3344a1946b8bfc623f4679d979ee10",
      "weight": 0.5331019613238275
    },
    {
      "source": "ce79649b126dfe9e3cbeb1ecd64a80708bbd5538",
      "target": "7aeadf7b9d839bdd9dbfbff5239f71792ff0c04c",
      "weight": 0.22900170642803017
    },
    {
      "source": "751b71158b7dcd2a7949e72a6ad8fb13657a401c",
      "target": "4702a22a3c2da1284a88d5e608d38cd106d66736",
      "weight": 0.39588281329608943
    },
    {
      "source": "6bdafb965e94c5240db2c30f20c37c4b4dd0e451",
      "target": "dc5d707a8f9efaf32d2319ec2ca8d2619ef2926e",
      "weight": 0.35692898224157255
    },
    {
      "source": "f27c847e2909f30745f4a3528b574f5acfd76ea7",
      "target": "9121dcd10df00e5cc51dc94400e0325e0ae47bb9",
      "weight": 0.37154792809648807
    },
    {
      "source": "f27c847e2909f30745f4a3528b574f5acfd76ea7",
      "target": "1ec9b653475287e95fdaef2f5247f82a8376c56c",
      "weight": 0.4550844435006144
    },
    {
      "source": "f27c847e2909f30745f4a3528b574f5acfd76ea7",
      "target": "cc24f933b343b6a9701088cf6ae1dbf3299c0c9e",
      "weight": 0.4228923722814588
    },
    {
      "source": "f27c847e2909f30745f4a3528b574f5acfd76ea7",
      "target": "dc5d707a8f9efaf32d2319ec2ca8d2619ef2926e",
      "weight": 0.404966700387716
    },
    {
      "source": "f27c847e2909f30745f4a3528b574f5acfd76ea7",
      "target": "6bdafb965e94c5240db2c30f20c37c4b4dd0e451",
      "weight": 0.2869813816301231
    },
    {
      "source": "0594eaa8dfe580678a2382aaf77ac3582c872a97",
      "target": "a4b728dbbf5afdc231afb95ad4e5c2ececdefc48",
      "weight": 0.44989532132576293
    },
    {
      "source": "442b5ec3aad4b099e71d6203a62eb7ec7519544c",
      "target": "e91934d66d9133d854ff0a4cafbe7966584bbf97",
      "weight": 0.3191520821240572
    },
    {
      "source": "50a260631a28bfed18eccf8ebfc75ff34917518f",
      "target": "4702a22a3c2da1284a88d5e608d38cd106d66736",
      "weight": 0.26224117270310743
    },
    {
      "source": "50a260631a28bfed18eccf8ebfc75ff34917518f",
      "target": "1970ace992d742bdf098de08a82817b05ef87477",
      "weight": 0.24683682971148732
    },
    {
      "source": "401c8d72a9b275e88e6ba159d8d646cfb9f397aa",
      "target": "595adb75ddeb90760c79e89b76d99e55079e0708",
      "weight": 0.33490524343224537
    },
    {
      "source": "ab70c5e1a338cb470ec39c22a4f10e0f19e61edd",
      "target": "d2fce7480111d66a74caa801a236f71ab021c42c",
      "weight": 0.2502327974667575
    },
    {
      "source": "ab70c5e1a338cb470ec39c22a4f10e0f19e61edd",
      "target": "ba12a9915553b3b42df17a33afcfd547821d8cc3",
      "weight": 0.28382877635388737
    },
    {
      "source": "ab70c5e1a338cb470ec39c22a4f10e0f19e61edd",
      "target": "d40c77c010c8dbef6142903a02f2a73a85012d5d",
      "weight": 0.32751255674663127
    },
    {
      "source": "39240f94c9915d9f9959c34b1dc68593894531e6",
      "target": "e38e70580acb204c05096de8da90b7ab1d4bdb6b",
      "weight": 0.2588188631692786
    },
    {
      "source": "1b026103e33b4c9eb637bc6f34715e22636b3492",
      "target": "50e997b23a534a6fbfd32d63990fa80373ec7c6b",
      "weight": 0.24506388855581093
    },
    {
      "source": "2fe2f849b94cf08b559226bc9d78adcaef5ef186",
      "target": "16fa1a8575ff56781b6b83726906754ed4e5f3a7",
      "weight": 0.2714542678829058
    },
    {
      "source": "2fe2f849b94cf08b559226bc9d78adcaef5ef186",
      "target": "50a260631a28bfed18eccf8ebfc75ff34917518f",
      "weight": 0.38921565784164225
    },
    {
      "source": "2fe2f849b94cf08b559226bc9d78adcaef5ef186",
      "target": "ac9cc0c28838a037e77f4e19433de170f47b3de9",
      "weight": 0.26925794142817494
    },
    {
      "source": "99fbe810d4194684be03458fdfebacb12d8a5c4e",
      "target": "9a4718faa07a32cf1dce745062181d3342e9b054",
      "weight": 0.21859318334284653
    },
    {
      "source": "99fbe810d4194684be03458fdfebacb12d8a5c4e",
      "target": "9998291d71f4e8ddf59f4b016b19df1f848eeed1",
      "weight": 0.27108603914335194
    },
    {
      "source": "5553f9508dd1056ecc20c5b1f367e9a07e2c7e81",
      "target": "72e23cdc3accca1f09e2e19446bc475368c912d0",
      "weight": 0.2469324564229376
    },
    {
      "source": "5553f9508dd1056ecc20c5b1f367e9a07e2c7e81",
      "target": "714e21409b8c4f7788ac8c93795249a4e45e51ce",
      "weight": 0.2648774239284136
    },
    {
      "source": "957a3d34303b424fe90a279cf5361253c93ac265",
      "target": "88589b0b2d2d8caa09d8ca94414343455ae87d7c",
      "weight": 0.3108251526229675
    },
    {
      "source": "17534840dc6016229a577a66f108a1564b8a0131",
      "target": "a4ef2c7c321dea04d8ea91ec1ce0710ebce4f848",
      "weight": 0.4217449113303252
    },
    {
      "source": "17534840dc6016229a577a66f108a1564b8a0131",
      "target": "c064efa0419b75ba131ec8470ed80f01e1a80f64",
      "weight": 0.2526642056205025
    },
    {
      "source": "ffc855594cad345ea5a1cce2ee27095bec767bc8",
      "target": "9998291d71f4e8ddf59f4b016b19df1f848eeed1",
      "weight": 0.286370244207236
    },
    {
      "source": "0afbaae7d0bfe98a04a3b7dd1ba69e25f32b24c6",
      "target": "2dcc1f5f1ba11f99e09f22c53d02aa66cec35c48",
      "weight": 0.31464934862230104
    },
    {
      "source": "0afbaae7d0bfe98a04a3b7dd1ba69e25f32b24c6",
      "target": "17534840dc6016229a577a66f108a1564b8a0131",
      "weight": 0.32689192008185053
    },
    {
      "source": "0afbaae7d0bfe98a04a3b7dd1ba69e25f32b24c6",
      "target": "a4ef2c7c321dea04d8ea91ec1ce0710ebce4f848",
      "weight": 0.3488047337602671
    },
    {
      "source": "0b8e8760d7dd64c7439019aeb3b6ac55dd5075d4",
      "target": "b1f0ac0b5e4044251dcdcce78e0046feb43cc7c7",
      "weight": 0.2487751215165584
    },
    {
      "source": "0b8e8760d7dd64c7439019aeb3b6ac55dd5075d4",
      "target": "c57467e652f3f9131b3e7e40c23059abe395f01d",
      "weight": 0.2914030869017028
    },
    {
      "source": "164e41a60120917d13fb69e183ee3c996b6c9414",
      "target": "10e9943b3a974ac6175ffec3228e670ec9d2cc18",
      "weight": 0.23893290933484365
    },
    {
      "source": "164e41a60120917d13fb69e183ee3c996b6c9414",
      "target": "7aeadf7b9d839bdd9dbfbff5239f71792ff0c04c",
      "weight": 0.21707529315816512
    },
    {
      "source": "164e41a60120917d13fb69e183ee3c996b6c9414",
      "target": "cec0cbc2dd6d7975714110632b6bfcb5c1927ec3",
      "weight": 0.2527104629407521
    },
    {
      "source": "164e41a60120917d13fb69e183ee3c996b6c9414",
      "target": "310f5543603bef94d42366878a14161db1bf45de",
      "weight": 0.48704401277148196
    },
    {
      "source": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e",
      "target": "f462bb00b8c4379c4a4699b66a19ce10da530b08",
      "weight": 0.27992112840047423
    },
    {
      "source": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e",
      "target": "714e21409b8c4f7788ac8c93795249a4e45e51ce",
      "weight": 0.28819683541124685
    },
    {
      "source": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e",
      "target": "1871c0bc27bd5cd5f2f6c3d6cedeb4b8c5a89499",
      "weight": 0.24086354439340713
    },
    {
      "source": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e",
      "target": "409b43b8cd8a2ba69f93e80c2bacc0126238b550",
      "weight": 0.4441792569109358
    },
    {
      "source": "e91934d66d9133d854ff0a4cafbe7966584bbf97",
      "target": "bbe5dfbecfd1bed7556b9c8269b0d363faa24973",
      "weight": 0.3378228235648426
    },
    {
      "source": "e91934d66d9133d854ff0a4cafbe7966584bbf97",
      "target": "bb5e6bbd4ad8c613a0bac3db23940c03ce94bec9",
      "weight": 0.40388593313609733
    },
    {
      "source": "4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a",
      "target": "1ec9b653475287e95fdaef2f5247f82a8376c56c",
      "weight": 0.26275292189494037
    },
    {
      "source": "4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a",
      "target": "f462bb00b8c4379c4a4699b66a19ce10da530b08",
      "weight": 0.24722496463665738
    },
    {
      "source": "4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a",
      "target": "3ea79430455304c782572dfb6ca3e5230b0351de",
      "weight": 0.41917445256917696
    },
    {
      "source": "4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a",
      "target": "cec0cbc2dd6d7975714110632b6bfcb5c1927ec3",
      "weight": 0.34626829533593384
    },
    {
      "source": "4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a",
      "target": "6f4093a7ad5378e8cd3b73a52fbec80b784c107d",
      "weight": 0.46667739746278
    },
    {
      "source": "d28fed119d9293af31776205150b3c34f3adc82b",
      "target": "e06b703146c46a6455fd0c33077de1bea5fdd877",
      "weight": 0.2938375140058552
    },
    {
      "source": "1ee05cd919590eaba129caa0fda5e850c87b75a5",
      "target": "9121dcd10df00e5cc51dc94400e0325e0ae47bb9",
      "weight": 0.2510291361046403
    },
    {
      "source": "1ee05cd919590eaba129caa0fda5e850c87b75a5",
      "target": "1ec9b653475287e95fdaef2f5247f82a8376c56c",
      "weight": 0.28809036717548475
    },
    {
      "source": "1ee05cd919590eaba129caa0fda5e850c87b75a5",
      "target": "cc24f933b343b6a9701088cf6ae1dbf3299c0c9e",
      "weight": 0.32146554942355854
    },
    {
      "source": "1ee05cd919590eaba129caa0fda5e850c87b75a5",
      "target": "f996d5ee3b8ad3c60510862a92fd72c6a41777e0",
      "weight": 0.40079054248775026
    },
    {
      "source": "1ee05cd919590eaba129caa0fda5e850c87b75a5",
      "target": "9fb327c55a30b9771a364f45f33f77778756a164",
      "weight": 0.35172051324332465
    },
    {
      "source": "96da196d6f8c947db03d13759f030642f8234abf",
      "target": "714e21409b8c4f7788ac8c93795249a4e45e51ce",
      "weight": 0.31131192559644627
    },
    {
      "source": "96da196d6f8c947db03d13759f030642f8234abf",
      "target": "981970d0f586761e7cdd978670c6a8f46990f514",
      "weight": 0.28449793576567406
    },
    {
      "source": "96da196d6f8c947db03d13759f030642f8234abf",
      "target": "d2fce7480111d66a74caa801a236f71ab021c42c",
      "weight": 0.2468694005468294
    },
    {
      "source": "96da196d6f8c947db03d13759f030642f8234abf",
      "target": "8ec10ffe0c1fc8f6a92d541f0e002e78080b564a",
      "weight": 0.0764967691762385
    },
    {
      "source": "96da196d6f8c947db03d13759f030642f8234abf",
      "target": "c57467e652f3f9131b3e7e40c23059abe395f01d",
      "weight": 0.40900107800609753
    },
    {
      "source": "96da196d6f8c947db03d13759f030642f8234abf",
      "target": "442b5ec3aad4b099e71d6203a62eb7ec7519544c",
      "weight": 0.27995191644269235
    },
    {
      "source": "96da196d6f8c947db03d13759f030642f8234abf",
      "target": "3ea79430455304c782572dfb6ca3e5230b0351de",
      "weight": 0.28362852432000685
    },
    {
      "source": "96da196d6f8c947db03d13759f030642f8234abf",
      "target": "29f86d6d1eaba6a466c231f6906b18eae4b2b484",
      "weight": 0.25179771654275745
    },
    {
      "source": "96da196d6f8c947db03d13759f030642f8234abf",
      "target": "0b8e8760d7dd64c7439019aeb3b6ac55dd5075d4",
      "weight": 0.2666873317874745
    },
    {
      "source": "96da196d6f8c947db03d13759f030642f8234abf",
      "target": "98e702ef2f64ab2643df9e80b1bd034334142e62",
      "weight": 0.2762756740782306
    },
    {
      "source": "96da196d6f8c947db03d13759f030642f8234abf",
      "target": "d28fed119d9293af31776205150b3c34f3adc82b",
      "weight": 0.2863867153448444
    },
    {
      "source": "96da196d6f8c947db03d13759f030642f8234abf",
      "target": "572ed945b06818472105bd17cfeb355d4e46c5e5",
      "weight": 0.28174917080709105
    },
    {
      "source": "96da196d6f8c947db03d13759f030642f8234abf",
      "target": "b66e4257aa8856df537f03f6a12341f489eb6500",
      "weight": 0.29084953382569056
    },
    {
      "source": "96da196d6f8c947db03d13759f030642f8234abf",
      "target": "e5cb26148791b57bfd36aa26ce2401e231d01b57",
      "weight": 0.29841156707605887
    },
    {
      "source": "96da196d6f8c947db03d13759f030642f8234abf",
      "target": "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e",
      "weight": 0.30920694908019997
    },
    {
      "source": "96da196d6f8c947db03d13759f030642f8234abf",
      "target": "ab70c5e1a338cb470ec39c22a4f10e0f19e61edd",
      "weight": 0.3358985612098959
    },
    {
      "source": "96da196d6f8c947db03d13759f030642f8234abf",
      "target": "d40c77c010c8dbef6142903a02f2a73a85012d5d",
      "weight": 0.29807408906676275
    },
    {
      "source": "96da196d6f8c947db03d13759f030642f8234abf",
      "target": "595adb75ddeb90760c79e89b76d99e55079e0708",
      "weight": 0.28461271705656754
    },
    {
      "source": "44ee4165b8a3811dc7d557f99150ff9e62f3733f",
      "target": "136b0ef7bd7f642a285b1d61ddf7abfc7ddc026f",
      "weight": 0.2754692690966295
    },
    {
      "source": "44ee4165b8a3811dc7d557f99150ff9e62f3733f",
      "target": "e38e70580acb204c05096de8da90b7ab1d4bdb6b",
      "weight": 0.28191317911394054
    },
    {
      "source": "f4e32b928d7cc27447e312bdc052aa75888045aa",
      "target": "5135a8f690c66c3b64928227443c4f9378bd20e1",
      "weight": 0.4023867479065338
    },
    {
      "source": "f4e32b928d7cc27447e312bdc052aa75888045aa",
      "target": "442b5ec3aad4b099e71d6203a62eb7ec7519544c",
      "weight": 0.4062217584497678
    },
    {
      "source": "9fb327c55a30b9771a364f45f33f77778756a164",
      "target": "cc24f933b343b6a9701088cf6ae1dbf3299c0c9e",
      "weight": 0.3240684062249988
    },
    {
      "source": "9fb327c55a30b9771a364f45f33f77778756a164",
      "target": "f996d5ee3b8ad3c60510862a92fd72c6a41777e0",
      "weight": 0.3310426389820659
    },
    {
      "source": "9fb327c55a30b9771a364f45f33f77778756a164",
      "target": "ac9cc0c28838a037e77f4e19433de170f47b3de9",
      "weight": 0.23814430258075997
    },
    {
      "source": "e5cb26148791b57bfd36aa26ce2401e231d01b57",
      "target": "714e21409b8c4f7788ac8c93795249a4e45e51ce",
      "weight": 0.30958689662798516
    },
    {
      "source": "e5cb26148791b57bfd36aa26ce2401e231d01b57",
      "target": "21c37fd6d00d6c727ec10f0c871c1ddf6e4f7ed9",
      "weight": 0.2827042852481757
    },
    {
      "source": "e5cb26148791b57bfd36aa26ce2401e231d01b57",
      "target": "981970d0f586761e7cdd978670c6a8f46990f514",
      "weight": 0.8825435935393697
    },
    {
      "source": "e5cb26148791b57bfd36aa26ce2401e231d01b57",
      "target": "c57467e652f3f9131b3e7e40c23059abe395f01d",
      "weight": 0.25976480040779437
    },
    {
      "source": "e5cb26148791b57bfd36aa26ce2401e231d01b57",
      "target": "53e5db85e2a7442f20670be2ae25019fcf9d27a2",
      "weight": 0.41922529106398854
    },
    {
      "source": "e5cb26148791b57bfd36aa26ce2401e231d01b57",
      "target": "861f670073679ba05990f3bc6d119b13ab62aca7",
      "weight": 0.37347832950437576
    },
    {
      "source": "b66e4257aa8856df537f03f6a12341f489eb6500",
      "target": "5b22bdc6aedf13d812509dd0f768353eb1469a79",
      "weight": 0.2645009042523857
    },
    {
      "source": "ba12a9915553b3b42df17a33afcfd547821d8cc3",
      "target": "d80166681f3344a1946b8bfc623f4679d979ee10",
      "weight": 0.5223190082962486
    },
    {
      "source": "58fc305734a0d5d849ae69b9233af082d712197e",
      "target": "72e23cdc3accca1f09e2e19446bc475368c912d0",
      "weight": 0.27605103223192456
    }
  ]
}