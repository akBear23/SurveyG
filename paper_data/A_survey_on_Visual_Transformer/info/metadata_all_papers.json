{
    "c8b25fab5608c3e033d34b4483ec47e68ba109b7.pdf": {
        "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
        "authors": [
            "Ze Liu",
            "Yutong Lin",
            "Yue Cao",
            "Han Hu",
            "Yixuan Wei",
            "Zheng Zhang",
            "Stephen Lin",
            "B. Guo"
        ],
        "published_date": "2021",
        "abstract": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c8b25fab5608c3e033d34b4483ec47e68ba109b7.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 24050,
        "score": 6012.5
    },
    "7a9a708ca61c14886aa0dcd6d13dac7879713f5f.pdf": {
        "title": "SwinIR: Image Restoration Using Swin Transformer",
        "authors": [
            "Jingyun Liang",
            "Jie Cao",
            "Guolei Sun",
            "K. Zhang",
            "L. Gool",
            "R. Timofte"
        ],
        "published_date": "2021",
        "abstract": "Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by up to 0.14\u223c0.45dB, while the total number of parameters can be reduced by up to 67%.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7a9a708ca61c14886aa0dcd6d13dac7879713f5f.pdf",
        "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
        "citationCount": 3340,
        "score": 835.0
    },
    "d40c77c010c8dbef6142903a02f2a73a85012d5d.pdf": {
        "title": "A Survey on Vision Transformer",
        "authors": [
            "Kai Han",
            "Yunhe Wang",
            "Hanting Chen",
            "Xinghao Chen",
            "Jianyuan Guo",
            "Zhenhua Liu",
            "Yehui Tang",
            "An Xiao",
            "Chunjing Xu",
            "Yixing Xu",
            "Zhaohui Yang",
            "Yiman Zhang",
            "D. Tao"
        ],
        "published_date": "2020",
        "abstract": "Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d40c77c010c8dbef6142903a02f2a73a85012d5d.pdf",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citationCount": 2562,
        "score": 512.4
    },
    "0eff37167876356da2163b2e396df2719adf7de9.pdf": {
        "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
        "authors": [
            "Chun-Fu Chen",
            "Quanfu Fan",
            "Rameswar Panda"
        ],
        "published_date": "2021",
        "abstract": "The recently developed vision transformer (ViT) has achieved promising results on image classification compared to convolutional neural networks. Inspired by this, in this paper, we study how to learn multi-scale feature representations in transformer models for image classification. To this end, we propose a dual-branch transformer to com-bine image patches (i.e., tokens in a transformer) of different sizes to produce stronger image features. Our approach processes small-patch and large-patch tokens with two separate branches of different computational complexity and these tokens are then fused purely by attention multiple times to complement each other. Furthermore, to reduce computation, we develop a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches. Our proposed cross-attention only requires linear time for both computational and memory complexity instead of quadratic time otherwise. Extensive experiments demonstrate that our approach performs better than or on par with several concurrent works on vision transformer, in addition to efficient CNN models. For example, on the ImageNet1K dataset, with some architectural changes, our approach outperforms the recent DeiT by a large margin of 2% with a small to moderate increase in FLOPs and model parameters. Our source codes and models are available at https://github.com/IBM/CrossViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0eff37167876356da2163b2e396df2719adf7de9.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 1640,
        "score": 410.0
    },
    "da74a10824193be9d3889ce0d6ed4c6f8ee48b9e.pdf": {
        "title": "MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer",
        "authors": [
            "Sachin Mehta",
            "Mohammad Rastegari"
        ],
        "published_date": "2021",
        "abstract": "Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters. Our source code is open-source and available at: https://github.com/apple/ml-cvnets",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/da74a10824193be9d3889ce0d6ed4c6f8ee48b9e.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 1482,
        "score": 370.5
    },
    "a09cbcaac305884f043810afc4fa4053099b5970.pdf": {
        "title": "Exploring Plain Vision Transformer Backbones for Object Detection",
        "authors": [
            "Yanghao Li",
            "Hanzi Mao",
            "Ross B. Girshick",
            "Kaiming He"
        ],
        "published_date": "2022",
        "abstract": "We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors. Code for ViTDet is available in Detectron2.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a09cbcaac305884f043810afc4fa4053099b5970.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 918,
        "score": 306.0
    },
    "2fe2f849b94cf08b559226bc9d78adcaef5ef186.pdf": {
        "title": "AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition",
        "authors": [
            "Shoufa Chen",
            "Chongjian Ge",
            "Zhan Tong",
            "Jiangliu Wang",
            "Yibing Song",
            "Jue Wang",
            "Ping Luo"
        ],
        "published_date": "2022",
        "abstract": "Pretraining Vision Transformers (ViTs) has achieved great success in visual recognition. A following scenario is to adapt a ViT to various image and video recognition tasks. The adaptation is challenging because of heavy computation and memory storage. Each model needs an independent and complete finetuning process to adapt to different tasks, which limits its transferability to different visual domains. To address this challenge, we propose an effective adaptation approach for Transformer, namely AdaptFormer, which can adapt the pre-trained ViTs into many different image and video tasks efficiently. It possesses several benefits more appealing than prior arts. Firstly, AdaptFormer introduces lightweight modules that only add less than 2% extra parameters to a ViT, while it is able to increase the ViT's transferability without updating its original pre-trained parameters, significantly outperforming the existing 100\\% fully fine-tuned models on action recognition benchmarks. Secondly, it can be plug-and-play in different Transformers and scalable to many visual tasks. Thirdly, extensive experiments on five image and video datasets show that AdaptFormer largely improves ViTs in the target domains. For example, when updating just 1.5% extra parameters, it achieves about 10% and 19% relative improvement compared to the fully fine-tuned models on Something-Something~v2 and HMDB51, respectively. Code is available at https://github.com/ShoufaChen/AdaptFormer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2fe2f849b94cf08b559226bc9d78adcaef5ef186.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 806,
        "score": 268.66666666666663
    },
    "e5cb26148791b57bfd36aa26ce2401e231d01b57.pdf": {
        "title": "Vision Transformer with Deformable Attention",
        "authors": [
            "Zhuofan Xia",
            "Xuran Pan",
            "S. Song",
            "Li Erran Li",
            "Gao Huang"
        ],
        "published_date": "2022",
        "abstract": "Transformers have recently shown superior performances on various vision tasks. The large, sometimes even global, receptive field endows Transformer models with higher representation power over their CNN counterparts. Nevertheless, simply enlarging receptive field also gives rise to several concerns. On the one hand, using dense attention e.g., in ViT, leads to excessive memory and computational cost, and features can be influenced by irrelevant parts which are beyond the region of interests. On the other hand, the sparse attention adopted in PVT or Swin Transformer is data agnostic and may limit the ability to model long range relations. To mitigate these issues, we propose a novel deformable selfattention module, where the positions of key and value pairs in selfattention are selected in a data-dependent way. This flexible scheme enables the self-attention module to focus on relevant re-gions and capture more informative features. On this basis, we present Deformable Attention Transformer, a general backbone model with deformable attention for both image classification and dense prediction tasks. Extensive experi-ments show that our models achieve consistently improved results on comprehensive benchmarks. Code is available at https://github.com/LeapLabTHU/DAT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e5cb26148791b57bfd36aa26ce2401e231d01b57.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 560,
        "score": 186.66666666666666
    },
    "96da196d6f8c947db03d13759f030642f8234abf.pdf": {
        "title": "DeepViT: Towards Deeper Vision Transformer",
        "authors": [
            "Daquan Zhou",
            "Bingyi Kang",
            "Xiaojie Jin",
            "Linjie Yang",
            "Xiaochen Lian",
            "Qibin Hou",
            "Jiashi Feng"
        ],
        "published_date": "2021",
        "abstract": "Vision transformers (ViTs) have been successfully applied in image classification tasks recently. In this paper, we show that, unlike convolution neural networks (CNNs)that can be improved by stacking more convolutional layers, the performance of ViTs saturate fast when scaled to be deeper. More specifically, we empirically observe that such scaling difficulty is caused by the attention collapse issue: as the transformer goes deeper, the attention maps gradually become similar and even much the same after certain layers. In other words, the feature maps tend to be identical in the top layers of deep ViT models. This fact demonstrates that in deeper layers of ViTs, the self-attention mechanism fails to learn effective concepts for representation learning and hinders the model from getting expected performance gain. Based on above observation, we propose a simple yet effective method, named Re-attention, to re-generate the attention maps to increase their diversity at different layers with negligible computation and memory cost. The pro-posed method makes it feasible to train deeper ViT models with consistent performance improvements via minor modification to existing ViT models. Notably, when training a deep ViT model with 32 transformer blocks, the Top-1 classification accuracy can be improved by 1.6% on ImageNet. Code is publicly available at https://github.com/zhoudaquan/dvit_repo.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/96da196d6f8c947db03d13759f030642f8234abf.pdf",
        "venue": "arXiv.org",
        "citationCount": 548,
        "score": 137.0
    },
    "751b71158b7dcd2a7949e72a6ad8fb13657a401c.pdf": {
        "title": "Visual Saliency Transformer",
        "authors": [
            "Nian Liu",
            "Ni Zhang",
            "Kaiyuan Wan",
            "Junwei Han",
            "Ling Shao"
        ],
        "published_date": "2021",
        "abstract": "Existing state-of-the-art saliency detection methods heavily rely on CNN-based architectures. Alternatively, we rethink this task from a convolution-free sequence-to-sequence perspective and predict saliency by modeling long-range dependencies, which can not be achieved by convolution. Specifically, we develop a novel unified model based on a pure transformer, namely, Visual Saliency Transformer (VST), for both RGB and RGB-D salient object detection (SOD). It takes image patches as inputs and leverages the transformer to propagate global contexts among image patches. Unlike conventional architectures used in Vision Transformer (ViT), we leverage multi-level token fusion and propose a new token upsampling method under the transformer framework to get high-resolution detection results. We also develop a token-based multi-task decoder to simultaneously perform saliency and boundary detection by introducing task-related tokens and a novel patch-task-attention mechanism. Experimental results show that our model outperforms existing methods on both RGB and RGB-D SOD benchmark datasets. Most importantly, our whole framework not only provides a new perspective for the SOD field but also shows a new paradigm for transformer-based dense prediction models. Code is available at https://github.com/nnizhang/VST.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/751b71158b7dcd2a7949e72a6ad8fb13657a401c.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 391,
        "score": 97.75
    },
    "164e41a60120917d13fb69e183ee3c996b6c9414.pdf": {
        "title": "Vision Transformer for Small-Size Datasets",
        "authors": [
            "Seung Hoon Lee",
            "Seunghyun Lee",
            "B. Song"
        ],
        "published_date": "2021",
        "abstract": "Recently, the Vision Transformer (ViT), which applied the transformer structure to the image classification task, has outperformed convolutional neural networks. However, the high performance of the ViT results from pre-training using a large-size dataset such as JFT-300M, and its dependence on a large dataset is interpreted as due to low locality inductive bias. This paper proposes Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA), which effectively solve the lack of locality inductive bias and enable it to learn from scratch even on small-size datasets. Moreover, SPT and LSA are generic and effective add-on modules that are easily applicable to various ViTs. Experimental results show that when both SPT and LSA were applied to the ViTs, the performance improved by an average of 2.96% in Tiny-ImageNet, which is a representative small-size dataset. Especially, Swin Transformer achieved an overwhelming performance improvement of 4.08% thanks to the proposed SPT and LSA.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/164e41a60120917d13fb69e183ee3c996b6c9414.pdf",
        "venue": "arXiv.org",
        "citationCount": 250,
        "score": 62.5
    },
    "5553f9508dd1056ecc20c5b1f367e9a07e2c7e81.pdf": {
        "title": "StyleSwin: Transformer-based GAN for High-resolution Image Generation",
        "authors": [
            "Bo Zhang",
            "Shuyang Gu",
            "Bo Zhang",
            "Jianmin Bao",
            "Dong Chen",
            "Fang Wen",
            "Yong Wang",
            "B. Guo"
        ],
        "published_date": "2021",
        "abstract": "Despite the tantalizing success in a broad of vision tasks, transformers have not yet demonstrated on-par ability as ConvNets in high-resolution image generative modeling. In this paper, we seek to explore using pure transformers to build a generative adversarial network for high-resolution image synthesis. To this end, we believe that local attention is crucial to strike the balance between computational efficiency and modeling capacity. Hence, the proposed generator adopts Swin transformer in a style-based architecture. To achieve a larger receptive field, we propose double attention which simultaneously leverages the context of the local and the shifted windows, leading to improved generation quality. Moreover, we show that offering the knowledge of the absolute position that has been lost in window-based transformers greatly benefits the generation quality. The proposed StyleSwin is scalable to high resolutions, with both the coarse geometry and fine structures benefit from the strong expressivity of transformers. However, blocking artifacts occur during high-resolution synthesis because performing the local attention in a block-wise manner may break the spatial coherency. To solve this, we empirically investigate various solutions, among which we find that employing a wavelet discriminator to examine the spectral discrepancy effectively suppresses the artifacts. Extensive experiments show the superiority over prior transformer-based GANs, especially on high resolutions, e.g., $1024 \\times$ 1024. The StyleSwin, without complex training strategies, excels over StyleGAN on CelebA-HQ 1024, and achieves on-par performance on FFHQ-1024, proving the promise of using transformers for high-resolution image generation. The code and pretrained models are available at https://github.com/microsoft/StyleSwin.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/5553f9508dd1056ecc20c5b1f367e9a07e2c7e81.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 249,
        "score": 62.25
    },
    "226fcbe55235d873bedb2fcf5b981bd5ec860d4f.pdf": {
        "title": "SwinBTS: A Method for 3D Multimodal Brain Tumor Segmentation Using Swin Transformer",
        "authors": [
            "Yun Jiang",
            "Yuan Zhang",
            "Xinyi Lin",
            "Jinkun Dong",
            "Tongtong Cheng",
            "Jing Liang"
        ],
        "published_date": "2022",
        "abstract": "Brain tumor semantic segmentation is a critical medical image processing work, which aids clinicians in diagnosing patients and determining the extent of lesions. Convolutional neural networks (CNNs) have demonstrated exceptional performance in computer vision tasks in recent years. For 3D medical image tasks, deep convolutional neural networks based on an encoder\u2013decoder structure and skip-connection have been frequently used. However, CNNs have the drawback of being unable to learn global and remote semantic information well. On the other hand, the transformer has recently found success in natural language processing and computer vision as a result of its usage of a self-attention mechanism for global information modeling. For demanding prediction tasks, such as 3D medical picture segmentation, local and global characteristics are critical. We propose SwinBTS, a new 3D medical picture segmentation approach, which combines a transformer, convolutional neural network, and encoder\u2013decoder structure to define the 3D brain tumor semantic segmentation job as a sequence-to-sequence prediction challenge in this research. To extract contextual data, the 3D Swin Transformer is utilized as the network\u2019s encoder and decoder, and convolutional operations are employed for upsampling and downsampling. Finally, we achieve segmentation results using an improved Transformer module that we built for increasing detail feature extraction. Extensive experimental results on the BraTS 2019, BraTS 2020, and BraTS 2021 datasets reveal that SwinBTS outperforms state-of-the-art 3D algorithms for brain tumor segmentation on 3D MRI scanned images.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/226fcbe55235d873bedb2fcf5b981bd5ec860d4f.pdf",
        "venue": "Brain Science",
        "citationCount": 181,
        "score": 60.33333333333333
    },
    "5c1b7e400ba8b3c3b04522f7bb3af17ad68d0585.pdf": {
        "title": "Vision transformer and explainable transfer learning models for auto detection of kidney cyst, stone and tumor from CT-radiography",
        "authors": [
            "Md. Nazmul Islam",
            "Madina Hasan",
            "Md. Kabir Hossain",
            "Md. Golam Rabiul Alam",
            "Md. Zia Uddin",
            "A. Soylu"
        ],
        "published_date": "2022",
        "abstract": "Renal failure, a public health concern, and the scarcity of nephrologists around the globe have necessitated the development of an AI-based system to auto-diagnose kidney diseases. This research deals with the three major renal diseases categories: kidney stones, cysts, and tumors, and gathered and annotated a total of 12,446 CT whole abdomen and urogram images in order to construct an AI-based kidney diseases diagnostic system and contribute to the AI community\u2019s research scope e.g., modeling digital-twin of renal functions. The collected images were exposed to exploratory data analysis, which revealed that the images from all of the classes had the same type of mean color distribution. Furthermore, six machine learning models were built, three of which are based on the state-of-the-art variants of the Vision transformers EANet, CCT, and Swin transformers, while the other three are based on well-known deep learning models Resnet, VGG16, and Inception v3, which were adjusted in the last layers. While the VGG16 and CCT models performed admirably, the swin transformer outperformed all of them in terms of accuracy, with an accuracy of 99.30 percent. The F1 score and precision and recall comparison reveal that the Swin transformer outperforms all other models and that it is the quickest to train. The study also revealed the blackbox of the VGG16, Resnet50, and Inception models, demonstrating that VGG16 is superior than Resnet50 and Inceptionv3 in terms of monitoring the necessary anatomy abnormalities. We believe that the superior accuracy of our Swin transformer-based model and the VGG16-based model can both be useful in diagnosing kidney tumors, cysts, and stones.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/5c1b7e400ba8b3c3b04522f7bb3af17ad68d0585.pdf",
        "venue": "Scientific Reports",
        "citationCount": 177,
        "score": 59.0
    },
    "a4b728dbbf5afdc231afb95ad4e5c2ececdefc48.pdf": {
        "title": "Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios",
        "authors": [
            "Jiashi Li",
            "Xin Xia",
            "W. Li",
            "Huixia Li",
            "Xing Wang",
            "Xuefeng Xiao",
            "Rui Wang",
            "Minghang Zheng",
            "Xin Pan"
        ],
        "published_date": "2022",
        "abstract": "Due to the complex attention mechanisms and model design, most existing vision Transformers (ViTs) can not perform as efficiently as convolutional neural networks (CNNs) in realistic industrial deployment scenarios, e.g. TensorRT and CoreML. This poses a distinct challenge: Can a visual neural network be designed to infer as fast as CNNs and perform as powerful as ViTs? Recent works have tried to design CNN-Transformer hybrid architectures to address this issue, yet the overall performance of these works is far away from satisfactory. To end these, we propose a next generation vision Transformer for efficient deployment in realistic industrial scenarios, namely Next-ViT, which dominates both CNNs and ViTs from the perspective of latency/accuracy trade-off. In this work, the Next Convolution Block (NCB) and Next Transformer Block (NTB) are respectively developed to capture local and global information with deployment-friendly mechanisms. Then, Next Hybrid Strategy (NHS) is designed to stack NCB and NTB in an efficient hybrid paradigm, which boosts performance in various downstream tasks. Extensive experiments show that Next-ViT significantly outperforms existing CNNs, ViTs and CNN-Transformer hybrid architectures with respect to the latency/accuracy trade-off across various vision tasks. On TensorRT, Next-ViT surpasses ResNet by 5.5 mAP (from 40.4 to 45.9) on COCO detection and 7.7% mIoU (from 38.8% to 46.5%) on ADE20K segmentation under similar latency. Meanwhile, it achieves comparable performance with CSWin, while the inference speed is accelerated by 3.6x. On CoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on COCO detection and 3.5% mIoU (from 45.1% to 48.6%) on ADE20K segmentation under similar latency. Our code and models are made public at: https://github.com/bytedance/Next-ViT",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a4b728dbbf5afdc231afb95ad4e5c2ececdefc48.pdf",
        "venue": "arXiv.org",
        "citationCount": 168,
        "score": 56.0
    },
    "0b8e8760d7dd64c7439019aeb3b6ac55dd5075d4.pdf": {
        "title": "Wave-ViT: Unifying Wavelet and Transformers for Visual Representation Learning",
        "authors": [
            "Ting Yao",
            "Yingwei Pan",
            "Yehao Li",
            "C. Ngo",
            "Tao Mei"
        ],
        "published_date": "2022",
        "abstract": "Multi-scale Vision Transformer (ViT) has emerged as a powerful backbone for computer vision tasks, while the self-attention computation in Transformer scales quadratically w.r.t. the input patch number. Thus, existing solutions commonly employ down-sampling operations (e.g., average pooling) over keys/values to dramatically reduce the computational cost. In this work, we argue that such over-aggressive down-sampling design is not invertible and inevitably causes information dropping especially for high-frequency components in objects (e.g., texture details). Motivated by the wavelet theory, we construct a new Wavelet Vision Transformer (\\textbf{Wave-ViT}) that formulates the invertible down-sampling with wavelet transforms and self-attention learning in a unified way. This proposal enables self-attention learning with lossless down-sampling over keys/values, facilitating the pursuing of a better efficiency-vs-accuracy trade-off. Furthermore, inverse wavelet transforms are leveraged to strengthen self-attention outputs by aggregating local contexts with enlarged receptive field. We validate the superiority of Wave-ViT through extensive experiments over multiple vision tasks (e.g., image recognition, object detection and instance segmentation). Its performances surpass state-of-the-art ViT backbones with comparable FLOPs. Source code is available at \\url{https://github.com/YehLi/ImageNetModel}.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0b8e8760d7dd64c7439019aeb3b6ac55dd5075d4.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 166,
        "score": 55.33333333333333
    },
    "17534840dc6016229a577a66f108a1564b8a0131.pdf": {
        "title": "A deep learning based approach for automated plant disease classification using vision transformer",
        "authors": [
            "Y. Borhani",
            "Javad Khoramdel",
            "E. Najafi"
        ],
        "published_date": "2022",
        "abstract": "Plant disease can diminish a considerable portion of the agricultural products on each farm. The main goal of this work is to provide visual information for the farmers to enable them to take the necessary preventive measures. A lightweight deep learning approach is proposed based on the Vision Transformer (ViT) for real-time automated plant disease classification. In addition to the ViT, the classical convolutional neural network (CNN) methods and the combination of CNN and ViT have been implemented for the plant disease classification. The models have been trained and evaluated on multiple datasets. Based on the comparison between the obtained results, it is concluded that although attention blocks increase the accuracy, they decelerate the prediction. Combining attention blocks with CNN blocks can compensate for the speed.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/17534840dc6016229a577a66f108a1564b8a0131.pdf",
        "venue": "Scientific Reports",
        "citationCount": 165,
        "score": 55.0
    },
    "b8cee43a51c44f8f4448e78e41ecf081987707cf.pdf": {
        "title": "Towards Robust Vision Transformer",
        "authors": [
            "Xiaofeng Mao",
            "Gege Qi",
            "Yuefeng Chen",
            "Xiaodan Li",
            "Ranjie Duan",
            "Shaokai Ye",
            "Yuan He",
            "Hui Xue"
        ],
        "published_date": "2021",
        "abstract": "Recent advances on Vision Transformer (ViT) and its improved variants have shown that self-attention-based networks surpass traditional Convolutional Neural Networks (CNNs) in most vision tasks. However, existing ViTs focus on the standard accuracy and computation cost, lacking the investigation of the intrinsic influence on model robustness and generalization. In this work, we conduct systematic evaluation on components of ViTs in terms of their impact on robustness to adversarial examples, common corruptions and distribution shifts. We find some components can be harmful to robustness. By leveraging robust components as building blocks of ViTs, we propose Robust Vision Transformer (RVT), which is a new vision transformer and has superior performance with strong robustness. Inspired by the findings during the evaluation, we further propose two new plug-and-play techniques called position-aware attention scaling and patch-wise augmentation to augment our RVT, which we abbreviate as RVT*. The experimental results of RVT on ImageNet and six robustness benchmarks demonstrate its advanced robustness and generalization ability compared with previous ViTs and state-of-the-art CNNs. Furthermore, RVT-S* achieves Top-1 rank on multiple robustness leaderboards including ImageNet-C, ImageNet-Sketch and ImageNet-R.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b8cee43a51c44f8f4448e78e41ecf081987707cf.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 205,
        "score": 51.25
    },
    "44ee4165b8a3811dc7d557f99150ff9e62f3733f.pdf": {
        "title": "ViT-V-Net: Vision Transformer for Unsupervised Volumetric Medical Image Registration",
        "authors": [
            "Junyu Chen",
            "Yufan He",
            "E. Frey",
            "Ye Li",
            "Yong Du"
        ],
        "published_date": "2021",
        "abstract": "In the last decade, convolutional neural networks (ConvNets) have dominated and achieved state-of-the-art performances in a variety of medical imaging applications. However, the performances of ConvNets are still limited by lacking the understanding of long-range spatial relations in an image. The recently proposed Vision Transformer (ViT) for image classification uses a purely self-attention-based model that learns long-range spatial relations to focus on the relevant parts of an image. Nevertheless, ViT emphasizes the low-resolution features because of the consecutive downsamplings, result in a lack of detailed localization information, making it unsuitable for image registration. Recently, several ViT-based image segmentation methods have been combined with ConvNets to improve the recovery of detailed localization information. Inspired by them, we present ViT-V-Net, which bridges ViT and ConvNet to provide volumetric medical image registration. The experimental results presented here demonstrate that the proposed architecture achieves superior performance to several top-performing registration methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/44ee4165b8a3811dc7d557f99150ff9e62f3733f.pdf",
        "venue": "arXiv.org",
        "citationCount": 202,
        "score": 50.5
    },
    "50a260631a28bfed18eccf8ebfc75ff34917518f.pdf": {
        "title": "Convolutional Bypasses Are Better Vision Transformer Adapters",
        "authors": [
            "Shibo Jie",
            "Zhi-Hong Deng"
        ],
        "published_date": "2022",
        "abstract": "The pretrain-then-finetune paradigm has been widely adopted in computer vision. But as the size of Vision Transformer (ViT) grows exponentially, the full finetuning becomes prohibitive in view of the heavier storage overhead. Motivated by parameter-efficient transfer learning (PETL) on language transformers, recent studies attempt to insert lightweight adaptation modules (e.g., adapter layers or prompt tokens) to pretrained ViT and only finetune these modules while the pretrained weights are frozen. However, these modules were originally proposed to finetune language models and did not take into account the prior knowledge specifically for visual tasks. In this paper, we propose to construct Convolutional Bypasses (Convpass) in ViT as adaptation modules, introducing only a small amount (less than 0.5% of model parameters) of trainable parameters to adapt the large ViT. Different from other PETL methods, Convpass benefits from the hard-coded inductive bias of convolutional layers and thus is more suitable for visual tasks, especially in the low-data regime. Experimental results on VTAB-1K benchmark and few-shot learning datasets show that Convpass outperforms current language-oriented adaptation modules, demonstrating the necessity to tailor vision-oriented adaptation modules for adapting vision models.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/50a260631a28bfed18eccf8ebfc75ff34917518f.pdf",
        "venue": "European Conference on Artificial Intelligence",
        "citationCount": 150,
        "score": 50.0
    },
    "3c14992a490cc31a7a38f5fab156c9da40a861d4.pdf": {
        "title": "SUNet: Swin Transformer UNet for Image Denoising",
        "authors": [
            "Chi-Mao Fan",
            "Tsung-Jung Liu",
            "Kuan-Hsien Liu"
        ],
        "published_date": "2022",
        "abstract": "Image restoration is a challenging ill-posed problem which also has been a long-standing issue. In the past few years, the convolution neural networks (CNNs) almost dominated the computer vision and had achieved considerable success in different levels of vision tasks including image restoration. However, recently the Swin Transformer-based model also shows impressive performance, even surpasses the CNN-based methods to become the state-of-the-art on high-level vision tasks. In this paper, we proposed a restoration model called SUNet which uses the Swin Transformer layer as our basic block and then is applied to UNet architecture for image denoising. The source code and pre-trained models are available at https://github.com/FanChiMao/SUNet.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3c14992a490cc31a7a38f5fab156c9da40a861d4.pdf",
        "venue": "International Symposium on Circuits and Systems",
        "citationCount": 139,
        "score": 46.33333333333333
    },
    "ab70c5e1a338cb470ec39c22a4f10e0f19e61edd.pdf": {
        "title": "CAT: Cross Attention in Vision Transformer",
        "authors": [
            "Hezheng Lin",
            "Xingyi Cheng",
            "Xiangyu Wu",
            "Fan Yang",
            "Dong Shen",
            "Zhongyuan Wang",
            "Qing Song",
            "Wei Yuan"
        ],
        "published_date": "2021",
        "abstract": "Since Transformer has found widespread use in NLP, the potential of Transformer in CV has been realized and has inspired many new approaches. However, the computation required for replacing word tokens with image patches for Transformer after the tokenization of the image is vast(e.g., ViT), which bottlenecks model training and inference. In this paper, we propose a new attention mechanism in Transformer termed Cross Attention, which alternates attention inner the image patch instead of the whole image to capture local information and apply attention between image patches which are divided from single-channel feature maps to capture global information. Both operations have less computation than standard self-attention in Transformer. Based on that, we build a hierarchical network called Cross Attention Transformer(CAT) for other vision tasks. Our model achieves 82.8% on ImageNet-1K, and improves the performance of other methods on COCO and ADE20K, illustrating that our network has the potential to serve as general backbones. The code and models are avalible at https://github.com/linhezheng19/CAT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ab70c5e1a338cb470ec39c22a4f10e0f19e61edd.pdf",
        "venue": "IEEE International Conference on Multimedia and Expo",
        "citationCount": 182,
        "score": 45.5
    },
    "1ee05cd919590eaba129caa0fda5e850c87b75a5.pdf": {
        "title": "FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer",
        "authors": [
            "Yang Lin",
            "Tianyu Zhang",
            "Peiqin Sun",
            "Zheng Li",
            "Shuchang Zhou"
        ],
        "published_date": "2021",
        "abstract": "Network quantization significantly reduces model inference complexity and has been widely used in real-world deployments. However, most existing quantization methods have been developed mainly on Convolutional Neural Networks (CNNs), and suffer severe degradation when applied to fully quantized vision transformers. In this work, we demonstrate that many of these difficulties arise because of serious inter-channel variation in LayerNorm inputs, and present, Power-of-Two Factor (PTF), a systematic method to reduce the performance degradation and inference complexity of fully quantized vision transformers. In addition, observing an extreme non-uniform distribution in attention maps, we propose Log-Int-Softmax (LIS) to sustain that and simplify inference by using 4-bit quantization and the BitShift operator. Comprehensive experiments on various transformer-based architectures and benchmarks show that our Fully Quantized Vision Transformer (FQ-ViT) outperforms previous works while even using lower bit-width on attention maps. For instance, we reach 84.89% top-1 accuracy with ViT-L on ImageNet and 50.8 mAP with Cascade Mask R-CNN (Swin-S) on COCO. To our knowledge, we are the first to achieve lossless accuracy degradation (~1%) on fully quantized vision transformers. The code is available at https://github.com/megvii-research/FQ-ViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1ee05cd919590eaba129caa0fda5e850c87b75a5.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 179,
        "score": 44.75
    },
    "9fb327c55a30b9771a364f45f33f77778756a164.pdf": {
        "title": "I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference",
        "authors": [
            "Zhikai Li",
            "Qingyi Gu"
        ],
        "published_date": "2022",
        "abstract": "Vision Transformers (ViTs) have achieved state-of-the-art performance on various computer vision applications. However, these models have considerable storage and computational overheads, making their deployment and efficient inference on edge devices challenging. Quantization is a promising approach to reducing model complexity, and the dyadic arithmetic pipeline can allow the quantized models to perform efficient integer-only inference. Unfortunately, dyadic arithmetic is based on the homogeneity condition in convolutional neural networks, which is not applicable to the non-linear components in ViTs, making integer-only inference of ViTs an open issue. In this paper, we propose I-ViT, an integer-only quantization scheme for ViTs, to enable ViTs to perform the entire computational graph of inference with integer arithmetic and bit-shifting, and without any floating-point arithmetic. In I-ViT, linear operations (e.g., MatMul and Dense) follow the integer-only pipeline with dyadic arithmetic, and non-linear operations (e.g., Softmax, GELU, and LayerNorm) are approximated by the proposed light-weight integer-only arithmetic methods. More specifically, I-ViT applies the proposed Shiftmax and ShiftGELU, which are designed to use integer bit-shifting to approximate the corresponding floating-point operations. We evaluate I-ViT on various benchmark models and the results show that integer-only INT8 quantization achieves comparable (or even slightly higher) accuracy to the full-precision (FP) baseline. Furthermore, we utilize TVM for practical hardware deployment on the GPU\u2019s integer arithmetic units, achieving 3.72 ~ 4.11 inference speedup compared to the FP model. Code of both Pytorch and TVM is released at https://github.com/zkkli/I-ViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9fb327c55a30b9771a364f45f33f77778756a164.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 122,
        "score": 40.666666666666664
    },
    "dfdb2894d50e095ce97f994ed6cee38554c4c84f.pdf": {
        "title": "Q-ViT: Accurate and Fully Quantized Low-bit Vision Transformer",
        "authors": [
            "Yanjing Li",
            "Sheng Xu",
            "Baochang Zhang",
            "Xianbin Cao",
            "Penglei Gao",
            "Guodong Guo"
        ],
        "published_date": "2022",
        "abstract": "The large pre-trained vision transformers (ViTs) have demonstrated remarkable performance on various visual tasks, but suffer from expensive computational and memory cost problems when deployed on resource-constrained devices. Among the powerful compression approaches, quantization extremely reduces the computation and memory consumption by low-bit parameters and bit-wise operations. However, low-bit ViTs remain largely unexplored and usually suffer from a significant performance drop compared with the real-valued counterparts. In this work, through extensive empirical analysis, we first identify the bottleneck for severe performance drop comes from the information distortion of the low-bit quantized self-attention map. We then develop an information rectification module (IRM) and a distribution guided distillation (DGD) scheme for fully quantized vision transformers (Q-ViT) to effectively eliminate such distortion, leading to a fully quantized ViTs. We evaluate our methods on popular DeiT and Swin backbones. Extensive experimental results show that our method achieves a much better performance than the prior arts. For example, our Q-ViT can theoretically accelerates the ViT-S by 6.14x and achieves about 80.9% Top-1 accuracy, even surpassing the full-precision counterpart by 1.0% on ImageNet dataset. Our codes and models are attached on https://github.com/YanjingLi0202/Q-ViT",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/dfdb2894d50e095ce97f994ed6cee38554c4c84f.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 115,
        "score": 38.33333333333333
    },
    "f27040f1f81144b17ec4c2b30610960e96353002.pdf": {
        "title": "TVT: Transferable Vision Transformer for Unsupervised Domain Adaptation",
        "authors": [
            "Jinyu Yang",
            "Jingjing Liu",
            "N. Xu",
            "Junzhou Huang"
        ],
        "published_date": "2021",
        "abstract": "Unsupervised domain adaptation (UDA) aims to transfer the knowledge learnt from a labeled source domain to an unlabeled target domain. Previous work is mainly built upon convolutional neural networks (CNNs) to learn domain-invariant representations. With the recent exponential increase in applying Vision Transformer (ViT) to vision tasks, the capability of ViT in adapting cross-domain knowledge, however, remains unexplored in the literature. To fill this gap, this paper first comprehensively investigates the performance of ViT on a variety of domain adaptation tasks. Surprisingly, ViT demonstrates superior generalization ability, while the performance can be further improved by incorporating adversarial adaptation. Notwithstanding, directly using CNNs-based adaptation strategies fails to take the advantage of ViT\u2019s intrinsic merits (e.g., attention mechanism and sequential image representation) which play an important role in knowledge transfer. To remedy this, we propose an unified framework, namely Transferable Vision Transformer (TVT), to fully exploit the transferability of ViT for domain adaptation. Specifically, we delicately devise a novel and effective unit, which we term Transferability Adaption Module (TAM). By injecting learned trans- ferabilities into attention blocks, TAM compels ViT focus on both transferable and discriminative features. Besides, we leverage discriminative clustering to enhance feature diversity and separation which are undermined during adversarial domain alignment. To verify its versatility, we perform extensive studies of TVT on four benchmarks and the experimental results demonstrate that TVT attains significant improvements compared to existing state-of-the-art UDA methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f27040f1f81144b17ec4c2b30610960e96353002.pdf",
        "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
        "citationCount": 142,
        "score": 35.5
    },
    "4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a.pdf": {
        "title": "Unified Visual Transformer Compression",
        "authors": [
            "Shixing Yu",
            "Tianlong Chen",
            "Jiayi Shen",
            "Huan Yuan",
            "Jianchao Tan",
            "Sen Yang",
            "Ji Liu",
            "Zhangyang Wang"
        ],
        "published_date": "2022",
        "abstract": "Vision transformers (ViTs) have gained popularity recently. Even without customized image operators such as convolutions, ViTs can yield competitive performance when properly trained on massive data. However, the computational overhead of ViTs remains prohibitive, due to stacking multi-head self-attention modules and else. Compared to the vast literature and prevailing success in compressing convolutional neural networks, the study of Vision Transformer compression has also just emerged, and existing works focused on one or two aspects of compression. This paper proposes a unified ViT compression framework that seamlessly assembles three effective techniques: pruning, layer skipping, and knowledge distillation. We formulate a budget-constrained, end-to-end optimization framework, targeting jointly learning model weights, layer-wise pruning ratios/masks, and skip configurations, under a distillation loss. The optimization problem is then solved using the primal-dual algorithm. Experiments are conducted with several ViT variants, e.g. DeiT and T2T-ViT backbones on the ImageNet dataset, and our approach consistently outperforms recent competitors. For example, DeiT-Tiny can be trimmed down to 50\\% of the original FLOPs almost without losing accuracy. Codes are available online:~\\url{https://github.com/VITA-Group/UVC}.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/4c69fdca6e8a1f10871ab9dc47f62c81ba7ead4a.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 103,
        "score": 34.33333333333333
    },
    "49030ae220c863e9b72ab380ecc749c9d0f0ad13.pdf": {
        "title": "UIA-ViT: Unsupervised Inconsistency-Aware Method based on Vision Transformer for Face Forgery Detection",
        "authors": [
            "Wanyi Zhuang",
            "Qi Chu",
            "Zhentao Tan",
            "Qiankun Liu",
            "Haojie Yuan",
            "Changtao Miao",
            "Zixiang Luo",
            "Nenghai Yu"
        ],
        "published_date": "2022",
        "abstract": "Intra-frame inconsistency has been proved to be effective for the generalization of face forgery detection. However, learning to focus on these inconsistency requires extra pixel-level forged location annotations. Acquiring such annotations is non-trivial. Some existing methods generate large-scale synthesized data with location annotations, which is only composed of real images and cannot capture the properties of forgery regions. Others generate forgery location labels by subtracting paired real and fake images, yet such paired data is difficult to collected and the generated label is usually discontinuous. To overcome these limitations, we propose a novel Unsupervised Inconsistency-Aware method based on Vision Transformer, called UIA-ViT, which only makes use of video-level labels and can learn inconsistency-aware feature without pixel-level annotations. Due to the self-attention mechanism, the attention map among patch embeddings naturally represents the consistency relation, making the vision Transformer suitable for the consistency representation learning. Based on vision Transformer, we propose two key components: Unsupervised Patch Consistency Learning (UPCL) and Progressive Consistency Weighted Assemble (PCWA). UPCL is designed for learning the consistency-related representation with progressive optimized pseudo annotations. PCWA enhances the final classification embedding with previous patch embeddings optimized by UPCL to further improve the detection performance. Extensive experiments demonstrate the effectiveness of the proposed method.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/49030ae220c863e9b72ab380ecc749c9d0f0ad13.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 92,
        "score": 30.666666666666664
    },
    "60b0f9af990349546f284dea666fbf52ebfa7004.pdf": {
        "title": "When CNNs Meet Vision Transformer: A Joint Framework for Remote Sensing Scene Classification",
        "authors": [
            "Peifang Deng",
            "Kejie Xu",
            "Hong Huang"
        ],
        "published_date": "2021",
        "abstract": "Scene classification is an indispensable part of remote sensing image interpretation, and various convolutional neural network (CNN)-based methods have been explored to improve classification accuracy. Although they have shown good classification performance on high-resolution remote sensing (HRRS) images, discriminative ability of extracted features is still limited. In this letter, a high-performance joint framework combined CNNs and vision transformer (ViT) (CTNet) is proposed to further boost the discriminative ability of features for HRRS scene classification. The CTNet method contains two modules, including the stream of ViT (T-stream) and the stream of CNNs (C-stream). For the T-stream, flattened image patches are sent into pretrained ViT model to mine semantic features in HRRS images. To complement with T-stream, pretrained CNN is transferred to extract local structural features in the C-stream. Then, semantic features and structural features are concatenated to predict labels of unknown samples. Finally, a joint loss function is developed to optimize the joint model and increase the intraclass aggregation. The highest accuracies on the aerial image dataset (AID) and Northwestern Polytechnical University (NWPU)-RESISC45 datasets obtained by the CTNet method are 97.70% and 95.49%, respectively. The classification results reveal that the proposed method achieves high classification performance compared with other state-of-the-art (SOTA) methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/60b0f9af990349546f284dea666fbf52ebfa7004.pdf",
        "venue": "IEEE Geoscience and Remote Sensing Letters",
        "citationCount": 121,
        "score": 30.25
    },
    "64d8af9153d68e9b50f616d227663385bece93b9.pdf": {
        "title": "Feature Fusion Vision Transformer for Fine-Grained Visual Categorization",
        "authors": [
            "Jun Wang",
            "Xiaohan Yu",
            "Yongsheng Gao"
        ],
        "published_date": "2021",
        "abstract": "The core for tackling the fine-grained visual categorization (FGVC) is to learn subtle yet discriminative features. Most previous works achieve this by explicitly selecting the discriminative parts or integrating the attention mechanism via CNN-based approaches.However, these methods enhance the computational complexity and make the modeldominated by the regions containing the most of the objects. Recently, vision trans-former (ViT) has achieved SOTA performance on general image recognition tasks. Theself-attention mechanism aggregates and weights the information from all patches to the classification token, making it perfectly suitable for FGVC. Nonetheless, the classifi-cation token in the deep layer pays more attention to the global information, lacking the local and low-level features that are essential for FGVC. In this work, we proposea novel pure transformer-based framework Feature Fusion Vision Transformer (FFVT)where we aggregate the important tokens from each transformer layer to compensate thelocal, low-level and middle-level information. We design a novel token selection mod-ule called mutual attention weight selection (MAWS) to guide the network effectively and efficiently towards selecting discriminative tokens without introducing extra param-eters. We verify the effectiveness of FFVT on three benchmarks where FFVT achieves the state-of-the-art performance.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/64d8af9153d68e9b50f616d227663385bece93b9.pdf",
        "venue": "British Machine Vision Conference",
        "citationCount": 116,
        "score": 29.0
    },
    "03384825d373aabe67c4288ef1eae4d1cf89dc00.pdf": {
        "title": "ViA: A Novel Vision-Transformer Accelerator Based on FPGA",
        "authors": [
            "Teng Wang",
            "Lei Gong",
            "Chao Wang",
            "Y. Yang",
            "Yingxue Gao",
            "Xuehai Zhou",
            "Huaping Chen"
        ],
        "published_date": "2022",
        "abstract": "Since Google proposed Transformer in 2017, it has made significant natural language processing (NLP) development. However, the increasing cost is a large amount of calculation and parameters. Previous researchers designed and proposed some accelerator structures for transformer models in field-programmable gate array (FPGA) to deal with NLP tasks efficiently. Now, the development of Transformer has also affected computer vision (CV) and has rapidly surpassed convolution neural networks (CNNs) in various image tasks. And there are apparent differences between the image data used in CV and the sequence data in NLP. The details in the models contained with transformer units in these two fields are also different. The difference in terms of data brings about the problem of the locality. The difference in the model structure brings about the problem of path dependence, which is not noticed in the existing related accelerator design. Therefore, in this work, we propose the ViA, a novel vision transformer (ViT) accelerator architecture based on FPGA, to execute the transformer application efficiently and avoid the cost of these challenges. By analyzing the data structure in the ViT, we design an appropriate partition strategy to reduce the impact of data locality in the image and improve the efficiency of computation and memory access. Meanwhile, by observing the computing flow of the ViT, we use the half-layer mapping and throughput analysis to reduce the impact of path dependence caused by the shortcut mechanism and fully utilize hardware resources to execute the Transformer efficiently. Based on optimization strategies, we design two reuse processing engines with the internal stream, different from the previous overlap or stream design patterns. In the stage of the experiment, we implement the ViA architecture in Xilinx Alveo U50 FPGA and finally achieved ~5.2 times improvement of energy efficiency compared with NVIDIA Tesla V100, and 4\u201310 times improvement of performance compared with related accelerators based on FPGA, that obtained nearly 309.6 GOP/s computing performance in the peek.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/03384825d373aabe67c4288ef1eae4d1cf89dc00.pdf",
        "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
        "citationCount": 84,
        "score": 28.0
    },
    "d28fed119d9293af31776205150b3c34f3adc82b.pdf": {
        "title": "Uniform Masking: Enabling MAE Pre-training for Pyramid-based Vision Transformers with Locality",
        "authors": [
            "Xiang Li",
            "Wenhai Wang",
            "Lingfeng Yang",
            "Jian Yang"
        ],
        "published_date": "2022",
        "abstract": "Masked AutoEncoder (MAE) has recently led the trends of visual self-supervision area by an elegant asymmetric encoder-decoder design, which significantly optimizes both the pre-training efficiency and fine-tuning accuracy. Notably, the success of the asymmetric structure relies on the\"global\"property of Vanilla Vision Transformer (ViT), whose self-attention mechanism reasons over arbitrary subset of discrete image patches. However, it is still unclear how the advanced Pyramid-based ViTs (e.g., PVT, Swin) can be adopted in MAE pre-training as they commonly introduce operators within\"local\"windows, making it difficult to handle the random sequence of partial vision tokens. In this paper, we propose Uniform Masking (UM), successfully enabling MAE pre-training for Pyramid-based ViTs with locality (termed\"UM-MAE\"for short). Specifically, UM includes a Uniform Sampling (US) that strictly samples $1$ random patch from each $2 \\times 2$ grid, and a Secondary Masking (SM) which randomly masks a portion of (usually $25\\%$) the already sampled regions as learnable tokens. US preserves equivalent elements across multiple non-overlapped local windows, resulting in the smooth support for popular Pyramid-based ViTs; whilst SM is designed for better transferable visual representations since US reduces the difficulty of pixel recovery pre-task that hinders the semantic learning. We demonstrate that UM-MAE significantly improves the pre-training efficiency (e.g., it speeds up and reduces the GPU memory by $\\sim 2\\times$) of Pyramid-based ViTs, but maintains the competitive fine-tuning performance across downstream tasks. For example using HTC++ detector, the pre-trained Swin-Large backbone self-supervised under UM-MAE only in ImageNet-1K can even outperform the one supervised in ImageNet-22K. The codes are available at https://github.com/implus/UM-MAE.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d28fed119d9293af31776205150b3c34f3adc82b.pdf",
        "venue": "arXiv.org",
        "citationCount": 81,
        "score": 27.0
    },
    "b52844a746dafd8a5051cef49abbbda64a312605.pdf": {
        "title": "When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism",
        "authors": [
            "Guangting Wang",
            "Yucheng Zhao",
            "Chuanxin Tang",
            "Chong Luo",
            "Wenjun Zeng"
        ],
        "published_date": "2022",
        "abstract": "Attention mechanism has been widely believed as the key to success of vision transformers (ViTs), since it provides a flexible and powerful way to model spatial relationships. However, is the attention mechanism truly an indispensable part of ViT? Can it be replaced by some other alternatives? To demystify the role of attention mechanism, we simplify it into an extremely simple case: ZERO FLOP and ZERO parameter. Concretely, we revisit the shift operation. It does not contain any parameter or arithmetic calculation. The only operation is to exchange a small portion of the channels between neighboring features. Based on this simple operation, we construct a new backbone network, namely ShiftViT, where the attention layers in ViT are substituted by shift operations. Surprisingly, ShiftViT works quite well in several mainstream tasks, e.g., classification, detection, and segmentation. The performance is on par with or even better than the strong baseline Swin Transformer. These results suggest that the attention mechanism might not be the vital factor that makes ViT successful. It can be even replaced by a zero-parameter operation. We should pay more attentions to the remaining parts of ViT in the future work. Code is available at github.com/microsoft/SPACH.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b52844a746dafd8a5051cef49abbbda64a312605.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 78,
        "score": 26.0
    },
    "35fccd11326e799ebf724f4150acef12a6538953.pdf": {
        "title": "TransVG++: End-to-End Visual Grounding With Language Conditioned Vision Transformer",
        "authors": [
            "Jiajun Deng",
            "Zhengyuan Yang",
            "Daqing Liu",
            "Tianlang Chen",
            "Wen-gang Zhou",
            "Yanyong Zhang",
            "Houqiang Li",
            "Wanli Ouyang"
        ],
        "published_date": "2022",
        "abstract": "In this work, we explore neat yet effective Transformer-based frameworks for visual grounding. The previous methods generally address the core problem of visual grounding, i.e., multi-modal fusion and reasoning, with manually-designed mechanisms. Such heuristic designs are not only complicated but also make models easily overfit specific data distributions. To avoid this, we first propose TransVG, which establishes multi-modal correspondences by Transformers and localizes referred regions by directly regressing box coordinates. We empirically show that complicated fusion modules can be replaced by a simple stack of Transformer encoder layers with higher performance. However, the core fusion Transformer in TransVG is stand-alone against uni-modal encoders, and thus should be trained from scratch on limited visual grounding data, which makes it hard to be optimized and leads to sub-optimal performance. To this end, we further introduce TransVG++ to make two-fold improvements. For one thing, we upgrade our framework to a purely Transformer-based one by leveraging Vision Transformer (ViT) for vision feature encoding. For another, we devise Language Conditioned Vision Transformer that removes external fusion modules and reuses the uni-modal ViT for vision-language fusion at the intermediate layers. We conduct extensive experiments on five prevalent datasets, and report a series of state-of-the-art records.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/35fccd11326e799ebf724f4150acef12a6538953.pdf",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citationCount": 73,
        "score": 24.333333333333332
    },
    "0d1e4faa1580266a56ad62ac2fdd72ed0c0bbbe9.pdf": {
        "title": "Temporally Efficient Vision Transformer for Video Instance Segmentation",
        "authors": [
            "Shusheng Yang",
            "Xinggang Wang",
            "Yu Li",
            "Yuxin Fang",
            "Jiemin Fang",
            "Wenyu Liu",
            "Xun Zhao",
            "Ying Shan"
        ],
        "published_date": "2022",
        "abstract": "Recently vision transformer has achieved tremendous success on image-level visual recognition tasks. To effectively and efficiently model the crucial temporal information within a video clip, we propose a Temporally Efficient Vision Transformer (TeViT) for video instance segmentation (VIS). Different from previous transformer-based VIS methods, TeViT is nearly convolution-free, which contains a transformer backbone and a query-based video instance segmentation head. In the backbone stage, we propose a nearly parameter-free messenger shift mechanism for early temporal context fusion. In the head stages, we propose a parameter-shared spatiotemporal query interaction mechanism to build the one-to-one correspondence between video instances and queries. Thus, TeViT fully utilizes both frame-level and instance-level temporal context information and obtains strong temporal modeling capacity with negligible extra computational cost. On three widely adopted VIS benchmarks, i.e., YouTube-VIS-2019, YouTube-VIS-2021, and OVIS, TeViT obtains state-of-the-art results and maintains high inference speed, e.g., 46.6 AP with 68.9 FPS on YouTube-VIS-2019. Code is available at https://github.com/hustvl/TeViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0d1e4faa1580266a56ad62ac2fdd72ed0c0bbbe9.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 73,
        "score": 24.333333333333332
    },
    "8f7ae0526f9c8706a9e0967eb7942d9b3ffb2788.pdf": {
        "title": "Vision Transformer for Classification of Breast Ultrasound Images",
        "authors": [
            "Behnaz Gheflati",
            "H. Rivaz"
        ],
        "published_date": "2021",
        "abstract": "Medical ultrasound (US) imaging has become a prominent modality for breast cancer imaging due to its ease-of-use, low-cost and safety. In the past decade, convolutional neural networks (CNNs) have emerged as the method of choice in vision applications and have shown excellent potential in automatic classification of US images. Despite their success, their restricted local receptive field limits their ability to learn global context information. Recently, Vision Transformer (ViT) designs that are based on self-attention between image patches have shown great potential to be an alternative to CNNs. In this study, for the first time, we utilize ViT to classify breast US images using different augmentation strategies. The results are provided as classification accuracy and Area Under the Curve (AUC) metrics, and the performance is compared with the state-of-the-art CNNs. The results indicate that the ViT models have comparable efficiency with or even better than the CNNs in classification of US breast images.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/8f7ae0526f9c8706a9e0967eb7942d9b3ffb2788.pdf",
        "venue": "arXiv.org",
        "citationCount": 94,
        "score": 23.5
    },
    "00f6041bdbdd00299ef317bd86db3660eb039a8c.pdf": {
        "title": "A Novel Fault Diagnosis Method of Rolling Bearing Based on Integrated Vision Transformer Model",
        "authors": [
            "Xinyu Tang",
            "Zengbing Xu",
            "Zhigang Wang"
        ],
        "published_date": "2022",
        "abstract": "In order to improve the diagnosis accuracy and generalization of bearing faults, an integrated vision transformer (ViT) model based on wavelet transform and the soft voting method is proposed in this paper. Firstly, the discrete wavelet transform (DWT) was utilized to decompose the vibration signal into the subsignals in the different frequency bands, and then these different subsignals were transformed into a time\u2013frequency representation (TFR) map by the continuous wavelet transform (CWT) method. Secondly, the TFR maps were input with respective to the multiple individual ViT models for preliminary diagnosis analysis. Finally, the final diagnosis decision was obtained by using the soft voting method to fuse all the preliminary diagnosis results. Through multifaceted diagnosis tests of rolling bearings on different datasets, the diagnosis results demonstrate that the proposed integrated ViT model based on the soft voting method can diagnose the different fault categories and fault severities of bearings accurately, and has a higher diagnostic accuracy and generalization ability by comparison analysis with integrated CNN and individual ViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/00f6041bdbdd00299ef317bd86db3660eb039a8c.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 68,
        "score": 22.666666666666664
    },
    "5f7fee3db8879d9d4e714ae20627f00fcbdfab38.pdf": {
        "title": "Mix-ViT: Mixing attentive vision transformer for ultra-fine-grained visual categorization",
        "authors": [
            "Xiaohan Yu",
            "Jun Wang",
            "Yang Zhao",
            "Yongsheng Gao"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/5f7fee3db8879d9d4e714ae20627f00fcbdfab38.pdf",
        "venue": "Pattern Recognition",
        "citationCount": 66,
        "score": 22.0
    },
    "070c917ab1a4d6b924a9613ca18443f260d8d5be.pdf": {
        "title": "MViT: Mask Vision Transformer for Facial Expression Recognition in the wild",
        "authors": [
            "Hanting Li",
            "Ming-Fa Sui",
            "Feng Zhao",
            "Zhengjun Zha",
            "Feng Wu"
        ],
        "published_date": "2021",
        "abstract": "Facial Expression Recognition (FER) in the wild is an extremely challenging task in computer vision due to variant backgrounds, low-quality facial images, and the subjectiveness of annotators. These uncertainties make it difficult for neural networks to learn robust features on limited-scale datasets. Moreover, the networks can be easily distributed by the above factors and perform incorrect decisions. Recently, vision transformer (ViT) and data-efficient image transformers (DeiT) present their significant performance in traditional classification tasks. The self-attention mechanism makes transformers obtain a global receptive field in the first layer which dramatically enhances the feature extraction capability. In this work, we first propose a novel pure transformer-based mask vision transformer (MVT) for FER in the wild, which consists of two modules: a transformer-based mask generation network (MGN) to generate a mask that can filter out complex backgrounds and occlusion of face images, and a dynamic relabeling module to rectify incorrect labels in FER datasets in the wild. Extensive experimental results demonstrate that our MVT outperforms state-of-the-art methods on RAF-DB with 88.62%, FERPlus with 89.22%, and AffectNet-7 with 64.57%, respectively, and achieves a comparable result on AffectNet-8 with 61.40%.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/070c917ab1a4d6b924a9613ca18443f260d8d5be.pdf",
        "venue": "arXiv.org",
        "citationCount": 83,
        "score": 20.75
    },
    "011f59c91bbee6de780d35ebe50fff62087e5b13.pdf": {
        "title": "Class-Guided Swin Transformer for Semantic Segmentation of Remote Sensing Imagery",
        "authors": [
            "Xiaoliang Meng",
            "Yuechi Yang",
            "Libo Wang",
            "Teng Wang",
            "Rui Li",
            "Ce Zhang"
        ],
        "published_date": "2022",
        "abstract": "Semantic segmentation of remote sensing images plays a crucial role in a wide variety of practical applications, including land cover mapping, environmental protection, and economic assessment. In the last decade, convolutional neural network (CNN) is the mainstream deep learning-based method of semantic segmentation. Compared with conventional methods, CNN-based methods learn semantic features automatically, thereby achieving strong representation capability. However, the local receptive field of the convolution operation limits CNN-based methods from capturing long-range dependencies. In contrast, Vision Transformer (ViT) demonstrates its great potential in modeling long-range dependencies and obtains superior results in semantic segmentation. Inspired by this, in this letter, we propose a class-guided Swin Transformer (CG-Swin) for semantic segmentation of remote sensing images. Specifically, we adopt a Transformer-based encoder\u2013decoder structure, which introduces the Swin Transformer backbone as the encoder and designs a class-guided Transformer block to construct the decoder. The experimental results on ISPRS Vaihingen and Potsdam datasets demonstrate the significant breakthrough of the proposed method over ten benchmarks, outperforming both advanced CNN-based and recent Transformer-based approaches.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/011f59c91bbee6de780d35ebe50fff62087e5b13.pdf",
        "venue": "IEEE Geoscience and Remote Sensing Letters",
        "citationCount": 60,
        "score": 20.0
    },
    "f27c847e2909f30745f4a3528b574f5acfd76ea7.pdf": {
        "title": "Auto-ViT-Acc: An FPGA-Aware Automatic Acceleration Framework for Vision Transformer with Mixed-Scheme Quantization",
        "authors": [
            "Z. Li",
            "Mengshu Sun",
            "Alec Lu",
            "Haoyu Ma",
            "Geng Yuan",
            "Yanyue Xie",
            "Hao Tang",
            "Yanyu Li",
            "M. Leeser",
            "Zhangyang Wang",
            "Xue Lin",
            "Zhenman Fang"
        ],
        "published_date": "2022",
        "abstract": "Vision transformers (ViTs) are emerging with significantly improved accuracy in computer vision tasks. However, their complex architecture and enormous computation/storage demand impose urgent needs for new hardware accelerator design methodology. This work proposes an FPGA-aware automatic ViT acceleration framework based on the proposed mixed-scheme quantization. To the best of our knowledge, this is the first FPGA-based ViT acceleration framework exploring model quantization. Compared with state-of-the-art ViT quantization work (algorithmic approach only without hardware acceleration), our quantization achieves 0.47% to 1.36% higher Top-l accuracy under the same bit-width. Compared with the 32-bit floating-point baseline FPGA accelerator, our accelerator achieves around 5.6x improvement on the frame rate (i.e., 56.8 FPS vs. 10.0 FPS) with 0.71% accuracy drop on ImageNet dataset for DeiT-base.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f27c847e2909f30745f4a3528b574f5acfd76ea7.pdf",
        "venue": "International Conference on Field-Programmable Logic and Applications",
        "citationCount": 59,
        "score": 19.666666666666664
    },
    "d717fa5a4355b5033e660ad58a7ae7f4ccce6939.pdf": {
        "title": "Bi-Modal Transformer-Based Approach for Visual Question Answering in Remote Sensing Imagery",
        "authors": [
            "Y. Bazi",
            "Mohamad Mahmoud Al Rahhal",
            "M. L. Mekhalfi",
            "M. Zuair",
            "F. Melgani"
        ],
        "published_date": "2022",
        "abstract": "Recently, vision-language models based on transformers are gaining popularity for joint modeling of visual and textual modalities. In particular, they show impressive results when transferred to several downstream tasks such as zero and few-shot classification. In this article, we propose a visual question answering (VQA) approach for remote sensing images based on these models. The VQA task attempts to provide answers to image-related questions. In contrast, VQA has gained popularity in computer vision, in remote sensing, it is not widespread. First, we use the contrastive language image pretraining (CLIP) network for embedding the image patches and question words into a sequence of visual and textual representations. Then, we learn attention mechanisms to capture the intradependencies and interdependencies within and between these representations. Afterward, we generate the final answer by averaging the predictions of two classifiers mounted on the top of the resulting contextual representations. In the experiments, we study the performance of the proposed approach on two datasets acquired with Sentinel-2 and aerial sensors. In particular, we demonstrate that our approach can achieve better results with reduced training size compared with the recent state-of-the-art.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d717fa5a4355b5033e660ad58a7ae7f4ccce6939.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 58,
        "score": 19.333333333333332
    },
    "a119cc83788701313d94746baecd2df5dd30199d.pdf": {
        "title": "Swin-MLP: a strawberry appearance quality identification method by Swin Transformer and multi-layer perceptron",
        "authors": [
            "Hao Zheng",
            "Guohui Wang",
            "Xuchen Li"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a119cc83788701313d94746baecd2df5dd30199d.pdf",
        "venue": "Journal of Food Measurement & Characterization",
        "citationCount": 57,
        "score": 19.0
    },
    "60f9fd4e6b29e6f11324ba4f6d3349a2d8107a1b.pdf": {
        "title": "COVID-VIT: Classification of COVID-19 from CT chest images based on vision transformer models",
        "authors": [
            "Xiaohong W. Gao",
            "Y. Qian",
            "Alice Gao"
        ],
        "published_date": "2021",
        "abstract": "This paper is responding to the MIA-COV19 challenge to classify COVID from non-COVID based on CT lung images. The COVID-19 virus has devastated the world in the last eighteen months by infecting more than 182 million people and causing over 3.9 million deaths. The overarching aim is to predict the diagnosis of the COVID-19 virus from chest radiographs, through the development of explainable vision transformer deep learning techniques, leading to population screening in a more rapid, accurate and transparent way. In this competition, there are 5381 three-dimensional (3D) datasets in total, including 1552 for training, 374 for evaluation and 3455 for testing. While most of the data volumes are in axial view, there are a number of subjects' data are in coronal or sagittal views with 1 or 2 slices are in axial view. Hence, while 3D data based classification is investigated, in this competition, 2D images remains the main focus. Two deep learning methods are studied, which are vision transformer (ViT) based on attention models and DenseNet that is built upon conventional convolutional neural network (CNN). Initial evaluation results based on validation datasets whereby the ground truth is known indicate that ViT performs better than DenseNet with F1 scores being 0.76 and 0.72 respectively. Codes are available at GitHub at <https://github/xiaohong1/COVID-ViT>.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/60f9fd4e6b29e6f11324ba4f6d3349a2d8107a1b.pdf",
        "venue": "arXiv.org",
        "citationCount": 75,
        "score": 18.75
    },
    "5ca02297d8d49f03f26148b74fea77272d09c78b.pdf": {
        "title": "Prompt Vision Transformer for Domain Generalization",
        "authors": [
            "Zangwei Zheng",
            "Xiangyu Yue",
            "Kai Wang",
            "Yang You"
        ],
        "published_date": "2022",
        "abstract": "Though vision transformers (ViTs) have exhibited impressive ability for representation learning, we empirically find that they cannot generalize well to unseen domains with previous domain generalization algorithms. In this paper, we propose a novel approach DoPrompt based on prompt learning to embed the knowledge of source domains in domain prompts for target domain prediction. Specifically, domain prompts are prepended before ViT input tokens from the corresponding source domain. Each domain prompt learns domain-specific knowledge efficiently since it is optimized only for one domain. Meanwhile, we train a prompt adapter to produce a suitable prompt for each input image based on the learned source domain prompts. At test time, the adapted prompt generated by the prompt adapter can exploit the similarity between the feature of the out-of-domain image and source domains to properly integrate the source domain knowledge. Extensive experiments are conducted on four benchmark datasets. Our approach achieves 1.4% improvements in the averaged accuracy, which is 3.5 times the improvement of the state-of-the-art algorithm with a ViT backbone.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/5ca02297d8d49f03f26148b74fea77272d09c78b.pdf",
        "venue": "arXiv.org",
        "citationCount": 56,
        "score": 18.666666666666664
    },
    "aed7e4bc195d838735c320ac40a78f123206831b.pdf": {
        "title": "Development of Deep Learning Methodology for Maize Seed Variety Recognition Based on Improved Swin Transformer",
        "authors": [
            "Chunguang Bi",
            "Nan Hu",
            "Yiqiang Zou",
            "Shuo Zhang",
            "Suzhen Xu",
            "Helong Yu"
        ],
        "published_date": "2022",
        "abstract": "In order to solve the problems of high subjectivity, frequent error occurrence and easy damage of traditional corn seed identification methods, this paper combines deep learning with machine vision and the utilization of the basis of the Swin Transformer to improve maize seed recognition. The study was focused on feature attention and multi-scale feature fusion learning. Firstly, input the seed image into the network to obtain shallow features and deep features; secondly, a feature attention layer was introduced to give weights to different stages of features to strengthen and suppress; and finally, the shallow features and deep features were fused to construct multi-scale fusion features of corn seed images, and the seed images are divided into 19 varieties through a classifier. The experimental results showed that the average precision, recall and F1 values of the MFSwin Transformer model on the test set were 96.53%, 96.46%, and 96.47%, respectively, and the parameter memory is 12.83 M. Compared to other models, the MFSwin Transformer model achieved the highest classification accuracy results. Therefore, the neural network proposed in this paper can classify corn seeds accurately and efficiently, could meet the high-precision classification requirements of corn seed images, and provide a reference tool for seed identification.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/aed7e4bc195d838735c320ac40a78f123206831b.pdf",
        "venue": "Agronomy",
        "citationCount": 55,
        "score": 18.333333333333332
    },
    "b66e4257aa8856df537f03f6a12341f489eb6500.pdf": {
        "title": "A Fast Inference Vision Transformer for Automatic Pavement Image Classification and Its Visual Interpretation Method",
        "authors": [
            "Yihan Chen",
            "Xingyu Gu",
            "Zhen Liu",
            "Jia-Yun Liang"
        ],
        "published_date": "2022",
        "abstract": "Traditional automatic pavement distress detection methods using convolutional neural networks (CNNs) require a great deal of time and resources for computing and are poor in terms of interpretability. Therefore, inspired by the successful application of Transformer architecture in natural language processing (NLP) tasks, a novel Transformer method called LeViT was introduced for automatic asphalt pavement image classification. LeViT consists of convolutional layers, transformer stages where Multi-layer Perception (MLP) and multi-head self-attention blocks alternate using the residual connection, and two classifier heads. To conduct the proposed methods, three different sources of pavement image datasets and pre-trained weights based on ImageNet were attained. The performance of the proposed model was compared with six state-of-the-art (SOTA) deep learning models. All of them were trained based on transfer learning strategy. Compared to the tested SOTA methods, LeViT has less than 1/8 of the parameters of the original Vision Transformer (ViT) and 1/2 of ResNet and InceptionNet. Experimental results show that after training for 100 epochs with a 16 batch-size, the proposed method acquired 91.56% accuracy, 91.72% precision, 91.56% recall, and 91.45% F1-score in the Chinese asphalt pavement dataset and 99.17% accuracy, 99.19% precision, 99.17% recall, and 99.17% F1-score in the German asphalt pavement dataset, which is the best performance among all the tested SOTA models. Moreover, it shows superiority in inference speed (86 ms/step), which is approximately 25% of the original ViT method and 80% of some prevailing CNN-based models, including DenseNet, VGG, and ResNet. Overall, the proposed method can achieve competitive performance with fewer computation costs. In addition, a visualization method combining Grad-CAM and Attention Rollout was proposed to analyze the classification results and explore what has been learned in every MLP and attention block of LeViT, which improved the interpretability of the proposed pavement image classification model.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b66e4257aa8856df537f03f6a12341f489eb6500.pdf",
        "venue": "Remote Sensing",
        "citationCount": 52,
        "score": 17.333333333333332
    },
    "f9480350e1986957919d49f346ba20dcab8f5b71.pdf": {
        "title": "CP-ViT: Cascade Vision Transformer Pruning via Progressive Sparsity Prediction",
        "authors": [
            "Zhuoran Song",
            "Yihong Xu",
            "Zhezhi He",
            "Li Jiang",
            "Naifeng Jing",
            "Xiaoyao Liang"
        ],
        "published_date": "2022",
        "abstract": "Vision transformer (ViT) has achieved competitive accuracy on a variety of computer vision applications, but its computational cost impedes the deployment on resource-limited mobile devices. We explore the sparsity in ViT and observe that informative patches and heads are sufficient for accurate image recognition. In this paper, we propose a cascade pruning framework named CP-ViT by predicting sparsity in ViT models progressively and dynamically to reduce computational redundancy while minimizing the accuracy loss. Specifically, we define the cumulative score to reserve the informative patches and heads across the ViT model for better accuracy. We also propose the dynamic pruning ratio adjustment technique based on layer-aware attention range. CP-ViT has great general applicability for practical deployment, which can be applied to a wide range of ViT models and can achieve superior accuracy with or without fine-tuning. Extensive experiments on ImageNet, CIFAR-10, and CIFAR-100 with various pre-trained models have demonstrated the effectiveness and efficiency of CP-ViT. By progressively pruning 50\\% patches, our CP-ViT method reduces over 40\\% FLOPs while maintaining accuracy loss within 1\\%.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f9480350e1986957919d49f346ba20dcab8f5b71.pdf",
        "venue": "arXiv.org",
        "citationCount": 51,
        "score": 17.0
    },
    "836dd64a4f606931029c5d68e74d81ef5885b622.pdf": {
        "title": "HashFormer: Vision Transformer Based Deep Hashing for Image Retrieval",
        "authors": [
            "Tao Li",
            "Zheng Zhang",
            "Lishen Pei",
            "Yan Gan"
        ],
        "published_date": "2022",
        "abstract": "Deep image hashing aims to map an input image to compact binary codes by deep neural network, to enable efficient image retrieval across large-scale dataset. Due to the explosive growth of modern data, deep hashing has gained growing attention from research community. Recently, convolutional neural networks like ResNet have dominated in deep hashing. Nevertheless, motivated by the recent advancements of vision transformers, we propose a pure transformer-based framework, called as HashFormer, to tackle the deep hashing task. Specifically, we utilize vision transformer (ViT) as our backbone, and treat binary codes as the intermediate representations for our surrogate task, i.e., image classification. In addition, we observe that the binary codes suitable for classification are sub-optimal for retrieval. To mitigate this problem, we present a novel average precision loss, which enables us to directly optimize the retrieval accuracy. To the best of our knowledge, our work is one of the pioneer works to address deep hashing learning problems without convolutional neural networks (CNNs). We perform comprehensive experiments on three widely-studied datasets: CIFAR-10, NUSWIDE and ImageNet. The proposed method demonstrates promising results against existing state-of-the-art works, validating the advantages and merits of our HashFormer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/836dd64a4f606931029c5d68e74d81ef5885b622.pdf",
        "venue": "IEEE Signal Processing Letters",
        "citationCount": 51,
        "score": 17.0
    },
    "16fa1a8575ff56781b6b83726906754ed4e5f3a7.pdf": {
        "title": "ViT-ReT: Vision and Recurrent Transformer Neural Networks for Human Activity Recognition in Videos",
        "authors": [
            "James Wensel",
            "Hayat Ullah",
            "Arslan Munir"
        ],
        "published_date": "2022",
        "abstract": "Human activity recognition is an emerging and important area in computer vision which seeks to determine the activity an individual or group of individuals are performing. The applications of this field ranges from generating highlight videos in sports, to intelligent surveillance and gesture recognition. Most activity recognition systems rely on a combination of convolutional neural networks (CNNs) to perform feature extraction from the data and recurrent neural networks (RNNs) to determine the time dependent nature of the data. This paper proposes and designs two transformer neural networks for human activity recognition: a recurrent transformer (ReT), a specialized neural network used to make predictions on sequences of data, as well as a vision transformer (ViT), a transformer optimized for extracting salient features from images, to improve speed and scalability of activity recognition. We have provided an extensive comparison of the proposed transformer neural networks with the contemporary CNN and RNN-based human activity recognition models in terms of speed and accuracy for four publicly available human action datasets. Experimental results reveal that the proposed ViT-ReT framework attains a speedup of $2\\times $ over the baseline ResNet50-LSTM approach while attaining nearly the same level of accuracy. Furthermore, results show that the proposed ViT-ReT framework attains significant improvements over the state-of-the-art human action recognition methods in terms of both model accuracy and runtime for each of the datasets used in our experiments, thus verifying the suitability of the proposed ViT-ReT framework for human activity recognition in resource-constrained and real-time environments.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/16fa1a8575ff56781b6b83726906754ed4e5f3a7.pdf",
        "venue": "IEEE Access",
        "citationCount": 51,
        "score": 17.0
    },
    "9f7f81b1c82828a45a52df8f0c6a92636af76c7e.pdf": {
        "title": "CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention",
        "authors": [
            "Wenxiao Wang",
            "Lu-yuan Yao",
            "Long Chen",
            "Deng Cai",
            "Xiaofei He",
            "Wei Liu"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9f7f81b1c82828a45a52df8f0c6a92636af76c7e.pdf",
        "venue": "arXiv.org",
        "citationCount": 64,
        "score": 16.0
    },
    "e678898301a66faab85dfa4c84e51118e434b8f2.pdf": {
        "title": "Vision-Language Transformer for Interpretable Pathology Visual Question Answering",
        "authors": [
            "Usman Naseem",
            "Matloob Khushi",
            "Jinman Kim"
        ],
        "published_date": "2022",
        "abstract": "Pathology visual question answering (PathVQA) attempts to answer a medical question posed by pathology images. Despite its great potential in healthcare, it is not widely adopted because it requires interactions on both the image (vision) and question (language) to generate an answer. Existing methods focused on treating vision and language features independently, which were unable to capture the high and low-level interactions that are required for VQA. Further, these methods failed to offer capabilities to interpret the retrieved answers, which are obscure to humans where the models\u2019 interpretability to justify the retrieved answers has remained largely unexplored. Motivated by these limitations, we introduce a vision-language transformer that embeds vision (images) and language (questions) features for an interpretable PathVQA. We present an interpretable transformer-based Path-VQA (TraP-VQA), where we embed transformers\u2019 encoder layers with vision and language features extracted using pre-trained CNN and domain-specific language model (LM), respectively. A decoder layer is then embedded to upsample the encoded features for the final prediction for PathVQA. Our experiments showed that our TraP-VQA outperformed the state-of-the-art comparative methods with public PathVQA dataset. Our experiments validated the robustness of our model on another medical VQA dataset, and the ablation study demonstrated the capability of our integrated transformer-based vision-language model for PathVQA. Finally, we present the visualization results of both text and images, which explain the reason for a retrieved answer in PathVQA.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e678898301a66faab85dfa4c84e51118e434b8f2.pdf",
        "venue": "IEEE journal of biomedical and health informatics",
        "citationCount": 46,
        "score": 15.333333333333332
    },
    "e33434a141bb2881a2e60c518a0cda4feed3f19a.pdf": {
        "title": "A vision transformer for emphysema classification using CT images",
        "authors": [
            "Yanan Wu",
            "Shouliang Qi",
            "Yu Sun",
            "Shuyue Xia",
            "Yudong Yao",
            "W. Qian"
        ],
        "published_date": "2021",
        "abstract": "Objective. Emphysema is characterized by the destruction and permanent enlargement of the alveoli in the lung. According to visual CT appearance, emphysema can be divided into three subtypes: centrilobular emphysema (CLE), panlobular emphysema (PLE), and paraseptal emphysema (PSE). Automating emphysema classification can help precisely determine the patterns of lung destruction and provide a quantitative evaluation. Approach. We propose a vision transformer (ViT) model to classify the emphysema subtypes via CT images. First, large patches (61 \u00d7 61) are cropped from CT images which contain the area of normal lung parenchyma, CLE, PLE, and PSE. After resizing, the large patch is divided into small patches and these small patches are converted to a sequence of patch embeddings by flattening and linear embedding. A class embedding is concatenated to the patch embedding, and the positional embedding is added to the resulting embeddings described above. Then, the obtained embedding is fed into the transformer encoder blocks to generate the final representation. Finally, the learnable class embedding is fed to a softmax layer to classify the emphysema. Main results. To overcome the lack of massive data, the transformer encoder blocks (pre-trained on ImageNet) are transferred and fine-tuned in our ViT model. The average accuracy of the pre-trained ViT model achieves 95.95% in our lab\u2019s own dataset, which is higher than that of AlexNet, Inception-V3, MobileNet-V2, ResNet34, and ResNet50. Meanwhile, the pre-trained ViT model outperforms the ViT model without the pre-training. The accuracy of our pre-trained ViT model is higher than or comparable to that by available methods for the public dataset. Significance. The results demonstrated that the proposed ViT model can accurately classify the subtypes of emphysema using CT images. The ViT model can help make an effective computer-aided diagnosis of emphysema, and the ViT method can be extended to other medical applications.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e33434a141bb2881a2e60c518a0cda4feed3f19a.pdf",
        "venue": "Physics in Medicine and Biology",
        "citationCount": 61,
        "score": 15.25
    },
    "9da3fadf092c864f61d6fd1e8eab5a6ca2397194.pdf": {
        "title": "Classification of Alzheimer's Disease via Vision Transformer: Classification of Alzheimer's Disease via Vision Transformer",
        "authors": [
            "Yanjun Lyu",
            "Xiao-Wen Yu",
            "Dajiang Zhu",
            "Lu Zhang"
        ],
        "published_date": "2022",
        "abstract": "Deep models are powerful in capturing the complex and non-linear relationship buried in brain imaging data. However, the huge number of parameters in deep models can easily overfit given limited imaging data samples. In this work, we proposed a cross-domain transfer learning method to solve the insufficient data problem in brain imaging domain by leveraging the knowledge learned in natural image domain. Specifically, we employed ViT as the backbone and firstly pretrained it using ImageNet-21K dataset and then transferred to the brain imaging dataset. A slice-wise convolution embedding method was developed to improve the standard patch operation in vanilla ViT. Our method was evaluated based on AD/CN classification task. We also conducted extensive experiments to compare the transfer performance with different transfer strategies, models, and sample size. The results suggest that the proposed method can effectively transfer the knowledge learned in natural image domain to brain imaging area and may provide a promising way to take advantages of the pretrained model in data-intensive applications. Moreover, the proposed cross-domain transfer learning method can obtain comparable classification performance compared to most recent studies.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9da3fadf092c864f61d6fd1e8eab5a6ca2397194.pdf",
        "venue": "Petra",
        "citationCount": 45,
        "score": 15.0
    },
    "9fab78222c7111702a5702ce5fae0f920722c316.pdf": {
        "title": "Vision Transformer based COVID-19 Detection using Chest X-rays",
        "authors": [
            "Koushik Sivarama Krishnan",
            "Karthik Sivarama Krishnan"
        ],
        "published_date": "2021",
        "abstract": "COVID-19 is a global pandemic, and detecting them is a momentous task for medical professionals today due to its rapid mutations. Current methods of examining chest X-rays and CT scan requires profound knowledge and are time consuming, which suggests that it shrinks the precious time of medical practitioners when people\u2019s lives are at stake. This study tries to assist this process by achieving state-of-the-art performance in classifying chest X-rays by fine-tuning Vision Transformer(ViT). The proposed approach uses pretrained models, fine-tuned for detecting the presence of COVID-19 disease on chest X-rays. This approach achieves an accuracy score of 97.61%, precision score of 95.34%, recall score of 93.84% and, fl-score of 94.58%. This result signifies the performance of transformer-based models on chest X-ray.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9fab78222c7111702a5702ce5fae0f920722c316.pdf",
        "venue": "2021 6th International Conference on Signal Processing, Computing and Control (ISPCC)",
        "citationCount": 59,
        "score": 14.75
    },
    "c051ee2ad7ac203a26fa8f50eb6312424c729b27.pdf": {
        "title": "Global Vision Transformer Pruning with Hessian-Aware Saliency",
        "authors": [
            "Huanrui Yang",
            "Hongxu Yin",
            "Maying Shen",
            "Pavlo Molchanov",
            "Hai Helen Li",
            "Jan Kautz"
        ],
        "published_date": "2021",
        "abstract": "Transformers yield state-of-the-art results across many tasks. However, their heuristically designed architecture impose huge computational costs during inference. This work aims on challenging the common design philosophy of the Vision Transformer (ViT) model with uniform dimension across all the stacked blocks in a model stage, where we redistribute the parameters both across transformer blocks and between different structures within the block via the first systematic attempt on global structural pruning. Dealing with diverse ViT structural components, we derive a novel Hessian-based structural pruning criteria comparable across all layers and structures, with latency-aware regularization for direct latency reduction. Performing iterative pruning on the DeiT-Base model leads to a new architecture family called NViT (Novel ViT), with a novel parameter redistribution that utilizes parameters more efficiently. On ImageNet-1K, NViT-Base achieves a $2.6\\times FLOPs$ reduction, $5.1\\times$ parameter reduction, and $1.9\\times run$-time speedup over the DeiT-Base model in a near lossless manner. Smaller NViT variants achieve more than 1% accuracy gain at the same throughput of the DeiT Small/Tiny variants, as well as a lossless $3.3\\times parameter$ reduction over the SWIN-Small model. These results outperform prior art by a large margin. Further analysis is provided on the parameter redistribution insight of NViT, where we show the high prunability of ViT models, distinct sensitivity within ViT block, and unique parameter distribution trend across stacked ViT blocks. Our insights provide viability for a simple yet effective parameter redistribution rule towards more efficient ViTs for off-the-shelf performance boost.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c051ee2ad7ac203a26fa8f50eb6312424c729b27.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 59,
        "score": 14.75
    },
    "13f7a106bb3814ad1fab25fd1356e99e91f402d3.pdf": {
        "title": "Q-ViT: Fully Differentiable Quantization for Vision Transformer",
        "authors": [
            "Zhexin Li",
            "Tong Yang",
            "Peisong Wang",
            "Jian Cheng"
        ],
        "published_date": "2022",
        "abstract": "In this paper, we propose a fully differentiable quantization method for vision transformer (ViT) named as Q-ViT, in which both of the quantization scales and bit-widths are learnable parameters. Specifically, based on our observation that heads in ViT display different quantization robustness, we leverage head-wise bit-width to squeeze the size of Q-ViT while preserving performance. In addition, we propose a novel technique named switchable scale to resolve the convergence problem in the joint training of quantization scales and bit-widths. In this way, Q-ViT pushes the limits of ViT quantization to 3-bit without heavy performance drop. Moreover, we analyze the quantization robustness of every architecture component of ViT and show that the Multi-head Self-Attention (MSA) and the Gaussian Error Linear Units (GELU) are the key aspects for ViT quantization. This study provides some insights for further research about ViT quantization. Extensive experiments on different ViT models, such as DeiT and Swin Transformer show the effectiveness of our quantization method. In particular, our method outperforms the state-of-the-art uniform quantization method by 1.5% on DeiT-Tiny.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/13f7a106bb3814ad1fab25fd1356e99e91f402d3.pdf",
        "venue": "arXiv.org",
        "citationCount": 44,
        "score": 14.666666666666666
    },
    "d04fbbec070b7e7af8152f9b7574971d22bf5997.pdf": {
        "title": "Land Cover Classification for Polarimetric SAR Images Based on Vision Transformer",
        "authors": [
            "Hongmiao Wang",
            "Cheng Xing",
            "Junjun Yin",
            "Jian Yang"
        ],
        "published_date": "2022",
        "abstract": "Deep learning methods have been widely studied for Polarimetric synthetic aperture radar (PolSAR) land cover classification. The scarcity of PolSAR labeled samples and the small receptive field of the model limit the performance of deep learning methods for land cover classification. In this paper, a vision Transformer (ViT)-based classification method is proposed. The ViT structure can extract features from the global range of images based on a self-attention block. The powerful feature representation capability of the model is equivalent to a flexible receptive field, which is suitable for PolSAR image classification at different resolutions. In addition, because of the lack of labeled data, the Mask Autoencoder method is used to pre-train the proposed model with unlabeled data. Experiments are carried out on the Flevoland dataset acquired by NASA/JPL AIRSAR and the Hainan dataset acquired by the Aerial Remote Sensing System of the Chinese Academy of Sciences. The experimental results on both datasets demonstrate the superiority of the proposed method.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d04fbbec070b7e7af8152f9b7574971d22bf5997.pdf",
        "venue": "Remote Sensing",
        "citationCount": 43,
        "score": 14.333333333333332
    },
    "e939b55a6f78bffeb00065aed897950c49d21182.pdf": {
        "title": "Searching the Search Space of Vision Transformer",
        "authors": [
            "Minghao Chen",
            "Kan Wu",
            "Bolin Ni",
            "Houwen Peng",
            "Bei Liu",
            "Jianlong Fu",
            "Hongyang Chao",
            "Haibin Ling"
        ],
        "published_date": "2021",
        "abstract": "Vision Transformer has shown great visual representation power in substantial vision tasks such as recognition and detection, and thus been attracting fast-growing efforts on manually designing more effective architectures. In this paper, we propose to use neural architecture search to automate this process, by searching not only the architecture but also the search space. The central idea is to gradually evolve different search dimensions guided by their E-T Error computed using a weight-sharing supernet. Moreover, we provide design guidelines of general vision transformers with extensive analysis according to the space searching process, which could promote the understanding of vision transformer. Remarkably, the searched models, named S3 (short for Searching the Search Space), from the searched space achieve superior performance to recently proposed models, such as Swin, DeiT and ViT, when evaluated on ImageNet. The effectiveness of S3 is also illustrated on object detection, semantic segmentation and visual question answering, demonstrating its generality to downstream vision and vision-language tasks. Code and models will be available at https://github.com/microsoft/Cream.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e939b55a6f78bffeb00065aed897950c49d21182.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 57,
        "score": 14.25
    },
    "6dc8693674a105c6daca5200141c50362e3044fc.pdf": {
        "title": "Transformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images",
        "authors": [
            "Teerapong Panboonyuen",
            "Kulsawasd Jitkajornwanich",
            "S. Lawawirojwong",
            "Panu Srestasathiern",
            "P. Vateekul"
        ],
        "published_date": "2021",
        "abstract": "Transformers have demonstrated remarkable accomplishments in several natural language processing (NLP) tasks as well as image processing tasks. Herein, we present a deep-learning (DL) model that is capable of improving the semantic segmentation network in two ways. First, utilizing the pre-training Swin Transformer (SwinTF) under Vision Transformer (ViT) as a backbone, the model weights downstream tasks by joining task layers upon the pretrained encoder. Secondly, decoder designs are applied to our DL network with three decoder designs, U-Net, pyramid scene parsing (PSP) network, and feature pyramid network (FPN), to perform pixel-level segmentation. The results are compared with other image labeling state of the art (SOTA) methods, such as global convolutional network (GCN) and ViT. Extensive experiments show that our Swin Transformer (SwinTF) with decoder designs reached a new state of the art on the Thailand Isan Landsat-8 corpus (89.8% F1 score), Thailand North Landsat-8 corpus (63.12% F1 score), and competitive results on ISPRS Vaihingen. Moreover, both our best-proposed methods (SwinTF-PSP and SwinTF-FPN) even outperformed SwinTF with supervised pre-training ViT on the ImageNet-1K in the Thailand, Landsat-8, and ISPRS Vaihingen corpora.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6dc8693674a105c6daca5200141c50362e3044fc.pdf",
        "venue": "Remote Sensing",
        "citationCount": 57,
        "score": 14.25
    },
    "494e200d8a2ef49d21ce3458a25a109ea917ebe4.pdf": {
        "title": "BTSwin-Unet: 3D U-shaped Symmetrical Swin Transformer-based Network for Brain Tumor Segmentation with Self-supervised Pre-training",
        "authors": [
            "Junjie Liang",
            "Cihui Yang",
            "Jingting Zhong",
            "Xiaoli Ye"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/494e200d8a2ef49d21ce3458a25a109ea917ebe4.pdf",
        "venue": "Neural Processing Letters",
        "citationCount": 42,
        "score": 14.0
    },
    "39240f94c9915d9f9959c34b1dc68593894531e6.pdf": {
        "title": "ConvNets vs. Transformers: Whose Visual Representations are More Transferable?",
        "authors": [
            "Hong-Yu Zhou",
            "Chi-Ken Lu",
            "Sibei Yang",
            "Yizhou Yu"
        ],
        "published_date": "2021",
        "abstract": "Vision transformers have attracted much attention from computer vision researchers as they are not restricted to the spatial inductive bias of ConvNets. However, although Transformer-based backbones have achieved much progress on ImageNet classification, it is still unclear whether the learned representations are as transferable as or even more transferable than ConvNets\u2019 features. To address this point, we systematically investigate the transfer learning ability of ConvNets and vision transformers in 15 single-task and multi-task performance evaluations. We observe consistent advantages of Transformer-based backbones on 13 downstream tasks (out of 15), including but not limited to fine-grained classification, scene recognition (classification, segmentation and depth estimation), open-domain classification, face recognition, etc. More specifically, we find that two ViT models heavily rely on whole network fine-tuning to achieve performance gains while Swin Transformer does not have such a requirement. Moreover, vision transformers behave more robustly in multi-task learning, i.e., bringing more improvements when managing mutually beneficial tasks and reducing performance losses when tackling irrelevant tasks. We hope our discoveries can facilitate the exploration and exploitation of vision transformers in the future.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/39240f94c9915d9f9959c34b1dc68593894531e6.pdf",
        "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
        "citationCount": 55,
        "score": 13.75
    },
    "8db96d4eaefdac8dff6d98d94a5e3f5b5558c63d.pdf": {
        "title": "Vision Transformer Hashing for Image Retrieval",
        "authors": [
            "S. Dubey",
            "S. Singh",
            "Wei Chu"
        ],
        "published_date": "2021",
        "abstract": "Recently, Transformer has emerged as a new architecture in deep learning by utilizing self-attention without convolution. Transformer is also extended to Vision Transformer (ViT) for the visual recognition with a promising performance on ImageNet. In this paper, we propose a Vision Transformer Hashing (VTS) for image retrieval. We utilize the pre-trained ViT on ImageNet as the backbone network and add the hashing head. The proposed VTS model is fine tuned for hashing under six different image retrieval frameworks with their objective functions. We perform the extensive experiments on CIFAR10, ImageNet, NUS-Wide, and COCO datasets. The proposed VTS based image retrieval outperforms the recent state-of-the-art hashing techniques with a significant margin. We also find the proposed VTS model as the backbone network is better than the existing networks, such as AlexNet and ResNet. The code is released at https://github.com/shivram1987/VisionTransformerHashing.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/8db96d4eaefdac8dff6d98d94a5e3f5b5558c63d.pdf",
        "venue": "IEEE International Conference on Multimedia and Expo",
        "citationCount": 55,
        "score": 13.75
    },
    "428d755f0c8397ee6d04c89787f3455d323d8280.pdf": {
        "title": "SpectralSWIN: a spectral-swin transformer network for hyperspectral image classification",
        "authors": [
            "Selen Ayas",
            "Esra Tunc-Gormus"
        ],
        "published_date": "2022",
        "abstract": "ABSTRACT Hyperspectral image (HSI) classification has received extensive attention by the development of deep learning and has achieved great success. However, most of the deep learning-based approaches tend to extract features of spatial content by disrupting spectral information or to extract sequential spectral features in short-range context. On the other hand, Transformers-based models address this problem by learning long-range relationship. This study introduces a novel spectral-swin transformer (SpectralSWIN) network. The proposed network effectively projects the HSI data from spectral characteristics into spatial and spectral feature representation. Specifically, SpectralSWIN network makes use of a newly proposed swin-spectral module (SSM) for processing the spatial and spectral features concurrently. As far as we know, this is the first time that a transformer backbone designed for vision domain has been proposed for HSI classification. Extensive experiments conducted on two different HSI prove the superiority and effectiveness of the proposed method over the state-of-the-art methods in terms of both quantitative and visual evaluations.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/428d755f0c8397ee6d04c89787f3455d323d8280.pdf",
        "venue": "International Journal of Remote Sensing",
        "citationCount": 41,
        "score": 13.666666666666666
    },
    "ff00791b780b10336cc02ee366446d16e1c5e17b.pdf": {
        "title": "TVT: Three-Way Vision Transformer through Multi-Modal Hypersphere Learning for Zero-Shot Sketch-Based Image Retrieval",
        "authors": [
            "Jialin Tian",
            "Xing Xu",
            "Fumin Shen",
            "Yang Yang",
            "Heng Tao Shen"
        ],
        "published_date": "2022",
        "abstract": "In this paper, we study the zero-shot sketch-based image retrieval (ZS-SBIR) task, which retrieves natural images related to sketch queries from unseen categories. In the literature, convolutional neural networks (CNNs) have become the de-facto standard and they are either trained end-to-end or used to extract pre-trained features for images and sketches. However, CNNs are limited in modeling the global structural information of objects due to the intrinsic locality of convolution operations. To this end, we propose a Transformer-based approach called Three-Way Vision Transformer (TVT) to leverage the ability of Vision Transformer (ViT) to model global contexts due to the global self-attention mechanism. Going beyond simply applying ViT to this task, we propose a token-based strategy of adding fusion and distillation tokens and making them complementary to each other. Specifically, we integrate three ViTs, which are pre-trained on data of each modality, into a three-way pipeline through the processes of distillation and multi-modal hypersphere learning. The distillation process is proposed to supervise fusion ViT (ViT with an extra fusion token) with soft targets from modality-specific ViTs, which prevents fusion ViT from catastrophic forgetting. Furthermore, our method learns a multi-modal hypersphere by performing inter- and intra-modal alignment without loss of uniformity, which aims to bridge the modal gap between modalities of sketch and image and avoid the collapse in dimensions. Extensive experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and QuickDraw, demonstrate the superiority of our TVT method over the state-of-the-art ZS-SBIR methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ff00791b780b10336cc02ee366446d16e1c5e17b.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 40,
        "score": 13.333333333333332
    },
    "957a3d34303b424fe90a279cf5361253c93ac265.pdf": {
        "title": "High Resolution SAR Image Classification Using Global-Local Network Structure Based on Vision Transformer and CNN",
        "authors": [
            "Xingyu Liu",
            "Yuehua Wu",
            "Wenkai Liang",
            "Yi Cao",
            "Ming Li"
        ],
        "published_date": "2022",
        "abstract": "High-resolution (HR) synthetic aperture radar (SAR) image classification is a challenging task for the limitation of its complex semantic scenes and coherent speckles. Convolutional neural networks (CNNs) have been proven the superior local spatial features representation capability for SAR images. However, it is hard to capture global information of images by convolutions. To solve such issues, this letter proposes an end-to-end network named global\u2013local network structure (GLNS) for HR SAR classification. In the GLNS framework, a lightweight CNN and a compact vision transformer (ViT) are designed to learn local and global features, and two types of features are fused in quality to mine complementary information through the fusion net. Then, our research devolves the twofold loss function to reduce the interclass distance of SAR images, which brings more compactness to classification features and less interference of coherent speckles. Experimental results on real HR SAR images indicate that the proposed method has more strong feature extraction capability and noise resistance performance. This method achieves the highest classification accuracy on both datasets compared with other related approaches based on CNN.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/957a3d34303b424fe90a279cf5361253c93ac265.pdf",
        "venue": "IEEE Geoscience and Remote Sensing Letters",
        "citationCount": 40,
        "score": 13.333333333333332
    },
    "401c8d72a9b275e88e6ba159d8d646cfb9f397aa.pdf": {
        "title": "A free lunch from ViT: adaptive attention multi-scale fusion Transformer for fine-grained visual recognition",
        "authors": [
            "Yuan Zhang",
            "Jian Cao",
            "Ling Zhang",
            "Xiangcheng Liu",
            "Zhiyi Wang",
            "Feng Ling",
            "Weiqian Chen"
        ],
        "published_date": "2021",
        "abstract": "Learning subtle representation about object parts plays a vital role in fine-grained visual recognition (FGVR) field. The vision transformer (ViT) achieves promising results on computer vision due to its attention mechanism. Nonetheless, with the fixed size of patches in ViT, the class token in deep layer focuses on the global receptive field and cannot generate multi-granularity features for FGVR. To capture region attention without box annotations and compensate for ViT shortcomings in FGVR, we propose a novel method named Adaptive attention multi-scale Fusion Transformer (AFTrans). The Selective Attention Collection Module (SACM) in our approach leverages attention weights in ViT and filters them adaptively to correspond with the relative importance of input patches. The multiple scales (global and local) pipeline is supervised by our weights sharing encoder and can be easily trained end-to-end. Comprehensive experiments demonstrate that AFTrans can achieve SOTA performance on three published fine-grained benchmarks: CUB-200-2011, Stanford Dogs and iNat2017.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/401c8d72a9b275e88e6ba159d8d646cfb9f397aa.pdf",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "citationCount": 53,
        "score": 13.25
    },
    "7dee2bc2be709c0009b7623b7af78246f32e0a60.pdf": {
        "title": "Demystifying Local Vision Transformer: Sparse Connectivity, Weight Sharing, and Dynamic Weight",
        "authors": [
            "Qi Han",
            "Zejia Fan",
            "Qi Dai",
            "Lei Sun",
            "Mingg-Ming Cheng",
            "Jiaying Liu",
            "Jingdong Wang"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7dee2bc2be709c0009b7623b7af78246f32e0a60.pdf",
        "venue": "arXiv.org",
        "citationCount": 53,
        "score": 13.25
    },
    "1b026103e33b4c9eb637bc6f34715e22636b3492.pdf": {
        "title": "Facial Expression Recognition Based on Squeeze Vision Transformer",
        "authors": [
            "Sangwon Kim",
            "J. Nam",
            "ByoungChul Ko"
        ],
        "published_date": "2022",
        "abstract": "In recent image classification approaches, a vision transformer (ViT) has shown an excellent performance beyond that of a convolutional neural network. A ViT achieves a high classification for natural images because it properly preserves the global image features. Conversely, a ViT still has many limitations in facial expression recognition (FER), which requires the detection of subtle changes in expression, because it can lose the local features of the image. Therefore, in this paper, we propose Squeeze ViT, a method for reducing the computational complexity by reducing the number of feature dimensions while increasing the FER performance by concurrently combining global and local features. To measure the FER performance of Squeeze ViT, experiments were conducted on lab-controlled FER datasets and a wild FER dataset. Through comparative experiments with previous state-of-the-art approaches, we proved that the proposed method achieves an excellent performance on both types of datasets.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1b026103e33b4c9eb637bc6f34715e22636b3492.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 39,
        "score": 13.0
    },
    "024c595ba03087399e68e51f87adb4eaf5379701.pdf": {
        "title": "ASI-DBNet: An Adaptive Sparse Interactive ResNet-Vision Transformer Dual-Branch Network for the Grading of Brain Cancer Histopathological Images",
        "authors": [
            "Xiaoli Zhou",
            "Chaowei Tang",
            "Pan Huang",
            "Sukun Tian",
            "F. Mercaldo",
            "A. Santone"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/024c595ba03087399e68e51f87adb4eaf5379701.pdf",
        "venue": "Interdisciplinary Sciences Computational Life Sciences",
        "citationCount": 39,
        "score": 13.0
    },
    "9500dbde37e0f8c6b9ea924aacc6591bc3414166.pdf": {
        "title": "Toward Human-Centered Automated Driving: A Novel Spatiotemporal Vision Transformer-Enabled Head Tracker",
        "authors": [
            "Zhongxu Hu",
            "Yiran Zhang",
            "Yang Xing",
            "Yifan Zhao",
            "Dongpu Cao",
            "Chen Lv"
        ],
        "published_date": "2022",
        "abstract": "Accurate dynamic driver head pose tracking is of great importance for driver\u2013automotive collaboration, intelligent copilot, head-up display (HUD), and other human-centered automated driving applications. To further advance this technology, this article proposes a low-cost and markerless head-tracking system using a deep learning-based dynamic head pose estimation model. The proposed system requires only a red, green, blue (RGB) camera without other hardware or markers. To enhance the accuracy of the driver\u2019s head pose estimation, a spatiotemporal vision transformer (ST-ViT) model, which takes an image pair as the input instead of a single frame, is proposed. Compared to a standard transformer, the ST-ViT contains a spatial\u2013convolutional vision transformer and a temporal transformer, which can improve the model performance. To handle the error fluctuation of the head pose estimation model, this article proposes an adaptive Kalman filter (AKF). By analyzing the error distribution of the estimation model and the user experience of the head tracker, the proposed AKF includes an adaptive observation noise coefficient; this can adaptively moderate the smoothness of the curve. Comprehensive experiments show that the proposed system is feasible and effective, and it achieves a state-of-the-art performance.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9500dbde37e0f8c6b9ea924aacc6591bc3414166.pdf",
        "venue": "IEEE Vehicular Technology Magazine",
        "citationCount": 38,
        "score": 12.666666666666666
    },
    "977351c92f156db27592e88b14dee2c22d4b312a.pdf": {
        "title": "Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference",
        "authors": [
            "Haoran You",
            "Yunyang Xiong",
            "Xiaoliang Dai",
            "Bichen Wu",
            "Peizhao Zhang",
            "Haoqi Fan",
            "P\u00e9ter Vajda",
            "Yingyan Lin"
        ],
        "published_date": "2022",
        "abstract": "Vision Transformers (ViTs) have shown impressive per-formance but still require a high computation cost as compared to convolutional neural networks (CNNs), one rea-son is that ViTs' attention measures global similarities and thus has a quadratic complexity with the number of in-put tokens. Existing efficient ViTs adopt local attention or linear attention, which sacrifice ViTs' capabilities of capturing either global or local context. In this work, we ask an important research question: Can ViTs learn both global and local context while being more efficient during inference? To this end, we propose a framework called Castling- ViT, which trains ViTs using both linear-angular attention and masked softmax-based quadratic attention, but then switches to having only linear-angular attention during inference. Our Castling- ViT leverages angular ker-nels to measure the similarities between queries and keys via spectral angles. And we further simplify it with two techniques: (1) a novel linear-angular attention mechanism: we decompose the angular kernels into linear terms and high-order residuals, and only keep the linear terms; and (2) we adopt two parameterized modules to approximate high-order residuals: a depthwise convolution and an aux-iliary masked softmax attention to help learn global and lo-cal information, where the masks for softmax attention are regularized to gradually become zeros and thus incur no overhead during inference. Extensive experiments validate the effectiveness of our Castling- ViT, e.g., achieving up to a 1.8% higher accuracy or 40% MACs reduction on classification and 1.2 higher mAP on detection under comparable FLOPs, as compared to ViTs with vanilla softmax-based at-tentions. Project page is available at here.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/977351c92f156db27592e88b14dee2c22d4b312a.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 37,
        "score": 12.333333333333332
    },
    "ba12a9915553b3b42df17a33afcfd547821d8cc3.pdf": {
        "title": "Beyond Fixation: Dynamic Window Visual Transformer",
        "authors": [
            "Pengzhen Ren",
            "Changlin Li",
            "Guangrun Wang",
            "Yun Xiao",
            "Qing Du Xiaodan Liang Xiaojun Chang"
        ],
        "published_date": "2022",
        "abstract": "Recently, a surge of interest in visual transformers is to reduce the computational cost by limiting the calculation of self-attention to a local window. Most current work uses a fixed single-scale window for modeling by default, ignoring the impact of window size on model performance. How-ever, this may limit the modeling potential of these window-based models for multi-scale information. In this paper, we propose a novel method, named Dynamic Window Vision Transformer (DW-ViT). The dynamic window strategy proposed by DW- ViT goes beyond the model that employs a fixed single window setting. To the best of our knowl-edge, we are the first to use dynamic multi-scale windows to explore the upper limit of the effect of window settings on model performance. In DW- ViT, multi-scale information is obtained by assigning windows of different sizes to different head groups of window multi-head self-attention. Then, the information is dynamically fused by assigning different weights to the multi-scale window branches. We con-ducted a detailed performance evaluation on three datasets, ImageNet-1K, ADE20K, and COCO. Compared with re-lated state-of-the-art (SoTA) methods, DW- ViT obtains the best performance. Specifically, compared with the current SoTA Swin Transformers [31], DW-ViT has achieved con-sistent and substantial improvements on all three datasets with similar parameters and computational costs. In addition, DW-ViT exhibits good scalability and can be easily inserted into any window-based visual transformers.11Code release: https://github.com/pzhren/DW-ViT. This work was done when the first author interned at Dark Matter AI..",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ba12a9915553b3b42df17a33afcfd547821d8cc3.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 37,
        "score": 12.333333333333332
    },
    "3efcd3a4c54694a093886981d59e3cffe0dd7149.pdf": {
        "title": "SwinGD: A Robust Grape Bunch Detection Model Based on Swin Transformer in Complex Vineyard Environment",
        "authors": [
            "Jinhai Wang",
            "Zongyin Zhang",
            "Lufeng Luo",
            "Wenbo Zhu",
            "Jianwen Chen",
            "Wen Wang"
        ],
        "published_date": "2021",
        "abstract": "Accurate recognition of fruits in the orchard is an important step for robot picking in the natural environment, since many CNN models have a low recognition rate when dealing with irregularly shaped and very dense fruits, such as a grape bunch. It is a new trend to use a transformer structure and apply it to a computer vision domain for image processing. This paper provides Swin Transformer and DETR models to achieve grape bunch detection. Additionally, they are compared with traditional CNN models, such as Faster-RCNN, SSD, and YOLO. In addition, the optimal number of stages for a Swin Transformer through experiments is selected. Furthermore, the latest YOLOX model is also used to make a comparison with the Swin Transformer, and the experimental results show that YOLOX has higher accuracy and better detection effect. The above models are trained under red grape datasets collected under natural light. In addition, the dataset is expanded through image data augmentation to achieve a better training effect. After 200 epochs of training, SwinGD obtained an exciting mAP value of 94% when IoU = 0.5. In case of overexposure, overdarkness, and occlusion, SwinGD can recognize more accurately and robustly compared with other models. At the same time, SwinGD still has a better effect when dealing with dense grape bunches. Furthermore, 100 pictures of grapes containing 655 grape bunches are downloaded from Baidu pictures to detect the effect. The Swin Transformer has an accuracy of 91.5%. In order to verify the universality of SwinGD, we conducted a test under green grape images. The experimental results show that SwinGD has a good effect in practical application. The success of SwinGD provides a new solution for precision harvesting in agriculture.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3efcd3a4c54694a093886981d59e3cffe0dd7149.pdf",
        "venue": "Horticulturae",
        "citationCount": 49,
        "score": 12.25
    },
    "58fc305734a0d5d849ae69b9233af082d712197e.pdf": {
        "title": "A Swin Transformer-Based Encoding Booster Integrated in U-Shaped Network for Building Extraction",
        "authors": [
            "Xiao Xiao",
            "Wenliang Guo",
            "Rui Chen",
            "Yilong Hui",
            "J. Wang",
            "Hongyu Zhao"
        ],
        "published_date": "2022",
        "abstract": "Building extraction is a popular topic in remote sensing image processing. Efficient building extraction algorithms can identify and segment building areas to provide informative data for downstream tasks. Currently, building extraction is mainly achieved by deep convolutional neural networks (CNNs) based on the U-shaped encoder\u2013decoder architecture. However, the local perceptive field of the convolutional operation poses a challenge for CNNs to fully capture the semantic information of large buildings, especially in high-resolution remote sensing images. Considering the recent success of the Transformer in computer vision tasks, in this paper, first we propose a shifted-window (swin) Transformer-based encoding booster. The proposed encoding booster includes a swin Transformer pyramid containing patch merging layers for down-sampling, which enables our encoding booster to extract semantics from multi-level features at different scales. Most importantly, the receptive field is significantly expanded by the global self-attention mechanism of the swin Transformer, allowing the encoding booster to capture the large-scale semantic information effectively and transcend the limitations of CNNs. Furthermore, we integrate the encoding booster in a specially designed U-shaped network through a novel manner, named the Swin Transformer-based Encoding Booster- U-shaped Network (STEB-UNet), to achieve the feature-level fusion of local and large-scale semantics. Remarkably, compared with other Transformer-included networks, the computational complexity and memory requirement of the STEB-UNet are significantly reduced due to the swin design, making the network training much easier. Experimental results show that the STEB-UNet can effectively discriminate and extract buildings of different scales and demonstrate higher accuracy than the state-of-the-art networks on public datasets.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/58fc305734a0d5d849ae69b9233af082d712197e.pdf",
        "venue": "Remote Sensing",
        "citationCount": 36,
        "score": 12.0
    },
    "54911915a13cf0138c06b696e6c604b12acfe228.pdf": {
        "title": "Distinguishing Malicious Drones Using Vision Transformer",
        "authors": [
            "Sonain Jamil",
            "Muhammad Sohail Abbas",
            "Anisha Roy"
        ],
        "published_date": "2022",
        "abstract": "Drones are commonly used in numerous applications, such as surveillance, navigation, spraying pesticides in autonomous agricultural systems, various military services, etc., due to their variable sizes and workloads. However, malicious drones that carry harmful objects are often adversely used to intrude restricted areas and attack critical public places. Thus, the timely detection of malicious drones can prevent potential harm. This article proposes a vision transformer (ViT) based framework to distinguish between drones and malicious drones. In the proposed ViT based model, drone images are split into fixed-size patches; then, linearly embeddings and position embeddings are applied, and the resulting sequence of vectors is finally fed to a standard ViT encoder. During classification, an additional learnable classification token associated to the sequence is used. The proposed framework is compared with several handcrafted and deep convolutional neural networks (D-CNN), which reveal that the proposed model has achieved an accuracy of 98.3%, outperforming various handcrafted and D-CNNs models. Additionally, the superiority of the proposed model is illustrated by comparing it with the existing state-of-the-art drone-detection methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/54911915a13cf0138c06b696e6c604b12acfe228.pdf",
        "venue": "Applied Informatics",
        "citationCount": 36,
        "score": 12.0
    },
    "b8585577d05cebd85d45b7c63f7011851412e794.pdf": {
        "title": "Transformer-Based Disease Identification for Small-Scale Imbalanced Capsule Endoscopy Dataset",
        "authors": [
            "Long Bai",
            "Liangyu Wang",
            "Tong Chen",
            "Yuanhao Zhao",
            "Hongliang Ren"
        ],
        "published_date": "2022",
        "abstract": "Vision Transformer (ViT) is emerging as a new leader in computer vision with its outstanding performance in many tasks (e.g., ImageNet-22k, JFT-300M). However, the success of ViT relies on pretraining on large datasets. It is difficult for us to use ViT to train from scratch on a small-scale imbalanced capsule endoscopic image dataset. This paper adopts a Transformer neural network with a spatial pooling configuration. Transfomer\u2019s self-attention mechanism enables it to capture long-range information effectively, and the exploration of ViT spatial structure by pooling can further improve the performance of ViT on our small-scale capsule endoscopy dataset. We trained from scratch on two publicly available datasets for capsule endoscopy disease classification, obtained 79.15% accuracy on the multi-classification task of the Kvasir-Capsule dataset, and 98.63% accuracy on the binary classification task of the Red Lesion Endoscopy dataset.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b8585577d05cebd85d45b7c63f7011851412e794.pdf",
        "venue": "Electronics",
        "citationCount": 36,
        "score": 12.0
    },
    "956d45f7a8916ec921df522c0641fd4f02beccb7.pdf": {
        "title": "Ship Detection in SAR Images Based on Feature Enhancement Swin Transformer and Adjacent Feature Fusion",
        "authors": [
            "Kuoyang Li",
            "Min Zhang",
            "Maiping Xu",
            "R. Tang",
            "Liang Wang",
            "Hai Wang"
        ],
        "published_date": "2022",
        "abstract": "Convolutional neural networks (CNNs) have achieved milestones in object detection of synthetic aperture radar (SAR) images. Recently, vision transformers and their variants have shown great promise in detection tasks. However, ship detection in SAR images remains a substantial challenge because of the characteristics of strong scattering, multi-scale, and complex backgrounds of ship objects in SAR images. This paper proposes an enhancement Swin transformer detection network, named ESTDNet, to complete the ship detection in SAR images to solve the above problems. We adopt the Swin transformer of Cascade-R-CNN (Cascade R-CNN Swin) as a benchmark model in ESTDNet. Based on this, we built two modules in ESTDNet: the feature enhancement Swin transformer (FESwin) module for improving feature extraction capability and the adjacent feature fusion (AFF) module for optimizing feature pyramids. Firstly, the FESwin module is employed as the backbone network, aggregating contextual information about perceptions before and after the Swin transformer model using CNN. It uses single-point channel information interaction as the primary and local spatial information interaction as the secondary for scale fusion based on capturing visual dependence through self-attention, which improves spatial-to-channel feature expression and increases the utilization of ship information from SAR images. Secondly, the AFF module is a weighted selection fusion of each high-level feature in the feature pyramid with its adjacent shallow-level features using learnable adaptive weights, allowing the ship information of SAR images to be focused on the feature maps at more scales and improving the recognition and localization capability for ships in SAR images. Finally, the ablation study conducted on the SSDD dataset validates the effectiveness of the two components proposed in the ESTDNet detector. Moreover, the experiments executed on two public datasets consisting of SSDD and SARShip demonstrate that the ESTDNet detector outperforms the state-of-the-art methods, which provides a new idea for ship detection in SAR images.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/956d45f7a8916ec921df522c0641fd4f02beccb7.pdf",
        "venue": "Remote Sensing",
        "citationCount": 35,
        "score": 11.666666666666666
    },
    "99fbe810d4194684be03458fdfebacb12d8a5c4e.pdf": {
        "title": "Self-Ensembling Vision Transformer (SEViT) for Robust Medical Image Classification",
        "authors": [
            "Faris Almalik",
            "Mohammad Yaqub",
            "Karthik Nandakumar"
        ],
        "published_date": "2022",
        "abstract": "Vision Transformers (ViT) are competing to replace Convolutional Neural Networks (CNN) for various computer vision tasks in medical imaging such as classification and segmentation. While the vulnerability of CNNs to adversarial attacks is a well-known problem, recent works have shown that ViTs are also susceptible to such attacks and suffer significant performance degradation under attack. The vulnerability of ViTs to carefully engineered adversarial samples raises serious concerns about their safety in clinical settings. In this paper, we propose a novel self-ensembling method to enhance the robustness of ViT in the presence of adversarial attacks. The proposed Self-Ensembling Vision Transformer (SEViT) leverages the fact that feature representations learned by initial blocks of a ViT are relatively unaffected by adversarial perturbations. Learning multiple classifiers based on these intermediate feature representations and combining these predictions with that of the final ViT classifier can provide robustness against adversarial attacks. Measuring the consistency between the various predictions can also help detect adversarial samples. Experiments on two modalities (chest X-ray and fundoscopy) demonstrate the efficacy of SEViT architecture to defend against various adversarial attacks in the gray-box (attacker has full knowledge of the target model, but not the defense mechanism) setting. Code: https://github.com/faresmalik/SEViT",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/99fbe810d4194684be03458fdfebacb12d8a5c4e.pdf",
        "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
        "citationCount": 35,
        "score": 11.666666666666666
    },
    "f4e32b928d7cc27447e312bdc052aa75888045aa.pdf": {
        "title": "MITformer: A Multiinstance Vision Transformer for Remote Sensing Scene Classification",
        "authors": [
            "Z. Sha",
            "Jianfeng Li"
        ],
        "published_date": "2022",
        "abstract": "The latest vision transformer (ViT) has stronger contextual feature representation capability than the existing convolutional neural networks and thus has the potential to depict the remote sensing scenes, which usually have more complicated object distribution and spatial arrangement than ground image scenes. However, recent researches reflect that while ViT learns global features, it also ignores the key local features, which poses a bottleneck for understanding remote sensing scenes. In this letter, we tackle this challenge by proposing a novel multiinstance vision transformer (MITformer). Its originality mainly lies in the classic multiple instance learning (MIL) formulation, where each image patch embedded in ViT is regarded as an instance and each image is regarded as a bag. The benefit of designing ViT under MIL formulation is straightforward, as it helps highlight the feature response of key local regions of remote sensing scenes. Moreover, to enhance the feature propagation of local features, attention-based multilayer perceptron (AMLP) head is embedded at the end of each encoder unit. Finally, to minimize the potential semantic prediction differences between the classic ViT and our MIL head, a semantic consistency loss is designed. Experiments on three remote sensing scene classification benchmarks show that our proposed MITformer outperforms the existing state-of-the-art methods and validate the effectiveness of each component in our MITformer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f4e32b928d7cc27447e312bdc052aa75888045aa.pdf",
        "venue": "IEEE Geoscience and Remote Sensing Letters",
        "citationCount": 34,
        "score": 11.333333333333332
    },
    "98e702ef2f64ab2643df9e80b1bd034334142e62.pdf": {
        "title": "HiViT: Hierarchical Vision Transformer Meets Masked Image Modeling",
        "authors": [
            "Xiaosong Zhang",
            "Yunjie Tian",
            "Wei Huang",
            "Qixiang Ye",
            "Qi Dai",
            "Lingxi Xie",
            "Qi Tian"
        ],
        "published_date": "2022",
        "abstract": "Recently, masked image modeling (MIM) has offered a new methodology of self-supervised pre-training of vision transformers. A key idea of efficient implementation is to discard the masked image patches (or tokens) throughout the target network (encoder), which requires the encoder to be a plain vision transformer (e.g., ViT), albeit hierarchical vision transformers (e.g., Swin Transformer) have potentially better properties in formulating vision inputs. In this paper, we offer a new design of hierarchical vision transformers named HiViT (short for Hierarchical ViT) that enjoys both high efficiency and good performance in MIM. The key is to remove the unnecessary\"local inter-unit operations\", deriving structurally simple hierarchical vision transformers in which mask-units can be serialized like plain vision transformers. For this purpose, we start with Swin Transformer and (i) set the masking unit size to be the token size in the main stage of Swin Transformer, (ii) switch off inter-unit self-attentions before the main stage, and (iii) eliminate all operations after the main stage. Empirical studies demonstrate the advantageous performance of HiViT in terms of fully-supervised, self-supervised, and transfer learning. In particular, in running MAE on ImageNet-1K, HiViT-B reports a +0.6% accuracy gain over ViT-B and a 1.9$\\times$ speed-up over Swin-B, and the performance gain generalizes to downstream tasks of detection and segmentation. Code will be made publicly available.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/98e702ef2f64ab2643df9e80b1bd034334142e62.pdf",
        "venue": "arXiv.org",
        "citationCount": 33,
        "score": 11.0
    },
    "0ef3e7efa5ded21a398d8ec90ff766b467c4fc54.pdf": {
        "title": "Vision Transformer in Industrial Visual Inspection",
        "authors": [
            "Nils H\u00fctten",
            "R. Meyes",
            "Tobias Meisen"
        ],
        "published_date": "2022",
        "abstract": "Artificial intelligence as an approach to visual inspection in industrial applications has been considered for decades. Recent successes, driven by advances in deep learning, present a potential paradigm shift and have the potential to facilitate an automated visual inspection, even under complex environmental conditions. Thereby, convolutional neural networks (CNN) have been the de facto standard in deep-learning-based computer vision (CV) for the last 10 years. Recently, attention-based vision transformer architectures emerged and surpassed the performance of CNNs on benchmark datasets, regarding regular CV tasks, such as image classification, object detection, or segmentation. Nevertheless, despite their outstanding results, the application of vision transformers to real world visual inspection is sparse. We suspect that this is likely due to the assumption that they require enormous amounts of data to be effective. In this study, we evaluate this assumption. For this, we perform a systematic comparison of seven widely-used state-of-the-art CNN and transformer based architectures trained in three different use cases in the domain of visual damage assessment for railway freight car maintenance. We show that vision transformer models achieve at least equivalent performance to CNNs in industrial applications with sparse data available, and significantly surpass them in increasingly complex tasks.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0ef3e7efa5ded21a398d8ec90ff766b467c4fc54.pdf",
        "venue": "Applied Sciences",
        "citationCount": 32,
        "score": 10.666666666666666
    },
    "a21fa5ff42db4b0bd0fefae3d710cad5f0175400.pdf": {
        "title": "UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation",
        "authors": [
            "Ali Hatamizadeh",
            "Ziyue Xu",
            "Dong Yang",
            "Wenqi Li",
            "H. Roth",
            "Daguang Xu"
        ],
        "published_date": "2022",
        "abstract": "Vision Transformers (ViT)s have recently become popular due to their outstanding modeling capabilities, in particular for capturing long-range information, and scalability to dataset and model sizes which has led to state-of-the-art performance in various computer vision and medical image analysis tasks. In this work, we introduce a unified framework consisting of two architectures, dubbed UNetFormer, with a 3D Swin Transformer-based encoder and Convolutional Neural Network (CNN) and transformer-based decoders. In the proposed model, the encoder is linked to the decoder via skip connections at five different resolutions with deep supervision. The design of proposed architecture allows for meeting a wide range of trade-off requirements between accuracy and computational cost. In addition, we present a methodology for self-supervised pre-training of the encoder backbone via learning to predict randomly masked volumetric tokens using contextual information of visible tokens. We pre-train our framework on a cohort of $5050$ CT images, gathered from publicly available CT datasets, and present a systematic investigation of various components such as masking ratio and patch size that affect the representation learning capability and performance of downstream tasks. We validate the effectiveness of our pre-training approach by fine-tuning and testing our model on liver and liver tumor segmentation task using the Medical Segmentation Decathlon (MSD) dataset and achieve state-of-the-art performance in terms of various segmentation metrics. To demonstrate its generalizability, we train and test the model on BraTS 21 dataset for brain tumor segmentation using MRI images and outperform other methods in terms of Dice score. Code: https://github.com/Project-MONAI/research-contributions",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a21fa5ff42db4b0bd0fefae3d710cad5f0175400.pdf",
        "venue": "arXiv.org",
        "citationCount": 31,
        "score": 10.333333333333332
    },
    "174919e5a4ef95ff66440d56614ad954c6f27df1.pdf": {
        "title": "ViT-HGR: Vision Transformer-based Hand Gesture Recognition from High Density Surface EMG Signals",
        "authors": [
            "Mansooreh Montazerin",
            "Soheil Zabihi",
            "E. Rahimian",
            "Arash Mohammadi",
            "Farnoosh Naderkhani"
        ],
        "published_date": "2022",
        "abstract": "Recently, there has been a surge of significant interest on application of Deep Learning (DL) models to autonomously perform hand gesture recognition using surface Electromyogram (sEMG) signals. Many of the existing DL models are, however, designed to be applied on sparse sEMG signals. Furthermore, due to the complex structure of these models, typically, we are faced with memory constraint issues, require large training times and a large number of training samples, and; there is the need to resort to data augmentation and/or transfer learning. In this paper, for the first time (to the best of our knowledge), we investigate and design a Vision Transformer (ViT) based architecture to perform hand gesture recognition from High Density (HD-sEMG) signals. Intuitively speaking, we capitalize on the recent breakthrough role of the transformer architecture in tackling different com-plex problems together with its potential for employing more input parallelization via its attention mechanism. The proposed Vision Transformer-based Hand Gesture Recognition (ViT-HGR) framework can overcome the aforementioned training time problems and can accurately classify a large number of hand gestures from scratch without any need for data augmentation and/or transfer learning. The efficiency of the proposed ViT-HGR framework is evaluated using a recently-released HD-sEMG dataset consisting of 65 isometric hand gestures. Our experiments with 64-sample (31.25 ms) window size yield average test accuracy of 84.62 \u00b1 3.07%, where only 78,210 learnable parameters are utilized in the model. The compact structure of the proposed ViT-based ViT-HGR framework (i.e., having significantly reduced number of trainable parameters) shows great potentials for its practical application for prosthetic control.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/174919e5a4ef95ff66440d56614ad954c6f27df1.pdf",
        "venue": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society",
        "citationCount": 30,
        "score": 10.0
    },
    "6971aee925639a8bd5b79c821570728ef49060c6.pdf": {
        "title": "Robustifying Vision Transformer without Retraining from Scratch by Test-Time Class-Conditional Feature Alignment",
        "authors": [
            "Takeshi Kojima",
            "Yutaka Matsuo",
            "Yusuke Iwasawa"
        ],
        "published_date": "2022",
        "abstract": "Vision Transformer (ViT) is becoming more popular in image processing. Specifically, we investigate the effectiveness of test-time adaptation (TTA) on ViT, a technique that has emerged to correct its prediction during test-time by itself. First, we benchmark various test-time adaptation approaches on ViT-B16 and ViT-L16. It is shown that the TTA is effective on ViT and the prior-convention (sensibly selecting modulation parameters) is not necessary when using proper loss function. Based on the observation, we propose a new test-time adaptation method called class-conditional feature alignment (CFA), which minimizes both the class-conditional distribution differences and the whole distribution differences of the hidden representation between the source and target in an online manner. Experiments of image classification tasks on common corruption (CIFAR-10-C, CIFAR-100-C, and ImageNet-C) and domain adaptation (digits datasets and ImageNet-Sketch) show that CFA stably outperforms the existing baselines on various datasets. We also verify that CFA is model agnostic by experimenting on ResNet, MLP-Mixer, and several ViT variants (ViT-AugReg, DeiT, and BeiT). Using BeiT backbone, CFA achieves 19.8% top-1 error rate on ImageNet-C, outperforming the existing test-time adaptation baseline 44.0%. This is a state-of-the-art result among TTA methods that do not need to alter training phase.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6971aee925639a8bd5b79c821570728ef49060c6.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 30,
        "score": 10.0
    },
    "15ad149fc1ba4d8b5de189fdfacac9daecc286d0.pdf": {
        "title": "Vision Transformer for Detecting Critical Situations And\u00a0Extracting Functional Scenario for Automated Vehicle Safety Assessment",
        "authors": [
            "Minhee Kang",
            "Wooseop Lee",
            "Keeyeon Hwang",
            "Young Yoon"
        ],
        "published_date": "2022",
        "abstract": "Automated Vehicles (AVs) are attracting attention as a safer mobility option thanks to the recent advancement of various sensing technologies that realize a much quicker Perception\u2013Reaction Time than Human-Driven Vehicles (HVs). However, AVs are not entirely free from the risk of accidents, and we currently lack a systematic and reliable method to improve AV safety functions. The manual composition of accident scenarios does not scale. Simulation-based methods do not fully cover the peculiar AV accident patterns that can occur in the real world. Artificial Intelligence (AI) techniques are employed to identify the moments of accidents from ego-vehicle videos. However, most AI-based approaches fall short in accounting for the probable causes of the accidents. Neither of these AI-driven methods offer details for authoring accident scenarios used for AV safety testing. In this paper, we present a customized Vision Transformer (named ViT-TA) that accurately classifies the critical situations around traffic accidents and automatically points out the objects as probable causes based on an Attention map. Using 24,740 frames from Dashcam Accident Dataset (DAD) as training data, ViT-TA detected critical moments at Time-To-Collision (TTC) \u2264 1 s with 34.92 higher accuracy than the state-of-the-art approach. ViT-TA\u2019s Attention map highlighting the critical objects helped us understand how the situations unfold to put the hypothetical ego vehicles with AV functions at risk. Based on the ViT-TA-assisted interpretation, we systematized the composition of Functional scenarios conceptualized by the PEGASUS project for describing a high-level plan to improve AVs\u2019 capability of evading critical situations. We propose a novel framework for automatically deriving Logical and Concrete scenarios specified with 6-Layer situational variables defined by the PEGASUS project. We believe our work is vital towards systematically generating highly reliable and trustworthy safety improvement plans for AVs in a scalable manner.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/15ad149fc1ba4d8b5de189fdfacac9daecc286d0.pdf",
        "venue": "Social Science Research Network",
        "citationCount": 30,
        "score": 10.0
    },
    "f66181828b7621892d02480fa1944b5f381be80d.pdf": {
        "title": "A deep ensemble learning-based automated detection of COVID-19 using lung CT images and Vision Transformer and ConvNeXt",
        "authors": [
            "Geng Tian",
            "Ziwei Wang",
            "Chang Wang",
            "Jianhua Chen",
            "Guangyi Liu",
            "He Xu",
            "Yuankang Lu",
            "Zhuoran Han",
            "Yubo Zhao",
            "Zejun Li",
            "Xueming Luo",
            "Lihong Peng"
        ],
        "published_date": "2022",
        "abstract": "Since the outbreak of COVID-19, hundreds of millions of people have been infected, causing millions of deaths, and resulting in a heavy impact on the daily life of countless people. Accurately identifying patients and taking timely isolation measures are necessary ways to stop the spread of COVID-19. Besides the nucleic acid test, lung CT image detection is also a path to quickly identify COVID-19 patients. In this context, deep learning technology can help radiologists identify COVID-19 patients from CT images rapidly. In this paper, we propose a deep learning ensemble framework called VitCNX which combines Vision Transformer and ConvNeXt for COVID-19 CT image identification. We compared our proposed model VitCNX with EfficientNetV2, DenseNet, ResNet-50, and Swin-Transformer which are state-of-the-art deep learning models in the field of image classification, and two individual models which we used for the ensemble (Vision Transformer and ConvNeXt) in binary and three-classification experiments. In the binary classification experiment, VitCNX achieves the best recall of 0.9907, accuracy of 0.9821, F1-score of 0.9855, AUC of 0.9985, and AUPR of 0.9991, which outperforms the other six models. Equally, in the three-classification experiment, VitCNX computes the best precision of 0.9668, an accuracy of 0.9696, and an F1-score of 0.9631, further demonstrating its excellent image classification capability. We hope our proposed VitCNX model could contribute to the recognition of COVID-19 patients.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f66181828b7621892d02480fa1944b5f381be80d.pdf",
        "venue": "Frontiers in Microbiology",
        "citationCount": 29,
        "score": 9.666666666666666
    },
    "cee8934975dfbe89747af60bbafc95e10a788dc2.pdf": {
        "title": "Analysis of CT scan images for COVID-19 pneumonia based on a deep ensemble framework with DenseNet, Swin transformer, and RegNet",
        "authors": [
            "Lihong Peng",
            "Chang Wang",
            "Geng Tian",
            "Guangyi Liu",
            "Gan Li",
            "Yuankang Lu",
            "Jialiang Yang",
            "Min Chen",
            "Zejun Li"
        ],
        "published_date": "2022",
        "abstract": "COVID-19 has caused enormous challenges to global economy and public health. The identification of patients with the COVID-19 infection by CT scan images helps prevent its pandemic. Manual screening COVID-19-related CT images spends a lot of time and resources. Artificial intelligence techniques including deep learning can effectively aid doctors and medical workers to screen the COVID-19 patients. In this study, we developed an ensemble deep learning framework, DeepDSR, by combining DenseNet, Swin transformer, and RegNet for COVID-19 image identification. First, we integrate three available COVID-19-related CT image datasets to one larger dataset. Second, we pretrain weights of DenseNet, Swin Transformer, and RegNet on the ImageNet dataset based on transformer learning. Third, we continue to train DenseNet, Swin Transformer, and RegNet on the integrated larger image dataset. Finally, the classification results are obtained by integrating results from the above three models and the soft voting approach. The proposed DeepDSR model is compared to three state-of-the-art deep learning models (EfficientNetV2, ResNet, and Vision transformer) and three individual models (DenseNet, Swin transformer, and RegNet) for binary classification and three-classification problems. The results show that DeepDSR computes the best precision of 0.9833, recall of 0.9895, accuracy of 0.9894, F1-score of 0.9864, AUC of 0.9991 and AUPR of 0.9986 under binary classification problem, and significantly outperforms other methods. Furthermore, DeepDSR obtains the best precision of 0.9740, recall of 0.9653, accuracy of 0.9737, and F1-score of 0.9695 under three-classification problem, further suggesting its powerful image identification ability. We anticipate that the proposed DeepDSR framework contributes to the diagnosis of COVID-19.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/cee8934975dfbe89747af60bbafc95e10a788dc2.pdf",
        "venue": "Frontiers in Microbiology",
        "citationCount": 29,
        "score": 9.666666666666666
    },
    "69e20583872b4f1384ae3f7dbdbec648c19d3d35.pdf": {
        "title": "Network Intrusion Detection via Flow-to-Image Conversion and Vision Transformer Classification",
        "authors": [
            "Chi M. K. Ho",
            "K. Yow",
            "Zhongwen Zhu",
            "Sarang Aravamuthan"
        ],
        "published_date": "2022",
        "abstract": "In recent years, computer networks have become an indispensable part of our life, and these networks are vulnerable to various type of network attacks, compromising the security of our data and the freedom of our communications. In this paper, we propose a new intrusion detection method that uses image conversion from network data flow to produce an RGB image that can be classified using advanced deep learning models. In this method, we proposed to use the decision tree algorithm to identify the important features, and a windowing and overlapping mechanism to convert the varying input size to a standard size image for the classifier. We then use a Vision Transfomer (ViT) classifier to classify the resulting image. Our experimental results show that we can achieve 98.5% accuracy in binary classification on the CIC IDS2017 dataset, and 96.3% on the UNSW-NB15 dataset, which is 8.09% higher than the next best algorithm, the Deep Belief Network with Improved Kernel-Based Extreme Learning (DBN-KELM) method. For multi-class classification, our proposed method can achieve a testing accuracy of 96.4%, which is 5.6% higher than the next best method, the DBN-KELM.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/69e20583872b4f1384ae3f7dbdbec648c19d3d35.pdf",
        "venue": "IEEE Access",
        "citationCount": 29,
        "score": 9.666666666666666
    },
    "0594eaa8dfe580678a2382aaf77ac3582c872a97.pdf": {
        "title": "TRT-ViT: TensorRT-oriented Vision Transformer",
        "authors": [
            "Xin Xia",
            "Jiashi Li",
            "Jie Wu",
            "Xing Wang",
            "Ming-Yu Wang",
            "Xuefeng Xiao",
            "Minghang Zheng",
            "Rui Wang"
        ],
        "published_date": "2022",
        "abstract": "We revisit the existing excellent Transformers from the perspective of practical application. Most of them are not even as efficient as the basic ResNets series and deviate from the realistic deployment scenario. It may be due to the current criterion to measure computation efficiency, such as FLOPs or parameters is one-sided, sub-optimal, and hardware-insensitive. Thus, this paper directly treats the TensorRT latency on the specific hardware as an efficiency metric, which provides more comprehensive feedback involving computational capacity, memory cost, and bandwidth. Based on a series of controlled experiments, this work derives four practical guidelines for TensorRT-oriented and deployment-friendly network design, e.g., early CNN and late Transformer at stage-level, early Transformer and late CNN at block-level. Accordingly, a family of TensortRT-oriented Transformers is presented, abbreviated as TRT-ViT. Extensive experiments demonstrate that TRT-ViT significantly outperforms existing ConvNets and vision Transformers with respect to the latency/accuracy trade-off across diverse visual tasks, e.g., image classification, object detection and semantic segmentation. For example, at 82.7% ImageNet-1k top-1 accuracy, TRT-ViT is 2.7$\\times$ faster than CSWin and 2.0$\\times$ faster than Twins. On the MS-COCO object detection task, TRT-ViT achieves comparable performance with Twins, while the inference speed is increased by 2.8$\\times$.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0594eaa8dfe580678a2382aaf77ac3582c872a97.pdf",
        "venue": "arXiv.org",
        "citationCount": 28,
        "score": 9.333333333333332
    },
    "3a0145f34bcd35f09db23b2edec3ed097894444c.pdf": {
        "title": "Recognition of penetration state in GTAW based on vision transformer using weld pool image",
        "authors": [
            "Zhenmin Wang",
            "Haoyu Chen",
            "Q. Zhong",
            "Sanbao Lin",
            "Jianwen Wu",
            "Mengjia Xu",
            "Qin Zhang"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3a0145f34bcd35f09db23b2edec3ed097894444c.pdf",
        "venue": "The International Journal of Advanced Manufacturing Technology",
        "citationCount": 28,
        "score": 9.333333333333332
    },
    "ce79649b126dfe9e3cbeb1ecd64a80708bbd5538.pdf": {
        "title": "Gait-ViT: Gait Recognition with Vision Transformer",
        "authors": [
            "Jashila Nair Mogan",
            "C. Lee",
            "K. Lim",
            "K. Anbananthen"
        ],
        "published_date": "2022",
        "abstract": "Identifying an individual based on their physical/behavioral characteristics is known as biometric recognition. Gait is one of the most reliable biometrics due to its advantages, such as being perceivable at a long distance and difficult to replicate. The existing works mostly leverage Convolutional Neural Networks for gait recognition. The Convolutional Neural Networks perform well in image recognition tasks; however, they lack the attention mechanism to emphasize more on the significant regions of the image. The attention mechanism encodes information in the image patches, which facilitates the model to learn the substantial features in the specific regions. In light of this, this work employs the Vision Transformer (ViT) with an attention mechanism for gait recognition, referred to as Gait-ViT. In the proposed Gait-ViT, the gait energy image is first obtained by averaging the series of images over the gait cycle. The images are then split into patches and transformed into sequences by flattening and patch embedding. Position embedding, along with patch embedding, are applied on the sequence of patches to restore the positional information of the patches. Subsequently, the sequence of vectors is fed to the Transformer encoder to produce the final gait representation. As for the classification, the first element of the sequence is sent to the multi-layer perceptron to predict the class label. The proposed method obtained 99.93% on CASIA-B, 100% on OU-ISIR D and 99.51% on OU-LP, which exhibit the ability of the Vision Transformer model to outperform the state-of-the-art methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ce79649b126dfe9e3cbeb1ecd64a80708bbd5538.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 27,
        "score": 9.0
    },
    "572ed945b06818472105bd17cfeb355d4e46c5e5.pdf": {
        "title": "Intrusion detection: A model based on the improved vision transformer",
        "authors": [
            "Yuguang Yang",
            "Hong\u2010Mei Fu",
            "Shang Gao",
            "Yihua Zhou",
            "Wei-Min shi"
        ],
        "published_date": "2022",
        "abstract": "We propose an intrusion detection model based on an improved vision transformer (ViT). More specifically, the model uses an attention mechanism to process data, which overcomes the flaw of the short\u2010term memory in recurrent neural network (RNN) and the difficulty of learning remote dependency in convolutional neural network. It supports parallelization and has a faster computing speed than RNN. A sliding window mechanism is presented to improve the capability of modeling local features for ViT. The hierarchical focal loss function is used to improve the classification effect, and solve the issue of the data imbalance. The public intrusion detection dataset NSL\u2010KDD is used for experimental simulations. By experimental simulations, the accuracy is up to 99.68%, the false\u2010positive rate is 0.22%, and the recall rate is 99.57%, which show that the improved ViT has better accuracy, false positive rate, and recall rate than existing intrusion detection models.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/572ed945b06818472105bd17cfeb355d4e46c5e5.pdf",
        "venue": "Transactions on Emerging Telecommunications Technologies",
        "citationCount": 27,
        "score": 9.0
    },
    "934942934a6a785e2a80daa6421fa79971558b89.pdf": {
        "title": "BViT: Broad Attention-Based Vision Transformer",
        "authors": [
            "Nannan Li",
            "Yaran Chen",
            "Weifan Li",
            "Zixiang Ding",
            "Dong Zhao"
        ],
        "published_date": "2022",
        "abstract": "Recent works have demonstrated that transformer can achieve promising performance in computer vision, by exploiting the relationship among image patches with self-attention. They only consider the attention in a single feature layer, but ignore the complementarity of attention in different layers. In this article, we propose broad attention to improve the performance by incorporating the attention relationship of different layers for vision transformer (ViT), which is called BViT. The broad attention is implemented by broad connection and parameter-free attention. Broad connection of each transformer layer promotes the transmission and integration of information for BViT. Without introducing additional trainable parameters, parameter-free attention jointly focuses on the already available attention information in different layers for extracting useful information and building their relationship. Experiments on image classification tasks demonstrate that BViT delivers superior accuracy of 75.0%/81.6% top-1 accuracy on ImageNet with 5M/22M parameters. Moreover, we transfer BViT to downstream object recognition benchmarks to achieve 98.9% and 89.9% on CIFAR10 and CIFAR100, respectively, that exceed ViT with fewer parameters. For the generalization test, the broad attention in Swin Transformer, T2T-ViT and LVT also brings an improvement of more than 1%. To sum up, broad attention is promising to promote the performance of attention-based models. Code and pretrained models are available at https://github.com/DRL/BViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/934942934a6a785e2a80daa6421fa79971558b89.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 27,
        "score": 9.0
    },
    "3ae8c0b646ddce95ffd09da31c02ed6fdb744e90.pdf": {
        "title": "BOAT: Bilateral Local Attention Vision Transformer",
        "authors": [
            "Tan Yu",
            "Gangming Zhao",
            "Ping Li",
            "Yizhou Yu"
        ],
        "published_date": "2022",
        "abstract": "Vision Transformers achieved outstanding performance in many computer vision tasks. Early Vision Transformers such as ViT and DeiT adopt global self-attention, which is computationally expensive when the number of patches is large. To improve efficiency, recent Vision Transformers adopt local self-attention mechanisms, where self-attention is computed within local windows. Despite the fact that window-based local self-attention significantly boosts efficiency, it fails to capture the relationships between distant but similar patches in the image plane. To overcome this limitation of image-space local attention, in this paper, we further exploit the locality of patches in the feature space. We group the patches into multiple clusters using their features, and self-attention is computed within every cluster. Such feature-space local attention effectively captures the connections between patches across different local windows but still relevant. We propose a Bilateral lOcal Attention vision Transformer (BOAT), which integrates feature-space local attention with image-space local attention. We further integrate BOAT with both Swin and CSWin models, and extensive experiments on several benchmark datasets demonstrate that our BOAT-CSWin model clearly and consistently outperforms existing state-of-the-art CNN models and vision Transformers.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3ae8c0b646ddce95ffd09da31c02ed6fdb744e90.pdf",
        "venue": "British Machine Vision Conference",
        "citationCount": 27,
        "score": 9.0
    },
    "bc8d9f11ad91d54e83ad7cc3900502a913499fcb.pdf": {
        "title": "Transformer Based Multi-Grained Features for Unsupervised Person Re-Identification",
        "authors": [
            "Jiacheng Li",
            "Menglin Wang",
            "Xiaojin Gong"
        ],
        "published_date": "2022",
        "abstract": "Multi-grained features extracted from convolutional neural networks (CNNs) have demonstrated their strong dis-crimination ability in supervised person re-identification (Re-ID) tasks. Inspired by them, this work investigates the way of extracting multi-grained features from a pure transformer network to address the unsupervised Re-ID problem that is label-free but much more challenging. To this end, we build a dual-branch network architecture based upon a modified Vision Transformer (ViT). The local tokens output in each branch are reshaped and then uniformly partitioned into multiple stripes to generate part-level features, while the global tokens of two branches are averaged to produce a global feature. Further, based upon offline-online associated camera-aware proxies (02CAP) that is a top-performing unsupervised Re-ID method, we define offline and online contrastive learning losses with respect to both global and part-level features to conduct unsupervised learning. Extensive experiments on three person Re-ID datasets show that the proposed method outperforms state-of-the-art unsupervised methods by a considerable margin, greatly mitigating the gap to supervised counterparts. Code will be available soon at https://github.com/RikoLi/WACV23-workshop-TMGF.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/bc8d9f11ad91d54e83ad7cc3900502a913499fcb.pdf",
        "venue": "2023 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)",
        "citationCount": 27,
        "score": 9.0
    },
    "cf6d947d5d2ee72873a5a7b97dde9f881f79a6b6.pdf": {
        "title": "Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI",
        "authors": [
            "Jiahao Huang",
            "Xiaodan Xing",
            "Zhifan Gao",
            "Guang Yang"
        ],
        "published_date": "2022",
        "abstract": ". Fast MRI aims to reconstruct a high \ufb01delity image from partially observed measurements. Exuberant development in fast MRI using deep learning has been witnessed recently. Meanwhile, novel deep learning paradigms, e.g., Transformer based models, are fast-growing in natural language processing and promptly developed for computer vision and medical image analysis due to their prominent performance. Nevertheless, due to the complexity of the Transformer, the application of fast MRI may not be straightforward. The main obstacle is the computational cost of the self-attention layer, which is the core part of the Transformer, can be expensive for high resolution MRI inputs. In this study, we propose a new Transformer architecture for solving fast MRI that coupled Shifted Windows Transformer with U-Net to reduce the network complexity. We incorporate deformable attention to construe the explainability of our reconstruction model. We empirically demonstrate that our method achieves consistently superior performance on the fast MRI task. Besides, compared to state-of-the-art Transformer models, our method has fewer network parameters while revealing explainability. The code is publicly available at https://github.com/ayanglab/SDAUT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/cf6d947d5d2ee72873a5a7b97dde9f881f79a6b6.pdf",
        "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
        "citationCount": 27,
        "score": 9.0
    },
    "9db52b92b48b8855f76d7e4b51b7727a2b3c0271.pdf": {
        "title": "SiRi: A Simple Selective Retraining Mechanism for Transformer-based Visual Grounding",
        "authors": [
            "Mengxue Qu",
            "Yu Wu",
            "Wu Liu",
            "Qiqi Gong",
            "Xiaodan Liang",
            "Olga Russakovsky",
            "Yao Zhao",
            "Yunchao Wei"
        ],
        "published_date": "2022",
        "abstract": "In this paper, we investigate how to achieve better visual grounding with modern vision-language transformers, and propose a simple yet powerful Selective Retraining (SiRi) mechanism for this challenging task. Particularly, SiRi conveys a significant principle to the research of visual grounding, i.e., a better initialized vision-language encoder would help the model converge to a better local minimum, advancing the performance accordingly. In specific, we continually update the parameters of the encoder as the training goes on, while periodically re-initialize rest of the parameters to compel the model to be better optimized based on an enhanced encoder. SiRi can significantly outperform previous approaches on three popular benchmarks. Specifically, our method achieves 83.04% Top1 accuracy on RefCOCO+ testA, outperforming the state-of-the-art approaches (training from scratch) by more than 10.21%. Additionally, we reveal that SiRi performs surprisingly superior even with limited training data. We also extend it to transformer-based visual grounding models and other vision-language tasks to verify the validity.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9db52b92b48b8855f76d7e4b51b7727a2b3c0271.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 26,
        "score": 8.666666666666666
    },
    "e4add4391dfa2a806a50cc1fbe9a9696dac9501f.pdf": {
        "title": "MPCViT: Searching for Accurate and Efficient MPC-Friendly Vision Transformer with Heterogeneous Attention",
        "authors": [
            "Wenyuan Zeng",
            "Meng Li",
            "Wenjie Xiong",
            "Tong Tong",
            "Wen-jie Lu",
            "Jin Tan",
            "Runsheng Wang",
            "Ru Huang"
        ],
        "published_date": "2022",
        "abstract": "Secure multi-party computation (MPC) enables computation directly on encrypted data and protects both data and model privacy in deep learning inference. However, existing neural network architectures, including Vision Transformers (ViTs), are not designed or optimized for MPC and incur significant latency overhead. We observe Softmax accounts for the major latency bottleneck due to a high communication complexity, but can be selectively replaced or linearized without compromising the model accuracy. Hence, in this paper, we propose an MPC-friendly ViT, dubbed MPCViT, to enable accurate yet efficient ViT inference in MPC. Based on a systematic latency and accuracy evaluation of the Softmax attention and other attention variants, we propose a heterogeneous attention optimization space. We also develop a simple yet effective MPC-aware neural architecture search algorithm for fast Pareto optimization. To further boost the inference efficiency, we propose MPCViT+, to jointly optimize the Softmax attention and other network components, including GeLU, matrix multiplication, etc. With extensive experiments, we demonstrate that MPCViT achieves 1.9%, 1.3% and 3.6% higher accuracy with 6.2\u00d7, 2.9\u00d7 and 1.9\u00d7 latency reduction compared with baseline ViT, MPCFormer and THE-X on the Tiny-ImageNet dataset, respectively. MPCViT+ further achieves a better Pareto front compared with MPCViT. The code and models for evaluation are available at https://github.com/PKU-SEC-Lab/mpcvit.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e4add4391dfa2a806a50cc1fbe9a9696dac9501f.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 26,
        "score": 8.666666666666666
    },
    "324f97d033efd97855488cf0b15511799fe7b7f7.pdf": {
        "title": "STPM_SAHI: A Small-Target Forest Fire Detection Model Based on Swin Transformer and Slicing Aided Hyper Inference",
        "authors": [
            "Ji Lin",
            "Haifeng Lin",
            "Fang Wang"
        ],
        "published_date": "2022",
        "abstract": "Forest fires seriously destroy the world\u2019s forest resources and endanger biodiversity. The traditional forest fire target detection models based on convolutional neural networks (CNNs) lack the ability to deal with the relationship between visual elements and objects. They also have low detection accuracy for small-target forest fires. Therefore, this paper proposes an improved small-target forest fire detection model, STPM_SAHI. We use the latest technology in the field of computer vision, the Swin Transformer backbone network, to extract the features of forest fires. Its self-attention mechanism can capture the global information of forest fires to obtain larger receptive fields and contextual information. We integrated the Swin Transformer backbone network into the Mask R-CNN detection framework, and PAFPN was used to replace the original FPN as the feature fusion network, which can reduce the propagation path of the main feature layer and eliminate the impact of down-sampling fusion. After the improved model was trained, the average precision (AP0.5) of forest fire target detection at different scales reached 89.4. Then, Slicing Aided Hyper Inference technology was integrated into the improved forest fire detection model, which solved the problem that small-target forest fires pixels only account for a small proportion and lack sufficient details, which are difficult to be detected by the traditional target detection models. The detection accuracy of small-target forest fires was significantly improved. The average precision (AP0.5) increased by 8.1. Through an ablation experiment, we have proved the effectiveness of each module of the improved forest fire detection model. Furthermore, the forest fire detection accuracy is significantly better than that of the mainstream models. Our model can also detect forest fire targets with very small pixels. Our model is very suitable for small-target forest fire detection. The detection accuracy of forest fire targets at different scales is also very high and meets the needs of real-time forest fire detection.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/324f97d033efd97855488cf0b15511799fe7b7f7.pdf",
        "venue": "Forests",
        "citationCount": 25,
        "score": 8.333333333333332
    },
    "bf52f09d648d78677f140e63c7c83ca44fcd438f.pdf": {
        "title": "Transformer-based ensemble method for multiple predominant instruments recognition in polyphonic music",
        "authors": [
            "L. Reghunath",
            "R. Rajan"
        ],
        "published_date": "2022",
        "abstract": "Multiple predominant instrument recognition in polyphonic music is addressed using decision level fusion of three transformer-based architectures on an ensemble of visual representations. The ensemble consists of Mel-spectrogram, modgdgram, and tempogram. Predominant instrument recognition refers to the problem where the prominent instrument is identified from a mixture of instruments being played together. We experimented with two transformer architectures like Vision transformer (Vi-T) and Shifted window transformer (Swin-T) for the proposed task. The performance of the proposed system is compared with that of the state-of-the-art Han\u2019s model, convolutional neural networks (CNN), and deep neural networks (DNN). Transformer networks learn the distinctive local characteristics from the visual representations and classify the instrument to the group where it belongs. The proposed system is systematically evaluated using the IRMAS dataset with eleven classes. A wave generative adversarial network (WaveGAN) architecture is also employed to generate audio files for data augmentation. We train our networks from fixed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from the variable-length test audio file without any sliding window analysis and aggregation strategy as in existing algorithms. The ensemble voting scheme using Swin-T reports a micro and macro F1 score of 0.66 and 0.62, respectively. These metrics are 3.12% and 12.72% relatively higher than those obtained by the state-of-the-art Han\u2019s model. The architectural choice of transformers with ensemble voting on Mel-spectro-/modgd-/tempogram has merit in recognizing the predominant instruments in polyphonic music.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/bf52f09d648d78677f140e63c7c83ca44fcd438f.pdf",
        "venue": "EURASIP Journal on Audio, Speech, and Music Processing",
        "citationCount": 25,
        "score": 8.333333333333332
    },
    "4eba4dccc7d19c40dfd02eef447a57613c3644e2.pdf": {
        "title": "Vision Transformer based Deep Learning Model for Monkeypox Detection",
        "authors": [
            "Dipanjali Kundu",
            "Umme Raihan Siddiqi",
            "Md. Mahbubur Rahman"
        ],
        "published_date": "2022",
        "abstract": "Images of skin lesions may be used to detect this virus, which is a reliable method for identifying the pox virus group. However, early identification and prediction are difficult due to the virus\u2019s resemblance to other pox viruses. An intelligent computer-aided detection system may be a great alternative to relying on labor-intensive human identification. Therefore, in this research an machine learning and deep learning classification method for monkeypox prediction has been proposed and trained, and tested over 1300 skin lesion images. A comparative analysis of machine learning algorithms (K-NN and SVM) and Deep learning algorithms (Vision Transformer, RestNet50) to establish the efficacy of this study. Layered Convolutional Neural Network (CNN) with transfer learning and pretrained models such as RestNet50 integrated, together with customized hyperparameters for extracting the features from the input images. The feed-forward, which is also completely integrated, helped the algorithm divide the visuals into two categories\u2013chickenpox and monkeypox. Among the ML model, the K-NN achieves the best accuracy of 84%. However, The Vision Transformer(ViT) outperforms the other models with an accuracy of 93%. In Addition to it, we analyze our pretrained model to achieve the desired outcome based on the relevant existing model as already established to the end user.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/4eba4dccc7d19c40dfd02eef447a57613c3644e2.pdf",
        "venue": "2022 25th International Conference on Computer and Information Technology (ICCIT)",
        "citationCount": 25,
        "score": 8.333333333333332
    },
    "0bdcea2f18e158bbd1723ed27d48e5211545e050.pdf": {
        "title": "Hierarchical Decoding Network Based on Swin Transformer for Detecting Salient Objects in RGB-T Images",
        "authors": [
            "Fan Sun",
            "Wujie Zhou",
            "Lv Ye",
            "Lu Yu"
        ],
        "published_date": "2022",
        "abstract": "Although conventional deep convolutional neural networks are effective for contextual semantic segmentation of objects, recent vision transformers can capture global information of an image and are better at capturing semantic associations over longer ranges. In addition, some existing saliency detection methods disregard the guidance of high-level semantic information for low-level features during decoding, and only use layer-by-layer transmission for encoding. Therefore, we propose a hierarchical decoding network based on a swin transformer to perform red\u2013green\u2013blue and thermal (RGB-T) salient object detection (SOD). First, a sine\u2013cosine fusion module performs multimodality intersections and exploits complementarity. As a second fusion stage, an advanced semantic information guidance module adjusts high-level semantic information and low-level detailed characteristics. Finally, a global saliency perception module fuses cross-layer information in a top-down path. Comprehensive experiments demonstrate that the proposed network outperforms 12 state-of-the-art methods on three RGB-T SOD datasets.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0bdcea2f18e158bbd1723ed27d48e5211545e050.pdf",
        "venue": "IEEE Signal Processing Letters",
        "citationCount": 24,
        "score": 8.0
    },
    "67b7ad5f3c818f42c8359abc87f353202f76f6f2.pdf": {
        "title": "ViT-Cap: A Novel Vision Transformer-Based Capsule Network Model for Finger Vein Recognition",
        "authors": [
            "Yupeng Li",
            "Huimin Lu",
            "Yifan Wang",
            "Ruoran Gao",
            "Chengcheng Zhao"
        ],
        "published_date": "2022",
        "abstract": "Finger vein recognition has been widely studied due to its advantages, such as high security, convenience, and living body recognition. At present, the performance of the most advanced finger vein recognition methods largely depends on the quality of finger vein images. However, when collecting finger vein images, due to the possible deviation of finger position, ambient lighting and other factors, the quality of the captured images is often relatively low, which directly affects the performance of finger vein recognition. In this study, we proposed a new model for finger vein recognition that combined the vision transformer architecture with the capsule network (ViT-Cap). The model can explore finger vein image information based on global and local attention and selectively focus on the important finger vein feature information. First, we split-finger vein images into patches and then linearly embedded each of the patches. Second, the resulting vector sequence was fed into a transformer encoder to extract the finger vein features. Third, the feature vectors generated by the vision transformer module were fed into the capsule module for further training. We tested the proposed method on four publicly available finger vein databases. Experimental results showed that the average recognition accuracy of the algorithm based on the proposed model was above 96%, which was better than the original vision transformer, capsule network, and other advanced finger vein recognition algorithms. Moreover, the equal error rate (EER) of our model achieved state-of-the-art performance, especially reaching less than 0.3% under the test of FV-USM datasets which proved the effectiveness and reliability of the proposed model in finger vein recognition.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/67b7ad5f3c818f42c8359abc87f353202f76f6f2.pdf",
        "venue": "Applied Sciences",
        "citationCount": 23,
        "score": 7.666666666666666
    },
    "3994996a202f0127a58f57b259324a5283a1ba27.pdf": {
        "title": "Predicting microsatellite instability and key biomarkers in colorectal cancer from H&E\u2010stained images: achieving state\u2010of\u2010the\u2010art predictive performance with fewer data using Swin Transformer",
        "authors": [
            "Bangwei Guo",
            "Xingyu Li",
            "Miao Yang",
            "J. Jonnagaddala",
            "Hong Zhang",
            "Xuesong Xu"
        ],
        "published_date": "2022",
        "abstract": "Many artificial intelligence models have been developed to predict clinically relevant biomarkers for colorectal cancer (CRC), including microsatellite instability (MSI). However, existing deep learning networks require large training datasets, which are often hard to obtain. In this study, based on the latest Hierarchical Vision Transformer using Shifted Windows (Swin Transformer [Swin\u2010T]), we developed an efficient workflow to predict biomarkers in CRC (MSI, hypermutation, chromosomal instability, CpG island methylator phenotype, and BRAF and TP53 mutation) that required relatively small datasets. Our Swin\u2010T workflow substantially achieved the state\u2010of\u2010the\u2010art (SOTA) predictive performance in an intra\u2010study cross\u2010validation experiment on the Cancer Genome Atlas colon and rectal cancer dataset (TCGA\u2010CRC\u2010DX). It also demonstrated excellent generalizability in cross\u2010study external validation and delivered a SOTA area under the receiver operating characteristic curve (AUROC) of 0.90 for MSI, using the Molecular and Cellular Oncology dataset for training (N = 1,065) and the TCGA\u2010CRC\u2010DX (N = 462) for testing. A similar performance (AUROC = 0.91) was reported in a recent study, using ~8,000 training samples (ResNet18) on the same testing dataset. Swin\u2010T was extremely efficient when using small training datasets and exhibited robust predictive performance with 200\u2013500 training samples. Our findings indicate that Swin\u2010T could be 5\u201310 times more efficient than existing algorithms for MSI prediction based on ResNet18 and ShuffleNet. Furthermore, the Swin\u2010T models demonstrated their capability in accurately predicting MSI and BRAF mutation status, which could exclude and therefore reduce samples before subsequent standard testing in a cascading diagnostic workflow, in turn reducing turnaround time and costs.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3994996a202f0127a58f57b259324a5283a1ba27.pdf",
        "venue": "The Journal of Pathology: Clinical Research",
        "citationCount": 23,
        "score": 7.666666666666666
    },
    "4767600089dd71c7ed74c6a0acaf9e5a6fb345c9.pdf": {
        "title": "Novel Recursive BiFPN Combining with Swin Transformer for Wildland Fire Smoke Detection",
        "authors": [
            "Ao Li",
            "Yaqin Zhao",
            "Zhaoxiang Zheng"
        ],
        "published_date": "2022",
        "abstract": "The technologies and models based on machine vision are widely used for early wildfire detection. Due to the broadness of wild scene and the occlusion of the vegetation, smoke is more easily detected than flame. However, the shapes of the smoke blown by the wind change constantly and the smoke colors from different combustors vary greatly. Therefore, the existing target detection networks have limitations in detecting wildland fire smoke, such as low detection accuracy and high false alarm rate. This paper designs the attention model Recursive Bidirectional Feature Pyramid Network (RBiFPN for short) for the fusion and enhancement of smoke features. We introduce RBiFPN into the backbone network of YOLOV5 frame to better distinguish the subtle difference between clouds and smoke. In addition, we replace the classification head of YOLOV5 with Swin Transformer, which helps to change the receptive fields of the network with the size of smoke regions and enhance the capability of modeling local features and global features. We tested the proposed model on the dataset containing a large number of interference objects such as clouds and fog. The experimental results show that our model can detect wildfire smoke with a higher performance than the state-of-the-art methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/4767600089dd71c7ed74c6a0acaf9e5a6fb345c9.pdf",
        "venue": "Forests",
        "citationCount": 23,
        "score": 7.666666666666666
    },
    "d5b8af7ae526e3a511b1172c26d7caf7421145bb.pdf": {
        "title": "MXT: A New Variant of Pyramid Vision Transformer for Multi-label Chest X-ray Image Classification",
        "authors": [
            "Xiaoben Jiang",
            "Yu Zhu",
            "Gan Cai",
            "Bingbing Zheng",
            "Dawei Yang"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d5b8af7ae526e3a511b1172c26d7caf7421145bb.pdf",
        "venue": "Cognitive Computation",
        "citationCount": 23,
        "score": 7.666666666666666
    },
    "4d491b6fbe529a3986ef50cc34ede7c9ad88126c.pdf": {
        "title": "FQ-ViT: Fully Quantized Vision Transformer without Retraining",
        "authors": [
            "Yang Lin",
            "Tianyu Zhang",
            "Peiqin Sun",
            "Zheng Li",
            "Shuchang Zhou"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/4d491b6fbe529a3986ef50cc34ede7c9ad88126c.pdf",
        "venue": "arXiv.org",
        "citationCount": 30,
        "score": 7.5
    },
    "d9e4af7739d4d5dcd44619bf276dc497f7334e34.pdf": {
        "title": "MSTRIQ: No Reference Image Quality Assessment Based on Swin Transformer with Multi-Stage Fusion",
        "authors": [
            "Jing Wang",
            "Haotian Fa",
            "X. Hou",
            "Yitian Xu",
            "Tao Li",
            "X. Lu",
            "Lean Fu"
        ],
        "published_date": "2022",
        "abstract": "Measuring the perceptual quality of images automatically is an essential task in the area of computer vision, as degradations on image quality can exist in many processes from image acquisition, transmission to enhancing. Many Image Quality Assessment(IQA) algorithms have been designed to tackle this problem. However, it still remains unsettled due to the various types of image distortions and the lack of large-scale human-rated datasets. In this paper, we propose a novel algorithm based on the Swin Transformer [31] with fused features from multiple stages, which aggregates information from both local and global features to better predict the quality. To address the issues of small-scale datasets, relative rankings of images have been taken into account together with regression loss to simultaneously optimize the model. Furthermore, effective data augmentation strategies are also used to improve the performance. In comparisons with previous works, experiments are carried out on two standard IQA datasets and a challenge dataset. The results demonstrate the effectiveness of our work. The proposed method outperforms other methods on standard datasets and ranks 2nd in the no-reference track of NTIRE 2022 Perceptual Image Quality Assessment Challenge [53]. It verifies that our method is promising in solving diverse IQA problems and thus can be used to real-word applications.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d9e4af7739d4d5dcd44619bf276dc497f7334e34.pdf",
        "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
        "citationCount": 22,
        "score": 7.333333333333333
    },
    "d69102eec0fff1084e3d1e24a411103280020a32.pdf": {
        "title": "Plant disease and insect pest identification based on vision transformer",
        "authors": [
            "Han Li",
            "Sufang Li",
            "Jiguo Yu",
            "Yubing Han",
            "Anming Dong"
        ],
        "published_date": "2022",
        "abstract": "With the rapid development of precision agriculture and smart agriculture, the need to build an automatic identification and detection system for diseases and insect pests is increasing. Using computers to correctly label plant diseases and insect pests is an important prerequisite for achieving accurate classification of plant diseases and insect pests and ensuring system performance. In order to improve the accuracy of computer classification of plant pests and diseases, this paper proposes an automatic pest identification method based on the Vision Transformer (ViT). In order to avoid training overfitting, the plant diseases and insect pests data sets are enhanced by methods such as Histogram Equalization, Laplacian, Gamma Transformation, CLAHE, Retinex-SSR, and Retinex-MSR. Then use the enhanced data set to train the constructed ViT neural network, so as to realize the automatic classification of plant diseases and insect pests. The simulation results show that the constructed ViT network has a test recognition accuracy rate of 96.71% on the plant disease and insect pest public data set Plant_Village, which is about 1.00% higher than the Plant disease and pest identification method based on traditional convolutional neural networks such as GoogleNet and EfficentNetV2.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d69102eec0fff1084e3d1e24a411103280020a32.pdf",
        "venue": "Other Conferences",
        "citationCount": 22,
        "score": 7.333333333333333
    },
    "38bef6f0b4de2f9809b4fd44e8b61f9bebb14db9.pdf": {
        "title": "Multi-Task Distributed Learning Using Vision Transformer With Random Patch Permutation",
        "authors": [
            "Sangjoon Park",
            "Jong-Chul Ye"
        ],
        "published_date": "2022",
        "abstract": "The widespread application of artificial intelligence in health research is currently hampered by limitations in data availability. Distributed learning methods such as federated learning (FL) and split learning (SL) are introduced to solve this problem as well as data management and ownership issues with their different strengths and weaknesses. The recent proposal of federated split task-agnostic (F eSTA) learning tries to reconcile the distinct merits of FL and SL by enabling the multi-task collaboration between participants through Vision Transformer (ViT) architecture, but they suffer from higher communication overhead. To address this, here we present a multi-task distributed learning using ViT with random patch permutation, dubbed <inline-formula> <tex-math notation=\"LaTeX\">${p}$ </tex-math></inline-formula>-F eSTA. Instead of using a CNN-based head as in F eSTA, <inline-formula> <tex-math notation=\"LaTeX\">${p}$ </tex-math></inline-formula>-F eSTA adopts a simple patch embedder with random permutation, improving the multi-task learning performance without sacrificing privacy. Experimental results confirm that the proposed method significantly enhances the benefit of multi-task collaboration, communication efficiency, and privacy preservation, shedding light on practical multi-task distributed learning in the field of medical imaging.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/38bef6f0b4de2f9809b4fd44e8b61f9bebb14db9.pdf",
        "venue": "IEEE Transactions on Medical Imaging",
        "citationCount": 22,
        "score": 7.333333333333333
    },
    "b80d0ce78652ffbf6b3ddebfc567f8594a6ffd4b.pdf": {
        "title": "Explainable Survival Analysis with Convolution-Involved Vision Transformer",
        "authors": [
            "Yifan Shen",
            "Li Liu",
            "Zhihao Tang",
            "Zongyi Chen",
            "Guixiang Ma",
            "Jiyan Dong",
            "Xi Zhang",
            "Lin Yang",
            "Q. Zheng"
        ],
        "published_date": "2022",
        "abstract": "Image-based survival prediction models can facilitate doctors in diagnosing and treating cancer patients. With the advance of digital pathology technologies, the big whole slide images (WSIs) provide increasing resolution and more details for diagnosis. However, the gigabyte-size WSIs would make most models computationally infeasible. To this end, instead of using the complete WSIs, most of existing models only use a pre-selected subset of key patches or patch clusters as input, which might fail to completely capture the patient's tumor morphology. In this work, we aim to develop a novel survival analysis model to fully utilize the complete WSI information. We show that the use of a Vision Transformer (ViT) backbone, together with convolution operations involved in it, is an effective framework to improve the prediction performance. Additionally, we present a post-hoc explainable method to identify the most salient patches and distinct morphology features, making the model more faithful and the results easier to comprehend by human users. Evaluations on two large cancer datasets show that our proposed model is more effective and has better interpretability for survival prediction.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b80d0ce78652ffbf6b3ddebfc567f8594a6ffd4b.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 22,
        "score": 7.333333333333333
    },
    "7d4d512445903e8ad75f0dcfa8baec27b1fe8620.pdf": {
        "title": "A Hyperspectral Image Classification Method Based on Adaptive Spectral Spatial Kernel Combined with Improved Vision Transformer",
        "authors": [
            "Aili Wang",
            "Shuang Xing",
            "Yan Zhao",
            "Haibin Wu",
            "Y. Iwahori"
        ],
        "published_date": "2022",
        "abstract": "In recent years, methods based on deep convolutional neural networks (CNNs) have dominated the classification task of hyperspectral images. Although CNN-based HSI classification methods have the advantages of spatial feature extraction, HSI images are characterized by approximately continuous spectral information, usually containing hundreds of spectral bands. CNN cannot mine and represent the sequence properties of spectral features well, and the transformer model of attention mechanism proves its advantages in processing sequence data. This study proposes a new spectral spatial kernel combined with the improved Vision Transformer (ViT) to jointly extract spatial spectral features to complete classification task. First, the hyperspectral data are dimensionally reduced by PCA; then, the shallow features are extracted with an spectral spatial kernel, and the extracted features are input into the improved ViT model. The improved ViT introduces a re-attention mechanism and a local mechanism based on the original ViT. The re-attention mechanism can increase the diversity of attention maps at different levels. The local mechanism is introduced into ViT to make full use of the local and global information of the data to improve the classification accuracy. Finally, a multi-layer perceptron is used to obtain the classification result. Among them, the Focal Loss function is used to increase the loss weight of small-class samples and difficult-to-classify samples in HSI data samples and reduce the loss weight of easy-to-classify samples, so that the network can learn more useful hyperspectral image information. In addition, using the Apollo optimizer to train the HSI classification model to better update and compute network parameters that affect model training and model output, thereby minimizing the loss function. We evaluated the classification performance of the proposed method on four different datasets, and achieved good classification results on urban land object classification, crop classification and mineral classification, respectively. Compared with the state-of-the-art backbone network, the method achieves a significant improvement and achieves very good classification accuracy.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7d4d512445903e8ad75f0dcfa8baec27b1fe8620.pdf",
        "venue": "Remote Sensing",
        "citationCount": 21,
        "score": 7.0
    },
    "7d5274f1155b85a6120491c9374b6983dac96552.pdf": {
        "title": "Evaluating Vision Transformer Methods for Deep Reinforcement Learning from Pixels",
        "authors": [
            "Tianxin Tao",
            "Daniele Reda",
            "M. V. D. Panne"
        ],
        "published_date": "2022",
        "abstract": "Vision Transformers (ViT) have recently demonstrated the significant potential of transformer architectures for computer vision. To what extent can image-based deep reinforcement learning also benefit from ViT architectures, as compared to standard convolutional neural network (CNN) architectures? To answer this question, we evaluate ViT training methods for image-based reinforcement learning (RL) control tasks and compare these results to a leading convolutional-network architecture method, RAD. For training the ViT encoder, we consider several recently-proposed self-supervised losses that are treated as auxiliary tasks, as well as a baseline with no additional loss terms. We find that the CNN architectures trained using RAD still generally provide superior performance. For the ViT methods, all three types of auxiliary tasks that we consider provide a benefit over plain ViT training. Furthermore, ViT reconstruction-based tasks are found to significantly outperform ViT contrastive-learning.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7d5274f1155b85a6120491c9374b6983dac96552.pdf",
        "venue": "arXiv.org",
        "citationCount": 21,
        "score": 7.0
    },
    "0afbaae7d0bfe98a04a3b7dd1ba69e25f32b24c6.pdf": {
        "title": "Multi-granularity Feature Extraction Based on Vision Transformer for Tomato Leaf Disease Recognition",
        "authors": [
            "Shupei Wu",
            "Youqiang Sun",
            "He Huang"
        ],
        "published_date": "2021",
        "abstract": "At present, the task of identifying crop diseases is mainly to simply distinguish the types of different crop diseases. However, the current classifiers cannot solve problems, such as accurate identification of similar disease categories. Compared with convolutional neural network (CNN), the recent vision transformer (VIT) has achieved good results on image tasks. Inspired by this, this paper proposed a multi-granularity feature extraction model based on vision transformer. By combining image block information of different scales, the model can learn image information from different granularities. At the same time, in order to further grasp the important areas, this paper developed a feature selection module. Through experimental comparison, the scheme has an accuracy improvement of nearly 2% compared with other classification models, and the model parameters have not improved much.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0afbaae7d0bfe98a04a3b7dd1ba69e25f32b24c6.pdf",
        "venue": "2021 3rd International Academic Exchange Conference on Science and Technology Innovation (IAECST)",
        "citationCount": 27,
        "score": 6.75
    },
    "6bcdf04ccd35ae971e765cdda25127ce005b7713.pdf": {
        "title": "Feature Pyramid Vision Transformer for MedMNIST Classification Decathlon",
        "authors": [
            "Jinwei Liu",
            "Yan Li",
            "Guitao Cao",
            "Yong Liu",
            "W. Cao"
        ],
        "published_date": "2022",
        "abstract": "MedMNIST is a medical dataset proposed to block the need for medical knowledge, but there is currently no model that can generalize well on all its sub-datasets. Owing to the inadequacy of long-range relation modeling, models based on convolutional neural networks (CNNs) cannot fully learn the information of images. Besides, relying only on high-level features limits the generalization effect as well. All of these remain challenges for MedMNIST Classification Decathlon. In this paper, we proposed Feature Pyramid Vision Transformer (FPViT), a strong alternative for MedMNIST Classification Decathlon. Our FPViT exhibits enhanced feature learning and modeling capabilities, which merits both residual network (ResNet) and Vision Transformer (ViT). Transformers in our model take the features extracted by ResNet as sequences to capture global contexts which compensate for the lack of locality of convolution operations. Moreover, the feature pyramid designed in our model effectively utilizes the multi-scale feature maps from basic layers of ResNet. These multi-scale features from low-level to high level enable our model to have better adaptability. And, the final prediction is based on the multi-scale ViT and the original ResNet heads. Through experiments, our FPViT can achieve superior classification and generalization on MedMNIST than state-of-the-art methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6bcdf04ccd35ae971e765cdda25127ce005b7713.pdf",
        "venue": "IEEE International Joint Conference on Neural Network",
        "citationCount": 20,
        "score": 6.666666666666666
    },
    "6fd1002f321240c75dfc86c6bad6c05068a8160a.pdf": {
        "title": "STMG: Swin transformer for multi-label image recognition with graph convolution network",
        "authors": [
            "Yangtao Wang",
            "Yanzhao Xie",
            "Lisheng Fan",
            "Guangxin Hu"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6fd1002f321240c75dfc86c6bad6c05068a8160a.pdf",
        "venue": "Neural computing & applications (Print)",
        "citationCount": 19,
        "score": 6.333333333333333
    },
    "c4560ab3855cb8ae4b9922458a0b6b94108e2c28.pdf": {
        "title": "Swin-Pose: Swin Transformer Based Human Pose Estimation",
        "authors": [
            "Zinan Xiong",
            "Chenxi Wang",
            "Ying Li",
            "Yan Luo",
            "Yu Cao"
        ],
        "published_date": "2022",
        "abstract": "Convolutional neural networks (CNNs) have been widely utilized in many computer vision tasks. However, CNNs have a fixed reception field and lack the ability of long-range perception, which is crucial to human pose estimation. Transformer architecture has been adopted to computer vision applications recently and is proven to be a highly effective architecture. We are interested in exploring its capability in human pose estimation, and thus propose a novel model based on transformer, enhanced with a feature pyramid fusion structure. More specifically, we use pre-trained Swin Transformer to extract features, and leverage a feature pyramid structure to extract and fuse feature maps from different stages. The experiment results of our study have demonstrated that the proposed transformer-based model can achieve better performance compared to the state-of-the-art CNN-based models.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c4560ab3855cb8ae4b9922458a0b6b94108e2c28.pdf",
        "venue": "Conference on Multimedia Information Processing and Retrieval",
        "citationCount": 19,
        "score": 6.333333333333333
    },
    "a0762777d6e46acbcd5b301285c2d9894065ff8f.pdf": {
        "title": "Efficient Lung Cancer Image Classification and Segmentation Algorithm Based on Improved Swin Transformer",
        "authors": [
            "Ruinan Sun",
            "Yu Pang"
        ],
        "published_date": "2022",
        "abstract": "With the development of computer technology, various models have emerged in artificial intelligence. The transformer model has been applied to the field of computer vision (CV) after its success in natural language processing (NLP). Radiologists continue to face multiple challenges in today's rapidly evolving medical field, such as increased workload and increased diagnostic demands. Although there are some conventional methods for lung cancer detection before, their accuracy still needs to be improved, especially in realistic diagnostic scenarios. This paper creatively proposes a segmentation method based on efficient transformer and applies it to medical image analysis. The algorithm completes the task of lung cancer classification and segmentation by analyzing lung cancer data, and aims to provide efficient technical support for medical staff. In addition, we evaluated and compared the results in various aspects. For the classification mission, the max accuracy of Swin-T by regular training and Swin-B in two resolutions by pre-training can be up to 82.3%. For the segmentation mission, we use pre-training to help the model improve the accuracy of our experiments. The accuracy of the three models reaches over 95%. The experiments demonstrate that the algorithm can be well applied to lung cancer classification and segmentation missions.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a0762777d6e46acbcd5b301285c2d9894065ff8f.pdf",
        "venue": "arXiv.org",
        "citationCount": 19,
        "score": 6.333333333333333
    },
    "d43950779dc86b728d7e002be6195526d35a26b0.pdf": {
        "title": "Privacy-Preserving Image Classification Using Vision Transformer",
        "authors": [
            "Zheng Qi",
            "AprilPyone Maungmaung",
            "Yuma Kinoshita",
            "H. Kiya"
        ],
        "published_date": "2022",
        "abstract": "In this paper, we propose a privacy-preserving image classification method that is based on the combined use of encrypted images and the vision transformer (ViT). The proposed method allows us not only to apply images without visual information to ViT models for both training and testing but to also maintain a high classification accuracy. ViT utilizes patch embedding and position embedding for image patches, so this architecture is shown to reduce the influence of block-wise image transformation. In an experiment, the proposed method for privacy-preserving image classification is demonstrated to outperform state-of-the-art methods in terms of classification accuracy and robustness against various attacks.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d43950779dc86b728d7e002be6195526d35a26b0.pdf",
        "venue": "European Signal Processing Conference",
        "citationCount": 19,
        "score": 6.333333333333333
    },
    "2e69d97ae22c6a3685cc548f8c19c696d5d7d363.pdf": {
        "title": "RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning",
        "authors": [
            "Xiaojian Ma",
            "Weili Nie",
            "Zhiding Yu",
            "Huaizu Jiang",
            "Chaowei Xiao",
            "Yuke Zhu",
            "Song-Chun Zhu",
            "Anima Anandkumar"
        ],
        "published_date": "2022",
        "abstract": "Reasoning about visual relationships is central to how humans interpret the visual world. This task remains challenging for current deep learning algorithms since it requires addressing three key technical problems jointly: 1) identifying object entities and their properties, 2) inferring semantic relations between pairs of entities, and 3) generalizing to novel object-relation combinations, i.e., systematic generalization. In this work, we use vision transformers (ViTs) as our base model for visual reasoning and make better use of concepts defined as object entities and their relations to improve the reasoning ability of ViTs. Specifically, we introduce a novel concept-feature dictionary to allow flexible image feature retrieval at training time with concept keys. This dictionary enables two new concept-guided auxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a local task for facilitating semantic object-centric correspondence learning. To examine the systematic generalization of visual reasoning models, we introduce systematic splits for the standard HICO and GQA benchmarks. We show the resulting model, Concept-guided Vision Transformer (or RelViT for short) significantly outperforms prior approaches on HICO and GQA by 16% and 13% in the original split, and by 43% and 18% in the systematic split. Our ablation analyses also reveal our model's compatibility with multiple ViT variants and robustness to hyper-parameters.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2e69d97ae22c6a3685cc548f8c19c696d5d7d363.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 19,
        "score": 6.333333333333333
    },
    "c25091718b22384cebece2da7f30fc1702a07c76.pdf": {
        "title": "Adversarial Vision Transformer for Medical Image Semantic Segmentation with Limited Annotations",
        "authors": [
            "Ziyang Wang",
            "Will Zhao",
            "Zixuan Ni",
            "Yuchen Zheng"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c25091718b22384cebece2da7f30fc1702a07c76.pdf",
        "venue": "British Machine Vision Conference",
        "citationCount": 19,
        "score": 6.333333333333333
    },
    "cec0cbc2dd6d7975714110632b6bfcb5c1927ec3.pdf": {
        "title": "Attention Distillation: self-supervised vision transformer students need more guidance",
        "authors": [
            "Kai Wang",
            "Fei Yang",
            "Joost van de Weijer"
        ],
        "published_date": "2022",
        "abstract": "Self-supervised learning has been widely applied to train high-quality vision transformers. Unleashing their excellent performance on memory and compute constraint devices is therefore an important research topic. However, how to distill knowledge from one self-supervised ViT to another has not yet been explored. Moreover, the existing self-supervised knowledge distillation (SSKD) methods focus on ConvNet based architectures are suboptimal for ViT knowledge distillation. In this paper, we study knowledge distillation of self-supervised vision transformers (ViT-SSKD). We show that directly distilling information from the crucial attention mechanism from teacher to student can significantly narrow the performance gap between both. In experiments on ImageNet-Subset and ImageNet-1K, we show that our method AttnDistill outperforms existing self-supervised knowledge distillation (SSKD) methods and achieves state-of-the-art k-NN accuracy compared with self-supervised learning (SSL) methods learning from scratch (with the ViT-S model). We are also the first to apply the tiny ViT-T model on self-supervised learning. Moreover, AttnDistill is independent of self-supervised learning algorithms, it can be adapted to ViT based SSL methods to improve the performance in future research. The code is here: https://github.com/wangkai930418/attndistill",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/cec0cbc2dd6d7975714110632b6bfcb5c1927ec3.pdf",
        "venue": "British Machine Vision Conference",
        "citationCount": 19,
        "score": 6.333333333333333
    },
    "ef93c81f90154a022e164be2f83c6cf6e602f33d.pdf": {
        "title": "Improving Classification of Remotely Sensed Images with the Swin Transformer",
        "authors": [
            "Fatema-E- Jannat",
            "A. Willis"
        ],
        "published_date": "2022",
        "abstract": "With the recent developments of transformer-based architecture in the image classification domain, the initial Vision Transformer (ViT) model has shown promising results compared to traditional CNN models. Inspired by this, this article reports on the efficacy of transformer-based models on remote sensing images for land cover classification. Our approach applies a variation of the vision transformer named the Swin (Shifted Window) Transformer model for analysis. This is a hierarchical transformer model that computes the representation with shifted windows. Results include an extensive study on the performance of this transformer for three different remote sensing datasets: EuroSat, NWPU-RESISC45, and AID. Findings indicate that the Swin architecture outperforms current state-of-the-art approaches for accurately classifying remote sensing images. Comparative analyses provide insights on the specific margin of improvement and an understanding of the prospect these transformer architectures have for improving image classification tasks of this type.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ef93c81f90154a022e164be2f83c6cf6e602f33d.pdf",
        "venue": "SoutheastCon",
        "citationCount": 18,
        "score": 6.0
    },
    "6a8d4a97ba3eafcb85d2d2a57d7d514bb592dbd0.pdf": {
        "title": "Mask-guided Vision Transformer (MG-ViT) for Few-Shot Learning",
        "authors": [
            "Yuzhong Chen",
            "Zhe Xiao",
            "Lin Zhao",
            "Lu Zhang",
            "Haixing Dai",
            "David Liu",
            "Zihao Wu",
            "Changhe Li",
            "Tuo Zhang",
            "Changying Li",
            "Dajiang Zhu",
            "Tianming Liu",
            "Xi Jiang"
        ],
        "published_date": "2022",
        "abstract": "Learning with little data is challenging but often inevitable in various application scenarios where the labeled data is limited and costly. Recently, few-shot learning (FSL) gained increasing attention because of its generalizability of prior knowledge to new tasks that contain only a few samples. However, for data-intensive models such as vision transformer (ViT), current fine-tuning based FSL approaches are inefficient in knowledge generalization and thus degenerate the downstream task performances. In this paper, we propose a novel mask-guided vision transformer (MG-ViT) to achieve an effective and efficient FSL on ViT model. The key idea is to apply a mask on image patches to screen out the task-irrelevant ones and to guide the ViT to focus on task-relevant and discriminative patches during FSL. Particularly, MG-ViT only introduces an additional mask operation and a residual connection, enabling the inheritance of parameters from pre-trained ViT without any other cost. To optimally select representative few-shot samples, we also include an active learning based sample selection method to further improve the generalizability of MG-ViT based FSL. We evaluate the proposed MG-ViT on both Agri-ImageNet classification task and ACFR apple detection task with gradient-weighted class activation mapping (Grad-CAM) as the mask. The experimental results show that the MG-ViT model significantly improves the performance when compared with general fine-tuning based ViT models, providing novel insights and a concrete approach towards generalizing data-intensive and large-scale deep learning models for FSL.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6a8d4a97ba3eafcb85d2d2a57d7d514bb592dbd0.pdf",
        "venue": "arXiv.org",
        "citationCount": 18,
        "score": 6.0
    },
    "64143b37ae41085c4907e344ff3a2362a3051d0c.pdf": {
        "title": "ViT-P: Classification of Genitourinary Syndrome of Menopause From OCT Images Based on Vision Transformer Models",
        "authors": [
            "Haoran Wang",
            "Yanju Ji",
            "Kaiwen Song",
            "Mingyang Sun",
            "Peitong Lv",
            "Tianyu Zhang"
        ],
        "published_date": "2021",
        "abstract": "Genitourinary syndrome of menopause (GSM) is a disease caused by a physiological decline in estrogen levels, and it can negatively affect a woman\u2019s overall health and quality of life in terms of sexual function. Real-time optical biopsy images can now be obtained with optical coherence tomography (OCT) systems. In this study, we introduce vision transformer (ViT) to the field of medical OCT images for the first time and propose a deep learning-based approach for GSM lesion screening. Specifically, we first build a GSM dataset to train and evaluate the experimental model performance. The study aims to propose a method that combines null convolution with a deep convolutional adversarial generative network classifier to generate the samples needed for training to alleviate the hindrance of such problems, in response to certain practical problems, such as category imbalance that occur during data collection. Next, the experiments present ViT PLUS (ViT-P) for the vaginal OCT image classification task used, which effectively improves the shortcomings of ViT in extracting Patch Embedding using a multibranch convolutional neural network combined with a channel attention mechanism. The clinical images acquired by the OCT device are then automatically classified on the basis of the OCT device to reduce the medical workload of gynecologists. Experimental results show that the ViT-P model outperforms the CNN model and ViT for case screening in the GSM and UCSD datasets, and the accuracy can reach 99.9% and 99.69%, respectively.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/64143b37ae41085c4907e344ff3a2362a3051d0c.pdf",
        "venue": "IEEE Transactions on Instrumentation and Measurement",
        "citationCount": 23,
        "score": 5.75
    },
    "dcd8617200724f0aa998276be339ff4af589ee42.pdf": {
        "title": "Audio-Visual Transformer Based Crowd Counting",
        "authors": [
            "Usman Sajid",
            "Xiangyu Chen",
            "Hasan Sajid",
            "Taejoon Kim",
            "Guanghui Wang"
        ],
        "published_date": "2021",
        "abstract": "Crowd estimation is a very challenging problem. The most recent study tries to exploit auditory information to aid the visual models, however, the performance is limited due to the lack of an effective approach for feature extraction and integration. The paper proposes a new audiovisual multi-task network to address the critical challenges in crowd counting by effectively utilizing both visual and audio inputs for better modalities association and productive feature extraction. The proposed network introduces the notion of auxiliary and explicit image patch-importance ranking (PIR) and patch-wise crowd estimate (PCE) information to produce a third (run-time) modality. These modalities (audio, visual, run-time) undergo a transformer-inspired cross-modality co-attention mechanism to finally output the crowd estimate. To acquire rich visual features, we propose a multi-branch structure with transformer-style fusion in-between. Extensive experimental evaluations show that the proposed scheme outperforms the state-of-the-art networks under all evaluation settings with up to 33.8% improvement. We also analyze and compare the vision-only variant of our network and empirically demonstrate its superiority over previous approaches.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/dcd8617200724f0aa998276be339ff4af589ee42.pdf",
        "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",
        "citationCount": 23,
        "score": 5.75
    },
    "46880aeca86695ca3117cc04f6bd9edaf088111b.pdf": {
        "title": "Residual Swin Transformer Channel Attention Network for Image Demosaicing",
        "authors": [
            "W. Xing",
            "K. Egiazarian"
        ],
        "published_date": "2022",
        "abstract": "Image demosaicing is problem of interpolating full-resolution color images from raw sensor (color filter array) data. During last decade, deep neural networks have been widely used in image restoration, and in particular, in demosaicing, attaining significant performance improvement. In recent years, vision transformers have been designed and successfully used in various computer vision applications. One of the recent methods of image restoration based on a Swin Transformer (ST), SwinIR, demonstrates state-of-the-art performance with a smaller number of parameters than neural network-based methods. Inspired by the success of SwinIR, we propose in this paper a novel Swin Transformer-based network for image demosaicing, called RSTCANet. To extract image features, RSTCANet stacks several residual Swin Transformer Channel Attention blocks (RSTCAB), introducing the channel attention for each two successive ST blocks. Extensive experiments demonstrate that RSTCANet outperforms state-of-the-art image demosaicing methods, and has a smaller number of parameters. The source code is available at https://github.com/xingwz/RSTCANet.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/46880aeca86695ca3117cc04f6bd9edaf088111b.pdf",
        "venue": "European Workshop on Visual Information Processing",
        "citationCount": 17,
        "score": 5.666666666666666
    },
    "7e0dd543471b66374fbf1639b9894d3d502533b6.pdf": {
        "title": "Vision transformer assisting rheumatologists in screening for capillaroscopy changes in systemic sclerosis: an artificial intelligence model",
        "authors": [
            "A. Garaiman",
            "F. Nooralahzadeh",
            "C. Mihai",
            "Nicolas Andres Perez Gonzalez",
            "N. Gkikopoulos",
            "M. Becker",
            "O. Distler",
            "M. Krauthammer",
            "B. Maurer"
        ],
        "published_date": "2022",
        "abstract": "Abstract Objectives The first objective of this study was to implement and assess the performance and reliability of a vision transformer (ViT)-based deep-learning model, an \u2018off-the-shelf\u2019 artificial intelligence solution, for identifying distinct signs of microangiopathy in nailfold capilloroscopy (NFC) images of patients with SSc. The second objective was to compare the ViT\u2019s analysis performance with that of practising rheumatologists. Methods NFC images of patients prospectively enrolled in our European Scleroderma Trials and Research group (EUSTAR) and Very Early Diagnosis of Systemic Sclerosis (VEDOSS) local registries were used. The primary outcome investigated was the ViT\u2019s classification performance for identifying disease-associated changes (enlarged capillaries, giant capillaries, capillary loss, microhaemorrhages) and the presence of the scleroderma pattern in these images using a cross-fold validation setting. The secondary outcome involved a comparison of the ViT\u2019s performance vs that of rheumatologists on a reliability set, consisting of a subset of 464 NFC images with majority vote\u2013derived ground-truth labels. Results We analysed 17\u2009126 NFC images derived from 234 EUSTAR and 55 VEDOSS patients. The ViT had good performance in identifying the various microangiopathic changes in capillaries by NFC [area under the curve (AUC) from 81.8% to 84.5%]. In the reliability set, the rheumatologists reached a higher average accuracy, as well as a better trade-off between sensitivity and specificity compared with the ViT. However, the annotators\u2019 performance was variable, and one out of four rheumatologists showed equal or lower classification measures compared with the ViT. Conclusions The ViT is a modern, well-performing and readily available tool for assessing patterns of microangiopathy on NFC images, and it may assist rheumatologists in generating consistent and high-quality NFC reports; however, the final diagnosis of a scleroderma pattern in any individual case needs the judgement of an experienced observer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7e0dd543471b66374fbf1639b9894d3d502533b6.pdf",
        "venue": "Rheumatology",
        "citationCount": 17,
        "score": 5.666666666666666
    },
    "845a154dbcde81de52b68d73c78fad5be4af3b20.pdf": {
        "title": "Computationally-Efficient Vision Transformer for Medical Image Semantic Segmentation Via Dual Pseudo-Label Supervision",
        "authors": [
            "Ziyang Wang",
            "Nanqing Dong",
            "I. Voiculescu"
        ],
        "published_date": "2022",
        "abstract": "Ubiquitous accumulation of large volumes of data, and increased availability of annotated medical data in particular, has made it possible to show the many and varied benefits of deep learning to the semantic segmentation of medical images. Nevertheless, data access and annotation come at a high cost in clinician time. The power of Vision Transformer (ViT) is well-documented for generic computer vision tasks involving millions of images of every day objects, of which only relatively few have been annotated. Its translation to relatively more modest (i.e. thousands of images of) medical data is not immediately straightforward. This paper presents practical avenues for training a Computationally-Efficient Semi-Supervised Vision Transformer (CESS-ViT) for medical image segmentation task.We propose a self-attention-based image segmentation network which requires only limited computational resources. Additionally, we develop a dual pseudo-label supervision scheme for use with semi-supervision in a simple pure ViT.Our method has been evaluated on a publicly available cardiac MRI dataset with direct comparison against other semi-supervised methods. Our results illustrate the proposed ViT-based semi-supervised method outperforms the existing methods in the semantic segmentation of cardiac ventricles.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/845a154dbcde81de52b68d73c78fad5be4af3b20.pdf",
        "venue": "International Conference on Information Photonics",
        "citationCount": 17,
        "score": 5.666666666666666
    },
    "6f4093a7ad5378e8cd3b73a52fbec80b784c107d.pdf": {
        "title": "Multi-Dimensional Vision Transformer Compression via Dependency Guided Gaussian Process Search",
        "authors": [
            "Zejiang Hou",
            "S. Kung"
        ],
        "published_date": "2022",
        "abstract": "Vision transformers (ViT) have recently attracted considerable attentions, but the huge computational cost remains an issue for practical deployment. Previous ViT pruning methods tend to prune the model along one dimension solely, which may suffer from excessive reduction and lead to sub-optimal model quality. In contrast, we advocate a multi-dimensional ViT compression paradigm, and propose to harness the redundancy reduction from attention head, neuron and sequence dimensions jointly. Firstly, we propose a statistical dependence based pruning criterion that is generalizable to different dimensions for identifying the deleterious components. Moreover, we cast the multidimensional ViT compression as an optimization problem, objective of which is to learn an optimal pruning policy across the three dimensions while maximizing the compressed model\u2019s accuracy under a computational budget. The problem is solved by an adapted Gaussian process search with expected improvement. Experimental results show that our method effectively reduces the computational cost of various ViT models. For example, our method reduces 40% FLOPs without top-1 accuracy loss for DeiT and T2T-ViT models on the ImageNet dataset, outperforming previous state-of-the-art ViT pruning methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6f4093a7ad5378e8cd3b73a52fbec80b784c107d.pdf",
        "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
        "citationCount": 17,
        "score": 5.666666666666666
    },
    "50405c1ee26c97ed5b9a54c7179317a424e6e471.pdf": {
        "title": "SWIN transformer based contrastive self-supervised learning for animal detection and classification",
        "authors": [
            "L. Agilandeeswari",
            "S. D. Meena"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/50405c1ee26c97ed5b9a54c7179317a424e6e471.pdf",
        "venue": "Multimedia tools and applications",
        "citationCount": 16,
        "score": 5.333333333333333
    },
    "1bf9a97a7581d1671d683c19ce36fed6511dc95b.pdf": {
        "title": "HTD-VIT: Spectral-Spatial Joint Hyperspectral Target Detection with Vision Transformer",
        "authors": [
            "Haonan Qin",
            "Weiying Xie",
            "Yunsong Li",
            "Qian Du"
        ],
        "published_date": "2022",
        "abstract": "In hyperspectral images (HSIs), spatial context provides complementary information to abundant spectral features. In this paper, a united spectral-spatial framework named HTD-ViT based on vision transformer (ViT) is proposed for HTD tasks. The HTD-ViT leverages the ViT to learn discriminative spectral-spatial features of each pixel and its neighboring pixels. Meanwhile, the spectral-spatial sequence construction operation uses spectrums in the cross region centered on the selected pixel to produce the corresponding spectral-spatial sequence for ViT processing. Furthermore, the spectral-spatial sample selection procedure based on coarse detection addresses the issue of lacking well-labeled training instances in the HTD tasks. Finally, the spectral-spatial pixel-level detection combines the discriminative feature from the spectral and the spatial domains to suppress the background. In contrast to traditional spatial-spectral feature extraction methods that stack the original spectral feature with spatial neighborhood information directly, joint spectral-spatial inference in HTD-ViT can effectively discover the underlying contextual and structure information in HSIs. Experiments on real HSIs verify the effectiveness of HTD-ViT, which takes full advantage of both the variable spectral and spatial features.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1bf9a97a7581d1671d683c19ce36fed6511dc95b.pdf",
        "venue": "IEEE International Geoscience and Remote Sensing Symposium",
        "citationCount": 16,
        "score": 5.333333333333333
    },
    "e58c2bc5333ee8f37150d9d15c0428d9658b2e50.pdf": {
        "title": "Automatic Mushroom Species Classification Model for Foodborne Disease Prevention Based on Vision Transformer",
        "authors": [
            "Boyuan Wang"
        ],
        "published_date": "2022",
        "abstract": "Mushrooms are the fleshy, spore-bearing structure of certain fungi, produced by a group of mycelia and buried in a substratum. Mushrooms are classified as edible, medicinal, and poisonous. However, many poisoning incidents occur yearly by consuming wild mushrooms. Thousands of poisoning incidents are reported each year globally, and 80% of these are from unidentified species of mushrooms. Mushroom poisoning is one of the most serious food safety issues worldwide. Motivated by this problem, this study uses an open-source mushroom dataset and employs several data augmentation approaches to decrease the probability of model overfitting. We propose a novel deep learning pipeline (ViT-Mushroom) for mushroom classification using the Vision Transformer large network (ViT-L/32). We compared the performance of our method against that of a convolutional neural network (CNN). We visualized the high-dimensional outputs of the ViT-L/32 model to achieve the interpretability of ViT-L/32 using the t-distributed stochastic neighbor embedding (t-SNE) method. The results show that ViT-L/32 is the best on the testing dataset, with an accuracy score of 95.97%. These results surpass previous approaches in reducing intraclass variability and generating well-separated feature embeddings. The proposed method is a promising deep learning model capable of automatically classifying mushroom species, helping wild mushroom consumers avoid eating toxic mushrooms, safeguarding food safety, and preventing public health incidents of food poisoning. The results will offer valuable resources for food scientists, nutritionists, and the public health sector regarding the safety and quality of mushrooms.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e58c2bc5333ee8f37150d9d15c0428d9658b2e50.pdf",
        "venue": "Journal of Food Quality",
        "citationCount": 16,
        "score": 5.333333333333333
    },
    "761f55a486e99ab5d3550aee48df34b6b65643c2.pdf": {
        "title": "Vision Transformer-Based Retina Vessel Segmentation with Deep Adaptive Gamma Correction",
        "authors": [
            "Hyunwoo Yu",
            "J. Shim",
            "Jaeho Kwak",
            "J. Song",
            "Suk-Ju Kang"
        ],
        "published_date": "2022",
        "abstract": "Accurate segmentation of the retina vessel is essential for the early diagnosis of eye-related diseases. Recently, convolutional neural networks have shown remarkable performance in retina vessel segmentation. However, the complexity of edge structural information and the changeable intensity distribution depending on retina images reduce the performance of the segmentation tasks. This paper proposes two novel deep learning-based modules, channel attention vision transformer (CAViT) and deep adaptive gamma correction (DAGC), to tackle these issues. The CAViT jointly applies the efficient channel attention (ECA) and the vision transformer (ViT), in which the channel attention module considers the interdependency among feature channels and the ViT discriminates meaningful edge structures by considering the global context. The DAGC module provides the optimal gamma correction value for each input image by jointly training a CNN model with the segmentation network so that all the retina images are mapped to a unified intensity distribution. The experimental results show that our proposed method achieves superior performance compared to conventional methods on widely used datasets, DRIVE and CHASE DB1.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/761f55a486e99ab5d3550aee48df34b6b65643c2.pdf",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "citationCount": 16,
        "score": 5.333333333333333
    },
    "2dcc1f5f1ba11f99e09f22c53d02aa66cec35c48.pdf": {
        "title": "Vision Transformer Based Models for Plant Disease Detection and Diagnosis",
        "authors": [
            "Rayene Amina Boukabouya",
            "A. Moussaoui",
            "Mohamed Berrimi"
        ],
        "published_date": "2022",
        "abstract": "Plant health is one of the most interesting aspects in the natural cycle, it needs to be conserved to keep the life of the organisms. Several plant diseases could be observed at early stages in the leaf level, where immediate interventions should be taken to prevent the progression of the disease. The use of deep learning has dramatically increased recently, owing to its remarkable performance in multiple applications in different research areas. In this study, we focus on the detection of tomato diseases at the leaf stage using recent deep learning architectures. Several deep learning models are put in comparative experiments to achieve a stable and robust classification performance with high precision that outperforms previous SOTA results. Vision Transformers (ViT) models reported the top classification re-sults, with an accuracy of 96.7%, 98.52%, 99.1% and 99.7%. The research funding will help in the early automatic detection of diseases in the leaf plants, thus providing necessary treatments and maintaining the natural cycle.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2dcc1f5f1ba11f99e09f22c53d02aa66cec35c48.pdf",
        "venue": "International Symposium on Information and Automation",
        "citationCount": 16,
        "score": 5.333333333333333
    },
    "52a7f15085f1b6815a4de2da26df51bb63470596.pdf": {
        "title": "Convolution-Embedded Vision Transformer With Elastic Positional Encoding for Pansharpening",
        "authors": [
            "Nan Wang",
            "Xiangjun Meng",
            "Xiangchao Meng",
            "Feng Shao"
        ],
        "published_date": "2022",
        "abstract": "Transformer, especially vision transformer (ViT), is attracting increasing attention in various computer vision (CV) tasks. However, two urgent problems exist for the ViT: 1) owing to its attending to an image in the patch level, the ViT seems to have a better performance in fetching global representations but is limited in extracting local features, which is an inherent advantage for the convolutional neural network (CNN) and 2) the learnable positional encoding plays a positive role, but limits the cross-resolution ability of the network. Specifically, the pre-trained model could only generate images of the same size during training. To conquer the two problems, we propose a novel convolution-embedded ViT with elastic positional encoding in this article. On one hand, we propose a joint CNN and self-attention (CSA) network to collaboratively extract local and global features. On the other hand, we propose to integrate the elastic CNN-based positional encoder into the framework to solve the rigid limitation of the ViT in cross resolution issues and improve the performance. Extensive experiments were conducted on IKONOS and WorldView-2 with 4- and 8-band multispectral (MS) images, respectively. The visual and numerical results show the competitive performance of the proposed method.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/52a7f15085f1b6815a4de2da26df51bb63470596.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 16,
        "score": 5.333333333333333
    },
    "649b706ba282de4eb5a161137f80eb49ed84a0a8.pdf": {
        "title": "UFO-ViT: High Performance Linear Vision Transformer without Softmax",
        "authors": [
            "Jeonggeun Song"
        ],
        "published_date": "2021",
        "abstract": "Vision transformers have become one of the most important models for computer vision tasks. Although they outperform prior works, they require heavy computational resources on a scale that is quadratic to $N$. This is a major drawback of the traditional self-attention (SA) algorithm. Here, we propose the Unit Force Operated Vision Transformer (UFO-ViT), a novel SA mechanism that has linear complexity. The main approach of this work is to eliminate nonlinearity from the original SA. We factorize the matrix multiplication of the SA mechanism without complicated linear approximation. By modifying only a few lines of code from the original SA, the proposed models outperform most transformer-based models on image classification and dense prediction tasks on most capacity regimes.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/649b706ba282de4eb5a161137f80eb49ed84a0a8.pdf",
        "venue": "arXiv.org",
        "citationCount": 21,
        "score": 5.25
    },
    "186295f7c79e46c0e4e5f40e094267c09714043d.pdf": {
        "title": "So-ViT: Mind Visual Tokens for Vision Transformer",
        "authors": [
            "Jiangtao Xie",
            "Rui Zeng",
            "Qilong Wang",
            "Ziqi Zhou",
            "Peihua Li"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/186295f7c79e46c0e4e5f40e094267c09714043d.pdf",
        "venue": "arXiv.org",
        "citationCount": 20,
        "score": 5.0
    },
    "d77288fc7de7b15c200ed75118de702caf841ec3.pdf": {
        "title": "DP-ViT: A Dual-Path Vision Transformer for Real-Time Sonar Target Detection",
        "authors": [
            "Yu-shan Sun",
            "Hao Zheng",
            "Guo-cheng Zhang",
            "Jingfei Ren",
            "Hao Xu",
            "Chaofei Xu"
        ],
        "published_date": "2022",
        "abstract": "Sonar image is the main way for underwater vehicles to obtain environmental information. The task of target detection in sonar images can distinguish multi-class targets in real time and accurately locate them, providing perception information for the decision-making system of underwater vehicles. However, there are many challenges in sonar image target detection, such as many kinds of sonar, complex and serious noise interference in images, and less datasets. This paper proposes a sonar image target detection method based on Dual Path Vision Transformer Network (DP-VIT) to accurately detect targets in forward-look sonar and side-scan sonar. DP-ViT increases receptive field by adding multi-scale to patch embedding enhances learning ability of model feature extraction by using Dual Path Transformer Block, then introduces Conv-Attention to reduce model training parameters, and finally uses Generalized Focal Loss to solve the problem of imbalance between positive and negative samples. The experimental results show that the performance of this sonar target detection method is superior to other mainstream methods on both forward-look sonar dataset and side-scan sonar dataset, and it can also maintain good performance in the case of adding noise.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d77288fc7de7b15c200ed75118de702caf841ec3.pdf",
        "venue": "Remote Sensing",
        "citationCount": 15,
        "score": 5.0
    },
    "2e4dbc3dbd400346be60318ae558a0293e65ba81.pdf": {
        "title": "TP-VIT: A Two-Pathway Vision Transformer for Video Action Recognition",
        "authors": [
            "Yanhao Jing",
            "Feng Wang"
        ],
        "published_date": "2022",
        "abstract": "Recently, inspired by the success of Transformer in natural language processing tasks, a number of works have attempted to apply Transformer-based models to video action recognition. Existing works only use one RGB stream as the input for Transformer. How to use multiple pathways and multiple streams with Transformer for action recognition has not been studied. To address this issue, we present a novel structure namely Two-Pathway Vision Transformer (TP-ViT). Two parallel spatial Transformer encoders are used as two pathways with different framerates and resolutions of the input video. The high-resolution pathway contains more spatial information, while the high-framerate pathway contains more temporal information. The two outputs are fused and fed into a temporal Transformer encoder for action recognition. Furthermore, we also fuse skeleton features into our model to get better results. Our experiments demonstrate that our proposed models achieve the state-of-the-art results on both the coarse-grained dataset Kinetics and the fine-grained dataset FineGym.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2e4dbc3dbd400346be60318ae558a0293e65ba81.pdf",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "citationCount": 14,
        "score": 4.666666666666666
    },
    "3de95f33c2b4f61a9c0f335b4810a966e209a47a.pdf": {
        "title": "Swinv2-Imagen: Hierarchical Vision Transformer Diffusion Models for Text-to-Image Generation",
        "authors": [
            "Rui Li",
            "Weihua Li",
            "Yi Yang",
            "Hanyu Wei",
            "Jianhua Jiang",
            "Quan Bai"
        ],
        "published_date": "2022",
        "abstract": "Recently, diffusion models have been proven to perform remarkably well in text-to-image synthesis tasks in a number of studies, immediately presenting new study opportunities for image generation. Google\u2019s Imagen follows this research trend and outperforms DALLE2 as the best model for text-to-image generation. However, Imagen merely uses a T5 language model for text processing, which cannot ensure learning the semantic information of the text. Furthermore, the Efficient UNet leveraged by Imagen is not the best choice in image processing. To address these issues, we propose the Swinv2-Imagen, a novel text-to-image diffusion model based on a Hierarchical Visual Transformer and a Scene Graph incorporating a semantic layout. In the proposed model, the feature vectors of entities and relationships are extracted and involved in the diffusion model, effectively improving the quality of generated images. On top of that, we also introduce a Swin-Transformer-based UNet architecture, called Swinv2-Unet, which can address the problems stemming from the CNN convolution operations. Extensive experiments are conducted to evaluate the performance of the proposed model by using three real-world datasets, i.e. MSCOCO, CUB and MM-CelebA-HQ. The experimental results show that the proposed Swinv2-Imagen model outperforms several popular state-of-the-art methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3de95f33c2b4f61a9c0f335b4810a966e209a47a.pdf",
        "venue": "Neural computing & applications (Print)",
        "citationCount": 14,
        "score": 4.666666666666666
    },
    "7817ecb816da8676ae21b401d60c99e706446f06.pdf": {
        "title": "An Extendable, Efficient and Effective Transformer-based Object Detector",
        "authors": [
            "Hwanjun Song",
            "Deqing Sun",
            "Sanghyuk Chun",
            "Varun Jampani",
            "Dongyoon Han",
            "Byeongho Heo",
            "Wonjae Kim",
            "Ming-Hsuan Yang"
        ],
        "published_date": "2022",
        "abstract": "Transformers have been widely used in numerous vision problems especially for visual recognition and detection. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to construct an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. In addition, we extend it to ViDT+ to support joint-task learning for object detection and instance segmentation. Specifically, we attach an efficient multi-scale feature fusion layer and utilize two more auxiliary training losses, IoU-aware loss and token labeling loss. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and its extended ViDT+ achieves 53.2AP owing to its high scalability for large models. The source code and trained models are available at https://github.com/naver-ai/vidt.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7817ecb816da8676ae21b401d60c99e706446f06.pdf",
        "venue": "arXiv.org",
        "citationCount": 14,
        "score": 4.666666666666666
    },
    "3502b661362b278eebacf1037fc3bb4e21963869.pdf": {
        "title": "ECG-ViT: A Transformer-Based ECG Classifier for Energy-Constraint Wearable Devices",
        "authors": [
            "Neha Shukla",
            "Anand Pandey",
            "A. P. Shukla",
            "Sanjeev Chandra Neupane"
        ],
        "published_date": "2022",
        "abstract": "The advancement in deep learning techniques has helped researchers acquire and process multimodal data signals from different healthcare domains. Now, the focus has shifted towards providing end-to-end solutions, i.e., processing these data and developing models that can be directly implemented on edge devices. To achieve this, the researchers try to solve two problems: (I) reduce the complex feature dependencies and (II) reduce the complexity of the deep learning model without compromising accuracy. In this paper, we focus on the later part of reducing the complexity of the model by using the knowledge distillation framework. We have introduced knowledge distillation on the Vision Transformer model to study the MIT-BIH Arrhythmia Database. A tenfold crossvalidation technique was used to validate the model, and we obtained a 99.7% F1 score and 99.3% accuracy. The model was further tested on the Xilinx Alveo U50 FPGA accelerator, and it is found fit for any low-powered wearable device implementation.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3502b661362b278eebacf1037fc3bb4e21963869.pdf",
        "venue": "J. Sensors",
        "citationCount": 14,
        "score": 4.666666666666666
    },
    "791d1e306eaa2e87657925ec4f45661baa8da58b.pdf": {
        "title": "Improving Local Features with Relevant Spatial Information by Vision Transformer for Crowd Counting",
        "authors": [
            "Nguyen H. Tran",
            "Ta Duc Huy",
            "S. T. Duong",
            "Nguyen Phan",
            "Dao Huu Hung",
            "Chanh D. Tr. Nguyen",
            "Trung Bui",
            "S. Q. Truong"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/791d1e306eaa2e87657925ec4f45661baa8da58b.pdf",
        "venue": "British Machine Vision Conference",
        "citationCount": 14,
        "score": 4.666666666666666
    },
    "1d8ecf2c3a78e5dc4de9e4c41961771bbd0033b3.pdf": {
        "title": "Training Object Detectors from Scratch: An Empirical Study in the Era of Vision Transformer",
        "authors": [
            "Weixiang Hong",
            "Jiangwei Lao",
            "Wang Ren",
            "Jian Wang",
            "Jingdong Chen"
        ],
        "published_date": "2022",
        "abstract": "Modeling in computer vision has long been dominated by convolutional neural networks (CNNs). Recently, in light of the excellent performances of self-attention mech-anism in the language field, transformers tailored for visual data have drawn numerous attention and triumphed CNNs in various vision tasks. These vision transformers heavily rely on large-scale pre-training to achieve competitive accuracy, which not only hinders the freedom of architectural design in downstream tasks like object detection, but also causes learning bias and domain mismatch in the fine-tuning stages. To this end, we aim to get rid of the \u201cpre-train & fine-tune\u201d paradigm of vision transformer and train transformer based object detector from scratch. Some earlier work in the CNNs era have successfully trained CNNs based detectors without pre-training, unfortunately, their findings do not generalize well when the backbone is switched from CNNs to vision transformer. Instead of proposing a specific vision transformer based detector, in this work, our goal is to reveal the insights of training vision transformer based detectors from scratch. In particular, we expect those insights can help other re-searchers and practitioners, and inspire more interesting research in other fields, such as semantic segmentation, visual-linguistic pre-training, etc. One of the key findings is that both architectural changes and more epochs play critical roles in training vision transformer based detectors from scratch. Experiments on MS COCO datasets demonstrate that vision transformer based detectors trained from scratch can also achieve similar performances to their counterparts with ImageNet pre-training.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1d8ecf2c3a78e5dc4de9e4c41961771bbd0033b3.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 14,
        "score": 4.666666666666666
    },
    "e1f7478294fe01ce271cdef9ba93f4c675d92dc9.pdf": {
        "title": "Object Detection of Road Assets Using Transformer-Based YOLOX with Feature Pyramid Decoder on Thai Highway Panorama",
        "authors": [
            "Teerapong Panboonyuen",
            "Sittinun Thongbai",
            "W. Wongweeranimit",
            "P. Santitamnont",
            "Kittiwan Suphan",
            "Chaiyut Charoenphon"
        ],
        "published_date": "2021",
        "abstract": "Due to the various sizes of each object, such as kilometer stones, detection is still a challenge, and it directly impacts the accuracy of these object counts. Transformers have demonstrated impressive results in various natural language processing (NLP) and image processing tasks due to long-range modeling dependencies. This paper aims to propose an exceeding you only look once (YOLO) series with two contributions: (i) We propose to employ a pre-training objective to gain the original visual tokens based on the image patches on road asset images. By utilizing pre-training Vision Transformer (ViT) as a backbone, we immediately fine-tune the model weights on downstream tasks by joining task layers upon the pre-trained encoder. (ii) We apply Feature Pyramid Network (FPN) decoder designs to our deep learning network to learn the importance of different input features instead of simply summing up or concatenating, which may cause feature mismatch and performance degradation. Conclusively, our proposed method (Transformer-Based YOLOX with FPN) learns very general representations of objects. It significantly outperforms other state-of-the-art (SOTA) detectors, including YOLOv5S, YOLOv5M, and YOLOv5L. We boosted it to 61.5% AP on the Thailand highway corpus, surpassing the current best practice (YOLOv5L) by 2.56% AP for the test-dev data set.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e1f7478294fe01ce271cdef9ba93f4c675d92dc9.pdf",
        "venue": "Inf.",
        "citationCount": 18,
        "score": 4.5
    },
    "a56f8e42e9efe5290602116b42a247b758052fe4.pdf": {
        "title": "Video captioning based on vision transformer and reinforcement learning",
        "authors": [
            "Hong Zhao",
            "Zhiwen Chen",
            "Lan Guo",
            "Zeyu Han"
        ],
        "published_date": "2022",
        "abstract": "Global encoding of visual features in video captioning is important for improving the description accuracy. In this paper, we propose a video captioning method that combines Vision Transformer (ViT) and reinforcement learning. Firstly, Resnet-152 and ResNeXt-101 are used to extract features from videos. Secondly, the encoding block of the ViT network is applied to encode video features. Thirdly, the encoded features are fed into a Long Short-Term Memory (LSTM) network to generate a video content description. Finally, the accuracy of video content description is further improved by fine-tuning reinforcement learning. We conducted experiments on the benchmark dataset MSR-VTT used for video captioning. The results show that compared with the current mainstream methods, the model in this paper has improved by 2.9%, 1.4%, 0.9% and 4.8% under the four evaluation indicators of LEU-4, METEOR, ROUGE-L and CIDEr-D, respectively.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a56f8e42e9efe5290602116b42a247b758052fe4.pdf",
        "venue": "PeerJ Computer Science",
        "citationCount": 13,
        "score": 4.333333333333333
    },
    "6eb10790689a4cf239d1ee2a3919dced4be6db8f.pdf": {
        "title": "Multi-Level Transformer-Based Social Relation Recognition",
        "authors": [
            "Yuchen Wang",
            "L. Qing",
            "Zhengyong Wang",
            "Yongqiang Cheng",
            "Yonghong Peng"
        ],
        "published_date": "2022",
        "abstract": "Social relationships refer to the connections that exist between people and indicate how people interact in society. The effective recognition of social relationships is conducive to further understanding human behavioral patterns and thus can be vital for more complex social intelligent systems, such as interactive robots and health self-management systems. The existing works about social relation recognition (SRR) focus on extracting features on different scales but lack a comprehensive mechanism to orchestrate various features which show different degrees of importance. In this paper, we propose a new SRR framework, namely Multi-level Transformer-Based Social Relation Recognition (MT-SRR), for better orchestrating features on different scales. Specifically, a vision transformer (ViT) is firstly employed as a feature extraction module for its advantage in exploiting global features. An intra-relation transformer (Intra-TRM) is then introduced to dynamically fuse the extracted features to generate more rational social relation representations. Next, an inter-relation transformer (Inter-TRM) is adopted to further enhance the social relation representations by attentionally utilizing the logical constraints among relationships. In addition, a new margin related to inter-class similarity and a sample number are added to alleviate the challenges of a data imbalance. Extensive experiments demonstrate that MT-SRR can better fuse features on different scales as well as ameliorate the bad effect caused by a data imbalance. The results on the benchmark datasets show that our proposed model outperforms the state-of-the-art methods with significant improvement.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6eb10790689a4cf239d1ee2a3919dced4be6db8f.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 13,
        "score": 4.333333333333333
    },
    "fc2e7fcdc1bd773f1eb097ae67c8f736108276e3.pdf": {
        "title": "NomMer: Nominate Synergistic Context in Vision Transformer for Visual Recognition",
        "authors": [
            "Hao Liu",
            "Xinghua Jiang",
            "Xin Li",
            "Zhimin Bao",
            "Deqiang Jiang",
            "Bo Ren"
        ],
        "published_date": "2021",
        "abstract": "Recently, Vision Transformers (ViT), with the self-attention (SA) as the de facto ingredients, have demon-strated great potential in the computer vision community. For the sake of trade-off between efficiency and performance, a group of works merely perform SA operation within local patches, whereas the global contextual information is abandoned, which would be indispensable for visual recognition tasks. To solve the issue, the subsequent global-local ViTs take a stab at marrying local SA with global one in parallel or alternative way in the model. Nevertheless, the exhaustively combined local and global context may exist redundancy for various visual data, and the receptive field within each layer is fixed. Alternatively, a more graceful way is that global and local context can adaptively contribute per se to accommodate different visual data. To achieve this goal, we in this paper propose a novel ViT architecture, termed NomMer, which can dynamically Nominate the synergistic global-local context in vision transforMer. By investigating the working pattern of NomMer, we further explore what context information is focused. Beneficial from this \u201cdynamic nomination\u201d mechanism, without bells and whistles, the NomMer can not only achieve 84.5% Top-1 classification accuracy on ImageNet with only 73M parameters, but also show promising performance on dense prediction tasks, i.e., object detection and semantic segmentation. The code and models are publicly available at https://github.com/TencentYoutuResearch/VisualRecognition-NomMer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/fc2e7fcdc1bd773f1eb097ae67c8f736108276e3.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 17,
        "score": 4.25
    },
    "16ad38c73f4897e1c58326e3562b74ae1c2fd219.pdf": {
        "title": "Histopathological image classification based on self-supervised vision transformer and weak labels",
        "authors": [
            "A. Gul",
            "Ozdemir Cetin",
            "Christoph Reich",
            "Nadine Flinner",
            "Tim Prangemeier",
            "H. Koeppl"
        ],
        "published_date": "2022",
        "abstract": "Whole Slide Image (WSI) analysis is a powerful method to facilitate the diagnosis of cancer in tissue samples. Automating this diagnosis poses various issues, most notably caused by the immense image resolution and limited annotations. WSI\u2019s commonly exhibit resolutions of 100, 000 \u00d7 100, 000 pixels. Annotating cancerous areas in WSI\u2019s on the pixel-level is prohibitively labor-intensive and requires a high level of expert knowledge. Multiple instance learning (MIL) alleviates the need for expensive pixel-level annotations. In MIL, learning is performed on slide-level labels, in which a pathologist provides information about whether a slide includes cancerous tissue. Here, we propose Self-ViT-MIL, a novel approach for classifying and localizing cancerous areas based on slide-level annotations, eliminating the need for pixel-wise annotated training data. Self-ViTMIL is pre-trained in a self-supervised setting to learn rich feature representation without relying on any labels. The recent Vision Transformer (ViT) architecture builds the feature extractor of Self-ViT-MIL. For localizing cancerous regions, a MIL aggregator with global attention is utilized. To the best of our knowledge, Self-ViTMIL is the first approach to introduce self-supervised ViT\u2019s in MIL-based WSI analysis tasks. We showcase the effectiveness of our approach on the common Camelyon16 dataset. Self-ViT-MIL surpasses existing stateof-the-art MIL-based approaches in terms of accuracy and area under the curve (AUC). Code is available at https://github.com/gokberkgul/self-learning-transformer-mil",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/16ad38c73f4897e1c58326e3562b74ae1c2fd219.pdf",
        "venue": "Medical Imaging",
        "citationCount": 12,
        "score": 4.0
    },
    "371e924dd270a213ee6e8d4104a38875105668df.pdf": {
        "title": "Lightweight Vision Transformer with Cross Feature Attention",
        "authors": [
            "Youpeng Zhao",
            "Huadong Tang",
            "Yingying Jiang",
            "A. Yong",
            "Qiang Wu"
        ],
        "published_date": "2022",
        "abstract": "Recent advances in vision transformers (ViTs) have achieved great performance in visual recognition tasks. Convolutional neural networks (CNNs) exploit spatial inductive bias to learn visual representations, but these networks are spatially local. ViTs can learn global representations with their self-attention mechanism, but they are usually heavy-weight and unsuitable for mobile devices. In this paper, we propose cross feature attention (XFA) to bring down computation cost for transformers, and combine efficient mobile CNNs to form a novel efficient light-weight CNN-ViT hybrid model, XFormer, which can serve as a general-purpose backbone to learn both global and local representation. Experimental results show that XFormer outperforms numerous CNN and ViT-based models across different tasks and datasets. On ImageNet1K dataset, XFormer achieves top-1 accuracy of 78.5% with 5.5 million parameters, which is 2.2% and 6.3% more accurate than EfficientNet-B0 (CNN-based) and DeiT (ViT-based) for similar number of parameters. Our model also performs well when transferring to object detection and semantic segmentation tasks. On MS COCO dataset, XFormer exceeds MobileNetV2 by 10.5 AP (22.7 ->33.2 AP) in YOLOv3 framework with only 6.3M parameters and 3.8G FLOPs. On Cityscapes dataset, with only a simple all-MLP decoder, XFormer achieves mIoU of 78.5 and FPS of 15.3, surpassing state-of-the-art lightweight segmentation networks.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/371e924dd270a213ee6e8d4104a38875105668df.pdf",
        "venue": "arXiv.org",
        "citationCount": 12,
        "score": 4.0
    },
    "0025c4241ffb2cce589dc2dcd82385ff06455542.pdf": {
        "title": "Hierarchical Vision Transformer with Channel Attention for RGB-D Image Segmentation",
        "authors": [
            "Yali Yang",
            "Yuanping Xu",
            "Chaolong Zhang",
            "Zhijie Xu",
            "Jian Huang"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0025c4241ffb2cce589dc2dcd82385ff06455542.pdf",
        "venue": "International Symposium on Signal Processing Systems",
        "citationCount": 12,
        "score": 4.0
    },
    "f2b3ece7cb0c88701ae0055236f6d6da39c8156d.pdf": {
        "title": "Decoding the User\u2019s Movements Preparation From EEG Signals Using Vision Transformer Architecture",
        "authors": [
            "M. S. Al-Quraishi",
            "I. Elamvazuthi",
            "T. Tang",
            "Muhammad Al-Qurishi",
            "Syed Hasan Adil",
            "Mansoor Ebrahim",
            "A. Borboni"
        ],
        "published_date": "2022",
        "abstract": "Electroencephalography (EEG) signals have a major impact on how well assistive rehabilitation devices work. These signals have become a common technique in recent studies to investigate human motion functions and behaviors. However, incorporating EEG signals to investigate motor planning or movement intention could benefit all patients who can plan motion but are unable to execute it. In this paper, the movement planning of the lower limb was investigated using EEG signal and bilateral movements were employed, including dorsiflexion and plantar flexion of the right and left ankle joint movements. The proposed system uses Continuous Wavelet Transform (CWT) to generate a time\u2013frequency (TF) map of each EEG signal in the motor cortex and then uses the extracted images as input to a deep learning model for classification. Deep Learning (DL) models are created based on vision transformer architecture (ViT) which is the state-of-the-art of image classification and also the proposed models were compared with residual neural network (ResNet). The proposed technique reveals a significant classification performance for the multiclass problem (<inline-formula> <tex-math notation=\"LaTeX\">$p < 0.0001$ </tex-math></inline-formula>) where the classification accuracy was <inline-formula> <tex-math notation=\"LaTeX\">$97.33~\\pm ~1.86$ </tex-math></inline-formula> % and the F score, recall and precision were <inline-formula> <tex-math notation=\"LaTeX\">$97.32~\\pm ~1.88$ </tex-math></inline-formula> %, <inline-formula> <tex-math notation=\"LaTeX\">$97.30~\\pm ~1.90$ </tex-math></inline-formula> % and <inline-formula> <tex-math notation=\"LaTeX\">$97.36~\\pm ~1.81$ </tex-math></inline-formula> % respectively. These results show that DL is a promising technique that can be applied to investigate the user\u2019s movements intention from EEG signals and highlight the potential of the proposed model for the development of future brain-machine interface (BMI) for neurorehabilitation purposes.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f2b3ece7cb0c88701ae0055236f6d6da39c8156d.pdf",
        "venue": "IEEE Access",
        "citationCount": 12,
        "score": 4.0
    },
    "1eb4bbaa204890a45da2ca713f34fcc2d763c6e1.pdf": {
        "title": "CvT-ASSD: Convolutional vision-Transformer Based Attentive Single Shot MultiBox Detector",
        "authors": [
            "Weiqiang Jin",
            "Hang Yu",
            "Xiangfeng Luo"
        ],
        "published_date": "2021",
        "abstract": "Due to the success of Bidirectional Encoder Representations from Transformers (BERT) in natural language process (NLP), the multi-head attention transformer has been more and more prevalent in computer-vision researches (CV). However, it still remains a challenge for researchers to put forward complex tasks such as vision detection and semantic segmentation. Although multiple Transformer-Based architectures like DETR and ViT-FRCNN have been proposed to complete object detection task, they inevitably decreases discrimination accuracy and brings down computational efficiency caused by the enormous learning parameters and heavy computational complexity incurred by the traditional self-attention operation. In order to alleviate these issues, we present a novel object detection architecture, named Convolutional vision Transformer-Based Attentive Single Shot MultiBox Detector (CvT-ASSD), that built on the top of Convolutional vision Transormer (CvT) with the efficient Attentive Single Shot MultiBox Detector (ASSD). We provide comprehensive empirical evidence showing that our model CvT-ASSD can leads to good system efficiency and performance while being pretrained on large-scale detection datasets such as PASCAL VOC and MS COCO. Code has been released on public github repository at https://github.com/albert-jin/CvT-ASSD.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1eb4bbaa204890a45da2ca713f34fcc2d763c6e1.pdf",
        "venue": "IEEE International Conference on Tools with Artificial Intelligence",
        "citationCount": 15,
        "score": 3.75
    },
    "1b18750ea0d26575f3e2c18f61e444c2790dbe4c.pdf": {
        "title": "Attribute De-biased Vision Transformer (AD-ViT) for Long-Term Person Re-identification",
        "authors": [
            "K. Lee",
            "Bhavin Jawade",
            "D. Mohan",
            "Srirangaraj Setlur",
            "V. Govindaraju"
        ],
        "published_date": "2022",
        "abstract": "Person re-identification (re-ID) aims to retrieve images of the same identity from a gallery of person images across cameras and viewpoints. However, most works in person re-ID assume a short-term setting characterized by invariance in appearance. In contrast, a high visual variance can be frequently seen in a long-term setting due to changes in apparel and accessories, which makes the task more challenging. Therefore, learning identity-specific features agnostic of temporally variant features is crucial for robust long-term person Re-ID. To this end, we propose an Attribute De-biased Vision Transformer (AD-ViT) to provide direct supervision to learn identity-specific features. Specifically, we produce attribute labels for person instances and utilize them to guide our model to focus on identity features through gradient reversal. Our experiments on two long-term re-ID datasets - LTCC and NKUP show that the proposed work consistently outperforms current state-of-the-art methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1b18750ea0d26575f3e2c18f61e444c2790dbe4c.pdf",
        "venue": "Advanced Video and Signal Based Surveillance",
        "citationCount": 11,
        "score": 3.6666666666666665
    },
    "08502153c9255399f8ff155e5f75900f121bd2ff.pdf": {
        "title": "Non-Intrusive Load Monitoring Based on Swin-Transformer with Adaptive Scaling Recurrence Plot",
        "authors": [
            "Yongtao Shi",
            "Xiaodong Zhao",
            "Fan Zhang",
            "Yaguang Kong"
        ],
        "published_date": "2022",
        "abstract": "Non-Intrusive Load Monitoring (NILM) is an effective energy consumption analysis technology, which just requires voltage and current signals on the user bus. This non-invasive monitoring approach can clarify the working state of multiple loads in the building with fewer sensing devices, thus reducing the cost of energy consumption monitoring. In this paper, an NILM method combining adaptive Recurrence Plot (RP) feature extraction and deep-learning-based image recognition is proposed. Firstly, the time-series signal of current is transformed into a threshold-free RP in phase space to obtain the image features. The Euclidean norm in threshold-free RP is scaled exponentially according to the voltage and current correlation to reflect the working characteristics of different loads adaptively. Afterwards, the obtained adaptive RP features can be mapped into images using the corresponding pixel value. In the load identification stage, an advanced computer vision deep network, Hierarchical Vision Transformer using Shifted Windows (Swin-Transformer), is applied to identify the adaptive RP images. The proposed solution is extensively verified by four real, measured load signal datasets, including industrial and household power situations, covering single-phase and three-phase electrical signals. The numerical results demonstrate that the proposed NILM method based on the adaptive RP can effectively improve the accuracy of load detection.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/08502153c9255399f8ff155e5f75900f121bd2ff.pdf",
        "venue": "Energies",
        "citationCount": 11,
        "score": 3.6666666666666665
    },
    "cb85a7648d2a4b4953abfc7437a33f91dc2f3673.pdf": {
        "title": "MC-ViT: Multi-path cross-scale vision transformer for thymoma histopathology whole slide image typing",
        "authors": [
            "Huaqi Zhang",
            "Huang Chen",
            "Jin Qin",
            "Bei-ning Wang",
            "Guolin Ma",
            "Pengyu Wang",
            "Dingrong Zhong",
            "Jie Liu"
        ],
        "published_date": "2022",
        "abstract": "Objectives Accurate histological typing plays an important role in diagnosing thymoma or thymic carcinoma (TC) and predicting the corresponding prognosis. In this paper, we develop and validate a deep learning-based thymoma typing method for hematoxylin & eosin (H&E)-stained whole slide images (WSIs), which provides useful histopathology information from patients to assist doctors for better diagnosing thymoma or TC. Methods We propose a multi-path cross-scale vision transformer (MC-ViT), which first uses the cross attentive scale-aware transformer (CAST) to classify the pathological information related to thymoma, and then uses such pathological information priors to assist the WSIs transformer (WT) for thymoma typing. To make full use of the multi-scale (10\u00d7, 20\u00d7, and 40\u00d7) information inherent in a WSI, CAST not only employs parallel multi-path to capture different receptive field features from multi-scale WSI inputs, but also introduces the cross-correlation attention module (CAM) to aggregate multi-scale features to achieve cross-scale spatial information complementarity. After that, WT can effectively convert full-scale WSIs into 1D feature matrices with pathological information labels to improve the efficiency and accuracy of thymoma typing. Results We construct a large-scale thymoma histopathology WSI (THW) dataset and annotate corresponding pathological information and thymoma typing labels. The proposed MC-ViT achieves the Top-1 accuracy of 0.939 and 0.951 in pathological information classification and thymoma typing, respectively. Moreover, the quantitative and statistical experiments on the THW dataset also demonstrate that our pipeline performs favorably against the existing classical convolutional neural networks, vision transformers, and deep learning-based medical image classification methods. Conclusion This paper demonstrates that comprehensively utilizing the pathological information contained in multi-scale WSIs is feasible for thymoma typing and achieves clinically acceptable performance. Specifically, the proposed MC-ViT can well predict pathological information classes as well as thymoma types, which show the application potential to the diagnosis of thymoma and TC and may assist doctors in improving diagnosis efficiency and accuracy.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/cb85a7648d2a4b4953abfc7437a33f91dc2f3673.pdf",
        "venue": "Frontiers in Oncology",
        "citationCount": 11,
        "score": 3.6666666666666665
    },
    "90f9b2892a437ca32c4ab26c09b8dbb7de8fa034.pdf": {
        "title": "A Vision Transformer-Based Approach to Bearing Fault Classification via Vibration Signals",
        "authors": [
            "Abid Hasan Zim",
            "Aeyan Ashraf",
            "Aquib Iqbal",
            "Asad Malik",
            "M. Kuribayashi"
        ],
        "published_date": "2022",
        "abstract": "Rolling bearings are the most crucial components of rotating machinery. Identifying defective bearings in a timely manner may prevent the malfunction of an entire machinery system. The mechanical condition monitoring field has entered the big data phase as a result of the fast advancement of machine parts. When working with large amounts of data, the manual feature extraction approach has the drawback of being inefficient and inaccurate. Data-driven methods like Deep Learning have been successfully used in recent years for mechanical intelligent fault detection. Convolutional neural networks (CNNs) were mostly used in earlier research to detect and identify bearing faults. The CNN model, however, suffers from the drawback of having trouble managing fault-time information, which results in a lack of classification results. In this study, bearing defects have been classified using a state-of-the-art Vision Transformer (ViT). Bearing defects were classified using Case Western Reserve University (CWRU) bearing failure laboratory experimental data. The research took into account 13 distinct kinds of defects under 0-load situations in addition to normal bearing conditions. Using the Short Time Fourier Transform (STFT), the vibration signals were converted into 2D time-frequency images. The 2D time-frequency images are then used as input parameters for the ViT. The model achieved an overall accuracy of 98.8%.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/90f9b2892a437ca32c4ab26c09b8dbb7de8fa034.pdf",
        "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference",
        "citationCount": 11,
        "score": 3.6666666666666665
    },
    "7ead0cae4e67f390b2eb0083117ea8ab90c53b47.pdf": {
        "title": "Landslide Susceptibility Mapping by Fusing Convolutional Neural Networks and Vision Transformer",
        "authors": [
            "Shuai Bao",
            "Jiping Liu",
            "Liang Wang",
            "Milan Kone\u010dn\u00fd",
            "Xianghong Che",
            "Shenghua Xu",
            "Peng Li"
        ],
        "published_date": "2022",
        "abstract": "Landslide susceptibility mapping (LSM) is an important decision basis for regional landslide hazard risk management, territorial spatial planning and landslide decision making. The current convolutional neural network (CNN)-based landslide susceptibility mapping models do not adequately take into account the spatial nature of texture features, and vision transformer (ViT)-based LSM models have high requirements for the amount of training data. In this study, we overcome the shortcomings of CNN and ViT by fusing these two deep learning models (bottleneck transformer network (BoTNet) and convolutional vision transformer network (ConViT)), and the fused model was used to predict the probability of landslide occurrence. First, we integrated historical landslide data and landslide evaluation factors and analysed whether there was covariance in the landslide evaluation factors. Then, the testing accuracy and generalisation ability of the CNN, ViT, BoTNet and ConViT models were compared and analysed. Finally, four landslide susceptibility mapping models were used to predict the probability of landslide occurrence in Pingwu County, Sichuan Province, China. Among them, BoTNet and ConViT had the highest accuracy, both at 87.78%, an improvement of 1.11% compared to a single model, while ConViT had the highest F1-socre at 87.64%, an improvement of 1.28% compared to a single model. The results indicate that the fusion model of CNN and ViT has better LSM performance than the single model. Meanwhile, the evaluation results of this study can be used as one of the basic tools for landslide hazard risk quantification and disaster prevention in Pingwu County.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7ead0cae4e67f390b2eb0083117ea8ab90c53b47.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 11,
        "score": 3.6666666666666665
    },
    "e4864ea54710b4c8f3b2ca9ac39854bdfd0dae28.pdf": {
        "title": "FPGA-aware automatic acceleration framework for vision transformer with mixed-scheme quantization: late breaking results",
        "authors": [
            "Mengshu Sun",
            "Z. Li",
            "Alec Lu",
            "Haoyu Ma",
            "Geng Yuan",
            "Yanyue Xie",
            "Hao Tang",
            "Yanyu Li",
            "M. Leeser",
            "Zhangyang Wang",
            "Xue Lin",
            "Zhenman Fang"
        ],
        "published_date": "2022",
        "abstract": "Vision transformers (ViTs) are emerging with significantly improved accuracy in computer vision tasks. However, their complex architecture and enormous computation/storage demand impose urgent needs for new hardware accelerator design methodology. This work proposes an FPGA-aware automatic ViT acceleration framework based on the proposed mixed-scheme quantization. To the best of our knowledge, this is the first FPGA-based ViT acceleration framework exploring model quantization. Compared with state-of-the-art ViT quantization work (algorithmic approach only without hardware acceleration), our quantization achieves 0.31% to 1.25% higher Top-1 accuracy under the same bit-width. Compared with the 32-bit floating-point baseline FPGA accelerator, our accelerator achieves around 5.6\u00d7 improvement on the frame rate (i.e., 56.4 FPS vs. 10.0 FPS) with 0.83% accuracy drop for DeiT-base.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e4864ea54710b4c8f3b2ca9ac39854bdfd0dae28.pdf",
        "venue": "Design Automation Conference",
        "citationCount": 11,
        "score": 3.6666666666666665
    },
    "fe6887ee065c4f64a0c8d6054d1fc562766452de.pdf": {
        "title": "Foreign Object Debris Detection for Airport Pavement Images Based on Self-Supervised Localization and Vision Transformer",
        "authors": [
            "Travis J. E. Munyer",
            "D. Brinkman",
            "Xin Zhong",
            "Chenyu Huang",
            "Iason Konstantzos"
        ],
        "published_date": "2022",
        "abstract": "Supervised object detection methods provide subpar performance when applied to Foreign Object Debris (FOD) detection because FOD could be arbitrary objects according to the Federal Aviation Administration (FAA) specification. Current supervised object detection algorithms require datasets that contain annotated examples of every to-be-detected object. While a large and expensive dataset could be developed to include common FOD examples, it is infeasible to collect all possible FOD examples in the dataset representation because of the open-ended nature of FOD. Limitations of the dataset could cause FOD detection systems driven by those supervised algorithms to miss certain FOD, which can become dangerous to airport operations. To this end, this paper presents a self-supervised FOD localization by learning to predict the runway images, which avoids the enumeration of FOD annotation examples. The localization method utilizes the Vision Transformer (ViT) to improve localization performance. The experiments show that the method successfully detects arbitrary FOD in real-world runway situations. The paper also provides an extension to the localization result to perform classification; a feature that can be useful to downstream tasks. To train the localization, this paper also presents a simple and realistic dataset creation framework that only collects clean runway images. The training and testing data for this method are collected at a local airport using unmanned aircraft systems (UAS). Additionally, the developed dataset is provided for public use and further studies.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/fe6887ee065c4f64a0c8d6054d1fc562766452de.pdf",
        "venue": "2022 International Conference on Computational Science and Computational Intelligence (CSCI)",
        "citationCount": 10,
        "score": 3.333333333333333
    },
    "280ea33e67484c442757fe761b75d871a399905d.pdf": {
        "title": "New intelligent fault diagnosis approach of rolling bearing based on improved vibration gray texture image and vision transformer",
        "authors": [
            "Hong-wei Fan",
            "Ningge Ma",
            "Xu-hui Zhang",
            "Ceyi Xue",
            "Jia-teng Ma",
            "Yan Yang"
        ],
        "published_date": "2022",
        "abstract": "Rolling bearing is a key component of rotating machines, its working state directly affects the performance and safety of the whole equipment. Deep learning based on big data is a mainstream means of intelligent mechanical fault diagnosis. The key lies in enhancing fault feature and improving diagnosis accuracy. Different from the Convolution Neural Network (CNN) which relies on the convolution layer to extract the image features, the Vision Transformer (VIT) uses the multi-head attention mechanism to establish the relationship among the pixels in an image. In order to improve the accuracy of rolling bearing fault diagnosis, a new fault diagnosis method based on VIT is proposed. The vibration gray texture images to be input are divided into the patches according to the predetermined size and linearly mapped into input sequences, and the global image information is integrated through the self-attention mechanism to realize fault diagnosis. In order to enhance the expressiveness and generalization ability, the pooling layer is introduced into VIT. The tested results show that the fault diagnosis accuracy of VIT on the test set reaches 94.6%, and the corresponding classification indexes top-1 is 84.2% and top-5 is 95.0%. The accuracy of the new Pooling Vision Transformer (PIT) is 3.3% higher than that of the original VIT, which proves that the introduction to pooling layer can improve the image identification performance of VIT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/280ea33e67484c442757fe761b75d871a399905d.pdf",
        "venue": "Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science",
        "citationCount": 10,
        "score": 3.333333333333333
    },
    "29f86d6d1eaba6a466c231f6906b18eae4b2b484.pdf": {
        "title": "Can We Solve 3D Vision Tasks Starting from A 2D Vision Transformer?",
        "authors": [
            "Yi Wang",
            "Zhiwen Fan",
            "Tianlong Chen",
            "Hehe Fan",
            "Zhangyang Wang"
        ],
        "published_date": "2022",
        "abstract": "Vision Transformers (ViTs) have proven to be effective, in solving 2D image understanding tasks by training over large-scale image datasets; and meanwhile as a somehow separate track, in modeling the 3D visual world too such as voxels or point clouds. However, with the growing hope that transformers can become the\"universal\"modeling tool for heterogeneous data, ViTs for 2D and 3D tasks have so far adopted vastly different architecture designs that are hardly transferable. That invites an (over-)ambitious question: can we close the gap between the 2D and 3D ViT architectures? As a piloting study, this paper demonstrates the appealing promise to understand the 3D visual world, using a standard 2D ViT architecture, with only minimal customization at the input and output levels without redesigning the pipeline. To build a 3D ViT from its 2D sibling, we\"inflate\"the patch embedding and token sequence, accompanied with new positional encoding mechanisms designed to match the 3D data geometry. The resultant\"minimalist\"3D ViT, named Simple3D-Former, performs surprisingly robustly on popular 3D tasks such as object classification, point cloud segmentation and indoor scene detection, compared to highly customized 3D-specific designs. It can hence act as a strong baseline for new 3D ViTs. Moreover, we note that pursing a unified 2D-3D ViT design has practical relevance besides just scientific curiosity. Specifically, we demonstrate that Simple3D-Former naturally enables to exploit the wealth of pre-trained weights from large-scale realistic 2D images (e.g., ImageNet), which can be plugged in to enhancing the 3D task performance\"for free\".",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/29f86d6d1eaba6a466c231f6906b18eae4b2b484.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 3.333333333333333
    },
    "d8bf4b494d255c6d9b81e9963f29e40831106e3e.pdf": {
        "title": "Crack45K: Integration of Vision Transformer with Tubularity Flow Field (TuFF) and Sliding-Window Approach for Crack-Segmentation in Pavement Structures",
        "authors": [
            "Luqman Ali",
            "Hamad Al Jassmi",
            "Wasif Khan",
            "F. Alnajjar"
        ],
        "published_date": "2022",
        "abstract": "Recently, deep-learning (DL)-based crack-detection systems have proven to be the method of choice for image processing-based inspection systems. However, human-like generalization remains challenging, owing to a wide variety of factors such as crack type and size. Additionally, because of their localized receptive fields, CNNs have a high false-detection rate and perform poorly when attempting to capture the relevant areas of an image. This study aims to propose a vision-transformer-based crack-detection framework that treats image data as a succession of small patches, to retrieve global contextual information (GCI) through self-attention (SA) methods, and which addresses the CNNs\u2019 problem of inductive biases, including the locally constrained receptive-fields and translation-invariance. The vision-transformer (ViT) classifier was tested to enhance crack classification, localization, and segmentation performance by blending with a sliding-window and tubularity-flow-field (TuFF) algorithm. Firstly, the ViT framework was trained on a custom dataset consisting of 45K images with 224 \u00d7 224 pixels resolution, and achieved accuracy, precision, recall, and F1 scores of 0.960, 0.971, 0.950, and 0.960, respectively. Secondly, the trained ViT was integrated with the sliding-window (SW) approach, to obtain a crack-localization map from large images. The SW-based ViT classifier was then merged with the TuFF algorithm, to acquire efficient crack-mapping by suppressing the unwanted regions in the last step. The robustness and adaptability of the proposed integrated-architecture were tested on new data acquired under different conditions and which were not utilized during the training and validation of the model. The proposed ViT-architecture performance was evaluated and compared with that of various state-of-the-art (SOTA) deep-learning approaches. The experimental results show that ViT equipped with a sliding-window and the TuFF algorithm can enhance real-world crack classification, localization, and segmentation performance.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d8bf4b494d255c6d9b81e9963f29e40831106e3e.pdf",
        "venue": "Buildings",
        "citationCount": 10,
        "score": 3.333333333333333
    },
    "abf037290e859a241a5af2c5adf9c08767971683.pdf": {
        "title": "Plant-Leaf Diseases Classification using CNN, CBAM and Vision Transformer",
        "authors": [
            "Abdeldjalil Chougui",
            "Achraf Moussaoui",
            "A. Moussaoui"
        ],
        "published_date": "2022",
        "abstract": "Detecting plant diseases is usually difficult without an experts knowledge. In this study we want to propose a new classification model based on deep learning that will be able to classify and identify different plant-leaf diseases with high accuracy that outperforms the state of the art approaches and previous works. Using only training images, CNN can automatically extract features for classification, and achieve high classification performance. We used two datasets in this study, PlantVillage dataset containing 54,303 healthy and unhealthy leaf images divided into 38 categories by species and disease, and Tomato dataset containing 11,000 healthy and unhealthy tomato leaf images with nine diseases to train the models. We propose a deep convolutional neural network architecture, with and without attention mechanism, and we tuned 4 pretrained models that have been trained on large dataset such as MobileNet, VGG-16, VGG-19 and ResNET. We also tuned 2 ViT models, the vit b32 from keras and the base patch 16 from google. Our porposed model obtained an accuracy up to 97.74%. The pretrained models gave an accuracy up to 99.52%. And the ViT models obtained an accuracy up to 99.7%. This study may aid in detecting the plant leaf diseases and improve life conditions to plants which will improve quality of humans life.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/abf037290e859a241a5af2c5adf9c08767971683.pdf",
        "venue": "International Symposium on Information and Automation",
        "citationCount": 10,
        "score": 3.333333333333333
    },
    "dd46070ce18f55a5714e53a096c8219d6934d188.pdf": {
        "title": "Deep-Learning-Based Diagnosis of Cassava Leaf Diseases Using Vision Transformer",
        "authors": [
            "Li Zhuang"
        ],
        "published_date": "2021",
        "abstract": "Viral diseases are major causes leading to the poor yields of cassava, which is the second-largest source of food carbohydrates in Africa. As symptoms of these diseases can usually be identified by inspecting cassava leafs, visual diagnosis of cassava leaf diseases is of significant importance in food security and agriculture development. Considering the shortage of qualified agricultural experts, automatic approaches for the image-based detection of cassava leaf diseases are in great demand. In this paper, on the basis of Vision Transformer, we propose a deep learning method to identify the type of viral disease in a cassava leaf image. The image dataset of cassava leaves is provided by the Makerere Artificial Intelligence Lab in a Kaggle competition, consisting of 4 subtypes of diseases and healthy cassava leaves. Our results show that Vision-Transformer-based model can effectively achieve an excellent performance regarding the classification of cassava leaf diseases. After applying the K-Fold cross validation technique, our model reaches a categorization accuracy 0.9002 on the private test set. This score ranks top 3% in the leaderboard, and can get a silver medal prize in the Kaggle competition. Our method can be applied for the identification of diseased plants, and potentially prevent the irreparable damage of crops.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/dd46070ce18f55a5714e53a096c8219d6934d188.pdf",
        "venue": "Artificial Intelligence and Cloud Computing Conference",
        "citationCount": 13,
        "score": 3.25
    },
    "829926d9943c99dd64eea5b4b96541eca1e5e91d.pdf": {
        "title": "Hyperspectral Image Classification Based on Multi-stage Vision Transformer with Stacked Samples",
        "authors": [
            "Xiaoyue Chen",
            "Sei-ichiro Kamata",
            "Weilian Zhou"
        ],
        "published_date": "2021",
        "abstract": "Hyperspectral image classification (HSIC) is a task assigning the correct label to each pixel. It is a hot topic in the remote sensing field, which has been processed in several deep learning methods. Recently, there are some works that apply Vision Transformer (ViT) methods to the HSIC task, but the performance is not as good as some CNN-structured methods, considering that Vision Transformer uses attention to capture global information but ignores local characteristics. In this paper, a multi-stage Vision Transformer model referring to the feature extraction structure of CNN is proposed, and the result shows the realizability and reliability. Besides, experiments show that the modified ViT structure needs more samples for training. An innovative data augmentation method is used to generate extended samples with virtual yet reliable labels. The generated samples are combined with the original ones as the stacked samples, which are used for the following feature extraction process. Experiments explain the optimization of the multi-stage Vision Transformer structure with stacked samples in the accuracy term compared with other methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/829926d9943c99dd64eea5b4b96541eca1e5e91d.pdf",
        "venue": "IEEE Region 10 Conference",
        "citationCount": 12,
        "score": 3.0
    },
    "e8dceb26166721014b8ecbd11fd212739c18d315.pdf": {
        "title": "MambaVision: A Hybrid Mamba-Transformer Vision Backbone",
        "authors": [
            "Ali Hatamizadeh",
            "Jan Kautz"
        ],
        "published_date": "2024",
        "abstract": "We propose a novel hybrid Mamba-Transformer backbone, MambaVision, specifically tailored for vision applications. Our core contribution includes redesigning the Mamba formulation to enhance its capability for efficient modeling of visual features. Through a comprehensive ablation study, we demonstrate the feasibility of integrating Vision Transformers (ViT) with Mamba. Our results show that equipping the Mamba architecture with self-attention blocks in the final layers greatly improves its capacity to capture longrange spatial dependencies. Based on these findings, we introduce a family of MambaVision models with a hierarchical architecture to meet various design criteria. For classification on the ImageNet-1K dataset, MambaVision variants achieve state-of-the-art (SOTA) performance in terms of both Top-1 accuracy and throughput. In downstream tasks such as object detection, instance segmentation, and semantic segmentation on MS COCO and ADE20K datasets, MambaVision outperforms comparably sized backbones while demonstrating favorable performance. Code: https://github.com/NVlabs/MambaVision",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e8dceb26166721014b8ecbd11fd212739c18d315.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 143,
        "score": 143.0
    },
    "e06b703146c46a6455fd0c33077de1bea5fdd877.pdf": {
        "title": "Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles",
        "authors": [
            "Chaitanya K. Ryali",
            "Yuan-Ting Hu",
            "Daniel Bolya",
            "Chen Wei",
            "Haoqi Fan",
            "Po-Yao (Bernie) Huang",
            "Vaibhav Aggarwal",
            "Arkabandhu Chowdhury",
            "Omid Poursaeed",
            "Judy Hoffman",
            "J. Malik",
            "Yanghao Li",
            "Christoph Feichtenhofer"
        ],
        "published_date": "2023",
        "abstract": "Modern hierarchical vision transformers have added several vision-specific components in the pursuit of supervised classification performance. While these components lead to effective accuracies and attractive FLOP counts, the added complexity actually makes these transformers slower than their vanilla ViT counterparts. In this paper, we argue that this additional bulk is unnecessary. By pretraining with a strong visual pretext task (MAE), we can strip out all the bells-and-whistles from a state-of-the-art multi-stage vision transformer without losing accuracy. In the process, we create Hiera, an extremely simple hierarchical vision transformer that is more accurate than previous models while being significantly faster both at inference and during training. We evaluate Hiera on a variety of tasks for image and video recognition. Our code and models are available at https://github.com/facebookresearch/hiera.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e06b703146c46a6455fd0c33077de1bea5fdd877.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 244,
        "score": 122.0
    },
    "3af375031a3e23b7daf2f1ed14b5b61147996ca0.pdf": {
        "title": "Extended Vision Transformer (ExViT) for Land Use and Land Cover Classification: A Multimodal Deep Learning Framework",
        "authors": [
            "Jing Yao",
            "Bing Zhang",
            "Chenyu Li",
            "D. Hong",
            "J. Chanussot"
        ],
        "published_date": "2023",
        "abstract": "The recent success of attention mechanism-driven deep models, like vision transformer (ViT) as one of the most representatives, has intrigued a wave of advanced research to explore their adaptation to broader domains. However, current transformer-based approaches in the remote sensing (RS) community pay more attention to single-modality data, which might lose expandability in making full use of the ever-growing multimodal Earth observation data. To this end, we propose a novel multimodal deep learning framework by extending conventional ViT with minimal modifications, aiming at the task of land use and land cover (LULC) classification. Unlike common stems that adopt either linear patch projection or deep regional embedder, our approach processes multimodal RS image patches with parallel branches of position-shared ViTs extended with separable convolution modules, which offers an economical solution to leverage both spatial and modality-specific channel information. Furthermore, to promote information exchange across heterogeneous modalities, their tokenized embeddings are then fused through a cross-modality attention (CMA) module by exploiting pixel-level spatial correlation in RS scenes. Both of these modifications significantly improve the discriminative ability of classification tokens in each modality and thus further performance increase can be finally attained by a full token-based decision-level fusion module. We conduct extensive experiments on two multimodal RS benchmark datasets, i.e., the Houston2013 dataset containing hyperspectral (HS) and light detection and ranging (LiDAR) data, and Berlin dataset with HS and synthetic aperture radar (SAR) data, to demonstrate that our extended vision transformer (ExViT) outperforms concurrent competitors based on transformer or convolutional neural network (CNN) backbones, in addition to several competitive machine-learning-based models. The source codes and investigated datasets of this work will be made publicly available at https://github.com/jingyao16/ExViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3af375031a3e23b7daf2f1ed14b5b61147996ca0.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 209,
        "score": 104.5
    },
    "d203076c28587895aa344d088b2788dbab5e82a1.pdf": {
        "title": "Transformer-Based Visual Segmentation: A Survey",
        "authors": [
            "Xiangtai Li",
            "Henghui Ding",
            "Wenwei Zhang",
            "Haobo Yuan",
            "Jiangmiao Pang",
            "Guangliang Cheng",
            "Kai Chen",
            "Ziwei Liu",
            "Chen Change Loy"
        ],
        "published_date": "2023",
        "abstract": "Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several specific subfields, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d203076c28587895aa344d088b2788dbab5e82a1.pdf",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citationCount": 194,
        "score": 97.0
    },
    "f3d0278649454f80ba52c966a979499ee33e26c2.pdf": {
        "title": "Hyperspectral Image Classification Using Groupwise Separable Convolutional Vision Transformer Network",
        "authors": [
            "Zhuoyi Zhao",
            "Xiang Xu",
            "Shutao Li",
            "A. Plaza"
        ],
        "published_date": "2024",
        "abstract": "Recently, vision transformer (ViT)-based deep learning (DL) models have achieved remarkable performance gains in hyperspectral image classification (HSIC) due to their abilities to model long-range dependencies and extract global spatial features. However, ViT is built with a stack of Transformer blocks and faces the challenge of learning a large number of parameters when processing hyperspectral data. Besides, the inherent modeling of global correlation in Transformer ignores the effective representation of local spatial and spectral features. To address these issues, we propose a lightweight ViT network known as groupwise separable convolutional ViT (GSC-ViT). First, a groupwise separable convolution (GSC) module, which is a combination of grouped pointwise convolution (GPWC) and group convolution, is designed to significantly decrease the number of convolutional kernel parameters, and effectively capture local spectral\u2013spatial information in hyperspectral image (HSI). Second, a groupwise separable multihead self-attention (GSSA) module is employed to substitute the conventional multihead self-attention (MSA) in ViT, in which the groupwise self-attention (GSA) provides local spatial feature extraction, and the pointwise self-attention (PWSA) provides global spatial feature extraction. Third, a simple pointwise layer with enhanced skip connection mechanism is employed to substitute the multilayer perceptron (MLP) layer in all Transformer blocks of ViT, so as to eliminate unnecessary nonlinear transformations and facilitate the fusion of features derived from GSC and GSSA modules. Extensive experiments on four benchmark hyperspectral datasets reveal that our GSC-ViT can achieve surprising classification performance with relatively few training samples as compared with some existing HSIC approaches. The source code is available at https://github.com/flyzzie/TGRS-GSC-VIT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f3d0278649454f80ba52c966a979499ee33e26c2.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 90,
        "score": 90.0
    },
    "918617dbc02fa4df1999599bcf967acd2ea84d71.pdf": {
        "title": "Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution",
        "authors": [
            "Mostafa Dehghani",
            "Basil Mustafa",
            "J. Djolonga",
            "J. Heek",
            "Matthias Minderer",
            "Mathilde Caron",
            "A. Steiner",
            "J. Puigcerver",
            "Robert Geirhos",
            "Ibrahim M. Alabdulmohsin",
            "Avital Oliver",
            "Piotr Padlewski",
            "A. Gritsenko",
            "Mario Luvci'c",
            "N. Houlsby"
        ],
        "published_date": "2023",
        "abstract": "The ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before processing them with computer vision models has not yet been successfully challenged. However, models such as the Vision Transformer (ViT) offer flexible sequence-based modeling, and hence varying input sequence lengths. We take advantage of this with NaViT (Native Resolution ViT) which uses sequence packing during training to process inputs of arbitrary resolutions and aspect ratios. Alongside flexible model usage, we demonstrate improved training efficiency for large-scale supervised and contrastive image-text pretraining. NaViT can be efficiently transferred to standard tasks such as image and video classification, object detection, and semantic segmentation and leads to improved results on robustness and fairness benchmarks. At inference time, the input resolution flexibility can be used to smoothly navigate the test-time cost-performance trade-off. We believe that NaViT marks a departure from the standard, CNN-designed, input and modelling pipeline used by most computer vision models, and represents a promising direction for ViTs.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/918617dbc02fa4df1999599bcf967acd2ea84d71.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 147,
        "score": 73.5
    },
    "51f38bd957fa863022feb5878fa1ba3bea6657cf.pdf": {
        "title": "Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures",
        "authors": [
            "Yuchen Duan",
            "Weiyun Wang",
            "Zhe Chen",
            "Xizhou Zhu",
            "Lewei Lu",
            "Tong Lu",
            "Yu Qiao",
            "Hongsheng Li",
            "Jifeng Dai",
            "Wenhai Wang"
        ],
        "published_date": "2024",
        "abstract": "Transformers have revolutionized computer vision and natural language processing, but their high computational complexity limits their application in high-resolution image processing and long-context analysis. This paper introduces Vision-RWKV (VRWKV), a model adapted from the RWKV model used in the NLP field with necessary modifications for vision tasks. Similar to the Vision Transformer (ViT), our model is designed to efficiently handle sparse inputs and demonstrate robust global processing capabilities, while also scaling up effectively, accommodating both large-scale parameters and extensive datasets. Its distinctive advantage lies in its reduced spatial aggregation complexity, which renders it exceptionally adept at processing high-resolution images seamlessly, eliminating the necessity for windowing operations. Our evaluations demonstrate that VRWKV surpasses ViT's performance in image classification and has significantly faster speeds and lower memory usage processing high-resolution inputs. In dense prediction tasks, it outperforms window-based models, maintaining comparable speeds. These results highlight VRWKV's potential as a more efficient alternative for visual perception tasks. Code is released at https://github.com/OpenGVLab/Vision-RWKV.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/51f38bd957fa863022feb5878fa1ba3bea6657cf.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 66,
        "score": 66.0
    },
    "1f389f54324790bfad6fc40ac4e56428757ea92b.pdf": {
        "title": "ViT-SmartAgri: Vision Transformer and Smartphone-Based Plant Disease Detection for Smart Agriculture",
        "authors": [
            "Utpal Barman",
            "Parismita Sarma",
            "Mirzanur Rahman",
            "Vaskar Deka",
            "Swati Lahkar",
            "Vaishali Sharma",
            "Manob Saikia"
        ],
        "published_date": "2024",
        "abstract": "Invading pests and diseases always degrade the quality and quantity of plants. Early and accurate identification of plant diseases is critical for plant health and growth. This work proposes a smartphone-based solution using a Vision Transformer (ViT) model for identifying healthy plants and unhealthy plants with diseases. The collected dataset of tomato leaves was used to collectively train Vision Transformer and Inception V3-based deep learning (DL) models to differentiate healthy and diseased plants. These models detected 10 different tomato disease classes from the dataset containing 10,010 images. The performance of the two DL models was compared. This work also presents a smartphone-based application (Android App) using a ViT-based model, which works on the basis of the self-attention mechanism and yielded a better performance (90.99% testing) than Inception V3 in our experimentation. The proposed ViT-SmartAgri is promising and can be implemented on a colossal scale for smart agriculture, thus inspiring future work in this area.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1f389f54324790bfad6fc40ac4e56428757ea92b.pdf",
        "venue": "Agronomy",
        "citationCount": 60,
        "score": 60.0
    },
    "05236fa766fc1a38a9eb895e77075fb65be8c258.pdf": {
        "title": "An efficient and robust Phonocardiography (PCG)-based Valvular Heart Diseases (VHD) detection framework using Vision Transformer (ViT)",
        "authors": [
            "Sonain Jamil",
            "Anisha Roy"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/05236fa766fc1a38a9eb895e77075fb65be8c258.pdf",
        "venue": "Comput. Biol. Medicine",
        "citationCount": 84,
        "score": 42.0
    },
    "0eec6c36da426f78b7091ba7ae8602e129742d30.pdf": {
        "title": "Enhancing Skin Cancer Diagnosis Using Swin Transformer with Hybrid Shifted Window-Based Multi-head Self-attention and SwiGLU-Based MLP",
        "authors": [
            "Ishak Pa\u00e7al",
            "Melek Alaftekin",
            "F. Zengul"
        ],
        "published_date": "2024",
        "abstract": "Skin cancer is one of the most frequently occurring cancers worldwide, and early detection is crucial for effective treatment. Dermatologists often face challenges such as heavy data demands, potential human errors, and strict time limits, which can negatively affect diagnostic outcomes. Deep learning\u2013based diagnostic systems offer quick, accurate testing and enhanced research capabilities, providing significant support to dermatologists. In this study, we enhanced the Swin Transformer architecture by implementing the hybrid shifted window-based multi-head self-attention (HSW-MSA) in place of the conventional shifted window-based multi-head self-attention (SW-MSA). This adjustment enables the model to more efficiently process areas of skin cancer overlap, capture finer details, and manage long-range dependencies, while maintaining memory usage and computational efficiency during training. Additionally, the study replaces the standard multi-layer perceptron (MLP) in the Swin Transformer with a SwiGLU-based MLP, an upgraded version of the gated linear unit (GLU) module, to achieve higher accuracy, faster training speeds, and better parameter efficiency. The modified Swin model-base was evaluated using the publicly accessible ISIC 2019 skin dataset with eight classes and was compared against popular convolutional neural networks (CNNs) and cutting-edge vision transformer (ViT) models. In an exhaustive assessment on the unseen test dataset, the proposed Swin-Base model demonstrated exceptional performance, achieving an accuracy of 89.36%, a recall of 85.13%, a precision of 88.22%, and an F1-score of 86.65%, surpassing all previously reported research and deep learning models documented in the literature.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0eec6c36da426f78b7091ba7ae8602e129742d30.pdf",
        "venue": "Journal of imaging informatics in medicine",
        "citationCount": 42,
        "score": 42.0
    },
    "689bc24f71f8f22784534c764d59baa93a62c2e0.pdf": {
        "title": "HiViT: A Simpler and More Efficient Design of Hierarchical Vision Transformer",
        "authors": [
            "Xiaosong Zhang",
            "Yunjie Tian",
            "Lingxi Xie",
            "Wei Huang",
            "Qi Dai",
            "Qixiang Ye",
            "Qi Tian"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/689bc24f71f8f22784534c764d59baa93a62c2e0.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 80,
        "score": 40.0
    },
    "afb7ff7096cd45de02de96b5937c3416ebca0cb0.pdf": {
        "title": "Skin Cancer Segmentation and Classification Using Vision Transformer for Automatic Analysis in Dermatoscopy-Based Noninvasive Digital System",
        "authors": [
            "Galib Muhammad Shahriar Himel",
            "Md. Masudul Islam",
            "Kh Abdullah Al-Aff",
            "Shams Ibne Karim",
            "Md. Kabir Uddin Sikder"
        ],
        "published_date": "2024",
        "abstract": "Skin cancer is a significant health concern worldwide, and early and accurate diagnosis plays a crucial role in improving patient outcomes. In recent years, deep learning models have shown remarkable success in various computer vision tasks, including image classification. In this research study, we introduce an approach for skin cancer classification using vision transformer, a state-of-the-art deep learning architecture that has demonstrated exceptional performance in diverse image analysis tasks. The study utilizes the HAM10000 dataset; a publicly available dataset comprising 10,015 skin lesion images classified into two categories: benign (6705 images) and malignant (3310 images). This dataset consists of high-resolution images captured using dermatoscopes and carefully annotated by expert dermatologists. Preprocessing techniques, such as normalization and augmentation, are applied to enhance the robustness and generalization of the model. The vision transformer architecture is adapted to the skin cancer classification task. The model leverages the self-attention mechanism to capture intricate spatial dependencies and long-range dependencies within the images, enabling it to effectively learn relevant features for accurate classification. Segment Anything Model (SAM) is employed to segment the cancerous areas from the images; achieving an IOU of 96.01% and Dice coefficient of 98.14% and then various pretrained models are used for classification using vision transformer architecture. Extensive experiments and evaluations are conducted to assess the performance of our approach. The results demonstrate the superiority of the vision transformer model over traditional deep learning architectures in skin cancer classification in general with some exceptions. Upon experimenting on six different models, ViT-Google, ViT-MAE, ViT-ResNet50, ViT-VAN, ViT-BEiT, and ViT-DiT, we found out that the ML approach achieves 96.15% accuracy using Google's ViT patch-32 model with a low false negative ratio on the test dataset, showcasing its potential as an effective tool for aiding dermatologists in the diagnosis of skin cancer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/afb7ff7096cd45de02de96b5937c3416ebca0cb0.pdf",
        "venue": "International Journal of Biomedical Imaging",
        "citationCount": 39,
        "score": 39.0
    },
    "595adb75ddeb90760c79e89b76d99e55079e0708.pdf": {
        "title": "Fine-Grained Visual Classification via Internal Ensemble Learning Transformer",
        "authors": [
            "Qin Xu",
            "Jiahui Wang",
            "Bo Jiang",
            "Bin Luo"
        ],
        "published_date": "2023",
        "abstract": "Recently, vision transformers (ViTs) have been investigated in fine-grained visual recognition (FGVC) and are now considered state of the art. However, most ViT-based works ignore the different learning performances of the heads in the multi-head self-attention (MHSA) mechanism and its layers. To address these issues, in this paper, we propose a novel internal ensemble learning transformer (IELT) for FGVC. The proposed IELT involves three main modules: multi-head voting (MHV) module, cross-layer refinement (CLR) module, and dynamic selection (DS) module. To solve the problem of the inconsistent performances of multiple heads, we propose the MHV module, which considers all of the heads in each layer as weak learners and votes for tokens of discriminative regions as cross-layer feature based on the attention maps and spatial relationships. To effectively mine the cross-layer feature and suppress the noise, the CLR module is proposed, where the refined feature is extracted and the assist logits operation is developed for the final prediction. In addition, a newly designed DS module adjusts the token selection number at each layer by weighting their contributions of the refined feature. In this way, the idea of ensemble learning is combined with the ViT to improve fine-grained feature representation. The experiments demonstrate that our method achieves competitive results compared with the state of the art on five popular FGVC datasets.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/595adb75ddeb90760c79e89b76d99e55079e0708.pdf",
        "venue": "IEEE transactions on multimedia",
        "citationCount": 76,
        "score": 38.0
    },
    "de20c6805b83a2f83ed75784920e91b913d888bb.pdf": {
        "title": "Trinity-Net: Gradient-Guided Swin Transformer-Based Remote Sensing Image Dehazing and Beyond",
        "authors": [
            "Kaichen Chi",
            "Yuan Yuan",
            "Qi Wang"
        ],
        "published_date": "2023",
        "abstract": "Haze superimposes a veil over remote sensing images, which severely limits the extraction of valuable military information. To this end, we present a novel trinity model to restore realistic surface information by integrating the merits of both prior- and deep learning-based strategies. Concretely, the critical insight of our Trinity-Net is to investigate how to incorporate prior information into convolutional neural networks (CNNs) and Swin Transformer for reasonable estimation of haze parameters. Then, haze-free images are obtained by reconstructing the remote sensing image formation model. Although Swin Transformer has shown tremendous potential in the dehazing task, which typically results in ambiguous details, we devise a gradient guidance module that naturally inherits structure priors of gradient maps, guiding the deep model to generate visually pleasing details. In light of the generality of image formation parameters, we successfully promote Trinity-Net to natural image dehazing and underwater image enhancement tasks. Notably, the acquisition of large-scale remote sensing hazy images and natural hazy images in military scenes is not feasible in practice. To bridge this gap, we construct a remote sensing image dehazing benchmark (RSID) and a natural image dehazing benchmark (NID), including 1000 real-world hazy images with corresponding ground-truth images. To our knowledge, this is the first exploration to develop dehazing benchmarks in the military field, alleviating the dilemma of data scarcity. Extensive experiments on three vision tasks illustrate the superiority of our Trinity-Net against multiple state-of-the-art methods. The datasets and code are available at https://github.com/chi-kaichen/Trinity-Net.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/de20c6805b83a2f83ed75784920e91b913d888bb.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 75,
        "score": 37.5
    },
    "c57467e652f3f9131b3e7e40c23059abe395f01d.pdf": {
        "title": "SpectFormer: Frequency and Attention is what you need in a Vision Transformer",
        "authors": [
            "Badri N. Patro",
            "Vinay P. Namboodiri",
            "Vijay Srinivas Agneeswaran"
        ],
        "published_date": "2023",
        "abstract": "Vision transformers have been applied successfully for image recognition tasks. There have been either multi-headed self-attention based (ViT [12], DelT [54]) simi-lar to the original work in textual models or more re-cently based on spectral layers (Fnet [29], GFNet [46], AFNO [15]). We hypothesize that spectral layers cap-ture high-frequency information such as lines and edges, while attention layers capture token interactions. We inves-tigate this hypothesis through this work and observe that indeed mixing spectral and multi-headed attention layers provides a better transformer architecture. We thus pro-pose the novel Spectformer architecture for vision trans-formers that has initial spectral and deeper multi-headed attention layers. We believe that the resulting representation allows the transformer to capture the feature repre-sentation appropriately and it yields improved performance over other transformer representations. For instance, it im-proves the top-1 accuracy by 2% on ImageNet compared to both GFNet-H and LiT. SpectFormer-H-S reaches 84.25% top-1 accuracy on ImageNet-1 K (state of the art for small version). Further, Spectformer-H-L achieves 85.7% which is the state of the art for the comparable base version of the transformers. We further validated the SpectFormer per-formance in other scenarios such as transfer learning on standard datasets such as ClFAR-10, ClFAR-100, Oxford-lIlT-flower, and Standford Car datasets. We then investigate its use in downstream tasks such as object detection and instance segmentation on the MS-COCO dataset and ob-serve that Spectformer shows consistent performance that is comparable to the best backbones and can be further optimized and improved. The source code is available on this website https://github.com/badripatro/SpectFormers.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c57467e652f3f9131b3e7e40c23059abe395f01d.pdf",
        "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
        "citationCount": 70,
        "score": 35.0
    },
    "53e5db85e2a7442f20670be2ae25019fcf9d27a2.pdf": {
        "title": "Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention",
        "authors": [
            "Xuran Pan",
            "Tianzhu Ye",
            "Zhuofan Xia",
            "S. Song",
            "Gao Huang"
        ],
        "published_date": "2023",
        "abstract": "Self-attention mechanism has been a key factor in the recent progress of Vision Transformer (ViT), which enables adaptive feature extraction from global contexts. However, existing self-attention methods either adopt sparse global attention or window attention to reduce the computation complexity, which may compromise the local feature learning or subject to some handcrafted designs. In contrast, local attention, which restricts the receptive field of each query to its own neighboring pixels, enjoys the benefits of both convolution and self-attention, namely local inductive bias and dynamic feature selection. Nevertheless, current local attention modules either use inefficient Im2Col function or rely on specific CUDA kernels that are hard to generalize to devices without CUDA support. In this paper, we propose a novel local attention module, Slide Attention, which leverages common convolution operations to achieve high efficiency, flexibility and generalizability. Specifically, we first re-interpret the column-based Im2Col function from a new row-based perspective and use Depthwise Convolution as an efficient substitution. On this basis, we propose a deformed shifting module based on the re-parameterization technique, which further relaxes the fixed key/value positions to deformed features in the local region. In this way, our module realizes the local attention paradigm in both efficient and flexible manner. Extensive experiments show that our slide attention module is applicable to a variety of advanced Vision Transformer models and compatible with various hardware devices, and achieves consistently improved performances on comprehensive benchmarks.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/53e5db85e2a7442f20670be2ae25019fcf9d27a2.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 69,
        "score": 34.5
    },
    "0682771fd5f611bce2a536bf83587532469a83df.pdf": {
        "title": "Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for Scribble-based Medical Image Segmentation",
        "authors": [
            "Ziyang Wang",
            "Chao Ma"
        ],
        "published_date": "2024",
        "abstract": "Medical image segmentation is increasingly reliant on deep learning techniques, yet the promising performance often come with high annotation costs. This paper introduces Weak-Mamba-UNet, an innovative weakly-supervised learning (WSL) framework that leverages the capabilities of Convolutional Neural Network (CNN), Vision Transformer (ViT), and the cutting-edge Visual Mamba (VMamba) architecture for medical image segmentation, especially when dealing with scribble-based annotations. The proposed WSL strategy incorporates three distinct architecture but same symmetrical encoder-decoder networks: a CNN-based UNet for detailed local feature extraction, a Swin Transformer-based SwinUNet for comprehensive global context understanding, and a VMamba-based Mamba-UNet for efficient long-range dependency modeling. The key concept of this framework is a collaborative and cross-supervisory mechanism that employs pseudo labels to facilitate iterative learning and refinement across the networks. The effectiveness of Weak-Mamba-UNet is validated on a publicly available MRI cardiac segmentation dataset with processed scribble annotations, where it surpasses the performance of a similar WSL framework utilizing only UNet or SwinUNet. This highlights its potential in scenarios with sparse or imprecise annotations. The source code is made publicly accessible.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0682771fd5f611bce2a536bf83587532469a83df.pdf",
        "venue": "arXiv.org",
        "citationCount": 32,
        "score": 32.0
    },
    "a4ef2c7c321dea04d8ea91ec1ce0710ebce4f848.pdf": {
        "title": "A Deep Features Extraction Model Based on the Transfer Learning Model and Vision Transformer \u201cTLMViT\u201d for Plant Disease Classification",
        "authors": [
            "A. Tabbakh",
            "Soubhagya Sankar Barpanda"
        ],
        "published_date": "2023",
        "abstract": "This paper proposes a novel approach for extracting deep features and classifying diseased plant leaves. The agriculture industry is negatively impacted by plant diseases causing crop and economic loss. Accurate and timely diagnosis is crucial for managing and controlling plant diseases, as traditional methods can be costly and time-consuming. Deep learning-based tools effectively detect plant diseases depending on the qualitative of extracted features. In this regard, a hybrid model for plant disease classification based on a Transfer Learning-based model followed by a vision transformer (TLMViT) is proposed. TLMViT has four stages: 1) data acquisition, where the PlantVillage and wheat datasets are used to train and evaluate the proposed model, 2) image augmentation to increase the number of training samples and overcome the overfitting issue, 3) leaf feature extraction by two consecutive phases: initial features extraction by using pre-trained based model and deep features extraction by using ViT model, and 4) classification by using MLP classifier. TLMViT is experimented with using five pre-trained-based models followed by ViT individually. TLMViT performs accurately in plant disease classification, obtaining 98.81% and 99.86% validation accuracy for VGG19 followed by the ViT model on PlantVillage and wheat datasets respectively. Moreover, TLMViT is compared with pre-trained-based architecture. The comparison result illustrates that TLMViT achieved an enhancement of 1.11% and 1.099% in validation accuracy, 2.576% and 2.92% in validation loss compared with the transfer learning-based model for PlantVillage and wheat datasets respectively. Thereby proposed model proves the efficiency of using ViT for extracting deep features from the leaf.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a4ef2c7c321dea04d8ea91ec1ce0710ebce4f848.pdf",
        "venue": "IEEE Access",
        "citationCount": 64,
        "score": 32.0
    },
    "243a056d1acb153f70e39cc80a10e7d211a4312f.pdf": {
        "title": "Conv-ViT: A Convolution and Vision Transformer-Based Hybrid Feature Extraction Method for Retinal Disease Detection",
        "authors": [
            "Pramit Dutta",
            "Khaleda Akhter Sathi",
            "Md.Azad Hossain",
            "M. Ali",
            "Akber Dewan"
        ],
        "published_date": "2023",
        "abstract": "The current advancement towards retinal disease detection mainly focused on distinct feature extraction using either a convolutional neural network (CNN) or a transformer-based end-to-end deep learning (DL) model. The individual end-to-end DL models are capable of only processing texture or shape-based information for performing detection tasks. However, extraction of only texture- or shape-based features does not provide the model robustness needed to classify different types of retinal diseases. Therefore, concerning these two features, this paper developed a fusion model called \u2018Conv-ViT\u2019 to detect retinal diseases from foveal cut optical coherence tomography (OCT) images. The transfer learning-based CNN models, such as Inception-V3 and ResNet-50, are utilized to process texture information by calculating the correlation of the nearby pixel. Additionally, the vision transformer model is fused to process shape-based features by determining the correlation between long-distance pixels. The hybridization of these three models results in shape-based texture feature learning during the classification of retinal diseases into its four classes, including choroidal neovascularization (CNV), diabetic macular edema (DME), DRUSEN, and NORMAL. The weighted average classification accuracy, precision, recall, and F1 score of the model are found to be approximately 94%. The results indicate that the fusion of both texture and shape features assisted the proposed Conv-ViT model to outperform the state-of-the-art retinal disease classification models.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/243a056d1acb153f70e39cc80a10e7d211a4312f.pdf",
        "venue": "Journal of Imaging",
        "citationCount": 61,
        "score": 30.5
    },
    "d8ab87176444f8b0747972310431c647a87de2df.pdf": {
        "title": "IFViT: Interpretable Fixed-Length Representation for Fingerprint Matching via Vision Transformer",
        "authors": [
            "Yuhang Qiu",
            "Honghui Chen",
            "Xingbo Dong",
            "Zheng Lin",
            "Iman Yi Liao",
            "Massimo Tistarelli",
            "Zhe Jin"
        ],
        "published_date": "2024",
        "abstract": "Determining dense feature points on fingerprints used in constructing deep fixed-length representations for accurate matching, particularly at the pixel level, is of significant interest. To explore the interpretability of fingerprint matching, we propose a multi-stage interpretable fingerprint matching network, namely Interpretable Fixed-length Representation for Fingerprint Matching via Vision Transformer (IFViT), which consists of two primary modules. The first module, an interpretable dense registration module, establishes a Vision Transformer (ViT)-based Siamese Network to capture long-range dependencies and the global context in fingerprint pairs. It provides interpretable dense pixel-wise correspondences of feature points for fingerprint alignment and enhances the interpretability in the subsequent matching stage. The second module takes into account both local and global representations of the aligned fingerprint pair to achieve an interpretable fixed-length representation extraction and matching. It employs the ViTs trained in the first module with the additional fully connected layer and retrains them to simultaneously produce the discriminative fixed-length representation and interpretable dense pixel-wise correspondences of feature points. Extensive experimental results on diverse publicly available fingerprint databases demonstrate that the proposed framework not only exhibits superior performance on dense registration and matching but also significantly promotes the interpretability in deep fixed-length representations-based fingerprint matching.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d8ab87176444f8b0747972310431c647a87de2df.pdf",
        "venue": "IEEE Transactions on Information Forensics and Security",
        "citationCount": 30,
        "score": 30.0
    },
    "a1e17a09b0df676f133c2175366ab7fcd0381ac1.pdf": {
        "title": "PMVT: a lightweight vision transformer for plant disease identification on mobile devices",
        "authors": [
            "Guoqiang Li",
            "Yuchao Wang",
            "Qing Zhao",
            "Peiyan Yuan",
            "Baofang Chang"
        ],
        "published_date": "2023",
        "abstract": "Due to the constraints of agricultural computing resources and the diversity of plant diseases, it is challenging to achieve the desired accuracy rate while keeping the network lightweight. In this paper, we proposed a computationally efficient deep learning architecture based on the mobile vision transformer (MobileViT) for real-time detection of plant diseases, which we called plant-based MobileViT (PMVT). Our proposed model was designed to be highly accurate and low-cost, making it suitable for deployment on mobile devices with limited resources. Specifically, we replaced the convolution block in MobileViT with an inverted residual structure that employs a 7\u00d77 convolution kernel to effectively model long-distance dependencies between different leaves in plant disease images. Furthermore, inspired by the concept of multi-level attention in computer vision tasks, we integrated a convolutional block attention module (CBAM) into the standard ViT encoder. This integration allows the network to effectively avoid irrelevant information and focus on essential features. The PMVT network achieves reduced parameter counts compared to alternative networks on various mobile devices while maintaining high accuracy across different vision tasks. Extensive experiments on multiple agricultural datasets, including wheat, coffee, and rice, demonstrate that the proposed method outperforms the current best lightweight and heavyweight models. On the wheat dataset, PMVT achieves the highest accuracy of 93.6% using approximately 0.98 million (M) parameters. This accuracy is 1.6% higher than that of MobileNetV3. Under the same parameters, PMVT achieved an accuracy of 85.4% on the coffee dataset, surpassing SqueezeNet by 2.3%. Furthermore, out method achieved an accuracy of 93.1% on the rice dataset, surpassing MobileNetV3 by 3.4%. Additionally, we developed a plant disease diagnosis app and successfully used the trained PMVT model to identify plant disease in different scenarios.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a1e17a09b0df676f133c2175366ab7fcd0381ac1.pdf",
        "venue": "Frontiers in Plant Science",
        "citationCount": 60,
        "score": 30.0
    },
    "77b625e6dec2732b96ca8cf57d7a123497873e1d.pdf": {
        "title": "TransFG: A Cross-View Geo-Localization of Satellite and UAVs Imagery Pipeline Using Transformer-Based Feature Aggregation and Gradient Guidance",
        "authors": [
            "Hu Zhao",
            "Keyan Ren",
            "Tianyi Yue",
            "Chun Zhang",
            "Shuai Yuan"
        ],
        "published_date": "2024",
        "abstract": "Cross-view geo-localization of satellite and unmanned aerial vehicles (UAVs) imagery has attracted extensive attention due to its tremendous potential for global navigation satellite system (GNSS) denied navigation. However, inadequate feature representation across different views coupled with positional shifts and distance-scale uncertainty are key challenges. Most of the existing research mainly focused on extracting comprehensive and fine-grained information, yet effective feature representation and alignment should be imposed equal importance. In this article, we propose an innovative transformer-based pipeline TransFG for robust cross-view image matching, which incorporates feature aggregation (FA) and gradient guidance (GG) module. TransFG synergically takes advantage of FA and GG, achieving an effective balance in feature representation and alignment. Specifically, the proposed FA module implicitly learns salient features and dynamically aggregates contextual features from the vision transformer (ViT). The proposed GG module uses the gradient information of local features to further enhance the cross-view feature representation and aligns specific instances across different views. Extensive experiments demonstrate that our pipeline outperforms existing methods in cross-view geo-localization. It achieves an impressive improvement in R@1 and AP than the state-of-the-art (SOTA) methods. The code has been released at https://github.com/happyboy1234/TransFG.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/77b625e6dec2732b96ca8cf57d7a123497873e1d.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 29,
        "score": 29.0
    },
    "e91934d66d9133d854ff0a4cafbe7966584bbf97.pdf": {
        "title": "Quantitative regularization in robust vision transformer for remote sensing image classification",
        "authors": [
            "Huaxiang Song",
            "Yuxuan Yuan",
            "Zhiwei Ouyang",
            "Yu Yang",
            "Hui Xiang"
        ],
        "published_date": "2024",
        "abstract": "Vision Transformers (ViTs) are exceptional at vision tasks. However, when applied to remote sensing images (RSIs), existing methods often necessitate extensive modifications of ViTs to rival convolutional neural networks (CNNs). This requirement significantly impedes the application of ViTs in geosciences, particularly for researchers who lack the time for comprehensive model redesign. To address this issue, we introduce the concept of quantitative regularization (QR), designed to enhance the performance of ViTs in RSI classification. QR represents an effective algorithm that adeptly manages domain discrepancies in RSIs and can be integrated with any ViTs in transfer learning. We evaluated the effectiveness of QR using three ViT architectures: vanilla ViT, Swin\u2010ViT and Next\u2010ViT, on four datasets: AID30, NWPU45, AFGR50 and UCM21. The results reveal that our Next\u2010ViT model surpasses 39 other advanced methods published in the past 3\u2009years, maintaining robust performance even with a limited number of training samples. We also discovered that our ViT and Swin\u2010ViT achieve significantly higher accuracy and robustness compared to other methods using the same backbone. Our findings confirm that ViTs can be as effective as CNNs for RSI classification, regardless of the dataset size. Our approach exclusively employs open\u2010source ViTs and easily accessible training strategies. Consequently, we believe that our method can significantly lower the barriers for geoscience researchers intending to use ViT for RSI applications.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e91934d66d9133d854ff0a4cafbe7966584bbf97.pdf",
        "venue": "Photogrammetric Record",
        "citationCount": 28,
        "score": 28.0
    },
    "e37539f5c943a92ef56b49b7fa067bd976e418d4.pdf": {
        "title": "Swin Unet3D: a three-dimensional medical image segmentation network combining vision transformer and convolution",
        "authors": [
            "Yimin Cai",
            "Yuqing Long",
            "Zhenggong Han",
            "Mingkun Liu",
            "Yuchen Zheng",
            "Wei Yang",
            "Liming Chen"
        ],
        "published_date": "2023",
        "abstract": "Background Semantic segmentation of brain tumors plays a critical role in clinical treatment, especially for three-dimensional (3D) magnetic resonance imaging, which is often used in clinical practice. Automatic segmentation of the 3D structure of brain tumors can quickly help physicians understand the properties of tumors, such as the shape and size, thus improving the efficiency of preoperative planning and the odds of successful surgery. In past decades, 3D convolutional neural networks (CNNs) have dominated automatic segmentation methods for 3D medical images, and these network structures have achieved good results. However, to reduce the number of neural network parameters, practitioners ensure that the size of convolutional kernels in 3D convolutional operations generally does not exceed $$7 \\times 7 \\times 7$$ 7 \u00d7 7 \u00d7 7 , which also leads to CNNs showing limitations in learning long-distance dependent information. Vision Transformer (ViT) is very good at learning long-distance dependent information in images, but it suffers from the problems of many parameters. What\u2019s worse, the ViT cannot learn local dependency information in the previous layers under the condition of insufficient data. However, in the image segmentation task, being able to learn this local dependency information in the previous layers makes a big impact on the performance of the model. Methods This paper proposes the Swin Unet3D model, which represents voxel segmentation on medical images as a sequence-to-sequence prediction. The feature extraction sub-module in the model is designed as a parallel structure of Convolution and ViT so that all layers of the model are able to adequately learn both global and local dependency information in the image. Results On the validation dataset of Brats2021, our proposed model achieves dice coefficients of 0.840, 0.874, and 0.911 on the ET channel, TC channel, and WT channel, respectively. On the validation dataset of Brats2018, our model achieves dice coefficients of 0.716, 0.761, and 0.874 on the corresponding channels, respectively. Conclusion We propose a new segmentation model that combines the advantages of Vision Transformer and Convolution and achieves a better balance between the number of model parameters and segmentation accuracy. The code can be found at https://github.com/1152545264/SwinUnet3D .",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e37539f5c943a92ef56b49b7fa067bd976e418d4.pdf",
        "venue": "BMC Medical Informatics and Decision Making",
        "citationCount": 55,
        "score": 27.5
    },
    "8fc9a7f23c5cf57e17790ffb99b6acb11693c673.pdf": {
        "title": "An enhanced speech emotion recognition using vision transformer",
        "authors": [
            "S. Akinpelu",
            "Serestina Viriri",
            "A. Adegun"
        ],
        "published_date": "2024",
        "abstract": "In human\u2013computer interaction systems, speech emotion recognition (SER) plays a crucial role because it enables computers to understand and react to users\u2019 emotions. In the past, SER has significantly emphasised acoustic properties extracted from speech signals. The use of visual signals for enhancing SER performance, however, has been made possible by recent developments in deep learning and computer vision. This work utilizes a lightweight Vision Transformer (ViT) model to propose a novel method for improving speech emotion recognition. We leverage the ViT model\u2019s capabilities to capture spatial dependencies and high-level features in images which are adequate indicators of emotional states from mel spectrogram input fed into the model. To determine the efficiency of our proposed approach, we conduct a comprehensive experiment on two benchmark speech emotion datasets, the Toronto English Speech Set (TESS) and the Berlin Emotional Database (EMODB). The results of our extensive experiment demonstrate a considerable improvement in speech emotion recognition accuracy attesting to its generalizability as it achieved 98%, 91%, and 93% (TESS-EMODB) accuracy respectively on the datasets. The outcomes of the comparative experiment show that the non-overlapping patch-based feature extraction method substantially improves the discipline of speech emotion recognition. Our research indicates the potential for integrating vision transformer models into SER systems, opening up fresh opportunities for real-world applications requiring accurate emotion recognition from speech compared with other state-of-the-art techniques.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/8fc9a7f23c5cf57e17790ffb99b6acb11693c673.pdf",
        "venue": "Scientific Reports",
        "citationCount": 26,
        "score": 26.0
    },
    "7ddbb9723420cb474c58d4b6b6834d5ebe112049.pdf": {
        "title": "Hybrid Deep Learning EfficientNetV2 and Vision Transformer (EffNetV2-ViT) Model for Breast Cancer Histopathological Image Classification",
        "authors": [
            "Mansoor Hayat",
            "Nouman Ahmad",
            "Anam Nasir",
            "Zeeshan Ahmad Tariq"
        ],
        "published_date": "2024",
        "abstract": "Breast cancer remains a leading cause of death among women, highlighting the urgent need for effective detection methods. In recent years, AI-based techniques, including computer vision, machine learning, and deep learning, have gained significant popularity in the field of medical imaging. The healthcare industry has witnessed remarkable progress due to these AI techniques, particularly in the early detection of cancer, which can greatly impact patient outcomes and survival rates. This research introduces a new approach to identifying breast cancer by combining two advanced computer technologies: EfficientNetV2 and vision transformer, using a specific dataset called BreakHis. EfficientNetV2 is praised for its quick processing and efficient use of resources, making it an excellent tool for initially identify important information in the data. We utilize three variants of EfficientNetV2 (small, medium and large) in order to discern the crucial features. Afterwards, this information is processed by a transformer, a type of model excellent at classifying or sorting data, to determine if breast cancer is present. Our experiments show that this method, especially when using the largest version of EfficientNetV2 paired with the vision transformer, is highly effective in accurately identifying breast cancer. It reached an impressive accuracy of nearly 99.83% when deciding between two possible categories and 98.10% when distinguishing among eight categories. These promising results depict that combining these two technologies could be a powerful way to improve breast cancer detection accuracy.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7ddbb9723420cb474c58d4b6b6834d5ebe112049.pdf",
        "venue": "IEEE Access",
        "citationCount": 25,
        "score": 25.0
    },
    "c7c0c84fa505995cd888a8fbaa9def21cb84cca3.pdf": {
        "title": "Learning Adaptive and View-Invariant Vision Transformer for Real-Time UAV Tracking",
        "authors": [
            "Yongxin Li",
            "Mengyuan Liu",
            "You Wu",
            "Xucheng Wang",
            "Xiangyang Yang",
            "Shuiwang Li"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c7c0c84fa505995cd888a8fbaa9def21cb84cca3.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 25,
        "score": 25.0
    },
    "1af8c50b4c7329fb2273aa7b755b0f9e8223e2d2.pdf": {
        "title": "Multi-Class Skin Cancer Classification Using Vision Transformer Networks and Convolutional Neural Network-Based Pre-Trained Models",
        "authors": [
            "Muhammad Asad Arshed",
            "Shahzad Mumtaz",
            "Muhammad Ibrahim",
            "Saeed Ahmed",
            "Muhammad Tahir",
            "Muhammad Shafi"
        ],
        "published_date": "2023",
        "abstract": "Skin cancer, particularly melanoma, has been recognized as one of the most lethal forms of cancer. Detecting and diagnosing skin lesions accurately can be challenging due to the striking similarities between the various types of skin lesions, such as melanoma and nevi, especially when examining the color images of the skin. However, early diagnosis plays a crucial role in saving lives and reducing the burden on medical resources. Consequently, the development of a robust autonomous system for skin cancer classification becomes imperative. Convolutional neural networks (CNNs) have been widely employed over the past decade to automate cancer diagnosis. Nonetheless, the emergence of the Vision Transformer (ViT) has recently gained a considerable level of popularity in the field and has emerged as a competitive alternative to CNNs. In light of this, the present study proposed an alternative method based on the off-the-shelf ViT for identifying various skin cancer diseases. To evaluate its performance, the proposed method was compared with 11 CNN-based transfer learning methods that have been known to outperform other deep learning techniques that are currently in use. Furthermore, this study addresses the issue of class imbalance within the dataset, a common challenge in skin cancer classification. In addressing this concern, the proposed study leverages the vision transformer and the CNN-based transfer learning models to classify seven distinct types of skin cancers. Through our investigation, we have found that the employment of pre-trained vision transformers achieved an impressive accuracy of 92.14%, surpassing CNN-based transfer learning models across several evaluation metrics for skin cancer diagnosis.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1af8c50b4c7329fb2273aa7b755b0f9e8223e2d2.pdf",
        "venue": "Inf.",
        "citationCount": 50,
        "score": 25.0
    },
    "bca0adcba0c26c5a34b05795c5bfbc1948bf2fae.pdf": {
        "title": "Image segmentation using Vision Transformer for tunnel defect assessment",
        "authors": [
            "S. Qin",
            "Taiyue Qi",
            "Tang Deng",
            "Xiaodong Huang"
        ],
        "published_date": "2024",
        "abstract": "Existing tunnel detection methods include crack and water\u2010leakage segmentation networks. However, if the automated detection algorithm cannot process all defect cases, manual detection is required to eliminate potential risks. The existing intelligent detection methods lack a universal method that can accurately segment all types of defects, particularly when multiple defects are superimposed. To address this issue, a defect segmentation model is proposed based on Vision Transformer (ViT), which is completely different from the network structure of a convolutional neural network. The model proposes an adapter and a decoding head to improve the training effect of the transformer encoder, allowing it to be fitted to small\u2010scale datasets. In post\u2010processing, a method is proposed to quantify the threat level for the defects, with the aim of outputting qualitative results that simulate human observation. The model showed impressive results on a real\u2010world dataset containing 11,781 defect images collected from a real subway tunnel. The visualizing results proved that this method is effective and has uniform criteria for single, multiple, and comprehensive defects. Moreover, the tests proved that the proposed model has a significant advantage in the case of multiple\u2010defect superposition, and it achieved 93.77%, 88.36%, and 92.93% for mean accuracy (Acc), mean intersection over union, and mean F1\u2010score, respectively. With similar training parameters, the Acc of the proposed method is improved by more than 10% over the DeepLabv3+, Mask R\u2010convolutional neural network, and UPerNet\u2010R50 models and by more than 5% over the Swin Transformer and ViT\u2010Adapter. This study implemented a general method that can process all defect cases and output the threat evaluation results, thereby making more intelligent tunnel detection.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/bca0adcba0c26c5a34b05795c5bfbc1948bf2fae.pdf",
        "venue": "Comput. Aided Civ. Infrastructure Eng.",
        "citationCount": 25,
        "score": 25.0
    },
    "c4895869637f73154d608cdd817234b0dbcd3508.pdf": {
        "title": "Plant-CNN-ViT: Plant Classification with Ensemble of Convolutional Neural Networks and Vision Transformer",
        "authors": [
            "C. Lee",
            "K. Lim",
            "Yu Xuan Song",
            "Ali Alqahtani"
        ],
        "published_date": "2023",
        "abstract": "Plant leaf classification involves identifying and categorizing plant species based on leaf characteristics, such as patterns, shapes, textures, and veins. In recent years, research has been conducted to improve the accuracy of plant classification using machine learning techniques. This involves training models on large datasets of plant images and using them to identify different plant species. However, these models are limited by their reliance on large amounts of training data, which can be difficult to obtain for many plant species. To overcome this challenge, this paper proposes a Plant-CNN-ViT ensemble model that combines the strengths of four pre-trained models: Vision Transformer, ResNet-50, DenseNet-201, and Xception. Vision Transformer utilizes self-attention to capture dependencies and focus on important leaf features. ResNet-50 introduces residual connections, aiding in efficient training and hierarchical feature extraction. DenseNet-201 employs dense connections, facilitating information flow and capturing intricate leaf patterns. Xception uses separable convolutions, reducing the computational cost while capturing fine-grained details in leaf images. The proposed Plant-CNN-ViT was evaluated on four plant leaf datasets and achieved remarkable accuracy of 100.00%, 100.00%, 100.00%, and 99.83% on the Flavia dataset, Folio Leaf dataset, Swedish Leaf dataset, and MalayaKew Leaf dataset, respectively.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c4895869637f73154d608cdd817234b0dbcd3508.pdf",
        "venue": "Plants",
        "citationCount": 48,
        "score": 24.0
    },
    "64811427a4427588bb049a6a254446ddd2cafacc.pdf": {
        "title": "Multi-task approach based on combined CNN-transformer for efficient segmentation and classification of breast tumors in ultrasound images",
        "authors": [
            "Jaouad Tagnamas",
            "Hiba Ramadan",
            "Ali Yahyaouy",
            "H. Tairi"
        ],
        "published_date": "2024",
        "abstract": "Nowadays, inspired by the great success of Transformers in Natural Language Processing, many applications of Vision Transformers (ViTs) have been investigated in the field of medical image analysis including breast ultrasound (BUS) image segmentation and classification. In this paper, we propose an efficient multi-task framework to segment and classify tumors in BUS images using hybrid convolutional neural networks (CNNs)-ViTs architecture and Multi-Perceptron (MLP)-Mixer. The proposed method uses a two-encoder architecture with EfficientNetV2 backbone and an adapted ViT encoder to extract tumor regions in BUS images. The self-attention (SA) mechanism in the Transformer encoder allows capturing a wide range of high-level and complex features while the EfficientNetV2 encoder preserves local information in image. To fusion the extracted features, a Channel Attention Fusion (CAF) module is introduced. The CAF module selectively emphasizes important features from both encoders, improving the integration of high-level and local information. The resulting feature maps are reconstructed to obtain the segmentation maps using a decoder. Then, our method classifies the segmented tumor regions into benign and malignant using a simple and efficient classifier based on MLP-Mixer, that is applied for the first time, to the best of our knowledge, for the task of lesion classification in BUS images. Experimental results illustrate the outperformance of our framework compared to recent works for the task of segmentation by producing 83.42% in terms of Dice coefficient as well as for the classification with 86% in terms of accuracy.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/64811427a4427588bb049a6a254446ddd2cafacc.pdf",
        "venue": "Visual Computing for Industry, Biomedicine, and Art",
        "citationCount": 21,
        "score": 21.0
    },
    "7a0f92968c63613730b0c06b3bc3bd40d2666571.pdf": {
        "title": "Adaptive and Background-Aware Vision Transformer for Real-Time UAV Tracking",
        "authors": [
            "Shuiwang Li",
            "Yangxiang Yang",
            "Dan Zeng",
            "Xucheng Wang"
        ],
        "published_date": "2023",
        "abstract": "While discriminative correlation filters (DCF)-based trackers prevail in UAV tracking for their favorable efficiency, lightweight convolutional neural network (CNN)-based trackers using filter pruning have also demonstrated remarkable efficiency and precision. However, the use of pure vision transformer models (ViTs) for UAV tracking remains unexplored, which is a surprising finding given that ViTs have been shown to produce better performance and greater efficiency than CNNs in image classification. In this paper, we propose an efficient ViT-based tracking framework, Aba-ViTrack, for UAV tracking. In our framework, feature learning and template-search coupling are integrated into an efficient one-stream ViT to avoid an extra heavy relation modeling module. The proposed Aba-ViT exploits an adaptive and background-aware token computation method to reduce inference time. This approach adaptively discards tokens based on learned halting probabilities, which a priori are higher for background tokens than target ones. Extensive experiments on six UAV tracking benchmarks demonstrate that the proposed Aba-ViTrack achieves state-of-the-art performance in UAV tracking. Code is available at https://github.com/xyyang317/Aba-ViTrack.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7a0f92968c63613730b0c06b3bc3bd40d2666571.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 39,
        "score": 19.5
    },
    "136b0ef7bd7f642a285b1d61ddf7abfc7ddc026f.pdf": {
        "title": "Classification of Mobile-Based Oral Cancer Images Using the Vision Transformer and the Swin Transformer",
        "authors": [
            "Bofan Song",
            "D. Kc",
            "Rubin Yuchan Yang",
            "Shaobai Li",
            "Chicheng Zhang",
            "Rongguang Liang"
        ],
        "published_date": "2024",
        "abstract": "Simple Summary Transformer models, originally successful in natural language processing, have found application in computer vision, demonstrating promising results in tasks related to cancer image analysis. Despite being one of the prevalent and swiftly spreading cancers globally, there is a pressing need for accurate automated analysis methods for oral cancer. This need is particularly critical for high-risk populations residing in low- and middle-income countries. In this study, we evaluated the performance of the Vision Transformer (ViT) and the Swin Transformer in the classification of mobile-based oral cancer images we collected from high-risk populations. The results showed that the Swin Transformer model achieved higher accuracy than the ViT model, and both transformer models work better than the conventional convolution model VGG19. Abstract Oral cancer, a pervasive and rapidly growing malignant disease, poses a significant global health concern. Early and accurate diagnosis is pivotal for improving patient outcomes. Automatic diagnosis methods based on artificial intelligence have shown promising results in the oral cancer field, but the accuracy still needs to be improved for realistic diagnostic scenarios. Vision Transformers (ViT) have outperformed learning CNN models recently in many computer vision benchmark tasks. This study explores the effectiveness of the Vision Transformer and the Swin Transformer, two cutting-edge variants of the transformer architecture, for the mobile-based oral cancer image classification application. The pre-trained Swin transformer model achieved 88.7% accuracy in the binary classification task, outperforming the ViT model by 2.3%, while the conventional convolutional network model VGG19 and ResNet50 achieved 85.2% and 84.5% accuracy. Our experiments demonstrate that these transformer-based architectures outperform traditional convolutional neural networks in terms of oral cancer image classification, and underscore the potential of the ViT and the Swin Transformer in advancing the state of the art in oral cancer image analysis.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/136b0ef7bd7f642a285b1d61ddf7abfc7ddc026f.pdf",
        "venue": "Cancers",
        "citationCount": 19,
        "score": 19.0
    },
    "cf439db0e071f19305ea1755aa108acdde73ed99.pdf": {
        "title": "Enhancing Melanoma Diagnosis with Advanced Deep Learning Models Focusing on Vision Transformer, Swin Transformer, and ConvNeXt",
        "authors": [
            "Serra Aksoy",
            "P. Demircio\u011flu",
            "I. Bogrekci"
        ],
        "published_date": "2024",
        "abstract": "Skin tumors, especially melanoma, which is highly aggressive and progresses quickly to other sites, are an issue in various parts of the world. Nevertheless, the one and only way to save lives is to detect it at its initial stages. This study explores the application of advanced deep learning models for classifying benign and malignant melanoma using dermoscopic images. The aim of the study is to enhance the accuracy and efficiency of melanoma diagnosis with the ConvNeXt, Vision Transformer (ViT) Base-16, and Swin Transformer V2 Small (Swin V2 S) deep learning models. The ConvNeXt model, which integrates principles of both convolutional neural networks and transformers, demonstrated superior performance, with balanced precision and recall metrics. The dataset, sourced from Kaggle, comprises 13,900 uniformly sized images, preprocessed to standardize the inputs for the models. Experimental results revealed that ConvNeXt achieved the highest diagnostic accuracy among the tested models. Experimental results revealed that ConvNeXt achieved an accuracy of 91.5%, with balanced precision and recall rates of 90.45% and 92.8% for benign cases, and 92.61% and 90.2% for malignant cases, respectively. The F1-scores for ConvNeXt were 91.61% for benign cases and 91.39% for malignant cases. This research points out the potential of hybrid deep learning architectures in medical image analysis, particularly for early melanoma detection.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/cf439db0e071f19305ea1755aa108acdde73ed99.pdf",
        "venue": "Dermatopathology",
        "citationCount": 19,
        "score": 19.0
    },
    "ffc855594cad345ea5a1cce2ee27095bec767bc8.pdf": {
        "title": "Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention",
        "authors": [
            "Saebom Leem",
            "Hyunseok Seo"
        ],
        "published_date": "2024",
        "abstract": "Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ffc855594cad345ea5a1cce2ee27095bec767bc8.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 19,
        "score": 19.0
    },
    "838d7862215df504dde41496cbe6ee711a12ae9f.pdf": {
        "title": "Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning",
        "authors": [
            "Shiming Chen",
            "W. Hou",
            "Salman H. Khan",
            "F. Khan"
        ],
        "published_date": "2024",
        "abstract": "Zero-shot learning (ZSL) recognizes the unseen classes by conducting visual-semantic interactions to transfer se-mantic knowledge from seen classes to unseen ones, sup-ported by semantic information (e.g., attributes). However, existing ZSL methods simply extract visual features using a pre-trained network backbone (i.e., CNN or ViT), which fail to learn matched visual-semantic correspondences for rep-resenting semantic-related visual features as lacking of the guidance of semantic information, resulting in undesirable visual-semantic interactions. To tackle this issue, we pro-pose a progressive semantic-guided vision transformer for zero-shot learning (dubbed ZSLViT). ZSLViT mainly consid-ers two properties in the whole network: i) discover the semantic-related visual representations explicitly, and ii) discard the semantic-unrelated visual information. Specif-ically, we first introduce semantic-embedded token learning to improve the visual-semantic correspondences via semantic enhancement and discover the semantic-related visual tokens explicitly with semantic-guided token attention. Then, we fuse low semantic-visual correspondence visual tokens to discard the semantic-unrelated visual in-formation for visual enhancement. These two operations are integrated into various encoders to progressively learn semantic-related visual representations for accurate visual-semantic interactions in ZSL. The extensive experiments show that our ZSLViT achieves significant performance gains on three popular benchmark datasets, i.e., CUB, SUN, and AWA2.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/838d7862215df504dde41496cbe6ee711a12ae9f.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 19,
        "score": 19.0
    },
    "9a35a0aac04f4f712f38ea9bbfd2f0b0108242f5.pdf": {
        "title": "MMST-ViT: Climate Change-aware Crop Yield Prediction via Multi-Modal Spatial-Temporal Vision Transformer",
        "authors": [
            "Fudong Lin",
            "Summer Crawford",
            "Kaleb Guillot",
            "Yihe Zhang",
            "Yan Chen",
            "Xu Yuan",
            "Li Chen",
            "Shelby Williams",
            "Robert Minvielle",
            "Xiangming Xiao",
            "Drew Gholson",
            "Nicolas Ashwell",
            "Tri Setiyono",
            "B. Tubana",
            "Lu Peng",
            "Magdy A. Bayoumi",
            "N. Tzeng"
        ],
        "published_date": "2023",
        "abstract": "Precise crop yield prediction provides valuable information for agricultural planning and decision-making processes. However, timely predicting crop yields remains challenging as crop growth is sensitive to growing season weather variation and climate change. In this work, we develop a deep learning-based solution, namely Multi-Modal Spatial-Temporal Vision Transformer (MMST-ViT), for predicting crop yields at the county level across the United States, by considering the effects of short-term meteorological variations during the growing season and the long-term climate change on crops. Specifically, our MMST-ViT consists of a Multi-Modal Transformer, a Spatial Transformer, and a Temporal Transformer. The Multi-Modal Transformer leverages both visual remote sensing data and short-term meteorological data for modeling the effect of growing season weather variations on crop growth. The Spatial Transformer learns the high-resolution spatial dependency among counties for accurate agricultural tracking. The Temporal Transformer captures the long-range temporal dependency for learning the impact of long-term climate change on crops. Meanwhile, we also devise a novel multi-modal contrastive learning technique to pre-train our model without extensive human supervision. Hence, our MMST-ViT captures the impacts of both short-term weather variations and long-term climate change on crops by leveraging both satellite images and meteorological data. We have conducted extensive experiments on over 200 counties in the United States, with the experimental results exhibiting that our MMST-ViT outperforms its counterparts under three performance metrics of interest. Our dataset and code are available at https://github.com/fudong03/MMST-ViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9a35a0aac04f4f712f38ea9bbfd2f0b0108242f5.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 37,
        "score": 18.5
    },
    "f6bf7787115affe22c410eb5b2606269912d59a0.pdf": {
        "title": "H-ViT: A Hierarchical Vision Transformer for Deformable Image Registration",
        "authors": [
            "Morteza Ghahremani",
            "Mohammad Khateri",
            "Bailiang Jian",
            "B. Wiestler",
            "Ehsan Adeli",
            "Christian Wachinger"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f6bf7787115affe22c410eb5b2606269912d59a0.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 18,
        "score": 18.0
    },
    "69e56df0ae079c83607bb48e68f8db39a4177cd0.pdf": {
        "title": "GiT: Towards Generalist Vision Transformer through Universal Language Interface",
        "authors": [
            "Haiyang Wang",
            "Hao Tang",
            "Li Jiang",
            "Shaoshuai Shi",
            "Muhammad Ferjad Naeem",
            "Hongsheng Li",
            "B. Schiele",
            "Liwei Wang"
        ],
        "published_date": "2024",
        "abstract": "This paper proposes a simple, yet effective framework, called GiT, simultaneously applicable for various vision tasks only with a vanilla ViT. Motivated by the universality of the Multi-layer Transformer architecture (e.g, GPT) widely used in large language models (LLMs), we seek to broaden its scope to serve as a powerful vision foundation model (VFM). However, unlike language modeling, visual tasks typically require specific modules, such as bounding box heads for detection and pixel decoders for segmentation, greatly hindering the application of powerful multi-layer transformers in the vision domain. To solve this, we design a universal language interface that empowers the successful auto-regressive decoding to adeptly unify various visual tasks, from image-level understanding (e.g., captioning), over sparse perception (e.g., detection), to dense prediction (e.g., segmentation). Based on the above designs, the entire model is composed solely of a ViT, without any specific additions, offering a remarkable architectural simplification. GiT is a multi-task visual model, jointly trained across five representative benchmarks without task-specific fine-tuning. Interestingly, our GiT builds a new benchmark in generalist performance, and fosters mutual enhancement across tasks, leading to significant improvements compared to isolated training. This reflects a similar impact observed in LLMs. Further enriching training with 27 datasets, GiT achieves strong zero-shot results over various tasks. Due to its simple design, this paradigm holds promise for narrowing the architectural gap between vision and language. Code and models will be available at \\url{https://github.com/Haiyang-W/GiT}.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/69e56df0ae079c83607bb48e68f8db39a4177cd0.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 18,
        "score": 18.0
    },
    "1528bab641830ea279bdf53ff9e7d11d2b7e5028.pdf": {
        "title": "Improving the Concrete Crack Detection Process via a Hybrid Visual Transformer Algorithm",
        "authors": [
            "Mohammad Shahin",
            "F. F. Chen",
            "Mazdak Maghanaki",
            "Aliakbar Hosseinzadeh",
            "Neda Zand",
            "Hamid Khodadadi Koodiani"
        ],
        "published_date": "2024",
        "abstract": "Inspections of concrete bridges across the United States represent a significant commitment of resources, given their biannual mandate for many structures. With a notable number of aging bridges, there is an imperative need to enhance the efficiency of these inspections. This study harnessed the power of computer vision to streamline the inspection process. Our experiment examined the efficacy of a state-of-the-art Visual Transformer (ViT) model combined with distinct image enhancement detector algorithms. We benchmarked against a deep learning Convolutional Neural Network (CNN) model. These models were applied to over 20,000 high-quality images from the Concrete Images for Classification dataset. Traditional crack detection methods often fall short due to their heavy reliance on time and resources. This research pioneers bridge inspection by integrating ViT with diverse image enhancement detectors, significantly improving concrete crack detection accuracy. Notably, a custom-built CNN achieves over 99% accuracy with substantially lower training time than ViT, making it an efficient solution for enhancing safety and resource conservation in infrastructure management. These advancements enhance safety by enabling reliable detection and timely maintenance, but they also align with Industry 4.0 objectives, automating manual inspections, reducing costs, and advancing technological integration in public infrastructure management.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1528bab641830ea279bdf53ff9e7d11d2b7e5028.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 18,
        "score": 18.0
    },
    "b43bb480caad36ab6fd667570275d42fe9050175.pdf": {
        "title": "WeakTr: Exploring Plain Vision Transformer for Weakly-supervised Semantic Segmentation",
        "authors": [
            "Liang Zhu",
            "Yingyue Li",
            "Jiemin Fang",
            "Yang Liu",
            "Hao Xin",
            "Wenyu Liu",
            "Xinggang Wang"
        ],
        "published_date": "2023",
        "abstract": "This paper explores the properties of the plain Vision Transformer (ViT) for Weakly-supervised Semantic Segmentation (WSSS). The class activation map (CAM) is of critical importance for understanding a classification network and launching WSSS. We observe that different attention heads of ViT focus on different image areas. Thus a novel weight-based method is proposed to end-to-end estimate the importance of attention heads, while the self-attention maps are adaptively fused for high-quality CAM results that tend to have more complete objects. Besides, we propose a ViT-based gradient clipping decoder for online retraining with the CAM results to complete the WSSS task. We name this plain Transformer-based Weakly-supervised learning framework WeakTr. It achieves the state-of-the-art WSSS performance on standard benchmarks, i.e., 78.4% mIoU on the val set of PASCAL VOC 2012 and 50.3% mIoU on the val set of COCO 2014. Code is available at https://github.com/hustvl/WeakTr.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b43bb480caad36ab6fd667570275d42fe9050175.pdf",
        "venue": "arXiv.org",
        "citationCount": 36,
        "score": 18.0
    },
    "1970ace992d742bdf098de08a82817b05ef87477.pdf": {
        "title": "Rethinking Vision Transformer and Masked Autoencoder in Multimodal Face Anti-Spoofing",
        "authors": [
            "Zitong Yu",
            "Rizhao Cai",
            "Yawen Cui",
            "Xin Liu",
            "Yongjian Hu",
            "A. Kot"
        ],
        "published_date": "2023",
        "abstract": "Recently, vision transformer (ViT) based multimodal learning methods have been proposed to improve the robustness of face anti-spoofing (FAS) systems. However, there are still no works to explore the fundamental natures (e.g., modality-aware inputs, suitable multimodal pre-training, and efficient finetuning) in vanilla ViT for multimodal FAS. In this paper, we investigate three key factors (i.e., inputs, pre-training, and finetuning) in ViT for multimodal FAS with RGB, Infrared (IR), and Depth. First, in terms of the ViT inputs, we find that leveraging local feature descriptors (such as histograms of oriented gradients) benefits the ViT on IR modality but not RGB or Depth modalities. Second, in consideration of the task (FAS vs. generic object classification) and modality (multimodal vs. unimodal) gaps, ImageNet pre-trained models might be sub-optimal for the multimodal FAS task. Finally, in observation of the inefficiency on direct finetuning the whole or partial ViT, we design an adaptive multimodal adapter (AMA), which can efficiently aggregate local multimodal features while freezing majority of ViT parameters. To bridge these gaps, we propose the modality-asymmetric masked autoencoder (M$$^{2}$$\n \n \n 2\n \n A$$^{2}$$\n \n \n 2\n \n E) for multimodal FAS self-supervised pre-training without costly annotated labels. Compared with the previous modality-symmetric autoencoder, the proposed M$$^{2}$$\n \n \n 2\n \n A$$^{2}$$\n \n \n 2\n \n E is able to learn more intrinsic task-aware representation and compatible with modality-agnostic (e.g., unimodal, bimodal, and trimodal) downstream settings. Extensive experiments with both unimodal (RGB, Depth, IR) and multimodal (RGB+Depth, RGB+IR, Depth+IR, RGB+Depth+IR) settings conducted on multimodal FAS benchmarks demonstrate the superior performance of the proposed methods. One highlight is that the proposed method is robust under various missing-modality cases where previous multimodal FAS models suffer serious performance drops. We hope these findings and solutions can facilitate the future research for ViT-based multimodal FAS.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1970ace992d742bdf098de08a82817b05ef87477.pdf",
        "venue": "International Journal of Computer Vision",
        "citationCount": 35,
        "score": 17.5
    },
    "fec86abbb29b559c1eaff31428f5b59f8070bb67.pdf": {
        "title": "Optimization of vision transformer-based detection of lung diseases from chest X-ray images",
        "authors": [
            "Jinsol Ko",
            "Soyeon Park",
            "H. G. Woo"
        ],
        "published_date": "2024",
        "abstract": "Background Recent advances in Vision Transformer (ViT)-based deep learning have significantly improved the accuracy of lung disease prediction from chest X-ray images. However, limited research exists on comparing the effectiveness of different optimizers for lung disease prediction within ViT models. This study aims to systematically evaluate and compare the performance of various optimization methods for ViT-based models in predicting lung diseases from chest X-ray images. Methods This study utilized a chest X-ray image dataset comprising 19,003 images containing both normal cases and six lung diseases: COVID-19, Viral Pneumonia, Bacterial Pneumonia, Middle East Respiratory Syndrome (MERS), Severe Acute Respiratory Syndrome (SARS), and Tuberculosis. Each ViT model (ViT, FastViT, and CrossViT) was individually trained with each optimization method (Adam, AdamW, NAdam, RAdam, SGDW, and Momentum) to assess their performance in lung disease prediction. Results When tested with ViT on the dataset with balanced-sample sized classes, RAdam demonstrated superior accuracy compared to other optimizers, achieving 95.87%. In the dataset with imbalanced sample size, FastViT with NAdam achieved the best performance with an accuracy of 97.63%. Conclusions We provide comprehensive optimization strategies for developing ViT-based model architectures, which can enhance the performance of these models for lung disease prediction from chest X-ray images. Supplementary Information The online version contains supplementary material available at 10.1186/s12911-024-02591-3.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/fec86abbb29b559c1eaff31428f5b59f8070bb67.pdf",
        "venue": "BMC Medical Informatics Decis. Mak.",
        "citationCount": 17,
        "score": 17.0
    },
    "cb8b0eba078098000f004d7e0f97a33189261f30.pdf": {
        "title": "An Unsupervised Method for Industrial Image Anomaly Detection with Vision Transformer-Based Autoencoder",
        "authors": [
            "Qiying Yang",
            "Rongzuo Guo"
        ],
        "published_date": "2024",
        "abstract": "Existing industrial image anomaly detection techniques predominantly utilize codecs based on convolutional neural networks (CNNs). However, traditional convolutional autoencoders are limited to local features, struggling to assimilate global feature information. CNNs\u2019 generalizability enables the reconstruction of certain anomalous regions. This is particularly evident when normal and abnormal regions, despite having similar pixel values, contain different semantic information, leading to ineffective anomaly detection. Furthermore, collecting abnormal image samples during actual industrial production poses challenges, often resulting in data imbalance. To mitigate these issues, this study proposes an unsupervised anomaly detection model employing the Vision Transformer (ViT) architecture, incorporating a Transformer structure to understand the global context between image blocks, thereby extracting a superior representation of feature information. It integrates a memory module to catalog normal sample features, both to counteract anomaly reconstruction issues and bolster feature representation, and additionally introduces a coordinate attention (CA) mechanism to intensify focus on image features at both spatial and channel dimensions, minimizing feature information loss and thereby enabling more precise anomaly identification and localization. Experiments conducted on two public datasets, MVTec AD and BeanTech AD, substantiate the method\u2019s effectiveness, demonstrating an approximate 20% improvement in average AUROC% at the image level over traditional convolutional encoders.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/cb8b0eba078098000f004d7e0f97a33189261f30.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 17,
        "score": 17.0
    },
    "9b4d81736637392adabe688b6a698cec58f9ce57.pdf": {
        "title": "Vision Transformer Model for Predicting the Severity of Diabetic Retinopathy in Fundus Photography-Based Retina Images",
        "authors": [
            "Waleed Nazih",
            "Ahmad O. Aseeri",
            "Osama Youssef Atallah",
            "Shaker El-Sappagh"
        ],
        "published_date": "2023",
        "abstract": "Diabetic Retinopathy (DR) is a result of prolonged diabetes with poor blood sugar management. It causes vision problems and blindness due to the deformation of the human retina. Recently, DR has become a crucial medical problem that affects the health and life of people. Diagnosis of DR can be done manually by ophthalmologists, but this is cumbersome and time consuming especially in the current overloaded physician\u2019s environment. The early detection and prevention of DR, a severe complication of diabetes that can lead to blindness, require an automatic, accurate, and personalized machine learning-based method. Various deep learning algorithms, particularly convolutional neural networks (CNNs), have been investigated for detecting different stages of DR. Recently, transformers have proved their capabilities in natural language processing. Vision transformers (ViTs) are extensions of these models to capture long-range dependencies in images, which achieved better results than CNN models. However, ViT always needs huge datasets to learn properly, and this condition reduced its applicability in DR domain. Recently, a new real-world and large fundus image dataset called fine-grained annotated diabetic retinopathy (FGADR) has been released which supported the application of ViT in DR diagnosis domain. The literature has not explored FGADR to optimize ViT models. In this paper, we propose a novel ViT based deep learning pipeline for detecting the severity stages of DR based on fundus photography-based retina images. The model has been built using FGADR dataset. The model has been optimized using a new optimizer called AdamW to detect the global context of images. Because FGADR is an imbalanced dataset, we combine several techniques for handling this issue including the usage of F1-score as the optimization metric, data augmentation, class weights, label smoothing, and focal loss. Extensive experiments have been conducted to explore the role of ViT with different data balancing techniques to detect DR. In addition, the proposed model has been compared with the state-of-the-art CNN algorithms such as ResNet50, Incep-tionV3, and VGG19. The adopted model was able to capture the crucial features of retinal images to understand DR severity better. It achieved superior results compared to other CNN and baseline ViT models (i.e., 0.825, 0.825, 0.826, 0.964, 0.825, 0.825, and 0.956 for F1-score, accuracy, balanced accuracy, AUC, precision, recall, specificity, respectively). The results of the proposed ViT model were quite encouraging to be applied in real medical environment for assisting physicians to make accurate, personalized, and timely decisions.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9b4d81736637392adabe688b6a698cec58f9ce57.pdf",
        "venue": "IEEE Access",
        "citationCount": 34,
        "score": 17.0
    },
    "981970d0f586761e7cdd978670c6a8f46990f514.pdf": {
        "title": "DAT++: Spatially Dynamic Vision Transformer with Deformable Attention",
        "authors": [
            "Zhuofan Xia",
            "Xuran Pan",
            "Shiji Song",
            "Li Erran Li",
            "Gao Huang"
        ],
        "published_date": "2023",
        "abstract": "Transformers have shown superior performance on various vision tasks. Their large receptive field endows Transformer models with higher representation power than their CNN counterparts. Nevertheless, simply enlarging the receptive field also raises several concerns. On the one hand, using dense attention in ViT leads to excessive memory and computational cost, and features can be influenced by irrelevant parts that are beyond the region of interests. On the other hand, the handcrafted attention adopted in PVT or Swin Transformer is data agnostic and may limit the ability to model long-range relations. To solve this dilemma, we propose a novel deformable multi-head attention module, where the positions of key and value pairs in self-attention are adaptively allocated in a data-dependent way. This flexible scheme enables the proposed deformable attention to dynamically focus on relevant regions while maintains the representation power of global attention. On this basis, we present Deformable Attention Transformer (DAT), a general vision backbone efficient and effective for visual recognition. We further build an enhanced version DAT++. Extensive experiments show that our DAT++ achieves state-of-the-art results on various visual recognition benchmarks, with 85.9% ImageNet accuracy, 54.5 and 47.0 MS-COCO instance segmentation mAP, and 51.5 ADE20K semantic segmentation mIoU.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/981970d0f586761e7cdd978670c6a8f46990f514.pdf",
        "venue": "arXiv.org",
        "citationCount": 34,
        "score": 17.0
    },
    "6bdafb965e94c5240db2c30f20c37c4b4dd0e451.pdf": {
        "title": "ViTA: A Vision Transformer Inference Accelerator for Edge Applications",
        "authors": [
            "Shashank Nag",
            "G. Datta",
            "Souvik Kundu",
            "N. Chandrachoodan",
            "P. Beerel"
        ],
        "published_date": "2023",
        "abstract": "Vision Transformer models, such as ViT, Swin Transformer, and Transformer-in-Transformer, have recently gained significant traction in computer vision tasks due to their ability to capture the global relation between features which leads to superior performance. However, they are compute-heavy and difficult to deploy in resource-constrained edge devices. Existing hardware accelerators, including those for the closely-related BERT transformer models, do not target highly resource-constrained environments. In this paper, we address this gap and propose ViTA - a configurable hardware accelerator for inference of vision transformer models, targeting resource-constrained edge computing devices and avoiding repeated off-chip memory accesses. We employ a head-level pipeline and inter-layer MLP optimizations, and can support several commonly used vision transformer models with changes solely in our control logic. We achieve nearly 90% hardware utilization efficiency on most vision transformer models, report a power of 0.88W when synthesised with a clock of 150 MHz, and get reasonable frame rates - all of which makes ViTA suitable for edge applications.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6bdafb965e94c5240db2c30f20c37c4b4dd0e451.pdf",
        "venue": "International Symposium on Circuits and Systems",
        "citationCount": 34,
        "score": 17.0
    },
    "a3d1cebf99262cc20d22863b9540769b49a15ede.pdf": {
        "title": "Deep Transformer-Based Asset Price and Direction Prediction",
        "authors": [
            "Abdul Haluk Batur Gezici",
            "Emre Sefer"
        ],
        "published_date": "2024",
        "abstract": "The field of algorithmic trading, driven by deep learning methodologies, has garnered substantial attention in recent times. Within this domain, transformers, convolutional neural networks, and patch embedding-based techniques have emerged as popular choices within the computer vision community. Here, inspired by the latest cutting-edge computer vision methodologies and the existing work showing the capability of image-like conversion for time-series datasets, we apply more advanced transformer-based and patch-based approaches for predicting asset prices and directional price movements. The employed transformer models include Vision Transformer (ViT), Data Efficient Image Transformers (DeiT), and Swin. We use ConvMixer for a patch embedding-based convolutional neural network architecture without a transformer. Our tested transformer-based and patch-based methodologies aim to predict asset prices and directional movements using historical price data by leveraging the inherent image-like properties within the historical time-series dataset. Before the implementation of attention-based architectures, the historical time series price dataset is transformed into two-dimensional images. This transformation is facilitated through the incorporation of various common technical financial indicators, each contributing to the data for a fixed number of consecutive days. Consequently, a diverse set of two-dimensional images is constructed, reflecting various dimensions of the dataset. Subsequently, the original images depicting market valleys and peaks are annotated with labels such as Hold, Buy, or Sell. According to the experiments, trained attention-based models consistently outperform the baseline convolutional architectures, particularly when applied to a subset of frequently traded Exchange-Traded Funds (ETFs). This better performance of attention-based architectures, especially ViT, is evident in terms of both accuracy and other financial evaluation metrics, particularly during extended testing and holding periods. These findings underscore the potential of transformer-based approaches to enhance predictive capabilities in asset price and directional forecasting. Our code and processed datasets are available at https://github.com/seferlab/price_transformer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a3d1cebf99262cc20d22863b9540769b49a15ede.pdf",
        "venue": "IEEE Access",
        "citationCount": 16,
        "score": 16.0
    },
    "f05176e2d3f7a6e95b60f0e96fd9bf20e8b335d2.pdf": {
        "title": "Efficient brain tumor segmentation using Swin transformer and enhanced local self-attention",
        "authors": [
            "Fethi Ghazouani",
            "Pierre Vera",
            "Su Ruan"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f05176e2d3f7a6e95b60f0e96fd9bf20e8b335d2.pdf",
        "venue": "International Journal of Computer Assisted Radiology and Surgery",
        "citationCount": 30,
        "score": 15.0
    },
    "442b5ec3aad4b099e71d6203a62eb7ec7519544c.pdf": {
        "title": "P2FEViT: Plug-and-Play CNN Feature Embedded Hybrid Vision Transformer for Remote Sensing Image Classification",
        "authors": [
            "Guanqun Wang",
            "He Chen",
            "Liang Chen",
            "Yin Zhuang",
            "Shanghang Zhang",
            "T. Zhang",
            "Hao Dong",
            "Peng Gao"
        ],
        "published_date": "2023",
        "abstract": "Remote sensing image classification (RSIC) is a classical and fundamental task in the intelligent interpretation of remote sensing imagery, which can provide unique labeling information for each acquired remote sensing image. Thanks to the potent global context information extraction ability of the multi-head self-attention (MSA) mechanism, visual transformer (ViT)-based architectures have shown excellent capability in natural scene image classification. However, in order to achieve powerful RSIC performance, it is insufficient to capture global spatial information alone. Specifically, for fine-grained target recognition tasks with high inter-class similarity, discriminative and effective local feature representations are key to correct classification. In addition, due to the lack of inductive biases, the powerful global spatial context representation capability of ViT requires lengthy training procedures and large-scale pre-training data volume. To solve the above problems, a hybrid architecture of convolution neural network (CNN) and ViT is proposed to improve the RSIC ability, called P2FEViT, which integrates plug-and-play CNN features with ViT. In this paper, the feature representation capabilities of CNN and ViT applying for RSIC are first analyzed. Second, aiming to integrate the advantages of CNN and ViT, a novel approach embedding CNN features into the ViT architecture is proposed, which can make the model synchronously capture and fuse global context and local multimodal information to further improve the classification capability of ViT. Third, based on the hybrid structure, only a simple cross-entropy loss is employed for model training. The model can also have rapid and comfortable convergence with relatively less training data than the original ViT. Finally, extensive experiments are conducted on the public and challenging remote sensing scene classification dataset of NWPU-RESISC45 (NWPU-R45) and the self-built fine-grained target classification dataset called BIT-AFGR50. The experimental results demonstrate that the proposed P2FEViT can effectively improve the feature description capability and obtain outstanding image classification performance, while significantly reducing the high dependence of ViT on large-scale pre-training data volume and accelerating the convergence speed. The code and self-built dataset will be released at our webpages.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/442b5ec3aad4b099e71d6203a62eb7ec7519544c.pdf",
        "venue": "Remote Sensing",
        "citationCount": 30,
        "score": 15.0
    },
    "635675452852e838644516e1eeefd1aaa8c8ac07.pdf": {
        "title": "Dual Class Token Vision Transformer for Direction of Arrival Estimation in Low SNR",
        "authors": [
            "Yu Guo",
            "Zhi Zhang",
            "Yuzhen Huang"
        ],
        "published_date": "2024",
        "abstract": "In this letter, we propose a deep learning-based method for the direction of arrival (DOA) estimation in the low signal-to-noise ratio (SNR) scenario. Specifically, the DOA estimation is modeled as a multi-label classification task, and a novel dual class token Vision Transformer (DCT-ViT) is designed to fit it. Different from the classical ViT architecture with a single class token, the DCT-ViT includes two class tokens which are located at the beginning and end of the latent vector sequence, respectively. This architecture enables enhanced information mining and feature extraction from the array signal data in order to improve the accuracy of DOA estimation. Furthermore, a single DCT-ViT model can accommodate different source numbers by leveraging a training dataset with different numbers of sources. Simulation results illustrate that our proposed method outperforms existing methods in the low SNR scenario, including classical model-based and other deep learning-based methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/635675452852e838644516e1eeefd1aaa8c8ac07.pdf",
        "venue": "IEEE Signal Processing Letters",
        "citationCount": 15,
        "score": 15.0
    },
    "d2fce7480111d66a74caa801a236f71ab021c42c.pdf": {
        "title": "Vision Transformer With Hybrid Shifted Windows for Gastrointestinal Endoscopy Image Classification",
        "authors": [
            "Wei Wang",
            "Xin Yang",
            "Jinhui Tang"
        ],
        "published_date": "2023",
        "abstract": "Automated classification of gastrointestinal endoscope images can help reduce the workload of doctors and improve the accuracy of diagnoses. The rapidly developed vision Transformer, represented by Swin Transformer, has become an impressive technique for medical image classification. However, Swin Transformer cannot capture the long-range dependency well in complex gastrointestinal endoscopy images. As a result, it fails to represent features of some widely-spread targets in digestive tract images, such as normal-z-line and esophagitis, effectively. To solve this problem, we propose a novel vision Transformer model based on hybrid shifted windows for digestive tract image classification, which can obtain both short-range and long-range dependency concurrently. Extensive experiments demonstrate the superiority of our method to the state-of-the-art methods with a classification accuracy of 95.42% on the Kvasir v2 dataset and a classification accuracy of 86.81% on the HyperKvasir dataset.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d2fce7480111d66a74caa801a236f71ab021c42c.pdf",
        "venue": "IEEE transactions on circuits and systems for video technology (Print)",
        "citationCount": 30,
        "score": 15.0
    },
    "5135a8f690c66c3b64928227443c4f9378bd20e1.pdf": {
        "title": "A Lightweight Dual-Branch Swin Transformer for Remote Sensing Scene Classification",
        "authors": [
            "Fujian Zheng",
            "Shuai Lin",
            "Wei Zhou",
            "Hong Huang"
        ],
        "published_date": "2023",
        "abstract": "The main challenge of scene classification is to understand the semantic context information of high-resolution remote sensing images. Although vision transformer (ViT)-based methods have been explored to boost the long-range dependencies of high-resolution remote sensing images, the connectivity between neighboring windows is still limited. Meanwhile, ViT-based methods commonly contain a large number of parameters, resulting in a huge computational consumption. In this paper, a novel lightweight dual-branch swin transformer (LDBST) method for remote sensing scene classification is proposed, and the discriminative ability of scene features is increased through combining a ViT branch and convolutional neural network (CNN) branch. First, based on the hierarchical swin transformer model, LDBST divides the input features of each stage into two parts, which are then separately fed into the two branches. For the ViT branch, a dual multilayer perceptron structure with a depthwise convolutional layer, termed Conv-MLP, is integrated into the branch to boost the connections with neighboring windows. Then, a simple-structured CNN branch with maximum pooling preserves the strong features of the scene feature map. Specifically, the CNN branch lightens the LDBST, by avoiding complex multi-head attention and multilayer perceptron computations. To obtain better feature representation, LDBST was pretrained on the large-scale remote scene classification images of the MLRSN and RSD46-WHU datasets. These two pretrained weights were fine-tuned on target scene classification datasets. The experimental results showed that the proposed LDBST method was more effective than some other advanced remote sensing scene classification methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/5135a8f690c66c3b64928227443c4f9378bd20e1.pdf",
        "venue": "Remote Sensing",
        "citationCount": 29,
        "score": 14.5
    },
    "77eea367f79e69995948699d806683c7731a60b1.pdf": {
        "title": "Gait-CNN-ViT: Multi-Model Gait Recognition with Convolutional Neural Networks and Vision Transformer",
        "authors": [
            "Jashila Nair Mogan",
            "C. Lee",
            "K. Lim",
            "M. Ali",
            "Ali Alqahtani"
        ],
        "published_date": "2023",
        "abstract": "Gait recognition, the task of identifying an individual based on their unique walking style, can be difficult because walking styles can be influenced by external factors such as clothing, viewing angle, and carrying conditions. To address these challenges, this paper proposes a multi-model gait recognition system that integrates Convolutional Neural Networks (CNNs) and Vision Transformer. The first step in the process is to obtain a gait energy image, which is achieved by applying an averaging technique to a gait cycle. The gait energy image is then fed into three different models, DenseNet-201, VGG-16, and a Vision Transformer. These models are pre-trained and fine-tuned to encode the salient gait features that are specific to an individual\u2019s walking style. Each model provides prediction scores for the classes based on the encoded features, and these scores are then summed and averaged to produce the final class label. The performance of this multi-model gait recognition system was evaluated on three datasets, CASIA-B, OU-ISIR dataset D, and OU-ISIR Large Population dataset. The experimental results showed substantial improvement compared to existing methods on all three datasets. The integration of CNNs and ViT allows the system to learn both the pre-defined and distinct features, providing a robust solution for gait recognition even under the influence of covariates.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/77eea367f79e69995948699d806683c7731a60b1.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 29,
        "score": 14.5
    },
    "861f670073679ba05990f3bc6d119b13ab62aca7.pdf": {
        "title": "PLG-ViT: Vision Transformer with Parallel Local and Global Self-Attention",
        "authors": [
            "Nikolas Ebert",
            "D. Stricker",
            "Oliver Wasenm\u00fcller"
        ],
        "published_date": "2023",
        "abstract": "Recently, transformer architectures have shown superior performance compared to their CNN counterparts in many computer vision tasks. The self-attention mechanism enables transformer networks to connect visual dependencies over short as well as long distances, thus generating a large, sometimes even a global receptive field. In this paper, we propose our Parallel Local-Global Vision Transformer (PLG-ViT), a general backbone model that fuses local window self-attention with global self-attention. By merging these local and global features, short- and long-range spatial interactions can be effectively and efficiently represented without the need for costly computational operations such as shifted windows. In a comprehensive evaluation, we demonstrate that our PLG-ViT outperforms CNN-based as well as state-of-the-art transformer-based architectures in image classification and in complex downstream tasks such as object detection, instance segmentation, and semantic segmentation. In particular, our PLG-ViT models outperformed similarly sized networks like ConvNeXt and Swin Transformer, achieving Top-1 accuracy values of 83.4%, 84.0%, and 84.5% on ImageNet-1K with 27M, 52M, and 91M parameters, respectively.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/861f670073679ba05990f3bc6d119b13ab62aca7.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 28,
        "score": 14.0
    },
    "f93f718bb2d3f5e4354e174e54e71c5596f56ea3.pdf": {
        "title": "Speech Swin-Transformer: Exploring a Hierarchical Transformer with Shifted Windows for Speech Emotion Recognition",
        "authors": [
            "Yong Wang",
            "Cheng Lu",
            "Hailun Lian",
            "Yan Zhao",
            "Bjorn Schuller",
            "Yuan Zong",
            "Wenming Zheng"
        ],
        "published_date": "2024",
        "abstract": "Swin-Transformer has demonstrated remarkable success in computer vision by leveraging its hierarchical feature representation based on Transformer. In speech signals, emotional information is distributed across different scales of speech features, e. g., word, phrase, and utterance. Drawing above inspiration, this paper presents a hierarchical speech Transformer with shifted windows to aggregate multi-scale emotion features for speech emotion recognition (SER), called Speech Swin-Transformer. Specifically, we first divide the speech spectrogram into segment-level patches in the time domain, composed of multiple frame patches. These segment-level patches are then encoded using a stack of Swin blocks, in which a local window Transformer is utilized to explore local inter-frame emotional information across frame patches of each segment patch. After that, we also design a shifted window Transformer to compensate for patch correlations near the boundaries of segment patches. Finally, we employ a patch merging operation to aggregate segment-level emotional features for hierarchical speech representation by expanding the receptive field of Transformer from frame-level to segment-level. Experimental results demonstrate that our proposed Speech Swin-Transformer outperforms the state-of-the-art methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f93f718bb2d3f5e4354e174e54e71c5596f56ea3.pdf",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "citationCount": 14,
        "score": 14.0
    },
    "c5c9005aae80795e241de18b595c2d01393808f8.pdf": {
        "title": "Galaxy morphology classification based on Convolutional vision Transformer (CvT)",
        "authors": [
            "Jie Cao",
            "Tingting Xu",
            "Yu-he Deng",
            "Linhua Deng",
            "Ming-cun Yang",
            "Zhi-jing Liu",
            "Weihong Zhou"
        ],
        "published_date": "2024",
        "abstract": "The classification of galaxy morphology is among the most active fields in astronomical research today. With the development of artificial intelligence technology, deep learning is a useful tool in the classification of black the morphology of galaxies and significant progress has been made in this domain. However, there is still some room for improvement in terms of classification accuracy, automation, and related issues. Convolutional vision Transformer (CvT) is an improved version of the Vision Transformer (ViT) model. It improves the performance of the ViT model by introducing a convolutional neural network (CNN). This study explores the performance of the CvT model in the area of galaxy morphology classification. In this work, the CvT model was applied, for the first time, in a black five-class classification task of galaxy morphology. We black added different types and degrees of noise to the original galaxy images to verify that the CvT model achieves good classification performance, even in galaxy images with low signal-to-noise ratios (S/Ns). Then, we also validated the classification performance of the CvT model for galaxy images at different redshifts based on the low-redshift dataset GZ2 and the high-redshift dataset Galaxy Zoo CANDELS. In addition, we black visualized and analyzed the classification results of the CvT model based on the t-distributed stochastic black neighborhood -embedding (t-SNE) algorithm. We find that (1) compared with other black five-class classification models of galaxy morphology based on CNN models, the average accuracy, precision, recall, and F1\\_score evaluation metrics of the CvT classification model are all higher than 98, which is an improvement of at least 1 compared with those based on CNNs; (2) the classification black visualization results show that different categories of galaxies are separated from each other in multi-dimensional space. The application of the CvT model to the classification study of galaxy morphology is a novel undertaking that carries important implications for future studies.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c5c9005aae80795e241de18b595c2d01393808f8.pdf",
        "venue": "Astronomy &amp; Astrophysics",
        "citationCount": 14,
        "score": 14.0
    },
    "14c42c0f2c94e0a1f4aa820886080263f9922047.pdf": {
        "title": "Vision transformer with masked autoencoders for referable diabetic retinopathy classification based on large-size retina image",
        "authors": [
            "Yaoming Yang",
            "Zhili Cai",
            "Shuxia Qiu",
            "Peng Xu"
        ],
        "published_date": "2024",
        "abstract": "Computer-aided diagnosis systems based on deep learning algorithms have shown potential applications in rapid diagnosis of diabetic retinopathy (DR). Due to the superior performance of Transformer over convolutional neural networks (CNN) on natural images, we attempted to develop a new model to classify referable DR based on a limited number of large-size retinal images by using Transformer. Vision Transformer (ViT) with Masked Autoencoders (MAE) was applied in this study to improve the classification performance of referable DR. We collected over 100,000 publicly fundus retinal images larger than 224\u00d7224, and then pre-trained ViT on these retinal images using MAE. The pre-trained ViT was applied to classify referable DR, the performance was also compared with that of ViT pre-trained using ImageNet. The improvement in model classification performance by pre-training with over 100,000 retinal images using MAE is superior to that pre-trained with ImageNet. The accuracy, area under curve (AUC), highest sensitivity and highest specificity of the present model are 93.42%, 0.9853, 0.973 and 0.9539, respectively. This study shows that MAE can provide more flexibility to the input image and substantially reduce the number of images required. Meanwhile, the pretraining dataset scale in this study is much smaller than ImageNet, and the pre-trained weights from ImageNet are not required also.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/14c42c0f2c94e0a1f4aa820886080263f9922047.pdf",
        "venue": "PLoS ONE",
        "citationCount": 14,
        "score": 14.0
    },
    "9998291d71f4e8ddf59f4b016b19df1f848eeed1.pdf": {
        "title": "EFFResNet-ViT: A Fusion-Based Convolutional and Vision Transformer Model for Explainable Medical Image Classification",
        "authors": [
            "Tahir Hussain",
            "Hayaru Shouno",
            "Abid Hussain",
            "Dostdar Hussain",
            "Muhammad Ismail",
            "Tatheer Hussain Mir",
            "Fang Rong Hsu",
            "Taukir Alam",
            "Shabnur Anonna Akhy"
        ],
        "published_date": "2025",
        "abstract": "The rapid advancement of medical imaging technologies requires the development of advanced, automated, and interpretable diagnostic tools for clinical decision-making. Although convolutional neural networks (CNNs) have shown significant promise in medical image analysis, they have limitations in capturing the global context and lack interpretability, thereby hindering their clinical adoption. This study presents EFFResNet-ViT, a novel hybrid deep learning (DL) model designed to address these challenges by combining EfficientNet-B0 and ResNet-50 CNN backbones with a vision transformer (ViT) module. The proposed architecture employs a feature fusion strategy to integrate the local feature extraction strengths of CNNs with the global dependency modeling capabilities of transformers. The extracted features are further refined through a post-transformer CNN and a global average pooling layer to enhance the classification performance. To improve interpretability, EFFResNet-ViT incorporates Grad-CAM visualization techniques to highlight regions contributing to classification decisions and employs t-distributed stochastic neighbor embedding for feature space analysis, providing insights into class separability. The proposed model was evaluated on two benchmark datasets: brain tumor (BT) CE-MRI for BT classification and a retinal image dataset for ophthalmological diagnosis. EFFResNet-ViT achieved state-of-the-art performance, with accuracies of 99.31% and 92.54% on the BT CE-MRI and retinal datasets, respectively. Comparative analyses demonstrate the superior classification performance and interpretability of EFFResNet-ViT over existing ViT and CNN-based hybrid models. The explainable design of EFFResNet-ViT addresses the critical need for transparency in artificial intelligence-driven medical diagnostics, facilitating its potential integration into clinical workflows to improve decision-making and patient outcomes.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9998291d71f4e8ddf59f4b016b19df1f848eeed1.pdf",
        "venue": "IEEE Access",
        "citationCount": 14,
        "score": 14.0
    },
    "9fa44f616e796f187d97b9cc324bd839bc21ad81.pdf": {
        "title": "SwinDepth: Unsupervised Depth Estimation using Monocular Sequences via Swin Transformer and Densely Cascaded Network",
        "authors": [
            "D. Shim",
            "H. J. Kim"
        ],
        "published_date": "2023",
        "abstract": "Monocular depth estimation plays a critical role in various computer vision and robotics applications such as localization, mapping, and 3D object detection. Recently, learning-based algorithms achieve huge success in depth estimation by training models with a large amount of data in a supervised manner. However, it is challenging to acquire dense ground truth depth labels for supervised training, and the unsupervised depth estimation using monocular sequences emerges as a promising alternative. Unfortunately, most studies on unsupervised depth estimation explore loss functions or occlusion masks, and there is little change in model architecture in that ConvNet-based encoder-decoder structure becomes a de-facto standard for depth estimation. In this paper, we employ a convolution-free Swin Transformer as an image feature extractor so that the network can capture both local geometric features and global semantic features for depth estimation. Also, we propose a Densely Cascaded Multi-scale Network (DCMNet) that connects every feature map directly with another from different scales via a top-down cascade pathway. This densely cascaded connectivity reinforces the interconnection between decoding layers and produces high-quality multi-scale depth outputs. The experiments on two different datasets, KITTI and Make3D, demonstrate that our proposed method outperforms existing state-of-the-art unsupervised algorithms.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9fa44f616e796f187d97b9cc324bd839bc21ad81.pdf",
        "venue": "IEEE International Conference on Robotics and Automation",
        "citationCount": 27,
        "score": 13.5
    },
    "d62e6ba83d3da5b75e1d428f20f8199e3754bd5d.pdf": {
        "title": "An Integrated Approach using YOLOv8 and ResNet, SeResNet & Vision Transformer (ViT) Algorithms based on ROI Fracture Prediction in X-ray Images of the Elbow.",
        "authors": [
            "Taukir Alam",
            "Wei-Cheng Yeh",
            "Fang Rong Hsu",
            "W. Shia",
            "Robert Singh",
            "Taimoor Hassan",
            "Wenru Lin",
            "Hong-Ye Yang",
            "Tahir Hussain"
        ],
        "published_date": "2024",
        "abstract": "INTRODUCTION\nIn this study, we harnessed three cutting-edge algorithms' capabilities to refine the elbow fracture prediction process through X-ray image analysis. Employing the YOLOv8 (You only look once) algorithm, we first identified Regions of Interest (ROI) within the X-ray images, significantly augmenting fracture prediction accuracy.\n\n\nMETHODS\nSubsequently, we integrated and compared the ResNet, the SeResNet (Squeeze-and-Excitation Residual Network) ViT (Vision Transformer) algorithms to refine our predictive capabilities. Furthermore, to ensure optimal precision, we implemented a series of meticulous refinements. This included recalibrating ROI regions to enable finer-grained identification of diagnostically significant areas within the X-ray images. Additionally, advanced image enhancement techniques were applied to optimize the X-ray images' visual quality and structural clarity.\n\n\nRESULTS\nThese methodological enhancements synergistically contributed to a substantial improvement in the overall accuracy of our fracture predictions. The dataset utilized for training, testing & validation, and comprehensive evaluation exclusively comprised elbow X-ray images, where predicting the fracture with three algorithms: Resnet50; accuracy 0.97, precision 1, recall 0.95, SeResnet50; accuracy 0.97, precision 1, recall 0.95 & ViTB- 16 with high accuracy of 0.99, precision same as the other two algorithms, with a recall of 0.95.\n\n\nCONCLUSION\nThis approach has the potential to increase the precision of diagnoses, lessen the burden of radiologists, easily integrate into current medical imaging systems, and assist clinical decision-making, all of which could lead to better patient care and health outcomes overall.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d62e6ba83d3da5b75e1d428f20f8199e3754bd5d.pdf",
        "venue": "Current medical imaging",
        "citationCount": 13,
        "score": 13.0
    },
    "d1faaa1d7d312dd5867683ce60519979de6b3349.pdf": {
        "title": "ViT-UperNet: a hybrid vision transformer with unified-perceptual-parsing network for medical image segmentation",
        "authors": [
            "Ruiping Yang",
            "Liu Kun",
            "Shaohua Xu",
            "Yin Jian",
            "Zhang Zhen"
        ],
        "published_date": "2024",
        "abstract": "The existing image semantic segmentation models have low accuracy in detecting tiny targets or multi-targets at overlapping regions. This work proposes a hybrid vision transformer with unified-perceptual-parsing network (ViT-UperNet) for medical image segmentation. A self-attention mechanism is embedded in a vision transformer to extract multi-level features. The image features are extracted hierarchically from low to high dimensions using 4 groups of Transformer blocks with different numbers. Then, it uses a unified-perceptual-parsing network based on a feature pyramid network (FPN) and a pyramid pooling module (PPM) for the fusion of multi-scale contextual features and semantic segmentation. FPN can naturally use hierarchical features, and generate strong semantic information on all scales. PPM can better use the global prior knowledge to understand complex scenes, and extract features with global context information to improve segmentation results. In the training process, a scalable self-supervised learner named masked autoencoder is used for pre-training, which strengthens the visual representation ability and improves the efficiency of the feature learning. Experiments are conducted on cardiac magnetic resonance image segmentation where the left and right atrium and ventricle are selected for segmentation. The pixels accuracy is 93.85%, the Dice coefficient is 92.61% and Hausdorff distance is 11.16, which are improved compared with the other methods. The results show the superiority of Vit-UperNet in medical images segmentation, especially for the low-recognition and serious-occlusion targets.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d1faaa1d7d312dd5867683ce60519979de6b3349.pdf",
        "venue": "Complex &amp; Intelligent Systems",
        "citationCount": 13,
        "score": 13.0
    },
    "d68aedc3a957c25e5f19438ec4b64fb53b48422c.pdf": {
        "title": "Multi-label classification of retinal disease via a novel vision transformer model",
        "authors": [
            "Dong Wang",
            "Jian Lian",
            "Wanzhen Jiao"
        ],
        "published_date": "2024",
        "abstract": "Introduction The precise identification of retinal disorders is of utmost importance in the prevention of both temporary and permanent visual impairment. Prior research has yielded encouraging results in the classification of retinal images pertaining to a specific retinal condition. In clinical practice, it is not uncommon for a single patient to present with multiple retinal disorders concurrently. Hence, the task of classifying retinal images into multiple labels remains a significant obstacle for existing methodologies, but its successful accomplishment would yield valuable insights into a diverse array of situations simultaneously. Methods This study presents a novel vision transformer architecture called retinal ViT, which incorporates the self-attention mechanism into the field of medical image analysis. To note that this study supposed to prove that the transformer-based models can achieve competitive performance comparing with the CNN-based models, hence the convolutional modules have been eliminated from the proposed model. The suggested model concludes with a multi-label classifier that utilizes a feed-forward network architecture. This classifier consists of two layers and employs a sigmoid activation function. Results and discussion The experimental findings provide evidence of the improved performance exhibited by the suggested model when compared to state-of-the-art approaches such as ResNet, VGG, DenseNet, and MobileNet, on the publicly available dataset ODIR-2019, and the proposed approach has outperformed the state-of-the-art algorithms in terms of Kappa, F1 score, AUC, and AVG.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d68aedc3a957c25e5f19438ec4b64fb53b48422c.pdf",
        "venue": "Frontiers in Neuroscience",
        "citationCount": 13,
        "score": 13.0
    },
    "bb5e6bbd4ad8c613a0bac3db23940c03ce94bec9.pdf": {
        "title": "QAGA-Net: enhanced vision transformer-based object detection for remote sensing images",
        "authors": [
            "Huaxiang Song",
            "Hanjun Xia",
            "Wenhui Wang",
            "Yang Zhou",
            "Wanbo Liu",
            "Qun Liu",
            "Jinling Liu"
        ],
        "published_date": "2024",
        "abstract": "PurposeVision transformers (ViT) detectors excel in processing natural images. However, when processing remote sensing images (RSIs), ViT methods generally exhibit inferior accuracy compared to approaches based on convolutional neural networks (CNNs). Recently, researchers have proposed various structural optimization strategies to enhance the performance of ViT detectors, but the progress has been insignificant. We contend that the frequent scarcity of RSI samples is the primary cause of this problem, and model modifications alone cannot solve it.Design/methodology/approachTo address this, we introduce a faster RCNN-based approach, termed QAGA-Net, which significantly enhances the performance of ViT detectors in RSI recognition. Initially, we propose a novel quantitative augmentation learning (QAL) strategy to address the sparse data distribution in RSIs. This strategy is integrated as the QAL module, a plug-and-play component active exclusively during the model\u2019s training phase. Subsequently, we enhanced the feature pyramid network (FPN) by introducing two efficient modules: a global attention (GA) module to model long-range feature dependencies and enhance multi-scale information fusion, and an efficient pooling (EP) module to optimize the model\u2019s capability to understand both high and low frequency information. Importantly, QAGA-Net has a compact model size and achieves a balance between computational efficiency and accuracy.FindingsWe verified the performance of QAGA-Net by using two different efficient ViT models as the detector\u2019s backbone. Extensive experiments on the NWPU-10 and DIOR20 datasets demonstrate that QAGA-Net achieves superior accuracy compared to 23 other ViT or CNN methods in the literature. Specifically, QAGA-Net shows an increase in mAP by 2.1% or 2.6% on the challenging DIOR20 dataset when compared to the top-ranked CNN or ViT detectors, respectively.Originality/valueThis paper highlights the impact of sparse data distribution on ViT detection performance. To address this, we introduce a fundamentally data-driven approach: the QAL module. Additionally, we introduced two efficient modules to enhance the performance of FPN. More importantly, our strategy has the potential to collaborate with other ViT detectors, as the proposed method does not require any structural modifications to the ViT backbone.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/bb5e6bbd4ad8c613a0bac3db23940c03ce94bec9.pdf",
        "venue": "International Journal of Intelligent Computing and Cybernetics",
        "citationCount": 13,
        "score": 13.0
    },
    "55156532cb9c20fdcaed9ead238f7a2cbaab2527.pdf": {
        "title": "FV-ViT: Vision Transformer for Finger Vein Recognition",
        "authors": [
            "Xiaoye Li",
            "Bin-Bin Zhang"
        ],
        "published_date": "2023",
        "abstract": "Vision Transformer (ViT) has drawn the attention of many researchers in computer vision due to its superior performance in many computer vision tasks. However, there is limited research based on ViT models in finger vein recognition. This may be because the excellent performance of the ViT models relies on the abundance of training data, but finger vein databases are typically small. In this study, we focus on this question and proposed a model for finger vein recognition, referred to as FV-ViT. With only rigorous regularization added in the MLP head, called regMLP, instead of changing architecture in the ViT backbone, the proposed FV-ViT shows outstanding performance compared to other state-of-the-art works: 0.042% EER for FV-USM and 1.033% EER for SDUMLA-HMT. In addition, we also compare the baseline FV-ViT model with the corresponding ViT model trained with pretrained weights: 0.068% EER from non-pretrained FV-ViT base versus 0.116% EER from pretrained for FV-USM, 1.258% EER from non-pretrained FV-ViT base versus 1.022% EER from pretrained for SDUMLA-HMT. This means that the ViT models can be trained from scratch on finger vein databases and achieve comparable performance when compared to the pretrained model.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/55156532cb9c20fdcaed9ead238f7a2cbaab2527.pdf",
        "venue": "IEEE Access",
        "citationCount": 25,
        "score": 12.5
    },
    "ae7c5ae7d377189c5a5f33a60b33c6cc5ca56124.pdf": {
        "title": "IML-ViT: Image Manipulation Localization by Vision Transformer",
        "authors": [
            "Xiaochen Ma",
            "Bo Du",
            "Xianggen Liu",
            "Ahmed Y. Al Hammadi",
            "Jizhe Zhou"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ae7c5ae7d377189c5a5f33a60b33c6cc5ca56124.pdf",
        "venue": "arXiv.org",
        "citationCount": 25,
        "score": 12.5
    },
    "3b427c8d3258968b9ac5eaf846d99ac027de9a76.pdf": {
        "title": "A human activity recognition method based on Vision Transformer",
        "authors": [
            "Huiyan Han",
            "H. Zeng",
            "Liqun Kuang",
            "Xie Han",
            "Hongxin Xue"
        ],
        "published_date": "2024",
        "abstract": "Human activity recognition has a wide range of applications in various fields, such as video surveillance, virtual reality and human\u2013computer intelligent interaction. It has emerged as a significant research area in computer vision. GCN (Graph Convolutional networks) have recently been widely used in these fields and have made great performance. However, there are still some challenges including over-smoothing problem caused by stack graph convolutions and deficient semantics correlation to capture the large movements between time sequences. Vision Transformer (ViT) is utilized in many 2D and 3D image fields and has surprised results. In our work, we propose a novel human activity recognition method based on ViT (HAR-ViT). We integrate enhanced AGCL (eAGCL) in 2s-AGCN to ViT to make it process spatio-temporal data (3D skeleton) and make full use of spatial features. The position encoder module orders the non-sequenced information while the transformer encoder efficiently compresses sequence data features to enhance calculation speed. Human activity recognition is accomplished through multi-layer perceptron (MLP) classifier. Experimental results demonstrate that the proposed method achieves SOTA performance on three extensively used datasets, NTU RGB+D 60, NTU RGB+D 120 and Kinetics-Skeleton 400.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3b427c8d3258968b9ac5eaf846d99ac027de9a76.pdf",
        "venue": "Scientific Reports",
        "citationCount": 12,
        "score": 12.0
    },
    "10e9943b3a974ac6175ffec3228e670ec9d2cc18.pdf": {
        "title": "An Explainable Vision Transformer Model Based White Blood Cells Classification and Localization",
        "authors": [
            "O\u011fuzhan Katar",
            "Ozal Yildirim"
        ],
        "published_date": "2023",
        "abstract": "White blood cells (WBCs) are crucial components of the immune system that play a vital role in defending the body against infections and diseases. The identification of WBCs subtypes is useful in the detection of various diseases, such as infections, leukemia, and other hematological malignancies. The manual screening of blood films is time-consuming and subjective, leading to inconsistencies and errors. Convolutional neural networks (CNN)-based models can automate such classification processes, but are incapable of capturing long-range dependencies and global context. This paper proposes an explainable Vision Transformer (ViT) model for automatic WBCs detection from blood films. The proposed model uses a self-attention mechanism to extract features from input images. Our proposed model was trained and validated on a public dataset of 16,633 samples containing five different types of WBCs. As a result of experiments on the classification of five different types of WBCs, our model achieved an accuracy of 99.40%. Moreover, the model\u2019s examination of misclassified test samples revealed a correlation between incorrect predictions and the presence or absence of granules in the cell samples. To validate this observation, we divided the dataset into two classes, Granulocytes and Agranulocytes, and conducted a secondary training process. The resulting ViT model, trained for binary classification, achieved impressive performance metrics during the test phase, including an accuracy of 99.70%, recall of 99.54%, precision of 99.32%, and F-1 score of 99.43%. To ensure the reliability of the ViT model\u2019s, we employed the Score-CAM algorithm to visualize the pixel areas on which the model focuses during its predictions. Our proposed method is suitable for clinical use due to its explainable structure as well as its superior performance compared to similar studies in the literature. The classification and localization of WBCs with this model can facilitate the detection and reporting process for the pathologist.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/10e9943b3a974ac6175ffec3228e670ec9d2cc18.pdf",
        "venue": "Diagnostics",
        "citationCount": 24,
        "score": 12.0
    },
    "1783c85f8b951dc11c9dcb4929bfcae2d52ecb63.pdf": {
        "title": "A Multitask Learning-Based Vision Transformer for Plant Disease Localization and Classification",
        "authors": [
            "S. Hemalatha",
            "Jayachandiran Jai Jaganath Babu"
        ],
        "published_date": "2024",
        "abstract": "Plant disease detection is a critical task in agriculture, essential for ensuring crop health and productivity. Traditional methods in this context are often labor-intensive and prone to errors, highlighting the need for automated solutions. While computer vision-based solutions have been successfully deployed in recent years for plant disease identification and localization tasks, these often operate independently, leading to suboptimal performance. It is essential to develop an integrated solution combining these two tasks for improved efficiency and accuracy. This research proposes the innovative Plant Disease Localization and Classification model based on Vision Transformer (PDLC-ViT), which integrates co-scale, co-attention, and cross-attention mechanisms and a ViT, within a Multi-Task Learning (MTL) framework. The model was trained and evaluated on the Plant Village dataset. Key hyperparameters, including learning rate, batch size, dropout ratio, and regularization factor, were optimized through a thorough grid search. Early stopping based on validation loss was employed to prevent overfitting. The PDLC-ViT model demonstrated significant improvements in plant disease localization and classification tasks. The integration of co-scale, co-attention, and cross-attention mechanisms allowed the model to capture multi-scale dependencies and enhance feature learning, leading to superior performance compared to existing models. The PDLC-ViT model evaluated on two public datasets achieved an accuracy of 99.97%, a Mean Average Precision (MAP) of 99.18%, and a Mean Average Recall (MAR) of 99.11%. These results underscore the model's exceptional precision and recall, highlighting its robustness and reliability in detecting and classifying plant diseases. The PDLC-ViT model sets a new benchmark in plant disease detection, offering a reliable and advanced tool for agricultural applications. Its ability to integrate localization and classification tasks within an MTL framework promotes timely and accurate disease management, contributing to sustainable agriculture and food security.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1783c85f8b951dc11c9dcb4929bfcae2d52ecb63.pdf",
        "venue": "International Journal of Computational Intelligence Systems",
        "citationCount": 12,
        "score": 12.0
    },
    "42bd7d3ca2b66eca5609fe77078fe8951ab9fd5c.pdf": {
        "title": "Interpretable Image Classification with Adaptive Prototype-based Vision Transformers",
        "authors": [
            "Chiyu Ma",
            "Jon Donnelly",
            "Wenjun Liu",
            "Soroush Vosoughi",
            "Cynthia Rudin",
            "Chaofan Chen"
        ],
        "published_date": "2024",
        "abstract": "We present ProtoViT, a method for interpretable image classification combining deep learning and case-based reasoning. This method classifies an image by comparing it to a set of learned prototypes, providing explanations of the form ``this looks like that.''In our model, a prototype consists of \\textit{parts}, which can deform over irregular geometries to create a better comparison between images. Unlike existing models that rely on Convolutional Neural Network (CNN) backbones and spatially rigid prototypes, our model integrates Vision Transformer (ViT) backbones into prototype based models, while offering spatially deformed prototypes that not only accommodate geometric variations of objects but also provide coherent and clear prototypical feature representations with an adaptive number of prototypical parts. Our experiments show that our model can generally achieve higher performance than the existing prototype based models. Our comprehensive analyses ensure that the prototypes are consistent and the interpretations are faithful.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/42bd7d3ca2b66eca5609fe77078fe8951ab9fd5c.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 12,
        "score": 12.0
    },
    "7aeadf7b9d839bdd9dbfbff5239f71792ff0c04c.pdf": {
        "title": "Vision Transformers (ViT) for Blanket-Penetrating Sleep Posture Recognition Using a Triple Ultra-Wideband (UWB) Radar System",
        "authors": [
            "D. K. Lai",
            "Zi-Han Yu",
            "Tommy Yau-Nam Leung",
            "Hyo-Jung Lim",
            "Andy Yiu-Chau Tam",
            "Bryan Pak-Hei So",
            "Ye-Jiao Mao",
            "D. Cheung",
            "D. Wong",
            "C. Cheung"
        ],
        "published_date": "2023",
        "abstract": "Sleep posture has a crucial impact on the incidence and severity of obstructive sleep apnea (OSA). Therefore, the surveillance and recognition of sleep postures could facilitate the assessment of OSA. The existing contact-based systems might interfere with sleeping, while camera-based systems introduce privacy concerns. Radar-based systems might overcome these challenges, especially when individuals are covered with blankets. The aim of this research is to develop a nonobstructive multiple ultra-wideband radar sleep posture recognition system based on machine learning models. We evaluated three single-radar configurations (top, side, and head), three dual-radar configurations (top + side, top + head, and side + head), and one tri-radar configuration (top + side + head), in addition to machine learning models, including CNN-based networks (ResNet50, DenseNet121, and EfficientNetV2) and vision transformer-based networks (traditional vision transformer and Swin Transformer V2). Thirty participants (n = 30) were invited to perform four recumbent postures (supine, left side-lying, right side-lying, and prone). Data from eighteen participants were randomly chosen for model training, another six participants\u2019 data (n = 6) for model validation, and the remaining six participants\u2019 data (n = 6) for model testing. The Swin Transformer with side and head radar configuration achieved the highest prediction accuracy (0.808). Future research may consider the application of the synthetic aperture radar technique.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7aeadf7b9d839bdd9dbfbff5239f71792ff0c04c.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 22,
        "score": 11.0
    },
    "b2becca9911c155bf97656df8e5079ca76767ab9.pdf": {
        "title": "A Timely Survey on Vision Transformer for Deepfake Detection",
        "authors": [
            "Zhikan Wang",
            "Zhongyao Cheng",
            "Jiajie Xiong",
            "Xun Xu",
            "Tianrui Li",
            "B. Veeravalli",
            "Xulei Yang"
        ],
        "published_date": "2024",
        "abstract": "In recent years, the rapid advancement of deepfake technology has revolutionized content creation, lowering forgery costs while elevating quality. However, this progress brings forth pressing concerns such as infringements on individual rights, national security threats, and risks to public safety. To counter these challenges, various detection methodologies have emerged, with Vision Transformer (ViT)-based approaches showcasing superior performance in generality and efficiency. This survey presents a timely overview of ViT-based deepfake detection models, categorized into standalone, sequential, and parallel architectures. Furthermore, it succinctly delineates the structure and characteristics of each model. By analyzing existing research and addressing future directions, this survey aims to equip researchers with a nuanced understanding of ViT's pivotal role in deepfake detection, serving as a valuable reference for both academic and practical pursuits in this domain.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b2becca9911c155bf97656df8e5079ca76767ab9.pdf",
        "venue": "arXiv.org",
        "citationCount": 11,
        "score": 11.0
    },
    "25a69bdd7f87ecc287b4e9a64eb4d6d562371add.pdf": {
        "title": "PanoSwin: a Pano-style Swin Transformer for Panorama Understanding",
        "authors": [
            "Zhixin Ling",
            "Zhen Xing",
            "Xiangdong Zhou",
            "Manliang Cao",
            "G. Zhou"
        ],
        "published_date": "2023",
        "abstract": "In panorama understanding, the widely used equirectangular projection (ERP) entails boundary discontinuity and spatial distortion. It severely deteriorates the conventional CNNs and vision Transformers on panoramas. In this paper, we propose a simple yet effective architecture named PanoSwin to learn panorama representations with ERP. To deal with the challenges brought by equirectangular projection, we explore a pano-style shift windowing scheme and novel pitch attention to address the boundary discontinuity and the spatial distortion, respectively. Besides, based on spherical distance and Cartesian coordinates, we adapt absolute positional embeddings and relative positional biases for panoramas to enhance panoramic geometry information. Realizing that planar image understanding might share some common knowledge with panorama understanding, we devise a novel two-stage learning framework to facilitate knowledge transfer from the planar images to panoramas. We conduct experiments against the state-of-the-art on various panoramic tasks, i.e., panoramic object detection, panoramic classification, and panoramic layout estimation. The experimental results demonstrate the effectiveness of PanoSwin in panorama understanding.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/25a69bdd7f87ecc287b4e9a64eb4d6d562371add.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 20,
        "score": 10.0
    },
    "50e997b23a534a6fbfd32d63990fa80373ec7c6b.pdf": {
        "title": "Students' Classroom Behavior Detection System Incorporating Deformable DETR with Swin Transformer and Light-Weight Feature Pyramid Network",
        "authors": [
            "Zhifeng Wang",
            "Jialong Yao",
            "Chunyan Zeng",
            "Longlong Li",
            "Cheng Tan"
        ],
        "published_date": "2023",
        "abstract": "Artificial intelligence (AI) and computer vision technologies have gained significant prominence in the field of education. These technologies enable the detection and analysis of students\u2019 classroom behaviors, providing valuable insights for assessing individual concentration levels. However, the accuracy of target detection methods based on Convolutional Neural Networks (CNNs) can be compromised in classrooms with multiple targets and varying scales, as convolutional operations may result in the loss of location information. In contrast, transformers, which leverage attention mechanisms, have the capability to learn global features and mitigate the information loss caused by convolutional operations. In this paper, we propose a students\u2019 classroom behavior detection system that combines deformable DETR with a Swin Transformer and light-weight Feature Pyramid Network (FPN). By employing a feature pyramid structure, the system can effectively process multi-scale feature maps extracted by the Swin Transformer, thereby improving the detection accuracy for targets of different sizes and scales. Moreover, the integration of the CARAFE lightweight operator into the FPN structure enhances the network\u2019s detection accuracy. To validate the effectiveness of our approach, extensive experiments are conducted on a real dataset of students\u2019 classroom behavior. The experimental results demonstrate a significant 6.1% improvement in detection accuracy compared to state-of-the-art methods. These findings highlight the superiority of our proposed network in accurately detecting and analyzing students\u2019 classroom behaviors. Overall, this research contributes to the field of education by addressing the limitations of CNN-based target detection methods and leveraging the capabilities of transformers to improve accuracy. The proposed system showcases the benefits of integrating deformable DETR, Swin Transformer, and the lightweight FPN in the context of students\u2019 classroom behavior detection. The experimental results provide compelling evidence of the system\u2019s effectiveness and its potential to enhance classroom monitoring and assessment practices.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/50e997b23a534a6fbfd32d63990fa80373ec7c6b.pdf",
        "venue": "Syst.",
        "citationCount": 19,
        "score": 9.5
    },
    "3ea79430455304c782572dfb6ca3e5230b0351de.pdf": {
        "title": "GOHSP: A Unified Framework of Graph and Optimization-based Heterogeneous Structured Pruning for Vision Transformer",
        "authors": [
            "Miao Yin",
            "Burak Uzkent",
            "Yilin Shen",
            "Hongxia Jin",
            "Bo Yuan"
        ],
        "published_date": "2023",
        "abstract": "The recently proposed Vision transformers (ViTs) have shown\nvery impressive empirical performance in various computer vision tasks,\nand they are viewed as an important type of foundation model. However, ViTs are typically constructed with large-scale sizes, which then\nseverely hinder their potential deployment in many practical resources constrained applications. \nTo mitigate this challenging problem, structured pruning is a promising solution to compress model size and enable\npractical efficiency. However, unlike its current popularity for CNNs and\nRNNs, structured pruning for ViT models is little explored.\nIn this paper, we propose GOHSP, a unified framework of Graph and\nOptimization-based Structured Pruning for ViT models. We first develop\na graph-based ranking for measuring the importance of attention heads,\nand the extracted importance information is further integrated to an\noptimization-based procedure to impose the heterogeneous structured\nsparsity patterns on the ViT models. Experimental results show that\nour proposed GOHSP demonstrates excellent compression performance.\nOn CIFAR-10 dataset, our approach can bring 40% parameters reduction\nwith no accuracy loss for ViT-Small model. On ImageNet dataset, with\n30% and 35% sparsity ratio for DeiT-Tiny and DeiT-Small models, our\napproach achieves 1.65% and 0.76% accuracy increase over the existing\nstructured pruning methods, respectively.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3ea79430455304c782572dfb6ca3e5230b0351de.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 19,
        "score": 9.5
    },
    "0284fa05879f1609bf459f15a4d40e22355cd5ae.pdf": {
        "title": "Image Caption Generation using Vision Transformer and GPT Architecture",
        "authors": [
            "Swapneel Mishra",
            "Saumya Seth",
            "Shrishti Jain",
            "Vasudev Pant",
            "Jolly Parikh",
            "Rachna Jain",
            "Sardar M. N. Islam"
        ],
        "published_date": "2024",
        "abstract": "Transformer-based models have reshaped image captioning but grapple with issues like caption accuracy, particularly for complex visuals. Addressing these shortcomings is essential. Motivated by existing challenges, the Vision Transformer (ViT) as encoder and Generative Pretrained Transformer 2 (GPT-2) as decoder have been employed to enhance caption quality, utilizing the Seq2Seq framework and training on Flickr8k. This work introduced a novel ViT-GPT-2 image captioning model, evaluating it against benchmarks including Flickr8k. The model excels with BLEU-4 at 39.76 and METEOR at 52.30, bridging visual-textual gaps effectively. This research advances image captioning, offering practitioners an improved model for content indexing, accessibility, and human-computer interaction. ViT- GPT-2\u2019s success underscores bridging semantic gaps in image captioning, with future work exploring diverse datasets and fine-tuning techniques for enhanced performance.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0284fa05879f1609bf459f15a4d40e22355cd5ae.pdf",
        "venue": "2024 2nd International Conference on Advancement in Computation & Computer Technologies (InCACCT)",
        "citationCount": 9,
        "score": 9.0
    },
    "714e21409b8c4f7788ac8c93795249a4e45e51ce.pdf": {
        "title": "Enhancing Efficiency in Vision Transformer Networks: Design Techniques and Insights",
        "authors": [
            "Moein Heidari",
            "Reza Azad",
            "Sina Ghorbani Kolahi",
            "Ren'e Arimond",
            "Leon Niggemeier",
            "Alaa Sulaiman",
            "Afshin Bozorgpour",
            "Ehsan Khodapanah Aghdam",
            "A. Kazerouni",
            "I. Hacihaliloglu",
            "D. Merhof"
        ],
        "published_date": "2024",
        "abstract": "Intrigued by the inherent ability of the human visual system to identify salient regions in complex scenes, attention mechanisms have been seamlessly integrated into various Computer Vision (CV) tasks. Building upon this paradigm, Vision Transformer (ViT) networks exploit attention mechanisms for improved efficiency. This review navigates the landscape of redesigned attention mechanisms within ViTs, aiming to enhance their performance. This paper provides a comprehensive exploration of techniques and insights for designing attention mechanisms, systematically reviewing recent literature in the field of CV. This survey begins with an introduction to the theoretical foundations and fundamental concepts underlying attention mechanisms. We then present a systematic taxonomy of various attention mechanisms within ViTs, employing redesigned approaches. A multi-perspective categorization is proposed based on their application, objectives, and the type of attention applied. The analysis includes an exploration of the novelty, strengths, weaknesses, and an in-depth evaluation of the different proposed strategies. This culminates in the development of taxonomies that highlight key properties and contributions. Finally, we gather the reviewed studies along with their available open-source implementations at our \\href{https://github.com/mindflow-institue/Awesome-Attention-Mechanism-in-Medical-Imaging}{GitHub}\\footnote{\\url{https://github.com/xmindflow/Awesome-Attention-Mechanism-in-Medical-Imaging}}. We aim to regularly update it with the most recent relevant papers.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/714e21409b8c4f7788ac8c93795249a4e45e51ce.pdf",
        "venue": "arXiv.org",
        "citationCount": 9,
        "score": 9.0
    },
    "2ef224f7a1f545eb8bfdf61c7b849f2676fe6a61.pdf": {
        "title": "A Novel Robotic Pushing and Grasping Method Based on Vision Transformer and Convolution",
        "authors": [
            "Sheng Yu",
            "Dihua Zhai",
            "Yuanqing Xia"
        ],
        "published_date": "2023",
        "abstract": "Robotic grasping techniques have been widely studied in recent years. However, it is always a challenging problem for robots to grasp in cluttered scenes. In this issue, objects are placed close to each other, and there is no space around for the robot to place the gripper, making it difficult to find a suitable grasping position. To solve this problem, this article proposes to use the combination of pushing and grasping (PG) actions to help grasp pose detection and robot grasping. We propose a pushing\u2013grasping combined grasping network (GN), PG method based on transformer and convolution (PGTC). For the pushing action, we propose a vision transformer (ViT)-based object position prediction network pushing transformer network (PTNet), which can well capture the global and temporal features and can better predict the position of objects after pushing. To perform the grasping detection, we propose a cross dense fusion network (CDFNet), which can make full use of the RGB image and depth image, and fuse and refine them several times. Compared with previous networks, CDFNet is able to detect the optimal grasping position more accurately. Finally, we use the network for both simulation and actual UR3 robot grasping experiments and achieve SOTA performance. Video and dataset are available at https://youtu.be/Q58YE-Cc250.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2ef224f7a1f545eb8bfdf61c7b849f2676fe6a61.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 18,
        "score": 9.0
    },
    "bc99c855d52ba3d432c428fb4096b3a22c04f8bf.pdf": {
        "title": "MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer",
        "authors": [
            "Qihao Zhao",
            "Yangyu Huang",
            "Wei Hu",
            "Fan Zhang",
            "J. Liu"
        ],
        "published_date": "2023",
        "abstract": "The recently proposed data augmentation TransMix employs attention labels to help visual transformers (ViT) achieve better robustness and performance. However, TransMix is deficient in two aspects: 1) The image cropping method of TransMix may not be suitable for ViTs. 2) At the early stage of training, the model produces unreliable attention maps. TransMix uses unreliable attention maps to compute mixed attention labels that can affect the model. To address the aforementioned issues, we propose MaskMix and Progressive Attention Labeling (PAL) in image and label space, respectively. In detail, from the perspective of image space, we design MaskMix, which mixes two images based on a patch-like grid mask. In particular, the size of each mask patch is adjustable and is a multiple of the image patch size, which ensures each image patch comes from only one image and contains more global contents. From the perspective of label space, we design PAL, which utilizes a progressive factor to dynamically re-weight the attention weights of the mixed attention label. Finally, we combine MaskMix and Progressive Attention Labeling as our new data augmentation method, named MixPro. The experimental results show that our method can improve various ViT-based models at scales on ImageNet classification (73.8\\% top-1 accuracy based on DeiT-T for 300 epochs). After being pre-trained with MixPro on ImageNet, the ViT-based models also demonstrate better transferability to semantic segmentation, object detection, and instance segmentation. Furthermore, compared to TransMix, MixPro also shows stronger robustness on several benchmarks. The code is available at https://github.com/fistyee/MixPro.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/bc99c855d52ba3d432c428fb4096b3a22c04f8bf.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 18,
        "score": 9.0
    },
    "3dee43cea71d5988a72a914121f3455106f89cc7.pdf": {
        "title": "Depth-Guided Vision Transformer With Normalizing Flows for Monocular 3D Object Detection",
        "authors": [
            "C. Pan",
            "Junran Peng",
            "Zhaoxiang Zhang"
        ],
        "published_date": "2024",
        "abstract": "Monocular 3D object detection is challenging due to the lack of accurate depth information. Some methods estimate the pixel-wise depth maps from off-the-shelf depth estimators and then use them as an additional input to augment the RGB images. Depth-based methods attempt to convert estimated depth maps to pseudo-LiDAR and then use LiDAR-based object detectors or focus on the perspective of image and depth fusion learning. However, they demonstrate limited performance and efficiency as a result of depth inaccuracy and complex fusion mode with convolutions. Different from these approaches, our proposed depth-guided vision transformer with a normalizing flows (NF-DVT) network uses normalizing flows to build priors in depth maps to achieve more accurate depth information. Then we develop a novel Swin-Transformer-based backbone with a fusion module to process RGB image patches and depth map patches with two separate branches and fuse them using cross-attention to exchange information with each other. Furthermore, with the help of pixel-wise relative depth values in depth maps, we develop new relative position embeddings in the cross-attention mechanism to capture more accurate sequence ordering of input tokens. Our method is the first Swin-Transformer-based backbone architecture for monocular 3D object detection. The experimental results on the KITTI and the challenging Waymo Open datasets show the effectiveness of our proposed method and superior performance over previous counterparts.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3dee43cea71d5988a72a914121f3455106f89cc7.pdf",
        "venue": "IEEE/CAA Journal of Automatica Sinica",
        "citationCount": 9,
        "score": 9.0
    },
    "1871c0bc27bd5cd5f2f6c3d6cedeb4b8c5a89499.pdf": {
        "title": "A lightweight hybrid vision transformer network for radar-based human activity recognition",
        "authors": [
            "Sha Huan",
            "Zhaoyue Wang",
            "Xiaoqiang Wang",
            "Limei Wu",
            "Xiaoxuan Yang",
            "Hongming Huang",
            "Gan E Dai"
        ],
        "published_date": "2023",
        "abstract": "Radar-based human activity recognition (HAR) offers a non-contact technique with privacy protection and lighting robustness for many advanced applications. Complex deep neural networks demonstrate significant performance advantages when classifying the radar micro-Doppler signals that have unique correspondences with human behavior. However, in embedded applications, the demand for lightweight and low latency poses challenges to the radar-based HAR network construction. In this paper, an efficient network based on a lightweight hybrid Vision Transformer (LH-ViT) is proposed to address the HAR accuracy and network lightweight simultaneously. This network combines the efficient convolution operations with the strength of the self-attention mechanism in ViT. Feature Pyramid architecture is applied for the multi-scale feature extraction for the micro-Doppler map. Feature enhancement is executed by the stacked Radar-ViT subsequently, in which the fold and unfold operations are added to lower the computational load of the attention mechanism. The convolution operator in the LH-ViT is replaced by the RES-SE block, an efficient structure that combines the residual learning framework with the Squeeze-and-Excitation network. Experiments based on two human activity datasets indicate our method\u2019s advantages in terms of expressiveness and computing efficiency over traditional methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1871c0bc27bd5cd5f2f6c3d6cedeb4b8c5a89499.pdf",
        "venue": "Scientific Reports",
        "citationCount": 17,
        "score": 8.5
    },
    "c064efa0419b75ba131ec8470ed80f01e1a80f64.pdf": {
        "title": "Global-Local Attention-Based Butterfly Vision Transformer for Visualization-Based Malware Classification",
        "authors": [
            "Mohamad Mulham Belal",
            "Dr. Divya Meena Sundaram"
        ],
        "published_date": "2023",
        "abstract": "In recent studies, convolutional neural networks (CNNs) are mostly used as dynamic techniques for visualization-based malware classification and detection. Though vision transformer (ViT) proved its efficiency in image classification, a few of the earlier studies developed a ViT-based malware classifier. This paper proposes a butterfly construction-based vision transformer (B_ViT) model for visualization-based malware classification and detection. B_ViT has four phases: (1) image partitioning and patches embeddings; (2) local attention; (3) global attention; and (4) training and malware classification. B_ViT is an enhanced ViT architecture that supports the parallel processing of image patches and captures local and global spatial representations of malware images. B_ViT is a transfer learning-based model that uses a pre-trained ViT model on the ImageNet dataset to initialize the training parameters of transformers. Four B_ViT variants are experimented and evaluated on grayscale malware images collected from MalImg, Microsoft BIG datasets or converted from portable executable imports. The experiments show that B_ViT variants outperform the Input Enhanced vision transformer (IEViT) and ViT variants, achieving an accuracy equal to 99.49% and 99.99% for malware classification and detection respectively. The experiments also show that B_ViT is time effective for malware classification and detection where the average speed-up of B_ViT variants over IEViT and ViT variants are equal to 2.42 and 1.81 respectively. The analysis proves the efficiency of texture-based malware detection as well as the resilience of B_ViT to polymorphic obfuscation. Finally, the proposed B_ViT-based malware classifier outperforms the CNN-based malware classification methods in well.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c064efa0419b75ba131ec8470ed80f01e1a80f64.pdf",
        "venue": "IEEE Access",
        "citationCount": 17,
        "score": 8.5
    },
    "b48a85980deb5f1baa64d862b9f0e4e62124e4de.pdf": {
        "title": "Bi-ViT: Pushing the Limit of Vision Transformer Quantization",
        "authors": [
            "Yanjing Li",
            "Sheng Xu",
            "Mingbao Lin",
            "Xianbin Cao",
            "Chuanjian Liu",
            "Xiao Sun",
            "Baochang Zhang"
        ],
        "published_date": "2023",
        "abstract": "Vision transformers (ViTs) quantization offers a promising prospect to facilitate deploying large pre-trained networks on resource-limited devices. Fully-binarized ViTs (Bi-ViT) that pushes the quantization of ViTs to its limit remain largely unexplored and a very challenging task yet, due to their unacceptable performance. Through extensive empirical analyses, we identify the severe drop in ViT binarization is caused by attention distortion in self-attention, which technically stems from the gradient vanishing and ranking disorder. To address these issues, we first introduce a learnable scaling factor to reactivate the vanished gradients and illustrate its effectiveness through theoretical and experimental analyses. We then propose a ranking-aware distillation method to rectify the disordered ranking in a teacher-student framework. Bi-ViT achieves significant improvements over popular DeiT and Swin backbones in terms of Top-1 accuracy and FLOPs. For example, with DeiT-Tiny and Swin-Tiny, our method significantly outperforms baselines by 22.1% and 21.4% respectively, while 61.5x and 56.1x theoretical acceleration in terms of FLOPs compared with real-valued counterparts on ImageNet. Our codes and models are attached on https://github.com/YanjingLi0202/Bi-ViT/ .",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b48a85980deb5f1baa64d862b9f0e4e62124e4de.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 16,
        "score": 8.0
    },
    "8ec10ffe0c1fc8f6a92d541f0e002e78080b564a.pdf": {
        "title": "Vision Transformer (ViT)-based Applications in Image Classification",
        "authors": [
            "Yingzi Huo",
            "Kai Jin",
            "Jiahong Cai",
            "Huixuan Xiong",
            "Jiacheng Pang"
        ],
        "published_date": "2023",
        "abstract": "In recent years, the ViT model has been widely used in the field of computer vision, especially for image classification tasks. This paper summarizes the application of ViT in image classification tasks, first introduces the image classification imple- mentation process and the basic architecture of the ViT model, then analyzes and summarizes the image classification methods, including traditional image classification methods, CNN-based image classification methods, and ViT-based image classification methods, and provides a comparative analysis of CNN and ViT. Subsequently, this paper outlines the application prospects of ViT in image classification and its future development and also outlines some shortcomings of ViT and its solutions.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/8ec10ffe0c1fc8f6a92d541f0e002e78080b564a.pdf",
        "venue": "2023 IEEE 9th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and Smart Computing, (HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS)",
        "citationCount": 16,
        "score": 8.0
    },
    "769ba82ab77fc46db4a594dc64576286d02cf1d7.pdf": {
        "title": "Vision Transformer-Based Feature Extraction for Generalized Zero-Shot Learning",
        "authors": [
            "Jiseob Kim",
            "Kyuhong Shim",
            "Junhan Kim",
            "B. Shim"
        ],
        "published_date": "2023",
        "abstract": "Generalized zero-shot learning (GZSL) is a technique to train a deep learning model to identify unseen classes using the image attribute. In this paper, we put forth a new GZSL technique exploiting Vision Transformer (ViT) to maximize the attribute-related information contained in the image feature. In ViT, the entire image region is processed without the degradation of the image resolution and the local image information is preserved in patch features. To fully enjoy the benefits of ViT, we exploit patch features as well as the CLS feature in the extraction of the attribute-related image feature. In particular, we propose a novel attention-based module, called attribute attention module (AAM), to aggregate the attribute-related information in the patch features. From extensive experiments on benchmark datasets, we demonstrate that the proposed technique outperforms the state-of-the-art GZSL approaches by a large margin.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/769ba82ab77fc46db4a594dc64576286d02cf1d7.pdf",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "citationCount": 16,
        "score": 8.0
    },
    "d1255f6e7e7e14b253fe2245e842d0b45f5e715e.pdf": {
        "title": "ViT-FRD: A Vision Transformer Model for Cardiac MRI Image Segmentation Based on Feature Recombination Distillation",
        "authors": [
            "Chunyu Fan",
            "Q. Su",
            "Zhifeng Xiao",
            "Haoran Su",
            "Aijie Hou",
            "Bo Luan"
        ],
        "published_date": "2023",
        "abstract": "Cardiac magnetic resonance imaging analysis has been a useful tool in screening patients for heart disease. Early, timely and accurate diagnosis of diseases of the heart series is the key to effective treatment. MRI provides important material for the diagnosis of cardiac diseases. The rise of deep learning has transformed computer-aided diagnostic systems, especially in the field of medical imaging. Existing work on cardiac structure segmentation models based on MRI imaging mainly relies on convolutional neural networks (CNNs), which lack model diversity and limit the prediction performance. This paper introduces Visual Transformer with Feature Recombination and Feature Distillation(ViT-FRD), a novel learning pipeline that combines a visual transformer (ViT) and a CNN through knowledge refinement. The training procedure allows the student model, i.e., ViT, to learn from the teacher model, i.e., CNN, by optimizing distillation losses. Meanwhile, ViT-FRD provides two performance boosters to increase the efficacy and efficiency of training. The proposed method is validated on two cardiac MRI image datasets. The findings demonstrate that ViT-FRD achieves SOTA and outperforms the widely used baseline model.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d1255f6e7e7e14b253fe2245e842d0b45f5e715e.pdf",
        "venue": "IEEE Access",
        "citationCount": 15,
        "score": 7.5
    },
    "5572237909914e23758115be6b8d7f99a8bd51dc.pdf": {
        "title": "ST-YOLOA: a Swin-transformer-based YOLO model with an attention mechanism for SAR ship detection under complex background",
        "authors": [
            "Kai Zhao",
            "Ruitao Lu",
            "Siyu Wang",
            "Xiaogang Yang",
            "Qingge Li",
            "Jiwei Fan"
        ],
        "published_date": "2023",
        "abstract": "A synthetic aperture radar (SAR) image is crucial for ship detection in computer vision. Due to the background clutter, pose variations, and scale changes, it is a challenge to construct a SAR ship detection model with low false-alarm rates and high accuracy. Therefore, this paper proposes a novel SAR ship detection model called ST-YOLOA. First, the Swin Transformer network architecture and coordinate attention (CA) model are embedded in the STCNet backbone network to enhance the feature extraction performance and capture global information. Second, we used the PANet path aggregation network with a residual structure to construct the feature pyramid to increase global feature extraction capability. Next, to cope with the local interference and semantic information loss problems, a novel up/down-sampling method is proposed. Finally, the decoupled detection head is used to achieve the predicted output of the target position and the boundary box to improve convergence speed and detection accuracy. To demonstrate the efficiency of the proposed method, we have constructed three SAR ship detection datasets: a norm test set (NTS), a complex test set (CTS), and a merged test set (MTS). The experimental results show that our ST-YOLOA achieved an accuracy of 97.37%, 75.69%, and 88.50% on the three datasets, respectively, superior to the effects of other state-of-the-art methods. Our ST-YOLOA performs favorably in complex scenarios, and the accuracy is 4.83% higher than YOLOX on the CTS. Moreover, ST-YOLOA achieves real-time detection with a speed of 21.4 FPS.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/5572237909914e23758115be6b8d7f99a8bd51dc.pdf",
        "venue": "Frontiers in Neurorobotics",
        "citationCount": 15,
        "score": 7.5
    },
    "21c37fd6d00d6c727ec10f0c871c1ddf6e4f7ed9.pdf": {
        "title": "ViT-MVT: A Unified Vision Transformer Network for Multiple Vision Tasks",
        "authors": [
            "Tao Xie",
            "Kun Dai",
            "Zhiqiang Jiang",
            "Ruifeng Li",
            "Shouren Mao",
            "Ke Wang",
            "Lijun Zhao"
        ],
        "published_date": "2023",
        "abstract": "In this work, we seek to learn multiple mainstream vision tasks concurrently using a unified network, which is storage-efficient as numerous networks with task-shared parameters can be implanted into a single consolidated network. Our framework, vision transformer (ViT)-MVT, built on a plain and nonhierarchical ViT, incorporates numerous visual tasks into a modest supernet and optimizes them jointly across various dataset domains. For the design of ViT-MVT, we augment the ViT with a multihead self-attention (MHSE) to offer complementary cues in the channel and spatial dimension, as well as a local perception unit (LPU) and locality feed-forward network (locality FFN) for information exchange in the local region, thus endowing ViT-MVT with the ability to effectively optimize multiple tasks. Besides, we construct a search space comprising potential architectures with a broad spectrum of model sizes to offer various optimum candidates for diverse tasks. After that, we design a layer-adaptive sharing technique that automatically determines whether each layer of the transformer block is shared or not for all tasks, enabling ViT-MVT to obtain task-shared parameters for a reduction of storage and task-specific parameters to learn task-related features such that boosting performance. Finally, we introduce a joint-task evolutionary search algorithm to discover an optimal backbone for all tasks under total model size constraint, which challenges the conventional wisdom that visual tasks are typically supplied with backbone networks developed for image classification. Extensive experiments reveal that ViT-MVT delivers exceptional performances for multiple visual tasks over state-of-the-art methods while necessitating considerably fewer total storage costs. We further demonstrate that once ViT-MVT has been trained, ViT-MVT is capable of incremental learning when generalized to new tasks while retaining identical performances for trained tasks. The code is available at https://github.com/XT-1997/vitmvt.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/21c37fd6d00d6c727ec10f0c871c1ddf6e4f7ed9.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 15,
        "score": 7.5
    },
    "e38e70580acb204c05096de8da90b7ab1d4bdb6b.pdf": {
        "title": "SwinCross: Cross-modal Swin Transformer for Head-and-Neck Tumor Segmentation in PET/CT Images",
        "authors": [
            "Gary Y. Li",
            "Junyu Chen",
            "Se-In Jang",
            "Kuang Gong",
            "Quanzheng Li"
        ],
        "published_date": "2023",
        "abstract": "BACKGROUND\nRadiotherapy (RT) combined with cetuximab is the standard treatment for patients with inoperable head and neck cancers. Segmentation of head and neck (H&N) tumors is a prerequisite for radiotherapy planning but a time-consuming process. In recent years, deep convolutional neural networks (DCNN) have become the de facto standard for automated image segmentation. However, due to the expensive computational cost associated with enlarging the field of view in DCNNs, their ability to model long-range dependency is still limited, and this can result in sub-optimal segmentation performance for objects with background context spanning over long distances. On the other hand, Transformer models have demonstrated excellent capabilities in capturing such long-range information in several semantic segmentation tasks performed on medical images.\n\n\nPURPOSE\nDespite the impressive representation capacity of vision transformer models, current vision transformer-based segmentation models still suffer from inconsistent and incorrect dense predictions when fed with multi-modal input data. We suspect that the power of their self-attention mechanism may be limited in extracting the complementary information that exists in multi-modal data. To this end, we propose a novel segmentation model, debuted, Cross-modal Swin Transformer (SwinCross), with cross-modal attention (CMA) module to incorporate cross-modal feature extraction at multiple resolutions.\n\n\nMETHODS\nWe propose a novel architecture for cross-modal 3D semantic segmentation with two main components: (1) a cross-modal 3D Swin Transformer for integrating information from multiple modalities (PET and CT), and (2) a cross-modal shifted window attention block for learning complementary information from the modalities. To evaluate the efficacy of our approach, we conducted experiments and ablation studies on the HECKTOR 2021 challenge dataset. We compared our method against nnU-Net (the backbone of the top-5 methods in HECKTOR 2021) and other state-of-the-art transformer-based models, including UNETR and Swin UNETR. The experiments employed a five-fold cross-validation setup using PET and CT images.\n\n\nRESULTS\nEmpirical evidence demonstrates that our proposed method consistently outperforms the comparative techniques. This success can be attributed to the CMA module's capacity to enhance inter-modality feature representations between PET and CT during head-and-neck tumor segmentation. Notably, SwinCross consistently surpasses Swin UNETR across all five folds, showcasing its proficiency in learning multi-modal feature representations at varying resolutions through the cross-modal attention modules.\n\n\nCONCLUSIONS\nWe introduced a cross-modal Swin Transformer for automating the delineation of head and neck tumors in PET and CT images. Our model incorporates a cross-modality attention module, enabling the exchange of features between modalities at multiple resolutions. The experimental results establish the superiority of our method in capturing improved inter-modality correlations between PET and CT for head-and-neck tumor segmentation. Furthermore, the proposed methodology holds applicability to other semantic segmentation tasks involving different imaging modalities like SPECT/CT or PET/MRI. Code:https://github.com/yli192/SwinCross_CrossModalSwinTransformer_for_Medical_Image_Segmentation.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e38e70580acb204c05096de8da90b7ab1d4bdb6b.pdf",
        "venue": "Medical Physics (Lancaster)",
        "citationCount": 15,
        "score": 7.5
    },
    "a246677a3d68e8cd7a63d24639a5cf3fc3b9f56e.pdf": {
        "title": "IML-ViT: Benchmarking Image Manipulation Localization by Vision Transformer",
        "authors": [
            "Xiaochen Ma",
            "Bo Du",
            "Zhuohang Jiang",
            "Xia Du",
            "Ahmed Y. Al Hammadi",
            "Jizhe Zhou"
        ],
        "published_date": "2023",
        "abstract": "Advanced image tampering techniques are increasingly challenging the trustworthiness of multimedia, leading to the development of Image Manipulation Localization (IML). But what makes a good IML model? The answer lies in the way to capture artifacts. Exploiting artifacts requires the model to extract non-semantic discrepancies between manipulated and authentic regions, necessitating explicit comparisons between the two areas. With the self-attention mechanism, naturally, the Transformer should be a better candidate to capture artifacts. However, due to limited datasets, there is currently no pure ViT-based approach for IML to serve as a benchmark, and CNNs dominate the entire task. Nevertheless, CNNs suffer from weak long-range and non-semantic modeling. To bridge this gap, based on the fact that artifacts are sensitive to image resolution, amplified under multi-scale features, and massive at the manipulation border, we formulate the answer to the former question as building a ViT with high-resolution capacity, multi-scale feature extraction capability, and manipulation edge supervision that could converge with a small amount of data. We term this simple but effective ViT paradigm IML-ViT, which has significant potential to become a new benchmark for IML. Extensive experiments on three different mainstream protocols verified our model outperforms the state-of-the-art manipulation localization methods. Code and models are available at https://github.com/SunnyHaze/IML-ViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a246677a3d68e8cd7a63d24639a5cf3fc3b9f56e.pdf",
        "venue": "",
        "citationCount": 15,
        "score": 7.5
    },
    "1397d1bcfeae68078756fdbc3ed7e62ef51a1e32.pdf": {
        "title": "Breast Cancer Classification Using Fine-Tuned SWIN Transformer Model on Mammographic Images",
        "authors": [
            "Oluwatosin Tanimola",
            "Olamilekan Shobayo",
            "O. Popoola",
            "O. Okoyeigbo"
        ],
        "published_date": "2024",
        "abstract": "Breast cancer is the most prevalent type of disease among women. It has become one of the foremost causes of death among women globally. Early detection plays a significant role in administering personalized treatment and improving patient outcomes. Mammography procedures are often used to detect early-stage cancer cells. This traditional method of mammography while valuable has limitations in its potential for false positives and negatives, patient discomfort, and radiation exposure. Therefore, there is a probe for more accurate techniques required in detecting breast cancer, leading to exploring the potential of machine learning in the classification of diagnostic images due to its efficiency and accuracy. This study conducted a comparative analysis of pre-trained CNNs (ResNet50 and VGG16) and vision transformers (ViT-base and SWIN transformer) with the inclusion of ViT-base trained from scratch model architectures to effectively classify mammographic breast cancer images into benign and malignant cases. The SWIN transformer exhibits superior performance with 99.9% accuracy and a precision of 99.8%. These findings demonstrate the efficiency of deep learning to accurately classify mammographic breast cancer images for the diagnosis of breast cancer, leading to improvements in patient outcomes.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1397d1bcfeae68078756fdbc3ed7e62ef51a1e32.pdf",
        "venue": "Analytics",
        "citationCount": 7,
        "score": 7.0
    },
    "52239e2338cd9f0e0e690203af8f0f1260ef7abf.pdf": {
        "title": "Swin-Fusion: Swin-Transformer with Feature Fusion for Human Action Recognition",
        "authors": [
            "Tiansheng Chen",
            "L. Mo"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/52239e2338cd9f0e0e690203af8f0f1260ef7abf.pdf",
        "venue": "Neural Processing Letters",
        "citationCount": 14,
        "score": 7.0
    },
    "f996d5ee3b8ad3c60510862a92fd72c6a41777e0.pdf": {
        "title": "LRP-QViT: Mixed-Precision Vision Transformer Quantization via Layer-wise Relevance Propagation",
        "authors": [
            "Navin Ranjan",
            "Andreas E. Savakis"
        ],
        "published_date": "2024",
        "abstract": "Vision transformers (ViTs) have demonstrated remarkable performance across various visual tasks. However, ViT models suffer from substantial computational and memory requirements, making it challenging to deploy them on resource-constrained platforms. Quantization is a popular approach for reducing model size, but most studies mainly focus on equal bit-width quantization for the entire network, resulting in sub-optimal solutions. While there are few works on mixed precision quantization (MPQ) for ViTs, they typically rely on search space-based methods or employ mixed precision arbitrarily. In this paper, we introduce LRP-QViT, an explainability-based method for assigning mixed-precision bit allocations to different layers based on their importance during classification. Specifically, to measure the contribution score of each layer in predicting the target class, we employ the Layer-wise Relevance Propagation (LRP) method. LRP assigns local relevance at the output layer and propagates it through all layers, distributing the relevance until it reaches the input layers. These relevance scores serve as indicators for computing the layer contribution score. Additionally, we have introduced a clipped channel-wise quantization aimed at eliminating outliers from post-LayerNorm activations to alleviate severe inter-channel variations. To validate and assess our approach, we employ LRP-QViT across ViT, DeiT, and Swin transformer models on various datasets. Our experimental findings demonstrate that both our fixed-bit and mixed-bit post-training quantization methods surpass existing models in the context of 4-bit and 6-bit quantization.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f996d5ee3b8ad3c60510862a92fd72c6a41777e0.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 7.0
    },
    "f1b6f564e7cba8121df0eef0d656673e3bd18621.pdf": {
        "title": "P3 ViT: A CIM-Based High-Utilization Architecture With Dynamic Pruning and Two-Way Ping-Pong Macro for Vision Transformer",
        "authors": [
            "Xiangqu Fu",
            "Qirui Ren",
            "Hao Wu",
            "Feibin Xiang",
            "Q. Luo",
            "Jinshan Yue",
            "Yong Chen",
            "Feng Zhang"
        ],
        "published_date": "2023",
        "abstract": "Transformers have made remarkable contributions to natural language processing (NLP) and many other fields. Recently, transformer-based models have achieved state-of-the-art (SOTA) performance on computer vision tasks compared with traditional convolutional neural networks (CNNs). Unfortunately, existing CNN accelerators cannot efficiently support transformer due to the high computational overhead and redundant data accesses associated with the \u2018KQV\u2019 matrix operations in the transformer models. If the recently-developed NLP transformer accelerators are applied to the vision transformer (ViT) models, their efficiency would decrease due to three challenges. 1) Redundant data storage and access still exist in ViT data flow scheduling. 2) For matrix transposition in transformer models, the previous transpose-operation schemes lack flexibility, resulting in extra area overhead. 3) The sparse acceleration schemes for NLP in prior transformer accelerators cannot efficiently accelerate ViT with relatively fewer tokens. To overcome these challenges, we propose <inline-formula> <tex-math notation=\"LaTeX\">$P^{3}$ </tex-math></inline-formula> ViT, a computing-in-memory (CIM)-based architecture, to efficiently accelerate ViT, achieving high utilization on data flow scheduling. There are three key contributions: 1) P3ViT architecture supports three ping-pong pipeline scheduling modes, involving inter-core parallel and intra-core ping-pong pipeline mode (IEP-IAP3), inter-core pipeline and parallel mode (IEP2), and full parallel mode, to eliminate redundant memory accesses. 2) A two-way ping-pong CIM macro is proposed, which can be configured to regular calculation mode and transpose calculation mode to adapt to both <inline-formula> <tex-math notation=\"LaTeX\">$\\text{Q}\\times \\text{K}^{\\mathrm {T}}$ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$\\text{A}\\times \\text{V}$ </tex-math></inline-formula> tasks. 3) P3ViT also runs a small prediction network. It prunes redundant tokens to be a standard number hierarchically and dynamically, enabling high-throughput and high-utilization attention computation. Measurements show that P3ViT achieves <inline-formula> <tex-math notation=\"LaTeX\">$1.13\\times $ </tex-math></inline-formula> higher energy efficiency than the state-of-the-art transformer accelerator and achieves <inline-formula> <tex-math notation=\"LaTeX\">$30.8\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$14.6\\times $ </tex-math></inline-formula> speedup compared to CPU and GPU.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f1b6f564e7cba8121df0eef0d656673e3bd18621.pdf",
        "venue": "IEEE Transactions on Circuits and Systems Part 1: Regular Papers",
        "citationCount": 13,
        "score": 6.5
    },
    "12106888ec74baf48d1e9fb64e8932c9ce509e84.pdf": {
        "title": "Face-based age estimation using improved Swin Transformer with attention-based convolution",
        "authors": [
            "Chaojun Shi",
            "Shiwei Zhao",
            "Kecheng Zhang",
            "Yibo Wang",
            "Longping Liang"
        ],
        "published_date": "2023",
        "abstract": "Recently Transformer models is new direction in the computer vision field, which is based on self multihead attention mechanism. Compared with the convolutional neural network, this Transformer uses the self-attention mechanism to capture global contextual information and extract more strong features by learning the association relationship between different features, which has achieved good results in many vision tasks. In face-based age estimation, some facial patches that contain rich age-specific information are critical in the age estimation task. The present study proposed an attention-based convolution (ABC) age estimation framework, called improved Swin Transformer with ABC, in which two separate regions were implemented, namely ABC and Swin Transformer. ABC extracted facial patches containing rich age-specific information using a shallow convolutional network and a multiheaded attention mechanism. Subsequently, the features obtained by ABC were spliced with the flattened image in the Swin Transformer, which were then input to the Swin Transformer to predict the age of the image. The ABC framework spliced the important regions that contained rich age-specific information into the original image, which could fully mobilize the long-dependency of the Swin Transformer, that is, extracting stronger features by learning the dependency relationship between different features. ABC also introduced loss of diversity to guide the training of self-attention mechanism, reducing overlap between patches so that the diverse and important patches were discovered. Through extensive experiments, this study showed that the proposed framework outperformed several state-of-the-art methods on age estimation benchmark datasets.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/12106888ec74baf48d1e9fb64e8932c9ce509e84.pdf",
        "venue": "Frontiers in Neuroscience",
        "citationCount": 13,
        "score": 6.5
    },
    "3798e7f16fe69c29307a9bab4f0f4d779478afc5.pdf": {
        "title": "GenConViT: Deepfake Video Detection Using Generative Convolutional Vision Transformer",
        "authors": [
            "Deressa Wodajo Deressa",
            "Hannes Mareen",
            "Peter Lambert",
            "Solomon Atnafu",
            "Z. Akhtar",
            "Glenn Van Wallendael"
        ],
        "published_date": "2023",
        "abstract": "Deepfakes have raised significant concerns due to their potential to spread false information and compromise the integrity of digital media. Current deepfake detection models often struggle to generalize across a diverse range of deepfake generation techniques and video content. In this work, we propose a Generative Convolutional Vision Transformer (GenConViT) for deepfake video detection. Our model combines ConvNeXt and Swin Transformer models for feature extraction, and it utilizes an Autoencoder and Variational Autoencoder to learn from latent data distributions. By learning from the visual artifacts and latent data distribution, GenConViT achieves an improved performance in detecting a wide range of deepfake videos. The model is trained and evaluated on DFDC, FF++, TM, DeepfakeTIMIT, and Celeb-DF (v2) datasets. The proposed GenConViT model demonstrates strong performance in deepfake video detection, achieving high accuracy across the tested datasets. While our model shows promising results in deepfake video detection by leveraging visual and latent features, we demonstrate that further work is needed to improve its generalizability when encountering out-of-distribution data. Our model provides an effective solution for identifying a wide range of fake videos while preserving the integrity of media.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3798e7f16fe69c29307a9bab4f0f4d779478afc5.pdf",
        "venue": "Applied Sciences",
        "citationCount": 13,
        "score": 6.5
    },
    "29a0077d198418bab2ea4d78d04a892ede860d68.pdf": {
        "title": "Performance Evaluation of Swin Vision Transformer Model using Gradient Accumulation Optimization Technique",
        "authors": [
            "Sanad Aburass",
            "O. Dorgham"
        ],
        "published_date": "2023",
        "abstract": "Vision Transformers (ViTs) have emerged as a promising approach for visual recognition tasks, revolutionizing the field by leveraging the power of transformer-based architectures. Among the various ViT models, Swin Transformers have gained considerable attention due to their hierarchical design and ability to capture both local and global visual features effectively. This paper evaluates the performance of Swin ViT model using gradient accumulation optimization (GAO) technique. We investigate the impact of gradient accumulation optimization technique on the model's accuracy and training time. Our experiments show that applying the GAO technique leads to a significant decrease in the accuracy of the Swin ViT model, compared to the standard Swin Transformer model. Moreover, we detect a significant increase in the training time of the Swin ViT model when GAO model is applied. These findings suggest that applying the GAO technique may not be suitable for the Swin ViT model, and concern should be undertaken when using GAO technique for other transformer-based models.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/29a0077d198418bab2ea4d78d04a892ede860d68.pdf",
        "venue": "arXiv.org",
        "citationCount": 12,
        "score": 6.0
    },
    "ac9cc0c28838a037e77f4e19433de170f47b3de9.pdf": {
        "title": "Transformers for Vision: A Survey on Innovative Methods for Computer Vision",
        "authors": [
            "Vikas Hassija",
            "Balamurugan Palanisamy",
            "Arpita Chatterjee",
            "Arpita Mandal",
            "Debanshi Chakraborty",
            "Amit Pandey",
            "G. Chalapathi",
            "Dhruv Kumar"
        ],
        "published_date": "2025",
        "abstract": "Transformers have emerged as a groundbreaking architecture in the field of computer vision, offering a compelling alternative to traditional convolutional neural networks (CNNs) by enabling the modeling of long-range dependencies and global context through self-attention mechanisms. Originally developed for natural language processing, transformers have now been successfully adapted for a wide range of vision tasks, leading to significant improvements in performance and generalization. This survey provides a comprehensive overview of the fundamental principles of transformer architectures, highlighting the core mechanisms such as self-attention, multi-head attention, and positional encoding that distinguish them from CNNs. We delve into the theoretical adaptations required to apply transformers to visual data, including image tokenization and the integration of positional embeddings. A detailed analysis of key transformer-based vision architectures such as ViT, DeiT, Swin Transformer, PVT, Twins, and CrossViT are presented, alongside their practical applications in image classification, object detection, video understanding, medical imaging, and cross-modal tasks. The paper further compares the performance of vision transformers with CNNs, examining their respective strengths, limitations, and the emergence of hybrid models. Finally, current challenges in deploying ViTs, such as computational cost, data efficiency, and interpretability, and explore recent advancements and future research directions including efficient architectures, self-supervised learning, and multimodal integration are discussed.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ac9cc0c28838a037e77f4e19433de170f47b3de9.pdf",
        "venue": "IEEE Access",
        "citationCount": 6,
        "score": 6.0
    },
    "c4357abf10ff937e4ad62df4289fbbf74f114725.pdf": {
        "title": "Resizer Swin Transformer-Based Classification Using sMRI for Alzheimer\u2019s Disease",
        "authors": [
            "Yihang Huang",
            "Wan Li"
        ],
        "published_date": "2023",
        "abstract": "Structural magnetic resonance imaging (sMRI) is widely used in the clinical diagnosis of diseases due to its advantages: high-definition and noninvasive visualization. Therefore, computer-aided diagnosis based on sMRI images is broadly applied in classifying Alzheimer\u2019s disease (AD). Due to the excellent performance of the Transformer in computer vision, the Vision Transformer (ViT) has been employed for AD classification in recent years. The ViT relies on access to large datasets, while the sample size of brain imaging datasets is relatively insufficient. Moreover, the preprocessing procedures of brain sMRI images are complex and labor-intensive. To overcome the limitations mentioned above, we propose the Resizer Swin Transformer (RST), a deep-learning model that can extract information from brain sMRI images that are only briefly processed to achieve multi-scale and cross-channel features. In addition, we pre-trained our RST on a natural image dataset and obtained better performance. We achieved 99.59% and 94.01% average accuracy on the ADNI and AIBL datasets, respectively. Importantly, the RST has a sensitivity of 99.59%, a specificity of 99.58%, and a precision of 99.83% on the ADNI dataset, which are better than or comparable to state-of-the-art approaches. The experimental results prove that RST can achieve better classification performance in AD prediction compared with CNN-based and Transformer models.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c4357abf10ff937e4ad62df4289fbbf74f114725.pdf",
        "venue": "Applied Sciences",
        "citationCount": 11,
        "score": 5.5
    },
    "0b41c18d0397e14ddacee4143db74a05d774434d.pdf": {
        "title": "ViT-A*: Legged Robot Path Planning using Vision Transformer A*",
        "authors": [
            "Jianwei Liu",
            "Shirui Lyu",
            "Denis Hadjivelichkov",
            "Valerio Modugno",
            "D. Kanoulas"
        ],
        "published_date": "2023",
        "abstract": "Legged robots, particularly quadrupeds, offer promising navigation capabilities, especially in scenarios requiring traversal over diverse terrains and obstacle avoidance. This paper addresses the challenge of enabling legged robots to navigate complex environments effectively through the integration of data-driven path-planning methods. We propose an approach that utilizes differentiable planners, allowing the learning of end-to-end global plans via a neural network for commanding quadruped robots. The approach leverages 2D maps and obstacle specifications as inputs to generate a global path. To enhance the functionality of the developed neural network-based path planner, we use Vision Transformers (ViT) for map preprocessing, to enable the effective handling of larger maps. Experimental evaluations on two real robotic quadrupeds (Boston Dynamics Spot and Unitree Gol) demonstrate the effectiveness and versatility of the proposed approach in generating reliable path plans.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0b41c18d0397e14ddacee4143db74a05d774434d.pdf",
        "venue": "IEEE-RAS International Conference on Humanoid Robots",
        "citationCount": 11,
        "score": 5.5
    },
    "f9e1a8754e77a4b5f240d11e9e81e2563a319b89.pdf": {
        "title": "VHR-BirdPose: Vision Transformer-Based HRNet for Bird Pose Estimation with Attention Mechanism",
        "authors": [
            "Ru He",
            "Xiaomin Wang",
            "Huazhen Chen",
            "Chang Liu"
        ],
        "published_date": "2023",
        "abstract": "Pose estimation plays a crucial role in recognizing and analyzing the postures, actions, and movements of humans and animals using computer vision and machine learning techniques. However, bird pose estimation encounters specific challenges, including bird diversity, posture variation, and the fine granularity of posture. To overcome these challenges, we propose VHR-BirdPose, a method that combines Vision Transformer (ViT) and Deep High-Resolution Network (HRNet) with an attention mechanism. VHR-BirdPose effectively extracts features using Vision Transformer\u2019s self-attention mechanism, which captures global dependencies in the images and allows for better capturing of pose details and changes. The attention mechanism is employed to enhance the focus on bird keypoints, improving the accuracy of pose estimation. By combining HRNet with Vision Transformer, our model can extract multi-scale features while maintaining high-resolution details and incorporating richer semantic information through the attention mechanism. This integration of HRNet and Vision Transformer leverages the advantages of both models, resulting in accurate and robust bird pose estimation. We conducted extensive experiments on the Animal Kingdom dataset to evaluate the performance of VHR-BirdPose. The results demonstrate that our proposed method achieves state-of-the-art performance in bird pose estimation. VHR-BirdPose based on bird images is of great significance for the advancement of bird behaviors, ecological understanding, and the protection of bird populations.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f9e1a8754e77a4b5f240d11e9e81e2563a319b89.pdf",
        "venue": "Electronics",
        "citationCount": 11,
        "score": 5.5
    },
    "4b9568a9798c527779a1f7479ffb3ba373640e3e.pdf": {
        "title": "Vision-Based Cow Tracking and Feeding Monitoring for Autonomous Livestock Farming: The YOLOv5s-CA+DeepSORT-Vision Transformer",
        "authors": [
            "Yangyang Guo",
            "Wenhao Hong",
            "Jiaxin Wu",
            "Xiaoping Huang",
            "Yongliang Qiao",
            "He Kong"
        ],
        "published_date": "2023",
        "abstract": "Animal tracking and feeding monitoring is crucial for automatic individual cow welfare measurement and naturally becomes a prerequisite for autonomous livestock farming systems. The deformable body posture and irregular movement of cows under complex farming environments make tracking of individual animals in a herd very challenging. To tackle the above challenge, a deep learning network-based approach, namely, YOLOv5s-CA+DeepSORT-ViT, is proposed in this article. In our proposed approach, coordinate attention (CA)-integrated YOLOv5 was developed to capture spatial location information to improve the face detection performance for overlapping regions. Then the vision transformer (ViT) was embedded in the reidentification (reID) network Deep Simple Online and Real-time Tracking (DeepSORT) to enhance feature matching and tracking accuracy. The comparative results of the multicow complex dataset constructed from a commercial farm show that the ID F1 score (IDF1) and multitarget tracking accuracy (MOTA) of the proposed YOLOv5s-CA+DeepSORT-ViT are 88.5% and 84.4%, respectively. Meanwhile, the ID switching (ID Sw.) times and the processing time are reduced by 50% and 20% compared to the YOLOv5s+DeepSORT model. Experimental results also showed that the overall cow tracking performance of our proposed approach outperformed the other baselines (e.g. SORT, ByteTrack, BoT-SORT, and DeepSORT).",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/4b9568a9798c527779a1f7479ffb3ba373640e3e.pdf",
        "venue": "IEEE robotics & automation magazine",
        "citationCount": 11,
        "score": 5.5
    },
    "34ca004eda791c8c4191e0ebe65cc4405a116b08.pdf": {
        "title": "Efficient Blind Hyperspectral Unmixing with Non-Local Spatial Information Based on Swin Transformer",
        "authors": [
            "Yun Wang",
            "Shuai Shi",
            "Jie Chen"
        ],
        "published_date": "2023",
        "abstract": "Blind hyperspectral unmixing (HU) involves identifying pixel spectra as distinct materials (endmembers) and simultaneously determining their proportions (abundances) at each pixel. In this paper, we present Swin-HU, a novel method based on the Swin Transformer, designed to efficiently tackle blind HU. This method addresses the limitations of existing techniques, such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViT), in capturing global spatial information and spectral sequence attributes. Swin-HU employs Window Multi-head Self-Attention (W-MSA) and Shifted Window Multi-head Self-Attention (SW-MSA) mechanisms to extract global spatial priors while maintaining linear computational complexity. We evaluate Swin-HU against six other unmixing methods on both synthetic and real datasets, demonstrating its superior performance in endmember extraction and abundance estimation. The source code is available at https://github.com/wangyunjeff/Swin-HU.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/34ca004eda791c8c4191e0ebe65cc4405a116b08.pdf",
        "venue": "IEEE International Geoscience and Remote Sensing Symposium",
        "citationCount": 9,
        "score": 4.5
    },
    "409b43b8cd8a2ba69f93e80c2bacc0126238b550.pdf": {
        "title": "Mobile Vision Transformer-based Visual Object Tracking",
        "authors": [
            "Goutam Yelluru Gopal",
            "Maria A. Amer"
        ],
        "published_date": "2023",
        "abstract": "The introduction of robust backbones, such as Vision Transformers, has improved the performance of object tracking algorithms in recent years. However, these state-of-the-art trackers are computationally expensive since they have a large number of model parameters and rely on specialized hardware (e.g., GPU) for faster inference. On the other hand, recent lightweight trackers are fast but are less accurate, especially on large-scale datasets. We propose a lightweight, accurate, and fast tracking algorithm using Mobile Vision Transformers (MobileViT) as the backbone for the first time. We also present a novel approach of fusing the template and search region representations in the MobileViT backbone, thereby generating superior feature encoding for target localization. The experimental results show that our MobileViT-based Tracker, MVT, surpasses the performance of recent lightweight trackers on the large-scale datasets GOT10k and TrackingNet, and with a high inference speed. In addition, our method outperforms the popular DiMP-50 tracker despite having 4.7 times fewer model parameters and running at 2.8 times its speed on a GPU. The tracker code and models are available at https://github.com/goutamyg/MVT",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/409b43b8cd8a2ba69f93e80c2bacc0126238b550.pdf",
        "venue": "British Machine Vision Conference",
        "citationCount": 8,
        "score": 4.0
    },
    "dc5d707a8f9efaf32d2319ec2ca8d2619ef2926e.pdf": {
        "title": "An Efficient FPGA-Based Accelerator for Swin Transformer",
        "authors": [
            "Zhiyang Liu",
            "Pengyu Yin",
            "Zhenhua Ren"
        ],
        "published_date": "2023",
        "abstract": "Since introduced, Swin Transformer has achieved remarkable results in the field of computer vision, it has sparked the need for dedicated hardware accelerators, specifically catering to edge computing demands. For the advantages of flexibility, low power consumption, FPGAs have been widely employed to accelerate the inference of convolutional neural networks (CNNs) and show potential in Transformer-based models. Unlike CNNs, which mainly involve multiply and accumulate (MAC) operations, Transformer involve non-linear computations such as Layer Normalization (LN), Softmax, and GELU. These nonlinear computations do pose challenges for accelerator design. In this paper, to propose an efficient FPGA-based hardware accelerator for Swin Transformer, we focused on using different strategies to deal with these nonlinear calculations and efficiently handling MAC computations to achieve the best acceleration results. We replaced LN with BN, Given that Batch Normalization (BN) can be fused with linear layers during inference to optimize inference efficiency. The modified Swin-T, Swin-S, and Swin-B respectively achieved Top-1 accuracy rates of 80.7%, 82.7%, and 82.8% in ImageNet. Furthermore, We employed strategies for approximate computation to design hardware-friendly architectures for Softmax and GELU computations. We also designed an efficient Matrix Multiplication Unit to handle all linear computations in Swin Transformer. As a conclude, compared with CPU (AMD Ryzen 5700X), our accelerator achieved 1.76x, 1.66x, and 1.25x speedup and achieved 20.45x, 18.60x, and 14.63x energy efficiency (FPS/power consumption) improvement on Swin-T, Swin-S, and Swin-B models, respectively. Compared to GPU (Nvidia RTX 2080 Ti), we achieved 5.05x, 4.42x, and 3.00x energy efficiency improvement respectively. As far as we know, the accelerator we proposed is the fastest FPGA-based accelerator for Swin Transformer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/dc5d707a8f9efaf32d2319ec2ca8d2619ef2926e.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 3.0
    },
    "9017053fb240d0870779b9658082488b392e7cde.pdf": {
        "title": "Vision Transformer: Vit and its Derivatives",
        "authors": [
            "Zujun Fu"
        ],
        "published_date": "2022",
        "abstract": "Transformer, an attention-based encoder-decoder architecture, has not only revolutionized the field of natural language processing (NLP), but has also done some pioneering work in the field of computer vision (CV). Compared to convolutional neural networks (CNNs), the Vision Transformer (ViT) relies on excellent modeling capabilities to achieve very good performance on several benchmarks such as ImageNet, COCO, and ADE20k. ViT is inspired by the self-attention mechanism in natural language processing, where word embeddings are replaced with patch embeddings. This paper reviews the derivatives of ViT and the cross-applications of ViT with other fields.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9017053fb240d0870779b9658082488b392e7cde.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 2.6666666666666665
    },
    "f27221a15f4c3c8af7ea0ada5bab25831ffe21ea.pdf": {
        "title": "Vision Transformer Based COVID-19 Detection Using Chest CT-scan images",
        "authors": [
            "P. Sahoo",
            "S. Saha",
            "S. Mondal",
            "Suraj Gowda"
        ],
        "published_date": "2022",
        "abstract": "The fast proliferation of the coronavirus around the globe has put several countries' healthcare systems in danger of collapsing. As a result, locating and separating COVID-19-positive patients is a critical task. Deep Learning approaches were used in several computer-aided automated systems that utilized chest computed tomography (CT-scan) or X-ray images to create diagnostic tools. However, current Convolutional Neural Network (CNN) based approaches cannot capture the global context because of inherent image-specific inductive bias. These techniques also require large and labeled datasets to train the algorithm, but not many labeled COVID-19 datasets exist publicly. To mitigate the problem, we have developed a self-attention-based Vision Transformer (ViT) architecture using CT-scan. The proposed ViT model achieves an accuracy of 98.39% on the popular SARS-CoV-2 datasets, outperforming the existing state-of-the-art CNN-based models by 1%. We also provide the characteristics of CT scan images of the COVID-19-affected patients and an error analysis of the model's outcome. Our findings show that the proposed ViT-based model can be an alternative option for medical professionals for effective COVID-19 screening. The implementation details of the proposed model can be accessed at https://github.com/Pranabiitp/ViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f27221a15f4c3c8af7ea0ada5bab25831ffe21ea.pdf",
        "venue": "2022 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)",
        "citationCount": 8,
        "score": 2.6666666666666665
    },
    "2d421d94bc9eed935870088a6f3218244e36dc97.pdf": {
        "title": "Question Aware Vision Transformer for Multimodal Reasoning",
        "authors": [
            "Roy Ganz",
            "Yair Kittenplon",
            "Aviad Aberdam",
            "Elad Ben Avraham",
            "Oren Nuriel",
            "Shai Mazor",
            "Ron Litman"
        ],
        "published_date": "2024",
        "abstract": "Vision-Language (VL) models have gained significant research focus, enabling remarkable advances in multimodal reasoning. These architectures typically comprise a vision encoder, a Large Language Model (LLM), and a projection module that aligns visual features with the LLM's representation space. Despite their success, a critical limitation persists: the vision encoding process remains decoupled from user queries, often in the form of image-related questions. Consequently, the resulting visual features may not be optimally attuned to the query-specific elements of the image. To address this, we introduce QA-ViT, a Question Aware Vision Transformer approach for multimodal reasoning, which embeds question awareness directly within the vision encoder. This integration results in dynamic visual features focusing on relevant image aspects to the posed question. QA-ViT is model-agnostic and can be incorporated efficiently into any VL architecture. Extensive experiments demonstrate the effectiveness of applying our method to various multimodal architectures, leading to consistent improvement across diverse tasks and showcasing its potential for enhancing visual and scene-text understanding.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2d421d94bc9eed935870088a6f3218244e36dc97.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 28,
        "score": 28.0
    },
    "be1aabb6460d49905575da88d564864da9f80417.pdf": {
        "title": "Data-Efficient Vision Transformer Models for Robust Classification of Sugarcane",
        "authors": [
            "Ishak Pa\u00e7al",
            "Ismail Kunduracioglu"
        ],
        "published_date": "2024",
        "abstract": "Sugar cane is an important agricultural product that provides 75% of the world's sugar production. As with all plant species, any disease affecting sugarcane can significantly impact yields and planning. Diagnosing diseases in sugarcane leaves using traditional methods is slow, inefficient, and often lacking in accuracy. This study presents a deep learning-based approach for accurate diagnosis of diseases in sugarcane leaves. Specifically, training and evaluation were conducted on the publicly available Sugarcane Leaf Dataset using leading ViT (Vision Transformer) architectures such as DeiT3-Small and DeiT-Tiny. This dataset includes 11 different disease classes and a total of 6748 images. Additionally, these models were compared with popular CNN models. The findings of the study show that there is no direct relationship between model complexity, depth, and accuracy for the 11-class sugarcane dataset. Among the 12 models tested, the DeiT3-Small model showed the highest performance with 93.79% accuracy, 91.27% precision, and 90.96% F1-score. These results highlight that rapid, accurate, and automatic disease diagnosis systems developed using deep learning techniques can significantly improve sugarcane disease management and contribute to increased yields.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/be1aabb6460d49905575da88d564864da9f80417.pdf",
        "venue": "Journal of Soft Computing and Decision Analytics",
        "citationCount": 19,
        "score": 19.0
    },
    "0d7d27fbd8193acf8db032441fd22945d26e9952.pdf": {
        "title": "YOLO-based CAD framework with ViT transformer for breast mass detection and classification in CESM and FFDM images",
        "authors": [
            "Nada M. Hassan",
            "Safwat Hamad",
            "Khaled Mahar"
        ],
        "published_date": "2024",
        "abstract": "Breast cancer detection is considered a challenging task for the average experienced radiologist due to the variation of the lesions\u2019 size and shape, especially with the existence of high fibro-glandular tissues. The revolution of deep learning and computer vision contributes recently in introducing systems that can provide an automated diagnosis for breast cancer that can act as a second opinion for doctors/radiologists. The most of previously proposed deep learning-based Computer-Aided Diagnosis (CAD) systems mainly utilized Convolutional Neural Networks (CNN) that focuses on local features. Recently, vision transformers (ViT) have shown great potential in image classification tasks due to its ability in learning the local and global spatial features. This paper proposes a fully automated CAD framework based on YOLOv4 network and ViT transformers for mass detection and classification of Contrast Enhanced Spectral Mammography (CESM) images. CESM is an evolution type of Full Field Digital Mammography (FFDM) images that provides enhanced visualization for breast tissues. Different experiments were conducted to evaluate the proposed framework on two different datasets that are INbreast and CDD-CESM that provides both FFDM and CESM images. The model achieved at mass detection a mean Average Precision (mAP) score of 98.69%, 81.52%, and 71.65% and mass classification accuracy of 95.65%, 97.61%, and 80% for INbreast, CE-CESM, and DM-CESM, respectively. The proposed framework showed competitive results regarding the state-of-the-art models in INbreast. It outperformed the previous work in the literature in terms of the F1-score by almost 5% for mass detection in CESM. Moreover, the experiments showed that the CESM could provide more morphological features that can be more informative, especially with the highly dense breast tissues.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0d7d27fbd8193acf8db032441fd22945d26e9952.pdf",
        "venue": "Neural computing & applications (Print)",
        "citationCount": 18,
        "score": 18.0
    },
    "0af7ccb9f6cf2edb35674dc97d0ec5d165f8433b.pdf": {
        "title": "A Deep Learning-Based Approach for Cervical Cancer Classification Using 3D CNN and Vision Transformer.",
        "authors": [
            "Abinaya K",
            "S. B"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0af7ccb9f6cf2edb35674dc97d0ec5d165f8433b.pdf",
        "venue": "Journal of imaging informatics in medicine",
        "citationCount": 16,
        "score": 16.0
    },
    "f9efe4918b9a20c2e8084d7a5ec624b87ec1f211.pdf": {
        "title": "QClusformer: A Quantum Transformer-based Framework for Unsupervised Visual Clustering",
        "authors": [
            "Xuan-Bac Nguyen",
            "Hoang-Quan Nguyen",
            "Samuel Yen-Chi Chen",
            "S. U. Khan",
            "Hugh Churchill",
            "Khoa Luu"
        ],
        "published_date": "2024",
        "abstract": "Unsupervised vision clustering, a cornerstone in computer vision, has been studied for decades, yielding signif-icant outcomes across numerous vision tasks. However, these algorithms involve substantial computational demands when confronted with vast amounts of unlabeled data. Conversely, quantum computing holds promise in expediting unsupervised algorithms when handling large-scale databases. In this study, we introduce QClusformer, a pioneering Transformer-based frame-work leveraging quantum machines to tackle unsupervised vision clustering challenges. Specifically, we design the Transformer architecture, including the self-attention module and transformer blocks, from a quantum perspective to enable execution on quan-tum hardware. In addition, we present QClusformer, a variant based on the Transformer architecture, tailored for unsupervised vision clustering tasks. By integrating these elements into an end-to-end framework, QClusformer consistently outperforms previous methods running on classical computers. Empirical evaluations across diverse benchmarks, including MS-Celeb-1M and DeepFashion, underscore the superior performance of QClusformer compared to state-of-the-art methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f9efe4918b9a20c2e8084d7a5ec624b87ec1f211.pdf",
        "venue": "International Conference on Quantum Computing and Engineering",
        "citationCount": 16,
        "score": 16.0
    },
    "f62cb393ab51505bdd22823e74b7cf28f4801e1c.pdf": {
        "title": "ViT-PSO-SVM: Cervical Cancer Predication Based on Integrating Vision Transformer with Particle Swarm Optimization and Support Vector Machine",
        "authors": [
            "Abdulaziz Almohimeed",
            "Mohamed Shehata",
            "Nora El-Rashidy",
            "Sherif Mostafa",
            "Amira Samy Talaat",
            "Hager Saleh"
        ],
        "published_date": "2024",
        "abstract": "Cervical cancer (CCa) is the fourth most prevalent and common cancer affecting women worldwide, with increasing incidence and mortality rates. Hence, early detection of CCa plays a crucial role in improving outcomes. Non-invasive imaging procedures with good diagnostic performance are desirable and have the potential to lessen the degree of intervention associated with the gold standard, biopsy. Recently, artificial intelligence-based diagnostic models such as Vision Transformers (ViT) have shown promising performance in image classification tasks, rivaling or surpassing traditional convolutional neural networks (CNNs). This paper studies the effect of applying a ViT to predict CCa using different image benchmark datasets. A newly developed approach (ViT-PSO-SVM) was presented for boosting the results of the ViT based on integrating the ViT with particle swarm optimization (PSO), and support vector machine (SVM). First, the proposed framework extracts features from the Vision Transformer. Then, PSO is used to reduce the complexity of extracted features and optimize feature representation. Finally, a softmax classification layer is replaced with an SVM classification model to precisely predict CCa. The models are evaluated using two benchmark cervical cell image datasets, namely SipakMed and Herlev, with different classification scenarios: two, three, and five classes. The proposed approach achieved 99.112% accuracy and 99.113% F1-score for SipakMed with two classes and achieved 97.778% accuracy and 97.805% F1-score for Herlev with two classes outperforming other Vision Transformers, CNN models, and pre-trained models. Finally, GradCAM is used as an explainable artificial intelligence (XAI) tool to visualize and understand the regions of a given image that are important for a model\u2019s prediction. The obtained experimental results demonstrate the feasibility and efficacy of the developed ViT-PSO-SVM approach and hold the promise of providing a robust, reliable, accurate, and non-invasive diagnostic tool that will lead to improved healthcare outcomes worldwide.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f62cb393ab51505bdd22823e74b7cf28f4801e1c.pdf",
        "venue": "Bioengineering",
        "citationCount": 14,
        "score": 14.0
    },
    "4702a22a3c2da1284a88d5e608d38cd106d66736.pdf": {
        "title": "A Simple Yet Effective Network Based on Vision Transformer for Camouflaged Object and Salient Object Detection",
        "authors": [
            "Chao Hao",
            "Zitong Yu",
            "Xin Liu",
            "Jun Xu",
            "Huanjing Yue",
            "Jingyu Yang"
        ],
        "published_date": "2024",
        "abstract": "Camouflaged object detection (COD) and salient object detection (SOD) are two distinct yet closely-related computer vision tasks widely studied during the past decades. Though sharing the same purpose of segmenting an image into binary foreground and background regions, their distinction lies in the fact that COD focuses on concealed objects hidden in the image, while SOD concentrates on the most prominent objects in the image. Building universal segmentation models is currently a hot topic in the community. Previous works achieved good performance on certain task by stacking various hand-designed modules and multi-scale features. However, these careful task-specific designs also make them lose their potential as general-purpose architectures. Therefore, we hope to build general architectures that can be applied to both tasks. In this work, we propose a simple yet effective network (SENet) based on vision Transformer (ViT), by employing a simple design of an asymmetric ViT-based encoder-decoder structure, we yield competitive results on both tasks, exhibiting greater versatility than meticulously crafted ones. To enhance the performance of universal architectures on both tasks, we propose some general methods targeting some common difficulties of the two tasks. First, we use image reconstruction as an auxiliary task during training to increase the difficulty of training, forcing the network to have a better perception of the image as a whole to help with segmentation tasks. In addition, we propose a local information capture module (LICM) to make up for the limitations of the patch-level attention mechanism in pixel-level COD and SOD tasks and a dynamic weighted loss (DW loss) to solve the problem that small target samples are more difficult to locate and segment in both tasks. Finally, we also conduct a preliminary exploration of joint training, trying to use one model to complete two tasks simultaneously. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method. The code is available at https://github.com/linuxsino/SENet.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/4702a22a3c2da1284a88d5e608d38cd106d66736.pdf",
        "venue": "IEEE Transactions on Image Processing",
        "citationCount": 14,
        "score": 14.0
    },
    "9fcea59a7076064f5ac3949177307c1637473ffd.pdf": {
        "title": "Scalable Industrial Visual Anomaly Detection With Partial Semantics Aggregation Vision Transformer",
        "authors": [
            "Haiming Yao",
            "Wei Luo",
            "Jianan Lou",
            "Wen-yong Yu",
            "Xiaotian Zhang",
            "Zhenfeng Qiang",
            "Hui Shi"
        ],
        "published_date": "2024",
        "abstract": "In recent years, the field of industrial visual anomaly detection (VAD) has attracted significant attention in the context of advanced smart manufacturing systems. However, several limitations remain unresolved in existing approaches. While these methods can achieve satisfactory performance when training separate models for different categories, their scalability and performance suffer when faced with the challenge of simultaneous training for multiple categories. Reconstruction-based methods generally suffer from the identical mapping problem. To address these limitations, this study introduces the partial semantic aggregation vision transformer (PSA-VT), a scalable framework for industrial visual anomaly detection (VAD) that enables simultaneous multicategory anomaly detection using a single model. Our proposed PSA-VT framework adopts a hybrid design strategy. First, a pretrained convolutional neural network (CNN) is employed to extract multiscale discriminative local representation. Subsequently, the PSA-VT is introduced to perform representation reconstruction through long-range global semantic aggregation. Finally, the anomalous properties can be estimated by evaluating the reconstruction error of the representations. We conducted extensive experiments using the Mvtec AD industrial anomaly detection dataset, as well as the semantic anomaly detection datasets. The experimental results demonstrate that our method achieves state-of-the-art (SOTA) performance by capturing high-level semantics. Notably, PSA-VT surpasses other methods for the one-model-15-category anomaly detection tasks on the Mvtec AD dataset. Furthermore, we applied incremental learning techniques to enable the rapid deployment of PSA-VT in a real industrial scenario.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9fcea59a7076064f5ac3949177307c1637473ffd.pdf",
        "venue": "IEEE Transactions on Instrumentation and Measurement",
        "citationCount": 14,
        "score": 14.0
    },
    "1ec9b653475287e95fdaef2f5247f82a8376c56c.pdf": {
        "title": "EQ-ViT: Algorithm-Hardware Co-Design for End-to-End Acceleration of Real-Time Vision Transformer Inference on Versal ACAP Architecture",
        "authors": [
            "Peiyan Dong",
            "Jinming Zhuang",
            "Zhuoping Yang",
            "Shixin Ji",
            "Yanyu Li",
            "Dongkuan Xu",
            "Heng Huang",
            "Jingtong Hu",
            "Alex K. Jones",
            "Yiyu Shi",
            "Yanzhi Wang",
            "Peipei Zhou"
        ],
        "published_date": "2024",
        "abstract": "While vision transformers (ViTs) have shown consistent progress in computer vision, deploying them for real-time decision-making scenarios (<1 ms) is challenging. Current computing platforms like CPUs, GPUs, or FPGA-based solutions struggle to meet this deterministic low-latency real-time requirement, even with quantized ViT models. Some approaches use pruning or sparsity to reduce the model size and latency, but this often results in accuracy loss. To address the aforementioned constraints, in this work, we propose EQ-ViT, an end-to-end acceleration framework with the novel algorithm and architecture co-design features to enable the real-time ViT acceleration on the AMD Versal adaptive compute acceleration platform (ACAP). The contributions are four-fold. First, we perform in-depth kernel-level performance profiling and analysis and explain the bottlenecks for the existing acceleration solutions on GPU, FPGA, and ACAP. Second, on the hardware level, we introduce a new spatial and heterogeneous accelerator architecture, the EQ-ViT architecture. This architecture leverages the heterogeneous features of ACAP, where both FPGA and artificial intelligence engines (AIEs) coexist on the same system-on-chip (SoC). Third, On the algorithm level, we create a comprehensive quantization-aware training strategy, the EQ-ViT algorithm. This strategy concurrently quantizes both the weights and activations into 8-bit integers, aiming to improve the accuracy rather than compromise it during quantization. Notably, the method also quantizes nonlinear functions for efficient hardware implementation. Fourth, we design the EQ-ViT automation framework to implement the EQ-ViT architecture for four different ViT applications on the AMD Versal ACAP VCK190 board, achieving accuracy improvement with 2.4%, and average speedups of 315.0, 3.39, 3.38, 14.92, 59.5, and $13.1\\times $ over computing solutions of Intel Xeon 8375C vCPU, Nvidia A10G, A100, Jetson AGX Orin GPUs, AMD ZCU102, and U250 FPGAs. The energy efficiency gains are 62.2, 15.33, 12.82, 13.31, 13.5, and $21.9\\times $ .",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1ec9b653475287e95fdaef2f5247f82a8376c56c.pdf",
        "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems",
        "citationCount": 13,
        "score": 13.0
    },
    "2b48f5f0769bf41b5a3025ace73417d19de65cf1.pdf": {
        "title": "Generalized Single-Image-Based Morphing Attack Detection Using Deep Representations from Vision Transformer",
        "authors": [
            "Haoyu Zhang",
            "Raghavendra Ramachandra",
            "Kiran B. Raja",
            "Christoph Busch"
        ],
        "published_date": "2024",
        "abstract": "Face morphing attacks have posed severe threats to Face Recognition Systems (FRS), which are operated in border control and passport issuance use cases. Correspondingly, morphing attack detection algorithms (MAD) are needed to defend against such attacks. MAD approaches must be robust enough to handle unknown attacks in an open-set scenario where attacks can originate from various morphing generation algorithms, post-processing and the diversity of printers/scanners. The problem of generalization is further pronounced when the detection has to be made on a single suspected image. In this paper, we propose a generalized single-image-based MAD (S-MAD) algorithm by learning the encoding from Vision Transformer (ViT) architecture. Compared to CNN-based architectures, ViT model has the advantage on integrating local and global information and hence can be suitable to detect the morphing traces widely distributed among the face region. Extensive experiments are carried out on face morphing datasets generated using publicly available FRGC face datasets. Several state-of-the-art (SOTA) MAD algorithms, including representative ones that have been publicly evaluated, have been selected and benchmarked with our ViT-based approach. Obtained results demonstrate the improved detection performance of the proposed S-MAD method on inter-dataset testing (when different data is used for training and testing) and comparable performance on intra-dataset testing (when the same data is used for training and testing) experimental protocol.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2b48f5f0769bf41b5a3025ace73417d19de65cf1.pdf",
        "venue": "2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
        "citationCount": 11,
        "score": 11.0
    },
    "bfb310434204d99fa5c420ac272966c5ae1c1bd2.pdf": {
        "title": "Facial Beauty Prediction Based on Vision Transformer",
        "authors": [
            "D. E. Boukhari"
        ],
        "published_date": "2024",
        "abstract": "Facial beauty analysis is a crucial subject in human culture among researchers across various applications. Recent studies have utilized multidisciplinary approaches to examine the relationship between facial traits, age, emotions, and other factors. Facial beauty prediction is a significant visual recognition challenge that evaluates facial attractiveness for human perception. This task demands considerable effort due to the novelty of the field and the limited resources available, including a small database for facial beauty prediction. In this context, a deep learning method has recently shown remarkable capabilities in predicting facial beauty. Additionally, vision Transformers have recently been introduced as novel deep learning approaches and have shown strong performance in various applications. The key issue is that the vision transformer performs significantly worse than ResNet when trained on a small ImageNet database. In this paper, we propose to address the challenges of predicting facial beauty by utilizing vision transformers instead of relying on feature extraction based on Convolutional Neural Networks, which are commonly used in traditional methods. Moreover, we define and optimize a set of hyperparameters according to the SCUT-FBP5500 benchmark dataset. The model achieves a Pearson coefficient of 0.9534. Experimental results indicated that using this proposed network leads to better predicting facial beauty closer to human evaluation than conventional technology that provides facial beauty assessment.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/bfb310434204d99fa5c420ac272966c5ae1c1bd2.pdf",
        "venue": "International Journal of Electrical and Electronic Engineering &amp; Telecommunications",
        "citationCount": 11,
        "score": 11.0
    },
    "bbe5dfbecfd1bed7556b9c8269b0d363faa24973.pdf": {
        "title": "Optimized Data Distribution Learning for Enhancing Vision Transformer\u2010Based Object Detection in Remote Sensing Images",
        "authors": [
            "Huaxiang Song",
            "Junping Xie",
            "Yunyang Wang",
            "Lihua Fu",
            "Yang Zhou",
            "Xing Zhou"
        ],
        "published_date": "2025",
        "abstract": "Existing Vision Transformer (ViT)\u2010based object detection methods for remote sensing images (RSIs) face significant challenges due to the scarcity of RSI samples and the over\u2010reliance on enhancement strategies originally developed for natural images. This often leads to inconsistent data distributions between training and testing subsets, resulting in degraded model performance. In this study, we introduce an optimized data distribution learning (ODDL) strategy and develop an object detection framework based on the Faster R\u2010CNN architecture, named ODDL\u2010Net. The ODDL strategy begins with an optimized augmentation (OA) technique, overcoming the limitations of conventional data augmentation methods. Next, we propose an optimized mosaic algorithm (OMA), improving upon the shortcomings of traditional Mosaic augmentation techniques. Additionally, we introduce a feature fusion regularization (FFR) method, addressing the inherent limitations of classic feature pyramid networks. These innovations are integrated into three modular, plug\u2010and\u2010play components\u2014namely, the OA, OMA, and FFR modules\u2014ensuring that the ODDL strategy can be seamlessly incorporated into existing detection frameworks without requiring significant modifications. To evaluate the effectiveness of the proposed ODDL\u2010Net, we develop two variants based on different ViT architectures: the Next ViT (NViT) small model and the Swin Transformer (SwinT) tiny model, both used as detection backbones. Experimental results on the NWPU10, DIOR20, MAR20, and GLH\u2010Bridge datasets demonstrate that both variants of ODDL\u2010Net achieve impressive accuracy, surpassing 23 state\u2010of\u2010the\u2010art methods introduced since 2023. Specifically, ODDL\u2010Net\u2010NViT attained accuracies of 78.3% on the challenging DIOR20 dataset and 61.4% on the GLH\u2010Bridge dataset. Notably, this represents a substantial improvement of approximately 23% over the Faster R\u2010CNN\u2010ResNet50 baseline on the DIOR20 dataset. In conclusion, this study demonstrates that ViTs are well suited for high\u2010accuracy object detection in RSIs. Furthermore, it provides a straightforward solution for building ViT\u2010based detectors, offering a practical approach that requires little model modification.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/bbe5dfbecfd1bed7556b9c8269b0d363faa24973.pdf",
        "venue": "Photogrammetric Record",
        "citationCount": 11,
        "score": 11.0
    },
    "be28edb064e2050e0852ff376434ee39f7c4b0a3.pdf": {
        "title": "VTIL: A multi-layer indoor location algorithm for RSSI images based on vision transformer",
        "authors": [
            "Heng Zhou",
            "Jingmin Yang",
            "Shanghui Deng",
            "Wenjie Zhang"
        ],
        "published_date": "2024",
        "abstract": "As WiFi technology becomes more widespread and integrated, precise location tracking within complex indoor environments is gaining significance in contemporary public spaces.Nevertheless, challenges persist in indoor scenarios, including issues such as noise and multi-path effects stemming from the degradation of the Received Signal Strength Indicator (RSSI). Additionally, obstacles causing signal shading contribute to a decrease in the accuracy of indoor location tracking. Within this paper, we introduce an innovative indoor localization algorithm termed Vision Transformer Indoor Localization (VTIL). This algorithmleverages RSSI and Vision-Transformer (ViT) technologies to enhance indoor positioning accuracy. Initially, the RSSI fingerprints undergo normalization by scaling them to the maximum and minimum values. To mitigate the impact of noise and irrelevant features, the Principal Component Analysis (PCA) algorithmis then employed for effective feature extraction.Secondly, the RSSI fingerprint library is converted into an RSSI gray image library. Then the RSSI gray image is divided into several small blocks, and the position-coding input is performed in the form of a sequence. The ViT model divides the weight ratio of each block in the RSSI image to alleviate the impact of multi-path effects. According to the experimental results using public datasets, our approach achieves a noteworthy 37.26% reduction in the average distance estimation error when compared to existing indoor fingerprint localization algorithms.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/be28edb064e2050e0852ff376434ee39f7c4b0a3.pdf",
        "venue": "Engineering Research Express",
        "citationCount": 10,
        "score": 10.0
    },
    "e25a0b06079966b8e43f8e1f2455913266cb7426.pdf": {
        "title": "Automated Ischemic Stroke Classification from MRI Scans: Using a Vision Transformer Approach",
        "authors": [
            "Wafae Abbaoui",
            "Sara Retal",
            "Soumia Ziti",
            "Brahim El Bhiri"
        ],
        "published_date": "2024",
        "abstract": "Background: This study evaluates the performance of a vision transformer (ViT) model, ViT-b16, in classifying ischemic stroke cases from Moroccan MRI scans and compares it to the Visual Geometry Group 16 (VGG-16) model used in a prior study. Methods: A dataset of 342 MRI scans, categorized into \u2018Normal\u2019 and \u2019Stroke\u2019 classes, underwent preprocessing using TensorFlow\u2019s tf.data API. Results: The ViT-b16 model was trained and evaluated, yielding an impressive accuracy of 97.59%, surpassing the VGG-16 model\u2019s 90% accuracy. Conclusions: This research highlights the ViT-b16 model\u2019s superior classification capabilities for ischemic stroke diagnosis, contributing to the field of medical image analysis. By showcasing the efficacy of advanced deep learning architectures, particularly in the context of Moroccan MRI scans, this study underscores the potential for real-world clinical applications. Ultimately, our findings emphasize the importance of further exploration into AI-based diagnostic tools for improving healthcare outcomes.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e25a0b06079966b8e43f8e1f2455913266cb7426.pdf",
        "venue": "Journal of Clinical Medicine",
        "citationCount": 10,
        "score": 10.0
    },
    "ecd9598308161557d6ac35b3e4d32770489e811d.pdf": {
        "title": "Adaptively Bypassing Vision Transformer Blocks for Efficient Visual Tracking",
        "authors": [
            "Xiangyang Yang",
            "Dan Zeng",
            "Xucheng Wang",
            "You Wu",
            "Hengzhou Ye",
            "Qijun Zhao",
            "Shuiwang Li"
        ],
        "published_date": "2024",
        "abstract": "Empowered by transformer-based models, visual tracking has advanced significantly. However, the slow speed of current trackers limits their applicability on devices with constrained computational resources. To address this challenge, we introduce ABTrack, an adaptive computation framework that adaptively bypassing transformer blocks for efficient visual tracking. The rationale behind ABTrack is rooted in the observation that semantic features or relations do not uniformly impact the tracking task across all abstraction levels. Instead, this impact varies based on the characteristics of the target and the scene it occupies. Consequently, disregarding insignificant semantic features or relations at certain abstraction levels may not significantly affect the tracking accuracy. We propose a Bypass Decision Module (BDM) to determine if a transformer block should be bypassed, which adaptively simplifies the architecture of ViTs and thus speeds up the inference process. To counteract the time cost incurred by the BDMs and further enhance the efficiency of ViTs, we introduce a novel ViT pruning method to reduce the dimension of the latent representation of tokens in each transformer block. Extensive experiments on multiple tracking benchmarks validate the effectiveness and generality of the proposed method and show that it achieves state-of-the-art performance. Code is released at: https://github.com/xyyang317/ABTrack.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ecd9598308161557d6ac35b3e4d32770489e811d.pdf",
        "venue": "Pattern Recognition",
        "citationCount": 10,
        "score": 10.0
    },
    "7dc4b2930870e66caa7ff23b5d447283a6171452.pdf": {
        "title": "SWHFormer: A Vision Transformer for Significant Wave Height Estimation From Nautical Radar Images",
        "authors": [
            "Zhiding Yang",
            "Weimin Huang"
        ],
        "published_date": "2024",
        "abstract": "This article presented a novel significant wave height (SWH) estimation method, SWHFormer, which incorporates the Vision Transformer (ViT) to estimate SWH from X-band nautical radar images. Unlike traditional convolutional neural networks (CNNs), the ViT model treats the input as a sequence, capitalizing on its attention mechanism to capture long-range dependencies, resulting in superior performance in capturing the complex patterns present in sea wave dynamics. The radar data undergo an image denoising routine, followed by patching, flattening, and embedding processes to form a sequence fed into the transformer encoding module. The outputs from the encoder are then aggregated to derive the final regression result, i.e., SWH estimation. To evaluate the performance of SWHFormer, the dataset collected by a Decca radar aboard a free-navigating vessel is analyzed, and both buoy and model-based data are used as ground truth. In this study, two traditional linear fitting methods, i.e., ensemble empirical mode decomposition (EEMD) and variational mode decomposition (VMD)-based approaches, and a recent deep learning algorithm, convolutional gated recurrent unit (CGRU) network, are exploited for comparison with SWHFormer. It is found that the root mean square error (RMSE) of the estimated results using the proposed SWHFormer is decreased from 0.29, 0.26, and 0.18 m to 0.16 m after the temporal moving average, respectively, compared with the above three methods, when the buoy-measured SWH is served as ground truth. Besides, it is decreased from 0.30, 0.28, and 0.16 m to 0.14 m, respectively, when the model-based SWH is used as reference.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7dc4b2930870e66caa7ff23b5d447283a6171452.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 10,
        "score": 10.0
    },
    "b1f0ac0b5e4044251dcdcce78e0046feb43cc7c7.pdf": {
        "title": "LF-ViT: Reducing Spatial Redundancy in Vision Transformer for Efficient Image Recognition",
        "authors": [
            "Youbing Hu",
            "Yun Cheng",
            "Anqi Lu",
            "Zhiqiang Cao",
            "Dawei Wei",
            "Jie Liu",
            "Zhijun Li"
        ],
        "published_date": "2024",
        "abstract": "The Vision Transformer (ViT) excels in accuracy when handling high-resolution images, yet it confronts the challenge of significant spatial redundancy, leading to increased computational and memory requirements. To address this, we present the Localization and Focus Vision Transformer (LF-ViT). This model operates by strategically curtailing computational demands without impinging on performance. In the Localization phase, a reduced-resolution image is processed; if a definitive prediction remains elusive, our pioneering Neighborhood Global Class Attention (NGCA) mechanism is triggered, effectively identifying and spotlighting class-discriminative regions based on initial findings. Subsequently, in the Focus phase, this designated region is used from the original image to enhance recognition. Uniquely, LF-ViT employs consistent parameters across both phases, ensuring seamless end-to-end optimization. Our empirical tests affirm LF-ViT's prowess: it remarkably decreases Deit-S's FLOPs by 63% and concurrently amplifies throughput twofold. Code of this project is at https://github.com/edgeai1/LF-ViT.git.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b1f0ac0b5e4044251dcdcce78e0046feb43cc7c7.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 9,
        "score": 9.0
    },
    "903c8a82f5539b3e482efcb23ff59819f04f2c0f.pdf": {
        "title": "Vision transformer-based visual language understanding of the construction process",
        "authors": [
            "Bin Yang",
            "Binghan Zhang",
            "Yilong Han",
            "Boda Liu",
            "Jiniming Hu",
            "Yiming Jin"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/903c8a82f5539b3e482efcb23ff59819f04f2c0f.pdf",
        "venue": "Alexandria Engineering Journal",
        "citationCount": 9,
        "score": 9.0
    },
    "1e95bb5827dc784547a46058793c15effd74dccc.pdf": {
        "title": "Liveness Detection in Computer Vision: Transformer-Based Self-Supervised Learning for Face Anti-Spoofing",
        "authors": [
            "Arman Keresh",
            "Pakizar Shamoi"
        ],
        "published_date": "2024",
        "abstract": "Face recognition systems are increasingly used in biometric security for convenience and effectiveness. However, they remain vulnerable to spoofing attacks, where attackers use photos, videos, or masks to impersonate legitimate users. This research addresses these vulnerabilities by exploring the Vision Transformer (ViT) architecture, fine-tuned with the DINO framework utilizing CelebA-Spoof, CASIA SURF, and a proprietary dataset. The DINO framework facilitates self-supervised learning, enabling the model to learn distinguishing features from unlabeled data. We compared the performance of the proposed fine-tuned ViT model using the DINO framework against traditional models, including CNN Model EfficientNet b2, EfficientNet b2 (Noisy Student), and Mobile ViT on the face anti-spoofing task. Numerous tests on standard datasets show that the ViT model performs better than other models in terms of accuracy and resistance to different spoofing methods. Our model\u2019s superior performance, particularly in APCER (1.6%), the most critical metric in this domain, underscores its improved ability to detect spoofing relative to other models. Additionally, we collected our own dataset from a biometric application to validate our findings further. This study highlights the superior performance of transformer-based architecture in identifying complex spoofing cues, leading to significant advancements in biometric security.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1e95bb5827dc784547a46058793c15effd74dccc.pdf",
        "venue": "IEEE Access",
        "citationCount": 9,
        "score": 9.0
    },
    "2104eea2b01b7a4b6ba2d256769328ac4919afad.pdf": {
        "title": "Synthetic CT generation based on CBCT using improved vision transformer CycleGAN",
        "authors": [
            "Yuxin Hu",
            "Han Zhou",
            "Ning Cao",
            "Can Li",
            "Can Hu"
        ],
        "published_date": "2024",
        "abstract": "Cone-beam computed tomography (CBCT) is a crucial component of adaptive radiation therapy; however, it frequently encounters challenges such as artifacts and noise, significantly constraining its clinical utility. While CycleGAN is a widely employed method for CT image synthesis, it has notable limitations regarding the inadequate capture of global features. To tackle these challenges, we introduce a refined unsupervised learning model called improved vision transformer CycleGAN (IViT-CycleGAN). Firstly, we integrate a U-net framework that builds upon ViT. Next, we augment the feed-forward neural network by incorporating deep convolutional networks. Lastly, we enhance the stability of the model training process by introducing gradient penalty and integrating an additional loss term into the generator loss. The experiment demonstrates from multiple perspectives that our model-generated synthesizing CT(sCT) has significant advantages compared to other unsupervised learning models, thereby validating the clinical applicability and robustness of our model. In future clinical practice, our model has the potential to assist clinical practitioners in formulating precise radiotherapy plans.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2104eea2b01b7a4b6ba2d256769328ac4919afad.pdf",
        "venue": "Scientific Reports",
        "citationCount": 9,
        "score": 9.0
    },
    "8972ebf4f9ec47ee60c3ed0effd520daff495d5a.pdf": {
        "title": "Identification of Anomalies in Lung and Colon Cancer Using Computer Vision-Based Swin Transformer with Ensemble Model on Histopathological Images",
        "authors": [
            "Abdulkream A Alsulami",
            "Aishah Albarakati",
            "A. A. Al-Ghamdi",
            "Mahmoud Ragab"
        ],
        "published_date": "2024",
        "abstract": "Lung and colon cancer (LCC) is a dominant life-threatening disease that needs timely attention and precise diagnosis for efficient treatment. The conventional diagnostic techniques for LCC regularly encounter constraints in terms of efficiency and accuracy, thus causing challenges in primary recognition and treatment. Early diagnosis of the disease can immensely reduce the probability of death. In medical practice, the histopathological study of the tissue samples generally uses a classical model. Still, the automated devices that exploit artificial intelligence (AI) techniques produce efficient results in disease diagnosis. In histopathology, both machine learning (ML) and deep learning (DL) approaches can be deployed owing to their latent ability in analyzing and predicting physically accurate molecular phenotypes and microsatellite uncertainty. In this background, this study presents a novel technique called Lung and Colon Cancer using a Swin Transformer with an Ensemble Model on the Histopathological Images (LCCST-EMHI). The proposed LCCST-EMHI method focuses on designing a DL model for the diagnosis and classification of the LCC using histopathological images (HI). In order to achieve this, the LCCST-EMHI model utilizes the bilateral filtering (BF) technique to get rid of the noise. Further, the Swin Transformer (ST) model is also employed for the purpose of feature extraction. For the LCC detection and classification process, an ensemble deep learning classifier is used with three techniques: bidirectional long short-term memory with multi-head attention (BiLSTM-MHA), Double Deep Q-Network (DDQN), and sparse stacked autoencoder (SSAE). Eventually, the hyperparameter selection of the three DL models can be implemented utilizing the walrus optimization algorithm (WaOA) method. In order to illustrate the promising performance of the LCCST-EMHI approach, an extensive range of simulation analyses was conducted on a benchmark dataset. The experimentation results demonstrated the promising performance of the LCCST-EMHI approach over other recent methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/8972ebf4f9ec47ee60c3ed0effd520daff495d5a.pdf",
        "venue": "Bioengineering",
        "citationCount": 8,
        "score": 8.0
    },
    "0fde8e3287ac04dcf210b73898c8ceb697ad489f.pdf": {
        "title": "ConViTML: A Convolutional Vision Transformer-Based Meta-Learning Framework for Real-Time Edge Network Traffic Classification",
        "authors": [
            "Lu Yang",
            "Songtao Guo",
            "Defang Liu",
            "Yue Zeng",
            "Xianlong Jiao",
            "Yuhao Zhou"
        ],
        "published_date": "2024",
        "abstract": "Traditional traffic classification methods struggle to identify emerging network traffic due to the need for model retraining, which hampers the real-time response of deployed edge devices. Moreover, emerging network traffic samples are often scarce, and traditional methods often treat a session as a single image, thereby overlooking essential structural features. These factors can result in poor generalization ability of the trained model. To overcome these challenges, we propose ConViTML (Convolutional Vision Transformer-based Meta-Learning), a real-time end-to-end network traffic classification framework that employs meta-learning to avoid model retraining. We propose a novel feature extraction network, Convolutional Visual Transformer (ConViT), merging Convolutional Neural Network (CNN) and Visual Transformer (ViT). ConViT can directly extract low-dimensional discriminative features containing basic and structural features of the session, which is vital for improving detection accuracy and accelerating convergence in a data-scarce environment. Furthermore, we employ a Packet-based Relation Network (PRN) to analyze the matching degree of support samples and query samples. Therefore, accurate classification in novel traffic identification tasks can be achieved with just a few labeled samples, eliminating extensive data collection and labeling operations. Finally, we replace various feature extractors and compare our approach with the classic meta-learning framework Relation Network (RelationNet). Extensive experimental results demonstrate that ConViTML outperforms others with various performance indicators.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0fde8e3287ac04dcf210b73898c8ceb697ad489f.pdf",
        "venue": "IEEE Transactions on Network and Service Management",
        "citationCount": 8,
        "score": 8.0
    },
    "9a4718faa07a32cf1dce745062181d3342e9b054.pdf": {
        "title": "GNViT- An enhanced image-based groundnut pest classification using Vision Transformer (ViT) model",
        "authors": [
            "Venkatasaichandrakanth P",
            "I. M"
        ],
        "published_date": "2024",
        "abstract": "Crop losses caused by diseases and pests present substantial challenges to global agriculture, with groundnut crops particularly vulnerable to their detrimental effects. This study introduces the Groundnut Vision Transformer (GNViT) model, a novel approach that harnesses a pre-trained Vision Transformer (ViT) on the ImageNet dataset. The primary goal is to detect and classify various pests affecting groundnut crops. Rigorous training and evaluation were conducted using a comprehensive dataset from IP102, encompassing pests such as Thrips, Aphids, Armyworms, and Wireworms. The GNViT model\u2019s effectiveness was assessed using reliability metrics, including the F1-score, recall, and overall accuracy. Data augmentation with GNViT resulted in a significant increase in training accuracy, achieving 99.52%. Comparative analysis highlighted the GNViT model\u2019s superior performance, particularly in accuracy, compared to state-of-the-art methodologies. These findings underscore the potential of deep learning models, such as GNViT, in providing reliable pest classification solutions for groundnut crops. The deployment of advanced technological solutions brings us closer to the overarching goal of reducing crop losses and enhancing global food security for the growing population.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9a4718faa07a32cf1dce745062181d3342e9b054.pdf",
        "venue": "PLoS ONE",
        "citationCount": 8,
        "score": 8.0
    },
    "6e97c1ba023afc87c1b99881f631af8146230d96.pdf": {
        "title": "A new ECT image reconstruction algorithm based on Vision transformer (ViT)",
        "authors": [
            "Xinhao Wu",
            "Sirui Xu",
            "Ming-Yu Gao",
            "Yan-Dong Liu",
            "Shiwei Liu",
            "Hua Yan",
            "Yan Wang"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6e97c1ba023afc87c1b99881f631af8146230d96.pdf",
        "venue": "Flow Measurement and Instrumentation",
        "citationCount": 8,
        "score": 8.0
    },
    "1d7183d481ae5a396743dde39984f1f0c8f47edf.pdf": {
        "title": "SWAT: An Efficient Swin Transformer Accelerator Based on FPGA",
        "authors": [
            "Qiwei Dong",
            "Xiaoru Xie",
            "Zhongfeng Wang"
        ],
        "published_date": "2024",
        "abstract": "Swin Transformer achieves greater efficiency than Vision Transformer by utilizing local self-attention and shifted windows. However, existing hardware accelerators designed for Transformer have not been optimized for the unique computation flow and data reuse property in Swin Transformer, resulting in lower hardware utilization and extra memory accesses. To address this issue, we develop SWAT, an efficient Swin Transformer Accelerator based on FPGA. Firstly, to eliminate the redundant computations in shifted windows, a novel tiling strategy is employed, which helps the developed multiplier array to fully utilize the sparsity. Additionally, we deploy a dynamic pipeline interleaving dataflow, which not only reduces the processing latency but also maximizes data reuse, thereby decreasing access to memories. Furthermore, customized quantization strategies and approximate calculations for non-linear calculations are adopted to simplify the hardware complexity with negligible network accuracy loss. We implement SWAT on the Xilinx Alveo U50 platform and evaluate it with Swin-T on the ImageNet dataset. The proposed architecture can achieve improvements of $2.02 \\times \\sim 3.11 \\times$ in power efficiency compared to existing Transformer accelerators on FPGAs.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1d7183d481ae5a396743dde39984f1f0c8f47edf.pdf",
        "venue": "Asia and South Pacific Design Automation Conference",
        "citationCount": 8,
        "score": 8.0
    },
    "9fbf36da9ee60d56a7675042bb0b24798a3b966d.pdf": {
        "title": "ViT-SENet-Tom: machine learning-based novel hybrid squeeze-excitation network and vision transformer framework for tomato fruits classification",
        "authors": [
            "S. M. M. Swapno",
            "S. N. Nobel",
            "Md Babul Islam",
            "Pronaya Bhattacharya",
            "Ebrahim A. Mattar"
        ],
        "published_date": "2025",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9fbf36da9ee60d56a7675042bb0b24798a3b966d.pdf",
        "venue": "Neural computing & applications (Print)",
        "citationCount": 8,
        "score": 8.0
    },
    "d80166681f3344a1946b8bfc623f4679d979ee10.pdf": {
        "title": "FSwin Transformer: Feature-Space Window Attention Vision Transformer for Image Classification",
        "authors": [
            "Dayeon Yoo",
            "Jeesu Kim",
            "Jinwoo Yoo"
        ],
        "published_date": "2024",
        "abstract": "The vision transformer (ViT) with global self-attention exhibits quadratic computational complexity that depends on the image size. To address this issue, window-based self-attention ViT limits attention area to a specific window, thereby mitigating the computational complexity. However, it cannot effectively capture the relationships between windows. The Swin Transformer, a representative window-based self-attention ViT, introduces shifted-window multi-head self-attention (SW-MSA) to capture the cross-window information. However, SW-MSA groups tokens that are close to each other in the image into one window and thus cannot capture relationships between distant tokens. Therefore, this paper introduces a feature-space window attention transformer (FSwin Transformer) that includes distant but similar tokens in one window. The proposed FSwin Transformer clusters similar tokens based on the feature space and conducts self-attention within the cluster. Thus, this approach helps understand the global context of the image by compensating for interactions between long-distance tokens, which cannot be captured when windows are set based on the image space. In addition, we incorporate a feature-space refinement method with channel and spatial attention to emphasize key parts and suppress non-essential parts. The refined feature map improves the representation power of the model, resulting in improved classification performance. Consequently, in classification tasks for ImageNet-1K, FSwin Transformer outperforms existing Transformer-based backbones, including the Swin Transformer.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d80166681f3344a1946b8bfc623f4679d979ee10.pdf",
        "venue": "IEEE Access",
        "citationCount": 8,
        "score": 8.0
    },
    "9285996627124b945ec601a763f6ff884bac3281.pdf": {
        "title": "Network Intrusion Detection Based on Feature Image and Deformable Vision Transformer Classification",
        "authors": [
            "Kan He",
            "Wei Zhang",
            "Xuejun Zong",
            "Lian Lian"
        ],
        "published_date": "2024",
        "abstract": "Network intrusion detection technology has always been an indispensable protection mechanism for industrial network security. The rise of new forms of network attacks has resulted in a heightened demand for these technologies. Nevertheless, the current models\u2019 effectiveness is subpar. We propose a new Deformable Vision Transformer (DE-VIT) method to address this issue. DE-VIT introduces a new deformable attention mechanism module, where the positions of key-value pairs in the attention mechanism are selected in a data-dependent manner, allowing it to focus on relevant areas, capture more informative features, and avoid excessive memory and computational costs. In addition to using deformable convolutions instead of regular convolutions in embedding layers to enhance the receptive field of patches, a sliding window mechanism is also employed to utilize edge information fully. In Parallel, we use a layered focal loss function to improve classification performance and address data imbalance issues. In summary, DE-VIT reduces computational complexity and achieves better results. We conduct experimental simulations on the public intrusion detection datasets, and the accuracy of the enhanced intrusion detection model surpasses that of the Deep Belief Network with Improved Kernel-Based Extreme Learning (DBN-KELM). It reaches 99.5% and 97.5% on the CIC IDS2017 and UNSW-NB15 datasets, exhibiting an increase of 8.5% and 9.1%, respectively.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9285996627124b945ec601a763f6ff884bac3281.pdf",
        "venue": "IEEE Access",
        "citationCount": 8,
        "score": 8.0
    },
    "3f002ea3ede9d97f1fe29b16691491219c1e626d.pdf": {
        "title": "Bearing Fault Diagnosis Based on Image Information Fusion and Vision Transformer Transfer Learning Model",
        "authors": [
            "Zichen Zhang",
            "Jing Li",
            "C. Cai",
            "Jianhua Ren",
            "Y. Xue"
        ],
        "published_date": "2024",
        "abstract": "In order to improve the accuracy of bearing fault diagnosis under a small sample, variable load, and noise conditions, a new fault diagnosis method based on an image information fusion and Vision Transformer (ViT) transfer learning model is proposed in this paper. Firstly, the method applies continuous wavelet transform (CWT), Gramian angular summation field (GASF), and Gramian angular difference field (GADF) to the time series data, and generates three grayscale images. Then, the generated three grayscale images are merged into an information fusion image (IFI) using image processing techniques. Finally, the obtained IFIs are fed into the advanced ViT model and trained based on transfer learning. In order to verify the effectiveness and superiority of the proposed method, the rolling bearing dataset from Case Western Reserve University (CWRU) is used to carry out experimental studies under different working conditions. Experimental results show that the method proposed in this paper is superior to other traditional methods in terms of accuracy, and the effect of ViT model based on transfer learning (TLViT) training is better than that of the Resnet50 model based on transfer learning training (TLResnet50) under variable loads and small sample conditions. In addition, the experimental results also prove that the IFI with multiple image information has better anti-noise ability than the single information image. Therefore, the method proposed in this paper can improve the accuracy of bearing fault diagnosis under small sample, variable load and noise conditions, and provide a new method for bearing fault diagnosis.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3f002ea3ede9d97f1fe29b16691491219c1e626d.pdf",
        "venue": "Applied Sciences",
        "citationCount": 8,
        "score": 8.0
    },
    "9121dcd10df00e5cc51dc94400e0325e0ae47bb9.pdf": {
        "title": "A 109-GOPs/W FPGA-Based Vision Transformer Accelerator With Weight-Loop Dataflow Featuring Data Reusing and Resource Saving",
        "authors": [
            "Yueqi Zhang",
            "Lichen Feng",
            "Hongwei Shan",
            "Zhangming Zhu"
        ],
        "published_date": "2024",
        "abstract": "The Vision Transformer (ViT) models have demonstrated excellent performance in computer vision tasks, but a large amount of computation and memory access for massive matrix multiplications lead to degraded hardware performance compared to convolutional neural network (CNN). In this paper, we propose a ViT accelerator with a novel \u201cWeight-Loop\u201d dataflow and its computing unit, for efficient matrix multiplication computation. By data partitioning and rearrangement, the number of memory accesses and the number of registers are greatly reduced, and the adder trees are eliminated. A computation pipeline with the proposed dataflow scheduling method is constructed to maintain a high utilization rate through zero bubble switching. Moreover, a novel accurate dual INT8 multiply-accumulate (DI8MAC) method for DSP optimization is introduced to eliminate the additional correction circuits by weight encoding. Verified in the Xilinx XCZU9EG FPGA, the proposed ViT accelerator achieves the lowest inference latencies of 3.91 ms and 13.98 ms for ViT-S and ViT-B, respectively. The throughput of the accelerator can reach up to 2330.2 GOPs with an energy efficiency of 109 GOPs/W, showing a significant improvement compared to the state-of-the-art works.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9121dcd10df00e5cc51dc94400e0325e0ae47bb9.pdf",
        "venue": "IEEE transactions on circuits and systems for video technology (Print)",
        "citationCount": 8,
        "score": 8.0
    },
    "08606a6a8b447909e714be2c3160074fdf1b91ad.pdf": {
        "title": "Vehicle Classification Algorithm Based on Improved Vision Transformer",
        "authors": [
            "Xinlong Dong",
            "Peicheng Shi",
            "Yueyue Tang",
            "Li Yang",
            "Aixi Yang",
            "Taonian Liang"
        ],
        "published_date": "2024",
        "abstract": "Vehicle classification technology is one of the foundations in the field of automatic driving. With the development of deep learning technology, visual transformer structures based on attention mechanisms can represent global information quickly and effectively. However, due to direct image segmentation, local feature details and information will be lost. To solve this problem, we propose an improved vision transformer vehicle classification network (IND-ViT). Specifically, we first design a CNN-In D branch module to extract local features before image segmentation to make up for the loss of detail information in the vision transformer. Then, in order to solve the problem of misdetection caused by the large similarity of some vehicles, we propose a sparse attention module, which can screen out the discernible regions in the image and further improve the detailed feature representation ability of the model. Finally, this paper uses the contrast loss function to further increase the intra-class consistency and inter-class difference of classification features and improve the accuracy of vehicle classification recognition. Experimental results show that the accuracy of the proposed model on the datasets of vehicle classification BIT-Vehicles, CIFAR-10, Oxford Flower-102, and Caltech-101 is higher than that of the original vision transformer model. Respectively, it increased by 1.3%, 1.21%, 7.54%, and 3.60%; at the same time, it also met a certain real-time requirement to achieve a balance of accuracy and real time.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/08606a6a8b447909e714be2c3160074fdf1b91ad.pdf",
        "venue": "World Electric Vehicle Journal",
        "citationCount": 7,
        "score": 7.0
    },
    "0222269c963f0902cc9eae6768a3c5948531488b.pdf": {
        "title": "A Vision-Transformer-Based Approach to Clutter Removal in GPR: DC-ViT",
        "authors": [
            "Yavuz Emre Kayacan",
            "I. Erer"
        ],
        "published_date": "2024",
        "abstract": "Since clutter encountered in ground-penetrating radar (GPR) systems deteriorates the performance of target detection algorithms, clutter removal is an active research area in the GPR community. In this letter, instead of convolutional neural network (CNN) architectures used in the recently proposed deep-learning-based clutter removal methods, we introduce declutter vision transformers (DC-ViTs) to remove the clutter. Transformer encoders in DC-ViT provide an alternative to CNNs which has limitations to capture long-range dependencies due to its local operations. In addition, the implementation of a convolutional layer instead of multilayer perceptron (MLP) in the transformer encoder increases the capturing ability of local dependencies. While deep features are extracted with blocks consisting of transformer encoders arranged sequentially, losses during information flow are reduced using dense connections between these blocks. Our proposed DC-ViT was compared with low-rank and sparse methods such as robust principle component analysis (RPCA), robust nonnegative matrix factorization (RNMF), and CNN-based deep networks such as convolutional autoencoder (CAE) and CR-NET. In comparisons made with the hybrid dataset, DC-ViT is 2.5% better in peak signal-to-noise ratio (PSNR) results than its closest competitor. As a result of the tests, we conducted using our experimental GPR data, and the proposed model provided an improvement of up to 20%, compared with its closest competitor in terms of signal-to-clutter ratio (SCR).",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0222269c963f0902cc9eae6768a3c5948531488b.pdf",
        "venue": "IEEE Geoscience and Remote Sensing Letters",
        "citationCount": 7,
        "score": 7.0
    },
    "8776fd7934dc48df4663dadf30c6da665d84fb19.pdf": {
        "title": "CFFI-Vit: Enhanced Vision Transformer for the Accurate Classification of Fish Feeding Intensity in Aquaculture",
        "authors": [
            "Jintao Liu",
            "Alfredo Tol\u00f3n Becerra",
            "Jos\u00e9 Fernando Bienvenido-Barcena",
            "Xinting Yang",
            "Zhenxi Zhao",
            "Chao Zhou"
        ],
        "published_date": "2024",
        "abstract": "The real-time classification of fish feeding behavior plays a crucial role in aquaculture, which is closely related to feeding cost and environmental preservation. In this paper, a Fish Feeding Intensity classification model based on the improved Vision Transformer (CFFI-Vit) is proposed, which is capable of quantifying the feeding behaviors of rainbow trout (Oncorhynchus mykiss) into three intensities: strong, moderate, and weak. The process is outlined as follows: firstly, we obtained 2685 raw feeding images of rainbow trout from recorded videos and classified them into three categories: strong, moderate, and weak. Secondly, the number of transformer encoder blocks in the internal structure of the ViT was reduced from 12 to 4, which can greatly reduce the computational load of the model, facilitating its deployment on mobile devices. And finally, a residual module was added to the head of the ViT, enhancing the model\u2019s ability to extract features. The proposed CFFI-Vit has a computational load of 5.81 G (Giga) Floating Point Operations per Second (FLOPs). Compared to the original ViT model, it reduces computational demands by 65.54% and improves classification accuracy on the validation set by 5.4 percentage points. On the test set, the model achieves precision, recall, and F1 score of 93.47%, 93.44%, and 93.42%, respectively. Additionally, compared to state-of-the-art models such as ResNet34, MobileNetv2, VGG16, and GoogLeNet, the CFFI-Vit model\u2019s classification accuracy is higher by 6.87, 8.43, 7.03, and 5.65 percentage points, respectively. Therefore, the proposed CFFI-Vit can achieve higher classification accuracy while significantly reducing computational demands. This provides a foundation for deploying lightweight deep network models on edge devices with limited hardware capabilities.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/8776fd7934dc48df4663dadf30c6da665d84fb19.pdf",
        "venue": "Journal of Marine Science and Engineering",
        "citationCount": 7,
        "score": 7.0
    },
    "cc24f933b343b6a9701088cf6ae1dbf3299c0c9e.pdf": {
        "title": "P2-ViT: Power-of-Two Post-Training Quantization and Acceleration for Fully Quantized Vision Transformer",
        "authors": [
            "Huihong Shi",
            "Xin Cheng",
            "Wendong Mao",
            "Zhongfeng Wang"
        ],
        "published_date": "2024",
        "abstract": "Vision transformers (ViTs) have excelled in computer vision (CV) tasks but are memory-consuming and computation-intensive, challenging their deployment on resource-constrained devices. To tackle this limitation, prior works have explored ViT-tailored quantization algorithms but retained floating-point scaling factors, which yield nonnegligible requantization overhead, limiting ViTs\u2019 hardware efficiency and motivating more hardware-friendly solutions. To this end, we propose P2-ViT, the first power-of-two (PoT) posttraining quantization (PTQ) and acceleration framework to accelerate fully quantized ViTs. Specifically, as for quantization, we explore a dedicated quantization scheme to effectively quantize ViTs with PoT scaling factors, thus minimizing the requantization overhead. Furthermore, we propose coarse-to-fine automatic mixed-precision quantization to enable better accuracy-efficiency tradeoffs. In terms of hardware, we develop a dedicated chunk-based accelerator featuring multiple tailored subprocessors to individually handle ViTs\u2019 different types of operations, alleviating reconfigurable overhead. In addition, we design a tailored row-stationary dataflow to seize the pipeline processing opportunity introduced by our PoT scaling factors, thereby enhancing throughput. Extensive experiments consistently validate P2-ViT\u2019s effectiveness. Particularly, we offer comparable or even superior quantization performance with PoT scaling factors when compared with the counterpart with floating-point scaling factors. Besides, we achieve up to <inline-formula> <tex-math notation=\"LaTeX\">$10.1\\times $ </tex-math></inline-formula> speedup and <inline-formula> <tex-math notation=\"LaTeX\">$36.8\\times $ </tex-math></inline-formula> energy saving over GPU\u2019s Turing Tensor Cores, and up to <inline-formula> <tex-math notation=\"LaTeX\">$1.84\\times $ </tex-math></inline-formula> higher computation utilization efficiency against SOTA quantization-based ViT accelerators. Codes are available at <uri>https://github.com/shihuihong214/P2-ViT</uri>.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/cc24f933b343b6a9701088cf6ae1dbf3299c0c9e.pdf",
        "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems",
        "citationCount": 7,
        "score": 7.0
    },
    "88589b0b2d2d8caa09d8ca94414343455ae87d7c.pdf": {
        "title": "PolSAR-MPIformer: A Vision Transformer Based on Mixed Patch Interaction for Dual-Frequency PolSAR Image Adaptive Fusion Classification",
        "authors": [
            "Xinyue Xin",
            "Ming Li",
            "Yan Wu",
            "Xiang Li",
            "Peng Zhang",
            "Dazhi Xu"
        ],
        "published_date": "2024",
        "abstract": "Vision transformer (ViT) provides new ideas for polarization synthetic aperture radar (PolSAR) image classification due to its advantages in learning global-spatial information. However, the lack of local-spatial information within samples and correlation information among samples, as well as the complexity of network structure, limit the application of ViT in practice. In addition, dual-frequency PolSAR data provide rich information, but there are fewer related studies compared to single-frequency classification algorithms. In this article, we adopt ViT as the basic framework, and propose a novel model based on mixed patch interaction for dual-frequency PolSAR image adaptive fusion classification (PolSAR-MPIformer). First, a mixed patch interaction (MPI) module is designed for the feature extraction, which replaces the high-complexity self-attention in ViT with patch interaction intra- and intersample. Besides the global-spatial information learning within samples by ViT, the MPI module adds the learning of local-spatial information within samples and correlation information among samples, thereby obtaining more discriminative features through a low-complexity network. Subsequently, a dual-frequency adaptive fusion (DAF) module is constructed as the classifier of PolSAR-MPIformer. On the one hand, the attention mechanism is utilized in DAF to reduce the impact of speckle noise while preserving details. On the other hand, the DAF evaluates the classification confidence of each band and assigns different weights accordingly, which achieves reasonable utilization of the complementarity between dual-frequency data and improves classification accuracy. Experiments on four real dual-frequency PolSAR datasets substantiate the superiority of the proposed PolSAR-MPIformer over other state-of-the-art algorithms.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/88589b0b2d2d8caa09d8ca94414343455ae87d7c.pdf",
        "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
        "citationCount": 7,
        "score": 7.0
    },
    "d629289cc2f3efbe374a66d57690dd84f19d59ec.pdf": {
        "title": "Vison Transformer-Based Automatic Crack Detection on Dam Surface",
        "authors": [
            "Jian Zhou",
            "Guochuan Zhao",
            "Yonglong Li"
        ],
        "published_date": "2024",
        "abstract": "Dam is an essential structure in hydraulic engineering, and its surface cracks pose significant threats to its integrity, impermeability, and durability. Automated crack detection methods based on computer vision offer substantial advantages over manual approaches with regard to efficiency, objectivity and precision. However, current methods face challenges such as misidentification, discontinuity, and loss of details when analyzing real-world dam crack images. These images often exhibit characteristics such as low contrast, complex backgrounds, and diverse crack morphologies. To address the above challenges, this paper presents a pure Vision Transformer (ViT)-based dam crack segmentation network (DCST-net). The DCST-net utilizes an improved Swin Transformer (SwinT) block as the fundamental block for enhancing the long-range dependencies within a SegNet-like encoder\u2013decoder structure. Additionally, we employ a weighted attention block to facilitate side fusion between the symmetric pair of encoder and decoder in each stage to sharpen the edge of crack. To demonstrate the superior performance of our proposed method, six semantic segmentation models have been trained and tested on both a self-built dam crack dataset and two publicly available datasets. Comparison results indicate that our proposed model outperforms the mainstream methods in terms of visualization and most evaluation metrics, highlighting its potential for practical application in dam safety inspection and maintenance.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d629289cc2f3efbe374a66d57690dd84f19d59ec.pdf",
        "venue": "Water",
        "citationCount": 7,
        "score": 7.0
    },
    "70211e2b04fcb4413edfd43e75b9f10e8d130171.pdf": {
        "title": "RI-ViT: A Multi-Scale Hybrid Method Based on Vision Transformer for Breast Cancer Detection in Histopathological Images",
        "authors": [
            "Ehsan Monjezi",
            "G. Akbarizadeh",
            "Karim Ansari-Asl"
        ],
        "published_date": "2024",
        "abstract": "Breast cancer is one of the most significant health threats to women worldwide. This disease manifests through abnormal proliferation of cells and the formation of tumors in breast tissue. Definitive breast cancer diagnosis is usually determined by analyzing tissue samples obtained from biopsies and reviewing them by pathologists. However, this method is highly dependent on the knowledge and experience of pathologists and may lead to errors due to the subjective nature of human interpretation and the high volume of cases. This study presents a multi-scale hybrid model based on Vision Transformer and residual networks for breast cancer detection in histopathological images, abbreviated as RI-ViT. In this approach, local features are extracted through a combination of residual stages and multi-scale learning, while global features are obtained using the attention mechanism in transformers. This combination enables simultaneous extraction of both local and global features from histopathological images, effectively improving the model\u2019s performance in detecting complex cases. We have used an imbalanced and publicly available dataset called BreakHis to evaluate the performance of the RI-ViT model. The experimental results of the proposed model show that it achieves accuracies of 99.75%, 98.80%, 98.01%, and 97.53% at magnifications of 40X, 100X, 200X, and 400X, respectively. The RI-ViT model can also perform well in an magnification-independent mode. Results show that, regardless of the magnification level, it achieves an accuracy of 99.37%, demonstrating its superiority over other state-of-the-art models.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/70211e2b04fcb4413edfd43e75b9f10e8d130171.pdf",
        "venue": "IEEE Access",
        "citationCount": 6,
        "score": 6.0
    },
    "cc817ba09f1c3c94bae73576463bcaf57c70261a.pdf": {
        "title": "Attention Score-Based Multi-Vision Transformer Technique for Plant Disease Classification",
        "authors": [
            "Eu-tteum Baek"
        ],
        "published_date": "2025",
        "abstract": "This study proposes an advanced plant disease classification framework leveraging the Attention Score-Based Multi-Vision Transformer (Multi-ViT) model. The framework introduces a novel attention mechanism to dynamically prioritize relevant features from multiple leaf images, overcoming the limitations of single-leaf-based diagnoses. Building on the Vision Transformer (ViT) architecture, the Multi-ViT model aggregates diverse feature representations by combining outputs from multiple ViTs, each capturing unique visual patterns. This approach allows for a holistic analysis of spatially distributed symptoms, crucial for accurately diagnosing diseases in trees. Extensive experiments conducted on apple, grape, and tomato leaf disease datasets demonstrate the model\u2019s superior performance, achieving over 99% accuracy and significantly improving F1 scores compared to traditional methods such as ResNet, VGG, and MobileNet. These findings underscore the effectiveness of the proposed model for precise and reliable plant disease classification.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/cc817ba09f1c3c94bae73576463bcaf57c70261a.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 6,
        "score": 6.0
    },
    "77e4ea46e7248408844f62fa3613aa246e35fdf0.pdf": {
        "title": "Automated Detection of Cervical Spinal Stenosis and Cord Compression via Vision Transformer and Rules-Based Classification",
        "authors": [
            "David L. Payne",
            "Xuan Xu",
            "Farshid Faraji",
            "Kevin John",
            "K. F. Pradas",
            "Vahni Vishala Bernard",
            "Lev Bangiyev",
            "Prateek Prasanna"
        ],
        "published_date": "2024",
        "abstract": "BACKGROUND AND PURPOSE: Cervical spinal cord compression, defined as spinal cord deformity and severe narrowing of the spinal canal in the cervical region, can lead to severe clinical consequences, including intractable pain, sensory disturbance, paralysis, and even death, and may require emergent intervention to prevent negative outcomes. Despite the critical nature of cord compression, no automated tool is available to alert clinical radiologists to the presence of such findings. This study aims to demonstrate the ability of a vision transformer (ViT) model for the accurate detection of cervical cord compression. MATERIALS AND METHODS: A clinically diverse cohort of 142 cervical spine MRIs was identified, 34% of which were normal or had mild stenosis, 31% with moderate stenosis, and 35% with cord compression. Utilizing gradient-echo images, slices were labeled as no cord compression/mild stenosis, moderate stenosis, or severe stenosis/cord compression. Segmentation of the spinal canal was performed and confirmed by neuroradiology faculty. A pretrained ViT model was fine-tuned to predict section-level severity by using a train:validation:test split of 60:20:20. Each examination was assigned an overall severity based on the highest level of section severity, with an examination labeled as positive for cord compression if \u22651 section was predicted in the severe category. Additionally, 2 convolutional neural network (CNN) models (ResNet50, DenseNet121) were tested in the same manner. RESULTS: The ViT model outperformed both CNN models at the section level, achieving section-level accuracy of 82%, compared with 72% and 78% for ResNet and DenseNet121, respectively. ViT patient-level classification achieved accuracy of 93%, sensitivity of 0.90, positive predictive value of 0.90, specificity of 0.95, and negative predictive value of 0.95. Receiver operating characteristic area under the curve was greater for ViT than either CNN. CONCLUSIONS: This classification approach using a ViT model and rules-based classification accurately detects the presence of cervical spinal cord compression at the patient level. In this study, the ViT model outperformed both conventional CNN approaches at the section and patient levels. If implemented into the clinical setting, such a tool may streamline neuroradiology workflow, improving efficiency and consistency.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/77e4ea46e7248408844f62fa3613aa246e35fdf0.pdf",
        "venue": "American Journal of Neuroradiology",
        "citationCount": 6,
        "score": 6.0
    },
    "271c6a87213b8b2a26ce3e1a6f20b07cc2f82b81.pdf": {
        "title": "Seizure prediction based on improved vision transformer model for EEG channel optimization",
        "authors": [
            "Nan Qi",
            "Yan Piao",
            "Hao Zhang",
            "Qi Wang",
            "Yue Wang"
        ],
        "published_date": "2024",
        "abstract": "Abstract Epileptic seizures are unpredictable events caused by abnormal discharges of a patient\u2019s brain cells. Extensive research has been conducted to develop seizure prediction algorithms based on long-term continuous electroencephalogram (EEG) signals. This paper describes a patient-specific seizure prediction method that can serve as a basis for the design of lightweight, wearable and effective seizure-prediction devices. We aim to achieve two objectives using this method. The first aim is to extract robust feature representations from multichannel EEG signals, and the second aim is to reduce the number of channels used for prediction by selecting an optimal set of channels from multichannel EEG signals while ensuring good prediction performance. We design a seizure-prediction algorithm based on a vision transformer (ViT) model. The algorithm selects channels that play a key role in seizure prediction from 22 channels of EEG signals. First, we perform a time-frequency analysis of processed time-series signals to obtain EEG spectrograms. We then segment the spectrograms of multiple channels into many non-overlapping patches of the same size, which are input into the channel selection layer of the proposed model, named Sel-JPM-ViT, enabling it to select channels. Application of the Sel-JPM-ViT model to the Boston Children\u2019s Hospital\u2013Massachusetts Institute of Technology scalp EEG dataset yields results using only three to six channels of EEG signals that are slightly better that the results obtained using 22 channels of EEG signals. Overall, the Sel-JPM-ViT model exhibits an average classification accuracy of 93.65%, an average sensitivity of 94.70% and an average specificity of 92.78%.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/271c6a87213b8b2a26ce3e1a6f20b07cc2f82b81.pdf",
        "venue": "Computer Methods in Biomechanics and Biomedical Engineering",
        "citationCount": 6,
        "score": 6.0
    },
    "05d15576d88f9384738908f98716f91bdb5dbc78.pdf": {
        "title": "Quantifying Dwell Time With Location-based Augmented Reality: Dynamic AOI Analysis on Mobile Eye Tracking Data With Vision Transformer",
        "authors": [
            "J. Mercier",
            "O. Ertz",
            "E. Bocher"
        ],
        "published_date": "2024",
        "abstract": "Mobile eye tracking captures egocentric vision and is well-suited for naturalistic studies. However, its data is noisy, especially when acquired outdoor with multiple participants over several sessions. Area of interest analysis on moving targets is difficult because A) camera and objects move nonlinearly and may disappear/reappear from the scene; and B) off-the-shelf analysis tools are limited to linearly moving objects. As a result, researchers resort to time-consuming manual annotation, which limits the use of mobile eye tracking in naturalistic studies. We introduce a method based on a fine-tuned Vision Transformer (ViT) model for classifying frames with overlaying gaze markers. After fine-tuning a model on a manually labelled training set made of 1.98% (=7845 frames) of our entire data for three epochs, our model reached 99.34% accuracy as evaluated on hold-out data. We used the method to quantify participants\u2019 dwell time on a tablet during the outdoor user test of a mobile augmented reality application for biodiversity education. We discuss the benefits and limitations of our approach and its potential to be applied to other contexts.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/05d15576d88f9384738908f98716f91bdb5dbc78.pdf",
        "venue": "Journal of Eye Movement Research",
        "citationCount": 6,
        "score": 6.0
    },
    "6feea99a1a6c876d6f4f714b463da9c95998dee7.pdf": {
        "title": "Utilizing adaptive deformable convolution and position embedding for colon polyp segmentation with a visual transformer",
        "authors": [
            "Mohamed Yacin Sikkandar",
            "S. Sundaram",
            "Ahmad Alassaf",
            "Ibrahim AlMohimeed",
            "Khalid Alhussaini",
            "Adham Aleid",
            "S. Alolayan",
            "P. Ramkumar",
            "Meshal Khalaf Almutairi",
            "S. Begum"
        ],
        "published_date": "2024",
        "abstract": "Polyp detection is a challenging task in the diagnosis of Colorectal Cancer (CRC), and it demands clinical expertise due to the diverse nature of polyps. The recent years have witnessed the development of automated polyp detection systems to assist the experts in early diagnosis, considerably reducing the time consumption and diagnostic errors. In automated CRC diagnosis, polyp segmentation is an important step which is carried out with deep learning segmentation models. Recently, Vision Transformers (ViT) are slowly replacing these models due to their ability to capture long range dependencies among image patches. However, the existing ViTs for polyp do not harness the inherent self-attention abilities and incorporate complex attention mechanisms. This paper presents Polyp-Vision Transformer (Polyp-ViT), a novel Transformer model based on the conventional Transformer architecture, which is enhanced with adaptive mechanisms for feature extraction and positional embedding. Polyp-ViT is tested on the Kvasir-seg and CVC-Clinic DB Datasets achieving segmentation accuracies of 0.9891\u2009\u00b1\u20090.01 and 0.9875\u2009\u00b1\u20090.71 respectively, outperforming state-of-the-art models. Polyp-ViT is a prospective tool for polyp segmentation which can be adapted to other medical image segmentation tasks as well due to its ability to generalize well.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6feea99a1a6c876d6f4f714b463da9c95998dee7.pdf",
        "venue": "Scientific Reports",
        "citationCount": 6,
        "score": 6.0
    },
    "c05744f690ab9db007012a63c3c5c3ca48201c66.pdf": {
        "title": "CSwT-SR: Conv-Swin Transformer for Blind Remote Sensing Image Super-Resolution With Amplitude-Phase Learning and Structural Detail Alternating Learning",
        "authors": [
            "Mingyang Hou",
            "Zhiyong Huang",
            "Zhi Yu",
            "Yan Yan",
            "Yunlan Zhao",
            "Xiao Han"
        ],
        "published_date": "2024",
        "abstract": "Image super-resolution (SR) stands as a pivotal process in the domains of image processing and computer vision, finding diverse applications in film, television, photography, surveillance, medical imaging, and remote sensing. In the context of remote sensing images (RSIs), the inherent challenge arises from low spatial resolution caused by factors such as sensor noise, orbit height, and weather conditions, necessitating SR reconstruction. An evident limitation of prevailing methods lies in their dependence on idealized fixed degradation models, which fail to capture the intricate degradation processes unique to remote sensing scenes. In response to these constraints, this article introduces an innovative blind image super-resolution reconstruction method tailored for remote sensing images. The proposed approach integrates convolution with a transformer and incorporates an amplitude-phase learning module (ALM) to comprehensively capture local and long-range dependencies while enhancing frequency information. The iterative optimization strategy refines texture information by carefully balancing structural and detail elements. Key contributions include a holistic approach to remote sensing image SR, ALM integration for precise feature representation, and the introduction of a patch-based frequency loss mechanism for evaluating frequency-domain features. Rigorous experiments demonstrate that compared with other state-of-the-art (SOTA) methods, the proposed algorithm delivers SR results with exceptional visual perception quality across three distinct remote sensing datasets.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c05744f690ab9db007012a63c3c5c3ca48201c66.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 6,
        "score": 6.0
    },
    "cebc29015a2827aa8a98ec39c90c93bca2f14848.pdf": {
        "title": "An Explainable CNN and Vision Transformer-Based Approach for Real-Time Food Recognition",
        "authors": [
            "Kintoh Allen Nfor",
            "Tagne Poupi Theodore Armand",
            "Kenesbaeva Periyzat Ismaylovna",
            "Moon-Il Joo",
            "Hee-Cheol Kim"
        ],
        "published_date": "2025",
        "abstract": "Background: Food image recognition, a crucial step in computational gastronomy, has diverse applications across nutritional platforms. Convolutional neural networks (CNNs) are widely used for this task due to their ability to capture hierarchical features. However, they struggle with long-range dependencies and global feature extraction, which are vital in distinguishing visually similar foods or images where the context of the whole dish is crucial, thus necessitating transformer architecture. Objectives: This research explores the capabilities of the CNNs and transformers to build a robust classification model that can handle both short- and long-range dependencies with global features to accurately classify food images and enhance food image recognition for better nutritional analysis. Methods: Our approach, which combines CNNs and Vision Transformers (ViTs), begins with the RestNet50 backbone model. This model is responsible for local feature extraction from the input image. The resulting feature map is then passed to the ViT encoder block, which handles further global feature extraction and classification using multi-head attention and fully connected layers with pre-trained weights. Results: Our experiments on five diverse datasets have confirmed a superior performance compared to the current state-of-the-art methods, and our combined dataset leveraging complementary features showed enhanced generalizability and robust performance in addressing global food diversity. We used explainable techniques like grad-CAM and LIME to understand how the models made their decisions, thereby enhancing the user\u2019s trust in the proposed system. This model has been integrated into a mobile application for food recognition and nutrition analysis, offering features like an intelligent diet-tracking system. Conclusion: This research paves the way for practical applications in personalized nutrition and healthcare, showcasing the extensive potential of AI in nutritional sciences across various dietary platforms.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/cebc29015a2827aa8a98ec39c90c93bca2f14848.pdf",
        "venue": "Nutrients",
        "citationCount": 6,
        "score": 6.0
    },
    "630c9993b6aaf812e1aa3dae8e243af131abd3d0.pdf": {
        "title": "A Fast and Robust Safety Helmet Network Based on a Mutilscale Swin Transformer",
        "authors": [
            "Changcheng Xiang",
            "Duofen Yin",
            "Fei Song",
            "Zaixue Yu",
            "Xu Jian",
            "Huaming Gong"
        ],
        "published_date": "2024",
        "abstract": "Visual inspection of the workplace and timely reminders of unsafe behaviors (e.g, not wearing a helmet) are particularly significant for avoiding injuries to workers on the construction site. Video surveillance systems generate large amounts of non-structure image data on site for this purpose; however, they require real-time recognition automation solutions based on computer vision. Although various deep-learning-based models have recently provided new ideas for identifying helmets in traffic monitoring, few solutions suitable for industry applications have been discussed due to the complex scenarios of construction sites. In this paper, a fast and robust network based on a mutilscale Swin Transformer is proposed for safety helmet detection (FRSHNet) at construction sites, which contains the following contributions. Firstly, MAE-NAS with the variant of MobileNetV3\u2019s MobBlock as a basic block is applied to implement feature extraction. Simultaneously, a multiscale Swin Transformer module is utilized to obtain the spatial and contexture relationships in the multiscale features. Subsequently, in order to meet the scheme requirements of real-time helmet detection, efficient RepGFPN are adopted to integrate refined multiscale features to form a pyramid structure. Extensive experiments were conducted on the publicly available Pictor-v3 and SHWD datasets. The experimental results show that FRSHNet consistently provided a favorable performance, outperforming the existing state-of-the-art models.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/630c9993b6aaf812e1aa3dae8e243af131abd3d0.pdf",
        "venue": "Buildings",
        "citationCount": 5,
        "score": 5.0
    },
    "d14a1677e416bd7ac6fbe01672cd3152fc1f983d.pdf": {
        "title": "Facial Expression Recognition Based on Vision Transformer with Hybrid Local Attention",
        "authors": [
            "Yuan Tian",
            "Jingxuan Zhu",
            "Huang Yao",
            "Di Chen"
        ],
        "published_date": "2024",
        "abstract": "Facial expression recognition has wide application prospects in many occasions. Due to the complexity and variability of facial expressions, facial expression recognition has become a very challenging research topic. This paper proposes a Vision Transformer expression recognition method based on hybrid local attention (HLA-ViT). The network adopts a dual-stream structure. One stream extracts the hybrid local features and the other stream extracts the global contextual features. These two streams constitute a global\u2013local fusion attention. The hybrid local attention module is proposed to enhance the network\u2019s robustness to face occlusion and head pose variations. The convolutional neural network is combined with the hybrid local attention module to obtain feature maps with local prominent information. Robust features are then captured by the ViT from the global perspective of the visual sequence context. Finally, the decision-level fusion mechanism fuses the expression features with local prominent information, adding complementary information to enhance the network\u2019s recognition performance and robustness against interference factors such as occlusion and head posture changes in natural scenes. Extensive experiments demonstrate that our HLA-ViT network achieves an excellent performance with 90.45% on RAF-DB, 90.13% on FERPlus, and 65.07% on AffectNet.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d14a1677e416bd7ac6fbe01672cd3152fc1f983d.pdf",
        "venue": "Applied Sciences",
        "citationCount": 5,
        "score": 5.0
    },
    "8ed42c0d60eb4c91e99ee184a17719f7c3ce3f3f.pdf": {
        "title": "ViT-UNet: A Vision Transformer Based UNet Model for Coastal Wetland Classification Based on High Spatial Resolution Imagery",
        "authors": [
            "Nan Zhou",
            "Mingming Xu",
            "Biaoqun Shen",
            "Ke Hou",
            "Shanwei Liu",
            "Hui Sheng",
            "Yanfen Liu",
            "Jianhua Wan"
        ],
        "published_date": "2024",
        "abstract": "High resolution remote sensing imagery plays a crucial role in monitoring coastal wetlands. Coastal wetland landscapes exhibit diverse features, ranging from fragmented patches to expansive areas. Mainstream convolutional neural networks cannot effectively analyze spatial relationships among consecutive image elements. This limitation impedes their performance in accurately classifying coastal wetlands. In order to tackle the above issues, we propose a Vision Transformer based UNet (ViT-UNet) model. This model extracts wetland features from high resolution remote sensing images by sensing and optimizing multiscale features. To establish global dependencies, the Vision Transformer (ViT) is introduced to replace the convolutional layer in the UNet encoder. Simultaneously, the model incorporates a convolutional block attention module and a multiple hierarchies attention module to restore attentional features and reduce feature loss. In addition, a skip connection is added to the single-skip structure of the original UNet model. This connection simultaneously links the output of the entire transformer and internal attention features to the corresponding decoder level. This enhancement aims to furnish the decoder with comprehensive global information guidance. Finally, all the extracted feature information is fused using Bilinear Polymerization Pooling (BPP). The BPP assists the network in obtaining a more comprehensive and detailed feature representation. Experimental results on the Gaofen-1 dataset demonstrate that the proposed ViT-UNet method achieves a Precision score of 93.50$\\%$, outperforming the original UNet model by 4.10$\\%$. Compared with other state-of-the-art networks, ViT-UNet performs more accurately and finer in the extraction of wetland information in the Yellow River Delta.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/8ed42c0d60eb4c91e99ee184a17719f7c3ce3f3f.pdf",
        "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing",
        "citationCount": 5,
        "score": 5.0
    },
    "c4cf58e104fc8d783dfd0bcf3ccd65f8e4028d7f.pdf": {
        "title": "Thoracic computed tomography (CT) image-based identification and severity classification of COVID-19 cases using vision transformer (ViT)",
        "authors": [
            "Gizatie Desalegn Taye",
            "Zewdie Habtie Sisay",
            "Genet Worku Gebeyhu",
            "Fisha Haileslassie Kidus"
        ],
        "published_date": "2024",
        "abstract": "In this research, we developed a two-stage deep learning (DL) model using Vision Transformer (ViT) to detect COVID-19 and assess its severity from thoracic CT images. In the first stage, we utilized a pre-trained ViT model (ViT_B/32) and a custom CNN model to classify CT images as COVID-19 or non-COVID-19. The ViT model achieved superior performance with a fivefold cross-validated accuracy of 99.7%, compared to the custom CNN\u2019s 98%. In the second stage, we employed a ViT-based U-Net model (Vision Transformer for Biomedical Image Segmentation, VITBIS) to segment lung and infection regions in COVID-19 positive CT images, determining the infection severity. This model uses transformers with attention mechanisms in both the encoder and decoder. The lung segmentation network achieved an Intersection Over Union (IOU) of 95.8% and a sensitivity of 99.67%, while the lesion segmentation network attained an IOU of 94% and a sensitivity of 98.3%.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c4cf58e104fc8d783dfd0bcf3ccd65f8e4028d7f.pdf",
        "venue": "Discover Applied Sciences",
        "citationCount": 5,
        "score": 5.0
    },
    "72e23cdc3accca1f09e2e19446bc475368c912d0.pdf": {
        "title": "Swin-GA-RF: genetic algorithm-based Swin Transformer and random forest for enhancing cervical cancer classification",
        "authors": [
            "Manal Abdullah Alohali",
            "Nora El-Rashidy",
            "Saad Alaklabi",
            "H. Elmannai",
            "Saleh Alharbi",
            "Hager Saleh"
        ],
        "published_date": "2024",
        "abstract": "Cervical cancer is a prevalent and concerning disease affecting women, with increasing incidence and mortality rates. Early detection plays a crucial role in improving outcomes. Recent advancements in computer vision, particularly the Swin transformer, have shown promising performance in image classification tasks, rivaling or surpassing traditional convolutional neural networks (CNNs). The Swin transformer adopts a hierarchical and efficient approach using shifted windows, enabling the capture of both local and global contextual information in images. In this paper, we propose a novel approach called Swin-GA-RF to enhance the classification performance of cervical cells in Pap smear images. Swin-GA-RF combines the strengths of the Swin transformer, genetic algorithm (GA) feature selection, and the replacement of the softmax layer with a random forest classifier. Our methodology involves extracting feature representations from the Swin transformer, utilizing GA to identify the optimal feature set, and employing random forest as the classification model. Additionally, data augmentation techniques are applied to augment the diversity and quantity of the SIPaKMeD1 cervical cancer image dataset. We compare the performance of the Swin-GA-RF Transformer with pre-trained CNN models using two classes and five classes of cervical cancer classification, employing both Adam and SGD optimizers. The experimental results demonstrate that Swin-GA-RF outperforms other Swin transformers and pre-trained CNN models. When utilizing the Adam optimizer, Swin-GA-RF achieves the highest performance in both binary and five-class classification tasks. Specifically, for binary classification, it achieves an accuracy, precision, recall, and F1-score of 99.012, 99.015, 99.012, and 99.011, respectively. In the five-class classification, it achieves an accuracy, precision, recall, and F1-score of 98.808, 98.812, 98.808, and 98.808, respectively. These results underscore the effectiveness of the Swin-GA-RF approach in cervical cancer classification, demonstrating its potential as a valuable tool for early diagnosis and screening programs.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/72e23cdc3accca1f09e2e19446bc475368c912d0.pdf",
        "venue": "Frontiers in Oncology",
        "citationCount": 5,
        "score": 5.0
    },
    "7b6d64097d16219c043df64e4576bd7d87656073.pdf": {
        "title": "Real-time quantitative detection of hydrocolloid adulteration in meat based on Swin Transformer and smartphone.",
        "authors": [
            "Zhenchang Gao",
            "Shanshan Chen",
            "Jinxian Huang",
            "H. Cai"
        ],
        "published_date": "2024",
        "abstract": "Hydrocolloids are widely used in meat products as common food additives. However, research has indicated that excessive consumption of these hydrocolloids may have potential health implications. Currently, consumers mainly rely on sensory evaluation to identify hydrocolloid adulteration in meat products. Although many studies on quantitative detection of hydrocolloids have been conducted by biochemical methods in laboratory environments, there is currently a lack of effective tools for consumers and regulators to obtain real-time and reliable information on hydrocolloid adulteration. To address this challenge, a smartphone-based computer vision method was developed to quantitatively detect carrageenan adulteration in beef in this work. Specifically, Swin Transformer models, along with pre-training and fine-tuning techniques, were used to successfully automate the classification of beef into nine different levels of carrageenan adulteration, ranging from 0% to 20%. Among the tested models, Swin-Tiny (Swin-T) achieved the highest trade-off performance, with a Top-1 accuracy of 0.997, a detection speed of 3.2\u00a0ms, and a model size of 103.45 Mb. Compared to computer vision, the electrochemical impedance spectroscopy achieved a lower accuracy of 0.792 and required a constant temperature environment and a waiting time of around 30 min for data stabilization. In addition, Swin-T model was also capable of distinguishing between different types of hydrocolloids with a Top-1 accuracy of 0.975. This study provides consumers and regulators with a valuable tool to obtain real-time quantitative information about meat adulteration anytime, anywhere. PRACTICAL APPLICATION: This research provides a practical solution for regulators and consumers to non-destructively and quantitatively detect the content and type of hydrocolloids in beef in real-time using smartphones. This innovation has the potential to significantly reduce the costs associated with meat quality testing, such as the use of chemical reagents and expensive instruments.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7b6d64097d16219c043df64e4576bd7d87656073.pdf",
        "venue": "Journal of Food Science",
        "citationCount": 5,
        "score": 5.0
    },
    "0820d2ac35cd55fd60f53c14460cca78fa996f1e.pdf": {
        "title": "ST-LaneNet: Lane Line Detection Method Based on Swin Transformer and LaneNet",
        "authors": [
            "Yufeng Du",
            "Rongyun Zhang",
            "Peicheng Shi",
            "Linfeng Zhao",
            "Bin Zhang",
            "Yaming Liu"
        ],
        "published_date": "2024",
        "abstract": "The advancement of autonomous driving heavily relies on the ability to accurate lane lines detection. As deep learning and computer vision technologies evolve, a variety of deep learning-based methods for lane line detection have been proposed by researchers in the field. However, owing to the simple appearance of lane lines and the lack of distinctive features, it is easy for other objects with similar local appearances to interfere with the process of detecting lane lines. The precision of lane line detection is limited by the unpredictable quantity and diversity of lane lines. To address the aforementioned challenges, we propose a novel deep learning approach for lane line detection. This method leverages the Swin Transformer in conjunction with LaneNet (called ST-LaneNet). The experience results showed that the true positive detection rate can reach 97.53% for easy lanes and 96.83% for difficult lanes (such as scenes with severe occlusion and extreme lighting conditions), which can better accomplish the objective of detecting lane lines. In 1000 detection samples, the average detection accuracy can reach 97.83%, the average inference time per image can reach 17.8\u00a0ms, and the average number of frames per second can reach 64.8\u00a0Hz. The programming scripts and associated models for this project can be accessed openly at the following GitHub repository: https://github.com/Duane711/Lane-line-detection-ST-LaneNet .",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/0820d2ac35cd55fd60f53c14460cca78fa996f1e.pdf",
        "venue": "Chinese Journal of Mechanical Engineering",
        "citationCount": 5,
        "score": 5.0
    },
    "b02144ef4ed94df78544959bc97eddef4580dd95.pdf": {
        "title": "CurrencyNet: A Vision Transformer-Based Approach for Indian Currency Note Classification with Optimizer Exploration",
        "authors": [
            "R. Tiwari",
            "Himani Maheshwari",
            "Vinay Gautam",
            "Neema Gupta",
            "N. Trivedi",
            "A. Agarwal"
        ],
        "published_date": "2024",
        "abstract": "This study proposes CurrencyNet, a new way to classify Indian rupee notes using Vision Transformer (ViT) deep architecture. CurrencyNet takes advantage of ViT's feature to manage spatial connections in images. Different optimizers, induding Adadelta, Gradient Descent, AdaGrad, RMS Prop, Adamax, Momentum, Adaptive Moment Estimation (Adam), and Nesterov Momentum, are explored to see how they affect CurrencyNet's performance. Testing results show that CurrencyNet gets 97.76 % accuracy, using both the Vision Transformer and the Adam optimizer together. Proposed CurrencyNet is compared with well-known deep architectures, such as VGG16, VGG18, Inception, Xception, ResNet, and MobileNet. The outcomes show that CurrencyNet is the best way to sort Indian rupee notes, making it the only choice.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b02144ef4ed94df78544959bc97eddef4580dd95.pdf",
        "venue": "2024 2nd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT)",
        "citationCount": 5,
        "score": 5.0
    },
    "b6488fded5fb0728d7c3bd04f3afa3fba68d0450.pdf": {
        "title": "A Deep Learning-Based Intrusion Detection Model Integrating Convolutional Neural Network and Vision Transformer for Network Traffic Attack in the Internet of Things",
        "authors": [
            "Chunlai Du",
            "Yanhui Guo",
            "Yuhang Zhang"
        ],
        "published_date": "2024",
        "abstract": "With the rapid expansion and ubiquitous presence of the Internet of Things (IoT), the proliferation of IoT devices has reached unprecedented levels, heightening concerns about IoT security. Intrusion detection based on deep learning has become a crucial approach for safeguarding IoT ecosystems. However, challenges remain in IoT intrusion detection research, including inadequate feature representation at the classifier level and poor correlation among extracted traffic features, leading to diminished classification accuracy. To address these issues, we propose a novel transformer-based IoT intrusion detection model, MBConv-ViT (MobileNet Convolution and Vision Transformer), which enhances the correlation of extracted features by fusing local and global features. By leveraging the high correlation of traffic flow, our model can identify subtle differences in IoT traffic flow, thereby achieving precise classification of attack traffic. Experiments based on the open datasets TON-IoT and Bot-IoT demonstrate that the accuracy of the MBConv-ViT model, respectively, 97.14% and 99.99%, is more effective than several existing typical models.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b6488fded5fb0728d7c3bd04f3afa3fba68d0450.pdf",
        "venue": "Electronics",
        "citationCount": 5,
        "score": 5.0
    },
    "24eba2f5345bc8279bb63f085e75cc26b37d0b1e.pdf": {
        "title": "A generalised vision transformer-based self-supervised model for diagnosing and grading prostate cancer using histological images",
        "authors": [
            "A. Chaurasia",
            "H. C. Harris",
            "P. W. Toohey",
            "A. W. Hewitt"
        ],
        "published_date": "2024",
        "abstract": "Background Gleason grading remains the gold standard for prostate cancer histological classification and prognosis, yet its subjectivity leads to grade variability between pathologists, potentially impacting clinical decision-making. Artificial intelligence (AI), particularly self-supervised vision transformer (ViT) architecture, can enhance diagnostic accuracy and consistency for prostate cancer. We trained and validated a generalised AI-driven system for diagnosing prostate cancer using diverse datasets from tissue microarray (TMA) core and whole slide images (WSIs) with Hematoxylin and Eosin staining. Methods We analysed eight prostate cancer datasets, which included 12,711 histological images from 3,648 patients, incorporating TMA core images and WSIs. Patches were extracted with 512 x 512 pixels size at 10x magnification from histological images with their corresponding mask annotations from segmentation data. The Macenko method was used to normalise colours for consistency across diverse images. Subsequently, we trained a multi-resolution (5x, 10x, 20x, and 40x) binary classifier to identify benign and malignant tissue. We then implemented a multi-class classifier for Gleason patterns (GP) sub-categorisation from malignant tissue. Finally, the models were externally validated on 11,132 histology images from 2,176 patients to determine the International Society of Urological Pathology (ISUP) grade. Models were assessed using various classification metrics, and the agreement between the model's predictions and the ground truth was quantified using the quadratic weighted Cohen's Kappa (k) score. Results Our multi-resolution binary classifier demonstrated robust performance in distinguishing malignant from benign tissue with k scores of 0.967 on internal validation. The model achieved k scores ranging from 0.876 to 0.995 across four unseen testing datasets. The multi-class classifier also performed well in distinguishing GP3, GP4, and GPs with an overall k score of 0.841. This model was further tested across four datasets, obtaining k scores ranging from 0.774 to 0.888. Attention maps generated by both classifiers revealed the clinical features of GPs. The models' performance was compared against an independent pathologist's annotation on an external dataset, achieving a k score of 0.752 for four classes. Conclusion Our self-supervised ViT-based model demonstrates high utility in diagnosing and grading prostate cancer using histological images. The models exhibit robust performance in categorising benign and malignant tissues, further differentiating malignant tissue based on the aggressiveness of cancer. Attention maps exposed high coherence with expert-confirmed pathological features. External validation reflects the robustness and generalizability of the models across different datasets, highlighting their clinical applicability for diagnosing and grading prostate cancer as a tool in digital pathology.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/24eba2f5345bc8279bb63f085e75cc26b37d0b1e.pdf",
        "venue": "medRxiv",
        "citationCount": 5,
        "score": 5.0
    },
    "8ce6872b26f88e52aeed57f6f7528dee30b5f7b2.pdf": {
        "title": "Residual Vision Transformer (ResViT) Based Self-Supervised Learning Model for Brain Tumor Classification",
        "authors": [
            "Meryem Altin Karag\u00f6z",
            "\u00d6zkan U. Nalbantoglu",
            "Geoffrey C. Fox"
        ],
        "published_date": "2024",
        "abstract": "Deep learning has proven very promising for interpreting MRI in brain tumor diagnosis. However, deep learning models suffer from a scarcity of brain MRI datasets for effective training. Self-supervised learning (SSL) models provide data-efficient and remarkable solutions to limited dataset problems. Therefore, this paper introduces a generative SSL model for brain tumor classification in two stages. The first stage is designed to pre-train a Residual Vision Transformer (ResViT) model for MRI synthesis as a pretext task. The second stage includes fine-tuning a ResViT-based classifier model as a downstream task. Accordingly, we aim to leverage local features via CNN and global features via ViT, employing a hybrid CNN-transformer architecture for ResViT in pretext and downstream tasks. Moreover, synthetic MRI images are utilized to balance the training set. The proposed model performs on public BraTs 2023, Figshare, and Kaggle datasets. Furthermore, we compare the proposed model with various deep learning models, including A-UNet, ResNet-9, pix2pix, pGAN for MRI synthesis, and ConvNeXtTiny, ResNet101, DenseNet12, Residual CNN, ViT for classification. According to the results, the proposed model pretraining on the MRI dataset is superior compared to the pretraining on the ImageNet dataset. Overall, the proposed model attains the highest accuracy, achieving 90.56% on the BraTs dataset with T1 sequence, 98.53% on the Figshare, and 98.47% on the Kaggle brain tumor datasets. As a result, the proposed model demonstrates a robust, effective, and successful approach to handling insufficient dataset challenges in MRI analysis by incorporating SSL, fine-tuning, data augmentation, and combining CNN and ViT.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/8ce6872b26f88e52aeed57f6f7528dee30b5f7b2.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 5.0
    },
    "dcf77b8e23bbc56c5c8f76ddc5d3cf68d0a051be.pdf": {
        "title": "Explainable vision transformer for automatic visual sleep staging on multimodal PSG signals",
        "authors": [
            "Hyojin Lee",
            "You Rim Choi",
            "Hyun Kyung Lee",
            "Jaemin Jeong",
            "Joopyo Hong",
            "Hyun-Woo Shin",
            "Hyung-Sin Kim"
        ],
        "published_date": "2025",
        "abstract": "Polysomnography (PSG) is crucial for diagnosing sleep disorders, but manual scoring of PSG is time-consuming and subjective, leading to high variability. While machine-learning models have improved PSG scoring, their clinical use is hindered by the \u2018black-box\u2019 nature. In this study, we present SleepXViT, an automatic sleep staging system using Vision Transformer (ViT) that provides intuitive, consistent explanations by mimicking human \u2018visual scoring\u2019. Tested on KISS\u2013a PSG image dataset from 7745 patients across four hospitals\u2013SleepXViT achieved a Macro F1 score of 81.94%, outperforming baseline models and showing robust performances on public datasets SHHS1 and SHHS2. Furthermore, SleepXViT offers well-calibrated confidence scores, enabling expert review for low-confidence predictions, alongside high-resolution heatmaps highlighting essential features and relevance scores for adjacent epochs\u2019 influence on sleep stage predictions. Together, these explanations reinforce the scoring consistency of SleepXViT, making it both reliable and interpretable, thereby facilitating the synergy between the AI model and human scorers in clinical settings.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/dcf77b8e23bbc56c5c8f76ddc5d3cf68d0a051be.pdf",
        "venue": "npj Digit. Medicine",
        "citationCount": 5,
        "score": 5.0
    },
    "f792dcf878e5bf8e7fb5aa31d231474d43462f4a.pdf": {
        "title": "Performance of vision transformer and swin transformer models for lemon quality classification in fruit juice factories",
        "authors": [
            "Sezer D\u00fcmen",
            "Esra Kavalc\u0131 Y\u0131lmaz",
            "Kemal Adem",
            "Erdin\u00e7 Avaroglu"
        ],
        "published_date": "2024",
        "abstract": "Assessing the quality of agricultural products holds vital significance in enhancing production efficiency and market viability. The adoption of artificial intelligence (AI) has notably surged for this purpose, employing deep learning and machine learning techniques to process and classify agricultural product images, adhering to defined standards. This study focuses on the lemon dataset, encompassing \u2018good\u2019 and \u2018bad\u2019 quality classes, initiate by augmenting data through rescaling, random zoom, flip, and rotation methods. Subsequently, employing eight diverse deep learning approaches and two transformer methods for classification, the study culminated in the ViT method achieving an unprecedented 99.84% accuracy, 99.95% recall, and 99.66% precision, marking the highest accuracy documented. These findings strongly advocate for the efficacy of the ViT method in successfully classifying lemon quality, spotlighting its potential impact on agricultural quality assessment.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f792dcf878e5bf8e7fb5aa31d231474d43462f4a.pdf",
        "venue": "European Food Research and Technology",
        "citationCount": 4,
        "score": 4.0
    },
    "310f5543603bef94d42366878a14161db1bf45de.pdf": {
        "title": "SPT-Swin: A Shifted Patch Tokenization Swin Transformer for Image Classification",
        "authors": [
            "Gazi Jannatul Ferdous",
            "Khaleda Akhter Sathi",
            "Md. Azad Hossain",
            "M. Ali Akber Dewan"
        ],
        "published_date": "2024",
        "abstract": "Recently, the transformer-based model e.g., the vision transformer (ViT) has been extensively used in computer vision tasks. The superior performance of the ViT leads to the requirement of an enormous dataset and the complexity of calculating self-attention between patches is quadratic in nature. To acknowledge these two concerns, this paper proposes a novel shifted patch tokenization swin transformer (SPT-Swin) for the image classification task. The shifted patch tokenization (SPT) compensates for the data deficiency by increasing the data samples based on spatial information of the image patches while the swin transformer provides linear computational complexity by calculating self-attention between the shifted window based patches. For model validation, the SPT-Swin framework is trained on popular benchmark image datasets such as ImageNet-1K, CIFAR-10 and CIFAR-100, and the classification accuracies are found 89.45%, 95.67% and 92.95% respectively. Moreover, the comparative analysis of the proposed model with the existing state-of-the-art models shows that the classification performances are improved by 7.05%, 4.14%, and 8.30% for the ImageNet-1K, CIFAR-10 and CIFAR-100 datasets respectively. Therefore, our proposed SPT-based data augmentation technique with the core swin transformer model could be a data-efficient linear complex-able model for future computer vision tasks.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/310f5543603bef94d42366878a14161db1bf45de.pdf",
        "venue": "IEEE Access",
        "citationCount": 4,
        "score": 4.0
    },
    "f9f524944dd9d1fe4b8956f9a822927b2eec29ad.pdf": {
        "title": "An enhanced Swin Transformer for soccer player reidentification",
        "authors": [
            "Sara Akan",
            "Song\u00fcl Varli",
            "Mohammad Alfrad Nobel Bhuiyan"
        ],
        "published_date": "2024",
        "abstract": "The re-identification (ReID) of objects in images is a widely studied topic in computer vision, with significant relevance to various applications. The ReID of players in broadcast videos of team sports is the focus of this study. We specifically focus on identifying the same player in images taken at any given moment during a game from various camera angles. This work varies from other person ReID apps since the same team wears very similar clothes, there are few samples for each identification, and image resolutions are low. One of the hardest parts of object ReID is robust feature representation extraction. Despite the great success of current convolutional neural network-based (CNN) methods, most studies only consider learning representations from images, neglecting long-range dependency. Transformer-based model studies are increasing and yielding encouraging results. Transformers still have trouble extracting features from small objects and visual cues. To address these issues, we enhanced the Swin Transformer with the levering of CNNs. We created a regional feature extraction Swin Transformer (RFES) backbone to increase local feature extraction and small-scale object feature extraction. We also use three loss functions to handle imbalanced data and highlight challenging situations. Re-ranking with k-reciprocal encoding was used in this study's retrieval phase, and its assessment findings were provided. Finally, we conducted experiments on the Market-1501 and SoccerNet-v3 ReID datasets. Experimental results show that the proposed re-ID method reaches rank-1 accuracy of 96.2% with mAP: 89.1 and rank-1 accuracy of 84.1% with mAP: 86.7 on the Market-1501 and SoccerNet-v3 datasets, respectively, outperforming the state-of-the-art approaches.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f9f524944dd9d1fe4b8956f9a822927b2eec29ad.pdf",
        "venue": "Scientific Reports",
        "citationCount": 4,
        "score": 4.0
    },
    "6ec48cb5fa3d149c9cc8a29373e64b63fe257913.pdf": {
        "title": "Tomato maturity stage prediction based on vision transformer and deep convolution neural networks",
        "authors": [
            "Pradeep Nahak",
            "D. K. Pratihar",
            "A. K. Deb"
        ],
        "published_date": "2024",
        "abstract": "Automated assessment of tomato crop maturity is vital for improving agricultural productivity and reducing food waste. Traditionally, farmers have relied on visual inspection and manual assessment to predict tomato maturity, which is prone to human error and time-consuming. Computer vision and deep learning automate this process by analysing visual characteristics, enabling data-driven harvest decisions, optimising quality, and reducing waste for sustainable and efficient agriculture. This research demonstrates deep learning models accurately classifying tomato maturity stages using computer vision techniques, utilising a novel dataset of 4,353 tomato images. The Vision Transformer (ViT) model exhibited superior performance in classifying tomatoes into three ripeness categories (immature, mature, and partially mature), achieving a remarkable testing accuracy of 98.67% and the Convolution neural network (CNN) models, including EfficientNetB1, EfficientNetB5, EfficientNetB7, InceptionV3, ResNet50, and VGG16, achieved testing accuracies of 88.52%, 89.84%, 91.16%, 90.94%, 93.15%, and 92.27%, respectively, when tested with unseen data. ViT significantly surpassed the performance of CNN models. This research highlights the potential for deploying ViT in agricultural environments to monitor tomato maturity stages and packaging facilities smartly. Transformer-based systems could substantially reduce food waste and improve producer profits and productivity by optimising fruit harvest time and sorting decisions.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6ec48cb5fa3d149c9cc8a29373e64b63fe257913.pdf",
        "venue": "International Journal of Hybrid Intelligent Systems",
        "citationCount": 4,
        "score": 4.0
    },
    "3c7e07304bab6a860dbbe4ff36a4d87010036d2a.pdf": {
        "title": "MAT-VIT:A Vision Transformer with MAE-Based Self-Supervised Auxiliary Task for Medical Image Classification",
        "authors": [
            "Yufei Han",
            "Haoyuan Chen",
            "Linwei Yao",
            "Kuan Li",
            "Jianping Yin"
        ],
        "published_date": "2024",
        "abstract": "In the current clinical healthcare environment, there is often a challenge where there is a wealth of unlabeled medical images, but a shortage of labeled images specific to particular medical cases. This limitation restricts the improvement of training effectiveness for deep learning models. This paper, starting from self-supervised learning and drawing inspiration from multi-task learning, explores a Vision Transformer-based self-supervised auxiliary task using MAE for medical image classification. This model establishes both a self-supervised auxiliary task and a supervised primary task, using a shared-weight VIT (Vision Transformer) encoder and synchronously updating network parameters. It effectively leverages both unlabeled and labeled medical images, resulting in promising outcomes. The model is alternately tested on two different medical image classification primary task datasets using three different types of self-supervised auxiliary task sets, and in most cases, it outperforms pure VIT models trained using both supervised and self-supervised learning methods under similar conditions.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3c7e07304bab6a860dbbe4ff36a4d87010036d2a.pdf",
        "venue": "International Conference on Computer Supported Cooperative Work in Design",
        "citationCount": 4,
        "score": 4.0
    },
    "819ae728828d50f56f234e35832b1222de081bfc.pdf": {
        "title": "BiCFormer: Swin Transformer based model for classification of benign and malignant pulmonary nodules",
        "authors": [
            "Xiaoping Zhao",
            "Jingjing Xu",
            "Zhichen Lin",
            "Xingan Xue"
        ],
        "published_date": "2024",
        "abstract": "Pulmonary cancer is one of the most common and deadliest cancers worldwide, and the detection of benign and malignant nodules in the lungs can be an important aid in the early diagnosis of lung cancer. Existing convolutional neural networks inherit their limitations by extracting global contextual information, and in most cases prove to be less efficient in obtaining satisfactory results. Transformer-based deep learning methods have obtained good performance in different computer vision tasks, and this study attempts to introduce them into the task of computed tomography (CT) image classification of lung nodules. However, the problems of sample scarcity and difficulty of local feature extraction in this field. To this end, we are inspired by Swin Transformer to propose a model named BiCFormer for the task of classifying and diagnosing CT scan images of lung nodules. Specifically, first we introduce a multi-layer discriminator generative adversarial network module for data augmentation to assist the model in extracting features more accurately. Second, unlike the encoder of traditional Transformer, we divide the encoder part of BiCFormer into two parts: bi-level coordinate (BiC) and fast-partial-window (FPW). The BiC module has a part similar to the traditional channel attention mechanism is able to enhance the performance of the model, and is more able to enhance the representation of attention object features by aggregating features along two spatial directions. The BiC module also has a dynamic sparse attention mechanism that filters out irrelevant key-value pairs in rough regions, allowing the model to focus more on features of interest. The FPW module is mainly used to reduce computational redundancy and minimize feature loss. We conducted extensive experiments on the LIDC-IDRI dataset. The experimental results show that our model achieves an accuracy of 97.4% compared to other studies using this dataset for lung nodule classification, making it an effective and competitive method.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/819ae728828d50f56f234e35832b1222de081bfc.pdf",
        "venue": "Measurement science and technology",
        "citationCount": 4,
        "score": 4.0
    },
    "6a9247fe471690218ef04cea5f6f4b59b3b50747.pdf": {
        "title": "A Contour-Aware Monocular Depth Estimation Network Using Swin Transformer and Cascaded Multiscale Fusion",
        "authors": [
            "Tao Li",
            "Yi Zhang"
        ],
        "published_date": "2024",
        "abstract": "Depth estimation from monocular vision sensor is a fundamental problem in scene perception with wide industrial applications. Previous works tend to predict the scene depth based on high-level features obtained by convolutional neural networks (CNNs) or rely on encoder\u2013decoder frameworks of Transformers. However, they achieved less satisfactory results, especially around object contours. In this article, we propose a Transformer-based contour-aware depth estimation module to recover the scene depth with the aid of the enhanced perception of object contours. Besides, we develop a cascaded multiscale fusion module to aggregate multilevel features, where we combine the global context with local information and refine the depth map to a higher resolution from coarse to fine. Finally, we model depth estimation as a classification problem and discretize the depth value in an adaptive way to further improve the performance of our network. Extensive experiments have been conducted on mainstream public datasets (KITTI and NYUv2) to demonstrate the effectiveness of our network, where our network exhibits superior performance against other state-of-the-art methods.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6a9247fe471690218ef04cea5f6f4b59b3b50747.pdf",
        "venue": "IEEE Sensors Journal",
        "citationCount": 4,
        "score": 4.0
    },
    "f462bb00b8c4379c4a4699b66a19ce10da530b08.pdf": {
        "title": "Efficient Visual Transformer by Learnable Token Merging",
        "authors": [
            "Yancheng Wang",
            "Yingzhen Yang"
        ],
        "published_date": "2024",
        "abstract": "Self-attention and transformers have been widely used in deep learning. Recent efforts have been devoted to incorporating transformer blocks into different neural architectures, including those with convolutions, leading to various visual transformers for computer vision tasks. In this paper, we propose a novel and compact transformer block, Transformer with Learnable Token Merging (LTM), or LTM-Transformer. LTM-Transformer performs token merging in a learnable scheme. LTM-Transformer is compatible with many popular and compact transformer networks, and it reduces the FLOPs and the inference time of the visual transformers while maintaining or even improving the prediction accuracy. In the experiments, we replace all the transformer blocks in popular visual transformers, including MobileViT, EfficientViT, ViT, and Swin, with LTM-Transformer blocks, leading to LTM-Transformer networks with different backbones. The LTM-Transformer is motivated by reduction of Information Bottleneck, and a novel and separable variational upper bound for the IB loss is derived. The architecture of the mask module in our LTM blocks which generates the token merging mask is designed to reduce the derived upper bound for the IB loss. Extensive results on computer vision tasks evidence that LTM-Transformer renders compact and efficient visual transformers with comparable or much better prediction accuracy than the original visual transformers.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f462bb00b8c4379c4a4699b66a19ce10da530b08.pdf",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citationCount": 4,
        "score": 4.0
    },
    "52fafbbf20a37ffe863f1fa1d44fde6b4fc3334e.pdf": {
        "title": "A Vision-Transformer-Based Convex Variational Network for Bridge Pavement Defect Segmentation",
        "authors": [
            "Haochen Qi",
            "Xiangwei Kong",
            "Zhibo Jin",
            "Jiqiang Zhang",
            "Zinan Wang"
        ],
        "published_date": "2024",
        "abstract": "This study addresses the fine-grained segmentation of defects in bridge pavements, which is crucial for the maintenance and structural safety of bridges. Although bridge pavements pose distinctive challenges owing to their unique characteristics and varied defect types, previous studies have primarily focused on the detection of slender cracks. To fill this research gap, we developed a novel end-to-end hybrid method that dynamically combines the vision transformer (ViT) and level set theory to handle the complex geometry of bridge pavement defects. The novelty of the proposed method lies in the configuration of two parallel decoders. These decoders, operating under a unified objective function, share weights and perform simultaneous optimization, thereby facilitating a holistic end-to-end training process. Furthermore, we compiled two new bridge pavement defect datasets, namely BdridgeDefX and BdridgeDef20, which offer broader applicability for practical defect detection. The results of a rigorous experimental validation on four datasets demonstrated the proposed method\u2019s capability of generating accurate defect boundaries and delivering state-of-the-art performance.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/52fafbbf20a37ffe863f1fa1d44fde6b4fc3334e.pdf",
        "venue": "IEEE transactions on intelligent transportation systems (Print)",
        "citationCount": 4,
        "score": 4.0
    },
    "b4be4f2ecb8711755f75dcb09a57d8c9722ac349.pdf": {
        "title": "Squeeze-and-excitation-attention-based mobile vision transformer for grading recognition of bladder prolapse in pelvic MRI images.",
        "authors": [
            "Shaojun Zhu",
            "Guotao Chen",
            "Hongguang Chen",
            "Ying Lu",
            "Maonian Wu",
            "Bo Zheng",
            "Dongquan Liu",
            "Cheng Qian",
            "Yun Chen"
        ],
        "published_date": "2024",
        "abstract": "BACKGROUND\nBladder prolapse is a common clinical disorder of pelvic floor dysfunction in women, and early diagnosis and treatment can help them recover. Pelvic magnetic resonance imaging (MRI) is one of the most important methods used by physicians to diagnose bladder prolapse; however, it is highly subjective and largely dependent on the clinical experience of physicians. The application of computer-aided diagnostic techniques to achieve a graded diagnosis of bladder prolapse can help improve its accuracy and shorten the learning curve.\n\n\nPURPOSE\nThe purpose of this study is to combine convolutional neural network (CNN) and vision transformer (ViT) for grading bladder prolapse in place of traditional neural networks, and to incorporate attention mechanisms into mobile vision transformer (MobileViT) for assisting in the grading of bladder prolapse.\n\n\nMETHODS\nThis study focuses on the grading of bladder prolapse in pelvic organs using a combination of a CNN and a ViT. First, this study used MobileNetV2 to extract the local features of the images. Next, a ViT was used to extract the global features by modeling the non-local dependencies at a distance. Finally, a channel attention module (i.e., squeeze-and-excitation network) was used to improve the feature extraction network and enhance its feature representation capability. The final grading of the degree of bladder prolapse was thus achieved.\n\n\nRESULTS\nUsing pelvic MRI images provided by a Huzhou Maternal and Child Health Care Hospital, this study used the proposed method to grade patients with bladder prolapse. The accuracy, Kappa value, sensitivity, specificity, precision, and area under the curve of our method were 86.34%, 78.27%, 83.75%, 95.43%, 85.70%, and 95.05%, respectively. In comparison with other CNN models, the proposed method performed better.\n\n\nCONCLUSIONS\nThus, the model based on attention mechanisms exhibits better classification performance than existing methods for grading bladder prolapse in pelvic organs, and it can effectively assist physicians in achieving a more accurate bladder prolapse diagnosis.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/b4be4f2ecb8711755f75dcb09a57d8c9722ac349.pdf",
        "venue": "Medical Physics (Lancaster)",
        "citationCount": 4,
        "score": 4.0
    },
    "7492734c76036143baf574d6602bd45a348c416f.pdf": {
        "title": "A Cutting-Edge Ensemble of Vision Transformer and ResNet101v2 Based Transfer Learning for the Precise Classification of Leukemia Sub-types from Peripheral Blood Smear Images",
        "authors": [
            "Barsha Roy",
            "Md. Farukuzzaman Faruk",
            "Md Nazmul Islam",
            "Azmain Yakin Srizon",
            "S. M. Hasan",
            "Md. Al Mamun",
            "Md. Rakib Hossain",
            "Md. Faruk Hossain"
        ],
        "published_date": "2024",
        "abstract": "Acute Lymphoblastic Leukemia (ALL) stands as the most prevalent form of cancer among children and its diagnosis predominantly involves microscopic blood evaluations of bone marrow. A quick and accurate diagnosis is crucial for effective treatment and better survival rates. The intricate challenge emerges from categorizing leukemia into distinct sub-types aligned with WHO standards. This task deviates from binary classification due to the striking similarity of inter-class features, leading to misclassification. In response, a ViT-CNN ensemble model was introduced in this study to aid in the automated diagnosis of ALL. The proposed ensemble architecture seamlessly integrated Vision Transformer (ViT) with Convolutional Neural Network (CNN) to achieve accurate classification of leukemia sub-types. The ViT-CNN ensemble model orchestrated the extraction of cell image features through two distinct pathways, culminating in enhanced classification outcomes. Leveraging a publicly accessible dataset comprising blood cell images adhering to WHO standards, this study empirically showcased the efficacy of the approach. The proposed ViT-ResNet101v2 ensemble model achieved an exceptional overall accuracy of 99.39%, outperforming numerous prior methods on this dataset. This achievement represents a significant advancement compared to the existing research on leukemia. The proposed methodology adeptly discriminated between leukemia sub-types, thus serving as an efficacious computer-aided diagnostic tool for ALL.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/7492734c76036143baf574d6602bd45a348c416f.pdf",
        "venue": "International Conference on Electrical Engineering and Information Communication Technology",
        "citationCount": 4,
        "score": 4.0
    },
    "eacb2581b480cf4a80bc601c8ea657651b41c0fc.pdf": {
        "title": "Multiple serous cavity effusion screening based on smear images using vision transformer",
        "authors": [
            "Chunbao Wang",
            "Xiangyu Wang",
            "Zeyu Gao",
            "Caihong Ran",
            "Chen Li",
            "Caixia Ding"
        ],
        "published_date": "2024",
        "abstract": "Serous cavity effusion is a prevalent pathological condition encountered in clinical settings. Fluid samples obtained from these effusions are vital for diagnostic and therapeutic purposes. Traditionally, cytological examination of smears is a common method for diagnosing serous cavity effusion, renowned for its convenience. However, this technique presents limitations that can compromise its efficiency and diagnostic accuracy. This study aims to overcome these challenges and introduce an improved method for the precise detection of malignant cells in serous cavity effusions. We have developed a transformer-based classification framework, specifically employing the vision transformer (ViT) model, to fulfill this objective. Our research involved collecting smear images and corresponding cytological reports from 161 patients who underwent serous cavity drainage. We meticulously annotated 4836 patches from these images, identifying regions with and without malignant cells, thus creating a unique dataset for smear image classification. The findings of our study reveal that deep learning models, particularly the ViT model, exhibit remarkable accuracy in classifying patches as malignant or non-malignant. The ViT model achieved an impressive area under the receiver operating characteristic curve (AUROC) of 0.99, surpassing the performance of the convolutional neural network (CNN) model, which recorded an AUROC of 0.86. Additionally, we validated our models using an external cohort of 127 patients. The ViT model sustained its high-level screening performance, achieving an AUROC of 0.98 at the patient level, compared to the CNN model\u2019s AUROC of 0.84. The visualization of our ViT models confirmed their capability to precisely identify regions containing malignant cells in multiple serous cavity effusion smear images. In summary, our study demonstrates the potential of deep learning models, particularly the ViT model, in automating the screening process for serous cavity effusions. These models offer significant assistance to cytologists in enhancing diagnostic accuracy and efficiency. The ViT model stands out for its advanced self-attention mechanism, making it exceptionally suitable for tasks that necessitate detailed analysis of small, sparsely distributed targets like cellular clusters in serous cavity effusions.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/eacb2581b480cf4a80bc601c8ea657651b41c0fc.pdf",
        "venue": "Scientific Reports",
        "citationCount": 4,
        "score": 4.0
    },
    "da7d671f61e53601bdfc760d336cc98e5d9c516b.pdf": {
        "title": "Spectrum Prediction With Deep 3D Pyramid Vision Transformer Learning",
        "authors": [
            "Guangliang Pan",
            "Qihui Wu",
            "Bo Zhou",
            "Jie Li",
            "Wei Wang",
            "Guoru Ding",
            "David K. Y. Yau"
        ],
        "published_date": "2024",
        "abstract": "In this paper, we propose a deep learning (DL)-based task-driven spectrum prediction framework, named DeepSPred. The DeepSPred comprises a feature encoder and a task predictor, where the encoder extracts spectrum usage pattern features, and the predictor configures different networks according to the task requirements to predict future spectrum. Based on the DeepSPred, we first propose a novel 3D spectrum prediction method combining a flow processing strategy with 3D vision Transformer (ViT, i.e., Swin) and a pyramid to serve possible applications such as spectrum monitoring task, named 3D-SwinSTB. 3D-SwinSTB unique 3D Patch Merging ViT-to-3D ViT Patch Expanding and pyramid designs help the model accurately learn the potential correlation of the evolution of the spectrogram over time. Then, we propose a novel spectrum occupancy rate (SOR) method by redesigning a predictor consisting exclusively of 3D convolutional and linear layers to serve possible applications such as dynamic spectrum access (DSA) task, named 3D-SwinLinear. Unlike the 3D-SwinSTB output spectrogram, 3D-SwinLinear projects the spectrogram directly as the SOR. Finally, we employ transfer learning (TL) to ensure the applicability of our two methods to diverse spectrum services. The results show that our 3D-SwinSTB outperforms recent benchmarks by more than 5%, while our 3D-SwinLinear achieves a 90% accuracy, with a performance improvement exceeding 10%.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/da7d671f61e53601bdfc760d336cc98e5d9c516b.pdf",
        "venue": "IEEE Transactions on Wireless Communications",
        "citationCount": 4,
        "score": 4.0
    },
    "2bb98b30fb9aae37858248cd5ac3221ebcf7eeba.pdf": {
        "title": "Vision transformer-based electronic nose for enhanced mixed gases classification",
        "authors": [
            "Haiying Du",
            "Jie Shen",
            "Jing Wang",
            "Qingyu Li",
            "Long Zhao",
            "Wanmin He",
            "Xianrong Li"
        ],
        "published_date": "2024",
        "abstract": "The classification of mixed gases is one of the major functions of the electronic nose. To address the challenges associated with complex feature construction and inadequate feature extraction in gas classification, we propose a classification model for gas mixtures based on the vision transformer (ViT). The whole-process signals of the sensor array are taken as input signals in the proposed classification model, and self-attention mechanism is employed for the fusion of global information and adaptive feature extraction to make full use of the dependence of responses at different stages of the whole-process signals to improve the model\u2019s classification accuracy. Our model exhibited a remarkable accuracy (96.66%) using a dataset containing acetone, methanol, ammonia, and their binary mixtures. In comparison, experiments conducted by support vector machine and a one-dimensional deep convolutional neural network model demonstrated classification accuracy of 90.56% and 92.75%, respectively. Experimental results indicate that the ViT gas classification model can be effectively combined with multi-channel time series data from the sensor array using the self-attention mechanism, thereby improving the accuracy of mixed gases classification. This advancement can be expected to become a standard method for classifying mixed gases.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2bb98b30fb9aae37858248cd5ac3221ebcf7eeba.pdf",
        "venue": "Measurement science and technology",
        "citationCount": 4,
        "score": 4.0
    },
    "90e9d0a35bcf7e8251c4c9601a224bd95db14dd9.pdf": {
        "title": "Building a Vision Transformer-Based Damage Severity Classifier with Ground-Level Imagery of Homes Affected by California Wildfires",
        "authors": [
            "Kevin Luo",
            "Ie-bin Lian"
        ],
        "published_date": "2024",
        "abstract": "The increase in both the frequency and magnitude of natural disasters, coupled with recent advancements in artificial intelligence, has introduced prospects for investigating the potential of new technologies to facilitate disaster response processes. Preliminary Damage Assessment (PDA), a labor-intensive procedure necessitating manual examination of residential structures to ascertain post-disaster damage severity, stands to significantly benefit from the integration of computer vision-based classification algorithms, promising efficiency gains and heightened accuracy. Our paper proposes a Vision Transformer (ViT)-based model for classifying damage severity, achieving an accuracy rate of 95%. Notably, our model, trained on a repository of over 18,000 ground-level images of homes with damage severity annotated by damage assessment professionals during the 2020\u20132022 California wildfires, represents a novel application of ViT technology within this domain. Furthermore, we have open sourced this dataset\u2014the first of its kind and scale\u2014to be used by the research community. Additionally, we have developed a publicly accessible web application prototype built on this classification algorithm, which we have demonstrated to disaster management practitioners and received feedback on. Hence, our contribution to the literature encompasses the provision of a novel imagery dataset, an applied framework from field professionals, and a damage severity classification model with high accuracy.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/90e9d0a35bcf7e8251c4c9601a224bd95db14dd9.pdf",
        "venue": "Fire",
        "citationCount": 4,
        "score": 4.0
    },
    "5ce1ebe051a64969caeb879c44c45320a5ca8011.pdf": {
        "title": "Driver Drowsiness Detection Using Swin Transformer and Diffusion Models for Robust Image Denoising",
        "authors": [
            "Samy Abd El-Nabi",
            "Ahmed F. Ibrahim",
            "El-Sayed M. El-Rabaie",
            "Osama F. Hassan",
            "N. Soliman",
            "K. Ramadan",
            "W. El-shafai"
        ],
        "published_date": "2025",
        "abstract": "With the rapid development of intelligent transportation systems and growing emphasis on driver safety, real-time detection of driver drowsiness has become a critical area of research. This study presents a robust and scalable driver drowsiness detection framework that integrates a Swin Transformer-based deep learning model with a diffusion model for image denoising. While conventional convolutional neural networks (CNNs) are effective in standard vision tasks, they often suffer performance degradation in real-world driving scenarios due to noise, poor lighting, motion blur, and adversarial attacks. To address these challenges, the proposed model focuses on eye-state detection, specifically, prolonged eye closure, as a primary indicator of driver disengagement and fatigue. Our system introduces a novel preprocessing stage using a denoising diffusion model built on a U-Net encoder-decoder architecture, effectively mitigating the impact of Gaussian noise and adversarial perturbations. Additionally, we incorporate adversarial training with Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks, demonstrating significant improvements in classification accuracy and resilience. Evaluations are conducted on two benchmark datasets, Eye-Blink and Closed Eyes in the Wild (CEW), under both clean and noisy conditions. Comparative experiments show that the proposed system outperforms several state-of-the-art models, including ViT, ResNet50V2, InceptionV3, MobileNet, DenseNet169, and VGG19, in terms of accuracy (up to 99.82%), PSNR (up to 41.61 dB), and SSIM (up to 0.984), while maintaining competitive inference times suitable for practical deployment. Moreover, a detailed sensitivity analysis of data augmentation strategies reveals that techniques such as rotation and horizontal flip substantially enhance the model\u2019s generalization across variable visual inputs. The system also demonstrates improved robustness under real-world black-box scenarios and adversarial conditions. While this study primarily targets static image datasets, preliminary evaluations on dynamic video frames suggest potential for real-time monitoring applications. Overall, this research delivers a high-performing driver monitoring system capable of real-time drowsiness detection, even under adverse visual conditions. It lays a strong foundation for future extensions, including temporal modeling, real-time deployment, and multimodal integration (e.g., combining visual input with physiological signals such as EEG and heart rate) to further enhance driver safety and awareness in smart vehicles.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/5ce1ebe051a64969caeb879c44c45320a5ca8011.pdf",
        "venue": "IEEE Access",
        "citationCount": 4,
        "score": 4.0
    },
    "3bba1b3376e43a39bf4f1bf4eab558758a37ea2e.pdf": {
        "title": "High precision banana variety identification using vision transformer based feature extraction and support vector machine",
        "authors": [
            "Ebru Erg\u00fcn"
        ],
        "published_date": "2025",
        "abstract": "Bananas, renowned for their delightful flavor, exceptional nutritional value, and digestibility, are among the most widely consumed fruits globally. The advent of advanced image processing, computer vision, and deep learning (DL) techniques has revolutionized agricultural diagnostics, offering innovative and automated solutions for detecting and classifying fruit varieties. Despite significant progress in DL, the accurate classification of banana varieties remains challenging, particularly due to the difficulty in identifying subtle features at early developmental stages. To address these challenges, this study presents a novel hybrid framework that integrates the Vision Transformer (ViT) model for global semantic feature representation with the robust classification capabilities of Support Vector Machines. The proposed framework was rigorously evaluated on two datasets: the four-class BananaImageBD and the six-class BananaSet. To mitigate data imbalance issues, a robust evaluation strategy was employed, resulting in a remarkable classification accuracy rate (CAR) of 99.86%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\pm\\:$$\\end{document}0.099 for BananaSet and 99.70%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\:\\pm\\:$$\\end{document}0.17 for BananaImageBD, surpassing traditional methods by a margin of 1.77%. The ViT model, leveraging self-supervised and semi-supervised learning mechanisms, demonstrated exceptional promise in extracting nuanced features critical for agricultural applications. By combining ViT features with cutting-edge machine learning classifiers, the proposed system establishes a new benchmark in precision and reliability for the automated detection and classification of banana varieties. These findings underscore the potential of hybrid DL frameworks in advancing agricultural diagnostics and pave the way for future innovations in the domain.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3bba1b3376e43a39bf4f1bf4eab558758a37ea2e.pdf",
        "venue": "Scientific Reports",
        "citationCount": 4,
        "score": 4.0
    },
    "3b11c8774bbeddd6722d65455bf9bb729a63c1cb.pdf": {
        "title": "Vision Transformer Based Semantic Communications for Next Generation Wireless Networks",
        "authors": [
            "Muhammad Ahmed Mohsin",
            "Muhammad Jazib",
            "Zeeshan Alam",
            "Muhammad Farhan Khan",
            "Muhammad Saad",
            "Muhammad Ali Jamshed"
        ],
        "published_date": "2025",
        "abstract": "In the evolving landscape of 6G networks, semantic communications are poised to revolutionize data transmission by prioritizing the transmission of semantic meaning over raw data accuracy. This paper presents a Vision Transformer (ViT)-based semantic communication framework that has been deliberately designed to achieve high semantic similarity during image transmission while simultaneously minimizing the demand for bandwidth. By equipping ViT as the encoder-decoder framework, the proposed architecture can proficiently encode images into a high semantic content at the transmitter and precisely reconstruct the images, considering real-world fading and noise consideration at the receiver. Building on the attention mechanisms inherent to ViTs, our model outperforms Convolution Neural Network (CNNs) and Generative Adversarial Networks (GANs) tailored for generating such images. The architecture based on the proposed ViT network achieves the Peak Signal-to-noise Ratio (PSNR) of 38 dB, which is higher than other Deep Learning (DL) approaches in maintaining semantic similarity across different communication environments. These findings establish our ViT-based approach as a significant breakthrough in semantic communications.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3b11c8774bbeddd6722d65455bf9bb729a63c1cb.pdf",
        "venue": "2025 IEEE International Conference on Communications Workshops (ICC Workshops)",
        "citationCount": 4,
        "score": 4.0
    },
    "04ed3b0f4e21d9f4884bf4db98edf83a34d005a9.pdf": {
        "title": "Pure Vision Transformer (CT-ViT) with Noise2Neighbors Interpolation for Low-Dose CT Image Denoising.",
        "authors": [
            "Luella Marcos",
            "Paul S. Babyn",
            "J. Alirezaie"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/04ed3b0f4e21d9f4884bf4db98edf83a34d005a9.pdf",
        "venue": "Journal of imaging informatics in medicine",
        "citationCount": 4,
        "score": 4.0
    },
    "23ce9c2814d6567efec884b7043977cefcb7602e.pdf": {
        "title": "Computer vision classification detection of chicken parts based on optimized Swin-Transformer",
        "authors": [
            "Xianhui Peng",
            "Chenchen Xu",
            "Peng Zhang",
            "Dandan Fu",
            "Yan Chen",
            "Zhigang Hu"
        ],
        "published_date": "2024",
        "abstract": "ABSTRACT In order to achieve real-time classification and detection of various chicken parts, this study introduces an optimized Swin-Transformer method for the classification and detection of multiple chicken parts. It initially leverages the Transformer\u2019s self-attention structure to capture more comprehensive high-level visual semantic information from chicken part images. The image enhancement technique was applied to the image in the preprocessing stage to enhance the feature information of the image, and the migration learning method was used to train and optimize the Swin-Transformer model on the enhanced chicken parts dataset for classification and detection of chicken parts. Furthermore, this model was compared to four commonly used models in object target detection tasks: YOLOV3-Darknet53, YOLOV3-MobileNetv3, SSD-MobileNetv3, and SSD-VGG16. The results indicated that the Swin-Transformer model outperforms these models with a higher mAP value by 1.62%, 2.13%, 5.26%, and 4.48%, accompanied by a reduction in detection time by 16.18\u2009ms, 5.08\u2009ms, 9.38\u2009ms, and 23.48\u2009ms, respectively. The method of this study fulfills the production line requirements while exhibiting superior performance and greater robustness compared to existing conventional methods. GRAPHICAL ABSTRACT",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/23ce9c2814d6567efec884b7043977cefcb7602e.pdf",
        "venue": "CyTA - Journal of Food",
        "citationCount": 3,
        "score": 3.0
    },
    "5b22bdc6aedf13d812509dd0f768353eb1469a79.pdf": {
        "title": "Enhancing Autonomous Visual Perception in Challenging Environments: Bilateral Models with Vision Transformer and Multilayer Perceptron for Traversable Area Detection",
        "authors": [
            "Claudio Urrea",
            "Maximiliano V\u00e9lez"
        ],
        "published_date": "2024",
        "abstract": "The development of autonomous vehicles has grown significantly recently due to the promise of improving safety and productivity in cities and industries. The scene perception module has benefited from the latest advances in computer vision and deep learning techniques, allowing the creation of more accurate and efficient models. This study develops and evaluates semantic segmentation models based on a bilateral architecture to enhance the detection of traversable areas for autonomous vehicles on unstructured routes, particularly in datasets where the distinction between the traversable area and the surrounding ground is minimal. The proposed hybrid models combine Convolutional Neural Networks (CNNs), Vision Transformer (ViT), and Multilayer Perceptron (MLP) techniques, achieving a balance between precision and computational efficiency. The results demonstrate that these models outperform the base architectures in prediction accuracy, capturing distant details more effectively while maintaining real-time operational capabilities.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/5b22bdc6aedf13d812509dd0f768353eb1469a79.pdf",
        "venue": "Technologies",
        "citationCount": 3,
        "score": 3.0
    },
    "d2fd0dc314023ebca1342f1379ba6d79c7ded84a.pdf": {
        "title": "BAE-ViT: An Efficient Multimodal Vision Transformer for Bone Age Estimation",
        "authors": [
            "Jinnian Zhang",
            "Weijie Chen",
            "Tanmayee Joshi",
            "Xiaomin Zhang",
            "Po-Ling Loh",
            "Varun Jog",
            "Richard J. Bruce",
            "John W. Garrett",
            "Alan B McMillan"
        ],
        "published_date": "2024",
        "abstract": "This research introduces BAE-ViT, a specialized vision transformer model developed for bone age estimation (BAE). This model is designed to efficiently merge image and sex data, a capability not present in traditional convolutional neural networks (CNNs). BAE-ViT employs a novel data fusion method to facilitate detailed interactions between visual and non-visual data by tokenizing non-visual information and concatenating all tokens (visual or non-visual) as the input to the model. The model underwent training on a large-scale dataset from the 2017 RSNA Pediatric Bone Age Machine Learning Challenge, where it exhibited commendable performance, particularly excelling in handling image distortions compared to existing models. The effectiveness of BAE-ViT was further affirmed through statistical analysis, demonstrating a strong correlation with the actual ground-truth labels. This study contributes to the field by showcasing the potential of vision transformers as a viable option for integrating multimodal data in medical imaging applications, specifically emphasizing their capacity to incorporate non-visual elements like sex information into the framework. This tokenization method not only demonstrates superior performance in this specific task but also offers a versatile framework for integrating multimodal data in medical imaging applications.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/d2fd0dc314023ebca1342f1379ba6d79c7ded84a.pdf",
        "venue": "Tomography",
        "citationCount": 3,
        "score": 3.0
    },
    "1c96dc4ad4dfa3bf7c6ca9cf0c6197e7ac00272b.pdf": {
        "title": "STC-ViT: Spatio Temporal Continuous Vision Transformer for Weather Forecasting",
        "authors": [
            "Hira Saleem",
            "Flora Salim",
            "Cormac Purcell"
        ],
        "published_date": "2024",
        "abstract": "Operational weather forecasting system relies on computationally expensive physics-based models. Recently, transformer based models have shown remarkable potential in weather forecasting achieving state-of-the-art results. However, transformers are discrete and physics-agnostic models which limit their ability to learn the continuous spatio-temporal features of the dynamical weather system. We address this issue with STC-ViT, a Spatio-Temporal Continuous Vision Transformer for weather forecasting. STC-ViT incorporates the continuous time Neural ODE layers with multi-head attention mechanism to learn the continuous weather evolution over time. The attention mechanism is encoded as a differentiable function in the transformer architecture to model the complex weather dynamics. Further, we define a customised physics informed loss for STC-ViT which penalize the model's predictions for deviating away from physical laws. We evaluate STC-ViT against operational Numerical Weather Prediction (NWP) model and several deep learning based weather forecasting models. STC-ViT, trained on 1.5-degree 6-hourly data, demonstrates computational efficiency and competitive performance compared to state-of-the-art data-driven models trained on higher-resolution data for global forecasting.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/1c96dc4ad4dfa3bf7c6ca9cf0c6197e7ac00272b.pdf",
        "venue": "",
        "citationCount": 3,
        "score": 3.0
    },
    "c35c316feba84334a4b08feb5281a5f83db0b8b2.pdf": {
        "title": "ViT-FuseNet: Multimodal Fusion of Vision Transformer for Vehicle-Infrastructure Cooperative Perception",
        "authors": [
            "Yang Zhou",
            "Cai Yang",
            "Ping Wang",
            "Chao Wang",
            "Xinhong Wang",
            "Nguyen Ngoc Van"
        ],
        "published_date": "2024",
        "abstract": "Perception plays a vital role in autonomous driving as it serves as a prerequisite for downstream planning and decision tasks. Existing research has mainly focused on developing vehicle-side perception models using a single type of sensors. However, relying solely on one type of on-board sensors to perceive the surrounding environment leads to perceptual deficiencies owing to inherent characteristics and sensor sparsity. To address this bottleneck, we propose ViT-FuseNet, a novel vehicle-infrastructure cooperative perception framework that utilizes a Vision Transformer to fuse feature maps extracted from LiDAR and camera data. The key component is a multimodal fusion module designed based on a cross-attention mechanism. ViT-FuseNet has two distinct advantages: i) it incorporates roadside LiDAR point clouds as additional inputs to enhance the 3D object detection capability of the vehicle; and ii) for the effective fusion of data from two different modal sensors, we employ a cross-attention mechanism for feature fusion, rather than directly merging camera features with point clouds at the raw data level. Extensive experiments are conducted using the DAIR-V2X Dataset to demonstrate the effectiveness of the proposed method. Compared with advanced cooperative perception methods, our method achieves a 6.17% improvement in 3D-mAP (IoU=0.5) and an 8.72% improvement in 3D-mAP (IoU=0.7). Moreover, the framework achieves the highest 3D-mAP (IoU=0.5) in all three object categories of benchmarks for single-vehicle perception.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c35c316feba84334a4b08feb5281a5f83db0b8b2.pdf",
        "venue": "IEEE Access",
        "citationCount": 3,
        "score": 3.0
    },
    "c8174af99bc92d96935683beccc4161c65a8aa46.pdf": {
        "title": "PolySegNet: improving polyp segmentation through swin transformer and vision transformer fusion.",
        "authors": [
            "P. Lijin",
            "M. Ullah",
            "Anuja Vats",
            "F. A. Cheikh",
            "G. Santhosh Kumar",
            "Madhu S. Nair"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/c8174af99bc92d96935683beccc4161c65a8aa46.pdf",
        "venue": "Biomedical Engineering Letters",
        "citationCount": 3,
        "score": 3.0
    },
    "05548c4b3db8be40fac236dfa8e21882fe9ce9de.pdf": {
        "title": "Research and implementation of multi-disease diagnosis on chest X-ray based on vision transformer",
        "authors": [
            "Lan Huang",
            "Jiong Ma",
            "Hui Yang",
            "Yan Wang"
        ],
        "published_date": "2024",
        "abstract": "Background Disease diagnosis in chest X-ray images has predominantly relied on convolutional neural networks (CNNs). However, Vision Transformer (ViT) offers several advantages over CNNs, as it excels at capturing long-term dependencies, exploring correlations, and extracting features with richer semantic information. Methods We adapted ViT for chest X-ray image analysis by making the following three key improvements: (I) employing a sliding window approach in the image sequence feature extraction module to divide the input image into blocks to identify small and difficult-to-detect lesion areas; (II) introducing an attention region selection module in the encoder layer of the ViT model to enhance the model\u2019s ability to focus on relevant regions; and (III) constructing a parallel patient metadata feature extraction network on top of the image feature extraction network to integrate multi-modal input data, enabling the model to synergistically learn and expand image-semantic information. Results The experimental results showed the effectiveness of our proposed model, which had an average area under the curve value of 0.831 in diagnosing 14 common chest diseases. The metadata feature network module effectively integrated patient metadata, further enhancing the model\u2019s accuracy in diagnosis. Our ViT-based model had a sensitivity of 0.863, a specificity of 0.821, and an accuracy of 0.834 in diagnosing these common chest diseases. Conclusions Our model has good general applicability and shows promise in chest X-ray image analysis, effectively integrating patient metadata and enhancing diagnostic capabilities.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/05548c4b3db8be40fac236dfa8e21882fe9ce9de.pdf",
        "venue": "Quantitative Imaging in Medicine and Surgery",
        "citationCount": 3,
        "score": 3.0
    },
    "bd7a4a9e020b594ebcacb886ddae8731a2885209.pdf": {
        "title": "A vision transformer-based deep transfer learning nomogram for predicting lymph node metastasis in lung adenocarcinoma.",
        "authors": [
            "Chuanyu Chen",
            "Yi Luo",
            "Qiuyang Hou",
            "Jun Qiu",
            "Shuya Yuan",
            "Kexue Deng"
        ],
        "published_date": "2024",
        "abstract": "BACKGROUND\nLymph node metastasis (LNM) plays a crucial role in the management of lung cancer; however, the ability of chest computed tomography (CT) imaging to detect LNM status is limited.\n\n\nPURPOSE\nThis study aimed to develop and validate a vision transformer-based deep transfer learning nomogram for predicting LNM in lung adenocarcinoma patients using preoperative unenhanced chest CT imaging.\n\n\nMETHODS\nThis study included 528 patients with lung adenocarcinoma who were randomly divided into training and validation cohorts at a 7:3 ratio. The pretrained vision transformer (ViT) was utilized to extract deep transfer learning (DTL) feature, and logistic regression was employed to construct a ViT-based DTL model. Subsequently, the model was compared with six classical convolutional neural network (CNN) models. Finally, the ViT-based DTL signature was combined with independent clinical predictors to construct a ViT-based deep transfer learning nomogram (DTLN).\n\n\nRESULTS\nThe ViT-based DTL model showed good performance, with an area under the curve (AUC) of 0.821 (95% CI, 0.775-0.867) in the training cohort and 0.825 (95% CI, 0.758-0.891) in the validation cohort. The ViT-based DTL model demonstrated comparable performance to classical CNN models in predicting LNM, and the ViT-based DTL signature was then used to construct ViT-based DTLN with independent clinical predictors such as tumor maximum diameter, location, and density. The DTLN achieved the best predictive performance, with AUCs of 0.865 (95% CI, 0.827-0.903) and 0.894 (95% CI, 0845-0942), respectively, surpassing both the clinical factor model and the ViT-based DTL model (p\u00a0<\u00a00.001).\n\n\nCONCLUSION\nThis study developed a new DTL model based on ViT to predict LNM status in lung adenocarcinoma patients and revealed that the performance of the ViT-based DTL model was comparable to that of classical CNN models, confirming that ViT was viable for deep learning tasks involving medical images. The ViT-based DTLN performed exceptionally well and can assist clinicians and radiologists in making accurate judgments and formulating appropriate treatment plans.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/bd7a4a9e020b594ebcacb886ddae8731a2885209.pdf",
        "venue": "Medical Physics (Lancaster)",
        "citationCount": 3,
        "score": 3.0
    },
    "a7df70e86f049a86b1c555f9a399d3540f466be7.pdf": {
        "title": "A Novel Framework based on a Hybrid Vision Transformer and Deep Neural Network for Deepfake Detection",
        "authors": [
            "Mohammed Shahin",
            "Mohamed Deriche"
        ],
        "published_date": "2024",
        "abstract": "Generative Adversarial Networks (GANs) have enabled the creation of photo-realistic images from random noise. GAN based technologies however, led to the dissemination of synthetic images, often containing inappropriate and miss leading content, on social media. Detecting such manipulated images is crucial, yet challenging. The issue is compounded by the fact that GAN-generated images can be indistinguishable from authentic ones, rendering traditional forgery detection techniques ineffective. Deepfake images further exacerbate this problem, posing threats to news integrity, legal proceedings, and societal security. To address these challenges, we harness the potential of Vision Transformer (ViT) in conjunction with Convolutional Autoencoders (CAE) to craft innovative Framework for image analysis and deepfake detection. We introduce two distinct models, each offering unique insights into image processing. The proposed models yield excellent accuracy rate of approximately 87%, reaffirming the robustness and consistency of the proposed approach and enhanced performance compared to state of the art.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/a7df70e86f049a86b1c555f9a399d3540f466be7.pdf",
        "venue": "International Multi-Conference on Systems, Signals & Devices",
        "citationCount": 3,
        "score": 3.0
    },
    "e4b0c4a1d7ee87ad666214172f329831ade4c25d.pdf": {
        "title": "Interpretable vision transformer based on prototype parts for COVID-19 detection",
        "authors": [
            "Yang Xu",
            "Zuqiang Meng"
        ],
        "published_date": "2024",
        "abstract": "Over the past few years, the COVID\u201019 virus has had a significant impact on the physical and mental health of people around the world. Therefore, in order to effectively distinguish COVID\u201019 patients, many deep learning efforts have used chest medical images to detect COVID\u201019. As with model accuracy, interpretability is also important in the work related to human health. This work introduces an interpretable vision transformer that uses the prototype method for the detection of positive patients with COVID\u201019. The model can learn the prototype features of each category based on the structural characteristics of ViT. The predictions of the model are obtained by comparing all the features of the prototype in the designed prototype block. The proposed model was applied to two chest X\u2010ray datasets and one chest CT dataset, achieving classification performance of 99.3%, 96.8%, and 98.5% respectively. Moreover, the prototype method can significantly improve the interpretability of the model. The decisions of the model can be interpreted based on prototype parts. In the prototype block, the entire inference process of the model can be shown and the predictions of the model can be demonstrated to be meaningful through the visualization of the prototype\u00a0features.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/e4b0c4a1d7ee87ad666214172f329831ade4c25d.pdf",
        "venue": "IET Image Processing",
        "citationCount": 3,
        "score": 3.0
    },
    "6604a900b9a7404a447b2167892a947012a9ffb8.pdf": {
        "title": "Vision Transformer-Based Semantic Communications With Importance-Aware Quantization",
        "authors": [
            "Joohyuk Park",
            "Yong-Nam Oh",
            "Yongjune Kim",
            "Yo-Seb Jeon"
        ],
        "published_date": "2024",
        "abstract": "Semantic communications provide significant performance gains over traditional communications by transmitting task-relevant semantic features through wireless channels. However, most existing studies rely on end-to-end (E2E) training of neural-type encoders and decoders to ensure effective transmission of these semantic features. To enable semantic communications without relying on E2E training, this article presents a vision transformer (ViT)-based semantic communication system with importance-aware quantization (IAQ) for wireless image transmission. The core idea of the presented system is to leverage the attention scores of a pretrained ViT model to quantify the importance levels of image patches. Based on this idea, our IAQ framework assigns different quantization bits to image patches based on their importance levels. This is achieved by formulating a weighted quantization error minimization problem, where the weight is set to be an increasing function of the attention score. Then, an optimal incremental allocation method and a low-complexity water-filling method are devised to solve the formulated problem. Our framework is further extended for realistic digital communication systems by modifying the bit allocation problem and the corresponding allocation methods based on an equivalent binary symmetric channel (BSC) model. Simulations on single-view image classification, multiview image classification, and single-object detection tasks demonstrate that our IAQ framework outperforms conventional image compression methods under both error-free and realistic communication scenarios.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/6604a900b9a7404a447b2167892a947012a9ffb8.pdf",
        "venue": "IEEE Internet of Things Journal",
        "citationCount": 3,
        "score": 3.0
    },
    "9814001811c4924171787de52e01cc31446e2f97.pdf": {
        "title": "PDC-ViT : Source Camera Identification using Pixel Difference Convolution and Vision Transformer",
        "authors": [
            "O. Elharrouss",
            "Y. Akbari",
            "Noor Almaadeed",
            "S. Al-maadeed",
            "F. Khelifi",
            "Ahmed Bouridane"
        ],
        "published_date": "2025",
        "abstract": "Source camera identification has emerged as a vital solution to unlock incidents involving critical cases like terrorism, violence, and other criminal activities. The ability to trace the origin of an image/video can aid law enforcement agencies in gathering evidence and constructing the timeline of events. Moreover, identifying the owner of a certain device narrows down the area of search in a criminal investigation where smartphone devices are involved. This paper proposes a new pixel-based method for source camera identification, integrating Pixel Difference Convolution (PDC) with a Vision Transformer network (ViT), and named PDC-ViT. While the PDC acts as the backbone for feature extraction by exploiting Angular PDC (APDC) and Radial PDC (RPDC). These techniques enhance the capability to capture subtle variations in pixel information, which are crucial for distinguishing between different source cameras. The second part of the methodology focuses on classification, which is based on a Vision Transformer network. Unlike traditional methods that utilize image patches directly for training the classification network, the proposed approach uniquely inputs PDC features into the Vision Transformer network. To demonstrate the effectiveness of the PDC-ViT approach, it has been assessed on five different datasets, which include various image contents and video scenes. The method has also been compared with state-of-the-art source camera identification methods. Experimental results demonstrate the effectiveness and superiority of the proposed system in terms of accuracy and robustness when compared to its competitors. For example, our proposed PDC-ViT has achieved an accuracy of 94.30%, 84%, 94.22% and 92.29% using the Vision dataset, Daxing dataset, Socrates dataset and QUFVD dataset, respectively.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/9814001811c4924171787de52e01cc31446e2f97.pdf",
        "venue": "Neural computing & applications (Print)",
        "citationCount": 3,
        "score": 3.0
    },
    "ab10aacab1a2672a154034c589dd0aa801912272.pdf": {
        "title": "InViT: GAN Inversion-Based Vision Transformer for Blind Image Inpainting",
        "authors": [
            "Yongqiang Du",
            "Haoran Liu",
            "Shengjie He",
            "Songnan Chen"
        ],
        "published_date": "2024",
        "abstract": "Blind image inpainting, the task of detecting corrupted regions with diverse patterns within an image and then generating plausible content for the corrupted regions, remains a both challenging and practical problem in computer vision. In this paper, we propose a novel model InViT for blind image inpainting, which leverages a combination of a pre-trained Generative Adversarial Network (GAN) and a learnable Vision Transformer (ViT). The proposed InViT mainly consists of two phases, the mask prediction phase and the image inpainting phase. Benefiting from the learned latent feature space from the full training data through GAN inversion, a pre-trained StyleGAN is able to provide reliable cues of corrupted regions for mask prediction. By further incorporating the predicted mask into the image inpainting phase, we design a vision Transformer with the mask-aware self-attention mechanism to capture long-range dependencies between pixels during content reconstruction. Besides, we propose a Prompt-augment Contextual Aggregation module to strengthen the reasonableness of generated content for the corrupted regions. Extensive experiments on several benchmark datasets for blind image inpainting demonstrate that our InViT model achieves state-of-the-art performance compared to existing methods in terms of both quantitative metrics and qualitative visual quality.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/ab10aacab1a2672a154034c589dd0aa801912272.pdf",
        "venue": "IEEE Access",
        "citationCount": 3,
        "score": 3.0
    },
    "325367f93439652efaa4bc6b50115bbb7371704e.pdf": {
        "title": "Force-EvT: A Closer Look at Robotic Gripper Force Measurement with Event-Based Vision Transformer",
        "authors": [
            "Qianyu Guo",
            "Ziqing Yu",
            "Jiaming Fu",
            "Yawen Lu",
            "Yahya H. Zweiri",
            "Dongming Gan"
        ],
        "published_date": "2024",
        "abstract": "Robotic grippers are receiving increasing attention in various industries as essential components of robots for interacting and manipulating objects. While significant progress has been made in the past, conventional rigid grippers still have limitations in handling irregular objects and can damage fragile objects. We have shown that soft grippers offer deformability to adapt to a variety of object shapes and maximize object protection. At the same time, dynamic vision sensors (e.g., event-based cameras) are capable of capturing small changes in brightness and streaming them asynchronously as events, unlike RGB cameras, which do not perform well in low-light and fast-moving environments. In this paper, a dynamic-vision-based algorithm is proposed to measure the force applied to the gripper. In particular, we first set up a DVXplorer Lite series event camera to capture twenty-five sets of event data. Second, motivated by the impressive performance of the Vision Transformer (ViT) algorithm in dense image prediction tasks, we propose a new approach that demonstrates the potential for force estimation and meets the requirements of real-world scenarios. We extensively evaluate the proposed algorithm on a wide range of scenarios and settings, and show that it consistently outperforms recent approaches.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/325367f93439652efaa4bc6b50115bbb7371704e.pdf",
        "venue": "2024 6th International Conference on Reconfigurable Mechanisms and Robots (ReMAR)",
        "citationCount": 3,
        "score": 3.0
    },
    "3c6980902883f03c37332d34ead343e1229062b3.pdf": {
        "title": "Temporal Shift Module-Based Vision Transformer Network for Action Recognition",
        "authors": [
            "Kunpeng Zhang",
            "Mengyan Lyu",
            "Xinxin Guo",
            "Liye Zhang",
            "Cong Liu"
        ],
        "published_date": "2024",
        "abstract": "This paper introduces a novel action recognition model named ViT-Shift, which combines the Time Shift Module (TSM) with the Vision Transformer (ViT) architecture. Traditional video action recognition tasks face significant computational challenges, requiring substantial computing resources. However, our model successfully addresses this issue by incorporating the TSM, achieving outstanding performance while significantly reducing computational costs. Our approach is based on the latest Transformer self-attention mechanism, applied to video sequence processing instead of traditional convolutional methods. To preserve the core architecture of ViT and transfer its excellent performance in image recognition to video action recognition, we strategically introduce the TSM only before the multi-head attention layer of ViT. This design allows us to simulate temporal interactions using channel shifts, effectively reducing computational complexity. We carefully design the position and shift parameters of the TSM to maximize the model\u2019s performance. Experimental results demonstrate that ViT-Shift achieves remarkable results on two standard action recognition datasets. With ImageNet-21K pretraining, we achieve an accuracy of 77.55% on the Kinetics-400 dataset and 93.07% on the UCF-101 dataset.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/3c6980902883f03c37332d34ead343e1229062b3.pdf",
        "venue": "IEEE Access",
        "citationCount": 3,
        "score": 3.0
    },
    "f2b1b0fb57cccaac51b44477726d510570c4c799.pdf": {
        "title": "A Radio Frequency Sensor-Based UAV Detection and Identification System Using Improved Vision Transformer-Based Model",
        "authors": [
            "Lu Xu",
            "Rui Shi",
            "Yijia Zhang"
        ],
        "published_date": "2025",
        "abstract": "In this article, we propose a novel approach to addressing the challenge of unmanned aerial vehicle (UAV) detection and identification by combining object detection and image classification techniques. Diverging from prevailing research that predominantly relies on convolutional neural networks (CNNs), we introduce the radio frequency drone vision transformer (RFDroneViT), a novel two-stage vision transformer (ViT) consisting of two distinct sub-ViT. The first stage employs the designed end-to-end object detection with transformer (DETR) to detect the UAV, while the second stage utilizes ViT-B/16 to identify the detected UAV. Based on RFDroneViT, we propose several detailed optimization algorithms, culminating in the development of a cost-effective system capable of UAV detection and identification. We evaluate the system\u2019s performance through rigorous experimentation. Our custom DETR model achieves the state-of-the-art performance in drone signal detection with an average precision (AP) of 74.3 and demonstrates an 8.7 improvement in small target AP (APS) for the signal detection task compared to the original DETR. Additionally, the classification model can achieve 98.7% Top-1. The experimental results demonstrated that our proposed system achieves great performance on drone detection and identification.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/f2b1b0fb57cccaac51b44477726d510570c4c799.pdf",
        "venue": "IEEE Sensors Journal",
        "citationCount": 3,
        "score": 3.0
    },
    "2456506ed87faa667a0c2b8af4028a5a86a49650.pdf": {
        "title": "A New Method for Vehicle Logo Recognition Based on Swin Transformer",
        "authors": [
            "Yang Li",
            "Doudou Zhang",
            "Jianli Xiao"
        ],
        "published_date": "2024",
        "abstract": "Intelligent Transportation Systems (ITS) utilize sensors, cameras, and big data analysis to monitor real-time traffic conditions, aiming to improve traffic efficiency and safety. Accurate vehicle recognition is crucial in this process, and Vehicle Logo Recognition (VLR) stands as a key method. VLR enables effective management and monitoring by distinguishing vehicles on the road. Convolutional Neural Networks (CNNs) have made impressive strides in VLR research. However, achieving higher performance demands significant time and computational resources for training. Recently, the rise of Transformer models has brought new opportunities to VLR. Swin Transformer, with its efficient computation and global feature modeling capabilities, outperforms CNNs under challenging conditions. In this paper, we implement real-time VLR using Swin Transformer and fine-tune it for optimal performance. Extensive experiments conducted on three public vehicle logo datasets (HFUT-VL1, XMU, CTGU-VLD) demonstrate impressive top accuracy results of 99.28%, 100%, and 99.17%, respectively. Additionally, the use of a transfer learning strategy enables our method to be on par with state-of-the-art VLR methods. These findings affirm the superiority of our approach over existing methods. Future research can explore and optimize the application of the Swin Transformer in other vehicle vision recognition tasks to drive advancements in ITS.",
        "file_path": "paper_data/A_survey_on_Visual_Transformer/info/2456506ed87faa667a0c2b8af4028a5a86a49650.pdf",
        "venue": "arXiv.org",
        "citationCount": 3,
        "score": 3.0
    }
}