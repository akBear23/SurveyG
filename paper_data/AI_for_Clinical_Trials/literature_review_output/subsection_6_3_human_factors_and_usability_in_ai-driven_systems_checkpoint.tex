\subsection*{Human Factors and Usability in AI-Driven Systems}

The successful integration of artificial intelligence (AI)-driven decision support systems into clinical trials and practice hinges critically on robust human factors engineering and usability. Without careful consideration of how humans interact with AI, these systems risk 'use error' and potential patient harm, regardless of their underlying algorithmic accuracy. The challenge lies in ensuring that AI outputs are not only interpretable and actionable but also seamlessly integrated into existing clinical workflows, necessitating iterative, science-based approaches to design and evaluation.

Early efforts to standardize the reporting of AI interventions in clinical trials recognized the paramount importance of human-AI interaction. The SPIRIT-AI extension provides guidelines for clinical trial protocols, recommending that investigators clearly describe the AI intervention, including necessary instructions and skills for use, its integration setting, data handling, and crucially, the nature of human-AI interaction and planned analysis of error cases \cite{rivera2020sg1}. Complementing this, the CONSORT-AI extension offers similar reporting guidelines for clinical trial reports, ensuring that these vital human factors considerations are transparently documented in published results \cite{chan2020egf}. These guidelines underscore a foundational shift towards mandating explicit consideration of the human element in AI clinical research.

Building upon these reporting frameworks, regulatory bodies have begun to translate broad requirements into practical expectations for manufacturers and researchers. The DECIDE-AI reporting guideline, for instance, focuses on the early-stage clinical evaluation of AI-based decision support systems, emphasizing the assessment of actual clinical performance, safety, and the human factors surrounding its use \cite{vasey2022oig}. This guideline advocates for the usability engineering process as an iterative, science-based methodology. This approach systematically applies knowledge from diverse fields to design products that are safe and effective for users, actively identifying, assessing, and mitigating potential patient and user safety risks throughout the device lifecycle \cite{vasey2022oig}. Specifically within the Great Britain medical device market, this guidance provides clarified regulatory interpretation for applying established usability engineering principles, thereby translating legal requirements into concrete steps for designing AI systems that minimize 'use error' and maximize clinical utility \cite{vasey2022oig}.

Despite the increasing emphasis on human factors in guidelines and regulations, empirical evidence reveals significant challenges in effective human-AI collaboration. Research by \textcite{rosenthal2025j23} empirically quantified cognitive biases in human-AI interaction among professional radiologists. Their large-scale randomized controlled experiment demonstrated that, even when AI performance was comparable to or surpassed human experts, AI assistance did not, on average, improve human diagnostic quality. This counterintuitive finding was attributed to human cognitive biases such as automation neglect (under-weighting AI predictions) and correlation neglect (treating human and AI information as statistically independent) \cite{rosenthal2025j23}. The study's critical insight was that optimal collaboration often involved delegating cases entirely to either humans or AI, but rarely to AI-assisted humans, due to these identified biases \cite{rosenthal2025j23}. This highlights that simply providing AI predictions is insufficient; the *design* of the interaction and the *context* of delegation are paramount to prevent 'use error' and ensure patient safety.

In conclusion, while reporting guidelines like SPIRIT-AI and CONSORT-AI, and regulatory frameworks such as DECIDE-AI, lay the groundwork for incorporating human factors into AI clinical trials, the empirical realities of human-AI interaction present complex challenges. The findings from studies like \textcite{rosenthal2025j23} underscore the critical need for continuous, iterative usability engineering throughout the development and deployment of AI-driven systems. Future research must bridge the gap between algorithmic accuracy and effective human integration by designing AI systems that are not only robust but also "bias-aware" in their interaction design, complemented by targeted training protocols for clinicians. This comprehensive approach is essential to maximize AI's clinical utility while minimizing risks within the intricate landscape of healthcare.