\subsection*{Explainable AI (XAI) for Interpretability and Trust}

The integration of complex Artificial Intelligence (AI) models into clinical trials, while promising, inherently introduces the "black box" problem, where model decisions are opaque and challenging for human understanding. Explainable AI (XAI) directly addresses this critical issue by providing methodologies to interpret AI predictions, thereby enhancing transparency, fostering trust among clinicians, patients, and regulatory bodies, and facilitating regulatory approval \cite{roy20223mf}. This subsection reviews the development and application of XAI techniques, emphasizing their crucial role in enabling clinicians to understand and validate AI-driven decisions, ensuring ethical considerations are met, and bridging the gap between advanced AI capabilities and their practical, responsible integration into clinical practice.

XAI techniques can broadly be categorized into several approaches relevant to clinical trials. **Feature attribution methods** (e.g., SHAP, LIME) identify the contribution of individual input features to a model's prediction, providing local explanations for specific instances. **Model-specific explanation methods** are tailored to certain architectures, such as Grad-CAM for convolutional neural networks, which highlights relevant regions in image data. **Surrogate models** involve training a simpler, interpretable model to approximate the behavior of a complex black-box model. Finally, **example-based explanations** provide insights by identifying similar training data points that influenced a prediction. These methods are vital for critical tasks like patient selection, safety monitoring, and outcome prediction, where understanding the 'why' behind an AI's recommendation is paramount.

The necessity for XAI intensifies as AI models in biomedicine become more sophisticated, integrating diverse data types. For instance, the development of \textit{multimodal biomedical AI} often involves complex deep learning architectures that combine imaging, genomic, and clinical text data \cite{acosta2022sxu}. While these models offer enhanced predictive power, their inherent complexity makes their decision-making processes particularly opaque. XAI techniques, such as multimodal feature attribution, can elucidate how different data modalities contribute to a given prediction. For example, by applying SHAP values to a multimodal model, researchers can quantify the relative importance of genetic markers versus imaging features in predicting disease progression, providing biologically plausible and clinically relevant insights for validation.

A prime example of AI's application in high-stakes clinical decision-making is the development of AI-derived biomarkers. \textcite{armstrong2023dwd} successfully developed and validated an AI-derived digital pathology-based biomarker to predict the benefit of long-term androgen deprivation therapy in men with localized high-risk prostate cancer. For such a biomarker to achieve widespread clinical adoption and regulatory approval, clinicians and patients must understand *why* the AI makes a particular recommendation. Here, XAI techniques like Grad-CAM could highlight specific pathological regions or cellular patterns within digital pathology images that drive the biomarker's prediction, transforming a black-box output into actionable, interpretable insights. In a practical clinical trial setting, \textcite{angus2020epl} demonstrated an AI-based early warning system for hypotension during surgery. This system provided a risk score along with a "read-out of key variables" used by the algorithm, and anesthesiologists received training on interpreting these features and suggested actions. This exemplifies a direct application of XAI, where the AI's internal logic, even if simplified, is communicated to the user to foster understanding and guide intervention.

The critical importance of XAI is further underscored by challenges related to model generalizability and reliability. \textcite{chekroud2024bvp} highlight the concerning issue of "illusory generalizability" in clinical prediction models, where high accuracy on development datasets fails to translate to independent clinical trials. This lack of robustness severely undermines confidence and poses a significant barrier to practical application and regulatory acceptance. XAI plays a crucial diagnostic role by providing insights into the features or patterns a model relies upon. By revealing if a model is leveraging spurious correlations or context-specific features that do not generalize, XAI can guide the development of more robust, generalizable, and ultimately trustworthy AI systems, directly addressing the limitations identified by Chekroud et al.

Beyond interpretability, XAI is fundamental for addressing ethical considerations and regulatory compliance. The imperative for diversity and equity in healthcare AI, as highlighted by \textcite{hilling2025qq3}, necessitates transparent AI development and fairness audits. XAI methods can reveal biases embedded in models, for instance, by showing if predictions for certain demographic groups rely on different or less robust features, enabling targeted interventions to ensure equitable outcomes. Furthermore, regulatory bodies and reporting guidelines increasingly mandate transparency. The CONSORT-AI guidelines, for example, call for "clear descriptions of the AI intervention, skills required, study setting, inputs and outputs of the AI intervention, analysis of errors, and the human and AI interactions" \cite{parums2021k6f}. Similarly, meta-research studies reveal "poor standards of reporting" in AI diagnostic accuracy studies, underscoring the need for AI-specific quality assessment tools \cite{jayakumar2022sav}. XAI directly supports these requirements by making the AI's decision-making process auditable and understandable, which is crucial for demonstrating safety and efficacy, especially for FDA-approved AI/ML devices often cleared via the 510(k) pathway that relies on substantial equivalence rather than new clinical trials \cite{joshi2024ajq}.

Despite its advancements, XAI faces limitations. Explanations can sometimes be unstable (small input changes lead to large explanation changes), unfaithful to the true model logic, or overly simplistic, potentially misleading clinicians \cite{roy20223mf}. The challenge lies in developing XAI methods that are not only technically sound but also clinically meaningful, actionable, and scalable across diverse AI architectures and data types. Future research must focus on robust validation of XAI explanations in clinical contexts, ensuring they accurately reflect model behavior and genuinely enhance human understanding and decision-making, rather than merely providing a post-hoc rationalization. This continuous innovation in XAI techniques is essential for fostering trust among all stakeholders and accelerating the responsible integration of AI into clinical practice.