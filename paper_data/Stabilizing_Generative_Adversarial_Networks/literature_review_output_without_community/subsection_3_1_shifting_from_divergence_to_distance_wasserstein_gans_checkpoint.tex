\subsection*{Shifting from Divergence to Distance: Wasserstein GANs}

Early Generative Adversarial Networks (GANs) frequently suffered from training instability, particularly vanishing gradients and mode collapse, largely attributable to the choice of divergence metric used to measure the distance between the generator's distribution and the true data distribution. This fundamental challenge was profoundly addressed by the introduction of Wasserstein GANs (WGANs), which marked a pivotal theoretical and practical advancement in the field.

The groundbreaking work by \cite{arjovsky2017ze5} introduced Wasserstein GANs, fundamentally altering the GAN loss function by replacing the problematic Jensen-Shannon (JS) divergence with the Earth Mover's (or Wasserstein-1) distance. The JS divergence, while theoretically sound for overlapping distributions, proved highly unsuitable for the typical scenario in GAN training where the generated and real data distributions often lie on low-dimensional manifolds and are non-overlapping. In such cases, the JS divergence becomes a constant, providing zero gradients almost everywhere, which severely hinders the discriminator's ability to provide meaningful feedback to the generator and leads to the notorious vanishing gradient problem.

The shift to the Earth Mover's distance offered a robust solution to this dilemma. Unlike f-divergences (like JS divergence), the Wasserstein distance provides a smoother and non-zero gradient everywhere, even when the two distributions are non-overlapping. This crucial property ensures that the critic (discriminator in WGANs) can always provide a useful gradient signal to the generator, regardless of how far apart the generated and real data distributions are. This directly mitigates the vanishing gradient problem, allowing for more stable and continuous learning throughout the training process. Furthermore, the Wasserstein distance offers a more meaningful loss metric that empirically correlates with the perceived quality of the generated samples, providing a reliable indicator of training progress that was often absent in traditional GANs.

A cornerstone of WGAN's theoretical stability is the requirement for its critic network to be a K-Lipschitz function. This constraint is derived from the Kantorovich-Rubinstein duality, which states that the Earth Mover's distance can be computed by finding the maximum value of a K-Lipschitz function. Enforcing this Lipschitz constraint on the critic is essential for ensuring that the critic's output is a valid approximation of the Wasserstein distance. Initially, \cite{arjovsky2017ze5} proposed weight clipping as a simple method to enforce this constraint, albeit with some practical limitations such as potentially reducing model capacity or requiring careful hyperparameter tuning. Despite these initial practical challenges, the theoretical foundation laid by WGANs, particularly the rigorous application of the Kantorovich-Rubinstein duality and the K-Lipschitz critic, represented a profound theoretical advancement. It moved GAN research from heuristic-driven stabilization to a more principled, mathematically grounded approach, paving the way for subsequent improvements in GAN training stability and performance.

In conclusion, the introduction of Wasserstein GANs by \cite{arjovsky2017ze5} marked a paradigm shift in generative modeling. By replacing the problematic Jensen-Shannon divergence with the Earth Mover's distance and introducing the K-Lipschitz critic requirement, WGANs provided a stable, theoretically sound framework that effectively addressed vanishing gradients and offered a more interpretable loss. This fundamental change not only stabilized GAN training but also opened new avenues for research into more robust and high-fidelity generative models, establishing a new benchmark for theoretical rigor in the field.