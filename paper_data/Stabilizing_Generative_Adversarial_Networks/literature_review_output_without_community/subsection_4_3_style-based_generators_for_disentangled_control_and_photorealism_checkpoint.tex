\subsection{Style-Based Generators for Disentangled Control and Photorealism}

The pursuit of both photorealistic image synthesis and intuitive, disentangled control over generated features has represented a significant challenge in the evolution of Generative Adversarial Networks (GANs). The seminal StyleGAN architecture \cite{Karras2019} marked a pivotal advancement, introducing a novel style-based generator that fundamentally reshaped the landscape of image generation by dramatically improving perceptual quality and the editability of synthetic images. This innovation distinguished itself from prior GANs, which often struggled with entangled latent spaces where a single latent dimension could influence multiple, unrelated visual attributes \cite{jabbar2020aj0}.

The core of StyleGAN's innovation lies in its unique generator design. Unlike earlier GANs that directly fed a latent code $z$ into the initial layers of the generator, StyleGAN employs a separate *mapping network*. This network transforms an initial, typically isotropic Gaussian latent code $z \in \mathcal{Z}$ into an intermediate latent vector $w \in \mathcal{W}$. The $\mathcal{W}$ space is designed to be less entangled than the original $z$ space, effectively linearizing the latent representation and making it more amenable to controlling specific visual attributes \cite{Karras2019}. This disentanglement is further enhanced through a technique called 'style mixing', where different $w$ vectors (derived from different $z$ codes) are applied to different layers of the generator during training. This forces each style-specific layer to specialize in certain features, promoting a more granular and independent control over the generated image \cite{Karras2019}.

The generator itself is a series of upsampling blocks, but critically, it does not receive the initial latent code $z$ directly. Instead, it starts with a learned constant input and injects 'style' information at multiple resolutions through Adaptive Instance Normalization (AdaIN) layers. AdaIN, originally introduced in the context of style transfer \cite{Huang2017AdaIN}, normalizes the mean and variance of feature map activations independently for each instance and then scales and biases them using learned parameters derived from the intermediate style vector $w$. This mechanism allows for hierarchical control: coarse styles injected at early layers influence high-level attributes such as pose, identity, and overall structural composition, while styles applied at later layers control finer details like hair color, texture, and micro-features \cite{Karras2019}. This approach contrasts with other normalization techniques like Batch Normalization \cite{xiang20171at} or Spectral Normalization \cite{miyato2018arc}, which primarily focus on stabilizing training and enforcing Lipschitz constraints, by explicitly modulating feature statistics to inject style information.

StyleGAN's architectural innovations significantly improved the perceptual quality and realism of generated images, setting new benchmarks. The disentangled $\mathcal{W}$ space, combined with the hierarchical style injection, facilitated unprecedented user-driven content creation, allowing for intuitive manipulation of facial features, age, and other attributes \cite{jabbar2020aj0}. Furthermore, the introduction of the 'truncation trick' allowed for a trade-off between sample diversity and quality, enabling the generation of higher-quality, albeit less diverse, samples by moving latent codes closer to the average $w$ in the $\mathcal{W}$ space \cite{Karras2019}.

Despite its revolutionary impact, the original StyleGAN architecture was not without limitations. While the $\mathcal{W}$ space offered improved disentanglement compared to direct $z$ manipulation, it was not perfectly orthogonal; some entanglement between attributes, such as pose and identity, or between global and local features, still persisted. As a "push-forward" generative model, StyleGAN's ability to fit highly multimodal distributions is theoretically constrained by the Lipschitz constant of its generator, where a large constant is often required for multimodal fitting but can conflict with training stability \cite{salmona202283g}. Moreover, the original StyleGAN exhibited certain characteristic visual artifacts, such as "blob" artifacts or texture sticking, which could manifest as repetitive patterns or unnatural textures, particularly when interpolating in the latent space. These issues, along with challenges related to normalization and upsampling artifacts, highlighted areas for further refinement, paving the way for subsequent architectural improvements aimed at enhancing image quality and consistency.

\bibliography{references}