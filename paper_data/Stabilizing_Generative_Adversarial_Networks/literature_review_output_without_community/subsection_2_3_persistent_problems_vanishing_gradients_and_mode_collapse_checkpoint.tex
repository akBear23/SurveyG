\subsection*{Persistent Problems: Vanishing Gradients and Mode Collapse}

Despite the initial promise of Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}, early architectures were plagued by significant training instabilities, primarily manifesting as vanishing gradients and mode collapse. These issues severely hampered the models' ability to learn diverse and high-fidelity data distributions, highlighting fundamental limitations in the original adversarial training framework.

The problem of vanishing gradients arises when the discriminator becomes overly effective at distinguishing between real and fake samples. In such scenarios, particularly when the real and generated data distributions have non-overlapping supports, the Jensen-Shannon divergence (JSD) used in the original GAN objective saturates. This saturation means the discriminator's loss becomes constant and near zero, providing negligible gradients to the generator. Consequently, the generator receives no meaningful learning signal, effectively halting its progress and preventing it from improving its sample quality. This fundamental limitation was acknowledged as a critical barrier to stable GAN training \cite{roth2017eui}.

Concurrently, mode collapse emerged as another pervasive issue. Instead of capturing the full diversity of the real data distribution, the generator would often converge to producing only a limited variety of samples, frequently focusing on a few distinct "modes" that were particularly effective at fooling the discriminator. This behavior results in a generator that fails to represent the true complexity and richness of the target data, leading to repetitive and uninteresting outputs. For instance, if trained on a dataset of diverse animal images, a generator suffering from mode collapse might only produce images of cats, ignoring dogs, birds, and other animals present in the training data.

Early attempts to address these instabilities often involved regularization techniques. \cite{che2016kho} proposed Mode Regularized Generative Adversarial Networks (MRGANs) to tackle mode collapse and instability. They argued that the "bad behaviors" of GANs stem from the discriminator's functional shape in high-dimensional spaces, which can lead to training stagnation or misdirection of probability mass. Their approach introduced several regularizers to the objective function, aiming to stabilize training and promote a fairer distribution of probability mass across data modes, thereby mitigating the missing modes problem. Similarly, \cite{roth2017eui} introduced a regularization approach specifically to stabilize GAN training, directly addressing the fragility caused by dimensional mismatch or non-overlapping support between the model and data distributions. They noted that such non-overlapping supports cause the density ratio and associated f-divergence to be undefined, a direct precursor to vanishing gradients. Their low-computational-cost regularizer aimed to overcome this fundamental limitation, making GAN models more reliable.

A more fundamental theoretical solution to the vanishing gradient problem was introduced by \cite{arjovsky2017ze5} with the Wasserstein Generative Adversarial Network (WGAN). This work fundamentally altered the loss function by replacing the Jensen-Shannon divergence with the Earth-Mover (Wasserstein-1) distance. The key insight was that the Wasserstein distance provides a continuous and differentiable metric even when the distributions are disjoint, ensuring that the critic (discriminator) can always provide a meaningful gradient to the generator. This property directly addressed the vanishing gradient problem, as the generator would consistently receive a learning signal regardless of how well the critic performed. Furthermore, by providing a smoother loss landscape, the Wasserstein distance inherently contributed to alleviating mode collapse by encouraging the generator to explore a broader range of the data distribution.

These persistent problems of vanishing gradients and mode collapse underscored that mere architectural tweaks were insufficient to stabilize GAN training. Instead, they highlighted the critical need for more fundamental theoretical and algorithmic solutions that could provide robust learning signals and encourage comprehensive mode coverage. This realization propelled the next wave of research, moving beyond empirical fixes to explore deeper mathematical and algorithmic foundations for GAN stabilization.