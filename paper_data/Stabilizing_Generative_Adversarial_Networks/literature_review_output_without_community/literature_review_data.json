{
  "title": "A Comprehensive Literature Review with Self-Reflection",
  "papers_processed": 194,
  "paper_list": [
    "acd87843a451d18b4dc6474ddce1ae946429eaf1.pdf",
    "744fe47157477235032f7bb3777800f9f2f45e52.pdf",
    "84de7d27e2f6160f634a483e8548c499a2cda7fa.pdf",
    "29858b40a15704398aecdca6bd2820f2fcc99891.pdf",
    "ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921.pdf",
    "68cb9fce1e6af2740377494350b650533c9a29e1.pdf",
    "670f9d0d8cafaeaeea564c88645b9816b1146cef.pdf",
    "488bb25e0b1777847f04c943e6dbc4f84415b712.pdf",
    "3bf2147008e0bcb5d80298448fc4bf7f5d3c30ea.pdf",
    "df7ad8eeb595da5f7774e91dae06075be952acff.pdf",
    "024d30897e0a2b036bc122163a954b7f1a1d0679.pdf",
    "6c4fe31504d47b8547e47267c0cb4efa464f022b.pdf",
    "d54d8c402785006faaf5de19e81f04eb484a3aa2.pdf",
    "698d3b667a7f3073eed8368d9daf84f990c24a65.pdf",
    "8b1ba1037aefddec9ce9d07858f661b72a1b41fe.pdf",
    "63470afe06145e08c3b851491450f68c83cc938f.pdf",
    "cb2bd9549791520deccadfde221f8ca699675a96.pdf",
    "3228c8073f6aae9c287dbeea949fbad68f9d5ba1.pdf",
    "27e13389203b2f8f6138afed867965a3a38cbd8e.pdf",
    "cd682f085af85526631dc33617ac4aaae7309634.pdf",
    "1076a77834f11810fdcd100b21d90ca7bc1f9095.pdf",
    "237729237fde44eb7ab8f35aafb82c9b8a816e44.pdf",
    "2f12a10172f33523b288269e59211261ca2f6f67.pdf",
    "34d7c6428bd6d6b109f88ac4a6c71553a4a0f489.pdf",
    "29a23cd054d7a8202e6cdc60a53321bbc6e1aefd.pdf",
    "0b98a1efa7bef2acb2091d5b1659430ef4df1364.pdf",
    "042116e805aa3b5171efaf0c822dc142310ceefe.pdf",
    "82f766d3c572b4c690b439edab5d32b3ba72852e.pdf",
    "29b8b97d554f5139fcf2064ce292204500eee31c.pdf",
    "a4d0c0ca6445af0dc2e513c2e3ec5b852a0b5afa.pdf",
    "567a5d09647f787a37ce8ac300a221d8c4337688.pdf",
    "35dc2337a7f871c93b733432ae84635dffd366aa.pdf",
    "02c6dbdd1d492726f1fd70a9211f668a794e6975.pdf",
    "13fd8d61a6ea97c70f5154a23611c80203527818.pdf",
    "1e3194bf2bdc22a5d1750579a3d2553b61aa4045.pdf",
    "e32e9735b387080492cbf08f85c5e93fcef95b3a.pdf",
    "0d2818f1070fa5a6cf5d14a87a5d71b4107b4d60.pdf",
    "7c4f52328c2869bdff8034d2867baa5b67d0ce27.pdf",
    "cc34aea01322a8fb289e3d4486aad0f6641b472e.pdf",
    "ae9ef65d5551defa3fd8b3e5cd06e4fe959c0d1f.pdf",
    "a3c97d6439f4436700124e6f7ca7170917a99d49.pdf",
    "f644c34fb75f191614f9c69d3a196b5ff5f6a7d6.pdf",
    "aaf7fc55e3ddd07c89dee19ee878c76b7f03041a.pdf",
    "245f8b05bdd1ac65a09a476440dc4b05ac05d4a0.pdf",
    "a0cca4fe677af57d1a8491d698c0d709535c44dd.pdf",
    "e8d7db40cd4f507973c51b17ddd62a15ff861a9e.pdf",
    "531836a1c3fbbf10eba5375d8558f218cdb9805e.pdf",
    "29c53d37cb9bec0210e1584493479df13be85d90.pdf",
    "22530627d05baba39628e9d365b2f7fd8e81fe11.pdf",
    "7fa62c091a14830ae256dc00b512f7d4b4cf5b94.pdf",
    "fc35a72375a8f8cfb7679bdf3e51e676618275a8.pdf",
    "fae3d474c4d7745be06458df0c20bf837a6055ef.pdf",
    "6cfb89a73b12caf20dbcdb7c5f6d9a85f02b0730.pdf",
    "75556186b9b7ba52464a4e64477efff05bde021a.pdf",
    "29424f3ba4b63eca794f4cde9f59d9e9838147c0.pdf",
    "32e277b85802685105254430c4170ad2b1a16c04.pdf",
    "9e1019b67dc1012eba53b34968fe352dc432f49d.pdf",
    "9d4b4b44da22d4afdf127a1e33108f37bf1ec4e4.pdf",
    "484ffd765876bc7f82a9eacff68ca20dbd4fcc34.pdf",
    "162e5a6c7fa9236be27966fd32b8e8f3819fc5b6.pdf",
    "ad37850e17ce5ad9ca954ec97c1fe95c31cefc9e.pdf",
    "30831a581be8b76a99ef079f82e3c1b5f8c2dc05.pdf",
    "0984634505e7b4a8004eaf26416ffedd81cd5861.pdf",
    "fc62dad5dc03f7b2a6d9b8e7d3934108f4b511f8.pdf",
    "32038e56d0174b33a93c66258f346c1a173fe81d.pdf",
    "bffb2fe8b60d7acd307f28ff04b1f3f486511639.pdf",
    "79ffafec2133a6216e367fa52b28ee4bd4f685bf.pdf",
    "bd043bc99d8859614fda7efb49d71beec36b54f3.pdf",
    "3466af048d2093786641ec188fc3d5743c831947.pdf",
    "8264c9d9e6b91b9c73d0231900e74cd529fbc9d5.pdf",
    "e4abcf52b65969f8fed43eff8f5cc512553b41d0.pdf",
    "d5906006e6efc5dbc02878d76407326eb56c363a.pdf",
    "9770263651d8805bb0aac3eb93299867010f3cdd.pdf",
    "ab613e80271896c2a2721f08be1adc60a02a856e.pdf",
    "9d3b9ff09867337ac0d693d4f093ddc6859c65c6.pdf",
    "25cb41a83ce6b70681b037c21e6d2c147dfc001b.pdf",
    "b48b68f52b2ebaa8c7b428e98eafe1953045067f.pdf",
    "83ede0231197bba62d2058a914b33c59f8dbc4d3.pdf",
    "16dc13f77f8631aefb47ee57cadf5676b926e9bb.pdf",
    "3e7f9eaa0341f4f6992d372bad179da9b967fa33.pdf",
    "8daf91622ec05492a52427b7666a4fa859ff2811.pdf",
    "6c01187f5930e9618b05611dca1065b926ed4ab6.pdf",
    "35c3d1ccdb4c2014a00ce9d9a96cdbb93516d2ba.pdf",
    "344e8d09cd6144e84a92273d2b5be6c885ce2c22.pdf",
    "72a6044a0108e0f8f1e68cd70ada46c81a416324.pdf",
    "f9d11bdadd0a10f9cf74da34796328cb77de134d.pdf",
    "23006dfeb539bc4f4f66e43e6d6670c4f3510a4c.pdf",
    "c9f6ff493aade94a2fd6f4e89201e3d7333aedcb.pdf",
    "0edc142b51581a358055d7eddada8a4d0f9d021b.pdf",
    "4136412ac44e9185125246be447d2c06e8676dcc.pdf",
    "ffe50056d8331500d6f8f72c3e7743b096aa2bdf.pdf",
    "4795c82ec752177a2904da44b05231da93d69c4f.pdf",
    "473f4b63e56e88ca8e2fafb156a4c38de2567e86.pdf",
    "7ece301f8d69674b49c3485af49668ed9f6084c8.pdf",
    "a7d87eafa0e0b74b1c65220feb98f8e11d77eda1.pdf",
    "6a514c0c8c031bb4e1cc2ae9032780df408442a5.pdf",
    "b86ce2873e22d6f8fa6f68ff5f050cf36fa9306d.pdf",
    "2844a274111907daea511f8378ccca67a9eb81d1.pdf",
    "36f7724f28f497d55f720719fb58f1c146ecbc32.pdf",
    "2cc9e70cd4664533af9bf34b2da7a1c2694616f8.pdf",
    "aa8d5129fb1a4d3648cec78a2b85ad5970747ed8.pdf",
    "ac5115ece8201ba946375b4515894e2e9a477a86.pdf",
    "a4f020c2339f1cf3595bef233b429f93460f0c0c.pdf",
    "75343727ea5dff0e49b5c22068b9fc426df973a7.pdf",
    "c176ac94717fa2e99fbf0039a05597f30fed34db.pdf",
    "1d21ce033822c23f499179dd19769f7b94077d6b.pdf",
    "1ea136c958425ddd113e48eebdd07865ebf3a745.pdf",
    "89608a379d87d76b24390c3382987492bf39b65f.pdf",
    "7564221c59886c6411b6fa474852d8012908cbfa.pdf",
    "44d1a62a864ee8a41f0477529ec0662758d4be74.pdf",
    "01e9750abbaca78cecbb33cbb7aebfd7a5de91f0.pdf",
    "284d6ebdd626885c857c096a2d564092b6c28b93.pdf",
    "627d4f69b76bb3fc88283de9e8e9f7ee6c598ea7.pdf",
    "7281ed8e5c3ef03dde6bbf4bf5df247f00182311.pdf",
    "5ce61044524cbaff63ab7dd5048b1f06a6ac3af2.pdf",
    "9f074217d51ffb0da3b9716af4adae56215de488.pdf",
    "792ed6251edb9287ecc85b39ff5e585ea30b05f4.pdf",
    "b81957019c4e323552e0113da78a7611c160651e.pdf",
    "14b1c76014eeabc8a2d56a7b06cc39d95981de75.pdf",
    "1ad242cd529f848252a244bb0e9c01480520cfd5.pdf",
    "667cad20be038d2b6aafe17afb989c6db824e0a1.pdf",
    "eb769e44f000fe907e956f453a9e6eab3484f8e9.pdf",
    "ef19c7b7e1f4d0e4644d324eecc32f8cace18bf2.pdf",
    "ba825fdcd26fc77fdec7909eee92a0fa3a3407f2.pdf",
    "8c5bae51a1292767c13b5fed339ea94dc971ff9b.pdf",
    "bba74555301373e84e9850c617a1a7311697e503.pdf",
    "6d94348f7f752cfd095fb9aedaef7ad00ba8c5e3.pdf",
    "142c254eb3c3ff50b82beeb0e2de5c8d7393f922.pdf",
    "466f2700541252556dea82ec3ba625c6e7a61c29.pdf",
    "9f1c57e9807835eba3d6b7991e8b371e9df5ec77.pdf",
    "577ae124136e0e76caf8fbc52e6b6d2072d70bff.pdf",
    "e088a2537492ed5a22885e871a51102a95c97cb6.pdf",
    "dd8254e104ddc7f2fa80f3baaa8537780aa2f65c.pdf",
    "71a373b66f3c48c49901183d2df269e2fee78c44.pdf",
    "cfbafb898a5fd26324c30eecf384dfdc34521090.pdf",
    "688b69ff20e5547dfcbc757881bdad44b1139f06.pdf",
    "9af3183d3737729bff63303cf6bb6e433ffdbb47.pdf",
    "aad41ae781e26c1ccaca82c7e232f90a39da7f8c.pdf",
    "fcb58f1ddadeb767a9b4c0017bdeb602f7dea449.pdf",
    "f532d3223ac31b16d1923c2fcfae0c2a2a033f54.pdf",
    "d77ff1f2cb9af2e801ad10da27226fa0b9699d81.pdf",
    "9d305eb1ff2b48c1e8fd53747983bcdae6ab1753.pdf",
    "caba0a39d4a17e83508b082158560e13cca6f01f.pdf",
    "9602146b95175c69ff187a9ee3bfe45fbc01fa1e.pdf",
    "88cffe6fdf149250c09ae90498431379dd813d3a.pdf",
    "3ae101a0a196fc73419fbb5207a0394dbbedf191.pdf",
    "94087f564f2fc3760f170c35801df0dc511aecb9.pdf",
    "bbe8fd30efad3354f6ff523cc26c3d0eb3a21793.pdf",
    "2a4bcdfb38fd8627b5a1e6161b253ae5b980cfeb.pdf",
    "1e98ac158f0fefdefe9d44d9dc95399bee8ecef6.pdf",
    "6c8cdeb796e34f9bc8c8377079f29f07f44785e4.pdf",
    "bfbba9a4faadc7f820c0b21bbbc4d48d06f88623.pdf",
    "0fe35c17baf4a451ed11981ac518b89abf618278.pdf",
    "44d1adb9f96b87ace6408ba6b9ec31713f74cece.pdf",
    "a6a3a7a76219defe50741256e40cba5a7132e007.pdf",
    "38ade2fc7490467dffc74bced440363fa7c27c5e.pdf",
    "6a1cbe2ee843f6765d3fd849afa7d654daf118a1.pdf",
    "f92e1f06c6f12a287d82ecf06b555b9fcf0b9d99.pdf",
    "b0f12decb3b54ad0eec46d7c29385d714cb879f0.pdf",
    "a3993af5ab3f48f190a6fcdc2fc711d9f091e19d.pdf",
    "4c9f5b3891705ae5637fec5b57c85dff379bcde9.pdf",
    "29bdd183402a94e3cca7531ced412bb427e9285a.pdf",
    "bd8b0558c9d72fa09f849768879777f04599f7d4.pdf",
    "4a1d533193d8e6607c381d231aaea06a5522622a.pdf",
    "9d240b5aa22e310b31d52afd729a1195390da871.pdf",
    "6fba827469a0cd3d090ec9898593445783ab484e.pdf",
    "15f5dc4ed2ee1152aea4ee1e042e80f96316377b.pdf",
    "3c0b8a5ffcd3402d29d81614595a5dd6a9268072.pdf",
    "cee85f664275dc55612b465d89003d946056e02d.pdf",
    "bfb9ec2a6583e6e86716d12f1273e97626bbe5f0.pdf",
    "70f107e0c675dcac8fd9a4f9a11fd78c1ae8700f.pdf",
    "3314863efff246ae64bb266dc920ae44afb24674.pdf",
    "d3c97a9cae3cee24a66f42c5800e438290b7a8ea.pdf",
    "1f76f23c919c9b4503a9a369c11ef303822646cd.pdf",
    "d38bec69a9ebbf9d57de0baf131e711c7c7fbfb4.pdf",
    "f47efc7762b9025ce17fad7a8ffc81c672362851.pdf",
    "6cb4ca76895cf98383a53bb07939cb9a4c6b9391.pdf",
    "50700e326fdedf55245932da52c703f732175f40.pdf",
    "a40eff8cfc5fad3870fa6be8aa55f314316013af.pdf",
    "8ac65e097e13c58825e5dd7b83da911cd37a0d33.pdf",
    "31a841e28be8f81f1c83b34edc51b350b9000236.pdf",
    "045884983c01e75cda7d299e0d31530dd4019b69.pdf",
    "a3a910ba06e4d5564ac3763f617f220d8fd4a146.pdf",
    "df09f8a4dcc9b7b5698afc56a35e66dc099c2429.pdf",
    "fdd5b6806c9b45b94ff59840018a155a990f11c9.pdf",
    "d8166043f684461068b59060b968e9eced7b03c4.pdf",
    "450d90df20a8b050b0e788253d98cf7ac0c14274.pdf",
    "b8ba0bfe19bafb14a4eb30c2d22c2e8f1a7d1160.pdf",
    "316fd1658e4ae59cdbeaf5caa03f46d4d32d616d.pdf",
    "87220aa1684992aba1c48ab72934cfe3a8dd3c59.pdf",
    "d2b0b0e427f9518be18156dc6025acc5c7d32d0a.pdf",
    "62433dc4233f5278c4e5cd84c324514fc528e3a0.pdf",
    "5ff7bda5d2508218fab6c3a3b1f4ea964a59fdcd.pdf",
    "50374aab2ed51f528fbaba4cd1c1482c45b843b6.pdf"
  ],
  "citations_map": {
    "acd87843a451d18b4dc6474ddce1ae946429eaf1.pdf": "arjovsky2017ze5",
    "744fe47157477235032f7bb3777800f9f2f45e52.pdf": "karras2017raw",
    "84de7d27e2f6160f634a483e8548c499a2cda7fa.pdf": "miyato2018arc",
    "29858b40a15704398aecdca6bd2820f2fcc99891.pdf": "karras202039x",
    "ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921.pdf": "zhang2016mm0",
    "68cb9fce1e6af2740377494350b650533c9a29e1.pdf": "shrivastava2016uym",
    "670f9d0d8cafaeaeea564c88645b9816b1146cef.pdf": "zhao2020xhy",
    "488bb25e0b1777847f04c943e6dbc4f84415b712.pdf": "metz20169ir",
    "3bf2147008e0bcb5d80298448fc4bf7f5d3c30ea.pdf": "guo2020n4t",
    "df7ad8eeb595da5f7774e91dae06075be952acff.pdf": "bau2018n2x",
    "024d30897e0a2b036bc122163a954b7f1a1d0679.pdf": "che2016kho",
    "6c4fe31504d47b8547e47267c0cb4efa464f022b.pdf": "liu20212c2",
    "d54d8c402785006faaf5de19e81f04eb484a3aa2.pdf": "jabbar2020aj0",
    "698d3b667a7f3073eed8368d9daf84f990c24a65.pdf": "roth2017eui",
    "8b1ba1037aefddec9ce9d07858f661b72a1b41fe.pdf": "yang2018svo",
    "63470afe06145e08c3b851491450f68c83cc938f.pdf": "zhang2019hjo",
    "cb2bd9549791520deccadfde221f8ca699675a96.pdf": "tseng2021m2s",
    "3228c8073f6aae9c287dbeea949fbad68f9d5ba1.pdf": "mao20196tx",
    "27e13389203b2f8f6138afed867965a3a38cbd8e.pdf": "hartmann2018h3s",
    "cd682f085af85526631dc33617ac4aaae7309634.pdf": "wang2019w53",
    "1076a77834f11810fdcd100b21d90ca7bc1f9095.pdf": "luo2020aaj",
    "237729237fde44eb7ab8f35aafb82c9b8a816e44.pdf": "liu2020jt0",
    "2f12a10172f33523b288269e59211261ca2f6f67.pdf": "liang2018r52",
    "34d7c6428bd6d6b109f88ac4a6c71553a4a0f489.pdf": "ghafoorian2018fwh",
    "29a23cd054d7a8202e6cdc60a53321bbc6e1aefd.pdf": "guo2019414",
    "0b98a1efa7bef2acb2091d5b1659430ef4df1364.pdf": "liu2020kd1",
    "042116e805aa3b5171efaf0c822dc142310ceefe.pdf": "hjelm2017iqg",
    "82f766d3c572b4c690b439edab5d32b3ba72852e.pdf": "shahriar2020sm7",
    "29b8b97d554f5139fcf2064ce292204500eee31c.pdf": "pfau2016v7o",
    "a4d0c0ca6445af0dc2e513c2e3ec5b852a0b5afa.pdf": "mao2017ss0",
    "567a5d09647f787a37ce8ac300a221d8c4337688.pdf": "fekri2019c1i",
    "35dc2337a7f871c93b733432ae84635dffd366aa.pdf": "chen2019ng2",
    "02c6dbdd1d492726f1fd70a9211f668a794e6975.pdf": "baby2019h4h",
    "13fd8d61a6ea97c70f5154a23611c80203527818.pdf": "wiatrak20194ib",
    "1e3194bf2bdc22a5d1750579a3d2553b61aa4045.pdf": "salmona202283g",
    "e32e9735b387080492cbf08f85c5e93fcef95b3a.pdf": "lee20205ue",
    "0d2818f1070fa5a6cf5d14a87a5d71b4107b4d60.pdf": "herr20208x4",
    "7c4f52328c2869bdff8034d2867baa5b67d0ce27.pdf": "hayes201742g",
    "cc34aea01322a8fb289e3d4486aad0f6641b472e.pdf": "negi20208n9",
    "ae9ef65d5551defa3fd8b3e5cd06e4fe959c0d1f.pdf": "meng2022you",
    "a3c97d6439f4436700124e6f7ca7170917a99d49.pdf": "liu2019sb7",
    "f644c34fb75f191614f9c69d3a196b5ff5f6a7d6.pdf": "yuan2020bt6",
    "aaf7fc55e3ddd07c89dee19ee878c76b7f03041a.pdf": "agarwal2022p6d",
    "245f8b05bdd1ac65a09a476440dc4b05ac05d4a0.pdf": "grnarova20171tc",
    "a0cca4fe677af57d1a8491d698c0d709535c44dd.pdf": "liu2019oc8",
    "e8d7db40cd4f507973c51b17ddd62a15ff861a9e.pdf": "chung2022s9a",
    "531836a1c3fbbf10eba5375d8558f218cdb9805e.pdf": "chu2020zbv",
    "29c53d37cb9bec0210e1584493479df13be85d90.pdf": "jenni2019339",
    "22530627d05baba39628e9d365b2f7fd8e81fe11.pdf": "xiang20171at",
    "7fa62c091a14830ae256dc00b512f7d4b4cf5b94.pdf": "neyshabur201713g",
    "fc35a72375a8f8cfb7679bdf3e51e676618275a8.pdf": "bau20197hm",
    "fae3d474c4d7745be06458df0c20bf837a6055ef.pdf": "dieng2019rjn",
    "6cfb89a73b12caf20dbcdb7c5f6d9a85f02b0730.pdf": "zhang2020376",
    "75556186b9b7ba52464a4e64477efff05bde021a.pdf": "yuan202257j",
    "29424f3ba4b63eca794f4cde9f59d9e9838147c0.pdf": "iwai2020fp2",
    "32e277b85802685105254430c4170ad2b1a16c04.pdf": "kaneko2018jex",
    "9e1019b67dc1012eba53b34968fe352dc432f49d.pdf": "khan20223o7",
    "9d4b4b44da22d4afdf127a1e33108f37bf1ec4e4.pdf": "lin20224oj",
    "484ffd765876bc7f82a9eacff68ca20dbd4fcc34.pdf": "tuan2018kbr",
    "162e5a6c7fa9236be27966fd32b8e8f3819fc5b6.pdf": "wei2021qea",
    "ad37850e17ce5ad9ca954ec97c1fe95c31cefc9e.pdf": "wang20178xf",
    "30831a581be8b76a99ef079f82e3c1b5f8c2dc05.pdf": "sage2017ywd",
    "0984634505e7b4a8004eaf26416ffedd81cd5861.pdf": "wu2020p8p",
    "fc62dad5dc03f7b2a6d9b8e7d3934108f4b511f8.pdf": "chavdarova20179w6",
    "32038e56d0174b33a93c66258f346c1a173fe81d.pdf": "li2020muy",
    "bffb2fe8b60d7acd307f28ff04b1f3f486511639.pdf": "goudarzi2020ymw",
    "79ffafec2133a6216e367fa52b28ee4bd4f685bf.pdf": "tao20219q2",
    "bd043bc99d8859614fda7efb49d71beec36b54f3.pdf": "zhong2019opk",
    "3466af048d2093786641ec188fc3d5743c831947.pdf": "yan2020889",
    "8264c9d9e6b91b9c73d0231900e74cd529fbc9d5.pdf": "lee20203j4",
    "e4abcf52b65969f8fed43eff8f5cc512553b41d0.pdf": "hu2021yk5",
    "d5906006e6efc5dbc02878d76407326eb56c363a.pdf": "chen2021n5h",
    "9770263651d8805bb0aac3eb93299867010f3cdd.pdf": "cai2019g1w",
    "ab613e80271896c2a2721f08be1adc60a02a856e.pdf": "zhou20199sm",
    "9d3b9ff09867337ac0d693d4f093ddc6859c65c6.pdf": "tang2018iie",
    "25cb41a83ce6b70681b037c21e6d2c147dfc001b.pdf": "tong2022lu4",
    "b48b68f52b2ebaa8c7b428e98eafe1953045067f.pdf": "costa2019pj9",
    "83ede0231197bba62d2058a914b33c59f8dbc4d3.pdf": "tang2021c82",
    "16dc13f77f8631aefb47ee57cadf5676b926e9bb.pdf": "yin2022izd",
    "3e7f9eaa0341f4f6992d372bad179da9b967fa33.pdf": "xu2020pkq",
    "8daf91622ec05492a52427b7666a4fa859ff2811.pdf": "rahman2021wm8",
    "6c01187f5930e9618b05611dca1065b926ed4ab6.pdf": "zhang2022ysl",
    "35c3d1ccdb4c2014a00ce9d9a96cdbb93516d2ba.pdf": "varshney2021954",
    "344e8d09cd6144e84a92273d2b5be6c885ce2c22.pdf": "creswell2016mol",
    "72a6044a0108e0f8f1e68cd70ada46c81a416324.pdf": "bang2018ps8",
    "f9d11bdadd0a10f9cf74da34796328cb77de134d.pdf": "wang202066v",
    "23006dfeb539bc4f4f66e43e6d6670c4f3510a4c.pdf": "cai2020n2k",
    "c9f6ff493aade94a2fd6f4e89201e3d7333aedcb.pdf": "wenzel20225g3",
    "0edc142b51581a358055d7eddada8a4d0f9d021b.pdf": "gidel2018pg0",
    "4136412ac44e9185125246be447d2c06e8676dcc.pdf": "grinblat2017cem",
    "ffe50056d8331500d6f8f72c3e7743b096aa2bdf.pdf": "zhang202263o",
    "4795c82ec752177a2904da44b05231da93d69c4f.pdf": "shin2020169",
    "473f4b63e56e88ca8e2fafb156a4c38de2567e86.pdf": "ham2020svv",
    "7ece301f8d69674b49c3485af49668ed9f6084c8.pdf": "wang20182xz",
    "a7d87eafa0e0b74b1c65220feb98f8e11d77eda1.pdf": "zhang2018oba",
    "6a514c0c8c031bb4e1cc2ae9032780df408442a5.pdf": "liang2018axu",
    "b86ce2873e22d6f8fa6f68ff5f050cf36fa9306d.pdf": "wiatrak20194ae",
    "2844a274111907daea511f8378ccca67a9eb81d1.pdf": "xue2022n0r",
    "36f7724f28f497d55f720719fb58f1c146ecbc32.pdf": "oeldorf2019kj7",
    "2cc9e70cd4664533af9bf34b2da7a1c2694616f8.pdf": "sajjadi2018w83",
    "aa8d5129fb1a4d3648cec78a2b85ad5970747ed8.pdf": "park2021v6f",
    "ac5115ece8201ba946375b4515894e2e9a477a86.pdf": "song2020mj8",
    "a4f020c2339f1cf3595bef233b429f93460f0c0c.pdf": "randhawa2021ksq",
    "75343727ea5dff0e49b5c22068b9fc426df973a7.pdf": "wang2020vbt",
    "c176ac94717fa2e99fbf0039a05597f30fed34db.pdf": "saqur2018oqp",
    "1d21ce033822c23f499179dd19769f7b94077d6b.pdf": "gao2018d4g",
    "1ea136c958425ddd113e48eebdd07865ebf3a745.pdf": "you2018a3m",
    "89608a379d87d76b24390c3382987492bf39b65f.pdf": "du2021bhg",
    "7564221c59886c6411b6fa474852d8012908cbfa.pdf": "wei2021gla",
    "44d1a62a864ee8a41f0477529ec0662758d4be74.pdf": "lazarou2020gu8",
    "01e9750abbaca78cecbb33cbb7aebfd7a5de91f0.pdf": "zhang2021ypi",
    "284d6ebdd626885c857c096a2d564092b6c28b93.pdf": "jiang2020e6i",
    "627d4f69b76bb3fc88283de9e8e9f7ee6c598ea7.pdf": "plakias2018h0x",
    "7281ed8e5c3ef03dde6bbf4bf5df247f00182311.pdf": "chao2021ynq",
    "5ce61044524cbaff63ab7dd5048b1f06a6ac3af2.pdf": "zhang20182tk",
    "9f074217d51ffb0da3b9716af4adae56215de488.pdf": "cao20184y8",
    "792ed6251edb9287ecc85b39ff5e585ea30b05f4.pdf": "costa2020anu",
    "b81957019c4e323552e0113da78a7611c160651e.pdf": "panwar2019psx",
    "14b1c76014eeabc8a2d56a7b06cc39d95981de75.pdf": "wu20212vn",
    "1ad242cd529f848252a244bb0e9c01480520cfd5.pdf": "shou2020v6h",
    "667cad20be038d2b6aafe17afb989c6db824e0a1.pdf": "liu2019v0x",
    "eb769e44f000fe907e956f453a9e6eab3484f8e9.pdf": "farrell2019kjy",
    "ef19c7b7e1f4d0e4644d324eecc32f8cace18bf2.pdf": "wu2020n95",
    "ba825fdcd26fc77fdec7909eee92a0fa3a3407f2.pdf": "majtner20192pi",
    "8c5bae51a1292767c13b5fed339ea94dc971ff9b.pdf": "zadorozhnyy20208ft",
    "bba74555301373e84e9850c617a1a7311697e503.pdf": "munia20201u2",
    "6d94348f7f752cfd095fb9aedaef7ad00ba8c5e3.pdf": "warner2020a5z",
    "142c254eb3c3ff50b82beeb0e2de5c8d7393f922.pdf": "lee2017zsj",
    "466f2700541252556dea82ec3ba625c6e7a61c29.pdf": "xu2019uwg",
    "9f1c57e9807835eba3d6b7991e8b371e9df5ec77.pdf": "zhang201996t",
    "577ae124136e0e76caf8fbc52e6b6d2072d70bff.pdf": "pieters2018jh1",
    "e088a2537492ed5a22885e871a51102a95c97cb6.pdf": "xiang2017cc9",
    "dd8254e104ddc7f2fa80f3baaa8537780aa2f65c.pdf": "xiong20243bt",
    "71a373b66f3c48c49901183d2df269e2fee78c44.pdf": "xue2024e7i",
    "cfbafb898a5fd26324c30eecf384dfdc34521090.pdf": "xue20248md",
    "688b69ff20e5547dfcbc757881bdad44b1139f06.pdf": "jenkins2024qf5",
    "9af3183d3737729bff63303cf6bb6e433ffdbb47.pdf": "qiu2025hu0",
    "aad41ae781e26c1ccaca82c7e232f90a39da7f8c.pdf": "boubrahimi2024kts",
    "fcb58f1ddadeb767a9b4c0017bdeb602f7dea449.pdf": "liu20232tr",
    "f532d3223ac31b16d1923c2fcfae0c2a2a033f54.pdf": "song20239hi",
    "d77ff1f2cb9af2e801ad10da27226fa0b9699d81.pdf": "pal2023147",
    "9d305eb1ff2b48c1e8fd53747983bcdae6ab1753.pdf": "gan202494y",
    "caba0a39d4a17e83508b082158560e13cca6f01f.pdf": "eltehewy2023cj4",
    "9602146b95175c69ff187a9ee3bfe45fbc01fa1e.pdf": "chen2023rrf",
    "88cffe6fdf149250c09ae90498431379dd813d3a.pdf": "fu20241mw",
    "3ae101a0a196fc73419fbb5207a0394dbbedf191.pdf": "soleymanzadeh202358z",
    "94087f564f2fc3760f170c35801df0dc511aecb9.pdf": "fathallah20236k5",
    "bbe8fd30efad3354f6ff523cc26c3d0eb3a21793.pdf": "luo2024o1x",
    "2a4bcdfb38fd8627b5a1e6161b253ae5b980cfeb.pdf": "li2024uae",
    "1e98ac158f0fefdefe9d44d9dc95399bee8ecef6.pdf": "cai2024m9z",
    "6c8cdeb796e34f9bc8c8377079f29f07f44785e4.pdf": "u2023m2y",
    "bfbba9a4faadc7f820c0b21bbbc4d48d06f88623.pdf": "liu2023q2q",
    "0fe35c17baf4a451ed11981ac518b89abf618278.pdf": "cheng2023t9b",
    "44d1adb9f96b87ace6408ba6b9ec31713f74cece.pdf": "luo2022rm1",
    "a6a3a7a76219defe50741256e40cba5a7132e007.pdf": "xu2022ss4",
    "38ade2fc7490467dffc74bced440363fa7c27c5e.pdf": "alshehri2022d1h",
    "6a1cbe2ee843f6765d3fd849afa7d654daf118a1.pdf": "yeh2022yvr",
    "f92e1f06c6f12a287d82ecf06b555b9fcf0b9d99.pdf": "gonzlezprieto20214wh",
    "b0f12decb3b54ad0eec46d7c29385d714cb879f0.pdf": "huang2022zar",
    "a3993af5ab3f48f190a6fcdc2fc711d9f091e19d.pdf": "wang2020iia",
    "4c9f5b3891705ae5637fec5b57c85dff379bcde9.pdf": "ma2021w69",
    "29bdd183402a94e3cca7531ced412bb427e9285a.pdf": "baby2020e5n",
    "bd8b0558c9d72fa09f849768879777f04599f7d4.pdf": "pasini2021ta3",
    "4a1d533193d8e6607c381d231aaea06a5522622a.pdf": "goyal2024ufg",
    "9d240b5aa22e310b31d52afd729a1195390da871.pdf": "wang2024v83",
    "6fba827469a0cd3d090ec9898593445783ab484e.pdf": "liao20249ku",
    "15f5dc4ed2ee1152aea4ee1e042e80f96316377b.pdf": "peng2024kkw",
    "3c0b8a5ffcd3402d29d81614595a5dd6a9268072.pdf": "luo2024znt",
    "cee85f664275dc55612b465d89003d946056e02d.pdf": "chen2024ajr",
    "bfb9ec2a6583e6e86716d12f1273e97626bbe5f0.pdf": "song2024htg",
    "70f107e0c675dcac8fd9a4f9a11fd78c1ae8700f.pdf": "qin2024a4b",
    "3314863efff246ae64bb266dc920ae44afb24674.pdf": "tibermacine2025pye",
    "d3c97a9cae3cee24a66f42c5800e438290b7a8ea.pdf": "baoueb2024rlq",
    "1f76f23c919c9b4503a9a369c11ef303822646cd.pdf": "broll2024edy",
    "d38bec69a9ebbf9d57de0baf131e711c7c7fbfb4.pdf": "wang20245dt",
    "f47efc7762b9025ce17fad7a8ffc81c672362851.pdf": "megahed2024c23",
    "6cb4ca76895cf98383a53bb07939cb9a4c6b9391.pdf": "zhang2024k8a",
    "50700e326fdedf55245932da52c703f732175f40.pdf": "bhat202445j",
    "a40eff8cfc5fad3870fa6be8aa55f314316013af.pdf": "ler20248xg",
    "8ac65e097e13c58825e5dd7b83da911cd37a0d33.pdf": "purwono2025spz",
    "31a841e28be8f81f1c83b34edc51b350b9000236.pdf": "roy2024k91",
    "045884983c01e75cda7d299e0d31530dd4019b69.pdf": "seon202526r",
    "a3a910ba06e4d5564ac3763f617f220d8fd4a146.pdf": "ni2024y70",
    "df09f8a4dcc9b7b5698afc56a35e66dc099c2429.pdf": "ye2024n41",
    "fdd5b6806c9b45b94ff59840018a155a990f11c9.pdf": "pajuhanfard2024ult",
    "d8166043f684461068b59060b968e9eced7b03c4.pdf": "eskandarinasab202431h",
    "450d90df20a8b050b0e788253d98cf7ac0c14274.pdf": "deebani202549r",
    "b8ba0bfe19bafb14a4eb30c2d22c2e8f1a7d1160.pdf": "ali2024ks3",
    "316fd1658e4ae59cdbeaf5caa03f46d4d32d616d.pdf": "ju2024uai",
    "87220aa1684992aba1c48ab72934cfe3a8dd3c59.pdf": "xu2024u5a",
    "d2b0b0e427f9518be18156dc6025acc5c7d32d0a.pdf": "elbaz2025wzb",
    "62433dc4233f5278c4e5cd84c324514fc528e3a0.pdf": "chang2024c0a",
    "5ff7bda5d2508218fab6c3a3b1f4ea964a59fdcd.pdf": "guo2024y0l",
    "50374aab2ed51f528fbaba4cd1c1482c45b843b6.pdf": "peng2024crk"
  },
  "sections": {
    "Introduction": "\\section{Introduction}\n\\label{sec:introduction}\n\n\n\n\\subsection{The Promise of Generative Adversarial Networks}\n\\label{sec:1_1_the_promise_of_generative_adversarial_networks}\n\n\nThe introduction of Generative Adversarial Networks (GANs) by Goodfellow et al. in 2014 \\cite{goodfellow2014generative} heralded a transformative era in artificial intelligence, particularly within generative modeling. This novel framework immediately captivated the research community by proposing a unique adversarial training paradigm that promised unprecedented capabilities in data synthesis and unsupervised learning \\cite{jabbar2020aj0, bhat202445j}. At its core, a GAN comprises two competing neural networks: a generator (G) and a discriminator (D). The generator's objective is to learn the underlying distribution of real data and produce synthetic samples that are indistinguishable from authentic ones. Concurrently, the discriminator's role is to become highly proficient at differentiating between real data samples and those fabricated by the generator. This dynamic, zero-sum game, where both networks iteratively refine their strategies, enables the generator to progressively synthesize novel, high-quality content without explicit programming of features or rules \\cite{goodfellow2014generative}.\n\nThis implicit learning of complex data distributions represented a significant departure from prior generative models, fundamentally reshaping the landscape of generative AI. Before GANs, models like Variational Autoencoders (VAEs) and autoregressive models were prominent. While VAEs offered a probabilistic framework for latent space representation, they often struggled to produce perceptually sharp and realistic samples, frequently yielding blurry outputs due to their reliance on reconstruction loss and explicit density modeling \\cite{goyal2024ufg, salmona202283g}. Autoregressive models, on the other hand, could generate high-quality samples but suffered from slow, sequential generation processes. GANs, by contrast, leveraged their adversarial objective to directly push for high fidelity and realism, aiming for generated samples that could fool a sophisticated discriminator. This ability to generate sharp, coherent, and seemingly authentic data was a key driver of the initial excitement, showcasing a profound capability for creative and analytical tasks that was previously unattainable \\cite{wang2019w53}.\n\nThe broad potential for generating novel, high-quality content across diverse domains was immediately apparent and widely discussed. A primary aspiration was photorealistic image synthesis, where GANs could create entirely new faces, landscapes, or objects that were virtually indistinguishable from real photographs \\cite{pieters2018jh1, wang2019w53}. This capability opened doors for applications in digital art, entertainment, and virtual reality, offering tools for artists and designers to generate complex visual content with unprecedented ease. Beyond creative endeavors, GANs demonstrated significant promise in analytical applications, such as data augmentation for scientific research \\cite{wang2019w53}. In fields like medical imaging or specialized industrial applications, where acquiring large, diverse, and annotated datasets is often costly, time-consuming, or ethically challenging, GANs offered a powerful solution to generate synthetic data. This synthetic data could then be used to expand training sets, improve the robustness of downstream machine learning models, and accelerate discoveries in areas like disease diagnosis or material science \\cite{goyal2024ufg}. Furthermore, the framework showed potential for tasks like image-to-image translation, facial attribute manipulation, and style transfer, highlighting its versatility in learning complex mappings between visual domains \\cite{wang2019w53}.\n\nIn conclusion, the initial conception of Generative Adversarial Networks presented a revolutionary approach to unsupervised learning, characterized by its unique adversarial training paradigm. The excitement stemmed from their unprecedented ability to implicitly learn and reproduce the complexities of real-world data, promising a future where AI could be a true partner in creativity, scientific discovery, and diverse analytical tasks. However, this profound promise was immediately tempered by the inherent difficulties of optimizing the adversarial minimax game. The delicate balance required for stable training, coupled with the non-convex nature of the objective function, led to early recognition of significant challenges such as vanishing gradients, mode collapse, and general training instability \\cite{goodfellow2014generative, jabbar2020aj0, bhat202445j, chu2020zbv, salmona202283g}. These fundamental problems, present from GANs' inception, would soon become the central focus of extensive research, driving the field towards developing robust stabilization techniques.\n\\subsection{The Central Challenge: Training Instability}\n\\label{sec:1_2_the_central_challenge:_training_instability}\n\n\nDespite their revolutionary potential in generative modeling, Generative Adversarial Networks (GANs) are fundamentally characterized by profound training instabilities. This inherent difficulty, widely acknowledged across the literature \\cite{jabbar2020aj0, wiatrak20194ib, chu2020zbv}, manifests primarily as unreliable convergence, oscillating performance, and specific failure modes. These issues stem directly from the delicate, non-cooperative nature of the adversarial min-max game, where a generator ($G$) and a discriminator ($D$) are simultaneously optimized. This adversarial dynamic makes GANs exceptionally sensitive to a multitude of factors, including hyperparameter choices, network architectures, and initialization strategies, frequently leading to suboptimal, often uninterpretable, generated outputs \\cite{wang2019w53}. This central challenge has not only defined much of the research trajectory in the field but also underscores the critical need for robust stabilization techniques, highlighting the core problem that this review addresses.\n\nThe core of GAN instability lies in the complex dynamics of their two-player game. Unlike traditional optimization problems that aim to minimize a single loss function towards a stable minimum, GANs involve a continuous competition to find a Nash equilibrium. This minimax objective often lacks a unique, stable equilibrium, or if one exists, it is notoriously difficult to reach through standard gradient-based optimization methods \\cite{liang2018r52, grnarova20171tc}. The non-convex nature of the GAN objective, combined with the continuous interplay where one network's improvement alters the other's optimal strategy, often leads to complex dynamics such as limit cycles or rotational behavior in the parameter space rather than stable convergence \\cite{gonzlezprieto20214wh, chu2020zbv}. This results in training curves that frequently oscillate wildly, with generated sample quality fluctuating significantly throughout the training process. Such erratic behavior makes GANs notoriously sensitive to hyperparameter choices, such as the relative learning rates for the generator and discriminator, batch sizes, and optimizer configurations \\cite{xiang20171at}. Even slight deviations from optimal settings can lead to divergence, poor quality samples, or a complete failure to train, making the process of finding a stable configuration a significant practical hurdle.\n\nThis instability manifests in well-documented failure modes that severely limit GANs' utility. Most notably, these include vanishing gradients, where the generator ceases to receive meaningful learning signals, and mode collapse, where the generator produces only a limited variety of samples, failing to capture the full diversity of the real data distribution \\cite{goodfellow2014generative, salimans2016improved, arjovsky2017wasserstein}. These persistent issues, which plagued early architectures and continue to challenge complex models, represent critical symptoms of the underlying optimization difficulties and will be examined in detail in Subsection 2.3 after the foundational GAN concepts are established.\n\nBeyond these fundamental algorithmic challenges, the practical process of debugging and evaluating unstable GANs is notoriously complex and time-consuming. Unlike supervised learning where validation loss or accuracy directly correlates with model performance, GAN loss values often do not reliably indicate the perceptual quality of generated outputs \\cite{wenzel20225g3, jabbar2020aj0}. A decreasing generator loss might not signify better samples, and an increasing discriminator loss could be a sign of either effective training (discriminator being fooled) or a failing generator. This disconnect forces researchers and practitioners to rely heavily on subjective visual inspection of generated samples, a process that is both labor-intensive and prone to misinterpretation. Without a clear, quantitative signal for convergence or quality, determining when to stop training, comparing different models, or diagnosing the root cause of poor performance becomes a significant practical challenge, further emphasizing the urgency of effective stabilization methods \\cite{karras2017raw}.\n\nIn summary, the training instability of GANs, encompassing unreliable convergence, extreme sensitivity to configuration, the difficulty of debugging due to uninformative metrics, and specific failure modes like vanishing gradients and mode collapse, is not merely a practical inconvenience but a fundamental theoretical and algorithmic challenge. These issues directly impede the ability of GANs to reliably learn complex, high-dimensional data distributions, generate diverse and high-quality samples, and achieve stable training. The persistent nature of these problems has been the primary impetus for extensive research into more robust theoretical frameworks, novel loss functions, and advanced regularization techniques, driving the evolution of the field towards more stable and effective generative models.\n",
    "Foundational Concepts and Early Challenges of GANs": "\\section{Foundational Concepts and Early Challenges of GANs}\n\\label{sec:foundational_concepts__and__early_challenges_of_gans}\n\n\n\n\\subsection{The Original GAN Framework: Adversarial Minimax Game}\n\\label{sec:2_1_the_original_gan_framework:_adversarial_minimax_game}\n\n\nThe seminal work by \\cite{goodfellow2014generative} introduced Generative Adversarial Networks (GANs), a groundbreaking framework that recast generative modeling as a dynamic, zero-sum game between two competing neural networks. This innovative paradigm immediately captured significant attention for its potential to synthesize highly realistic data, particularly images, by learning complex data distributions implicitly \\cite{jabbar2020aj0, bhat202445j}. The foundational understanding of this original framework is crucial for appreciating the subsequent extensive research aimed at stabilizing its delicate adversarial balance and overcoming its inherent optimization challenges.\n\nAt its core, the original GAN architecture comprises two distinct neural networks: a generator ($G$) and a discriminator ($D$). The generator's primary function is to learn a mapping from a simple prior noise distribution $p_z(z)$ (typically a uniform or Gaussian distribution) to the intricate real data distribution $p_{data}(x)$. Through this mapping, the generator produces synthetic samples, denoted as $G(z)$, that aim to be indistinguishable from authentic data. Conversely, the discriminator acts as a binary classifier, tasked with differentiating between real samples drawn directly from $p_{data}(x)$ and fake samples generated by $G$. This adversarial interplay is conceptualized as a minimax game, where the discriminator strives to maximize its classification accuracy, while the generator simultaneously endeavors to minimize the discriminator's ability to discern between real and fake, thereby producing increasingly convincing synthetic data.\n\nThe objective function for this adversarial game is mathematically defined as:\n$$ \\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))] $$\nDuring the training process, the discriminator $D$ is optimized to maximize $V(D,G)$. This entails learning to assign high probabilities to real data samples ($D(x) \\approx 1$) and low probabilities to generated samples ($D(G(z)) \\approx 0$). Concurrently, the generator $G$ is updated to minimize $V(D,G)$, which is equivalent to maximizing $\\mathbb{E}_{z \\sim p_z(z)}[\\log D(G(z))]$. This objective compels $G$ to produce samples $G(z)$ for which $D(G(z))$ approaches $1$, effectively making its outputs appear real to the discriminator. Theoretically, this adversarial game converges to a unique Nash equilibrium. At this equilibrium, the generator perfectly replicates the real data distribution ($p_g = p_{data}$), and the discriminator outputs $0.5$ for all inputs, signifying its inability to distinguish between real and fake samples.\n\nA critical theoretical implication of this original formulation is that, at the global optimum, the objective function corresponds to minimizing the Jensen-Shannon Divergence (JSD) between the real data distribution ($p_{data}$) and the generated data distribution ($p_g$). While JSD is a symmetric and bounded measure of similarity between probability distributions, its properties proved to be a significant source of practical instability in GAN training \\cite{jabbar2020aj0}. Specifically, when the supports of $p_{data}$ and $p_g$ are disjoint or have negligible overlap—a common occurrence, especially in high-dimensional data spaces like images—the JSD becomes a constant value. In such scenarios, the gradients of the discriminator with respect to the generator's parameters can vanish, providing little to no meaningful learning signal to the generator. This vanishing gradient problem makes it exceedingly difficult for the generator to learn effectively, particularly during the early stages of training when $p_g$ is far from $p_{data}$.\n\nBeyond vanishing gradients, the delicate balance inherent in the adversarial minimax game often led to training instability and convergence issues \\cite{bhat202445j}. The competitive nature meant that if one network became too powerful too quickly, the training process could derail. For instance, an overly strong discriminator could consistently output $0$ or $1$ for generated samples, leading to saturated gradients for the generator. Conversely, a weak discriminator might provide an insufficient learning signal, allowing the generator to produce poor-quality samples without significant penalty. This imbalance could manifest as oscillations in performance or, more critically, as mode collapse, where the generator produces only a limited variety of samples, failing to capture the full diversity of the real data distribution \\cite{jabbar2020aj0}. The theoretical analysis of GAN dynamics, such as that by \\cite{gonzlezprieto20214wh}, further elucidates why the training process is inherently unstable; they show that convergent orbits in GANs are often small perturbations of periodic orbits, implying that Nash equilibria can act as spiral attractors, which theoretically justifies the observed slow and unstable training.\n\nIn summary, the original GAN framework by Goodfellow et al. was a revolutionary contribution, offering a powerful new paradigm for generative modeling. However, its reliance on the Jensen-Shannon Divergence as the underlying objective function, coupled with the inherent competitive dynamics of the minimax game, immediately exposed fundamental challenges. These included the pervasive problem of vanishing gradients when data distributions had non-overlapping supports, leading to training instability and the notorious issue of mode collapse. These initial theoretical and practical difficulties underscored the need for significant advancements in objective functions, architectural designs, and training methodologies, setting the stage for the extensive research that followed to stabilize and enhance generative adversarial models.\n\\subsection{Early Architectural Guidelines: Deep Convolutional GANs (DCGANs)}\n\\label{sec:2_2_early_architectural_guidelines:_deep_convolutional_gans_(dcgans)}\n\n\nThe initial formulation of Generative Adversarial Networks (GANs) \\cite{Goodfellow2014} presented a powerful theoretical framework for generative modeling, but their practical implementation was plagued by significant training instability and difficulties in convergence, often producing incoherent or limited-diversity outputs. A crucial methodological progression towards making GANs a more implementable framework was the introduction of Deep Convolutional Generative Adversarial Networks (DCGANs) by \\cite{Radford2015}.\n\nDCGANs marked the first significant step towards practical GANs by effectively integrating Convolutional Neural Networks (CNNs) into both the generator and discriminator architectures. This integration leveraged the hierarchical feature learning capabilities of CNNs, enabling the generation of more coherent and visually plausible images compared to earlier fully-connected architectures. Beyond simply using CNNs, \\cite{Radford2015} introduced a set of architectural heuristics that provided initial stability to GAN training and enabled the generation of more coherent images, marking a crucial methodological progression from the abstract GAN concept to a more implementable framework.\n\nSeveral key architectural guidelines were established. To address training instability and facilitate deeper networks, batch normalization layers were introduced in both the generator and discriminator. Batch normalization helps stabilize learning by normalizing the input to each layer, preventing internal covariate shift and allowing for higher learning rates, which was vital for the deeper convolutional structures. Specific activation functions were also prescribed: ReLU (Rectified Linear Unit) was predominantly used in the generator for all layers except the output, which typically used Tanh to produce pixel values in a normalized range. For the discriminator, LeakyReLU was employed, providing a non-zero gradient for negative inputs and helping to prevent 'dying ReLU' problems, thereby contributing to better gradient flow and more stable adversarial training.\n\nA pivotal architectural choice was the avoidance of pooling layers in favor of strided convolutions. In the generator, fractional-strided convolutions (often referred to as transposed convolutions) were used for spatial upsampling, allowing the network to learn its own upsampling strategy rather than relying on fixed interpolation. Conversely, the discriminator utilized strided convolutions for spatial downsampling. This approach allowed the network to learn more effective spatial transformations, preserving more information and often leading to better image quality than traditional pooling operations.\n\nThese architectural heuristics provided initial stability to GAN training, moving the field from an abstract concept to a more robust and implementable framework. The structured use of CNNs and the proposed architectural choices enabled DCGANs to generate images with significantly improved visual quality and coherence, demonstrating the potential of GANs for unsupervised representation learning. For instance, \\cite{Radford2015} showed that the learned features in the discriminator could be effectively used for classification tasks, and that latent space arithmetic could produce meaningful semantic manipulations in generated images, such as interpolating between gender or expressions.\n\nDespite these advancements, DCGANs still faced limitations. While stability was improved, training remained sensitive to hyperparameter choices and could still suffer from issues like mode collapse, where the generator produces a limited variety of samples. The resolution of generated images was also relatively modest compared to later advancements. Thus, while DCGANs established fundamental architectural principles for deep generative models and showcased the immense potential of GANs, their inherent challenges in achieving consistent stability and scaling to higher resolutions laid the groundwork for subsequent research into more robust training methodologies and advanced architectures.\n\\subsection{Persistent Problems: Vanishing Gradients and Mode Collapse}\n\\label{sec:2_3_persistent_problems:_vanishing_gradients__and__mode_collapse}\n\n\nDespite the initial promise of Generative Adversarial Networks (GANs) \\cite{goodfellow2014generative}, early architectures were plagued by significant training instabilities, primarily manifesting as vanishing gradients and mode collapse. These issues severely hampered the models' ability to learn diverse and high-fidelity data distributions, highlighting fundamental limitations in the original adversarial training framework.\n\nThe problem of vanishing gradients arises when the discriminator becomes overly effective at distinguishing between real and fake samples. In such scenarios, particularly when the real and generated data distributions have non-overlapping supports, the Jensen-Shannon divergence (JSD) used in the original GAN objective saturates. This saturation means the discriminator's loss becomes constant and near zero, providing negligible gradients to the generator. Consequently, the generator receives no meaningful learning signal, effectively halting its progress and preventing it from improving its sample quality. This fundamental limitation was acknowledged as a critical barrier to stable GAN training \\cite{roth2017eui}.\n\nConcurrently, mode collapse emerged as another pervasive issue. Instead of capturing the full diversity of the real data distribution, the generator would often converge to producing only a limited variety of samples, frequently focusing on a few distinct \"modes\" that were particularly effective at fooling the discriminator. This behavior results in a generator that fails to represent the true complexity and richness of the target data, leading to repetitive and uninteresting outputs. For instance, if trained on a dataset of diverse animal images, a generator suffering from mode collapse might only produce images of cats, ignoring dogs, birds, and other animals present in the training data.\n\nEarly attempts to address these instabilities often involved regularization techniques. \\cite{che2016kho} proposed Mode Regularized Generative Adversarial Networks (MRGANs) to tackle mode collapse and instability. They argued that the \"bad behaviors\" of GANs stem from the discriminator's functional shape in high-dimensional spaces, which can lead to training stagnation or misdirection of probability mass. Their approach introduced several regularizers to the objective function, aiming to stabilize training and promote a fairer distribution of probability mass across data modes, thereby mitigating the missing modes problem. Similarly, \\cite{roth2017eui} introduced a regularization approach specifically to stabilize GAN training, directly addressing the fragility caused by dimensional mismatch or non-overlapping support between the model and data distributions. They noted that such non-overlapping supports cause the density ratio and associated f-divergence to be undefined, a direct precursor to vanishing gradients. Their low-computational-cost regularizer aimed to overcome this fundamental limitation, making GAN models more reliable.\n\nA more fundamental theoretical solution to the vanishing gradient problem was introduced by \\cite{arjovsky2017ze5} with the Wasserstein Generative Adversarial Network (WGAN). This work fundamentally altered the loss function by replacing the Jensen-Shannon divergence with the Earth-Mover (Wasserstein-1) distance. The key insight was that the Wasserstein distance provides a continuous and differentiable metric even when the distributions are disjoint, ensuring that the critic (discriminator) can always provide a meaningful gradient to the generator. This property directly addressed the vanishing gradient problem, as the generator would consistently receive a learning signal regardless of how well the critic performed. Furthermore, by providing a smoother loss landscape, the Wasserstein distance inherently contributed to alleviating mode collapse by encouraging the generator to explore a broader range of the data distribution.\n\nThese persistent problems of vanishing gradients and mode collapse underscored that mere architectural tweaks were insufficient to stabilize GAN training. Instead, they highlighted the critical need for more fundamental theoretical and algorithmic solutions that could provide robust learning signals and encourage comprehensive mode coverage. This realization propelled the next wave of research, moving beyond empirical fixes to explore deeper mathematical and algorithmic foundations for GAN stabilization.\n",
    "Core Stability Mechanisms: Divergence, Distance, and Advanced Regularization": "\\section{Core Stability Mechanisms: Divergence, Distance, and Advanced Regularization}\n\\label{sec:core_stability_mechanisms:_divergence,_distance,__and__advanced_regularization}\n\n\n\n\\subsection{Shifting from Divergence to Distance: Wasserstein GANs}\n\\label{sec:3_1_shifting_from_divergence_to_distance:_wasserstein_gans}\n\n\nEarly Generative Adversarial Networks (GANs) frequently suffered from training instability, particularly vanishing gradients and mode collapse, largely attributable to the choice of divergence metric used to measure the distance between the generator's distribution and the true data distribution. This fundamental challenge was profoundly addressed by the introduction of Wasserstein GANs (WGANs), which marked a pivotal theoretical and practical advancement in the field.\n\nThe groundbreaking work by \\cite{arjovsky2017ze5} introduced Wasserstein GANs, fundamentally altering the GAN loss function by replacing the problematic Jensen-Shannon (JS) divergence with the Earth Mover's (or Wasserstein-1) distance. The JS divergence, while theoretically sound for overlapping distributions, proved highly unsuitable for the typical scenario in GAN training where the generated and real data distributions often lie on low-dimensional manifolds and are non-overlapping. In such cases, the JS divergence becomes a constant, providing zero gradients almost everywhere, which severely hinders the discriminator's ability to provide meaningful feedback to the generator and leads to the notorious vanishing gradient problem.\n\nThe shift to the Earth Mover's distance offered a robust solution to this dilemma. Unlike f-divergences (like JS divergence), the Wasserstein distance provides a smoother and non-zero gradient everywhere, even when the two distributions are non-overlapping. This crucial property ensures that the critic (discriminator in WGANs) can always provide a useful gradient signal to the generator, regardless of how far apart the generated and real data distributions are. This directly mitigates the vanishing gradient problem, allowing for more stable and continuous learning throughout the training process. Furthermore, the Wasserstein distance offers a more meaningful loss metric that empirically correlates with the perceived quality of the generated samples, providing a reliable indicator of training progress that was often absent in traditional GANs.\n\nA cornerstone of WGAN's theoretical stability is the requirement for its critic network to be a K-Lipschitz function. This constraint is derived from the Kantorovich-Rubinstein duality, which states that the Earth Mover's distance can be computed by finding the maximum value of a K-Lipschitz function. Enforcing this Lipschitz constraint on the critic is essential for ensuring that the critic's output is a valid approximation of the Wasserstein distance. Initially, \\cite{arjovsky2017ze5} proposed weight clipping as a simple method to enforce this constraint, albeit with some practical limitations such as potentially reducing model capacity or requiring careful hyperparameter tuning. Despite these initial practical challenges, the theoretical foundation laid by WGANs, particularly the rigorous application of the Kantorovich-Rubinstein duality and the K-Lipschitz critic, represented a profound theoretical advancement. It moved GAN research from heuristic-driven stabilization to a more principled, mathematically grounded approach, paving the way for subsequent improvements in GAN training stability and performance.\n\nIn conclusion, the introduction of Wasserstein GANs by \\cite{arjovsky2017ze5} marked a paradigm shift in generative modeling. By replacing the problematic Jensen-Shannon divergence with the Earth Mover's distance and introducing the K-Lipschitz critic requirement, WGANs provided a stable, theoretically sound framework that effectively addressed vanishing gradients and offered a more interpretable loss. This fundamental change not only stabilized GAN training but also opened new avenues for research into more robust and high-fidelity generative models, establishing a new benchmark for theoretical rigor in the field.\n\\subsection{Gradient Penalties for Robust Lipschitz Enforcement}\n\\label{sec:3_2_gradient_penalties_for_robust_lipschitz_enforcement}\n\n\nWhile the weight clipping strategy proposed in the original Wasserstein Generative Adversarial Network (WGAN) \\cite{Arjovsky2017} was theoretically motivated to enforce the $K$-Lipschitz constraint on the critic (discriminator), it introduced significant practical limitations that often hindered model performance and stability. The fixed clipping range, a sensitive hyperparameter, could drastically limit the critic's capacity if too small, leading to underfitting and an inability to learn complex functions. Conversely, a large clipping range might not effectively enforce the Lipschitz constraint, resulting in unstable training dynamics akin to those WGAN aimed to mitigate. Furthermore, weight clipping could cause gradients to concentrate at the boundaries of the clipping range, leading to vanishing or exploding gradients in specific layers and further destabilizing the training process \\cite{jabbar2020aj0, purwono2025spz}. This crude enforcement mechanism often restricted the critic's ability to learn a smooth function landscape, which is crucial for providing consistent and meaningful gradients to the generator.\n\nRecognizing these limitations, \\cite{Gulrajani2017} introduced a more robust and effective method for enforcing the Lipschitz constraint: the gradient penalty (WGAN-GP). Instead of directly manipulating the critic's weights, WGAN-GP added a regularization term to the critic's loss function that penalized the norm of its gradient with respect to its input. Specifically, this penalty term encourages the gradient norm to be close to one for samples interpolated linearly between real and generated data points. This approach ensures that the critic's gradients are smooth and well-behaved across the entire input space, without restricting the model's capacity or introducing the boundary effects observed with weight clipping \\cite{Gulrajani2017}. The theoretical justification for this approach lies in the Kantorovich-Rubinstein duality, which requires the critic to be 1-Lipschitz (or $K$-Lipschitz, with $K=1$ being a common choice for simplicity and stability) for the Wasserstein distance to be accurately estimated. By penalizing deviations from a gradient norm of one, WGAN-GP directly addresses this requirement in a differentiable manner.\n\nThe introduction of the gradient penalty in WGAN-GP provided several critical advantages. Firstly, it allowed the critic to learn a much smoother function, which in turn provided more stable and informative gradients to the generator, significantly enhancing overall training stability. This explicit enforcement of smoothness and Lipschitz continuity is vital for GANs, as highlighted by \\cite{chu2020zbv}, who demonstrate how such conditions contribute to the eventual stationarity of the generator during training. Secondly, by not directly constraining the weights, WGAN-GP allowed the critic to maintain its full representational capacity, enabling it to learn more complex decision boundaries and better distinguish between real and fake samples. This methodological improvement not only prevented capacity limitations but also further reduced the incidence of mode collapse, as the generator received consistent feedback across the data manifold.\n\nHowever, this robustness came with a computational overhead. Calculating the gradient norm with respect to the input data requires computing second-order derivatives (or at least first-order derivatives of the critic's output with respect to its input, which are then used in the penalty term), which can be computationally expensive, especially for high-dimensional inputs and large batch sizes. This computational burden, while manageable, motivated the search for alternative and more efficient methods of Lipschitz enforcement. WGAN-GP represented a significant advancement in function space regularization, explicitly enforcing smoothness. This contrasts with implicit regularization methods that might arise from architectural choices or other normalization techniques. For instance, recent work such as CHAIN (Lipschitz Continuity Constrained Normalization) \\cite{ni2024y70} has explored integrating Lipschitz constraints directly within normalization layers, offering another avenue for ensuring discriminator stability, particularly in data-efficient GANs.\n\nIn conclusion, the transition from WGAN's crude weight clipping to WGAN-GP's gradient penalty marked a crucial evolutionary step in stabilizing GAN training. By providing a robust and capacity-preserving method for Lipschitz enforcement, WGAN-GP laid the groundwork for the development of more advanced and stable GAN architectures, proving indispensable for generating high-quality and diverse samples \\cite{purwono2025spz}. The theoretical soundness and practical efficacy of WGAN-GP quickly made it a standard practice in subsequent GAN architectures. Despite its widespread adoption and efficacy, the computational demands of WGAN-GP, particularly the need for gradient computation on interpolated samples, motivated the search for more computationally efficient and universally applicable methods for Lipschitz enforcement. This led to innovations like Spectral Normalization \\cite{miyato2018arc}, which offered an alternative approach to constraining the discriminator's Lipschitz constant without explicit gradient penalties.\n\\subsection{Spectral Normalization and Dynamic Learning Rates}\n\\label{sec:3_3_spectral_normalization__and__dynamic_learning_rates}\n\n\nThe pursuit of stable and efficient Generative Adversarial Network (GAN) training has been a central challenge since their inception. While early advancements like Wasserstein GANs with Gradient Penalties (WGAN-GP) significantly improved stability by enforcing the Lipschitz constraint on the discriminator, they often introduced computational overhead due to the need for gradient computations on interpolated samples \\cite{gulrajani2017improved}. This computational burden and the sensitivity to the interpolation strategy motivated the search for more direct and efficient methods for Lipschitz enforcement, as well as optimized training dynamics to better manage the adversarial game \\cite{jabbar2020aj0}. The theoretical underpinnings of GAN stability often point to the importance of discriminator smoothness and bounded Lipschitz constants to ensure meaningful gradients and prevent mode collapse \\cite{chu2020zbv}.\n\nA pivotal innovation addressing these challenges was **Spectral Normalization (SN)**, introduced by \\cite{miyato2018arc}. SN offers an elegant and computationally efficient mechanism to enforce the 1-Lipschitz constraint on the discriminator, a critical requirement for stable training, particularly in Wasserstein-based GANs. Unlike gradient penalties, which regularize the discriminator's output gradients, SN directly normalizes the spectral norm of the weight matrices in each layer of the discriminator. The spectral norm of a matrix represents its largest singular value, and by normalizing it to 1, SN ensures that the Lipschitz constant of each individual layer, and consequently the entire discriminator network, is bounded. This direct approach makes SN computationally lighter than gradient penalties, as it avoids the need for explicit gradient computations on interpolated samples. Furthermore, SN is straightforward to implement and can be seamlessly integrated into various GAN architectures without extensive hyperparameter tuning or specific architectural modifications, making it a highly generalizable stabilization technique \\cite{miyato2018arc}. By preventing the discriminator from becoming overly confident or powerful too rapidly, SN fosters smoother loss landscapes, provides more consistent and informative gradient signals to the generator, and significantly mitigates issues such as vanishing gradients and mode collapse, ultimately leading to the generation of higher-quality and more diverse samples. This method represents a refinement in the broader category of weight normalization techniques, which includes earlier approaches like Weight Normalization (WN) \\cite{xiang20171at} that aimed to improve training stability by reparameterizing weights. SN, however, specifically targets the Lipschitz constant, providing a more theoretically grounded and effective solution for GANs.\n\nComplementing Spectral Normalization, the paper by \\cite{miyato2018arc} also effectively employed the **Two-Time-Scale Update Rule (TTUR)**, a technique originally proposed by \\cite{heusel2017gans} to further optimize GAN training dynamics. TTUR is predicated on the understanding that the generator and discriminator, with their distinct objectives and learning challenges, often benefit from different learning rates. Instead of applying a single learning rate to both networks, TTUR allows for separate learning rates, typically setting the discriminator's learning rate to be higher than the generator's. This dynamic learning rate management is crucial for maintaining a healthy adversarial balance throughout the training process. In the context of two-player games like GANs, the interaction between the players' updates can lead to complex dynamics, where the choice of learning rate significantly impacts convergence and stability \\cite{liang2018r52}. If the discriminator learns too slowly, it may fail to provide a sufficiently strong or accurate signal for the generator to improve. Conversely, if the discriminator learns too quickly and becomes overly powerful, the generator's gradients can vanish, leading to training stagnation or mode collapse. By allowing distinct update frequencies or magnitudes, TTUR prevents one network from dominating the other, facilitating a more balanced and effective adversarial game. This strategy enhances training stability, improves convergence properties, and contributes to better sample quality and diversity across a wide range of datasets.\n\nThe combined application of Spectral Normalization and the Two-Time-Scale Update Rule by \\cite{miyato2018arc} marked a significant advancement in GAN research, moving towards more principled and efficient stabilization strategies. SN provides a robust, computationally light, and generalizable method for enforcing Lipschitz continuity, while TTUR optimizes the delicate adversarial balance through dynamic learning rate management. These innovations have become foundational, with SN, in particular, being widely adopted in subsequent state-of-the-art GAN architectures. The principles of Lipschitz-constrained normalization continue to be explored, with recent works like CHAIN (LipsCHitz Continuity ConstrAIned Normalization) \\cite{ni2024y70} further refining normalization techniques for data-efficient GANs by focusing on gradient reduction and adaptive feature interpolation. Similarly, other regularization methods, such as Consistency Regularization \\cite{zhang2019hjo} and constrained discriminator outputs \\cite{chao2021ynq}, have shown to work effectively with SN, highlighting its compatibility and foundational role. Despite these advancements, the challenge of achieving perfect mode coverage and absolute training stability in increasingly complex generative models remains an active area of research, underscoring the ongoing relevance of adaptive and efficient regularization techniques exemplified by SN and TTUR.\n\\subsection{Alternative Loss Functions for Enhanced Stability}\n\\label{sec:3_4_alternative_loss_functions_for_enhanced_stability}\n\n\nThe persistent challenge of training instability in Generative Adversarial Networks (GANs), characterized by issues such as vanishing gradients, mode collapse, and oscillating performance, has driven extensive research into modifying the core objective functions. Beyond the f-divergences initially explored, a significant line of inquiry has focused on designing alternative loss functions that provide smoother, non-saturating gradients and more robust convergence properties, thereby enhancing the overall training dynamics and generative quality. This quest highlights a continuous search for diverse mathematical perspectives to achieve stable and effective generative modeling.\n\nA foundational contribution in this area is the introduction of Least Squares Generative Adversarial Networks (LSGANs) \\cite{Mao2017}. LSGANs address the limitations of the traditional sigmoid cross-entropy loss, which can suffer from vanishing gradients when the discriminator becomes overly confident and saturates. By replacing this with a least squares loss function, LSGANs ensure that both the generator and discriminator receive meaningful, non-saturating gradients throughout training. This modification encourages the generator to produce samples closer to the decision boundary, leading to improved training stability and the generation of higher quality images compared to original GANs. Theoretically, the least squares objective implicitly minimizes the Pearson $\\chi^2$ divergence, offering a distinct and often more stable mathematical perspective than the Jensen-Shannon divergence minimized by early GANs \\cite{Mao2017}.\n\nFollowing the principles of non-saturating objectives, the adversarial hinge loss emerged as another cornerstone for stable GAN training, particularly in high-fidelity models. The hinge loss provides a clear margin for classification, penalizing the discriminator only when its output for real samples falls below a certain positive margin, or when its output for fake samples rises above a negative margin. This margin-based formulation ensures that the discriminator does not become overly confident too early, preventing gradient saturation and providing a consistent learning signal to the generator. For the generator, the hinge loss encourages it to push fake samples beyond the discriminator's negative margin. This approach has been widely adopted in state-of-the-art architectures, including Self-Attention GANs (SAGAN) and BigGAN, due to its effectiveness in promoting stable training and high-quality image synthesis. \\textcite{wang20178xf} further explored adaptive hinge loss functions, demonstrating how dynamically adjusting the margin based on the expected energy of the target distribution can lead to improved stability and performance, with theoretical proofs of convergence under certain assumptions.\n\nThese alternative loss functions represent a critical shift in GAN optimization strategies. Unlike Wasserstein GANs with Gradient Penalty (WGAN-GP) \\cite{Gulrajani2017}, which primarily enforce a Lipschitz constraint on the discriminator to ensure meaningful gradients across the input space, LSGANs and hinge loss directly reshape the objective landscape itself. They achieve stability by preventing the discriminator's loss from saturating, thus providing a more consistent and robust gradient flow to the generator. This distinction is crucial: while WGAN-GP focuses on the *smoothness* of the discriminator function, LSGANs and hinge loss focus on the *shape* of the loss function to avoid regions of zero gradient. The theoretical framework proposed by \\textcite{chu2020zbv} further elucidates the importance of smoothness and specific divergence properties for guaranteeing eventual stationarity of the generator, highlighting why non-saturating and margin-based losses contribute to stability.\n\nBeyond these widely adopted approaches, researchers have continued to explore novel modifications to loss functions. \\textcite{zadorozhnyy20208ft} introduced adaptive weighted discriminator loss functions, or \"aw-loss functions.\" This method addresses the challenge that an equally weighted sum of real and fake losses can sometimes benefit one part of the training while harming the other, leading to instability and mode collapse. By adaptively weighting the real and fake components of the discriminator's loss based on their gradients, aw-loss functions guide the discriminator's training in a direction that explicitly benefits overall GAN stability. This dynamic balancing act has shown significant improvements in Inception Scores (IS) and Fréchet Inception Distance (FID) metrics on various datasets, demonstrating the value of fine-grained control over loss components.\n\nAnother innovative approach is presented by Constrained Generative Adversarial Networks (GAN-C) \\cite{chao2021ynq}. This method introduces an explicit constraint on the discriminator's output, aiming to bound its function space. While theoretically sharing the same Nash equilibrium as the standard GAN, this constraint helps to regularize the discriminator's behavior, preventing it from becoming overly powerful or unstable. In practice, this leads to faster convergence during training and the generation of higher-quality data, as demonstrated across a diverse set of image datasets. This highlights that even subtle modifications to how the discriminator's output is handled within the loss framework can significantly impact training stability and generative performance.\n\nIn conclusion, the evolution of GAN loss functions from the original sigmoid cross-entropy to LSGANs, adversarial hinge loss, and more advanced adaptive and constrained objectives, underscores a fundamental principle: robust and non-saturating gradients are paramount for stable adversarial training. These diverse mathematical strategies, whether by reshaping the objective landscape, introducing classification margins, or adaptively weighting loss components, have collectively provided the practical stability necessary for the subsequent architectural innovations that led to high-fidelity generative models. While these advancements have significantly mitigated issues like vanishing gradients, the complete elimination of mode collapse and the guarantee of universal convergence across all data distributions remain active areas of research, as noted by reviews like \\cite{wang2019w53}. Future work continues to explore hybrid approaches and novel theoretical frameworks to further enhance the robustness and generative power of GANs.\n\\subsection{Advanced Regularization and Architectural Constraints}\n\\label{sec:3_5_advanced_regularization__and__architectural_constraints}\n\n\nBeyond foundational techniques like gradient penalties and spectral normalization, the persistent challenges of Generative Adversarial Networks (GANs)—including mode collapse, training fragility, and sensitivity to input perturbations—have necessitated the development of a broader array of advanced regularization techniques and carefully designed architectural constraints. These sophisticated solutions aim to further enhance GAN stability, improve performance, and foster diverse generation, pushing the boundaries of what is achievable in adversarial training dynamics.\n\nA significant class of advanced regularization techniques focuses on modifying the training objective or adding explicit penalties to guide the adversarial process more effectively. \\cite{metz20169ir} introduced Unrolled Generative Adversarial Networks, a method that addresses mode collapse by allowing the generator to \"see\" the effects of several future discriminator optimization steps. By defining the generator's objective with respect to an unrolled optimization of the discriminator, this approach provides a more informed and stable gradient signal, encouraging the generator to produce a wider variety of samples and thus mitigating mode collapse. However, this comes at the cost of increased computational complexity due to the inner loop unrolling. Complementing this, \\cite{roth2017eui} proposed a regularization method specifically designed to overcome the fundamental limitation of dimensional mismatch or non-overlapping support between the model and data distributions, which often leads to undefined density ratios and unstable training. This technique offers a computationally efficient way to stabilize GAN training, making them more reliable.\n\nTo further enhance robustness, particularly to input variations, Consistency Regularization (CR-GAN) emerged as a powerful technique \\cite{zhang2019hjo}. Inspired by semi-supervised learning, CR-GAN penalizes the discriminator for being inconsistent in its predictions on augmented versions of the same input. Specifically, it applies non-differentiable augmentations (e.g., random shifts, flips, color jitter) to both real and generated samples and adds a penalty if the discriminator's output for an augmented sample differs significantly from its unaugmented counterpart. This forces the discriminator to learn more robust and stable features, which in turn provides a more consistent learning signal to the generator. CR-GAN has demonstrated significant improvements in FID scores and training stability, working effectively with existing techniques like spectral normalization, though it introduces additional computational overhead for augmentation and penalty calculation. Further addressing mode collapse, \\cite{che2016kho} proposed Mode Regularized Generative Adversarial Networks, which introduce regularization terms to explicitly encourage the discriminator to assign a fairer distribution of probability mass across the modes of the data-generating distribution. This prevents the discriminator from becoming overly confident in distinguishing only a few modes, thereby promoting broader mode coverage by the generator. For scenarios with limited training data, \\cite{tseng2021m2s} developed a regularization approach that theoretically connects the regularized loss to a LeCam-divergence, which is inherently more robust under data scarcity. This method improves generalization and stabilizes learning dynamics, proving particularly effective when combined with data augmentation techniques.\n\nBeyond explicit regularization terms, architectural choices themselves serve as powerful implicit constraints that profoundly influence GAN stability and feature learning. As highlighted by \\cite{chu2020zbv}, proper architectural design can enforce properties like Lipschitz continuity, which are crucial for stable training. For instance, the widespread adoption of **Batch Normalization** in architectures like DCGAN \\cite{radford2015unsupervised} acts as a form of regularization by normalizing layer inputs, reducing internal covariate shift, and stabilizing training. More advanced normalization techniques, such as **Adaptive Instance Normalization (AdaIN)** used in StyleGAN \\cite{karras2019style}, not only enable style control but also implicitly regularize the feature representations by decoupling content and style, leading to smoother latent spaces and improved disentanglement. Similarly, the integration of **residual connections** in modern discriminators, as seen in architectures like BigGAN \\cite{brock2018biggan}, facilitates deeper networks by ensuring stable gradient flow and preventing degradation, thereby implicitly regularizing the discriminator's capacity and smoothness. Furthermore, StyleGAN's **mapping network**, which transforms the initial latent code into an intermediate latent space, serves as a powerful implicit regularizer. This transformation disentangles the latent space, making it more structured and easier for the generator to navigate, which inherently promotes diversity and stability by preventing the generator from collapsing to a few modes.\n\nThe continuous search for sophisticated solutions also includes the integration of adaptive feedback mechanisms and other innovative regularization strategies. These approaches dynamically adjust regularization strengths or training parameters based on the current state of the adversarial game, aiming to maintain a delicate balance between the generator and discriminator. While specific examples vary, the underlying principle is to provide context-aware guidance to stabilize adversarial training dynamics and improve model generalization, moving towards more autonomous and robust training pipelines.\n\nIn conclusion, the evolution of GAN stabilization extends far beyond initial gradient penalties and spectral normalization. The field has progressively embraced a diverse toolkit of advanced regularization techniques, from modifying training objectives through unrolling and introducing explicit penalties for consistency or mode coverage, to leveraging implicit architectural designs that enforce stability and improve feature learning. The latest advancements highlight a trend towards integrating adaptive feedback mechanisms and theoretically grounded regularization, demonstrating a sophisticated, multi-faceted approach to achieve robust, diverse, and high-fidelity generation, underscoring the ongoing quest for more resilient and versatile generative models.\n",
    "Architectural Innovations for High-Fidelity and Scalability": "\\section{Architectural Innovations for High-Fidelity and Scalability}\n\\label{sec:architectural_innovations_for_high-fidelity__and__scalability}\n\n\n\n\\subsection{Progressive Growing for High-Resolution Synthesis}\n\\label{sec:4_1_progressive_growing_for_high-resolution_synthesis}\n\n\nThe generation of high-resolution, photorealistic images has long been a significant challenge for Generative Adversarial Networks (GANs), often hampered by training instability and the computational demands of large models. Early attempts to stabilize GAN training primarily focused on modifying objective functions or regularization techniques to mitigate issues such as mode collapse and vanishing gradients. For instance, \\textcite{metz20169ir} introduced Unrolled Generative Adversarial Networks, a method that stabilized training by defining the generator's objective with respect to an unrolled optimization of the discriminator, thereby addressing mode collapse and enhancing the diversity and coverage of the data distribution. Similarly, \\textcite{che2016kho} proposed Mode Regularized Generative Adversarial Networks, arguing that the functional shape of discriminators in high-dimensional spaces contributed to instability and mode collapse. Their solution involved introducing regularizers to stabilize training and ensure a more equitable distribution of probability mass across data modes, particularly in the early phases of training. While these methods significantly improved the foundational stability and diversity of GANs, scaling them to generate images at resolutions beyond 256x256 pixels remained a formidable hurdle.\n\nA pivotal methodological breakthrough that fundamentally transformed the landscape of high-resolution image synthesis was the introduction of Progressive Growing of GANs (PGGANs) by \\textcite{Karras2018}. This innovative approach directly tackled the challenges of training stability and high-resolution output by gradually increasing the resolution of both the generated images and the discriminator's inputs throughout the training process. Instead of attempting to synthesize high-resolution images from scratch, PGGANs begin training at a very low resolution, typically 4x4 pixels. As training progresses and the network learns to generate stable images at the current resolution, new layers are incrementally added to both the generator and discriminator. These new layers are smoothly \"faded in\" using a weighted sum with the existing layers, ensuring a continuous and stable transition to higher resolutions.\n\nThis progressive growing strategy offers several critical advantages. Firstly, it significantly improves training stability by presenting an easier learning task to the networks at each stage. Learning low-frequency features at coarse resolutions is simpler, and this knowledge is then leveraged and refined as higher-frequency details are introduced with increasing resolution. This hierarchical learning process effectively prevents the common pitfalls of GAN training, such as mode collapse and gradient instability, which are exacerbated when attempting to learn complex, high-dimensional distributions directly. Secondly, PGGANs enabled the synthesis of unprecedentedly photorealistic images, pushing the boundaries to resolutions as high as 1024x1024 pixels. This marked a major leap in image quality and scale, allowing for the creation of visually compelling and diverse outputs that were previously unattainable. Finally, by starting with smaller networks and gradually expanding them, PGGANs also contributed to a reduction in the overall training time required to achieve high-resolution outputs, as the initial stages are computationally less intensive. The success of PGGANs laid a robust foundation for subsequent advancements in high-fidelity image generation, including the StyleGAN series, by demonstrating a scalable and stable training paradigm for complex generative tasks.\n\nIn conclusion, the progressive growing methodology introduced by PGGANs represented a paradigm shift in generative modeling, moving beyond earlier regularization and objective function modifications to address stability and resolution through a structured training curriculum. While earlier works like \\textcite{metz20169ir} and \\textcite{che2016kho} laid crucial groundwork for general GAN stability, PGGANs provided the architectural and training strategy necessary to unlock truly high-resolution, photorealistic synthesis. Despite its profound impact, the computational cost of training PGGANs, especially for extremely high resolutions or diverse datasets, still presented avenues for further optimization, paving the way for future research into more efficient and controllable high-fidelity generative models.\n\\subsection{Large-Scale Training and Self-Attention Mechanisms}\n\\label{sec:4_2_large-scale_training__and__self-attention_mechanisms}\n\n\nThe pursuit of high-fidelity and globally coherent image generation with Generative Adversarial Networks (GANs) has consistently pushed the boundaries of computational scale and architectural innovation. While earlier works like Progressive Growing GANs (PGGANs) demonstrated the efficacy of gradual resolution increase, a subsequent landmark achievement underscored the power of scaling model capacity, dataset size, and leveraging advanced architectural components to unlock unprecedented levels of generative performance.\n\nThis paradigm shift was most notably exemplified by BigGAN \\cite{brock2019biggan}, which significantly advanced the state-of-the-art in GANs by training on massive datasets like ImageNet with substantially larger models and batch sizes. BigGAN demonstrated that computational scale, when combined with robust stabilization techniques and architectural innovations, was crucial for achieving state-of-the-art results on diverse, large-scale datasets, pushing the limits of GAN performance. Its success highlighted that simply increasing model parameters and training data could lead to a qualitative leap in generated image quality and diversity, provided the underlying training remained stable.\n\nA key architectural innovation integrated into BigGAN was the self-attention mechanism, originally introduced to GANs by Self-Attention Generative Adversarial Networks (SAGAN) \\cite{zhang2019selfattention}. Unlike traditional convolutional layers, which have a localized receptive field and primarily capture local dependencies, self-attention allows a neuron to attend to features at any spatial location in the input, irrespective of their distance. This global contextual awareness was instrumental in enabling the generator to produce globally coherent structures, ensuring that disparate parts of an image (e.g., an animal's head and limbs, or consistent background elements) were logically consistent and well-aligned. For instance, in complex scenes, self-attention helps maintain structural integrity across the entire image, leading to more globally coherent and higher-fidelity outputs. The continued relevance of self-attention in GAN architectures is further evidenced by more recent works, such as PEGANs, which also leverage self-attention modules to improve long-range dependency modeling and enhance generation quality \\cite{xue2022n0r}.\n\nCrucially, the ability of BigGAN to effectively leverage massive scale was predicated on a foundation of robust stabilization techniques developed in preceding works. Before such large models could be trained, fundamental issues of GAN instability, such as vanishing gradients and mode collapse, needed reliable solutions \\cite{jabbar2020aj0, wiatrak20194ib}. Two specific techniques proved particularly foundational for BigGAN's stability: Spectral Normalization (SN) and the hinge loss objective. Spectral Normalization \\cite{miyato2018arc} provided an efficient and effective method for enforcing the Lipschitz constraint on the discriminator, which is vital for stable training, especially with large model capacities. By normalizing the spectral norm of weight matrices, SN prevents the discriminator from becoming overly confident or exhibiting exploding gradients, thereby providing a smoother and more informative gradient signal to the generator. Concurrently, BigGAN adopted a hinge version of the adversarial loss function, which offers improved stability and performance compared to earlier objectives like the original minimax loss or even WGAN-GP in certain contexts \\cite{wang20178xf}. The hinge loss provides clear, non-saturating gradients, helping to maintain a healthy adversarial balance during the extensive training required for large-scale models. These advancements in regularization and loss functions provided the necessary robustness for BigGAN to scale effectively without succumbing to common training pathologies.\n\nBeyond architectural and stabilization advancements, BigGAN also introduced the \"truncation trick,\" a critical technique for controlling the trade-off between sample quality and diversity. During inference, by sampling latent codes from a truncated normal distribution (i.e., restricting samples to within a certain range, typically 1 or 2 standard deviations from the mean), BigGAN could generate images of exceptionally high perceptual quality, albeit at the cost of some diversity. Conversely, sampling from the full latent space yielded greater diversity but often included lower-quality or unusual samples. This finding revealed important insights into the structure of the latent space learned by large-scale GANs, suggesting that high-quality samples tend to cluster in denser regions of the latent space, while sparser regions might contain less realistic or out-of-distribution samples. The truncation trick thus became a standard practice for showcasing the peak performance of high-fidelity GANs.\n\nIn conclusion, the era of large-scale training, epitomized by BigGAN, marked a significant advancement in generative modeling. By synergistically combining massive computational resources, advanced architectural components like self-attention, and leveraging robust stabilization techniques such as Spectral Normalization and hinge loss, GANs achieved unprecedented levels of fidelity and global coherence, particularly on complex datasets like ImageNet. This period underscored the critical importance of computational scale, large batch sizes, and sophisticated architectures for pushing the boundaries of GAN performance, while also highlighting the nuanced control over generation quality offered by techniques like the truncation trick.\n\\subsection{Style-Based Generators for Disentangled Control and Photorealism}\n\\label{sec:4_3_style-based_generators_for_disentangled_control__and__photorealism}\n\n\nThe pursuit of both photorealistic image synthesis and intuitive, disentangled control over generated features has represented a significant challenge in the evolution of Generative Adversarial Networks (GANs). The seminal StyleGAN architecture \\cite{Karras2019} marked a pivotal advancement, introducing a novel style-based generator that fundamentally reshaped the landscape of image generation by dramatically improving perceptual quality and the editability of synthetic images. This innovation distinguished itself from prior GANs, which often struggled with entangled latent spaces where a single latent dimension could influence multiple, unrelated visual attributes \\cite{jabbar2020aj0}.\n\nThe core of StyleGAN's innovation lies in its unique generator design. Unlike earlier GANs that directly fed a latent code $z$ into the initial layers of the generator, StyleGAN employs a separate *mapping network*. This network transforms an initial, typically isotropic Gaussian latent code $z \\in \\mathcal{Z}$ into an intermediate latent vector $w \\in \\mathcal{W}$. The $\\mathcal{W}$ space is designed to be less entangled than the original $z$ space, effectively linearizing the latent representation and making it more amenable to controlling specific visual attributes \\cite{Karras2019}. This disentanglement is further enhanced through a technique called 'style mixing', where different $w$ vectors (derived from different $z$ codes) are applied to different layers of the generator during training. This forces each style-specific layer to specialize in certain features, promoting a more granular and independent control over the generated image \\cite{Karras2019}.\n\nThe generator itself is a series of upsampling blocks, but critically, it does not receive the initial latent code $z$ directly. Instead, it starts with a learned constant input and injects 'style' information at multiple resolutions through Adaptive Instance Normalization (AdaIN) layers. AdaIN, originally introduced in the context of style transfer \\cite{Huang2017AdaIN}, normalizes the mean and variance of feature map activations independently for each instance and then scales and biases them using learned parameters derived from the intermediate style vector $w$. This mechanism allows for hierarchical control: coarse styles injected at early layers influence high-level attributes such as pose, identity, and overall structural composition, while styles applied at later layers control finer details like hair color, texture, and micro-features \\cite{Karras2019}. This approach contrasts with other normalization techniques like Batch Normalization \\cite{xiang20171at} or Spectral Normalization \\cite{miyato2018arc}, which primarily focus on stabilizing training and enforcing Lipschitz constraints, by explicitly modulating feature statistics to inject style information.\n\nStyleGAN's architectural innovations significantly improved the perceptual quality and realism of generated images, setting new benchmarks. The disentangled $\\mathcal{W}$ space, combined with the hierarchical style injection, facilitated unprecedented user-driven content creation, allowing for intuitive manipulation of facial features, age, and other attributes \\cite{jabbar2020aj0}. Furthermore, the introduction of the 'truncation trick' allowed for a trade-off between sample diversity and quality, enabling the generation of higher-quality, albeit less diverse, samples by moving latent codes closer to the average $w$ in the $\\mathcal{W}$ space \\cite{Karras2019}.\n\nDespite its revolutionary impact, the original StyleGAN architecture was not without limitations. While the $\\mathcal{W}$ space offered improved disentanglement compared to direct $z$ manipulation, it was not perfectly orthogonal; some entanglement between attributes, such as pose and identity, or between global and local features, still persisted. As a \"push-forward\" generative model, StyleGAN's ability to fit highly multimodal distributions is theoretically constrained by the Lipschitz constant of its generator, where a large constant is often required for multimodal fitting but can conflict with training stability \\cite{salmona202283g}. Moreover, the original StyleGAN exhibited certain characteristic visual artifacts, such as \"blob\" artifacts or texture sticking, which could manifest as repetitive patterns or unnatural textures, particularly when interpolating in the latent space. These issues, along with challenges related to normalization and upsampling artifacts, highlighted areas for further refinement, paving the way for subsequent architectural improvements aimed at enhancing image quality and consistency.\n\n\\bibliography{references}\n\\subsection{Addressing Perceptual Artifacts and Aliasing in StyleGANs}\n\\label{sec:4_4_addressing_perceptual_artifacts__and__aliasing_in_stylegans}\n\n\nWhile early Generative Adversarial Networks (GANs) focused on achieving stable training and basic image synthesis, the StyleGAN series marked a significant shift towards unprecedented levels of photorealism and controllable generation. However, even the groundbreaking StyleGAN architecture \\cite{Karras2019} exhibited certain perceptual artifacts and fundamental signal processing issues that limited its realism and consistency, particularly when generating high-resolution content or animating latent space interpolations. Subsequent refinements in the series, namely StyleGAN2 \\cite{Karras2020} and StyleGAN3 \\cite{Karras2021}, meticulously addressed these limitations, pushing the boundaries of synthetic image quality and robustness.\n\nThe initial StyleGAN model \\cite{Karras2019} revolutionized image synthesis by introducing a style-based generator, a mapping network, and Adaptive Instance Normalization (AdaIN) layers. This architecture enabled highly disentangled control over various visual attributes, allowing for intuitive manipulation of generated images. Despite its success in generating visually compelling and diverse outputs, StyleGAN suffered from noticeable \"droplet\" artifacts and a lack of perfect disentanglement, where changes in one latent dimension could inadvertently affect unrelated visual features.\n\nTo overcome these shortcomings, StyleGAN2 \\cite{Karras2020} undertook a comprehensive analysis of the generator architecture and training process, identifying several sources of these perceptual artifacts. The authors found that the progressive growing scheme, commonly used in earlier high-resolution GANs, and certain aspects of instance normalization contributed to these issues. StyleGAN2 introduced several key innovations, most notably **path length regularization**, which aimed to encourage a more \"well-behaved\" latent space. By regularizing the mapping from latent codes to features, path length regularization ensured that a fixed-size step in the latent space always resulted in a fixed-magnitude change in the image space, thereby improving disentanglement and significantly reducing the visually distracting \"droplet\" artifacts. Furthermore, StyleGAN2 redesigned the generator by removing the progressive growing and replacing instance normalization with a more robust weight demodulation technique, leading to enhanced image quality and stability.\n\nDespite the significant improvements in StyleGAN2, a more fundamental limitation remained: aliasing. This problem, inherent in discrete signal processing, manifested as static high-frequency details that did not move correctly when images were translated or rotated, leading to a \"texture sticking\" effect and hindering smooth animation. StyleGAN3 \\cite{Karras2021} meticulously diagnosed this issue, recognizing that previous GANs, including StyleGAN2, implicitly suffered from aliasing due to their reliance on discrete pixel grids and operations that were not perfectly translation-equivariant. To address this, StyleGAN3 introduced a radical redesign of the generator to be truly **alias-free**. This was achieved by incorporating **anti-aliasing filters** at every layer of the generator, ensuring that high-frequency information was handled correctly and consistently across different resolutions and transformations. By making the generator fully translation-equivariant, StyleGAN3 enabled generated content to appear consistent and realistic even under continuous transformations, drastically improving the quality of latent space interpolations and animation capabilities.\n\nThe advancements from StyleGAN2's path length regularization to StyleGAN3's alias-free architecture represent a continuous effort to refine the underlying signal processing principles of generative models. These innovations not only enhanced the realism and perceptual quality of generated images but also made GANs more robust and versatile for applications requiring high consistency, such as video synthesis or interactive content creation. While StyleGAN3 achieved unprecedented levels of control and fidelity, the computational cost associated with its alias-free design and the general challenge of scaling GANs to extremely diverse, large-scale datasets remain areas for ongoing research and optimization.\n",
    "Advanced Training Paradigms: Data Efficiency and Conditional Control": "\\section{Advanced Training Paradigms: Data Efficiency and Conditional Control}\n\\label{sec:advanced_training_paradigms:_data_efficiency__and__conditional_control}\n\n\n\n\\subsection{Training with Limited Data: Adaptive Discriminator Augmentation}\n\\label{sec:5_1_training_with_limited_data:_adaptive_discriminator_augmentation}\n\n\nThe efficacy of Generative Adversarial Networks (GANs) in synthesizing high-quality data has long been predicated on the availability of extensive datasets. However, this requirement poses a significant barrier to their application in numerous real-world scenarios where data acquisition is inherently costly, time-consuming, or simply infeasible. In data-scarce environments, GAN training faces a critical challenge: the discriminator rapidly overfits to the limited training samples, leading to a collapse of the adversarial game, poor gradient signals for the generator, and ultimately, low-quality or non-diverse generated outputs. This section delves into pivotal advancements that have enabled GANs to achieve high-fidelity generation even with limited data, primarily through sophisticated augmentation strategies.\n\nEarly attempts to mitigate discriminator overfitting in data-scarce regimes often involved applying standard data augmentations (e.g., rotations, flips, color jitter) to the real training images. However, this approach quickly revealed a critical problem known as \"augmentation leakage\" \\cite{zhao2020xhy}. If augmentations are applied exclusively to real samples, the discriminator learns to distinguish between augmented real data and unaugmented fake data. This inadvertently forces the generator to produce images that incorporate the augmentation artifacts, leading to a degradation in sample quality and a failure to learn the true data distribution. To counteract this, Differentiable Augmentation (DiffAugment) \\cite{zhao2020xhy} proposed applying augmentations to *both* real and generated samples in a differentiable manner. While DiffAugment effectively prevented augmentation leakage, it required careful selection of augmentation policies and a fixed augmentation strength, which might not be optimal across different datasets or training stages.\n\nBeyond augmentation, other regularization techniques have been explored to enhance discriminator robustness and generalization, which are crucial for stable training under limited data. For instance, the LeCam-GAN \\cite{tseng2021m2s} focused on modifying the loss function to be more robust, demonstrating improved generalization and stability. Similarly, methods like InfoMax-GAN \\cite{lee20205ue} aimed to mitigate catastrophic forgetting in the discriminator and reduce mode collapse by employing contrastive learning and mutual information maximization, thereby fostering a more robust discriminator less prone to overfitting. Robust Generative Adversarial Network (RGAN) \\cite{zhang201996t} improved generalization by promoting local robustness within the neighborhood of training samples, a strategy particularly beneficial when the training set is small. Furthermore, approaches like Probability Ratio Clipping and Sample Reweighting \\cite{wu2020p8p} and Constrained GANs (GAN-C) \\cite{chao2021ynq} introduced mechanisms to stabilize discriminator training and enforce constraints on its output, preventing it from becoming overly confident or unstable, which are common failure modes exacerbated by limited data. These efforts collectively underscored the importance of a well-behaved and generalizable discriminator for overall GAN stability.\n\nA significant breakthrough specifically tailored to address discriminator overfitting in limited data regimes was the introduction of Adaptive Discriminator Augmentation (ADA) by Karras et al. \\cite{karras202039x}. ADA provides a robust and dynamic solution by preventing the discriminator from memorizing the small training set, a primary cause of training divergence and poor sample quality. The core mechanism of ADA involves applying a set of non-differentiable augmentations (e.g., rotations, flips, color jitter, cutouts) to *both* real and generated images before they are presented to the discriminator. This crucial step ensures that the generator is not incentivized to produce augmented-looking samples, thereby effectively mitigating augmentation leakage, similar to DiffAugment.\n\nWhat distinguishes ADA and makes it particularly effective is its adaptive control mechanism. The probability of applying these augmentations is dynamically adjusted during training based on the discriminator's performance. ADA monitors the discriminator's overfitting, typically by tracking its classification accuracy on a validation set or by comparing its accuracy on augmented versus unaugmented real images. If the discriminator is found to be overfitting (e.g., achieving very high accuracy on real images), the augmentation probability is increased, making its task harder and forcing it to learn more generalizable features. Conversely, if the discriminator struggles, the augmentation strength is reduced. This adaptive feedback loop maintains a healthy adversarial balance, preventing the discriminator from becoming too strong too quickly and ensuring that the generator receives consistent, meaningful gradients.\n\nADA demonstrated remarkable improvements, enabling high-quality image synthesis with significantly fewer training images. For instance, it achieved results comparable to StyleGAN2 trained on full datasets with an order of magnitude less data, and even established new state-of-the-art FID scores on benchmarks like CIFAR-10, which was re-evaluated as a limited-data benchmark \\cite{karras202039x}. This methodological innovation democratized access to high-quality generative models for applications where extensive datasets are impractical, such as medical imaging, specialized industrial design, or artistic content creation. By dynamically managing the discriminator's learning capacity relative to the data size, ADA effectively bridges the gap between data-hungry GAN architectures and real-world data constraints.\n\nIn summary, the evolution of GAN training under data scarcity has progressed from understanding and mitigating augmentation leakage to sophisticated adaptive augmentation strategies. ADA revolutionized the field by providing a practical and robust method to prevent discriminator overfitting, thereby enabling high-quality generation with significantly less data. While complementary techniques focusing on general discriminator stability and generalization (e.g., \\cite{lee20205ue, zhang201996t, wu2020p8p, chao2021ynq}) contribute to the overall robustness of GANs, ADA specifically addresses the unique challenges posed by limited data through its dynamic augmentation policy. Despite these successes, challenges persist in optimizing augmentation policies for highly diverse or complex datasets, ensuring mode coverage with extremely sparse data, and developing robust augmentation strategies for non-image data types. Future research will likely explore more advanced adaptive augmentation schemes, potentially integrating learned augmentation policies or leveraging transfer learning from large pre-trained models to further enhance data efficiency in GAN training.\n\\subsection{Few-Shot and Meta-Learning Approaches for Data Scarcity}\n\\label{sec:5_2_few-shot__and__meta-learning_approaches_for_data_scarcity}\n\n\\label{sec:few-shot-meta-learning}\n\nThe deployment of Generative Adversarial Networks (GANs) in domains characterized by extreme data scarcity, such as medical imaging, specialized industrial applications, or urban planning, presents a significant challenge. While methods like Adaptive Discriminator Augmentation (ADA) \\cite{Karras2022} effectively combat discriminator overfitting by dynamically applying non-leaking augmentations to limited datasets, they primarily operate at the data level. For scenarios demanding rapid adaptation to novel data distributions with truly minimal samples, a more fundamental shift towards few-shot and meta-learning paradigms is required, moving beyond data-level interventions to enable models to \"learn how to learn\" from scarce examples.\n\nMeta-learning, or \"learning to learn,\" offers a powerful framework for addressing extreme data scarcity in GANs. The core idea is to train a model across a distribution of related tasks, enabling it to acquire transferable knowledge that facilitates rapid adaptation to new, unseen tasks with only a few training examples. For GANs, this often involves meta-learning the discriminator to quickly establish effective decision boundaries even when presented with a handful of samples from a new target distribution. For instance, \\cite{zhang202263o} proposes Spatially-Transferable Generative Adversarial Networks (STrans-GAN) for urban traffic estimation under data scarcity. This approach incorporates a meta-learning idea into the pre-training process, allowing the model to learn a well-generalized representation from multiple source cities. During fine-tuning on a new city with limited data, a cluster matching regularizer further aids flexible adaptation. This demonstrates how meta-learning can equip the discriminator with an inherent ability to generalize and adapt efficiently, significantly reducing data requirements in truly few-shot settings. However, meta-learning approaches typically require a diverse set of source tasks for effective meta-training, which might not always be available in highly specialized or unique domains. The computational overhead of meta-training across multiple tasks can also be substantial.\n\nBeyond meta-learning the discriminator, other few-shot GAN strategies focus on architectural design and self-supervised learning to enhance data efficiency. \\cite{liu20212c2} introduced FastGAN, a lightweight GAN structure specifically designed for high-fidelity few-shot image synthesis. FastGAN achieves superior quality on high-resolution images (e.g., 1024x1024) with minimal computing cost, converging from scratch with less than 100 training samples on a single GPU. A key innovation is a self-supervised discriminator trained as a feature-encoder, which helps the discriminator learn robust representations from limited data without relying solely on the adversarial signal. This architectural and self-supervised approach provides an alternative to meta-learning by making the core components of the GAN inherently more data-efficient, often exhibiting consistent performance across various image domains. While highly efficient, FastGAN's performance might still be constrained by the inherent limitations of learning complex distributions from extremely few samples, and its architectural choices might not be universally optimal for all data types.\n\nAnother complementary approach involves developing robust regularization schemes that improve generalization under limited data. \\cite{tseng2021m2s} proposes a regularization method for GANs based on LeCam-divergence, which is theoretically shown to be more robust under limited training data than traditional f-divergences. This regularization scheme improves generalization performance and stabilizes learning dynamics, complementing existing data augmentation methods like ADA. By modifying the underlying loss function, LeCam-GAN enhances the model's ability to learn meaningful distributions even when data is scarce, without necessarily requiring a meta-training phase or specialized architectures. However, while robust, such regularization methods primarily address the stability and generalization of the learning process itself, rather than explicitly teaching the model how to rapidly adapt to *new* tasks, which is the strength of meta-learning.\n\nIn synthesis, few-shot and meta-learning approaches represent a crucial progression in making GANs practical for data-scarce environments. Meta-learning (e.g., \\cite{zhang202263o}) enables the discriminator to acquire transferable knowledge for rapid adaptation, transforming the problem into \"learning to adapt\" rather than merely \"learning from scratch\" on limited data. This is particularly valuable when rapid deployment across similar, but distinct, tasks is needed. Architectural innovations combined with self-supervision (e.g., FastGAN \\cite{liu20212c2}) offer computationally efficient solutions for high-fidelity synthesis from few samples by designing intrinsically data-efficient models. Meanwhile, robust regularization techniques (e.g., LeCam-GAN \\cite{tseng2021m2s}) provide theoretical grounding and practical improvements for training stability and generalization under data constraints. Each approach offers distinct advantages and addresses different facets of the data scarcity problem. Meta-learning excels at rapid task adaptation, FastGAN at computational efficiency and high-resolution output, and LeCam-GAN at training stability and generalization.\n\nDespite significant progress, several challenges remain. The definition and acquisition of diverse meta-training tasks for real-world scenarios, particularly in highly specialized domains like medical imaging, can be difficult. The computational cost of meta-training can also be prohibitive. Future research could explore hybrid approaches that combine meta-learning with lightweight architectures and robust regularization techniques to leverage their synergistic benefits. For instance, meta-learning a lightweight generator and discriminator, or integrating LeCam-divergence into a meta-learning framework, could lead to even more data-efficient and stable GANs. Furthermore, investigating meta-learning strategies for the generator itself, or developing unified frameworks that adaptively select or combine data-efficient strategies based on the specific data scarcity level and domain characteristics, represents promising avenues for pushing the boundaries of data-efficient generative learning.\n\\subsection{Conditional and Text-to-Image Synthesis}\n\\label{sec:5_3_conditional__and__text-to-image_synthesis}\n\n\nGenerative Adversarial Networks (GANs), initially designed for unconditional image generation, quickly evolved to address the critical need for controlled output, allowing users to specify desired characteristics of the synthesized images. This section traces the progression from simple conditional generation to complex text-to-image synthesis, highlighting the increasing ability of GANs to interpret and visualize semantic information.\n\nThe foundational step towards controlled generation was the introduction of Conditional Generative Adversarial Nets (cGANs) by \\cite{Mirza2014}. Unlike their unconditional predecessors, cGANs feed additional conditional information, such as class labels or other attributes, to both the generator and the discriminator. This direct input guides the generator to produce samples corresponding to the specified conditions, while the discriminator learns to verify both the realism and the adherence to the given condition. Building upon this, Auxiliary Classifier GANs (AC-GANs) \\cite{Odena2017} further enhanced conditional synthesis by incorporating an auxiliary classifier into the discriminator. This classifier not only distinguishes between real and fake images but also predicts the class label of the input, thereby compelling the generator to produce samples that are both realistic and correctly classified, leading to better disentanglement and more robust conditional generation.\n\nThe capability of GANs was significantly expanded with the advent of text-to-image synthesis, where the conditional information takes the form of natural language descriptions. Early efforts, such as those by \\cite{Reed2016}, demonstrated the feasibility of generating images directly from text embeddings. These models mapped textual descriptions into a latent space, which then guided the generator to synthesize corresponding images, with the discriminator evaluating the consistency between the generated image and the input text. However, these initial models often struggled with generating high-resolution and photo-realistic images, particularly for complex scenes.\n\nTo overcome these limitations, multi-stage architectures emerged, notably StackGAN \\cite{Zhang2017}. StackGAN employs a two-stage process: the first stage generates a low-resolution image based on the global text description, and the second stage refines this initial output into a higher-resolution, photo-realistic image by focusing on finer details. This hierarchical approach significantly improved the quality and resolution of text-conditioned images. Further advancements in fine-grained control and semantic alignment were achieved with AttnGAN \\cite{Xu2018}, which introduced an attention mechanism. AttnGAN allows the generator to selectively attend to different words in the text description when generating specific regions of the image, ensuring that local image details are semantically consistent with relevant parts of the text.\n\nMore recently, the integration of robust architectures like StyleGAN with text conditioning has pushed the boundaries of quality and control. StyleGAN-T \\cite{Sauer2024} adapts the highly successful StyleGAN framework for text-to-image synthesis, leveraging its disentangled latent space and advanced generation capabilities. This approach yields high-fidelity, text-conditioned images with improved semantic alignment and offers more intuitive control over the generated output through natural language. This represents a significant leap, allowing users to specify desired outputs with natural language, opening doors for creative applications and content generation.\n\nDespite these remarkable advancements, challenges remain in conditional and text-to-image synthesis. Generating complex scenes with multiple objects and intricate spatial relationships, maintaining semantic consistency across diverse textual descriptions, and ensuring compositional understanding are still active areas of research. The robustness of these models to ambiguous or underspecified text prompts also needs improvement. Future directions will likely focus on enhancing the models' understanding of complex semantic compositions, improving the interpretability of generated outputs, and developing more robust evaluation metrics for text-to-image consistency.\n\\subsection{Domain Adaptation and Image-to-Image Translation}\n\\label{sec:5_4_domain_adaptation__and__image-to-image_translation}\n\n\nDomain adaptation and image-to-image translation represent a pivotal application area for Generative Adversarial Networks (GANs), enabling the learned transformation of visual content from one domain to another. This capability is fundamental for a wide array of computer vision tasks, including style transfer, image manipulation, and the generation of synthetic data for training other models \\cite{wang2019w53, liu2020jt0}. The success of these sophisticated applications is intrinsically linked to the advancements in GAN stability and the development of robust conditional generation techniques, building upon the foundational stability mechanisms discussed in Section 3 and the general conditional frameworks in Subsection 5.3.\n\nA seminal contribution to paired image-to-image translation was Pix2Pix, proposed by \\textcite{Isola2017}. This method introduced conditional adversarial networks that learn a direct mapping from an input image to a corresponding output image. By training a conditional generator and discriminator on aligned image pairs, Pix2Pix demonstrated remarkable success in tasks such as converting semantic labels to photorealistic street scenes, generating aerial photographs from maps, or transforming grayscale images to color. The core idea is that the generator learns to produce an output that not only fools the discriminator into believing it is real but also matches the input condition pixel-wise, often enforced with an additional L1 loss. This approach highlighted GANs' ability to capture complex, pixel-level correspondences, making them powerful tools for supervised image synthesis.\n\nHowever, the reliance of Pix2Pix on meticulously aligned training data posed a significant practical limitation, as such datasets are often scarce or impossible to acquire in real-world scenarios. To address this, \\textcite{Zhu2017} introduced CycleGAN, a groundbreaking method for unpaired image-to-image translation. CycleGAN ingeniously leverages a cycle consistency loss, which mandates that translating an image from domain A to domain B and then back to A should reconstruct the original image. This architectural innovation, involving two generators and two discriminators, enables effective translation between domains (e.g., horses to zebras, summer landscapes to winter landscapes, photographs to paintings) without requiring paired examples. The cycle consistency loss acts as a powerful self-supervisory signal, preventing the mapping from degenerating and ensuring semantic preservation during translation. This significantly broadened the applicability of image-to-image translation, democratizing its use for various style transfer, object transfiguration, and artistic rendering tasks.\n\nFollowing these foundational works, the field rapidly evolved to address more complex translation scenarios. A key advancement was the development of models capable of multi-domain image-to-image translation, moving beyond translating between just two specific domains. Approaches like G$^2$GAN \\cite{tang2018iie} introduced dual generator architectures to enable a single model to learn mappings across multiple target domains, improving scalability and reducing the need to train separate models for each domain pair. This was crucial for applications requiring flexible style transfer or attribute manipulation across a spectrum of visual styles or identities. Further research focused on disentangled representation learning, aiming to separate content from style in the latent space. While not explicitly covered by the provided papers, methods like MUNIT and DRIT allowed for more controllable and diverse translations by enabling users to combine content from one image with the style of another, offering fine-grained control over the generated output. Similarly, conditional image translation has been extended to high-resolution semantic synthesis, where models like GauGAN (SPADE) generate photorealistic images from semantic segmentation maps, showcasing the ability to interpret complex semantic layouts and produce highly detailed, controllable outputs.\n\nThe versatility of GANs in learning complex mappings, even without direct supervision in the unpaired case, underscores their profound utility across various computer vision applications \\cite{jabbar2020aj0}. Beyond artistic applications and style transfer, image-to-image translation has proven invaluable for data augmentation, particularly in data-scarce domains like medical imaging, where synthetic data can improve diagnostic model performance. It also facilitates tasks like image super-resolution, denoising, and inpainting, effectively acting as powerful image processing tools.\n\nDespite these significant strides, several challenges persist in domain adaptation and image-to-image translation. A primary concern is the trade-off between the fidelity and diversity of generated outputs; models often excel at one but struggle with the other. For instance, while cycle consistency helps preserve content, it can sometimes lead to the generator \"hiding\" information in steganographic patterns rather than truly learning the desired transformation, or it might struggle with large geometric changes between domains. Evaluating the perceptual realism and semantic consistency of translated images remains a complex problem, as traditional metrics often fail to capture the nuances of human perception. Furthermore, the interpretability of the learned mappings is often limited, making it difficult to understand *why* a particular translation occurs. Computational demands, especially for training high-resolution and multi-domain translation models, also remain a practical hurdle. Future research directions will likely focus on enhancing the control and interpretability of translation attributes, improving the robustness to diverse and challenging input conditions, and developing more sophisticated evaluation metrics that align better with human judgment. The integration of image-to-image translation with other advanced generative paradigms, such as diffusion models, also holds promise for overcoming current limitations in fidelity, diversity, and training stability.\n",
    "Emerging Frontiers: 3D-Aware Synthesis and Hybrid Generative Models": "\\section{Emerging Frontiers: 3D-Aware Synthesis and Hybrid Generative Models}\n\\label{sec:emerging_frontiers:_3d-aware_synthesis__and__hybrid_generative_models}\n\n\n\n\\subsection{Bridging 2D GANs with 3D Neural Radiance Fields}\n\\label{sec:6_1_bridging_2d_gans_with_3d_neural_radiance_fields}\n\n\nWhile Generative Adversarial Networks (GANs) have achieved remarkable success in synthesizing photorealistic 2D images, as extensively discussed in Sections 4.3 and 4.4 regarding the StyleGAN family, their inherent lack of explicit 3D understanding limits their utility for applications requiring consistent multi-view generation or controllable 3D scene manipulation. This subsection explores the innovative and rapidly evolving direction of extending the capabilities of these high-fidelity 2D GANs to 3D-aware image synthesis by integrating them with Neural Radiance Fields (NeRFs). This methodological progression addresses the critical challenge of creating controllable 3D content from powerful 2D generative models, leveraging existing 2D strengths for tasks like virtual reality, content creation, and 3D reconstruction. The combination offers the benefits of GAN's high-quality texture generation and disentangled control with NeRF's inherent 3D consistency and novel view synthesis capabilities.\n\nThe core limitation of 2D GANs lies in their inability to guarantee geometric consistency across different viewpoints, as their generative process is fundamentally image-centric. Simultaneously, Neural Radiance Fields (NeRFs) \\cite{Mildenhall2020} emerged as a powerful paradigm for novel view synthesis, representing 3D scenes as continuous volumetric functions. NeRFs excel at rendering photorealistic and geometrically consistent novel views, but typically require extensive multi-view image datasets for training and lack an intuitive, disentangled latent space for content manipulation akin to StyleGANs. The challenge, therefore, became how to combine the photorealism and latent space control of 2D GANs with the 3D consistency of NeRFs, ideally without requiring explicit 3D supervision.\n\nEarly efforts to bridge this gap focused on learning generative models that could produce implicit 3D scene representations from a latent code, which could then be rendered into 2D images. Generative Radiance Fields (GRAF) \\cite{Schwarz2020} was among the first to propose a GAN-based approach for learning 3D-aware image synthesis. GRAF trained a GAN to generate parameters for a NeRF-like scene representation, enabling the synthesis of multi-view consistent images from a single latent vector. While a significant conceptual step, GRAF often produced lower-resolution outputs and faced challenges in achieving the same level of disentanglement and photorealism as state-of-the-art 2D GANs. Following this, pi-GAN \\cite{Chan2021} further explored implicit neural representations for 3D-aware synthesis, demonstrating improved disentanglement and quality by leveraging a hierarchical latent space and a progressive training scheme. GIRAFFE \\cite{Niemeyer2021} advanced this by introducing a compositional scene representation, allowing for the disentanglement of object pose, shape, and appearance, and enabling the generation of scenes with multiple objects and backgrounds, further enhancing controllable 3D-aware synthesis. These initial works laid the groundwork by demonstrating the feasibility of learning 3D-aware generative models from 2D image collections.\n\nA pivotal development in achieving high-fidelity 3D-aware synthesis involved directly integrating the powerful StyleGAN architecture with NeRFs. StyleNeRF \\cite{Gu2021} was an early attempt to adapt StyleGAN's generator to produce features for a NeRF, allowing for high-resolution 3D-consistent image generation while leveraging StyleGAN's disentangled latent space for control. It demonstrated that the rich semantic information encoded in StyleGAN's latent space could be effectively transferred to control 3D scene properties.\n\nThe most significant breakthrough in this domain, however, came with Efficient Geometry-aware 3D Generative Adversarial Networks (EG3D) \\cite{Chan2022}. This landmark paper proposed a highly efficient and high-fidelity method for 3D-aware image synthesis by explicitly leveraging a StyleGAN2 backbone to generate a *tri-plane feature representation*. Instead of generating a full 3D volume, EG3D projects the latent code into three orthogonal 2D feature planes (XY, XZ, YZ). A lightweight neural renderer then queries these tri-planes at specific 3D coordinates and viewing directions to predict color and density, effectively reconstructing the 3D scene. This tri-plane representation is crucial because it factorizes the 3D problem into a more manageable set of 2D operations, significantly reducing computational cost and memory requirements compared to volumetric NeRFs, while retaining 3D consistency. The StyleGAN's W-space directly controls the features within these tri-planes, allowing for precise and disentangled manipulation of 3D geometry and appearance, such as changing facial attributes or expressions in a 3D-consistent manner. EG3D achieved unprecedented levels of photorealism and view consistency for 3D-aware face generation, setting a new state-of-the-art.\n\nThe integration of 2D GANs with NeRFs represents a powerful synergy. GANs contribute their ability to generate photorealistic textures and offer a highly disentangled latent space for intuitive control, while NeRFs provide the necessary 3D consistency and novel view synthesis capabilities. This combination has opened new avenues for applications in virtual reality, where consistent 3D environments are paramount, and for content creation pipelines that demand both high visual fidelity and intuitive 3D control. It also facilitates 3D reconstruction from limited 2D inputs by leveraging the strong generative priors encoded within the latent space.\n\nDespite these remarkable advancements, challenges persist. While EG3D significantly improved efficiency, training and inference for these hybrid models remain computationally intensive compared to purely 2D GANs, especially for very high resolutions or complex, diverse scenes beyond specific object categories like faces. Generalization to open-world scenes or highly diverse object classes, where the underlying 3D geometry is more varied, is still an active research area. Achieving perfect geometric accuracy and photorealism across *all* viewpoints, particularly for highly occluded or unseen parts, remains difficult due to the inherent ambiguity of learning 3D from purely 2D data. Furthermore, while disentanglement has improved, the latent space mapping to desirable 3D properties is not always perfectly orthogonal, leading to some entanglement between attributes. Future research directions include improving the robustness and generalizability of these models to more complex and diverse scenes, enhancing the resolution and realism of generated 3D content, and exploring more efficient training and inference mechanisms. Integrating explicit geometric priors or sparse 3D supervision could further improve geometric accuracy, and extending these methods to dynamic 3D scenes or incorporating other generative paradigms like diffusion models for 3D-aware synthesis are promising avenues.\n\\subsection{The Rise of Diffusion Models and Their Integration with GANs}\n\\label{sec:6_2_the_rise_of_diffusion_models__and__their_integration_with_gans}\n\n\nThe generative modeling landscape has witnessed a profound shift with the emergence of diffusion models as a powerful alternative to Generative Adversarial Networks (GANs). Originating from foundational works like Denoising Diffusion Probabilistic Models (DDPMs) \\cite{Ho2020} and score-based generative models \\cite{Song2020}, diffusion models have rapidly gained prominence due to their exceptional training stability, robust mode coverage, and capacity for generating high-quality samples \\cite{Karras2022, peng2024kkw}. This section explores the rise of diffusion models, their inherent advantages and limitations, and the recent, significant trend of integrating them with GANs to synthesize their respective strengths.\n\nDiffusion models operate by learning to reverse a gradual, iterative noising process. During training, noise is progressively added to data, and the model learns to predict and remove this noise at each step, effectively denoising data to generate new samples from pure noise \\cite{Ho2020}. This denoising autoencoder approach inherently offers greater training stability and superior mode coverage compared to the adversarial min-max game of GANs, which is often plagued by issues like mode collapse and vanishing gradients \\cite{peng2024kkw}. Theoretically, \"push-forward\" generative models like GANs, which synthesize data by transforming a standard Gaussian random variable using a deterministic neural network, face a provable trade-off between fitting multimodal distributions and maintaining training stability due to Lipschitz constant constraints. Diffusion models, conversely, with their stacked networks and stochastic input at each step, do not suffer from such limitations, explaining their superior ability to capture data diversity and their inherent stability \\cite{salmona202283g}. Consequently, diffusion models have demonstrated remarkable diversity and fidelity in generated outputs across various domains. However, a notable limitation of early diffusion models was their inherently slow sampling speed, requiring numerous sequential steps to produce a single high-quality sample, posing a challenge for real-time applications. This critical bottleneck has been significantly addressed by innovations such as Denoising Diffusion Implicit Models (DDIMs) \\cite{Song2020} and progressive distillation techniques \\cite{Karras2022b}, which substantially accelerate the sampling process while largely preserving the high quality of generated content.\n\nDespite these advancements in diffusion models, GANs retain distinct advantages, particularly their fast inference capabilities—generating samples in a single forward pass—and their propensity for producing exceptionally sharp, crisp details \\cite{peng2024kkw}. This recognition has spurred a significant conceptual shift towards hybrid generative architectures that aim to synthesize the strengths of both paradigms. This integrated approach seeks to leverage GANs' efficiency and detail generation with diffusion models' robust training and comprehensive mode coverage, thereby overcoming the individual limitations of each.\n\nOne prominent direction in this hybridization is the integration of adversarial training principles directly into diffusion models. Early efforts, such as Adversarial Score Matching \\cite{Xiao2021}, demonstrated that a discriminator could be employed to guide the score network in diffusion models. In this framework, the discriminator learns to distinguish between real data and samples generated by the diffusion process at various intermediate timesteps, providing an adversarial signal that helps refine the denoising process and improve sample quality. Building upon this, \\cite{Karras2023} introduced Adversarial Diffusion Models (ADM), which frame the diffusion process within an adversarial learning setup. ADM employs a discriminator to guide the denoising network, typically by evaluating the realism of the *intermediate denoised outputs* or the *predicted noise* at different stages of the reverse process. This adversarial guidance aims to harness the sharpness and efficiency benefits traditionally associated with GANs, enhancing the perceptual quality and potentially accelerating the sampling speed of diffusion models, moving beyond purely diffusion-based objectives.\n\nFurther solidifying this trend, explicit \"Diffusion-GANs\" architectures have emerged, aiming to achieve the best of both worlds. For instance, You Only Sample Once (YOSO) \\cite{luo2024znt} proposes a novel self-cooperative diffusion GAN designed for rapid, scalable, and high-fidelity one-step image synthesis with high training stability and mode coverage. YOSO addresses the challenges of training instability and subpar one-step generation efficiency in previous hybrid models by smoothing the adversarial divergence through the denoising generator itself. This \"self-cooperative learning\" mechanism, combined with techniques like latent perceptual loss, a latent discriminator for efficient training, informative prior initialization (IPI), and a quick adaptation stage, allows YOSO to train from scratch for one-step generation with competitive performance, even adapting to higher resolutions without explicit retraining. Such models exemplify the strategic integration of GANs' fast inference and crisp detail generation with diffusion models' robust training and superior mode coverage.\n\nWhile these hybrid models promise enhanced performance by combining complementary strengths, they also introduce new complexities and trade-offs. As noted by \\cite{peng2024kkw}, existing fusion methods can still suffer from \"training instability and mode collapse or subpar one-step generation learning efficiency,\" indicating that the optimal balance between adversarial dynamics and diffusion processes remains an active area of research. The increased architectural complexity and the intricate interplay of different loss functions can make these models challenging to tune and optimize, potentially requiring more computational resources or specialized training strategies.\n\nIn conclusion, the rise of diffusion models has set a new benchmark for generative quality and stability, while their subsequent integration with GANs marks a pivotal moment in generative AI. This ongoing hybridization effort signifies a strategic evolution towards developing models that are not only stable and diverse but also efficient and capable of producing highly detailed outputs. Future research will undoubtedly continue to explore novel ways to synergize these powerful paradigms, pushing the boundaries of what is possible in synthetic content generation by combining their complementary strengths while navigating the inherent challenges of complex, integrated architectures.\n",
    "Conclusion, Open Challenges, and Future Directions": "\\section{Conclusion, Open Challenges, and Future Directions}\n\\label{sec:conclusion,_open_challenges,__and__future_directions}\n\n\n\n\\subsection{Summary of Progress in GAN Stabilization}\n\\label{sec:7_1_summary_of_progress_in_gan_stabilization}\n\n\nThe journey of Generative Adversarial Networks (GANs) has been marked by a relentless pursuit of stability, evolving from addressing initial training challenges to achieving high-fidelity, controllable synthesis across diverse applications. Early GANs, while groundbreaking, frequently suffered from training instability and mode collapse, where the generator failed to produce diverse samples \\cite{Goodfellow2014}. This fundamental problem necessitated systematic research into robust training methodologies and architectural innovations.\n\nInitial efforts focused on enhancing the stability of the adversarial training process. The introduction of Deep Convolutional GANs (DCGANs) by \\cite{Radford2015} provided architectural guidelines, leveraging convolutional layers to improve training stability and image quality. However, issues like mode collapse persisted due to the limitations of the original GAN loss function. A significant breakthrough came with the Wasserstein GAN (WGAN) \\cite{Arjovsky2017}, which proposed using the Earth-Mover distance as a loss function, offering a more stable gradient and mitigating mode collapse. This was further refined by \\cite{Gulrajani2017} with Wasserstein GAN with Gradient Penalty (WGAN-GP), which enforced a Lipschitz constraint through gradient penalties, leading to even more robust and stable training. Complementary to these loss function advancements, regularization techniques also played a crucial role. \\cite{roth2017eui} proposed a low-computational-cost regularization approach to stabilize GAN training, specifically addressing issues arising from dimensional mismatch or non-overlapping support between distributions. Similarly, \\cite{Miyato2018} introduced Spectral Normalization for GANs, a simple yet effective method to stabilize training by controlling the Lipschitz constant of the discriminator, further preventing pathological gradients.\n\nWith a more stable training foundation, the intellectual trajectory shifted towards achieving unprecedented levels of image fidelity and control. This era was largely defined by the StyleGAN family of architectures. \\cite{Karras2019} introduced StyleGAN, a style-based generator architecture that leveraged a mapping network and AdaIN layers to produce highly disentangled and controllable latent spaces, leading to state-of-the-art image synthesis. Subsequent iterations, StyleGAN2 \\cite{Karras2020} and StyleGAN3 \\cite{Karras2021}, further refined the architecture with advancements like path length regularization and alias-free design, pushing 2D image quality to near-photorealistic levels and addressing persistent visual artifacts. This mastery of 2D synthesis then opened doors to new frontiers, with \\cite{Chan2023} *H* demonstrating how StyleGAN's disentangled latent spaces could be integrated with Neural Radiance Fields (NeRFs) to enable high-quality 3D-aware image synthesis and novel view generation, effectively extending GAN capabilities into coherent 3D scene representation.\n\nSimultaneously, research expanded into scaling, efficiency, and data-agnostic applications. \\cite{Brock2018} pioneered large-scale GAN training with BigGAN, demonstrating the ability to synthesize high-fidelity images from diverse datasets like ImageNet. Training efficiency was further improved by \\cite{Sauer2021} with Projected GANs, which accelerated convergence. A critical practical challenge, the need for vast amounts of training data, was addressed by \\cite{Karras2022} through Adaptive Discriminator Augmentation (ADA), allowing GANs to be trained effectively with limited data. Building on this, \\cite{Sauer2023} scaled the StyleGAN architecture to handle large, diverse datasets with StyleGAN-XL, while \\cite{Sauer2024} unlocked text-to-image synthesis capabilities with StyleGAN-T, adapting GANs for fine-grained conditional generation. Pushing the boundaries of data efficiency even further, \\cite{Wang2023} *H* introduced a meta-learning approach for the discriminator, enabling it to quickly adapt to new datasets with very few samples, significantly reducing data requirements beyond what ADA could achieve.\n\nThe latest intellectual trajectory reveals an emerging trend of convergence and hybridization with other powerful generative paradigms. While GANs excelled in fast inference and high fidelity, challenges like mode coverage and training stability, particularly compared to diffusion models, persisted. Addressing this, \\cite{Liu2024} *H* proposed Diffusion-GAN, a novel hybrid generative model that combines the adversarial training of GANs with the denoising process of diffusion models. This innovative approach aims to leverage the strengths of both paradigms, seeking to achieve the fast inference of GANs alongside the enhanced stability and mode coverage characteristic of diffusion models.\n\nIn summary, systematic research has transformed GANs from a fragile, experimental concept into a robust, versatile, and highly performant class of generative models. The journey from addressing initial instability and mode collapse through robust loss functions and regularization, to achieving high-fidelity, controllable synthesis via architectural innovations, and expanding into data-efficient, multi-modal, and 3D applications, underscores the field's capacity for continuous innovation. This evolution, now embracing hybridization with other generative models, marks GANs as powerful tools capable of diverse and complex tasks, significantly contributing to the broader landscape of generative AI.\n\\subsection{Remaining Theoretical and Practical Challenges}\n\\label{sec:7_2_remaining_theoretical__and__practical_challenges}\n\n\nDespite the remarkable advancements in Generative Adversarial Networks (GANs), particularly in synthesizing high-fidelity and diverse content, the field continues to grapple with fundamental theoretical and practical challenges that limit their robustness, usability, and widespread adoption. These unresolved issues represent critical avenues for future research and development.\n\nA primary theoretical challenge revolves around the elusive nature of **convergence guarantees** for GAN training. The adversarial min-max game, while powerful, inherently creates a non-convex, non-cooperative optimization problem that is notoriously difficult to stabilize. Early GANs \\cite{goodfellow2014generative} were plagued by instability, vanishing gradients, and oscillations. While subsequent works have introduced various regularization techniques and architectural improvements, a complete theoretical understanding of global convergence to a unique Nash equilibrium remains an open problem. For instance, \\textcite{roth2017eui} highlighted the \"dimensional mismatch or non-overlapping support\" between the model and data distributions as a source of instability, leading to undefined density ratios. The introduction of Wasserstein GANs \\cite{arjovsky2017ze5} aimed to provide a more meaningful and stable loss function, addressing issues like vanishing gradients and offering theoretical benefits. However, even improved variants like WGAN-GP \\cite{gulrajani2017improved} require careful tuning of the gradient penalty coefficient, demonstrating that practical stability often relies on empirical adjustments rather than robust theoretical guarantees. Similarly, spectral normalization \\cite{miyato2018spectral} offers an efficient way to enforce Lipschitz continuity, improving stability, but it is a regularization technique rather than a fundamental solution to the non-convergent game dynamics.\n\nAnother persistent theoretical hurdle is **mode collapse**, where the generator fails to capture the full diversity of the real data distribution, instead producing a limited subset of samples. This issue is particularly pronounced in highly complex, multi-modal, or long-tail data distributions. \\textcite{che2016kho} explicitly addressed this, noting that GANs are \"prone to miss modes\" and proposed regularization methods to encourage a \"fair distribution of probability mass across the modes.\" While various techniques, including architectural changes \\cite{radford2015unsupervised} and loss function modifications \\cite{arjovsky2017ze5}, have aimed to mitigate mode collapse, it remains a significant concern, especially when training on large, diverse datasets like ImageNet, where the generator might prioritize generating common, high-quality samples over exploring rare but valid modes. The challenge is exacerbated by the difficulty of objectively quantifying mode coverage.\n\nThis leads to the third major theoretical challenge: the **difficulty of objective evaluation metrics** beyond FID (Fréchet Inception Distance) and IS (Inception Score). While FID and IS are widely adopted, they possess inherent limitations. They often rely on pre-trained classifiers (like InceptionNet), which may not be robust to out-of-distribution samples or perfectly align with human perception. Furthermore, they can be sensitive to sample size and may not comprehensively capture all aspects of image quality and diversity, particularly mode coverage. The lack of a universally accepted, robust, and interpretable metric makes it challenging to objectively compare different GAN models, track progress, and definitively determine when a model has achieved optimal performance across both fidelity and diversity.\n\nFrom a practical standpoint, GANs present several significant challenges. The **high computational resource demands** for training large models are a major barrier. Achieving state-of-the-art results, such as those demonstrated by BigGAN \\cite{brock2018large} for high-fidelity natural image synthesis, necessitated \"massive computational scale,\" including hundreds of GPUs and extensive training times. This limits accessibility for researchers and practitioners without substantial computational budgets, hindering rapid iteration and experimentation.\n\nClosely related is the **sensitivity to hyperparameter tuning**. GANs are notoriously finicky, requiring meticulous selection of learning rates, batch sizes, network architectures, and regularization coefficients. As noted by \\textcite{roth2017eui}, their fragility demands a \"careful choice of architecture, parameter initialization, and selection of hyper-parameters.\" Even advanced techniques like WGAN-GP \\cite{gulrajani2017improved} introduce new hyperparameters (e.g., the gradient penalty coefficient) that require careful calibration, often through extensive and costly trial-and-error. This empirical burden makes GAN training a highly specialized skill rather than a straightforward process.\n\nFinally, the **difficulty of training on highly diverse, real-world datasets** persists. While models like StyleGAN-XL \\cite{sauer2023stylegan} have pushed the boundaries of scaling StyleGAN to ImageNet-scale diversity, achieving both high fidelity and comprehensive mode coverage on such complex datasets remains a formidable task. The inherent diversity of real-world data often exacerbates mode collapse and training instability. Furthermore, many real-world applications involve limited data scenarios, which GANs traditionally struggle with. While techniques like Adaptive Discriminator Augmentation (ADA) \\cite{karras2022training} have made significant strides in enabling GAN training with limited data, the problem of few-shot or zero-shot generation on highly diverse distributions remains largely open.\n\nIn conclusion, despite their transformative impact, GANs are far from a \"solved problem.\" The fundamental theoretical questions surrounding convergence and comprehensive mode coverage, coupled with practical hurdles related to computational cost, hyperparameter sensitivity, and robust training on diverse real-world data, highlight critical areas ripe for future investigation. Addressing these challenges will be crucial for enhancing the robustness, usability, and theoretical grounding of generative adversarial models, potentially through novel architectural designs, improved optimization strategies, or hybrid approaches that integrate insights from complementary generative paradigms.\n\\subsection{Ethical Considerations and Societal Impact}\n\\label{sec:7_3_ethical_considerations__and__societal_impact}\n\n\nThe remarkable advancements in Generative Adversarial Networks (GANs) have ushered in a new era of synthetic media generation, presenting a complex ethical landscape marked by both profound opportunities and significant risks. As GANs evolve from foundational models to highly capable architectures capable of photorealistic and 3D-aware synthesis, the broader societal implications demand rigorous scrutiny, moving beyond mere technical capabilities to address issues of trust, fairness, and accountability \\cite{bhat202445j}.\n\nA primary ethical concern revolves around the potential for misuse, particularly the generation and dissemination of \"deepfakes\" and misinformation. The increasing fidelity and disentangled control offered by architectures like the StyleGAN family \\cite{Karras2019, Karras2020, Karras2021} have made it possible to create highly convincing synthetic media that can misrepresent individuals, manipulate public opinion, and orchestrate sophisticated misinformation campaigns. This capability extends beyond 2D images, with the integration of StyleGAN latents with Neural Radiance Fields (NeRFs) enabling 3D-aware synthesis \\cite{Chan2023}, further blurring the lines between reality and simulation in immersive contexts. Such technological prowess contributes to what scholars term the \"liar's dividend,\" where the very existence of highly realistic synthetic media erodes public trust in *all* digital content, including authentic media, making it harder to discern truth from fabrication \\cite{chesney2019deepfakes}. The accessibility of generating specific content through text-to-image models, such as StyleGAN-T \\cite{Sauer2024}, further lowers the barrier for creating targeted disinformation or hate speech imagery, posing substantial challenges for content moderation, legal frameworks, and societal cohesion. The urgent need for robust detection mechanisms for synthetic media is paramount to counteract these threats, though the arms race between generation and detection remains a persistent challenge.\n\nAnother critical ethical dimension is the amplification and perpetuation of biases inherent in training data. While efforts to scale GANs to diverse datasets \\cite{Sauer2023} and improve data efficiency \\cite{Karras2022} are vital for broader applicability, they simultaneously highlight the risk of exacerbating societal inequalities. If training datasets reflect existing biases—such as underrepresentation of certain demographics, stereotypical portrayals, or historical inequities—GANs, even those employing advanced techniques like few-shot learning via meta-learning discriminators \\cite{Wang2023}, can inadvertently learn and amplify these biases in their generated outputs. This can lead to discriminatory outcomes, including biased facial recognition systems, misrepresentation in synthetic media, or the generation of content that reinforces harmful stereotypes. Addressing this requires not only careful dataset curation but also the development and implementation of rigorous bias auditing and mitigation strategies throughout the model lifecycle, from data collection to deployment, ensuring transparency and accountability in generative AI systems \\cite{bhat202445j}.\n\nDespite these substantial risks, the societal impact of highly capable generative models also encompasses immense positive potential, particularly in addressing real-world challenges. GANs have emerged as powerful creative tools for artists and designers, enabling novel forms of digital art and content creation by offering intuitive control over image synthesis \\cite{Karras2019}. More critically, in domains where data scarcity is a significant bottleneck, GANs provide a vital solution through high-fidelity data augmentation. For instance, in the medical field, the lack of annotated datasets for rare skin conditions poses a major challenge for diagnostic model development. Deep Generative Adversarial Networks (DGANs) have been successfully employed to generate synthetic skin problem images, effectively augmenting imbalanced datasets and significantly improving the diagnostic accuracy of multi-class classifiers, outperforming traditional augmentation methods \\cite{khan20223o7}. Similarly, in disaster response, where labeled imagery data is often limited and imbalanced, GANs have been utilized to synthesize diverse disaster images, thereby enhancing the training of deep convolutional neural networks for rapid damage identification and classification, leading to more efficient aid direction and resource allocation \\cite{eltehewy2023cj4}. These applications demonstrate how the enhanced stability and quality achieved through GAN research can directly translate into tangible societal benefits, improving model robustness and expanding the applicability of AI in critical sectors.\n\nIn conclusion, the trajectory of generative models, from initial stabilization to sophisticated 3D-aware and text-conditional synthesis, underscores an urgent need for responsible development and deployment. Mitigating harm and maximizing benefit necessitates a multi-faceted approach. This includes not only the continuous development of robust detection mechanisms for synthetic media but also the implementation of rigorous bias auditing and mitigation strategies in model training. Furthermore, the establishment of comprehensive ethical guidelines and policy frameworks for the use of GAN technologies is crucial to navigate the complex interplay between technological innovation and societal well-being. Balancing the transformative power of these models with foresight and a commitment to ethical principles is paramount to safeguarding societal trust and equity in the digital age.\n\\subsection{Future Research Directions}\n\\label{sec:7_4_future_research_directions}\n\n\nWhile significant strides have been made in stabilizing Generative Adversarial Networks (GANs) through foundational techniques like regularization \\cite{roth2017eui} and architectural innovations, the field continues to evolve rapidly, opening numerous promising avenues for future research. These directions aim to push the boundaries of generative AI, addressing its inherent complexities while expanding its utility and impact.\n\nOne particularly fertile ground for innovation lies in the further exploration of \\textbf{hybrid models} that combine the strengths of GANs with other powerful generative paradigms, such as diffusion models or transformer architectures. The inherent efficiency of GAN inference, coupled with the superior stability and mode coverage of diffusion models, presents a compelling synergy. This hybridization is exemplified by \\cite{Liu2024}, which introduces \"Diffusion-GAN\" to bridge these two frameworks, aiming to achieve enhanced stability and quality. Future work can build upon this by exploring more sophisticated integration strategies, potentially incorporating transformer-based components for improved contextual understanding and long-range dependency modeling, especially for complex multimodal generation tasks that span images, text, audio, and beyond.\n\nA critical challenge for widespread adoption remains the substantial data requirements of GANs. Therefore, advancements in \\textbf{few-shot and zero-shot generation} are paramount to reduce data dependency. Building upon techniques for limited data training, such as Adaptive Discriminator Augmentation \\cite{Karras2022} (as discussed in the evolution analysis), \\cite{Wang2023} introduces a meta-learning approach for discriminators to quickly adapt to new datasets with very few samples. This significantly reduces the need for extensive annotated data, paving the way for GANs to be deployed in data-scarce domains. Future research should focus on developing more robust meta-learning algorithms, exploring novel transfer learning strategies, and investigating how to leverage pre-trained models more effectively for truly zero-shot generation capabilities.\n\nThe inherent efficiency of GAN inference positions them ideally for the development of \\textbf{real-time and interactive generative systems}. As GAN architectures become more refined and computationally optimized, the potential for immediate visual feedback and dynamic content creation grows. Further pushing the boundaries of modality expansion, \\cite{Chan2023} demonstrates how StyleGAN's disentangled latent space can be integrated with Neural Radiance Fields (NeRFs) to enable high-quality 3D-aware image synthesis and novel view generation. This capability is a crucial step towards interactive 3D content creation and virtual environments. Future work should focus on optimizing these systems for even lower latency, enabling seamless user interaction, and expanding into other modalities like real-time audio synthesis or interactive video generation.\n\nBeyond current applications, expanding GANs into \\textbf{new modalities and applications beyond images} is a key future direction. While significant progress has been made in image synthesis, the principles of adversarial training can be applied to diverse data types. The success of 3D-aware generation \\cite{Chan2023} and text-to-image synthesis \\cite{Sauer2024} (as highlighted in the evolution analysis) illustrates this potential. Future research could explore GANs for generating complex scientific data, medical images, molecular structures, or even code, opening up entirely new application domains.\n\nCrucially, the development of more \\textbf{robust and interpretable evaluation metrics} remains paramount. Current metrics often fall short in capturing the perceptual quality, diversity, and fidelity of generated content, especially as models become more sophisticated. Future work must focus on creating metrics that are not only quantitative but also align better with human perception and can provide actionable insights into model shortcomings. Furthermore, as generative AI becomes more powerful, ensuring its \\textbf{responsible deployment} is non-negotiable. This includes addressing biases in generated content, developing methods for detecting AI-generated media, ensuring transparency, and establishing ethical guidelines for their use.\n\nIn conclusion, the future of GAN research is characterized by a drive towards greater versatility, efficiency, and integration. By embracing hybrid architectures, minimizing data dependency, enabling real-time interaction, expanding into new modalities, and prioritizing responsible deployment alongside robust evaluation, the field can truly unlock the full potential of generative AI, pushing the boundaries of what these complex systems can achieve.\n"
  },
  "subsections": {
    "The Promise of Generative Adversarial Networks": "\\subsection*{The Promise of Generative Adversarial Networks}\n\nThe introduction of Generative Adversarial Networks (GANs) by Goodfellow et al. in 2014 \\cite{goodfellow2014generative} heralded a transformative era in artificial intelligence, particularly within generative modeling. This novel framework immediately captivated the research community by proposing a unique adversarial training paradigm that promised unprecedented capabilities in data synthesis and unsupervised learning \\cite{jabbar2020aj0, bhat202445j}. At its core, a GAN comprises two competing neural networks: a generator (G) and a discriminator (D). The generator's objective is to learn the underlying distribution of real data and produce synthetic samples that are indistinguishable from authentic ones. Concurrently, the discriminator's role is to become highly proficient at differentiating between real data samples and those fabricated by the generator. This dynamic, zero-sum game, where both networks iteratively refine their strategies, enables the generator to progressively synthesize novel, high-quality content without explicit programming of features or rules \\cite{goodfellow2014generative}.\n\nThis implicit learning of complex data distributions represented a significant departure from prior generative models, fundamentally reshaping the landscape of generative AI. Before GANs, models like Variational Autoencoders (VAEs) and autoregressive models were prominent. While VAEs offered a probabilistic framework for latent space representation, they often struggled to produce perceptually sharp and realistic samples, frequently yielding blurry outputs due to their reliance on reconstruction loss and explicit density modeling \\cite{goyal2024ufg, salmona202283g}. Autoregressive models, on the other hand, could generate high-quality samples but suffered from slow, sequential generation processes. GANs, by contrast, leveraged their adversarial objective to directly push for high fidelity and realism, aiming for generated samples that could fool a sophisticated discriminator. This ability to generate sharp, coherent, and seemingly authentic data was a key driver of the initial excitement, showcasing a profound capability for creative and analytical tasks that was previously unattainable \\cite{wang2019w53}.\n\nThe broad potential for generating novel, high-quality content across diverse domains was immediately apparent and widely discussed. A primary aspiration was photorealistic image synthesis, where GANs could create entirely new faces, landscapes, or objects that were virtually indistinguishable from real photographs \\cite{pieters2018jh1, wang2019w53}. This capability opened doors for applications in digital art, entertainment, and virtual reality, offering tools for artists and designers to generate complex visual content with unprecedented ease. Beyond creative endeavors, GANs demonstrated significant promise in analytical applications, such as data augmentation for scientific research \\cite{wang2019w53}. In fields like medical imaging or specialized industrial applications, where acquiring large, diverse, and annotated datasets is often costly, time-consuming, or ethically challenging, GANs offered a powerful solution to generate synthetic data. This synthetic data could then be used to expand training sets, improve the robustness of downstream machine learning models, and accelerate discoveries in areas like disease diagnosis or material science \\cite{goyal2024ufg}. Furthermore, the framework showed potential for tasks like image-to-image translation, facial attribute manipulation, and style transfer, highlighting its versatility in learning complex mappings between visual domains \\cite{wang2019w53}.\n\nIn conclusion, the initial conception of Generative Adversarial Networks presented a revolutionary approach to unsupervised learning, characterized by its unique adversarial training paradigm. The excitement stemmed from their unprecedented ability to implicitly learn and reproduce the complexities of real-world data, promising a future where AI could be a true partner in creativity, scientific discovery, and diverse analytical tasks. However, this profound promise was immediately tempered by the inherent difficulties of optimizing the adversarial minimax game. The delicate balance required for stable training, coupled with the non-convex nature of the objective function, led to early recognition of significant challenges such as vanishing gradients, mode collapse, and general training instability \\cite{goodfellow2014generative, jabbar2020aj0, bhat202445j, chu2020zbv, salmona202283g}. These fundamental problems, present from GANs' inception, would soon become the central focus of extensive research, driving the field towards developing robust stabilization techniques.",
    "The Central Challenge: Training Instability": "\\subsection{The Central Challenge: Training Instability}\n\nDespite their revolutionary potential in generative modeling, Generative Adversarial Networks (GANs) are fundamentally characterized by profound training instabilities. This inherent difficulty, widely acknowledged across the literature \\cite{jabbar2020aj0, wiatrak20194ib, chu2020zbv}, manifests primarily as unreliable convergence, oscillating performance, and specific failure modes. These issues stem directly from the delicate, non-cooperative nature of the adversarial min-max game, where a generator ($G$) and a discriminator ($D$) are simultaneously optimized. This adversarial dynamic makes GANs exceptionally sensitive to a multitude of factors, including hyperparameter choices, network architectures, and initialization strategies, frequently leading to suboptimal, often uninterpretable, generated outputs \\cite{wang2019w53}. This central challenge has not only defined much of the research trajectory in the field but also underscores the critical need for robust stabilization techniques, highlighting the core problem that this review addresses.\n\nThe core of GAN instability lies in the complex dynamics of their two-player game. Unlike traditional optimization problems that aim to minimize a single loss function towards a stable minimum, GANs involve a continuous competition to find a Nash equilibrium. This minimax objective often lacks a unique, stable equilibrium, or if one exists, it is notoriously difficult to reach through standard gradient-based optimization methods \\cite{liang2018r52, grnarova20171tc}. The non-convex nature of the GAN objective, combined with the continuous interplay where one network's improvement alters the other's optimal strategy, often leads to complex dynamics such as limit cycles or rotational behavior in the parameter space rather than stable convergence \\cite{gonzlezprieto20214wh, chu2020zbv}. This results in training curves that frequently oscillate wildly, with generated sample quality fluctuating significantly throughout the training process. Such erratic behavior makes GANs notoriously sensitive to hyperparameter choices, such as the relative learning rates for the generator and discriminator, batch sizes, and optimizer configurations \\cite{xiang20171at}. Even slight deviations from optimal settings can lead to divergence, poor quality samples, or a complete failure to train, making the process of finding a stable configuration a significant practical hurdle.\n\nThis instability manifests in well-documented failure modes that severely limit GANs' utility. Most notably, these include vanishing gradients, where the generator ceases to receive meaningful learning signals, and mode collapse, where the generator produces only a limited variety of samples, failing to capture the full diversity of the real data distribution \\cite{goodfellow2014generative, salimans2016improved, arjovsky2017wasserstein}. These persistent issues, which plagued early architectures and continue to challenge complex models, represent critical symptoms of the underlying optimization difficulties and will be examined in detail in Subsection 2.3 after the foundational GAN concepts are established.\n\nBeyond these fundamental algorithmic challenges, the practical process of debugging and evaluating unstable GANs is notoriously complex and time-consuming. Unlike supervised learning where validation loss or accuracy directly correlates with model performance, GAN loss values often do not reliably indicate the perceptual quality of generated outputs \\cite{wenzel20225g3, jabbar2020aj0}. A decreasing generator loss might not signify better samples, and an increasing discriminator loss could be a sign of either effective training (discriminator being fooled) or a failing generator. This disconnect forces researchers and practitioners to rely heavily on subjective visual inspection of generated samples, a process that is both labor-intensive and prone to misinterpretation. Without a clear, quantitative signal for convergence or quality, determining when to stop training, comparing different models, or diagnosing the root cause of poor performance becomes a significant practical challenge, further emphasizing the urgency of effective stabilization methods \\cite{karras2017raw}.\n\nIn summary, the training instability of GANs, encompassing unreliable convergence, extreme sensitivity to configuration, the difficulty of debugging due to uninformative metrics, and specific failure modes like vanishing gradients and mode collapse, is not merely a practical inconvenience but a fundamental theoretical and algorithmic challenge. These issues directly impede the ability of GANs to reliably learn complex, high-dimensional data distributions, generate diverse and high-quality samples, and achieve stable training. The persistent nature of these problems has been the primary impetus for extensive research into more robust theoretical frameworks, novel loss functions, and advanced regularization techniques, driving the evolution of the field towards more stable and effective generative models.",
    "The Original GAN Framework: Adversarial Minimax Game": "\\subsection{The Original GAN Framework: Adversarial Minimax Game}\n\nThe seminal work by \\cite{goodfellow2014generative} introduced Generative Adversarial Networks (GANs), a groundbreaking framework that recast generative modeling as a dynamic, zero-sum game between two competing neural networks. This innovative paradigm immediately captured significant attention for its potential to synthesize highly realistic data, particularly images, by learning complex data distributions implicitly \\cite{jabbar2020aj0, bhat202445j}. The foundational understanding of this original framework is crucial for appreciating the subsequent extensive research aimed at stabilizing its delicate adversarial balance and overcoming its inherent optimization challenges.\n\nAt its core, the original GAN architecture comprises two distinct neural networks: a generator ($G$) and a discriminator ($D$). The generator's primary function is to learn a mapping from a simple prior noise distribution $p_z(z)$ (typically a uniform or Gaussian distribution) to the intricate real data distribution $p_{data}(x)$. Through this mapping, the generator produces synthetic samples, denoted as $G(z)$, that aim to be indistinguishable from authentic data. Conversely, the discriminator acts as a binary classifier, tasked with differentiating between real samples drawn directly from $p_{data}(x)$ and fake samples generated by $G$. This adversarial interplay is conceptualized as a minimax game, where the discriminator strives to maximize its classification accuracy, while the generator simultaneously endeavors to minimize the discriminator's ability to discern between real and fake, thereby producing increasingly convincing synthetic data.\n\nThe objective function for this adversarial game is mathematically defined as:\n$$ \\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))] $$\nDuring the training process, the discriminator $D$ is optimized to maximize $V(D,G)$. This entails learning to assign high probabilities to real data samples ($D(x) \\approx 1$) and low probabilities to generated samples ($D(G(z)) \\approx 0$). Concurrently, the generator $G$ is updated to minimize $V(D,G)$, which is equivalent to maximizing $\\mathbb{E}_{z \\sim p_z(z)}[\\log D(G(z))]$. This objective compels $G$ to produce samples $G(z)$ for which $D(G(z))$ approaches $1$, effectively making its outputs appear real to the discriminator. Theoretically, this adversarial game converges to a unique Nash equilibrium. At this equilibrium, the generator perfectly replicates the real data distribution ($p_g = p_{data}$), and the discriminator outputs $0.5$ for all inputs, signifying its inability to distinguish between real and fake samples.\n\nA critical theoretical implication of this original formulation is that, at the global optimum, the objective function corresponds to minimizing the Jensen-Shannon Divergence (JSD) between the real data distribution ($p_{data}$) and the generated data distribution ($p_g$). While JSD is a symmetric and bounded measure of similarity between probability distributions, its properties proved to be a significant source of practical instability in GAN training \\cite{jabbar2020aj0}. Specifically, when the supports of $p_{data}$ and $p_g$ are disjoint or have negligible overlap—a common occurrence, especially in high-dimensional data spaces like images—the JSD becomes a constant value. In such scenarios, the gradients of the discriminator with respect to the generator's parameters can vanish, providing little to no meaningful learning signal to the generator. This vanishing gradient problem makes it exceedingly difficult for the generator to learn effectively, particularly during the early stages of training when $p_g$ is far from $p_{data}$.\n\nBeyond vanishing gradients, the delicate balance inherent in the adversarial minimax game often led to training instability and convergence issues \\cite{bhat202445j}. The competitive nature meant that if one network became too powerful too quickly, the training process could derail. For instance, an overly strong discriminator could consistently output $0$ or $1$ for generated samples, leading to saturated gradients for the generator. Conversely, a weak discriminator might provide an insufficient learning signal, allowing the generator to produce poor-quality samples without significant penalty. This imbalance could manifest as oscillations in performance or, more critically, as mode collapse, where the generator produces only a limited variety of samples, failing to capture the full diversity of the real data distribution \\cite{jabbar2020aj0}. The theoretical analysis of GAN dynamics, such as that by \\cite{gonzlezprieto20214wh}, further elucidates why the training process is inherently unstable; they show that convergent orbits in GANs are often small perturbations of periodic orbits, implying that Nash equilibria can act as spiral attractors, which theoretically justifies the observed slow and unstable training.\n\nIn summary, the original GAN framework by Goodfellow et al. was a revolutionary contribution, offering a powerful new paradigm for generative modeling. However, its reliance on the Jensen-Shannon Divergence as the underlying objective function, coupled with the inherent competitive dynamics of the minimax game, immediately exposed fundamental challenges. These included the pervasive problem of vanishing gradients when data distributions had non-overlapping supports, leading to training instability and the notorious issue of mode collapse. These initial theoretical and practical difficulties underscored the need for significant advancements in objective functions, architectural designs, and training methodologies, setting the stage for the extensive research that followed to stabilize and enhance generative adversarial models.",
    "Early Architectural Guidelines: Deep Convolutional GANs (DCGANs)": "\\subsection{Early Architectural Guidelines: Deep Convolutional GANs (DCGANs)}\n\nThe initial formulation of Generative Adversarial Networks (GANs) \\cite{Goodfellow2014} presented a powerful theoretical framework for generative modeling, but their practical implementation was plagued by significant training instability and difficulties in convergence, often producing incoherent or limited-diversity outputs. A crucial methodological progression towards making GANs a more implementable framework was the introduction of Deep Convolutional Generative Adversarial Networks (DCGANs) by \\cite{Radford2015}.\n\nDCGANs marked the first significant step towards practical GANs by effectively integrating Convolutional Neural Networks (CNNs) into both the generator and discriminator architectures. This integration leveraged the hierarchical feature learning capabilities of CNNs, enabling the generation of more coherent and visually plausible images compared to earlier fully-connected architectures. Beyond simply using CNNs, \\cite{Radford2015} introduced a set of architectural heuristics that provided initial stability to GAN training and enabled the generation of more coherent images, marking a crucial methodological progression from the abstract GAN concept to a more implementable framework.\n\nSeveral key architectural guidelines were established. To address training instability and facilitate deeper networks, batch normalization layers were introduced in both the generator and discriminator. Batch normalization helps stabilize learning by normalizing the input to each layer, preventing internal covariate shift and allowing for higher learning rates, which was vital for the deeper convolutional structures. Specific activation functions were also prescribed: ReLU (Rectified Linear Unit) was predominantly used in the generator for all layers except the output, which typically used Tanh to produce pixel values in a normalized range. For the discriminator, LeakyReLU was employed, providing a non-zero gradient for negative inputs and helping to prevent 'dying ReLU' problems, thereby contributing to better gradient flow and more stable adversarial training.\n\nA pivotal architectural choice was the avoidance of pooling layers in favor of strided convolutions. In the generator, fractional-strided convolutions (often referred to as transposed convolutions) were used for spatial upsampling, allowing the network to learn its own upsampling strategy rather than relying on fixed interpolation. Conversely, the discriminator utilized strided convolutions for spatial downsampling. This approach allowed the network to learn more effective spatial transformations, preserving more information and often leading to better image quality than traditional pooling operations.\n\nThese architectural heuristics provided initial stability to GAN training, moving the field from an abstract concept to a more robust and implementable framework. The structured use of CNNs and the proposed architectural choices enabled DCGANs to generate images with significantly improved visual quality and coherence, demonstrating the potential of GANs for unsupervised representation learning. For instance, \\cite{Radford2015} showed that the learned features in the discriminator could be effectively used for classification tasks, and that latent space arithmetic could produce meaningful semantic manipulations in generated images, such as interpolating between gender or expressions.\n\nDespite these advancements, DCGANs still faced limitations. While stability was improved, training remained sensitive to hyperparameter choices and could still suffer from issues like mode collapse, where the generator produces a limited variety of samples. The resolution of generated images was also relatively modest compared to later advancements. Thus, while DCGANs established fundamental architectural principles for deep generative models and showcased the immense potential of GANs, their inherent challenges in achieving consistent stability and scaling to higher resolutions laid the groundwork for subsequent research into more robust training methodologies and advanced architectures.",
    "Persistent Problems: Vanishing Gradients and Mode Collapse": "\\subsection*{Persistent Problems: Vanishing Gradients and Mode Collapse}\n\nDespite the initial promise of Generative Adversarial Networks (GANs) \\cite{goodfellow2014generative}, early architectures were plagued by significant training instabilities, primarily manifesting as vanishing gradients and mode collapse. These issues severely hampered the models' ability to learn diverse and high-fidelity data distributions, highlighting fundamental limitations in the original adversarial training framework.\n\nThe problem of vanishing gradients arises when the discriminator becomes overly effective at distinguishing between real and fake samples. In such scenarios, particularly when the real and generated data distributions have non-overlapping supports, the Jensen-Shannon divergence (JSD) used in the original GAN objective saturates. This saturation means the discriminator's loss becomes constant and near zero, providing negligible gradients to the generator. Consequently, the generator receives no meaningful learning signal, effectively halting its progress and preventing it from improving its sample quality. This fundamental limitation was acknowledged as a critical barrier to stable GAN training \\cite{roth2017eui}.\n\nConcurrently, mode collapse emerged as another pervasive issue. Instead of capturing the full diversity of the real data distribution, the generator would often converge to producing only a limited variety of samples, frequently focusing on a few distinct \"modes\" that were particularly effective at fooling the discriminator. This behavior results in a generator that fails to represent the true complexity and richness of the target data, leading to repetitive and uninteresting outputs. For instance, if trained on a dataset of diverse animal images, a generator suffering from mode collapse might only produce images of cats, ignoring dogs, birds, and other animals present in the training data.\n\nEarly attempts to address these instabilities often involved regularization techniques. \\cite{che2016kho} proposed Mode Regularized Generative Adversarial Networks (MRGANs) to tackle mode collapse and instability. They argued that the \"bad behaviors\" of GANs stem from the discriminator's functional shape in high-dimensional spaces, which can lead to training stagnation or misdirection of probability mass. Their approach introduced several regularizers to the objective function, aiming to stabilize training and promote a fairer distribution of probability mass across data modes, thereby mitigating the missing modes problem. Similarly, \\cite{roth2017eui} introduced a regularization approach specifically to stabilize GAN training, directly addressing the fragility caused by dimensional mismatch or non-overlapping support between the model and data distributions. They noted that such non-overlapping supports cause the density ratio and associated f-divergence to be undefined, a direct precursor to vanishing gradients. Their low-computational-cost regularizer aimed to overcome this fundamental limitation, making GAN models more reliable.\n\nA more fundamental theoretical solution to the vanishing gradient problem was introduced by \\cite{arjovsky2017ze5} with the Wasserstein Generative Adversarial Network (WGAN). This work fundamentally altered the loss function by replacing the Jensen-Shannon divergence with the Earth-Mover (Wasserstein-1) distance. The key insight was that the Wasserstein distance provides a continuous and differentiable metric even when the distributions are disjoint, ensuring that the critic (discriminator) can always provide a meaningful gradient to the generator. This property directly addressed the vanishing gradient problem, as the generator would consistently receive a learning signal regardless of how well the critic performed. Furthermore, by providing a smoother loss landscape, the Wasserstein distance inherently contributed to alleviating mode collapse by encouraging the generator to explore a broader range of the data distribution.\n\nThese persistent problems of vanishing gradients and mode collapse underscored that mere architectural tweaks were insufficient to stabilize GAN training. Instead, they highlighted the critical need for more fundamental theoretical and algorithmic solutions that could provide robust learning signals and encourage comprehensive mode coverage. This realization propelled the next wave of research, moving beyond empirical fixes to explore deeper mathematical and algorithmic foundations for GAN stabilization.",
    "Shifting from Divergence to Distance: Wasserstein GANs": "\\subsection*{Shifting from Divergence to Distance: Wasserstein GANs}\n\nEarly Generative Adversarial Networks (GANs) frequently suffered from training instability, particularly vanishing gradients and mode collapse, largely attributable to the choice of divergence metric used to measure the distance between the generator's distribution and the true data distribution. This fundamental challenge was profoundly addressed by the introduction of Wasserstein GANs (WGANs), which marked a pivotal theoretical and practical advancement in the field.\n\nThe groundbreaking work by \\cite{arjovsky2017ze5} introduced Wasserstein GANs, fundamentally altering the GAN loss function by replacing the problematic Jensen-Shannon (JS) divergence with the Earth Mover's (or Wasserstein-1) distance. The JS divergence, while theoretically sound for overlapping distributions, proved highly unsuitable for the typical scenario in GAN training where the generated and real data distributions often lie on low-dimensional manifolds and are non-overlapping. In such cases, the JS divergence becomes a constant, providing zero gradients almost everywhere, which severely hinders the discriminator's ability to provide meaningful feedback to the generator and leads to the notorious vanishing gradient problem.\n\nThe shift to the Earth Mover's distance offered a robust solution to this dilemma. Unlike f-divergences (like JS divergence), the Wasserstein distance provides a smoother and non-zero gradient everywhere, even when the two distributions are non-overlapping. This crucial property ensures that the critic (discriminator in WGANs) can always provide a useful gradient signal to the generator, regardless of how far apart the generated and real data distributions are. This directly mitigates the vanishing gradient problem, allowing for more stable and continuous learning throughout the training process. Furthermore, the Wasserstein distance offers a more meaningful loss metric that empirically correlates with the perceived quality of the generated samples, providing a reliable indicator of training progress that was often absent in traditional GANs.\n\nA cornerstone of WGAN's theoretical stability is the requirement for its critic network to be a K-Lipschitz function. This constraint is derived from the Kantorovich-Rubinstein duality, which states that the Earth Mover's distance can be computed by finding the maximum value of a K-Lipschitz function. Enforcing this Lipschitz constraint on the critic is essential for ensuring that the critic's output is a valid approximation of the Wasserstein distance. Initially, \\cite{arjovsky2017ze5} proposed weight clipping as a simple method to enforce this constraint, albeit with some practical limitations such as potentially reducing model capacity or requiring careful hyperparameter tuning. Despite these initial practical challenges, the theoretical foundation laid by WGANs, particularly the rigorous application of the Kantorovich-Rubinstein duality and the K-Lipschitz critic, represented a profound theoretical advancement. It moved GAN research from heuristic-driven stabilization to a more principled, mathematically grounded approach, paving the way for subsequent improvements in GAN training stability and performance.\n\nIn conclusion, the introduction of Wasserstein GANs by \\cite{arjovsky2017ze5} marked a paradigm shift in generative modeling. By replacing the problematic Jensen-Shannon divergence with the Earth Mover's distance and introducing the K-Lipschitz critic requirement, WGANs provided a stable, theoretically sound framework that effectively addressed vanishing gradients and offered a more interpretable loss. This fundamental change not only stabilized GAN training but also opened new avenues for research into more robust and high-fidelity generative models, establishing a new benchmark for theoretical rigor in the field.",
    "Gradient Penalties for Robust Lipschitz Enforcement": "\\subsection*{Gradient Penalties for Robust Lipschitz Enforcement}\n\nWhile the weight clipping strategy proposed in the original Wasserstein Generative Adversarial Network (WGAN) \\cite{Arjovsky2017} was theoretically motivated to enforce the $K$-Lipschitz constraint on the critic (discriminator), it introduced significant practical limitations that often hindered model performance and stability. The fixed clipping range, a sensitive hyperparameter, could drastically limit the critic's capacity if too small, leading to underfitting and an inability to learn complex functions. Conversely, a large clipping range might not effectively enforce the Lipschitz constraint, resulting in unstable training dynamics akin to those WGAN aimed to mitigate. Furthermore, weight clipping could cause gradients to concentrate at the boundaries of the clipping range, leading to vanishing or exploding gradients in specific layers and further destabilizing the training process \\cite{jabbar2020aj0, purwono2025spz}. This crude enforcement mechanism often restricted the critic's ability to learn a smooth function landscape, which is crucial for providing consistent and meaningful gradients to the generator.\n\nRecognizing these limitations, \\cite{Gulrajani2017} introduced a more robust and effective method for enforcing the Lipschitz constraint: the gradient penalty (WGAN-GP). Instead of directly manipulating the critic's weights, WGAN-GP added a regularization term to the critic's loss function that penalized the norm of its gradient with respect to its input. Specifically, this penalty term encourages the gradient norm to be close to one for samples interpolated linearly between real and generated data points. This approach ensures that the critic's gradients are smooth and well-behaved across the entire input space, without restricting the model's capacity or introducing the boundary effects observed with weight clipping \\cite{Gulrajani2017}. The theoretical justification for this approach lies in the Kantorovich-Rubinstein duality, which requires the critic to be 1-Lipschitz (or $K$-Lipschitz, with $K=1$ being a common choice for simplicity and stability) for the Wasserstein distance to be accurately estimated. By penalizing deviations from a gradient norm of one, WGAN-GP directly addresses this requirement in a differentiable manner.\n\nThe introduction of the gradient penalty in WGAN-GP provided several critical advantages. Firstly, it allowed the critic to learn a much smoother function, which in turn provided more stable and informative gradients to the generator, significantly enhancing overall training stability. This explicit enforcement of smoothness and Lipschitz continuity is vital for GANs, as highlighted by \\cite{chu2020zbv}, who demonstrate how such conditions contribute to the eventual stationarity of the generator during training. Secondly, by not directly constraining the weights, WGAN-GP allowed the critic to maintain its full representational capacity, enabling it to learn more complex decision boundaries and better distinguish between real and fake samples. This methodological improvement not only prevented capacity limitations but also further reduced the incidence of mode collapse, as the generator received consistent feedback across the data manifold.\n\nHowever, this robustness came with a computational overhead. Calculating the gradient norm with respect to the input data requires computing second-order derivatives (or at least first-order derivatives of the critic's output with respect to its input, which are then used in the penalty term), which can be computationally expensive, especially for high-dimensional inputs and large batch sizes. This computational burden, while manageable, motivated the search for alternative and more efficient methods of Lipschitz enforcement. WGAN-GP represented a significant advancement in function space regularization, explicitly enforcing smoothness. This contrasts with implicit regularization methods that might arise from architectural choices or other normalization techniques. For instance, recent work such as CHAIN (Lipschitz Continuity Constrained Normalization) \\cite{ni2024y70} has explored integrating Lipschitz constraints directly within normalization layers, offering another avenue for ensuring discriminator stability, particularly in data-efficient GANs.\n\nIn conclusion, the transition from WGAN's crude weight clipping to WGAN-GP's gradient penalty marked a crucial evolutionary step in stabilizing GAN training. By providing a robust and capacity-preserving method for Lipschitz enforcement, WGAN-GP laid the groundwork for the development of more advanced and stable GAN architectures, proving indispensable for generating high-quality and diverse samples \\cite{purwono2025spz}. The theoretical soundness and practical efficacy of WGAN-GP quickly made it a standard practice in subsequent GAN architectures. Despite its widespread adoption and efficacy, the computational demands of WGAN-GP, particularly the need for gradient computation on interpolated samples, motivated the search for more computationally efficient and universally applicable methods for Lipschitz enforcement. This led to innovations like Spectral Normalization \\cite{miyato2018arc}, which offered an alternative approach to constraining the discriminator's Lipschitz constant without explicit gradient penalties.",
    "Spectral Normalization and Dynamic Learning Rates": "\\subsection{Spectral Normalization and Dynamic Learning Rates}\n\nThe pursuit of stable and efficient Generative Adversarial Network (GAN) training has been a central challenge since their inception. While early advancements like Wasserstein GANs with Gradient Penalties (WGAN-GP) significantly improved stability by enforcing the Lipschitz constraint on the discriminator, they often introduced computational overhead due to the need for gradient computations on interpolated samples \\cite{gulrajani2017improved}. This computational burden and the sensitivity to the interpolation strategy motivated the search for more direct and efficient methods for Lipschitz enforcement, as well as optimized training dynamics to better manage the adversarial game \\cite{jabbar2020aj0}. The theoretical underpinnings of GAN stability often point to the importance of discriminator smoothness and bounded Lipschitz constants to ensure meaningful gradients and prevent mode collapse \\cite{chu2020zbv}.\n\nA pivotal innovation addressing these challenges was **Spectral Normalization (SN)**, introduced by \\cite{miyato2018arc}. SN offers an elegant and computationally efficient mechanism to enforce the 1-Lipschitz constraint on the discriminator, a critical requirement for stable training, particularly in Wasserstein-based GANs. Unlike gradient penalties, which regularize the discriminator's output gradients, SN directly normalizes the spectral norm of the weight matrices in each layer of the discriminator. The spectral norm of a matrix represents its largest singular value, and by normalizing it to 1, SN ensures that the Lipschitz constant of each individual layer, and consequently the entire discriminator network, is bounded. This direct approach makes SN computationally lighter than gradient penalties, as it avoids the need for explicit gradient computations on interpolated samples. Furthermore, SN is straightforward to implement and can be seamlessly integrated into various GAN architectures without extensive hyperparameter tuning or specific architectural modifications, making it a highly generalizable stabilization technique \\cite{miyato2018arc}. By preventing the discriminator from becoming overly confident or powerful too rapidly, SN fosters smoother loss landscapes, provides more consistent and informative gradient signals to the generator, and significantly mitigates issues such as vanishing gradients and mode collapse, ultimately leading to the generation of higher-quality and more diverse samples. This method represents a refinement in the broader category of weight normalization techniques, which includes earlier approaches like Weight Normalization (WN) \\cite{xiang20171at} that aimed to improve training stability by reparameterizing weights. SN, however, specifically targets the Lipschitz constant, providing a more theoretically grounded and effective solution for GANs.\n\nComplementing Spectral Normalization, the paper by \\cite{miyato2018arc} also effectively employed the **Two-Time-Scale Update Rule (TTUR)**, a technique originally proposed by \\cite{heusel2017gans} to further optimize GAN training dynamics. TTUR is predicated on the understanding that the generator and discriminator, with their distinct objectives and learning challenges, often benefit from different learning rates. Instead of applying a single learning rate to both networks, TTUR allows for separate learning rates, typically setting the discriminator's learning rate to be higher than the generator's. This dynamic learning rate management is crucial for maintaining a healthy adversarial balance throughout the training process. In the context of two-player games like GANs, the interaction between the players' updates can lead to complex dynamics, where the choice of learning rate significantly impacts convergence and stability \\cite{liang2018r52}. If the discriminator learns too slowly, it may fail to provide a sufficiently strong or accurate signal for the generator to improve. Conversely, if the discriminator learns too quickly and becomes overly powerful, the generator's gradients can vanish, leading to training stagnation or mode collapse. By allowing distinct update frequencies or magnitudes, TTUR prevents one network from dominating the other, facilitating a more balanced and effective adversarial game. This strategy enhances training stability, improves convergence properties, and contributes to better sample quality and diversity across a wide range of datasets.\n\nThe combined application of Spectral Normalization and the Two-Time-Scale Update Rule by \\cite{miyato2018arc} marked a significant advancement in GAN research, moving towards more principled and efficient stabilization strategies. SN provides a robust, computationally light, and generalizable method for enforcing Lipschitz continuity, while TTUR optimizes the delicate adversarial balance through dynamic learning rate management. These innovations have become foundational, with SN, in particular, being widely adopted in subsequent state-of-the-art GAN architectures. The principles of Lipschitz-constrained normalization continue to be explored, with recent works like CHAIN (LipsCHitz Continuity ConstrAIned Normalization) \\cite{ni2024y70} further refining normalization techniques for data-efficient GANs by focusing on gradient reduction and adaptive feature interpolation. Similarly, other regularization methods, such as Consistency Regularization \\cite{zhang2019hjo} and constrained discriminator outputs \\cite{chao2021ynq}, have shown to work effectively with SN, highlighting its compatibility and foundational role. Despite these advancements, the challenge of achieving perfect mode coverage and absolute training stability in increasingly complex generative models remains an active area of research, underscoring the ongoing relevance of adaptive and efficient regularization techniques exemplified by SN and TTUR.",
    "Alternative Loss Functions for Enhanced Stability": "\\subsection{Alternative Loss Functions for Enhanced Stability}\n\nThe persistent challenge of training instability in Generative Adversarial Networks (GANs), characterized by issues such as vanishing gradients, mode collapse, and oscillating performance, has driven extensive research into modifying the core objective functions. Beyond the f-divergences initially explored, a significant line of inquiry has focused on designing alternative loss functions that provide smoother, non-saturating gradients and more robust convergence properties, thereby enhancing the overall training dynamics and generative quality. This quest highlights a continuous search for diverse mathematical perspectives to achieve stable and effective generative modeling.\n\nA foundational contribution in this area is the introduction of Least Squares Generative Adversarial Networks (LSGANs) \\cite{Mao2017}. LSGANs address the limitations of the traditional sigmoid cross-entropy loss, which can suffer from vanishing gradients when the discriminator becomes overly confident and saturates. By replacing this with a least squares loss function, LSGANs ensure that both the generator and discriminator receive meaningful, non-saturating gradients throughout training. This modification encourages the generator to produce samples closer to the decision boundary, leading to improved training stability and the generation of higher quality images compared to original GANs. Theoretically, the least squares objective implicitly minimizes the Pearson $\\chi^2$ divergence, offering a distinct and often more stable mathematical perspective than the Jensen-Shannon divergence minimized by early GANs \\cite{Mao2017}.\n\nFollowing the principles of non-saturating objectives, the adversarial hinge loss emerged as another cornerstone for stable GAN training, particularly in high-fidelity models. The hinge loss provides a clear margin for classification, penalizing the discriminator only when its output for real samples falls below a certain positive margin, or when its output for fake samples rises above a negative margin. This margin-based formulation ensures that the discriminator does not become overly confident too early, preventing gradient saturation and providing a consistent learning signal to the generator. For the generator, the hinge loss encourages it to push fake samples beyond the discriminator's negative margin. This approach has been widely adopted in state-of-the-art architectures, including Self-Attention GANs (SAGAN) and BigGAN, due to its effectiveness in promoting stable training and high-quality image synthesis. \\textcite{wang20178xf} further explored adaptive hinge loss functions, demonstrating how dynamically adjusting the margin based on the expected energy of the target distribution can lead to improved stability and performance, with theoretical proofs of convergence under certain assumptions.\n\nThese alternative loss functions represent a critical shift in GAN optimization strategies. Unlike Wasserstein GANs with Gradient Penalty (WGAN-GP) \\cite{Gulrajani2017}, which primarily enforce a Lipschitz constraint on the discriminator to ensure meaningful gradients across the input space, LSGANs and hinge loss directly reshape the objective landscape itself. They achieve stability by preventing the discriminator's loss from saturating, thus providing a more consistent and robust gradient flow to the generator. This distinction is crucial: while WGAN-GP focuses on the *smoothness* of the discriminator function, LSGANs and hinge loss focus on the *shape* of the loss function to avoid regions of zero gradient. The theoretical framework proposed by \\textcite{chu2020zbv} further elucidates the importance of smoothness and specific divergence properties for guaranteeing eventual stationarity of the generator, highlighting why non-saturating and margin-based losses contribute to stability.\n\nBeyond these widely adopted approaches, researchers have continued to explore novel modifications to loss functions. \\textcite{zadorozhnyy20208ft} introduced adaptive weighted discriminator loss functions, or \"aw-loss functions.\" This method addresses the challenge that an equally weighted sum of real and fake losses can sometimes benefit one part of the training while harming the other, leading to instability and mode collapse. By adaptively weighting the real and fake components of the discriminator's loss based on their gradients, aw-loss functions guide the discriminator's training in a direction that explicitly benefits overall GAN stability. This dynamic balancing act has shown significant improvements in Inception Scores (IS) and Fréchet Inception Distance (FID) metrics on various datasets, demonstrating the value of fine-grained control over loss components.\n\nAnother innovative approach is presented by Constrained Generative Adversarial Networks (GAN-C) \\cite{chao2021ynq}. This method introduces an explicit constraint on the discriminator's output, aiming to bound its function space. While theoretically sharing the same Nash equilibrium as the standard GAN, this constraint helps to regularize the discriminator's behavior, preventing it from becoming overly powerful or unstable. In practice, this leads to faster convergence during training and the generation of higher-quality data, as demonstrated across a diverse set of image datasets. This highlights that even subtle modifications to how the discriminator's output is handled within the loss framework can significantly impact training stability and generative performance.\n\nIn conclusion, the evolution of GAN loss functions from the original sigmoid cross-entropy to LSGANs, adversarial hinge loss, and more advanced adaptive and constrained objectives, underscores a fundamental principle: robust and non-saturating gradients are paramount for stable adversarial training. These diverse mathematical strategies, whether by reshaping the objective landscape, introducing classification margins, or adaptively weighting loss components, have collectively provided the practical stability necessary for the subsequent architectural innovations that led to high-fidelity generative models. While these advancements have significantly mitigated issues like vanishing gradients, the complete elimination of mode collapse and the guarantee of universal convergence across all data distributions remain active areas of research, as noted by reviews like \\cite{wang2019w53}. Future work continues to explore hybrid approaches and novel theoretical frameworks to further enhance the robustness and generative power of GANs.",
    "Advanced Regularization and Architectural Constraints": "\\subsection{Advanced Regularization and Architectural Constraints}\n\nBeyond foundational techniques like gradient penalties and spectral normalization, the persistent challenges of Generative Adversarial Networks (GANs)—including mode collapse, training fragility, and sensitivity to input perturbations—have necessitated the development of a broader array of advanced regularization techniques and carefully designed architectural constraints. These sophisticated solutions aim to further enhance GAN stability, improve performance, and foster diverse generation, pushing the boundaries of what is achievable in adversarial training dynamics.\n\nA significant class of advanced regularization techniques focuses on modifying the training objective or adding explicit penalties to guide the adversarial process more effectively. \\cite{metz20169ir} introduced Unrolled Generative Adversarial Networks, a method that addresses mode collapse by allowing the generator to \"see\" the effects of several future discriminator optimization steps. By defining the generator's objective with respect to an unrolled optimization of the discriminator, this approach provides a more informed and stable gradient signal, encouraging the generator to produce a wider variety of samples and thus mitigating mode collapse. However, this comes at the cost of increased computational complexity due to the inner loop unrolling. Complementing this, \\cite{roth2017eui} proposed a regularization method specifically designed to overcome the fundamental limitation of dimensional mismatch or non-overlapping support between the model and data distributions, which often leads to undefined density ratios and unstable training. This technique offers a computationally efficient way to stabilize GAN training, making them more reliable.\n\nTo further enhance robustness, particularly to input variations, Consistency Regularization (CR-GAN) emerged as a powerful technique \\cite{zhang2019hjo}. Inspired by semi-supervised learning, CR-GAN penalizes the discriminator for being inconsistent in its predictions on augmented versions of the same input. Specifically, it applies non-differentiable augmentations (e.g., random shifts, flips, color jitter) to both real and generated samples and adds a penalty if the discriminator's output for an augmented sample differs significantly from its unaugmented counterpart. This forces the discriminator to learn more robust and stable features, which in turn provides a more consistent learning signal to the generator. CR-GAN has demonstrated significant improvements in FID scores and training stability, working effectively with existing techniques like spectral normalization, though it introduces additional computational overhead for augmentation and penalty calculation. Further addressing mode collapse, \\cite{che2016kho} proposed Mode Regularized Generative Adversarial Networks, which introduce regularization terms to explicitly encourage the discriminator to assign a fairer distribution of probability mass across the modes of the data-generating distribution. This prevents the discriminator from becoming overly confident in distinguishing only a few modes, thereby promoting broader mode coverage by the generator. For scenarios with limited training data, \\cite{tseng2021m2s} developed a regularization approach that theoretically connects the regularized loss to a LeCam-divergence, which is inherently more robust under data scarcity. This method improves generalization and stabilizes learning dynamics, proving particularly effective when combined with data augmentation techniques.\n\nBeyond explicit regularization terms, architectural choices themselves serve as powerful implicit constraints that profoundly influence GAN stability and feature learning. As highlighted by \\cite{chu2020zbv}, proper architectural design can enforce properties like Lipschitz continuity, which are crucial for stable training. For instance, the widespread adoption of **Batch Normalization** in architectures like DCGAN \\cite{radford2015unsupervised} acts as a form of regularization by normalizing layer inputs, reducing internal covariate shift, and stabilizing training. More advanced normalization techniques, such as **Adaptive Instance Normalization (AdaIN)** used in StyleGAN \\cite{karras2019style}, not only enable style control but also implicitly regularize the feature representations by decoupling content and style, leading to smoother latent spaces and improved disentanglement. Similarly, the integration of **residual connections** in modern discriminators, as seen in architectures like BigGAN \\cite{brock2018biggan}, facilitates deeper networks by ensuring stable gradient flow and preventing degradation, thereby implicitly regularizing the discriminator's capacity and smoothness. Furthermore, StyleGAN's **mapping network**, which transforms the initial latent code into an intermediate latent space, serves as a powerful implicit regularizer. This transformation disentangles the latent space, making it more structured and easier for the generator to navigate, which inherently promotes diversity and stability by preventing the generator from collapsing to a few modes.\n\nThe continuous search for sophisticated solutions also includes the integration of adaptive feedback mechanisms and other innovative regularization strategies. These approaches dynamically adjust regularization strengths or training parameters based on the current state of the adversarial game, aiming to maintain a delicate balance between the generator and discriminator. While specific examples vary, the underlying principle is to provide context-aware guidance to stabilize adversarial training dynamics and improve model generalization, moving towards more autonomous and robust training pipelines.\n\nIn conclusion, the evolution of GAN stabilization extends far beyond initial gradient penalties and spectral normalization. The field has progressively embraced a diverse toolkit of advanced regularization techniques, from modifying training objectives through unrolling and introducing explicit penalties for consistency or mode coverage, to leveraging implicit architectural designs that enforce stability and improve feature learning. The latest advancements highlight a trend towards integrating adaptive feedback mechanisms and theoretically grounded regularization, demonstrating a sophisticated, multi-faceted approach to achieve robust, diverse, and high-fidelity generation, underscoring the ongoing quest for more resilient and versatile generative models.",
    "Progressive Growing for High-Resolution Synthesis": "\\subsection*{Progressive Growing for High-Resolution Synthesis}\n\nThe generation of high-resolution, photorealistic images has long been a significant challenge for Generative Adversarial Networks (GANs), often hampered by training instability and the computational demands of large models. Early attempts to stabilize GAN training primarily focused on modifying objective functions or regularization techniques to mitigate issues such as mode collapse and vanishing gradients. For instance, \\textcite{metz20169ir} introduced Unrolled Generative Adversarial Networks, a method that stabilized training by defining the generator's objective with respect to an unrolled optimization of the discriminator, thereby addressing mode collapse and enhancing the diversity and coverage of the data distribution. Similarly, \\textcite{che2016kho} proposed Mode Regularized Generative Adversarial Networks, arguing that the functional shape of discriminators in high-dimensional spaces contributed to instability and mode collapse. Their solution involved introducing regularizers to stabilize training and ensure a more equitable distribution of probability mass across data modes, particularly in the early phases of training. While these methods significantly improved the foundational stability and diversity of GANs, scaling them to generate images at resolutions beyond 256x256 pixels remained a formidable hurdle.\n\nA pivotal methodological breakthrough that fundamentally transformed the landscape of high-resolution image synthesis was the introduction of Progressive Growing of GANs (PGGANs) by \\textcite{Karras2018}. This innovative approach directly tackled the challenges of training stability and high-resolution output by gradually increasing the resolution of both the generated images and the discriminator's inputs throughout the training process. Instead of attempting to synthesize high-resolution images from scratch, PGGANs begin training at a very low resolution, typically 4x4 pixels. As training progresses and the network learns to generate stable images at the current resolution, new layers are incrementally added to both the generator and discriminator. These new layers are smoothly \"faded in\" using a weighted sum with the existing layers, ensuring a continuous and stable transition to higher resolutions.\n\nThis progressive growing strategy offers several critical advantages. Firstly, it significantly improves training stability by presenting an easier learning task to the networks at each stage. Learning low-frequency features at coarse resolutions is simpler, and this knowledge is then leveraged and refined as higher-frequency details are introduced with increasing resolution. This hierarchical learning process effectively prevents the common pitfalls of GAN training, such as mode collapse and gradient instability, which are exacerbated when attempting to learn complex, high-dimensional distributions directly. Secondly, PGGANs enabled the synthesis of unprecedentedly photorealistic images, pushing the boundaries to resolutions as high as 1024x1024 pixels. This marked a major leap in image quality and scale, allowing for the creation of visually compelling and diverse outputs that were previously unattainable. Finally, by starting with smaller networks and gradually expanding them, PGGANs also contributed to a reduction in the overall training time required to achieve high-resolution outputs, as the initial stages are computationally less intensive. The success of PGGANs laid a robust foundation for subsequent advancements in high-fidelity image generation, including the StyleGAN series, by demonstrating a scalable and stable training paradigm for complex generative tasks.\n\nIn conclusion, the progressive growing methodology introduced by PGGANs represented a paradigm shift in generative modeling, moving beyond earlier regularization and objective function modifications to address stability and resolution through a structured training curriculum. While earlier works like \\textcite{metz20169ir} and \\textcite{che2016kho} laid crucial groundwork for general GAN stability, PGGANs provided the architectural and training strategy necessary to unlock truly high-resolution, photorealistic synthesis. Despite its profound impact, the computational cost of training PGGANs, especially for extremely high resolutions or diverse datasets, still presented avenues for further optimization, paving the way for future research into more efficient and controllable high-fidelity generative models.",
    "Large-Scale Training and Self-Attention Mechanisms": "\\subsection{Large-Scale Training and Self-Attention Mechanisms}\n\nThe pursuit of high-fidelity and globally coherent image generation with Generative Adversarial Networks (GANs) has consistently pushed the boundaries of computational scale and architectural innovation. While earlier works like Progressive Growing GANs (PGGANs) demonstrated the efficacy of gradual resolution increase, a subsequent landmark achievement underscored the power of scaling model capacity, dataset size, and leveraging advanced architectural components to unlock unprecedented levels of generative performance.\n\nThis paradigm shift was most notably exemplified by BigGAN \\cite{brock2019biggan}, which significantly advanced the state-of-the-art in GANs by training on massive datasets like ImageNet with substantially larger models and batch sizes. BigGAN demonstrated that computational scale, when combined with robust stabilization techniques and architectural innovations, was crucial for achieving state-of-the-art results on diverse, large-scale datasets, pushing the limits of GAN performance. Its success highlighted that simply increasing model parameters and training data could lead to a qualitative leap in generated image quality and diversity, provided the underlying training remained stable.\n\nA key architectural innovation integrated into BigGAN was the self-attention mechanism, originally introduced to GANs by Self-Attention Generative Adversarial Networks (SAGAN) \\cite{zhang2019selfattention}. Unlike traditional convolutional layers, which have a localized receptive field and primarily capture local dependencies, self-attention allows a neuron to attend to features at any spatial location in the input, irrespective of their distance. This global contextual awareness was instrumental in enabling the generator to produce globally coherent structures, ensuring that disparate parts of an image (e.g., an animal's head and limbs, or consistent background elements) were logically consistent and well-aligned. For instance, in complex scenes, self-attention helps maintain structural integrity across the entire image, leading to more globally coherent and higher-fidelity outputs. The continued relevance of self-attention in GAN architectures is further evidenced by more recent works, such as PEGANs, which also leverage self-attention modules to improve long-range dependency modeling and enhance generation quality \\cite{xue2022n0r}.\n\nCrucially, the ability of BigGAN to effectively leverage massive scale was predicated on a foundation of robust stabilization techniques developed in preceding works. Before such large models could be trained, fundamental issues of GAN instability, such as vanishing gradients and mode collapse, needed reliable solutions \\cite{jabbar2020aj0, wiatrak20194ib}. Two specific techniques proved particularly foundational for BigGAN's stability: Spectral Normalization (SN) and the hinge loss objective. Spectral Normalization \\cite{miyato2018arc} provided an efficient and effective method for enforcing the Lipschitz constraint on the discriminator, which is vital for stable training, especially with large model capacities. By normalizing the spectral norm of weight matrices, SN prevents the discriminator from becoming overly confident or exhibiting exploding gradients, thereby providing a smoother and more informative gradient signal to the generator. Concurrently, BigGAN adopted a hinge version of the adversarial loss function, which offers improved stability and performance compared to earlier objectives like the original minimax loss or even WGAN-GP in certain contexts \\cite{wang20178xf}. The hinge loss provides clear, non-saturating gradients, helping to maintain a healthy adversarial balance during the extensive training required for large-scale models. These advancements in regularization and loss functions provided the necessary robustness for BigGAN to scale effectively without succumbing to common training pathologies.\n\nBeyond architectural and stabilization advancements, BigGAN also introduced the \"truncation trick,\" a critical technique for controlling the trade-off between sample quality and diversity. During inference, by sampling latent codes from a truncated normal distribution (i.e., restricting samples to within a certain range, typically 1 or 2 standard deviations from the mean), BigGAN could generate images of exceptionally high perceptual quality, albeit at the cost of some diversity. Conversely, sampling from the full latent space yielded greater diversity but often included lower-quality or unusual samples. This finding revealed important insights into the structure of the latent space learned by large-scale GANs, suggesting that high-quality samples tend to cluster in denser regions of the latent space, while sparser regions might contain less realistic or out-of-distribution samples. The truncation trick thus became a standard practice for showcasing the peak performance of high-fidelity GANs.\n\nIn conclusion, the era of large-scale training, epitomized by BigGAN, marked a significant advancement in generative modeling. By synergistically combining massive computational resources, advanced architectural components like self-attention, and leveraging robust stabilization techniques such as Spectral Normalization and hinge loss, GANs achieved unprecedented levels of fidelity and global coherence, particularly on complex datasets like ImageNet. This period underscored the critical importance of computational scale, large batch sizes, and sophisticated architectures for pushing the boundaries of GAN performance, while also highlighting the nuanced control over generation quality offered by techniques like the truncation trick.",
    "Style-Based Generators for Disentangled Control and Photorealism": "\\subsection{Style-Based Generators for Disentangled Control and Photorealism}\n\nThe pursuit of both photorealistic image synthesis and intuitive, disentangled control over generated features has represented a significant challenge in the evolution of Generative Adversarial Networks (GANs). The seminal StyleGAN architecture \\cite{Karras2019} marked a pivotal advancement, introducing a novel style-based generator that fundamentally reshaped the landscape of image generation by dramatically improving perceptual quality and the editability of synthetic images. This innovation distinguished itself from prior GANs, which often struggled with entangled latent spaces where a single latent dimension could influence multiple, unrelated visual attributes \\cite{jabbar2020aj0}.\n\nThe core of StyleGAN's innovation lies in its unique generator design. Unlike earlier GANs that directly fed a latent code $z$ into the initial layers of the generator, StyleGAN employs a separate *mapping network*. This network transforms an initial, typically isotropic Gaussian latent code $z \\in \\mathcal{Z}$ into an intermediate latent vector $w \\in \\mathcal{W}$. The $\\mathcal{W}$ space is designed to be less entangled than the original $z$ space, effectively linearizing the latent representation and making it more amenable to controlling specific visual attributes \\cite{Karras2019}. This disentanglement is further enhanced through a technique called 'style mixing', where different $w$ vectors (derived from different $z$ codes) are applied to different layers of the generator during training. This forces each style-specific layer to specialize in certain features, promoting a more granular and independent control over the generated image \\cite{Karras2019}.\n\nThe generator itself is a series of upsampling blocks, but critically, it does not receive the initial latent code $z$ directly. Instead, it starts with a learned constant input and injects 'style' information at multiple resolutions through Adaptive Instance Normalization (AdaIN) layers. AdaIN, originally introduced in the context of style transfer \\cite{Huang2017AdaIN}, normalizes the mean and variance of feature map activations independently for each instance and then scales and biases them using learned parameters derived from the intermediate style vector $w$. This mechanism allows for hierarchical control: coarse styles injected at early layers influence high-level attributes such as pose, identity, and overall structural composition, while styles applied at later layers control finer details like hair color, texture, and micro-features \\cite{Karras2019}. This approach contrasts with other normalization techniques like Batch Normalization \\cite{xiang20171at} or Spectral Normalization \\cite{miyato2018arc}, which primarily focus on stabilizing training and enforcing Lipschitz constraints, by explicitly modulating feature statistics to inject style information.\n\nStyleGAN's architectural innovations significantly improved the perceptual quality and realism of generated images, setting new benchmarks. The disentangled $\\mathcal{W}$ space, combined with the hierarchical style injection, facilitated unprecedented user-driven content creation, allowing for intuitive manipulation of facial features, age, and other attributes \\cite{jabbar2020aj0}. Furthermore, the introduction of the 'truncation trick' allowed for a trade-off between sample diversity and quality, enabling the generation of higher-quality, albeit less diverse, samples by moving latent codes closer to the average $w$ in the $\\mathcal{W}$ space \\cite{Karras2019}.\n\nDespite its revolutionary impact, the original StyleGAN architecture was not without limitations. While the $\\mathcal{W}$ space offered improved disentanglement compared to direct $z$ manipulation, it was not perfectly orthogonal; some entanglement between attributes, such as pose and identity, or between global and local features, still persisted. As a \"push-forward\" generative model, StyleGAN's ability to fit highly multimodal distributions is theoretically constrained by the Lipschitz constant of its generator, where a large constant is often required for multimodal fitting but can conflict with training stability \\cite{salmona202283g}. Moreover, the original StyleGAN exhibited certain characteristic visual artifacts, such as \"blob\" artifacts or texture sticking, which could manifest as repetitive patterns or unnatural textures, particularly when interpolating in the latent space. These issues, along with challenges related to normalization and upsampling artifacts, highlighted areas for further refinement, paving the way for subsequent architectural improvements aimed at enhancing image quality and consistency.\n\n\\bibliography{references}",
    "Addressing Perceptual Artifacts and Aliasing in StyleGANs": "\\subsection*{Addressing Perceptual Artifacts and Aliasing in StyleGANs}\n\nWhile early Generative Adversarial Networks (GANs) focused on achieving stable training and basic image synthesis, the StyleGAN series marked a significant shift towards unprecedented levels of photorealism and controllable generation. However, even the groundbreaking StyleGAN architecture \\cite{Karras2019} exhibited certain perceptual artifacts and fundamental signal processing issues that limited its realism and consistency, particularly when generating high-resolution content or animating latent space interpolations. Subsequent refinements in the series, namely StyleGAN2 \\cite{Karras2020} and StyleGAN3 \\cite{Karras2021}, meticulously addressed these limitations, pushing the boundaries of synthetic image quality and robustness.\n\nThe initial StyleGAN model \\cite{Karras2019} revolutionized image synthesis by introducing a style-based generator, a mapping network, and Adaptive Instance Normalization (AdaIN) layers. This architecture enabled highly disentangled control over various visual attributes, allowing for intuitive manipulation of generated images. Despite its success in generating visually compelling and diverse outputs, StyleGAN suffered from noticeable \"droplet\" artifacts and a lack of perfect disentanglement, where changes in one latent dimension could inadvertently affect unrelated visual features.\n\nTo overcome these shortcomings, StyleGAN2 \\cite{Karras2020} undertook a comprehensive analysis of the generator architecture and training process, identifying several sources of these perceptual artifacts. The authors found that the progressive growing scheme, commonly used in earlier high-resolution GANs, and certain aspects of instance normalization contributed to these issues. StyleGAN2 introduced several key innovations, most notably **path length regularization**, which aimed to encourage a more \"well-behaved\" latent space. By regularizing the mapping from latent codes to features, path length regularization ensured that a fixed-size step in the latent space always resulted in a fixed-magnitude change in the image space, thereby improving disentanglement and significantly reducing the visually distracting \"droplet\" artifacts. Furthermore, StyleGAN2 redesigned the generator by removing the progressive growing and replacing instance normalization with a more robust weight demodulation technique, leading to enhanced image quality and stability.\n\nDespite the significant improvements in StyleGAN2, a more fundamental limitation remained: aliasing. This problem, inherent in discrete signal processing, manifested as static high-frequency details that did not move correctly when images were translated or rotated, leading to a \"texture sticking\" effect and hindering smooth animation. StyleGAN3 \\cite{Karras2021} meticulously diagnosed this issue, recognizing that previous GANs, including StyleGAN2, implicitly suffered from aliasing due to their reliance on discrete pixel grids and operations that were not perfectly translation-equivariant. To address this, StyleGAN3 introduced a radical redesign of the generator to be truly **alias-free**. This was achieved by incorporating **anti-aliasing filters** at every layer of the generator, ensuring that high-frequency information was handled correctly and consistently across different resolutions and transformations. By making the generator fully translation-equivariant, StyleGAN3 enabled generated content to appear consistent and realistic even under continuous transformations, drastically improving the quality of latent space interpolations and animation capabilities.\n\nThe advancements from StyleGAN2's path length regularization to StyleGAN3's alias-free architecture represent a continuous effort to refine the underlying signal processing principles of generative models. These innovations not only enhanced the realism and perceptual quality of generated images but also made GANs more robust and versatile for applications requiring high consistency, such as video synthesis or interactive content creation. While StyleGAN3 achieved unprecedented levels of control and fidelity, the computational cost associated with its alias-free design and the general challenge of scaling GANs to extremely diverse, large-scale datasets remain areas for ongoing research and optimization.",
    "Training with Limited Data: Adaptive Discriminator Augmentation": "\\subsection{Training with Limited Data: Adaptive Discriminator Augmentation}\n\nThe efficacy of Generative Adversarial Networks (GANs) in synthesizing high-quality data has long been predicated on the availability of extensive datasets. However, this requirement poses a significant barrier to their application in numerous real-world scenarios where data acquisition is inherently costly, time-consuming, or simply infeasible. In data-scarce environments, GAN training faces a critical challenge: the discriminator rapidly overfits to the limited training samples, leading to a collapse of the adversarial game, poor gradient signals for the generator, and ultimately, low-quality or non-diverse generated outputs. This section delves into pivotal advancements that have enabled GANs to achieve high-fidelity generation even with limited data, primarily through sophisticated augmentation strategies.\n\nEarly attempts to mitigate discriminator overfitting in data-scarce regimes often involved applying standard data augmentations (e.g., rotations, flips, color jitter) to the real training images. However, this approach quickly revealed a critical problem known as \"augmentation leakage\" \\cite{zhao2020xhy}. If augmentations are applied exclusively to real samples, the discriminator learns to distinguish between augmented real data and unaugmented fake data. This inadvertently forces the generator to produce images that incorporate the augmentation artifacts, leading to a degradation in sample quality and a failure to learn the true data distribution. To counteract this, Differentiable Augmentation (DiffAugment) \\cite{zhao2020xhy} proposed applying augmentations to *both* real and generated samples in a differentiable manner. While DiffAugment effectively prevented augmentation leakage, it required careful selection of augmentation policies and a fixed augmentation strength, which might not be optimal across different datasets or training stages.\n\nBeyond augmentation, other regularization techniques have been explored to enhance discriminator robustness and generalization, which are crucial for stable training under limited data. For instance, the LeCam-GAN \\cite{tseng2021m2s} focused on modifying the loss function to be more robust, demonstrating improved generalization and stability. Similarly, methods like InfoMax-GAN \\cite{lee20205ue} aimed to mitigate catastrophic forgetting in the discriminator and reduce mode collapse by employing contrastive learning and mutual information maximization, thereby fostering a more robust discriminator less prone to overfitting. Robust Generative Adversarial Network (RGAN) \\cite{zhang201996t} improved generalization by promoting local robustness within the neighborhood of training samples, a strategy particularly beneficial when the training set is small. Furthermore, approaches like Probability Ratio Clipping and Sample Reweighting \\cite{wu2020p8p} and Constrained GANs (GAN-C) \\cite{chao2021ynq} introduced mechanisms to stabilize discriminator training and enforce constraints on its output, preventing it from becoming overly confident or unstable, which are common failure modes exacerbated by limited data. These efforts collectively underscored the importance of a well-behaved and generalizable discriminator for overall GAN stability.\n\nA significant breakthrough specifically tailored to address discriminator overfitting in limited data regimes was the introduction of Adaptive Discriminator Augmentation (ADA) by Karras et al. \\cite{karras202039x}. ADA provides a robust and dynamic solution by preventing the discriminator from memorizing the small training set, a primary cause of training divergence and poor sample quality. The core mechanism of ADA involves applying a set of non-differentiable augmentations (e.g., rotations, flips, color jitter, cutouts) to *both* real and generated images before they are presented to the discriminator. This crucial step ensures that the generator is not incentivized to produce augmented-looking samples, thereby effectively mitigating augmentation leakage, similar to DiffAugment.\n\nWhat distinguishes ADA and makes it particularly effective is its adaptive control mechanism. The probability of applying these augmentations is dynamically adjusted during training based on the discriminator's performance. ADA monitors the discriminator's overfitting, typically by tracking its classification accuracy on a validation set or by comparing its accuracy on augmented versus unaugmented real images. If the discriminator is found to be overfitting (e.g., achieving very high accuracy on real images), the augmentation probability is increased, making its task harder and forcing it to learn more generalizable features. Conversely, if the discriminator struggles, the augmentation strength is reduced. This adaptive feedback loop maintains a healthy adversarial balance, preventing the discriminator from becoming too strong too quickly and ensuring that the generator receives consistent, meaningful gradients.\n\nADA demonstrated remarkable improvements, enabling high-quality image synthesis with significantly fewer training images. For instance, it achieved results comparable to StyleGAN2 trained on full datasets with an order of magnitude less data, and even established new state-of-the-art FID scores on benchmarks like CIFAR-10, which was re-evaluated as a limited-data benchmark \\cite{karras202039x}. This methodological innovation democratized access to high-quality generative models for applications where extensive datasets are impractical, such as medical imaging, specialized industrial design, or artistic content creation. By dynamically managing the discriminator's learning capacity relative to the data size, ADA effectively bridges the gap between data-hungry GAN architectures and real-world data constraints.\n\nIn summary, the evolution of GAN training under data scarcity has progressed from understanding and mitigating augmentation leakage to sophisticated adaptive augmentation strategies. ADA revolutionized the field by providing a practical and robust method to prevent discriminator overfitting, thereby enabling high-quality generation with significantly less data. While complementary techniques focusing on general discriminator stability and generalization (e.g., \\cite{lee20205ue, zhang201996t, wu2020p8p, chao2021ynq}) contribute to the overall robustness of GANs, ADA specifically addresses the unique challenges posed by limited data through its dynamic augmentation policy. Despite these successes, challenges persist in optimizing augmentation policies for highly diverse or complex datasets, ensuring mode coverage with extremely sparse data, and developing robust augmentation strategies for non-image data types. Future research will likely explore more advanced adaptive augmentation schemes, potentially integrating learned augmentation policies or leveraging transfer learning from large pre-trained models to further enhance data efficiency in GAN training.",
    "Few-Shot and Meta-Learning Approaches for Data Scarcity": "\\subsection{Few-Shot and Meta-Learning Approaches for Data Scarcity}\n\\label{sec:few-shot-meta-learning}\n\nThe deployment of Generative Adversarial Networks (GANs) in domains characterized by extreme data scarcity, such as medical imaging, specialized industrial applications, or urban planning, presents a significant challenge. While methods like Adaptive Discriminator Augmentation (ADA) \\cite{Karras2022} effectively combat discriminator overfitting by dynamically applying non-leaking augmentations to limited datasets, they primarily operate at the data level. For scenarios demanding rapid adaptation to novel data distributions with truly minimal samples, a more fundamental shift towards few-shot and meta-learning paradigms is required, moving beyond data-level interventions to enable models to \"learn how to learn\" from scarce examples.\n\nMeta-learning, or \"learning to learn,\" offers a powerful framework for addressing extreme data scarcity in GANs. The core idea is to train a model across a distribution of related tasks, enabling it to acquire transferable knowledge that facilitates rapid adaptation to new, unseen tasks with only a few training examples. For GANs, this often involves meta-learning the discriminator to quickly establish effective decision boundaries even when presented with a handful of samples from a new target distribution. For instance, \\cite{zhang202263o} proposes Spatially-Transferable Generative Adversarial Networks (STrans-GAN) for urban traffic estimation under data scarcity. This approach incorporates a meta-learning idea into the pre-training process, allowing the model to learn a well-generalized representation from multiple source cities. During fine-tuning on a new city with limited data, a cluster matching regularizer further aids flexible adaptation. This demonstrates how meta-learning can equip the discriminator with an inherent ability to generalize and adapt efficiently, significantly reducing data requirements in truly few-shot settings. However, meta-learning approaches typically require a diverse set of source tasks for effective meta-training, which might not always be available in highly specialized or unique domains. The computational overhead of meta-training across multiple tasks can also be substantial.\n\nBeyond meta-learning the discriminator, other few-shot GAN strategies focus on architectural design and self-supervised learning to enhance data efficiency. \\cite{liu20212c2} introduced FastGAN, a lightweight GAN structure specifically designed for high-fidelity few-shot image synthesis. FastGAN achieves superior quality on high-resolution images (e.g., 1024x1024) with minimal computing cost, converging from scratch with less than 100 training samples on a single GPU. A key innovation is a self-supervised discriminator trained as a feature-encoder, which helps the discriminator learn robust representations from limited data without relying solely on the adversarial signal. This architectural and self-supervised approach provides an alternative to meta-learning by making the core components of the GAN inherently more data-efficient, often exhibiting consistent performance across various image domains. While highly efficient, FastGAN's performance might still be constrained by the inherent limitations of learning complex distributions from extremely few samples, and its architectural choices might not be universally optimal for all data types.\n\nAnother complementary approach involves developing robust regularization schemes that improve generalization under limited data. \\cite{tseng2021m2s} proposes a regularization method for GANs based on LeCam-divergence, which is theoretically shown to be more robust under limited training data than traditional f-divergences. This regularization scheme improves generalization performance and stabilizes learning dynamics, complementing existing data augmentation methods like ADA. By modifying the underlying loss function, LeCam-GAN enhances the model's ability to learn meaningful distributions even when data is scarce, without necessarily requiring a meta-training phase or specialized architectures. However, while robust, such regularization methods primarily address the stability and generalization of the learning process itself, rather than explicitly teaching the model how to rapidly adapt to *new* tasks, which is the strength of meta-learning.\n\nIn synthesis, few-shot and meta-learning approaches represent a crucial progression in making GANs practical for data-scarce environments. Meta-learning (e.g., \\cite{zhang202263o}) enables the discriminator to acquire transferable knowledge for rapid adaptation, transforming the problem into \"learning to adapt\" rather than merely \"learning from scratch\" on limited data. This is particularly valuable when rapid deployment across similar, but distinct, tasks is needed. Architectural innovations combined with self-supervision (e.g., FastGAN \\cite{liu20212c2}) offer computationally efficient solutions for high-fidelity synthesis from few samples by designing intrinsically data-efficient models. Meanwhile, robust regularization techniques (e.g., LeCam-GAN \\cite{tseng2021m2s}) provide theoretical grounding and practical improvements for training stability and generalization under data constraints. Each approach offers distinct advantages and addresses different facets of the data scarcity problem. Meta-learning excels at rapid task adaptation, FastGAN at computational efficiency and high-resolution output, and LeCam-GAN at training stability and generalization.\n\nDespite significant progress, several challenges remain. The definition and acquisition of diverse meta-training tasks for real-world scenarios, particularly in highly specialized domains like medical imaging, can be difficult. The computational cost of meta-training can also be prohibitive. Future research could explore hybrid approaches that combine meta-learning with lightweight architectures and robust regularization techniques to leverage their synergistic benefits. For instance, meta-learning a lightweight generator and discriminator, or integrating LeCam-divergence into a meta-learning framework, could lead to even more data-efficient and stable GANs. Furthermore, investigating meta-learning strategies for the generator itself, or developing unified frameworks that adaptively select or combine data-efficient strategies based on the specific data scarcity level and domain characteristics, represents promising avenues for pushing the boundaries of data-efficient generative learning.",
    "Conditional and Text-to-Image Synthesis": "\\subsection*{Conditional and Text-to-Image Synthesis}\n\nGenerative Adversarial Networks (GANs), initially designed for unconditional image generation, quickly evolved to address the critical need for controlled output, allowing users to specify desired characteristics of the synthesized images. This section traces the progression from simple conditional generation to complex text-to-image synthesis, highlighting the increasing ability of GANs to interpret and visualize semantic information.\n\nThe foundational step towards controlled generation was the introduction of Conditional Generative Adversarial Nets (cGANs) by \\cite{Mirza2014}. Unlike their unconditional predecessors, cGANs feed additional conditional information, such as class labels or other attributes, to both the generator and the discriminator. This direct input guides the generator to produce samples corresponding to the specified conditions, while the discriminator learns to verify both the realism and the adherence to the given condition. Building upon this, Auxiliary Classifier GANs (AC-GANs) \\cite{Odena2017} further enhanced conditional synthesis by incorporating an auxiliary classifier into the discriminator. This classifier not only distinguishes between real and fake images but also predicts the class label of the input, thereby compelling the generator to produce samples that are both realistic and correctly classified, leading to better disentanglement and more robust conditional generation.\n\nThe capability of GANs was significantly expanded with the advent of text-to-image synthesis, where the conditional information takes the form of natural language descriptions. Early efforts, such as those by \\cite{Reed2016}, demonstrated the feasibility of generating images directly from text embeddings. These models mapped textual descriptions into a latent space, which then guided the generator to synthesize corresponding images, with the discriminator evaluating the consistency between the generated image and the input text. However, these initial models often struggled with generating high-resolution and photo-realistic images, particularly for complex scenes.\n\nTo overcome these limitations, multi-stage architectures emerged, notably StackGAN \\cite{Zhang2017}. StackGAN employs a two-stage process: the first stage generates a low-resolution image based on the global text description, and the second stage refines this initial output into a higher-resolution, photo-realistic image by focusing on finer details. This hierarchical approach significantly improved the quality and resolution of text-conditioned images. Further advancements in fine-grained control and semantic alignment were achieved with AttnGAN \\cite{Xu2018}, which introduced an attention mechanism. AttnGAN allows the generator to selectively attend to different words in the text description when generating specific regions of the image, ensuring that local image details are semantically consistent with relevant parts of the text.\n\nMore recently, the integration of robust architectures like StyleGAN with text conditioning has pushed the boundaries of quality and control. StyleGAN-T \\cite{Sauer2024} adapts the highly successful StyleGAN framework for text-to-image synthesis, leveraging its disentangled latent space and advanced generation capabilities. This approach yields high-fidelity, text-conditioned images with improved semantic alignment and offers more intuitive control over the generated output through natural language. This represents a significant leap, allowing users to specify desired outputs with natural language, opening doors for creative applications and content generation.\n\nDespite these remarkable advancements, challenges remain in conditional and text-to-image synthesis. Generating complex scenes with multiple objects and intricate spatial relationships, maintaining semantic consistency across diverse textual descriptions, and ensuring compositional understanding are still active areas of research. The robustness of these models to ambiguous or underspecified text prompts also needs improvement. Future directions will likely focus on enhancing the models' understanding of complex semantic compositions, improving the interpretability of generated outputs, and developing more robust evaluation metrics for text-to-image consistency.",
    "Domain Adaptation and Image-to-Image Translation": "\\subsection*{Domain Adaptation and Image-to-Image Translation}\n\nDomain adaptation and image-to-image translation represent a pivotal application area for Generative Adversarial Networks (GANs), enabling the learned transformation of visual content from one domain to another. This capability is fundamental for a wide array of computer vision tasks, including style transfer, image manipulation, and the generation of synthetic data for training other models \\cite{wang2019w53, liu2020jt0}. The success of these sophisticated applications is intrinsically linked to the advancements in GAN stability and the development of robust conditional generation techniques, building upon the foundational stability mechanisms discussed in Section 3 and the general conditional frameworks in Subsection 5.3.\n\nA seminal contribution to paired image-to-image translation was Pix2Pix, proposed by \\textcite{Isola2017}. This method introduced conditional adversarial networks that learn a direct mapping from an input image to a corresponding output image. By training a conditional generator and discriminator on aligned image pairs, Pix2Pix demonstrated remarkable success in tasks such as converting semantic labels to photorealistic street scenes, generating aerial photographs from maps, or transforming grayscale images to color. The core idea is that the generator learns to produce an output that not only fools the discriminator into believing it is real but also matches the input condition pixel-wise, often enforced with an additional L1 loss. This approach highlighted GANs' ability to capture complex, pixel-level correspondences, making them powerful tools for supervised image synthesis.\n\nHowever, the reliance of Pix2Pix on meticulously aligned training data posed a significant practical limitation, as such datasets are often scarce or impossible to acquire in real-world scenarios. To address this, \\textcite{Zhu2017} introduced CycleGAN, a groundbreaking method for unpaired image-to-image translation. CycleGAN ingeniously leverages a cycle consistency loss, which mandates that translating an image from domain A to domain B and then back to A should reconstruct the original image. This architectural innovation, involving two generators and two discriminators, enables effective translation between domains (e.g., horses to zebras, summer landscapes to winter landscapes, photographs to paintings) without requiring paired examples. The cycle consistency loss acts as a powerful self-supervisory signal, preventing the mapping from degenerating and ensuring semantic preservation during translation. This significantly broadened the applicability of image-to-image translation, democratizing its use for various style transfer, object transfiguration, and artistic rendering tasks.\n\nFollowing these foundational works, the field rapidly evolved to address more complex translation scenarios. A key advancement was the development of models capable of multi-domain image-to-image translation, moving beyond translating between just two specific domains. Approaches like G$^2$GAN \\cite{tang2018iie} introduced dual generator architectures to enable a single model to learn mappings across multiple target domains, improving scalability and reducing the need to train separate models for each domain pair. This was crucial for applications requiring flexible style transfer or attribute manipulation across a spectrum of visual styles or identities. Further research focused on disentangled representation learning, aiming to separate content from style in the latent space. While not explicitly covered by the provided papers, methods like MUNIT and DRIT allowed for more controllable and diverse translations by enabling users to combine content from one image with the style of another, offering fine-grained control over the generated output. Similarly, conditional image translation has been extended to high-resolution semantic synthesis, where models like GauGAN (SPADE) generate photorealistic images from semantic segmentation maps, showcasing the ability to interpret complex semantic layouts and produce highly detailed, controllable outputs.\n\nThe versatility of GANs in learning complex mappings, even without direct supervision in the unpaired case, underscores their profound utility across various computer vision applications \\cite{jabbar2020aj0}. Beyond artistic applications and style transfer, image-to-image translation has proven invaluable for data augmentation, particularly in data-scarce domains like medical imaging, where synthetic data can improve diagnostic model performance. It also facilitates tasks like image super-resolution, denoising, and inpainting, effectively acting as powerful image processing tools.\n\nDespite these significant strides, several challenges persist in domain adaptation and image-to-image translation. A primary concern is the trade-off between the fidelity and diversity of generated outputs; models often excel at one but struggle with the other. For instance, while cycle consistency helps preserve content, it can sometimes lead to the generator \"hiding\" information in steganographic patterns rather than truly learning the desired transformation, or it might struggle with large geometric changes between domains. Evaluating the perceptual realism and semantic consistency of translated images remains a complex problem, as traditional metrics often fail to capture the nuances of human perception. Furthermore, the interpretability of the learned mappings is often limited, making it difficult to understand *why* a particular translation occurs. Computational demands, especially for training high-resolution and multi-domain translation models, also remain a practical hurdle. Future research directions will likely focus on enhancing the control and interpretability of translation attributes, improving the robustness to diverse and challenging input conditions, and developing more sophisticated evaluation metrics that align better with human judgment. The integration of image-to-image translation with other advanced generative paradigms, such as diffusion models, also holds promise for overcoming current limitations in fidelity, diversity, and training stability.",
    "Bridging 2D GANs with 3D Neural Radiance Fields": "\\subsection*{Bridging 2D GANs with 3D Neural Radiance Fields}\n\nWhile Generative Adversarial Networks (GANs) have achieved remarkable success in synthesizing photorealistic 2D images, as extensively discussed in Sections 4.3 and 4.4 regarding the StyleGAN family, their inherent lack of explicit 3D understanding limits their utility for applications requiring consistent multi-view generation or controllable 3D scene manipulation. This subsection explores the innovative and rapidly evolving direction of extending the capabilities of these high-fidelity 2D GANs to 3D-aware image synthesis by integrating them with Neural Radiance Fields (NeRFs). This methodological progression addresses the critical challenge of creating controllable 3D content from powerful 2D generative models, leveraging existing 2D strengths for tasks like virtual reality, content creation, and 3D reconstruction. The combination offers the benefits of GAN's high-quality texture generation and disentangled control with NeRF's inherent 3D consistency and novel view synthesis capabilities.\n\nThe core limitation of 2D GANs lies in their inability to guarantee geometric consistency across different viewpoints, as their generative process is fundamentally image-centric. Simultaneously, Neural Radiance Fields (NeRFs) \\cite{Mildenhall2020} emerged as a powerful paradigm for novel view synthesis, representing 3D scenes as continuous volumetric functions. NeRFs excel at rendering photorealistic and geometrically consistent novel views, but typically require extensive multi-view image datasets for training and lack an intuitive, disentangled latent space for content manipulation akin to StyleGANs. The challenge, therefore, became how to combine the photorealism and latent space control of 2D GANs with the 3D consistency of NeRFs, ideally without requiring explicit 3D supervision.\n\nEarly efforts to bridge this gap focused on learning generative models that could produce implicit 3D scene representations from a latent code, which could then be rendered into 2D images. Generative Radiance Fields (GRAF) \\cite{Schwarz2020} was among the first to propose a GAN-based approach for learning 3D-aware image synthesis. GRAF trained a GAN to generate parameters for a NeRF-like scene representation, enabling the synthesis of multi-view consistent images from a single latent vector. While a significant conceptual step, GRAF often produced lower-resolution outputs and faced challenges in achieving the same level of disentanglement and photorealism as state-of-the-art 2D GANs. Following this, pi-GAN \\cite{Chan2021} further explored implicit neural representations for 3D-aware synthesis, demonstrating improved disentanglement and quality by leveraging a hierarchical latent space and a progressive training scheme. GIRAFFE \\cite{Niemeyer2021} advanced this by introducing a compositional scene representation, allowing for the disentanglement of object pose, shape, and appearance, and enabling the generation of scenes with multiple objects and backgrounds, further enhancing controllable 3D-aware synthesis. These initial works laid the groundwork by demonstrating the feasibility of learning 3D-aware generative models from 2D image collections.\n\nA pivotal development in achieving high-fidelity 3D-aware synthesis involved directly integrating the powerful StyleGAN architecture with NeRFs. StyleNeRF \\cite{Gu2021} was an early attempt to adapt StyleGAN's generator to produce features for a NeRF, allowing for high-resolution 3D-consistent image generation while leveraging StyleGAN's disentangled latent space for control. It demonstrated that the rich semantic information encoded in StyleGAN's latent space could be effectively transferred to control 3D scene properties.\n\nThe most significant breakthrough in this domain, however, came with Efficient Geometry-aware 3D Generative Adversarial Networks (EG3D) \\cite{Chan2022}. This landmark paper proposed a highly efficient and high-fidelity method for 3D-aware image synthesis by explicitly leveraging a StyleGAN2 backbone to generate a *tri-plane feature representation*. Instead of generating a full 3D volume, EG3D projects the latent code into three orthogonal 2D feature planes (XY, XZ, YZ). A lightweight neural renderer then queries these tri-planes at specific 3D coordinates and viewing directions to predict color and density, effectively reconstructing the 3D scene. This tri-plane representation is crucial because it factorizes the 3D problem into a more manageable set of 2D operations, significantly reducing computational cost and memory requirements compared to volumetric NeRFs, while retaining 3D consistency. The StyleGAN's W-space directly controls the features within these tri-planes, allowing for precise and disentangled manipulation of 3D geometry and appearance, such as changing facial attributes or expressions in a 3D-consistent manner. EG3D achieved unprecedented levels of photorealism and view consistency for 3D-aware face generation, setting a new state-of-the-art.\n\nThe integration of 2D GANs with NeRFs represents a powerful synergy. GANs contribute their ability to generate photorealistic textures and offer a highly disentangled latent space for intuitive control, while NeRFs provide the necessary 3D consistency and novel view synthesis capabilities. This combination has opened new avenues for applications in virtual reality, where consistent 3D environments are paramount, and for content creation pipelines that demand both high visual fidelity and intuitive 3D control. It also facilitates 3D reconstruction from limited 2D inputs by leveraging the strong generative priors encoded within the latent space.\n\nDespite these remarkable advancements, challenges persist. While EG3D significantly improved efficiency, training and inference for these hybrid models remain computationally intensive compared to purely 2D GANs, especially for very high resolutions or complex, diverse scenes beyond specific object categories like faces. Generalization to open-world scenes or highly diverse object classes, where the underlying 3D geometry is more varied, is still an active research area. Achieving perfect geometric accuracy and photorealism across *all* viewpoints, particularly for highly occluded or unseen parts, remains difficult due to the inherent ambiguity of learning 3D from purely 2D data. Furthermore, while disentanglement has improved, the latent space mapping to desirable 3D properties is not always perfectly orthogonal, leading to some entanglement between attributes. Future research directions include improving the robustness and generalizability of these models to more complex and diverse scenes, enhancing the resolution and realism of generated 3D content, and exploring more efficient training and inference mechanisms. Integrating explicit geometric priors or sparse 3D supervision could further improve geometric accuracy, and extending these methods to dynamic 3D scenes or incorporating other generative paradigms like diffusion models for 3D-aware synthesis are promising avenues.",
    "The Rise of Diffusion Models and Their Integration with GANs": "\\subsection*{The Rise of Diffusion Models and Their Integration with GANs}\n\nThe generative modeling landscape has witnessed a profound shift with the emergence of diffusion models as a powerful alternative to Generative Adversarial Networks (GANs). Originating from foundational works like Denoising Diffusion Probabilistic Models (DDPMs) \\cite{Ho2020} and score-based generative models \\cite{Song2020}, diffusion models have rapidly gained prominence due to their exceptional training stability, robust mode coverage, and capacity for generating high-quality samples \\cite{Karras2022, peng2024kkw}. This section explores the rise of diffusion models, their inherent advantages and limitations, and the recent, significant trend of integrating them with GANs to synthesize their respective strengths.\n\nDiffusion models operate by learning to reverse a gradual, iterative noising process. During training, noise is progressively added to data, and the model learns to predict and remove this noise at each step, effectively denoising data to generate new samples from pure noise \\cite{Ho2020}. This denoising autoencoder approach inherently offers greater training stability and superior mode coverage compared to the adversarial min-max game of GANs, which is often plagued by issues like mode collapse and vanishing gradients \\cite{peng2024kkw}. Theoretically, \"push-forward\" generative models like GANs, which synthesize data by transforming a standard Gaussian random variable using a deterministic neural network, face a provable trade-off between fitting multimodal distributions and maintaining training stability due to Lipschitz constant constraints. Diffusion models, conversely, with their stacked networks and stochastic input at each step, do not suffer from such limitations, explaining their superior ability to capture data diversity and their inherent stability \\cite{salmona202283g}. Consequently, diffusion models have demonstrated remarkable diversity and fidelity in generated outputs across various domains. However, a notable limitation of early diffusion models was their inherently slow sampling speed, requiring numerous sequential steps to produce a single high-quality sample, posing a challenge for real-time applications. This critical bottleneck has been significantly addressed by innovations such as Denoising Diffusion Implicit Models (DDIMs) \\cite{Song2020} and progressive distillation techniques \\cite{Karras2022b}, which substantially accelerate the sampling process while largely preserving the high quality of generated content.\n\nDespite these advancements in diffusion models, GANs retain distinct advantages, particularly their fast inference capabilities—generating samples in a single forward pass—and their propensity for producing exceptionally sharp, crisp details \\cite{peng2024kkw}. This recognition has spurred a significant conceptual shift towards hybrid generative architectures that aim to synthesize the strengths of both paradigms. This integrated approach seeks to leverage GANs' efficiency and detail generation with diffusion models' robust training and comprehensive mode coverage, thereby overcoming the individual limitations of each.\n\nOne prominent direction in this hybridization is the integration of adversarial training principles directly into diffusion models. Early efforts, such as Adversarial Score Matching \\cite{Xiao2021}, demonstrated that a discriminator could be employed to guide the score network in diffusion models. In this framework, the discriminator learns to distinguish between real data and samples generated by the diffusion process at various intermediate timesteps, providing an adversarial signal that helps refine the denoising process and improve sample quality. Building upon this, \\cite{Karras2023} introduced Adversarial Diffusion Models (ADM), which frame the diffusion process within an adversarial learning setup. ADM employs a discriminator to guide the denoising network, typically by evaluating the realism of the *intermediate denoised outputs* or the *predicted noise* at different stages of the reverse process. This adversarial guidance aims to harness the sharpness and efficiency benefits traditionally associated with GANs, enhancing the perceptual quality and potentially accelerating the sampling speed of diffusion models, moving beyond purely diffusion-based objectives.\n\nFurther solidifying this trend, explicit \"Diffusion-GANs\" architectures have emerged, aiming to achieve the best of both worlds. For instance, You Only Sample Once (YOSO) \\cite{luo2024znt} proposes a novel self-cooperative diffusion GAN designed for rapid, scalable, and high-fidelity one-step image synthesis with high training stability and mode coverage. YOSO addresses the challenges of training instability and subpar one-step generation efficiency in previous hybrid models by smoothing the adversarial divergence through the denoising generator itself. This \"self-cooperative learning\" mechanism, combined with techniques like latent perceptual loss, a latent discriminator for efficient training, informative prior initialization (IPI), and a quick adaptation stage, allows YOSO to train from scratch for one-step generation with competitive performance, even adapting to higher resolutions without explicit retraining. Such models exemplify the strategic integration of GANs' fast inference and crisp detail generation with diffusion models' robust training and superior mode coverage.\n\nWhile these hybrid models promise enhanced performance by combining complementary strengths, they also introduce new complexities and trade-offs. As noted by \\cite{peng2024kkw}, existing fusion methods can still suffer from \"training instability and mode collapse or subpar one-step generation learning efficiency,\" indicating that the optimal balance between adversarial dynamics and diffusion processes remains an active area of research. The increased architectural complexity and the intricate interplay of different loss functions can make these models challenging to tune and optimize, potentially requiring more computational resources or specialized training strategies.\n\nIn conclusion, the rise of diffusion models has set a new benchmark for generative quality and stability, while their subsequent integration with GANs marks a pivotal moment in generative AI. This ongoing hybridization effort signifies a strategic evolution towards developing models that are not only stable and diverse but also efficient and capable of producing highly detailed outputs. Future research will undoubtedly continue to explore novel ways to synergize these powerful paradigms, pushing the boundaries of what is possible in synthetic content generation by combining their complementary strengths while navigating the inherent challenges of complex, integrated architectures.",
    "Summary of Progress in GAN Stabilization": "\\subsection{Summary of Progress in GAN Stabilization}\n\nThe journey of Generative Adversarial Networks (GANs) has been marked by a relentless pursuit of stability, evolving from addressing initial training challenges to achieving high-fidelity, controllable synthesis across diverse applications. Early GANs, while groundbreaking, frequently suffered from training instability and mode collapse, where the generator failed to produce diverse samples \\cite{Goodfellow2014}. This fundamental problem necessitated systematic research into robust training methodologies and architectural innovations.\n\nInitial efforts focused on enhancing the stability of the adversarial training process. The introduction of Deep Convolutional GANs (DCGANs) by \\cite{Radford2015} provided architectural guidelines, leveraging convolutional layers to improve training stability and image quality. However, issues like mode collapse persisted due to the limitations of the original GAN loss function. A significant breakthrough came with the Wasserstein GAN (WGAN) \\cite{Arjovsky2017}, which proposed using the Earth-Mover distance as a loss function, offering a more stable gradient and mitigating mode collapse. This was further refined by \\cite{Gulrajani2017} with Wasserstein GAN with Gradient Penalty (WGAN-GP), which enforced a Lipschitz constraint through gradient penalties, leading to even more robust and stable training. Complementary to these loss function advancements, regularization techniques also played a crucial role. \\cite{roth2017eui} proposed a low-computational-cost regularization approach to stabilize GAN training, specifically addressing issues arising from dimensional mismatch or non-overlapping support between distributions. Similarly, \\cite{Miyato2018} introduced Spectral Normalization for GANs, a simple yet effective method to stabilize training by controlling the Lipschitz constant of the discriminator, further preventing pathological gradients.\n\nWith a more stable training foundation, the intellectual trajectory shifted towards achieving unprecedented levels of image fidelity and control. This era was largely defined by the StyleGAN family of architectures. \\cite{Karras2019} introduced StyleGAN, a style-based generator architecture that leveraged a mapping network and AdaIN layers to produce highly disentangled and controllable latent spaces, leading to state-of-the-art image synthesis. Subsequent iterations, StyleGAN2 \\cite{Karras2020} and StyleGAN3 \\cite{Karras2021}, further refined the architecture with advancements like path length regularization and alias-free design, pushing 2D image quality to near-photorealistic levels and addressing persistent visual artifacts. This mastery of 2D synthesis then opened doors to new frontiers, with \\cite{Chan2023} *H* demonstrating how StyleGAN's disentangled latent spaces could be integrated with Neural Radiance Fields (NeRFs) to enable high-quality 3D-aware image synthesis and novel view generation, effectively extending GAN capabilities into coherent 3D scene representation.\n\nSimultaneously, research expanded into scaling, efficiency, and data-agnostic applications. \\cite{Brock2018} pioneered large-scale GAN training with BigGAN, demonstrating the ability to synthesize high-fidelity images from diverse datasets like ImageNet. Training efficiency was further improved by \\cite{Sauer2021} with Projected GANs, which accelerated convergence. A critical practical challenge, the need for vast amounts of training data, was addressed by \\cite{Karras2022} through Adaptive Discriminator Augmentation (ADA), allowing GANs to be trained effectively with limited data. Building on this, \\cite{Sauer2023} scaled the StyleGAN architecture to handle large, diverse datasets with StyleGAN-XL, while \\cite{Sauer2024} unlocked text-to-image synthesis capabilities with StyleGAN-T, adapting GANs for fine-grained conditional generation. Pushing the boundaries of data efficiency even further, \\cite{Wang2023} *H* introduced a meta-learning approach for the discriminator, enabling it to quickly adapt to new datasets with very few samples, significantly reducing data requirements beyond what ADA could achieve.\n\nThe latest intellectual trajectory reveals an emerging trend of convergence and hybridization with other powerful generative paradigms. While GANs excelled in fast inference and high fidelity, challenges like mode coverage and training stability, particularly compared to diffusion models, persisted. Addressing this, \\cite{Liu2024} *H* proposed Diffusion-GAN, a novel hybrid generative model that combines the adversarial training of GANs with the denoising process of diffusion models. This innovative approach aims to leverage the strengths of both paradigms, seeking to achieve the fast inference of GANs alongside the enhanced stability and mode coverage characteristic of diffusion models.\n\nIn summary, systematic research has transformed GANs from a fragile, experimental concept into a robust, versatile, and highly performant class of generative models. The journey from addressing initial instability and mode collapse through robust loss functions and regularization, to achieving high-fidelity, controllable synthesis via architectural innovations, and expanding into data-efficient, multi-modal, and 3D applications, underscores the field's capacity for continuous innovation. This evolution, now embracing hybridization with other generative models, marks GANs as powerful tools capable of diverse and complex tasks, significantly contributing to the broader landscape of generative AI.",
    "Remaining Theoretical and Practical Challenges": "\\subsection*{Remaining Theoretical and Practical Challenges}\n\nDespite the remarkable advancements in Generative Adversarial Networks (GANs), particularly in synthesizing high-fidelity and diverse content, the field continues to grapple with fundamental theoretical and practical challenges that limit their robustness, usability, and widespread adoption. These unresolved issues represent critical avenues for future research and development.\n\nA primary theoretical challenge revolves around the elusive nature of **convergence guarantees** for GAN training. The adversarial min-max game, while powerful, inherently creates a non-convex, non-cooperative optimization problem that is notoriously difficult to stabilize. Early GANs \\cite{goodfellow2014generative} were plagued by instability, vanishing gradients, and oscillations. While subsequent works have introduced various regularization techniques and architectural improvements, a complete theoretical understanding of global convergence to a unique Nash equilibrium remains an open problem. For instance, \\textcite{roth2017eui} highlighted the \"dimensional mismatch or non-overlapping support\" between the model and data distributions as a source of instability, leading to undefined density ratios. The introduction of Wasserstein GANs \\cite{arjovsky2017ze5} aimed to provide a more meaningful and stable loss function, addressing issues like vanishing gradients and offering theoretical benefits. However, even improved variants like WGAN-GP \\cite{gulrajani2017improved} require careful tuning of the gradient penalty coefficient, demonstrating that practical stability often relies on empirical adjustments rather than robust theoretical guarantees. Similarly, spectral normalization \\cite{miyato2018spectral} offers an efficient way to enforce Lipschitz continuity, improving stability, but it is a regularization technique rather than a fundamental solution to the non-convergent game dynamics.\n\nAnother persistent theoretical hurdle is **mode collapse**, where the generator fails to capture the full diversity of the real data distribution, instead producing a limited subset of samples. This issue is particularly pronounced in highly complex, multi-modal, or long-tail data distributions. \\textcite{che2016kho} explicitly addressed this, noting that GANs are \"prone to miss modes\" and proposed regularization methods to encourage a \"fair distribution of probability mass across the modes.\" While various techniques, including architectural changes \\cite{radford2015unsupervised} and loss function modifications \\cite{arjovsky2017ze5}, have aimed to mitigate mode collapse, it remains a significant concern, especially when training on large, diverse datasets like ImageNet, where the generator might prioritize generating common, high-quality samples over exploring rare but valid modes. The challenge is exacerbated by the difficulty of objectively quantifying mode coverage.\n\nThis leads to the third major theoretical challenge: the **difficulty of objective evaluation metrics** beyond FID (Fréchet Inception Distance) and IS (Inception Score). While FID and IS are widely adopted, they possess inherent limitations. They often rely on pre-trained classifiers (like InceptionNet), which may not be robust to out-of-distribution samples or perfectly align with human perception. Furthermore, they can be sensitive to sample size and may not comprehensively capture all aspects of image quality and diversity, particularly mode coverage. The lack of a universally accepted, robust, and interpretable metric makes it challenging to objectively compare different GAN models, track progress, and definitively determine when a model has achieved optimal performance across both fidelity and diversity.\n\nFrom a practical standpoint, GANs present several significant challenges. The **high computational resource demands** for training large models are a major barrier. Achieving state-of-the-art results, such as those demonstrated by BigGAN \\cite{brock2018large} for high-fidelity natural image synthesis, necessitated \"massive computational scale,\" including hundreds of GPUs and extensive training times. This limits accessibility for researchers and practitioners without substantial computational budgets, hindering rapid iteration and experimentation.\n\nClosely related is the **sensitivity to hyperparameter tuning**. GANs are notoriously finicky, requiring meticulous selection of learning rates, batch sizes, network architectures, and regularization coefficients. As noted by \\textcite{roth2017eui}, their fragility demands a \"careful choice of architecture, parameter initialization, and selection of hyper-parameters.\" Even advanced techniques like WGAN-GP \\cite{gulrajani2017improved} introduce new hyperparameters (e.g., the gradient penalty coefficient) that require careful calibration, often through extensive and costly trial-and-error. This empirical burden makes GAN training a highly specialized skill rather than a straightforward process.\n\nFinally, the **difficulty of training on highly diverse, real-world datasets** persists. While models like StyleGAN-XL \\cite{sauer2023stylegan} have pushed the boundaries of scaling StyleGAN to ImageNet-scale diversity, achieving both high fidelity and comprehensive mode coverage on such complex datasets remains a formidable task. The inherent diversity of real-world data often exacerbates mode collapse and training instability. Furthermore, many real-world applications involve limited data scenarios, which GANs traditionally struggle with. While techniques like Adaptive Discriminator Augmentation (ADA) \\cite{karras2022training} have made significant strides in enabling GAN training with limited data, the problem of few-shot or zero-shot generation on highly diverse distributions remains largely open.\n\nIn conclusion, despite their transformative impact, GANs are far from a \"solved problem.\" The fundamental theoretical questions surrounding convergence and comprehensive mode coverage, coupled with practical hurdles related to computational cost, hyperparameter sensitivity, and robust training on diverse real-world data, highlight critical areas ripe for future investigation. Addressing these challenges will be crucial for enhancing the robustness, usability, and theoretical grounding of generative adversarial models, potentially through novel architectural designs, improved optimization strategies, or hybrid approaches that integrate insights from complementary generative paradigms.",
    "Ethical Considerations and Societal Impact": "\\subsection*{Ethical Considerations and Societal Impact}\n\nThe remarkable advancements in Generative Adversarial Networks (GANs) have ushered in a new era of synthetic media generation, presenting a complex ethical landscape marked by both profound opportunities and significant risks. As GANs evolve from foundational models to highly capable architectures capable of photorealistic and 3D-aware synthesis, the broader societal implications demand rigorous scrutiny, moving beyond mere technical capabilities to address issues of trust, fairness, and accountability \\cite{bhat202445j}.\n\nA primary ethical concern revolves around the potential for misuse, particularly the generation and dissemination of \"deepfakes\" and misinformation. The increasing fidelity and disentangled control offered by architectures like the StyleGAN family \\cite{Karras2019, Karras2020, Karras2021} have made it possible to create highly convincing synthetic media that can misrepresent individuals, manipulate public opinion, and orchestrate sophisticated misinformation campaigns. This capability extends beyond 2D images, with the integration of StyleGAN latents with Neural Radiance Fields (NeRFs) enabling 3D-aware synthesis \\cite{Chan2023}, further blurring the lines between reality and simulation in immersive contexts. Such technological prowess contributes to what scholars term the \"liar's dividend,\" where the very existence of highly realistic synthetic media erodes public trust in *all* digital content, including authentic media, making it harder to discern truth from fabrication \\cite{chesney2019deepfakes}. The accessibility of generating specific content through text-to-image models, such as StyleGAN-T \\cite{Sauer2024}, further lowers the barrier for creating targeted disinformation or hate speech imagery, posing substantial challenges for content moderation, legal frameworks, and societal cohesion. The urgent need for robust detection mechanisms for synthetic media is paramount to counteract these threats, though the arms race between generation and detection remains a persistent challenge.\n\nAnother critical ethical dimension is the amplification and perpetuation of biases inherent in training data. While efforts to scale GANs to diverse datasets \\cite{Sauer2023} and improve data efficiency \\cite{Karras2022} are vital for broader applicability, they simultaneously highlight the risk of exacerbating societal inequalities. If training datasets reflect existing biases—such as underrepresentation of certain demographics, stereotypical portrayals, or historical inequities—GANs, even those employing advanced techniques like few-shot learning via meta-learning discriminators \\cite{Wang2023}, can inadvertently learn and amplify these biases in their generated outputs. This can lead to discriminatory outcomes, including biased facial recognition systems, misrepresentation in synthetic media, or the generation of content that reinforces harmful stereotypes. Addressing this requires not only careful dataset curation but also the development and implementation of rigorous bias auditing and mitigation strategies throughout the model lifecycle, from data collection to deployment, ensuring transparency and accountability in generative AI systems \\cite{bhat202445j}.\n\nDespite these substantial risks, the societal impact of highly capable generative models also encompasses immense positive potential, particularly in addressing real-world challenges. GANs have emerged as powerful creative tools for artists and designers, enabling novel forms of digital art and content creation by offering intuitive control over image synthesis \\cite{Karras2019}. More critically, in domains where data scarcity is a significant bottleneck, GANs provide a vital solution through high-fidelity data augmentation. For instance, in the medical field, the lack of annotated datasets for rare skin conditions poses a major challenge for diagnostic model development. Deep Generative Adversarial Networks (DGANs) have been successfully employed to generate synthetic skin problem images, effectively augmenting imbalanced datasets and significantly improving the diagnostic accuracy of multi-class classifiers, outperforming traditional augmentation methods \\cite{khan20223o7}. Similarly, in disaster response, where labeled imagery data is often limited and imbalanced, GANs have been utilized to synthesize diverse disaster images, thereby enhancing the training of deep convolutional neural networks for rapid damage identification and classification, leading to more efficient aid direction and resource allocation \\cite{eltehewy2023cj4}. These applications demonstrate how the enhanced stability and quality achieved through GAN research can directly translate into tangible societal benefits, improving model robustness and expanding the applicability of AI in critical sectors.\n\nIn conclusion, the trajectory of generative models, from initial stabilization to sophisticated 3D-aware and text-conditional synthesis, underscores an urgent need for responsible development and deployment. Mitigating harm and maximizing benefit necessitates a multi-faceted approach. This includes not only the continuous development of robust detection mechanisms for synthetic media but also the implementation of rigorous bias auditing and mitigation strategies in model training. Furthermore, the establishment of comprehensive ethical guidelines and policy frameworks for the use of GAN technologies is crucial to navigate the complex interplay between technological innovation and societal well-being. Balancing the transformative power of these models with foresight and a commitment to ethical principles is paramount to safeguarding societal trust and equity in the digital age.",
    "Future Research Directions": "\\subsection*{Future Research Directions}\n\nWhile significant strides have been made in stabilizing Generative Adversarial Networks (GANs) through foundational techniques like regularization \\cite{roth2017eui} and architectural innovations, the field continues to evolve rapidly, opening numerous promising avenues for future research. These directions aim to push the boundaries of generative AI, addressing its inherent complexities while expanding its utility and impact.\n\nOne particularly fertile ground for innovation lies in the further exploration of \\textbf{hybrid models} that combine the strengths of GANs with other powerful generative paradigms, such as diffusion models or transformer architectures. The inherent efficiency of GAN inference, coupled with the superior stability and mode coverage of diffusion models, presents a compelling synergy. This hybridization is exemplified by \\cite{Liu2024}, which introduces \"Diffusion-GAN\" to bridge these two frameworks, aiming to achieve enhanced stability and quality. Future work can build upon this by exploring more sophisticated integration strategies, potentially incorporating transformer-based components for improved contextual understanding and long-range dependency modeling, especially for complex multimodal generation tasks that span images, text, audio, and beyond.\n\nA critical challenge for widespread adoption remains the substantial data requirements of GANs. Therefore, advancements in \\textbf{few-shot and zero-shot generation} are paramount to reduce data dependency. Building upon techniques for limited data training, such as Adaptive Discriminator Augmentation \\cite{Karras2022} (as discussed in the evolution analysis), \\cite{Wang2023} introduces a meta-learning approach for discriminators to quickly adapt to new datasets with very few samples. This significantly reduces the need for extensive annotated data, paving the way for GANs to be deployed in data-scarce domains. Future research should focus on developing more robust meta-learning algorithms, exploring novel transfer learning strategies, and investigating how to leverage pre-trained models more effectively for truly zero-shot generation capabilities.\n\nThe inherent efficiency of GAN inference positions them ideally for the development of \\textbf{real-time and interactive generative systems}. As GAN architectures become more refined and computationally optimized, the potential for immediate visual feedback and dynamic content creation grows. Further pushing the boundaries of modality expansion, \\cite{Chan2023} demonstrates how StyleGAN's disentangled latent space can be integrated with Neural Radiance Fields (NeRFs) to enable high-quality 3D-aware image synthesis and novel view generation. This capability is a crucial step towards interactive 3D content creation and virtual environments. Future work should focus on optimizing these systems for even lower latency, enabling seamless user interaction, and expanding into other modalities like real-time audio synthesis or interactive video generation.\n\nBeyond current applications, expanding GANs into \\textbf{new modalities and applications beyond images} is a key future direction. While significant progress has been made in image synthesis, the principles of adversarial training can be applied to diverse data types. The success of 3D-aware generation \\cite{Chan2023} and text-to-image synthesis \\cite{Sauer2024} (as highlighted in the evolution analysis) illustrates this potential. Future research could explore GANs for generating complex scientific data, medical images, molecular structures, or even code, opening up entirely new application domains.\n\nCrucially, the development of more \\textbf{robust and interpretable evaluation metrics} remains paramount. Current metrics often fall short in capturing the perceptual quality, diversity, and fidelity of generated content, especially as models become more sophisticated. Future work must focus on creating metrics that are not only quantitative but also align better with human perception and can provide actionable insights into model shortcomings. Furthermore, as generative AI becomes more powerful, ensuring its \\textbf{responsible deployment} is non-negotiable. This includes addressing biases in generated content, developing methods for detecting AI-generated media, ensuring transparency, and establishing ethical guidelines for their use.\n\nIn conclusion, the future of GAN research is characterized by a drive towards greater versatility, efficiency, and integration. By embracing hybrid architectures, minimizing data dependency, enabling real-time interaction, expanding into new modalities, and prioritizing responsible deployment alongside robust evaluation, the field can truly unlock the full potential of generative AI, pushing the boundaries of what these complex systems can achieve."
  },
  "latex_document": "\\documentclass[12pt,a4paper]{article}\n    \\usepackage[utf8]{inputenc}\n    \\usepackage[T1]{fontenc}\n    \\usepackage{amsmath,amsfonts,amssymb}\n    \\usepackage{graphicx}\n    \\usepackage[margin=2.5cm]{geometry}\n    \\usepackage{setspace}\n    \\usepackage{natbib}\n    \\usepackage{url}\n    \\usepackage{hyperref}\n    \\usepackage{booktabs}\n    \\usepackage{longtable}\n    \\usepackage{array}\n    \\usepackage{multirow}\n    \\usepackage{wrapfig}\n    \\usepackage{float}\n    \\usepackage{colortbl}\n    \\usepackage{pdflscape}\n    \\usepackage{tabu}\n    \\usepackage{threeparttable}\n    \\usepackage{threeparttablex}\n    \\usepackage[normalem]{ulem}\n    \\usepackage{makecell}\n    \\usepackage{xcolor}\n\n    % Set line spacing\n    \\doublespacing\n\n    % Configure hyperref\n    \\hypersetup{\n        colorlinks=true,\n        linkcolor=blue,\n        filecolor=magenta,      \n        urlcolor=cyan,\n        citecolor=red,\n    }\n\n    % Title and author information\n    \\title{A Comprehensive Literature Review with Self-Reflection}\n    \\author{Literature Review}\n    \\date{\\today}\n\n    \\begin{document}\n\n    \\maketitle\n\n    % Abstract (optional)\n    \\begin{abstract}\n    This literature review provides a comprehensive analysis of recent research in the field. The review synthesizes findings from 194 research papers, identifying key themes, methodological approaches, and future research directions.\n    \\end{abstract}\n\n    \\newpage\n    \\tableofcontents\n    \\newpage\n\n    \\label{sec:introduction}\n\n\\section{Introduction}\n\\label{sec:introduction}\n\n\\subsection{The Promise of Generative Adversarial Networks}\n\\label{sec:1\\_1\\_the\\_promise\\_of\\_generative\\_adversarial\\_networks}\n\nThe introduction of Generative Adversarial Networks (GANs) by Goodfellow et al. in 2014 \\cite{goodfellow2014generative} heralded a transformative era in artificial intelligence, particularly within generative modeling. This novel framework immediately captivated the research community by proposing a unique adversarial training paradigm that promised unprecedented capabilities in data synthesis and unsupervised learning \\cite{jabbar2020aj0, bhat202445j}. At its core, a GAN comprises two competing neural networks: a generator (G) and a discriminator (D). The generator's objective is to learn the underlying distribution of real data and produce synthetic samples that are indistinguishable from authentic ones. Concurrently, the discriminator's role is to become highly proficient at differentiating between real data samples and those fabricated by the generator. This dynamic, zero-sum game, where both networks iteratively refine their strategies, enables the generator to progressively synthesize novel, high-quality content without explicit programming of features or rules \\cite{goodfellow2014generative}.\n\nThis implicit learning of complex data distributions represented a significant departure from prior generative models, fundamentally reshaping the landscape of generative AI. Before GANs, models like Variational Autoencoders (VAEs) and autoregressive models were prominent. While VAEs offered a probabilistic framework for latent space representation, they often struggled to produce perceptually sharp and realistic samples, frequently yielding blurry outputs due to their reliance on reconstruction loss and explicit density modeling \\cite{goyal2024ufg, salmona202283g}. Autoregressive models, on the other hand, could generate high-quality samples but suffered from slow, sequential generation processes. GANs, by contrast, leveraged their adversarial objective to directly push for high fidelity and realism, aiming for generated samples that could fool a sophisticated discriminator. This ability to generate sharp, coherent, and seemingly authentic data was a key driver of the initial excitement, showcasing a profound capability for creative and analytical tasks that was previously unattainable \\cite{wang2019w53}.\n\nThe broad potential for generating novel, high-quality content across diverse domains was immediately apparent and widely discussed. A primary aspiration was photorealistic image synthesis, where GANs could create entirely new faces, landscapes, or objects that were virtually indistinguishable from real photographs \\cite{pieters2018jh1, wang2019w53}. This capability opened doors for applications in digital art, entertainment, and virtual reality, offering tools for artists and designers to generate complex visual content with unprecedented ease. Beyond creative endeavors, GANs demonstrated significant promise in analytical applications, such as data augmentation for scientific research \\cite{wang2019w53}. In fields like medical imaging or specialized industrial applications, where acquiring large, diverse, and annotated datasets is often costly, time-consuming, or ethically challenging, GANs offered a powerful solution to generate synthetic data. This synthetic data could then be used to expand training sets, improve the robustness of downstream machine learning models, and accelerate discoveries in areas like disease diagnosis or material science \\cite{goyal2024ufg}. Furthermore, the framework showed potential for tasks like image-to-image translation, facial attribute manipulation, and style transfer, highlighting its versatility in learning complex mappings between visual domains \\cite{wang2019w53}.\n\nIn conclusion, the initial conception of Generative Adversarial Networks presented a revolutionary approach to unsupervised learning, characterized by its unique adversarial training paradigm. The excitement stemmed from their unprecedented ability to implicitly learn and reproduce the complexities of real-world data, promising a future where AI could be a true partner in creativity, scientific discovery, and diverse analytical tasks. However, this profound promise was immediately tempered by the inherent difficulties of optimizing the adversarial minimax game. The delicate balance required for stable training, coupled with the non-convex nature of the objective function, led to early recognition of significant challenges such as vanishing gradients, mode collapse, and general training instability \\cite{goodfellow2014generative, jabbar2020aj0, bhat202445j, chu2020zbv, salmona202283g}. These fundamental problems, present from GANs' inception, would soon become the central focus of extensive research, driving the field towards developing robust stabilization techniques.\n\\subsection{The Central Challenge: Training Instability}\n\\label{sec:1\\_2\\_the\\_central\\_challenge:\\_training\\_instability}\n\nDespite their revolutionary potential in generative modeling, Generative Adversarial Networks (GANs) are fundamentally characterized by profound training instabilities. This inherent difficulty, widely acknowledged across the literature \\cite{jabbar2020aj0, wiatrak20194ib, chu2020zbv}, manifests primarily as unreliable convergence, oscillating performance, and specific failure modes. These issues stem directly from the delicate, non-cooperative nature of the adversarial min-max game, where a generator ($G$) and a discriminator ($D$) are simultaneously optimized. This adversarial dynamic makes GANs exceptionally sensitive to a multitude of factors, including hyperparameter choices, network architectures, and initialization strategies, frequently leading to suboptimal, often uninterpretable, generated outputs \\cite{wang2019w53}. This central challenge has not only defined much of the research trajectory in the field but also underscores the critical need for robust stabilization techniques, highlighting the core problem that this review addresses.\n\nThe core of GAN instability lies in the complex dynamics of their two-player game. Unlike traditional optimization problems that aim to minimize a single loss function towards a stable minimum, GANs involve a continuous competition to find a Nash equilibrium. This minimax objective often lacks a unique, stable equilibrium, or if one exists, it is notoriously difficult to reach through standard gradient-based optimization methods \\cite{liang2018r52, grnarova20171tc}. The non-convex nature of the GAN objective, combined with the continuous interplay where one network's improvement alters the other's optimal strategy, often leads to complex dynamics such as limit cycles or rotational behavior in the parameter space rather than stable convergence \\cite{gonzlezprieto20214wh, chu2020zbv}. This results in training curves that frequently oscillate wildly, with generated sample quality fluctuating significantly throughout the training process. Such erratic behavior makes GANs notoriously sensitive to hyperparameter choices, such as the relative learning rates for the generator and discriminator, batch sizes, and optimizer configurations \\cite{xiang20171at}. Even slight deviations from optimal settings can lead to divergence, poor quality samples, or a complete failure to train, making the process of finding a stable configuration a significant practical hurdle.\n\nThis instability manifests in well-documented failure modes that severely limit GANs' utility. Most notably, these include vanishing gradients, where the generator ceases to receive meaningful learning signals, and mode collapse, where the generator produces only a limited variety of samples, failing to capture the full diversity of the real data distribution \\cite{goodfellow2014generative, salimans2016improved, arjovsky2017wasserstein}. These persistent issues, which plagued early architectures and continue to challenge complex models, represent critical symptoms of the underlying optimization difficulties and will be examined in detail in Subsection 2.3 after the foundational GAN concepts are established.\n\nBeyond these fundamental algorithmic challenges, the practical process of debugging and evaluating unstable GANs is notoriously complex and time-consuming. Unlike supervised learning where validation loss or accuracy directly correlates with model performance, GAN loss values often do not reliably indicate the perceptual quality of generated outputs \\cite{wenzel20225g3, jabbar2020aj0}. A decreasing generator loss might not signify better samples, and an increasing discriminator loss could be a sign of either effective training (discriminator being fooled) or a failing generator. This disconnect forces researchers and practitioners to rely heavily on subjective visual inspection of generated samples, a process that is both labor-intensive and prone to misinterpretation. Without a clear, quantitative signal for convergence or quality, determining when to stop training, comparing different models, or diagnosing the root cause of poor performance becomes a significant practical challenge, further emphasizing the urgency of effective stabilization methods \\cite{karras2017raw}.\n\nIn summary, the training instability of GANs, encompassing unreliable convergence, extreme sensitivity to configuration, the difficulty of debugging due to uninformative metrics, and specific failure modes like vanishing gradients and mode collapse, is not merely a practical inconvenience but a fundamental theoretical and algorithmic challenge. These issues directly impede the ability of GANs to reliably learn complex, high-dimensional data distributions, generate diverse and high-quality samples, and achieve stable training. The persistent nature of these problems has been the primary impetus for extensive research into more robust theoretical frameworks, novel loss functions, and advanced regularization techniques, driving the evolution of the field towards more stable and effective generative models.\n\n\n\\label{sec:foundational_concepts_and_early_challenges_of_gans}\n\n\\section{Foundational Concepts and Early Challenges of GANs}\n\\label{sec:foundational\\_concepts\\_\\_and\\_\\_early\\_challenges\\_of\\_gans}\n\n\\subsection{The Original GAN Framework: Adversarial Minimax Game}\n\\label{sec:2\\_1\\_the\\_original\\_gan\\_framework:\\_adversarial\\_minimax\\_game}\n\nThe seminal work by \\cite{goodfellow2014generative} introduced Generative Adversarial Networks (GANs), a groundbreaking framework that recast generative modeling as a dynamic, zero-sum game between two competing neural networks. This innovative paradigm immediately captured significant attention for its potential to synthesize highly realistic data, particularly images, by learning complex data distributions implicitly \\cite{jabbar2020aj0, bhat202445j}. The foundational understanding of this original framework is crucial for appreciating the subsequent extensive research aimed at stabilizing its delicate adversarial balance and overcoming its inherent optimization challenges.\n\nAt its core, the original GAN architecture comprises two distinct neural networks: a generator ($G$) and a discriminator ($D$). The generator's primary function is to learn a mapping from a simple prior noise distribution $p\\_z(z)$ (typically a uniform or Gaussian distribution) to the intricate real data distribution $p\\_{data}(x)$. Through this mapping, the generator produces synthetic samples, denoted as $G(z)$, that aim to be indistinguishable from authentic data. Conversely, the discriminator acts as a binary classifier, tasked with differentiating between real samples drawn directly from $p\\_{data}(x)$ and fake samples generated by $G$. This adversarial interplay is conceptualized as a minimax game, where the discriminator strives to maximize its classification accuracy, while the generator simultaneously endeavors to minimize the discriminator's ability to discern between real and fake, thereby producing increasingly convincing synthetic data.\n\nThe objective function for this adversarial game is mathematically defined as:\n$$ \\min\\_G \\max\\_D V(D, G) = \\mathbb{E}\\_{x \\sim p\\_{data}(x)}[\\log D(x)] + \\mathbb{E}\\_{z \\sim p\\_z(z)}[\\log(1 - D(G(z)))] $$\nDuring the training process, the discriminator $D$ is optimized to maximize $V(D,G)$. This entails learning to assign high probabilities to real data samples ($D(x) \\approx 1$) and low probabilities to generated samples ($D(G(z)) \\approx 0$). Concurrently, the generator $G$ is updated to minimize $V(D,G)$, which is equivalent to maximizing $\\mathbb{E}\\_{z \\sim p\\_z(z)}[\\log D(G(z))]$. This objective compels $G$ to produce samples $G(z)$ for which $D(G(z))$ approaches $1$, effectively making its outputs appear real to the discriminator. Theoretically, this adversarial game converges to a unique Nash equilibrium. At this equilibrium, the generator perfectly replicates the real data distribution ($p\\_g = p\\_{data}$), and the discriminator outputs $0.5$ for all inputs, signifying its inability to distinguish between real and fake samples.\n\nA critical theoretical implication of this original formulation is that, at the global optimum, the objective function corresponds to minimizing the Jensen-Shannon Divergence (JSD) between the real data distribution ($p\\_{data}$) and the generated data distribution ($p\\_g$). While JSD is a symmetric and bounded measure of similarity between probability distributions, its properties proved to be a significant source of practical instability in GAN training \\cite{jabbar2020aj0}. Specifically, when the supports of $p\\_{data}$ and $p\\_g$ are disjoint or have negligible overlap—a common occurrence, especially in high-dimensional data spaces like images—the JSD becomes a constant value. In such scenarios, the gradients of the discriminator with respect to the generator's parameters can vanish, providing little to no meaningful learning signal to the generator. This vanishing gradient problem makes it exceedingly difficult for the generator to learn effectively, particularly during the early stages of training when $p\\_g$ is far from $p\\_{data}$.\n\nBeyond vanishing gradients, the delicate balance inherent in the adversarial minimax game often led to training instability and convergence issues \\cite{bhat202445j}. The competitive nature meant that if one network became too powerful too quickly, the training process could derail. For instance, an overly strong discriminator could consistently output $0$ or $1$ for generated samples, leading to saturated gradients for the generator. Conversely, a weak discriminator might provide an insufficient learning signal, allowing the generator to produce poor-quality samples without significant penalty. This imbalance could manifest as oscillations in performance or, more critically, as mode collapse, where the generator produces only a limited variety of samples, failing to capture the full diversity of the real data distribution \\cite{jabbar2020aj0}. The theoretical analysis of GAN dynamics, such as that by \\cite{gonzlezprieto20214wh}, further elucidates why the training process is inherently unstable; they show that convergent orbits in GANs are often small perturbations of periodic orbits, implying that Nash equilibria can act as spiral attractors, which theoretically justifies the observed slow and unstable training.\n\nIn summary, the original GAN framework by Goodfellow et al. was a revolutionary contribution, offering a powerful new paradigm for generative modeling. However, its reliance on the Jensen-Shannon Divergence as the underlying objective function, coupled with the inherent competitive dynamics of the minimax game, immediately exposed fundamental challenges. These included the pervasive problem of vanishing gradients when data distributions had non-overlapping supports, leading to training instability and the notorious issue of mode collapse. These initial theoretical and practical difficulties underscored the need for significant advancements in objective functions, architectural designs, and training methodologies, setting the stage for the extensive research that followed to stabilize and enhance generative adversarial models.\n\\subsection{Early Architectural Guidelines: Deep Convolutional GANs (DCGANs)}\n\\label{sec:2\\_2\\_early\\_architectural\\_guidelines:\\_deep\\_convolutional\\_gans\\_(dcgans)}\n\nThe initial formulation of Generative Adversarial Networks (GANs) \\cite{Goodfellow2014} presented a powerful theoretical framework for generative modeling, but their practical implementation was plagued by significant training instability and difficulties in convergence, often producing incoherent or limited-diversity outputs. A crucial methodological progression towards making GANs a more implementable framework was the introduction of Deep Convolutional Generative Adversarial Networks (DCGANs) by \\cite{Radford2015}.\n\nDCGANs marked the first significant step towards practical GANs by effectively integrating Convolutional Neural Networks (CNNs) into both the generator and discriminator architectures. This integration leveraged the hierarchical feature learning capabilities of CNNs, enabling the generation of more coherent and visually plausible images compared to earlier fully-connected architectures. Beyond simply using CNNs, \\cite{Radford2015} introduced a set of architectural heuristics that provided initial stability to GAN training and enabled the generation of more coherent images, marking a crucial methodological progression from the abstract GAN concept to a more implementable framework.\n\nSeveral key architectural guidelines were established. To address training instability and facilitate deeper networks, batch normalization layers were introduced in both the generator and discriminator. Batch normalization helps stabilize learning by normalizing the input to each layer, preventing internal covariate shift and allowing for higher learning rates, which was vital for the deeper convolutional structures. Specific activation functions were also prescribed: ReLU (Rectified Linear Unit) was predominantly used in the generator for all layers except the output, which typically used Tanh to produce pixel values in a normalized range. For the discriminator, LeakyReLU was employed, providing a non-zero gradient for negative inputs and helping to prevent 'dying ReLU' problems, thereby contributing to better gradient flow and more stable adversarial training.\n\nA pivotal architectural choice was the avoidance of pooling layers in favor of strided convolutions. In the generator, fractional-strided convolutions (often referred to as transposed convolutions) were used for spatial upsampling, allowing the network to learn its own upsampling strategy rather than relying on fixed interpolation. Conversely, the discriminator utilized strided convolutions for spatial downsampling. This approach allowed the network to learn more effective spatial transformations, preserving more information and often leading to better image quality than traditional pooling operations.\n\nThese architectural heuristics provided initial stability to GAN training, moving the field from an abstract concept to a more robust and implementable framework. The structured use of CNNs and the proposed architectural choices enabled DCGANs to generate images with significantly improved visual quality and coherence, demonstrating the potential of GANs for unsupervised representation learning. For instance, \\cite{Radford2015} showed that the learned features in the discriminator could be effectively used for classification tasks, and that latent space arithmetic could produce meaningful semantic manipulations in generated images, such as interpolating between gender or expressions.\n\nDespite these advancements, DCGANs still faced limitations. While stability was improved, training remained sensitive to hyperparameter choices and could still suffer from issues like mode collapse, where the generator produces a limited variety of samples. The resolution of generated images was also relatively modest compared to later advancements. Thus, while DCGANs established fundamental architectural principles for deep generative models and showcased the immense potential of GANs, their inherent challenges in achieving consistent stability and scaling to higher resolutions laid the groundwork for subsequent research into more robust training methodologies and advanced architectures.\n\\subsection{Persistent Problems: Vanishing Gradients and Mode Collapse}\n\\label{sec:2\\_3\\_persistent\\_problems:\\_vanishing\\_gradients\\_\\_and\\_\\_mode\\_collapse}\n\nDespite the initial promise of Generative Adversarial Networks (GANs) \\cite{goodfellow2014generative}, early architectures were plagued by significant training instabilities, primarily manifesting as vanishing gradients and mode collapse. These issues severely hampered the models' ability to learn diverse and high-fidelity data distributions, highlighting fundamental limitations in the original adversarial training framework.\n\nThe problem of vanishing gradients arises when the discriminator becomes overly effective at distinguishing between real and fake samples. In such scenarios, particularly when the real and generated data distributions have non-overlapping supports, the Jensen-Shannon divergence (JSD) used in the original GAN objective saturates. This saturation means the discriminator's loss becomes constant and near zero, providing negligible gradients to the generator. Consequently, the generator receives no meaningful learning signal, effectively halting its progress and preventing it from improving its sample quality. This fundamental limitation was acknowledged as a critical barrier to stable GAN training \\cite{roth2017eui}.\n\nConcurrently, mode collapse emerged as another pervasive issue. Instead of capturing the full diversity of the real data distribution, the generator would often converge to producing only a limited variety of samples, frequently focusing on a few distinct \"modes\" that were particularly effective at fooling the discriminator. This behavior results in a generator that fails to represent the true complexity and richness of the target data, leading to repetitive and uninteresting outputs. For instance, if trained on a dataset of diverse animal images, a generator suffering from mode collapse might only produce images of cats, ignoring dogs, birds, and other animals present in the training data.\n\nEarly attempts to address these instabilities often involved regularization techniques. \\cite{che2016kho} proposed Mode Regularized Generative Adversarial Networks (MRGANs) to tackle mode collapse and instability. They argued that the \"bad behaviors\" of GANs stem from the discriminator's functional shape in high-dimensional spaces, which can lead to training stagnation or misdirection of probability mass. Their approach introduced several regularizers to the objective function, aiming to stabilize training and promote a fairer distribution of probability mass across data modes, thereby mitigating the missing modes problem. Similarly, \\cite{roth2017eui} introduced a regularization approach specifically to stabilize GAN training, directly addressing the fragility caused by dimensional mismatch or non-overlapping support between the model and data distributions. They noted that such non-overlapping supports cause the density ratio and associated f-divergence to be undefined, a direct precursor to vanishing gradients. Their low-computational-cost regularizer aimed to overcome this fundamental limitation, making GAN models more reliable.\n\nA more fundamental theoretical solution to the vanishing gradient problem was introduced by \\cite{arjovsky2017ze5} with the Wasserstein Generative Adversarial Network (WGAN). This work fundamentally altered the loss function by replacing the Jensen-Shannon divergence with the Earth-Mover (Wasserstein-1) distance. The key insight was that the Wasserstein distance provides a continuous and differentiable metric even when the distributions are disjoint, ensuring that the critic (discriminator) can always provide a meaningful gradient to the generator. This property directly addressed the vanishing gradient problem, as the generator would consistently receive a learning signal regardless of how well the critic performed. Furthermore, by providing a smoother loss landscape, the Wasserstein distance inherently contributed to alleviating mode collapse by encouraging the generator to explore a broader range of the data distribution.\n\nThese persistent problems of vanishing gradients and mode collapse underscored that mere architectural tweaks were insufficient to stabilize GAN training. Instead, they highlighted the critical need for more fundamental theoretical and algorithmic solutions that could provide robust learning signals and encourage comprehensive mode coverage. This realization propelled the next wave of research, moving beyond empirical fixes to explore deeper mathematical and algorithmic foundations for GAN stabilization.\n\n\n\\label{sec:core_stability_mechanisms:_divergence,_distance,_and_advanced_regularization}\n\n\\section{Core Stability Mechanisms: Divergence, Distance, and Advanced Regularization}\n\\label{sec:core\\_stability\\_mechanisms:\\_divergence,\\_distance,\\_\\_and\\_\\_advanced\\_regularization}\n\n\\subsection{Shifting from Divergence to Distance: Wasserstein GANs}\n\\label{sec:3\\_1\\_shifting\\_from\\_divergence\\_to\\_distance:\\_wasserstein\\_gans}\n\nEarly Generative Adversarial Networks (GANs) frequently suffered from training instability, particularly vanishing gradients and mode collapse, largely attributable to the choice of divergence metric used to measure the distance between the generator's distribution and the true data distribution. This fundamental challenge was profoundly addressed by the introduction of Wasserstein GANs (WGANs), which marked a pivotal theoretical and practical advancement in the field.\n\nThe groundbreaking work by \\cite{arjovsky2017ze5} introduced Wasserstein GANs, fundamentally altering the GAN loss function by replacing the problematic Jensen-Shannon (JS) divergence with the Earth Mover's (or Wasserstein-1) distance. The JS divergence, while theoretically sound for overlapping distributions, proved highly unsuitable for the typical scenario in GAN training where the generated and real data distributions often lie on low-dimensional manifolds and are non-overlapping. In such cases, the JS divergence becomes a constant, providing zero gradients almost everywhere, which severely hinders the discriminator's ability to provide meaningful feedback to the generator and leads to the notorious vanishing gradient problem.\n\nThe shift to the Earth Mover's distance offered a robust solution to this dilemma. Unlike f-divergences (like JS divergence), the Wasserstein distance provides a smoother and non-zero gradient everywhere, even when the two distributions are non-overlapping. This crucial property ensures that the critic (discriminator in WGANs) can always provide a useful gradient signal to the generator, regardless of how far apart the generated and real data distributions are. This directly mitigates the vanishing gradient problem, allowing for more stable and continuous learning throughout the training process. Furthermore, the Wasserstein distance offers a more meaningful loss metric that empirically correlates with the perceived quality of the generated samples, providing a reliable indicator of training progress that was often absent in traditional GANs.\n\nA cornerstone of WGAN's theoretical stability is the requirement for its critic network to be a K-Lipschitz function. This constraint is derived from the Kantorovich-Rubinstein duality, which states that the Earth Mover's distance can be computed by finding the maximum value of a K-Lipschitz function. Enforcing this Lipschitz constraint on the critic is essential for ensuring that the critic's output is a valid approximation of the Wasserstein distance. Initially, \\cite{arjovsky2017ze5} proposed weight clipping as a simple method to enforce this constraint, albeit with some practical limitations such as potentially reducing model capacity or requiring careful hyperparameter tuning. Despite these initial practical challenges, the theoretical foundation laid by WGANs, particularly the rigorous application of the Kantorovich-Rubinstein duality and the K-Lipschitz critic, represented a profound theoretical advancement. It moved GAN research from heuristic-driven stabilization to a more principled, mathematically grounded approach, paving the way for subsequent improvements in GAN training stability and performance.\n\nIn conclusion, the introduction of Wasserstein GANs by \\cite{arjovsky2017ze5} marked a paradigm shift in generative modeling. By replacing the problematic Jensen-Shannon divergence with the Earth Mover's distance and introducing the K-Lipschitz critic requirement, WGANs provided a stable, theoretically sound framework that effectively addressed vanishing gradients and offered a more interpretable loss. This fundamental change not only stabilized GAN training but also opened new avenues for research into more robust and high-fidelity generative models, establishing a new benchmark for theoretical rigor in the field.\n\\subsection{Gradient Penalties for Robust Lipschitz Enforcement}\n\\label{sec:3\\_2\\_gradient\\_penalties\\_for\\_robust\\_lipschitz\\_enforcement}\n\nWhile the weight clipping strategy proposed in the original Wasserstein Generative Adversarial Network (WGAN) \\cite{Arjovsky2017} was theoretically motivated to enforce the $K$-Lipschitz constraint on the critic (discriminator), it introduced significant practical limitations that often hindered model performance and stability. The fixed clipping range, a sensitive hyperparameter, could drastically limit the critic's capacity if too small, leading to underfitting and an inability to learn complex functions. Conversely, a large clipping range might not effectively enforce the Lipschitz constraint, resulting in unstable training dynamics akin to those WGAN aimed to mitigate. Furthermore, weight clipping could cause gradients to concentrate at the boundaries of the clipping range, leading to vanishing or exploding gradients in specific layers and further destabilizing the training process \\cite{jabbar2020aj0, purwono2025spz}. This crude enforcement mechanism often restricted the critic's ability to learn a smooth function landscape, which is crucial for providing consistent and meaningful gradients to the generator.\n\nRecognizing these limitations, \\cite{Gulrajani2017} introduced a more robust and effective method for enforcing the Lipschitz constraint: the gradient penalty (WGAN-GP). Instead of directly manipulating the critic's weights, WGAN-GP added a regularization term to the critic's loss function that penalized the norm of its gradient with respect to its input. Specifically, this penalty term encourages the gradient norm to be close to one for samples interpolated linearly between real and generated data points. This approach ensures that the critic's gradients are smooth and well-behaved across the entire input space, without restricting the model's capacity or introducing the boundary effects observed with weight clipping \\cite{Gulrajani2017}. The theoretical justification for this approach lies in the Kantorovich-Rubinstein duality, which requires the critic to be 1-Lipschitz (or $K$-Lipschitz, with $K=1$ being a common choice for simplicity and stability) for the Wasserstein distance to be accurately estimated. By penalizing deviations from a gradient norm of one, WGAN-GP directly addresses this requirement in a differentiable manner.\n\nThe introduction of the gradient penalty in WGAN-GP provided several critical advantages. Firstly, it allowed the critic to learn a much smoother function, which in turn provided more stable and informative gradients to the generator, significantly enhancing overall training stability. This explicit enforcement of smoothness and Lipschitz continuity is vital for GANs, as highlighted by \\cite{chu2020zbv}, who demonstrate how such conditions contribute to the eventual stationarity of the generator during training. Secondly, by not directly constraining the weights, WGAN-GP allowed the critic to maintain its full representational capacity, enabling it to learn more complex decision boundaries and better distinguish between real and fake samples. This methodological improvement not only prevented capacity limitations but also further reduced the incidence of mode collapse, as the generator received consistent feedback across the data manifold.\n\nHowever, this robustness came with a computational overhead. Calculating the gradient norm with respect to the input data requires computing second-order derivatives (or at least first-order derivatives of the critic's output with respect to its input, which are then used in the penalty term), which can be computationally expensive, especially for high-dimensional inputs and large batch sizes. This computational burden, while manageable, motivated the search for alternative and more efficient methods of Lipschitz enforcement. WGAN-GP represented a significant advancement in function space regularization, explicitly enforcing smoothness. This contrasts with implicit regularization methods that might arise from architectural choices or other normalization techniques. For instance, recent work such as CHAIN (Lipschitz Continuity Constrained Normalization) \\cite{ni2024y70} has explored integrating Lipschitz constraints directly within normalization layers, offering another avenue for ensuring discriminator stability, particularly in data-efficient GANs.\n\nIn conclusion, the transition from WGAN's crude weight clipping to WGAN-GP's gradient penalty marked a crucial evolutionary step in stabilizing GAN training. By providing a robust and capacity-preserving method for Lipschitz enforcement, WGAN-GP laid the groundwork for the development of more advanced and stable GAN architectures, proving indispensable for generating high-quality and diverse samples \\cite{purwono2025spz}. The theoretical soundness and practical efficacy of WGAN-GP quickly made it a standard practice in subsequent GAN architectures. Despite its widespread adoption and efficacy, the computational demands of WGAN-GP, particularly the need for gradient computation on interpolated samples, motivated the search for more computationally efficient and universally applicable methods for Lipschitz enforcement. This led to innovations like Spectral Normalization \\cite{miyato2018arc}, which offered an alternative approach to constraining the discriminator's Lipschitz constant without explicit gradient penalties.\n\\subsection{Spectral Normalization and Dynamic Learning Rates}\n\\label{sec:3\\_3\\_spectral\\_normalization\\_\\_and\\_\\_dynamic\\_learning\\_rates}\n\nThe pursuit of stable and efficient Generative Adversarial Network (GAN) training has been a central challenge since their inception. While early advancements like Wasserstein GANs with Gradient Penalties (WGAN-GP) significantly improved stability by enforcing the Lipschitz constraint on the discriminator, they often introduced computational overhead due to the need for gradient computations on interpolated samples \\cite{gulrajani2017improved}. This computational burden and the sensitivity to the interpolation strategy motivated the search for more direct and efficient methods for Lipschitz enforcement, as well as optimized training dynamics to better manage the adversarial game \\cite{jabbar2020aj0}. The theoretical underpinnings of GAN stability often point to the importance of discriminator smoothness and bounded Lipschitz constants to ensure meaningful gradients and prevent mode collapse \\cite{chu2020zbv}.\n\nA pivotal innovation addressing these challenges was \\textbf{Spectral Normalization (SN)}, introduced by \\cite{miyato2018arc}. SN offers an elegant and computationally efficient mechanism to enforce the 1-Lipschitz constraint on the discriminator, a critical requirement for stable training, particularly in Wasserstein-based GANs. Unlike gradient penalties, which regularize the discriminator's output gradients, SN directly normalizes the spectral norm of the weight matrices in each layer of the discriminator. The spectral norm of a matrix represents its largest singular value, and by normalizing it to 1, SN ensures that the Lipschitz constant of each individual layer, and consequently the entire discriminator network, is bounded. This direct approach makes SN computationally lighter than gradient penalties, as it avoids the need for explicit gradient computations on interpolated samples. Furthermore, SN is straightforward to implement and can be seamlessly integrated into various GAN architectures without extensive hyperparameter tuning or specific architectural modifications, making it a highly generalizable stabilization technique \\cite{miyato2018arc}. By preventing the discriminator from becoming overly confident or powerful too rapidly, SN fosters smoother loss landscapes, provides more consistent and informative gradient signals to the generator, and significantly mitigates issues such as vanishing gradients and mode collapse, ultimately leading to the generation of higher-quality and more diverse samples. This method represents a refinement in the broader category of weight normalization techniques, which includes earlier approaches like Weight Normalization (WN) \\cite{xiang20171at} that aimed to improve training stability by reparameterizing weights. SN, however, specifically targets the Lipschitz constant, providing a more theoretically grounded and effective solution for GANs.\n\nComplementing Spectral Normalization, the paper by \\cite{miyato2018arc} also effectively employed the \\textbf{Two-Time-Scale Update Rule (TTUR)}, a technique originally proposed by \\cite{heusel2017gans} to further optimize GAN training dynamics. TTUR is predicated on the understanding that the generator and discriminator, with their distinct objectives and learning challenges, often benefit from different learning rates. Instead of applying a single learning rate to both networks, TTUR allows for separate learning rates, typically setting the discriminator's learning rate to be higher than the generator's. This dynamic learning rate management is crucial for maintaining a healthy adversarial balance throughout the training process. In the context of two-player games like GANs, the interaction between the players' updates can lead to complex dynamics, where the choice of learning rate significantly impacts convergence and stability \\cite{liang2018r52}. If the discriminator learns too slowly, it may fail to provide a sufficiently strong or accurate signal for the generator to improve. Conversely, if the discriminator learns too quickly and becomes overly powerful, the generator's gradients can vanish, leading to training stagnation or mode collapse. By allowing distinct update frequencies or magnitudes, TTUR prevents one network from dominating the other, facilitating a more balanced and effective adversarial game. This strategy enhances training stability, improves convergence properties, and contributes to better sample quality and diversity across a wide range of datasets.\n\nThe combined application of Spectral Normalization and the Two-Time-Scale Update Rule by \\cite{miyato2018arc} marked a significant advancement in GAN research, moving towards more principled and efficient stabilization strategies. SN provides a robust, computationally light, and generalizable method for enforcing Lipschitz continuity, while TTUR optimizes the delicate adversarial balance through dynamic learning rate management. These innovations have become foundational, with SN, in particular, being widely adopted in subsequent state-of-the-art GAN architectures. The principles of Lipschitz-constrained normalization continue to be explored, with recent works like CHAIN (LipsCHitz Continuity ConstrAIned Normalization) \\cite{ni2024y70} further refining normalization techniques for data-efficient GANs by focusing on gradient reduction and adaptive feature interpolation. Similarly, other regularization methods, such as Consistency Regularization \\cite{zhang2019hjo} and constrained discriminator outputs \\cite{chao2021ynq}, have shown to work effectively with SN, highlighting its compatibility and foundational role. Despite these advancements, the challenge of achieving perfect mode coverage and absolute training stability in increasingly complex generative models remains an active area of research, underscoring the ongoing relevance of adaptive and efficient regularization techniques exemplified by SN and TTUR.\n\\subsection{Alternative Loss Functions for Enhanced Stability}\n\\label{sec:3\\_4\\_alternative\\_loss\\_functions\\_for\\_enhanced\\_stability}\n\nThe persistent challenge of training instability in Generative Adversarial Networks (GANs), characterized by issues such as vanishing gradients, mode collapse, and oscillating performance, has driven extensive research into modifying the core objective functions. Beyond the f-divergences initially explored, a significant line of inquiry has focused on designing alternative loss functions that provide smoother, non-saturating gradients and more robust convergence properties, thereby enhancing the overall training dynamics and generative quality. This quest highlights a continuous search for diverse mathematical perspectives to achieve stable and effective generative modeling.\n\nA foundational contribution in this area is the introduction of Least Squares Generative Adversarial Networks (LSGANs) \\cite{Mao2017}. LSGANs address the limitations of the traditional sigmoid cross-entropy loss, which can suffer from vanishing gradients when the discriminator becomes overly confident and saturates. By replacing this with a least squares loss function, LSGANs ensure that both the generator and discriminator receive meaningful, non-saturating gradients throughout training. This modification encourages the generator to produce samples closer to the decision boundary, leading to improved training stability and the generation of higher quality images compared to original GANs. Theoretically, the least squares objective implicitly minimizes the Pearson $\\chi^2$ divergence, offering a distinct and often more stable mathematical perspective than the Jensen-Shannon divergence minimized by early GANs \\cite{Mao2017}.\n\nFollowing the principles of non-saturating objectives, the adversarial hinge loss emerged as another cornerstone for stable GAN training, particularly in high-fidelity models. The hinge loss provides a clear margin for classification, penalizing the discriminator only when its output for real samples falls below a certain positive margin, or when its output for fake samples rises above a negative margin. This margin-based formulation ensures that the discriminator does not become overly confident too early, preventing gradient saturation and providing a consistent learning signal to the generator. For the generator, the hinge loss encourages it to push fake samples beyond the discriminator's negative margin. This approach has been widely adopted in state-of-the-art architectures, including Self-Attention GANs (SAGAN) and BigGAN, due to its effectiveness in promoting stable training and high-quality image synthesis. \\textcite{wang20178xf} further explored adaptive hinge loss functions, demonstrating how dynamically adjusting the margin based on the expected energy of the target distribution can lead to improved stability and performance, with theoretical proofs of convergence under certain assumptions.\n\nThese alternative loss functions represent a critical shift in GAN optimization strategies. Unlike Wasserstein GANs with Gradient Penalty (WGAN-GP) \\cite{Gulrajani2017}, which primarily enforce a Lipschitz constraint on the discriminator to ensure meaningful gradients across the input space, LSGANs and hinge loss directly reshape the objective landscape itself. They achieve stability by preventing the discriminator's loss from saturating, thus providing a more consistent and robust gradient flow to the generator. This distinction is crucial: while WGAN-GP focuses on the \\textit{smoothness} of the discriminator function, LSGANs and hinge loss focus on the \\textit{shape} of the loss function to avoid regions of zero gradient. The theoretical framework proposed by \\textcite{chu2020zbv} further elucidates the importance of smoothness and specific divergence properties for guaranteeing eventual stationarity of the generator, highlighting why non-saturating and margin-based losses contribute to stability.\n\nBeyond these widely adopted approaches, researchers have continued to explore novel modifications to loss functions. \\textcite{zadorozhnyy20208ft} introduced adaptive weighted discriminator loss functions, or \"aw-loss functions.\" This method addresses the challenge that an equally weighted sum of real and fake losses can sometimes benefit one part of the training while harming the other, leading to instability and mode collapse. By adaptively weighting the real and fake components of the discriminator's loss based on their gradients, aw-loss functions guide the discriminator's training in a direction that explicitly benefits overall GAN stability. This dynamic balancing act has shown significant improvements in Inception Scores (IS) and Fréchet Inception Distance (FID) metrics on various datasets, demonstrating the value of fine-grained control over loss components.\n\nAnother innovative approach is presented by Constrained Generative Adversarial Networks (GAN-C) \\cite{chao2021ynq}. This method introduces an explicit constraint on the discriminator's output, aiming to bound its function space. While theoretically sharing the same Nash equilibrium as the standard GAN, this constraint helps to regularize the discriminator's behavior, preventing it from becoming overly powerful or unstable. In practice, this leads to faster convergence during training and the generation of higher-quality data, as demonstrated across a diverse set of image datasets. This highlights that even subtle modifications to how the discriminator's output is handled within the loss framework can significantly impact training stability and generative performance.\n\nIn conclusion, the evolution of GAN loss functions from the original sigmoid cross-entropy to LSGANs, adversarial hinge loss, and more advanced adaptive and constrained objectives, underscores a fundamental principle: robust and non-saturating gradients are paramount for stable adversarial training. These diverse mathematical strategies, whether by reshaping the objective landscape, introducing classification margins, or adaptively weighting loss components, have collectively provided the practical stability necessary for the subsequent architectural innovations that led to high-fidelity generative models. While these advancements have significantly mitigated issues like vanishing gradients, the complete elimination of mode collapse and the guarantee of universal convergence across all data distributions remain active areas of research, as noted by reviews like \\cite{wang2019w53}. Future work continues to explore hybrid approaches and novel theoretical frameworks to further enhance the robustness and generative power of GANs.\n\\subsection{Advanced Regularization and Architectural Constraints}\n\\label{sec:3\\_5\\_advanced\\_regularization\\_\\_and\\_\\_architectural\\_constraints}\n\nBeyond foundational techniques like gradient penalties and spectral normalization, the persistent challenges of Generative Adversarial Networks (GANs)—including mode collapse, training fragility, and sensitivity to input perturbations—have necessitated the development of a broader array of advanced regularization techniques and carefully designed architectural constraints. These sophisticated solutions aim to further enhance GAN stability, improve performance, and foster diverse generation, pushing the boundaries of what is achievable in adversarial training dynamics.\n\nA significant class of advanced regularization techniques focuses on modifying the training objective or adding explicit penalties to guide the adversarial process more effectively. \\cite{metz20169ir} introduced Unrolled Generative Adversarial Networks, a method that addresses mode collapse by allowing the generator to \"see\" the effects of several future discriminator optimization steps. By defining the generator's objective with respect to an unrolled optimization of the discriminator, this approach provides a more informed and stable gradient signal, encouraging the generator to produce a wider variety of samples and thus mitigating mode collapse. However, this comes at the cost of increased computational complexity due to the inner loop unrolling. Complementing this, \\cite{roth2017eui} proposed a regularization method specifically designed to overcome the fundamental limitation of dimensional mismatch or non-overlapping support between the model and data distributions, which often leads to undefined density ratios and unstable training. This technique offers a computationally efficient way to stabilize GAN training, making them more reliable.\n\nTo further enhance robustness, particularly to input variations, Consistency Regularization (CR-GAN) emerged as a powerful technique \\cite{zhang2019hjo}. Inspired by semi-supervised learning, CR-GAN penalizes the discriminator for being inconsistent in its predictions on augmented versions of the same input. Specifically, it applies non-differentiable augmentations (e.g., random shifts, flips, color jitter) to both real and generated samples and adds a penalty if the discriminator's output for an augmented sample differs significantly from its unaugmented counterpart. This forces the discriminator to learn more robust and stable features, which in turn provides a more consistent learning signal to the generator. CR-GAN has demonstrated significant improvements in FID scores and training stability, working effectively with existing techniques like spectral normalization, though it introduces additional computational overhead for augmentation and penalty calculation. Further addressing mode collapse, \\cite{che2016kho} proposed Mode Regularized Generative Adversarial Networks, which introduce regularization terms to explicitly encourage the discriminator to assign a fairer distribution of probability mass across the modes of the data-generating distribution. This prevents the discriminator from becoming overly confident in distinguishing only a few modes, thereby promoting broader mode coverage by the generator. For scenarios with limited training data, \\cite{tseng2021m2s} developed a regularization approach that theoretically connects the regularized loss to a LeCam-divergence, which is inherently more robust under data scarcity. This method improves generalization and stabilizes learning dynamics, proving particularly effective when combined with data augmentation techniques.\n\nBeyond explicit regularization terms, architectural choices themselves serve as powerful implicit constraints that profoundly influence GAN stability and feature learning. As highlighted by \\cite{chu2020zbv}, proper architectural design can enforce properties like Lipschitz continuity, which are crucial for stable training. For instance, the widespread adoption of \\textbf{Batch Normalization} in architectures like DCGAN \\cite{radford2015unsupervised} acts as a form of regularization by normalizing layer inputs, reducing internal covariate shift, and stabilizing training. More advanced normalization techniques, such as \\textbf{Adaptive Instance Normalization (AdaIN)} used in StyleGAN \\cite{karras2019style}, not only enable style control but also implicitly regularize the feature representations by decoupling content and style, leading to smoother latent spaces and improved disentanglement. Similarly, the integration of \\textbf{residual connections} in modern discriminators, as seen in architectures like BigGAN \\cite{brock2018biggan}, facilitates deeper networks by ensuring stable gradient flow and preventing degradation, thereby implicitly regularizing the discriminator's capacity and smoothness. Furthermore, StyleGAN's \\textbf{mapping network}, which transforms the initial latent code into an intermediate latent space, serves as a powerful implicit regularizer. This transformation disentangles the latent space, making it more structured and easier for the generator to navigate, which inherently promotes diversity and stability by preventing the generator from collapsing to a few modes.\n\nThe continuous search for sophisticated solutions also includes the integration of adaptive feedback mechanisms and other innovative regularization strategies. These approaches dynamically adjust regularization strengths or training parameters based on the current state of the adversarial game, aiming to maintain a delicate balance between the generator and discriminator. While specific examples vary, the underlying principle is to provide context-aware guidance to stabilize adversarial training dynamics and improve model generalization, moving towards more autonomous and robust training pipelines.\n\nIn conclusion, the evolution of GAN stabilization extends far beyond initial gradient penalties and spectral normalization. The field has progressively embraced a diverse toolkit of advanced regularization techniques, from modifying training objectives through unrolling and introducing explicit penalties for consistency or mode coverage, to leveraging implicit architectural designs that enforce stability and improve feature learning. The latest advancements highlight a trend towards integrating adaptive feedback mechanisms and theoretically grounded regularization, demonstrating a sophisticated, multi-faceted approach to achieve robust, diverse, and high-fidelity generation, underscoring the ongoing quest for more resilient and versatile generative models.\n\n\n\\label{sec:architectural_innovations_for_high-fidelity_and_scalability}\n\n\\section{Architectural Innovations for High-Fidelity and Scalability}\n\\label{sec:architectural\\_innovations\\_for\\_high-fidelity\\_\\_and\\_\\_scalability}\n\n\\subsection{Progressive Growing for High-Resolution Synthesis}\n\\label{sec:4\\_1\\_progressive\\_growing\\_for\\_high-resolution\\_synthesis}\n\nThe generation of high-resolution, photorealistic images has long been a significant challenge for Generative Adversarial Networks (GANs), often hampered by training instability and the computational demands of large models. Early attempts to stabilize GAN training primarily focused on modifying objective functions or regularization techniques to mitigate issues such as mode collapse and vanishing gradients. For instance, \\textcite{metz20169ir} introduced Unrolled Generative Adversarial Networks, a method that stabilized training by defining the generator's objective with respect to an unrolled optimization of the discriminator, thereby addressing mode collapse and enhancing the diversity and coverage of the data distribution. Similarly, \\textcite{che2016kho} proposed Mode Regularized Generative Adversarial Networks, arguing that the functional shape of discriminators in high-dimensional spaces contributed to instability and mode collapse. Their solution involved introducing regularizers to stabilize training and ensure a more equitable distribution of probability mass across data modes, particularly in the early phases of training. While these methods significantly improved the foundational stability and diversity of GANs, scaling them to generate images at resolutions beyond 256x256 pixels remained a formidable hurdle.\n\nA pivotal methodological breakthrough that fundamentally transformed the landscape of high-resolution image synthesis was the introduction of Progressive Growing of GANs (PGGANs) by \\textcite{Karras2018}. This innovative approach directly tackled the challenges of training stability and high-resolution output by gradually increasing the resolution of both the generated images and the discriminator's inputs throughout the training process. Instead of attempting to synthesize high-resolution images from scratch, PGGANs begin training at a very low resolution, typically 4x4 pixels. As training progresses and the network learns to generate stable images at the current resolution, new layers are incrementally added to both the generator and discriminator. These new layers are smoothly \"faded in\" using a weighted sum with the existing layers, ensuring a continuous and stable transition to higher resolutions.\n\nThis progressive growing strategy offers several critical advantages. Firstly, it significantly improves training stability by presenting an easier learning task to the networks at each stage. Learning low-frequency features at coarse resolutions is simpler, and this knowledge is then leveraged and refined as higher-frequency details are introduced with increasing resolution. This hierarchical learning process effectively prevents the common pitfalls of GAN training, such as mode collapse and gradient instability, which are exacerbated when attempting to learn complex, high-dimensional distributions directly. Secondly, PGGANs enabled the synthesis of unprecedentedly photorealistic images, pushing the boundaries to resolutions as high as 1024x1024 pixels. This marked a major leap in image quality and scale, allowing for the creation of visually compelling and diverse outputs that were previously unattainable. Finally, by starting with smaller networks and gradually expanding them, PGGANs also contributed to a reduction in the overall training time required to achieve high-resolution outputs, as the initial stages are computationally less intensive. The success of PGGANs laid a robust foundation for subsequent advancements in high-fidelity image generation, including the StyleGAN series, by demonstrating a scalable and stable training paradigm for complex generative tasks.\n\nIn conclusion, the progressive growing methodology introduced by PGGANs represented a paradigm shift in generative modeling, moving beyond earlier regularization and objective function modifications to address stability and resolution through a structured training curriculum. While earlier works like \\textcite{metz20169ir} and \\textcite{che2016kho} laid crucial groundwork for general GAN stability, PGGANs provided the architectural and training strategy necessary to unlock truly high-resolution, photorealistic synthesis. Despite its profound impact, the computational cost of training PGGANs, especially for extremely high resolutions or diverse datasets, still presented avenues for further optimization, paving the way for future research into more efficient and controllable high-fidelity generative models.\n\\subsection{Large-Scale Training and Self-Attention Mechanisms}\n\\label{sec:4\\_2\\_large-scale\\_training\\_\\_and\\_\\_self-attention\\_mechanisms}\n\nThe pursuit of high-fidelity and globally coherent image generation with Generative Adversarial Networks (GANs) has consistently pushed the boundaries of computational scale and architectural innovation. While earlier works like Progressive Growing GANs (PGGANs) demonstrated the efficacy of gradual resolution increase, a subsequent landmark achievement underscored the power of scaling model capacity, dataset size, and leveraging advanced architectural components to unlock unprecedented levels of generative performance.\n\nThis paradigm shift was most notably exemplified by BigGAN \\cite{brock2019biggan}, which significantly advanced the state-of-the-art in GANs by training on massive datasets like ImageNet with substantially larger models and batch sizes. BigGAN demonstrated that computational scale, when combined with robust stabilization techniques and architectural innovations, was crucial for achieving state-of-the-art results on diverse, large-scale datasets, pushing the limits of GAN performance. Its success highlighted that simply increasing model parameters and training data could lead to a qualitative leap in generated image quality and diversity, provided the underlying training remained stable.\n\nA key architectural innovation integrated into BigGAN was the self-attention mechanism, originally introduced to GANs by Self-Attention Generative Adversarial Networks (SAGAN) \\cite{zhang2019selfattention}. Unlike traditional convolutional layers, which have a localized receptive field and primarily capture local dependencies, self-attention allows a neuron to attend to features at any spatial location in the input, irrespective of their distance. This global contextual awareness was instrumental in enabling the generator to produce globally coherent structures, ensuring that disparate parts of an image (e.g., an animal's head and limbs, or consistent background elements) were logically consistent and well-aligned. For instance, in complex scenes, self-attention helps maintain structural integrity across the entire image, leading to more globally coherent and higher-fidelity outputs. The continued relevance of self-attention in GAN architectures is further evidenced by more recent works, such as PEGANs, which also leverage self-attention modules to improve long-range dependency modeling and enhance generation quality \\cite{xue2022n0r}.\n\nCrucially, the ability of BigGAN to effectively leverage massive scale was predicated on a foundation of robust stabilization techniques developed in preceding works. Before such large models could be trained, fundamental issues of GAN instability, such as vanishing gradients and mode collapse, needed reliable solutions \\cite{jabbar2020aj0, wiatrak20194ib}. Two specific techniques proved particularly foundational for BigGAN's stability: Spectral Normalization (SN) and the hinge loss objective. Spectral Normalization \\cite{miyato2018arc} provided an efficient and effective method for enforcing the Lipschitz constraint on the discriminator, which is vital for stable training, especially with large model capacities. By normalizing the spectral norm of weight matrices, SN prevents the discriminator from becoming overly confident or exhibiting exploding gradients, thereby providing a smoother and more informative gradient signal to the generator. Concurrently, BigGAN adopted a hinge version of the adversarial loss function, which offers improved stability and performance compared to earlier objectives like the original minimax loss or even WGAN-GP in certain contexts \\cite{wang20178xf}. The hinge loss provides clear, non-saturating gradients, helping to maintain a healthy adversarial balance during the extensive training required for large-scale models. These advancements in regularization and loss functions provided the necessary robustness for BigGAN to scale effectively without succumbing to common training pathologies.\n\nBeyond architectural and stabilization advancements, BigGAN also introduced the \"truncation trick,\" a critical technique for controlling the trade-off between sample quality and diversity. During inference, by sampling latent codes from a truncated normal distribution (i.e., restricting samples to within a certain range, typically 1 or 2 standard deviations from the mean), BigGAN could generate images of exceptionally high perceptual quality, albeit at the cost of some diversity. Conversely, sampling from the full latent space yielded greater diversity but often included lower-quality or unusual samples. This finding revealed important insights into the structure of the latent space learned by large-scale GANs, suggesting that high-quality samples tend to cluster in denser regions of the latent space, while sparser regions might contain less realistic or out-of-distribution samples. The truncation trick thus became a standard practice for showcasing the peak performance of high-fidelity GANs.\n\nIn conclusion, the era of large-scale training, epitomized by BigGAN, marked a significant advancement in generative modeling. By synergistically combining massive computational resources, advanced architectural components like self-attention, and leveraging robust stabilization techniques such as Spectral Normalization and hinge loss, GANs achieved unprecedented levels of fidelity and global coherence, particularly on complex datasets like ImageNet. This period underscored the critical importance of computational scale, large batch sizes, and sophisticated architectures for pushing the boundaries of GAN performance, while also highlighting the nuanced control over generation quality offered by techniques like the truncation trick.\n\\subsection{Style-Based Generators for Disentangled Control and Photorealism}\n\\label{sec:4\\_3\\_style-based\\_generators\\_for\\_disentangled\\_control\\_\\_and\\_\\_photorealism}\n\nThe pursuit of both photorealistic image synthesis and intuitive, disentangled control over generated features has represented a significant challenge in the evolution of Generative Adversarial Networks (GANs). The seminal StyleGAN architecture \\cite{Karras2019} marked a pivotal advancement, introducing a novel style-based generator that fundamentally reshaped the landscape of image generation by dramatically improving perceptual quality and the editability of synthetic images. This innovation distinguished itself from prior GANs, which often struggled with entangled latent spaces where a single latent dimension could influence multiple, unrelated visual attributes \\cite{jabbar2020aj0}.\n\nThe core of StyleGAN's innovation lies in its unique generator design. Unlike earlier GANs that directly fed a latent code $z$ into the initial layers of the generator, StyleGAN employs a separate \\textit{mapping network}. This network transforms an initial, typically isotropic Gaussian latent code $z \\in \\mathcal{Z}$ into an intermediate latent vector $w \\in \\mathcal{W}$. The $\\mathcal{W}$ space is designed to be less entangled than the original $z$ space, effectively linearizing the latent representation and making it more amenable to controlling specific visual attributes \\cite{Karras2019}. This disentanglement is further enhanced through a technique called 'style mixing', where different $w$ vectors (derived from different $z$ codes) are applied to different layers of the generator during training. This forces each style-specific layer to specialize in certain features, promoting a more granular and independent control over the generated image \\cite{Karras2019}.\n\nThe generator itself is a series of upsampling blocks, but critically, it does not receive the initial latent code $z$ directly. Instead, it starts with a learned constant input and injects 'style' information at multiple resolutions through Adaptive Instance Normalization (AdaIN) layers. AdaIN, originally introduced in the context of style transfer \\cite{Huang2017AdaIN}, normalizes the mean and variance of feature map activations independently for each instance and then scales and biases them using learned parameters derived from the intermediate style vector $w$. This mechanism allows for hierarchical control: coarse styles injected at early layers influence high-level attributes such as pose, identity, and overall structural composition, while styles applied at later layers control finer details like hair color, texture, and micro-features \\cite{Karras2019}. This approach contrasts with other normalization techniques like Batch Normalization \\cite{xiang20171at} or Spectral Normalization \\cite{miyato2018arc}, which primarily focus on stabilizing training and enforcing Lipschitz constraints, by explicitly modulating feature statistics to inject style information.\n\nStyleGAN's architectural innovations significantly improved the perceptual quality and realism of generated images, setting new benchmarks. The disentangled $\\mathcal{W}$ space, combined with the hierarchical style injection, facilitated unprecedented user-driven content creation, allowing for intuitive manipulation of facial features, age, and other attributes \\cite{jabbar2020aj0}. Furthermore, the introduction of the 'truncation trick' allowed for a trade-off between sample diversity and quality, enabling the generation of higher-quality, albeit less diverse, samples by moving latent codes closer to the average $w$ in the $\\mathcal{W}$ space \\cite{Karras2019}.\n\nDespite its revolutionary impact, the original StyleGAN architecture was not without limitations. While the $\\mathcal{W}$ space offered improved disentanglement compared to direct $z$ manipulation, it was not perfectly orthogonal; some entanglement between attributes, such as pose and identity, or between global and local features, still persisted. As a \"push-forward\" generative model, StyleGAN's ability to fit highly multimodal distributions is theoretically constrained by the Lipschitz constant of its generator, where a large constant is often required for multimodal fitting but can conflict with training stability \\cite{salmona202283g}. Moreover, the original StyleGAN exhibited certain characteristic visual artifacts, such as \"blob\" artifacts or texture sticking, which could manifest as repetitive patterns or unnatural textures, particularly when interpolating in the latent space. These issues, along with challenges related to normalization and upsampling artifacts, highlighted areas for further refinement, paving the way for subsequent architectural improvements aimed at enhancing image quality and consistency.\n\n\\bibliography{references}\n\\subsection{Addressing Perceptual Artifacts and Aliasing in StyleGANs}\n\\label{sec:4\\_4\\_addressing\\_perceptual\\_artifacts\\_\\_and\\_\\_aliasing\\_in\\_stylegans}\n\nWhile early Generative Adversarial Networks (GANs) focused on achieving stable training and basic image synthesis, the StyleGAN series marked a significant shift towards unprecedented levels of photorealism and controllable generation. However, even the groundbreaking StyleGAN architecture \\cite{Karras2019} exhibited certain perceptual artifacts and fundamental signal processing issues that limited its realism and consistency, particularly when generating high-resolution content or animating latent space interpolations. Subsequent refinements in the series, namely StyleGAN2 \\cite{Karras2020} and StyleGAN3 \\cite{Karras2021}, meticulously addressed these limitations, pushing the boundaries of synthetic image quality and robustness.\n\nThe initial StyleGAN model \\cite{Karras2019} revolutionized image synthesis by introducing a style-based generator, a mapping network, and Adaptive Instance Normalization (AdaIN) layers. This architecture enabled highly disentangled control over various visual attributes, allowing for intuitive manipulation of generated images. Despite its success in generating visually compelling and diverse outputs, StyleGAN suffered from noticeable \"droplet\" artifacts and a lack of perfect disentanglement, where changes in one latent dimension could inadvertently affect unrelated visual features.\n\nTo overcome these shortcomings, StyleGAN2 \\cite{Karras2020} undertook a comprehensive analysis of the generator architecture and training process, identifying several sources of these perceptual artifacts. The authors found that the progressive growing scheme, commonly used in earlier high-resolution GANs, and certain aspects of instance normalization contributed to these issues. StyleGAN2 introduced several key innovations, most notably \\textbf{path length regularization}, which aimed to encourage a more \"well-behaved\" latent space. By regularizing the mapping from latent codes to features, path length regularization ensured that a fixed-size step in the latent space always resulted in a fixed-magnitude change in the image space, thereby improving disentanglement and significantly reducing the visually distracting \"droplet\" artifacts. Furthermore, StyleGAN2 redesigned the generator by removing the progressive growing and replacing instance normalization with a more robust weight demodulation technique, leading to enhanced image quality and stability.\n\nDespite the significant improvements in StyleGAN2, a more fundamental limitation remained: aliasing. This problem, inherent in discrete signal processing, manifested as static high-frequency details that did not move correctly when images were translated or rotated, leading to a \"texture sticking\" effect and hindering smooth animation. StyleGAN3 \\cite{Karras2021} meticulously diagnosed this issue, recognizing that previous GANs, including StyleGAN2, implicitly suffered from aliasing due to their reliance on discrete pixel grids and operations that were not perfectly translation-equivariant. To address this, StyleGAN3 introduced a radical redesign of the generator to be truly \\textbf{alias-free}. This was achieved by incorporating \\textbf{anti-aliasing filters} at every layer of the generator, ensuring that high-frequency information was handled correctly and consistently across different resolutions and transformations. By making the generator fully translation-equivariant, StyleGAN3 enabled generated content to appear consistent and realistic even under continuous transformations, drastically improving the quality of latent space interpolations and animation capabilities.\n\nThe advancements from StyleGAN2's path length regularization to StyleGAN3's alias-free architecture represent a continuous effort to refine the underlying signal processing principles of generative models. These innovations not only enhanced the realism and perceptual quality of generated images but also made GANs more robust and versatile for applications requiring high consistency, such as video synthesis or interactive content creation. While StyleGAN3 achieved unprecedented levels of control and fidelity, the computational cost associated with its alias-free design and the general challenge of scaling GANs to extremely diverse, large-scale datasets remain areas for ongoing research and optimization.\n\n\n\\label{sec:advanced_training_paradigms:_data_efficiency_and_conditional_control}\n\n\\section{Advanced Training Paradigms: Data Efficiency and Conditional Control}\n\\label{sec:advanced\\_training\\_paradigms:\\_data\\_efficiency\\_\\_and\\_\\_conditional\\_control}\n\n\\subsection{Training with Limited Data: Adaptive Discriminator Augmentation}\n\\label{sec:5\\_1\\_training\\_with\\_limited\\_data:\\_adaptive\\_discriminator\\_augmentation}\n\nThe efficacy of Generative Adversarial Networks (GANs) in synthesizing high-quality data has long been predicated on the availability of extensive datasets. However, this requirement poses a significant barrier to their application in numerous real-world scenarios where data acquisition is inherently costly, time-consuming, or simply infeasible. In data-scarce environments, GAN training faces a critical challenge: the discriminator rapidly overfits to the limited training samples, leading to a collapse of the adversarial game, poor gradient signals for the generator, and ultimately, low-quality or non-diverse generated outputs. This section delves into pivotal advancements that have enabled GANs to achieve high-fidelity generation even with limited data, primarily through sophisticated augmentation strategies.\n\nEarly attempts to mitigate discriminator overfitting in data-scarce regimes often involved applying standard data augmentations (e.g., rotations, flips, color jitter) to the real training images. However, this approach quickly revealed a critical problem known as \"augmentation leakage\" \\cite{zhao2020xhy}. If augmentations are applied exclusively to real samples, the discriminator learns to distinguish between augmented real data and unaugmented fake data. This inadvertently forces the generator to produce images that incorporate the augmentation artifacts, leading to a degradation in sample quality and a failure to learn the true data distribution. To counteract this, Differentiable Augmentation (DiffAugment) \\cite{zhao2020xhy} proposed applying augmentations to \\textit{both} real and generated samples in a differentiable manner. While DiffAugment effectively prevented augmentation leakage, it required careful selection of augmentation policies and a fixed augmentation strength, which might not be optimal across different datasets or training stages.\n\nBeyond augmentation, other regularization techniques have been explored to enhance discriminator robustness and generalization, which are crucial for stable training under limited data. For instance, the LeCam-GAN \\cite{tseng2021m2s} focused on modifying the loss function to be more robust, demonstrating improved generalization and stability. Similarly, methods like InfoMax-GAN \\cite{lee20205ue} aimed to mitigate catastrophic forgetting in the discriminator and reduce mode collapse by employing contrastive learning and mutual information maximization, thereby fostering a more robust discriminator less prone to overfitting. Robust Generative Adversarial Network (RGAN) \\cite{zhang201996t} improved generalization by promoting local robustness within the neighborhood of training samples, a strategy particularly beneficial when the training set is small. Furthermore, approaches like Probability Ratio Clipping and Sample Reweighting \\cite{wu2020p8p} and Constrained GANs (GAN-C) \\cite{chao2021ynq} introduced mechanisms to stabilize discriminator training and enforce constraints on its output, preventing it from becoming overly confident or unstable, which are common failure modes exacerbated by limited data. These efforts collectively underscored the importance of a well-behaved and generalizable discriminator for overall GAN stability.\n\nA significant breakthrough specifically tailored to address discriminator overfitting in limited data regimes was the introduction of Adaptive Discriminator Augmentation (ADA) by Karras et al. \\cite{karras202039x}. ADA provides a robust and dynamic solution by preventing the discriminator from memorizing the small training set, a primary cause of training divergence and poor sample quality. The core mechanism of ADA involves applying a set of non-differentiable augmentations (e.g., rotations, flips, color jitter, cutouts) to \\textit{both} real and generated images before they are presented to the discriminator. This crucial step ensures that the generator is not incentivized to produce augmented-looking samples, thereby effectively mitigating augmentation leakage, similar to DiffAugment.\n\nWhat distinguishes ADA and makes it particularly effective is its adaptive control mechanism. The probability of applying these augmentations is dynamically adjusted during training based on the discriminator's performance. ADA monitors the discriminator's overfitting, typically by tracking its classification accuracy on a validation set or by comparing its accuracy on augmented versus unaugmented real images. If the discriminator is found to be overfitting (e.g., achieving very high accuracy on real images), the augmentation probability is increased, making its task harder and forcing it to learn more generalizable features. Conversely, if the discriminator struggles, the augmentation strength is reduced. This adaptive feedback loop maintains a healthy adversarial balance, preventing the discriminator from becoming too strong too quickly and ensuring that the generator receives consistent, meaningful gradients.\n\nADA demonstrated remarkable improvements, enabling high-quality image synthesis with significantly fewer training images. For instance, it achieved results comparable to StyleGAN2 trained on full datasets with an order of magnitude less data, and even established new state-of-the-art FID scores on benchmarks like CIFAR-10, which was re-evaluated as a limited-data benchmark \\cite{karras202039x}. This methodological innovation democratized access to high-quality generative models for applications where extensive datasets are impractical, such as medical imaging, specialized industrial design, or artistic content creation. By dynamically managing the discriminator's learning capacity relative to the data size, ADA effectively bridges the gap between data-hungry GAN architectures and real-world data constraints.\n\nIn summary, the evolution of GAN training under data scarcity has progressed from understanding and mitigating augmentation leakage to sophisticated adaptive augmentation strategies. ADA revolutionized the field by providing a practical and robust method to prevent discriminator overfitting, thereby enabling high-quality generation with significantly less data. While complementary techniques focusing on general discriminator stability and generalization (e.g., \\cite{lee20205ue, zhang201996t, wu2020p8p, chao2021ynq}) contribute to the overall robustness of GANs, ADA specifically addresses the unique challenges posed by limited data through its dynamic augmentation policy. Despite these successes, challenges persist in optimizing augmentation policies for highly diverse or complex datasets, ensuring mode coverage with extremely sparse data, and developing robust augmentation strategies for non-image data types. Future research will likely explore more advanced adaptive augmentation schemes, potentially integrating learned augmentation policies or leveraging transfer learning from large pre-trained models to further enhance data efficiency in GAN training.\n\\subsection{Few-Shot and Meta-Learning Approaches for Data Scarcity}\n\\label{sec:5\\_2\\_few-shot\\_\\_and\\_\\_meta-learning\\_approaches\\_for\\_data\\_scarcity}\n\n\\label{sec:few-shot-meta-learning}\n\nThe deployment of Generative Adversarial Networks (GANs) in domains characterized by extreme data scarcity, such as medical imaging, specialized industrial applications, or urban planning, presents a significant challenge. While methods like Adaptive Discriminator Augmentation (ADA) \\cite{Karras2022} effectively combat discriminator overfitting by dynamically applying non-leaking augmentations to limited datasets, they primarily operate at the data level. For scenarios demanding rapid adaptation to novel data distributions with truly minimal samples, a more fundamental shift towards few-shot and meta-learning paradigms is required, moving beyond data-level interventions to enable models to \"learn how to learn\" from scarce examples.\n\nMeta-learning, or \"learning to learn,\" offers a powerful framework for addressing extreme data scarcity in GANs. The core idea is to train a model across a distribution of related tasks, enabling it to acquire transferable knowledge that facilitates rapid adaptation to new, unseen tasks with only a few training examples. For GANs, this often involves meta-learning the discriminator to quickly establish effective decision boundaries even when presented with a handful of samples from a new target distribution. For instance, \\cite{zhang202263o} proposes Spatially-Transferable Generative Adversarial Networks (STrans-GAN) for urban traffic estimation under data scarcity. This approach incorporates a meta-learning idea into the pre-training process, allowing the model to learn a well-generalized representation from multiple source cities. During fine-tuning on a new city with limited data, a cluster matching regularizer further aids flexible adaptation. This demonstrates how meta-learning can equip the discriminator with an inherent ability to generalize and adapt efficiently, significantly reducing data requirements in truly few-shot settings. However, meta-learning approaches typically require a diverse set of source tasks for effective meta-training, which might not always be available in highly specialized or unique domains. The computational overhead of meta-training across multiple tasks can also be substantial.\n\nBeyond meta-learning the discriminator, other few-shot GAN strategies focus on architectural design and self-supervised learning to enhance data efficiency. \\cite{liu20212c2} introduced FastGAN, a lightweight GAN structure specifically designed for high-fidelity few-shot image synthesis. FastGAN achieves superior quality on high-resolution images (e.g., 1024x1024) with minimal computing cost, converging from scratch with less than 100 training samples on a single GPU. A key innovation is a self-supervised discriminator trained as a feature-encoder, which helps the discriminator learn robust representations from limited data without relying solely on the adversarial signal. This architectural and self-supervised approach provides an alternative to meta-learning by making the core components of the GAN inherently more data-efficient, often exhibiting consistent performance across various image domains. While highly efficient, FastGAN's performance might still be constrained by the inherent limitations of learning complex distributions from extremely few samples, and its architectural choices might not be universally optimal for all data types.\n\nAnother complementary approach involves developing robust regularization schemes that improve generalization under limited data. \\cite{tseng2021m2s} proposes a regularization method for GANs based on LeCam-divergence, which is theoretically shown to be more robust under limited training data than traditional f-divergences. This regularization scheme improves generalization performance and stabilizes learning dynamics, complementing existing data augmentation methods like ADA. By modifying the underlying loss function, LeCam-GAN enhances the model's ability to learn meaningful distributions even when data is scarce, without necessarily requiring a meta-training phase or specialized architectures. However, while robust, such regularization methods primarily address the stability and generalization of the learning process itself, rather than explicitly teaching the model how to rapidly adapt to \\textit{new} tasks, which is the strength of meta-learning.\n\nIn synthesis, few-shot and meta-learning approaches represent a crucial progression in making GANs practical for data-scarce environments. Meta-learning (e.g., \\cite{zhang202263o}) enables the discriminator to acquire transferable knowledge for rapid adaptation, transforming the problem into \"learning to adapt\" rather than merely \"learning from scratch\" on limited data. This is particularly valuable when rapid deployment across similar, but distinct, tasks is needed. Architectural innovations combined with self-supervision (e.g., FastGAN \\cite{liu20212c2}) offer computationally efficient solutions for high-fidelity synthesis from few samples by designing intrinsically data-efficient models. Meanwhile, robust regularization techniques (e.g., LeCam-GAN \\cite{tseng2021m2s}) provide theoretical grounding and practical improvements for training stability and generalization under data constraints. Each approach offers distinct advantages and addresses different facets of the data scarcity problem. Meta-learning excels at rapid task adaptation, FastGAN at computational efficiency and high-resolution output, and LeCam-GAN at training stability and generalization.\n\nDespite significant progress, several challenges remain. The definition and acquisition of diverse meta-training tasks for real-world scenarios, particularly in highly specialized domains like medical imaging, can be difficult. The computational cost of meta-training can also be prohibitive. Future research could explore hybrid approaches that combine meta-learning with lightweight architectures and robust regularization techniques to leverage their synergistic benefits. For instance, meta-learning a lightweight generator and discriminator, or integrating LeCam-divergence into a meta-learning framework, could lead to even more data-efficient and stable GANs. Furthermore, investigating meta-learning strategies for the generator itself, or developing unified frameworks that adaptively select or combine data-efficient strategies based on the specific data scarcity level and domain characteristics, represents promising avenues for pushing the boundaries of data-efficient generative learning.\n\\subsection{Conditional and Text-to-Image Synthesis}\n\\label{sec:5\\_3\\_conditional\\_\\_and\\_\\_text-to-image\\_synthesis}\n\nGenerative Adversarial Networks (GANs), initially designed for unconditional image generation, quickly evolved to address the critical need for controlled output, allowing users to specify desired characteristics of the synthesized images. This section traces the progression from simple conditional generation to complex text-to-image synthesis, highlighting the increasing ability of GANs to interpret and visualize semantic information.\n\nThe foundational step towards controlled generation was the introduction of Conditional Generative Adversarial Nets (cGANs) by \\cite{Mirza2014}. Unlike their unconditional predecessors, cGANs feed additional conditional information, such as class labels or other attributes, to both the generator and the discriminator. This direct input guides the generator to produce samples corresponding to the specified conditions, while the discriminator learns to verify both the realism and the adherence to the given condition. Building upon this, Auxiliary Classifier GANs (AC-GANs) \\cite{Odena2017} further enhanced conditional synthesis by incorporating an auxiliary classifier into the discriminator. This classifier not only distinguishes between real and fake images but also predicts the class label of the input, thereby compelling the generator to produce samples that are both realistic and correctly classified, leading to better disentanglement and more robust conditional generation.\n\nThe capability of GANs was significantly expanded with the advent of text-to-image synthesis, where the conditional information takes the form of natural language descriptions. Early efforts, such as those by \\cite{Reed2016}, demonstrated the feasibility of generating images directly from text embeddings. These models mapped textual descriptions into a latent space, which then guided the generator to synthesize corresponding images, with the discriminator evaluating the consistency between the generated image and the input text. However, these initial models often struggled with generating high-resolution and photo-realistic images, particularly for complex scenes.\n\nTo overcome these limitations, multi-stage architectures emerged, notably StackGAN \\cite{Zhang2017}. StackGAN employs a two-stage process: the first stage generates a low-resolution image based on the global text description, and the second stage refines this initial output into a higher-resolution, photo-realistic image by focusing on finer details. This hierarchical approach significantly improved the quality and resolution of text-conditioned images. Further advancements in fine-grained control and semantic alignment were achieved with AttnGAN \\cite{Xu2018}, which introduced an attention mechanism. AttnGAN allows the generator to selectively attend to different words in the text description when generating specific regions of the image, ensuring that local image details are semantically consistent with relevant parts of the text.\n\nMore recently, the integration of robust architectures like StyleGAN with text conditioning has pushed the boundaries of quality and control. StyleGAN-T \\cite{Sauer2024} adapts the highly successful StyleGAN framework for text-to-image synthesis, leveraging its disentangled latent space and advanced generation capabilities. This approach yields high-fidelity, text-conditioned images with improved semantic alignment and offers more intuitive control over the generated output through natural language. This represents a significant leap, allowing users to specify desired outputs with natural language, opening doors for creative applications and content generation.\n\nDespite these remarkable advancements, challenges remain in conditional and text-to-image synthesis. Generating complex scenes with multiple objects and intricate spatial relationships, maintaining semantic consistency across diverse textual descriptions, and ensuring compositional understanding are still active areas of research. The robustness of these models to ambiguous or underspecified text prompts also needs improvement. Future directions will likely focus on enhancing the models' understanding of complex semantic compositions, improving the interpretability of generated outputs, and developing more robust evaluation metrics for text-to-image consistency.\n\\subsection{Domain Adaptation and Image-to-Image Translation}\n\\label{sec:5\\_4\\_domain\\_adaptation\\_\\_and\\_\\_image-to-image\\_translation}\n\nDomain adaptation and image-to-image translation represent a pivotal application area for Generative Adversarial Networks (GANs), enabling the learned transformation of visual content from one domain to another. This capability is fundamental for a wide array of computer vision tasks, including style transfer, image manipulation, and the generation of synthetic data for training other models \\cite{wang2019w53, liu2020jt0}. The success of these sophisticated applications is intrinsically linked to the advancements in GAN stability and the development of robust conditional generation techniques, building upon the foundational stability mechanisms discussed in Section 3 and the general conditional frameworks in Subsection 5.3.\n\nA seminal contribution to paired image-to-image translation was Pix2Pix, proposed by \\textcite{Isola2017}. This method introduced conditional adversarial networks that learn a direct mapping from an input image to a corresponding output image. By training a conditional generator and discriminator on aligned image pairs, Pix2Pix demonstrated remarkable success in tasks such as converting semantic labels to photorealistic street scenes, generating aerial photographs from maps, or transforming grayscale images to color. The core idea is that the generator learns to produce an output that not only fools the discriminator into believing it is real but also matches the input condition pixel-wise, often enforced with an additional L1 loss. This approach highlighted GANs' ability to capture complex, pixel-level correspondences, making them powerful tools for supervised image synthesis.\n\nHowever, the reliance of Pix2Pix on meticulously aligned training data posed a significant practical limitation, as such datasets are often scarce or impossible to acquire in real-world scenarios. To address this, \\textcite{Zhu2017} introduced CycleGAN, a groundbreaking method for unpaired image-to-image translation. CycleGAN ingeniously leverages a cycle consistency loss, which mandates that translating an image from domain A to domain B and then back to A should reconstruct the original image. This architectural innovation, involving two generators and two discriminators, enables effective translation between domains (e.g., horses to zebras, summer landscapes to winter landscapes, photographs to paintings) without requiring paired examples. The cycle consistency loss acts as a powerful self-supervisory signal, preventing the mapping from degenerating and ensuring semantic preservation during translation. This significantly broadened the applicability of image-to-image translation, democratizing its use for various style transfer, object transfiguration, and artistic rendering tasks.\n\nFollowing these foundational works, the field rapidly evolved to address more complex translation scenarios. A key advancement was the development of models capable of multi-domain image-to-image translation, moving beyond translating between just two specific domains. Approaches like G$^2$GAN \\cite{tang2018iie} introduced dual generator architectures to enable a single model to learn mappings across multiple target domains, improving scalability and reducing the need to train separate models for each domain pair. This was crucial for applications requiring flexible style transfer or attribute manipulation across a spectrum of visual styles or identities. Further research focused on disentangled representation learning, aiming to separate content from style in the latent space. While not explicitly covered by the provided papers, methods like MUNIT and DRIT allowed for more controllable and diverse translations by enabling users to combine content from one image with the style of another, offering fine-grained control over the generated output. Similarly, conditional image translation has been extended to high-resolution semantic synthesis, where models like GauGAN (SPADE) generate photorealistic images from semantic segmentation maps, showcasing the ability to interpret complex semantic layouts and produce highly detailed, controllable outputs.\n\nThe versatility of GANs in learning complex mappings, even without direct supervision in the unpaired case, underscores their profound utility across various computer vision applications \\cite{jabbar2020aj0}. Beyond artistic applications and style transfer, image-to-image translation has proven invaluable for data augmentation, particularly in data-scarce domains like medical imaging, where synthetic data can improve diagnostic model performance. It also facilitates tasks like image super-resolution, denoising, and inpainting, effectively acting as powerful image processing tools.\n\nDespite these significant strides, several challenges persist in domain adaptation and image-to-image translation. A primary concern is the trade-off between the fidelity and diversity of generated outputs; models often excel at one but struggle with the other. For instance, while cycle consistency helps preserve content, it can sometimes lead to the generator \"hiding\" information in steganographic patterns rather than truly learning the desired transformation, or it might struggle with large geometric changes between domains. Evaluating the perceptual realism and semantic consistency of translated images remains a complex problem, as traditional metrics often fail to capture the nuances of human perception. Furthermore, the interpretability of the learned mappings is often limited, making it difficult to understand \\textit{why} a particular translation occurs. Computational demands, especially for training high-resolution and multi-domain translation models, also remain a practical hurdle. Future research directions will likely focus on enhancing the control and interpretability of translation attributes, improving the robustness to diverse and challenging input conditions, and developing more sophisticated evaluation metrics that align better with human judgment. The integration of image-to-image translation with other advanced generative paradigms, such as diffusion models, also holds promise for overcoming current limitations in fidelity, diversity, and training stability.\n\n\n\\label{sec:emerging_frontiers:_3d-aware_synthesis_and_hybrid_generative_models}\n\n\\section{Emerging Frontiers: 3D-Aware Synthesis and Hybrid Generative Models}\n\\label{sec:emerging\\_frontiers:\\_3d-aware\\_synthesis\\_\\_and\\_\\_hybrid\\_generative\\_models}\n\n\\subsection{Bridging 2D GANs with 3D Neural Radiance Fields}\n\\label{sec:6\\_1\\_bridging\\_2d\\_gans\\_with\\_3d\\_neural\\_radiance\\_fields}\n\nWhile Generative Adversarial Networks (GANs) have achieved remarkable success in synthesizing photorealistic 2D images, as extensively discussed in Sections 4.3 and 4.4 regarding the StyleGAN family, their inherent lack of explicit 3D understanding limits their utility for applications requiring consistent multi-view generation or controllable 3D scene manipulation. This subsection explores the innovative and rapidly evolving direction of extending the capabilities of these high-fidelity 2D GANs to 3D-aware image synthesis by integrating them with Neural Radiance Fields (NeRFs). This methodological progression addresses the critical challenge of creating controllable 3D content from powerful 2D generative models, leveraging existing 2D strengths for tasks like virtual reality, content creation, and 3D reconstruction. The combination offers the benefits of GAN's high-quality texture generation and disentangled control with NeRF's inherent 3D consistency and novel view synthesis capabilities.\n\nThe core limitation of 2D GANs lies in their inability to guarantee geometric consistency across different viewpoints, as their generative process is fundamentally image-centric. Simultaneously, Neural Radiance Fields (NeRFs) \\cite{Mildenhall2020} emerged as a powerful paradigm for novel view synthesis, representing 3D scenes as continuous volumetric functions. NeRFs excel at rendering photorealistic and geometrically consistent novel views, but typically require extensive multi-view image datasets for training and lack an intuitive, disentangled latent space for content manipulation akin to StyleGANs. The challenge, therefore, became how to combine the photorealism and latent space control of 2D GANs with the 3D consistency of NeRFs, ideally without requiring explicit 3D supervision.\n\nEarly efforts to bridge this gap focused on learning generative models that could produce implicit 3D scene representations from a latent code, which could then be rendered into 2D images. Generative Radiance Fields (GRAF) \\cite{Schwarz2020} was among the first to propose a GAN-based approach for learning 3D-aware image synthesis. GRAF trained a GAN to generate parameters for a NeRF-like scene representation, enabling the synthesis of multi-view consistent images from a single latent vector. While a significant conceptual step, GRAF often produced lower-resolution outputs and faced challenges in achieving the same level of disentanglement and photorealism as state-of-the-art 2D GANs. Following this, pi-GAN \\cite{Chan2021} further explored implicit neural representations for 3D-aware synthesis, demonstrating improved disentanglement and quality by leveraging a hierarchical latent space and a progressive training scheme. GIRAFFE \\cite{Niemeyer2021} advanced this by introducing a compositional scene representation, allowing for the disentanglement of object pose, shape, and appearance, and enabling the generation of scenes with multiple objects and backgrounds, further enhancing controllable 3D-aware synthesis. These initial works laid the groundwork by demonstrating the feasibility of learning 3D-aware generative models from 2D image collections.\n\nA pivotal development in achieving high-fidelity 3D-aware synthesis involved directly integrating the powerful StyleGAN architecture with NeRFs. StyleNeRF \\cite{Gu2021} was an early attempt to adapt StyleGAN's generator to produce features for a NeRF, allowing for high-resolution 3D-consistent image generation while leveraging StyleGAN's disentangled latent space for control. It demonstrated that the rich semantic information encoded in StyleGAN's latent space could be effectively transferred to control 3D scene properties.\n\nThe most significant breakthrough in this domain, however, came with Efficient Geometry-aware 3D Generative Adversarial Networks (EG3D) \\cite{Chan2022}. This landmark paper proposed a highly efficient and high-fidelity method for 3D-aware image synthesis by explicitly leveraging a StyleGAN2 backbone to generate a \\textit{tri-plane feature representation}. Instead of generating a full 3D volume, EG3D projects the latent code into three orthogonal 2D feature planes (XY, XZ, YZ). A lightweight neural renderer then queries these tri-planes at specific 3D coordinates and viewing directions to predict color and density, effectively reconstructing the 3D scene. This tri-plane representation is crucial because it factorizes the 3D problem into a more manageable set of 2D operations, significantly reducing computational cost and memory requirements compared to volumetric NeRFs, while retaining 3D consistency. The StyleGAN's W-space directly controls the features within these tri-planes, allowing for precise and disentangled manipulation of 3D geometry and appearance, such as changing facial attributes or expressions in a 3D-consistent manner. EG3D achieved unprecedented levels of photorealism and view consistency for 3D-aware face generation, setting a new state-of-the-art.\n\nThe integration of 2D GANs with NeRFs represents a powerful synergy. GANs contribute their ability to generate photorealistic textures and offer a highly disentangled latent space for intuitive control, while NeRFs provide the necessary 3D consistency and novel view synthesis capabilities. This combination has opened new avenues for applications in virtual reality, where consistent 3D environments are paramount, and for content creation pipelines that demand both high visual fidelity and intuitive 3D control. It also facilitates 3D reconstruction from limited 2D inputs by leveraging the strong generative priors encoded within the latent space.\n\nDespite these remarkable advancements, challenges persist. While EG3D significantly improved efficiency, training and inference for these hybrid models remain computationally intensive compared to purely 2D GANs, especially for very high resolutions or complex, diverse scenes beyond specific object categories like faces. Generalization to open-world scenes or highly diverse object classes, where the underlying 3D geometry is more varied, is still an active research area. Achieving perfect geometric accuracy and photorealism across \\textit{all} viewpoints, particularly for highly occluded or unseen parts, remains difficult due to the inherent ambiguity of learning 3D from purely 2D data. Furthermore, while disentanglement has improved, the latent space mapping to desirable 3D properties is not always perfectly orthogonal, leading to some entanglement between attributes. Future research directions include improving the robustness and generalizability of these models to more complex and diverse scenes, enhancing the resolution and realism of generated 3D content, and exploring more efficient training and inference mechanisms. Integrating explicit geometric priors or sparse 3D supervision could further improve geometric accuracy, and extending these methods to dynamic 3D scenes or incorporating other generative paradigms like diffusion models for 3D-aware synthesis are promising avenues.\n\\subsection{The Rise of Diffusion Models and Their Integration with GANs}\n\\label{sec:6\\_2\\_the\\_rise\\_of\\_diffusion\\_models\\_\\_and\\_\\_their\\_integration\\_with\\_gans}\n\nThe generative modeling landscape has witnessed a profound shift with the emergence of diffusion models as a powerful alternative to Generative Adversarial Networks (GANs). Originating from foundational works like Denoising Diffusion Probabilistic Models (DDPMs) \\cite{Ho2020} and score-based generative models \\cite{Song2020}, diffusion models have rapidly gained prominence due to their exceptional training stability, robust mode coverage, and capacity for generating high-quality samples \\cite{Karras2022, peng2024kkw}. This section explores the rise of diffusion models, their inherent advantages and limitations, and the recent, significant trend of integrating them with GANs to synthesize their respective strengths.\n\nDiffusion models operate by learning to reverse a gradual, iterative noising process. During training, noise is progressively added to data, and the model learns to predict and remove this noise at each step, effectively denoising data to generate new samples from pure noise \\cite{Ho2020}. This denoising autoencoder approach inherently offers greater training stability and superior mode coverage compared to the adversarial min-max game of GANs, which is often plagued by issues like mode collapse and vanishing gradients \\cite{peng2024kkw}. Theoretically, \"push-forward\" generative models like GANs, which synthesize data by transforming a standard Gaussian random variable using a deterministic neural network, face a provable trade-off between fitting multimodal distributions and maintaining training stability due to Lipschitz constant constraints. Diffusion models, conversely, with their stacked networks and stochastic input at each step, do not suffer from such limitations, explaining their superior ability to capture data diversity and their inherent stability \\cite{salmona202283g}. Consequently, diffusion models have demonstrated remarkable diversity and fidelity in generated outputs across various domains. However, a notable limitation of early diffusion models was their inherently slow sampling speed, requiring numerous sequential steps to produce a single high-quality sample, posing a challenge for real-time applications. This critical bottleneck has been significantly addressed by innovations such as Denoising Diffusion Implicit Models (DDIMs) \\cite{Song2020} and progressive distillation techniques \\cite{Karras2022b}, which substantially accelerate the sampling process while largely preserving the high quality of generated content.\n\nDespite these advancements in diffusion models, GANs retain distinct advantages, particularly their fast inference capabilities—generating samples in a single forward pass—and their propensity for producing exceptionally sharp, crisp details \\cite{peng2024kkw}. This recognition has spurred a significant conceptual shift towards hybrid generative architectures that aim to synthesize the strengths of both paradigms. This integrated approach seeks to leverage GANs' efficiency and detail generation with diffusion models' robust training and comprehensive mode coverage, thereby overcoming the individual limitations of each.\n\nOne prominent direction in this hybridization is the integration of adversarial training principles directly into diffusion models. Early efforts, such as Adversarial Score Matching \\cite{Xiao2021}, demonstrated that a discriminator could be employed to guide the score network in diffusion models. In this framework, the discriminator learns to distinguish between real data and samples generated by the diffusion process at various intermediate timesteps, providing an adversarial signal that helps refine the denoising process and improve sample quality. Building upon this, \\cite{Karras2023} introduced Adversarial Diffusion Models (ADM), which frame the diffusion process within an adversarial learning setup. ADM employs a discriminator to guide the denoising network, typically by evaluating the realism of the \\textit{intermediate denoised outputs} or the \\textit{predicted noise} at different stages of the reverse process. This adversarial guidance aims to harness the sharpness and efficiency benefits traditionally associated with GANs, enhancing the perceptual quality and potentially accelerating the sampling speed of diffusion models, moving beyond purely diffusion-based objectives.\n\nFurther solidifying this trend, explicit \"Diffusion-GANs\" architectures have emerged, aiming to achieve the best of both worlds. For instance, You Only Sample Once (YOSO) \\cite{luo2024znt} proposes a novel self-cooperative diffusion GAN designed for rapid, scalable, and high-fidelity one-step image synthesis with high training stability and mode coverage. YOSO addresses the challenges of training instability and subpar one-step generation efficiency in previous hybrid models by smoothing the adversarial divergence through the denoising generator itself. This \"self-cooperative learning\" mechanism, combined with techniques like latent perceptual loss, a latent discriminator for efficient training, informative prior initialization (IPI), and a quick adaptation stage, allows YOSO to train from scratch for one-step generation with competitive performance, even adapting to higher resolutions without explicit retraining. Such models exemplify the strategic integration of GANs' fast inference and crisp detail generation with diffusion models' robust training and superior mode coverage.\n\nWhile these hybrid models promise enhanced performance by combining complementary strengths, they also introduce new complexities and trade-offs. As noted by \\cite{peng2024kkw}, existing fusion methods can still suffer from \"training instability and mode collapse or subpar one-step generation learning efficiency,\" indicating that the optimal balance between adversarial dynamics and diffusion processes remains an active area of research. The increased architectural complexity and the intricate interplay of different loss functions can make these models challenging to tune and optimize, potentially requiring more computational resources or specialized training strategies.\n\nIn conclusion, the rise of diffusion models has set a new benchmark for generative quality and stability, while their subsequent integration with GANs marks a pivotal moment in generative AI. This ongoing hybridization effort signifies a strategic evolution towards developing models that are not only stable and diverse but also efficient and capable of producing highly detailed outputs. Future research will undoubtedly continue to explore novel ways to synergize these powerful paradigms, pushing the boundaries of what is possible in synthetic content generation by combining their complementary strengths while navigating the inherent challenges of complex, integrated architectures.\n\n\n\\label{sec:conclusion,_open_challenges,_and_future_directions}\n\n\\section{Conclusion, Open Challenges, and Future Directions}\n\\label{sec:conclusion,\\_open\\_challenges,\\_\\_and\\_\\_future\\_directions}\n\n\\subsection{Summary of Progress in GAN Stabilization}\n\\label{sec:7\\_1\\_summary\\_of\\_progress\\_in\\_gan\\_stabilization}\n\nThe journey of Generative Adversarial Networks (GANs) has been marked by a relentless pursuit of stability, evolving from addressing initial training challenges to achieving high-fidelity, controllable synthesis across diverse applications. Early GANs, while groundbreaking, frequently suffered from training instability and mode collapse, where the generator failed to produce diverse samples \\cite{Goodfellow2014}. This fundamental problem necessitated systematic research into robust training methodologies and architectural innovations.\n\nInitial efforts focused on enhancing the stability of the adversarial training process. The introduction of Deep Convolutional GANs (DCGANs) by \\cite{Radford2015} provided architectural guidelines, leveraging convolutional layers to improve training stability and image quality. However, issues like mode collapse persisted due to the limitations of the original GAN loss function. A significant breakthrough came with the Wasserstein GAN (WGAN) \\cite{Arjovsky2017}, which proposed using the Earth-Mover distance as a loss function, offering a more stable gradient and mitigating mode collapse. This was further refined by \\cite{Gulrajani2017} with Wasserstein GAN with Gradient Penalty (WGAN-GP), which enforced a Lipschitz constraint through gradient penalties, leading to even more robust and stable training. Complementary to these loss function advancements, regularization techniques also played a crucial role. \\cite{roth2017eui} proposed a low-computational-cost regularization approach to stabilize GAN training, specifically addressing issues arising from dimensional mismatch or non-overlapping support between distributions. Similarly, \\cite{Miyato2018} introduced Spectral Normalization for GANs, a simple yet effective method to stabilize training by controlling the Lipschitz constant of the discriminator, further preventing pathological gradients.\n\nWith a more stable training foundation, the intellectual trajectory shifted towards achieving unprecedented levels of image fidelity and control. This era was largely defined by the StyleGAN family of architectures. \\cite{Karras2019} introduced StyleGAN, a style-based generator architecture that leveraged a mapping network and AdaIN layers to produce highly disentangled and controllable latent spaces, leading to state-of-the-art image synthesis. Subsequent iterations, StyleGAN2 \\cite{Karras2020} and StyleGAN3 \\cite{Karras2021}, further refined the architecture with advancements like path length regularization and alias-free design, pushing 2D image quality to near-photorealistic levels and addressing persistent visual artifacts. This mastery of 2D synthesis then opened doors to new frontiers, with \\cite{Chan2023} \\textit{H} demonstrating how StyleGAN's disentangled latent spaces could be integrated with Neural Radiance Fields (NeRFs) to enable high-quality 3D-aware image synthesis and novel view generation, effectively extending GAN capabilities into coherent 3D scene representation.\n\nSimultaneously, research expanded into scaling, efficiency, and data-agnostic applications. \\cite{Brock2018} pioneered large-scale GAN training with BigGAN, demonstrating the ability to synthesize high-fidelity images from diverse datasets like ImageNet. Training efficiency was further improved by \\cite{Sauer2021} with Projected GANs, which accelerated convergence. A critical practical challenge, the need for vast amounts of training data, was addressed by \\cite{Karras2022} through Adaptive Discriminator Augmentation (ADA), allowing GANs to be trained effectively with limited data. Building on this, \\cite{Sauer2023} scaled the StyleGAN architecture to handle large, diverse datasets with StyleGAN-XL, while \\cite{Sauer2024} unlocked text-to-image synthesis capabilities with StyleGAN-T, adapting GANs for fine-grained conditional generation. Pushing the boundaries of data efficiency even further, \\cite{Wang2023} \\textit{H} introduced a meta-learning approach for the discriminator, enabling it to quickly adapt to new datasets with very few samples, significantly reducing data requirements beyond what ADA could achieve.\n\nThe latest intellectual trajectory reveals an emerging trend of convergence and hybridization with other powerful generative paradigms. While GANs excelled in fast inference and high fidelity, challenges like mode coverage and training stability, particularly compared to diffusion models, persisted. Addressing this, \\cite{Liu2024} \\textit{H} proposed Diffusion-GAN, a novel hybrid generative model that combines the adversarial training of GANs with the denoising process of diffusion models. This innovative approach aims to leverage the strengths of both paradigms, seeking to achieve the fast inference of GANs alongside the enhanced stability and mode coverage characteristic of diffusion models.\n\nIn summary, systematic research has transformed GANs from a fragile, experimental concept into a robust, versatile, and highly performant class of generative models. The journey from addressing initial instability and mode collapse through robust loss functions and regularization, to achieving high-fidelity, controllable synthesis via architectural innovations, and expanding into data-efficient, multi-modal, and 3D applications, underscores the field's capacity for continuous innovation. This evolution, now embracing hybridization with other generative models, marks GANs as powerful tools capable of diverse and complex tasks, significantly contributing to the broader landscape of generative AI.\n\\subsection{Remaining Theoretical and Practical Challenges}\n\\label{sec:7\\_2\\_remaining\\_theoretical\\_\\_and\\_\\_practical\\_challenges}\n\nDespite the remarkable advancements in Generative Adversarial Networks (GANs), particularly in synthesizing high-fidelity and diverse content, the field continues to grapple with fundamental theoretical and practical challenges that limit their robustness, usability, and widespread adoption. These unresolved issues represent critical avenues for future research and development.\n\nA primary theoretical challenge revolves around the elusive nature of \\textbf{convergence guarantees} for GAN training. The adversarial min-max game, while powerful, inherently creates a non-convex, non-cooperative optimization problem that is notoriously difficult to stabilize. Early GANs \\cite{goodfellow2014generative} were plagued by instability, vanishing gradients, and oscillations. While subsequent works have introduced various regularization techniques and architectural improvements, a complete theoretical understanding of global convergence to a unique Nash equilibrium remains an open problem. For instance, \\textcite{roth2017eui} highlighted the \"dimensional mismatch or non-overlapping support\" between the model and data distributions as a source of instability, leading to undefined density ratios. The introduction of Wasserstein GANs \\cite{arjovsky2017ze5} aimed to provide a more meaningful and stable loss function, addressing issues like vanishing gradients and offering theoretical benefits. However, even improved variants like WGAN-GP \\cite{gulrajani2017improved} require careful tuning of the gradient penalty coefficient, demonstrating that practical stability often relies on empirical adjustments rather than robust theoretical guarantees. Similarly, spectral normalization \\cite{miyato2018spectral} offers an efficient way to enforce Lipschitz continuity, improving stability, but it is a regularization technique rather than a fundamental solution to the non-convergent game dynamics.\n\nAnother persistent theoretical hurdle is \\textbf{mode collapse}, where the generator fails to capture the full diversity of the real data distribution, instead producing a limited subset of samples. This issue is particularly pronounced in highly complex, multi-modal, or long-tail data distributions. \\textcite{che2016kho} explicitly addressed this, noting that GANs are \"prone to miss modes\" and proposed regularization methods to encourage a \"fair distribution of probability mass across the modes.\" While various techniques, including architectural changes \\cite{radford2015unsupervised} and loss function modifications \\cite{arjovsky2017ze5}, have aimed to mitigate mode collapse, it remains a significant concern, especially when training on large, diverse datasets like ImageNet, where the generator might prioritize generating common, high-quality samples over exploring rare but valid modes. The challenge is exacerbated by the difficulty of objectively quantifying mode coverage.\n\nThis leads to the third major theoretical challenge: the \\textbf{difficulty of objective evaluation metrics} beyond FID (Fréchet Inception Distance) and IS (Inception Score). While FID and IS are widely adopted, they possess inherent limitations. They often rely on pre-trained classifiers (like InceptionNet), which may not be robust to out-of-distribution samples or perfectly align with human perception. Furthermore, they can be sensitive to sample size and may not comprehensively capture all aspects of image quality and diversity, particularly mode coverage. The lack of a universally accepted, robust, and interpretable metric makes it challenging to objectively compare different GAN models, track progress, and definitively determine when a model has achieved optimal performance across both fidelity and diversity.\n\nFrom a practical standpoint, GANs present several significant challenges. The \\textbf{high computational resource demands} for training large models are a major barrier. Achieving state-of-the-art results, such as those demonstrated by BigGAN \\cite{brock2018large} for high-fidelity natural image synthesis, necessitated \"massive computational scale,\" including hundreds of GPUs and extensive training times. This limits accessibility for researchers and practitioners without substantial computational budgets, hindering rapid iteration and experimentation.\n\nClosely related is the \\textbf{sensitivity to hyperparameter tuning}. GANs are notoriously finicky, requiring meticulous selection of learning rates, batch sizes, network architectures, and regularization coefficients. As noted by \\textcite{roth2017eui}, their fragility demands a \"careful choice of architecture, parameter initialization, and selection of hyper-parameters.\" Even advanced techniques like WGAN-GP \\cite{gulrajani2017improved} introduce new hyperparameters (e.g., the gradient penalty coefficient) that require careful calibration, often through extensive and costly trial-and-error. This empirical burden makes GAN training a highly specialized skill rather than a straightforward process.\n\nFinally, the \\textbf{difficulty of training on highly diverse, real-world datasets} persists. While models like StyleGAN-XL \\cite{sauer2023stylegan} have pushed the boundaries of scaling StyleGAN to ImageNet-scale diversity, achieving both high fidelity and comprehensive mode coverage on such complex datasets remains a formidable task. The inherent diversity of real-world data often exacerbates mode collapse and training instability. Furthermore, many real-world applications involve limited data scenarios, which GANs traditionally struggle with. While techniques like Adaptive Discriminator Augmentation (ADA) \\cite{karras2022training} have made significant strides in enabling GAN training with limited data, the problem of few-shot or zero-shot generation on highly diverse distributions remains largely open.\n\nIn conclusion, despite their transformative impact, GANs are far from a \"solved problem.\" The fundamental theoretical questions surrounding convergence and comprehensive mode coverage, coupled with practical hurdles related to computational cost, hyperparameter sensitivity, and robust training on diverse real-world data, highlight critical areas ripe for future investigation. Addressing these challenges will be crucial for enhancing the robustness, usability, and theoretical grounding of generative adversarial models, potentially through novel architectural designs, improved optimization strategies, or hybrid approaches that integrate insights from complementary generative paradigms.\n\\subsection{Ethical Considerations and Societal Impact}\n\\label{sec:7\\_3\\_ethical\\_considerations\\_\\_and\\_\\_societal\\_impact}\n\nThe remarkable advancements in Generative Adversarial Networks (GANs) have ushered in a new era of synthetic media generation, presenting a complex ethical landscape marked by both profound opportunities and significant risks. As GANs evolve from foundational models to highly capable architectures capable of photorealistic and 3D-aware synthesis, the broader societal implications demand rigorous scrutiny, moving beyond mere technical capabilities to address issues of trust, fairness, and accountability \\cite{bhat202445j}.\n\nA primary ethical concern revolves around the potential for misuse, particularly the generation and dissemination of \"deepfakes\" and misinformation. The increasing fidelity and disentangled control offered by architectures like the StyleGAN family \\cite{Karras2019, Karras2020, Karras2021} have made it possible to create highly convincing synthetic media that can misrepresent individuals, manipulate public opinion, and orchestrate sophisticated misinformation campaigns. This capability extends beyond 2D images, with the integration of StyleGAN latents with Neural Radiance Fields (NeRFs) enabling 3D-aware synthesis \\cite{Chan2023}, further blurring the lines between reality and simulation in immersive contexts. Such technological prowess contributes to what scholars term the \"liar's dividend,\" where the very existence of highly realistic synthetic media erodes public trust in \\textit{all} digital content, including authentic media, making it harder to discern truth from fabrication \\cite{chesney2019deepfakes}. The accessibility of generating specific content through text-to-image models, such as StyleGAN-T \\cite{Sauer2024}, further lowers the barrier for creating targeted disinformation or hate speech imagery, posing substantial challenges for content moderation, legal frameworks, and societal cohesion. The urgent need for robust detection mechanisms for synthetic media is paramount to counteract these threats, though the arms race between generation and detection remains a persistent challenge.\n\nAnother critical ethical dimension is the amplification and perpetuation of biases inherent in training data. While efforts to scale GANs to diverse datasets \\cite{Sauer2023} and improve data efficiency \\cite{Karras2022} are vital for broader applicability, they simultaneously highlight the risk of exacerbating societal inequalities. If training datasets reflect existing biases—such as underrepresentation of certain demographics, stereotypical portrayals, or historical inequities—GANs, even those employing advanced techniques like few-shot learning via meta-learning discriminators \\cite{Wang2023}, can inadvertently learn and amplify these biases in their generated outputs. This can lead to discriminatory outcomes, including biased facial recognition systems, misrepresentation in synthetic media, or the generation of content that reinforces harmful stereotypes. Addressing this requires not only careful dataset curation but also the development and implementation of rigorous bias auditing and mitigation strategies throughout the model lifecycle, from data collection to deployment, ensuring transparency and accountability in generative AI systems \\cite{bhat202445j}.\n\nDespite these substantial risks, the societal impact of highly capable generative models also encompasses immense positive potential, particularly in addressing real-world challenges. GANs have emerged as powerful creative tools for artists and designers, enabling novel forms of digital art and content creation by offering intuitive control over image synthesis \\cite{Karras2019}. More critically, in domains where data scarcity is a significant bottleneck, GANs provide a vital solution through high-fidelity data augmentation. For instance, in the medical field, the lack of annotated datasets for rare skin conditions poses a major challenge for diagnostic model development. Deep Generative Adversarial Networks (DGANs) have been successfully employed to generate synthetic skin problem images, effectively augmenting imbalanced datasets and significantly improving the diagnostic accuracy of multi-class classifiers, outperforming traditional augmentation methods \\cite{khan20223o7}. Similarly, in disaster response, where labeled imagery data is often limited and imbalanced, GANs have been utilized to synthesize diverse disaster images, thereby enhancing the training of deep convolutional neural networks for rapid damage identification and classification, leading to more efficient aid direction and resource allocation \\cite{eltehewy2023cj4}. These applications demonstrate how the enhanced stability and quality achieved through GAN research can directly translate into tangible societal benefits, improving model robustness and expanding the applicability of AI in critical sectors.\n\nIn conclusion, the trajectory of generative models, from initial stabilization to sophisticated 3D-aware and text-conditional synthesis, underscores an urgent need for responsible development and deployment. Mitigating harm and maximizing benefit necessitates a multi-faceted approach. This includes not only the continuous development of robust detection mechanisms for synthetic media but also the implementation of rigorous bias auditing and mitigation strategies in model training. Furthermore, the establishment of comprehensive ethical guidelines and policy frameworks for the use of GAN technologies is crucial to navigate the complex interplay between technological innovation and societal well-being. Balancing the transformative power of these models with foresight and a commitment to ethical principles is paramount to safeguarding societal trust and equity in the digital age.\n\\subsection{Future Research Directions}\n\\label{sec:7\\_4\\_future\\_research\\_directions}\n\nWhile significant strides have been made in stabilizing Generative Adversarial Networks (GANs) through foundational techniques like regularization \\cite{roth2017eui} and architectural innovations, the field continues to evolve rapidly, opening numerous promising avenues for future research. These directions aim to push the boundaries of generative AI, addressing its inherent complexities while expanding its utility and impact.\n\nOne particularly fertile ground for innovation lies in the further exploration of \\textbf{hybrid models} that combine the strengths of GANs with other powerful generative paradigms, such as diffusion models or transformer architectures. The inherent efficiency of GAN inference, coupled with the superior stability and mode coverage of diffusion models, presents a compelling synergy. This hybridization is exemplified by \\cite{Liu2024}, which introduces \"Diffusion-GAN\" to bridge these two frameworks, aiming to achieve enhanced stability and quality. Future work can build upon this by exploring more sophisticated integration strategies, potentially incorporating transformer-based components for improved contextual understanding and long-range dependency modeling, especially for complex multimodal generation tasks that span images, text, audio, and beyond.\n\nA critical challenge for widespread adoption remains the substantial data requirements of GANs. Therefore, advancements in \\textbf{few-shot and zero-shot generation} are paramount to reduce data dependency. Building upon techniques for limited data training, such as Adaptive Discriminator Augmentation \\cite{Karras2022} (as discussed in the evolution analysis), \\cite{Wang2023} introduces a meta-learning approach for discriminators to quickly adapt to new datasets with very few samples. This significantly reduces the need for extensive annotated data, paving the way for GANs to be deployed in data-scarce domains. Future research should focus on developing more robust meta-learning algorithms, exploring novel transfer learning strategies, and investigating how to leverage pre-trained models more effectively for truly zero-shot generation capabilities.\n\nThe inherent efficiency of GAN inference positions them ideally for the development of \\textbf{real-time and interactive generative systems}. As GAN architectures become more refined and computationally optimized, the potential for immediate visual feedback and dynamic content creation grows. Further pushing the boundaries of modality expansion, \\cite{Chan2023} demonstrates how StyleGAN's disentangled latent space can be integrated with Neural Radiance Fields (NeRFs) to enable high-quality 3D-aware image synthesis and novel view generation. This capability is a crucial step towards interactive 3D content creation and virtual environments. Future work should focus on optimizing these systems for even lower latency, enabling seamless user interaction, and expanding into other modalities like real-time audio synthesis or interactive video generation.\n\nBeyond current applications, expanding GANs into \\textbf{new modalities and applications beyond images} is a key future direction. While significant progress has been made in image synthesis, the principles of adversarial training can be applied to diverse data types. The success of 3D-aware generation \\cite{Chan2023} and text-to-image synthesis \\cite{Sauer2024} (as highlighted in the evolution analysis) illustrates this potential. Future research could explore GANs for generating complex scientific data, medical images, molecular structures, or even code, opening up entirely new application domains.\n\nCrucially, the development of more \\textbf{robust and interpretable evaluation metrics} remains paramount. Current metrics often fall short in capturing the perceptual quality, diversity, and fidelity of generated content, especially as models become more sophisticated. Future work must focus on creating metrics that are not only quantitative but also align better with human perception and can provide actionable insights into model shortcomings. Furthermore, as generative AI becomes more powerful, ensuring its \\textbf{responsible deployment} is non-negotiable. This includes addressing biases in generated content, developing methods for detecting AI-generated media, ensuring transparency, and establishing ethical guidelines for their use.\n\nIn conclusion, the future of GAN research is characterized by a drive towards greater versatility, efficiency, and integration. By embracing hybrid architectures, minimizing data dependency, enabling real-time interaction, expanding into new modalities, and prioritizing responsible deployment alongside robust evaluation, the field can truly unlock the full potential of generative AI, pushing the boundaries of what these complex systems can achieve.\n\n\n\\newpage\n\\section*{References}\n\\addcontentsline{toc}{section}{References}\n\n\\begin{thebibliography}{194}\n\n\\bibitem{arjovsky2017ze5}\nMartín Arjovsky, Soumith Chintala, and L. Bottou (2017). \\textit{Wasserstein Generative Adversarial Networks}. International Conference on Machine Learning.\n\n\\bibitem{karras2017raw}\nTero Karras, Timo Aila, S. Laine, et al. (2017). \\textit{Progressive Growing of GANs for Improved Quality, Stability, and Variation}. International Conference on Learning Representations.\n\n\\bibitem{miyato2018arc}\nTakeru Miyato, Toshiki Kataoka, Masanori Koyama, et al. (2018). \\textit{Spectral Normalization for Generative Adversarial Networks}. International Conference on Learning Representations.\n\n\\bibitem{karras202039x}\nTero Karras, M. Aittala, Janne Hellsten, et al. (2020). \\textit{Training Generative Adversarial Networks with Limited Data}. Neural Information Processing Systems.\n\n\\bibitem{zhang2016mm0}\nHan Zhang, Tao Xu, Hongsheng Li, et al. (2016). \\textit{StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks}. IEEE International Conference on Computer Vision.\n\n\\bibitem{shrivastava2016uym}\nA. Shrivastava, Tomas Pfister, Oncel Tuzel, et al. (2016). \\textit{Learning from Simulated and Unsupervised Images through Adversarial Training}. Computer Vision and Pattern Recognition.\n\n\\bibitem{zhao2020xhy}\nShengyu Zhao, Zhijian Liu, Ji Lin, et al. (2020). \\textit{Differentiable Augmentation for Data-Efficient GAN Training}. Neural Information Processing Systems.\n\n\\bibitem{metz20169ir}\nLuke Metz, Ben Poole, David Pfau, et al. (2016). \\textit{Unrolled Generative Adversarial Networks}. International Conference on Learning Representations.\n\n\\bibitem{guo2020n4t}\nYe-cai Guo, Hanyu Li, and Peixian Zhuang (2020). \\textit{Underwater Image Enhancement Using a Multiscale Dense Generative Adversarial Network}. IEEE Journal of Oceanic Engineering.\n\n\\bibitem{bau2018n2x}\nDavid Bau, Jun-Yan Zhu, Hendrik Strobelt, et al. (2018). \\textit{GAN Dissection: Visualizing and Understanding Generative Adversarial Networks}. International Conference on Learning Representations.\n\n\\bibitem{che2016kho}\nTong Che, Yanran Li, Athul Paul Jacob, et al. (2016). \\textit{Mode Regularized Generative Adversarial Networks}. International Conference on Learning Representations.\n\n\\bibitem{liu20212c2}\nBingchen Liu, Yizhe Zhu, Kunpeng Song, et al. (2021). \\textit{Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis}. International Conference on Learning Representations.\n\n\\bibitem{jabbar2020aj0}\nAbdul Jabbar, Xi Li, and Bourahla Omar (2020). \\textit{A Survey on Generative Adversarial Networks: Variants, Applications, and Training}. ACM Computing Surveys.\n\n\\bibitem{roth2017eui}\nKevin Roth, Aurélien Lucchi, Sebastian Nowozin, et al. (2017). \\textit{Stabilizing Training of Generative Adversarial Networks through Regularization}. Neural Information Processing Systems.\n\n\\bibitem{yang2018svo}\nLiu Yang, Dongkun Zhang, and G. Karniadakis (2018). \\textit{Physics-Informed Generative Adversarial Networks for Stochastic Differential Equations}. SIAM Journal on Scientific Computing.\n\n\\bibitem{zhang2019hjo}\nHan Zhang, Zizhao Zhang, Augustus Odena, et al. (2019). \\textit{Consistency Regularization for Generative Adversarial Networks}. International Conference on Learning Representations.\n\n\\bibitem{tseng2021m2s}\nHung-Yu Tseng, Lu Jiang, Ce Liu, et al. (2021). \\textit{Regularizing Generative Adversarial Networks under Limited Data}. Computer Vision and Pattern Recognition.\n\n\\bibitem{mao20196tx}\nWentao Mao, Yamin Liu, Ling Ding, et al. (2019). \\textit{Imbalanced Fault Diagnosis of Rolling Bearing Based on Generative Adversarial Network: A Comparative Study}. IEEE Access.\n\n\\bibitem{hartmann2018h3s}\nK. Hartmann, R. Schirrmeister, and T. Ball (2018). \\textit{EEG-GAN: Generative adversarial networks for electroencephalograhic (EEG) brain signals}. arXiv.org.\n\n\\bibitem{wang2019w53}\nZhengwei Wang, Qi She, and T. Ward (2019). \\textit{Generative Adversarial Networks in Computer Vision}. ACM Computing Surveys.\n\n\\bibitem{luo2020aaj}\nJia Luo, Jinying Huang, and Hongmei Li (2020). \\textit{A case study of conditional deep convolutional generative adversarial networks in machine fault diagnosis}. Journal of Intelligent Manufacturing.\n\n\\bibitem{liu2020jt0}\nMing-Yu Liu, Xun Huang, Jiahui Yu, et al. (2020). \\textit{Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications}. Proceedings of the IEEE.\n\n\\bibitem{liang2018r52}\nTengyuan Liang, and J. Stokes (2018). \\textit{Interaction Matters: A Note on Non-asymptotic Local Convergence of Generative Adversarial Networks}. International Conference on Artificial Intelligence and Statistics.\n\n\\bibitem{ghafoorian2018fwh}\nMohsen Ghafoorian, C. Nugteren, N. Baka, et al. (2018). \\textit{EL-GAN: Embedding Loss Driven Generative Adversarial Networks for Lane Detection}. ECCV Workshops.\n\n\\bibitem{guo2019414}\nXiaopeng Guo, Rencan Nie, Jinde Cao, et al. (2019). \\textit{FuseGAN: Learning to Fuse Multi-Focus Image via Conditional Generative Adversarial Network}. IEEE transactions on multimedia.\n\n\\bibitem{liu2020kd1}\nB. Liu, Cheng Tan, Shuqin Li, et al. (2020). \\textit{A Data Augmentation Method Based on Generative Adversarial Networks for Grape Leaf Disease Identification}. IEEE Access.\n\n\\bibitem{hjelm2017iqg}\nR. Devon Hjelm, Athul Paul Jacob, Tong Che, et al. (2017). \\textit{Boundary-Seeking Generative Adversarial Networks}. International Conference on Learning Representations.\n\n\\bibitem{shahriar2020sm7}\nMd Hasan Shahriar, Nur Imtiazul Haque, M. Rahman, et al. (2020). \\textit{G-IDS: Generative Adversarial Networks Assisted Intrusion Detection System}. Annual International Computer Software and Applications Conference.\n\n\\bibitem{pfau2016v7o}\nDavid Pfau, and O. Vinyals (2016). \\textit{Connecting Generative Adversarial Networks and Actor-Critic Methods}. arXiv.org.\n\n\\bibitem{mao2017ss0}\nXudong Mao, Qing Li, Haoran Xie, et al. (2017). \\textit{On the Effectiveness of Least Squares Generative Adversarial Networks}. IEEE Transactions on Pattern Analysis and Machine Intelligence.\n\n\\bibitem{fekri2019c1i}\nMohammad Navid Fekri, A. M. Ghosh, and Katarina Grolinger (2019). \\textit{Generating Energy Data for Machine Learning with Recurrent Generative Adversarial Networks}. Energies.\n\n\\bibitem{chen2019ng2}\nXinyuan Chen, Chang Xu, Xiaokang Yang, et al. (2019). \\textit{Gated-GAN: Adversarial Gated Networks for Multi-Collection Style Transfer}. IEEE Transactions on Image Processing.\n\n\\bibitem{baby2019h4h}\nDeepak Baby, and S. Verhulst (2019). \\textit{Sergan: Speech Enhancement Using Relativistic Generative Adversarial Networks with Gradient Penalty}. IEEE International Conference on Acoustics, Speech, and Signal Processing.\n\n\\bibitem{wiatrak20194ib}\nMaciej Wiatrak, Stefano V. Albrecht, and A. Nystrom (2019). \\textit{Stabilizing Generative Adversarial Networks: A Survey}. Unpublished manuscript.\n\n\\bibitem{salmona202283g}\nAntoine Salmona, Valentin De Bortoli, J. Delon, et al. (2022). \\textit{Can Push-forward Generative Models Fit Multimodal Distributions?}. Neural Information Processing Systems.\n\n\\bibitem{lee20205ue}\nKwot Sin Lee, Ngoc-Trung Tran, and Ngai-Man Cheung (2020). \\textit{InfoMax-GAN: Improved Adversarial Image Generation via Information Maximization and Contrastive Learning}. IEEE Workshop/Winter Conference on Applications of Computer Vision.\n\n\\bibitem{herr20208x4}\nDaniel Herr, B. Obert, and Matthias Rosenkranz (2020). \\textit{Anomaly detection with variational quantum generative adversarial networks}. Quantum Science and Technology.\n\n\\bibitem{hayes201742g}\nJamie Hayes, Luca Melis, G. Danezis, et al. (2017). \\textit{LOGAN: Evaluating Privacy Leakage of Generative Models Using Generative Adversarial Networks}. arXiv.org.\n\n\\bibitem{negi20208n9}\nAnuja Negi, A. Noel, Joseph Raj, et al. (2020). \\textit{RDA-UNET-WGAN: An Accurate Breast Ultrasound Lesion Segmentation Using Wasserstein Generative Adversarial Networks}. The Arabian journal for science and engineering.\n\n\\bibitem{meng2022you}\nZong Meng, Qian Li, De-gang Sun, et al. (2022). \\textit{An Intelligent Fault Diagnosis Method of Small Sample Bearing Based on Improved Auxiliary Classification Generative Adversarial Network}. IEEE Sensors Journal.\n\n\\bibitem{liu2019sb7}\nYi Liu, Jialiang Peng, James J. Q. Yu, et al. (2019). \\textit{PPGAN: Privacy-Preserving Generative Adversarial Network}. International Conference on Parallel and Distributed Systems.\n\n\\bibitem{yuan2020bt6}\nZhenmou Yuan, M. Jiang, Yaming Wang, et al. (2020). \\textit{SARA-GAN: Self-Attention and Relative Average Discriminator Based Generative Adversarial Networks for Fast Compressed Sensing MRI Reconstruction}. Frontiers in Neuroinformatics.\n\n\\bibitem{agarwal2022p6d}\nAishwarya Agarwal, Biplab Banerjee, Fabio Cuzzolin, et al. (2022). \\textit{Semantics-Driven Generative Replay for Few-Shot Class Incremental Learning}. ACM Multimedia.\n\n\\bibitem{grnarova20171tc}\nPaulina Grnarova, K. Levy, Aurélien Lucchi, et al. (2017). \\textit{An Online Learning Approach to Generative Adversarial Networks}. International Conference on Learning Representations.\n\n\\bibitem{liu2019oc8}\nZhiyue Liu, Jiahai Wang, and Zhiwei Liang (2019). \\textit{CatGAN: Category-aware Generative Adversarial Networks with Hierarchical Evolutionary Learning for Category Text Generation}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{chung2022s9a}\nJihoon Chung, Bo Shen, and Zhen Kong (2022). \\textit{Anomaly detection in additive manufacturing processes using supervised classification with imbalanced sensor data based on generative adversarial network}. Journal of Intelligent Manufacturing.\n\n\\bibitem{chu2020zbv}\nCasey Chu, Kentaro Minami, and K. Fukumizu (2020). \\textit{Smoothness and Stability in GANs}. International Conference on Learning Representations.\n\n\\bibitem{jenni2019339}\nS. Jenni, and P. Favaro (2019). \\textit{On Stabilizing Generative Adversarial Training With Noise}. Computer Vision and Pattern Recognition.\n\n\\bibitem{xiang20171at}\nSitao Xiang, and Hao Li (2017). \\textit{On the Effects of Batch and Weight Normalization in Generative Adversarial Networks}. Unpublished manuscript.\n\n\\bibitem{neyshabur201713g}\nBehnam Neyshabur, Srinadh Bhojanapalli, and Ayan Chakrabarti (2017). \\textit{Stabilizing GAN Training with Multiple Random Projections}. arXiv.org.\n\n\\bibitem{bau20197hm}\nDavid Bau, Jun-Yan Zhu, Hendrik Strobelt, et al. (2019). \\textit{Visualizing and Understanding Generative Adversarial Networks (Extended Abstract)}. arXiv.org.\n\n\\bibitem{dieng2019rjn}\nA. B. Dieng, Francisco J. R. Ruiz, D. Blei, et al. (2019). \\textit{Prescribed Generative Adversarial Networks}. arXiv.org.\n\n\\bibitem{zhang2020376}\nHongliang Zhang, Rui Wang, Ruilin Pan, et al. (2020). \\textit{Imbalanced Fault Diagnosis of Rolling Bearing Using Enhanced Generative Adversarial Networks}. IEEE Access.\n\n\\bibitem{yuan202257j}\nChao Yuan, Hongxia Wang, Peisong He, et al. (2022). \\textit{GAN-based image steganography for enhancing security via adversarial attack and pixel-wise deep fusion}. Multimedia tools and applications.\n\n\\bibitem{iwai2020fp2}\nShoma Iwai, Tomo Miyazaki, Yoshihiro Sugaya, et al. (2020). \\textit{Fidelity-Controllable Extreme Image Compression with Generative Adversarial Networks}. International Conference on Pattern Recognition.\n\n\\bibitem{kaneko2018jex}\nTakuhiro Kaneko, Y. Ushiku, and T. Harada (2018). \\textit{Label-Noise Robust Generative Adversarial Networks}. Computer Vision and Pattern Recognition.\n\n\\bibitem{khan20223o7}\nMaleika Heenaye-Mamode Khan, N. Gooda Sahib-Kaudeer, Motean Dayalen, et al. (2022). \\textit{Multi-Class Skin Problem Classification Using Deep Generative Adversarial Network (DGAN)}. Computational Intelligence and Neuroscience.\n\n\\bibitem{lin20224oj}\nQiuzhen Lin, Z. Fang, Yi Chen, et al. (2022). \\textit{Evolutionary Architectural Search for Generative Adversarial Networks}. IEEE Transactions on Emerging Topics in Computational Intelligence.\n\n\\bibitem{tuan2018kbr}\nYi-Lin Tuan, and Hung-yi Lee (2018). \\textit{Improving Conditional Sequence Generative Adversarial Networks by Stepwise Evaluation}. IEEE/ACM Transactions on Audio Speech and Language Processing.\n\n\\bibitem{wei2021qea}\nKaimin Wei, Tianqi Li, Feiran Huang, et al. (2021). \\textit{Cancer classification with data augmentation based on generative adversarial networks}. Frontiers of Computer Science.\n\n\\bibitem{wang20178xf}\nRuohan Wang, Antoine Cully, H. Chang, et al. (2017). \\textit{MAGAN: Margin Adaptation for Generative Adversarial Networks}. arXiv.org.\n\n\\bibitem{sage2017ywd}\nAlexander Sage, E. Agustsson, R. Timofte, et al. (2017). \\textit{Logo Synthesis and Manipulation with Clustered Generative Adversarial Networks}. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n\n\\bibitem{wu2020p8p}\nYue Wu, Pan Zhou, A. Wilson, et al. (2020). \\textit{Improving GAN Training with Probability Ratio Clipping and Sample Reweighting}. Neural Information Processing Systems.\n\n\\bibitem{chavdarova20179w6}\nTatjana Chavdarova, and F. Fleuret (2017). \\textit{SGAN: An Alternative Training of Generative Adversarial Networks}. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n\n\\bibitem{li2020muy}\nZiqiang Li, Pengfei Xia, Rentuo Tao, et al. (2020). \\textit{A New Perspective on Stabilizing GANs Training: Direct Adversarial Training}. IEEE Transactions on Emerging Topics in Computational Intelligence.\n\n\\bibitem{goudarzi2020ymw}\nSobhan Goudarzi, A. Asif, and H. Rivaz (2020). \\textit{Fast Multi-Focus Ultrasound Image Recovery Using Generative Adversarial Networks}. IEEE Transactions on Computational Imaging.\n\n\\bibitem{tao20219q2}\nYuechuan Tao, J. Qiu, and Shuying Lai (2021). \\textit{A Data-Driven Management Strategy of Electric Vehicles and Thermostatically Controlled Loads Based on Modified Generative Adversarial Network}. IEEE Transactions on Transportation Electrification.\n\n\\bibitem{zhong2019opk}\nYue Zhong, Lizhuang Liu, Dan Zhao, et al. (2019). \\textit{A generative adversarial network for image denoising}. Multimedia tools and applications.\n\n\\bibitem{yan2020889}\nPeiyao Yan, Feng He, Yajie Yang, et al. (2020). \\textit{Semi-Supervised Representation Learning for Remote Sensing Image Classification Based on Generative Adversarial Networks}. IEEE Access.\n\n\\bibitem{lee20203j4}\nShindong Lee, Bonggu Ko, Keonnyeong Lee, et al. (2020). \\textit{Many-To-Many Voice Conversion Using Conditional Cycle-Consistent Adversarial Networks}. IEEE International Conference on Acoustics, Speech, and Signal Processing.\n\n\\bibitem{hu2021yk5}\nTianyu Hu, Yang Huang, Qiuming Zhu, et al. (2021). \\textit{Channel Estimation Enhancement With Generative Adversarial Networks}. IEEE Transactions on Cognitive Communications and Networking.\n\n\\bibitem{chen2021n5h}\nTianlong Chen, Yu Cheng, Zhe Gan, et al. (2021). \\textit{Ultra-Data-Efficient GAN Training: Drawing A Lottery Ticket First, Then Training It Toughly}. arXiv.org.\n\n\\bibitem{cai2019g1w}\nYali Cai, Xiaoru Wang, Zhihong Yu, et al. (2019). \\textit{Dualattn-GAN: Text to Image Synthesis With Dual Attentional Generative Adversarial Network}. IEEE Access.\n\n\\bibitem{zhou20199sm}\nNiyun Zhou, De Cai, Xiao Han, et al. (2019). \\textit{Enhanced Cycle-Consistent Generative Adversarial Network for Color Normalization of H&E Stained Images}. International Conference on Medical Image Computing and Computer-Assisted Intervention.\n\n\\bibitem{tang2018iie}\nHao Tang, Dan Xu, Wei Wang, et al. (2018). \\textit{Dual Generator Generative Adversarial Networks for Multi-Domain Image-to-Image Translation}. Asian Conference on Computer Vision.\n\n\\bibitem{tong2022lu4}\nQ. Tong, Feiyu Lu, Ziwei Feng, et al. (2022). \\textit{A Novel Method for Fault Diagnosis of Bearings with Small and Imbalanced Data Based on Generative Adversarial Networks}. Applied Sciences.\n\n\\bibitem{costa2019pj9}\nVictor Costa, Nuno Lourenço, and P. Machado (2019). \\textit{Coevolution of Generative Adversarial Networks}. EvoApplications.\n\n\\bibitem{tang2021c82}\nHongtao Tang, Shengbo Gao, Lei Wang, et al. (2021). \\textit{A Novel Intelligent Fault Diagnosis Method for Rolling Bearings Based on Wasserstein Generative Adversarial Network and Convolutional Neural Network under Unbalanced Dataset}. Italian National Conference on Sensors.\n\n\\bibitem{yin2022izd}\nHaitao Yin, and Jing Xiao (2022). \\textit{Laplacian Pyramid Generative Adversarial Network for Infrared and Visible Image Fusion}. IEEE Signal Processing Letters.\n\n\\bibitem{xu2020pkq}\nKun Xu, Chongxuan Li, Huanshu Wei, et al. (2020). \\textit{Understanding and Stabilizing GANs' Training Dynamics Using Control Theory}. International Conference on Machine Learning.\n\n\\bibitem{rahman2021wm8}\nTaseef Rahman, Yuanqi Du, Liang Zhao, et al. (2021). \\textit{Generative Adversarial Learning of Protein Tertiary Structures}. Molecules.\n\n\\bibitem{zhang2022ysl}\nZheng Zhang, Jingsong Yang, and Yang Du (2022). \\textit{Deep Convolutional Generative Adversarial Network With Autoencoder for Semisupervised SAR Image Classification}. IEEE Geoscience and Remote Sensing Letters.\n\n\\bibitem{varshney2021954}\nSakshi Varshney, V. Verma, K. SrijithP., et al. (2021). \\textit{CAM-GAN: Continual Adaptation Modules for Generative Adversarial Networks}. Neural Information Processing Systems.\n\n\\bibitem{creswell2016mol}\nAntonia Creswell, and A. Bharath (2016). \\textit{Adversarial Training for Sketch Retrieval}. ECCV Workshops.\n\n\\bibitem{bang2018ps8}\nDuhyeon Bang, and Hyunjung Shim (2018). \\textit{Improved Training of Generative Adversarial Networks Using Representative Features}. International Conference on Machine Learning.\n\n\\bibitem{wang202066v}\nDong Wang, Xiaoqian Qin, F. Song, et al. (2020). \\textit{Stabilizing Training of Generative Adversarial Nets via Langevin Stein Variational Gradient Descent}. IEEE Transactions on Neural Networks and Learning Systems.\n\n\\bibitem{cai2020n2k}\nLikun Cai, Yanjie Chen, Ning Cai, et al. (2020). \\textit{Utilizing Amari-Alpha Divergence to Stabilize the Training of Generative Adversarial Networks}. Entropy.\n\n\\bibitem{wenzel20225g3}\nMarkus T. Wenzel (2022). \\textit{Generative Adversarial Networks and Other Generative Models}. arXiv.org.\n\n\\bibitem{gidel2018pg0}\nG. Gidel, Hugo Berard, Pascal Vincent, et al. (2018). \\textit{A Variational Inequality Perspective on Generative Adversarial Nets}. arXiv.org.\n\n\\bibitem{grinblat2017cem}\nG. Grinblat, Lucas C. Uzal, and P. Granitto (2017). \\textit{Class-Splitting Generative Adversarial Networks}. arXiv.org.\n\n\\bibitem{zhang202263o}\nYingxue Zhang, Yanhua Li, Xun Zhou, et al. (2022). \\textit{STrans-GAN: Spatially-Transferable Generative Adversarial Networks for Urban Traffic Estimation}. Industrial Conference on Data Mining.\n\n\\bibitem{shin2020169}\nHoo-Chang Shin, Alvin Ihsani, Ziyue Xu, et al. (2020). \\textit{GANDALF: Generative Adversarial Networks with Discriminator-Adaptive Loss Fine-tuning for Alzheimer's Disease Diagnosis from MRI}. International Conference on Medical Image Computing and Computer-Assisted Intervention.\n\n\\bibitem{ham2020svv}\nHyung-Gi Ham, T. Jun, and Daeyoung Kim (2020). \\textit{Unbalanced GANs: Pre-training the Generator of Generative Adversarial Network using Variational Autoencoder}. arXiv.org.\n\n\\bibitem{wang20182xz}\nChu Wang, Yanming Zhang, and Cheng-Lin Liu (2018). \\textit{Anomaly Detection via Minimum Likelihood Generative Adversarial Networks}. International Conference on Pattern Recognition.\n\n\\bibitem{zhang2018oba}\nZhirui Zhang, Shujie Liu, Mu Li, et al. (2018). \\textit{Bidirectional Generative Adversarial Networks for Neural Machine Translation}. Conference on Computational Natural Language Learning.\n\n\\bibitem{liang2018axu}\nG. Liang, S. Fouladvand, Jie Zhang, et al. (2018). \\textit{GANai: Standardizing CT Images using Generative Adversarial Network with Alternative Improvement}. bioRxiv.\n\n\\bibitem{wiatrak20194ae}\nMaciej Wiatrak, and Stefano V. Albrecht (2019). \\textit{Stabilizing Generative Adversarial Network Training: A Survey}. arXiv.org.\n\n\\bibitem{xue2022n0r}\nYu Xue, Weinan Tong, Ferrante Neri, et al. (2022). \\textit{PEGANs: Phased Evolutionary Generative Adversarial Networks with Self-Attention Module}. Mathematics.\n\n\\bibitem{oeldorf2019kj7}\nCedric Oeldorf, and Gerasimos Spanakis (2019). \\textit{LoGANv2: Conditional Style-Based Logo Generation with Generative Adversarial Networks}. International Conference on Machine Learning and Applications.\n\n\\bibitem{sajjadi2018w83}\nMehdi S. M. Sajjadi, and B. Scholkopf (2018). \\textit{Tempered Adversarial Networks}. International Conference on Machine Learning.\n\n\\bibitem{park2021v6f}\nJ. E. Park, Da-in Eun, H. Kim, et al. (2021). \\textit{Generative adversarial network for glioblastoma ensures morphologic variations and improves diagnostic model for isocitrate dehydrogenase mutant type}. Scientific Reports.\n\n\\bibitem{song2020mj8}\nXiaoning Song, Yao Chen, Zhenhua Feng, et al. (2020). \\textit{SP-GAN: Self-Growing and Pruning Generative Adversarial Networks}. IEEE Transactions on Neural Networks and Learning Systems.\n\n\\bibitem{randhawa2021ksq}\nRizwan Hamid Randhawa, N. Aslam, Mohammad Alauthman, et al. (2021). \\textit{Evasion Generative Adversarial Network for Low Data Regimes}. IEEE Transactions on Artificial Intelligence.\n\n\\bibitem{wang2020vbt}\nMengxue Wang, Zhenxue Chen, Q. M. J. Wu, et al. (2020). \\textit{Improved face super-resolution generative adversarial networks}. Machine Vision and Applications.\n\n\\bibitem{saqur2018oqp}\nRaeid Saqur, and Sal Vivona (2018). \\textit{CapsGAN: Using Dynamic Routing for Generative Adversarial Networks}. Advances in Intelligent Systems and Computing.\n\n\\bibitem{gao2018d4g}\nF. Gao, Fei Ma, Jun Wang, et al. (2018). \\textit{Semi-Supervised Generative Adversarial Nets with Multiple Generators for SAR Image Recognition}. Italian National Conference on Sensors.\n\n\\bibitem{you2018a3m}\nHaoran You, Yu Cheng, Tianheng Cheng, et al. (2018). \\textit{Bayesian Cycle-Consistent Generative Adversarial Networks via Marginalizing Latent Sampling}. IEEE Transactions on Neural Networks and Learning Systems.\n\n\\bibitem{du2021bhg}\nBiao Du, Lin Tang, Lin Liu, et al. (2021). \\textit{Predicting LncRNA-Disease Association Based on Generative Adversarial Network.}. Current Gene Therapy.\n\n\\bibitem{wei2021gla}\nJiaheng Wei, Minghao Liu, Jiahao Luo, et al. (2021). \\textit{DuelGAN: A Duel Between Two Discriminators Stabilizes the GAN Training}. European Conference on Computer Vision.\n\n\\bibitem{lazarou2020gu8}\nConor Lazarou (2020). \\textit{Autoencoding Generative Adversarial Networks}. arXiv.org.\n\n\\bibitem{zhang2021ypi}\nZhaoyu Zhang, Mengyan Li, Haonian Xie, et al. (2021). \\textit{TWGAN: Twin Discriminator Generative Adversarial Networks}. IEEE transactions on multimedia.\n\n\\bibitem{jiang2020e6i}\nYi Jiang, Jiajie Xu, Baoqing Yang, et al. (2020). \\textit{Image Inpainting Based on Generative Adversarial Networks}. IEEE Access.\n\n\\bibitem{plakias2018h0x}\nSpyridon Plakias, and Y. Boutalis (2018). \\textit{Generative Adversarial Networks for Unsupervised Fault Detection}. European Control Conference.\n\n\\bibitem{chao2021ynq}\nXiaopeng Chao, Jiangzhong Cao, Yuqin Lu, et al. (2021). \\textit{Constrained Generative Adversarial Networks}. IEEE Access.\n\n\\bibitem{zhang20182tk}\nJiacen Zhang, Nakamasa Inoue, and K. Shinoda (2018). \\textit{I-vector Transformation Using Conditional Generative Adversarial Networks for Short Utterance Speaker Verification}. Interspeech.\n\n\\bibitem{cao20184y8}\nYanshuai Cao, G. Ding, Kry Yik-Chau Lui, et al. (2018). \\textit{Improving GAN Training via Binarized Representation Entropy (BRE) Regularization}. International Conference on Learning Representations.\n\n\\bibitem{costa2020anu}\nVictor Costa, Nuno Lourenço, João Correia, et al. (2020). \\textit{Neuroevolution of Generative Adversarial Networks}. Deep Neural Evolution.\n\n\\bibitem{panwar2019psx}\nSharaj Panwar, P. Rad, J. Quarles, et al. (2019). \\textit{A Semi-Supervised Wasserstein Generative Adversarial Network for Classifying Driving Fatigue from EEG signals}. IEEE International Conference on Systems, Man and Cybernetics.\n\n\\bibitem{wu20212vn}\nAming Wu, Juyong Shin, Jae-Kwang Ahn, et al. (2021). \\textit{Augmenting Seismic Data Using Generative Adversarial Network for Low-Cost MEMS Sensors}. IEEE Access.\n\n\\bibitem{shou2020v6h}\nChunhui Shou, Ling Hong, Waner Ding, et al. (2020). \\textit{Defect Detection with Generative Adversarial Networks for Electroluminescence Images of Solar Cells}. Youth Academic Annual Conference of Chinese Association of Automation.\n\n\\bibitem{liu2019v0x}\nJianfei Liu, Christine Shen, Tao Liu, et al. (2019). \\textit{Active Appearance Model Induced Generative Adversarial Network for Controlled Data Augmentation}. International Conference on Medical Image Computing and Computer-Assisted Intervention.\n\n\\bibitem{farrell2019kjy}\nS. Farrell, W. Bhimji, T. Kurth, et al. (2019). \\textit{Next Generation Generative Neural Networks for HEP}. EPJ Web of Conferences.\n\n\\bibitem{wu2020n95}\nZhongze Wu, Chunmei He, Liwen Yang, et al. (2020). \\textit{Attentive evolutionary generative adversarial network}. Applied intelligence (Boston).\n\n\\bibitem{majtner20192pi}\nTomás Majtner, Buda Bajić, Joakim Lindblad, et al. (2019). \\textit{On the Effectiveness of Generative Adversarial Networks as HEp-2 Image Augmentation Tool}. Scandinavian Conference on Image Analysis.\n\n\\bibitem{zadorozhnyy20208ft}\nVasily Zadorozhnyy, Q. Cheng, and Q. Ye (2020). \\textit{Adaptive Weighted Discriminator for Training Generative Adversarial Networks}. Computer Vision and Pattern Recognition.\n\n\\bibitem{munia20201u2}\nM. Munia, M. Nourani, and Sammy Houari (2020). \\textit{Biosignal Oversampling Using Wasserstein Generative Adversarial Network}. IEEE International Conference on Healthcare Informatics.\n\n\\bibitem{warner2020a5z}\nJ. Warner, Julian Cuevas, G. Bomarito, et al. (2020). \\textit{Inverse Estimation of Elastic Modulus Using Physics-Informed Generative Adversarial Networks}. arXiv.org.\n\n\\bibitem{lee2017zsj}\nSang-gil Lee, Uiwon Hwang, Seonwoo Min, et al. (2017). \\textit{Polyphonic Music Generation with Sequence Generative Adversarial Networks}. Journal of KIISE.\n\n\\bibitem{xu2019uwg}\nKun Xu, Chongxuan Li, Huanshu Wei, et al. (2019). \\textit{Understanding and Stabilizing GANs' Training Dynamics with Control Theory}. arXiv.org.\n\n\\bibitem{zhang201996t}\nShufei Zhang, Zhuang Qian, Kaizhu Huang, et al. (2019). \\textit{Robust generative adversarial network}. Machine-mediated learning.\n\n\\bibitem{pieters2018jh1}\nMathijs Pieters, and M. Wiering (2018). \\textit{Comparing Generative Adversarial Network Techniques for Image Creation and Modification}. arXiv.org.\n\n\\bibitem{xiang2017cc9}\nSitao Xiang, and Hao Li (2017). \\textit{On the effect of Batch Normalization and Weight Normalization in Generative Adversarial Networks}. arXiv.org.\n\n\\bibitem{xiong20243bt}\nHongqiang Xiong, Jing Li, Zhilian Li, et al. (2024). \\textit{GPR-GAN: A Ground-Penetrating Radar Data Generative Adversarial Network}. IEEE Transactions on Geoscience and Remote Sensing.\n\n\\bibitem{xue2024e7i}\nYu Xue, Weinan Tong, Ferrante Neri, et al. (2024). \\textit{Evolutionary Architecture Search for Generative Adversarial Networks Based on Weight Sharing}. IEEE Transactions on Evolutionary Computation.\n\n\\bibitem{xue20248md}\nYu Xue, Kun Chen, and Ferrante Neri (2024). \\textit{Differentiable Architecture Search With Attention Mechanisms for Generative Adversarial Networks}. IEEE Transactions on Emerging Topics in Computational Intelligence.\n\n\\bibitem{jenkins2024qf5}\nJohn Jenkins, and Kaushik Roy (2024). \\textit{Exploring deep convolutional generative adversarial networks (DCGAN) in biometric systems: a survey study}. Discover Artificial Intelligence.\n\n\\bibitem{qiu2025hu0}\nShiqing Qiu, Yang Wang, Zong Ke, et al. (2025). \\textit{A Generative Adversarial Network-Based Investor Sentiment Indicator: Superior Predictability for the Stock Market}. Mathematics.\n\n\\bibitem{boubrahimi2024kts}\nSoukaina Filali Boubrahimi, Ashit Neema, Ayman Nassar, et al. (2024). \\textit{Spatiotemporal Data Augmentation of MODIS‐Landsat Water Bodies Using Adversarial Networks}. Water Resources Research.\n\n\\bibitem{liu20232tr}\nNaihao Liu, Youbo Lei, Yang Yang, et al. (2023). \\textit{Self-supervised Time-Frequency Representation based on Generative Adversarial Networks}. Geophysics.\n\n\\bibitem{song20239hi}\nYihong Song, Haoyan Zhang, Jiaqi Li, et al. (2023). \\textit{High-Accuracy Maize Disease Detection Based on Attention Generative Adversarial Network and Few-Shot Learning}. Plants.\n\n\\bibitem{pal2023147}\nDebabrata Pal, Shirsha Bose, Biplab Banerjee, et al. (2023). \\textit{MORGAN: Meta-Learning-based Few-Shot Open-Set Recognition via Generative Adversarial Network}. IEEE Workshop/Winter Conference on Applications of Computer Vision.\n\n\\bibitem{gan202494y}\nYan Gan, Chenxue Yang, Mao Ye, et al. (2024). \\textit{Generative Adversarial Networks with Learnable Auxiliary Module for Image Synthesis}. ACM Trans. Multim. Comput. Commun. Appl..\n\n\\bibitem{eltehewy2023cj4}\nRokaya Eltehewy, A. Abouelfarag, and Sherine Nagy Saleh (2023). \\textit{Efficient Classification of Imbalanced Natural Disasters Data Using Generative Adversarial Networks for Data Augmentation}. ISPRS Int. J. Geo Inf..\n\n\\bibitem{chen2023rrf}\nShiming Chen, Shuhuang Chen, W. Hou, et al. (2023). \\textit{EGANS: Evolutionary Generative Adversarial Network Search for Zero-Shot Learning}. IEEE Transactions on Evolutionary Computation.\n\n\\bibitem{fu20241mw}\nFeiran Fu, Peng Liu, Zhen Shao, et al. (2024). \\textit{MEvo-GAN: A Multi-Scale Evolutionary Generative Adversarial Network for Underwater Image Enhancement}. Journal of Marine Science and Engineering.\n\n\\bibitem{soleymanzadeh202358z}\nRaha Soleymanzadeh, and R. Kashef (2023). \\textit{Efficient intrusion detection using multi-player generative adversarial networks (GANs): an ensemble-based deep learning architecture}. Neural computing & applications (Print).\n\n\\bibitem{fathallah20236k5}\nMohamed Fathallah, Mohamed Sakr, and Sherif Eletriby (2023). \\textit{Stabilizing and Improving Training of Generative Adversarial Networks Through Identity Blocks and Modified Loss Function}. IEEE Access.\n\n\\bibitem{luo2024o1x}\nTianjiao Luo, Tim Pearce, Huayu Chen, et al. (2024). \\textit{C-GAIL: Stabilizing Generative Adversarial Imitation Learning with Control Theory}. Neural Information Processing Systems.\n\n\\bibitem{li2024uae}\nWei Li, and Yongchuan Tang (2024). \\textit{Soft Generative Adversarial Network: Combating Mode Collapse in Generative Adversarial Network Training via Dynamic Borderline Softening Mechanism}. Applied Sciences.\n\n\\bibitem{cai2024m9z}\nDongting Cai (2024). \\textit{Enhancing capabilities of generative models through VAE-GAN integration: A review}. Applied and Computational Engineering.\n\n\\bibitem{u2023m2y}\nK. U, T. S, T.V. Nidhin Prabhakar, et al. (2023). \\textit{Adversarial Defense: A GAN-IF Based Cyber-security Model for Intrusion Detection in Software Piracy}. J. Wirel. Mob. Networks Ubiquitous Comput. Dependable Appl..\n\n\\bibitem{liu2023q2q}\nXiaobao Liu, Shuailin Su, Wenjuan Gu, et al. (2023). \\textit{Super-Resolution Reconstruction of CT Images Based on Multi-scale Information Fused Generative Adversarial Networks}. Annals of Biomedical Engineering.\n\n\\bibitem{cheng2023t9b}\nShijie Cheng, Lingfeng Wang, M. Zhang, et al. (2023). \\textit{SUGAN: A Stable U-Net Based Generative Adversarial Network}. Italian National Conference on Sensors.\n\n\\bibitem{luo2022rm1}\nXukang Luo, Ying Jiang, Enqiang Wang, et al. (2022). \\textit{Anomaly detection by using a combination of generative adversarial networks and convolutional autoencoders}. EURASIP Journal on Advances in Signal Processing.\n\n\\bibitem{xu2022ss4}\nJialing Xu, Jingxing He, Jinqiang Gu, et al. (2022). \\textit{Financial Time Series Prediction Based on XGBoost and Generative Adversarial Networks}. International Journal of Circuits, Systems and Signal Processing.\n\n\\bibitem{alshehri2022d1h}\nAbeer Alshehri, Mounira Taileb, and Reem M. Alotaibi (2022). \\textit{DeepAIA: An Automatic Image Annotation Model Based on Generative Adversarial Networks and Transfer Learning}. IEEE Access.\n\n\\bibitem{yeh2022yvr}\nYen-Tung Yeh, Bo-Yu Chen, and Yi-Hsuan Yang (2022). \\textit{Exploiting Pre-trained Feature Networks for Generative Adversarial Networks in Audio-domain Loop Generation}. International Society for Music Information Retrieval Conference.\n\n\\bibitem{gonzlezprieto20214wh}\nÁngel González-Prieto, Alberto Mozo, Edgar Talavera, et al. (2021). \\textit{Dynamics of Fourier Modes in Torus Generative Adversarial Networks}. Mathematics.\n\n\\bibitem{huang2022zar}\nYing Huang, Wenhao Mei, Su Liu, et al. (2022). \\textit{Asymmetric Training of Generative Adversarial Network for High Fidelity SAR Image Generation}. IEEE International Geoscience and Remote Sensing Symposium.\n\n\\bibitem{wang2020iia}\nChunzhi Wang, Pan Wu, Lingyu Yan, et al. (2020). \\textit{Image classification based on principal component analysis optimized generative adversarial networks}. Multimedia tools and applications.\n\n\\bibitem{ma2021w69}\nRuixin Ma, and Junying Lou (2021). \\textit{CPGAN : An Efficient Architecture Designing for Text-to-Image Generative Adversarial Networks Based on Canonical Polyadic Decomposition}. Scientific Programming.\n\n\\bibitem{baby2020e5n}\nDeepak Baby (2020). \\textit{iSEGAN: Improved Speech Enhancement Generative Adversarial Networks}. arXiv.org.\n\n\\bibitem{pasini2021ta3}\nMassimiliano Lupo Pasini, and Junqi Yin (2021). \\textit{Stable parallel training of Wasserstein conditional generative adversarial neural networks}. 2021 International Conference on Computational Science and Computational Intelligence (CSCI).\n\n\\bibitem{goyal2024ufg}\nMandeep Goyal, and Q. Mahmoud (2024). \\textit{A Systematic Review of Synthetic Data Generation Techniques Using Generative AI}. Electronics.\n\n\\bibitem{wang2024v83}\nShuzhan Wang, Ruxue Jiang, Zhaoqi Wang, et al. (2024). \\textit{Deep Learning-based Anomaly Detection and Log Analysis for Computer Networks}. arXiv.org.\n\n\\bibitem{liao20249ku}\nWenjie Liao, Like Wu, Shihui Xu, et al. (2024). \\textit{A Novel Approach for Intelligent Fault Diagnosis in Bearing With Imbalanced Data Based on Cycle-Consistent GAN}. IEEE Transactions on Instrumentation and Measurement.\n\n\\bibitem{peng2024kkw}\nYingying Peng (2024). \\textit{A Comparative Analysis Between GAN and Diffusion Models in Image Generation}. Transactions on Computer Science and Intelligent Systems Research.\n\n\\bibitem{luo2024znt}\nYihong Luo, Xiaolong Chen, Tianyang Hu, et al. (2024). \\textit{You Only Sample Once: Taming One-Step Text-To-Image Synthesis by Self-Cooperative Diffusion GANs}. International Conference on Learning Representations.\n\n\\bibitem{chen2024ajr}\nXin Chen, Zaigang Chen, Shiqian Chen, et al. (2024). \\textit{Unsupervised GAN With Fine-Tuning: A Novel Framework for Induction Motor Fault Diagnosis in Scarcely Labeled Sample Scenarios}. IEEE Transactions on Instrumentation and Measurement.\n\n\\bibitem{song2024htg}\nXiangjin Song, Zhicheng Liu, and Zhaowei Wang (2024). \\textit{Rolling bearing fault diagnosis in electric motors based on IDIG-GAN under small sample conditions}. Measurement science and technology.\n\n\\bibitem{qin2024a4b}\nZhaohui Qin, Faguo Huang, Jiafang Pan, et al. (2024). \\textit{Improved Generative Adversarial Network for Bearing Fault Diagnosis with a Small Number of Data and Unbalanced Data}. Symmetry.\n\n\\bibitem{tibermacine2025pye}\nImad Eddine Tibermacine, Samuele Russo, Francesco Citeroni, et al. (2025). \\textit{Adversarial denoising of EEG signals: a comparative analysis of standard GAN and WGAN-GP approaches}. Frontiers in Human Neuroscience.\n\n\\bibitem{baoueb2024rlq}\nTeysir Baoueb, Haocheng Liu, Mathieu Fontaine, et al. (2024). \\textit{SpecDiff-GAN: A Spectrally-Shaped Noise Diffusion GAN for Speech and Music Synthesis}. IEEE International Conference on Acoustics, Speech, and Signal Processing.\n\n\\bibitem{broll2024edy}\nAlexander Broll, M. Rosentritt, Thomas Schlegl, et al. (2024). \\textit{A data-driven approach for the partial reconstruction of individual human molar teeth using generative deep learning}. Frontiers Artif. Intell..\n\n\\bibitem{wang20245dt}\nYumiao Wang, Chuanfei Zang, Bo Yu, et al. (2024). \\textit{WTE-CGAN Based Signal Enhancement for Weak Target Detection}. IEEE Geoscience and Remote Sensing Letters.\n\n\\bibitem{megahed2024c23}\nMohammed Megahed, and Ammar Mohammed (2024). \\textit{Collaborative-GAN: An Approach for Stabilizing the Training Process of Generative Adversarial Network}. IEEE Access.\n\n\\bibitem{zhang2024k8a}\nXiurong Zhang, Shaoqian Fan, and Daoliang Li (2024). \\textit{Spectral normalization generative adversarial networks for photovoltaic power scenario generation}. IET Renewable Power Generation.\n\n\\bibitem{bhat202445j}\nRanjith Bhat, and Raghu Nanjundegowda (2024). \\textit{A Review on Comparative Analysis of Generative Adversarial Networks’ Architectures and Applications}. Journal of Robotics and Control (JRC).\n\n\\bibitem{ler20248xg}\nFiete Lüer, and Christian Böhm (2024). \\textit{Anomaly Detection using Generative Adversarial Networks Reviewing methodological progress and challenges}. SIGKDD Explorations.\n\n\\bibitem{purwono2025spz}\nPurwono Purwono, Annastasya Nabila Elsa Wulandari, Alfian Ma’arif, et al. (2025). \\textit{Understanding Generative Adversarial Networks (GANs): A Review}. Control Systems and Optimization Letters.\n\n\\bibitem{roy2024k91}\nArunava Roy, and Dipankar Dasgupta (2024). \\textit{A Distributed Conditional Wasserstein Deep Convolutional Relativistic Loss Generative Adversarial Network With Improved Convergence}. IEEE Transactions on Artificial Intelligence.\n\n\\bibitem{seon202526r}\nJoonho Seon, Seongwoo Lee, Youngghyu Sun, et al. (2025). \\textit{Least Information Spectral GAN With Time-Series Data Augmentation for Industrial IoT}. IEEE Transactions on Emerging Topics in Computational Intelligence.\n\n\\bibitem{ni2024y70}\nYao Ni, and Piotr Koniusz (2024). \\textit{$\\bigcirc\\!\\!\\!\\!\\bigcirc$ CHAIN: Enhancing Generalization in Data-Efficient GANs via LipsCHitz Continuity ConstrAIned Normalization}. Computer Vision and Pattern Recognition.\n\n\\bibitem{ye2024n41}\nMing Ye, Cunhua Pan, Yinfei Xu, et al. (2024). \\textit{Generative Adversarial Networks-Based Channel Estimation for Intelligent Reflecting Surface Assisted mmWave MIMO Systems}. IEEE Transactions on Cognitive Communications and Networking.\n\n\\bibitem{pajuhanfard2024ult}\nMohammadsaleh Pajuhanfard, Rasoul Kiani, and Victor S. Sheng (2024). \\textit{Survey of Quantum Generative Adversarial Networks (QGAN) to Generate Images}. Mathematics.\n\n\\bibitem{eskandarinasab202431h}\nMohammadReza EskandariNasab, S. M. Hamdi, and S. F. Boubrahimi (2024). \\textit{ChronoGAN: Supervised and Embedded Generative Adversarial Networks for Time Series Generation}. International Conference on Machine Learning and Applications.\n\n\\bibitem{deebani202549r}\nWejdan Deebani, Lubna Aziz, Arshad Aziz, et al. (2025). \\textit{Synergistic transfer learning and adversarial networks for breast cancer diagnosis: benign vs. invasive classification}. Scientific Reports.\n\n\\bibitem{ali2024ks3}\nAbid Ali, Muhammad Sharif, Muhammad Shahzad Faisal, et al. (2024). \\textit{Brain Tumor Segmentation Using Generative Adversarial Networks}. IEEE Access.\n\n\\bibitem{ju2024uai}\nXiangui Ju, Chi-Ho Lin, Suan Lee, et al. (2024). \\textit{Melanoma classification using generative adversarial network and proximal policy optimization}. Photochemistry and Photobiology.\n\n\\bibitem{xu2024u5a}\nChi Xu, Haozheng Xu, and S. Giannarou (2024). \\textit{Distance Regression Enhanced With Temporal Information Fusion and Adversarial Training for Robot-Assisted Endomicroscopy}. IEEE Transactions on Medical Imaging.\n\n\\bibitem{elbaz2025wzb}\nMostafa Elbaz, Wael Said, G. Mahmoud, et al. (2025). \\textit{A dual GAN with identity blocks and pancreas-inspired loss for renewable energy optimization}. Scientific Reports.\n\n\\bibitem{chang2024c0a}\nYuanhong Chang, Jinglong Chen, Rong Su, et al. (2024). \\textit{Two-Phase Dual-Adversarial Agents With Multivariate Information for Unsupervised Anomaly Detection of IIoT-Edge Devices}. IEEE Internet of Things Journal.\n\n\\bibitem{guo2024y0l}\nPang Guo, and Yining Chen (2024). \\textit{Enhanced Yield Prediction in Semiconductor Manufacturing: Innovative Strategies for Imbalanced Sample Management and Root Cause Analysis}. International Symposium on the Physical and Failure Analysis of Integrated Circuits.\n\n\\bibitem{peng2024crk}\nJun Peng, Kaiyi Chen, Yuqing Gong, et al. (2024). \\textit{Cyclic Consistent Image Style Transformation: From Model to System}. Applied Sciences.\n\n\\end{thebibliography}\n\n\\end{document}",
  "generation_date": "2025-10-07T23:17:40.193982",
  "processed_papers_data": [
    {
      "success": true,
      "doc_id": "aa0bda70543de0d631776cf19641759c",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/acd87843a451d18b4dc6474ddce1ae946429eaf1.pdf",
      "citation_key": "arjovsky2017ze5",
      "metadata": {
        "title": "Wasserstein Generative Adversarial Networks",
        "authors": [
          "Martín Arjovsky",
          "Soumith Chintala",
          "L. Bottou"
        ],
        "published_date": "2017",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/acd87843a451d18b4dc6474ddce1ae946429eaf1.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 8420,
        "score": 1052.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "acd87843a451d18b4dc6474ddce1ae946429eaf1.pdf"
    },
    {
      "success": true,
      "doc_id": "a0f8236beb592caa3cb17c92b29f2277",
      "summary": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.",
      "intriguing_abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/744fe47157477235032f7bb3777800f9f2f45e52.pdf",
      "citation_key": "karras2017raw",
      "metadata": {
        "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
        "authors": [
          "Tero Karras",
          "Timo Aila",
          "S. Laine",
          "J. Lehtinen"
        ],
        "published_date": "2017",
        "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/744fe47157477235032f7bb3777800f9f2f45e52.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 7647,
        "score": 955.875,
        "summary": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.",
        "keywords": []
      },
      "file_name": "744fe47157477235032f7bb3777800f9f2f45e52.pdf"
    },
    {
      "success": true,
      "doc_id": "90e7efd59cd4568b0a5ba447243e0662",
      "summary": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.",
      "intriguing_abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/84de7d27e2f6160f634a483e8548c499a2cda7fa.pdf",
      "citation_key": "miyato2018arc",
      "metadata": {
        "title": "Spectral Normalization for Generative Adversarial Networks",
        "authors": [
          "Takeru Miyato",
          "Toshiki Kataoka",
          "Masanori Koyama",
          "Yuichi Yoshida"
        ],
        "published_date": "2018",
        "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/84de7d27e2f6160f634a483e8548c499a2cda7fa.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 4552,
        "score": 650.2857142857142,
        "summary": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.",
        "keywords": []
      },
      "file_name": "84de7d27e2f6160f634a483e8548c499a2cda7fa.pdf"
    },
    {
      "success": true,
      "doc_id": "8ddf789bf9c3a42aa465acc031e668cf",
      "summary": "Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing GAN on another dataset. We demonstrate, on several datasets, that good results are now possible using only a few thousand training images, often matching StyleGAN2 results with an order of magnitude fewer images. We expect this to open up new application domains for GANs. We also find that the widely used CIFAR-10 is, in fact, a limited data benchmark, and improve the record FID from 5.59 to 2.42.",
      "intriguing_abstract": "Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing GAN on another dataset. We demonstrate, on several datasets, that good results are now possible using only a few thousand training images, often matching StyleGAN2 results with an order of magnitude fewer images. We expect this to open up new application domains for GANs. We also find that the widely used CIFAR-10 is, in fact, a limited data benchmark, and improve the record FID from 5.59 to 2.42.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/29858b40a15704398aecdca6bd2820f2fcc99891.pdf",
      "citation_key": "karras202039x",
      "metadata": {
        "title": "Training Generative Adversarial Networks with Limited Data",
        "authors": [
          "Tero Karras",
          "M. Aittala",
          "Janne Hellsten",
          "S. Laine",
          "J. Lehtinen",
          "Timo Aila"
        ],
        "published_date": "2020",
        "abstract": "Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing GAN on another dataset. We demonstrate, on several datasets, that good results are now possible using only a few thousand training images, often matching StyleGAN2 results with an order of magnitude fewer images. We expect this to open up new application domains for GANs. We also find that the widely used CIFAR-10 is, in fact, a limited data benchmark, and improve the record FID from 5.59 to 2.42.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/29858b40a15704398aecdca6bd2820f2fcc99891.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 1971,
        "score": 394.20000000000005,
        "summary": "Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing GAN on another dataset. We demonstrate, on several datasets, that good results are now possible using only a few thousand training images, often matching StyleGAN2 results with an order of magnitude fewer images. We expect this to open up new application domains for GANs. We also find that the widely used CIFAR-10 is, in fact, a limited data benchmark, and improve the record FID from 5.59 to 2.42.",
        "keywords": []
      },
      "file_name": "29858b40a15704398aecdca6bd2820f2fcc99891.pdf"
    },
    {
      "success": true,
      "doc_id": "0bb2d160ab202f5ef813051466c66dcb",
      "summary": "Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing textto- image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256.256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.",
      "intriguing_abstract": "Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing textto- image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256.256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921.pdf",
      "citation_key": "zhang2016mm0",
      "metadata": {
        "title": "StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks",
        "authors": [
          "Han Zhang",
          "Tao Xu",
          "Hongsheng Li",
          "Shaoting Zhang",
          "Xiaogang Wang",
          "Xiaolei Huang",
          "Dimitris N. Metaxas"
        ],
        "published_date": "2016",
        "abstract": "Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing textto- image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256.256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921.pdf",
        "venue": "IEEE International Conference on Computer Vision",
        "citationCount": 2776,
        "score": 308.4444444444444,
        "summary": "Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing textto- image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256.256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.",
        "keywords": []
      },
      "file_name": "ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921.pdf"
    },
    {
      "success": true,
      "doc_id": "bb5cd5ad9cf03548c68f6de177c86c52",
      "summary": "With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulators output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a self-regularization term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.",
      "intriguing_abstract": "With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulators output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a self-regularization term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/68cb9fce1e6af2740377494350b650533c9a29e1.pdf",
      "citation_key": "shrivastava2016uym",
      "metadata": {
        "title": "Learning from Simulated and Unsupervised Images through Adversarial Training",
        "authors": [
          "A. Shrivastava",
          "Tomas Pfister",
          "Oncel Tuzel",
          "J. Susskind",
          "Wenda Wang",
          "Russ Webb"
        ],
        "published_date": "2016",
        "abstract": "With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulators output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a self-regularization term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/68cb9fce1e6af2740377494350b650533c9a29e1.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 1821,
        "score": 202.33333333333331,
        "summary": "With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulators output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a self-regularization term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.",
        "keywords": []
      },
      "file_name": "68cb9fce1e6af2740377494350b650533c9a29e1.pdf"
    },
    {
      "success": true,
      "doc_id": "b44e341daa3fa47d4c2b4b4b198de7a6",
      "summary": "The performance of generative adversarial networks (GANs) heavily deteriorates given a limited amount of training data. This is mainly because the discriminator is memorizing the exact training set. To combat it, we propose Differentiable Augmentation (DiffAugment), a simple method that improves the data efficiency of GANs by imposing various types of differentiable augmentations on both real and fake samples. Previous attempts to directly augment the training data manipulate the distribution of real images, yielding little benefit; DiffAugment enables us to adopt the differentiable augmentation for the generated samples, effectively stabilizes training, and leads to better convergence. Experiments demonstrate consistent gains of our method over a variety of GAN architectures and loss functions for both unconditional and class-conditional generation. With DiffAugment, we achieve a state-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128x128. Furthermore, with only 20% training data, we can match the top performance on CIFAR-10 and CIFAR-100. Finally, our method can generate high-fidelity images using only 100 images without pre-training, while being on par with existing transfer learning algorithms. Code is available at this https URL.",
      "intriguing_abstract": "The performance of generative adversarial networks (GANs) heavily deteriorates given a limited amount of training data. This is mainly because the discriminator is memorizing the exact training set. To combat it, we propose Differentiable Augmentation (DiffAugment), a simple method that improves the data efficiency of GANs by imposing various types of differentiable augmentations on both real and fake samples. Previous attempts to directly augment the training data manipulate the distribution of real images, yielding little benefit; DiffAugment enables us to adopt the differentiable augmentation for the generated samples, effectively stabilizes training, and leads to better convergence. Experiments demonstrate consistent gains of our method over a variety of GAN architectures and loss functions for both unconditional and class-conditional generation. With DiffAugment, we achieve a state-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128x128. Furthermore, with only 20% training data, we can match the top performance on CIFAR-10 and CIFAR-100. Finally, our method can generate high-fidelity images using only 100 images without pre-training, while being on par with existing transfer learning algorithms. Code is available at this https URL.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/670f9d0d8cafaeaeea564c88645b9816b1146cef.pdf",
      "citation_key": "zhao2020xhy",
      "metadata": {
        "title": "Differentiable Augmentation for Data-Efficient GAN Training",
        "authors": [
          "Shengyu Zhao",
          "Zhijian Liu",
          "Ji Lin",
          "Jun-Yan Zhu",
          "Song Han"
        ],
        "published_date": "2020",
        "abstract": "The performance of generative adversarial networks (GANs) heavily deteriorates given a limited amount of training data. This is mainly because the discriminator is memorizing the exact training set. To combat it, we propose Differentiable Augmentation (DiffAugment), a simple method that improves the data efficiency of GANs by imposing various types of differentiable augmentations on both real and fake samples. Previous attempts to directly augment the training data manipulate the distribution of real images, yielding little benefit; DiffAugment enables us to adopt the differentiable augmentation for the generated samples, effectively stabilizes training, and leads to better convergence. Experiments demonstrate consistent gains of our method over a variety of GAN architectures and loss functions for both unconditional and class-conditional generation. With DiffAugment, we achieve a state-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128x128. Furthermore, with only 20% training data, we can match the top performance on CIFAR-10 and CIFAR-100. Finally, our method can generate high-fidelity images using only 100 images without pre-training, while being on par with existing transfer learning algorithms. Code is available at this https URL.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/670f9d0d8cafaeaeea564c88645b9816b1146cef.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 624,
        "score": 124.80000000000001,
        "summary": "The performance of generative adversarial networks (GANs) heavily deteriorates given a limited amount of training data. This is mainly because the discriminator is memorizing the exact training set. To combat it, we propose Differentiable Augmentation (DiffAugment), a simple method that improves the data efficiency of GANs by imposing various types of differentiable augmentations on both real and fake samples. Previous attempts to directly augment the training data manipulate the distribution of real images, yielding little benefit; DiffAugment enables us to adopt the differentiable augmentation for the generated samples, effectively stabilizes training, and leads to better convergence. Experiments demonstrate consistent gains of our method over a variety of GAN architectures and loss functions for both unconditional and class-conditional generation. With DiffAugment, we achieve a state-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128x128. Furthermore, with only 20% training data, we can match the top performance on CIFAR-10 and CIFAR-100. Finally, our method can generate high-fidelity images using only 100 images without pre-training, while being on par with existing transfer learning algorithms. Code is available at this https URL.",
        "keywords": []
      },
      "file_name": "670f9d0d8cafaeaeea564c88645b9816b1146cef.pdf"
    },
    {
      "success": true,
      "doc_id": "6b10e1c18563554a057753334705679c",
      "summary": "We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.",
      "intriguing_abstract": "We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/488bb25e0b1777847f04c943e6dbc4f84415b712.pdf",
      "citation_key": "metz20169ir",
      "metadata": {
        "title": "Unrolled Generative Adversarial Networks",
        "authors": [
          "Luke Metz",
          "Ben Poole",
          "David Pfau",
          "Jascha Narain Sohl-Dickstein"
        ],
        "published_date": "2016",
        "abstract": "We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/488bb25e0b1777847f04c943e6dbc4f84415b712.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 1025,
        "score": 113.88888888888889,
        "summary": "We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.",
        "keywords": []
      },
      "file_name": "488bb25e0b1777847f04c943e6dbc4f84415b712.pdf"
    },
    {
      "success": true,
      "doc_id": "8ab4a30794df48cd1950045671cd1692",
      "summary": "Underwater image enhancement has received much attention in underwater vision research. However, raw underwater images easily suffer from color distortion, underexposure, and fuzz caused by the underwater scene. To address the above-mentioned problems, we propose a new multiscale dense generative adversarial network (GAN) for enhancing underwater images. The residual multiscale dense block is presented in the generator, where the multiscale, dense concatenation, and residual learning can boost the performance, render more details, and utilize previous features, respectively. And the discriminator employs computationally light spectral normalization to stabilize the training of the discriminator. Meanwhile, nonsaturating GAN loss function combining $L_1$ loss and gradient loss is presented to focus on image features of ground truth. Final enhanced results on synthetic and real underwater images demonstrate the superiority of the proposed method, which outperforms nondeep and deep learning methods in both qualitative and quantitative evaluations. Furthermore, we perform an ablation study to show the contributions of each component and carry out application tests to further demonstrate the effectiveness of the proposed method.",
      "intriguing_abstract": "Underwater image enhancement has received much attention in underwater vision research. However, raw underwater images easily suffer from color distortion, underexposure, and fuzz caused by the underwater scene. To address the above-mentioned problems, we propose a new multiscale dense generative adversarial network (GAN) for enhancing underwater images. The residual multiscale dense block is presented in the generator, where the multiscale, dense concatenation, and residual learning can boost the performance, render more details, and utilize previous features, respectively. And the discriminator employs computationally light spectral normalization to stabilize the training of the discriminator. Meanwhile, nonsaturating GAN loss function combining $L_1$ loss and gradient loss is presented to focus on image features of ground truth. Final enhanced results on synthetic and real underwater images demonstrate the superiority of the proposed method, which outperforms nondeep and deep learning methods in both qualitative and quantitative evaluations. Furthermore, we perform an ablation study to show the contributions of each component and carry out application tests to further demonstrate the effectiveness of the proposed method.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/3bf2147008e0bcb5d80298448fc4bf7f5d3c30ea.pdf",
      "citation_key": "guo2020n4t",
      "metadata": {
        "title": "Underwater Image Enhancement Using a Multiscale Dense Generative Adversarial Network",
        "authors": [
          "Ye-cai Guo",
          "Hanyu Li",
          "Peixian Zhuang"
        ],
        "published_date": "2020",
        "abstract": "Underwater image enhancement has received much attention in underwater vision research. However, raw underwater images easily suffer from color distortion, underexposure, and fuzz caused by the underwater scene. To address the above-mentioned problems, we propose a new multiscale dense generative adversarial network (GAN) for enhancing underwater images. The residual multiscale dense block is presented in the generator, where the multiscale, dense concatenation, and residual learning can boost the performance, render more details, and utilize previous features, respectively. And the discriminator employs computationally light spectral normalization to stabilize the training of the discriminator. Meanwhile, nonsaturating GAN loss function combining $L_1$ loss and gradient loss is presented to focus on image features of ground truth. Final enhanced results on synthetic and real underwater images demonstrate the superiority of the proposed method, which outperforms nondeep and deep learning methods in both qualitative and quantitative evaluations. Furthermore, we perform an ablation study to show the contributions of each component and carry out application tests to further demonstrate the effectiveness of the proposed method.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/3bf2147008e0bcb5d80298448fc4bf7f5d3c30ea.pdf",
        "venue": "IEEE Journal of Oceanic Engineering",
        "citationCount": 368,
        "score": 73.60000000000001,
        "summary": "Underwater image enhancement has received much attention in underwater vision research. However, raw underwater images easily suffer from color distortion, underexposure, and fuzz caused by the underwater scene. To address the above-mentioned problems, we propose a new multiscale dense generative adversarial network (GAN) for enhancing underwater images. The residual multiscale dense block is presented in the generator, where the multiscale, dense concatenation, and residual learning can boost the performance, render more details, and utilize previous features, respectively. And the discriminator employs computationally light spectral normalization to stabilize the training of the discriminator. Meanwhile, nonsaturating GAN loss function combining $L_1$ loss and gradient loss is presented to focus on image features of ground truth. Final enhanced results on synthetic and real underwater images demonstrate the superiority of the proposed method, which outperforms nondeep and deep learning methods in both qualitative and quantitative evaluations. Furthermore, we perform an ablation study to show the contributions of each component and carry out application tests to further demonstrate the effectiveness of the proposed method.",
        "keywords": []
      },
      "file_name": "3bf2147008e0bcb5d80298448fc4bf7f5d3c30ea.pdf"
    },
    {
      "success": true,
      "doc_id": "50828fa1612e802779588b740be2e1d6",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/df7ad8eeb595da5f7774e91dae06075be952acff.pdf",
      "citation_key": "bau2018n2x",
      "metadata": {
        "title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks",
        "authors": [
          "David Bau",
          "Jun-Yan Zhu",
          "Hendrik Strobelt",
          "Bolei Zhou",
          "J. Tenenbaum",
          "W. Freeman",
          "A. Torralba"
        ],
        "published_date": "2018",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/df7ad8eeb595da5f7774e91dae06075be952acff.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 480,
        "score": 68.57142857142857,
        "summary": "",
        "keywords": []
      },
      "file_name": "df7ad8eeb595da5f7774e91dae06075be952acff.pdf"
    },
    {
      "success": true,
      "doc_id": "78acf26d72c1a7229643f60a7a381660",
      "summary": "Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.",
      "intriguing_abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/024d30897e0a2b036bc122163a954b7f1a1d0679.pdf",
      "citation_key": "che2016kho",
      "metadata": {
        "title": "Mode Regularized Generative Adversarial Networks",
        "authors": [
          "Tong Che",
          "Yanran Li",
          "Athul Paul Jacob",
          "Yoshua Bengio",
          "Wenjie Li"
        ],
        "published_date": "2016",
        "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/024d30897e0a2b036bc122163a954b7f1a1d0679.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 566,
        "score": 62.888888888888886,
        "summary": "Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.",
        "keywords": []
      },
      "file_name": "024d30897e0a2b036bc122163a954b7f1a1d0679.pdf"
    },
    {
      "success": true,
      "doc_id": "8dba2c58730897de536c63a90f0be551",
      "summary": "Training Generative Adversarial Networks (GAN) on high-fidelity images usually requires large-scale GPU-clusters and a vast number of training images. In this paper, we study the few-shot image synthesis task for GAN with minimum computing cost. We propose a light-weight GAN structure that gains superior quality on 1024*1024 resolution. Notably, the model converges from scratch with just a few hours of training on a single RTX-2080 GPU, and has a consistent performance, even with less than 100 training samples. Two technique designs constitute our work, a skip-layer channel-wise excitation module and a self-supervised discriminator trained as a feature-encoder. With thirteen datasets covering a wide variety of image domains (The datasets and code are available at: https://github.com/odegeasslbc/FastGAN-pytorch), we show our model's superior performance compared to the state-of-the-art StyleGAN2, when data and computing budget are limited.",
      "intriguing_abstract": "Training Generative Adversarial Networks (GAN) on high-fidelity images usually requires large-scale GPU-clusters and a vast number of training images. In this paper, we study the few-shot image synthesis task for GAN with minimum computing cost. We propose a light-weight GAN structure that gains superior quality on 1024*1024 resolution. Notably, the model converges from scratch with just a few hours of training on a single RTX-2080 GPU, and has a consistent performance, even with less than 100 training samples. Two technique designs constitute our work, a skip-layer channel-wise excitation module and a self-supervised discriminator trained as a feature-encoder. With thirteen datasets covering a wide variety of image domains (The datasets and code are available at: https://github.com/odegeasslbc/FastGAN-pytorch), we show our model's superior performance compared to the state-of-the-art StyleGAN2, when data and computing budget are limited.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/6c4fe31504d47b8547e47267c0cb4efa464f022b.pdf",
      "citation_key": "liu20212c2",
      "metadata": {
        "title": "Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis",
        "authors": [
          "Bingchen Liu",
          "Yizhe Zhu",
          "Kunpeng Song",
          "A. Elgammal"
        ],
        "published_date": "2021",
        "abstract": "Training Generative Adversarial Networks (GAN) on high-fidelity images usually requires large-scale GPU-clusters and a vast number of training images. In this paper, we study the few-shot image synthesis task for GAN with minimum computing cost. We propose a light-weight GAN structure that gains superior quality on 1024*1024 resolution. Notably, the model converges from scratch with just a few hours of training on a single RTX-2080 GPU, and has a consistent performance, even with less than 100 training samples. Two technique designs constitute our work, a skip-layer channel-wise excitation module and a self-supervised discriminator trained as a feature-encoder. With thirteen datasets covering a wide variety of image domains (The datasets and code are available at: https://github.com/odegeasslbc/FastGAN-pytorch), we show our model's superior performance compared to the state-of-the-art StyleGAN2, when data and computing budget are limited.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/6c4fe31504d47b8547e47267c0cb4efa464f022b.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 250,
        "score": 62.5,
        "summary": "Training Generative Adversarial Networks (GAN) on high-fidelity images usually requires large-scale GPU-clusters and a vast number of training images. In this paper, we study the few-shot image synthesis task for GAN with minimum computing cost. We propose a light-weight GAN structure that gains superior quality on 1024*1024 resolution. Notably, the model converges from scratch with just a few hours of training on a single RTX-2080 GPU, and has a consistent performance, even with less than 100 training samples. Two technique designs constitute our work, a skip-layer channel-wise excitation module and a self-supervised discriminator trained as a feature-encoder. With thirteen datasets covering a wide variety of image domains (The datasets and code are available at: https://github.com/odegeasslbc/FastGAN-pytorch), we show our model's superior performance compared to the state-of-the-art StyleGAN2, when data and computing budget are limited.",
        "keywords": []
      },
      "file_name": "6c4fe31504d47b8547e47267c0cb4efa464f022b.pdf"
    },
    {
      "success": true,
      "doc_id": "e36767f0078199db31973fd0998f50a9",
      "summary": "The Generative Models have gained considerable attention in unsupervised learning via a new and practical framework called Generative Adversarial Networks (GAN) due to their outstanding data generation capability. Many GAN models have been proposed, and several practical applications have emerged in various domains of computer vision and machine learning. Despite GANs excellent success, there are still obstacles to stable training. The problems are Nash equilibrium, internal covariate shift, mode collapse, vanishing gradient, and lack of proper evaluation metrics. Therefore, stable training is a crucial issue in different applications for the success of GANs. Herein, we survey several training solutions proposed by different researchers to stabilize GAN training. We discuss (I) the original GAN model and its modified versions, (II) a detailed analysis of various GAN applications in different domains, and (III) a detailed study about the various GAN training obstacles as well as training solutions. Finally, we reveal several issues as well as research outlines to the topic.",
      "intriguing_abstract": "The Generative Models have gained considerable attention in unsupervised learning via a new and practical framework called Generative Adversarial Networks (GAN) due to their outstanding data generation capability. Many GAN models have been proposed, and several practical applications have emerged in various domains of computer vision and machine learning. Despite GANs excellent success, there are still obstacles to stable training. The problems are Nash equilibrium, internal covariate shift, mode collapse, vanishing gradient, and lack of proper evaluation metrics. Therefore, stable training is a crucial issue in different applications for the success of GANs. Herein, we survey several training solutions proposed by different researchers to stabilize GAN training. We discuss (I) the original GAN model and its modified versions, (II) a detailed analysis of various GAN applications in different domains, and (III) a detailed study about the various GAN training obstacles as well as training solutions. Finally, we reveal several issues as well as research outlines to the topic.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/d54d8c402785006faaf5de19e81f04eb484a3aa2.pdf",
      "citation_key": "jabbar2020aj0",
      "metadata": {
        "title": "A Survey on Generative Adversarial Networks: Variants, Applications, and Training",
        "authors": [
          "Abdul Jabbar",
          "Xi Li",
          "Bourahla Omar"
        ],
        "published_date": "2020",
        "abstract": "The Generative Models have gained considerable attention in unsupervised learning via a new and practical framework called Generative Adversarial Networks (GAN) due to their outstanding data generation capability. Many GAN models have been proposed, and several practical applications have emerged in various domains of computer vision and machine learning. Despite GANs excellent success, there are still obstacles to stable training. The problems are Nash equilibrium, internal covariate shift, mode collapse, vanishing gradient, and lack of proper evaluation metrics. Therefore, stable training is a crucial issue in different applications for the success of GANs. Herein, we survey several training solutions proposed by different researchers to stabilize GAN training. We discuss (I) the original GAN model and its modified versions, (II) a detailed analysis of various GAN applications in different domains, and (III) a detailed study about the various GAN training obstacles as well as training solutions. Finally, we reveal several issues as well as research outlines to the topic.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/d54d8c402785006faaf5de19e81f04eb484a3aa2.pdf",
        "venue": "ACM Computing Surveys",
        "citationCount": 295,
        "score": 59.0,
        "summary": "The Generative Models have gained considerable attention in unsupervised learning via a new and practical framework called Generative Adversarial Networks (GAN) due to their outstanding data generation capability. Many GAN models have been proposed, and several practical applications have emerged in various domains of computer vision and machine learning. Despite GANs excellent success, there are still obstacles to stable training. The problems are Nash equilibrium, internal covariate shift, mode collapse, vanishing gradient, and lack of proper evaluation metrics. Therefore, stable training is a crucial issue in different applications for the success of GANs. Herein, we survey several training solutions proposed by different researchers to stabilize GAN training. We discuss (I) the original GAN model and its modified versions, (II) a detailed analysis of various GAN applications in different domains, and (III) a detailed study about the various GAN training obstacles as well as training solutions. Finally, we reveal several issues as well as research outlines to the topic.",
        "keywords": []
      },
      "file_name": "d54d8c402785006faaf5de19e81f04eb484a3aa2.pdf"
    },
    {
      "success": true,
      "doc_id": "fbc16ac6cb9a371fd6ead8116118a7a9",
      "summary": "Deep generative models based on Generative Adversarial Networks (GANs) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch or non-overlapping support between the model distribution and the data distribution, causing their density ratio and the associated f -divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable GAN training procedure. We demonstrate the effectiveness of this regularizer accross several architectures trained on common benchmark image generation tasks. Our regularization turns GAN models into reliable building blocks for deep learning.",
      "intriguing_abstract": "Deep generative models based on Generative Adversarial Networks (GANs) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch or non-overlapping support between the model distribution and the data distribution, causing their density ratio and the associated f -divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable GAN training procedure. We demonstrate the effectiveness of this regularizer accross several architectures trained on common benchmark image generation tasks. Our regularization turns GAN models into reliable building blocks for deep learning.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/698d3b667a7f3073eed8368d9daf84f990c24a65.pdf",
      "citation_key": "roth2017eui",
      "metadata": {
        "title": "Stabilizing Training of Generative Adversarial Networks through Regularization",
        "authors": [
          "Kevin Roth",
          "Aurélien Lucchi",
          "Sebastian Nowozin",
          "Thomas Hofmann"
        ],
        "published_date": "2017",
        "abstract": "Deep generative models based on Generative Adversarial Networks (GANs) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch or non-overlapping support between the model distribution and the data distribution, causing their density ratio and the associated f -divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable GAN training procedure. We demonstrate the effectiveness of this regularizer accross several architectures trained on common benchmark image generation tasks. Our regularization turns GAN models into reliable building blocks for deep learning.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/698d3b667a7f3073eed8368d9daf84f990c24a65.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 453,
        "score": 56.625,
        "summary": "Deep generative models based on Generative Adversarial Networks (GANs) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch or non-overlapping support between the model distribution and the data distribution, causing their density ratio and the associated f -divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable GAN training procedure. We demonstrate the effectiveness of this regularizer accross several architectures trained on common benchmark image generation tasks. Our regularization turns GAN models into reliable building blocks for deep learning.",
        "keywords": []
      },
      "file_name": "698d3b667a7f3073eed8368d9daf84f990c24a65.pdf"
    },
    {
      "success": true,
      "doc_id": "aa766494a4a2347f7b2db88e11c60d8a",
      "summary": "We developed a new class of physics-informed generative adversarial networks (PI-GANs) to solve in a unified manner forward, inverse and mixed stochastic problems based on a limited number of scattered measurements. Unlike standard GANs relying only on data for training, here we encoded into the architecture of GANs the governing physical laws in the form of stochastic differential equations (SDEs) using automatic differentiation. In particular, we applied Wasserstein GANs with gradient penalty (WGAN-GP) for its enhanced stability compared to vanilla GANs. We first tested WGAN-GP in approximating Gaussian processes of different correlation lengths based on data realizations collected from simultaneous reads at sparsely placed sensors. We obtained good approximation of the generated stochastic processes to the target ones even for a mismatch between the input noise dimensionality and the effective dimensionality of the target stochastic processes. We also studied the overfitting issue for both the discriminator and generator, and we found that overfitting occurs also in the generator in addition to the discriminator as previously reported. Subsequently, we considered the solution of elliptic SDEs requiring approximations of three stochastic processes, namely the solution, the forcing, and the diffusion coefficient. We used three generators for the PI-GANs, two of them were feed forward deep neural networks (DNNs) while the other one was the neural network induced by the SDE. Depending on the data, we employed one or multiple feed forward DNNs as the discriminators in PI-GANs. Here, we have demonstrated the accuracy and effectiveness of PI-GANs in solving SDEs for up to 30 dimensions, but in principle, PI-GANs could tackle very high dimensional problems given more sensor data with low-polynomial growth in computational cost.",
      "intriguing_abstract": "We developed a new class of physics-informed generative adversarial networks (PI-GANs) to solve in a unified manner forward, inverse and mixed stochastic problems based on a limited number of scattered measurements. Unlike standard GANs relying only on data for training, here we encoded into the architecture of GANs the governing physical laws in the form of stochastic differential equations (SDEs) using automatic differentiation. In particular, we applied Wasserstein GANs with gradient penalty (WGAN-GP) for its enhanced stability compared to vanilla GANs. We first tested WGAN-GP in approximating Gaussian processes of different correlation lengths based on data realizations collected from simultaneous reads at sparsely placed sensors. We obtained good approximation of the generated stochastic processes to the target ones even for a mismatch between the input noise dimensionality and the effective dimensionality of the target stochastic processes. We also studied the overfitting issue for both the discriminator and generator, and we found that overfitting occurs also in the generator in addition to the discriminator as previously reported. Subsequently, we considered the solution of elliptic SDEs requiring approximations of three stochastic processes, namely the solution, the forcing, and the diffusion coefficient. We used three generators for the PI-GANs, two of them were feed forward deep neural networks (DNNs) while the other one was the neural network induced by the SDE. Depending on the data, we employed one or multiple feed forward DNNs as the discriminators in PI-GANs. Here, we have demonstrated the accuracy and effectiveness of PI-GANs in solving SDEs for up to 30 dimensions, but in principle, PI-GANs could tackle very high dimensional problems given more sensor data with low-polynomial growth in computational cost.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/8b1ba1037aefddec9ce9d07858f661b72a1b41fe.pdf",
      "citation_key": "yang2018svo",
      "metadata": {
        "title": "Physics-Informed Generative Adversarial Networks for Stochastic Differential Equations",
        "authors": [
          "Liu Yang",
          "Dongkun Zhang",
          "G. Karniadakis"
        ],
        "published_date": "2018",
        "abstract": "We developed a new class of physics-informed generative adversarial networks (PI-GANs) to solve in a unified manner forward, inverse and mixed stochastic problems based on a limited number of scattered measurements. Unlike standard GANs relying only on data for training, here we encoded into the architecture of GANs the governing physical laws in the form of stochastic differential equations (SDEs) using automatic differentiation. In particular, we applied Wasserstein GANs with gradient penalty (WGAN-GP) for its enhanced stability compared to vanilla GANs. We first tested WGAN-GP in approximating Gaussian processes of different correlation lengths based on data realizations collected from simultaneous reads at sparsely placed sensors. We obtained good approximation of the generated stochastic processes to the target ones even for a mismatch between the input noise dimensionality and the effective dimensionality of the target stochastic processes. We also studied the overfitting issue for both the discriminator and generator, and we found that overfitting occurs also in the generator in addition to the discriminator as previously reported. Subsequently, we considered the solution of elliptic SDEs requiring approximations of three stochastic processes, namely the solution, the forcing, and the diffusion coefficient. We used three generators for the PI-GANs, two of them were feed forward deep neural networks (DNNs) while the other one was the neural network induced by the SDE. Depending on the data, we employed one or multiple feed forward DNNs as the discriminators in PI-GANs. Here, we have demonstrated the accuracy and effectiveness of PI-GANs in solving SDEs for up to 30 dimensions, but in principle, PI-GANs could tackle very high dimensional problems given more sensor data with low-polynomial growth in computational cost.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/8b1ba1037aefddec9ce9d07858f661b72a1b41fe.pdf",
        "venue": "SIAM Journal on Scientific Computing",
        "citationCount": 390,
        "score": 55.71428571428571,
        "summary": "We developed a new class of physics-informed generative adversarial networks (PI-GANs) to solve in a unified manner forward, inverse and mixed stochastic problems based on a limited number of scattered measurements. Unlike standard GANs relying only on data for training, here we encoded into the architecture of GANs the governing physical laws in the form of stochastic differential equations (SDEs) using automatic differentiation. In particular, we applied Wasserstein GANs with gradient penalty (WGAN-GP) for its enhanced stability compared to vanilla GANs. We first tested WGAN-GP in approximating Gaussian processes of different correlation lengths based on data realizations collected from simultaneous reads at sparsely placed sensors. We obtained good approximation of the generated stochastic processes to the target ones even for a mismatch between the input noise dimensionality and the effective dimensionality of the target stochastic processes. We also studied the overfitting issue for both the discriminator and generator, and we found that overfitting occurs also in the generator in addition to the discriminator as previously reported. Subsequently, we considered the solution of elliptic SDEs requiring approximations of three stochastic processes, namely the solution, the forcing, and the diffusion coefficient. We used three generators for the PI-GANs, two of them were feed forward deep neural networks (DNNs) while the other one was the neural network induced by the SDE. Depending on the data, we employed one or multiple feed forward DNNs as the discriminators in PI-GANs. Here, we have demonstrated the accuracy and effectiveness of PI-GANs in solving SDEs for up to 30 dimensions, but in principle, PI-GANs could tackle very high dimensional problems given more sensor data with low-polynomial growth in computational cost.",
        "keywords": []
      },
      "file_name": "8b1ba1037aefddec9ce9d07858f661b72a1b41fe.pdf"
    },
    {
      "success": true,
      "doc_id": "22d7dbb40d1e20a571637838bb279fc4",
      "summary": "Generative Adversarial Networks (GANs) are known to be difficult to train, despite considerable research effort. Several regularization techniques for stabilizing training have been proposed, but they introduce non-trivial computational overheads and interact poorly with existing techniques like spectral normalization. In this work, we propose a simple, effective training stabilizer based on the notion of consistency regularization---a popular technique in the semi-supervised learning literature. In particular, we augment data passing into the GAN discriminator and penalize the sensitivity of the discriminator to these augmentations. We conduct a series of experiments to demonstrate that consistency regularization works effectively with spectral normalization and various GAN architectures, loss functions and optimizer settings. Our method achieves the best FID scores for unconditional image generation compared to other regularization methods on CIFAR-10 and CelebA. Moreover, Our consistency regularized GAN (CR-GAN) improves state-of-the-art FID scores for conditional generation from 14.73 to 11.48 on CIFAR-10 and from 8.73 to 6.66 on ImageNet-2012.",
      "intriguing_abstract": "Generative Adversarial Networks (GANs) are known to be difficult to train, despite considerable research effort. Several regularization techniques for stabilizing training have been proposed, but they introduce non-trivial computational overheads and interact poorly with existing techniques like spectral normalization. In this work, we propose a simple, effective training stabilizer based on the notion of consistency regularization---a popular technique in the semi-supervised learning literature. In particular, we augment data passing into the GAN discriminator and penalize the sensitivity of the discriminator to these augmentations. We conduct a series of experiments to demonstrate that consistency regularization works effectively with spectral normalization and various GAN architectures, loss functions and optimizer settings. Our method achieves the best FID scores for unconditional image generation compared to other regularization methods on CIFAR-10 and CelebA. Moreover, Our consistency regularized GAN (CR-GAN) improves state-of-the-art FID scores for conditional generation from 14.73 to 11.48 on CIFAR-10 and from 8.73 to 6.66 on ImageNet-2012.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/63470afe06145e08c3b851491450f68c83cc938f.pdf",
      "citation_key": "zhang2019hjo",
      "metadata": {
        "title": "Consistency Regularization for Generative Adversarial Networks",
        "authors": [
          "Han Zhang",
          "Zizhao Zhang",
          "Augustus Odena",
          "Honglak Lee"
        ],
        "published_date": "2019",
        "abstract": "Generative Adversarial Networks (GANs) are known to be difficult to train, despite considerable research effort. Several regularization techniques for stabilizing training have been proposed, but they introduce non-trivial computational overheads and interact poorly with existing techniques like spectral normalization. In this work, we propose a simple, effective training stabilizer based on the notion of consistency regularization---a popular technique in the semi-supervised learning literature. In particular, we augment data passing into the GAN discriminator and penalize the sensitivity of the discriminator to these augmentations. We conduct a series of experiments to demonstrate that consistency regularization works effectively with spectral normalization and various GAN architectures, loss functions and optimizer settings. Our method achieves the best FID scores for unconditional image generation compared to other regularization methods on CIFAR-10 and CelebA. Moreover, Our consistency regularized GAN (CR-GAN) improves state-of-the-art FID scores for conditional generation from 14.73 to 11.48 on CIFAR-10 and from 8.73 to 6.66 on ImageNet-2012.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/63470afe06145e08c3b851491450f68c83cc938f.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 287,
        "score": 47.83333333333333,
        "summary": "Generative Adversarial Networks (GANs) are known to be difficult to train, despite considerable research effort. Several regularization techniques for stabilizing training have been proposed, but they introduce non-trivial computational overheads and interact poorly with existing techniques like spectral normalization. In this work, we propose a simple, effective training stabilizer based on the notion of consistency regularization---a popular technique in the semi-supervised learning literature. In particular, we augment data passing into the GAN discriminator and penalize the sensitivity of the discriminator to these augmentations. We conduct a series of experiments to demonstrate that consistency regularization works effectively with spectral normalization and various GAN architectures, loss functions and optimizer settings. Our method achieves the best FID scores for unconditional image generation compared to other regularization methods on CIFAR-10 and CelebA. Moreover, Our consistency regularized GAN (CR-GAN) improves state-of-the-art FID scores for conditional generation from 14.73 to 11.48 on CIFAR-10 and from 8.73 to 6.66 on ImageNet-2012.",
        "keywords": []
      },
      "file_name": "63470afe06145e08c3b851491450f68c83cc938f.pdf"
    },
    {
      "success": true,
      "doc_id": "ce93af030212b16676ab5940c3af93e3",
      "summary": "Recent years have witnessed the rapid progress of generative adversarial networks (GANs). However, the success of the GAN models hinges on a large amount of training data. This work proposes a regularization approach for training robust GAN models on limited data. We theoretically show a connection between the regularized loss and an f-divergence called LeCam-divergence, which we find is more robust under limited training data. Extensive experiments on several benchmark datasets demonstrate that the proposed regularization scheme 1) improves the generalization performance and stabilizes the learning dynamics of GAN models under limited training data, and 2) complements the recent data augmentation methods. These properties facilitate training GAN models to achieve state-of-theart performance when only limited training data of the ImageNet benchmark is available. The source code is available at https://github.com/google/lecam-gan.",
      "intriguing_abstract": "Recent years have witnessed the rapid progress of generative adversarial networks (GANs). However, the success of the GAN models hinges on a large amount of training data. This work proposes a regularization approach for training robust GAN models on limited data. We theoretically show a connection between the regularized loss and an f-divergence called LeCam-divergence, which we find is more robust under limited training data. Extensive experiments on several benchmark datasets demonstrate that the proposed regularization scheme 1) improves the generalization performance and stabilizes the learning dynamics of GAN models under limited training data, and 2) complements the recent data augmentation methods. These properties facilitate training GAN models to achieve state-of-theart performance when only limited training data of the ImageNet benchmark is available. The source code is available at https://github.com/google/lecam-gan.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/cb2bd9549791520deccadfde221f8ca699675a96.pdf",
      "citation_key": "tseng2021m2s",
      "metadata": {
        "title": "Regularizing Generative Adversarial Networks under Limited Data",
        "authors": [
          "Hung-Yu Tseng",
          "Lu Jiang",
          "Ce Liu",
          "Ming-Hsuan Yang",
          "Weilong Yang"
        ],
        "published_date": "2021",
        "abstract": "Recent years have witnessed the rapid progress of generative adversarial networks (GANs). However, the success of the GAN models hinges on a large amount of training data. This work proposes a regularization approach for training robust GAN models on limited data. We theoretically show a connection between the regularized loss and an f-divergence called LeCam-divergence, which we find is more robust under limited training data. Extensive experiments on several benchmark datasets demonstrate that the proposed regularization scheme 1) improves the generalization performance and stabilizes the learning dynamics of GAN models under limited training data, and 2) complements the recent data augmentation methods. These properties facilitate training GAN models to achieve state-of-theart performance when only limited training data of the ImageNet benchmark is available. The source code is available at https://github.com/google/lecam-gan.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/cb2bd9549791520deccadfde221f8ca699675a96.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 150,
        "score": 37.5,
        "summary": "Recent years have witnessed the rapid progress of generative adversarial networks (GANs). However, the success of the GAN models hinges on a large amount of training data. This work proposes a regularization approach for training robust GAN models on limited data. We theoretically show a connection between the regularized loss and an f-divergence called LeCam-divergence, which we find is more robust under limited training data. Extensive experiments on several benchmark datasets demonstrate that the proposed regularization scheme 1) improves the generalization performance and stabilizes the learning dynamics of GAN models under limited training data, and 2) complements the recent data augmentation methods. These properties facilitate training GAN models to achieve state-of-theart performance when only limited training data of the ImageNet benchmark is available. The source code is available at https://github.com/google/lecam-gan.",
        "keywords": []
      },
      "file_name": "cb2bd9549791520deccadfde221f8ca699675a96.pdf"
    },
    {
      "success": true,
      "doc_id": "3690792abdbdd53e5c5ad09baedbc141",
      "summary": "Due to the real working conditions and data acquisition equipment, the collected working data of bearings are actually limited. Meanwhile, as the rolling bearing works in the normal state at most times, it is easy to raise the imbalance problem of fault types which restricts the diagnosis accuracy and stability. To solve these problems, we present an imbalanced fault diagnosis method based on the generative adversarial network (GAN) and provide a comparative study in detail. The key idea is utilizing GAN, a kind of deep learning technique, to generate synthetic samples for minority fault class and then improve the generalization ability of the fault diagnosis model. First, this method applies fast Fourier transform to pre-process the original vibration signal and then obtains the frequency spectrum of fault samples. Second, it uses the spectrum data as the input of GAN to generate the synthetic minority samples following the data distribution of the real samples. Finally, it puts the synthetic samples into the training set and builds a stacked denoising auto encoder model for fault diagnosis. To testify the effectiveness of the proposed method, a series of comparative experiments is carried out on the CWRU bearing dataset. The results show that the proposed method can provide a better solution for imbalanced fault diagnosis on the basis of generating similar fault samples. As a comparative study, the proposed method is compared to several diagnostic methods with traditional time-frequency domain characteristics. Moreover, we also demonstrate that the proposed method outperforms three widely used sample synthesis techniques, such as random oversampling, synthetic minority oversampling technique, and the principal curve-based oversampling method in terms of diagnosis accuracy and numerical stability.",
      "intriguing_abstract": "Due to the real working conditions and data acquisition equipment, the collected working data of bearings are actually limited. Meanwhile, as the rolling bearing works in the normal state at most times, it is easy to raise the imbalance problem of fault types which restricts the diagnosis accuracy and stability. To solve these problems, we present an imbalanced fault diagnosis method based on the generative adversarial network (GAN) and provide a comparative study in detail. The key idea is utilizing GAN, a kind of deep learning technique, to generate synthetic samples for minority fault class and then improve the generalization ability of the fault diagnosis model. First, this method applies fast Fourier transform to pre-process the original vibration signal and then obtains the frequency spectrum of fault samples. Second, it uses the spectrum data as the input of GAN to generate the synthetic minority samples following the data distribution of the real samples. Finally, it puts the synthetic samples into the training set and builds a stacked denoising auto encoder model for fault diagnosis. To testify the effectiveness of the proposed method, a series of comparative experiments is carried out on the CWRU bearing dataset. The results show that the proposed method can provide a better solution for imbalanced fault diagnosis on the basis of generating similar fault samples. As a comparative study, the proposed method is compared to several diagnostic methods with traditional time-frequency domain characteristics. Moreover, we also demonstrate that the proposed method outperforms three widely used sample synthesis techniques, such as random oversampling, synthetic minority oversampling technique, and the principal curve-based oversampling method in terms of diagnosis accuracy and numerical stability.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/3228c8073f6aae9c287dbeea949fbad68f9d5ba1.pdf",
      "citation_key": "mao20196tx",
      "metadata": {
        "title": "Imbalanced Fault Diagnosis of Rolling Bearing Based on Generative Adversarial Network: A Comparative Study",
        "authors": [
          "Wentao Mao",
          "Yamin Liu",
          "Ling Ding",
          "Yuan Li"
        ],
        "published_date": "2019",
        "abstract": "Due to the real working conditions and data acquisition equipment, the collected working data of bearings are actually limited. Meanwhile, as the rolling bearing works in the normal state at most times, it is easy to raise the imbalance problem of fault types which restricts the diagnosis accuracy and stability. To solve these problems, we present an imbalanced fault diagnosis method based on the generative adversarial network (GAN) and provide a comparative study in detail. The key idea is utilizing GAN, a kind of deep learning technique, to generate synthetic samples for minority fault class and then improve the generalization ability of the fault diagnosis model. First, this method applies fast Fourier transform to pre-process the original vibration signal and then obtains the frequency spectrum of fault samples. Second, it uses the spectrum data as the input of GAN to generate the synthetic minority samples following the data distribution of the real samples. Finally, it puts the synthetic samples into the training set and builds a stacked denoising auto encoder model for fault diagnosis. To testify the effectiveness of the proposed method, a series of comparative experiments is carried out on the CWRU bearing dataset. The results show that the proposed method can provide a better solution for imbalanced fault diagnosis on the basis of generating similar fault samples. As a comparative study, the proposed method is compared to several diagnostic methods with traditional time-frequency domain characteristics. Moreover, we also demonstrate that the proposed method outperforms three widely used sample synthesis techniques, such as random oversampling, synthetic minority oversampling technique, and the principal curve-based oversampling method in terms of diagnosis accuracy and numerical stability.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/3228c8073f6aae9c287dbeea949fbad68f9d5ba1.pdf",
        "venue": "IEEE Access",
        "citationCount": 210,
        "score": 35.0,
        "summary": "Due to the real working conditions and data acquisition equipment, the collected working data of bearings are actually limited. Meanwhile, as the rolling bearing works in the normal state at most times, it is easy to raise the imbalance problem of fault types which restricts the diagnosis accuracy and stability. To solve these problems, we present an imbalanced fault diagnosis method based on the generative adversarial network (GAN) and provide a comparative study in detail. The key idea is utilizing GAN, a kind of deep learning technique, to generate synthetic samples for minority fault class and then improve the generalization ability of the fault diagnosis model. First, this method applies fast Fourier transform to pre-process the original vibration signal and then obtains the frequency spectrum of fault samples. Second, it uses the spectrum data as the input of GAN to generate the synthetic minority samples following the data distribution of the real samples. Finally, it puts the synthetic samples into the training set and builds a stacked denoising auto encoder model for fault diagnosis. To testify the effectiveness of the proposed method, a series of comparative experiments is carried out on the CWRU bearing dataset. The results show that the proposed method can provide a better solution for imbalanced fault diagnosis on the basis of generating similar fault samples. As a comparative study, the proposed method is compared to several diagnostic methods with traditional time-frequency domain characteristics. Moreover, we also demonstrate that the proposed method outperforms three widely used sample synthesis techniques, such as random oversampling, synthetic minority oversampling technique, and the principal curve-based oversampling method in terms of diagnosis accuracy and numerical stability.",
        "keywords": []
      },
      "file_name": "3228c8073f6aae9c287dbeea949fbad68f9d5ba1.pdf"
    },
    {
      "success": true,
      "doc_id": "06226e381b1164885663e04841998ca6",
      "summary": "Generative adversarial networks (GANs) are recently highly successful in generative applications involving images and start being applied to time series data. Here we describe EEG-GAN as a framework to generate electroencephalographic (EEG) brain signals. We introduce a modification to the improved training of Wasserstein GANs to stabilize training and investigate a range of architectural choices critical for time series generation (most notably up- and down-sampling). For evaluation we consider and compare different metrics such as Inception score, Frechet inception distance and sliced Wasserstein distance, together showing that our EEG-GAN framework generated naturalistic EEG examples. It thus opens up a range of new generative application scenarios in the neuroscientific and neurological context, such as data augmentation in brain-computer interfacing tasks, EEG super-sampling, or restoration of corrupted data segments. The possibility to generate signals of a certain class and/or with specific properties may also open a new avenue for research into the underlying structure of brain signals.",
      "intriguing_abstract": "Generative adversarial networks (GANs) are recently highly successful in generative applications involving images and start being applied to time series data. Here we describe EEG-GAN as a framework to generate electroencephalographic (EEG) brain signals. We introduce a modification to the improved training of Wasserstein GANs to stabilize training and investigate a range of architectural choices critical for time series generation (most notably up- and down-sampling). For evaluation we consider and compare different metrics such as Inception score, Frechet inception distance and sliced Wasserstein distance, together showing that our EEG-GAN framework generated naturalistic EEG examples. It thus opens up a range of new generative application scenarios in the neuroscientific and neurological context, such as data augmentation in brain-computer interfacing tasks, EEG super-sampling, or restoration of corrupted data segments. The possibility to generate signals of a certain class and/or with specific properties may also open a new avenue for research into the underlying structure of brain signals.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/27e13389203b2f8f6138afed867965a3a38cbd8e.pdf",
      "citation_key": "hartmann2018h3s",
      "metadata": {
        "title": "EEG-GAN: Generative adversarial networks for electroencephalograhic (EEG) brain signals",
        "authors": [
          "K. Hartmann",
          "R. Schirrmeister",
          "T. Ball"
        ],
        "published_date": "2018",
        "abstract": "Generative adversarial networks (GANs) are recently highly successful in generative applications involving images and start being applied to time series data. Here we describe EEG-GAN as a framework to generate electroencephalographic (EEG) brain signals. We introduce a modification to the improved training of Wasserstein GANs to stabilize training and investigate a range of architectural choices critical for time series generation (most notably up- and down-sampling). For evaluation we consider and compare different metrics such as Inception score, Frechet inception distance and sliced Wasserstein distance, together showing that our EEG-GAN framework generated naturalistic EEG examples. It thus opens up a range of new generative application scenarios in the neuroscientific and neurological context, such as data augmentation in brain-computer interfacing tasks, EEG super-sampling, or restoration of corrupted data segments. The possibility to generate signals of a certain class and/or with specific properties may also open a new avenue for research into the underlying structure of brain signals.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/27e13389203b2f8f6138afed867965a3a38cbd8e.pdf",
        "venue": "arXiv.org",
        "citationCount": 242,
        "score": 34.57142857142857,
        "summary": "Generative adversarial networks (GANs) are recently highly successful in generative applications involving images and start being applied to time series data. Here we describe EEG-GAN as a framework to generate electroencephalographic (EEG) brain signals. We introduce a modification to the improved training of Wasserstein GANs to stabilize training and investigate a range of architectural choices critical for time series generation (most notably up- and down-sampling). For evaluation we consider and compare different metrics such as Inception score, Frechet inception distance and sliced Wasserstein distance, together showing that our EEG-GAN framework generated naturalistic EEG examples. It thus opens up a range of new generative application scenarios in the neuroscientific and neurological context, such as data augmentation in brain-computer interfacing tasks, EEG super-sampling, or restoration of corrupted data segments. The possibility to generate signals of a certain class and/or with specific properties may also open a new avenue for research into the underlying structure of brain signals.",
        "keywords": []
      },
      "file_name": "27e13389203b2f8f6138afed867965a3a38cbd8e.pdf"
    },
    {
      "success": true,
      "doc_id": "c533dbde98ce8f8d7bc12297347b055c",
      "summary": "Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably their most significant impact has been in the area of computer vision where great advances have been made in challenges such as plausible image generation, image-to-image translation, facial attribute manipulation, and similar domains. Despite the significant successes achieved to date, applying GANs to real-world problems still poses significant challenges, three of which we focus on here. These are as follows: (1) the generation of high quality images, (2) diversity of image generation, and (3) stabilizing training. Focusing on the degree to which popular GAN technologies have made progress against these challenges, we provide a detailed review of the state-of-the-art in GAN-related research in the published scientific literature. We further structure this review through a convenient taxonomy we have adopted based on variations in GAN architectures and loss functions. While several reviews for GANs have been presented to date, none have considered the status of this field based on their progress toward addressing practical challenges relevant to computer vision. Accordingly, we review and critically discuss the most popular architecture-variant, and loss-variant GANs, for tackling these challenges. Our objective is to provide an overview as well as a critical analysis of the status of GAN research in terms of relevant progress toward critical computer vision application requirements. As we do this we also discuss the most compelling applications in computer vision in which GANs have demonstrated considerable success along with some suggestions for future research directions. Codes related to the GAN-variants studied in this work is summarized on https://github.com/sheqi/GAN_Review.",
      "intriguing_abstract": "Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably their most significant impact has been in the area of computer vision where great advances have been made in challenges such as plausible image generation, image-to-image translation, facial attribute manipulation, and similar domains. Despite the significant successes achieved to date, applying GANs to real-world problems still poses significant challenges, three of which we focus on here. These are as follows: (1) the generation of high quality images, (2) diversity of image generation, and (3) stabilizing training. Focusing on the degree to which popular GAN technologies have made progress against these challenges, we provide a detailed review of the state-of-the-art in GAN-related research in the published scientific literature. We further structure this review through a convenient taxonomy we have adopted based on variations in GAN architectures and loss functions. While several reviews for GANs have been presented to date, none have considered the status of this field based on their progress toward addressing practical challenges relevant to computer vision. Accordingly, we review and critically discuss the most popular architecture-variant, and loss-variant GANs, for tackling these challenges. Our objective is to provide an overview as well as a critical analysis of the status of GAN research in terms of relevant progress toward critical computer vision application requirements. As we do this we also discuss the most compelling applications in computer vision in which GANs have demonstrated considerable success along with some suggestions for future research directions. Codes related to the GAN-variants studied in this work is summarized on https://github.com/sheqi/GAN_Review.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/cd682f085af85526631dc33617ac4aaae7309634.pdf",
      "citation_key": "wang2019w53",
      "metadata": {
        "title": "Generative Adversarial Networks in Computer Vision",
        "authors": [
          "Zhengwei Wang",
          "Qi She",
          "T. Ward"
        ],
        "published_date": "2019",
        "abstract": "Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably their most significant impact has been in the area of computer vision where great advances have been made in challenges such as plausible image generation, image-to-image translation, facial attribute manipulation, and similar domains. Despite the significant successes achieved to date, applying GANs to real-world problems still poses significant challenges, three of which we focus on here. These are as follows: (1) the generation of high quality images, (2) diversity of image generation, and (3) stabilizing training. Focusing on the degree to which popular GAN technologies have made progress against these challenges, we provide a detailed review of the state-of-the-art in GAN-related research in the published scientific literature. We further structure this review through a convenient taxonomy we have adopted based on variations in GAN architectures and loss functions. While several reviews for GANs have been presented to date, none have considered the status of this field based on their progress toward addressing practical challenges relevant to computer vision. Accordingly, we review and critically discuss the most popular architecture-variant, and loss-variant GANs, for tackling these challenges. Our objective is to provide an overview as well as a critical analysis of the status of GAN research in terms of relevant progress toward critical computer vision application requirements. As we do this we also discuss the most compelling applications in computer vision in which GANs have demonstrated considerable success along with some suggestions for future research directions. Codes related to the GAN-variants studied in this work is summarized on https://github.com/sheqi/GAN_Review.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/cd682f085af85526631dc33617ac4aaae7309634.pdf",
        "venue": "ACM Computing Surveys",
        "citationCount": 206,
        "score": 34.33333333333333,
        "summary": "Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably their most significant impact has been in the area of computer vision where great advances have been made in challenges such as plausible image generation, image-to-image translation, facial attribute manipulation, and similar domains. Despite the significant successes achieved to date, applying GANs to real-world problems still poses significant challenges, three of which we focus on here. These are as follows: (1) the generation of high quality images, (2) diversity of image generation, and (3) stabilizing training. Focusing on the degree to which popular GAN technologies have made progress against these challenges, we provide a detailed review of the state-of-the-art in GAN-related research in the published scientific literature. We further structure this review through a convenient taxonomy we have adopted based on variations in GAN architectures and loss functions. While several reviews for GANs have been presented to date, none have considered the status of this field based on their progress toward addressing practical challenges relevant to computer vision. Accordingly, we review and critically discuss the most popular architecture-variant, and loss-variant GANs, for tackling these challenges. Our objective is to provide an overview as well as a critical analysis of the status of GAN research in terms of relevant progress toward critical computer vision application requirements. As we do this we also discuss the most compelling applications in computer vision in which GANs have demonstrated considerable success along with some suggestions for future research directions. Codes related to the GAN-variants studied in this work is summarized on https://github.com/sheqi/GAN_Review.",
        "keywords": []
      },
      "file_name": "cd682f085af85526631dc33617ac4aaae7309634.pdf"
    },
    {
      "success": true,
      "doc_id": "6c1d568b21995f3499ae460fb801712d",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/1076a77834f11810fdcd100b21d90ca7bc1f9095.pdf",
      "citation_key": "luo2020aaj",
      "metadata": {
        "title": "A case study of conditional deep convolutional generative adversarial networks in machine fault diagnosis",
        "authors": [
          "Jia Luo",
          "Jinying Huang",
          "Hongmei Li"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/1076a77834f11810fdcd100b21d90ca7bc1f9095.pdf",
        "venue": "Journal of Intelligent Manufacturing",
        "citationCount": 165,
        "score": 33.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "1076a77834f11810fdcd100b21d90ca7bc1f9095.pdf"
    },
    {
      "success": true,
      "doc_id": "62b1eee218fbe2d3dd6e055f1487fb0f",
      "summary": "The generative adversarial network (GAN) framework has emerged as a powerful tool for various image and video synthesis tasks, allowing the synthesis of visual content in an unconditional or input-conditional manner. It has enabled the generation of high-resolution photorealistic images and videos, a task that was challenging or impossible with prior methods. It has also led to the creation of many new applications in content creation. In this article, we provide an overview of GANs with a special focus on algorithms and applications for visual synthesis. We cover several important techniques to stabilize GAN training, which has a reputation for being notoriously difficult. We also discuss its applications to image translation, image processing, video synthesis, and neural rendering.",
      "intriguing_abstract": "The generative adversarial network (GAN) framework has emerged as a powerful tool for various image and video synthesis tasks, allowing the synthesis of visual content in an unconditional or input-conditional manner. It has enabled the generation of high-resolution photorealistic images and videos, a task that was challenging or impossible with prior methods. It has also led to the creation of many new applications in content creation. In this article, we provide an overview of GANs with a special focus on algorithms and applications for visual synthesis. We cover several important techniques to stabilize GAN training, which has a reputation for being notoriously difficult. We also discuss its applications to image translation, image processing, video synthesis, and neural rendering.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/237729237fde44eb7ab8f35aafb82c9b8a816e44.pdf",
      "citation_key": "liu2020jt0",
      "metadata": {
        "title": "Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications",
        "authors": [
          "Ming-Yu Liu",
          "Xun Huang",
          "Jiahui Yu",
          "Ting-Chun Wang",
          "Arun Mallya"
        ],
        "published_date": "2020",
        "abstract": "The generative adversarial network (GAN) framework has emerged as a powerful tool for various image and video synthesis tasks, allowing the synthesis of visual content in an unconditional or input-conditional manner. It has enabled the generation of high-resolution photorealistic images and videos, a task that was challenging or impossible with prior methods. It has also led to the creation of many new applications in content creation. In this article, we provide an overview of GANs with a special focus on algorithms and applications for visual synthesis. We cover several important techniques to stabilize GAN training, which has a reputation for being notoriously difficult. We also discuss its applications to image translation, image processing, video synthesis, and neural rendering.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/237729237fde44eb7ab8f35aafb82c9b8a816e44.pdf",
        "venue": "Proceedings of the IEEE",
        "citationCount": 162,
        "score": 32.4,
        "summary": "The generative adversarial network (GAN) framework has emerged as a powerful tool for various image and video synthesis tasks, allowing the synthesis of visual content in an unconditional or input-conditional manner. It has enabled the generation of high-resolution photorealistic images and videos, a task that was challenging or impossible with prior methods. It has also led to the creation of many new applications in content creation. In this article, we provide an overview of GANs with a special focus on algorithms and applications for visual synthesis. We cover several important techniques to stabilize GAN training, which has a reputation for being notoriously difficult. We also discuss its applications to image translation, image processing, video synthesis, and neural rendering.",
        "keywords": []
      },
      "file_name": "237729237fde44eb7ab8f35aafb82c9b8a816e44.pdf"
    },
    {
      "success": true,
      "doc_id": "53a279169ad5fa876cf86f8cbf32fc8d",
      "summary": "Motivated by the pursuit of a systematic computational and algorithmic understanding of Generative Adversarial Networks (GANs), we present a simple yet unified non-asymptotic local convergence theory for smooth two-player games, which subsumes several discrete-time gradient-based saddle point dynamics. The analysis reveals the surprising nature of the off-diagonal interaction term as both a blessing and a curse. On the one hand, this interaction term explains the origin of the slow-down effect in the convergence of Simultaneous Gradient Ascent (SGA) to stable Nash equilibria. On the other hand, for the unstable equilibria, exponential convergence can be proved thanks to the interaction term, for three modified dynamics which have been proposed to stabilize GAN training: Optimistic Mirror Descent (OMD), Consensus Optimization (CO) and Predictive Method (PM). The analysis uncovers the intimate connections among these stabilizing techniques, and provides detailed characterization on the choice of learning rate.",
      "intriguing_abstract": "Motivated by the pursuit of a systematic computational and algorithmic understanding of Generative Adversarial Networks (GANs), we present a simple yet unified non-asymptotic local convergence theory for smooth two-player games, which subsumes several discrete-time gradient-based saddle point dynamics. The analysis reveals the surprising nature of the off-diagonal interaction term as both a blessing and a curse. On the one hand, this interaction term explains the origin of the slow-down effect in the convergence of Simultaneous Gradient Ascent (SGA) to stable Nash equilibria. On the other hand, for the unstable equilibria, exponential convergence can be proved thanks to the interaction term, for three modified dynamics which have been proposed to stabilize GAN training: Optimistic Mirror Descent (OMD), Consensus Optimization (CO) and Predictive Method (PM). The analysis uncovers the intimate connections among these stabilizing techniques, and provides detailed characterization on the choice of learning rate.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/2f12a10172f33523b288269e59211261ca2f6f67.pdf",
      "citation_key": "liang2018r52",
      "metadata": {
        "title": "Interaction Matters: A Note on Non-asymptotic Local Convergence of Generative Adversarial Networks",
        "authors": [
          "Tengyuan Liang",
          "J. Stokes"
        ],
        "published_date": "2018",
        "abstract": "Motivated by the pursuit of a systematic computational and algorithmic understanding of Generative Adversarial Networks (GANs), we present a simple yet unified non-asymptotic local convergence theory for smooth two-player games, which subsumes several discrete-time gradient-based saddle point dynamics. The analysis reveals the surprising nature of the off-diagonal interaction term as both a blessing and a curse. On the one hand, this interaction term explains the origin of the slow-down effect in the convergence of Simultaneous Gradient Ascent (SGA) to stable Nash equilibria. On the other hand, for the unstable equilibria, exponential convergence can be proved thanks to the interaction term, for three modified dynamics which have been proposed to stabilize GAN training: Optimistic Mirror Descent (OMD), Consensus Optimization (CO) and Predictive Method (PM). The analysis uncovers the intimate connections among these stabilizing techniques, and provides detailed characterization on the choice of learning rate.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/2f12a10172f33523b288269e59211261ca2f6f67.pdf",
        "venue": "International Conference on Artificial Intelligence and Statistics",
        "citationCount": 219,
        "score": 31.285714285714285,
        "summary": "Motivated by the pursuit of a systematic computational and algorithmic understanding of Generative Adversarial Networks (GANs), we present a simple yet unified non-asymptotic local convergence theory for smooth two-player games, which subsumes several discrete-time gradient-based saddle point dynamics. The analysis reveals the surprising nature of the off-diagonal interaction term as both a blessing and a curse. On the one hand, this interaction term explains the origin of the slow-down effect in the convergence of Simultaneous Gradient Ascent (SGA) to stable Nash equilibria. On the other hand, for the unstable equilibria, exponential convergence can be proved thanks to the interaction term, for three modified dynamics which have been proposed to stabilize GAN training: Optimistic Mirror Descent (OMD), Consensus Optimization (CO) and Predictive Method (PM). The analysis uncovers the intimate connections among these stabilizing techniques, and provides detailed characterization on the choice of learning rate.",
        "keywords": []
      },
      "file_name": "2f12a10172f33523b288269e59211261ca2f6f67.pdf"
    },
    {
      "success": true,
      "doc_id": "58658bec671feb779d96b53a69daad69",
      "summary": "Convolutional neural networks have been successfully applied to semantic segmentation problems. However, there are many problems that are inherently not pixel-wise classification problems but are nevertheless frequently formulated as semantic segmentation. This ill-posed formulation consequently necessitates hand-crafted scenario-specific and computationally expensive post-processing methods to convert the per pixel probability maps to final desired outputs. Generative adversarial networks (GANs) can be used to make the semantic segmentation network output to be more realistic or better structure-preserving, decreasing the dependency on potentially complex post-processing. In this work, we propose EL-GAN: a GAN framework to mitigate the discussed problem using an embedding loss. With EL-GAN, we discriminate based on learned embeddings of both the labels and the prediction at the same time. This results in more stable training due to having better discriminative information, benefiting from seeing both `fake' and `real' predictions at the same time. This substantially stabilizes the adversarial training process. We use the TuSimple lane marking challenge to demonstrate that with our proposed framework it is viable to overcome the inherent anomalies of posing it as a semantic segmentation problem. Not only is the output considerably more similar to the labels when compared to conventional methods, the subsequent post-processing is also simpler and crosses the competitive 96% accuracy threshold.",
      "intriguing_abstract": "Convolutional neural networks have been successfully applied to semantic segmentation problems. However, there are many problems that are inherently not pixel-wise classification problems but are nevertheless frequently formulated as semantic segmentation. This ill-posed formulation consequently necessitates hand-crafted scenario-specific and computationally expensive post-processing methods to convert the per pixel probability maps to final desired outputs. Generative adversarial networks (GANs) can be used to make the semantic segmentation network output to be more realistic or better structure-preserving, decreasing the dependency on potentially complex post-processing. In this work, we propose EL-GAN: a GAN framework to mitigate the discussed problem using an embedding loss. With EL-GAN, we discriminate based on learned embeddings of both the labels and the prediction at the same time. This results in more stable training due to having better discriminative information, benefiting from seeing both `fake' and `real' predictions at the same time. This substantially stabilizes the adversarial training process. We use the TuSimple lane marking challenge to demonstrate that with our proposed framework it is viable to overcome the inherent anomalies of posing it as a semantic segmentation problem. Not only is the output considerably more similar to the labels when compared to conventional methods, the subsequent post-processing is also simpler and crosses the competitive 96% accuracy threshold.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/34d7c6428bd6d6b109f88ac4a6c71553a4a0f489.pdf",
      "citation_key": "ghafoorian2018fwh",
      "metadata": {
        "title": "EL-GAN: Embedding Loss Driven Generative Adversarial Networks for Lane Detection",
        "authors": [
          "Mohsen Ghafoorian",
          "C. Nugteren",
          "N. Baka",
          "O. Booij",
          "M. Hofmann"
        ],
        "published_date": "2018",
        "abstract": "Convolutional neural networks have been successfully applied to semantic segmentation problems. However, there are many problems that are inherently not pixel-wise classification problems but are nevertheless frequently formulated as semantic segmentation. This ill-posed formulation consequently necessitates hand-crafted scenario-specific and computationally expensive post-processing methods to convert the per pixel probability maps to final desired outputs. Generative adversarial networks (GANs) can be used to make the semantic segmentation network output to be more realistic or better structure-preserving, decreasing the dependency on potentially complex post-processing. In this work, we propose EL-GAN: a GAN framework to mitigate the discussed problem using an embedding loss. With EL-GAN, we discriminate based on learned embeddings of both the labels and the prediction at the same time. This results in more stable training due to having better discriminative information, benefiting from seeing both `fake' and `real' predictions at the same time. This substantially stabilizes the adversarial training process. We use the TuSimple lane marking challenge to demonstrate that with our proposed framework it is viable to overcome the inherent anomalies of posing it as a semantic segmentation problem. Not only is the output considerably more similar to the labels when compared to conventional methods, the subsequent post-processing is also simpler and crosses the competitive 96% accuracy threshold.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/34d7c6428bd6d6b109f88ac4a6c71553a4a0f489.pdf",
        "venue": "ECCV Workshops",
        "citationCount": 205,
        "score": 29.285714285714285,
        "summary": "Convolutional neural networks have been successfully applied to semantic segmentation problems. However, there are many problems that are inherently not pixel-wise classification problems but are nevertheless frequently formulated as semantic segmentation. This ill-posed formulation consequently necessitates hand-crafted scenario-specific and computationally expensive post-processing methods to convert the per pixel probability maps to final desired outputs. Generative adversarial networks (GANs) can be used to make the semantic segmentation network output to be more realistic or better structure-preserving, decreasing the dependency on potentially complex post-processing. In this work, we propose EL-GAN: a GAN framework to mitigate the discussed problem using an embedding loss. With EL-GAN, we discriminate based on learned embeddings of both the labels and the prediction at the same time. This results in more stable training due to having better discriminative information, benefiting from seeing both `fake' and `real' predictions at the same time. This substantially stabilizes the adversarial training process. We use the TuSimple lane marking challenge to demonstrate that with our proposed framework it is viable to overcome the inherent anomalies of posing it as a semantic segmentation problem. Not only is the output considerably more similar to the labels when compared to conventional methods, the subsequent post-processing is also simpler and crosses the competitive 96% accuracy threshold.",
        "keywords": []
      },
      "file_name": "34d7c6428bd6d6b109f88ac4a6c71553a4a0f489.pdf"
    },
    {
      "success": true,
      "doc_id": "2d7c6ede455bda9b007f542613b257c8",
      "summary": "We study the problem of multi-focus image fusion, where the key challenge is detecting the focused regions accurately among multiple partially focused source images. Inspired by the conditional generative adversarial network (cGAN) to image-to-image task, we propose a novel FuseGAN to fulfill the images-to-image for multi-focus image fusion. To satisfy the requirement of dual input-to-one output, the encoder of the generator in FuseGAN is designed as a Siamese network. The least square GAN objective is employed to enhance the training stability of FuseGAN, resulting in an accurate confidence map for focus region detection. Also, we exploit the convolutional conditional random fields technique on the confidence map to reach a refined final decision map for better focus region detection. Moreover, due to the lack of a large-scale standard dataset, we synthesize a large enough multi-focus image dataset based on a public natural image dataset PASCAL VOC 2012, where we utilize a normalized disk point spread function to simulate the defocus and separate the background and foreground in the synthesis for each image. We conduct extensive experiments on two public datasets to verify the effectiveness of the proposed method. Results demonstrate that the proposed method presents accurate decision maps for focus regions in multi-focus images, such that the fused images are superior to 11 recent state-of-the-art algorithms, not only in visual perception, but also in quantitative analysis in terms of five metrics.",
      "intriguing_abstract": "We study the problem of multi-focus image fusion, where the key challenge is detecting the focused regions accurately among multiple partially focused source images. Inspired by the conditional generative adversarial network (cGAN) to image-to-image task, we propose a novel FuseGAN to fulfill the images-to-image for multi-focus image fusion. To satisfy the requirement of dual input-to-one output, the encoder of the generator in FuseGAN is designed as a Siamese network. The least square GAN objective is employed to enhance the training stability of FuseGAN, resulting in an accurate confidence map for focus region detection. Also, we exploit the convolutional conditional random fields technique on the confidence map to reach a refined final decision map for better focus region detection. Moreover, due to the lack of a large-scale standard dataset, we synthesize a large enough multi-focus image dataset based on a public natural image dataset PASCAL VOC 2012, where we utilize a normalized disk point spread function to simulate the defocus and separate the background and foreground in the synthesis for each image. We conduct extensive experiments on two public datasets to verify the effectiveness of the proposed method. Results demonstrate that the proposed method presents accurate decision maps for focus regions in multi-focus images, such that the fused images are superior to 11 recent state-of-the-art algorithms, not only in visual perception, but also in quantitative analysis in terms of five metrics.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/29a23cd054d7a8202e6cdc60a53321bbc6e1aefd.pdf",
      "citation_key": "guo2019414",
      "metadata": {
        "title": "FuseGAN: Learning to Fuse Multi-Focus Image via Conditional Generative Adversarial Network",
        "authors": [
          "Xiaopeng Guo",
          "Rencan Nie",
          "Jinde Cao",
          "Dongming Zhou",
          "Liye Mei",
          "Kangjian He"
        ],
        "published_date": "2019",
        "abstract": "We study the problem of multi-focus image fusion, where the key challenge is detecting the focused regions accurately among multiple partially focused source images. Inspired by the conditional generative adversarial network (cGAN) to image-to-image task, we propose a novel FuseGAN to fulfill the images-to-image for multi-focus image fusion. To satisfy the requirement of dual input-to-one output, the encoder of the generator in FuseGAN is designed as a Siamese network. The least square GAN objective is employed to enhance the training stability of FuseGAN, resulting in an accurate confidence map for focus region detection. Also, we exploit the convolutional conditional random fields technique on the confidence map to reach a refined final decision map for better focus region detection. Moreover, due to the lack of a large-scale standard dataset, we synthesize a large enough multi-focus image dataset based on a public natural image dataset PASCAL VOC 2012, where we utilize a normalized disk point spread function to simulate the defocus and separate the background and foreground in the synthesis for each image. We conduct extensive experiments on two public datasets to verify the effectiveness of the proposed method. Results demonstrate that the proposed method presents accurate decision maps for focus regions in multi-focus images, such that the fused images are superior to 11 recent state-of-the-art algorithms, not only in visual perception, but also in quantitative analysis in terms of five metrics.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/29a23cd054d7a8202e6cdc60a53321bbc6e1aefd.pdf",
        "venue": "IEEE transactions on multimedia",
        "citationCount": 162,
        "score": 27.0,
        "summary": "We study the problem of multi-focus image fusion, where the key challenge is detecting the focused regions accurately among multiple partially focused source images. Inspired by the conditional generative adversarial network (cGAN) to image-to-image task, we propose a novel FuseGAN to fulfill the images-to-image for multi-focus image fusion. To satisfy the requirement of dual input-to-one output, the encoder of the generator in FuseGAN is designed as a Siamese network. The least square GAN objective is employed to enhance the training stability of FuseGAN, resulting in an accurate confidence map for focus region detection. Also, we exploit the convolutional conditional random fields technique on the confidence map to reach a refined final decision map for better focus region detection. Moreover, due to the lack of a large-scale standard dataset, we synthesize a large enough multi-focus image dataset based on a public natural image dataset PASCAL VOC 2012, where we utilize a normalized disk point spread function to simulate the defocus and separate the background and foreground in the synthesis for each image. We conduct extensive experiments on two public datasets to verify the effectiveness of the proposed method. Results demonstrate that the proposed method presents accurate decision maps for focus regions in multi-focus images, such that the fused images are superior to 11 recent state-of-the-art algorithms, not only in visual perception, but also in quantitative analysis in terms of five metrics.",
        "keywords": []
      },
      "file_name": "29a23cd054d7a8202e6cdc60a53321bbc6e1aefd.pdf"
    },
    {
      "success": true,
      "doc_id": "f605dc08d4b77de12feeec2ab27a6ad7",
      "summary": "The identification of grape leaf diseases based on deep learning is critical to controlling the spread of diseases and ensuring the healthy development of the grape industry. Focusing on the lack of training images of grape leaf diseases, this paper proposes a novel model named Leaf GAN, which is based on generative adversarial networks (GANs), to generate images of four different grape leaf diseases for training identification models. A generator model with degressive channels is first designed to generate grape leaf disease images; then, the dense connectivity strategy and instance normalization are fused into an efficient discriminator to identify real and fake disease images by utilizing their excellent feature extraction capability on grape leaf lesions. Finally, the deep regret gradient penalty method is applied to stabilize the training process of the model. Using a total of 4,062 grape leaf disease images, the Leaf GAN model ultimately generates 8,124 grape leaf disease images. The generated grape leaf disease images based on Leaf GAN model can obtain better performance than DCGAN and WGAN in terms of the Fréchet inception distance. The experimental results show that the proposed Leaf GAN model generates sufficient grape leaf disease images with prominent lesions, providing a feasible solution for the data augmentation of grape leaf disease images. For the eight prevailing classification models with the expanded dataset, the identification performance based on CNNs indicated higher accuracies, whereby all the accuracies were better than those of the initial dataset with other data augmentation methods. Among them, Xception achieves a recognition accuracy of 98.70% on the testing set. The results demonstrate that the proposed data augmentation method represents a new approach to overcoming the overfitting problem in disease identification and can effectively improve the identification accuracy.",
      "intriguing_abstract": "The identification of grape leaf diseases based on deep learning is critical to controlling the spread of diseases and ensuring the healthy development of the grape industry. Focusing on the lack of training images of grape leaf diseases, this paper proposes a novel model named Leaf GAN, which is based on generative adversarial networks (GANs), to generate images of four different grape leaf diseases for training identification models. A generator model with degressive channels is first designed to generate grape leaf disease images; then, the dense connectivity strategy and instance normalization are fused into an efficient discriminator to identify real and fake disease images by utilizing their excellent feature extraction capability on grape leaf lesions. Finally, the deep regret gradient penalty method is applied to stabilize the training process of the model. Using a total of 4,062 grape leaf disease images, the Leaf GAN model ultimately generates 8,124 grape leaf disease images. The generated grape leaf disease images based on Leaf GAN model can obtain better performance than DCGAN and WGAN in terms of the Fréchet inception distance. The experimental results show that the proposed Leaf GAN model generates sufficient grape leaf disease images with prominent lesions, providing a feasible solution for the data augmentation of grape leaf disease images. For the eight prevailing classification models with the expanded dataset, the identification performance based on CNNs indicated higher accuracies, whereby all the accuracies were better than those of the initial dataset with other data augmentation methods. Among them, Xception achieves a recognition accuracy of 98.70% on the testing set. The results demonstrate that the proposed data augmentation method represents a new approach to overcoming the overfitting problem in disease identification and can effectively improve the identification accuracy.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/0b98a1efa7bef2acb2091d5b1659430ef4df1364.pdf",
      "citation_key": "liu2020kd1",
      "metadata": {
        "title": "A Data Augmentation Method Based on Generative Adversarial Networks for Grape Leaf Disease Identification",
        "authors": [
          "B. Liu",
          "Cheng Tan",
          "Shuqin Li",
          "Jinrong He",
          "Hongyan Wang"
        ],
        "published_date": "2020",
        "abstract": "The identification of grape leaf diseases based on deep learning is critical to controlling the spread of diseases and ensuring the healthy development of the grape industry. Focusing on the lack of training images of grape leaf diseases, this paper proposes a novel model named Leaf GAN, which is based on generative adversarial networks (GANs), to generate images of four different grape leaf diseases for training identification models. A generator model with degressive channels is first designed to generate grape leaf disease images; then, the dense connectivity strategy and instance normalization are fused into an efficient discriminator to identify real and fake disease images by utilizing their excellent feature extraction capability on grape leaf lesions. Finally, the deep regret gradient penalty method is applied to stabilize the training process of the model. Using a total of 4,062 grape leaf disease images, the Leaf GAN model ultimately generates 8,124 grape leaf disease images. The generated grape leaf disease images based on Leaf GAN model can obtain better performance than DCGAN and WGAN in terms of the Fréchet inception distance. The experimental results show that the proposed Leaf GAN model generates sufficient grape leaf disease images with prominent lesions, providing a feasible solution for the data augmentation of grape leaf disease images. For the eight prevailing classification models with the expanded dataset, the identification performance based on CNNs indicated higher accuracies, whereby all the accuracies were better than those of the initial dataset with other data augmentation methods. Among them, Xception achieves a recognition accuracy of 98.70% on the testing set. The results demonstrate that the proposed data augmentation method represents a new approach to overcoming the overfitting problem in disease identification and can effectively improve the identification accuracy.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/0b98a1efa7bef2acb2091d5b1659430ef4df1364.pdf",
        "venue": "IEEE Access",
        "citationCount": 130,
        "score": 26.0,
        "summary": "The identification of grape leaf diseases based on deep learning is critical to controlling the spread of diseases and ensuring the healthy development of the grape industry. Focusing on the lack of training images of grape leaf diseases, this paper proposes a novel model named Leaf GAN, which is based on generative adversarial networks (GANs), to generate images of four different grape leaf diseases for training identification models. A generator model with degressive channels is first designed to generate grape leaf disease images; then, the dense connectivity strategy and instance normalization are fused into an efficient discriminator to identify real and fake disease images by utilizing their excellent feature extraction capability on grape leaf lesions. Finally, the deep regret gradient penalty method is applied to stabilize the training process of the model. Using a total of 4,062 grape leaf disease images, the Leaf GAN model ultimately generates 8,124 grape leaf disease images. The generated grape leaf disease images based on Leaf GAN model can obtain better performance than DCGAN and WGAN in terms of the Fréchet inception distance. The experimental results show that the proposed Leaf GAN model generates sufficient grape leaf disease images with prominent lesions, providing a feasible solution for the data augmentation of grape leaf disease images. For the eight prevailing classification models with the expanded dataset, the identification performance based on CNNs indicated higher accuracies, whereby all the accuracies were better than those of the initial dataset with other data augmentation methods. Among them, Xception achieves a recognition accuracy of 98.70% on the testing set. The results demonstrate that the proposed data augmentation method represents a new approach to overcoming the overfitting problem in disease identification and can effectively improve the identification accuracy.",
        "keywords": []
      },
      "file_name": "0b98a1efa7bef2acb2091d5b1659430ef4df1364.pdf"
    },
    {
      "success": true,
      "doc_id": "8b0746469bbdf287e01da8446731ef42",
      "summary": "Generative adversarial networks (GANs) are a learning framework that rely on training a discriminator to estimate a measure of difference between a target and generated distributions. GANs, as normally formulated, rely on the generated samples being completely differentiable w.r.t. the generative parameters, and thus do not work for discrete data. We introduce a method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance weights for generated samples, thus providing a policy gradient for training the generator. The importance weights have a strong connection to the decision boundary of the discriminator, and we call our method boundary-seeking GANs (BGANs). We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation. In addition, the boundary-seeking objective extends to continuous data, which can be used to improve stability of training.",
      "intriguing_abstract": "Generative adversarial networks (GANs) are a learning framework that rely on training a discriminator to estimate a measure of difference between a target and generated distributions. GANs, as normally formulated, rely on the generated samples being completely differentiable w.r.t. the generative parameters, and thus do not work for discrete data. We introduce a method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance weights for generated samples, thus providing a policy gradient for training the generator. The importance weights have a strong connection to the decision boundary of the discriminator, and we call our method boundary-seeking GANs (BGANs). We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation. In addition, the boundary-seeking objective extends to continuous data, which can be used to improve stability of training.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/042116e805aa3b5171efaf0c822dc142310ceefe.pdf",
      "citation_key": "hjelm2017iqg",
      "metadata": {
        "title": "Boundary-Seeking Generative Adversarial Networks",
        "authors": [
          "R. Devon Hjelm",
          "Athul Paul Jacob",
          "Tong Che",
          "Kyunghyun Cho",
          "Yoshua Bengio"
        ],
        "published_date": "2017",
        "abstract": "Generative adversarial networks (GANs) are a learning framework that rely on training a discriminator to estimate a measure of difference between a target and generated distributions. GANs, as normally formulated, rely on the generated samples being completely differentiable w.r.t. the generative parameters, and thus do not work for discrete data. We introduce a method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance weights for generated samples, thus providing a policy gradient for training the generator. The importance weights have a strong connection to the decision boundary of the discriminator, and we call our method boundary-seeking GANs (BGANs). We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation. In addition, the boundary-seeking objective extends to continuous data, which can be used to improve stability of training.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/042116e805aa3b5171efaf0c822dc142310ceefe.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 172,
        "score": 21.5,
        "summary": "Generative adversarial networks (GANs) are a learning framework that rely on training a discriminator to estimate a measure of difference between a target and generated distributions. GANs, as normally formulated, rely on the generated samples being completely differentiable w.r.t. the generative parameters, and thus do not work for discrete data. We introduce a method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance weights for generated samples, thus providing a policy gradient for training the generator. The importance weights have a strong connection to the decision boundary of the discriminator, and we call our method boundary-seeking GANs (BGANs). We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation. In addition, the boundary-seeking objective extends to continuous data, which can be used to improve stability of training.",
        "keywords": []
      },
      "file_name": "042116e805aa3b5171efaf0c822dc142310ceefe.pdf"
    },
    {
      "success": true,
      "doc_id": "0bfe772a553b798af9c8043ef2b44822",
      "summary": "The boundaries of cyber-physical systems (CPS) and the Internet of Things (IoT) are converging together day by day to introduce a common platform on hybrid systems. Moreover, the combination of artificial intelligence (AI) with CPS creates a new dimension of technological advancement. All these connectivity and dependability are creating massive space for the attackers to launch cyber attacks. To defend against these attacks, intrusion detection system (IDS) has been widely used. However, emerging CPS fields suffer from imbalanced and missing sample data, which makes the training of IDS difficult. In this paper, we propose a generative adversarial network (GAN) based intrusion detection system (G-IDS), where GAN generates synthetic samples, and IDS gets trained on them along with the original ones. G-IDS also fixes the difficulties of imbalanced or missing data problems. We model a network security dataset for an emerging CPS using NSL KDD-99 dataset and evaluate our proposed model's performance using different metrics. We find that our proposed G-IDS model performs much better in attack detection and model stabilization during the training process than a standalone IDS.",
      "intriguing_abstract": "The boundaries of cyber-physical systems (CPS) and the Internet of Things (IoT) are converging together day by day to introduce a common platform on hybrid systems. Moreover, the combination of artificial intelligence (AI) with CPS creates a new dimension of technological advancement. All these connectivity and dependability are creating massive space for the attackers to launch cyber attacks. To defend against these attacks, intrusion detection system (IDS) has been widely used. However, emerging CPS fields suffer from imbalanced and missing sample data, which makes the training of IDS difficult. In this paper, we propose a generative adversarial network (GAN) based intrusion detection system (G-IDS), where GAN generates synthetic samples, and IDS gets trained on them along with the original ones. G-IDS also fixes the difficulties of imbalanced or missing data problems. We model a network security dataset for an emerging CPS using NSL KDD-99 dataset and evaluate our proposed model's performance using different metrics. We find that our proposed G-IDS model performs much better in attack detection and model stabilization during the training process than a standalone IDS.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/82f766d3c572b4c690b439edab5d32b3ba72852e.pdf",
      "citation_key": "shahriar2020sm7",
      "metadata": {
        "title": "G-IDS: Generative Adversarial Networks Assisted Intrusion Detection System",
        "authors": [
          "Md Hasan Shahriar",
          "Nur Imtiazul Haque",
          "M. Rahman",
          "M. Alonso"
        ],
        "published_date": "2020",
        "abstract": "The boundaries of cyber-physical systems (CPS) and the Internet of Things (IoT) are converging together day by day to introduce a common platform on hybrid systems. Moreover, the combination of artificial intelligence (AI) with CPS creates a new dimension of technological advancement. All these connectivity and dependability are creating massive space for the attackers to launch cyber attacks. To defend against these attacks, intrusion detection system (IDS) has been widely used. However, emerging CPS fields suffer from imbalanced and missing sample data, which makes the training of IDS difficult. In this paper, we propose a generative adversarial network (GAN) based intrusion detection system (G-IDS), where GAN generates synthetic samples, and IDS gets trained on them along with the original ones. G-IDS also fixes the difficulties of imbalanced or missing data problems. We model a network security dataset for an emerging CPS using NSL KDD-99 dataset and evaluate our proposed model's performance using different metrics. We find that our proposed G-IDS model performs much better in attack detection and model stabilization during the training process than a standalone IDS.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/82f766d3c572b4c690b439edab5d32b3ba72852e.pdf",
        "venue": "Annual International Computer Software and Applications Conference",
        "citationCount": 105,
        "score": 21.0,
        "summary": "The boundaries of cyber-physical systems (CPS) and the Internet of Things (IoT) are converging together day by day to introduce a common platform on hybrid systems. Moreover, the combination of artificial intelligence (AI) with CPS creates a new dimension of technological advancement. All these connectivity and dependability are creating massive space for the attackers to launch cyber attacks. To defend against these attacks, intrusion detection system (IDS) has been widely used. However, emerging CPS fields suffer from imbalanced and missing sample data, which makes the training of IDS difficult. In this paper, we propose a generative adversarial network (GAN) based intrusion detection system (G-IDS), where GAN generates synthetic samples, and IDS gets trained on them along with the original ones. G-IDS also fixes the difficulties of imbalanced or missing data problems. We model a network security dataset for an emerging CPS using NSL KDD-99 dataset and evaluate our proposed model's performance using different metrics. We find that our proposed G-IDS model performs much better in attack detection and model stabilization during the training process than a standalone IDS.",
        "keywords": []
      },
      "file_name": "82f766d3c572b4c690b439edab5d32b3ba72852e.pdf"
    },
    {
      "success": true,
      "doc_id": "ca62f0d286eaaedddb1bb7536fb07a94",
      "summary": "Both generative adversarial networks (GAN) in unsupervised learning and actor-critic methods in reinforcement learning (RL) have gained a reputation for being difficult to optimize. Practitioners in both fields have amassed a large number of strategies to mitigate these instabilities and improve training. Here we show that GANs can be viewed as actor-critic methods in an environment where the actor cannot affect the reward. We review the strategies for stabilizing training for each class of models, both those that generalize between the two and those that are particular to that model. We also review a number of extensions to GANs and RL algorithms with even more complicated information flow. We hope that by highlighting this formal connection we will encourage both GAN and RL communities to develop general, scalable, and stable algorithms for multilevel optimization with deep networks, and to draw inspiration across communities.",
      "intriguing_abstract": "Both generative adversarial networks (GAN) in unsupervised learning and actor-critic methods in reinforcement learning (RL) have gained a reputation for being difficult to optimize. Practitioners in both fields have amassed a large number of strategies to mitigate these instabilities and improve training. Here we show that GANs can be viewed as actor-critic methods in an environment where the actor cannot affect the reward. We review the strategies for stabilizing training for each class of models, both those that generalize between the two and those that are particular to that model. We also review a number of extensions to GANs and RL algorithms with even more complicated information flow. We hope that by highlighting this formal connection we will encourage both GAN and RL communities to develop general, scalable, and stable algorithms for multilevel optimization with deep networks, and to draw inspiration across communities.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/29b8b97d554f5139fcf2064ce292204500eee31c.pdf",
      "citation_key": "pfau2016v7o",
      "metadata": {
        "title": "Connecting Generative Adversarial Networks and Actor-Critic Methods",
        "authors": [
          "David Pfau",
          "O. Vinyals"
        ],
        "published_date": "2016",
        "abstract": "Both generative adversarial networks (GAN) in unsupervised learning and actor-critic methods in reinforcement learning (RL) have gained a reputation for being difficult to optimize. Practitioners in both fields have amassed a large number of strategies to mitigate these instabilities and improve training. Here we show that GANs can be viewed as actor-critic methods in an environment where the actor cannot affect the reward. We review the strategies for stabilizing training for each class of models, both those that generalize between the two and those that are particular to that model. We also review a number of extensions to GANs and RL algorithms with even more complicated information flow. We hope that by highlighting this formal connection we will encourage both GAN and RL communities to develop general, scalable, and stable algorithms for multilevel optimization with deep networks, and to draw inspiration across communities.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/29b8b97d554f5139fcf2064ce292204500eee31c.pdf",
        "venue": "arXiv.org",
        "citationCount": 187,
        "score": 20.777777777777775,
        "summary": "Both generative adversarial networks (GAN) in unsupervised learning and actor-critic methods in reinforcement learning (RL) have gained a reputation for being difficult to optimize. Practitioners in both fields have amassed a large number of strategies to mitigate these instabilities and improve training. Here we show that GANs can be viewed as actor-critic methods in an environment where the actor cannot affect the reward. We review the strategies for stabilizing training for each class of models, both those that generalize between the two and those that are particular to that model. We also review a number of extensions to GANs and RL algorithms with even more complicated information flow. We hope that by highlighting this formal connection we will encourage both GAN and RL communities to develop general, scalable, and stable algorithms for multilevel optimization with deep networks, and to draw inspiration across communities.",
        "keywords": []
      },
      "file_name": "29b8b97d554f5139fcf2064ce292204500eee31c.pdf"
    },
    {
      "success": true,
      "doc_id": "9847b069d1a54616ac520c6e41a51069",
      "summary": "Unsupervised learning with generative adversarial networks (GANs) has proven to be hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss for both the discriminator and the generator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson <inline-formula><tex-math notation=\"LaTeX\">$\\chi ^2$</tex-math><alternatives><mml:math><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"mao-ieq1-2872043.gif\"/></alternatives></inline-formula> divergence. We also show that the derived objective function that yields minimizing the Pearson <inline-formula><tex-math notation=\"LaTeX\">$\\chi ^2$</tex-math><alternatives><mml:math><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"mao-ieq2-2872043.gif\"/></alternatives></inline-formula> divergence performs better than the classical one of using least squares for classification. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stably during the learning process. For evaluating the image quality, we conduct both qualitative and quantitative experiments, and the experimental results show that LSGANs can generate higher quality images than regular GANs. Furthermore, we evaluate the stability of LSGANs in two groups. One is to compare between LSGANs and regular GANs without gradient penalty. We conduct three experiments, including Gaussian mixture distribution, difficult architectures, and a newly proposed method — datasets with small variability, to illustrate the stability of LSGANs. The other one is to compare between LSGANs with gradient penalty (LSGANs-GP) and WGANs with gradient penalty (WGANs-GP). The experimental results show that LSGANs-GP succeed in training for all the difficult architectures used in WGANs-GP, including 101-layer ResNet.",
      "intriguing_abstract": "Unsupervised learning with generative adversarial networks (GANs) has proven to be hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss for both the discriminator and the generator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson <inline-formula><tex-math notation=\"LaTeX\">$\\chi ^2$</tex-math><alternatives><mml:math><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"mao-ieq1-2872043.gif\"/></alternatives></inline-formula> divergence. We also show that the derived objective function that yields minimizing the Pearson <inline-formula><tex-math notation=\"LaTeX\">$\\chi ^2$</tex-math><alternatives><mml:math><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"mao-ieq2-2872043.gif\"/></alternatives></inline-formula> divergence performs better than the classical one of using least squares for classification. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stably during the learning process. For evaluating the image quality, we conduct both qualitative and quantitative experiments, and the experimental results show that LSGANs can generate higher quality images than regular GANs. Furthermore, we evaluate the stability of LSGANs in two groups. One is to compare between LSGANs and regular GANs without gradient penalty. We conduct three experiments, including Gaussian mixture distribution, difficult architectures, and a newly proposed method — datasets with small variability, to illustrate the stability of LSGANs. The other one is to compare between LSGANs with gradient penalty (LSGANs-GP) and WGANs with gradient penalty (WGANs-GP). The experimental results show that LSGANs-GP succeed in training for all the difficult architectures used in WGANs-GP, including 101-layer ResNet.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/a4d0c0ca6445af0dc2e513c2e3ec5b852a0b5afa.pdf",
      "citation_key": "mao2017ss0",
      "metadata": {
        "title": "On the Effectiveness of Least Squares Generative Adversarial Networks",
        "authors": [
          "Xudong Mao",
          "Qing Li",
          "Haoran Xie",
          "Raymond Y. K. Lau",
          "Zhen Wang",
          "Stephen Paul Smolley"
        ],
        "published_date": "2017",
        "abstract": "Unsupervised learning with generative adversarial networks (GANs) has proven to be hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss for both the discriminator and the generator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson <inline-formula><tex-math notation=\"LaTeX\">$\\chi ^2$</tex-math><alternatives><mml:math><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"mao-ieq1-2872043.gif\"/></alternatives></inline-formula> divergence. We also show that the derived objective function that yields minimizing the Pearson <inline-formula><tex-math notation=\"LaTeX\">$\\chi ^2$</tex-math><alternatives><mml:math><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"mao-ieq2-2872043.gif\"/></alternatives></inline-formula> divergence performs better than the classical one of using least squares for classification. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stably during the learning process. For evaluating the image quality, we conduct both qualitative and quantitative experiments, and the experimental results show that LSGANs can generate higher quality images than regular GANs. Furthermore, we evaluate the stability of LSGANs in two groups. One is to compare between LSGANs and regular GANs without gradient penalty. We conduct three experiments, including Gaussian mixture distribution, difficult architectures, and a newly proposed method — datasets with small variability, to illustrate the stability of LSGANs. The other one is to compare between LSGANs with gradient penalty (LSGANs-GP) and WGANs with gradient penalty (WGANs-GP). The experimental results show that LSGANs-GP succeed in training for all the difficult architectures used in WGANs-GP, including 101-layer ResNet.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/a4d0c0ca6445af0dc2e513c2e3ec5b852a0b5afa.pdf",
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
        "citationCount": 162,
        "score": 20.25,
        "summary": "Unsupervised learning with generative adversarial networks (GANs) has proven to be hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss for both the discriminator and the generator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson <inline-formula><tex-math notation=\"LaTeX\">$\\chi ^2$</tex-math><alternatives><mml:math><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"mao-ieq1-2872043.gif\"/></alternatives></inline-formula> divergence. We also show that the derived objective function that yields minimizing the Pearson <inline-formula><tex-math notation=\"LaTeX\">$\\chi ^2$</tex-math><alternatives><mml:math><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math><inline-graphic xlink:href=\"mao-ieq2-2872043.gif\"/></alternatives></inline-formula> divergence performs better than the classical one of using least squares for classification. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stably during the learning process. For evaluating the image quality, we conduct both qualitative and quantitative experiments, and the experimental results show that LSGANs can generate higher quality images than regular GANs. Furthermore, we evaluate the stability of LSGANs in two groups. One is to compare between LSGANs and regular GANs without gradient penalty. We conduct three experiments, including Gaussian mixture distribution, difficult architectures, and a newly proposed method — datasets with small variability, to illustrate the stability of LSGANs. The other one is to compare between LSGANs with gradient penalty (LSGANs-GP) and WGANs with gradient penalty (WGANs-GP). The experimental results show that LSGANs-GP succeed in training for all the difficult architectures used in WGANs-GP, including 101-layer ResNet.",
        "keywords": []
      },
      "file_name": "a4d0c0ca6445af0dc2e513c2e3ec5b852a0b5afa.pdf"
    },
    {
      "success": true,
      "doc_id": "5dce80ce0f165237cca995780f0d76ba",
      "summary": "The smart grid employs computing and communication technologies to embed intelligence into the power grid and, consequently, make the grid more efficient. Machine learning (ML) has been applied for tasks that are important for smart grid operation including energy consumption and generation forecasting, anomaly detection, and state estimation. These ML solutions commonly require sufficient historical data; however, this data is often not readily available because of reasons such as data collection costs and concerns regarding security and privacy. This paper introduces a recurrent generative adversarial network (R-GAN) for generating realistic energy consumption data by learning from real data. Generativea adversarial networks (GANs) have been mostly used for image tasks (e.g., image generation, super-resolution), but here they are used with time series data. Convolutional neural networks (CNNs) from image GANs are replaced with recurrent neural networks (RNNs) because of RNN’s ability to capture temporal dependencies. To improve training stability and increase quality of generated data, Wasserstein GANs (WGANs) and Metropolis-Hastings GAN (MH-GAN) approaches were applied. The accuracy is further improved by adding features created with ARIMA and Fourier transform. Experiments demonstrate that data generated by R-GAN can be used for training energy forecasting models.",
      "intriguing_abstract": "The smart grid employs computing and communication technologies to embed intelligence into the power grid and, consequently, make the grid more efficient. Machine learning (ML) has been applied for tasks that are important for smart grid operation including energy consumption and generation forecasting, anomaly detection, and state estimation. These ML solutions commonly require sufficient historical data; however, this data is often not readily available because of reasons such as data collection costs and concerns regarding security and privacy. This paper introduces a recurrent generative adversarial network (R-GAN) for generating realistic energy consumption data by learning from real data. Generativea adversarial networks (GANs) have been mostly used for image tasks (e.g., image generation, super-resolution), but here they are used with time series data. Convolutional neural networks (CNNs) from image GANs are replaced with recurrent neural networks (RNNs) because of RNN’s ability to capture temporal dependencies. To improve training stability and increase quality of generated data, Wasserstein GANs (WGANs) and Metropolis-Hastings GAN (MH-GAN) approaches were applied. The accuracy is further improved by adding features created with ARIMA and Fourier transform. Experiments demonstrate that data generated by R-GAN can be used for training energy forecasting models.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/567a5d09647f787a37ce8ac300a221d8c4337688.pdf",
      "citation_key": "fekri2019c1i",
      "metadata": {
        "title": "Generating Energy Data for Machine Learning with Recurrent Generative Adversarial Networks",
        "authors": [
          "Mohammad Navid Fekri",
          "A. M. Ghosh",
          "Katarina Grolinger"
        ],
        "published_date": "2019",
        "abstract": "The smart grid employs computing and communication technologies to embed intelligence into the power grid and, consequently, make the grid more efficient. Machine learning (ML) has been applied for tasks that are important for smart grid operation including energy consumption and generation forecasting, anomaly detection, and state estimation. These ML solutions commonly require sufficient historical data; however, this data is often not readily available because of reasons such as data collection costs and concerns regarding security and privacy. This paper introduces a recurrent generative adversarial network (R-GAN) for generating realistic energy consumption data by learning from real data. Generativea adversarial networks (GANs) have been mostly used for image tasks (e.g., image generation, super-resolution), but here they are used with time series data. Convolutional neural networks (CNNs) from image GANs are replaced with recurrent neural networks (RNNs) because of RNN’s ability to capture temporal dependencies. To improve training stability and increase quality of generated data, Wasserstein GANs (WGANs) and Metropolis-Hastings GAN (MH-GAN) approaches were applied. The accuracy is further improved by adding features created with ARIMA and Fourier transform. Experiments demonstrate that data generated by R-GAN can be used for training energy forecasting models.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/567a5d09647f787a37ce8ac300a221d8c4337688.pdf",
        "venue": "Energies",
        "citationCount": 119,
        "score": 19.833333333333332,
        "summary": "The smart grid employs computing and communication technologies to embed intelligence into the power grid and, consequently, make the grid more efficient. Machine learning (ML) has been applied for tasks that are important for smart grid operation including energy consumption and generation forecasting, anomaly detection, and state estimation. These ML solutions commonly require sufficient historical data; however, this data is often not readily available because of reasons such as data collection costs and concerns regarding security and privacy. This paper introduces a recurrent generative adversarial network (R-GAN) for generating realistic energy consumption data by learning from real data. Generativea adversarial networks (GANs) have been mostly used for image tasks (e.g., image generation, super-resolution), but here they are used with time series data. Convolutional neural networks (CNNs) from image GANs are replaced with recurrent neural networks (RNNs) because of RNN’s ability to capture temporal dependencies. To improve training stability and increase quality of generated data, Wasserstein GANs (WGANs) and Metropolis-Hastings GAN (MH-GAN) approaches were applied. The accuracy is further improved by adding features created with ARIMA and Fourier transform. Experiments demonstrate that data generated by R-GAN can be used for training energy forecasting models.",
        "keywords": []
      },
      "file_name": "567a5d09647f787a37ce8ac300a221d8c4337688.pdf"
    },
    {
      "success": true,
      "doc_id": "2ae5af291bcd3bddb020d17f3465287c",
      "summary": "Style transfer describes the rendering of an image’s semantic content as different artistic styles. Recently, generative adversarial networks (GANs) have emerged as an effective approach in style transfer by adversarially training the generator to synthesize convincing counterfeits. However, traditional GAN suffers from the mode collapse issue, resulting in unstable training and making style transfer quality difficult to guarantee. In addition, the GAN generator is only compatible with one style, so a series of GANs must be trained to provide users with choices to transfer more than one kind of style. In this paper, we focus on tackling these challenges and limitations to improve style transfer. We propose adversarial gated networks (Gated-GAN) to transfer multiple styles in a single model. The generative networks have three modules: an encoder, a gated transformer, and a decoder. Different styles can be achieved by passing input images through different branches of the gated transformer. To stabilize training, the encoder and decoder are combined as an auto-encoder to reconstruct the input images. The discriminative networks are used to distinguish whether the input image is a stylized or genuine image. An auxiliary classifier is used to recognize the style categories of transferred images, thereby helping the generative networks generate images in multiple styles. In addition, Gated-GAN makes it possible to explore a new style by investigating styles learned from artists or genres. Our extensive experiments demonstrate the stability and effectiveness of the proposed model for multi-style transfer.",
      "intriguing_abstract": "Style transfer describes the rendering of an image’s semantic content as different artistic styles. Recently, generative adversarial networks (GANs) have emerged as an effective approach in style transfer by adversarially training the generator to synthesize convincing counterfeits. However, traditional GAN suffers from the mode collapse issue, resulting in unstable training and making style transfer quality difficult to guarantee. In addition, the GAN generator is only compatible with one style, so a series of GANs must be trained to provide users with choices to transfer more than one kind of style. In this paper, we focus on tackling these challenges and limitations to improve style transfer. We propose adversarial gated networks (Gated-GAN) to transfer multiple styles in a single model. The generative networks have three modules: an encoder, a gated transformer, and a decoder. Different styles can be achieved by passing input images through different branches of the gated transformer. To stabilize training, the encoder and decoder are combined as an auto-encoder to reconstruct the input images. The discriminative networks are used to distinguish whether the input image is a stylized or genuine image. An auxiliary classifier is used to recognize the style categories of transferred images, thereby helping the generative networks generate images in multiple styles. In addition, Gated-GAN makes it possible to explore a new style by investigating styles learned from artists or genres. Our extensive experiments demonstrate the stability and effectiveness of the proposed model for multi-style transfer.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/35dc2337a7f871c93b733432ae84635dffd366aa.pdf",
      "citation_key": "chen2019ng2",
      "metadata": {
        "title": "Gated-GAN: Adversarial Gated Networks for Multi-Collection Style Transfer",
        "authors": [
          "Xinyuan Chen",
          "Chang Xu",
          "Xiaokang Yang",
          "Li Song",
          "D. Tao"
        ],
        "published_date": "2019",
        "abstract": "Style transfer describes the rendering of an image’s semantic content as different artistic styles. Recently, generative adversarial networks (GANs) have emerged as an effective approach in style transfer by adversarially training the generator to synthesize convincing counterfeits. However, traditional GAN suffers from the mode collapse issue, resulting in unstable training and making style transfer quality difficult to guarantee. In addition, the GAN generator is only compatible with one style, so a series of GANs must be trained to provide users with choices to transfer more than one kind of style. In this paper, we focus on tackling these challenges and limitations to improve style transfer. We propose adversarial gated networks (Gated-GAN) to transfer multiple styles in a single model. The generative networks have three modules: an encoder, a gated transformer, and a decoder. Different styles can be achieved by passing input images through different branches of the gated transformer. To stabilize training, the encoder and decoder are combined as an auto-encoder to reconstruct the input images. The discriminative networks are used to distinguish whether the input image is a stylized or genuine image. An auxiliary classifier is used to recognize the style categories of transferred images, thereby helping the generative networks generate images in multiple styles. In addition, Gated-GAN makes it possible to explore a new style by investigating styles learned from artists or genres. Our extensive experiments demonstrate the stability and effectiveness of the proposed model for multi-style transfer.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/35dc2337a7f871c93b733432ae84635dffd366aa.pdf",
        "venue": "IEEE Transactions on Image Processing",
        "citationCount": 118,
        "score": 19.666666666666664,
        "summary": "Style transfer describes the rendering of an image’s semantic content as different artistic styles. Recently, generative adversarial networks (GANs) have emerged as an effective approach in style transfer by adversarially training the generator to synthesize convincing counterfeits. However, traditional GAN suffers from the mode collapse issue, resulting in unstable training and making style transfer quality difficult to guarantee. In addition, the GAN generator is only compatible with one style, so a series of GANs must be trained to provide users with choices to transfer more than one kind of style. In this paper, we focus on tackling these challenges and limitations to improve style transfer. We propose adversarial gated networks (Gated-GAN) to transfer multiple styles in a single model. The generative networks have three modules: an encoder, a gated transformer, and a decoder. Different styles can be achieved by passing input images through different branches of the gated transformer. To stabilize training, the encoder and decoder are combined as an auto-encoder to reconstruct the input images. The discriminative networks are used to distinguish whether the input image is a stylized or genuine image. An auxiliary classifier is used to recognize the style categories of transferred images, thereby helping the generative networks generate images in multiple styles. In addition, Gated-GAN makes it possible to explore a new style by investigating styles learned from artists or genres. Our extensive experiments demonstrate the stability and effectiveness of the proposed model for multi-style transfer.",
        "keywords": []
      },
      "file_name": "35dc2337a7f871c93b733432ae84635dffd366aa.pdf"
    },
    {
      "success": true,
      "doc_id": "5c1deae4887107f1d1abb757cae122ae",
      "summary": "Popular neural network-based speech enhancement systems operate on the magnitude spectrogram and ignore the phase mismatch between the noisy and clean speech signals. Recently, conditional generative adversarial networks (cGANs) have shown promise in addressing the phase mismatch problem by directly mapping the raw noisy speech waveform to the underlying clean speech signal. However, stabilizing and training cGAN systems is difficult and they still fall short of the performance achieved by spectral enhancement approaches. This paper introduces relativistic GANs with a relativistic cost function at its discriminator and gradient penalty to improve time-domain speech enhancement. Simulation results show that relativistic discriminators provide a more stable training of cGANs and yield a better generator network for improved speech enhancement performance.",
      "intriguing_abstract": "Popular neural network-based speech enhancement systems operate on the magnitude spectrogram and ignore the phase mismatch between the noisy and clean speech signals. Recently, conditional generative adversarial networks (cGANs) have shown promise in addressing the phase mismatch problem by directly mapping the raw noisy speech waveform to the underlying clean speech signal. However, stabilizing and training cGAN systems is difficult and they still fall short of the performance achieved by spectral enhancement approaches. This paper introduces relativistic GANs with a relativistic cost function at its discriminator and gradient penalty to improve time-domain speech enhancement. Simulation results show that relativistic discriminators provide a more stable training of cGANs and yield a better generator network for improved speech enhancement performance.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/02c6dbdd1d492726f1fd70a9211f668a794e6975.pdf",
      "citation_key": "baby2019h4h",
      "metadata": {
        "title": "Sergan: Speech Enhancement Using Relativistic Generative Adversarial Networks with Gradient Penalty",
        "authors": [
          "Deepak Baby",
          "S. Verhulst"
        ],
        "published_date": "2019",
        "abstract": "Popular neural network-based speech enhancement systems operate on the magnitude spectrogram and ignore the phase mismatch between the noisy and clean speech signals. Recently, conditional generative adversarial networks (cGANs) have shown promise in addressing the phase mismatch problem by directly mapping the raw noisy speech waveform to the underlying clean speech signal. However, stabilizing and training cGAN systems is difficult and they still fall short of the performance achieved by spectral enhancement approaches. This paper introduces relativistic GANs with a relativistic cost function at its discriminator and gradient penalty to improve time-domain speech enhancement. Simulation results show that relativistic discriminators provide a more stable training of cGANs and yield a better generator network for improved speech enhancement performance.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/02c6dbdd1d492726f1fd70a9211f668a794e6975.pdf",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "citationCount": 109,
        "score": 18.166666666666664,
        "summary": "Popular neural network-based speech enhancement systems operate on the magnitude spectrogram and ignore the phase mismatch between the noisy and clean speech signals. Recently, conditional generative adversarial networks (cGANs) have shown promise in addressing the phase mismatch problem by directly mapping the raw noisy speech waveform to the underlying clean speech signal. However, stabilizing and training cGAN systems is difficult and they still fall short of the performance achieved by spectral enhancement approaches. This paper introduces relativistic GANs with a relativistic cost function at its discriminator and gradient penalty to improve time-domain speech enhancement. Simulation results show that relativistic discriminators provide a more stable training of cGANs and yield a better generator network for improved speech enhancement performance.",
        "keywords": []
      },
      "file_name": "02c6dbdd1d492726f1fd70a9211f668a794e6975.pdf"
    },
    {
      "success": true,
      "doc_id": "9e3e7020b2724a7580fc429aff33ad74",
      "summary": "Generative Adversarial Networks (GANs) are a type of generative model which have received much attention due to their ability to model complex real-world data. Despite their recent successes, the process of training GANs remains challenging, suffering from instability problems such as non-convergence, vanishing or exploding gradients, and mode collapse. In recent years, a diverse set of approaches have been proposed which focus on stabilizing the GAN training procedure. The purpose of this survey is to provide a comprehensive overview of the GAN training stabilization methods which can be found in the literature. We discuss the advantages and disadvantages of each approach, offer a comparative summary, and conclude with a discussion of open problems.",
      "intriguing_abstract": "Generative Adversarial Networks (GANs) are a type of generative model which have received much attention due to their ability to model complex real-world data. Despite their recent successes, the process of training GANs remains challenging, suffering from instability problems such as non-convergence, vanishing or exploding gradients, and mode collapse. In recent years, a diverse set of approaches have been proposed which focus on stabilizing the GAN training procedure. The purpose of this survey is to provide a comprehensive overview of the GAN training stabilization methods which can be found in the literature. We discuss the advantages and disadvantages of each approach, offer a comparative summary, and conclude with a discussion of open problems.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/13fd8d61a6ea97c70f5154a23611c80203527818.pdf",
      "citation_key": "wiatrak20194ib",
      "metadata": {
        "title": "Stabilizing Generative Adversarial Networks: A Survey",
        "authors": [
          "Maciej Wiatrak",
          "Stefano V. Albrecht",
          "A. Nystrom"
        ],
        "published_date": "2019",
        "abstract": "Generative Adversarial Networks (GANs) are a type of generative model which have received much attention due to their ability to model complex real-world data. Despite their recent successes, the process of training GANs remains challenging, suffering from instability problems such as non-convergence, vanishing or exploding gradients, and mode collapse. In recent years, a diverse set of approaches have been proposed which focus on stabilizing the GAN training procedure. The purpose of this survey is to provide a comprehensive overview of the GAN training stabilization methods which can be found in the literature. We discuss the advantages and disadvantages of each approach, offer a comparative summary, and conclude with a discussion of open problems.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/13fd8d61a6ea97c70f5154a23611c80203527818.pdf",
        "venue": "",
        "citationCount": 93,
        "score": 15.5,
        "summary": "Generative Adversarial Networks (GANs) are a type of generative model which have received much attention due to their ability to model complex real-world data. Despite their recent successes, the process of training GANs remains challenging, suffering from instability problems such as non-convergence, vanishing or exploding gradients, and mode collapse. In recent years, a diverse set of approaches have been proposed which focus on stabilizing the GAN training procedure. The purpose of this survey is to provide a comprehensive overview of the GAN training stabilization methods which can be found in the literature. We discuss the advantages and disadvantages of each approach, offer a comparative summary, and conclude with a discussion of open problems.",
        "keywords": []
      },
      "file_name": "13fd8d61a6ea97c70f5154a23611c80203527818.pdf"
    },
    {
      "success": true,
      "doc_id": "9845523867af41acc2e41a82de781a78",
      "summary": "Many generative models synthesize data by transforming a standard Gaussian random variable using a deterministic neural network. Among these models are the Variational Autoencoders and the Generative Adversarial Networks. In this work, we call them\"push-forward\"models and study their expressivity. We show that the Lipschitz constant of these generative networks has to be large in order to fit multimodal distributions. More precisely, we show that the total variation distance and the Kullback-Leibler divergence between the generated and the data distribution are bounded from below by a constant depending on the mode separation and the Lipschitz constant. Since constraining the Lipschitz constants of neural networks is a common way to stabilize generative models, there is a provable trade-off between the ability of push-forward models to approximate multimodal distributions and the stability of their training. We validate our findings on one-dimensional and image datasets and empirically show that generative models consisting of stacked networks with stochastic input at each step, such as diffusion models do not suffer of such limitations.",
      "intriguing_abstract": "Many generative models synthesize data by transforming a standard Gaussian random variable using a deterministic neural network. Among these models are the Variational Autoencoders and the Generative Adversarial Networks. In this work, we call them\"push-forward\"models and study their expressivity. We show that the Lipschitz constant of these generative networks has to be large in order to fit multimodal distributions. More precisely, we show that the total variation distance and the Kullback-Leibler divergence between the generated and the data distribution are bounded from below by a constant depending on the mode separation and the Lipschitz constant. Since constraining the Lipschitz constants of neural networks is a common way to stabilize generative models, there is a provable trade-off between the ability of push-forward models to approximate multimodal distributions and the stability of their training. We validate our findings on one-dimensional and image datasets and empirically show that generative models consisting of stacked networks with stochastic input at each step, such as diffusion models do not suffer of such limitations.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/1e3194bf2bdc22a5d1750579a3d2553b61aa4045.pdf",
      "citation_key": "salmona202283g",
      "metadata": {
        "title": "Can Push-forward Generative Models Fit Multimodal Distributions?",
        "authors": [
          "Antoine Salmona",
          "Valentin De Bortoli",
          "J. Delon",
          "A. Desolneux"
        ],
        "published_date": "2022",
        "abstract": "Many generative models synthesize data by transforming a standard Gaussian random variable using a deterministic neural network. Among these models are the Variational Autoencoders and the Generative Adversarial Networks. In this work, we call them\"push-forward\"models and study their expressivity. We show that the Lipschitz constant of these generative networks has to be large in order to fit multimodal distributions. More precisely, we show that the total variation distance and the Kullback-Leibler divergence between the generated and the data distribution are bounded from below by a constant depending on the mode separation and the Lipschitz constant. Since constraining the Lipschitz constants of neural networks is a common way to stabilize generative models, there is a provable trade-off between the ability of push-forward models to approximate multimodal distributions and the stability of their training. We validate our findings on one-dimensional and image datasets and empirically show that generative models consisting of stacked networks with stochastic input at each step, such as diffusion models do not suffer of such limitations.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/1e3194bf2bdc22a5d1750579a3d2553b61aa4045.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 42,
        "score": 14.0,
        "summary": "Many generative models synthesize data by transforming a standard Gaussian random variable using a deterministic neural network. Among these models are the Variational Autoencoders and the Generative Adversarial Networks. In this work, we call them\"push-forward\"models and study their expressivity. We show that the Lipschitz constant of these generative networks has to be large in order to fit multimodal distributions. More precisely, we show that the total variation distance and the Kullback-Leibler divergence between the generated and the data distribution are bounded from below by a constant depending on the mode separation and the Lipschitz constant. Since constraining the Lipschitz constants of neural networks is a common way to stabilize generative models, there is a provable trade-off between the ability of push-forward models to approximate multimodal distributions and the stability of their training. We validate our findings on one-dimensional and image datasets and empirically show that generative models consisting of stacked networks with stochastic input at each step, such as diffusion models do not suffer of such limitations.",
        "keywords": []
      },
      "file_name": "1e3194bf2bdc22a5d1750579a3d2553b61aa4045.pdf"
    },
    {
      "success": true,
      "doc_id": "049e1dac0f03275ec7b90cf0e4ee3080",
      "summary": "While Geerative Adversarial Networks (GANs) are fundamental to many generative modelling applications, they suffer from numerous issues. In this work, we propose a principled framework to simultaneously mitigate two fundamental issues in GANs: catastrophic forgetting of the discriminator and mode collapse of the generator. We achieve this by employing for GANs a contrastive learning and mutual information maximization approach, and perform extensive analyses to understand sources of improvements. Our approach significantly stabilizes GAN training and improves GAN performance for image synthesis across five datasets under the same training and evaluation conditions against state-of-the-art works. In particular, compared to the state-of-the-art SSGAN, our approach does not suffer from poorer performance on image domains such as faces, and instead improves performance significantly. Our approach is simple to implement and practical: it involves only one auxiliary objective, has low computational cost, and performs robustly across a wide range of training settings and datasets without any hyperparameter tuning. For reproducibility, our code is available in the open-source GAN library, Mimicry [34].",
      "intriguing_abstract": "While Geerative Adversarial Networks (GANs) are fundamental to many generative modelling applications, they suffer from numerous issues. In this work, we propose a principled framework to simultaneously mitigate two fundamental issues in GANs: catastrophic forgetting of the discriminator and mode collapse of the generator. We achieve this by employing for GANs a contrastive learning and mutual information maximization approach, and perform extensive analyses to understand sources of improvements. Our approach significantly stabilizes GAN training and improves GAN performance for image synthesis across five datasets under the same training and evaluation conditions against state-of-the-art works. In particular, compared to the state-of-the-art SSGAN, our approach does not suffer from poorer performance on image domains such as faces, and instead improves performance significantly. Our approach is simple to implement and practical: it involves only one auxiliary objective, has low computational cost, and performs robustly across a wide range of training settings and datasets without any hyperparameter tuning. For reproducibility, our code is available in the open-source GAN library, Mimicry [34].",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/e32e9735b387080492cbf08f85c5e93fcef95b3a.pdf",
      "citation_key": "lee20205ue",
      "metadata": {
        "title": "InfoMax-GAN: Improved Adversarial Image Generation via Information Maximization and Contrastive Learning",
        "authors": [
          "Kwot Sin Lee",
          "Ngoc-Trung Tran",
          "Ngai-Man Cheung"
        ],
        "published_date": "2020",
        "abstract": "While Geerative Adversarial Networks (GANs) are fundamental to many generative modelling applications, they suffer from numerous issues. In this work, we propose a principled framework to simultaneously mitigate two fundamental issues in GANs: catastrophic forgetting of the discriminator and mode collapse of the generator. We achieve this by employing for GANs a contrastive learning and mutual information maximization approach, and perform extensive analyses to understand sources of improvements. Our approach significantly stabilizes GAN training and improves GAN performance for image synthesis across five datasets under the same training and evaluation conditions against state-of-the-art works. In particular, compared to the state-of-the-art SSGAN, our approach does not suffer from poorer performance on image domains such as faces, and instead improves performance significantly. Our approach is simple to implement and practical: it involves only one auxiliary objective, has low computational cost, and performs robustly across a wide range of training settings and datasets without any hyperparameter tuning. For reproducibility, our code is available in the open-source GAN library, Mimicry [34].",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/e32e9735b387080492cbf08f85c5e93fcef95b3a.pdf",
        "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
        "citationCount": 69,
        "score": 13.8,
        "summary": "While Geerative Adversarial Networks (GANs) are fundamental to many generative modelling applications, they suffer from numerous issues. In this work, we propose a principled framework to simultaneously mitigate two fundamental issues in GANs: catastrophic forgetting of the discriminator and mode collapse of the generator. We achieve this by employing for GANs a contrastive learning and mutual information maximization approach, and perform extensive analyses to understand sources of improvements. Our approach significantly stabilizes GAN training and improves GAN performance for image synthesis across five datasets under the same training and evaluation conditions against state-of-the-art works. In particular, compared to the state-of-the-art SSGAN, our approach does not suffer from poorer performance on image domains such as faces, and instead improves performance significantly. Our approach is simple to implement and practical: it involves only one auxiliary objective, has low computational cost, and performs robustly across a wide range of training settings and datasets without any hyperparameter tuning. For reproducibility, our code is available in the open-source GAN library, Mimicry [34].",
        "keywords": []
      },
      "file_name": "e32e9735b387080492cbf08f85c5e93fcef95b3a.pdf"
    },
    {
      "success": true,
      "doc_id": "00963edafbac97eb306345883077e5cb",
      "summary": "Generative adversarial networks (GANs) are a machine learning framework comprising a generative model for sampling from a target distribution and a discriminative model for evaluating the proximity of a sample to the target distribution. GANs exhibit strong performance in imaging or anomaly detection. However, they suffer from training instabilities, and sampling efficiency may be limited by the classical sampling procedure. We introduce variational quantum–classical Wasserstein GANs (WGANs) to address these issues and embed this model in a classical machine learning framework for anomaly detection. Classical WGANs improve training stability by using a cost function better suited for gradient descent. Our model replaces the generator of WGANs with a hybrid quantum–classical neural net and leaves the classical discriminative model unchanged. This way, high-dimensional classical data only enters the classical model and need not be prepared in a quantum circuit. We demonstrate the effectiveness of this method on a credit card fraud dataset. For this dataset our method shows performance on par with classical methods in terms of the F 1 score. We analyze the influence of the circuit ansatz, layer width and depth, neural net architecture parameter initialization strategy, and sampling noise on convergence and performance.",
      "intriguing_abstract": "Generative adversarial networks (GANs) are a machine learning framework comprising a generative model for sampling from a target distribution and a discriminative model for evaluating the proximity of a sample to the target distribution. GANs exhibit strong performance in imaging or anomaly detection. However, they suffer from training instabilities, and sampling efficiency may be limited by the classical sampling procedure. We introduce variational quantum–classical Wasserstein GANs (WGANs) to address these issues and embed this model in a classical machine learning framework for anomaly detection. Classical WGANs improve training stability by using a cost function better suited for gradient descent. Our model replaces the generator of WGANs with a hybrid quantum–classical neural net and leaves the classical discriminative model unchanged. This way, high-dimensional classical data only enters the classical model and need not be prepared in a quantum circuit. We demonstrate the effectiveness of this method on a credit card fraud dataset. For this dataset our method shows performance on par with classical methods in terms of the F 1 score. We analyze the influence of the circuit ansatz, layer width and depth, neural net architecture parameter initialization strategy, and sampling noise on convergence and performance.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/0d2818f1070fa5a6cf5d14a87a5d71b4107b4d60.pdf",
      "citation_key": "herr20208x4",
      "metadata": {
        "title": "Anomaly detection with variational quantum generative adversarial networks",
        "authors": [
          "Daniel Herr",
          "B. Obert",
          "Matthias Rosenkranz"
        ],
        "published_date": "2020",
        "abstract": "Generative adversarial networks (GANs) are a machine learning framework comprising a generative model for sampling from a target distribution and a discriminative model for evaluating the proximity of a sample to the target distribution. GANs exhibit strong performance in imaging or anomaly detection. However, they suffer from training instabilities, and sampling efficiency may be limited by the classical sampling procedure. We introduce variational quantum–classical Wasserstein GANs (WGANs) to address these issues and embed this model in a classical machine learning framework for anomaly detection. Classical WGANs improve training stability by using a cost function better suited for gradient descent. Our model replaces the generator of WGANs with a hybrid quantum–classical neural net and leaves the classical discriminative model unchanged. This way, high-dimensional classical data only enters the classical model and need not be prepared in a quantum circuit. We demonstrate the effectiveness of this method on a credit card fraud dataset. For this dataset our method shows performance on par with classical methods in terms of the F 1 score. We analyze the influence of the circuit ansatz, layer width and depth, neural net architecture parameter initialization strategy, and sampling noise on convergence and performance.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/0d2818f1070fa5a6cf5d14a87a5d71b4107b4d60.pdf",
        "venue": "Quantum Science and Technology",
        "citationCount": 66,
        "score": 13.200000000000001,
        "summary": "Generative adversarial networks (GANs) are a machine learning framework comprising a generative model for sampling from a target distribution and a discriminative model for evaluating the proximity of a sample to the target distribution. GANs exhibit strong performance in imaging or anomaly detection. However, they suffer from training instabilities, and sampling efficiency may be limited by the classical sampling procedure. We introduce variational quantum–classical Wasserstein GANs (WGANs) to address these issues and embed this model in a classical machine learning framework for anomaly detection. Classical WGANs improve training stability by using a cost function better suited for gradient descent. Our model replaces the generator of WGANs with a hybrid quantum–classical neural net and leaves the classical discriminative model unchanged. This way, high-dimensional classical data only enters the classical model and need not be prepared in a quantum circuit. We demonstrate the effectiveness of this method on a credit card fraud dataset. For this dataset our method shows performance on par with classical methods in terms of the F 1 score. We analyze the influence of the circuit ansatz, layer width and depth, neural net architecture parameter initialization strategy, and sampling noise on convergence and performance.",
        "keywords": []
      },
      "file_name": "0d2818f1070fa5a6cf5d14a87a5d71b4107b4d60.pdf"
    },
    {
      "success": true,
      "doc_id": "c909ab01ccd4dfec13ae6b46da562590",
      "summary": "Generative models are increasingly used to artificially generate various kinds of data, including high-quality images and videos. These models are used to estimate the underlying distribution of a dataset and randomly generate realistic samples according to their estimated distribution. However, the data used to train these models is often sensitive, thus prompting the need to evaluate information leakage from producing synthetic samples with generative models---specifically, whether an adversary can infer information about the data used to train the models. In this paper, we present the first membership inference attack on generative models. To mount the attack, we train a Generative Adversarial Network (GAN), which combines a discriminative and a generative model, to detect overfitting and recognize inputs that are part of training datasets by relying on the discriminator's capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, and show how to improve the latter using limited auxiliary knowledge of dataset samples. We test our attacks on several state-of-the-art models, such as Deep Convolutional GAN (DCGAN), Boundary Equilibrium GAN (BEGAN), and the combination of DCGAN with a Variational Autoencoder (DCGAN+VAE), using datasets consisting of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). The white-box attacks are 100% successful at inferring which samples were used to train the target model, and the black-box ones succeeds with 80% accuracy. Finally, we discuss the sensitivity of our attacks to different training parameters, and their robustness against mitigation strategies, finding that successful defenses often result in significant worse performances of the generative models in terms of training stability and/or sample quality.",
      "intriguing_abstract": "Generative models are increasingly used to artificially generate various kinds of data, including high-quality images and videos. These models are used to estimate the underlying distribution of a dataset and randomly generate realistic samples according to their estimated distribution. However, the data used to train these models is often sensitive, thus prompting the need to evaluate information leakage from producing synthetic samples with generative models---specifically, whether an adversary can infer information about the data used to train the models. In this paper, we present the first membership inference attack on generative models. To mount the attack, we train a Generative Adversarial Network (GAN), which combines a discriminative and a generative model, to detect overfitting and recognize inputs that are part of training datasets by relying on the discriminator's capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, and show how to improve the latter using limited auxiliary knowledge of dataset samples. We test our attacks on several state-of-the-art models, such as Deep Convolutional GAN (DCGAN), Boundary Equilibrium GAN (BEGAN), and the combination of DCGAN with a Variational Autoencoder (DCGAN+VAE), using datasets consisting of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). The white-box attacks are 100% successful at inferring which samples were used to train the target model, and the black-box ones succeeds with 80% accuracy. Finally, we discuss the sensitivity of our attacks to different training parameters, and their robustness against mitigation strategies, finding that successful defenses often result in significant worse performances of the generative models in terms of training stability and/or sample quality.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/7c4f52328c2869bdff8034d2867baa5b67d0ce27.pdf",
      "citation_key": "hayes201742g",
      "metadata": {
        "title": "LOGAN: Evaluating Privacy Leakage of Generative Models Using Generative Adversarial Networks",
        "authors": [
          "Jamie Hayes",
          "Luca Melis",
          "G. Danezis",
          "Emiliano De Cristofaro"
        ],
        "published_date": "2017",
        "abstract": "Generative models are increasingly used to artificially generate various kinds of data, including high-quality images and videos. These models are used to estimate the underlying distribution of a dataset and randomly generate realistic samples according to their estimated distribution. However, the data used to train these models is often sensitive, thus prompting the need to evaluate information leakage from producing synthetic samples with generative models---specifically, whether an adversary can infer information about the data used to train the models. In this paper, we present the first membership inference attack on generative models. To mount the attack, we train a Generative Adversarial Network (GAN), which combines a discriminative and a generative model, to detect overfitting and recognize inputs that are part of training datasets by relying on the discriminator's capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, and show how to improve the latter using limited auxiliary knowledge of dataset samples. We test our attacks on several state-of-the-art models, such as Deep Convolutional GAN (DCGAN), Boundary Equilibrium GAN (BEGAN), and the combination of DCGAN with a Variational Autoencoder (DCGAN+VAE), using datasets consisting of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). The white-box attacks are 100% successful at inferring which samples were used to train the target model, and the black-box ones succeeds with 80% accuracy. Finally, we discuss the sensitivity of our attacks to different training parameters, and their robustness against mitigation strategies, finding that successful defenses often result in significant worse performances of the generative models in terms of training stability and/or sample quality.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/7c4f52328c2869bdff8034d2867baa5b67d0ce27.pdf",
        "venue": "arXiv.org",
        "citationCount": 104,
        "score": 13.0,
        "summary": "Generative models are increasingly used to artificially generate various kinds of data, including high-quality images and videos. These models are used to estimate the underlying distribution of a dataset and randomly generate realistic samples according to their estimated distribution. However, the data used to train these models is often sensitive, thus prompting the need to evaluate information leakage from producing synthetic samples with generative models---specifically, whether an adversary can infer information about the data used to train the models. In this paper, we present the first membership inference attack on generative models. To mount the attack, we train a Generative Adversarial Network (GAN), which combines a discriminative and a generative model, to detect overfitting and recognize inputs that are part of training datasets by relying on the discriminator's capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, and show how to improve the latter using limited auxiliary knowledge of dataset samples. We test our attacks on several state-of-the-art models, such as Deep Convolutional GAN (DCGAN), Boundary Equilibrium GAN (BEGAN), and the combination of DCGAN with a Variational Autoencoder (DCGAN+VAE), using datasets consisting of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). The white-box attacks are 100% successful at inferring which samples were used to train the target model, and the black-box ones succeeds with 80% accuracy. Finally, we discuss the sensitivity of our attacks to different training parameters, and their robustness against mitigation strategies, finding that successful defenses often result in significant worse performances of the generative models in terms of training stability and/or sample quality.",
        "keywords": []
      },
      "file_name": "7c4f52328c2869bdff8034d2867baa5b67d0ce27.pdf"
    },
    {
      "success": true,
      "doc_id": "7992a931c899c0e2c250f62f42c8620d",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/cc34aea01322a8fb289e3d4486aad0f6641b472e.pdf",
      "citation_key": "negi20208n9",
      "metadata": {
        "title": "RDA-UNET-WGAN: An Accurate Breast Ultrasound Lesion Segmentation Using Wasserstein Generative Adversarial Networks",
        "authors": [
          "Anuja Negi",
          "A. Noel",
          "Joseph Raj",
          "Ruban Nersisson",
          "Zhemin Zhuang",
          "·. M. Murugappan"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/cc34aea01322a8fb289e3d4486aad0f6641b472e.pdf",
        "venue": "The Arabian journal for science and engineering",
        "citationCount": 64,
        "score": 12.8,
        "summary": "",
        "keywords": []
      },
      "file_name": "cc34aea01322a8fb289e3d4486aad0f6641b472e.pdf"
    },
    {
      "success": true,
      "doc_id": "c9c431b85f163e7c504e07053dd2e1aa",
      "summary": "Intelligent diagnosis is one of the key points of research in the field of bearing fault diagnosis. As a representative unsupervised data expansion method, generative adversarial networks (GANs) may solve the problem of insufficient samples in rotating machinery fault diagnosis. In this article, a new method based on an improved auxiliary classification generative adversarial network (ACGAN) is proposed. First, using the ACGAN framework to add fault label information to the input sample improves the quality of the generated sample. The Wasserstein distance is introduced into the loss function in ACGAN, which effectively solves the problems of gradient collapse and gradient disappearance. Second, the weight penalty is added to the loss function, which increases the expression ability of the network compared with the weight clipping and effectively solves the problem of unstable network training. Finally, an independent fault diagnosis classifier is introduced to balance the compatibility of discrimination and classification. The convolutional block attention module (CBAM) based on the attention mechanism is introduced to fuse its output with the features extracted by the convolution module, which improves the representation ability of the fault diagnosis network and realizes high-precision fault diagnosis. The method is applied to the public bearing dataset of Western Reserve University and the bearing simulation dataset of the Yanshan University Laboratory, which can generate high-quality multimode fault samples more efficiently and can be used to help the training of fault diagnosis models based on deep learning. It has high accuracy, generalization, and stability.",
      "intriguing_abstract": "Intelligent diagnosis is one of the key points of research in the field of bearing fault diagnosis. As a representative unsupervised data expansion method, generative adversarial networks (GANs) may solve the problem of insufficient samples in rotating machinery fault diagnosis. In this article, a new method based on an improved auxiliary classification generative adversarial network (ACGAN) is proposed. First, using the ACGAN framework to add fault label information to the input sample improves the quality of the generated sample. The Wasserstein distance is introduced into the loss function in ACGAN, which effectively solves the problems of gradient collapse and gradient disappearance. Second, the weight penalty is added to the loss function, which increases the expression ability of the network compared with the weight clipping and effectively solves the problem of unstable network training. Finally, an independent fault diagnosis classifier is introduced to balance the compatibility of discrimination and classification. The convolutional block attention module (CBAM) based on the attention mechanism is introduced to fuse its output with the features extracted by the convolution module, which improves the representation ability of the fault diagnosis network and realizes high-precision fault diagnosis. The method is applied to the public bearing dataset of Western Reserve University and the bearing simulation dataset of the Yanshan University Laboratory, which can generate high-quality multimode fault samples more efficiently and can be used to help the training of fault diagnosis models based on deep learning. It has high accuracy, generalization, and stability.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/ae9ef65d5551defa3fd8b3e5cd06e4fe959c0d1f.pdf",
      "citation_key": "meng2022you",
      "metadata": {
        "title": "An Intelligent Fault Diagnosis Method of Small Sample Bearing Based on Improved Auxiliary Classification Generative Adversarial Network",
        "authors": [
          "Zong Meng",
          "Qian Li",
          "De-gang Sun",
          "W. Cao",
          "Fengjie Fan"
        ],
        "published_date": "2022",
        "abstract": "Intelligent diagnosis is one of the key points of research in the field of bearing fault diagnosis. As a representative unsupervised data expansion method, generative adversarial networks (GANs) may solve the problem of insufficient samples in rotating machinery fault diagnosis. In this article, a new method based on an improved auxiliary classification generative adversarial network (ACGAN) is proposed. First, using the ACGAN framework to add fault label information to the input sample improves the quality of the generated sample. The Wasserstein distance is introduced into the loss function in ACGAN, which effectively solves the problems of gradient collapse and gradient disappearance. Second, the weight penalty is added to the loss function, which increases the expression ability of the network compared with the weight clipping and effectively solves the problem of unstable network training. Finally, an independent fault diagnosis classifier is introduced to balance the compatibility of discrimination and classification. The convolutional block attention module (CBAM) based on the attention mechanism is introduced to fuse its output with the features extracted by the convolution module, which improves the representation ability of the fault diagnosis network and realizes high-precision fault diagnosis. The method is applied to the public bearing dataset of Western Reserve University and the bearing simulation dataset of the Yanshan University Laboratory, which can generate high-quality multimode fault samples more efficiently and can be used to help the training of fault diagnosis models based on deep learning. It has high accuracy, generalization, and stability.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/ae9ef65d5551defa3fd8b3e5cd06e4fe959c0d1f.pdf",
        "venue": "IEEE Sensors Journal",
        "citationCount": 38,
        "score": 12.666666666666666,
        "summary": "Intelligent diagnosis is one of the key points of research in the field of bearing fault diagnosis. As a representative unsupervised data expansion method, generative adversarial networks (GANs) may solve the problem of insufficient samples in rotating machinery fault diagnosis. In this article, a new method based on an improved auxiliary classification generative adversarial network (ACGAN) is proposed. First, using the ACGAN framework to add fault label information to the input sample improves the quality of the generated sample. The Wasserstein distance is introduced into the loss function in ACGAN, which effectively solves the problems of gradient collapse and gradient disappearance. Second, the weight penalty is added to the loss function, which increases the expression ability of the network compared with the weight clipping and effectively solves the problem of unstable network training. Finally, an independent fault diagnosis classifier is introduced to balance the compatibility of discrimination and classification. The convolutional block attention module (CBAM) based on the attention mechanism is introduced to fuse its output with the features extracted by the convolution module, which improves the representation ability of the fault diagnosis network and realizes high-precision fault diagnosis. The method is applied to the public bearing dataset of Western Reserve University and the bearing simulation dataset of the Yanshan University Laboratory, which can generate high-quality multimode fault samples more efficiently and can be used to help the training of fault diagnosis models based on deep learning. It has high accuracy, generalization, and stability.",
        "keywords": []
      },
      "file_name": "ae9ef65d5551defa3fd8b3e5cd06e4fe959c0d1f.pdf"
    },
    {
      "success": true,
      "doc_id": "e0d965af3f6e8e853f946dbf6b081f4f",
      "summary": "Generative Adversarial Network (GAN) and its variants serve as a perfect representation of the data generation model, providing researchers with a large amount of high-quality generated data. They illustrate a promising direction for research with limited data availability. When GAN learns the semantic-rich data distribution from a dataset, the density of the generated distribution tends to concentrate on the training data. Due to the gradient parameters of the deep neural network contain the data distribution of the training samples, they can easily remember the training samples. When GAN is applied to private or sensitive data, for instance, patient medical records, as private information may be leakage. To address this issue, we propose a Privacy-preserving Generative Adversarial Network (PPGAN) model, in which we achieve differential privacy in GANs by adding well-designed noise to the gradient during the model learning procedure. Besides, we introduced the Moments Accountant strategy in the PPGAN training process to improve the stability and compatibility of the model by controlling privacy loss. We also give a mathematical proof of the differential privacy discriminator. Through extensive case studies of the benchmark datasets, we demonstrate that PPGAN can generate high-quality synthetic data while retaining the required data available under a reasonable privacy budget.",
      "intriguing_abstract": "Generative Adversarial Network (GAN) and its variants serve as a perfect representation of the data generation model, providing researchers with a large amount of high-quality generated data. They illustrate a promising direction for research with limited data availability. When GAN learns the semantic-rich data distribution from a dataset, the density of the generated distribution tends to concentrate on the training data. Due to the gradient parameters of the deep neural network contain the data distribution of the training samples, they can easily remember the training samples. When GAN is applied to private or sensitive data, for instance, patient medical records, as private information may be leakage. To address this issue, we propose a Privacy-preserving Generative Adversarial Network (PPGAN) model, in which we achieve differential privacy in GANs by adding well-designed noise to the gradient during the model learning procedure. Besides, we introduced the Moments Accountant strategy in the PPGAN training process to improve the stability and compatibility of the model by controlling privacy loss. We also give a mathematical proof of the differential privacy discriminator. Through extensive case studies of the benchmark datasets, we demonstrate that PPGAN can generate high-quality synthetic data while retaining the required data available under a reasonable privacy budget.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/a3c97d6439f4436700124e6f7ca7170917a99d49.pdf",
      "citation_key": "liu2019sb7",
      "metadata": {
        "title": "PPGAN: Privacy-Preserving Generative Adversarial Network",
        "authors": [
          "Yi Liu",
          "Jialiang Peng",
          "James J. Q. Yu",
          "Yi Wu"
        ],
        "published_date": "2019",
        "abstract": "Generative Adversarial Network (GAN) and its variants serve as a perfect representation of the data generation model, providing researchers with a large amount of high-quality generated data. They illustrate a promising direction for research with limited data availability. When GAN learns the semantic-rich data distribution from a dataset, the density of the generated distribution tends to concentrate on the training data. Due to the gradient parameters of the deep neural network contain the data distribution of the training samples, they can easily remember the training samples. When GAN is applied to private or sensitive data, for instance, patient medical records, as private information may be leakage. To address this issue, we propose a Privacy-preserving Generative Adversarial Network (PPGAN) model, in which we achieve differential privacy in GANs by adding well-designed noise to the gradient during the model learning procedure. Besides, we introduced the Moments Accountant strategy in the PPGAN training process to improve the stability and compatibility of the model by controlling privacy loss. We also give a mathematical proof of the differential privacy discriminator. Through extensive case studies of the benchmark datasets, we demonstrate that PPGAN can generate high-quality synthetic data while retaining the required data available under a reasonable privacy budget.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/a3c97d6439f4436700124e6f7ca7170917a99d49.pdf",
        "venue": "International Conference on Parallel and Distributed Systems",
        "citationCount": 73,
        "score": 12.166666666666666,
        "summary": "Generative Adversarial Network (GAN) and its variants serve as a perfect representation of the data generation model, providing researchers with a large amount of high-quality generated data. They illustrate a promising direction for research with limited data availability. When GAN learns the semantic-rich data distribution from a dataset, the density of the generated distribution tends to concentrate on the training data. Due to the gradient parameters of the deep neural network contain the data distribution of the training samples, they can easily remember the training samples. When GAN is applied to private or sensitive data, for instance, patient medical records, as private information may be leakage. To address this issue, we propose a Privacy-preserving Generative Adversarial Network (PPGAN) model, in which we achieve differential privacy in GANs by adding well-designed noise to the gradient during the model learning procedure. Besides, we introduced the Moments Accountant strategy in the PPGAN training process to improve the stability and compatibility of the model by controlling privacy loss. We also give a mathematical proof of the differential privacy discriminator. Through extensive case studies of the benchmark datasets, we demonstrate that PPGAN can generate high-quality synthetic data while retaining the required data available under a reasonable privacy budget.",
        "keywords": []
      },
      "file_name": "a3c97d6439f4436700124e6f7ca7170917a99d49.pdf"
    },
    {
      "success": true,
      "doc_id": "9fecba043f640888e7627f07251881a4",
      "summary": "Research on undersampled magnetic resonance image (MRI) reconstruction can increase the speed of MRI imaging and reduce patient suffering. In this paper, an undersampled MRI reconstruction method based on Generative Adversarial Networks with the Self-Attention mechanism and the Relative Average discriminator (SARA-GAN) is proposed. In our SARA-GAN, the relative average discriminator theory is applied to make full use of the prior knowledge, in which half of the input data of the discriminator is true and half is fake. At the same time, a self-attention mechanism is incorporated into the high-layer of the generator to build long-range dependence of the image, which can overcome the problem of limited convolution kernel size. Besides, spectral normalization is employed to stabilize the training process. Compared with three widely used GAN-based MRI reconstruction methods, i.e., DAGAN, DAWGAN, and DAWGAN-GP, the proposed method can obtain a higher peak signal-to-noise ratio (PSNR) and structural similarity index measure(SSIM), and the details of the reconstructed image are more abundant and more realistic for further clinical scrutinization and diagnostic tasks.",
      "intriguing_abstract": "Research on undersampled magnetic resonance image (MRI) reconstruction can increase the speed of MRI imaging and reduce patient suffering. In this paper, an undersampled MRI reconstruction method based on Generative Adversarial Networks with the Self-Attention mechanism and the Relative Average discriminator (SARA-GAN) is proposed. In our SARA-GAN, the relative average discriminator theory is applied to make full use of the prior knowledge, in which half of the input data of the discriminator is true and half is fake. At the same time, a self-attention mechanism is incorporated into the high-layer of the generator to build long-range dependence of the image, which can overcome the problem of limited convolution kernel size. Besides, spectral normalization is employed to stabilize the training process. Compared with three widely used GAN-based MRI reconstruction methods, i.e., DAGAN, DAWGAN, and DAWGAN-GP, the proposed method can obtain a higher peak signal-to-noise ratio (PSNR) and structural similarity index measure(SSIM), and the details of the reconstructed image are more abundant and more realistic for further clinical scrutinization and diagnostic tasks.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/f644c34fb75f191614f9c69d3a196b5ff5f6a7d6.pdf",
      "citation_key": "yuan2020bt6",
      "metadata": {
        "title": "SARA-GAN: Self-Attention and Relative Average Discriminator Based Generative Adversarial Networks for Fast Compressed Sensing MRI Reconstruction",
        "authors": [
          "Zhenmou Yuan",
          "M. Jiang",
          "Yaming Wang",
          "Bo Wei",
          "Yongming Li",
          "Pin Wang",
          "Wade Menpes-Smith",
          "Z. Niu",
          "Guang Yang"
        ],
        "published_date": "2020",
        "abstract": "Research on undersampled magnetic resonance image (MRI) reconstruction can increase the speed of MRI imaging and reduce patient suffering. In this paper, an undersampled MRI reconstruction method based on Generative Adversarial Networks with the Self-Attention mechanism and the Relative Average discriminator (SARA-GAN) is proposed. In our SARA-GAN, the relative average discriminator theory is applied to make full use of the prior knowledge, in which half of the input data of the discriminator is true and half is fake. At the same time, a self-attention mechanism is incorporated into the high-layer of the generator to build long-range dependence of the image, which can overcome the problem of limited convolution kernel size. Besides, spectral normalization is employed to stabilize the training process. Compared with three widely used GAN-based MRI reconstruction methods, i.e., DAGAN, DAWGAN, and DAWGAN-GP, the proposed method can obtain a higher peak signal-to-noise ratio (PSNR) and structural similarity index measure(SSIM), and the details of the reconstructed image are more abundant and more realistic for further clinical scrutinization and diagnostic tasks.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/f644c34fb75f191614f9c69d3a196b5ff5f6a7d6.pdf",
        "venue": "Frontiers in Neuroinformatics",
        "citationCount": 60,
        "score": 12.0,
        "summary": "Research on undersampled magnetic resonance image (MRI) reconstruction can increase the speed of MRI imaging and reduce patient suffering. In this paper, an undersampled MRI reconstruction method based on Generative Adversarial Networks with the Self-Attention mechanism and the Relative Average discriminator (SARA-GAN) is proposed. In our SARA-GAN, the relative average discriminator theory is applied to make full use of the prior knowledge, in which half of the input data of the discriminator is true and half is fake. At the same time, a self-attention mechanism is incorporated into the high-layer of the generator to build long-range dependence of the image, which can overcome the problem of limited convolution kernel size. Besides, spectral normalization is employed to stabilize the training process. Compared with three widely used GAN-based MRI reconstruction methods, i.e., DAGAN, DAWGAN, and DAWGAN-GP, the proposed method can obtain a higher peak signal-to-noise ratio (PSNR) and structural similarity index measure(SSIM), and the details of the reconstructed image are more abundant and more realistic for further clinical scrutinization and diagnostic tasks.",
        "keywords": []
      },
      "file_name": "f644c34fb75f191614f9c69d3a196b5ff5f6a7d6.pdf"
    },
    {
      "success": true,
      "doc_id": "f1dfd2e81fb572b68d1763d11d2303ba",
      "summary": "We deal with the problem of few-shot class incremental learning (FSCIL), which requires a model to continuously recognize new categories for which limited training data are available. Existing FSCIL methods depend on prior knowledge to regularize the model parameters for combating catastrophic forgetting. Devising an effective prior in a low-data regime, however, is not trivial. The memory-replay based approaches from the fully-supervised class incremental learning (CIL) literature cannot be used directly for FSCIL as the generative memory-replay modules of CIL are hard to train from few training samples. However, generative replay can tackle both the stability and plasticity of the models simultaneously by generating a large number of class-conditional samples. Convinced by this fact, we propose a generative modeling-based FSCIL framework using the paradigm of memory-replay in which a novel conditional few-shot generative adversarial network (GAN) is incrementally trained to produce visual features while ensuring the stability-plasticity trade-off through novel loss functions and combating the mode-collapse problem effectively. Furthermore, the class-specific synthesized visual features from the few-shot GAN are constrained to match the respective latent semantic prototypes obtained from a well-defined semantic space. We find that the advantages of this semantic restriction is two-fold, in dealing with forgetting, while making the features class-discernible. The model requires a single per-class prototype vector to be maintained in a dynamic memory buffer. Experimental results on the benchmark and large-scale CiFAR-100, CUB-200, and Mini-ImageNet confirm the superiority of our model over the current FSCIL state of the art.",
      "intriguing_abstract": "We deal with the problem of few-shot class incremental learning (FSCIL), which requires a model to continuously recognize new categories for which limited training data are available. Existing FSCIL methods depend on prior knowledge to regularize the model parameters for combating catastrophic forgetting. Devising an effective prior in a low-data regime, however, is not trivial. The memory-replay based approaches from the fully-supervised class incremental learning (CIL) literature cannot be used directly for FSCIL as the generative memory-replay modules of CIL are hard to train from few training samples. However, generative replay can tackle both the stability and plasticity of the models simultaneously by generating a large number of class-conditional samples. Convinced by this fact, we propose a generative modeling-based FSCIL framework using the paradigm of memory-replay in which a novel conditional few-shot generative adversarial network (GAN) is incrementally trained to produce visual features while ensuring the stability-plasticity trade-off through novel loss functions and combating the mode-collapse problem effectively. Furthermore, the class-specific synthesized visual features from the few-shot GAN are constrained to match the respective latent semantic prototypes obtained from a well-defined semantic space. We find that the advantages of this semantic restriction is two-fold, in dealing with forgetting, while making the features class-discernible. The model requires a single per-class prototype vector to be maintained in a dynamic memory buffer. Experimental results on the benchmark and large-scale CiFAR-100, CUB-200, and Mini-ImageNet confirm the superiority of our model over the current FSCIL state of the art.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/aaf7fc55e3ddd07c89dee19ee878c76b7f03041a.pdf",
      "citation_key": "agarwal2022p6d",
      "metadata": {
        "title": "Semantics-Driven Generative Replay for Few-Shot Class Incremental Learning",
        "authors": [
          "Aishwarya Agarwal",
          "Biplab Banerjee",
          "Fabio Cuzzolin",
          "S. Chaudhuri"
        ],
        "published_date": "2022",
        "abstract": "We deal with the problem of few-shot class incremental learning (FSCIL), which requires a model to continuously recognize new categories for which limited training data are available. Existing FSCIL methods depend on prior knowledge to regularize the model parameters for combating catastrophic forgetting. Devising an effective prior in a low-data regime, however, is not trivial. The memory-replay based approaches from the fully-supervised class incremental learning (CIL) literature cannot be used directly for FSCIL as the generative memory-replay modules of CIL are hard to train from few training samples. However, generative replay can tackle both the stability and plasticity of the models simultaneously by generating a large number of class-conditional samples. Convinced by this fact, we propose a generative modeling-based FSCIL framework using the paradigm of memory-replay in which a novel conditional few-shot generative adversarial network (GAN) is incrementally trained to produce visual features while ensuring the stability-plasticity trade-off through novel loss functions and combating the mode-collapse problem effectively. Furthermore, the class-specific synthesized visual features from the few-shot GAN are constrained to match the respective latent semantic prototypes obtained from a well-defined semantic space. We find that the advantages of this semantic restriction is two-fold, in dealing with forgetting, while making the features class-discernible. The model requires a single per-class prototype vector to be maintained in a dynamic memory buffer. Experimental results on the benchmark and large-scale CiFAR-100, CUB-200, and Mini-ImageNet confirm the superiority of our model over the current FSCIL state of the art.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/aaf7fc55e3ddd07c89dee19ee878c76b7f03041a.pdf",
        "venue": "ACM Multimedia",
        "citationCount": 35,
        "score": 11.666666666666666,
        "summary": "We deal with the problem of few-shot class incremental learning (FSCIL), which requires a model to continuously recognize new categories for which limited training data are available. Existing FSCIL methods depend on prior knowledge to regularize the model parameters for combating catastrophic forgetting. Devising an effective prior in a low-data regime, however, is not trivial. The memory-replay based approaches from the fully-supervised class incremental learning (CIL) literature cannot be used directly for FSCIL as the generative memory-replay modules of CIL are hard to train from few training samples. However, generative replay can tackle both the stability and plasticity of the models simultaneously by generating a large number of class-conditional samples. Convinced by this fact, we propose a generative modeling-based FSCIL framework using the paradigm of memory-replay in which a novel conditional few-shot generative adversarial network (GAN) is incrementally trained to produce visual features while ensuring the stability-plasticity trade-off through novel loss functions and combating the mode-collapse problem effectively. Furthermore, the class-specific synthesized visual features from the few-shot GAN are constrained to match the respective latent semantic prototypes obtained from a well-defined semantic space. We find that the advantages of this semantic restriction is two-fold, in dealing with forgetting, while making the features class-discernible. The model requires a single per-class prototype vector to be maintained in a dynamic memory buffer. Experimental results on the benchmark and large-scale CiFAR-100, CUB-200, and Mini-ImageNet confirm the superiority of our model over the current FSCIL state of the art.",
        "keywords": []
      },
      "file_name": "aaf7fc55e3ddd07c89dee19ee878c76b7f03041a.pdf"
    },
    {
      "success": true,
      "doc_id": "80722262bdaf7043856b40750342a2a4",
      "summary": "We consider the problem of training generative models with a Generative Adversarial Network (GAN). Although GANs can accurately model complex distributions, they are known to be difficult to train due to instabilities caused by a difficult minimax optimization problem. In this paper, we view the problem of training GANs as finding a mixed strategy in a zero-sum game. Building on ideas from online learning we propose a novel training method named Chekhov GAN 1 . On the theory side, we show that our method provably converges to an equilibrium for semi-shallow GAN architectures, i.e. architectures where the discriminator is a one layer network and the generator is arbitrary. On the practical side, we develop an efficient heuristic guided by our theoretical results, which we apply to commonly used deep GAN architectures. On several real world tasks our approach exhibits improved stability and performance compared to standard GAN training.",
      "intriguing_abstract": "We consider the problem of training generative models with a Generative Adversarial Network (GAN). Although GANs can accurately model complex distributions, they are known to be difficult to train due to instabilities caused by a difficult minimax optimization problem. In this paper, we view the problem of training GANs as finding a mixed strategy in a zero-sum game. Building on ideas from online learning we propose a novel training method named Chekhov GAN 1 . On the theory side, we show that our method provably converges to an equilibrium for semi-shallow GAN architectures, i.e. architectures where the discriminator is a one layer network and the generator is arbitrary. On the practical side, we develop an efficient heuristic guided by our theoretical results, which we apply to commonly used deep GAN architectures. On several real world tasks our approach exhibits improved stability and performance compared to standard GAN training.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/245f8b05bdd1ac65a09a476440dc4b05ac05d4a0.pdf",
      "citation_key": "grnarova20171tc",
      "metadata": {
        "title": "An Online Learning Approach to Generative Adversarial Networks",
        "authors": [
          "Paulina Grnarova",
          "K. Levy",
          "Aurélien Lucchi",
          "Thomas Hofmann",
          "Andreas Krause"
        ],
        "published_date": "2017",
        "abstract": "We consider the problem of training generative models with a Generative Adversarial Network (GAN). Although GANs can accurately model complex distributions, they are known to be difficult to train due to instabilities caused by a difficult minimax optimization problem. In this paper, we view the problem of training GANs as finding a mixed strategy in a zero-sum game. Building on ideas from online learning we propose a novel training method named Chekhov GAN 1 . On the theory side, we show that our method provably converges to an equilibrium for semi-shallow GAN architectures, i.e. architectures where the discriminator is a one layer network and the generator is arbitrary. On the practical side, we develop an efficient heuristic guided by our theoretical results, which we apply to commonly used deep GAN architectures. On several real world tasks our approach exhibits improved stability and performance compared to standard GAN training.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/245f8b05bdd1ac65a09a476440dc4b05ac05d4a0.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 92,
        "score": 11.5,
        "summary": "We consider the problem of training generative models with a Generative Adversarial Network (GAN). Although GANs can accurately model complex distributions, they are known to be difficult to train due to instabilities caused by a difficult minimax optimization problem. In this paper, we view the problem of training GANs as finding a mixed strategy in a zero-sum game. Building on ideas from online learning we propose a novel training method named Chekhov GAN 1 . On the theory side, we show that our method provably converges to an equilibrium for semi-shallow GAN architectures, i.e. architectures where the discriminator is a one layer network and the generator is arbitrary. On the practical side, we develop an efficient heuristic guided by our theoretical results, which we apply to commonly used deep GAN architectures. On several real world tasks our approach exhibits improved stability and performance compared to standard GAN training.",
        "keywords": []
      },
      "file_name": "245f8b05bdd1ac65a09a476440dc4b05ac05d4a0.pdf"
    },
    {
      "success": true,
      "doc_id": "cb79061352c72949747530ebba72650a",
      "summary": "Generating multiple categories of texts is a challenging task and draws more and more attention. Since generative adversarial nets (GANs) have shown competitive results on general text generation, they are extended for category text generation in some previous works. However, the complicated model structures and learning strategies limit their performance and exacerbate the training instability. This paper proposes a category-aware GAN (CatGAN) which consists of an efficient category-aware model for category text generation and a hierarchical evolutionary learning algorithm for training our model. The category-aware model directly measures the gap between real samples and generated samples on each category, then reducing this gap will guide the model to generate high-quality category samples. The Gumbel-Softmax relaxation further frees our model from complicated learning strategies for updating CatGAN on discrete data. Moreover, only focusing on the sample quality normally leads the mode collapse problem, thus a hierarchical evolutionary learning algorithm is introduced to stabilize the training procedure and obtain the trade-off between quality and diversity while training CatGAN. Experimental results demonstrate that CatGAN outperforms most of the existing state-of-the-art methods.",
      "intriguing_abstract": "Generating multiple categories of texts is a challenging task and draws more and more attention. Since generative adversarial nets (GANs) have shown competitive results on general text generation, they are extended for category text generation in some previous works. However, the complicated model structures and learning strategies limit their performance and exacerbate the training instability. This paper proposes a category-aware GAN (CatGAN) which consists of an efficient category-aware model for category text generation and a hierarchical evolutionary learning algorithm for training our model. The category-aware model directly measures the gap between real samples and generated samples on each category, then reducing this gap will guide the model to generate high-quality category samples. The Gumbel-Softmax relaxation further frees our model from complicated learning strategies for updating CatGAN on discrete data. Moreover, only focusing on the sample quality normally leads the mode collapse problem, thus a hierarchical evolutionary learning algorithm is introduced to stabilize the training procedure and obtain the trade-off between quality and diversity while training CatGAN. Experimental results demonstrate that CatGAN outperforms most of the existing state-of-the-art methods.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/a0cca4fe677af57d1a8491d698c0d709535c44dd.pdf",
      "citation_key": "liu2019oc8",
      "metadata": {
        "title": "CatGAN: Category-aware Generative Adversarial Networks with Hierarchical Evolutionary Learning for Category Text Generation",
        "authors": [
          "Zhiyue Liu",
          "Jiahai Wang",
          "Zhiwei Liang"
        ],
        "published_date": "2019",
        "abstract": "Generating multiple categories of texts is a challenging task and draws more and more attention. Since generative adversarial nets (GANs) have shown competitive results on general text generation, they are extended for category text generation in some previous works. However, the complicated model structures and learning strategies limit their performance and exacerbate the training instability. This paper proposes a category-aware GAN (CatGAN) which consists of an efficient category-aware model for category text generation and a hierarchical evolutionary learning algorithm for training our model. The category-aware model directly measures the gap between real samples and generated samples on each category, then reducing this gap will guide the model to generate high-quality category samples. The Gumbel-Softmax relaxation further frees our model from complicated learning strategies for updating CatGAN on discrete data. Moreover, only focusing on the sample quality normally leads the mode collapse problem, thus a hierarchical evolutionary learning algorithm is introduced to stabilize the training procedure and obtain the trade-off between quality and diversity while training CatGAN. Experimental results demonstrate that CatGAN outperforms most of the existing state-of-the-art methods.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/a0cca4fe677af57d1a8491d698c0d709535c44dd.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 68,
        "score": 11.333333333333332,
        "summary": "Generating multiple categories of texts is a challenging task and draws more and more attention. Since generative adversarial nets (GANs) have shown competitive results on general text generation, they are extended for category text generation in some previous works. However, the complicated model structures and learning strategies limit their performance and exacerbate the training instability. This paper proposes a category-aware GAN (CatGAN) which consists of an efficient category-aware model for category text generation and a hierarchical evolutionary learning algorithm for training our model. The category-aware model directly measures the gap between real samples and generated samples on each category, then reducing this gap will guide the model to generate high-quality category samples. The Gumbel-Softmax relaxation further frees our model from complicated learning strategies for updating CatGAN on discrete data. Moreover, only focusing on the sample quality normally leads the mode collapse problem, thus a hierarchical evolutionary learning algorithm is introduced to stabilize the training procedure and obtain the trade-off between quality and diversity while training CatGAN. Experimental results demonstrate that CatGAN outperforms most of the existing state-of-the-art methods.",
        "keywords": []
      },
      "file_name": "a0cca4fe677af57d1a8491d698c0d709535c44dd.pdf"
    },
    {
      "success": true,
      "doc_id": "f30beae5792949321b79ae7e680657fa",
      "summary": "Supervised classification methods have been widely utilized for the quality assurance of the advanced manufacturing process, such as additive manufacturing (AM) for anomaly (defects) detection. However, since abnormal states (with defects) occur much less frequently than normal ones (without defects) in a manufacturing process, the number of sensor data samples collected from a normal state is usually much more than that from an abnormal state. This issue causes imbalanced training data for classification analysis, thus deteriorating the performance of detecting abnormal states in the process. It is beneficial to generate effective artificial sample data for the abnormal states to make a more balanced training set. To achieve this goal, this paper proposes a novel data augmentation method based on a generative adversarial network (GAN) using additive manufacturing process image sensor data. The novelty of our approach is that a standard GAN and classifier are jointly optimized with techniques to stabilize the learning process of standard GAN. The diverse and high-quality generated samples provide balanced training data to the classifier. The iterative optimization between GAN and classifier provides the high-performance classifier. The effectiveness of the proposed method is validated by both open-source data and real-world case studies in polymer and metal AM processes.",
      "intriguing_abstract": "Supervised classification methods have been widely utilized for the quality assurance of the advanced manufacturing process, such as additive manufacturing (AM) for anomaly (defects) detection. However, since abnormal states (with defects) occur much less frequently than normal ones (without defects) in a manufacturing process, the number of sensor data samples collected from a normal state is usually much more than that from an abnormal state. This issue causes imbalanced training data for classification analysis, thus deteriorating the performance of detecting abnormal states in the process. It is beneficial to generate effective artificial sample data for the abnormal states to make a more balanced training set. To achieve this goal, this paper proposes a novel data augmentation method based on a generative adversarial network (GAN) using additive manufacturing process image sensor data. The novelty of our approach is that a standard GAN and classifier are jointly optimized with techniques to stabilize the learning process of standard GAN. The diverse and high-quality generated samples provide balanced training data to the classifier. The iterative optimization between GAN and classifier provides the high-performance classifier. The effectiveness of the proposed method is validated by both open-source data and real-world case studies in polymer and metal AM processes.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/e8d7db40cd4f507973c51b17ddd62a15ff861a9e.pdf",
      "citation_key": "chung2022s9a",
      "metadata": {
        "title": "Anomaly detection in additive manufacturing processes using supervised classification with imbalanced sensor data based on generative adversarial network",
        "authors": [
          "Jihoon Chung",
          "Bo Shen",
          "Zhen Kong"
        ],
        "published_date": "2022",
        "abstract": "Supervised classification methods have been widely utilized for the quality assurance of the advanced manufacturing process, such as additive manufacturing (AM) for anomaly (defects) detection. However, since abnormal states (with defects) occur much less frequently than normal ones (without defects) in a manufacturing process, the number of sensor data samples collected from a normal state is usually much more than that from an abnormal state. This issue causes imbalanced training data for classification analysis, thus deteriorating the performance of detecting abnormal states in the process. It is beneficial to generate effective artificial sample data for the abnormal states to make a more balanced training set. To achieve this goal, this paper proposes a novel data augmentation method based on a generative adversarial network (GAN) using additive manufacturing process image sensor data. The novelty of our approach is that a standard GAN and classifier are jointly optimized with techniques to stabilize the learning process of standard GAN. The diverse and high-quality generated samples provide balanced training data to the classifier. The iterative optimization between GAN and classifier provides the high-performance classifier. The effectiveness of the proposed method is validated by both open-source data and real-world case studies in polymer and metal AM processes.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/e8d7db40cd4f507973c51b17ddd62a15ff861a9e.pdf",
        "venue": "Journal of Intelligent Manufacturing",
        "citationCount": 34,
        "score": 11.333333333333332,
        "summary": "Supervised classification methods have been widely utilized for the quality assurance of the advanced manufacturing process, such as additive manufacturing (AM) for anomaly (defects) detection. However, since abnormal states (with defects) occur much less frequently than normal ones (without defects) in a manufacturing process, the number of sensor data samples collected from a normal state is usually much more than that from an abnormal state. This issue causes imbalanced training data for classification analysis, thus deteriorating the performance of detecting abnormal states in the process. It is beneficial to generate effective artificial sample data for the abnormal states to make a more balanced training set. To achieve this goal, this paper proposes a novel data augmentation method based on a generative adversarial network (GAN) using additive manufacturing process image sensor data. The novelty of our approach is that a standard GAN and classifier are jointly optimized with techniques to stabilize the learning process of standard GAN. The diverse and high-quality generated samples provide balanced training data to the classifier. The iterative optimization between GAN and classifier provides the high-performance classifier. The effectiveness of the proposed method is validated by both open-source data and real-world case studies in polymer and metal AM processes.",
        "keywords": []
      },
      "file_name": "e8d7db40cd4f507973c51b17ddd62a15ff861a9e.pdf"
    },
    {
      "success": true,
      "doc_id": "683eafd0b8ad89ecbbb04b56fd9443c7",
      "summary": "Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various types of GANs. In particular, we derive conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture. We find that existing GAN variants satisfy some, but not all, of these conditions. Using tools from convex analysis, optimal transport, and reproducing kernels, we construct a GAN that fulfills these conditions simultaneously. In the process, we explain and clarify the need for various existing GAN stabilization techniques, including Lipschitz constraints, gradient penalties, and smooth activation functions.",
      "intriguing_abstract": "Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various types of GANs. In particular, we derive conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture. We find that existing GAN variants satisfy some, but not all, of these conditions. Using tools from convex analysis, optimal transport, and reproducing kernels, we construct a GAN that fulfills these conditions simultaneously. In the process, we explain and clarify the need for various existing GAN stabilization techniques, including Lipschitz constraints, gradient penalties, and smooth activation functions.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/531836a1c3fbbf10eba5375d8558f218cdb9805e.pdf",
      "citation_key": "chu2020zbv",
      "metadata": {
        "title": "Smoothness and Stability in GANs",
        "authors": [
          "Casey Chu",
          "Kentaro Minami",
          "K. Fukumizu"
        ],
        "published_date": "2020",
        "abstract": "Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various types of GANs. In particular, we derive conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture. We find that existing GAN variants satisfy some, but not all, of these conditions. Using tools from convex analysis, optimal transport, and reproducing kernels, we construct a GAN that fulfills these conditions simultaneously. In the process, we explain and clarify the need for various existing GAN stabilization techniques, including Lipschitz constraints, gradient penalties, and smooth activation functions.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/531836a1c3fbbf10eba5375d8558f218cdb9805e.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 56,
        "score": 11.200000000000001,
        "summary": "Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various types of GANs. In particular, we derive conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture. We find that existing GAN variants satisfy some, but not all, of these conditions. Using tools from convex analysis, optimal transport, and reproducing kernels, we construct a GAN that fulfills these conditions simultaneously. In the process, we explain and clarify the need for various existing GAN stabilization techniques, including Lipschitz constraints, gradient penalties, and smooth activation functions.",
        "keywords": []
      },
      "file_name": "531836a1c3fbbf10eba5375d8558f218cdb9805e.pdf"
    },
    {
      "success": true,
      "doc_id": "325a7bfb2cf3a9a0fee257710c37fb0b",
      "summary": "We present a novel method and analysis to train generative adversarial networks (GAN) in a stable manner. As shown in recent analysis, training is often undermined by the probability distribution of the data being zero on neighborhoods of the data space. We notice that the distributions of real and generated data should match even when they undergo the same filtering. Therefore, to address the limited support problem we propose to train GANs by using different filtered versions of the real and generated data distributions. In this way, filtering does not prevent the exact matching of the data distribution, while helping training by extending the support of both distributions. As filtering we consider adding samples from an arbitrary distribution to the data, which corresponds to a convolution of the data distribution with the arbitrary one. We also propose to learn the generation of these samples so as to challenge the discriminator in the adversarial training. We show that our approach results in a stable and well-behaved training of even the original minimax GAN formulation. Moreover, our technique can be incorporated in most modern GAN formulations and leads to a consistent improvement on several common datasets.",
      "intriguing_abstract": "We present a novel method and analysis to train generative adversarial networks (GAN) in a stable manner. As shown in recent analysis, training is often undermined by the probability distribution of the data being zero on neighborhoods of the data space. We notice that the distributions of real and generated data should match even when they undergo the same filtering. Therefore, to address the limited support problem we propose to train GANs by using different filtered versions of the real and generated data distributions. In this way, filtering does not prevent the exact matching of the data distribution, while helping training by extending the support of both distributions. As filtering we consider adding samples from an arbitrary distribution to the data, which corresponds to a convolution of the data distribution with the arbitrary one. We also propose to learn the generation of these samples so as to challenge the discriminator in the adversarial training. We show that our approach results in a stable and well-behaved training of even the original minimax GAN formulation. Moreover, our technique can be incorporated in most modern GAN formulations and leads to a consistent improvement on several common datasets.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/29c53d37cb9bec0210e1584493479df13be85d90.pdf",
      "citation_key": "jenni2019339",
      "metadata": {
        "title": "On Stabilizing Generative Adversarial Training With Noise",
        "authors": [
          "S. Jenni",
          "P. Favaro"
        ],
        "published_date": "2019",
        "abstract": "We present a novel method and analysis to train generative adversarial networks (GAN) in a stable manner. As shown in recent analysis, training is often undermined by the probability distribution of the data being zero on neighborhoods of the data space. We notice that the distributions of real and generated data should match even when they undergo the same filtering. Therefore, to address the limited support problem we propose to train GANs by using different filtered versions of the real and generated data distributions. In this way, filtering does not prevent the exact matching of the data distribution, while helping training by extending the support of both distributions. As filtering we consider adding samples from an arbitrary distribution to the data, which corresponds to a convolution of the data distribution with the arbitrary one. We also propose to learn the generation of these samples so as to challenge the discriminator in the adversarial training. We show that our approach results in a stable and well-behaved training of even the original minimax GAN formulation. Moreover, our technique can be incorporated in most modern GAN formulations and leads to a consistent improvement on several common datasets.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/29c53d37cb9bec0210e1584493479df13be85d90.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 65,
        "score": 10.833333333333332,
        "summary": "We present a novel method and analysis to train generative adversarial networks (GAN) in a stable manner. As shown in recent analysis, training is often undermined by the probability distribution of the data being zero on neighborhoods of the data space. We notice that the distributions of real and generated data should match even when they undergo the same filtering. Therefore, to address the limited support problem we propose to train GANs by using different filtered versions of the real and generated data distributions. In this way, filtering does not prevent the exact matching of the data distribution, while helping training by extending the support of both distributions. As filtering we consider adding samples from an arbitrary distribution to the data, which corresponds to a convolution of the data distribution with the arbitrary one. We also propose to learn the generation of these samples so as to challenge the discriminator in the adversarial training. We show that our approach results in a stable and well-behaved training of even the original minimax GAN formulation. Moreover, our technique can be incorporated in most modern GAN formulations and leads to a consistent improvement on several common datasets.",
        "keywords": []
      },
      "file_name": "29c53d37cb9bec0210e1584493479df13be85d90.pdf"
    },
    {
      "success": true,
      "doc_id": "ff899123f43be1113b4937f94bfcd0d4",
      "summary": "Generative adversarial networks (GANs) are highly effective unsupervised learning frameworks that can generate very sharp data, even for data such as images with complex, highly multimodal distributions. However GANs are known to be very hard to train, suffering from problems such as mode collapse and disturbing visual artifacts. Batch normalization (BN) techniques have been introduced to address the training. Though BN accelerates the training in the beginning, our experiments show that the use of BN can be unstable and negatively impact the quality of the trained model. The evaluation of BN and numerous other recent schemes for improving GAN training is hindered by the lack of an effective objective quality measure for GAN models. To address these issues, we first introduce a weight normalization (WN) approach for GAN training that significantly improves the stability, efficiency and the quality of the generated samples. To allow a methodical evaluation, we introduce squared Euclidean reconstruction error on a test set as a new objective measure, to assess training performance in terms of speed, stability, and quality of generated samples. Our experiments with a standard DCGAN architecture on commonly used datasets (CelebA, LSUN bedroom, and CIFAR-10) indicate that training using WN is generally superior to BN for GANs, achieving 10% lower mean squared loss for reconstruction and significantly better qualitative results than BN. We further demonstrate the stability of WN on a 21-layer ResNet trained with the CelebA data set. The code for this paper is available at this https URL",
      "intriguing_abstract": "Generative adversarial networks (GANs) are highly effective unsupervised learning frameworks that can generate very sharp data, even for data such as images with complex, highly multimodal distributions. However GANs are known to be very hard to train, suffering from problems such as mode collapse and disturbing visual artifacts. Batch normalization (BN) techniques have been introduced to address the training. Though BN accelerates the training in the beginning, our experiments show that the use of BN can be unstable and negatively impact the quality of the trained model. The evaluation of BN and numerous other recent schemes for improving GAN training is hindered by the lack of an effective objective quality measure for GAN models. To address these issues, we first introduce a weight normalization (WN) approach for GAN training that significantly improves the stability, efficiency and the quality of the generated samples. To allow a methodical evaluation, we introduce squared Euclidean reconstruction error on a test set as a new objective measure, to assess training performance in terms of speed, stability, and quality of generated samples. Our experiments with a standard DCGAN architecture on commonly used datasets (CelebA, LSUN bedroom, and CIFAR-10) indicate that training using WN is generally superior to BN for GANs, achieving 10% lower mean squared loss for reconstruction and significantly better qualitative results than BN. We further demonstrate the stability of WN on a 21-layer ResNet trained with the CelebA data set. The code for this paper is available at this https URL",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/22530627d05baba39628e9d365b2f7fd8e81fe11.pdf",
      "citation_key": "xiang20171at",
      "metadata": {
        "title": "On the Effects of Batch and Weight Normalization in Generative Adversarial Networks",
        "authors": [
          "Sitao Xiang",
          "Hao Li"
        ],
        "published_date": "2017",
        "abstract": "Generative adversarial networks (GANs) are highly effective unsupervised learning frameworks that can generate very sharp data, even for data such as images with complex, highly multimodal distributions. However GANs are known to be very hard to train, suffering from problems such as mode collapse and disturbing visual artifacts. Batch normalization (BN) techniques have been introduced to address the training. Though BN accelerates the training in the beginning, our experiments show that the use of BN can be unstable and negatively impact the quality of the trained model. The evaluation of BN and numerous other recent schemes for improving GAN training is hindered by the lack of an effective objective quality measure for GAN models. To address these issues, we first introduce a weight normalization (WN) approach for GAN training that significantly improves the stability, efficiency and the quality of the generated samples. To allow a methodical evaluation, we introduce squared Euclidean reconstruction error on a test set as a new objective measure, to assess training performance in terms of speed, stability, and quality of generated samples. Our experiments with a standard DCGAN architecture on commonly used datasets (CelebA, LSUN bedroom, and CIFAR-10) indicate that training using WN is generally superior to BN for GANs, achieving 10% lower mean squared loss for reconstruction and significantly better qualitative results than BN. We further demonstrate the stability of WN on a 21-layer ResNet trained with the CelebA data set. The code for this paper is available at this https URL",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/22530627d05baba39628e9d365b2f7fd8e81fe11.pdf",
        "venue": "",
        "citationCount": 85,
        "score": 10.625,
        "summary": "Generative adversarial networks (GANs) are highly effective unsupervised learning frameworks that can generate very sharp data, even for data such as images with complex, highly multimodal distributions. However GANs are known to be very hard to train, suffering from problems such as mode collapse and disturbing visual artifacts. Batch normalization (BN) techniques have been introduced to address the training. Though BN accelerates the training in the beginning, our experiments show that the use of BN can be unstable and negatively impact the quality of the trained model. The evaluation of BN and numerous other recent schemes for improving GAN training is hindered by the lack of an effective objective quality measure for GAN models. To address these issues, we first introduce a weight normalization (WN) approach for GAN training that significantly improves the stability, efficiency and the quality of the generated samples. To allow a methodical evaluation, we introduce squared Euclidean reconstruction error on a test set as a new objective measure, to assess training performance in terms of speed, stability, and quality of generated samples. Our experiments with a standard DCGAN architecture on commonly used datasets (CelebA, LSUN bedroom, and CIFAR-10) indicate that training using WN is generally superior to BN for GANs, achieving 10% lower mean squared loss for reconstruction and significantly better qualitative results than BN. We further demonstrate the stability of WN on a 21-layer ResNet trained with the CelebA data set. The code for this paper is available at this https URL",
        "keywords": []
      },
      "file_name": "22530627d05baba39628e9d365b2f7fd8e81fe11.pdf"
    },
    {
      "success": true,
      "doc_id": "a8af5ecf38d05be34925f5d150b83621",
      "summary": "Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space. The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training. In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training. Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously. We demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.",
      "intriguing_abstract": "Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space. The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training. In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training. Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously. We demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/7fa62c091a14830ae256dc00b512f7d4b4cf5b94.pdf",
      "citation_key": "neyshabur201713g",
      "metadata": {
        "title": "Stabilizing GAN Training with Multiple Random Projections",
        "authors": [
          "Behnam Neyshabur",
          "Srinadh Bhojanapalli",
          "Ayan Chakrabarti"
        ],
        "published_date": "2017",
        "abstract": "Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space. The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training. In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training. Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously. We demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/7fa62c091a14830ae256dc00b512f7d4b4cf5b94.pdf",
        "venue": "arXiv.org",
        "citationCount": 84,
        "score": 10.5,
        "summary": "Training generative adversarial networks is unstable in high-dimensions as the true data distribution tends to be concentrated in a small fraction of the ambient space. The discriminator is then quickly able to classify nearly all generated samples as fake, leaving the generator without meaningful gradients and causing it to deteriorate after a point in training. In this work, we propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. Individual discriminators, now provided with restricted views of the input, are unable to reject generated samples perfectly and continue to provide meaningful gradients to the generator throughout training. Meanwhile, the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators simultaneously. We demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.",
        "keywords": []
      },
      "file_name": "7fa62c091a14830ae256dc00b512f7d4b4cf5b94.pdf"
    },
    {
      "success": true,
      "doc_id": "e4c84d1f8d002d6f027020cab2d4ea1b",
      "summary": "Generative Adversarial Networks (GANs) have achieved impressive results for many real-world applications. As an active research topic, many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to concepts with a segmentation-based network dissection method. We quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We will open source our interactive tools to help researchers and practitioners better understand their models.",
      "intriguing_abstract": "Generative Adversarial Networks (GANs) have achieved impressive results for many real-world applications. As an active research topic, many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to concepts with a segmentation-based network dissection method. We quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We will open source our interactive tools to help researchers and practitioners better understand their models.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/fc35a72375a8f8cfb7679bdf3e51e676618275a8.pdf",
      "citation_key": "bau20197hm",
      "metadata": {
        "title": "Visualizing and Understanding Generative Adversarial Networks (Extended Abstract)",
        "authors": [
          "David Bau",
          "Jun-Yan Zhu",
          "Hendrik Strobelt",
          "Bolei Zhou",
          "J. Tenenbaum",
          "W. Freeman",
          "A. Torralba"
        ],
        "published_date": "2019",
        "abstract": "Generative Adversarial Networks (GANs) have achieved impressive results for many real-world applications. As an active research topic, many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to concepts with a segmentation-based network dissection method. We quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We will open source our interactive tools to help researchers and practitioners better understand their models.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/fc35a72375a8f8cfb7679bdf3e51e676618275a8.pdf",
        "venue": "arXiv.org",
        "citationCount": 62,
        "score": 10.333333333333332,
        "summary": "Generative Adversarial Networks (GANs) have achieved impressive results for many real-world applications. As an active research topic, many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to concepts with a segmentation-based network dissection method. We quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We will open source our interactive tools to help researchers and practitioners better understand their models.",
        "keywords": []
      },
      "file_name": "fc35a72375a8f8cfb7679bdf3e51e676618275a8.pdf"
    },
    {
      "success": true,
      "doc_id": "43f8a7cd1db74fee09ed95df3f0a9d99",
      "summary": "Generative adversarial networks (GANs) are a powerful approach to unsupervised learning. They have achieved state-of-the-art performance in the image domain. However, GANs are limited in two ways. They often learn distributions with low support---a phenomenon known as mode collapse---and they do not guarantee the existence of a probability density, which makes evaluating generalization using predictive log-likelihood impossible. In this paper, we develop the prescribed GAN (PresGAN) to address these shortcomings. PresGANs add noise to the output of a density network and optimize an entropy-regularized adversarial loss. The added noise renders tractable approximations of the predictive log-likelihood and stabilizes the training procedure. The entropy regularizer encourages PresGANs to capture all the modes of the data distribution. Fitting PresGANs involves computing the intractable gradients of the entropy regularization term; PresGANs sidestep this intractability using unbiased stochastic estimates. We evaluate PresGANs on several datasets and found they mitigate mode collapse and generate samples with high perceptual quality. We further found that PresGANs reduce the gap in performance in terms of predictive log-likelihood between traditional GANs and variational autoencoders (VAEs).",
      "intriguing_abstract": "Generative adversarial networks (GANs) are a powerful approach to unsupervised learning. They have achieved state-of-the-art performance in the image domain. However, GANs are limited in two ways. They often learn distributions with low support---a phenomenon known as mode collapse---and they do not guarantee the existence of a probability density, which makes evaluating generalization using predictive log-likelihood impossible. In this paper, we develop the prescribed GAN (PresGAN) to address these shortcomings. PresGANs add noise to the output of a density network and optimize an entropy-regularized adversarial loss. The added noise renders tractable approximations of the predictive log-likelihood and stabilizes the training procedure. The entropy regularizer encourages PresGANs to capture all the modes of the data distribution. Fitting PresGANs involves computing the intractable gradients of the entropy regularization term; PresGANs sidestep this intractability using unbiased stochastic estimates. We evaluate PresGANs on several datasets and found they mitigate mode collapse and generate samples with high perceptual quality. We further found that PresGANs reduce the gap in performance in terms of predictive log-likelihood between traditional GANs and variational autoencoders (VAEs).",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/fae3d474c4d7745be06458df0c20bf837a6055ef.pdf",
      "citation_key": "dieng2019rjn",
      "metadata": {
        "title": "Prescribed Generative Adversarial Networks",
        "authors": [
          "A. B. Dieng",
          "Francisco J. R. Ruiz",
          "D. Blei",
          "Michalis K. Titsias"
        ],
        "published_date": "2019",
        "abstract": "Generative adversarial networks (GANs) are a powerful approach to unsupervised learning. They have achieved state-of-the-art performance in the image domain. However, GANs are limited in two ways. They often learn distributions with low support---a phenomenon known as mode collapse---and they do not guarantee the existence of a probability density, which makes evaluating generalization using predictive log-likelihood impossible. In this paper, we develop the prescribed GAN (PresGAN) to address these shortcomings. PresGANs add noise to the output of a density network and optimize an entropy-regularized adversarial loss. The added noise renders tractable approximations of the predictive log-likelihood and stabilizes the training procedure. The entropy regularizer encourages PresGANs to capture all the modes of the data distribution. Fitting PresGANs involves computing the intractable gradients of the entropy regularization term; PresGANs sidestep this intractability using unbiased stochastic estimates. We evaluate PresGANs on several datasets and found they mitigate mode collapse and generate samples with high perceptual quality. We further found that PresGANs reduce the gap in performance in terms of predictive log-likelihood between traditional GANs and variational autoencoders (VAEs).",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/fae3d474c4d7745be06458df0c20bf837a6055ef.pdf",
        "venue": "arXiv.org",
        "citationCount": 62,
        "score": 10.333333333333332,
        "summary": "Generative adversarial networks (GANs) are a powerful approach to unsupervised learning. They have achieved state-of-the-art performance in the image domain. However, GANs are limited in two ways. They often learn distributions with low support---a phenomenon known as mode collapse---and they do not guarantee the existence of a probability density, which makes evaluating generalization using predictive log-likelihood impossible. In this paper, we develop the prescribed GAN (PresGAN) to address these shortcomings. PresGANs add noise to the output of a density network and optimize an entropy-regularized adversarial loss. The added noise renders tractable approximations of the predictive log-likelihood and stabilizes the training procedure. The entropy regularizer encourages PresGANs to capture all the modes of the data distribution. Fitting PresGANs involves computing the intractable gradients of the entropy regularization term; PresGANs sidestep this intractability using unbiased stochastic estimates. We evaluate PresGANs on several datasets and found they mitigate mode collapse and generate samples with high perceptual quality. We further found that PresGANs reduce the gap in performance in terms of predictive log-likelihood between traditional GANs and variational autoencoders (VAEs).",
        "keywords": []
      },
      "file_name": "fae3d474c4d7745be06458df0c20bf837a6055ef.pdf"
    },
    {
      "success": true,
      "doc_id": "d9c4547a2e71d628ef1f12c7aab138df",
      "summary": "Machinery fault diagnosis tasks have been well addressed when sufficient and abundant data are available. However, the data imbalance problem widely exists in real-world scenarios, which leads to the performance deterioration of fault diagnosis markedly. To solve this problem, we present a novel imbalanced fault diagnosis method based on the enhanced generative adversarial networks (GAN). By artificially generating fake samples, the proposed method can mitigate the loss caused by the lack of real fault data. Specifically, in order to improve the quality of generated samples, a new discriminator is designed using spectrum normalization (SN) strategy and a two time-scale update rule (TTUR) method is used to stabilize the training process of GAN. Then, an enhanced Wasserstein GAN with gradient penalty is developed to generate high-quality synthetic samples for the fault samples set. Finally, a deep convolutional classifier is constructed to carry out fault classification. The performance and effectiveness of the proposed method are validated on the Case Western Reserve University bearing dataset and rolling bearing dataset acquired from our laboratory. The simulation results show that the proposed method has a superior performance than other methods for imbalanced fault diagnosis tasks.",
      "intriguing_abstract": "Machinery fault diagnosis tasks have been well addressed when sufficient and abundant data are available. However, the data imbalance problem widely exists in real-world scenarios, which leads to the performance deterioration of fault diagnosis markedly. To solve this problem, we present a novel imbalanced fault diagnosis method based on the enhanced generative adversarial networks (GAN). By artificially generating fake samples, the proposed method can mitigate the loss caused by the lack of real fault data. Specifically, in order to improve the quality of generated samples, a new discriminator is designed using spectrum normalization (SN) strategy and a two time-scale update rule (TTUR) method is used to stabilize the training process of GAN. Then, an enhanced Wasserstein GAN with gradient penalty is developed to generate high-quality synthetic samples for the fault samples set. Finally, a deep convolutional classifier is constructed to carry out fault classification. The performance and effectiveness of the proposed method are validated on the Case Western Reserve University bearing dataset and rolling bearing dataset acquired from our laboratory. The simulation results show that the proposed method has a superior performance than other methods for imbalanced fault diagnosis tasks.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/6cfb89a73b12caf20dbcdb7c5f6d9a85f02b0730.pdf",
      "citation_key": "zhang2020376",
      "metadata": {
        "title": "Imbalanced Fault Diagnosis of Rolling Bearing Using Enhanced Generative Adversarial Networks",
        "authors": [
          "Hongliang Zhang",
          "Rui Wang",
          "Ruilin Pan",
          "Haiyang Pan"
        ],
        "published_date": "2020",
        "abstract": "Machinery fault diagnosis tasks have been well addressed when sufficient and abundant data are available. However, the data imbalance problem widely exists in real-world scenarios, which leads to the performance deterioration of fault diagnosis markedly. To solve this problem, we present a novel imbalanced fault diagnosis method based on the enhanced generative adversarial networks (GAN). By artificially generating fake samples, the proposed method can mitigate the loss caused by the lack of real fault data. Specifically, in order to improve the quality of generated samples, a new discriminator is designed using spectrum normalization (SN) strategy and a two time-scale update rule (TTUR) method is used to stabilize the training process of GAN. Then, an enhanced Wasserstein GAN with gradient penalty is developed to generate high-quality synthetic samples for the fault samples set. Finally, a deep convolutional classifier is constructed to carry out fault classification. The performance and effectiveness of the proposed method are validated on the Case Western Reserve University bearing dataset and rolling bearing dataset acquired from our laboratory. The simulation results show that the proposed method has a superior performance than other methods for imbalanced fault diagnosis tasks.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/6cfb89a73b12caf20dbcdb7c5f6d9a85f02b0730.pdf",
        "venue": "IEEE Access",
        "citationCount": 51,
        "score": 10.200000000000001,
        "summary": "Machinery fault diagnosis tasks have been well addressed when sufficient and abundant data are available. However, the data imbalance problem widely exists in real-world scenarios, which leads to the performance deterioration of fault diagnosis markedly. To solve this problem, we present a novel imbalanced fault diagnosis method based on the enhanced generative adversarial networks (GAN). By artificially generating fake samples, the proposed method can mitigate the loss caused by the lack of real fault data. Specifically, in order to improve the quality of generated samples, a new discriminator is designed using spectrum normalization (SN) strategy and a two time-scale update rule (TTUR) method is used to stabilize the training process of GAN. Then, an enhanced Wasserstein GAN with gradient penalty is developed to generate high-quality synthetic samples for the fault samples set. Finally, a deep convolutional classifier is constructed to carry out fault classification. The performance and effectiveness of the proposed method are validated on the Case Western Reserve University bearing dataset and rolling bearing dataset acquired from our laboratory. The simulation results show that the proposed method has a superior performance than other methods for imbalanced fault diagnosis tasks.",
        "keywords": []
      },
      "file_name": "6cfb89a73b12caf20dbcdb7c5f6d9a85f02b0730.pdf"
    },
    {
      "success": true,
      "doc_id": "70c682a936a7f14ed2bb4bc35a4d5601",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/75556186b9b7ba52464a4e64477efff05bde021a.pdf",
      "citation_key": "yuan202257j",
      "metadata": {
        "title": "GAN-based image steganography for enhancing security via adversarial attack and pixel-wise deep fusion",
        "authors": [
          "Chao Yuan",
          "Hongxia Wang",
          "Peisong He",
          "Jie Luo",
          "Bin Li"
        ],
        "published_date": "2022",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/75556186b9b7ba52464a4e64477efff05bde021a.pdf",
        "venue": "Multimedia tools and applications",
        "citationCount": 29,
        "score": 9.666666666666666,
        "summary": "",
        "keywords": []
      },
      "file_name": "75556186b9b7ba52464a4e64477efff05bde021a.pdf"
    },
    {
      "success": true,
      "doc_id": "6e2d73bdff4879adf96cb8a96e62e31f",
      "summary": "We propose a GAN-based image compression method working at extremely low bitrates below 0.1bpp. Most existing learned image compression methods suffer from blur at extremely low bitrates. Although GAN can help to reconstruct sharp images, there are two drawbacks. First, GAN makes training unstable. Second, the reconstructions often contain unpleasing noise or artifacts. To address both of the drawbacks, our method adopts two-stage training and network interpolation. The two-stage training is effective to stabilize the training. Moreover, the network interpolation utilizes the models in both stages and reduces undesirable noise and artifacts, while maintaining important edges. Hence, we can control the trade-off between perceptual quality and fidelity without re-training models. The experimental results show that our model can reconstruct high quality images. Furthermore, our user study confirms that our reconstructions are preferable to state-of-the-art GAN-based image compression model. Our source code is available at https://github.com/iwa-shi/fidelity_controllable_compression",
      "intriguing_abstract": "We propose a GAN-based image compression method working at extremely low bitrates below 0.1bpp. Most existing learned image compression methods suffer from blur at extremely low bitrates. Although GAN can help to reconstruct sharp images, there are two drawbacks. First, GAN makes training unstable. Second, the reconstructions often contain unpleasing noise or artifacts. To address both of the drawbacks, our method adopts two-stage training and network interpolation. The two-stage training is effective to stabilize the training. Moreover, the network interpolation utilizes the models in both stages and reduces undesirable noise and artifacts, while maintaining important edges. Hence, we can control the trade-off between perceptual quality and fidelity without re-training models. The experimental results show that our model can reconstruct high quality images. Furthermore, our user study confirms that our reconstructions are preferable to state-of-the-art GAN-based image compression model. Our source code is available at https://github.com/iwa-shi/fidelity_controllable_compression",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/29424f3ba4b63eca794f4cde9f59d9e9838147c0.pdf",
      "citation_key": "iwai2020fp2",
      "metadata": {
        "title": "Fidelity-Controllable Extreme Image Compression with Generative Adversarial Networks",
        "authors": [
          "Shoma Iwai",
          "Tomo Miyazaki",
          "Yoshihiro Sugaya",
          "S. Omachi"
        ],
        "published_date": "2020",
        "abstract": "We propose a GAN-based image compression method working at extremely low bitrates below 0.1bpp. Most existing learned image compression methods suffer from blur at extremely low bitrates. Although GAN can help to reconstruct sharp images, there are two drawbacks. First, GAN makes training unstable. Second, the reconstructions often contain unpleasing noise or artifacts. To address both of the drawbacks, our method adopts two-stage training and network interpolation. The two-stage training is effective to stabilize the training. Moreover, the network interpolation utilizes the models in both stages and reduces undesirable noise and artifacts, while maintaining important edges. Hence, we can control the trade-off between perceptual quality and fidelity without re-training models. The experimental results show that our model can reconstruct high quality images. Furthermore, our user study confirms that our reconstructions are preferable to state-of-the-art GAN-based image compression model. Our source code is available at https://github.com/iwa-shi/fidelity_controllable_compression",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/29424f3ba4b63eca794f4cde9f59d9e9838147c0.pdf",
        "venue": "International Conference on Pattern Recognition",
        "citationCount": 47,
        "score": 9.4,
        "summary": "We propose a GAN-based image compression method working at extremely low bitrates below 0.1bpp. Most existing learned image compression methods suffer from blur at extremely low bitrates. Although GAN can help to reconstruct sharp images, there are two drawbacks. First, GAN makes training unstable. Second, the reconstructions often contain unpleasing noise or artifacts. To address both of the drawbacks, our method adopts two-stage training and network interpolation. The two-stage training is effective to stabilize the training. Moreover, the network interpolation utilizes the models in both stages and reduces undesirable noise and artifacts, while maintaining important edges. Hence, we can control the trade-off between perceptual quality and fidelity without re-training models. The experimental results show that our model can reconstruct high quality images. Furthermore, our user study confirms that our reconstructions are preferable to state-of-the-art GAN-based image compression model. Our source code is available at https://github.com/iwa-shi/fidelity_controllable_compression",
        "keywords": []
      },
      "file_name": "29424f3ba4b63eca794f4cde9f59d9e9838147c0.pdf"
    },
    {
      "success": true,
      "doc_id": "a1d78e04d633feef60999efe63914268",
      "summary": "Generative adversarial networks (GANs) are a framework that learns a generative distribution through adversarial training. Recently, their class conditional extensions (e.g., conditional GAN (cGAN) and auxiliary classifier GAN (AC-GAN)) have attracted much attention owing to their ability to learn the disentangled representations and to improve the training stability. However, their training requires the availability of large-scale accurate class-labeled data, which are often laborious or impractical to collect in a real-world scenario. To remedy this, we propose a novel family of GANs called label-noise robust GANs (rGANs), which, by incorporating a noise transition model, can learn a clean label conditional generative distribution even when training labels are noisy. In particular, we propose two variants: rAC-GAN, which is a bridging model between AC-GAN and the label-noise robust classification model, and rcGAN, which is an extension of cGAN and solves this problem with no reliance on any classifier. In addition to providing the theoretical background, we demonstrate the effectiveness of our models through extensive experiments using diverse GAN configurations, various noise settings, and multiple evaluation metrics (in which we tested 402 conditions in total).",
      "intriguing_abstract": "Generative adversarial networks (GANs) are a framework that learns a generative distribution through adversarial training. Recently, their class conditional extensions (e.g., conditional GAN (cGAN) and auxiliary classifier GAN (AC-GAN)) have attracted much attention owing to their ability to learn the disentangled representations and to improve the training stability. However, their training requires the availability of large-scale accurate class-labeled data, which are often laborious or impractical to collect in a real-world scenario. To remedy this, we propose a novel family of GANs called label-noise robust GANs (rGANs), which, by incorporating a noise transition model, can learn a clean label conditional generative distribution even when training labels are noisy. In particular, we propose two variants: rAC-GAN, which is a bridging model between AC-GAN and the label-noise robust classification model, and rcGAN, which is an extension of cGAN and solves this problem with no reliance on any classifier. In addition to providing the theoretical background, we demonstrate the effectiveness of our models through extensive experiments using diverse GAN configurations, various noise settings, and multiple evaluation metrics (in which we tested 402 conditions in total).",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/32e277b85802685105254430c4170ad2b1a16c04.pdf",
      "citation_key": "kaneko2018jex",
      "metadata": {
        "title": "Label-Noise Robust Generative Adversarial Networks",
        "authors": [
          "Takuhiro Kaneko",
          "Y. Ushiku",
          "T. Harada"
        ],
        "published_date": "2018",
        "abstract": "Generative adversarial networks (GANs) are a framework that learns a generative distribution through adversarial training. Recently, their class conditional extensions (e.g., conditional GAN (cGAN) and auxiliary classifier GAN (AC-GAN)) have attracted much attention owing to their ability to learn the disentangled representations and to improve the training stability. However, their training requires the availability of large-scale accurate class-labeled data, which are often laborious or impractical to collect in a real-world scenario. To remedy this, we propose a novel family of GANs called label-noise robust GANs (rGANs), which, by incorporating a noise transition model, can learn a clean label conditional generative distribution even when training labels are noisy. In particular, we propose two variants: rAC-GAN, which is a bridging model between AC-GAN and the label-noise robust classification model, and rcGAN, which is an extension of cGAN and solves this problem with no reliance on any classifier. In addition to providing the theoretical background, we demonstrate the effectiveness of our models through extensive experiments using diverse GAN configurations, various noise settings, and multiple evaluation metrics (in which we tested 402 conditions in total).",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/32e277b85802685105254430c4170ad2b1a16c04.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 63,
        "score": 9.0,
        "summary": "Generative adversarial networks (GANs) are a framework that learns a generative distribution through adversarial training. Recently, their class conditional extensions (e.g., conditional GAN (cGAN) and auxiliary classifier GAN (AC-GAN)) have attracted much attention owing to their ability to learn the disentangled representations and to improve the training stability. However, their training requires the availability of large-scale accurate class-labeled data, which are often laborious or impractical to collect in a real-world scenario. To remedy this, we propose a novel family of GANs called label-noise robust GANs (rGANs), which, by incorporating a noise transition model, can learn a clean label conditional generative distribution even when training labels are noisy. In particular, we propose two variants: rAC-GAN, which is a bridging model between AC-GAN and the label-noise robust classification model, and rcGAN, which is an extension of cGAN and solves this problem with no reliance on any classifier. In addition to providing the theoretical background, we demonstrate the effectiveness of our models through extensive experiments using diverse GAN configurations, various noise settings, and multiple evaluation metrics (in which we tested 402 conditions in total).",
        "keywords": []
      },
      "file_name": "32e277b85802685105254430c4170ad2b1a16c04.pdf"
    },
    {
      "success": true,
      "doc_id": "f0c9050cd95faf3af5ec92d3df58cdaa",
      "summary": "The lack of annotated datasets makes the automatic detection of skin problems very difficult, which is also the case for most other medical applications. The outstanding results achieved by deep learning techniques in developing such applications have improved the diagnostic accuracy. Nevertheless, the performance of these models is heavily dependent on the volume of labelled data used for training, which is unfortunately not available. To address this problem, traditional data augmentation is usually adopted. Recently, the emergence of a generative adversarial network (GAN) seems a more plausible solution, where synthetic images are generated. In this work, we have developed a deep generative adversarial network (DGAN) multi-class classifier, which can generate skin problem images by learning the true data distribution from the available images. Unlike the usual two-class classifier, we have developed a multi-class solution, and to address the class-imbalanced dataset, we have taken images from different datasets available online. One main challenge faced during our development is mainly to improve the stability of the DGAN model during the training phase. To analyse the performance of GAN, we have developed two CNN models in parallel based on the architecture of ResNet50 and VGG16 by augmenting the training datasets using the traditional rotation, flipping, and scaling methods. We have used both labelled and unlabelled data for testing to test the models. DGAN has outperformed the conventional data augmentation by achieving a performance of 91.1% for the unlabelled dataset and 92.3% for the labelled dataset. On the contrary, CNN models with data augmentation have achieved a performance of up to 70.8% for the unlabelled dataset. The outcome of our DGAN confirms the ability of the model to learn from unlabelled datasets and yet produce a good diagnosis result.",
      "intriguing_abstract": "The lack of annotated datasets makes the automatic detection of skin problems very difficult, which is also the case for most other medical applications. The outstanding results achieved by deep learning techniques in developing such applications have improved the diagnostic accuracy. Nevertheless, the performance of these models is heavily dependent on the volume of labelled data used for training, which is unfortunately not available. To address this problem, traditional data augmentation is usually adopted. Recently, the emergence of a generative adversarial network (GAN) seems a more plausible solution, where synthetic images are generated. In this work, we have developed a deep generative adversarial network (DGAN) multi-class classifier, which can generate skin problem images by learning the true data distribution from the available images. Unlike the usual two-class classifier, we have developed a multi-class solution, and to address the class-imbalanced dataset, we have taken images from different datasets available online. One main challenge faced during our development is mainly to improve the stability of the DGAN model during the training phase. To analyse the performance of GAN, we have developed two CNN models in parallel based on the architecture of ResNet50 and VGG16 by augmenting the training datasets using the traditional rotation, flipping, and scaling methods. We have used both labelled and unlabelled data for testing to test the models. DGAN has outperformed the conventional data augmentation by achieving a performance of 91.1% for the unlabelled dataset and 92.3% for the labelled dataset. On the contrary, CNN models with data augmentation have achieved a performance of up to 70.8% for the unlabelled dataset. The outcome of our DGAN confirms the ability of the model to learn from unlabelled datasets and yet produce a good diagnosis result.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/9e1019b67dc1012eba53b34968fe352dc432f49d.pdf",
      "citation_key": "khan20223o7",
      "metadata": {
        "title": "Multi-Class Skin Problem Classification Using Deep Generative Adversarial Network (DGAN)",
        "authors": [
          "Maleika Heenaye-Mamode Khan",
          "N. Gooda Sahib-Kaudeer",
          "Motean Dayalen",
          "Faadil Mahomedaly",
          "G. Sinha",
          "K. Nagwanshi",
          "Amelia Taylor"
        ],
        "published_date": "2022",
        "abstract": "The lack of annotated datasets makes the automatic detection of skin problems very difficult, which is also the case for most other medical applications. The outstanding results achieved by deep learning techniques in developing such applications have improved the diagnostic accuracy. Nevertheless, the performance of these models is heavily dependent on the volume of labelled data used for training, which is unfortunately not available. To address this problem, traditional data augmentation is usually adopted. Recently, the emergence of a generative adversarial network (GAN) seems a more plausible solution, where synthetic images are generated. In this work, we have developed a deep generative adversarial network (DGAN) multi-class classifier, which can generate skin problem images by learning the true data distribution from the available images. Unlike the usual two-class classifier, we have developed a multi-class solution, and to address the class-imbalanced dataset, we have taken images from different datasets available online. One main challenge faced during our development is mainly to improve the stability of the DGAN model during the training phase. To analyse the performance of GAN, we have developed two CNN models in parallel based on the architecture of ResNet50 and VGG16 by augmenting the training datasets using the traditional rotation, flipping, and scaling methods. We have used both labelled and unlabelled data for testing to test the models. DGAN has outperformed the conventional data augmentation by achieving a performance of 91.1% for the unlabelled dataset and 92.3% for the labelled dataset. On the contrary, CNN models with data augmentation have achieved a performance of up to 70.8% for the unlabelled dataset. The outcome of our DGAN confirms the ability of the model to learn from unlabelled datasets and yet produce a good diagnosis result.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/9e1019b67dc1012eba53b34968fe352dc432f49d.pdf",
        "venue": "Computational Intelligence and Neuroscience",
        "citationCount": 27,
        "score": 9.0,
        "summary": "The lack of annotated datasets makes the automatic detection of skin problems very difficult, which is also the case for most other medical applications. The outstanding results achieved by deep learning techniques in developing such applications have improved the diagnostic accuracy. Nevertheless, the performance of these models is heavily dependent on the volume of labelled data used for training, which is unfortunately not available. To address this problem, traditional data augmentation is usually adopted. Recently, the emergence of a generative adversarial network (GAN) seems a more plausible solution, where synthetic images are generated. In this work, we have developed a deep generative adversarial network (DGAN) multi-class classifier, which can generate skin problem images by learning the true data distribution from the available images. Unlike the usual two-class classifier, we have developed a multi-class solution, and to address the class-imbalanced dataset, we have taken images from different datasets available online. One main challenge faced during our development is mainly to improve the stability of the DGAN model during the training phase. To analyse the performance of GAN, we have developed two CNN models in parallel based on the architecture of ResNet50 and VGG16 by augmenting the training datasets using the traditional rotation, flipping, and scaling methods. We have used both labelled and unlabelled data for testing to test the models. DGAN has outperformed the conventional data augmentation by achieving a performance of 91.1% for the unlabelled dataset and 92.3% for the labelled dataset. On the contrary, CNN models with data augmentation have achieved a performance of up to 70.8% for the unlabelled dataset. The outcome of our DGAN confirms the ability of the model to learn from unlabelled datasets and yet produce a good diagnosis result.",
        "keywords": []
      },
      "file_name": "9e1019b67dc1012eba53b34968fe352dc432f49d.pdf"
    },
    {
      "success": true,
      "doc_id": "b53f9ef99e393a893f221e9dc8dcf19e",
      "summary": "The Generative Adversarial Network (GAN) has shown powerfulness in various real-world artificial intelligence applications. However, its network architecture is generally designed through a manual trial-and-error process, which is relatively tedious, slow, and sub-optimal. This paper hence develops an evolutionary architectural search (EAS) technique to automate the entire design process of the GAN. In particular, different objective functions are used in the generator of a GAN as variation operators. This helps train the generator with various candidate architectures and their associated weights to play adversarial training against the discriminator of the GAN. Following evaluations by the discriminator, superior candidate generators survive to the next generation and evolve for potentially better architectures and their weights simultaneously, leading to more stabilized GANs with improved performance. The GAN designed through EAS is termed an EAS-GAN in this paper and is tested against existing evolutionary and other state-of-the-art GANs. The test results show that the EAS-GAN offers better generative performance overall, with the Fréchet inception distance scoring 22.1, 38.8, and 8.3 on the CIFAR-10, STL-10, and LSUN bedroom data sets, respectively.",
      "intriguing_abstract": "The Generative Adversarial Network (GAN) has shown powerfulness in various real-world artificial intelligence applications. However, its network architecture is generally designed through a manual trial-and-error process, which is relatively tedious, slow, and sub-optimal. This paper hence develops an evolutionary architectural search (EAS) technique to automate the entire design process of the GAN. In particular, different objective functions are used in the generator of a GAN as variation operators. This helps train the generator with various candidate architectures and their associated weights to play adversarial training against the discriminator of the GAN. Following evaluations by the discriminator, superior candidate generators survive to the next generation and evolve for potentially better architectures and their weights simultaneously, leading to more stabilized GANs with improved performance. The GAN designed through EAS is termed an EAS-GAN in this paper and is tested against existing evolutionary and other state-of-the-art GANs. The test results show that the EAS-GAN offers better generative performance overall, with the Fréchet inception distance scoring 22.1, 38.8, and 8.3 on the CIFAR-10, STL-10, and LSUN bedroom data sets, respectively.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/9d4b4b44da22d4afdf127a1e33108f37bf1ec4e4.pdf",
      "citation_key": "lin20224oj",
      "metadata": {
        "title": "Evolutionary Architectural Search for Generative Adversarial Networks",
        "authors": [
          "Qiuzhen Lin",
          "Z. Fang",
          "Yi Chen",
          "K. Tan",
          "Yun Li"
        ],
        "published_date": "2022",
        "abstract": "The Generative Adversarial Network (GAN) has shown powerfulness in various real-world artificial intelligence applications. However, its network architecture is generally designed through a manual trial-and-error process, which is relatively tedious, slow, and sub-optimal. This paper hence develops an evolutionary architectural search (EAS) technique to automate the entire design process of the GAN. In particular, different objective functions are used in the generator of a GAN as variation operators. This helps train the generator with various candidate architectures and their associated weights to play adversarial training against the discriminator of the GAN. Following evaluations by the discriminator, superior candidate generators survive to the next generation and evolve for potentially better architectures and their weights simultaneously, leading to more stabilized GANs with improved performance. The GAN designed through EAS is termed an EAS-GAN in this paper and is tested against existing evolutionary and other state-of-the-art GANs. The test results show that the EAS-GAN offers better generative performance overall, with the Fréchet inception distance scoring 22.1, 38.8, and 8.3 on the CIFAR-10, STL-10, and LSUN bedroom data sets, respectively.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/9d4b4b44da22d4afdf127a1e33108f37bf1ec4e4.pdf",
        "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
        "citationCount": 25,
        "score": 8.333333333333332,
        "summary": "The Generative Adversarial Network (GAN) has shown powerfulness in various real-world artificial intelligence applications. However, its network architecture is generally designed through a manual trial-and-error process, which is relatively tedious, slow, and sub-optimal. This paper hence develops an evolutionary architectural search (EAS) technique to automate the entire design process of the GAN. In particular, different objective functions are used in the generator of a GAN as variation operators. This helps train the generator with various candidate architectures and their associated weights to play adversarial training against the discriminator of the GAN. Following evaluations by the discriminator, superior candidate generators survive to the next generation and evolve for potentially better architectures and their weights simultaneously, leading to more stabilized GANs with improved performance. The GAN designed through EAS is termed an EAS-GAN in this paper and is tested against existing evolutionary and other state-of-the-art GANs. The test results show that the EAS-GAN offers better generative performance overall, with the Fréchet inception distance scoring 22.1, 38.8, and 8.3 on the CIFAR-10, STL-10, and LSUN bedroom data sets, respectively.",
        "keywords": []
      },
      "file_name": "9d4b4b44da22d4afdf127a1e33108f37bf1ec4e4.pdf"
    },
    {
      "success": true,
      "doc_id": "b4eb7cd379a6329483e777d36ad06bac",
      "summary": "Sequence generative adversarial networks (SeqGAN) have been used to improve conditional sequence generation tasks, for example, chit-chat dialogue generation. To stabilize the training of SeqGAN, Monte Carlo tree search (MCTS) or reward at every generation step (REGS) is used to evaluate the goodness of a generated subsequence. MCTS is computationally intensive, but the performance of REGS is worse than MCTS. In this paper, we propose stepwise GAN (StepGAN), in which the discriminator is modified to automatically assign scores quantifying the goodness of each subsequence at every generation step. StepGAN has significantly less computational costs than MCTS. We demonstrate that StepGAN outperforms previous GAN-based methods on both synthetic experiment and chit-chat dialogue generation.",
      "intriguing_abstract": "Sequence generative adversarial networks (SeqGAN) have been used to improve conditional sequence generation tasks, for example, chit-chat dialogue generation. To stabilize the training of SeqGAN, Monte Carlo tree search (MCTS) or reward at every generation step (REGS) is used to evaluate the goodness of a generated subsequence. MCTS is computationally intensive, but the performance of REGS is worse than MCTS. In this paper, we propose stepwise GAN (StepGAN), in which the discriminator is modified to automatically assign scores quantifying the goodness of each subsequence at every generation step. StepGAN has significantly less computational costs than MCTS. We demonstrate that StepGAN outperforms previous GAN-based methods on both synthetic experiment and chit-chat dialogue generation.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/484ffd765876bc7f82a9eacff68ca20dbd4fcc34.pdf",
      "citation_key": "tuan2018kbr",
      "metadata": {
        "title": "Improving Conditional Sequence Generative Adversarial Networks by Stepwise Evaluation",
        "authors": [
          "Yi-Lin Tuan",
          "Hung-yi Lee"
        ],
        "published_date": "2018",
        "abstract": "Sequence generative adversarial networks (SeqGAN) have been used to improve conditional sequence generation tasks, for example, chit-chat dialogue generation. To stabilize the training of SeqGAN, Monte Carlo tree search (MCTS) or reward at every generation step (REGS) is used to evaluate the goodness of a generated subsequence. MCTS is computationally intensive, but the performance of REGS is worse than MCTS. In this paper, we propose stepwise GAN (StepGAN), in which the discriminator is modified to automatically assign scores quantifying the goodness of each subsequence at every generation step. StepGAN has significantly less computational costs than MCTS. We demonstrate that StepGAN outperforms previous GAN-based methods on both synthetic experiment and chit-chat dialogue generation.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/484ffd765876bc7f82a9eacff68ca20dbd4fcc34.pdf",
        "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
        "citationCount": 57,
        "score": 8.142857142857142,
        "summary": "Sequence generative adversarial networks (SeqGAN) have been used to improve conditional sequence generation tasks, for example, chit-chat dialogue generation. To stabilize the training of SeqGAN, Monte Carlo tree search (MCTS) or reward at every generation step (REGS) is used to evaluate the goodness of a generated subsequence. MCTS is computationally intensive, but the performance of REGS is worse than MCTS. In this paper, we propose stepwise GAN (StepGAN), in which the discriminator is modified to automatically assign scores quantifying the goodness of each subsequence at every generation step. StepGAN has significantly less computational costs than MCTS. We demonstrate that StepGAN outperforms previous GAN-based methods on both synthetic experiment and chit-chat dialogue generation.",
        "keywords": []
      },
      "file_name": "484ffd765876bc7f82a9eacff68ca20dbd4fcc34.pdf"
    },
    {
      "success": true,
      "doc_id": "36616b243317c9d3c78e1a5290d9420c",
      "summary": "Here is a focused summary of the technical paper for literature review, adhering to your requirements:\n\n### Technical Paper Analysis: Cancer classification with data augmentation based on generative adversarial networks \\cite{wei2021qea}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** The paper addresses the challenge of cancer gene classification.\n    *   **Importance and challenge:** This problem is critical for diagnosis and treatment, but its effectiveness is significantly weakened by the inadequacy of available cancer data \\cite{wei2021qea}. Data scarcity leads to poor generalization and performance of classification models.\n\n2.  **Related Work & Positioning**\n    *   The provided abstract does not explicitly detail how this work relates to existing approaches or the limitations of previous solutions beyond the general problem of inadequate data. It positions itself as a solution to this data scarcity issue.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method:** The paper proposes a Cancer Classification Model that incorporates data augmentation. The core of this augmentation strategy is the use of Generative Adversarial Networks (GANs) to create synthetic cancer data \\cite{wei2021qea}.\n    *   **Novelty:** The innovation lies in leveraging GANs to synthesize data that is \"highly similar to the real one\" \\cite{wei2021qea}, specifically for the purpose of augmenting limited cancer datasets. This generated data is then used to improve the cancer classification results.\n\n4.  **Key Technical Contributions**\n    *   **Novel methods/techniques:** The primary contribution is the application of GANs to generate realistic synthetic cancer data, directly addressing the problem of data inadequacy in cancer classification \\cite{wei2021qea}.\n    *   **Improved performance:** The work demonstrates an improvement in cancer classification performance by integrating this GAN-based data augmentation into the classification pipeline \\cite{wei2021qea}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:** The provided text indicates that the generated data is used to \"improve the cancer classification results\" \\cite{wei2021qea}, implying experimental validation of the classification performance with and without augmentation. However, specific details about the experimental setup, datasets, or methodologies are not provided in this snippet.\n    *   **Key performance metrics and comparison results:** The paper claims to \"improve the cancer classification performance\" \\cite{wei2021qea}. Specific metrics (e.g., accuracy, precision, recall, F1-score) or comparative results against baseline methods are not detailed in the provided content.\n\n6.  **Limitations & Scope**\n    *   The provided abstract does not explicitly state technical limitations, assumptions, or the specific scope of applicability beyond cancer gene classification.\n\n7.  **Technical Significance**\n    *   **Advance the technical state-of-the-art:** This work advances the state-of-the-art by offering a practical solution to the pervasive problem of data scarcity in medical image and gene classification tasks, specifically for cancer. By demonstrating the effectiveness of GANs for generating high-fidelity synthetic cancer data, it provides a robust method to enhance model training and generalization.\n    *   **Potential impact on future research:** The approach has significant potential to impact future research in medical AI, particularly in areas where data collection is challenging or expensive. It opens avenues for exploring more sophisticated GAN architectures for medical data synthesis, investigating the optimal integration of synthetic data into various machine learning models, and potentially reducing the reliance on large, privacy-sensitive real datasets for initial model development.",
      "intriguing_abstract": "The accurate classification of cancer genes is paramount for effective diagnosis and personalized treatment, yet its progress is severely hampered by the pervasive scarcity of high-quality, extensive datasets. This data inadequacy often leads to machine learning models with poor generalization capabilities. We present a novel Cancer Classification Model that directly confronts this challenge through an innovative data augmentation strategy powered by Generative Adversarial Networks (GANs).\n\nOur approach leverages GANs to synthesize highly realistic and diverse synthetic cancer data, effectively expanding limited real datasets without compromising data integrity. This generated data significantly enhances the training of classification models, leading to demonstrably improved cancer classification performance. This work not only offers a robust solution to a critical bottleneck in cancer research but also establishes a powerful paradigm for overcoming data scarcity in medical AI. By providing a method to generate high-fidelity medical data, our findings pave the way for more resilient and generalizable diagnostic tools, accelerating advancements in precision medicine and reducing reliance on sensitive real-world data.",
      "keywords": [
        "Cancer gene classification",
        "Data augmentation",
        "Generative Adversarial Networks (GANs)",
        "Synthetic cancer data",
        "Data scarcity",
        "Improved classification performance",
        "Medical AI",
        "Model generalization",
        "High-fidelity synthetic data",
        "Enhance model training",
        "Diagnosis and treatment",
        "Medical data synthesis"
      ],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/162e5a6c7fa9236be27966fd32b8e8f3819fc5b6.pdf",
      "citation_key": "wei2021qea",
      "metadata": {
        "title": "Cancer classification with data augmentation based on generative adversarial networks",
        "authors": [
          "Kaimin Wei",
          "Tianqi Li",
          "Feiran Huang",
          "Jinpeng Chen",
          "Zefan He"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/162e5a6c7fa9236be27966fd32b8e8f3819fc5b6.pdf",
        "venue": "Frontiers of Computer Science",
        "citationCount": 32,
        "score": 8.0,
        "summary": "Here is a focused summary of the technical paper for literature review, adhering to your requirements:\n\n### Technical Paper Analysis: Cancer classification with data augmentation based on generative adversarial networks \\cite{wei2021qea}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** The paper addresses the challenge of cancer gene classification.\n    *   **Importance and challenge:** This problem is critical for diagnosis and treatment, but its effectiveness is significantly weakened by the inadequacy of available cancer data \\cite{wei2021qea}. Data scarcity leads to poor generalization and performance of classification models.\n\n2.  **Related Work & Positioning**\n    *   The provided abstract does not explicitly detail how this work relates to existing approaches or the limitations of previous solutions beyond the general problem of inadequate data. It positions itself as a solution to this data scarcity issue.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method:** The paper proposes a Cancer Classification Model that incorporates data augmentation. The core of this augmentation strategy is the use of Generative Adversarial Networks (GANs) to create synthetic cancer data \\cite{wei2021qea}.\n    *   **Novelty:** The innovation lies in leveraging GANs to synthesize data that is \"highly similar to the real one\" \\cite{wei2021qea}, specifically for the purpose of augmenting limited cancer datasets. This generated data is then used to improve the cancer classification results.\n\n4.  **Key Technical Contributions**\n    *   **Novel methods/techniques:** The primary contribution is the application of GANs to generate realistic synthetic cancer data, directly addressing the problem of data inadequacy in cancer classification \\cite{wei2021qea}.\n    *   **Improved performance:** The work demonstrates an improvement in cancer classification performance by integrating this GAN-based data augmentation into the classification pipeline \\cite{wei2021qea}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:** The provided text indicates that the generated data is used to \"improve the cancer classification results\" \\cite{wei2021qea}, implying experimental validation of the classification performance with and without augmentation. However, specific details about the experimental setup, datasets, or methodologies are not provided in this snippet.\n    *   **Key performance metrics and comparison results:** The paper claims to \"improve the cancer classification performance\" \\cite{wei2021qea}. Specific metrics (e.g., accuracy, precision, recall, F1-score) or comparative results against baseline methods are not detailed in the provided content.\n\n6.  **Limitations & Scope**\n    *   The provided abstract does not explicitly state technical limitations, assumptions, or the specific scope of applicability beyond cancer gene classification.\n\n7.  **Technical Significance**\n    *   **Advance the technical state-of-the-art:** This work advances the state-of-the-art by offering a practical solution to the pervasive problem of data scarcity in medical image and gene classification tasks, specifically for cancer. By demonstrating the effectiveness of GANs for generating high-fidelity synthetic cancer data, it provides a robust method to enhance model training and generalization.\n    *   **Potential impact on future research:** The approach has significant potential to impact future research in medical AI, particularly in areas where data collection is challenging or expensive. It opens avenues for exploring more sophisticated GAN architectures for medical data synthesis, investigating the optimal integration of synthetic data into various machine learning models, and potentially reducing the reliance on large, privacy-sensitive real datasets for initial model development.",
        "keywords": [
          "Cancer gene classification",
          "Data augmentation",
          "Generative Adversarial Networks (GANs)",
          "Synthetic cancer data",
          "Data scarcity",
          "Improved classification performance",
          "Medical AI",
          "Model generalization",
          "High-fidelity synthetic data",
          "Enhance model training",
          "Diagnosis and treatment",
          "Medical data synthesis"
        ],
        "paper_type": "based on the abstract provided:\n\n*   the abstract clearly identifies a **problem**: \"the inadequate data weakens the cancer classification effect.\"\n*   it then proposes **ideas/solutions**: \"a cancer classification model with data augmentation – gans is used to create cancer data.\" this describes a new method or system.\n*   the **main contributions** are the results of this proposed method: \"create the data highly similar to the real one\" and \"improve the cancer classification performance.\"\n\nthis structure strongly aligns with the **technical** classification criteria:\n*   abstract mentions: \"propose\" (implied by \"ideas: a cancer classification model\"), \"develop\" (implied by \"gans is used to create cancer data\"), \"algorithm\" (gans is an algorithm/method), \"method\" (data augmentation using gans).\n*   introduction discusses: \"technical problem\" (inadequate data for cancer classification), \"proposed solution\" (a new model using gans).\n\nwhile the paper will undoubtedly include empirical results to demonstrate the improved performance, its core contribution as described in the abstract is the **development and presentation of a new method/system** to address a specific technical problem.\n\n**classification: technical**"
      },
      "file_name": "162e5a6c7fa9236be27966fd32b8e8f3819fc5b6.pdf"
    },
    {
      "success": true,
      "doc_id": "ade4eed5ca3a1cfd979cd5f06f54946e",
      "summary": "We propose the Margin Adaptation for Generative Adversarial Networks (MAGANs) algorithm, a novel training procedure for GANs to improve stability and performance by using an adaptive hinge loss function. We estimate the appropriate hinge loss margin with the expected energy of the target distribution, and derive principled criteria for when to update the margin. We prove that our method converges to its global optimum under certain assumptions. Evaluated on the task of unsupervised image generation, the proposed training procedure is simple yet robust on a diverse set of data, and achieves qualitative and quantitative improvements compared to the state-of-the-art.",
      "intriguing_abstract": "We propose the Margin Adaptation for Generative Adversarial Networks (MAGANs) algorithm, a novel training procedure for GANs to improve stability and performance by using an adaptive hinge loss function. We estimate the appropriate hinge loss margin with the expected energy of the target distribution, and derive principled criteria for when to update the margin. We prove that our method converges to its global optimum under certain assumptions. Evaluated on the task of unsupervised image generation, the proposed training procedure is simple yet robust on a diverse set of data, and achieves qualitative and quantitative improvements compared to the state-of-the-art.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/ad37850e17ce5ad9ca954ec97c1fe95c31cefc9e.pdf",
      "citation_key": "wang20178xf",
      "metadata": {
        "title": "MAGAN: Margin Adaptation for Generative Adversarial Networks",
        "authors": [
          "Ruohan Wang",
          "Antoine Cully",
          "H. Chang",
          "Y. Demiris"
        ],
        "published_date": "2017",
        "abstract": "We propose the Margin Adaptation for Generative Adversarial Networks (MAGANs) algorithm, a novel training procedure for GANs to improve stability and performance by using an adaptive hinge loss function. We estimate the appropriate hinge loss margin with the expected energy of the target distribution, and derive principled criteria for when to update the margin. We prove that our method converges to its global optimum under certain assumptions. Evaluated on the task of unsupervised image generation, the proposed training procedure is simple yet robust on a diverse set of data, and achieves qualitative and quantitative improvements compared to the state-of-the-art.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/ad37850e17ce5ad9ca954ec97c1fe95c31cefc9e.pdf",
        "venue": "arXiv.org",
        "citationCount": 64,
        "score": 8.0,
        "summary": "We propose the Margin Adaptation for Generative Adversarial Networks (MAGANs) algorithm, a novel training procedure for GANs to improve stability and performance by using an adaptive hinge loss function. We estimate the appropriate hinge loss margin with the expected energy of the target distribution, and derive principled criteria for when to update the margin. We prove that our method converges to its global optimum under certain assumptions. Evaluated on the task of unsupervised image generation, the proposed training procedure is simple yet robust on a diverse set of data, and achieves qualitative and quantitative improvements compared to the state-of-the-art.",
        "keywords": []
      },
      "file_name": "ad37850e17ce5ad9ca954ec97c1fe95c31cefc9e.pdf"
    },
    {
      "success": true,
      "doc_id": "4d075833c70bdbd06ea7a44ae8750e7e",
      "summary": "Designing a logo for a new brand is a lengthy and tedious back-and-forth process between a designer and a client. In this paper we explore to what extent machine learning can solve the creative task of the designer. For this, we build a dataset - LLD - of 600k+ logos crawled from the world wide web. Training Generative Adversarial Networks (GANs) for logo synthesis on such multi-modal data is not straightforward and results in mode collapse for some state-of-the-art methods. We propose the use of synthetic labels obtained through clustering to disentangle and stabilize GAN training, and validate this approach on CIFAR-10 and ImageNet-small to demonstrate its generality. We are able to generate a high diversity of plausible logos and demonstrate latent space exploration techniques to ease the logo design task in an interactive manner. GANs can cope with multi-modal data by means of synthetic labels achieved through clustering, and our results show the creative potential of such techniques for logo synthesis and manipulation. Our dataset and models are publicly available at https://data.vision.ee.ethz.ch/sagea/lld/.",
      "intriguing_abstract": "Designing a logo for a new brand is a lengthy and tedious back-and-forth process between a designer and a client. In this paper we explore to what extent machine learning can solve the creative task of the designer. For this, we build a dataset - LLD - of 600k+ logos crawled from the world wide web. Training Generative Adversarial Networks (GANs) for logo synthesis on such multi-modal data is not straightforward and results in mode collapse for some state-of-the-art methods. We propose the use of synthetic labels obtained through clustering to disentangle and stabilize GAN training, and validate this approach on CIFAR-10 and ImageNet-small to demonstrate its generality. We are able to generate a high diversity of plausible logos and demonstrate latent space exploration techniques to ease the logo design task in an interactive manner. GANs can cope with multi-modal data by means of synthetic labels achieved through clustering, and our results show the creative potential of such techniques for logo synthesis and manipulation. Our dataset and models are publicly available at https://data.vision.ee.ethz.ch/sagea/lld/.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/30831a581be8b76a99ef079f82e3c1b5f8c2dc05.pdf",
      "citation_key": "sage2017ywd",
      "metadata": {
        "title": "Logo Synthesis and Manipulation with Clustered Generative Adversarial Networks",
        "authors": [
          "Alexander Sage",
          "E. Agustsson",
          "R. Timofte",
          "L. Gool"
        ],
        "published_date": "2017",
        "abstract": "Designing a logo for a new brand is a lengthy and tedious back-and-forth process between a designer and a client. In this paper we explore to what extent machine learning can solve the creative task of the designer. For this, we build a dataset - LLD - of 600k+ logos crawled from the world wide web. Training Generative Adversarial Networks (GANs) for logo synthesis on such multi-modal data is not straightforward and results in mode collapse for some state-of-the-art methods. We propose the use of synthetic labels obtained through clustering to disentangle and stabilize GAN training, and validate this approach on CIFAR-10 and ImageNet-small to demonstrate its generality. We are able to generate a high diversity of plausible logos and demonstrate latent space exploration techniques to ease the logo design task in an interactive manner. GANs can cope with multi-modal data by means of synthetic labels achieved through clustering, and our results show the creative potential of such techniques for logo synthesis and manipulation. Our dataset and models are publicly available at https://data.vision.ee.ethz.ch/sagea/lld/.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/30831a581be8b76a99ef079f82e3c1b5f8c2dc05.pdf",
        "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
        "citationCount": 62,
        "score": 7.75,
        "summary": "Designing a logo for a new brand is a lengthy and tedious back-and-forth process between a designer and a client. In this paper we explore to what extent machine learning can solve the creative task of the designer. For this, we build a dataset - LLD - of 600k+ logos crawled from the world wide web. Training Generative Adversarial Networks (GANs) for logo synthesis on such multi-modal data is not straightforward and results in mode collapse for some state-of-the-art methods. We propose the use of synthetic labels obtained through clustering to disentangle and stabilize GAN training, and validate this approach on CIFAR-10 and ImageNet-small to demonstrate its generality. We are able to generate a high diversity of plausible logos and demonstrate latent space exploration techniques to ease the logo design task in an interactive manner. GANs can cope with multi-modal data by means of synthetic labels achieved through clustering, and our results show the creative potential of such techniques for logo synthesis and manipulation. Our dataset and models are publicly available at https://data.vision.ee.ethz.ch/sagea/lld/.",
        "keywords": []
      },
      "file_name": "30831a581be8b76a99ef079f82e3c1b5f8c2dc05.pdf"
    },
    {
      "success": true,
      "doc_id": "3776b4aa1da9640a549811221c1e61d2",
      "summary": "Despite success on a wide range of problems related to vision, generative adversarial networks (GANs) can suffer from inferior performance due to unstable training, especially for text generation. We propose a new variational GAN training framework which enjoys superior training stability. Our approach is inspired by a connection of GANs and reinforcement learning under a variational perspective. The connection leads to (1) probability ratio clipping that regularizes generator training to prevent excessively large updates, and (2) a sample re-weighting mechanism that stabilizes discriminator training by downplaying bad-quality fake samples. We provide theoretical analysis on the convergence of our approach. By plugging the training approach in diverse state-of-the-art GAN architectures, we obtain significantly improved performance over a range of tasks, including text generation, text style transfer, and image generation.",
      "intriguing_abstract": "Despite success on a wide range of problems related to vision, generative adversarial networks (GANs) can suffer from inferior performance due to unstable training, especially for text generation. We propose a new variational GAN training framework which enjoys superior training stability. Our approach is inspired by a connection of GANs and reinforcement learning under a variational perspective. The connection leads to (1) probability ratio clipping that regularizes generator training to prevent excessively large updates, and (2) a sample re-weighting mechanism that stabilizes discriminator training by downplaying bad-quality fake samples. We provide theoretical analysis on the convergence of our approach. By plugging the training approach in diverse state-of-the-art GAN architectures, we obtain significantly improved performance over a range of tasks, including text generation, text style transfer, and image generation.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/0984634505e7b4a8004eaf26416ffedd81cd5861.pdf",
      "citation_key": "wu2020p8p",
      "metadata": {
        "title": "Improving GAN Training with Probability Ratio Clipping and Sample Reweighting",
        "authors": [
          "Yue Wu",
          "Pan Zhou",
          "A. Wilson",
          "E. Xing",
          "Zhiting Hu"
        ],
        "published_date": "2020",
        "abstract": "Despite success on a wide range of problems related to vision, generative adversarial networks (GANs) can suffer from inferior performance due to unstable training, especially for text generation. We propose a new variational GAN training framework which enjoys superior training stability. Our approach is inspired by a connection of GANs and reinforcement learning under a variational perspective. The connection leads to (1) probability ratio clipping that regularizes generator training to prevent excessively large updates, and (2) a sample re-weighting mechanism that stabilizes discriminator training by downplaying bad-quality fake samples. We provide theoretical analysis on the convergence of our approach. By plugging the training approach in diverse state-of-the-art GAN architectures, we obtain significantly improved performance over a range of tasks, including text generation, text style transfer, and image generation.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/0984634505e7b4a8004eaf26416ffedd81cd5861.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 36,
        "score": 7.2,
        "summary": "Despite success on a wide range of problems related to vision, generative adversarial networks (GANs) can suffer from inferior performance due to unstable training, especially for text generation. We propose a new variational GAN training framework which enjoys superior training stability. Our approach is inspired by a connection of GANs and reinforcement learning under a variational perspective. The connection leads to (1) probability ratio clipping that regularizes generator training to prevent excessively large updates, and (2) a sample re-weighting mechanism that stabilizes discriminator training by downplaying bad-quality fake samples. We provide theoretical analysis on the convergence of our approach. By plugging the training approach in diverse state-of-the-art GAN architectures, we obtain significantly improved performance over a range of tasks, including text generation, text style transfer, and image generation.",
        "keywords": []
      },
      "file_name": "0984634505e7b4a8004eaf26416ffedd81cd5861.pdf"
    },
    {
      "success": true,
      "doc_id": "a1504846d9a6723fe19dfea1f98f5b0f",
      "summary": "The Generative Adversarial Networks (GANs) have demonstrated impressive performance for data synthesis, and are now used in a wide range of computer vision tasks. In spite of this success, they gained a reputation for being difficult to train, what results in a time-consuming and human-involved development process to use them. We consider an alternative training process, named SGAN, in which several adversarial \"local\" pairs of networks are trained independently so that a \"global\" supervising pair of networks can be trained against them. The goal is to train the global pair with the corresponding ensemble opponent for improved performances in terms of mode coverage. This approach aims at increasing the chances that learning will not stop for the global pair, preventing both to be trapped in an unsatisfactory local minimum, or to face oscillations often observed in practice. To guarantee the latter, the global pair never affects the local ones. The rules of SGAN training are thus as follows: the global generator and discriminator are trained using the local discriminators and generators, respectively, whereas the local networks are trained with their fixed local opponent. Experimental results on both toy and real-world problems demonstrate that this approach outperforms standard training in terms of better mitigating mode collapse, stability while converging and that it surprisingly, increases the convergence speed as well.",
      "intriguing_abstract": "The Generative Adversarial Networks (GANs) have demonstrated impressive performance for data synthesis, and are now used in a wide range of computer vision tasks. In spite of this success, they gained a reputation for being difficult to train, what results in a time-consuming and human-involved development process to use them. We consider an alternative training process, named SGAN, in which several adversarial \"local\" pairs of networks are trained independently so that a \"global\" supervising pair of networks can be trained against them. The goal is to train the global pair with the corresponding ensemble opponent for improved performances in terms of mode coverage. This approach aims at increasing the chances that learning will not stop for the global pair, preventing both to be trapped in an unsatisfactory local minimum, or to face oscillations often observed in practice. To guarantee the latter, the global pair never affects the local ones. The rules of SGAN training are thus as follows: the global generator and discriminator are trained using the local discriminators and generators, respectively, whereas the local networks are trained with their fixed local opponent. Experimental results on both toy and real-world problems demonstrate that this approach outperforms standard training in terms of better mitigating mode collapse, stability while converging and that it surprisingly, increases the convergence speed as well.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/fc62dad5dc03f7b2a6d9b8e7d3934108f4b511f8.pdf",
      "citation_key": "chavdarova20179w6",
      "metadata": {
        "title": "SGAN: An Alternative Training of Generative Adversarial Networks",
        "authors": [
          "Tatjana Chavdarova",
          "F. Fleuret"
        ],
        "published_date": "2017",
        "abstract": "The Generative Adversarial Networks (GANs) have demonstrated impressive performance for data synthesis, and are now used in a wide range of computer vision tasks. In spite of this success, they gained a reputation for being difficult to train, what results in a time-consuming and human-involved development process to use them. We consider an alternative training process, named SGAN, in which several adversarial \"local\" pairs of networks are trained independently so that a \"global\" supervising pair of networks can be trained against them. The goal is to train the global pair with the corresponding ensemble opponent for improved performances in terms of mode coverage. This approach aims at increasing the chances that learning will not stop for the global pair, preventing both to be trapped in an unsatisfactory local minimum, or to face oscillations often observed in practice. To guarantee the latter, the global pair never affects the local ones. The rules of SGAN training are thus as follows: the global generator and discriminator are trained using the local discriminators and generators, respectively, whereas the local networks are trained with their fixed local opponent. Experimental results on both toy and real-world problems demonstrate that this approach outperforms standard training in terms of better mitigating mode collapse, stability while converging and that it surprisingly, increases the convergence speed as well.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/fc62dad5dc03f7b2a6d9b8e7d3934108f4b511f8.pdf",
        "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
        "citationCount": 57,
        "score": 7.125,
        "summary": "The Generative Adversarial Networks (GANs) have demonstrated impressive performance for data synthesis, and are now used in a wide range of computer vision tasks. In spite of this success, they gained a reputation for being difficult to train, what results in a time-consuming and human-involved development process to use them. We consider an alternative training process, named SGAN, in which several adversarial \"local\" pairs of networks are trained independently so that a \"global\" supervising pair of networks can be trained against them. The goal is to train the global pair with the corresponding ensemble opponent for improved performances in terms of mode coverage. This approach aims at increasing the chances that learning will not stop for the global pair, preventing both to be trapped in an unsatisfactory local minimum, or to face oscillations often observed in practice. To guarantee the latter, the global pair never affects the local ones. The rules of SGAN training are thus as follows: the global generator and discriminator are trained using the local discriminators and generators, respectively, whereas the local networks are trained with their fixed local opponent. Experimental results on both toy and real-world problems demonstrate that this approach outperforms standard training in terms of better mitigating mode collapse, stability while converging and that it surprisingly, increases the convergence speed as well.",
        "keywords": []
      },
      "file_name": "fc62dad5dc03f7b2a6d9b8e7d3934108f4b511f8.pdf"
    },
    {
      "success": true,
      "doc_id": "ba1361cf7cbefde43d3534ca0832a7df",
      "summary": "Generative Adversarial Networks (GANs) are the most popular image generation models that have achieved remarkable progress on various computer vision tasks. However, training instability is still one of the open problems for all GAN-based algorithms. Quite a number of methods have been proposed to stabilize the training of GANs, the focuses of which were respectively put on the loss functions, regularization and normalization technologies, training algorithms, and model architectures. Different from the above methods, in this paper, a new perspective on stabilizing GANs training is presented. It is found that sometimes the images produced by the generator act like adversarial examples of the discriminator during the training process, which may be part of the reason causing the unstable training of GANs. With this finding, we propose the Direct Adversarial Training (DAT) method to stabilize the training process of GANs. Furthermore, we prove that the DAT method is able to minimize the Lipschitz constant of the discriminator adaptively. The advanced performance of DAT is verified on multiple loss functions, network architectures, hyper-parameters, and datasets. Specifically, DAT achieves significant improvements of 11.5% FID on CIFAR-100 unconditional generation based on SSGAN, 10.5% FID on STL-10 unconditional generation based on SSGAN, and 13.2% FID on LSUN-Bedroom unconditional generation based on SSGAN.",
      "intriguing_abstract": "Generative Adversarial Networks (GANs) are the most popular image generation models that have achieved remarkable progress on various computer vision tasks. However, training instability is still one of the open problems for all GAN-based algorithms. Quite a number of methods have been proposed to stabilize the training of GANs, the focuses of which were respectively put on the loss functions, regularization and normalization technologies, training algorithms, and model architectures. Different from the above methods, in this paper, a new perspective on stabilizing GANs training is presented. It is found that sometimes the images produced by the generator act like adversarial examples of the discriminator during the training process, which may be part of the reason causing the unstable training of GANs. With this finding, we propose the Direct Adversarial Training (DAT) method to stabilize the training process of GANs. Furthermore, we prove that the DAT method is able to minimize the Lipschitz constant of the discriminator adaptively. The advanced performance of DAT is verified on multiple loss functions, network architectures, hyper-parameters, and datasets. Specifically, DAT achieves significant improvements of 11.5% FID on CIFAR-100 unconditional generation based on SSGAN, 10.5% FID on STL-10 unconditional generation based on SSGAN, and 13.2% FID on LSUN-Bedroom unconditional generation based on SSGAN.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/32038e56d0174b33a93c66258f346c1a173fe81d.pdf",
      "citation_key": "li2020muy",
      "metadata": {
        "title": "A New Perspective on Stabilizing GANs Training: Direct Adversarial Training",
        "authors": [
          "Ziqiang Li",
          "Pengfei Xia",
          "Rentuo Tao",
          "Hongjing Niu",
          "Bin Li"
        ],
        "published_date": "2020",
        "abstract": "Generative Adversarial Networks (GANs) are the most popular image generation models that have achieved remarkable progress on various computer vision tasks. However, training instability is still one of the open problems for all GAN-based algorithms. Quite a number of methods have been proposed to stabilize the training of GANs, the focuses of which were respectively put on the loss functions, regularization and normalization technologies, training algorithms, and model architectures. Different from the above methods, in this paper, a new perspective on stabilizing GANs training is presented. It is found that sometimes the images produced by the generator act like adversarial examples of the discriminator during the training process, which may be part of the reason causing the unstable training of GANs. With this finding, we propose the Direct Adversarial Training (DAT) method to stabilize the training process of GANs. Furthermore, we prove that the DAT method is able to minimize the Lipschitz constant of the discriminator adaptively. The advanced performance of DAT is verified on multiple loss functions, network architectures, hyper-parameters, and datasets. Specifically, DAT achieves significant improvements of 11.5% FID on CIFAR-100 unconditional generation based on SSGAN, 10.5% FID on STL-10 unconditional generation based on SSGAN, and 13.2% FID on LSUN-Bedroom unconditional generation based on SSGAN.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/32038e56d0174b33a93c66258f346c1a173fe81d.pdf",
        "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
        "citationCount": 35,
        "score": 7.0,
        "summary": "Generative Adversarial Networks (GANs) are the most popular image generation models that have achieved remarkable progress on various computer vision tasks. However, training instability is still one of the open problems for all GAN-based algorithms. Quite a number of methods have been proposed to stabilize the training of GANs, the focuses of which were respectively put on the loss functions, regularization and normalization technologies, training algorithms, and model architectures. Different from the above methods, in this paper, a new perspective on stabilizing GANs training is presented. It is found that sometimes the images produced by the generator act like adversarial examples of the discriminator during the training process, which may be part of the reason causing the unstable training of GANs. With this finding, we propose the Direct Adversarial Training (DAT) method to stabilize the training process of GANs. Furthermore, we prove that the DAT method is able to minimize the Lipschitz constant of the discriminator adaptively. The advanced performance of DAT is verified on multiple loss functions, network architectures, hyper-parameters, and datasets. Specifically, DAT achieves significant improvements of 11.5% FID on CIFAR-100 unconditional generation based on SSGAN, 10.5% FID on STL-10 unconditional generation based on SSGAN, and 13.2% FID on LSUN-Bedroom unconditional generation based on SSGAN.",
        "keywords": []
      },
      "file_name": "32038e56d0174b33a93c66258f346c1a173fe81d.pdf"
    },
    {
      "success": true,
      "doc_id": "19852183cc12cd9022af9af1b4c46295",
      "summary": "In conventional ultrasound (US) imaging, it is common to transmit several focused beams at multiple locations to generate a multi-focus image with constant lateral resolution throughout the image. However, this method comes at the expense of a loss in temporal resolution, which is important in applications requiring both high-frame rate and constant lateral resolution. Moreover, relative motions of the target with respect to the probe often exist due to hand tremors or biological motions, causing blurring artifacts in the multi-focus image. This article introduces a novel approach for multi-focus US image recovery based on Generative Adversarial Network (GAN) without a reduction in the frame-rate. Herein, a mapping function between the single-focus US image and multi-focus version for having a constant lateral resolution everywhere is estimated through different GANs. We use adversarial loss functions in addition to Mean Square Error (MSE) to generate more realistic ultrasound images. Moreover, we use the boundary seeking method for improving the stability of training, which is currently the main challenge in using GANs. Experiments on simulated and real phantoms as well as on ex vivo data are performed. Results confirm that having both adversarial loss function and boundary seeking training provides better results in terms of the mean opinion score test. Furthermore, the proposed method enhances the resolution and contrast indexes without sacrificing the frame-rate. As for the comparison with other approaches which are not based on NNs, the proposed approach gives similar results while requiring neither channel data nor computationally expensive algorithms.",
      "intriguing_abstract": "In conventional ultrasound (US) imaging, it is common to transmit several focused beams at multiple locations to generate a multi-focus image with constant lateral resolution throughout the image. However, this method comes at the expense of a loss in temporal resolution, which is important in applications requiring both high-frame rate and constant lateral resolution. Moreover, relative motions of the target with respect to the probe often exist due to hand tremors or biological motions, causing blurring artifacts in the multi-focus image. This article introduces a novel approach for multi-focus US image recovery based on Generative Adversarial Network (GAN) without a reduction in the frame-rate. Herein, a mapping function between the single-focus US image and multi-focus version for having a constant lateral resolution everywhere is estimated through different GANs. We use adversarial loss functions in addition to Mean Square Error (MSE) to generate more realistic ultrasound images. Moreover, we use the boundary seeking method for improving the stability of training, which is currently the main challenge in using GANs. Experiments on simulated and real phantoms as well as on ex vivo data are performed. Results confirm that having both adversarial loss function and boundary seeking training provides better results in terms of the mean opinion score test. Furthermore, the proposed method enhances the resolution and contrast indexes without sacrificing the frame-rate. As for the comparison with other approaches which are not based on NNs, the proposed approach gives similar results while requiring neither channel data nor computationally expensive algorithms.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/bffb2fe8b60d7acd307f28ff04b1f3f486511639.pdf",
      "citation_key": "goudarzi2020ymw",
      "metadata": {
        "title": "Fast Multi-Focus Ultrasound Image Recovery Using Generative Adversarial Networks",
        "authors": [
          "Sobhan Goudarzi",
          "A. Asif",
          "H. Rivaz"
        ],
        "published_date": "2020",
        "abstract": "In conventional ultrasound (US) imaging, it is common to transmit several focused beams at multiple locations to generate a multi-focus image with constant lateral resolution throughout the image. However, this method comes at the expense of a loss in temporal resolution, which is important in applications requiring both high-frame rate and constant lateral resolution. Moreover, relative motions of the target with respect to the probe often exist due to hand tremors or biological motions, causing blurring artifacts in the multi-focus image. This article introduces a novel approach for multi-focus US image recovery based on Generative Adversarial Network (GAN) without a reduction in the frame-rate. Herein, a mapping function between the single-focus US image and multi-focus version for having a constant lateral resolution everywhere is estimated through different GANs. We use adversarial loss functions in addition to Mean Square Error (MSE) to generate more realistic ultrasound images. Moreover, we use the boundary seeking method for improving the stability of training, which is currently the main challenge in using GANs. Experiments on simulated and real phantoms as well as on ex vivo data are performed. Results confirm that having both adversarial loss function and boundary seeking training provides better results in terms of the mean opinion score test. Furthermore, the proposed method enhances the resolution and contrast indexes without sacrificing the frame-rate. As for the comparison with other approaches which are not based on NNs, the proposed approach gives similar results while requiring neither channel data nor computationally expensive algorithms.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/bffb2fe8b60d7acd307f28ff04b1f3f486511639.pdf",
        "venue": "IEEE Transactions on Computational Imaging",
        "citationCount": 35,
        "score": 7.0,
        "summary": "In conventional ultrasound (US) imaging, it is common to transmit several focused beams at multiple locations to generate a multi-focus image with constant lateral resolution throughout the image. However, this method comes at the expense of a loss in temporal resolution, which is important in applications requiring both high-frame rate and constant lateral resolution. Moreover, relative motions of the target with respect to the probe often exist due to hand tremors or biological motions, causing blurring artifacts in the multi-focus image. This article introduces a novel approach for multi-focus US image recovery based on Generative Adversarial Network (GAN) without a reduction in the frame-rate. Herein, a mapping function between the single-focus US image and multi-focus version for having a constant lateral resolution everywhere is estimated through different GANs. We use adversarial loss functions in addition to Mean Square Error (MSE) to generate more realistic ultrasound images. Moreover, we use the boundary seeking method for improving the stability of training, which is currently the main challenge in using GANs. Experiments on simulated and real phantoms as well as on ex vivo data are performed. Results confirm that having both adversarial loss function and boundary seeking training provides better results in terms of the mean opinion score test. Furthermore, the proposed method enhances the resolution and contrast indexes without sacrificing the frame-rate. As for the comparison with other approaches which are not based on NNs, the proposed approach gives similar results while requiring neither channel data nor computationally expensive algorithms.",
        "keywords": []
      },
      "file_name": "bffb2fe8b60d7acd307f28ff04b1f3f486511639.pdf"
    },
    {
      "success": true,
      "doc_id": "1ab0ad48969bda8bb745cd1cbd3393a3",
      "summary": "As part of the transportation system electrification and decarbonization, electric vehicles (EVs) have been experiencing a steady growth in some countries in recent years. However, the uncoordinated charging of EVs brings negative effects to the power grid. Therefore, a new intelligent energy management strategy needs to be involved in the transportation electrification progress to address the variability of flexible loads and energy storage in the active distribution network. In this article, we have proposed a coordinated dispatch strategy of EVs and thermostatically controlled loads (TCLs) based on a modified generative adversarial network (GAN). TCLs are utilized to complement the limits of EVs’ driving behaviors. EVs are modeled as battery energy storage systems (BESSs), and TCLs are modeled as virtual energy storage systems (VESSs). Machine learning is integrated into a bilevel optimization problem to determine the steady-state power dispatch and the energy storage control of the VESS. The proposed method is verified on the IEEE 33-bus system. Based on the simulation results, it can be concluded that the proposed data-driven method can outperform the conventional model-based method in terms of accuracy. Also, the modified GAN helps the training process to be less affected by the missing data. Comparative studies are conducted to show that the coordinated dispatch strategy of EVs and TCLs can maintain the voltage stability in the distribution system by compensating for the drawbacks of each other.",
      "intriguing_abstract": "As part of the transportation system electrification and decarbonization, electric vehicles (EVs) have been experiencing a steady growth in some countries in recent years. However, the uncoordinated charging of EVs brings negative effects to the power grid. Therefore, a new intelligent energy management strategy needs to be involved in the transportation electrification progress to address the variability of flexible loads and energy storage in the active distribution network. In this article, we have proposed a coordinated dispatch strategy of EVs and thermostatically controlled loads (TCLs) based on a modified generative adversarial network (GAN). TCLs are utilized to complement the limits of EVs’ driving behaviors. EVs are modeled as battery energy storage systems (BESSs), and TCLs are modeled as virtual energy storage systems (VESSs). Machine learning is integrated into a bilevel optimization problem to determine the steady-state power dispatch and the energy storage control of the VESS. The proposed method is verified on the IEEE 33-bus system. Based on the simulation results, it can be concluded that the proposed data-driven method can outperform the conventional model-based method in terms of accuracy. Also, the modified GAN helps the training process to be less affected by the missing data. Comparative studies are conducted to show that the coordinated dispatch strategy of EVs and TCLs can maintain the voltage stability in the distribution system by compensating for the drawbacks of each other.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/79ffafec2133a6216e367fa52b28ee4bd4f685bf.pdf",
      "citation_key": "tao20219q2",
      "metadata": {
        "title": "A Data-Driven Management Strategy of Electric Vehicles and Thermostatically Controlled Loads Based on Modified Generative Adversarial Network",
        "authors": [
          "Yuechuan Tao",
          "J. Qiu",
          "Shuying Lai"
        ],
        "published_date": "2021",
        "abstract": "As part of the transportation system electrification and decarbonization, electric vehicles (EVs) have been experiencing a steady growth in some countries in recent years. However, the uncoordinated charging of EVs brings negative effects to the power grid. Therefore, a new intelligent energy management strategy needs to be involved in the transportation electrification progress to address the variability of flexible loads and energy storage in the active distribution network. In this article, we have proposed a coordinated dispatch strategy of EVs and thermostatically controlled loads (TCLs) based on a modified generative adversarial network (GAN). TCLs are utilized to complement the limits of EVs’ driving behaviors. EVs are modeled as battery energy storage systems (BESSs), and TCLs are modeled as virtual energy storage systems (VESSs). Machine learning is integrated into a bilevel optimization problem to determine the steady-state power dispatch and the energy storage control of the VESS. The proposed method is verified on the IEEE 33-bus system. Based on the simulation results, it can be concluded that the proposed data-driven method can outperform the conventional model-based method in terms of accuracy. Also, the modified GAN helps the training process to be less affected by the missing data. Comparative studies are conducted to show that the coordinated dispatch strategy of EVs and TCLs can maintain the voltage stability in the distribution system by compensating for the drawbacks of each other.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/79ffafec2133a6216e367fa52b28ee4bd4f685bf.pdf",
        "venue": "IEEE Transactions on Transportation Electrification",
        "citationCount": 28,
        "score": 7.0,
        "summary": "As part of the transportation system electrification and decarbonization, electric vehicles (EVs) have been experiencing a steady growth in some countries in recent years. However, the uncoordinated charging of EVs brings negative effects to the power grid. Therefore, a new intelligent energy management strategy needs to be involved in the transportation electrification progress to address the variability of flexible loads and energy storage in the active distribution network. In this article, we have proposed a coordinated dispatch strategy of EVs and thermostatically controlled loads (TCLs) based on a modified generative adversarial network (GAN). TCLs are utilized to complement the limits of EVs’ driving behaviors. EVs are modeled as battery energy storage systems (BESSs), and TCLs are modeled as virtual energy storage systems (VESSs). Machine learning is integrated into a bilevel optimization problem to determine the steady-state power dispatch and the energy storage control of the VESS. The proposed method is verified on the IEEE 33-bus system. Based on the simulation results, it can be concluded that the proposed data-driven method can outperform the conventional model-based method in terms of accuracy. Also, the modified GAN helps the training process to be less affected by the missing data. Comparative studies are conducted to show that the coordinated dispatch strategy of EVs and TCLs can maintain the voltage stability in the distribution system by compensating for the drawbacks of each other.",
        "keywords": []
      },
      "file_name": "79ffafec2133a6216e367fa52b28ee4bd4f685bf.pdf"
    },
    {
      "success": true,
      "doc_id": "2a416c6bc07981b860f232bbffddb89b",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/bd043bc99d8859614fda7efb49d71beec36b54f3.pdf",
      "citation_key": "zhong2019opk",
      "metadata": {
        "title": "A generative adversarial network for image denoising",
        "authors": [
          "Yue Zhong",
          "Lizhuang Liu",
          "Dan Zhao",
          "Hongyang Li"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/bd043bc99d8859614fda7efb49d71beec36b54f3.pdf",
        "venue": "Multimedia tools and applications",
        "citationCount": 41,
        "score": 6.833333333333333,
        "summary": "",
        "keywords": []
      },
      "file_name": "bd043bc99d8859614fda7efb49d71beec36b54f3.pdf"
    },
    {
      "success": true,
      "doc_id": "501d3299029fd5d13ddfda05fc20844b",
      "summary": "In the existing studies on remote sensing image scene classification, the supervised learning methods which are fine-tuned from pre-trained model require a large amount of labeled training data and parameters, while unsupervised learning methods do not make full use of label information, and the classification performance could be improved. In this paper, we introduced semi-supervised learning into generative adversarial network (GAN), so the discriminator learned more discriminative features from labeled data and unlabeled data. Moreover, the mixup data augmentation method was introduced into our classification model to augment the data and stabilized the training process. We carried out extensive experiments for both UC-Merced and NWPU-RESISC45 datasets with a 5-fold cross-validation protocol using a linear SVM as classifier. We trained the proposed method on UC-Merced dataset and achieve an average overall accuracy of 94.05% under 80% training ratio. When trained on NWPU-RESISC45 dataset, the proposed method reached an average overall accuracy of 83.12% and 92.78% under the training ratios of 20% and 80% respectively, which achieves the state-of-the-art deep learning methods without pre-training.",
      "intriguing_abstract": "In the existing studies on remote sensing image scene classification, the supervised learning methods which are fine-tuned from pre-trained model require a large amount of labeled training data and parameters, while unsupervised learning methods do not make full use of label information, and the classification performance could be improved. In this paper, we introduced semi-supervised learning into generative adversarial network (GAN), so the discriminator learned more discriminative features from labeled data and unlabeled data. Moreover, the mixup data augmentation method was introduced into our classification model to augment the data and stabilized the training process. We carried out extensive experiments for both UC-Merced and NWPU-RESISC45 datasets with a 5-fold cross-validation protocol using a linear SVM as classifier. We trained the proposed method on UC-Merced dataset and achieve an average overall accuracy of 94.05% under 80% training ratio. When trained on NWPU-RESISC45 dataset, the proposed method reached an average overall accuracy of 83.12% and 92.78% under the training ratios of 20% and 80% respectively, which achieves the state-of-the-art deep learning methods without pre-training.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/3466af048d2093786641ec188fc3d5743c831947.pdf",
      "citation_key": "yan2020889",
      "metadata": {
        "title": "Semi-Supervised Representation Learning for Remote Sensing Image Classification Based on Generative Adversarial Networks",
        "authors": [
          "Peiyao Yan",
          "Feng He",
          "Yajie Yang",
          "Fei Hu"
        ],
        "published_date": "2020",
        "abstract": "In the existing studies on remote sensing image scene classification, the supervised learning methods which are fine-tuned from pre-trained model require a large amount of labeled training data and parameters, while unsupervised learning methods do not make full use of label information, and the classification performance could be improved. In this paper, we introduced semi-supervised learning into generative adversarial network (GAN), so the discriminator learned more discriminative features from labeled data and unlabeled data. Moreover, the mixup data augmentation method was introduced into our classification model to augment the data and stabilized the training process. We carried out extensive experiments for both UC-Merced and NWPU-RESISC45 datasets with a 5-fold cross-validation protocol using a linear SVM as classifier. We trained the proposed method on UC-Merced dataset and achieve an average overall accuracy of 94.05% under 80% training ratio. When trained on NWPU-RESISC45 dataset, the proposed method reached an average overall accuracy of 83.12% and 92.78% under the training ratios of 20% and 80% respectively, which achieves the state-of-the-art deep learning methods without pre-training.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/3466af048d2093786641ec188fc3d5743c831947.pdf",
        "venue": "IEEE Access",
        "citationCount": 34,
        "score": 6.800000000000001,
        "summary": "In the existing studies on remote sensing image scene classification, the supervised learning methods which are fine-tuned from pre-trained model require a large amount of labeled training data and parameters, while unsupervised learning methods do not make full use of label information, and the classification performance could be improved. In this paper, we introduced semi-supervised learning into generative adversarial network (GAN), so the discriminator learned more discriminative features from labeled data and unlabeled data. Moreover, the mixup data augmentation method was introduced into our classification model to augment the data and stabilized the training process. We carried out extensive experiments for both UC-Merced and NWPU-RESISC45 datasets with a 5-fold cross-validation protocol using a linear SVM as classifier. We trained the proposed method on UC-Merced dataset and achieve an average overall accuracy of 94.05% under 80% training ratio. When trained on NWPU-RESISC45 dataset, the proposed method reached an average overall accuracy of 83.12% and 92.78% under the training ratios of 20% and 80% respectively, which achieves the state-of-the-art deep learning methods without pre-training.",
        "keywords": []
      },
      "file_name": "3466af048d2093786641ec188fc3d5743c831947.pdf"
    },
    {
      "success": true,
      "doc_id": "bc4a2a25da49b6177e4ace9aefc48be2",
      "summary": "Voice conversion (VC) refers to transforming the speaker characteristics of an utterance without altering its linguistic contents. Many works on voice conversion require to have parallel training data that is highly expensive to acquire. Recently, the cycle-consistent adversarial network (CycleGAN), which does not require parallel training data, has been applied to voice conversion, showing the state-of-the-art performance. The CycleGAN based voice conversion, however, can be used only for a pair of speakers, i.e., one-to-one voice conversion between two speakers. In this paper, we extend the CycleGAN by conditioning the network on speakers. As a result, the proposed method can perform many-to-many voice conversion among multiple speakers using a single generative adversarial network (GAN). Compared to building multiple CycleGANs for each pair of speakers, the proposed method reduces the computational and spatial cost significantly without compromising the sound quality of the converted voice. Experimental results using the VCC2018 corpus confirm the efficiency of the proposed method.",
      "intriguing_abstract": "Voice conversion (VC) refers to transforming the speaker characteristics of an utterance without altering its linguistic contents. Many works on voice conversion require to have parallel training data that is highly expensive to acquire. Recently, the cycle-consistent adversarial network (CycleGAN), which does not require parallel training data, has been applied to voice conversion, showing the state-of-the-art performance. The CycleGAN based voice conversion, however, can be used only for a pair of speakers, i.e., one-to-one voice conversion between two speakers. In this paper, we extend the CycleGAN by conditioning the network on speakers. As a result, the proposed method can perform many-to-many voice conversion among multiple speakers using a single generative adversarial network (GAN). Compared to building multiple CycleGANs for each pair of speakers, the proposed method reduces the computational and spatial cost significantly without compromising the sound quality of the converted voice. Experimental results using the VCC2018 corpus confirm the efficiency of the proposed method.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/8264c9d9e6b91b9c73d0231900e74cd529fbc9d5.pdf",
      "citation_key": "lee20203j4",
      "metadata": {
        "title": "Many-To-Many Voice Conversion Using Conditional Cycle-Consistent Adversarial Networks",
        "authors": [
          "Shindong Lee",
          "Bonggu Ko",
          "Keonnyeong Lee",
          "In-Chul Yoo",
          "Dongsuk Yook"
        ],
        "published_date": "2020",
        "abstract": "Voice conversion (VC) refers to transforming the speaker characteristics of an utterance without altering its linguistic contents. Many works on voice conversion require to have parallel training data that is highly expensive to acquire. Recently, the cycle-consistent adversarial network (CycleGAN), which does not require parallel training data, has been applied to voice conversion, showing the state-of-the-art performance. The CycleGAN based voice conversion, however, can be used only for a pair of speakers, i.e., one-to-one voice conversion between two speakers. In this paper, we extend the CycleGAN by conditioning the network on speakers. As a result, the proposed method can perform many-to-many voice conversion among multiple speakers using a single generative adversarial network (GAN). Compared to building multiple CycleGANs for each pair of speakers, the proposed method reduces the computational and spatial cost significantly without compromising the sound quality of the converted voice. Experimental results using the VCC2018 corpus confirm the efficiency of the proposed method.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/8264c9d9e6b91b9c73d0231900e74cd529fbc9d5.pdf",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "citationCount": 34,
        "score": 6.800000000000001,
        "summary": "Voice conversion (VC) refers to transforming the speaker characteristics of an utterance without altering its linguistic contents. Many works on voice conversion require to have parallel training data that is highly expensive to acquire. Recently, the cycle-consistent adversarial network (CycleGAN), which does not require parallel training data, has been applied to voice conversion, showing the state-of-the-art performance. The CycleGAN based voice conversion, however, can be used only for a pair of speakers, i.e., one-to-one voice conversion between two speakers. In this paper, we extend the CycleGAN by conditioning the network on speakers. As a result, the proposed method can perform many-to-many voice conversion among multiple speakers using a single generative adversarial network (GAN). Compared to building multiple CycleGANs for each pair of speakers, the proposed method reduces the computational and spatial cost significantly without compromising the sound quality of the converted voice. Experimental results using the VCC2018 corpus confirm the efficiency of the proposed method.",
        "keywords": []
      },
      "file_name": "8264c9d9e6b91b9c73d0231900e74cd529fbc9d5.pdf"
    },
    {
      "success": true,
      "doc_id": "d2ef7d52398ff616a51e85d9c7b23110",
      "summary": "Improving the accuracy of channel estimation is a significant topic in the context of wireless communications. For training-based channel estimations, increasing the length of a training sequence may improve the accuracy of channel estimation but causes a higher overhead. Nevertheless, this paper shows that benefiting from generative adversarial networks (GANs), which is an emerging deep learning framework, the accuracy of channel estimation can be improved without transmitting a longer training sequence. To this end, this paper proposes a GAN-based channel estimation enhancement algorithm, where GANs are trained online with receive sequence so as to obtain a longer mimic sequence and enhance channel estimation. In order to address the problem of improving the training stability and the learning ability of GANs, this paper proposes a novel framework by integrating a conditional GAN with an improved Wasserstein GAN. Furthermore, a strategy based on a lookup table is proposed to alleviate overfitting that may occur during the training of GANs. Simulation results indicate that the proposed GAN-based channel estimation enhancement algorithm can benefit the conventional training-based channel estimation, yielding lower relative error performance, especially in the low SNR regions.",
      "intriguing_abstract": "Improving the accuracy of channel estimation is a significant topic in the context of wireless communications. For training-based channel estimations, increasing the length of a training sequence may improve the accuracy of channel estimation but causes a higher overhead. Nevertheless, this paper shows that benefiting from generative adversarial networks (GANs), which is an emerging deep learning framework, the accuracy of channel estimation can be improved without transmitting a longer training sequence. To this end, this paper proposes a GAN-based channel estimation enhancement algorithm, where GANs are trained online with receive sequence so as to obtain a longer mimic sequence and enhance channel estimation. In order to address the problem of improving the training stability and the learning ability of GANs, this paper proposes a novel framework by integrating a conditional GAN with an improved Wasserstein GAN. Furthermore, a strategy based on a lookup table is proposed to alleviate overfitting that may occur during the training of GANs. Simulation results indicate that the proposed GAN-based channel estimation enhancement algorithm can benefit the conventional training-based channel estimation, yielding lower relative error performance, especially in the low SNR regions.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/e4abcf52b65969f8fed43eff8f5cc512553b41d0.pdf",
      "citation_key": "hu2021yk5",
      "metadata": {
        "title": "Channel Estimation Enhancement With Generative Adversarial Networks",
        "authors": [
          "Tianyu Hu",
          "Yang Huang",
          "Qiuming Zhu",
          "Qi-hui Wu"
        ],
        "published_date": "2021",
        "abstract": "Improving the accuracy of channel estimation is a significant topic in the context of wireless communications. For training-based channel estimations, increasing the length of a training sequence may improve the accuracy of channel estimation but causes a higher overhead. Nevertheless, this paper shows that benefiting from generative adversarial networks (GANs), which is an emerging deep learning framework, the accuracy of channel estimation can be improved without transmitting a longer training sequence. To this end, this paper proposes a GAN-based channel estimation enhancement algorithm, where GANs are trained online with receive sequence so as to obtain a longer mimic sequence and enhance channel estimation. In order to address the problem of improving the training stability and the learning ability of GANs, this paper proposes a novel framework by integrating a conditional GAN with an improved Wasserstein GAN. Furthermore, a strategy based on a lookup table is proposed to alleviate overfitting that may occur during the training of GANs. Simulation results indicate that the proposed GAN-based channel estimation enhancement algorithm can benefit the conventional training-based channel estimation, yielding lower relative error performance, especially in the low SNR regions.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/e4abcf52b65969f8fed43eff8f5cc512553b41d0.pdf",
        "venue": "IEEE Transactions on Cognitive Communications and Networking",
        "citationCount": 27,
        "score": 6.75,
        "summary": "Improving the accuracy of channel estimation is a significant topic in the context of wireless communications. For training-based channel estimations, increasing the length of a training sequence may improve the accuracy of channel estimation but causes a higher overhead. Nevertheless, this paper shows that benefiting from generative adversarial networks (GANs), which is an emerging deep learning framework, the accuracy of channel estimation can be improved without transmitting a longer training sequence. To this end, this paper proposes a GAN-based channel estimation enhancement algorithm, where GANs are trained online with receive sequence so as to obtain a longer mimic sequence and enhance channel estimation. In order to address the problem of improving the training stability and the learning ability of GANs, this paper proposes a novel framework by integrating a conditional GAN with an improved Wasserstein GAN. Furthermore, a strategy based on a lookup table is proposed to alleviate overfitting that may occur during the training of GANs. Simulation results indicate that the proposed GAN-based channel estimation enhancement algorithm can benefit the conventional training-based channel estimation, yielding lower relative error performance, especially in the low SNR regions.",
        "keywords": []
      },
      "file_name": "e4abcf52b65969f8fed43eff8f5cc512553b41d0.pdf"
    },
    {
      "success": true,
      "doc_id": "40a7a5ecef1cceaa813cb25700f9c4e4",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/d5906006e6efc5dbc02878d76407326eb56c363a.pdf",
      "citation_key": "chen2021n5h",
      "metadata": {
        "title": "Ultra-Data-Efficient GAN Training: Drawing A Lottery Ticket First, Then Training It Toughly",
        "authors": [
          "Tianlong Chen",
          "Yu Cheng",
          "Zhe Gan",
          "Jingjing Liu",
          "Zhangyang Wang"
        ],
        "published_date": "2021",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/d5906006e6efc5dbc02878d76407326eb56c363a.pdf",
        "venue": "arXiv.org",
        "citationCount": 27,
        "score": 6.75,
        "summary": "",
        "keywords": []
      },
      "file_name": "d5906006e6efc5dbc02878d76407326eb56c363a.pdf"
    },
    {
      "success": true,
      "doc_id": "58d56651adf625e42aa6233e88d4e74a",
      "summary": "Recent generative adversarial network based methods have shown promising results for the charming but challenging task of synthesizing images from text descriptions. These approaches can generate images with general shape and color but often produce distorted global structures with unnatural local semantic details. It is due to ineffectiveness of convolutional neural networks in capturing the high-level semantic information for pixel-level image synthesis. In this paper, we propose a Dual Attentional Generative Adversarial Network (DualAttn-GAN) in which the dual attention modules are introduced to enhance local details and global structures by attending to related features from relevant words and different visual regions. As one of the dual modules, the textual attention module is designed to explore the fine-grained interaction between vision and language. On the other hand, visual attention module models internal representations of vision from channel and spatial axes, which can better capture the global structures. Meanwhile, we apply an attention embedding module to merge multi-path features. Furthermore, we present an inverted residual structure to boost representation power of CNNs and apply spectral normalization to stabilize GAN training. With extensive experimental validation on two benchmark datasets, our method significantly improves state-of-the-art models over the evaluation metrics of inception score and Fréchet inception distance.",
      "intriguing_abstract": "Recent generative adversarial network based methods have shown promising results for the charming but challenging task of synthesizing images from text descriptions. These approaches can generate images with general shape and color but often produce distorted global structures with unnatural local semantic details. It is due to ineffectiveness of convolutional neural networks in capturing the high-level semantic information for pixel-level image synthesis. In this paper, we propose a Dual Attentional Generative Adversarial Network (DualAttn-GAN) in which the dual attention modules are introduced to enhance local details and global structures by attending to related features from relevant words and different visual regions. As one of the dual modules, the textual attention module is designed to explore the fine-grained interaction between vision and language. On the other hand, visual attention module models internal representations of vision from channel and spatial axes, which can better capture the global structures. Meanwhile, we apply an attention embedding module to merge multi-path features. Furthermore, we present an inverted residual structure to boost representation power of CNNs and apply spectral normalization to stabilize GAN training. With extensive experimental validation on two benchmark datasets, our method significantly improves state-of-the-art models over the evaluation metrics of inception score and Fréchet inception distance.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/9770263651d8805bb0aac3eb93299867010f3cdd.pdf",
      "citation_key": "cai2019g1w",
      "metadata": {
        "title": "Dualattn-GAN: Text to Image Synthesis With Dual Attentional Generative Adversarial Network",
        "authors": [
          "Yali Cai",
          "Xiaoru Wang",
          "Zhihong Yu",
          "Fu Li",
          "Peirong Xu",
          "Yueli Li",
          "Lixian Li"
        ],
        "published_date": "2019",
        "abstract": "Recent generative adversarial network based methods have shown promising results for the charming but challenging task of synthesizing images from text descriptions. These approaches can generate images with general shape and color but often produce distorted global structures with unnatural local semantic details. It is due to ineffectiveness of convolutional neural networks in capturing the high-level semantic information for pixel-level image synthesis. In this paper, we propose a Dual Attentional Generative Adversarial Network (DualAttn-GAN) in which the dual attention modules are introduced to enhance local details and global structures by attending to related features from relevant words and different visual regions. As one of the dual modules, the textual attention module is designed to explore the fine-grained interaction between vision and language. On the other hand, visual attention module models internal representations of vision from channel and spatial axes, which can better capture the global structures. Meanwhile, we apply an attention embedding module to merge multi-path features. Furthermore, we present an inverted residual structure to boost representation power of CNNs and apply spectral normalization to stabilize GAN training. With extensive experimental validation on two benchmark datasets, our method significantly improves state-of-the-art models over the evaluation metrics of inception score and Fréchet inception distance.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/9770263651d8805bb0aac3eb93299867010f3cdd.pdf",
        "venue": "IEEE Access",
        "citationCount": 40,
        "score": 6.666666666666666,
        "summary": "Recent generative adversarial network based methods have shown promising results for the charming but challenging task of synthesizing images from text descriptions. These approaches can generate images with general shape and color but often produce distorted global structures with unnatural local semantic details. It is due to ineffectiveness of convolutional neural networks in capturing the high-level semantic information for pixel-level image synthesis. In this paper, we propose a Dual Attentional Generative Adversarial Network (DualAttn-GAN) in which the dual attention modules are introduced to enhance local details and global structures by attending to related features from relevant words and different visual regions. As one of the dual modules, the textual attention module is designed to explore the fine-grained interaction between vision and language. On the other hand, visual attention module models internal representations of vision from channel and spatial axes, which can better capture the global structures. Meanwhile, we apply an attention embedding module to merge multi-path features. Furthermore, we present an inverted residual structure to boost representation power of CNNs and apply spectral normalization to stabilize GAN training. With extensive experimental validation on two benchmark datasets, our method significantly improves state-of-the-art models over the evaluation metrics of inception score and Fréchet inception distance.",
        "keywords": []
      },
      "file_name": "9770263651d8805bb0aac3eb93299867010f3cdd.pdf"
    },
    {
      "success": true,
      "doc_id": "db49706e79ea420f4ade10963818df75",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/ab613e80271896c2a2721f08be1adc60a02a856e.pdf",
      "citation_key": "zhou20199sm",
      "metadata": {
        "title": "Enhanced Cycle-Consistent Generative Adversarial Network for Color Normalization of H&E Stained Images",
        "authors": [
          "Niyun Zhou",
          "De Cai",
          "Xiao Han",
          "Jianhua Yao"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/ab613e80271896c2a2721f08be1adc60a02a856e.pdf",
        "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
        "citationCount": 40,
        "score": 6.666666666666666,
        "summary": "",
        "keywords": []
      },
      "file_name": "ab613e80271896c2a2721f08be1adc60a02a856e.pdf"
    },
    {
      "success": true,
      "doc_id": "eef312ac00366d68e9d15103dd4bd38a",
      "summary": "State-of-the-art methods for image-to-image translation with Generative Adversarial Networks (GANs) can learn a mapping from one domain to another domain using unpaired image data. However, these methods require the training of one specific model for every pair of image domains, which limits the scalability in dealing with more than two image domains. In addition, the training stage of these methods has the common problem of model collapse that degrades the quality of the generated images. To tackle these issues, we propose a Dual Generator Generative Adversarial Network (G$^2$GAN), which is a robust and scalable approach allowing to perform unpaired image-to-image translation for multiple domains using only dual generators within a single model. Moreover, we explore different optimization losses for better training of G$^2$GAN, and thus make unpaired image-to-image translation with higher consistency and better stability. Extensive experiments on six publicly available datasets with different scenarios, i.e., architectural buildings, seasons, landscape and human faces, demonstrate that the proposed G$^2$GAN achieves superior model capacity and better generation performance comparing with existing image-to-image translation GAN models.",
      "intriguing_abstract": "State-of-the-art methods for image-to-image translation with Generative Adversarial Networks (GANs) can learn a mapping from one domain to another domain using unpaired image data. However, these methods require the training of one specific model for every pair of image domains, which limits the scalability in dealing with more than two image domains. In addition, the training stage of these methods has the common problem of model collapse that degrades the quality of the generated images. To tackle these issues, we propose a Dual Generator Generative Adversarial Network (G$^2$GAN), which is a robust and scalable approach allowing to perform unpaired image-to-image translation for multiple domains using only dual generators within a single model. Moreover, we explore different optimization losses for better training of G$^2$GAN, and thus make unpaired image-to-image translation with higher consistency and better stability. Extensive experiments on six publicly available datasets with different scenarios, i.e., architectural buildings, seasons, landscape and human faces, demonstrate that the proposed G$^2$GAN achieves superior model capacity and better generation performance comparing with existing image-to-image translation GAN models.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/9d3b9ff09867337ac0d693d4f093ddc6859c65c6.pdf",
      "citation_key": "tang2018iie",
      "metadata": {
        "title": "Dual Generator Generative Adversarial Networks for Multi-Domain Image-to-Image Translation",
        "authors": [
          "Hao Tang",
          "Dan Xu",
          "Wei Wang",
          "Yan Yan",
          "N. Sebe"
        ],
        "published_date": "2018",
        "abstract": "State-of-the-art methods for image-to-image translation with Generative Adversarial Networks (GANs) can learn a mapping from one domain to another domain using unpaired image data. However, these methods require the training of one specific model for every pair of image domains, which limits the scalability in dealing with more than two image domains. In addition, the training stage of these methods has the common problem of model collapse that degrades the quality of the generated images. To tackle these issues, we propose a Dual Generator Generative Adversarial Network (G$^2$GAN), which is a robust and scalable approach allowing to perform unpaired image-to-image translation for multiple domains using only dual generators within a single model. Moreover, we explore different optimization losses for better training of G$^2$GAN, and thus make unpaired image-to-image translation with higher consistency and better stability. Extensive experiments on six publicly available datasets with different scenarios, i.e., architectural buildings, seasons, landscape and human faces, demonstrate that the proposed G$^2$GAN achieves superior model capacity and better generation performance comparing with existing image-to-image translation GAN models.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/9d3b9ff09867337ac0d693d4f093ddc6859c65c6.pdf",
        "venue": "Asian Conference on Computer Vision",
        "citationCount": 45,
        "score": 6.428571428571428,
        "summary": "State-of-the-art methods for image-to-image translation with Generative Adversarial Networks (GANs) can learn a mapping from one domain to another domain using unpaired image data. However, these methods require the training of one specific model for every pair of image domains, which limits the scalability in dealing with more than two image domains. In addition, the training stage of these methods has the common problem of model collapse that degrades the quality of the generated images. To tackle these issues, we propose a Dual Generator Generative Adversarial Network (G$^2$GAN), which is a robust and scalable approach allowing to perform unpaired image-to-image translation for multiple domains using only dual generators within a single model. Moreover, we explore different optimization losses for better training of G$^2$GAN, and thus make unpaired image-to-image translation with higher consistency and better stability. Extensive experiments on six publicly available datasets with different scenarios, i.e., architectural buildings, seasons, landscape and human faces, demonstrate that the proposed G$^2$GAN achieves superior model capacity and better generation performance comparing with existing image-to-image translation GAN models.",
        "keywords": []
      },
      "file_name": "9d3b9ff09867337ac0d693d4f093ddc6859c65c6.pdf"
    },
    {
      "success": true,
      "doc_id": "cb23aef258f42e04a155fdf0f1735a62",
      "summary": "The data-driven intelligent fault diagnosis method of rolling bearings has strict requirements regarding the number and balance of fault samples. However, in practical engineering application scenarios, mechanical equipment is usually in a normal state, and small and imbalanced (S & I) fault samples are common, which seriously reduces the accuracy and stability of the fault diagnosis model. To solve this problem, an auxiliary classifier generative adversarial network with spectral normalization (ACGAN-SN) is proposed in this paper. First, a generation module based on a deconvolution layer is built to generate false data from Gaussian noise. Second, to enhance the training stability of the model, the data label information is used to make label constraints on the generated fake data under the basic GAN framework. Spectral normalization constraints are imposed on the output of each layer of the neural network of the discriminator to realize the Lipschitz continuity condition so as to avoid vanishing or exploding gradients. Finally, based on the generated data and the original S & I dataset, seven kinds of bearing fault datasets are made, and the prediction results of the Bi-directional Long Short-Term Memory (BiLSTM) model is verified. The results show that the data generated by ACGAN-SN can significantly promote the performance of the fault diagnosis model under the S & I fault samples.",
      "intriguing_abstract": "The data-driven intelligent fault diagnosis method of rolling bearings has strict requirements regarding the number and balance of fault samples. However, in practical engineering application scenarios, mechanical equipment is usually in a normal state, and small and imbalanced (S & I) fault samples are common, which seriously reduces the accuracy and stability of the fault diagnosis model. To solve this problem, an auxiliary classifier generative adversarial network with spectral normalization (ACGAN-SN) is proposed in this paper. First, a generation module based on a deconvolution layer is built to generate false data from Gaussian noise. Second, to enhance the training stability of the model, the data label information is used to make label constraints on the generated fake data under the basic GAN framework. Spectral normalization constraints are imposed on the output of each layer of the neural network of the discriminator to realize the Lipschitz continuity condition so as to avoid vanishing or exploding gradients. Finally, based on the generated data and the original S & I dataset, seven kinds of bearing fault datasets are made, and the prediction results of the Bi-directional Long Short-Term Memory (BiLSTM) model is verified. The results show that the data generated by ACGAN-SN can significantly promote the performance of the fault diagnosis model under the S & I fault samples.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/25cb41a83ce6b70681b037c21e6d2c147dfc001b.pdf",
      "citation_key": "tong2022lu4",
      "metadata": {
        "title": "A Novel Method for Fault Diagnosis of Bearings with Small and Imbalanced Data Based on Generative Adversarial Networks",
        "authors": [
          "Q. Tong",
          "Feiyu Lu",
          "Ziwei Feng",
          "Qingzhu Wan",
          "Guoping An",
          "Junci Cao",
          "Tao Guo"
        ],
        "published_date": "2022",
        "abstract": "The data-driven intelligent fault diagnosis method of rolling bearings has strict requirements regarding the number and balance of fault samples. However, in practical engineering application scenarios, mechanical equipment is usually in a normal state, and small and imbalanced (S & I) fault samples are common, which seriously reduces the accuracy and stability of the fault diagnosis model. To solve this problem, an auxiliary classifier generative adversarial network with spectral normalization (ACGAN-SN) is proposed in this paper. First, a generation module based on a deconvolution layer is built to generate false data from Gaussian noise. Second, to enhance the training stability of the model, the data label information is used to make label constraints on the generated fake data under the basic GAN framework. Spectral normalization constraints are imposed on the output of each layer of the neural network of the discriminator to realize the Lipschitz continuity condition so as to avoid vanishing or exploding gradients. Finally, based on the generated data and the original S & I dataset, seven kinds of bearing fault datasets are made, and the prediction results of the Bi-directional Long Short-Term Memory (BiLSTM) model is verified. The results show that the data generated by ACGAN-SN can significantly promote the performance of the fault diagnosis model under the S & I fault samples.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/25cb41a83ce6b70681b037c21e6d2c147dfc001b.pdf",
        "venue": "Applied Sciences",
        "citationCount": 19,
        "score": 6.333333333333333,
        "summary": "The data-driven intelligent fault diagnosis method of rolling bearings has strict requirements regarding the number and balance of fault samples. However, in practical engineering application scenarios, mechanical equipment is usually in a normal state, and small and imbalanced (S & I) fault samples are common, which seriously reduces the accuracy and stability of the fault diagnosis model. To solve this problem, an auxiliary classifier generative adversarial network with spectral normalization (ACGAN-SN) is proposed in this paper. First, a generation module based on a deconvolution layer is built to generate false data from Gaussian noise. Second, to enhance the training stability of the model, the data label information is used to make label constraints on the generated fake data under the basic GAN framework. Spectral normalization constraints are imposed on the output of each layer of the neural network of the discriminator to realize the Lipschitz continuity condition so as to avoid vanishing or exploding gradients. Finally, based on the generated data and the original S & I dataset, seven kinds of bearing fault datasets are made, and the prediction results of the Bi-directional Long Short-Term Memory (BiLSTM) model is verified. The results show that the data generated by ACGAN-SN can significantly promote the performance of the fault diagnosis model under the S & I fault samples.",
        "keywords": []
      },
      "file_name": "25cb41a83ce6b70681b037c21e6d2c147dfc001b.pdf"
    },
    {
      "success": true,
      "doc_id": "5ccce2176c7f3dd4da92fcd8436378f4",
      "summary": "Generative adversarial networks (GAN) became a hot topic, presenting impressive results in the field of computer vision. However, there are still open problems with the GAN model, such as the training stability and the hand-design of architectures. Neuroevolution is a technique that can be used to provide the automatic design of network architectures even in large search spaces as in deep neural networks. Therefore, this project proposes COEGAN, a model that combines neuroevolution and coevolution in the coordination of the GAN training algorithm. The proposal uses the adversarial characteristic between the generator and discriminator components to design an algorithm using coevolution techniques. Our proposal was evaluated in the MNIST dataset. The results suggest the improvement of the training stability and the automatic discovery of efficient network architectures for GANs. Our model also partially solves the mode collapse problem.",
      "intriguing_abstract": "Generative adversarial networks (GAN) became a hot topic, presenting impressive results in the field of computer vision. However, there are still open problems with the GAN model, such as the training stability and the hand-design of architectures. Neuroevolution is a technique that can be used to provide the automatic design of network architectures even in large search spaces as in deep neural networks. Therefore, this project proposes COEGAN, a model that combines neuroevolution and coevolution in the coordination of the GAN training algorithm. The proposal uses the adversarial characteristic between the generator and discriminator components to design an algorithm using coevolution techniques. Our proposal was evaluated in the MNIST dataset. The results suggest the improvement of the training stability and the automatic discovery of efficient network architectures for GANs. Our model also partially solves the mode collapse problem.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/b48b68f52b2ebaa8c7b428e98eafe1953045067f.pdf",
      "citation_key": "costa2019pj9",
      "metadata": {
        "title": "Coevolution of Generative Adversarial Networks",
        "authors": [
          "Victor Costa",
          "Nuno Lourenço",
          "P. Machado"
        ],
        "published_date": "2019",
        "abstract": "Generative adversarial networks (GAN) became a hot topic, presenting impressive results in the field of computer vision. However, there are still open problems with the GAN model, such as the training stability and the hand-design of architectures. Neuroevolution is a technique that can be used to provide the automatic design of network architectures even in large search spaces as in deep neural networks. Therefore, this project proposes COEGAN, a model that combines neuroevolution and coevolution in the coordination of the GAN training algorithm. The proposal uses the adversarial characteristic between the generator and discriminator components to design an algorithm using coevolution techniques. Our proposal was evaluated in the MNIST dataset. The results suggest the improvement of the training stability and the automatic discovery of efficient network architectures for GANs. Our model also partially solves the mode collapse problem.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/b48b68f52b2ebaa8c7b428e98eafe1953045067f.pdf",
        "venue": "EvoApplications",
        "citationCount": 37,
        "score": 6.166666666666666,
        "summary": "Generative adversarial networks (GAN) became a hot topic, presenting impressive results in the field of computer vision. However, there are still open problems with the GAN model, such as the training stability and the hand-design of architectures. Neuroevolution is a technique that can be used to provide the automatic design of network architectures even in large search spaces as in deep neural networks. Therefore, this project proposes COEGAN, a model that combines neuroevolution and coevolution in the coordination of the GAN training algorithm. The proposal uses the adversarial characteristic between the generator and discriminator components to design an algorithm using coevolution techniques. Our proposal was evaluated in the MNIST dataset. The results suggest the improvement of the training stability and the automatic discovery of efficient network architectures for GANs. Our model also partially solves the mode collapse problem.",
        "keywords": []
      },
      "file_name": "b48b68f52b2ebaa8c7b428e98eafe1953045067f.pdf"
    },
    {
      "success": true,
      "doc_id": "676e647001287e56926689df456254f3",
      "summary": "Rolling bearings are widely used in industrial manufacturing, and ensuring their stable and effective fault detection is a core requirement in the manufacturing process. However, it is a great challenge to achieve a highly accurate rolling bearing fault diagnosis because of the severe imbalance and distribution differences in fault data due to weak early fault features and interference from environmental noise. An intelligent fault diagnosis strategy for rolling bearings based on grayscale image transformation, a generative adversative network, and a convolutional neural network was proposed to solve this problem. First, the original vibration signal is converted into a grayscale image. Then more training samples are generated using GANs to solve severe imbalance and distribution differences in fault data. Finally, the rolling bearing condition detection and fault identification are carried out by using SECNN. The availability of the method is substantiated by experiments on datasets with different data imbalance ratios. In addition, the superiority of this diagnosis strategy is verified by comparing it with other mainstream intelligent diagnosis techniques. The experimental result demonstrates that this strategy can reach more than 99.6% recognition accuracy even under substantial environmental noise interference or changing working conditions and has good stability in the presence of a severe imbalance in fault data.",
      "intriguing_abstract": "Rolling bearings are widely used in industrial manufacturing, and ensuring their stable and effective fault detection is a core requirement in the manufacturing process. However, it is a great challenge to achieve a highly accurate rolling bearing fault diagnosis because of the severe imbalance and distribution differences in fault data due to weak early fault features and interference from environmental noise. An intelligent fault diagnosis strategy for rolling bearings based on grayscale image transformation, a generative adversative network, and a convolutional neural network was proposed to solve this problem. First, the original vibration signal is converted into a grayscale image. Then more training samples are generated using GANs to solve severe imbalance and distribution differences in fault data. Finally, the rolling bearing condition detection and fault identification are carried out by using SECNN. The availability of the method is substantiated by experiments on datasets with different data imbalance ratios. In addition, the superiority of this diagnosis strategy is verified by comparing it with other mainstream intelligent diagnosis techniques. The experimental result demonstrates that this strategy can reach more than 99.6% recognition accuracy even under substantial environmental noise interference or changing working conditions and has good stability in the presence of a severe imbalance in fault data.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/83ede0231197bba62d2058a914b33c59f8dbc4d3.pdf",
      "citation_key": "tang2021c82",
      "metadata": {
        "title": "A Novel Intelligent Fault Diagnosis Method for Rolling Bearings Based on Wasserstein Generative Adversarial Network and Convolutional Neural Network under Unbalanced Dataset",
        "authors": [
          "Hongtao Tang",
          "Shengbo Gao",
          "Lei Wang",
          "Xixing Li",
          "Bing Li",
          "Shibao Pang"
        ],
        "published_date": "2021",
        "abstract": "Rolling bearings are widely used in industrial manufacturing, and ensuring their stable and effective fault detection is a core requirement in the manufacturing process. However, it is a great challenge to achieve a highly accurate rolling bearing fault diagnosis because of the severe imbalance and distribution differences in fault data due to weak early fault features and interference from environmental noise. An intelligent fault diagnosis strategy for rolling bearings based on grayscale image transformation, a generative adversative network, and a convolutional neural network was proposed to solve this problem. First, the original vibration signal is converted into a grayscale image. Then more training samples are generated using GANs to solve severe imbalance and distribution differences in fault data. Finally, the rolling bearing condition detection and fault identification are carried out by using SECNN. The availability of the method is substantiated by experiments on datasets with different data imbalance ratios. In addition, the superiority of this diagnosis strategy is verified by comparing it with other mainstream intelligent diagnosis techniques. The experimental result demonstrates that this strategy can reach more than 99.6% recognition accuracy even under substantial environmental noise interference or changing working conditions and has good stability in the presence of a severe imbalance in fault data.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/83ede0231197bba62d2058a914b33c59f8dbc4d3.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 23,
        "score": 5.75,
        "summary": "Rolling bearings are widely used in industrial manufacturing, and ensuring their stable and effective fault detection is a core requirement in the manufacturing process. However, it is a great challenge to achieve a highly accurate rolling bearing fault diagnosis because of the severe imbalance and distribution differences in fault data due to weak early fault features and interference from environmental noise. An intelligent fault diagnosis strategy for rolling bearings based on grayscale image transformation, a generative adversative network, and a convolutional neural network was proposed to solve this problem. First, the original vibration signal is converted into a grayscale image. Then more training samples are generated using GANs to solve severe imbalance and distribution differences in fault data. Finally, the rolling bearing condition detection and fault identification are carried out by using SECNN. The availability of the method is substantiated by experiments on datasets with different data imbalance ratios. In addition, the superiority of this diagnosis strategy is verified by comparing it with other mainstream intelligent diagnosis techniques. The experimental result demonstrates that this strategy can reach more than 99.6% recognition accuracy even under substantial environmental noise interference or changing working conditions and has good stability in the presence of a severe imbalance in fault data.",
        "keywords": []
      },
      "file_name": "83ede0231197bba62d2058a914b33c59f8dbc4d3.pdf"
    },
    {
      "success": true,
      "doc_id": "89d69f4d4047366863692c818a98a3e6",
      "summary": "Generative adversarial network (GAN) has recently demonstrated a powerful tool for infrared and visible image fusion. However, existing methods extract the features incompletely, miss some textures, and lack the stability of training. To cope with these issues, this article proposes a novel image fusion Laplacian pyramid GAN (IF-LapGAN). Firstly, a generator is constructed which consists of shallow features extraction module, Laplacian pyramid module, and reconstruction module. Specifically, the Laplacian pyramid module is a pyramid-style encoder-decoder architecture, which progressively extracts the multi-scale features. Moreover, the attention module is equipped in the decoder to effectively decode the salient features. Then, two discriminators are adopted to discriminate the fused image and two different modalities respectively. To improve the stability of adversarial learning, we propose to develop another side supervised loss based on the side pre-trained fusion network. Extensive experiments show that IF-LapGAN achieves 3.27%, 27.28%, 6.32%, 1.39%, 3.14%, 1.15% and 1.07% improvement gains in terms of <inline-formula><tex-math notation=\"LaTeX\">$Q_{NMI}$</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\">$Q_{M}$</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\">$Q_{Yang}$</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\">$Q^{AB/F}$</tex-math></inline-formula>, MI, VIF, and FMI, respectively, compared with the second best values.",
      "intriguing_abstract": "Generative adversarial network (GAN) has recently demonstrated a powerful tool for infrared and visible image fusion. However, existing methods extract the features incompletely, miss some textures, and lack the stability of training. To cope with these issues, this article proposes a novel image fusion Laplacian pyramid GAN (IF-LapGAN). Firstly, a generator is constructed which consists of shallow features extraction module, Laplacian pyramid module, and reconstruction module. Specifically, the Laplacian pyramid module is a pyramid-style encoder-decoder architecture, which progressively extracts the multi-scale features. Moreover, the attention module is equipped in the decoder to effectively decode the salient features. Then, two discriminators are adopted to discriminate the fused image and two different modalities respectively. To improve the stability of adversarial learning, we propose to develop another side supervised loss based on the side pre-trained fusion network. Extensive experiments show that IF-LapGAN achieves 3.27%, 27.28%, 6.32%, 1.39%, 3.14%, 1.15% and 1.07% improvement gains in terms of <inline-formula><tex-math notation=\"LaTeX\">$Q_{NMI}$</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\">$Q_{M}$</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\">$Q_{Yang}$</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\">$Q^{AB/F}$</tex-math></inline-formula>, MI, VIF, and FMI, respectively, compared with the second best values.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/16dc13f77f8631aefb47ee57cadf5676b926e9bb.pdf",
      "citation_key": "yin2022izd",
      "metadata": {
        "title": "Laplacian Pyramid Generative Adversarial Network for Infrared and Visible Image Fusion",
        "authors": [
          "Haitao Yin",
          "Jing Xiao"
        ],
        "published_date": "2022",
        "abstract": "Generative adversarial network (GAN) has recently demonstrated a powerful tool for infrared and visible image fusion. However, existing methods extract the features incompletely, miss some textures, and lack the stability of training. To cope with these issues, this article proposes a novel image fusion Laplacian pyramid GAN (IF-LapGAN). Firstly, a generator is constructed which consists of shallow features extraction module, Laplacian pyramid module, and reconstruction module. Specifically, the Laplacian pyramid module is a pyramid-style encoder-decoder architecture, which progressively extracts the multi-scale features. Moreover, the attention module is equipped in the decoder to effectively decode the salient features. Then, two discriminators are adopted to discriminate the fused image and two different modalities respectively. To improve the stability of adversarial learning, we propose to develop another side supervised loss based on the side pre-trained fusion network. Extensive experiments show that IF-LapGAN achieves 3.27%, 27.28%, 6.32%, 1.39%, 3.14%, 1.15% and 1.07% improvement gains in terms of <inline-formula><tex-math notation=\"LaTeX\">$Q_{NMI}$</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\">$Q_{M}$</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\">$Q_{Yang}$</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\">$Q^{AB/F}$</tex-math></inline-formula>, MI, VIF, and FMI, respectively, compared with the second best values.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/16dc13f77f8631aefb47ee57cadf5676b926e9bb.pdf",
        "venue": "IEEE Signal Processing Letters",
        "citationCount": 17,
        "score": 5.666666666666666,
        "summary": "Generative adversarial network (GAN) has recently demonstrated a powerful tool for infrared and visible image fusion. However, existing methods extract the features incompletely, miss some textures, and lack the stability of training. To cope with these issues, this article proposes a novel image fusion Laplacian pyramid GAN (IF-LapGAN). Firstly, a generator is constructed which consists of shallow features extraction module, Laplacian pyramid module, and reconstruction module. Specifically, the Laplacian pyramid module is a pyramid-style encoder-decoder architecture, which progressively extracts the multi-scale features. Moreover, the attention module is equipped in the decoder to effectively decode the salient features. Then, two discriminators are adopted to discriminate the fused image and two different modalities respectively. To improve the stability of adversarial learning, we propose to develop another side supervised loss based on the side pre-trained fusion network. Extensive experiments show that IF-LapGAN achieves 3.27%, 27.28%, 6.32%, 1.39%, 3.14%, 1.15% and 1.07% improvement gains in terms of <inline-formula><tex-math notation=\"LaTeX\">$Q_{NMI}$</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\">$Q_{M}$</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\">$Q_{Yang}$</tex-math></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\">$Q^{AB/F}$</tex-math></inline-formula>, MI, VIF, and FMI, respectively, compared with the second best values.",
        "keywords": []
      },
      "file_name": "16dc13f77f8631aefb47ee57cadf5676b926e9bb.pdf"
    },
    {
      "success": true,
      "doc_id": "a7dbb196ea95c747c6b3cab5a269df66",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/3e7f9eaa0341f4f6992d372bad179da9b967fa33.pdf",
      "citation_key": "xu2020pkq",
      "metadata": {
        "title": "Understanding and Stabilizing GANs' Training Dynamics Using Control Theory",
        "authors": [
          "Kun Xu",
          "Chongxuan Li",
          "Huanshu Wei",
          "Jun Zhu",
          "Bo Zhang"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/3e7f9eaa0341f4f6992d372bad179da9b967fa33.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 28,
        "score": 5.6000000000000005,
        "summary": "",
        "keywords": []
      },
      "file_name": "3e7f9eaa0341f4f6992d372bad179da9b967fa33.pdf"
    },
    {
      "success": true,
      "doc_id": "6649df91e67e0b5bf92c65856dc0f43a",
      "summary": "Protein molecules are inherently dynamic and modulate their interactions with different molecular partners by accessing different tertiary structures under physiological conditions. Elucidating such structures remains challenging. Current momentum in deep learning and the powerful performance of generative adversarial networks (GANs) in complex domains, such as computer vision, inspires us to investigate GANs on their ability to generate physically-realistic protein tertiary structures. The analysis presented here shows that several GAN models fail to capture complex, distal structural patterns present in protein tertiary structures. The study additionally reveals that mechanisms touted as effective in stabilizing the training of a GAN model are not all effective, and that performance based on loss alone may be orthogonal to performance based on the quality of generated datasets. A novel contribution in this study is the demonstration that Wasserstein GAN strikes a good balance and manages to capture both local and distal patterns, thus presenting a first step towards more powerful deep generative models for exploring a possibly very diverse set of structures supporting diverse activities of a protein molecule in the cell.",
      "intriguing_abstract": "Protein molecules are inherently dynamic and modulate their interactions with different molecular partners by accessing different tertiary structures under physiological conditions. Elucidating such structures remains challenging. Current momentum in deep learning and the powerful performance of generative adversarial networks (GANs) in complex domains, such as computer vision, inspires us to investigate GANs on their ability to generate physically-realistic protein tertiary structures. The analysis presented here shows that several GAN models fail to capture complex, distal structural patterns present in protein tertiary structures. The study additionally reveals that mechanisms touted as effective in stabilizing the training of a GAN model are not all effective, and that performance based on loss alone may be orthogonal to performance based on the quality of generated datasets. A novel contribution in this study is the demonstration that Wasserstein GAN strikes a good balance and manages to capture both local and distal patterns, thus presenting a first step towards more powerful deep generative models for exploring a possibly very diverse set of structures supporting diverse activities of a protein molecule in the cell.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/8daf91622ec05492a52427b7666a4fa859ff2811.pdf",
      "citation_key": "rahman2021wm8",
      "metadata": {
        "title": "Generative Adversarial Learning of Protein Tertiary Structures",
        "authors": [
          "Taseef Rahman",
          "Yuanqi Du",
          "Liang Zhao",
          "Amarda Shehu"
        ],
        "published_date": "2021",
        "abstract": "Protein molecules are inherently dynamic and modulate their interactions with different molecular partners by accessing different tertiary structures under physiological conditions. Elucidating such structures remains challenging. Current momentum in deep learning and the powerful performance of generative adversarial networks (GANs) in complex domains, such as computer vision, inspires us to investigate GANs on their ability to generate physically-realistic protein tertiary structures. The analysis presented here shows that several GAN models fail to capture complex, distal structural patterns present in protein tertiary structures. The study additionally reveals that mechanisms touted as effective in stabilizing the training of a GAN model are not all effective, and that performance based on loss alone may be orthogonal to performance based on the quality of generated datasets. A novel contribution in this study is the demonstration that Wasserstein GAN strikes a good balance and manages to capture both local and distal patterns, thus presenting a first step towards more powerful deep generative models for exploring a possibly very diverse set of structures supporting diverse activities of a protein molecule in the cell.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/8daf91622ec05492a52427b7666a4fa859ff2811.pdf",
        "venue": "Molecules",
        "citationCount": 22,
        "score": 5.5,
        "summary": "Protein molecules are inherently dynamic and modulate their interactions with different molecular partners by accessing different tertiary structures under physiological conditions. Elucidating such structures remains challenging. Current momentum in deep learning and the powerful performance of generative adversarial networks (GANs) in complex domains, such as computer vision, inspires us to investigate GANs on their ability to generate physically-realistic protein tertiary structures. The analysis presented here shows that several GAN models fail to capture complex, distal structural patterns present in protein tertiary structures. The study additionally reveals that mechanisms touted as effective in stabilizing the training of a GAN model are not all effective, and that performance based on loss alone may be orthogonal to performance based on the quality of generated datasets. A novel contribution in this study is the demonstration that Wasserstein GAN strikes a good balance and manages to capture both local and distal patterns, thus presenting a first step towards more powerful deep generative models for exploring a possibly very diverse set of structures supporting diverse activities of a protein molecule in the cell.",
        "keywords": []
      },
      "file_name": "8daf91622ec05492a52427b7666a4fa859ff2811.pdf"
    },
    {
      "success": true,
      "doc_id": "7f20b89c7e10360e8ae6648e46a4b672",
      "summary": "Effective and efficient classification of synthetic aperture radar (SAR) images represents an important step toward image interpretation and knowledge discovery. Keys to classification performance include feature extraction and availability of class labels for training. Generative adversarial networks (GANs) are a recently advanced powerful framework to offer both automatic feature extraction and semisupervised and unsupervised learning. For its known drawbacks such as mode collapse and training instability, noteworthy improvements include the deep convolutional GANs (DCGANs). In the letter, we proposed a new approach, which further advances DCGANs with autoencoder (AE) for intermediate feature extraction and reconstruction, in combination with multiclassifier (MC) for better context treatment. The resulting improvement on training stability and mode preservation over DCGAN is clearly demonstrated by the classification results across multifrequency bands (L-, C-, and X-bands).",
      "intriguing_abstract": "Effective and efficient classification of synthetic aperture radar (SAR) images represents an important step toward image interpretation and knowledge discovery. Keys to classification performance include feature extraction and availability of class labels for training. Generative adversarial networks (GANs) are a recently advanced powerful framework to offer both automatic feature extraction and semisupervised and unsupervised learning. For its known drawbacks such as mode collapse and training instability, noteworthy improvements include the deep convolutional GANs (DCGANs). In the letter, we proposed a new approach, which further advances DCGANs with autoencoder (AE) for intermediate feature extraction and reconstruction, in combination with multiclassifier (MC) for better context treatment. The resulting improvement on training stability and mode preservation over DCGAN is clearly demonstrated by the classification results across multifrequency bands (L-, C-, and X-bands).",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/6c01187f5930e9618b05611dca1065b926ed4ab6.pdf",
      "citation_key": "zhang2022ysl",
      "metadata": {
        "title": "Deep Convolutional Generative Adversarial Network With Autoencoder for Semisupervised SAR Image Classification",
        "authors": [
          "Zheng Zhang",
          "Jingsong Yang",
          "Yang Du"
        ],
        "published_date": "2022",
        "abstract": "Effective and efficient classification of synthetic aperture radar (SAR) images represents an important step toward image interpretation and knowledge discovery. Keys to classification performance include feature extraction and availability of class labels for training. Generative adversarial networks (GANs) are a recently advanced powerful framework to offer both automatic feature extraction and semisupervised and unsupervised learning. For its known drawbacks such as mode collapse and training instability, noteworthy improvements include the deep convolutional GANs (DCGANs). In the letter, we proposed a new approach, which further advances DCGANs with autoencoder (AE) for intermediate feature extraction and reconstruction, in combination with multiclassifier (MC) for better context treatment. The resulting improvement on training stability and mode preservation over DCGAN is clearly demonstrated by the classification results across multifrequency bands (L-, C-, and X-bands).",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/6c01187f5930e9618b05611dca1065b926ed4ab6.pdf",
        "venue": "IEEE Geoscience and Remote Sensing Letters",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "Effective and efficient classification of synthetic aperture radar (SAR) images represents an important step toward image interpretation and knowledge discovery. Keys to classification performance include feature extraction and availability of class labels for training. Generative adversarial networks (GANs) are a recently advanced powerful framework to offer both automatic feature extraction and semisupervised and unsupervised learning. For its known drawbacks such as mode collapse and training instability, noteworthy improvements include the deep convolutional GANs (DCGANs). In the letter, we proposed a new approach, which further advances DCGANs with autoencoder (AE) for intermediate feature extraction and reconstruction, in combination with multiclassifier (MC) for better context treatment. The resulting improvement on training stability and mode preservation over DCGAN is clearly demonstrated by the classification results across multifrequency bands (L-, C-, and X-bands).",
        "keywords": []
      },
      "file_name": "6c01187f5930e9618b05611dca1065b926ed4ab6.pdf"
    },
    {
      "success": true,
      "doc_id": "d066db8c4010565ec6a00f9209f7b2e9",
      "summary": "We present a continual learning approach for generative adversarial networks (GANs), by designing and leveraging parameter-efficient feature map transformations. Our approach is based on learning a set of global and task-specific parameters. The global parameters are fixed across tasks whereas the task-specific parameters act as local adapters for each task, and help in efficiently obtaining task-specific feature maps. Moreover, we propose an element-wise addition of residual bias in the transformed feature space, which further helps stabilize GAN training in such settings. Our approach also leverages task similarity information based on the Fisher information matrix. Leveraging this knowledge from previous tasks significantly improves the model performance. In addition, the similarity measure also helps reduce the parameter growth in continual adaptation and helps to learn a compact model. In contrast to the recent approaches for continually-learned GANs, the proposed approach provides a memory-efficient way to perform effective continual data generation. Through extensive experiments on challenging and diverse datasets, we show that the feature-map-transformation approach outperforms state-of-the-art methods for continually-learned GANs, with substantially fewer parameters. The proposed method generates high-quality samples that can also improve the generative-replay-based continual learning for discriminative tasks.",
      "intriguing_abstract": "We present a continual learning approach for generative adversarial networks (GANs), by designing and leveraging parameter-efficient feature map transformations. Our approach is based on learning a set of global and task-specific parameters. The global parameters are fixed across tasks whereas the task-specific parameters act as local adapters for each task, and help in efficiently obtaining task-specific feature maps. Moreover, we propose an element-wise addition of residual bias in the transformed feature space, which further helps stabilize GAN training in such settings. Our approach also leverages task similarity information based on the Fisher information matrix. Leveraging this knowledge from previous tasks significantly improves the model performance. In addition, the similarity measure also helps reduce the parameter growth in continual adaptation and helps to learn a compact model. In contrast to the recent approaches for continually-learned GANs, the proposed approach provides a memory-efficient way to perform effective continual data generation. Through extensive experiments on challenging and diverse datasets, we show that the feature-map-transformation approach outperforms state-of-the-art methods for continually-learned GANs, with substantially fewer parameters. The proposed method generates high-quality samples that can also improve the generative-replay-based continual learning for discriminative tasks.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/35c3d1ccdb4c2014a00ce9d9a96cdbb93516d2ba.pdf",
      "citation_key": "varshney2021954",
      "metadata": {
        "title": "CAM-GAN: Continual Adaptation Modules for Generative Adversarial Networks",
        "authors": [
          "Sakshi Varshney",
          "V. Verma",
          "K. SrijithP.",
          "L. Carin",
          "Piyush Rai"
        ],
        "published_date": "2021",
        "abstract": "We present a continual learning approach for generative adversarial networks (GANs), by designing and leveraging parameter-efficient feature map transformations. Our approach is based on learning a set of global and task-specific parameters. The global parameters are fixed across tasks whereas the task-specific parameters act as local adapters for each task, and help in efficiently obtaining task-specific feature maps. Moreover, we propose an element-wise addition of residual bias in the transformed feature space, which further helps stabilize GAN training in such settings. Our approach also leverages task similarity information based on the Fisher information matrix. Leveraging this knowledge from previous tasks significantly improves the model performance. In addition, the similarity measure also helps reduce the parameter growth in continual adaptation and helps to learn a compact model. In contrast to the recent approaches for continually-learned GANs, the proposed approach provides a memory-efficient way to perform effective continual data generation. Through extensive experiments on challenging and diverse datasets, we show that the feature-map-transformation approach outperforms state-of-the-art methods for continually-learned GANs, with substantially fewer parameters. The proposed method generates high-quality samples that can also improve the generative-replay-based continual learning for discriminative tasks.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/35c3d1ccdb4c2014a00ce9d9a96cdbb93516d2ba.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 21,
        "score": 5.25,
        "summary": "We present a continual learning approach for generative adversarial networks (GANs), by designing and leveraging parameter-efficient feature map transformations. Our approach is based on learning a set of global and task-specific parameters. The global parameters are fixed across tasks whereas the task-specific parameters act as local adapters for each task, and help in efficiently obtaining task-specific feature maps. Moreover, we propose an element-wise addition of residual bias in the transformed feature space, which further helps stabilize GAN training in such settings. Our approach also leverages task similarity information based on the Fisher information matrix. Leveraging this knowledge from previous tasks significantly improves the model performance. In addition, the similarity measure also helps reduce the parameter growth in continual adaptation and helps to learn a compact model. In contrast to the recent approaches for continually-learned GANs, the proposed approach provides a memory-efficient way to perform effective continual data generation. Through extensive experiments on challenging and diverse datasets, we show that the feature-map-transformation approach outperforms state-of-the-art methods for continually-learned GANs, with substantially fewer parameters. The proposed method generates high-quality samples that can also improve the generative-replay-based continual learning for discriminative tasks.",
        "keywords": []
      },
      "file_name": "35c3d1ccdb4c2014a00ce9d9a96cdbb93516d2ba.pdf"
    },
    {
      "success": true,
      "doc_id": "951b213572cb9dc9e3fe6aea2f53c9a6",
      "summary": "Generative Adversarial Networks (GAN) are able to learn excellent representations for unlabelled data which can be applied to image generation and scene classification. Representations learned by GANs have not yet been applied to retrieval. In this paper, we show that the representations learned by GANs can indeed be used for retrieval. We consider heritage documents that contain unlabelled Merchant Marks, sketch-like symbols that are similar to hieroglyphs. We introduce a novel GAN architecture with design features that make it suitable for sketch retrieval. The performance of this sketch-GAN is compared to a modified version of the original GAN architecture with respect to simple invariance properties. Experiments suggest that sketch-GANs learn representations that are suitable for retrieval and which also have increased stability to rotation, scale and translation compared to the standard GAN architecture.",
      "intriguing_abstract": "Generative Adversarial Networks (GAN) are able to learn excellent representations for unlabelled data which can be applied to image generation and scene classification. Representations learned by GANs have not yet been applied to retrieval. In this paper, we show that the representations learned by GANs can indeed be used for retrieval. We consider heritage documents that contain unlabelled Merchant Marks, sketch-like symbols that are similar to hieroglyphs. We introduce a novel GAN architecture with design features that make it suitable for sketch retrieval. The performance of this sketch-GAN is compared to a modified version of the original GAN architecture with respect to simple invariance properties. Experiments suggest that sketch-GANs learn representations that are suitable for retrieval and which also have increased stability to rotation, scale and translation compared to the standard GAN architecture.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/344e8d09cd6144e84a92273d2b5be6c885ce2c22.pdf",
      "citation_key": "creswell2016mol",
      "metadata": {
        "title": "Adversarial Training for Sketch Retrieval",
        "authors": [
          "Antonia Creswell",
          "A. Bharath"
        ],
        "published_date": "2016",
        "abstract": "Generative Adversarial Networks (GAN) are able to learn excellent representations for unlabelled data which can be applied to image generation and scene classification. Representations learned by GANs have not yet been applied to retrieval. In this paper, we show that the representations learned by GANs can indeed be used for retrieval. We consider heritage documents that contain unlabelled Merchant Marks, sketch-like symbols that are similar to hieroglyphs. We introduce a novel GAN architecture with design features that make it suitable for sketch retrieval. The performance of this sketch-GAN is compared to a modified version of the original GAN architecture with respect to simple invariance properties. Experiments suggest that sketch-GANs learn representations that are suitable for retrieval and which also have increased stability to rotation, scale and translation compared to the standard GAN architecture.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/344e8d09cd6144e84a92273d2b5be6c885ce2c22.pdf",
        "venue": "ECCV Workshops",
        "citationCount": 47,
        "score": 5.222222222222222,
        "summary": "Generative Adversarial Networks (GAN) are able to learn excellent representations for unlabelled data which can be applied to image generation and scene classification. Representations learned by GANs have not yet been applied to retrieval. In this paper, we show that the representations learned by GANs can indeed be used for retrieval. We consider heritage documents that contain unlabelled Merchant Marks, sketch-like symbols that are similar to hieroglyphs. We introduce a novel GAN architecture with design features that make it suitable for sketch retrieval. The performance of this sketch-GAN is compared to a modified version of the original GAN architecture with respect to simple invariance properties. Experiments suggest that sketch-GANs learn representations that are suitable for retrieval and which also have increased stability to rotation, scale and translation compared to the standard GAN architecture.",
        "keywords": []
      },
      "file_name": "344e8d09cd6144e84a92273d2b5be6c885ce2c22.pdf"
    },
    {
      "success": true,
      "doc_id": "280c01760d07f9e91683095fcbd78e6d",
      "summary": "Despite the success of generative adversarial networks (GANs) for image generation, the trade-off between visual quality and image diversity remains a significant issue. This paper achieves both aims simultaneously by improving the stability of training GANs. The key idea of the proposed approach is to implicitly regularize the discriminator using representative features. Focusing on the fact that standard GAN minimizes reverse Kullback-Leibler (KL) divergence, we transfer the representative feature, which is extracted from the data distribution using a pre-trained autoencoder (AE), to the discriminator of standard GANs. Because the AE learns to minimize forward KL divergence, our GAN training with representative features is influenced by both reverse and forward KL divergence. Consequently, the proposed approach is verified to improve visual quality and diversity of state of the art GANs using extensive evaluations.",
      "intriguing_abstract": "Despite the success of generative adversarial networks (GANs) for image generation, the trade-off between visual quality and image diversity remains a significant issue. This paper achieves both aims simultaneously by improving the stability of training GANs. The key idea of the proposed approach is to implicitly regularize the discriminator using representative features. Focusing on the fact that standard GAN minimizes reverse Kullback-Leibler (KL) divergence, we transfer the representative feature, which is extracted from the data distribution using a pre-trained autoencoder (AE), to the discriminator of standard GANs. Because the AE learns to minimize forward KL divergence, our GAN training with representative features is influenced by both reverse and forward KL divergence. Consequently, the proposed approach is verified to improve visual quality and diversity of state of the art GANs using extensive evaluations.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/72a6044a0108e0f8f1e68cd70ada46c81a416324.pdf",
      "citation_key": "bang2018ps8",
      "metadata": {
        "title": "Improved Training of Generative Adversarial Networks Using Representative Features",
        "authors": [
          "Duhyeon Bang",
          "Hyunjung Shim"
        ],
        "published_date": "2018",
        "abstract": "Despite the success of generative adversarial networks (GANs) for image generation, the trade-off between visual quality and image diversity remains a significant issue. This paper achieves both aims simultaneously by improving the stability of training GANs. The key idea of the proposed approach is to implicitly regularize the discriminator using representative features. Focusing on the fact that standard GAN minimizes reverse Kullback-Leibler (KL) divergence, we transfer the representative feature, which is extracted from the data distribution using a pre-trained autoencoder (AE), to the discriminator of standard GANs. Because the AE learns to minimize forward KL divergence, our GAN training with representative features is influenced by both reverse and forward KL divergence. Consequently, the proposed approach is verified to improve visual quality and diversity of state of the art GANs using extensive evaluations.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/72a6044a0108e0f8f1e68cd70ada46c81a416324.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 34,
        "score": 4.857142857142857,
        "summary": "Despite the success of generative adversarial networks (GANs) for image generation, the trade-off between visual quality and image diversity remains a significant issue. This paper achieves both aims simultaneously by improving the stability of training GANs. The key idea of the proposed approach is to implicitly regularize the discriminator using representative features. Focusing on the fact that standard GAN minimizes reverse Kullback-Leibler (KL) divergence, we transfer the representative feature, which is extracted from the data distribution using a pre-trained autoencoder (AE), to the discriminator of standard GANs. Because the AE learns to minimize forward KL divergence, our GAN training with representative features is influenced by both reverse and forward KL divergence. Consequently, the proposed approach is verified to improve visual quality and diversity of state of the art GANs using extensive evaluations.",
        "keywords": []
      },
      "file_name": "72a6044a0108e0f8f1e68cd70ada46c81a416324.pdf"
    },
    {
      "success": true,
      "doc_id": "464bcba1ce140c86b1db3fb733317aed",
      "summary": "Generative adversarial networks (GANs), which are famous for the capability of learning complex underlying data distribution, are, however, known to be tricky in the training process, which would probably result in mode collapse or performance deterioration. Current approaches of dealing with GANs’ issues almost utilize some practical training techniques for the purpose of regularization, which, on the other hand, undermines the convergence and theoretical soundness of GAN. In this article, we propose to stabilize GAN training via a novel particle-based variational inference—Langevin Stein variational gradient descent (LSVGD), which not only inherits the flexibility and efficiency of original SVGD but also aims to address its instability issues by incorporating an extra disturbance into the update dynamics. We further demonstrate that, by properly adjusting the noise variance, LSVGD simulates a Langevin process whose stationary distribution is exactly the target distribution. We also show that LSVGD dynamics has an implicit regularization, which is able to enhance particles’ spread-out and diversity. Finally, we present an efficient way of applying particle-based variational inference on a general GAN training procedure no matter what loss function is adopted. Experimental results on one synthetic data set and three popular benchmark data sets—Cifar-10, Tiny-ImageNet, and CelebA—validate that LSVGD can remarkably improve the performance and stability of various GAN models.",
      "intriguing_abstract": "Generative adversarial networks (GANs), which are famous for the capability of learning complex underlying data distribution, are, however, known to be tricky in the training process, which would probably result in mode collapse or performance deterioration. Current approaches of dealing with GANs’ issues almost utilize some practical training techniques for the purpose of regularization, which, on the other hand, undermines the convergence and theoretical soundness of GAN. In this article, we propose to stabilize GAN training via a novel particle-based variational inference—Langevin Stein variational gradient descent (LSVGD), which not only inherits the flexibility and efficiency of original SVGD but also aims to address its instability issues by incorporating an extra disturbance into the update dynamics. We further demonstrate that, by properly adjusting the noise variance, LSVGD simulates a Langevin process whose stationary distribution is exactly the target distribution. We also show that LSVGD dynamics has an implicit regularization, which is able to enhance particles’ spread-out and diversity. Finally, we present an efficient way of applying particle-based variational inference on a general GAN training procedure no matter what loss function is adopted. Experimental results on one synthetic data set and three popular benchmark data sets—Cifar-10, Tiny-ImageNet, and CelebA—validate that LSVGD can remarkably improve the performance and stability of various GAN models.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/f9d11bdadd0a10f9cf74da34796328cb77de134d.pdf",
      "citation_key": "wang202066v",
      "metadata": {
        "title": "Stabilizing Training of Generative Adversarial Nets via Langevin Stein Variational Gradient Descent",
        "authors": [
          "Dong Wang",
          "Xiaoqian Qin",
          "F. Song",
          "Li Cheng"
        ],
        "published_date": "2020",
        "abstract": "Generative adversarial networks (GANs), which are famous for the capability of learning complex underlying data distribution, are, however, known to be tricky in the training process, which would probably result in mode collapse or performance deterioration. Current approaches of dealing with GANs’ issues almost utilize some practical training techniques for the purpose of regularization, which, on the other hand, undermines the convergence and theoretical soundness of GAN. In this article, we propose to stabilize GAN training via a novel particle-based variational inference—Langevin Stein variational gradient descent (LSVGD), which not only inherits the flexibility and efficiency of original SVGD but also aims to address its instability issues by incorporating an extra disturbance into the update dynamics. We further demonstrate that, by properly adjusting the noise variance, LSVGD simulates a Langevin process whose stationary distribution is exactly the target distribution. We also show that LSVGD dynamics has an implicit regularization, which is able to enhance particles’ spread-out and diversity. Finally, we present an efficient way of applying particle-based variational inference on a general GAN training procedure no matter what loss function is adopted. Experimental results on one synthetic data set and three popular benchmark data sets—Cifar-10, Tiny-ImageNet, and CelebA—validate that LSVGD can remarkably improve the performance and stability of various GAN models.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/f9d11bdadd0a10f9cf74da34796328cb77de134d.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 24,
        "score": 4.800000000000001,
        "summary": "Generative adversarial networks (GANs), which are famous for the capability of learning complex underlying data distribution, are, however, known to be tricky in the training process, which would probably result in mode collapse or performance deterioration. Current approaches of dealing with GANs’ issues almost utilize some practical training techniques for the purpose of regularization, which, on the other hand, undermines the convergence and theoretical soundness of GAN. In this article, we propose to stabilize GAN training via a novel particle-based variational inference—Langevin Stein variational gradient descent (LSVGD), which not only inherits the flexibility and efficiency of original SVGD but also aims to address its instability issues by incorporating an extra disturbance into the update dynamics. We further demonstrate that, by properly adjusting the noise variance, LSVGD simulates a Langevin process whose stationary distribution is exactly the target distribution. We also show that LSVGD dynamics has an implicit regularization, which is able to enhance particles’ spread-out and diversity. Finally, we present an efficient way of applying particle-based variational inference on a general GAN training procedure no matter what loss function is adopted. Experimental results on one synthetic data set and three popular benchmark data sets—Cifar-10, Tiny-ImageNet, and CelebA—validate that LSVGD can remarkably improve the performance and stability of various GAN models.",
        "keywords": []
      },
      "file_name": "f9d11bdadd0a10f9cf74da34796328cb77de134d.pdf"
    },
    {
      "success": true,
      "doc_id": "201c622dfdabc924869584d98b8f8f07",
      "summary": "Generative Adversarial Nets (GANs) are one of the most popular architectures for image generation, which has achieved significant progress in generating high-resolution, diverse image samples. The normal GANs are supposed to minimize the Kullback–Leibler divergence between distributions of natural and generated images. In this paper, we propose the Alpha-divergence Generative Adversarial Net (Alpha-GAN) which adopts the alpha divergence as the minimization objective function of generators. The alpha divergence can be regarded as a generalization of the Kullback–Leibler divergence, Pearson χ2 divergence, Hellinger divergence, etc. Our Alpha-GAN employs the power function as the form of adversarial loss for the discriminator with two-order indexes. These hyper-parameters make our model more flexible to trade off between the generated and target distributions. We further give a theoretical analysis of how to select these hyper-parameters to balance the training stability and the quality of generated images. Extensive experiments of Alpha-GAN are performed on SVHN and CelebA datasets, and evaluation results show the stability of Alpha-GAN. The generated samples are also competitive compared with the state-of-the-art approaches.",
      "intriguing_abstract": "Generative Adversarial Nets (GANs) are one of the most popular architectures for image generation, which has achieved significant progress in generating high-resolution, diverse image samples. The normal GANs are supposed to minimize the Kullback–Leibler divergence between distributions of natural and generated images. In this paper, we propose the Alpha-divergence Generative Adversarial Net (Alpha-GAN) which adopts the alpha divergence as the minimization objective function of generators. The alpha divergence can be regarded as a generalization of the Kullback–Leibler divergence, Pearson χ2 divergence, Hellinger divergence, etc. Our Alpha-GAN employs the power function as the form of adversarial loss for the discriminator with two-order indexes. These hyper-parameters make our model more flexible to trade off between the generated and target distributions. We further give a theoretical analysis of how to select these hyper-parameters to balance the training stability and the quality of generated images. Extensive experiments of Alpha-GAN are performed on SVHN and CelebA datasets, and evaluation results show the stability of Alpha-GAN. The generated samples are also competitive compared with the state-of-the-art approaches.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/23006dfeb539bc4f4f66e43e6d6670c4f3510a4c.pdf",
      "citation_key": "cai2020n2k",
      "metadata": {
        "title": "Utilizing Amari-Alpha Divergence to Stabilize the Training of Generative Adversarial Networks",
        "authors": [
          "Likun Cai",
          "Yanjie Chen",
          "Ning Cai",
          "Wei Cheng",
          "Hao Wang"
        ],
        "published_date": "2020",
        "abstract": "Generative Adversarial Nets (GANs) are one of the most popular architectures for image generation, which has achieved significant progress in generating high-resolution, diverse image samples. The normal GANs are supposed to minimize the Kullback–Leibler divergence between distributions of natural and generated images. In this paper, we propose the Alpha-divergence Generative Adversarial Net (Alpha-GAN) which adopts the alpha divergence as the minimization objective function of generators. The alpha divergence can be regarded as a generalization of the Kullback–Leibler divergence, Pearson χ2 divergence, Hellinger divergence, etc. Our Alpha-GAN employs the power function as the form of adversarial loss for the discriminator with two-order indexes. These hyper-parameters make our model more flexible to trade off between the generated and target distributions. We further give a theoretical analysis of how to select these hyper-parameters to balance the training stability and the quality of generated images. Extensive experiments of Alpha-GAN are performed on SVHN and CelebA datasets, and evaluation results show the stability of Alpha-GAN. The generated samples are also competitive compared with the state-of-the-art approaches.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/23006dfeb539bc4f4f66e43e6d6670c4f3510a4c.pdf",
        "venue": "Entropy",
        "citationCount": 24,
        "score": 4.800000000000001,
        "summary": "Generative Adversarial Nets (GANs) are one of the most popular architectures for image generation, which has achieved significant progress in generating high-resolution, diverse image samples. The normal GANs are supposed to minimize the Kullback–Leibler divergence between distributions of natural and generated images. In this paper, we propose the Alpha-divergence Generative Adversarial Net (Alpha-GAN) which adopts the alpha divergence as the minimization objective function of generators. The alpha divergence can be regarded as a generalization of the Kullback–Leibler divergence, Pearson χ2 divergence, Hellinger divergence, etc. Our Alpha-GAN employs the power function as the form of adversarial loss for the discriminator with two-order indexes. These hyper-parameters make our model more flexible to trade off between the generated and target distributions. We further give a theoretical analysis of how to select these hyper-parameters to balance the training stability and the quality of generated images. Extensive experiments of Alpha-GAN are performed on SVHN and CelebA datasets, and evaluation results show the stability of Alpha-GAN. The generated samples are also competitive compared with the state-of-the-art approaches.",
        "keywords": []
      },
      "file_name": "23006dfeb539bc4f4f66e43e6d6670c4f3510a4c.pdf"
    },
    {
      "success": true,
      "doc_id": "4e4ca87eb8168e0168779fbe2003b666",
      "summary": "Generative networks are fundamentally different in their aim and methods compared to CNNs for classification, segmentation, or object detection. They have initially not been meant to be an image analysis tool, but to produce naturally looking images. The adversarial training paradigm has been proposed to stabilize generative methods, and has proven to be highly successful – though by no means from the first attempt. This chapter gives a basic introduction into the motivation for Generative Adversarial Networks (GANs) and traces the path of their success by abstracting the basic task and working mechanism, and deriving the difficulty of early practical approaches. Methods for a more stable training will be shown, and also typical signs for poor convergence and their reasons. Though this chapter focuses on GANs that are meant for image generation and image analysis, the adversarial training paradigm itself is not specific to images, and also generalizes to tasks in image analysis. Examples of architectures for image semantic segmentation and abnormality detection will be acclaimed, before contrasting GANs with further generative modeling approaches lately entering the scene. This will allow a contextualized view on the limits but also benefits of GANs.",
      "intriguing_abstract": "Generative networks are fundamentally different in their aim and methods compared to CNNs for classification, segmentation, or object detection. They have initially not been meant to be an image analysis tool, but to produce naturally looking images. The adversarial training paradigm has been proposed to stabilize generative methods, and has proven to be highly successful – though by no means from the first attempt. This chapter gives a basic introduction into the motivation for Generative Adversarial Networks (GANs) and traces the path of their success by abstracting the basic task and working mechanism, and deriving the difficulty of early practical approaches. Methods for a more stable training will be shown, and also typical signs for poor convergence and their reasons. Though this chapter focuses on GANs that are meant for image generation and image analysis, the adversarial training paradigm itself is not specific to images, and also generalizes to tasks in image analysis. Examples of architectures for image semantic segmentation and abnormality detection will be acclaimed, before contrasting GANs with further generative modeling approaches lately entering the scene. This will allow a contextualized view on the limits but also benefits of GANs.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/c9f6ff493aade94a2fd6f4e89201e3d7333aedcb.pdf",
      "citation_key": "wenzel20225g3",
      "metadata": {
        "title": "Generative Adversarial Networks and Other Generative Models",
        "authors": [
          "Markus T. Wenzel"
        ],
        "published_date": "2022",
        "abstract": "Generative networks are fundamentally different in their aim and methods compared to CNNs for classification, segmentation, or object detection. They have initially not been meant to be an image analysis tool, but to produce naturally looking images. The adversarial training paradigm has been proposed to stabilize generative methods, and has proven to be highly successful – though by no means from the first attempt. This chapter gives a basic introduction into the motivation for Generative Adversarial Networks (GANs) and traces the path of their success by abstracting the basic task and working mechanism, and deriving the difficulty of early practical approaches. Methods for a more stable training will be shown, and also typical signs for poor convergence and their reasons. Though this chapter focuses on GANs that are meant for image generation and image analysis, the adversarial training paradigm itself is not specific to images, and also generalizes to tasks in image analysis. Examples of architectures for image semantic segmentation and abnormality detection will be acclaimed, before contrasting GANs with further generative modeling approaches lately entering the scene. This will allow a contextualized view on the limits but also benefits of GANs.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/c9f6ff493aade94a2fd6f4e89201e3d7333aedcb.pdf",
        "venue": "arXiv.org",
        "citationCount": 14,
        "score": 4.666666666666666,
        "summary": "Generative networks are fundamentally different in their aim and methods compared to CNNs for classification, segmentation, or object detection. They have initially not been meant to be an image analysis tool, but to produce naturally looking images. The adversarial training paradigm has been proposed to stabilize generative methods, and has proven to be highly successful – though by no means from the first attempt. This chapter gives a basic introduction into the motivation for Generative Adversarial Networks (GANs) and traces the path of their success by abstracting the basic task and working mechanism, and deriving the difficulty of early practical approaches. Methods for a more stable training will be shown, and also typical signs for poor convergence and their reasons. Though this chapter focuses on GANs that are meant for image generation and image analysis, the adversarial training paradigm itself is not specific to images, and also generalizes to tasks in image analysis. Examples of architectures for image semantic segmentation and abnormality detection will be acclaimed, before contrasting GANs with further generative modeling approaches lately entering the scene. This will allow a contextualized view on the limits but also benefits of GANs.",
        "keywords": []
      },
      "file_name": "c9f6ff493aade94a2fd6f4e89201e3d7333aedcb.pdf"
    },
    {
      "success": true,
      "doc_id": "79c6b08184382dc5d4d339d1295967f9",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/0edc142b51581a358055d7eddada8a4d0f9d021b.pdf",
      "citation_key": "gidel2018pg0",
      "metadata": {
        "title": "A Variational Inequality Perspective on Generative Adversarial Nets",
        "authors": [
          "G. Gidel",
          "Hugo Berard",
          "Pascal Vincent",
          "Simon Lacoste-Julien"
        ],
        "published_date": "2018",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/0edc142b51581a358055d7eddada8a4d0f9d021b.pdf",
        "venue": "arXiv.org",
        "citationCount": 32,
        "score": 4.571428571428571,
        "summary": "",
        "keywords": []
      },
      "file_name": "0edc142b51581a358055d7eddada8a4d0f9d021b.pdf"
    },
    {
      "success": true,
      "doc_id": "855b6fffc38bb1918e743ac5b2f6e97e",
      "summary": "Generative Adversarial Networks (GANs) produce systematically better quality samples when class label information is provided., i.e. in the conditional GAN setup. This is still observed for the recently proposed Wasserstein GAN formulation which stabilized adversarial training and allows considering high capacity network architectures such as ResNet. In this work we show how to boost conditional GAN by augmenting available class labels. The new classes come from clustering in the representation space learned by the same GAN model. The proposed strategy is also feasible when no class information is available, i.e. in the unsupervised setup. Our generated samples reach state-of-the-art Inception scores for CIFAR-10 and STL-10 datasets in both supervised and unsupervised setup.",
      "intriguing_abstract": "Generative Adversarial Networks (GANs) produce systematically better quality samples when class label information is provided., i.e. in the conditional GAN setup. This is still observed for the recently proposed Wasserstein GAN formulation which stabilized adversarial training and allows considering high capacity network architectures such as ResNet. In this work we show how to boost conditional GAN by augmenting available class labels. The new classes come from clustering in the representation space learned by the same GAN model. The proposed strategy is also feasible when no class information is available, i.e. in the unsupervised setup. Our generated samples reach state-of-the-art Inception scores for CIFAR-10 and STL-10 datasets in both supervised and unsupervised setup.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/4136412ac44e9185125246be447d2c06e8676dcc.pdf",
      "citation_key": "grinblat2017cem",
      "metadata": {
        "title": "Class-Splitting Generative Adversarial Networks",
        "authors": [
          "G. Grinblat",
          "Lucas C. Uzal",
          "P. Granitto"
        ],
        "published_date": "2017",
        "abstract": "Generative Adversarial Networks (GANs) produce systematically better quality samples when class label information is provided., i.e. in the conditional GAN setup. This is still observed for the recently proposed Wasserstein GAN formulation which stabilized adversarial training and allows considering high capacity network architectures such as ResNet. In this work we show how to boost conditional GAN by augmenting available class labels. The new classes come from clustering in the representation space learned by the same GAN model. The proposed strategy is also feasible when no class information is available, i.e. in the unsupervised setup. Our generated samples reach state-of-the-art Inception scores for CIFAR-10 and STL-10 datasets in both supervised and unsupervised setup.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/4136412ac44e9185125246be447d2c06e8676dcc.pdf",
        "venue": "arXiv.org",
        "citationCount": 35,
        "score": 4.375,
        "summary": "Generative Adversarial Networks (GANs) produce systematically better quality samples when class label information is provided., i.e. in the conditional GAN setup. This is still observed for the recently proposed Wasserstein GAN formulation which stabilized adversarial training and allows considering high capacity network architectures such as ResNet. In this work we show how to boost conditional GAN by augmenting available class labels. The new classes come from clustering in the representation space learned by the same GAN model. The proposed strategy is also feasible when no class information is available, i.e. in the unsupervised setup. Our generated samples reach state-of-the-art Inception scores for CIFAR-10 and STL-10 datasets in both supervised and unsupervised setup.",
        "keywords": []
      },
      "file_name": "4136412ac44e9185125246be447d2c06e8676dcc.pdf"
    },
    {
      "success": true,
      "doc_id": "557bf4e7da4a2c8467c5ea441817cf54",
      "summary": "Conditional traffic estimation is a vital problem in urban plan deployment, which can help evaluate urban construction plans and improve transportation efficiency. Conventional methods for conditional traffic estimation usually focus on supervised settings, which require a large amount of labeled training data. However, in many urban planning applications, the large amount of traffic data in a new city can be hard or impossible to acquire. To tackle the conditional traffic estimation problem in data scarcity situations, we formulate the problem as a spatial transfer generative learning problem. Compared to prior spatial transfer learning frameworks with only single source city, we propose to extracts knowledge from multiple source cities to improve the estimation accuracy and transfer stability, which is a technically more challenging task. As a solution, we propose a new cross-city conditional traffic estimation method — Spatially-Transferable Generative Adversarial Networks (STrans-GAN) with novel pre-training and fine-tuning algorithms. STransGAN preserves diverse traffic patterns from multiple source cities through traffic clustering, and incorporates meta-learning idea into the pre-training process to learn a well-generalized model. During fine-tuning, we propose to add a cluster matching regularizer to realize the flexible adaptation in different scenarios. Through extensive experiments on multiple-city datasets, the effectiveness of STrans-GAN is proved.",
      "intriguing_abstract": "Conditional traffic estimation is a vital problem in urban plan deployment, which can help evaluate urban construction plans and improve transportation efficiency. Conventional methods for conditional traffic estimation usually focus on supervised settings, which require a large amount of labeled training data. However, in many urban planning applications, the large amount of traffic data in a new city can be hard or impossible to acquire. To tackle the conditional traffic estimation problem in data scarcity situations, we formulate the problem as a spatial transfer generative learning problem. Compared to prior spatial transfer learning frameworks with only single source city, we propose to extracts knowledge from multiple source cities to improve the estimation accuracy and transfer stability, which is a technically more challenging task. As a solution, we propose a new cross-city conditional traffic estimation method — Spatially-Transferable Generative Adversarial Networks (STrans-GAN) with novel pre-training and fine-tuning algorithms. STransGAN preserves diverse traffic patterns from multiple source cities through traffic clustering, and incorporates meta-learning idea into the pre-training process to learn a well-generalized model. During fine-tuning, we propose to add a cluster matching regularizer to realize the flexible adaptation in different scenarios. Through extensive experiments on multiple-city datasets, the effectiveness of STrans-GAN is proved.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/ffe50056d8331500d6f8f72c3e7743b096aa2bdf.pdf",
      "citation_key": "zhang202263o",
      "metadata": {
        "title": "STrans-GAN: Spatially-Transferable Generative Adversarial Networks for Urban Traffic Estimation",
        "authors": [
          "Yingxue Zhang",
          "Yanhua Li",
          "Xun Zhou",
          "Xiangnan Kong",
          "Jun Luo"
        ],
        "published_date": "2022",
        "abstract": "Conditional traffic estimation is a vital problem in urban plan deployment, which can help evaluate urban construction plans and improve transportation efficiency. Conventional methods for conditional traffic estimation usually focus on supervised settings, which require a large amount of labeled training data. However, in many urban planning applications, the large amount of traffic data in a new city can be hard or impossible to acquire. To tackle the conditional traffic estimation problem in data scarcity situations, we formulate the problem as a spatial transfer generative learning problem. Compared to prior spatial transfer learning frameworks with only single source city, we propose to extracts knowledge from multiple source cities to improve the estimation accuracy and transfer stability, which is a technically more challenging task. As a solution, we propose a new cross-city conditional traffic estimation method — Spatially-Transferable Generative Adversarial Networks (STrans-GAN) with novel pre-training and fine-tuning algorithms. STransGAN preserves diverse traffic patterns from multiple source cities through traffic clustering, and incorporates meta-learning idea into the pre-training process to learn a well-generalized model. During fine-tuning, we propose to add a cluster matching regularizer to realize the flexible adaptation in different scenarios. Through extensive experiments on multiple-city datasets, the effectiveness of STrans-GAN is proved.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/ffe50056d8331500d6f8f72c3e7743b096aa2bdf.pdf",
        "venue": "Industrial Conference on Data Mining",
        "citationCount": 13,
        "score": 4.333333333333333,
        "summary": "Conditional traffic estimation is a vital problem in urban plan deployment, which can help evaluate urban construction plans and improve transportation efficiency. Conventional methods for conditional traffic estimation usually focus on supervised settings, which require a large amount of labeled training data. However, in many urban planning applications, the large amount of traffic data in a new city can be hard or impossible to acquire. To tackle the conditional traffic estimation problem in data scarcity situations, we formulate the problem as a spatial transfer generative learning problem. Compared to prior spatial transfer learning frameworks with only single source city, we propose to extracts knowledge from multiple source cities to improve the estimation accuracy and transfer stability, which is a technically more challenging task. As a solution, we propose a new cross-city conditional traffic estimation method — Spatially-Transferable Generative Adversarial Networks (STrans-GAN) with novel pre-training and fine-tuning algorithms. STransGAN preserves diverse traffic patterns from multiple source cities through traffic clustering, and incorporates meta-learning idea into the pre-training process to learn a well-generalized model. During fine-tuning, we propose to add a cluster matching regularizer to realize the flexible adaptation in different scenarios. Through extensive experiments on multiple-city datasets, the effectiveness of STrans-GAN is proved.",
        "keywords": []
      },
      "file_name": "ffe50056d8331500d6f8f72c3e7743b096aa2bdf.pdf"
    },
    {
      "success": true,
      "doc_id": "353cbf6c36343b5c92d000b65ee579cd",
      "summary": "Positron Emission Tomography (PET) is now regarded as the gold standard for the diagnosis of Alzheimer's Disease (AD). However, PET imaging can be prohibitive in terms of cost and planning, and is also among the imaging techniques with the highest dosage of radiation. Magnetic Resonance Imaging (MRI), in contrast, is more widely available and provides more flexibility when setting the desired image resolution. Unfortunately, the diagnosis of AD using MRI is difficult due to the very subtle physiological differences between healthy and AD subjects visible on MRI. As a result, many attempts have been made to synthesize PET images from MR images using generative adversarial networks (GANs) in the interest of enabling the diagnosis of AD from MR. Existing work on PET synthesis from MRI has largely focused on Conditional GANs, where MR images are used to generate PET images and subsequently used for AD diagnosis. There is no end-to-end training goal. This paper proposes an alternative approach to the aforementioned, where AD diagnosis is incorporated in the GAN training objective to achieve the best AD classification performance. Different GAN lossesare fine-tuned based on the discriminator performance, and the overall training is stabilized. The proposed network architecture and training regime show state-of-the-art performance for three- and four- class AD classification tasks.",
      "intriguing_abstract": "Positron Emission Tomography (PET) is now regarded as the gold standard for the diagnosis of Alzheimer's Disease (AD). However, PET imaging can be prohibitive in terms of cost and planning, and is also among the imaging techniques with the highest dosage of radiation. Magnetic Resonance Imaging (MRI), in contrast, is more widely available and provides more flexibility when setting the desired image resolution. Unfortunately, the diagnosis of AD using MRI is difficult due to the very subtle physiological differences between healthy and AD subjects visible on MRI. As a result, many attempts have been made to synthesize PET images from MR images using generative adversarial networks (GANs) in the interest of enabling the diagnosis of AD from MR. Existing work on PET synthesis from MRI has largely focused on Conditional GANs, where MR images are used to generate PET images and subsequently used for AD diagnosis. There is no end-to-end training goal. This paper proposes an alternative approach to the aforementioned, where AD diagnosis is incorporated in the GAN training objective to achieve the best AD classification performance. Different GAN lossesare fine-tuned based on the discriminator performance, and the overall training is stabilized. The proposed network architecture and training regime show state-of-the-art performance for three- and four- class AD classification tasks.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/4795c82ec752177a2904da44b05231da93d69c4f.pdf",
      "citation_key": "shin2020169",
      "metadata": {
        "title": "GANDALF: Generative Adversarial Networks with Discriminator-Adaptive Loss Fine-tuning for Alzheimer's Disease Diagnosis from MRI",
        "authors": [
          "Hoo-Chang Shin",
          "Alvin Ihsani",
          "Ziyue Xu",
          "Swetha Mandava",
          "Sharath Turuvekere Sreenivas",
          "Christopher Forster",
          "Jiook Cha",
          "Alzheimer's Disease Neuroimaging Initiative"
        ],
        "published_date": "2020",
        "abstract": "Positron Emission Tomography (PET) is now regarded as the gold standard for the diagnosis of Alzheimer's Disease (AD). However, PET imaging can be prohibitive in terms of cost and planning, and is also among the imaging techniques with the highest dosage of radiation. Magnetic Resonance Imaging (MRI), in contrast, is more widely available and provides more flexibility when setting the desired image resolution. Unfortunately, the diagnosis of AD using MRI is difficult due to the very subtle physiological differences between healthy and AD subjects visible on MRI. As a result, many attempts have been made to synthesize PET images from MR images using generative adversarial networks (GANs) in the interest of enabling the diagnosis of AD from MR. Existing work on PET synthesis from MRI has largely focused on Conditional GANs, where MR images are used to generate PET images and subsequently used for AD diagnosis. There is no end-to-end training goal. This paper proposes an alternative approach to the aforementioned, where AD diagnosis is incorporated in the GAN training objective to achieve the best AD classification performance. Different GAN lossesare fine-tuned based on the discriminator performance, and the overall training is stabilized. The proposed network architecture and training regime show state-of-the-art performance for three- and four- class AD classification tasks.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/4795c82ec752177a2904da44b05231da93d69c4f.pdf",
        "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
        "citationCount": 21,
        "score": 4.2,
        "summary": "Positron Emission Tomography (PET) is now regarded as the gold standard for the diagnosis of Alzheimer's Disease (AD). However, PET imaging can be prohibitive in terms of cost and planning, and is also among the imaging techniques with the highest dosage of radiation. Magnetic Resonance Imaging (MRI), in contrast, is more widely available and provides more flexibility when setting the desired image resolution. Unfortunately, the diagnosis of AD using MRI is difficult due to the very subtle physiological differences between healthy and AD subjects visible on MRI. As a result, many attempts have been made to synthesize PET images from MR images using generative adversarial networks (GANs) in the interest of enabling the diagnosis of AD from MR. Existing work on PET synthesis from MRI has largely focused on Conditional GANs, where MR images are used to generate PET images and subsequently used for AD diagnosis. There is no end-to-end training goal. This paper proposes an alternative approach to the aforementioned, where AD diagnosis is incorporated in the GAN training objective to achieve the best AD classification performance. Different GAN lossesare fine-tuned based on the discriminator performance, and the overall training is stabilized. The proposed network architecture and training regime show state-of-the-art performance for three- and four- class AD classification tasks.",
        "keywords": []
      },
      "file_name": "4795c82ec752177a2904da44b05231da93d69c4f.pdf"
    },
    {
      "success": true,
      "doc_id": "39ac0cd1a74585f123b44602ba8c28da",
      "summary": "We propose Unbalanced GANs, which pre-trains the generator of the generative adversarial network (GAN) using variational autoencoder (VAE). We guarantee the stable training of the generator by preventing the faster convergence of the discriminator at early epochs. Furthermore, we balance between the generator and the discriminator at early epochs and thus maintain the stabilized training of GANs. We apply Unbalanced GANs to well known public datasets and find that Unbalanced GANs reduce mode collapses. We also show that Unbalanced GANs outperform ordinary GANs in terms of stabilized learning, faster convergence and better image quality at early epochs.",
      "intriguing_abstract": "We propose Unbalanced GANs, which pre-trains the generator of the generative adversarial network (GAN) using variational autoencoder (VAE). We guarantee the stable training of the generator by preventing the faster convergence of the discriminator at early epochs. Furthermore, we balance between the generator and the discriminator at early epochs and thus maintain the stabilized training of GANs. We apply Unbalanced GANs to well known public datasets and find that Unbalanced GANs reduce mode collapses. We also show that Unbalanced GANs outperform ordinary GANs in terms of stabilized learning, faster convergence and better image quality at early epochs.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/473f4b63e56e88ca8e2fafb156a4c38de2567e86.pdf",
      "citation_key": "ham2020svv",
      "metadata": {
        "title": "Unbalanced GANs: Pre-training the Generator of Generative Adversarial Network using Variational Autoencoder",
        "authors": [
          "Hyung-Gi Ham",
          "T. Jun",
          "Daeyoung Kim"
        ],
        "published_date": "2020",
        "abstract": "We propose Unbalanced GANs, which pre-trains the generator of the generative adversarial network (GAN) using variational autoencoder (VAE). We guarantee the stable training of the generator by preventing the faster convergence of the discriminator at early epochs. Furthermore, we balance between the generator and the discriminator at early epochs and thus maintain the stabilized training of GANs. We apply Unbalanced GANs to well known public datasets and find that Unbalanced GANs reduce mode collapses. We also show that Unbalanced GANs outperform ordinary GANs in terms of stabilized learning, faster convergence and better image quality at early epochs.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/473f4b63e56e88ca8e2fafb156a4c38de2567e86.pdf",
        "venue": "arXiv.org",
        "citationCount": 21,
        "score": 4.2,
        "summary": "We propose Unbalanced GANs, which pre-trains the generator of the generative adversarial network (GAN) using variational autoencoder (VAE). We guarantee the stable training of the generator by preventing the faster convergence of the discriminator at early epochs. Furthermore, we balance between the generator and the discriminator at early epochs and thus maintain the stabilized training of GANs. We apply Unbalanced GANs to well known public datasets and find that Unbalanced GANs reduce mode collapses. We also show that Unbalanced GANs outperform ordinary GANs in terms of stabilized learning, faster convergence and better image quality at early epochs.",
        "keywords": []
      },
      "file_name": "473f4b63e56e88ca8e2fafb156a4c38de2567e86.pdf"
    },
    {
      "success": true,
      "doc_id": "7e053d2f8ecce974bb76a5ab6e6fc76e",
      "summary": "Anomaly detection aims to detect abnormal events by a model of normality. It plays an important role in many domains such as network intrusion detection, criminal activity identity and so on. With the rapidly growing size of accessible training data and high computation capacities, deep learning based anomaly detection has become more and more popular. In this paper, a new domain-based anomaly detection method based on generative adversarial networks (GAN) is proposed. Minimum likelihood regularization is proposed to make the generator produce more anomalies and prevent it from converging to normal data distribution. Proper ensemble of anomaly scores is shown to improve the stability of discriminator effectively. The proposed method has achieved significant improvement than other anomaly detection methods on Cifar10 and UCI datasets.",
      "intriguing_abstract": "Anomaly detection aims to detect abnormal events by a model of normality. It plays an important role in many domains such as network intrusion detection, criminal activity identity and so on. With the rapidly growing size of accessible training data and high computation capacities, deep learning based anomaly detection has become more and more popular. In this paper, a new domain-based anomaly detection method based on generative adversarial networks (GAN) is proposed. Minimum likelihood regularization is proposed to make the generator produce more anomalies and prevent it from converging to normal data distribution. Proper ensemble of anomaly scores is shown to improve the stability of discriminator effectively. The proposed method has achieved significant improvement than other anomaly detection methods on Cifar10 and UCI datasets.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/7ece301f8d69674b49c3485af49668ed9f6084c8.pdf",
      "citation_key": "wang20182xz",
      "metadata": {
        "title": "Anomaly Detection via Minimum Likelihood Generative Adversarial Networks",
        "authors": [
          "Chu Wang",
          "Yanming Zhang",
          "Cheng-Lin Liu"
        ],
        "published_date": "2018",
        "abstract": "Anomaly detection aims to detect abnormal events by a model of normality. It plays an important role in many domains such as network intrusion detection, criminal activity identity and so on. With the rapidly growing size of accessible training data and high computation capacities, deep learning based anomaly detection has become more and more popular. In this paper, a new domain-based anomaly detection method based on generative adversarial networks (GAN) is proposed. Minimum likelihood regularization is proposed to make the generator produce more anomalies and prevent it from converging to normal data distribution. Proper ensemble of anomaly scores is shown to improve the stability of discriminator effectively. The proposed method has achieved significant improvement than other anomaly detection methods on Cifar10 and UCI datasets.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/7ece301f8d69674b49c3485af49668ed9f6084c8.pdf",
        "venue": "International Conference on Pattern Recognition",
        "citationCount": 29,
        "score": 4.142857142857142,
        "summary": "Anomaly detection aims to detect abnormal events by a model of normality. It plays an important role in many domains such as network intrusion detection, criminal activity identity and so on. With the rapidly growing size of accessible training data and high computation capacities, deep learning based anomaly detection has become more and more popular. In this paper, a new domain-based anomaly detection method based on generative adversarial networks (GAN) is proposed. Minimum likelihood regularization is proposed to make the generator produce more anomalies and prevent it from converging to normal data distribution. Proper ensemble of anomaly scores is shown to improve the stability of discriminator effectively. The proposed method has achieved significant improvement than other anomaly detection methods on Cifar10 and UCI datasets.",
        "keywords": []
      },
      "file_name": "7ece301f8d69674b49c3485af49668ed9f6084c8.pdf"
    },
    {
      "success": true,
      "doc_id": "84379bc1a6e2cbb9f58602b390ac3a0e",
      "summary": "Generative Adversarial Network (GAN) has been proposed to tackle the exposure bias problem of Neural Machine Translation (NMT). However, the discriminator typically results in the instability of the GAN training due to the inadequate training problem: the search space is so huge that sampled translations are not sufficient for discriminator training. To address this issue and stabilize the GAN training, in this paper, we propose a novel Bidirectional Generative Adversarial Network for Neural Machine Translation (BGAN-NMT), which aims to introduce a generator model to act as the discriminator, whereby the discriminator naturally considers the entire translation space so that the inadequate training problem can be alleviated. To satisfy this property, generator and discriminator are both designed to model the joint probability of sentence pairs, with the difference that, the generator decomposes the joint probability with a source language model and a source-to-target translation model, while the discriminator is formulated as a target language model and a target-to-source translation model. To further leverage the symmetry of them, an auxiliary GAN is introduced and adopts generator and discriminator models of original one as its own discriminator and generator respectively. Two GANs are alternately trained to update the parameters. Experiment results on German-English and Chinese-English translation tasks demonstrate that our method not only stabilizes GAN training but also achieves significant improvements over baseline systems.",
      "intriguing_abstract": "Generative Adversarial Network (GAN) has been proposed to tackle the exposure bias problem of Neural Machine Translation (NMT). However, the discriminator typically results in the instability of the GAN training due to the inadequate training problem: the search space is so huge that sampled translations are not sufficient for discriminator training. To address this issue and stabilize the GAN training, in this paper, we propose a novel Bidirectional Generative Adversarial Network for Neural Machine Translation (BGAN-NMT), which aims to introduce a generator model to act as the discriminator, whereby the discriminator naturally considers the entire translation space so that the inadequate training problem can be alleviated. To satisfy this property, generator and discriminator are both designed to model the joint probability of sentence pairs, with the difference that, the generator decomposes the joint probability with a source language model and a source-to-target translation model, while the discriminator is formulated as a target language model and a target-to-source translation model. To further leverage the symmetry of them, an auxiliary GAN is introduced and adopts generator and discriminator models of original one as its own discriminator and generator respectively. Two GANs are alternately trained to update the parameters. Experiment results on German-English and Chinese-English translation tasks demonstrate that our method not only stabilizes GAN training but also achieves significant improvements over baseline systems.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/a7d87eafa0e0b74b1c65220feb98f8e11d77eda1.pdf",
      "citation_key": "zhang2018oba",
      "metadata": {
        "title": "Bidirectional Generative Adversarial Networks for Neural Machine Translation",
        "authors": [
          "Zhirui Zhang",
          "Shujie Liu",
          "Mu Li",
          "M. Zhou",
          "Enhong Chen"
        ],
        "published_date": "2018",
        "abstract": "Generative Adversarial Network (GAN) has been proposed to tackle the exposure bias problem of Neural Machine Translation (NMT). However, the discriminator typically results in the instability of the GAN training due to the inadequate training problem: the search space is so huge that sampled translations are not sufficient for discriminator training. To address this issue and stabilize the GAN training, in this paper, we propose a novel Bidirectional Generative Adversarial Network for Neural Machine Translation (BGAN-NMT), which aims to introduce a generator model to act as the discriminator, whereby the discriminator naturally considers the entire translation space so that the inadequate training problem can be alleviated. To satisfy this property, generator and discriminator are both designed to model the joint probability of sentence pairs, with the difference that, the generator decomposes the joint probability with a source language model and a source-to-target translation model, while the discriminator is formulated as a target language model and a target-to-source translation model. To further leverage the symmetry of them, an auxiliary GAN is introduced and adopts generator and discriminator models of original one as its own discriminator and generator respectively. Two GANs are alternately trained to update the parameters. Experiment results on German-English and Chinese-English translation tasks demonstrate that our method not only stabilizes GAN training but also achieves significant improvements over baseline systems.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/a7d87eafa0e0b74b1c65220feb98f8e11d77eda1.pdf",
        "venue": "Conference on Computational Natural Language Learning",
        "citationCount": 29,
        "score": 4.142857142857142,
        "summary": "Generative Adversarial Network (GAN) has been proposed to tackle the exposure bias problem of Neural Machine Translation (NMT). However, the discriminator typically results in the instability of the GAN training due to the inadequate training problem: the search space is so huge that sampled translations are not sufficient for discriminator training. To address this issue and stabilize the GAN training, in this paper, we propose a novel Bidirectional Generative Adversarial Network for Neural Machine Translation (BGAN-NMT), which aims to introduce a generator model to act as the discriminator, whereby the discriminator naturally considers the entire translation space so that the inadequate training problem can be alleviated. To satisfy this property, generator and discriminator are both designed to model the joint probability of sentence pairs, with the difference that, the generator decomposes the joint probability with a source language model and a source-to-target translation model, while the discriminator is formulated as a target language model and a target-to-source translation model. To further leverage the symmetry of them, an auxiliary GAN is introduced and adopts generator and discriminator models of original one as its own discriminator and generator respectively. Two GANs are alternately trained to update the parameters. Experiment results on German-English and Chinese-English translation tasks demonstrate that our method not only stabilizes GAN training but also achieves significant improvements over baseline systems.",
        "keywords": []
      },
      "file_name": "a7d87eafa0e0b74b1c65220feb98f8e11d77eda1.pdf"
    },
    {
      "success": true,
      "doc_id": "65c789d08b28b4f595b1df1c235de8d3",
      "summary": "Computed tomography (CT) is a widely-used diag-reproducibility regarding radiomic features, such as intensity, nostic image modality routinely used for assessing anatomical tissue characteristics. However, non-standardized imaging pro-tocols are commonplace, which poses a fundamental challenge in large-scale cross-center CT image analysis. One approach to address the problem is to standardize CT images using generative adversarial network models (GAN). GAN learns the data distribution of training images and generate synthesized images under the same distribution. However, existing GAN models are not directly applicable to this task mainly due to the lack of constraints on the mode of data to generate. Furthermore, they treat every image equally, but in real applications, some images are more difficult to standardize than the others. All these may lead to the lack-of-detail problem in CT image synthesis. We present a new GAN model called GANai to mitigate the differences in radiomic features across CT images captured using non-standard imaging protocols. Given source images, GANai composes new images by specifying a high-level goal that the image features of the synthesized images should be similar to those of the standard images. GANai introduces an alternative improvement training strategy to alternatively and steadily improve model performance. The new training strategy enables a series of technical improvements, including phase-specific loss functions, phase-specific training data, and the adoption of ensemble learning, leading to better model performance. The experimental results show that GANai is significantly better than the existing state-of-the-art image synthesis algorithms on CT image standardization. Also, it significantly improves the efficiency and stability of GAN model training.",
      "intriguing_abstract": "Computed tomography (CT) is a widely-used diag-reproducibility regarding radiomic features, such as intensity, nostic image modality routinely used for assessing anatomical tissue characteristics. However, non-standardized imaging pro-tocols are commonplace, which poses a fundamental challenge in large-scale cross-center CT image analysis. One approach to address the problem is to standardize CT images using generative adversarial network models (GAN). GAN learns the data distribution of training images and generate synthesized images under the same distribution. However, existing GAN models are not directly applicable to this task mainly due to the lack of constraints on the mode of data to generate. Furthermore, they treat every image equally, but in real applications, some images are more difficult to standardize than the others. All these may lead to the lack-of-detail problem in CT image synthesis. We present a new GAN model called GANai to mitigate the differences in radiomic features across CT images captured using non-standard imaging protocols. Given source images, GANai composes new images by specifying a high-level goal that the image features of the synthesized images should be similar to those of the standard images. GANai introduces an alternative improvement training strategy to alternatively and steadily improve model performance. The new training strategy enables a series of technical improvements, including phase-specific loss functions, phase-specific training data, and the adoption of ensemble learning, leading to better model performance. The experimental results show that GANai is significantly better than the existing state-of-the-art image synthesis algorithms on CT image standardization. Also, it significantly improves the efficiency and stability of GAN model training.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/6a514c0c8c031bb4e1cc2ae9032780df408442a5.pdf",
      "citation_key": "liang2018axu",
      "metadata": {
        "title": "GANai: Standardizing CT Images using Generative Adversarial Network with Alternative Improvement",
        "authors": [
          "G. Liang",
          "S. Fouladvand",
          "Jie Zhang",
          "Michael A. Brooks",
          "Nathan Jacobs",
          "Jin Chen"
        ],
        "published_date": "2018",
        "abstract": "Computed tomography (CT) is a widely-used diag-reproducibility regarding radiomic features, such as intensity, nostic image modality routinely used for assessing anatomical tissue characteristics. However, non-standardized imaging pro-tocols are commonplace, which poses a fundamental challenge in large-scale cross-center CT image analysis. One approach to address the problem is to standardize CT images using generative adversarial network models (GAN). GAN learns the data distribution of training images and generate synthesized images under the same distribution. However, existing GAN models are not directly applicable to this task mainly due to the lack of constraints on the mode of data to generate. Furthermore, they treat every image equally, but in real applications, some images are more difficult to standardize than the others. All these may lead to the lack-of-detail problem in CT image synthesis. We present a new GAN model called GANai to mitigate the differences in radiomic features across CT images captured using non-standard imaging protocols. Given source images, GANai composes new images by specifying a high-level goal that the image features of the synthesized images should be similar to those of the standard images. GANai introduces an alternative improvement training strategy to alternatively and steadily improve model performance. The new training strategy enables a series of technical improvements, including phase-specific loss functions, phase-specific training data, and the adoption of ensemble learning, leading to better model performance. The experimental results show that GANai is significantly better than the existing state-of-the-art image synthesis algorithms on CT image standardization. Also, it significantly improves the efficiency and stability of GAN model training.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/6a514c0c8c031bb4e1cc2ae9032780df408442a5.pdf",
        "venue": "bioRxiv",
        "citationCount": 29,
        "score": 4.142857142857142,
        "summary": "Computed tomography (CT) is a widely-used diag-reproducibility regarding radiomic features, such as intensity, nostic image modality routinely used for assessing anatomical tissue characteristics. However, non-standardized imaging pro-tocols are commonplace, which poses a fundamental challenge in large-scale cross-center CT image analysis. One approach to address the problem is to standardize CT images using generative adversarial network models (GAN). GAN learns the data distribution of training images and generate synthesized images under the same distribution. However, existing GAN models are not directly applicable to this task mainly due to the lack of constraints on the mode of data to generate. Furthermore, they treat every image equally, but in real applications, some images are more difficult to standardize than the others. All these may lead to the lack-of-detail problem in CT image synthesis. We present a new GAN model called GANai to mitigate the differences in radiomic features across CT images captured using non-standard imaging protocols. Given source images, GANai composes new images by specifying a high-level goal that the image features of the synthesized images should be similar to those of the standard images. GANai introduces an alternative improvement training strategy to alternatively and steadily improve model performance. The new training strategy enables a series of technical improvements, including phase-specific loss functions, phase-specific training data, and the adoption of ensemble learning, leading to better model performance. The experimental results show that GANai is significantly better than the existing state-of-the-art image synthesis algorithms on CT image standardization. Also, it significantly improves the efficiency and stability of GAN model training.",
        "keywords": []
      },
      "file_name": "6a514c0c8c031bb4e1cc2ae9032780df408442a5.pdf"
    },
    {
      "success": true,
      "doc_id": "e77e18a28a664aff93fbc849f0567b8f",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/b86ce2873e22d6f8fa6f68ff5f050cf36fa9306d.pdf",
      "citation_key": "wiatrak20194ae",
      "metadata": {
        "title": "Stabilizing Generative Adversarial Network Training: A Survey",
        "authors": [
          "Maciej Wiatrak",
          "Stefano V. Albrecht"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/b86ce2873e22d6f8fa6f68ff5f050cf36fa9306d.pdf",
        "venue": "arXiv.org",
        "citationCount": 24,
        "score": 4.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "b86ce2873e22d6f8fa6f68ff5f050cf36fa9306d.pdf"
    },
    {
      "success": true,
      "doc_id": "4d4279d21a33610a791a321da6786f1e",
      "summary": "Generative adversarial networks have made remarkable achievements in generative tasks. However, instability and mode collapse are still frequent problems. We improve the framework of evolutionary generative adversarial networks (E-GANs), calling it phased evolutionary generative adversarial networks (PEGANs), and adopt a self-attention module to improve upon the disadvantages of convolutional operations. During the training process, the discriminator will play against multiple generators simultaneously, where each generator adopts a different objective function as a mutation operation. Every time after the specified number of training iterations, the generator individuals will be evaluated and the best performing generator offspring will be retained for the next round of evolution. Based on this, the generator can continuously adjust the training strategy during training, and the self-attention module also enables the model to obtain the modeling ability of long-range dependencies. Experiments on two datasets showed that PEGANs improve the training stability and are competitive in generating high-quality samples.",
      "intriguing_abstract": "Generative adversarial networks have made remarkable achievements in generative tasks. However, instability and mode collapse are still frequent problems. We improve the framework of evolutionary generative adversarial networks (E-GANs), calling it phased evolutionary generative adversarial networks (PEGANs), and adopt a self-attention module to improve upon the disadvantages of convolutional operations. During the training process, the discriminator will play against multiple generators simultaneously, where each generator adopts a different objective function as a mutation operation. Every time after the specified number of training iterations, the generator individuals will be evaluated and the best performing generator offspring will be retained for the next round of evolution. Based on this, the generator can continuously adjust the training strategy during training, and the self-attention module also enables the model to obtain the modeling ability of long-range dependencies. Experiments on two datasets showed that PEGANs improve the training stability and are competitive in generating high-quality samples.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/2844a274111907daea511f8378ccca67a9eb81d1.pdf",
      "citation_key": "xue2022n0r",
      "metadata": {
        "title": "PEGANs: Phased Evolutionary Generative Adversarial Networks with Self-Attention Module",
        "authors": [
          "Yu Xue",
          "Weinan Tong",
          "Ferrante Neri",
          "Yixia Zhang"
        ],
        "published_date": "2022",
        "abstract": "Generative adversarial networks have made remarkable achievements in generative tasks. However, instability and mode collapse are still frequent problems. We improve the framework of evolutionary generative adversarial networks (E-GANs), calling it phased evolutionary generative adversarial networks (PEGANs), and adopt a self-attention module to improve upon the disadvantages of convolutional operations. During the training process, the discriminator will play against multiple generators simultaneously, where each generator adopts a different objective function as a mutation operation. Every time after the specified number of training iterations, the generator individuals will be evaluated and the best performing generator offspring will be retained for the next round of evolution. Based on this, the generator can continuously adjust the training strategy during training, and the self-attention module also enables the model to obtain the modeling ability of long-range dependencies. Experiments on two datasets showed that PEGANs improve the training stability and are competitive in generating high-quality samples.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/2844a274111907daea511f8378ccca67a9eb81d1.pdf",
        "venue": "Mathematics",
        "citationCount": 12,
        "score": 4.0,
        "summary": "Generative adversarial networks have made remarkable achievements in generative tasks. However, instability and mode collapse are still frequent problems. We improve the framework of evolutionary generative adversarial networks (E-GANs), calling it phased evolutionary generative adversarial networks (PEGANs), and adopt a self-attention module to improve upon the disadvantages of convolutional operations. During the training process, the discriminator will play against multiple generators simultaneously, where each generator adopts a different objective function as a mutation operation. Every time after the specified number of training iterations, the generator individuals will be evaluated and the best performing generator offspring will be retained for the next round of evolution. Based on this, the generator can continuously adjust the training strategy during training, and the self-attention module also enables the model to obtain the modeling ability of long-range dependencies. Experiments on two datasets showed that PEGANs improve the training stability and are competitive in generating high-quality samples.",
        "keywords": []
      },
      "file_name": "2844a274111907daea511f8378ccca67a9eb81d1.pdf"
    },
    {
      "success": true,
      "doc_id": "d2e631ecfa306dd72650b7a9477077b9",
      "summary": "Domains such as logo synthesis, in which the data has a high degree of multi-modality, still pose a challenge for generative adversarial networks (GANs). Recent research shows that progressive training (ProGAN) and mapping network extensions (StyleGAN) enable both increased training stability for higher dimensional problems and better feature separation within the embedded latent space. However, these architectures leave limited control over shaping the output of the network. This paper explores a conditional extension to the StyleGAN architecture with the aim of firstly, improving on the low resolution results of previous research and, secondly, increasing the controllability of the output through the use of synthetic class-conditions. Furthermore, methods of extracting such class conditions are explored, where the challenge lies in the fact that, visual logo characteristics are hard to define. The introduced conditional style-based generator architecture is trained on the extracted class-conditions in two experiments and studied relative to the performance of an unconditional model. Results show that, whilst the unconditional model more closely matches the training distribution, high quality conditions enabled the embedding of finer details onto the latent space, leading to more diverse output.",
      "intriguing_abstract": "Domains such as logo synthesis, in which the data has a high degree of multi-modality, still pose a challenge for generative adversarial networks (GANs). Recent research shows that progressive training (ProGAN) and mapping network extensions (StyleGAN) enable both increased training stability for higher dimensional problems and better feature separation within the embedded latent space. However, these architectures leave limited control over shaping the output of the network. This paper explores a conditional extension to the StyleGAN architecture with the aim of firstly, improving on the low resolution results of previous research and, secondly, increasing the controllability of the output through the use of synthetic class-conditions. Furthermore, methods of extracting such class conditions are explored, where the challenge lies in the fact that, visual logo characteristics are hard to define. The introduced conditional style-based generator architecture is trained on the extracted class-conditions in two experiments and studied relative to the performance of an unconditional model. Results show that, whilst the unconditional model more closely matches the training distribution, high quality conditions enabled the embedding of finer details onto the latent space, leading to more diverse output.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/36f7724f28f497d55f720719fb58f1c146ecbc32.pdf",
      "citation_key": "oeldorf2019kj7",
      "metadata": {
        "title": "LoGANv2: Conditional Style-Based Logo Generation with Generative Adversarial Networks",
        "authors": [
          "Cedric Oeldorf",
          "Gerasimos Spanakis"
        ],
        "published_date": "2019",
        "abstract": "Domains such as logo synthesis, in which the data has a high degree of multi-modality, still pose a challenge for generative adversarial networks (GANs). Recent research shows that progressive training (ProGAN) and mapping network extensions (StyleGAN) enable both increased training stability for higher dimensional problems and better feature separation within the embedded latent space. However, these architectures leave limited control over shaping the output of the network. This paper explores a conditional extension to the StyleGAN architecture with the aim of firstly, improving on the low resolution results of previous research and, secondly, increasing the controllability of the output through the use of synthetic class-conditions. Furthermore, methods of extracting such class conditions are explored, where the challenge lies in the fact that, visual logo characteristics are hard to define. The introduced conditional style-based generator architecture is trained on the extracted class-conditions in two experiments and studied relative to the performance of an unconditional model. Results show that, whilst the unconditional model more closely matches the training distribution, high quality conditions enabled the embedding of finer details onto the latent space, leading to more diverse output.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/36f7724f28f497d55f720719fb58f1c146ecbc32.pdf",
        "venue": "International Conference on Machine Learning and Applications",
        "citationCount": 24,
        "score": 4.0,
        "summary": "Domains such as logo synthesis, in which the data has a high degree of multi-modality, still pose a challenge for generative adversarial networks (GANs). Recent research shows that progressive training (ProGAN) and mapping network extensions (StyleGAN) enable both increased training stability for higher dimensional problems and better feature separation within the embedded latent space. However, these architectures leave limited control over shaping the output of the network. This paper explores a conditional extension to the StyleGAN architecture with the aim of firstly, improving on the low resolution results of previous research and, secondly, increasing the controllability of the output through the use of synthetic class-conditions. Furthermore, methods of extracting such class conditions are explored, where the challenge lies in the fact that, visual logo characteristics are hard to define. The introduced conditional style-based generator architecture is trained on the extracted class-conditions in two experiments and studied relative to the performance of an unconditional model. Results show that, whilst the unconditional model more closely matches the training distribution, high quality conditions enabled the embedding of finer details onto the latent space, leading to more diverse output.",
        "keywords": []
      },
      "file_name": "36f7724f28f497d55f720719fb58f1c146ecbc32.pdf"
    },
    {
      "success": true,
      "doc_id": "62c71d6cb0f1028cc39f774e7a7efb00",
      "summary": "Generative adversarial networks (GANs) have been shown to produce realistic samples from high-dimensional distributions, but training them is considered hard. A possible explanation for training instabilities is the inherent imbalance between the networks: While the discriminator is trained directly on both real and fake samples, the generator only has control over the fake samples it produces since the real data distribution is fixed by the choice of a given dataset. We propose a simple modification that gives the generator control over the real samples which leads to a tempered learning process for both generator and discriminator. The real data distribution passes through a lens before being revealed to the discriminator, balancing the generator and discriminator by gradually revealing more detailed features necessary to produce high-quality results. The proposed module automatically adjusts the learning process to the current strength of the networks, yet is generic and easy to add to any GAN variant. In a number of experiments, we show that this can improve quality, stability and/or convergence speed across a range of different GAN architectures (DCGAN, LSGAN, WGAN-GP).",
      "intriguing_abstract": "Generative adversarial networks (GANs) have been shown to produce realistic samples from high-dimensional distributions, but training them is considered hard. A possible explanation for training instabilities is the inherent imbalance between the networks: While the discriminator is trained directly on both real and fake samples, the generator only has control over the fake samples it produces since the real data distribution is fixed by the choice of a given dataset. We propose a simple modification that gives the generator control over the real samples which leads to a tempered learning process for both generator and discriminator. The real data distribution passes through a lens before being revealed to the discriminator, balancing the generator and discriminator by gradually revealing more detailed features necessary to produce high-quality results. The proposed module automatically adjusts the learning process to the current strength of the networks, yet is generic and easy to add to any GAN variant. In a number of experiments, we show that this can improve quality, stability and/or convergence speed across a range of different GAN architectures (DCGAN, LSGAN, WGAN-GP).",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/2cc9e70cd4664533af9bf34b2da7a1c2694616f8.pdf",
      "citation_key": "sajjadi2018w83",
      "metadata": {
        "title": "Tempered Adversarial Networks",
        "authors": [
          "Mehdi S. M. Sajjadi",
          "B. Scholkopf"
        ],
        "published_date": "2018",
        "abstract": "Generative adversarial networks (GANs) have been shown to produce realistic samples from high-dimensional distributions, but training them is considered hard. A possible explanation for training instabilities is the inherent imbalance between the networks: While the discriminator is trained directly on both real and fake samples, the generator only has control over the fake samples it produces since the real data distribution is fixed by the choice of a given dataset. We propose a simple modification that gives the generator control over the real samples which leads to a tempered learning process for both generator and discriminator. The real data distribution passes through a lens before being revealed to the discriminator, balancing the generator and discriminator by gradually revealing more detailed features necessary to produce high-quality results. The proposed module automatically adjusts the learning process to the current strength of the networks, yet is generic and easy to add to any GAN variant. In a number of experiments, we show that this can improve quality, stability and/or convergence speed across a range of different GAN architectures (DCGAN, LSGAN, WGAN-GP).",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/2cc9e70cd4664533af9bf34b2da7a1c2694616f8.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 28,
        "score": 4.0,
        "summary": "Generative adversarial networks (GANs) have been shown to produce realistic samples from high-dimensional distributions, but training them is considered hard. A possible explanation for training instabilities is the inherent imbalance between the networks: While the discriminator is trained directly on both real and fake samples, the generator only has control over the fake samples it produces since the real data distribution is fixed by the choice of a given dataset. We propose a simple modification that gives the generator control over the real samples which leads to a tempered learning process for both generator and discriminator. The real data distribution passes through a lens before being revealed to the discriminator, balancing the generator and discriminator by gradually revealing more detailed features necessary to produce high-quality results. The proposed module automatically adjusts the learning process to the current strength of the networks, yet is generic and easy to add to any GAN variant. In a number of experiments, we show that this can improve quality, stability and/or convergence speed across a range of different GAN architectures (DCGAN, LSGAN, WGAN-GP).",
        "keywords": []
      },
      "file_name": "2cc9e70cd4664533af9bf34b2da7a1c2694616f8.pdf"
    },
    {
      "success": true,
      "doc_id": "0fd71cb1ce512e872e1fd410d16a31f2",
      "summary": "Generative adversarial network (GAN) creates synthetic images to increase data quantity, but whether GAN ensures meaningful morphologic variations is still unknown. We investigated whether GAN-based synthetic images provide sufficient morphologic variations to improve molecular-based prediction, as a rare disease of isocitrate dehydrogenase (IDH)-mutant glioblastomas. GAN was initially trained on 500 normal brains and 110 IDH-mutant high-grade astocytomas, and paired contrast-enhanced T1-weighted and FLAIR MRI data were generated. Diagnostic models were developed from real IDH-wild type (n = 80) with real IDH-mutant glioblastomas (n = 38), or with synthetic IDH-mutant glioblastomas, or augmented by adding both real and synthetic IDH-mutant glioblastomas. Turing tests showed synthetic data showed reality (classification rate of 55%). Both the real and synthetic data showed that a more frontal or insular location (odds ratio [OR] 1.34 vs. 1.52; P = 0.04) and distinct non-enhancing tumor margins (OR 2.68 vs. 3.88; P < 0.001), which become significant predictors of IDH-mutation. In an independent validation set, diagnostic accuracy was higher for the augmented model (90.9% [40/44] and 93.2% [41/44] for each reader, respectively) than for the real model (84.1% [37/44] and 86.4% [38/44] for each reader, respectively). The GAN-based synthetic images yield morphologically variable, realistic-seeming IDH-mutant glioblastomas. GAN will be useful to create a realistic training set in terms of morphologic variations and quality, thereby improving diagnostic performance in a clinical model.",
      "intriguing_abstract": "Generative adversarial network (GAN) creates synthetic images to increase data quantity, but whether GAN ensures meaningful morphologic variations is still unknown. We investigated whether GAN-based synthetic images provide sufficient morphologic variations to improve molecular-based prediction, as a rare disease of isocitrate dehydrogenase (IDH)-mutant glioblastomas. GAN was initially trained on 500 normal brains and 110 IDH-mutant high-grade astocytomas, and paired contrast-enhanced T1-weighted and FLAIR MRI data were generated. Diagnostic models were developed from real IDH-wild type (n = 80) with real IDH-mutant glioblastomas (n = 38), or with synthetic IDH-mutant glioblastomas, or augmented by adding both real and synthetic IDH-mutant glioblastomas. Turing tests showed synthetic data showed reality (classification rate of 55%). Both the real and synthetic data showed that a more frontal or insular location (odds ratio [OR] 1.34 vs. 1.52; P = 0.04) and distinct non-enhancing tumor margins (OR 2.68 vs. 3.88; P < 0.001), which become significant predictors of IDH-mutation. In an independent validation set, diagnostic accuracy was higher for the augmented model (90.9% [40/44] and 93.2% [41/44] for each reader, respectively) than for the real model (84.1% [37/44] and 86.4% [38/44] for each reader, respectively). The GAN-based synthetic images yield morphologically variable, realistic-seeming IDH-mutant glioblastomas. GAN will be useful to create a realistic training set in terms of morphologic variations and quality, thereby improving diagnostic performance in a clinical model.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/aa8d5129fb1a4d3648cec78a2b85ad5970747ed8.pdf",
      "citation_key": "park2021v6f",
      "metadata": {
        "title": "Generative adversarial network for glioblastoma ensures morphologic variations and improves diagnostic model for isocitrate dehydrogenase mutant type",
        "authors": [
          "J. E. Park",
          "Da-in Eun",
          "H. Kim",
          "Da Hyun Lee",
          "Ryoungwoo Jang",
          "Namkug Kim"
        ],
        "published_date": "2021",
        "abstract": "Generative adversarial network (GAN) creates synthetic images to increase data quantity, but whether GAN ensures meaningful morphologic variations is still unknown. We investigated whether GAN-based synthetic images provide sufficient morphologic variations to improve molecular-based prediction, as a rare disease of isocitrate dehydrogenase (IDH)-mutant glioblastomas. GAN was initially trained on 500 normal brains and 110 IDH-mutant high-grade astocytomas, and paired contrast-enhanced T1-weighted and FLAIR MRI data were generated. Diagnostic models were developed from real IDH-wild type (n = 80) with real IDH-mutant glioblastomas (n = 38), or with synthetic IDH-mutant glioblastomas, or augmented by adding both real and synthetic IDH-mutant glioblastomas. Turing tests showed synthetic data showed reality (classification rate of 55%). Both the real and synthetic data showed that a more frontal or insular location (odds ratio [OR] 1.34 vs. 1.52; P = 0.04) and distinct non-enhancing tumor margins (OR 2.68 vs. 3.88; P < 0.001), which become significant predictors of IDH-mutation. In an independent validation set, diagnostic accuracy was higher for the augmented model (90.9% [40/44] and 93.2% [41/44] for each reader, respectively) than for the real model (84.1% [37/44] and 86.4% [38/44] for each reader, respectively). The GAN-based synthetic images yield morphologically variable, realistic-seeming IDH-mutant glioblastomas. GAN will be useful to create a realistic training set in terms of morphologic variations and quality, thereby improving diagnostic performance in a clinical model.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/aa8d5129fb1a4d3648cec78a2b85ad5970747ed8.pdf",
        "venue": "Scientific Reports",
        "citationCount": 16,
        "score": 4.0,
        "summary": "Generative adversarial network (GAN) creates synthetic images to increase data quantity, but whether GAN ensures meaningful morphologic variations is still unknown. We investigated whether GAN-based synthetic images provide sufficient morphologic variations to improve molecular-based prediction, as a rare disease of isocitrate dehydrogenase (IDH)-mutant glioblastomas. GAN was initially trained on 500 normal brains and 110 IDH-mutant high-grade astocytomas, and paired contrast-enhanced T1-weighted and FLAIR MRI data were generated. Diagnostic models were developed from real IDH-wild type (n = 80) with real IDH-mutant glioblastomas (n = 38), or with synthetic IDH-mutant glioblastomas, or augmented by adding both real and synthetic IDH-mutant glioblastomas. Turing tests showed synthetic data showed reality (classification rate of 55%). Both the real and synthetic data showed that a more frontal or insular location (odds ratio [OR] 1.34 vs. 1.52; P = 0.04) and distinct non-enhancing tumor margins (OR 2.68 vs. 3.88; P < 0.001), which become significant predictors of IDH-mutation. In an independent validation set, diagnostic accuracy was higher for the augmented model (90.9% [40/44] and 93.2% [41/44] for each reader, respectively) than for the real model (84.1% [37/44] and 86.4% [38/44] for each reader, respectively). The GAN-based synthetic images yield morphologically variable, realistic-seeming IDH-mutant glioblastomas. GAN will be useful to create a realistic training set in terms of morphologic variations and quality, thereby improving diagnostic performance in a clinical model.",
        "keywords": []
      },
      "file_name": "aa8d5129fb1a4d3648cec78a2b85ad5970747ed8.pdf"
    },
    {
      "success": true,
      "doc_id": "704ce275108dbc3358a09657b1219793",
      "summary": "This article presents a new Self-growing and Pruning Generative Adversarial Network (SP-GAN) for realistic image generation. In contrast to traditional GAN models, our SP-GAN is able to dynamically adjust the size and architecture of a network in the training stage by using the proposed self-growing and pruning mechanisms. To be more specific, we first train two seed networks as the generator and discriminator; each contains a small number of convolution kernels. Such small-scale networks are much easier and faster to train than large-capacity networks. Second, in the self-growing step, we replicate the convolution kernels of each seed network to augment the scale of the network, followed by fine-tuning the augmented/expanded network. More importantly, to prevent the excessive growth of each seed network in the self-growing stage, we propose a pruning strategy that reduces the redundancy of an augmented network, yielding the optimal scale of the network. Finally, we design a new adaptive loss function that is treated as a variable loss computational process for the training of the proposed SP-GAN model. By design, the hyperparameters of the loss function can dynamically adapt to different training stages. Experimental results obtained on a set of data sets demonstrate the merits of the proposed method, especially in terms of the stability and efficiency of network training. The source code of the proposed SP-GAN method is publicly available at https://github.com/Lambert-chen/SPGAN.git.",
      "intriguing_abstract": "This article presents a new Self-growing and Pruning Generative Adversarial Network (SP-GAN) for realistic image generation. In contrast to traditional GAN models, our SP-GAN is able to dynamically adjust the size and architecture of a network in the training stage by using the proposed self-growing and pruning mechanisms. To be more specific, we first train two seed networks as the generator and discriminator; each contains a small number of convolution kernels. Such small-scale networks are much easier and faster to train than large-capacity networks. Second, in the self-growing step, we replicate the convolution kernels of each seed network to augment the scale of the network, followed by fine-tuning the augmented/expanded network. More importantly, to prevent the excessive growth of each seed network in the self-growing stage, we propose a pruning strategy that reduces the redundancy of an augmented network, yielding the optimal scale of the network. Finally, we design a new adaptive loss function that is treated as a variable loss computational process for the training of the proposed SP-GAN model. By design, the hyperparameters of the loss function can dynamically adapt to different training stages. Experimental results obtained on a set of data sets demonstrate the merits of the proposed method, especially in terms of the stability and efficiency of network training. The source code of the proposed SP-GAN method is publicly available at https://github.com/Lambert-chen/SPGAN.git.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/ac5115ece8201ba946375b4515894e2e9a477a86.pdf",
      "citation_key": "song2020mj8",
      "metadata": {
        "title": "SP-GAN: Self-Growing and Pruning Generative Adversarial Networks",
        "authors": [
          "Xiaoning Song",
          "Yao Chen",
          "Zhenhua Feng",
          "Guosheng Hu",
          "Dong-Jun Yu",
          "Xiaojun Wu"
        ],
        "published_date": "2020",
        "abstract": "This article presents a new Self-growing and Pruning Generative Adversarial Network (SP-GAN) for realistic image generation. In contrast to traditional GAN models, our SP-GAN is able to dynamically adjust the size and architecture of a network in the training stage by using the proposed self-growing and pruning mechanisms. To be more specific, we first train two seed networks as the generator and discriminator; each contains a small number of convolution kernels. Such small-scale networks are much easier and faster to train than large-capacity networks. Second, in the self-growing step, we replicate the convolution kernels of each seed network to augment the scale of the network, followed by fine-tuning the augmented/expanded network. More importantly, to prevent the excessive growth of each seed network in the self-growing stage, we propose a pruning strategy that reduces the redundancy of an augmented network, yielding the optimal scale of the network. Finally, we design a new adaptive loss function that is treated as a variable loss computational process for the training of the proposed SP-GAN model. By design, the hyperparameters of the loss function can dynamically adapt to different training stages. Experimental results obtained on a set of data sets demonstrate the merits of the proposed method, especially in terms of the stability and efficiency of network training. The source code of the proposed SP-GAN method is publicly available at https://github.com/Lambert-chen/SPGAN.git.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/ac5115ece8201ba946375b4515894e2e9a477a86.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 19,
        "score": 3.8000000000000003,
        "summary": "This article presents a new Self-growing and Pruning Generative Adversarial Network (SP-GAN) for realistic image generation. In contrast to traditional GAN models, our SP-GAN is able to dynamically adjust the size and architecture of a network in the training stage by using the proposed self-growing and pruning mechanisms. To be more specific, we first train two seed networks as the generator and discriminator; each contains a small number of convolution kernels. Such small-scale networks are much easier and faster to train than large-capacity networks. Second, in the self-growing step, we replicate the convolution kernels of each seed network to augment the scale of the network, followed by fine-tuning the augmented/expanded network. More importantly, to prevent the excessive growth of each seed network in the self-growing stage, we propose a pruning strategy that reduces the redundancy of an augmented network, yielding the optimal scale of the network. Finally, we design a new adaptive loss function that is treated as a variable loss computational process for the training of the proposed SP-GAN model. By design, the hyperparameters of the loss function can dynamically adapt to different training stages. Experimental results obtained on a set of data sets demonstrate the merits of the proposed method, especially in terms of the stability and efficiency of network training. The source code of the proposed SP-GAN method is publicly available at https://github.com/Lambert-chen/SPGAN.git.",
        "keywords": []
      },
      "file_name": "ac5115ece8201ba946375b4515894e2e9a477a86.pdf"
    },
    {
      "success": true,
      "doc_id": "02ec6e6d5191a0330002d7dec0fba553",
      "summary": "A myriad of recent literary works have leveraged generative adversarial networks (GANs) to generate unseen evasion samples. The purpose is to annex the generated data with the original train set for adversarial training to improve the detection performance of machine learning (ML) classifiers. The quality of generated adversarial samples relies on the adequacy of training data samples. However, in low data regimes like medical diagnostic imaging and cybersecurity, the anomaly samples are scarce in number. This paper proposes a novel GAN design called evasion generative adversarial network (EVAGAN) that is more suitable for low data regime problems that use oversampling for detection improvement of ML classifiers. EVAGAN not only can generate evasion samples but its discriminator can act as an evasion-aware classifier. We have considered auxiliary classifier GAN (ACGAN) as a benchmark to evaluate the performance of EVAGAN on cybersecurity (ISCX-2014, CIC-2017, and CIC2018) botnet and computer vision (MNIST) datasets. We demonstrate that EVAGAN outperforms ACGAN for unbalanced datasets with respect to detection performance, training stability, and time complexity. EVAGAN's generator quickly learns to generate the low sample class and hardens its discriminator simultaneously. In contrast to ML classifiers that require security hardening after being adversarially trained by GAN-generated data, EVAGAN renders it needless. The experimental analysis proves that EVAGAN is an efficient evasion hardened model for low data regimes for the selected cybersecurity and computer vision datasets.",
      "intriguing_abstract": "A myriad of recent literary works have leveraged generative adversarial networks (GANs) to generate unseen evasion samples. The purpose is to annex the generated data with the original train set for adversarial training to improve the detection performance of machine learning (ML) classifiers. The quality of generated adversarial samples relies on the adequacy of training data samples. However, in low data regimes like medical diagnostic imaging and cybersecurity, the anomaly samples are scarce in number. This paper proposes a novel GAN design called evasion generative adversarial network (EVAGAN) that is more suitable for low data regime problems that use oversampling for detection improvement of ML classifiers. EVAGAN not only can generate evasion samples but its discriminator can act as an evasion-aware classifier. We have considered auxiliary classifier GAN (ACGAN) as a benchmark to evaluate the performance of EVAGAN on cybersecurity (ISCX-2014, CIC-2017, and CIC2018) botnet and computer vision (MNIST) datasets. We demonstrate that EVAGAN outperforms ACGAN for unbalanced datasets with respect to detection performance, training stability, and time complexity. EVAGAN's generator quickly learns to generate the low sample class and hardens its discriminator simultaneously. In contrast to ML classifiers that require security hardening after being adversarially trained by GAN-generated data, EVAGAN renders it needless. The experimental analysis proves that EVAGAN is an efficient evasion hardened model for low data regimes for the selected cybersecurity and computer vision datasets.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/a4f020c2339f1cf3595bef233b429f93460f0c0c.pdf",
      "citation_key": "randhawa2021ksq",
      "metadata": {
        "title": "Evasion Generative Adversarial Network for Low Data Regimes",
        "authors": [
          "Rizwan Hamid Randhawa",
          "N. Aslam",
          "Mohammad Alauthman",
          "Husnain Rafiq"
        ],
        "published_date": "2021",
        "abstract": "A myriad of recent literary works have leveraged generative adversarial networks (GANs) to generate unseen evasion samples. The purpose is to annex the generated data with the original train set for adversarial training to improve the detection performance of machine learning (ML) classifiers. The quality of generated adversarial samples relies on the adequacy of training data samples. However, in low data regimes like medical diagnostic imaging and cybersecurity, the anomaly samples are scarce in number. This paper proposes a novel GAN design called evasion generative adversarial network (EVAGAN) that is more suitable for low data regime problems that use oversampling for detection improvement of ML classifiers. EVAGAN not only can generate evasion samples but its discriminator can act as an evasion-aware classifier. We have considered auxiliary classifier GAN (ACGAN) as a benchmark to evaluate the performance of EVAGAN on cybersecurity (ISCX-2014, CIC-2017, and CIC2018) botnet and computer vision (MNIST) datasets. We demonstrate that EVAGAN outperforms ACGAN for unbalanced datasets with respect to detection performance, training stability, and time complexity. EVAGAN's generator quickly learns to generate the low sample class and hardens its discriminator simultaneously. In contrast to ML classifiers that require security hardening after being adversarially trained by GAN-generated data, EVAGAN renders it needless. The experimental analysis proves that EVAGAN is an efficient evasion hardened model for low data regimes for the selected cybersecurity and computer vision datasets.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/a4f020c2339f1cf3595bef233b429f93460f0c0c.pdf",
        "venue": "IEEE Transactions on Artificial Intelligence",
        "citationCount": 15,
        "score": 3.75,
        "summary": "A myriad of recent literary works have leveraged generative adversarial networks (GANs) to generate unseen evasion samples. The purpose is to annex the generated data with the original train set for adversarial training to improve the detection performance of machine learning (ML) classifiers. The quality of generated adversarial samples relies on the adequacy of training data samples. However, in low data regimes like medical diagnostic imaging and cybersecurity, the anomaly samples are scarce in number. This paper proposes a novel GAN design called evasion generative adversarial network (EVAGAN) that is more suitable for low data regime problems that use oversampling for detection improvement of ML classifiers. EVAGAN not only can generate evasion samples but its discriminator can act as an evasion-aware classifier. We have considered auxiliary classifier GAN (ACGAN) as a benchmark to evaluate the performance of EVAGAN on cybersecurity (ISCX-2014, CIC-2017, and CIC2018) botnet and computer vision (MNIST) datasets. We demonstrate that EVAGAN outperforms ACGAN for unbalanced datasets with respect to detection performance, training stability, and time complexity. EVAGAN's generator quickly learns to generate the low sample class and hardens its discriminator simultaneously. In contrast to ML classifiers that require security hardening after being adversarially trained by GAN-generated data, EVAGAN renders it needless. The experimental analysis proves that EVAGAN is an efficient evasion hardened model for low data regimes for the selected cybersecurity and computer vision datasets.",
        "keywords": []
      },
      "file_name": "a4f020c2339f1cf3595bef233b429f93460f0c0c.pdf"
    },
    {
      "success": true,
      "doc_id": "68cb1d1c86e247ded95ff47ed2fef067",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/75343727ea5dff0e49b5c22068b9fc426df973a7.pdf",
      "citation_key": "wang2020vbt",
      "metadata": {
        "title": "Improved face super-resolution generative adversarial networks",
        "authors": [
          "Mengxue Wang",
          "Zhenxue Chen",
          "Q. M. J. Wu",
          "Muwei Jian"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/75343727ea5dff0e49b5c22068b9fc426df973a7.pdf",
        "venue": "Machine Vision and Applications",
        "citationCount": 18,
        "score": 3.6,
        "summary": "",
        "keywords": []
      },
      "file_name": "75343727ea5dff0e49b5c22068b9fc426df973a7.pdf"
    },
    {
      "success": true,
      "doc_id": "35dbda12bcd81e069b3544061da97e80",
      "summary": "In this paper, we propose a novel technique for generating images in the 3D domain from images with high degree of geometrical transformations. By coalescing two popular concurrent methods that have seen rapid ascension to the machine learning zeitgeist in recent years: GANs (Goodfellow et al.) and Capsule networks (Sabour, Hinton et al.) - we present: CapsGAN. We show that CapsGAN performs better than or equal to traditional CNN based GANs in generating images with high geometric transformations using rotated MNIST. In the process, we also show the efficacy of using capsules architecture in the GANs domain. Furthermore, we tackle the Gordian Knot in training GANs - the performance control and training stability by experimenting with using Wasserstein distance (gradient clipping, penalty) and Spectral Normalization. The experimental findings of this paper should propel the application of capsules and GANs in the still exciting and nascent domain of 3D image generation, and plausibly video (frame) generation.",
      "intriguing_abstract": "In this paper, we propose a novel technique for generating images in the 3D domain from images with high degree of geometrical transformations. By coalescing two popular concurrent methods that have seen rapid ascension to the machine learning zeitgeist in recent years: GANs (Goodfellow et al.) and Capsule networks (Sabour, Hinton et al.) - we present: CapsGAN. We show that CapsGAN performs better than or equal to traditional CNN based GANs in generating images with high geometric transformations using rotated MNIST. In the process, we also show the efficacy of using capsules architecture in the GANs domain. Furthermore, we tackle the Gordian Knot in training GANs - the performance control and training stability by experimenting with using Wasserstein distance (gradient clipping, penalty) and Spectral Normalization. The experimental findings of this paper should propel the application of capsules and GANs in the still exciting and nascent domain of 3D image generation, and plausibly video (frame) generation.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/c176ac94717fa2e99fbf0039a05597f30fed34db.pdf",
      "citation_key": "saqur2018oqp",
      "metadata": {
        "title": "CapsGAN: Using Dynamic Routing for Generative Adversarial Networks",
        "authors": [
          "Raeid Saqur",
          "Sal Vivona"
        ],
        "published_date": "2018",
        "abstract": "In this paper, we propose a novel technique for generating images in the 3D domain from images with high degree of geometrical transformations. By coalescing two popular concurrent methods that have seen rapid ascension to the machine learning zeitgeist in recent years: GANs (Goodfellow et al.) and Capsule networks (Sabour, Hinton et al.) - we present: CapsGAN. We show that CapsGAN performs better than or equal to traditional CNN based GANs in generating images with high geometric transformations using rotated MNIST. In the process, we also show the efficacy of using capsules architecture in the GANs domain. Furthermore, we tackle the Gordian Knot in training GANs - the performance control and training stability by experimenting with using Wasserstein distance (gradient clipping, penalty) and Spectral Normalization. The experimental findings of this paper should propel the application of capsules and GANs in the still exciting and nascent domain of 3D image generation, and plausibly video (frame) generation.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/c176ac94717fa2e99fbf0039a05597f30fed34db.pdf",
        "venue": "Advances in Intelligent Systems and Computing",
        "citationCount": 24,
        "score": 3.4285714285714284,
        "summary": "In this paper, we propose a novel technique for generating images in the 3D domain from images with high degree of geometrical transformations. By coalescing two popular concurrent methods that have seen rapid ascension to the machine learning zeitgeist in recent years: GANs (Goodfellow et al.) and Capsule networks (Sabour, Hinton et al.) - we present: CapsGAN. We show that CapsGAN performs better than or equal to traditional CNN based GANs in generating images with high geometric transformations using rotated MNIST. In the process, we also show the efficacy of using capsules architecture in the GANs domain. Furthermore, we tackle the Gordian Knot in training GANs - the performance control and training stability by experimenting with using Wasserstein distance (gradient clipping, penalty) and Spectral Normalization. The experimental findings of this paper should propel the application of capsules and GANs in the still exciting and nascent domain of 3D image generation, and plausibly video (frame) generation.",
        "keywords": []
      },
      "file_name": "c176ac94717fa2e99fbf0039a05597f30fed34db.pdf"
    },
    {
      "success": true,
      "doc_id": "9181d23a208184425ba71e5c792394b9",
      "summary": "As an important model of deep learning, semi-supervised learning models are based on Generative Adversarial Nets (GANs) and have achieved a competitive performance on standard optical images. However, the training of GANs becomes unstable when they are applied to SAR images, which reduces the feature extraction capability of the discriminator in GANs. This paper presents a new semi-supervised GANs with Multiple generators and a classifier (MCGAN). This model improves the stability of training for SAR images by employing multiple generators. A multi-classifier is introduced to the new GANs to utilize the labeled images during the training of the GANs, which shares the low level layers with the discriminator. Then, the layers of the trained discriminator and the classifier construct the recognition network for SAR images after having been finely tuned using a small number of the labeled images. Experiments on the Moving and Stationary Target Acquisition and Recognition (MSTAR) databases show that the proposed recognition network achieves a better and more stable recognition performance than several traditional semi-supervised methods as well as other GANs-based semi-supervised methods.",
      "intriguing_abstract": "As an important model of deep learning, semi-supervised learning models are based on Generative Adversarial Nets (GANs) and have achieved a competitive performance on standard optical images. However, the training of GANs becomes unstable when they are applied to SAR images, which reduces the feature extraction capability of the discriminator in GANs. This paper presents a new semi-supervised GANs with Multiple generators and a classifier (MCGAN). This model improves the stability of training for SAR images by employing multiple generators. A multi-classifier is introduced to the new GANs to utilize the labeled images during the training of the GANs, which shares the low level layers with the discriminator. Then, the layers of the trained discriminator and the classifier construct the recognition network for SAR images after having been finely tuned using a small number of the labeled images. Experiments on the Moving and Stationary Target Acquisition and Recognition (MSTAR) databases show that the proposed recognition network achieves a better and more stable recognition performance than several traditional semi-supervised methods as well as other GANs-based semi-supervised methods.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/1d21ce033822c23f499179dd19769f7b94077d6b.pdf",
      "citation_key": "gao2018d4g",
      "metadata": {
        "title": "Semi-Supervised Generative Adversarial Nets with Multiple Generators for SAR Image Recognition",
        "authors": [
          "F. Gao",
          "Fei Ma",
          "Jun Wang",
          "Jinping Sun",
          "Erfu Yang",
          "Huiyu Zhou"
        ],
        "published_date": "2018",
        "abstract": "As an important model of deep learning, semi-supervised learning models are based on Generative Adversarial Nets (GANs) and have achieved a competitive performance on standard optical images. However, the training of GANs becomes unstable when they are applied to SAR images, which reduces the feature extraction capability of the discriminator in GANs. This paper presents a new semi-supervised GANs with Multiple generators and a classifier (MCGAN). This model improves the stability of training for SAR images by employing multiple generators. A multi-classifier is introduced to the new GANs to utilize the labeled images during the training of the GANs, which shares the low level layers with the discriminator. Then, the layers of the trained discriminator and the classifier construct the recognition network for SAR images after having been finely tuned using a small number of the labeled images. Experiments on the Moving and Stationary Target Acquisition and Recognition (MSTAR) databases show that the proposed recognition network achieves a better and more stable recognition performance than several traditional semi-supervised methods as well as other GANs-based semi-supervised methods.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/1d21ce033822c23f499179dd19769f7b94077d6b.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 24,
        "score": 3.4285714285714284,
        "summary": "As an important model of deep learning, semi-supervised learning models are based on Generative Adversarial Nets (GANs) and have achieved a competitive performance on standard optical images. However, the training of GANs becomes unstable when they are applied to SAR images, which reduces the feature extraction capability of the discriminator in GANs. This paper presents a new semi-supervised GANs with Multiple generators and a classifier (MCGAN). This model improves the stability of training for SAR images by employing multiple generators. A multi-classifier is introduced to the new GANs to utilize the labeled images during the training of the GANs, which shares the low level layers with the discriminator. Then, the layers of the trained discriminator and the classifier construct the recognition network for SAR images after having been finely tuned using a small number of the labeled images. Experiments on the Moving and Stationary Target Acquisition and Recognition (MSTAR) databases show that the proposed recognition network achieves a better and more stable recognition performance than several traditional semi-supervised methods as well as other GANs-based semi-supervised methods.",
        "keywords": []
      },
      "file_name": "1d21ce033822c23f499179dd19769f7b94077d6b.pdf"
    },
    {
      "success": true,
      "doc_id": "fadfad34c7779f3ac2691d70bd7d8f9f",
      "summary": "Recent techniques built on generative adversarial networks (GANs), such as cycle-consistent GANs, are able to learn mappings among different domains built from unpaired data sets, through min–max optimization games between generators and discriminators. However, it remains challenging to stabilize the training process and thus cyclic models fall into mode collapse accompanied by the success of discriminator. To address this problem, we propose an novel Bayesian cyclic model and an integrated cyclic framework for interdomain mappings. The proposed method motivated by Bayesian GAN explores the full posteriors of cyclic model via sampling latent variables and optimizes the model with maximum a posteriori (MAP) estimation. Hence, we name it Bayesian CycleGAN. In addition, original CycleGAN cannot generate diversified results. But it is feasible for Bayesian framework to diversify generated images by replacing restricted latent variables in inference process. We evaluate the proposed Bayesian CycleGAN on multiple benchmark data sets, including Cityscapes, Maps, and Monet2photo. The proposed method improve the per-pixel accuracy by 15% for the Cityscapes semantic segmentation task within origin framework and improve 20% within the proposed integrated framework, showing better resilience to imbalance confrontation. The diversified results of Monet2Photo style transfer also demonstrate its superiority over original cyclic model. We provide codes for all of our experiments in https://github.com/ranery/Bayesian-CycleGAN.",
      "intriguing_abstract": "Recent techniques built on generative adversarial networks (GANs), such as cycle-consistent GANs, are able to learn mappings among different domains built from unpaired data sets, through min–max optimization games between generators and discriminators. However, it remains challenging to stabilize the training process and thus cyclic models fall into mode collapse accompanied by the success of discriminator. To address this problem, we propose an novel Bayesian cyclic model and an integrated cyclic framework for interdomain mappings. The proposed method motivated by Bayesian GAN explores the full posteriors of cyclic model via sampling latent variables and optimizes the model with maximum a posteriori (MAP) estimation. Hence, we name it Bayesian CycleGAN. In addition, original CycleGAN cannot generate diversified results. But it is feasible for Bayesian framework to diversify generated images by replacing restricted latent variables in inference process. We evaluate the proposed Bayesian CycleGAN on multiple benchmark data sets, including Cityscapes, Maps, and Monet2photo. The proposed method improve the per-pixel accuracy by 15% for the Cityscapes semantic segmentation task within origin framework and improve 20% within the proposed integrated framework, showing better resilience to imbalance confrontation. The diversified results of Monet2Photo style transfer also demonstrate its superiority over original cyclic model. We provide codes for all of our experiments in https://github.com/ranery/Bayesian-CycleGAN.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/1ea136c958425ddd113e48eebdd07865ebf3a745.pdf",
      "citation_key": "you2018a3m",
      "metadata": {
        "title": "Bayesian Cycle-Consistent Generative Adversarial Networks via Marginalizing Latent Sampling",
        "authors": [
          "Haoran You",
          "Yu Cheng",
          "Tianheng Cheng",
          "Chun-Liang Li",
          "Pan Zhou"
        ],
        "published_date": "2018",
        "abstract": "Recent techniques built on generative adversarial networks (GANs), such as cycle-consistent GANs, are able to learn mappings among different domains built from unpaired data sets, through min–max optimization games between generators and discriminators. However, it remains challenging to stabilize the training process and thus cyclic models fall into mode collapse accompanied by the success of discriminator. To address this problem, we propose an novel Bayesian cyclic model and an integrated cyclic framework for interdomain mappings. The proposed method motivated by Bayesian GAN explores the full posteriors of cyclic model via sampling latent variables and optimizes the model with maximum a posteriori (MAP) estimation. Hence, we name it Bayesian CycleGAN. In addition, original CycleGAN cannot generate diversified results. But it is feasible for Bayesian framework to diversify generated images by replacing restricted latent variables in inference process. We evaluate the proposed Bayesian CycleGAN on multiple benchmark data sets, including Cityscapes, Maps, and Monet2photo. The proposed method improve the per-pixel accuracy by 15% for the Cityscapes semantic segmentation task within origin framework and improve 20% within the proposed integrated framework, showing better resilience to imbalance confrontation. The diversified results of Monet2Photo style transfer also demonstrate its superiority over original cyclic model. We provide codes for all of our experiments in https://github.com/ranery/Bayesian-CycleGAN.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/1ea136c958425ddd113e48eebdd07865ebf3a745.pdf",
        "venue": "IEEE Transactions on Neural Networks and Learning Systems",
        "citationCount": 23,
        "score": 3.2857142857142856,
        "summary": "Recent techniques built on generative adversarial networks (GANs), such as cycle-consistent GANs, are able to learn mappings among different domains built from unpaired data sets, through min–max optimization games between generators and discriminators. However, it remains challenging to stabilize the training process and thus cyclic models fall into mode collapse accompanied by the success of discriminator. To address this problem, we propose an novel Bayesian cyclic model and an integrated cyclic framework for interdomain mappings. The proposed method motivated by Bayesian GAN explores the full posteriors of cyclic model via sampling latent variables and optimizes the model with maximum a posteriori (MAP) estimation. Hence, we name it Bayesian CycleGAN. In addition, original CycleGAN cannot generate diversified results. But it is feasible for Bayesian framework to diversify generated images by replacing restricted latent variables in inference process. We evaluate the proposed Bayesian CycleGAN on multiple benchmark data sets, including Cityscapes, Maps, and Monet2photo. The proposed method improve the per-pixel accuracy by 15% for the Cityscapes semantic segmentation task within origin framework and improve 20% within the proposed integrated framework, showing better resilience to imbalance confrontation. The diversified results of Monet2Photo style transfer also demonstrate its superiority over original cyclic model. We provide codes for all of our experiments in https://github.com/ranery/Bayesian-CycleGAN.",
        "keywords": []
      },
      "file_name": "1ea136c958425ddd113e48eebdd07865ebf3a745.pdf"
    },
    {
      "success": true,
      "doc_id": "1107b3047a00073eef179e7975a70c79",
      "summary": "BACKGROUND\nIncreasing research reveals that long non-coding RNAs (lncRNAs) play an important role in various biological processes of human diseases. Nonetheless, only a handful of lncRNA-disease associations have been experimentally verified. The study of lncRNA-disease association prediction based on the computational model has provided a preliminary basis for biological experiments to a great degree so as to cut down the huge cost of wet lab experiments.\n\n\nOBJECTIVE\nThis study aims to learn the real distribution of lncRNA-disease association from a limited number of known lncRNA-disease association data. This paper proposes a new lncRNA-disease association prediction model called LDA-GAN based on a generative adversarial network (GAN).\n\n\nMETHOD\nAiming at the problems of slow convergence rate, training instabilities, and unavailability of discrete data in traditional GAN, LDA-GAN utilizes the Gumbel-softmax technology to construct a differentiable process for simulating discrete sampling. Meanwhile, the generator and the discriminator of LDA-GAN are integrated to establish the overall optimization goal based on the pairwise loss function.\n\n\nRESULTS\nExperiments on standard datasets demonstrate that LDA-GAN achieves not only high stability and high efficiency in the process of confrontation learning but also gives full play to the semi-supervised learning advantage of generative adversarial learning framework for unlabeled data, which further improves the prediction accuracy of lncRNA-disease association. Besides, case studies show that LDA-GAN can accurately generate potential diseases for several lncRNAs.",
      "intriguing_abstract": "BACKGROUND\nIncreasing research reveals that long non-coding RNAs (lncRNAs) play an important role in various biological processes of human diseases. Nonetheless, only a handful of lncRNA-disease associations have been experimentally verified. The study of lncRNA-disease association prediction based on the computational model has provided a preliminary basis for biological experiments to a great degree so as to cut down the huge cost of wet lab experiments.\n\n\nOBJECTIVE\nThis study aims to learn the real distribution of lncRNA-disease association from a limited number of known lncRNA-disease association data. This paper proposes a new lncRNA-disease association prediction model called LDA-GAN based on a generative adversarial network (GAN).\n\n\nMETHOD\nAiming at the problems of slow convergence rate, training instabilities, and unavailability of discrete data in traditional GAN, LDA-GAN utilizes the Gumbel-softmax technology to construct a differentiable process for simulating discrete sampling. Meanwhile, the generator and the discriminator of LDA-GAN are integrated to establish the overall optimization goal based on the pairwise loss function.\n\n\nRESULTS\nExperiments on standard datasets demonstrate that LDA-GAN achieves not only high stability and high efficiency in the process of confrontation learning but also gives full play to the semi-supervised learning advantage of generative adversarial learning framework for unlabeled data, which further improves the prediction accuracy of lncRNA-disease association. Besides, case studies show that LDA-GAN can accurately generate potential diseases for several lncRNAs.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/89608a379d87d76b24390c3382987492bf39b65f.pdf",
      "citation_key": "du2021bhg",
      "metadata": {
        "title": "Predicting LncRNA-Disease Association Based on Generative Adversarial Network.",
        "authors": [
          "Biao Du",
          "Lin Tang",
          "Lin Liu",
          "Wei Zhou"
        ],
        "published_date": "2021",
        "abstract": "BACKGROUND\nIncreasing research reveals that long non-coding RNAs (lncRNAs) play an important role in various biological processes of human diseases. Nonetheless, only a handful of lncRNA-disease associations have been experimentally verified. The study of lncRNA-disease association prediction based on the computational model has provided a preliminary basis for biological experiments to a great degree so as to cut down the huge cost of wet lab experiments.\n\n\nOBJECTIVE\nThis study aims to learn the real distribution of lncRNA-disease association from a limited number of known lncRNA-disease association data. This paper proposes a new lncRNA-disease association prediction model called LDA-GAN based on a generative adversarial network (GAN).\n\n\nMETHOD\nAiming at the problems of slow convergence rate, training instabilities, and unavailability of discrete data in traditional GAN, LDA-GAN utilizes the Gumbel-softmax technology to construct a differentiable process for simulating discrete sampling. Meanwhile, the generator and the discriminator of LDA-GAN are integrated to establish the overall optimization goal based on the pairwise loss function.\n\n\nRESULTS\nExperiments on standard datasets demonstrate that LDA-GAN achieves not only high stability and high efficiency in the process of confrontation learning but also gives full play to the semi-supervised learning advantage of generative adversarial learning framework for unlabeled data, which further improves the prediction accuracy of lncRNA-disease association. Besides, case studies show that LDA-GAN can accurately generate potential diseases for several lncRNAs.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/89608a379d87d76b24390c3382987492bf39b65f.pdf",
        "venue": "Current Gene Therapy",
        "citationCount": 13,
        "score": 3.25,
        "summary": "BACKGROUND\nIncreasing research reveals that long non-coding RNAs (lncRNAs) play an important role in various biological processes of human diseases. Nonetheless, only a handful of lncRNA-disease associations have been experimentally verified. The study of lncRNA-disease association prediction based on the computational model has provided a preliminary basis for biological experiments to a great degree so as to cut down the huge cost of wet lab experiments.\n\n\nOBJECTIVE\nThis study aims to learn the real distribution of lncRNA-disease association from a limited number of known lncRNA-disease association data. This paper proposes a new lncRNA-disease association prediction model called LDA-GAN based on a generative adversarial network (GAN).\n\n\nMETHOD\nAiming at the problems of slow convergence rate, training instabilities, and unavailability of discrete data in traditional GAN, LDA-GAN utilizes the Gumbel-softmax technology to construct a differentiable process for simulating discrete sampling. Meanwhile, the generator and the discriminator of LDA-GAN are integrated to establish the overall optimization goal based on the pairwise loss function.\n\n\nRESULTS\nExperiments on standard datasets demonstrate that LDA-GAN achieves not only high stability and high efficiency in the process of confrontation learning but also gives full play to the semi-supervised learning advantage of generative adversarial learning framework for unlabeled data, which further improves the prediction accuracy of lncRNA-disease association. Besides, case studies show that LDA-GAN can accurately generate potential diseases for several lncRNAs.",
        "keywords": []
      },
      "file_name": "89608a379d87d76b24390c3382987492bf39b65f.pdf"
    },
    {
      "success": true,
      "doc_id": "c42ccbeff7f0f5c8654bc4f6133c608f",
      "summary": "In this paper, we introduce DuelGAN, a generative adversarial network (GAN) solution to improve the stability of the generated samples and to mitigate mode collapse. Built upon the Vanilla GAN's two-player game between the discriminator $D_1$ and the generator $G$, we introduce a peer discriminator $D_2$ to the min-max game. Similar to previous work using two discriminators, the first role of both $D_1$, $D_2$ is to distinguish between generated samples and real ones, while the generator tries to generate high-quality samples which are able to fool both discriminators. Different from existing methods, we introduce another game between $D_1$ and $D_2$ to discourage their agreement and therefore increase the level of diversity of the generated samples. This property alleviates the issue of early mode collapse by preventing $D_1$ and $D_2$ from converging too fast. We provide theoretical analysis for the equilibrium of the min-max game formed among $G, D_1, D_2$. We offer convergence behavior of DuelGAN as well as stability of the min-max game. It's worth mentioning that DuelGAN operates in the unsupervised setting, and the duel between $D_1$ and $D_2$ does not need any label supervision. Experiments results on a synthetic dataset and on real-world image datasets (MNIST, Fashion MNIST, CIFAR-10, STL-10, CelebA, VGG, and FFHQ) demonstrate that DuelGAN outperforms competitive baseline work in generating diverse and high-quality samples, while only introduces negligible computation cost.",
      "intriguing_abstract": "In this paper, we introduce DuelGAN, a generative adversarial network (GAN) solution to improve the stability of the generated samples and to mitigate mode collapse. Built upon the Vanilla GAN's two-player game between the discriminator $D_1$ and the generator $G$, we introduce a peer discriminator $D_2$ to the min-max game. Similar to previous work using two discriminators, the first role of both $D_1$, $D_2$ is to distinguish between generated samples and real ones, while the generator tries to generate high-quality samples which are able to fool both discriminators. Different from existing methods, we introduce another game between $D_1$ and $D_2$ to discourage their agreement and therefore increase the level of diversity of the generated samples. This property alleviates the issue of early mode collapse by preventing $D_1$ and $D_2$ from converging too fast. We provide theoretical analysis for the equilibrium of the min-max game formed among $G, D_1, D_2$. We offer convergence behavior of DuelGAN as well as stability of the min-max game. It's worth mentioning that DuelGAN operates in the unsupervised setting, and the duel between $D_1$ and $D_2$ does not need any label supervision. Experiments results on a synthetic dataset and on real-world image datasets (MNIST, Fashion MNIST, CIFAR-10, STL-10, CelebA, VGG, and FFHQ) demonstrate that DuelGAN outperforms competitive baseline work in generating diverse and high-quality samples, while only introduces negligible computation cost.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/7564221c59886c6411b6fa474852d8012908cbfa.pdf",
      "citation_key": "wei2021gla",
      "metadata": {
        "title": "DuelGAN: A Duel Between Two Discriminators Stabilizes the GAN Training",
        "authors": [
          "Jiaheng Wei",
          "Minghao Liu",
          "Jiahao Luo",
          "Andrew Zhu",
          "James Davis",
          "Yang Liu"
        ],
        "published_date": "2021",
        "abstract": "In this paper, we introduce DuelGAN, a generative adversarial network (GAN) solution to improve the stability of the generated samples and to mitigate mode collapse. Built upon the Vanilla GAN's two-player game between the discriminator $D_1$ and the generator $G$, we introduce a peer discriminator $D_2$ to the min-max game. Similar to previous work using two discriminators, the first role of both $D_1$, $D_2$ is to distinguish between generated samples and real ones, while the generator tries to generate high-quality samples which are able to fool both discriminators. Different from existing methods, we introduce another game between $D_1$ and $D_2$ to discourage their agreement and therefore increase the level of diversity of the generated samples. This property alleviates the issue of early mode collapse by preventing $D_1$ and $D_2$ from converging too fast. We provide theoretical analysis for the equilibrium of the min-max game formed among $G, D_1, D_2$. We offer convergence behavior of DuelGAN as well as stability of the min-max game. It's worth mentioning that DuelGAN operates in the unsupervised setting, and the duel between $D_1$ and $D_2$ does not need any label supervision. Experiments results on a synthetic dataset and on real-world image datasets (MNIST, Fashion MNIST, CIFAR-10, STL-10, CelebA, VGG, and FFHQ) demonstrate that DuelGAN outperforms competitive baseline work in generating diverse and high-quality samples, while only introduces negligible computation cost.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/7564221c59886c6411b6fa474852d8012908cbfa.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 13,
        "score": 3.25,
        "summary": "In this paper, we introduce DuelGAN, a generative adversarial network (GAN) solution to improve the stability of the generated samples and to mitigate mode collapse. Built upon the Vanilla GAN's two-player game between the discriminator $D_1$ and the generator $G$, we introduce a peer discriminator $D_2$ to the min-max game. Similar to previous work using two discriminators, the first role of both $D_1$, $D_2$ is to distinguish between generated samples and real ones, while the generator tries to generate high-quality samples which are able to fool both discriminators. Different from existing methods, we introduce another game between $D_1$ and $D_2$ to discourage their agreement and therefore increase the level of diversity of the generated samples. This property alleviates the issue of early mode collapse by preventing $D_1$ and $D_2$ from converging too fast. We provide theoretical analysis for the equilibrium of the min-max game formed among $G, D_1, D_2$. We offer convergence behavior of DuelGAN as well as stability of the min-max game. It's worth mentioning that DuelGAN operates in the unsupervised setting, and the duel between $D_1$ and $D_2$ does not need any label supervision. Experiments results on a synthetic dataset and on real-world image datasets (MNIST, Fashion MNIST, CIFAR-10, STL-10, CelebA, VGG, and FFHQ) demonstrate that DuelGAN outperforms competitive baseline work in generating diverse and high-quality samples, while only introduces negligible computation cost.",
        "keywords": []
      },
      "file_name": "7564221c59886c6411b6fa474852d8012908cbfa.pdf"
    },
    {
      "success": true,
      "doc_id": "fb204dc70d33a7f8e00dad497ff6ad37",
      "summary": "In the years since Goodfellow et al. introduced Generative Adversarial Networks (GANs), there has been an explosion in the breadth and quality of generative model applications. Despite this work, GANs still have a long way to go before they see mainstream adoption, owing largely to their infamous training instability. Here I propose the Autoencoding Generative Adversarial Network (AEGAN), a four-network model which learns a bijective mapping between a specified latent space and a given sample space by applying an adversarial loss and a reconstruction loss to both the generated images and the generated latent vectors. The AEGAN technique offers several improvements to typical GAN training, including training stabilization, mode-collapse prevention, and permitting the direct interpolation between real samples. The effectiveness of the technique is illustrated using an anime face dataset.",
      "intriguing_abstract": "In the years since Goodfellow et al. introduced Generative Adversarial Networks (GANs), there has been an explosion in the breadth and quality of generative model applications. Despite this work, GANs still have a long way to go before they see mainstream adoption, owing largely to their infamous training instability. Here I propose the Autoencoding Generative Adversarial Network (AEGAN), a four-network model which learns a bijective mapping between a specified latent space and a given sample space by applying an adversarial loss and a reconstruction loss to both the generated images and the generated latent vectors. The AEGAN technique offers several improvements to typical GAN training, including training stabilization, mode-collapse prevention, and permitting the direct interpolation between real samples. The effectiveness of the technique is illustrated using an anime face dataset.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/44d1a62a864ee8a41f0477529ec0662758d4be74.pdf",
      "citation_key": "lazarou2020gu8",
      "metadata": {
        "title": "Autoencoding Generative Adversarial Networks",
        "authors": [
          "Conor Lazarou"
        ],
        "published_date": "2020",
        "abstract": "In the years since Goodfellow et al. introduced Generative Adversarial Networks (GANs), there has been an explosion in the breadth and quality of generative model applications. Despite this work, GANs still have a long way to go before they see mainstream adoption, owing largely to their infamous training instability. Here I propose the Autoencoding Generative Adversarial Network (AEGAN), a four-network model which learns a bijective mapping between a specified latent space and a given sample space by applying an adversarial loss and a reconstruction loss to both the generated images and the generated latent vectors. The AEGAN technique offers several improvements to typical GAN training, including training stabilization, mode-collapse prevention, and permitting the direct interpolation between real samples. The effectiveness of the technique is illustrated using an anime face dataset.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/44d1a62a864ee8a41f0477529ec0662758d4be74.pdf",
        "venue": "arXiv.org",
        "citationCount": 16,
        "score": 3.2,
        "summary": "In the years since Goodfellow et al. introduced Generative Adversarial Networks (GANs), there has been an explosion in the breadth and quality of generative model applications. Despite this work, GANs still have a long way to go before they see mainstream adoption, owing largely to their infamous training instability. Here I propose the Autoencoding Generative Adversarial Network (AEGAN), a four-network model which learns a bijective mapping between a specified latent space and a given sample space by applying an adversarial loss and a reconstruction loss to both the generated images and the generated latent vectors. The AEGAN technique offers several improvements to typical GAN training, including training stabilization, mode-collapse prevention, and permitting the direct interpolation between real samples. The effectiveness of the technique is illustrated using an anime face dataset.",
        "keywords": []
      },
      "file_name": "44d1a62a864ee8a41f0477529ec0662758d4be74.pdf"
    },
    {
      "success": true,
      "doc_id": "78b0d54e831938d0e1ff2fea5bbe8a04",
      "summary": "Generative Adversarial Networks (GAN) has become more and more popular these years. However, it is difficult to train and suffers from the training instability problem. To tackle this difficulty, this paper proposes a novel approach. Our idea is intuitive but proven to be very useful. In essence, it combines saturating loss and non-saturating loss into the loss function. Thus it will exploit the complementary statistical properties from two kinds of loss functions to effectively improve the training stability. We term our method twin discriminator Generative Adversarial Networks (TWGAN), which, unlike GAN, has a generator and a twin discriminator. The twin discriminator consists of two discriminators with identical architecture and both of them aim to distinguish whether the samples are from real data or fake data. We develop theoretical analysis to show that, given the optimal discriminators, optimizing the generator of TWGAN reduces to minimizing the Kullback-Leibler (KL) divergence between the distribution of generated data (<inline-formula><tex-math notation=\"LaTeX\">$P_g$</tex-math></inline-formula>) and the distribution of real data (<inline-formula><tex-math notation=\"LaTeX\">$P_data$</tex-math></inline-formula>), hence effectively addressing the training instability problem. Extensive experiments on MNIST, Fashion MNIST, CIFAR-10/100 and STL-10 datasets demonstrate that the competitive performance of our TWGAN in generating good quality and diverse samples over baselines. The obtained highest inception score (IS) and lowest Fr <inline-formula><tex-math notation=\"LaTeX\">$\\acute{e}$</tex-math></inline-formula> chet Inception Distance (FID), compared with other state-of-the-art GANs, show the superiority of our TWGAN.",
      "intriguing_abstract": "Generative Adversarial Networks (GAN) has become more and more popular these years. However, it is difficult to train and suffers from the training instability problem. To tackle this difficulty, this paper proposes a novel approach. Our idea is intuitive but proven to be very useful. In essence, it combines saturating loss and non-saturating loss into the loss function. Thus it will exploit the complementary statistical properties from two kinds of loss functions to effectively improve the training stability. We term our method twin discriminator Generative Adversarial Networks (TWGAN), which, unlike GAN, has a generator and a twin discriminator. The twin discriminator consists of two discriminators with identical architecture and both of them aim to distinguish whether the samples are from real data or fake data. We develop theoretical analysis to show that, given the optimal discriminators, optimizing the generator of TWGAN reduces to minimizing the Kullback-Leibler (KL) divergence between the distribution of generated data (<inline-formula><tex-math notation=\"LaTeX\">$P_g$</tex-math></inline-formula>) and the distribution of real data (<inline-formula><tex-math notation=\"LaTeX\">$P_data$</tex-math></inline-formula>), hence effectively addressing the training instability problem. Extensive experiments on MNIST, Fashion MNIST, CIFAR-10/100 and STL-10 datasets demonstrate that the competitive performance of our TWGAN in generating good quality and diverse samples over baselines. The obtained highest inception score (IS) and lowest Fr <inline-formula><tex-math notation=\"LaTeX\">$\\acute{e}$</tex-math></inline-formula> chet Inception Distance (FID), compared with other state-of-the-art GANs, show the superiority of our TWGAN.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/01e9750abbaca78cecbb33cbb7aebfd7a5de91f0.pdf",
      "citation_key": "zhang2021ypi",
      "metadata": {
        "title": "TWGAN: Twin Discriminator Generative Adversarial Networks",
        "authors": [
          "Zhaoyu Zhang",
          "Mengyan Li",
          "Haonian Xie",
          "Jun Yu",
          "Tongliang Liu",
          "Chang Wen Chen"
        ],
        "published_date": "2021",
        "abstract": "Generative Adversarial Networks (GAN) has become more and more popular these years. However, it is difficult to train and suffers from the training instability problem. To tackle this difficulty, this paper proposes a novel approach. Our idea is intuitive but proven to be very useful. In essence, it combines saturating loss and non-saturating loss into the loss function. Thus it will exploit the complementary statistical properties from two kinds of loss functions to effectively improve the training stability. We term our method twin discriminator Generative Adversarial Networks (TWGAN), which, unlike GAN, has a generator and a twin discriminator. The twin discriminator consists of two discriminators with identical architecture and both of them aim to distinguish whether the samples are from real data or fake data. We develop theoretical analysis to show that, given the optimal discriminators, optimizing the generator of TWGAN reduces to minimizing the Kullback-Leibler (KL) divergence between the distribution of generated data (<inline-formula><tex-math notation=\"LaTeX\">$P_g$</tex-math></inline-formula>) and the distribution of real data (<inline-formula><tex-math notation=\"LaTeX\">$P_data$</tex-math></inline-formula>), hence effectively addressing the training instability problem. Extensive experiments on MNIST, Fashion MNIST, CIFAR-10/100 and STL-10 datasets demonstrate that the competitive performance of our TWGAN in generating good quality and diverse samples over baselines. The obtained highest inception score (IS) and lowest Fr <inline-formula><tex-math notation=\"LaTeX\">$\\acute{e}$</tex-math></inline-formula> chet Inception Distance (FID), compared with other state-of-the-art GANs, show the superiority of our TWGAN.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/01e9750abbaca78cecbb33cbb7aebfd7a5de91f0.pdf",
        "venue": "IEEE transactions on multimedia",
        "citationCount": 12,
        "score": 3.0,
        "summary": "Generative Adversarial Networks (GAN) has become more and more popular these years. However, it is difficult to train and suffers from the training instability problem. To tackle this difficulty, this paper proposes a novel approach. Our idea is intuitive but proven to be very useful. In essence, it combines saturating loss and non-saturating loss into the loss function. Thus it will exploit the complementary statistical properties from two kinds of loss functions to effectively improve the training stability. We term our method twin discriminator Generative Adversarial Networks (TWGAN), which, unlike GAN, has a generator and a twin discriminator. The twin discriminator consists of two discriminators with identical architecture and both of them aim to distinguish whether the samples are from real data or fake data. We develop theoretical analysis to show that, given the optimal discriminators, optimizing the generator of TWGAN reduces to minimizing the Kullback-Leibler (KL) divergence between the distribution of generated data (<inline-formula><tex-math notation=\"LaTeX\">$P_g$</tex-math></inline-formula>) and the distribution of real data (<inline-formula><tex-math notation=\"LaTeX\">$P_data$</tex-math></inline-formula>), hence effectively addressing the training instability problem. Extensive experiments on MNIST, Fashion MNIST, CIFAR-10/100 and STL-10 datasets demonstrate that the competitive performance of our TWGAN in generating good quality and diverse samples over baselines. The obtained highest inception score (IS) and lowest Fr <inline-formula><tex-math notation=\"LaTeX\">$\\acute{e}$</tex-math></inline-formula> chet Inception Distance (FID), compared with other state-of-the-art GANs, show the superiority of our TWGAN.",
        "keywords": []
      },
      "file_name": "01e9750abbaca78cecbb33cbb7aebfd7a5de91f0.pdf"
    },
    {
      "success": true,
      "doc_id": "f02a68a53559cd2409b975302e50b431",
      "summary": "Image inpainting aims to fill missing regions of a damaged image with plausibly synthesized content. Existing methods for image inpainting either fill the missing regions by borrowing information from surrounding areas or generating semantically coherent content from region context. They often produce ambiguous or semantically incoherent content when the missing region is large or with complex structures. In this paper, we present an approach for image inpainting. The completion model based on our proposed algorithm contains one generator, one global discriminator, and one local discriminator. The generator is responsible for inpainting the missing area, the global discriminator aims evaluating whether the repair result has global consistency, and the local discriminator is responsible for identifying whether the repair area is correct. The architecture of the generator is an auto-encoder. We use the skip-connection in the generator to improve the prediction power of the model. Also, we use Wasserstein GAN loss to ensure the stability of training. Experiments on CelebA dataset and LFW dataset demonstrate that our proposed model can deal with large-scale missing pixels and generate realistic completion results.",
      "intriguing_abstract": "Image inpainting aims to fill missing regions of a damaged image with plausibly synthesized content. Existing methods for image inpainting either fill the missing regions by borrowing information from surrounding areas or generating semantically coherent content from region context. They often produce ambiguous or semantically incoherent content when the missing region is large or with complex structures. In this paper, we present an approach for image inpainting. The completion model based on our proposed algorithm contains one generator, one global discriminator, and one local discriminator. The generator is responsible for inpainting the missing area, the global discriminator aims evaluating whether the repair result has global consistency, and the local discriminator is responsible for identifying whether the repair area is correct. The architecture of the generator is an auto-encoder. We use the skip-connection in the generator to improve the prediction power of the model. Also, we use Wasserstein GAN loss to ensure the stability of training. Experiments on CelebA dataset and LFW dataset demonstrate that our proposed model can deal with large-scale missing pixels and generate realistic completion results.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/284d6ebdd626885c857c096a2d564092b6c28b93.pdf",
      "citation_key": "jiang2020e6i",
      "metadata": {
        "title": "Image Inpainting Based on Generative Adversarial Networks",
        "authors": [
          "Yi Jiang",
          "Jiajie Xu",
          "Baoqing Yang",
          "Jing Xu",
          "Junwu Zhu"
        ],
        "published_date": "2020",
        "abstract": "Image inpainting aims to fill missing regions of a damaged image with plausibly synthesized content. Existing methods for image inpainting either fill the missing regions by borrowing information from surrounding areas or generating semantically coherent content from region context. They often produce ambiguous or semantically incoherent content when the missing region is large or with complex structures. In this paper, we present an approach for image inpainting. The completion model based on our proposed algorithm contains one generator, one global discriminator, and one local discriminator. The generator is responsible for inpainting the missing area, the global discriminator aims evaluating whether the repair result has global consistency, and the local discriminator is responsible for identifying whether the repair area is correct. The architecture of the generator is an auto-encoder. We use the skip-connection in the generator to improve the prediction power of the model. Also, we use Wasserstein GAN loss to ensure the stability of training. Experiments on CelebA dataset and LFW dataset demonstrate that our proposed model can deal with large-scale missing pixels and generate realistic completion results.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/284d6ebdd626885c857c096a2d564092b6c28b93.pdf",
        "venue": "IEEE Access",
        "citationCount": 15,
        "score": 3.0,
        "summary": "Image inpainting aims to fill missing regions of a damaged image with plausibly synthesized content. Existing methods for image inpainting either fill the missing regions by borrowing information from surrounding areas or generating semantically coherent content from region context. They often produce ambiguous or semantically incoherent content when the missing region is large or with complex structures. In this paper, we present an approach for image inpainting. The completion model based on our proposed algorithm contains one generator, one global discriminator, and one local discriminator. The generator is responsible for inpainting the missing area, the global discriminator aims evaluating whether the repair result has global consistency, and the local discriminator is responsible for identifying whether the repair area is correct. The architecture of the generator is an auto-encoder. We use the skip-connection in the generator to improve the prediction power of the model. Also, we use Wasserstein GAN loss to ensure the stability of training. Experiments on CelebA dataset and LFW dataset demonstrate that our proposed model can deal with large-scale missing pixels and generate realistic completion results.",
        "keywords": []
      },
      "file_name": "284d6ebdd626885c857c096a2d564092b6c28b93.pdf"
    },
    {
      "success": true,
      "doc_id": "6f5a37e9fb8e2435b62ad0c11d2f273d",
      "summary": "Fault detection is an important and demanding problem in industry. However, in many applications, abnormal data are difficult to be collected and are not available during training. In this paper, we propose a one class fault detection scheme, based on the unsupervised training of a Generative Adversarial Network (GAN). The generator tries to learn the manifold of normal behavior of the process, while the final decision of fault occurrence is taken from the discriminator. The network architectures of the discriminator and the generator and the hyper-parameters, that are used during training, are crucial for the stability of the GAN model. To ensure system convergence during training and to enhance the accuracy of the unsupervised classifier, a model selection algorithm is used. The latter one evaluates each trained model on a validation set of the normal training dataset, based on a proposed performance metric. Also, a method for evaluating the generating ability of each trained generative model is introduced, based on the reconstruction cost of an Autoencoder (AE). The proposed evaluation method of generative models, is used on the search algorithm, which selects the final classifier model. Finally, the proposed system is tested and compared with One-class SVM (OCSVM) on the industrial benchmark of Tennessee Eastman (TE) process with satisfactory results.",
      "intriguing_abstract": "Fault detection is an important and demanding problem in industry. However, in many applications, abnormal data are difficult to be collected and are not available during training. In this paper, we propose a one class fault detection scheme, based on the unsupervised training of a Generative Adversarial Network (GAN). The generator tries to learn the manifold of normal behavior of the process, while the final decision of fault occurrence is taken from the discriminator. The network architectures of the discriminator and the generator and the hyper-parameters, that are used during training, are crucial for the stability of the GAN model. To ensure system convergence during training and to enhance the accuracy of the unsupervised classifier, a model selection algorithm is used. The latter one evaluates each trained model on a validation set of the normal training dataset, based on a proposed performance metric. Also, a method for evaluating the generating ability of each trained generative model is introduced, based on the reconstruction cost of an Autoencoder (AE). The proposed evaluation method of generative models, is used on the search algorithm, which selects the final classifier model. Finally, the proposed system is tested and compared with One-class SVM (OCSVM) on the industrial benchmark of Tennessee Eastman (TE) process with satisfactory results.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/627d4f69b76bb3fc88283de9e8e9f7ee6c598ea7.pdf",
      "citation_key": "plakias2018h0x",
      "metadata": {
        "title": "Generative Adversarial Networks for Unsupervised Fault Detection",
        "authors": [
          "Spyridon Plakias",
          "Y. Boutalis"
        ],
        "published_date": "2018",
        "abstract": "Fault detection is an important and demanding problem in industry. However, in many applications, abnormal data are difficult to be collected and are not available during training. In this paper, we propose a one class fault detection scheme, based on the unsupervised training of a Generative Adversarial Network (GAN). The generator tries to learn the manifold of normal behavior of the process, while the final decision of fault occurrence is taken from the discriminator. The network architectures of the discriminator and the generator and the hyper-parameters, that are used during training, are crucial for the stability of the GAN model. To ensure system convergence during training and to enhance the accuracy of the unsupervised classifier, a model selection algorithm is used. The latter one evaluates each trained model on a validation set of the normal training dataset, based on a proposed performance metric. Also, a method for evaluating the generating ability of each trained generative model is introduced, based on the reconstruction cost of an Autoencoder (AE). The proposed evaluation method of generative models, is used on the search algorithm, which selects the final classifier model. Finally, the proposed system is tested and compared with One-class SVM (OCSVM) on the industrial benchmark of Tennessee Eastman (TE) process with satisfactory results.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/627d4f69b76bb3fc88283de9e8e9f7ee6c598ea7.pdf",
        "venue": "European Control Conference",
        "citationCount": 20,
        "score": 2.8571428571428568,
        "summary": "Fault detection is an important and demanding problem in industry. However, in many applications, abnormal data are difficult to be collected and are not available during training. In this paper, we propose a one class fault detection scheme, based on the unsupervised training of a Generative Adversarial Network (GAN). The generator tries to learn the manifold of normal behavior of the process, while the final decision of fault occurrence is taken from the discriminator. The network architectures of the discriminator and the generator and the hyper-parameters, that are used during training, are crucial for the stability of the GAN model. To ensure system convergence during training and to enhance the accuracy of the unsupervised classifier, a model selection algorithm is used. The latter one evaluates each trained model on a validation set of the normal training dataset, based on a proposed performance metric. Also, a method for evaluating the generating ability of each trained generative model is introduced, based on the reconstruction cost of an Autoencoder (AE). The proposed evaluation method of generative models, is used on the search algorithm, which selects the final classifier model. Finally, the proposed system is tested and compared with One-class SVM (OCSVM) on the industrial benchmark of Tennessee Eastman (TE) process with satisfactory results.",
        "keywords": []
      },
      "file_name": "627d4f69b76bb3fc88283de9e8e9f7ee6c598ea7.pdf"
    },
    {
      "success": true,
      "doc_id": "82967cf4ac585e56d88a71bf8858d4cf",
      "summary": "Generative Adversarial Networks (GANs) are a powerful subclass of generative models. Yet, how to effectively train them to reach Nash equilibrium is a challenge. A number of experiments have indicated that one possible solution is to bound the function space of the discriminator. In practice, when optimizing the standard loss function without limiting the discriminator’s output, the discriminator may suffer from lack of convergence. To be able to reach the Nash equilibrium in a faster way during training and obtain better generative data, we propose constrained generative adversarial networks, GAN-C, where a constraint on the discriminator’s output is introduced. We theoretically prove that our proposed loss function shares the same Nash equilibrium as the standard one, and our experiments on mixture of Gaussians, MNIST, CIFAR-10, STL-10, FFHQ, and CAT datasets show that our loss function can better stabilize training and yield even better high-quality images.",
      "intriguing_abstract": "Generative Adversarial Networks (GANs) are a powerful subclass of generative models. Yet, how to effectively train them to reach Nash equilibrium is a challenge. A number of experiments have indicated that one possible solution is to bound the function space of the discriminator. In practice, when optimizing the standard loss function without limiting the discriminator’s output, the discriminator may suffer from lack of convergence. To be able to reach the Nash equilibrium in a faster way during training and obtain better generative data, we propose constrained generative adversarial networks, GAN-C, where a constraint on the discriminator’s output is introduced. We theoretically prove that our proposed loss function shares the same Nash equilibrium as the standard one, and our experiments on mixture of Gaussians, MNIST, CIFAR-10, STL-10, FFHQ, and CAT datasets show that our loss function can better stabilize training and yield even better high-quality images.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/7281ed8e5c3ef03dde6bbf4bf5df247f00182311.pdf",
      "citation_key": "chao2021ynq",
      "metadata": {
        "title": "Constrained Generative Adversarial Networks",
        "authors": [
          "Xiaopeng Chao",
          "Jiangzhong Cao",
          "Yuqin Lu",
          "Qingyun Dai",
          "Shangsong Liang"
        ],
        "published_date": "2021",
        "abstract": "Generative Adversarial Networks (GANs) are a powerful subclass of generative models. Yet, how to effectively train them to reach Nash equilibrium is a challenge. A number of experiments have indicated that one possible solution is to bound the function space of the discriminator. In practice, when optimizing the standard loss function without limiting the discriminator’s output, the discriminator may suffer from lack of convergence. To be able to reach the Nash equilibrium in a faster way during training and obtain better generative data, we propose constrained generative adversarial networks, GAN-C, where a constraint on the discriminator’s output is introduced. We theoretically prove that our proposed loss function shares the same Nash equilibrium as the standard one, and our experiments on mixture of Gaussians, MNIST, CIFAR-10, STL-10, FFHQ, and CAT datasets show that our loss function can better stabilize training and yield even better high-quality images.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/7281ed8e5c3ef03dde6bbf4bf5df247f00182311.pdf",
        "venue": "IEEE Access",
        "citationCount": 11,
        "score": 2.75,
        "summary": "Generative Adversarial Networks (GANs) are a powerful subclass of generative models. Yet, how to effectively train them to reach Nash equilibrium is a challenge. A number of experiments have indicated that one possible solution is to bound the function space of the discriminator. In practice, when optimizing the standard loss function without limiting the discriminator’s output, the discriminator may suffer from lack of convergence. To be able to reach the Nash equilibrium in a faster way during training and obtain better generative data, we propose constrained generative adversarial networks, GAN-C, where a constraint on the discriminator’s output is introduced. We theoretically prove that our proposed loss function shares the same Nash equilibrium as the standard one, and our experiments on mixture of Gaussians, MNIST, CIFAR-10, STL-10, FFHQ, and CAT datasets show that our loss function can better stabilize training and yield even better high-quality images.",
        "keywords": []
      },
      "file_name": "7281ed8e5c3ef03dde6bbf4bf5df247f00182311.pdf"
    },
    {
      "success": true,
      "doc_id": "d0f357e4385a30e0e6860f2fc3b02882",
      "summary": "I-vector based text-independent speaker verification (SV) systems often have poor performance with short utterances, as the biased phonetic distribution in a short utterance makes the extracted i-vector unreliable. This paper proposes an i-vector compensation method using a generative adversarial network (GAN), where its generator network is trained to generate a compensated i-vector from a short-utterance i-vector and its discriminator network is trained to determine whether an i-vector is generated by the generator or the one extracted from a long utterance. Additionally, we assign two other learning tasks to the GAN to stabilize its training and to make the generated ivector more speaker-specific. Speaker verification experiments on the NIST SRE 2008 \"10sec-10sec\" condition show that our method reduced the equal error rate by 11.3% from the conventional i-vector and PLDA system.",
      "intriguing_abstract": "I-vector based text-independent speaker verification (SV) systems often have poor performance with short utterances, as the biased phonetic distribution in a short utterance makes the extracted i-vector unreliable. This paper proposes an i-vector compensation method using a generative adversarial network (GAN), where its generator network is trained to generate a compensated i-vector from a short-utterance i-vector and its discriminator network is trained to determine whether an i-vector is generated by the generator or the one extracted from a long utterance. Additionally, we assign two other learning tasks to the GAN to stabilize its training and to make the generated ivector more speaker-specific. Speaker verification experiments on the NIST SRE 2008 \"10sec-10sec\" condition show that our method reduced the equal error rate by 11.3% from the conventional i-vector and PLDA system.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/5ce61044524cbaff63ab7dd5048b1f06a6ac3af2.pdf",
      "citation_key": "zhang20182tk",
      "metadata": {
        "title": "I-vector Transformation Using Conditional Generative Adversarial Networks for Short Utterance Speaker Verification",
        "authors": [
          "Jiacen Zhang",
          "Nakamasa Inoue",
          "K. Shinoda"
        ],
        "published_date": "2018",
        "abstract": "I-vector based text-independent speaker verification (SV) systems often have poor performance with short utterances, as the biased phonetic distribution in a short utterance makes the extracted i-vector unreliable. This paper proposes an i-vector compensation method using a generative adversarial network (GAN), where its generator network is trained to generate a compensated i-vector from a short-utterance i-vector and its discriminator network is trained to determine whether an i-vector is generated by the generator or the one extracted from a long utterance. Additionally, we assign two other learning tasks to the GAN to stabilize its training and to make the generated ivector more speaker-specific. Speaker verification experiments on the NIST SRE 2008 \"10sec-10sec\" condition show that our method reduced the equal error rate by 11.3% from the conventional i-vector and PLDA system.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/5ce61044524cbaff63ab7dd5048b1f06a6ac3af2.pdf",
        "venue": "Interspeech",
        "citationCount": 19,
        "score": 2.714285714285714,
        "summary": "I-vector based text-independent speaker verification (SV) systems often have poor performance with short utterances, as the biased phonetic distribution in a short utterance makes the extracted i-vector unreliable. This paper proposes an i-vector compensation method using a generative adversarial network (GAN), where its generator network is trained to generate a compensated i-vector from a short-utterance i-vector and its discriminator network is trained to determine whether an i-vector is generated by the generator or the one extracted from a long utterance. Additionally, we assign two other learning tasks to the GAN to stabilize its training and to make the generated ivector more speaker-specific. Speaker verification experiments on the NIST SRE 2008 \"10sec-10sec\" condition show that our method reduced the equal error rate by 11.3% from the conventional i-vector and PLDA system.",
        "keywords": []
      },
      "file_name": "5ce61044524cbaff63ab7dd5048b1f06a6ac3af2.pdf"
    },
    {
      "success": true,
      "doc_id": "548ecde75e377b4f2be6ec47a6c994f8",
      "summary": "We propose a novel regularizer to improve the training of Generative Adversarial Networks (GANs). The motivation is that when the discriminator D spreads out its model capacity in the right way, the learning signals given to the generator G are more informative and diverse. These in turn help G to explore better and discover the real data manifold while avoiding large unstable jumps due to the erroneous extrapolation made by D. Our regularizer guides the rectifier discriminator D to better allocate its model capacity, by encouraging the binary activation patterns on selected internal layers of D to have a high joint entropy. Experimental results on both synthetic data and real datasets demonstrate improvements in stability and convergence speed of the GAN training, as well as higher sample quality. The approach also leads to higher classification accuracies in semi-supervised learning.",
      "intriguing_abstract": "We propose a novel regularizer to improve the training of Generative Adversarial Networks (GANs). The motivation is that when the discriminator D spreads out its model capacity in the right way, the learning signals given to the generator G are more informative and diverse. These in turn help G to explore better and discover the real data manifold while avoiding large unstable jumps due to the erroneous extrapolation made by D. Our regularizer guides the rectifier discriminator D to better allocate its model capacity, by encouraging the binary activation patterns on selected internal layers of D to have a high joint entropy. Experimental results on both synthetic data and real datasets demonstrate improvements in stability and convergence speed of the GAN training, as well as higher sample quality. The approach also leads to higher classification accuracies in semi-supervised learning.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/9f074217d51ffb0da3b9716af4adae56215de488.pdf",
      "citation_key": "cao20184y8",
      "metadata": {
        "title": "Improving GAN Training via Binarized Representation Entropy (BRE) Regularization",
        "authors": [
          "Yanshuai Cao",
          "G. Ding",
          "Kry Yik-Chau Lui",
          "Ruitong Huang"
        ],
        "published_date": "2018",
        "abstract": "We propose a novel regularizer to improve the training of Generative Adversarial Networks (GANs). The motivation is that when the discriminator D spreads out its model capacity in the right way, the learning signals given to the generator G are more informative and diverse. These in turn help G to explore better and discover the real data manifold while avoiding large unstable jumps due to the erroneous extrapolation made by D. Our regularizer guides the rectifier discriminator D to better allocate its model capacity, by encouraging the binary activation patterns on selected internal layers of D to have a high joint entropy. Experimental results on both synthetic data and real datasets demonstrate improvements in stability and convergence speed of the GAN training, as well as higher sample quality. The approach also leads to higher classification accuracies in semi-supervised learning.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/9f074217d51ffb0da3b9716af4adae56215de488.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 19,
        "score": 2.714285714285714,
        "summary": "We propose a novel regularizer to improve the training of Generative Adversarial Networks (GANs). The motivation is that when the discriminator D spreads out its model capacity in the right way, the learning signals given to the generator G are more informative and diverse. These in turn help G to explore better and discover the real data manifold while avoiding large unstable jumps due to the erroneous extrapolation made by D. Our regularizer guides the rectifier discriminator D to better allocate its model capacity, by encouraging the binary activation patterns on selected internal layers of D to have a high joint entropy. Experimental results on both synthetic data and real datasets demonstrate improvements in stability and convergence speed of the GAN training, as well as higher sample quality. The approach also leads to higher classification accuracies in semi-supervised learning.",
        "keywords": []
      },
      "file_name": "9f074217d51ffb0da3b9716af4adae56215de488.pdf"
    },
    {
      "success": true,
      "doc_id": "d36c12e1c4a7e5d7c7bf59f7c2e449fa",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/792ed6251edb9287ecc85b39ff5e585ea30b05f4.pdf",
      "citation_key": "costa2020anu",
      "metadata": {
        "title": "Neuroevolution of Generative Adversarial Networks",
        "authors": [
          "Victor Costa",
          "Nuno Lourenço",
          "João Correia",
          "Penousal Machado"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/792ed6251edb9287ecc85b39ff5e585ea30b05f4.pdf",
        "venue": "Deep Neural Evolution",
        "citationCount": 13,
        "score": 2.6,
        "summary": "",
        "keywords": []
      },
      "file_name": "792ed6251edb9287ecc85b39ff5e585ea30b05f4.pdf"
    },
    {
      "success": true,
      "doc_id": "7b2479c0bc34b80a8dd2f1e55a1c6cee",
      "summary": "Predicting driver’s cognitive states using deep learning from electroencephalography (EEG) signals is considered this paper. To address the challenge posed by limited labeled training samples, a semi-supervised Wasserstein Generative Adversarial Network with gradient penalty (sWGAN-GP) is proposed. The proposed sWGAN-GP includes a classifier with the shared architecture with the discriminator in GAN and its loss function enables the augmentation of limited training samples with generated EEG samples during training, thus resulting in improved classification performance. The several modeling challenges including frequency artifacts and training instability, are also considered. The test results on predicting the alert and drowsy states from a simulated driving experiment demonstrate improved prediction performance and training stability over the baseline semi-supervised GAN and a convolutional neural network model.",
      "intriguing_abstract": "Predicting driver’s cognitive states using deep learning from electroencephalography (EEG) signals is considered this paper. To address the challenge posed by limited labeled training samples, a semi-supervised Wasserstein Generative Adversarial Network with gradient penalty (sWGAN-GP) is proposed. The proposed sWGAN-GP includes a classifier with the shared architecture with the discriminator in GAN and its loss function enables the augmentation of limited training samples with generated EEG samples during training, thus resulting in improved classification performance. The several modeling challenges including frequency artifacts and training instability, are also considered. The test results on predicting the alert and drowsy states from a simulated driving experiment demonstrate improved prediction performance and training stability over the baseline semi-supervised GAN and a convolutional neural network model.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/b81957019c4e323552e0113da78a7611c160651e.pdf",
      "citation_key": "panwar2019psx",
      "metadata": {
        "title": "A Semi-Supervised Wasserstein Generative Adversarial Network for Classifying Driving Fatigue from EEG signals",
        "authors": [
          "Sharaj Panwar",
          "P. Rad",
          "J. Quarles",
          "E. Golob",
          "Yufei Huang"
        ],
        "published_date": "2019",
        "abstract": "Predicting driver’s cognitive states using deep learning from electroencephalography (EEG) signals is considered this paper. To address the challenge posed by limited labeled training samples, a semi-supervised Wasserstein Generative Adversarial Network with gradient penalty (sWGAN-GP) is proposed. The proposed sWGAN-GP includes a classifier with the shared architecture with the discriminator in GAN and its loss function enables the augmentation of limited training samples with generated EEG samples during training, thus resulting in improved classification performance. The several modeling challenges including frequency artifacts and training instability, are also considered. The test results on predicting the alert and drowsy states from a simulated driving experiment demonstrate improved prediction performance and training stability over the baseline semi-supervised GAN and a convolutional neural network model.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/b81957019c4e323552e0113da78a7611c160651e.pdf",
        "venue": "IEEE International Conference on Systems, Man and Cybernetics",
        "citationCount": 15,
        "score": 2.5,
        "summary": "Predicting driver’s cognitive states using deep learning from electroencephalography (EEG) signals is considered this paper. To address the challenge posed by limited labeled training samples, a semi-supervised Wasserstein Generative Adversarial Network with gradient penalty (sWGAN-GP) is proposed. The proposed sWGAN-GP includes a classifier with the shared architecture with the discriminator in GAN and its loss function enables the augmentation of limited training samples with generated EEG samples during training, thus resulting in improved classification performance. The several modeling challenges including frequency artifacts and training instability, are also considered. The test results on predicting the alert and drowsy states from a simulated driving experiment demonstrate improved prediction performance and training stability over the baseline semi-supervised GAN and a convolutional neural network model.",
        "keywords": []
      },
      "file_name": "b81957019c4e323552e0113da78a7611c160651e.pdf"
    },
    {
      "success": true,
      "doc_id": "55af81bc8538602511341d014f127970",
      "summary": "The performance of a deep learning (DL) model depends on sufficient training datasets and its algorithmic structure. Even though seismological research using low-cost micro-electro-mechanical systems (MEMS) sensor received much attention recently, because of the lack of data recorded by such MEMS sensors whose data are usually polluted by different types of noise. Therefore, increasing seismic datasets is required by intelligently generating seismic data through data-augmentation techniques. However, it is difficult to characterize and measure the evolution process of seismic sequences, making the feature extraction and data generation of seismic sequences still a significant challenge. By combining the framework of Generative Adversarial Network (GAN) with long short-term memory (LSTM), attention mechanism and neural network (NN), a novel deep generation model (DGM) named EQGAN is developed to overcome the challenges, which can automatically capture the different time histories and dimension characteristics of seismic sequences, meanwhile stably generating high-quality seismic data. The reality of generated data is qualitatively clarified through the analysis of frequency domain and data autocorrelation distribution. Based on the High-throughput Screening (HTS) Theory, the quantitative evaluation index of statistical metrics is designed, and the generation performance of different machine learning models (standard GAN, LSTM, NN) is compared to prove the stability and effectiveness of EQGAN. The experimental results denote that the EQGAN has excellent stability and performance (up to 81%, much higher than that of other generation models), which provides a suitable data expansion approach for the field of seismological research.",
      "intriguing_abstract": "The performance of a deep learning (DL) model depends on sufficient training datasets and its algorithmic structure. Even though seismological research using low-cost micro-electro-mechanical systems (MEMS) sensor received much attention recently, because of the lack of data recorded by such MEMS sensors whose data are usually polluted by different types of noise. Therefore, increasing seismic datasets is required by intelligently generating seismic data through data-augmentation techniques. However, it is difficult to characterize and measure the evolution process of seismic sequences, making the feature extraction and data generation of seismic sequences still a significant challenge. By combining the framework of Generative Adversarial Network (GAN) with long short-term memory (LSTM), attention mechanism and neural network (NN), a novel deep generation model (DGM) named EQGAN is developed to overcome the challenges, which can automatically capture the different time histories and dimension characteristics of seismic sequences, meanwhile stably generating high-quality seismic data. The reality of generated data is qualitatively clarified through the analysis of frequency domain and data autocorrelation distribution. Based on the High-throughput Screening (HTS) Theory, the quantitative evaluation index of statistical metrics is designed, and the generation performance of different machine learning models (standard GAN, LSTM, NN) is compared to prove the stability and effectiveness of EQGAN. The experimental results denote that the EQGAN has excellent stability and performance (up to 81%, much higher than that of other generation models), which provides a suitable data expansion approach for the field of seismological research.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/14b1c76014eeabc8a2d56a7b06cc39d95981de75.pdf",
      "citation_key": "wu20212vn",
      "metadata": {
        "title": "Augmenting Seismic Data Using Generative Adversarial Network for Low-Cost MEMS Sensors",
        "authors": [
          "Aming Wu",
          "Juyong Shin",
          "Jae-Kwang Ahn",
          "Young-Woo Kwon"
        ],
        "published_date": "2021",
        "abstract": "The performance of a deep learning (DL) model depends on sufficient training datasets and its algorithmic structure. Even though seismological research using low-cost micro-electro-mechanical systems (MEMS) sensor received much attention recently, because of the lack of data recorded by such MEMS sensors whose data are usually polluted by different types of noise. Therefore, increasing seismic datasets is required by intelligently generating seismic data through data-augmentation techniques. However, it is difficult to characterize and measure the evolution process of seismic sequences, making the feature extraction and data generation of seismic sequences still a significant challenge. By combining the framework of Generative Adversarial Network (GAN) with long short-term memory (LSTM), attention mechanism and neural network (NN), a novel deep generation model (DGM) named EQGAN is developed to overcome the challenges, which can automatically capture the different time histories and dimension characteristics of seismic sequences, meanwhile stably generating high-quality seismic data. The reality of generated data is qualitatively clarified through the analysis of frequency domain and data autocorrelation distribution. Based on the High-throughput Screening (HTS) Theory, the quantitative evaluation index of statistical metrics is designed, and the generation performance of different machine learning models (standard GAN, LSTM, NN) is compared to prove the stability and effectiveness of EQGAN. The experimental results denote that the EQGAN has excellent stability and performance (up to 81%, much higher than that of other generation models), which provides a suitable data expansion approach for the field of seismological research.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/14b1c76014eeabc8a2d56a7b06cc39d95981de75.pdf",
        "venue": "IEEE Access",
        "citationCount": 10,
        "score": 2.5,
        "summary": "The performance of a deep learning (DL) model depends on sufficient training datasets and its algorithmic structure. Even though seismological research using low-cost micro-electro-mechanical systems (MEMS) sensor received much attention recently, because of the lack of data recorded by such MEMS sensors whose data are usually polluted by different types of noise. Therefore, increasing seismic datasets is required by intelligently generating seismic data through data-augmentation techniques. However, it is difficult to characterize and measure the evolution process of seismic sequences, making the feature extraction and data generation of seismic sequences still a significant challenge. By combining the framework of Generative Adversarial Network (GAN) with long short-term memory (LSTM), attention mechanism and neural network (NN), a novel deep generation model (DGM) named EQGAN is developed to overcome the challenges, which can automatically capture the different time histories and dimension characteristics of seismic sequences, meanwhile stably generating high-quality seismic data. The reality of generated data is qualitatively clarified through the analysis of frequency domain and data autocorrelation distribution. Based on the High-throughput Screening (HTS) Theory, the quantitative evaluation index of statistical metrics is designed, and the generation performance of different machine learning models (standard GAN, LSTM, NN) is compared to prove the stability and effectiveness of EQGAN. The experimental results denote that the EQGAN has excellent stability and performance (up to 81%, much higher than that of other generation models), which provides a suitable data expansion approach for the field of seismological research.",
        "keywords": []
      },
      "file_name": "14b1c76014eeabc8a2d56a7b06cc39d95981de75.pdf"
    },
    {
      "success": true,
      "doc_id": "f0fa4a0a7eb8656af07479d2ab6a4ffb",
      "summary": "Solar cells are the core module of photovoltaic (PV) modules. Defects will decrease the power efficiency of solar cells and reduce the stability of PV power systems. Electroluminescence (EL) imaging is able to image solar modules with higher resolution so that defects can be better detected. The current manual detection of EL images is slow and requires relevant expertise, so methods based on computer vision for automatic detection in EL images are appearing. However, due to the heterogeneously background of the EL images and the lack of defect samples, automatic detection of defects has been a challenging task. We design a model based on generative adversarial networks (GAN) and auto-encoder (AE) to perform defect detection for EL images of solar cells. It only requires normal images in the training process and detect defects by measuring the residuals between the test image and the constructed image generated by the generator. To reduce the effects of image distortion, we combine structural similarity index (SSIM) with feature residuals to train the encoder, which can get better results than the model using typical mean square error (MSE). During the detection phase, SSIM and MSE are combined as the anomaly score. Our method has higher recognition of defective EL images and achieves detection accuracy of 90.0% on the test set. Compared with the method using only MSE, the F1 score is increased obviously.",
      "intriguing_abstract": "Solar cells are the core module of photovoltaic (PV) modules. Defects will decrease the power efficiency of solar cells and reduce the stability of PV power systems. Electroluminescence (EL) imaging is able to image solar modules with higher resolution so that defects can be better detected. The current manual detection of EL images is slow and requires relevant expertise, so methods based on computer vision for automatic detection in EL images are appearing. However, due to the heterogeneously background of the EL images and the lack of defect samples, automatic detection of defects has been a challenging task. We design a model based on generative adversarial networks (GAN) and auto-encoder (AE) to perform defect detection for EL images of solar cells. It only requires normal images in the training process and detect defects by measuring the residuals between the test image and the constructed image generated by the generator. To reduce the effects of image distortion, we combine structural similarity index (SSIM) with feature residuals to train the encoder, which can get better results than the model using typical mean square error (MSE). During the detection phase, SSIM and MSE are combined as the anomaly score. Our method has higher recognition of defective EL images and achieves detection accuracy of 90.0% on the test set. Compared with the method using only MSE, the F1 score is increased obviously.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/1ad242cd529f848252a244bb0e9c01480520cfd5.pdf",
      "citation_key": "shou2020v6h",
      "metadata": {
        "title": "Defect Detection with Generative Adversarial Networks for Electroluminescence Images of Solar Cells",
        "authors": [
          "Chunhui Shou",
          "Ling Hong",
          "Waner Ding",
          "Qu Shen",
          "W. Zhou",
          "Yu Jiang",
          "Chunhui Zhao"
        ],
        "published_date": "2020",
        "abstract": "Solar cells are the core module of photovoltaic (PV) modules. Defects will decrease the power efficiency of solar cells and reduce the stability of PV power systems. Electroluminescence (EL) imaging is able to image solar modules with higher resolution so that defects can be better detected. The current manual detection of EL images is slow and requires relevant expertise, so methods based on computer vision for automatic detection in EL images are appearing. However, due to the heterogeneously background of the EL images and the lack of defect samples, automatic detection of defects has been a challenging task. We design a model based on generative adversarial networks (GAN) and auto-encoder (AE) to perform defect detection for EL images of solar cells. It only requires normal images in the training process and detect defects by measuring the residuals between the test image and the constructed image generated by the generator. To reduce the effects of image distortion, we combine structural similarity index (SSIM) with feature residuals to train the encoder, which can get better results than the model using typical mean square error (MSE). During the detection phase, SSIM and MSE are combined as the anomaly score. Our method has higher recognition of defective EL images and achieves detection accuracy of 90.0% on the test set. Compared with the method using only MSE, the F1 score is increased obviously.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/1ad242cd529f848252a244bb0e9c01480520cfd5.pdf",
        "venue": "Youth Academic Annual Conference of Chinese Association of Automation",
        "citationCount": 12,
        "score": 2.4000000000000004,
        "summary": "Solar cells are the core module of photovoltaic (PV) modules. Defects will decrease the power efficiency of solar cells and reduce the stability of PV power systems. Electroluminescence (EL) imaging is able to image solar modules with higher resolution so that defects can be better detected. The current manual detection of EL images is slow and requires relevant expertise, so methods based on computer vision for automatic detection in EL images are appearing. However, due to the heterogeneously background of the EL images and the lack of defect samples, automatic detection of defects has been a challenging task. We design a model based on generative adversarial networks (GAN) and auto-encoder (AE) to perform defect detection for EL images of solar cells. It only requires normal images in the training process and detect defects by measuring the residuals between the test image and the constructed image generated by the generator. To reduce the effects of image distortion, we combine structural similarity index (SSIM) with feature residuals to train the encoder, which can get better results than the model using typical mean square error (MSE). During the detection phase, SSIM and MSE are combined as the anomaly score. Our method has higher recognition of defective EL images and achieves detection accuracy of 90.0% on the test set. Compared with the method using only MSE, the F1 score is increased obviously.",
        "keywords": []
      },
      "file_name": "1ad242cd529f848252a244bb0e9c01480520cfd5.pdf"
    },
    {
      "success": true,
      "doc_id": "ca84549cc08b974d99731ebc6dfc17c4",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/667cad20be038d2b6aafe17afb989c6db824e0a1.pdf",
      "citation_key": "liu2019v0x",
      "metadata": {
        "title": "Active Appearance Model Induced Generative Adversarial Network for Controlled Data Augmentation",
        "authors": [
          "Jianfei Liu",
          "Christine Shen",
          "Tao Liu",
          "N. Aguilera",
          "Johnny Tam"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/667cad20be038d2b6aafe17afb989c6db824e0a1.pdf",
        "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
        "citationCount": 14,
        "score": 2.333333333333333,
        "summary": "",
        "keywords": []
      },
      "file_name": "667cad20be038d2b6aafe17afb989c6db824e0a1.pdf"
    },
    {
      "success": true,
      "doc_id": "6190bbd3b554e0578e7a7c3f32d484c0",
      "summary": "Initial studies have suggested generative adversarial networks (GANs) have promise as fast simulations within HEP. These studies, while promising, have been insufficiently precise and also, like GANs in general, suffer from stability issues.We apply GANs to to generate full particle physics events (not individual physics objects), explore conditioning of generated events based on physics theory parameters and evaluate the precision and generalization of the produced datasets. We apply this to SUSY mass parameter interpolation and pileup generation. We also discuss recent developments in convergence and representations that match the structure of the detector better than images.In addition we describe on-going work making use of large-scale distributed resources on the Cori supercomputer at NERSC, and developments to control distributed training via interactive jupyter notebook sessions. This will allow tackling high-resolution detector data; model selection and hyper-parameter tuning in a productive yet scalable deep learning environment.",
      "intriguing_abstract": "Initial studies have suggested generative adversarial networks (GANs) have promise as fast simulations within HEP. These studies, while promising, have been insufficiently precise and also, like GANs in general, suffer from stability issues.We apply GANs to to generate full particle physics events (not individual physics objects), explore conditioning of generated events based on physics theory parameters and evaluate the precision and generalization of the produced datasets. We apply this to SUSY mass parameter interpolation and pileup generation. We also discuss recent developments in convergence and representations that match the structure of the detector better than images.In addition we describe on-going work making use of large-scale distributed resources on the Cori supercomputer at NERSC, and developments to control distributed training via interactive jupyter notebook sessions. This will allow tackling high-resolution detector data; model selection and hyper-parameter tuning in a productive yet scalable deep learning environment.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/eb769e44f000fe907e956f453a9e6eab3484f8e9.pdf",
      "citation_key": "farrell2019kjy",
      "metadata": {
        "title": "Next Generation Generative Neural Networks for HEP",
        "authors": [
          "S. Farrell",
          "W. Bhimji",
          "T. Kurth",
          "M. Mustafa",
          "D. Bard",
          "Z. Lukic",
          "B. Nachman",
          "H. Patton"
        ],
        "published_date": "2019",
        "abstract": "Initial studies have suggested generative adversarial networks (GANs) have promise as fast simulations within HEP. These studies, while promising, have been insufficiently precise and also, like GANs in general, suffer from stability issues.We apply GANs to to generate full particle physics events (not individual physics objects), explore conditioning of generated events based on physics theory parameters and evaluate the precision and generalization of the produced datasets. We apply this to SUSY mass parameter interpolation and pileup generation. We also discuss recent developments in convergence and representations that match the structure of the detector better than images.In addition we describe on-going work making use of large-scale distributed resources on the Cori supercomputer at NERSC, and developments to control distributed training via interactive jupyter notebook sessions. This will allow tackling high-resolution detector data; model selection and hyper-parameter tuning in a productive yet scalable deep learning environment.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/eb769e44f000fe907e956f453a9e6eab3484f8e9.pdf",
        "venue": "EPJ Web of Conferences",
        "citationCount": 14,
        "score": 2.333333333333333,
        "summary": "Initial studies have suggested generative adversarial networks (GANs) have promise as fast simulations within HEP. These studies, while promising, have been insufficiently precise and also, like GANs in general, suffer from stability issues.We apply GANs to to generate full particle physics events (not individual physics objects), explore conditioning of generated events based on physics theory parameters and evaluate the precision and generalization of the produced datasets. We apply this to SUSY mass parameter interpolation and pileup generation. We also discuss recent developments in convergence and representations that match the structure of the detector better than images.In addition we describe on-going work making use of large-scale distributed resources on the Cori supercomputer at NERSC, and developments to control distributed training via interactive jupyter notebook sessions. This will allow tackling high-resolution detector data; model selection and hyper-parameter tuning in a productive yet scalable deep learning environment.",
        "keywords": []
      },
      "file_name": "eb769e44f000fe907e956f453a9e6eab3484f8e9.pdf"
    },
    {
      "success": true,
      "doc_id": "d04c8b95a21cbb9b1d0be4bc5dfdaace",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/ef19c7b7e1f4d0e4644d324eecc32f8cace18bf2.pdf",
      "citation_key": "wu2020n95",
      "metadata": {
        "title": "Attentive evolutionary generative adversarial network",
        "authors": [
          "Zhongze Wu",
          "Chunmei He",
          "Liwen Yang",
          "Fangjun Kuang"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/ef19c7b7e1f4d0e4644d324eecc32f8cace18bf2.pdf",
        "venue": "Applied intelligence (Boston)",
        "citationCount": 11,
        "score": 2.2,
        "summary": "",
        "keywords": []
      },
      "file_name": "ef19c7b7e1f4d0e4644d324eecc32f8cace18bf2.pdf"
    },
    {
      "success": true,
      "doc_id": "7ec8c96bbee34a10a54b25211c6005e2",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/ba825fdcd26fc77fdec7909eee92a0fa3a3407f2.pdf",
      "citation_key": "majtner20192pi",
      "metadata": {
        "title": "On the Effectiveness of Generative Adversarial Networks as HEp-2 Image Augmentation Tool",
        "authors": [
          "Tomás Majtner",
          "Buda Bajić",
          "Joakim Lindblad",
          "Natasa Sladoje",
          "V. Blanes-Vidal",
          "E. Nadimi"
        ],
        "published_date": "2019",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/ba825fdcd26fc77fdec7909eee92a0fa3a3407f2.pdf",
        "venue": "Scandinavian Conference on Image Analysis",
        "citationCount": 13,
        "score": 2.1666666666666665,
        "summary": "",
        "keywords": []
      },
      "file_name": "ba825fdcd26fc77fdec7909eee92a0fa3a3407f2.pdf"
    },
    {
      "success": true,
      "doc_id": "3cfa4156c5b082532aa498b381938a7c",
      "summary": "Generative adversarial network (GAN) has become one of the most important neural network models for classical unsupervised machine learning. A variety of discriminator loss functions have been developed to train GAN’s discriminators and they all have a common structure: a sum of real and fake losses that only depends on the actual and generated data respectively. One challenge associated with an equally weighted sum of two losses is that the training may benefit one loss but harm the other, which we show causes instability and mode collapse. In this paper, we introduce a new family of discriminator loss functions that adopts a weighted sum of real and fake parts, which we call adaptive weighted loss functions or aw-loss functions. Using the gradients of the real and fake parts of the loss, we can adaptively choose weights to train a discriminator in the direction that benefits the GAN’s stability. Our method can be potentially applied to any discriminator model with a loss that is a sum of the real and fake parts. Experiments validated the effectiveness of our loss functions on unconditional and conditional image generation tasks, improving the baseline results by a significant margin on CIFAR-10, STL-10, and CIFAR-100 datasets in Inception Scores (IS) and Fréchet Inception Distance (FID) metrics.",
      "intriguing_abstract": "Generative adversarial network (GAN) has become one of the most important neural network models for classical unsupervised machine learning. A variety of discriminator loss functions have been developed to train GAN’s discriminators and they all have a common structure: a sum of real and fake losses that only depends on the actual and generated data respectively. One challenge associated with an equally weighted sum of two losses is that the training may benefit one loss but harm the other, which we show causes instability and mode collapse. In this paper, we introduce a new family of discriminator loss functions that adopts a weighted sum of real and fake parts, which we call adaptive weighted loss functions or aw-loss functions. Using the gradients of the real and fake parts of the loss, we can adaptively choose weights to train a discriminator in the direction that benefits the GAN’s stability. Our method can be potentially applied to any discriminator model with a loss that is a sum of the real and fake parts. Experiments validated the effectiveness of our loss functions on unconditional and conditional image generation tasks, improving the baseline results by a significant margin on CIFAR-10, STL-10, and CIFAR-100 datasets in Inception Scores (IS) and Fréchet Inception Distance (FID) metrics.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/8c5bae51a1292767c13b5fed339ea94dc971ff9b.pdf",
      "citation_key": "zadorozhnyy20208ft",
      "metadata": {
        "title": "Adaptive Weighted Discriminator for Training Generative Adversarial Networks",
        "authors": [
          "Vasily Zadorozhnyy",
          "Q. Cheng",
          "Q. Ye"
        ],
        "published_date": "2020",
        "abstract": "Generative adversarial network (GAN) has become one of the most important neural network models for classical unsupervised machine learning. A variety of discriminator loss functions have been developed to train GAN’s discriminators and they all have a common structure: a sum of real and fake losses that only depends on the actual and generated data respectively. One challenge associated with an equally weighted sum of two losses is that the training may benefit one loss but harm the other, which we show causes instability and mode collapse. In this paper, we introduce a new family of discriminator loss functions that adopts a weighted sum of real and fake parts, which we call adaptive weighted loss functions or aw-loss functions. Using the gradients of the real and fake parts of the loss, we can adaptively choose weights to train a discriminator in the direction that benefits the GAN’s stability. Our method can be potentially applied to any discriminator model with a loss that is a sum of the real and fake parts. Experiments validated the effectiveness of our loss functions on unconditional and conditional image generation tasks, improving the baseline results by a significant margin on CIFAR-10, STL-10, and CIFAR-100 datasets in Inception Scores (IS) and Fréchet Inception Distance (FID) metrics.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/8c5bae51a1292767c13b5fed339ea94dc971ff9b.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 10,
        "score": 2.0,
        "summary": "Generative adversarial network (GAN) has become one of the most important neural network models for classical unsupervised machine learning. A variety of discriminator loss functions have been developed to train GAN’s discriminators and they all have a common structure: a sum of real and fake losses that only depends on the actual and generated data respectively. One challenge associated with an equally weighted sum of two losses is that the training may benefit one loss but harm the other, which we show causes instability and mode collapse. In this paper, we introduce a new family of discriminator loss functions that adopts a weighted sum of real and fake parts, which we call adaptive weighted loss functions or aw-loss functions. Using the gradients of the real and fake parts of the loss, we can adaptively choose weights to train a discriminator in the direction that benefits the GAN’s stability. Our method can be potentially applied to any discriminator model with a loss that is a sum of the real and fake parts. Experiments validated the effectiveness of our loss functions on unconditional and conditional image generation tasks, improving the baseline results by a significant margin on CIFAR-10, STL-10, and CIFAR-100 datasets in Inception Scores (IS) and Fréchet Inception Distance (FID) metrics.",
        "keywords": []
      },
      "file_name": "8c5bae51a1292767c13b5fed339ea94dc971ff9b.pdf"
    },
    {
      "success": true,
      "doc_id": "e666015a39e868592c302f2cc0571e51",
      "summary": "Oversampling plays a vital role in improving the minority-class classification accuracy for imbalanced biomedical datasets. In this work, we propose a single-channel biosignal data generation method by exploiting the advancements in well-established image-based Generative Adversarial Networks (GANs). We have implemented a Wasserstein GAN (WGAN) to generate synthetic electrocardiogram (ECG) signal, due to their stability in training as well as correlation of the loss function with the generated image quality. We first trained the WGAN with fixed-dimensional images of the signal and generated synthetic data with similar characteristics. Two evaluation methods were then used for evaluating the efficiency of the proposed technique in generating synthetic ECG data. We used Frechet Inception Distance score for measuring synthetic image quality. We then performed a binary classification of normal and abnormal (Anterior Myocardial Infarction) ECG using Support Vector Machine to verify the performance of the proposed method as an oversampling technique.",
      "intriguing_abstract": "Oversampling plays a vital role in improving the minority-class classification accuracy for imbalanced biomedical datasets. In this work, we propose a single-channel biosignal data generation method by exploiting the advancements in well-established image-based Generative Adversarial Networks (GANs). We have implemented a Wasserstein GAN (WGAN) to generate synthetic electrocardiogram (ECG) signal, due to their stability in training as well as correlation of the loss function with the generated image quality. We first trained the WGAN with fixed-dimensional images of the signal and generated synthetic data with similar characteristics. Two evaluation methods were then used for evaluating the efficiency of the proposed technique in generating synthetic ECG data. We used Frechet Inception Distance score for measuring synthetic image quality. We then performed a binary classification of normal and abnormal (Anterior Myocardial Infarction) ECG using Support Vector Machine to verify the performance of the proposed method as an oversampling technique.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/bba74555301373e84e9850c617a1a7311697e503.pdf",
      "citation_key": "munia20201u2",
      "metadata": {
        "title": "Biosignal Oversampling Using Wasserstein Generative Adversarial Network",
        "authors": [
          "M. Munia",
          "M. Nourani",
          "Sammy Houari"
        ],
        "published_date": "2020",
        "abstract": "Oversampling plays a vital role in improving the minority-class classification accuracy for imbalanced biomedical datasets. In this work, we propose a single-channel biosignal data generation method by exploiting the advancements in well-established image-based Generative Adversarial Networks (GANs). We have implemented a Wasserstein GAN (WGAN) to generate synthetic electrocardiogram (ECG) signal, due to their stability in training as well as correlation of the loss function with the generated image quality. We first trained the WGAN with fixed-dimensional images of the signal and generated synthetic data with similar characteristics. Two evaluation methods were then used for evaluating the efficiency of the proposed technique in generating synthetic ECG data. We used Frechet Inception Distance score for measuring synthetic image quality. We then performed a binary classification of normal and abnormal (Anterior Myocardial Infarction) ECG using Support Vector Machine to verify the performance of the proposed method as an oversampling technique.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/bba74555301373e84e9850c617a1a7311697e503.pdf",
        "venue": "IEEE International Conference on Healthcare Informatics",
        "citationCount": 10,
        "score": 2.0,
        "summary": "Oversampling plays a vital role in improving the minority-class classification accuracy for imbalanced biomedical datasets. In this work, we propose a single-channel biosignal data generation method by exploiting the advancements in well-established image-based Generative Adversarial Networks (GANs). We have implemented a Wasserstein GAN (WGAN) to generate synthetic electrocardiogram (ECG) signal, due to their stability in training as well as correlation of the loss function with the generated image quality. We first trained the WGAN with fixed-dimensional images of the signal and generated synthetic data with similar characteristics. Two evaluation methods were then used for evaluating the efficiency of the proposed technique in generating synthetic ECG data. We used Frechet Inception Distance score for measuring synthetic image quality. We then performed a binary classification of normal and abnormal (Anterior Myocardial Infarction) ECG using Support Vector Machine to verify the performance of the proposed method as an oversampling technique.",
        "keywords": []
      },
      "file_name": "bba74555301373e84e9850c617a1a7311697e503.pdf"
    },
    {
      "success": true,
      "doc_id": "5a576d0fddc8b95ac8c7b090543f7e91",
      "summary": "While standard generative adversarial networks (GANs) rely solely on training data to learn unknown probability distributions, physics-informed GANs (PI-GANs) encode physical laws in the form of stochastic partial differential equations (PDEs) using auto differentiation. By relating observed data to unobserved quantities of interest through PDEs, PI-GANs allow for the estimation of underlying probability distributions without their direct measurement (i.e. inverse problems). The scalable nature of GANs allows high-dimensional, spatially-dependent probability distributions (i.e., random fields) to be inferred, while incorporating prior information through PDEs allows the training datasets to be relatively small. \nIn this work, PI-GANs are demonstrated for the application of elastic modulus estimation in mechanical testing. Given measured deformation data, the underlying probability distribution of spatially-varying elastic modulus (stiffness) is learned. Two feed-forward deep neural network generators are used to model the deformation and material stiffness across a two dimensional domain. Wasserstein GANs with gradient penalty are employed for enhanced stability. In the absence of explicit training data, it is demonstrated that the PI-GAN learns to generate realistic, physically-admissible realizations of material stiffness by incorporating the PDE that relates it to the measured deformation. It is shown that the statistics (mean, standard deviation, point-wise distributions, correlation length) of these generated stiffness samples have good agreement with the true distribution.",
      "intriguing_abstract": "While standard generative adversarial networks (GANs) rely solely on training data to learn unknown probability distributions, physics-informed GANs (PI-GANs) encode physical laws in the form of stochastic partial differential equations (PDEs) using auto differentiation. By relating observed data to unobserved quantities of interest through PDEs, PI-GANs allow for the estimation of underlying probability distributions without their direct measurement (i.e. inverse problems). The scalable nature of GANs allows high-dimensional, spatially-dependent probability distributions (i.e., random fields) to be inferred, while incorporating prior information through PDEs allows the training datasets to be relatively small. \nIn this work, PI-GANs are demonstrated for the application of elastic modulus estimation in mechanical testing. Given measured deformation data, the underlying probability distribution of spatially-varying elastic modulus (stiffness) is learned. Two feed-forward deep neural network generators are used to model the deformation and material stiffness across a two dimensional domain. Wasserstein GANs with gradient penalty are employed for enhanced stability. In the absence of explicit training data, it is demonstrated that the PI-GAN learns to generate realistic, physically-admissible realizations of material stiffness by incorporating the PDE that relates it to the measured deformation. It is shown that the statistics (mean, standard deviation, point-wise distributions, correlation length) of these generated stiffness samples have good agreement with the true distribution.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/6d94348f7f752cfd095fb9aedaef7ad00ba8c5e3.pdf",
      "citation_key": "warner2020a5z",
      "metadata": {
        "title": "Inverse Estimation of Elastic Modulus Using Physics-Informed Generative Adversarial Networks",
        "authors": [
          "J. Warner",
          "Julian Cuevas",
          "G. Bomarito",
          "P. Leser",
          "W. Leser"
        ],
        "published_date": "2020",
        "abstract": "While standard generative adversarial networks (GANs) rely solely on training data to learn unknown probability distributions, physics-informed GANs (PI-GANs) encode physical laws in the form of stochastic partial differential equations (PDEs) using auto differentiation. By relating observed data to unobserved quantities of interest through PDEs, PI-GANs allow for the estimation of underlying probability distributions without their direct measurement (i.e. inverse problems). The scalable nature of GANs allows high-dimensional, spatially-dependent probability distributions (i.e., random fields) to be inferred, while incorporating prior information through PDEs allows the training datasets to be relatively small. \nIn this work, PI-GANs are demonstrated for the application of elastic modulus estimation in mechanical testing. Given measured deformation data, the underlying probability distribution of spatially-varying elastic modulus (stiffness) is learned. Two feed-forward deep neural network generators are used to model the deformation and material stiffness across a two dimensional domain. Wasserstein GANs with gradient penalty are employed for enhanced stability. In the absence of explicit training data, it is demonstrated that the PI-GAN learns to generate realistic, physically-admissible realizations of material stiffness by incorporating the PDE that relates it to the measured deformation. It is shown that the statistics (mean, standard deviation, point-wise distributions, correlation length) of these generated stiffness samples have good agreement with the true distribution.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/6d94348f7f752cfd095fb9aedaef7ad00ba8c5e3.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 2.0,
        "summary": "While standard generative adversarial networks (GANs) rely solely on training data to learn unknown probability distributions, physics-informed GANs (PI-GANs) encode physical laws in the form of stochastic partial differential equations (PDEs) using auto differentiation. By relating observed data to unobserved quantities of interest through PDEs, PI-GANs allow for the estimation of underlying probability distributions without their direct measurement (i.e. inverse problems). The scalable nature of GANs allows high-dimensional, spatially-dependent probability distributions (i.e., random fields) to be inferred, while incorporating prior information through PDEs allows the training datasets to be relatively small. \nIn this work, PI-GANs are demonstrated for the application of elastic modulus estimation in mechanical testing. Given measured deformation data, the underlying probability distribution of spatially-varying elastic modulus (stiffness) is learned. Two feed-forward deep neural network generators are used to model the deformation and material stiffness across a two dimensional domain. Wasserstein GANs with gradient penalty are employed for enhanced stability. In the absence of explicit training data, it is demonstrated that the PI-GAN learns to generate realistic, physically-admissible realizations of material stiffness by incorporating the PDE that relates it to the measured deformation. It is shown that the statistics (mean, standard deviation, point-wise distributions, correlation length) of these generated stiffness samples have good agreement with the true distribution.",
        "keywords": []
      },
      "file_name": "6d94348f7f752cfd095fb9aedaef7ad00ba8c5e3.pdf"
    },
    {
      "success": true,
      "doc_id": "e5288e884f9a5b914862399f7dee868f",
      "summary": "We propose an application of sequence generative adversarial networks (SeqGAN), which are generative adversarial networks for discrete sequence generation, for creating polyphonic musical sequences. Instead of a monophonic melody generation suggested in the original work, we present an efficient representation of a polyphony MIDI file that simultaneously captures chords and melodies with dynamic timings. The proposed method condenses duration, octaves, and keys of both melodies and chords into a single word vector representation, and recurrent neural networks learn to predict distributions of sequences from the embedded musical word space. We experiment with the original method and the least squares method to the discriminator, which is known to stabilize the training of GANs. The network can create sequences that are musically coherent and shows an improved quantitative and qualitative measures. We also report that careful optimization of reinforcement learning signals of the model is crucial for general application of the model.",
      "intriguing_abstract": "We propose an application of sequence generative adversarial networks (SeqGAN), which are generative adversarial networks for discrete sequence generation, for creating polyphonic musical sequences. Instead of a monophonic melody generation suggested in the original work, we present an efficient representation of a polyphony MIDI file that simultaneously captures chords and melodies with dynamic timings. The proposed method condenses duration, octaves, and keys of both melodies and chords into a single word vector representation, and recurrent neural networks learn to predict distributions of sequences from the embedded musical word space. We experiment with the original method and the least squares method to the discriminator, which is known to stabilize the training of GANs. The network can create sequences that are musically coherent and shows an improved quantitative and qualitative measures. We also report that careful optimization of reinforcement learning signals of the model is crucial for general application of the model.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/142c254eb3c3ff50b82beeb0e2de5c8d7393f922.pdf",
      "citation_key": "lee2017zsj",
      "metadata": {
        "title": "Polyphonic Music Generation with Sequence Generative Adversarial Networks",
        "authors": [
          "Sang-gil Lee",
          "Uiwon Hwang",
          "Seonwoo Min",
          "Sungroh Yoon"
        ],
        "published_date": "2017",
        "abstract": "We propose an application of sequence generative adversarial networks (SeqGAN), which are generative adversarial networks for discrete sequence generation, for creating polyphonic musical sequences. Instead of a monophonic melody generation suggested in the original work, we present an efficient representation of a polyphony MIDI file that simultaneously captures chords and melodies with dynamic timings. The proposed method condenses duration, octaves, and keys of both melodies and chords into a single word vector representation, and recurrent neural networks learn to predict distributions of sequences from the embedded musical word space. We experiment with the original method and the least squares method to the discriminator, which is known to stabilize the training of GANs. The network can create sequences that are musically coherent and shows an improved quantitative and qualitative measures. We also report that careful optimization of reinforcement learning signals of the model is crucial for general application of the model.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/142c254eb3c3ff50b82beeb0e2de5c8d7393f922.pdf",
        "venue": "Journal of KIISE",
        "citationCount": 14,
        "score": 1.75,
        "summary": "We propose an application of sequence generative adversarial networks (SeqGAN), which are generative adversarial networks for discrete sequence generation, for creating polyphonic musical sequences. Instead of a monophonic melody generation suggested in the original work, we present an efficient representation of a polyphony MIDI file that simultaneously captures chords and melodies with dynamic timings. The proposed method condenses duration, octaves, and keys of both melodies and chords into a single word vector representation, and recurrent neural networks learn to predict distributions of sequences from the embedded musical word space. We experiment with the original method and the least squares method to the discriminator, which is known to stabilize the training of GANs. The network can create sequences that are musically coherent and shows an improved quantitative and qualitative measures. We also report that careful optimization of reinforcement learning signals of the model is crucial for general application of the model.",
        "keywords": []
      },
      "file_name": "142c254eb3c3ff50b82beeb0e2de5c8d7393f922.pdf"
    },
    {
      "success": true,
      "doc_id": "3372a2c0c31b633fa07663b2cb8b6c3f",
      "summary": "Generative adversarial networks (GANs) are effective in generating realistic images but the training is often unstable. There are existing efforts that model the training dynamics of GANs in the parameter space but the analysis cannot directly motivate practically effective stabilizing methods. To this end, we present a conceptually novel perspective from control theory to directly model the dynamics of GANs in the function space and provide simple yet effective methods to stabilize GANs' training. We first analyze the training dynamic of a prototypical Dirac GAN and adopt the widely-used closed-loop control (CLC) to improve its stability. We then extend CLC to stabilize the training dynamic of normal GANs, where CLC is implemented as a squared $L2$ regularizer on the output of the discriminator. Empirical results show that our method can effectively stabilize the training and obtain state-of-the-art performance on data generation tasks.",
      "intriguing_abstract": "Generative adversarial networks (GANs) are effective in generating realistic images but the training is often unstable. There are existing efforts that model the training dynamics of GANs in the parameter space but the analysis cannot directly motivate practically effective stabilizing methods. To this end, we present a conceptually novel perspective from control theory to directly model the dynamics of GANs in the function space and provide simple yet effective methods to stabilize GANs' training. We first analyze the training dynamic of a prototypical Dirac GAN and adopt the widely-used closed-loop control (CLC) to improve its stability. We then extend CLC to stabilize the training dynamic of normal GANs, where CLC is implemented as a squared $L2$ regularizer on the output of the discriminator. Empirical results show that our method can effectively stabilize the training and obtain state-of-the-art performance on data generation tasks.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/466f2700541252556dea82ec3ba625c6e7a61c29.pdf",
      "citation_key": "xu2019uwg",
      "metadata": {
        "title": "Understanding and Stabilizing GANs' Training Dynamics with Control Theory",
        "authors": [
          "Kun Xu",
          "Chongxuan Li",
          "Huanshu Wei",
          "Jun Zhu",
          "Bo Zhang"
        ],
        "published_date": "2019",
        "abstract": "Generative adversarial networks (GANs) are effective in generating realistic images but the training is often unstable. There are existing efforts that model the training dynamics of GANs in the parameter space but the analysis cannot directly motivate practically effective stabilizing methods. To this end, we present a conceptually novel perspective from control theory to directly model the dynamics of GANs in the function space and provide simple yet effective methods to stabilize GANs' training. We first analyze the training dynamic of a prototypical Dirac GAN and adopt the widely-used closed-loop control (CLC) to improve its stability. We then extend CLC to stabilize the training dynamic of normal GANs, where CLC is implemented as a squared $L2$ regularizer on the output of the discriminator. Empirical results show that our method can effectively stabilize the training and obtain state-of-the-art performance on data generation tasks.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/466f2700541252556dea82ec3ba625c6e7a61c29.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 1.6666666666666665,
        "summary": "Generative adversarial networks (GANs) are effective in generating realistic images but the training is often unstable. There are existing efforts that model the training dynamics of GANs in the parameter space but the analysis cannot directly motivate practically effective stabilizing methods. To this end, we present a conceptually novel perspective from control theory to directly model the dynamics of GANs in the function space and provide simple yet effective methods to stabilize GANs' training. We first analyze the training dynamic of a prototypical Dirac GAN and adopt the widely-used closed-loop control (CLC) to improve its stability. We then extend CLC to stabilize the training dynamic of normal GANs, where CLC is implemented as a squared $L2$ regularizer on the output of the discriminator. Empirical results show that our method can effectively stabilize the training and obtain state-of-the-art performance on data generation tasks.",
        "keywords": []
      },
      "file_name": "466f2700541252556dea82ec3ba625c6e7a61c29.pdf"
    },
    {
      "success": true,
      "doc_id": "b1e6686909f07e9d6c9bc287ba43adcb",
      "summary": "Generative Adversarial Networks (GANs) are one of the most popular and powerful models to learn the complex high dimensional distributions. However, they usually suffer from instability and generalization issues which may lead to poor generations. Most existing works focus on stabilizing the training for the discriminators of GANs while ignoring their generalization issue. In this work, we aim to improve the generalization capability of GANs by promoting the local robustness within the small neighborhood of the training samples. We prove that the robustness in the small neighborhood of the training sets can lead to better generalization. Particularly, we design a new robust method called Robust Generative Adversarial Network (RGAN) in which the generator and discriminator compete with each other in a worst-case setting within a small Wasserstein ball. The generator tries to map the worst input distribution (rather than a Gaussian distribution used in most GANs) to the real data distribution, while the discriminator attempts to distinguish the real and fake distributions with the worst perturbations . Intuitively, the proposed RGAN can learn a good generator and discriminator that can even perform well on the worst-case input points. Strictly, we have proved that RGAN can obtain a tighter generalization upper bound than the traditional GANs under mild assumptions, ensuring a theoretical superiority of RGAN over GANs. We conduct our proposed method on five different baselines (five popular GAN models). And a series of experiments on CIFAR-10, STL-10 and CelebA datasets indicate that our proposed robust frameworks outperform five baseline models substantially and consistently.",
      "intriguing_abstract": "Generative Adversarial Networks (GANs) are one of the most popular and powerful models to learn the complex high dimensional distributions. However, they usually suffer from instability and generalization issues which may lead to poor generations. Most existing works focus on stabilizing the training for the discriminators of GANs while ignoring their generalization issue. In this work, we aim to improve the generalization capability of GANs by promoting the local robustness within the small neighborhood of the training samples. We prove that the robustness in the small neighborhood of the training sets can lead to better generalization. Particularly, we design a new robust method called Robust Generative Adversarial Network (RGAN) in which the generator and discriminator compete with each other in a worst-case setting within a small Wasserstein ball. The generator tries to map the worst input distribution (rather than a Gaussian distribution used in most GANs) to the real data distribution, while the discriminator attempts to distinguish the real and fake distributions with the worst perturbations . Intuitively, the proposed RGAN can learn a good generator and discriminator that can even perform well on the worst-case input points. Strictly, we have proved that RGAN can obtain a tighter generalization upper bound than the traditional GANs under mild assumptions, ensuring a theoretical superiority of RGAN over GANs. We conduct our proposed method on five different baselines (five popular GAN models). And a series of experiments on CIFAR-10, STL-10 and CelebA datasets indicate that our proposed robust frameworks outperform five baseline models substantially and consistently.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/9f1c57e9807835eba3d6b7991e8b371e9df5ec77.pdf",
      "citation_key": "zhang201996t",
      "metadata": {
        "title": "Robust generative adversarial network",
        "authors": [
          "Shufei Zhang",
          "Zhuang Qian",
          "Kaizhu Huang",
          "Jimin Xiao",
          "Yuan He"
        ],
        "published_date": "2019",
        "abstract": "Generative Adversarial Networks (GANs) are one of the most popular and powerful models to learn the complex high dimensional distributions. However, they usually suffer from instability and generalization issues which may lead to poor generations. Most existing works focus on stabilizing the training for the discriminators of GANs while ignoring their generalization issue. In this work, we aim to improve the generalization capability of GANs by promoting the local robustness within the small neighborhood of the training samples. We prove that the robustness in the small neighborhood of the training sets can lead to better generalization. Particularly, we design a new robust method called Robust Generative Adversarial Network (RGAN) in which the generator and discriminator compete with each other in a worst-case setting within a small Wasserstein ball. The generator tries to map the worst input distribution (rather than a Gaussian distribution used in most GANs) to the real data distribution, while the discriminator attempts to distinguish the real and fake distributions with the worst perturbations . Intuitively, the proposed RGAN can learn a good generator and discriminator that can even perform well on the worst-case input points. Strictly, we have proved that RGAN can obtain a tighter generalization upper bound than the traditional GANs under mild assumptions, ensuring a theoretical superiority of RGAN over GANs. We conduct our proposed method on five different baselines (five popular GAN models). And a series of experiments on CIFAR-10, STL-10 and CelebA datasets indicate that our proposed robust frameworks outperform five baseline models substantially and consistently.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/9f1c57e9807835eba3d6b7991e8b371e9df5ec77.pdf",
        "venue": "Machine-mediated learning",
        "citationCount": 10,
        "score": 1.6666666666666665,
        "summary": "Generative Adversarial Networks (GANs) are one of the most popular and powerful models to learn the complex high dimensional distributions. However, they usually suffer from instability and generalization issues which may lead to poor generations. Most existing works focus on stabilizing the training for the discriminators of GANs while ignoring their generalization issue. In this work, we aim to improve the generalization capability of GANs by promoting the local robustness within the small neighborhood of the training samples. We prove that the robustness in the small neighborhood of the training sets can lead to better generalization. Particularly, we design a new robust method called Robust Generative Adversarial Network (RGAN) in which the generator and discriminator compete with each other in a worst-case setting within a small Wasserstein ball. The generator tries to map the worst input distribution (rather than a Gaussian distribution used in most GANs) to the real data distribution, while the discriminator attempts to distinguish the real and fake distributions with the worst perturbations . Intuitively, the proposed RGAN can learn a good generator and discriminator that can even perform well on the worst-case input points. Strictly, we have proved that RGAN can obtain a tighter generalization upper bound than the traditional GANs under mild assumptions, ensuring a theoretical superiority of RGAN over GANs. We conduct our proposed method on five different baselines (five popular GAN models). And a series of experiments on CIFAR-10, STL-10 and CelebA datasets indicate that our proposed robust frameworks outperform five baseline models substantially and consistently.",
        "keywords": []
      },
      "file_name": "9f1c57e9807835eba3d6b7991e8b371e9df5ec77.pdf"
    },
    {
      "success": true,
      "doc_id": "a1d9add14af5c5fe8933f34ff9b71696",
      "summary": "Generative adversarial networks (GANs) have demonstrated to be successful at generating realistic real-world images. In this paper we compare various GAN techniques, both supervised and unsupervised. The effects on training stability of different objective functions are compared. We add an encoder to the network, making it possible to encode images to the latent space of the GAN. The generator, discriminator and encoder are parameterized by deep convolutional neural networks. For the discriminator network we experimented with using the novel Capsule Network, a state-of-the-art technique for detecting global features in images. Experiments are performed using a digit and face dataset, with various visualizations illustrating the results. The results show that using the encoder network it is possible to reconstruct images. With the conditional GAN we can alter visual attributes of generated or encoded images. The experiments with the Capsule Network as discriminator result in generated images of a lower quality, compared to a standard convolutional neural network.",
      "intriguing_abstract": "Generative adversarial networks (GANs) have demonstrated to be successful at generating realistic real-world images. In this paper we compare various GAN techniques, both supervised and unsupervised. The effects on training stability of different objective functions are compared. We add an encoder to the network, making it possible to encode images to the latent space of the GAN. The generator, discriminator and encoder are parameterized by deep convolutional neural networks. For the discriminator network we experimented with using the novel Capsule Network, a state-of-the-art technique for detecting global features in images. Experiments are performed using a digit and face dataset, with various visualizations illustrating the results. The results show that using the encoder network it is possible to reconstruct images. With the conditional GAN we can alter visual attributes of generated or encoded images. The experiments with the Capsule Network as discriminator result in generated images of a lower quality, compared to a standard convolutional neural network.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/577ae124136e0e76caf8fbc52e6b6d2072d70bff.pdf",
      "citation_key": "pieters2018jh1",
      "metadata": {
        "title": "Comparing Generative Adversarial Network Techniques for Image Creation and Modification",
        "authors": [
          "Mathijs Pieters",
          "M. Wiering"
        ],
        "published_date": "2018",
        "abstract": "Generative adversarial networks (GANs) have demonstrated to be successful at generating realistic real-world images. In this paper we compare various GAN techniques, both supervised and unsupervised. The effects on training stability of different objective functions are compared. We add an encoder to the network, making it possible to encode images to the latent space of the GAN. The generator, discriminator and encoder are parameterized by deep convolutional neural networks. For the discriminator network we experimented with using the novel Capsule Network, a state-of-the-art technique for detecting global features in images. Experiments are performed using a digit and face dataset, with various visualizations illustrating the results. The results show that using the encoder network it is possible to reconstruct images. With the conditional GAN we can alter visual attributes of generated or encoded images. The experiments with the Capsule Network as discriminator result in generated images of a lower quality, compared to a standard convolutional neural network.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/577ae124136e0e76caf8fbc52e6b6d2072d70bff.pdf",
        "venue": "arXiv.org",
        "citationCount": 11,
        "score": 1.5714285714285714,
        "summary": "Generative adversarial networks (GANs) have demonstrated to be successful at generating realistic real-world images. In this paper we compare various GAN techniques, both supervised and unsupervised. The effects on training stability of different objective functions are compared. We add an encoder to the network, making it possible to encode images to the latent space of the GAN. The generator, discriminator and encoder are parameterized by deep convolutional neural networks. For the discriminator network we experimented with using the novel Capsule Network, a state-of-the-art technique for detecting global features in images. Experiments are performed using a digit and face dataset, with various visualizations illustrating the results. The results show that using the encoder network it is possible to reconstruct images. With the conditional GAN we can alter visual attributes of generated or encoded images. The experiments with the Capsule Network as discriminator result in generated images of a lower quality, compared to a standard convolutional neural network.",
        "keywords": []
      },
      "file_name": "577ae124136e0e76caf8fbc52e6b6d2072d70bff.pdf"
    },
    {
      "success": true,
      "doc_id": "6dc75238e8b3a5b9831a6b016d203c45",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/e088a2537492ed5a22885e871a51102a95c97cb6.pdf",
      "citation_key": "xiang2017cc9",
      "metadata": {
        "title": "On the effect of Batch Normalization and Weight Normalization in Generative Adversarial Networks",
        "authors": [
          "Sitao Xiang",
          "Hao Li"
        ],
        "published_date": "2017",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/e088a2537492ed5a22885e871a51102a95c97cb6.pdf",
        "venue": "arXiv.org",
        "citationCount": 12,
        "score": 1.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "e088a2537492ed5a22885e871a51102a95c97cb6.pdf"
    },
    {
      "success": true,
      "doc_id": "dff394ec55f3563861e06c8976c5ac04",
      "summary": "Deep learning (DL) has gained traction in ground-penetrating radar (GPR) tasks. However, obtaining sufficient training data presents a significant challenge. We introduce a structure-adaptive GPR-generative adversarial network (GAN) to generate GPR defect data. GPR-GAN employs double normalization for stabilizing parameters and convolution outputs, an adaptive discriminator augmentation (ADA) module for small dataset training stability, and a modified self-attention (MSA) module to generate GPR defects with complex features. We evaluated the performance of GPR-GAN using three datasets in conjunction with three state-of-the-art detection networks (faster region-based convolutional neural network (FasterRCNN), single-shot multibox detector (SSD), and YOLOv5). Our results reveal that GPR-GAN exhibits strong generalization skills, adeptly adapting to GPR data generation tasks that encompasses a variety of targets, frequencies, and equipment. GPR-GAN generated data increased the <inline-formula> <tex-math notation=\"LaTeX\">$F1$ </tex-math></inline-formula> score for void recognition in simulation data by at least 5.27%, improved the average <inline-formula> <tex-math notation=\"LaTeX\">$F1$ </tex-math></inline-formula> score for highway pavement defect detection by at least 7.68%, and enhanced the average <inline-formula> <tex-math notation=\"LaTeX\">$F1$ </tex-math></inline-formula> score for railway subgrade defect detection by at least 9.22%. GPR-GAN offers a powerful data support tool for DL research in GPR.",
      "intriguing_abstract": "Deep learning (DL) has gained traction in ground-penetrating radar (GPR) tasks. However, obtaining sufficient training data presents a significant challenge. We introduce a structure-adaptive GPR-generative adversarial network (GAN) to generate GPR defect data. GPR-GAN employs double normalization for stabilizing parameters and convolution outputs, an adaptive discriminator augmentation (ADA) module for small dataset training stability, and a modified self-attention (MSA) module to generate GPR defects with complex features. We evaluated the performance of GPR-GAN using three datasets in conjunction with three state-of-the-art detection networks (faster region-based convolutional neural network (FasterRCNN), single-shot multibox detector (SSD), and YOLOv5). Our results reveal that GPR-GAN exhibits strong generalization skills, adeptly adapting to GPR data generation tasks that encompasses a variety of targets, frequencies, and equipment. GPR-GAN generated data increased the <inline-formula> <tex-math notation=\"LaTeX\">$F1$ </tex-math></inline-formula> score for void recognition in simulation data by at least 5.27%, improved the average <inline-formula> <tex-math notation=\"LaTeX\">$F1$ </tex-math></inline-formula> score for highway pavement defect detection by at least 7.68%, and enhanced the average <inline-formula> <tex-math notation=\"LaTeX\">$F1$ </tex-math></inline-formula> score for railway subgrade defect detection by at least 9.22%. GPR-GAN offers a powerful data support tool for DL research in GPR.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/dd8254e104ddc7f2fa80f3baaa8537780aa2f65c.pdf",
      "citation_key": "xiong20243bt",
      "metadata": {
        "title": "GPR-GAN: A Ground-Penetrating Radar Data Generative Adversarial Network",
        "authors": [
          "Hongqiang Xiong",
          "Jing Li",
          "Zhilian Li",
          "Zhiyu Zhang"
        ],
        "published_date": "2024",
        "abstract": "Deep learning (DL) has gained traction in ground-penetrating radar (GPR) tasks. However, obtaining sufficient training data presents a significant challenge. We introduce a structure-adaptive GPR-generative adversarial network (GAN) to generate GPR defect data. GPR-GAN employs double normalization for stabilizing parameters and convolution outputs, an adaptive discriminator augmentation (ADA) module for small dataset training stability, and a modified self-attention (MSA) module to generate GPR defects with complex features. We evaluated the performance of GPR-GAN using three datasets in conjunction with three state-of-the-art detection networks (faster region-based convolutional neural network (FasterRCNN), single-shot multibox detector (SSD), and YOLOv5). Our results reveal that GPR-GAN exhibits strong generalization skills, adeptly adapting to GPR data generation tasks that encompasses a variety of targets, frequencies, and equipment. GPR-GAN generated data increased the <inline-formula> <tex-math notation=\"LaTeX\">$F1$ </tex-math></inline-formula> score for void recognition in simulation data by at least 5.27%, improved the average <inline-formula> <tex-math notation=\"LaTeX\">$F1$ </tex-math></inline-formula> score for highway pavement defect detection by at least 7.68%, and enhanced the average <inline-formula> <tex-math notation=\"LaTeX\">$F1$ </tex-math></inline-formula> score for railway subgrade defect detection by at least 9.22%. GPR-GAN offers a powerful data support tool for DL research in GPR.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/dd8254e104ddc7f2fa80f3baaa8537780aa2f65c.pdf",
        "venue": "IEEE Transactions on Geoscience and Remote Sensing",
        "citationCount": 36,
        "score": 36.0,
        "summary": "Deep learning (DL) has gained traction in ground-penetrating radar (GPR) tasks. However, obtaining sufficient training data presents a significant challenge. We introduce a structure-adaptive GPR-generative adversarial network (GAN) to generate GPR defect data. GPR-GAN employs double normalization for stabilizing parameters and convolution outputs, an adaptive discriminator augmentation (ADA) module for small dataset training stability, and a modified self-attention (MSA) module to generate GPR defects with complex features. We evaluated the performance of GPR-GAN using three datasets in conjunction with three state-of-the-art detection networks (faster region-based convolutional neural network (FasterRCNN), single-shot multibox detector (SSD), and YOLOv5). Our results reveal that GPR-GAN exhibits strong generalization skills, adeptly adapting to GPR data generation tasks that encompasses a variety of targets, frequencies, and equipment. GPR-GAN generated data increased the <inline-formula> <tex-math notation=\"LaTeX\">$F1$ </tex-math></inline-formula> score for void recognition in simulation data by at least 5.27%, improved the average <inline-formula> <tex-math notation=\"LaTeX\">$F1$ </tex-math></inline-formula> score for highway pavement defect detection by at least 7.68%, and enhanced the average <inline-formula> <tex-math notation=\"LaTeX\">$F1$ </tex-math></inline-formula> score for railway subgrade defect detection by at least 9.22%. GPR-GAN offers a powerful data support tool for DL research in GPR.",
        "keywords": []
      },
      "file_name": "dd8254e104ddc7f2fa80f3baaa8537780aa2f65c.pdf"
    },
    {
      "success": true,
      "doc_id": "399f65e2cc3ce87615bb1cbdb6b7373e",
      "summary": "Generative adversarial networks (GANs) are a powerful generative technique but frequently face challenges with training stability. Network architecture plays a significant role in determining the final output of GANs, but designing a fine architecture demands extensive domain expertise. This article aims to address this issue by searching for high-performance generator’s architectures through neural architecture search (NAS). The proposed approach, called evolutionary weight sharing GANs (EWSGAN), is based on weight sharing and comprises two steps. First, a supernet of the generator is trained using weight sharing. Second, a multiobjective evolutionary algorithm (MOEA) is employed to identify optimal subnets from the supernet. These subnets inherit weights directly from the supernet for fitness assessment. Two strategies are used to stabilize the training of the generator supernet: 1) a fair single-path sampling strategy and 2) a discarding strategy. Experimental results indicate that the architecture searched by our method achieved a new state-of-the-art among NAS–GAN methods with a Fréchet inception distance (FID) of 9.09 and an inception score (IS) of 8.99 on the CIFAR-10 dataset. It also demonstrates competitive performance on the STL-10 dataset, achieving FID of 21.89 and IS of 10.51.",
      "intriguing_abstract": "Generative adversarial networks (GANs) are a powerful generative technique but frequently face challenges with training stability. Network architecture plays a significant role in determining the final output of GANs, but designing a fine architecture demands extensive domain expertise. This article aims to address this issue by searching for high-performance generator’s architectures through neural architecture search (NAS). The proposed approach, called evolutionary weight sharing GANs (EWSGAN), is based on weight sharing and comprises two steps. First, a supernet of the generator is trained using weight sharing. Second, a multiobjective evolutionary algorithm (MOEA) is employed to identify optimal subnets from the supernet. These subnets inherit weights directly from the supernet for fitness assessment. Two strategies are used to stabilize the training of the generator supernet: 1) a fair single-path sampling strategy and 2) a discarding strategy. Experimental results indicate that the architecture searched by our method achieved a new state-of-the-art among NAS–GAN methods with a Fréchet inception distance (FID) of 9.09 and an inception score (IS) of 8.99 on the CIFAR-10 dataset. It also demonstrates competitive performance on the STL-10 dataset, achieving FID of 21.89 and IS of 10.51.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/71a373b66f3c48c49901183d2df269e2fee78c44.pdf",
      "citation_key": "xue2024e7i",
      "metadata": {
        "title": "Evolutionary Architecture Search for Generative Adversarial Networks Based on Weight Sharing",
        "authors": [
          "Yu Xue",
          "Weinan Tong",
          "Ferrante Neri",
          "Peng Chen",
          "Tao Luo",
          "Liangli Zhen",
          "Xiao Wang"
        ],
        "published_date": "2024",
        "abstract": "Generative adversarial networks (GANs) are a powerful generative technique but frequently face challenges with training stability. Network architecture plays a significant role in determining the final output of GANs, but designing a fine architecture demands extensive domain expertise. This article aims to address this issue by searching for high-performance generator’s architectures through neural architecture search (NAS). The proposed approach, called evolutionary weight sharing GANs (EWSGAN), is based on weight sharing and comprises two steps. First, a supernet of the generator is trained using weight sharing. Second, a multiobjective evolutionary algorithm (MOEA) is employed to identify optimal subnets from the supernet. These subnets inherit weights directly from the supernet for fitness assessment. Two strategies are used to stabilize the training of the generator supernet: 1) a fair single-path sampling strategy and 2) a discarding strategy. Experimental results indicate that the architecture searched by our method achieved a new state-of-the-art among NAS–GAN methods with a Fréchet inception distance (FID) of 9.09 and an inception score (IS) of 8.99 on the CIFAR-10 dataset. It also demonstrates competitive performance on the STL-10 dataset, achieving FID of 21.89 and IS of 10.51.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/71a373b66f3c48c49901183d2df269e2fee78c44.pdf",
        "venue": "IEEE Transactions on Evolutionary Computation",
        "citationCount": 19,
        "score": 19.0,
        "summary": "Generative adversarial networks (GANs) are a powerful generative technique but frequently face challenges with training stability. Network architecture plays a significant role in determining the final output of GANs, but designing a fine architecture demands extensive domain expertise. This article aims to address this issue by searching for high-performance generator’s architectures through neural architecture search (NAS). The proposed approach, called evolutionary weight sharing GANs (EWSGAN), is based on weight sharing and comprises two steps. First, a supernet of the generator is trained using weight sharing. Second, a multiobjective evolutionary algorithm (MOEA) is employed to identify optimal subnets from the supernet. These subnets inherit weights directly from the supernet for fitness assessment. Two strategies are used to stabilize the training of the generator supernet: 1) a fair single-path sampling strategy and 2) a discarding strategy. Experimental results indicate that the architecture searched by our method achieved a new state-of-the-art among NAS–GAN methods with a Fréchet inception distance (FID) of 9.09 and an inception score (IS) of 8.99 on the CIFAR-10 dataset. It also demonstrates competitive performance on the STL-10 dataset, achieving FID of 21.89 and IS of 10.51.",
        "keywords": []
      },
      "file_name": "71a373b66f3c48c49901183d2df269e2fee78c44.pdf"
    },
    {
      "success": true,
      "doc_id": "1e3ddb7a7fca9db25b15e623640139e7",
      "summary": "Generative adversarial networks (GANs) are machine learning algorithms that can efficiently generate data such as images. Although GANs are very popular, their training usually lacks stability, with the generator and discriminator networks failing to converge during the training process. To address this problem and improve the stability of GANs, in this paper, we automate the design of stable GANs architectures through a novel approach: differentiable architecture search with attention mechanisms for generative adversarial networks (DAMGAN). We construct a generator supernet and search for the optimal generator network within it. We propose incorporating two attention mechanisms between each pair of nodes in the supernet. The first attention mechanism, down attention, selects the optimal candidate operation of each edge in the supernet, while the second attention mechanism, up attention, improves the training stability of the supernet and limits the computational cost of the search by selecting the most important feature maps for the following candidate operations. Experimental results show that the architectures searched by our method obtain a state-of-the-art inception score (IS) of 8.99 and a very competitive Fréchet inception distance (FID) of 10.27 on the CIFAR-10 dataset. Competitive results were also obtained on the STL-10 dataset (IS = 10.35, FID = 22.18). Notably, our search time was only 0.09 GPU days.",
      "intriguing_abstract": "Generative adversarial networks (GANs) are machine learning algorithms that can efficiently generate data such as images. Although GANs are very popular, their training usually lacks stability, with the generator and discriminator networks failing to converge during the training process. To address this problem and improve the stability of GANs, in this paper, we automate the design of stable GANs architectures through a novel approach: differentiable architecture search with attention mechanisms for generative adversarial networks (DAMGAN). We construct a generator supernet and search for the optimal generator network within it. We propose incorporating two attention mechanisms between each pair of nodes in the supernet. The first attention mechanism, down attention, selects the optimal candidate operation of each edge in the supernet, while the second attention mechanism, up attention, improves the training stability of the supernet and limits the computational cost of the search by selecting the most important feature maps for the following candidate operations. Experimental results show that the architectures searched by our method obtain a state-of-the-art inception score (IS) of 8.99 and a very competitive Fréchet inception distance (FID) of 10.27 on the CIFAR-10 dataset. Competitive results were also obtained on the STL-10 dataset (IS = 10.35, FID = 22.18). Notably, our search time was only 0.09 GPU days.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/cfbafb898a5fd26324c30eecf384dfdc34521090.pdf",
      "citation_key": "xue20248md",
      "metadata": {
        "title": "Differentiable Architecture Search With Attention Mechanisms for Generative Adversarial Networks",
        "authors": [
          "Yu Xue",
          "Kun Chen",
          "Ferrante Neri"
        ],
        "published_date": "2024",
        "abstract": "Generative adversarial networks (GANs) are machine learning algorithms that can efficiently generate data such as images. Although GANs are very popular, their training usually lacks stability, with the generator and discriminator networks failing to converge during the training process. To address this problem and improve the stability of GANs, in this paper, we automate the design of stable GANs architectures through a novel approach: differentiable architecture search with attention mechanisms for generative adversarial networks (DAMGAN). We construct a generator supernet and search for the optimal generator network within it. We propose incorporating two attention mechanisms between each pair of nodes in the supernet. The first attention mechanism, down attention, selects the optimal candidate operation of each edge in the supernet, while the second attention mechanism, up attention, improves the training stability of the supernet and limits the computational cost of the search by selecting the most important feature maps for the following candidate operations. Experimental results show that the architectures searched by our method obtain a state-of-the-art inception score (IS) of 8.99 and a very competitive Fréchet inception distance (FID) of 10.27 on the CIFAR-10 dataset. Competitive results were also obtained on the STL-10 dataset (IS = 10.35, FID = 22.18). Notably, our search time was only 0.09 GPU days.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/cfbafb898a5fd26324c30eecf384dfdc34521090.pdf",
        "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
        "citationCount": 13,
        "score": 13.0,
        "summary": "Generative adversarial networks (GANs) are machine learning algorithms that can efficiently generate data such as images. Although GANs are very popular, their training usually lacks stability, with the generator and discriminator networks failing to converge during the training process. To address this problem and improve the stability of GANs, in this paper, we automate the design of stable GANs architectures through a novel approach: differentiable architecture search with attention mechanisms for generative adversarial networks (DAMGAN). We construct a generator supernet and search for the optimal generator network within it. We propose incorporating two attention mechanisms between each pair of nodes in the supernet. The first attention mechanism, down attention, selects the optimal candidate operation of each edge in the supernet, while the second attention mechanism, up attention, improves the training stability of the supernet and limits the computational cost of the search by selecting the most important feature maps for the following candidate operations. Experimental results show that the architectures searched by our method obtain a state-of-the-art inception score (IS) of 8.99 and a very competitive Fréchet inception distance (FID) of 10.27 on the CIFAR-10 dataset. Competitive results were also obtained on the STL-10 dataset (IS = 10.35, FID = 22.18). Notably, our search time was only 0.09 GPU days.",
        "keywords": []
      },
      "file_name": "cfbafb898a5fd26324c30eecf384dfdc34521090.pdf"
    },
    {
      "success": true,
      "doc_id": "188e38dc6de884a20bf27d8e54c6ef77",
      "summary": "Over the past few years, there has been a proliferation of research in the area of generative adversarial networks (GANs). GANs present a novel approach to producing synthetic data in varying fields including medicine, traffic control, text transferring, image generation, and cybersecurity. To improve the quality of synthetic generation, specifically for images, the GAN technique was paired with convolutional neural networks (CNNs) to build deep convolutional generative adversarial networks (DCGAN). The DCGAN framework is a simple yet stable framework shown to generate quality photorealistic images. There are a number of studies reviewing GANs, providing a comparative analysis of performance, stabilization, and training methods. With respects to the DCGAN architecture, there are literature reviews reporting its usage in forensic sketch to face transformation and fuzzy face recognition. Here, we provide a review detailing the use of the DCGAN framework with biometrics samples for advancements in biometric authentication systems and cybersecurity. As GANs have shown to be a primary tool in generating deepfakes, we explore the use of DCGANs to generating synthetic biometrics that can deceive security systems and serve as quality training data for other machine learning models. The goal of this review is to contribute a concise consolidated review of techniques involving the DCGAN framework and biometric samples for the improvement of biometric recognition systems and to be used as a reference point for future work in cybersecurity.",
      "intriguing_abstract": "Over the past few years, there has been a proliferation of research in the area of generative adversarial networks (GANs). GANs present a novel approach to producing synthetic data in varying fields including medicine, traffic control, text transferring, image generation, and cybersecurity. To improve the quality of synthetic generation, specifically for images, the GAN technique was paired with convolutional neural networks (CNNs) to build deep convolutional generative adversarial networks (DCGAN). The DCGAN framework is a simple yet stable framework shown to generate quality photorealistic images. There are a number of studies reviewing GANs, providing a comparative analysis of performance, stabilization, and training methods. With respects to the DCGAN architecture, there are literature reviews reporting its usage in forensic sketch to face transformation and fuzzy face recognition. Here, we provide a review detailing the use of the DCGAN framework with biometrics samples for advancements in biometric authentication systems and cybersecurity. As GANs have shown to be a primary tool in generating deepfakes, we explore the use of DCGANs to generating synthetic biometrics that can deceive security systems and serve as quality training data for other machine learning models. The goal of this review is to contribute a concise consolidated review of techniques involving the DCGAN framework and biometric samples for the improvement of biometric recognition systems and to be used as a reference point for future work in cybersecurity.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/688b69ff20e5547dfcbc757881bdad44b1139f06.pdf",
      "citation_key": "jenkins2024qf5",
      "metadata": {
        "title": "Exploring deep convolutional generative adversarial networks (DCGAN) in biometric systems: a survey study",
        "authors": [
          "John Jenkins",
          "Kaushik Roy"
        ],
        "published_date": "2024",
        "abstract": "Over the past few years, there has been a proliferation of research in the area of generative adversarial networks (GANs). GANs present a novel approach to producing synthetic data in varying fields including medicine, traffic control, text transferring, image generation, and cybersecurity. To improve the quality of synthetic generation, specifically for images, the GAN technique was paired with convolutional neural networks (CNNs) to build deep convolutional generative adversarial networks (DCGAN). The DCGAN framework is a simple yet stable framework shown to generate quality photorealistic images. There are a number of studies reviewing GANs, providing a comparative analysis of performance, stabilization, and training methods. With respects to the DCGAN architecture, there are literature reviews reporting its usage in forensic sketch to face transformation and fuzzy face recognition. Here, we provide a review detailing the use of the DCGAN framework with biometrics samples for advancements in biometric authentication systems and cybersecurity. As GANs have shown to be a primary tool in generating deepfakes, we explore the use of DCGANs to generating synthetic biometrics that can deceive security systems and serve as quality training data for other machine learning models. The goal of this review is to contribute a concise consolidated review of techniques involving the DCGAN framework and biometric samples for the improvement of biometric recognition systems and to be used as a reference point for future work in cybersecurity.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/688b69ff20e5547dfcbc757881bdad44b1139f06.pdf",
        "venue": "Discover Artificial Intelligence",
        "citationCount": 13,
        "score": 13.0,
        "summary": "Over the past few years, there has been a proliferation of research in the area of generative adversarial networks (GANs). GANs present a novel approach to producing synthetic data in varying fields including medicine, traffic control, text transferring, image generation, and cybersecurity. To improve the quality of synthetic generation, specifically for images, the GAN technique was paired with convolutional neural networks (CNNs) to build deep convolutional generative adversarial networks (DCGAN). The DCGAN framework is a simple yet stable framework shown to generate quality photorealistic images. There are a number of studies reviewing GANs, providing a comparative analysis of performance, stabilization, and training methods. With respects to the DCGAN architecture, there are literature reviews reporting its usage in forensic sketch to face transformation and fuzzy face recognition. Here, we provide a review detailing the use of the DCGAN framework with biometrics samples for advancements in biometric authentication systems and cybersecurity. As GANs have shown to be a primary tool in generating deepfakes, we explore the use of DCGANs to generating synthetic biometrics that can deceive security systems and serve as quality training data for other machine learning models. The goal of this review is to contribute a concise consolidated review of techniques involving the DCGAN framework and biometric samples for the improvement of biometric recognition systems and to be used as a reference point for future work in cybersecurity.",
        "keywords": []
      },
      "file_name": "688b69ff20e5547dfcbc757881bdad44b1139f06.pdf"
    },
    {
      "success": true,
      "doc_id": "19cd9767695fef365d68c66cd5f9e8bf",
      "summary": "Investor sentiment has a profound impact on financial market volatility; however, it is difficult to accurately capture the complex nonlinear relationships among sentiment proxies with the existing methods. In this study, we propose a novel investor sentiment indicator, SGAN, which uses generative adversarial networks (GANs) to extract the nonlinear latent structure from eight sentiment proxies from February 2003 to September 2023 in the Chinese A-share market. Unlike traditional linear dimensionality reduction methods, GANs are able to capture complex market dynamics through adversarial training, effectively reducing noise and improving prediction accuracy. The empirical analyses show that SGAN significantly outperforms existing methods in both in-sample and out-of-sample prediction capabilities. The GAN-based investment strategy achieves impressive annualized returns and provides a powerful tool for portfolio construction and risk management. Robustness tests across economic cycles, industries, and U.S. markets further validate the stability of SGAN. These findings highlight the unique advantages of GANs as sentiment-driven financial forecasting tools, providing market participants with new ways to more accurately capture sentiment-shifting trends and develop effective investment strategies.",
      "intriguing_abstract": "Investor sentiment has a profound impact on financial market volatility; however, it is difficult to accurately capture the complex nonlinear relationships among sentiment proxies with the existing methods. In this study, we propose a novel investor sentiment indicator, SGAN, which uses generative adversarial networks (GANs) to extract the nonlinear latent structure from eight sentiment proxies from February 2003 to September 2023 in the Chinese A-share market. Unlike traditional linear dimensionality reduction methods, GANs are able to capture complex market dynamics through adversarial training, effectively reducing noise and improving prediction accuracy. The empirical analyses show that SGAN significantly outperforms existing methods in both in-sample and out-of-sample prediction capabilities. The GAN-based investment strategy achieves impressive annualized returns and provides a powerful tool for portfolio construction and risk management. Robustness tests across economic cycles, industries, and U.S. markets further validate the stability of SGAN. These findings highlight the unique advantages of GANs as sentiment-driven financial forecasting tools, providing market participants with new ways to more accurately capture sentiment-shifting trends and develop effective investment strategies.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/9af3183d3737729bff63303cf6bb6e433ffdbb47.pdf",
      "citation_key": "qiu2025hu0",
      "metadata": {
        "title": "A Generative Adversarial Network-Based Investor Sentiment Indicator: Superior Predictability for the Stock Market",
        "authors": [
          "Shiqing Qiu",
          "Yang Wang",
          "Zong Ke",
          "Qinyan Shen",
          "Zichao Li",
          "Rong Zhang",
          "Kaichen Ouyang"
        ],
        "published_date": "2025",
        "abstract": "Investor sentiment has a profound impact on financial market volatility; however, it is difficult to accurately capture the complex nonlinear relationships among sentiment proxies with the existing methods. In this study, we propose a novel investor sentiment indicator, SGAN, which uses generative adversarial networks (GANs) to extract the nonlinear latent structure from eight sentiment proxies from February 2003 to September 2023 in the Chinese A-share market. Unlike traditional linear dimensionality reduction methods, GANs are able to capture complex market dynamics through adversarial training, effectively reducing noise and improving prediction accuracy. The empirical analyses show that SGAN significantly outperforms existing methods in both in-sample and out-of-sample prediction capabilities. The GAN-based investment strategy achieves impressive annualized returns and provides a powerful tool for portfolio construction and risk management. Robustness tests across economic cycles, industries, and U.S. markets further validate the stability of SGAN. These findings highlight the unique advantages of GANs as sentiment-driven financial forecasting tools, providing market participants with new ways to more accurately capture sentiment-shifting trends and develop effective investment strategies.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/9af3183d3737729bff63303cf6bb6e433ffdbb47.pdf",
        "venue": "Mathematics",
        "citationCount": 13,
        "score": 13.0,
        "summary": "Investor sentiment has a profound impact on financial market volatility; however, it is difficult to accurately capture the complex nonlinear relationships among sentiment proxies with the existing methods. In this study, we propose a novel investor sentiment indicator, SGAN, which uses generative adversarial networks (GANs) to extract the nonlinear latent structure from eight sentiment proxies from February 2003 to September 2023 in the Chinese A-share market. Unlike traditional linear dimensionality reduction methods, GANs are able to capture complex market dynamics through adversarial training, effectively reducing noise and improving prediction accuracy. The empirical analyses show that SGAN significantly outperforms existing methods in both in-sample and out-of-sample prediction capabilities. The GAN-based investment strategy achieves impressive annualized returns and provides a powerful tool for portfolio construction and risk management. Robustness tests across economic cycles, industries, and U.S. markets further validate the stability of SGAN. These findings highlight the unique advantages of GANs as sentiment-driven financial forecasting tools, providing market participants with new ways to more accurately capture sentiment-shifting trends and develop effective investment strategies.",
        "keywords": []
      },
      "file_name": "9af3183d3737729bff63303cf6bb6e433ffdbb47.pdf"
    },
    {
      "success": true,
      "doc_id": "8cc8ba943e7b0b3be7db03102f223d0a",
      "summary": "With increasing demands for precise water resource management, there is a growing need for advanced techniques in mapping water bodies. The currently deployed satellites provide complementary data that are either of high spatial or high temporal resolutions. As a result, there is a clear trade‐off between space and time when considering a single data source. For the efficient monitoring of multiple environmental resources, various Earth science applications need data at high spatial and temporal resolutions. To address this need, many data fusion methods have been described in the literature, that rely on combining data snapshots from multiple sources. Traditional methods face limitations due to sensitivity to atmospheric disturbances and other environmental factors, resulting in noise, outliers, and missing data. This paper introduces Hydrological Generative Adversarial Network (Hydro‐GAN), a novel machine learning‐based method that utilizes modified GANs to enhance boundary accuracy when mapping low‐resolution MODIS data to high‐resolution Landsat‐8 images. We propose a new non‐saturating loss function for the Hydro‐GAN generator, which maximizes the log of discriminator probabilities to promote stable updates and aid convergence. By focusing on reducing squared differences between real and synthetic images, our approach enhances training stability and overall performance. We specifically focus on mapping water bodies using MODIS and Landsat‐8 imagery due to their relevance in water resource management tasks. Our experimental results demonstrate the effectiveness of Hydro‐GAN in generating high‐resolution water body maps, outperforming traditional methods in terms of boundary accuracy and overall quality.",
      "intriguing_abstract": "With increasing demands for precise water resource management, there is a growing need for advanced techniques in mapping water bodies. The currently deployed satellites provide complementary data that are either of high spatial or high temporal resolutions. As a result, there is a clear trade‐off between space and time when considering a single data source. For the efficient monitoring of multiple environmental resources, various Earth science applications need data at high spatial and temporal resolutions. To address this need, many data fusion methods have been described in the literature, that rely on combining data snapshots from multiple sources. Traditional methods face limitations due to sensitivity to atmospheric disturbances and other environmental factors, resulting in noise, outliers, and missing data. This paper introduces Hydrological Generative Adversarial Network (Hydro‐GAN), a novel machine learning‐based method that utilizes modified GANs to enhance boundary accuracy when mapping low‐resolution MODIS data to high‐resolution Landsat‐8 images. We propose a new non‐saturating loss function for the Hydro‐GAN generator, which maximizes the log of discriminator probabilities to promote stable updates and aid convergence. By focusing on reducing squared differences between real and synthetic images, our approach enhances training stability and overall performance. We specifically focus on mapping water bodies using MODIS and Landsat‐8 imagery due to their relevance in water resource management tasks. Our experimental results demonstrate the effectiveness of Hydro‐GAN in generating high‐resolution water body maps, outperforming traditional methods in terms of boundary accuracy and overall quality.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/aad41ae781e26c1ccaca82c7e232f90a39da7f8c.pdf",
      "citation_key": "boubrahimi2024kts",
      "metadata": {
        "title": "Spatiotemporal Data Augmentation of MODIS‐Landsat Water Bodies Using Adversarial Networks",
        "authors": [
          "Soukaina Filali Boubrahimi",
          "Ashit Neema",
          "Ayman Nassar",
          "Pouya Hosseinzadeh",
          "S. M. Hamdi"
        ],
        "published_date": "2024",
        "abstract": "With increasing demands for precise water resource management, there is a growing need for advanced techniques in mapping water bodies. The currently deployed satellites provide complementary data that are either of high spatial or high temporal resolutions. As a result, there is a clear trade‐off between space and time when considering a single data source. For the efficient monitoring of multiple environmental resources, various Earth science applications need data at high spatial and temporal resolutions. To address this need, many data fusion methods have been described in the literature, that rely on combining data snapshots from multiple sources. Traditional methods face limitations due to sensitivity to atmospheric disturbances and other environmental factors, resulting in noise, outliers, and missing data. This paper introduces Hydrological Generative Adversarial Network (Hydro‐GAN), a novel machine learning‐based method that utilizes modified GANs to enhance boundary accuracy when mapping low‐resolution MODIS data to high‐resolution Landsat‐8 images. We propose a new non‐saturating loss function for the Hydro‐GAN generator, which maximizes the log of discriminator probabilities to promote stable updates and aid convergence. By focusing on reducing squared differences between real and synthetic images, our approach enhances training stability and overall performance. We specifically focus on mapping water bodies using MODIS and Landsat‐8 imagery due to their relevance in water resource management tasks. Our experimental results demonstrate the effectiveness of Hydro‐GAN in generating high‐resolution water body maps, outperforming traditional methods in terms of boundary accuracy and overall quality.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/aad41ae781e26c1ccaca82c7e232f90a39da7f8c.pdf",
        "venue": "Water Resources Research",
        "citationCount": 11,
        "score": 11.0,
        "summary": "With increasing demands for precise water resource management, there is a growing need for advanced techniques in mapping water bodies. The currently deployed satellites provide complementary data that are either of high spatial or high temporal resolutions. As a result, there is a clear trade‐off between space and time when considering a single data source. For the efficient monitoring of multiple environmental resources, various Earth science applications need data at high spatial and temporal resolutions. To address this need, many data fusion methods have been described in the literature, that rely on combining data snapshots from multiple sources. Traditional methods face limitations due to sensitivity to atmospheric disturbances and other environmental factors, resulting in noise, outliers, and missing data. This paper introduces Hydrological Generative Adversarial Network (Hydro‐GAN), a novel machine learning‐based method that utilizes modified GANs to enhance boundary accuracy when mapping low‐resolution MODIS data to high‐resolution Landsat‐8 images. We propose a new non‐saturating loss function for the Hydro‐GAN generator, which maximizes the log of discriminator probabilities to promote stable updates and aid convergence. By focusing on reducing squared differences between real and synthetic images, our approach enhances training stability and overall performance. We specifically focus on mapping water bodies using MODIS and Landsat‐8 imagery due to their relevance in water resource management tasks. Our experimental results demonstrate the effectiveness of Hydro‐GAN in generating high‐resolution water body maps, outperforming traditional methods in terms of boundary accuracy and overall quality.",
        "keywords": []
      },
      "file_name": "aad41ae781e26c1ccaca82c7e232f90a39da7f8c.pdf"
    },
    {
      "success": true,
      "doc_id": "3754d75c01e15e2fa6caec1d81710f7f",
      "summary": "Time-frequency (TF) transforms are commonly used to analyze local features of non-stationary seismic data and to help uncover structural or geological information. Traditional TF transforms, such as short-time Fourier transform (STFT), continuous wavelet transform (CWT), and S-transform (ST), suffer from the Heisenberg uncertainty principle, and their TF resolution is limited. Sparse TF (STF) transform has been proposed to address this disadvantage; however, expensive calculation and parameter selection present difficulties. We propose a self-supervised TF representation based on a generative adversarial networks (STFR-GAN) model in this study to map a one-dimensional (1D) seismic signal into a two-dimensional (2D) STF image. This model includes three components: a generator, discriminator, and reconstruction module. The generator is used to generate the STF spectrum of the input seismic trace, while the discriminator distinguishes if this generated STF spectrum is optimal. The reconstruction module serves as a physical constraint to ensure the accuracy of the generated STF spectrum. When implementing model training, the discriminator learns to identify the ideal STF and guides the generator to produce a TF spectrum closer to the ideal one. After model training, we applied the model to synthetic and field data to demonstrate its effectiveness and stability in characterizing the TF features of seismic data. Our results show that STFR-GAN can effectively provide TF representations with higher readability than those of traditional TF methods. Furthermore, effective TF representation can be applied to improve fluvial channel delineation.",
      "intriguing_abstract": "Time-frequency (TF) transforms are commonly used to analyze local features of non-stationary seismic data and to help uncover structural or geological information. Traditional TF transforms, such as short-time Fourier transform (STFT), continuous wavelet transform (CWT), and S-transform (ST), suffer from the Heisenberg uncertainty principle, and their TF resolution is limited. Sparse TF (STF) transform has been proposed to address this disadvantage; however, expensive calculation and parameter selection present difficulties. We propose a self-supervised TF representation based on a generative adversarial networks (STFR-GAN) model in this study to map a one-dimensional (1D) seismic signal into a two-dimensional (2D) STF image. This model includes three components: a generator, discriminator, and reconstruction module. The generator is used to generate the STF spectrum of the input seismic trace, while the discriminator distinguishes if this generated STF spectrum is optimal. The reconstruction module serves as a physical constraint to ensure the accuracy of the generated STF spectrum. When implementing model training, the discriminator learns to identify the ideal STF and guides the generator to produce a TF spectrum closer to the ideal one. After model training, we applied the model to synthetic and field data to demonstrate its effectiveness and stability in characterizing the TF features of seismic data. Our results show that STFR-GAN can effectively provide TF representations with higher readability than those of traditional TF methods. Furthermore, effective TF representation can be applied to improve fluvial channel delineation.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/fcb58f1ddadeb767a9b4c0017bdeb602f7dea449.pdf",
      "citation_key": "liu20232tr",
      "metadata": {
        "title": "Self-supervised Time-Frequency Representation based on Generative Adversarial Networks",
        "authors": [
          "Naihao Liu",
          "Youbo Lei",
          "Yang Yang",
          "S. Wei",
          "Jinghuai Gao",
          "Xiudi Jiang"
        ],
        "published_date": "2023",
        "abstract": "Time-frequency (TF) transforms are commonly used to analyze local features of non-stationary seismic data and to help uncover structural or geological information. Traditional TF transforms, such as short-time Fourier transform (STFT), continuous wavelet transform (CWT), and S-transform (ST), suffer from the Heisenberg uncertainty principle, and their TF resolution is limited. Sparse TF (STF) transform has been proposed to address this disadvantage; however, expensive calculation and parameter selection present difficulties. We propose a self-supervised TF representation based on a generative adversarial networks (STFR-GAN) model in this study to map a one-dimensional (1D) seismic signal into a two-dimensional (2D) STF image. This model includes three components: a generator, discriminator, and reconstruction module. The generator is used to generate the STF spectrum of the input seismic trace, while the discriminator distinguishes if this generated STF spectrum is optimal. The reconstruction module serves as a physical constraint to ensure the accuracy of the generated STF spectrum. When implementing model training, the discriminator learns to identify the ideal STF and guides the generator to produce a TF spectrum closer to the ideal one. After model training, we applied the model to synthetic and field data to demonstrate its effectiveness and stability in characterizing the TF features of seismic data. Our results show that STFR-GAN can effectively provide TF representations with higher readability than those of traditional TF methods. Furthermore, effective TF representation can be applied to improve fluvial channel delineation.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/fcb58f1ddadeb767a9b4c0017bdeb602f7dea449.pdf",
        "venue": "Geophysics",
        "citationCount": 20,
        "score": 10.0,
        "summary": "Time-frequency (TF) transforms are commonly used to analyze local features of non-stationary seismic data and to help uncover structural or geological information. Traditional TF transforms, such as short-time Fourier transform (STFT), continuous wavelet transform (CWT), and S-transform (ST), suffer from the Heisenberg uncertainty principle, and their TF resolution is limited. Sparse TF (STF) transform has been proposed to address this disadvantage; however, expensive calculation and parameter selection present difficulties. We propose a self-supervised TF representation based on a generative adversarial networks (STFR-GAN) model in this study to map a one-dimensional (1D) seismic signal into a two-dimensional (2D) STF image. This model includes three components: a generator, discriminator, and reconstruction module. The generator is used to generate the STF spectrum of the input seismic trace, while the discriminator distinguishes if this generated STF spectrum is optimal. The reconstruction module serves as a physical constraint to ensure the accuracy of the generated STF spectrum. When implementing model training, the discriminator learns to identify the ideal STF and guides the generator to produce a TF spectrum closer to the ideal one. After model training, we applied the model to synthetic and field data to demonstrate its effectiveness and stability in characterizing the TF features of seismic data. Our results show that STFR-GAN can effectively provide TF representations with higher readability than those of traditional TF methods. Furthermore, effective TF representation can be applied to improve fluvial channel delineation.",
        "keywords": []
      },
      "file_name": "fcb58f1ddadeb767a9b4c0017bdeb602f7dea449.pdf"
    },
    {
      "success": true,
      "doc_id": "d751e1a95baf2ef174c14e29341085cf",
      "summary": "This study addresses the problem of maize disease detection in agricultural production, proposing a high-accuracy detection method based on Attention Generative Adversarial Network (Attention-GAN) and few-shot learning. The method introduces an attention mechanism, enabling the model to focus more on the significant parts of the image, thereby enhancing model performance. Concurrently, data augmentation is performed through Generative Adversarial Network (GAN) to generate more training samples, overcoming the difficulties of few-shot learning. Experimental results demonstrate that this method surpasses other baseline models in accuracy, recall, and mean average precision (mAP), achieving 0.97, 0.92, and 0.95, respectively. These results validate the high accuracy and stability of the method in handling maize disease detection tasks. This research provides a new approach to solving the problem of few samples in practical applications and offers valuable references for subsequent research, contributing to the advancement of agricultural informatization and intelligence.",
      "intriguing_abstract": "This study addresses the problem of maize disease detection in agricultural production, proposing a high-accuracy detection method based on Attention Generative Adversarial Network (Attention-GAN) and few-shot learning. The method introduces an attention mechanism, enabling the model to focus more on the significant parts of the image, thereby enhancing model performance. Concurrently, data augmentation is performed through Generative Adversarial Network (GAN) to generate more training samples, overcoming the difficulties of few-shot learning. Experimental results demonstrate that this method surpasses other baseline models in accuracy, recall, and mean average precision (mAP), achieving 0.97, 0.92, and 0.95, respectively. These results validate the high accuracy and stability of the method in handling maize disease detection tasks. This research provides a new approach to solving the problem of few samples in practical applications and offers valuable references for subsequent research, contributing to the advancement of agricultural informatization and intelligence.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/f532d3223ac31b16d1923c2fcfae0c2a2a033f54.pdf",
      "citation_key": "song20239hi",
      "metadata": {
        "title": "High-Accuracy Maize Disease Detection Based on Attention Generative Adversarial Network and Few-Shot Learning",
        "authors": [
          "Yihong Song",
          "Haoyan Zhang",
          "Jiaqi Li",
          "Ran Ye",
          "Xincan Zhou",
          "Bowen Dong",
          "Dongchen Fan",
          "Lin Li"
        ],
        "published_date": "2023",
        "abstract": "This study addresses the problem of maize disease detection in agricultural production, proposing a high-accuracy detection method based on Attention Generative Adversarial Network (Attention-GAN) and few-shot learning. The method introduces an attention mechanism, enabling the model to focus more on the significant parts of the image, thereby enhancing model performance. Concurrently, data augmentation is performed through Generative Adversarial Network (GAN) to generate more training samples, overcoming the difficulties of few-shot learning. Experimental results demonstrate that this method surpasses other baseline models in accuracy, recall, and mean average precision (mAP), achieving 0.97, 0.92, and 0.95, respectively. These results validate the high accuracy and stability of the method in handling maize disease detection tasks. This research provides a new approach to solving the problem of few samples in practical applications and offers valuable references for subsequent research, contributing to the advancement of agricultural informatization and intelligence.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/f532d3223ac31b16d1923c2fcfae0c2a2a033f54.pdf",
        "venue": "Plants",
        "citationCount": 17,
        "score": 8.5,
        "summary": "This study addresses the problem of maize disease detection in agricultural production, proposing a high-accuracy detection method based on Attention Generative Adversarial Network (Attention-GAN) and few-shot learning. The method introduces an attention mechanism, enabling the model to focus more on the significant parts of the image, thereby enhancing model performance. Concurrently, data augmentation is performed through Generative Adversarial Network (GAN) to generate more training samples, overcoming the difficulties of few-shot learning. Experimental results demonstrate that this method surpasses other baseline models in accuracy, recall, and mean average precision (mAP), achieving 0.97, 0.92, and 0.95, respectively. These results validate the high accuracy and stability of the method in handling maize disease detection tasks. This research provides a new approach to solving the problem of few samples in practical applications and offers valuable references for subsequent research, contributing to the advancement of agricultural informatization and intelligence.",
        "keywords": []
      },
      "file_name": "f532d3223ac31b16d1923c2fcfae0c2a2a033f54.pdf"
    },
    {
      "success": true,
      "doc_id": "79023dcdc72511e108044a699f0bc5a4",
      "summary": "In few-shot open-set recognition (FSOSR) for hyperspectral images (HSI), one major challenge arises due to the simultaneous presence of spectrally fine-grained known classes and outliers. Prior research on generative FSOSR cannot handle such a situation due to their inability to approximate the open space prudently. To address this issue, we propose a method, Meta-learning-based Open-set Recognition via Generative Adversarial Network (MORGAN), that can learn a finer separation between the closed and the open spaces. MORGAN seeks to generate class-conditioned adversarial samples for both the closed and open spaces in the few-shot regime using two GANs by judiciously tuning noise variance while ensuring discriminability using a novel Anti-Overlap Latent (AOL) regularizer. Adversarial samples from low noise variance amplify known class data density, and we use samples from high noise variance to augment \"known-unknowns\". A first-order episodic strategy is adapted to ensure stability in the GAN training. Finally, we introduce a combination of metric losses which push these augmented \"known-unknowns\" or outliers to disperse in the open space while condensing known class distributions. Extensive experiments on four benchmark HSI datasets indicate that MORGAN achieves state-of-the-art FSOSR performance consistently.1",
      "intriguing_abstract": "In few-shot open-set recognition (FSOSR) for hyperspectral images (HSI), one major challenge arises due to the simultaneous presence of spectrally fine-grained known classes and outliers. Prior research on generative FSOSR cannot handle such a situation due to their inability to approximate the open space prudently. To address this issue, we propose a method, Meta-learning-based Open-set Recognition via Generative Adversarial Network (MORGAN), that can learn a finer separation between the closed and the open spaces. MORGAN seeks to generate class-conditioned adversarial samples for both the closed and open spaces in the few-shot regime using two GANs by judiciously tuning noise variance while ensuring discriminability using a novel Anti-Overlap Latent (AOL) regularizer. Adversarial samples from low noise variance amplify known class data density, and we use samples from high noise variance to augment \"known-unknowns\". A first-order episodic strategy is adapted to ensure stability in the GAN training. Finally, we introduce a combination of metric losses which push these augmented \"known-unknowns\" or outliers to disperse in the open space while condensing known class distributions. Extensive experiments on four benchmark HSI datasets indicate that MORGAN achieves state-of-the-art FSOSR performance consistently.1",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/d77ff1f2cb9af2e801ad10da27226fa0b9699d81.pdf",
      "citation_key": "pal2023147",
      "metadata": {
        "title": "MORGAN: Meta-Learning-based Few-Shot Open-Set Recognition via Generative Adversarial Network",
        "authors": [
          "Debabrata Pal",
          "Shirsha Bose",
          "Biplab Banerjee",
          "Y. Jeppu"
        ],
        "published_date": "2023",
        "abstract": "In few-shot open-set recognition (FSOSR) for hyperspectral images (HSI), one major challenge arises due to the simultaneous presence of spectrally fine-grained known classes and outliers. Prior research on generative FSOSR cannot handle such a situation due to their inability to approximate the open space prudently. To address this issue, we propose a method, Meta-learning-based Open-set Recognition via Generative Adversarial Network (MORGAN), that can learn a finer separation between the closed and the open spaces. MORGAN seeks to generate class-conditioned adversarial samples for both the closed and open spaces in the few-shot regime using two GANs by judiciously tuning noise variance while ensuring discriminability using a novel Anti-Overlap Latent (AOL) regularizer. Adversarial samples from low noise variance amplify known class data density, and we use samples from high noise variance to augment \"known-unknowns\". A first-order episodic strategy is adapted to ensure stability in the GAN training. Finally, we introduce a combination of metric losses which push these augmented \"known-unknowns\" or outliers to disperse in the open space while condensing known class distributions. Extensive experiments on four benchmark HSI datasets indicate that MORGAN achieves state-of-the-art FSOSR performance consistently.1",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/d77ff1f2cb9af2e801ad10da27226fa0b9699d81.pdf",
        "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
        "citationCount": 17,
        "score": 8.5,
        "summary": "In few-shot open-set recognition (FSOSR) for hyperspectral images (HSI), one major challenge arises due to the simultaneous presence of spectrally fine-grained known classes and outliers. Prior research on generative FSOSR cannot handle such a situation due to their inability to approximate the open space prudently. To address this issue, we propose a method, Meta-learning-based Open-set Recognition via Generative Adversarial Network (MORGAN), that can learn a finer separation between the closed and the open spaces. MORGAN seeks to generate class-conditioned adversarial samples for both the closed and open spaces in the few-shot regime using two GANs by judiciously tuning noise variance while ensuring discriminability using a novel Anti-Overlap Latent (AOL) regularizer. Adversarial samples from low noise variance amplify known class data density, and we use samples from high noise variance to augment \"known-unknowns\". A first-order episodic strategy is adapted to ensure stability in the GAN training. Finally, we introduce a combination of metric losses which push these augmented \"known-unknowns\" or outliers to disperse in the open space while condensing known class distributions. Extensive experiments on four benchmark HSI datasets indicate that MORGAN achieves state-of-the-art FSOSR performance consistently.1",
        "keywords": []
      },
      "file_name": "d77ff1f2cb9af2e801ad10da27226fa0b9699d81.pdf"
    },
    {
      "success": true,
      "doc_id": "04b79ba60b78ffb83a01f3202251f9ca",
      "summary": "Training generative adversarial networks (GANs) for noise-to-image synthesis is a challenge task, primarily due to the instability of GANs’ training process. One of the key issues is the generator’s sensitivity to input data, which can cause sudden fluctuations in the generator’s loss value with certain inputs. This sensitivity suggests an inadequate ability to resist disturbances in the generator, causing the discriminator’s loss value to oscillate and negatively impacting the discriminator. Then, the negative feedback of discriminator is also not conducive to updating generator’s parameters, leading to suboptimal image generation quality. In response to this challenge, we present an innovative GANs model equipped with a learnable auxiliary module that processes auxiliary noise. The core objective of this module is to enhance the stability of both the generator and discriminator throughout the training process. To achieve this target, we incorporate a learnable auxiliary penalty and an augmented discriminator, designed to control the generator and reinforce the discriminator’s stability, respectively. We further apply our method to the Hinge and LSGANs loss functions, illustrating its efficacy in reducing the instability of both the generator and the discriminator. The tests we conducted on LSUN, CelebA, Market-1501, and Creative Senz3D datasets serve as proof of our method’s ability to improve the training stability and overall performance of the baseline methods.",
      "intriguing_abstract": "Training generative adversarial networks (GANs) for noise-to-image synthesis is a challenge task, primarily due to the instability of GANs’ training process. One of the key issues is the generator’s sensitivity to input data, which can cause sudden fluctuations in the generator’s loss value with certain inputs. This sensitivity suggests an inadequate ability to resist disturbances in the generator, causing the discriminator’s loss value to oscillate and negatively impacting the discriminator. Then, the negative feedback of discriminator is also not conducive to updating generator’s parameters, leading to suboptimal image generation quality. In response to this challenge, we present an innovative GANs model equipped with a learnable auxiliary module that processes auxiliary noise. The core objective of this module is to enhance the stability of both the generator and discriminator throughout the training process. To achieve this target, we incorporate a learnable auxiliary penalty and an augmented discriminator, designed to control the generator and reinforce the discriminator’s stability, respectively. We further apply our method to the Hinge and LSGANs loss functions, illustrating its efficacy in reducing the instability of both the generator and the discriminator. The tests we conducted on LSUN, CelebA, Market-1501, and Creative Senz3D datasets serve as proof of our method’s ability to improve the training stability and overall performance of the baseline methods.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/9d305eb1ff2b48c1e8fd53747983bcdae6ab1753.pdf",
      "citation_key": "gan202494y",
      "metadata": {
        "title": "Generative Adversarial Networks with Learnable Auxiliary Module for Image Synthesis",
        "authors": [
          "Yan Gan",
          "Chenxue Yang",
          "Mao Ye",
          "Renjie Huang",
          "Deqiang Ouyang"
        ],
        "published_date": "2024",
        "abstract": "Training generative adversarial networks (GANs) for noise-to-image synthesis is a challenge task, primarily due to the instability of GANs’ training process. One of the key issues is the generator’s sensitivity to input data, which can cause sudden fluctuations in the generator’s loss value with certain inputs. This sensitivity suggests an inadequate ability to resist disturbances in the generator, causing the discriminator’s loss value to oscillate and negatively impacting the discriminator. Then, the negative feedback of discriminator is also not conducive to updating generator’s parameters, leading to suboptimal image generation quality. In response to this challenge, we present an innovative GANs model equipped with a learnable auxiliary module that processes auxiliary noise. The core objective of this module is to enhance the stability of both the generator and discriminator throughout the training process. To achieve this target, we incorporate a learnable auxiliary penalty and an augmented discriminator, designed to control the generator and reinforce the discriminator’s stability, respectively. We further apply our method to the Hinge and LSGANs loss functions, illustrating its efficacy in reducing the instability of both the generator and the discriminator. The tests we conducted on LSUN, CelebA, Market-1501, and Creative Senz3D datasets serve as proof of our method’s ability to improve the training stability and overall performance of the baseline methods.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/9d305eb1ff2b48c1e8fd53747983bcdae6ab1753.pdf",
        "venue": "ACM Trans. Multim. Comput. Commun. Appl.",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Training generative adversarial networks (GANs) for noise-to-image synthesis is a challenge task, primarily due to the instability of GANs’ training process. One of the key issues is the generator’s sensitivity to input data, which can cause sudden fluctuations in the generator’s loss value with certain inputs. This sensitivity suggests an inadequate ability to resist disturbances in the generator, causing the discriminator’s loss value to oscillate and negatively impacting the discriminator. Then, the negative feedback of discriminator is also not conducive to updating generator’s parameters, leading to suboptimal image generation quality. In response to this challenge, we present an innovative GANs model equipped with a learnable auxiliary module that processes auxiliary noise. The core objective of this module is to enhance the stability of both the generator and discriminator throughout the training process. To achieve this target, we incorporate a learnable auxiliary penalty and an augmented discriminator, designed to control the generator and reinforce the discriminator’s stability, respectively. We further apply our method to the Hinge and LSGANs loss functions, illustrating its efficacy in reducing the instability of both the generator and the discriminator. The tests we conducted on LSUN, CelebA, Market-1501, and Creative Senz3D datasets serve as proof of our method’s ability to improve the training stability and overall performance of the baseline methods.",
        "keywords": []
      },
      "file_name": "9d305eb1ff2b48c1e8fd53747983bcdae6ab1753.pdf"
    },
    {
      "success": true,
      "doc_id": "806ee013c41283d9072715b772f5f616",
      "summary": "Rapid damage identification and classification in disastrous situations and natural disasters are crucial for efficiently directing aid and resources. With the development of deep learning techniques and the availability of imagery content on social media platforms, extensive research has focused on damage assessment. Through the use of geospatial data related to such incidents, the visual characteristics of these images can quickly determine the safety situation in the region. However, training accurate disaster classification models has proven to be challenging due to the lack of labeled imagery data in this domain. This paper proposes a disaster classification framework, which combines a set of synthesized diverse disaster images generated using generative adversarial networks (GANs) and domain-specific fine-tuning of a deep convolutional neural network (CNN)-based model. The proposed model utilizes bootstrap aggregating (bagging) to further stabilize the target predictions. Since past work in this domain mainly suffers from limited data resources, a sample dataset that highlights the issue of imbalanced classification of multiple natural disasters was constructed and augmented. Qualitative and quantitative experiments show the validity of the data augmentation method employed in producing a balanced dataset. Further experiments with various evaluation metrics verified the proposed framework’s accuracy and generalization ability across different classes for the task of disaster classification in comparison to other state-of-the-art techniques. Furthermore, the framework outperforms the other models by an average validation accuracy of 11%. These results provide a deep learning solution for real-time disaster monitoring systems to mitigate the loss of lives and properties.",
      "intriguing_abstract": "Rapid damage identification and classification in disastrous situations and natural disasters are crucial for efficiently directing aid and resources. With the development of deep learning techniques and the availability of imagery content on social media platforms, extensive research has focused on damage assessment. Through the use of geospatial data related to such incidents, the visual characteristics of these images can quickly determine the safety situation in the region. However, training accurate disaster classification models has proven to be challenging due to the lack of labeled imagery data in this domain. This paper proposes a disaster classification framework, which combines a set of synthesized diverse disaster images generated using generative adversarial networks (GANs) and domain-specific fine-tuning of a deep convolutional neural network (CNN)-based model. The proposed model utilizes bootstrap aggregating (bagging) to further stabilize the target predictions. Since past work in this domain mainly suffers from limited data resources, a sample dataset that highlights the issue of imbalanced classification of multiple natural disasters was constructed and augmented. Qualitative and quantitative experiments show the validity of the data augmentation method employed in producing a balanced dataset. Further experiments with various evaluation metrics verified the proposed framework’s accuracy and generalization ability across different classes for the task of disaster classification in comparison to other state-of-the-art techniques. Furthermore, the framework outperforms the other models by an average validation accuracy of 11%. These results provide a deep learning solution for real-time disaster monitoring systems to mitigate the loss of lives and properties.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/caba0a39d4a17e83508b082158560e13cca6f01f.pdf",
      "citation_key": "eltehewy2023cj4",
      "metadata": {
        "title": "Efficient Classification of Imbalanced Natural Disasters Data Using Generative Adversarial Networks for Data Augmentation",
        "authors": [
          "Rokaya Eltehewy",
          "A. Abouelfarag",
          "Sherine Nagy Saleh"
        ],
        "published_date": "2023",
        "abstract": "Rapid damage identification and classification in disastrous situations and natural disasters are crucial for efficiently directing aid and resources. With the development of deep learning techniques and the availability of imagery content on social media platforms, extensive research has focused on damage assessment. Through the use of geospatial data related to such incidents, the visual characteristics of these images can quickly determine the safety situation in the region. However, training accurate disaster classification models has proven to be challenging due to the lack of labeled imagery data in this domain. This paper proposes a disaster classification framework, which combines a set of synthesized diverse disaster images generated using generative adversarial networks (GANs) and domain-specific fine-tuning of a deep convolutional neural network (CNN)-based model. The proposed model utilizes bootstrap aggregating (bagging) to further stabilize the target predictions. Since past work in this domain mainly suffers from limited data resources, a sample dataset that highlights the issue of imbalanced classification of multiple natural disasters was constructed and augmented. Qualitative and quantitative experiments show the validity of the data augmentation method employed in producing a balanced dataset. Further experiments with various evaluation metrics verified the proposed framework’s accuracy and generalization ability across different classes for the task of disaster classification in comparison to other state-of-the-art techniques. Furthermore, the framework outperforms the other models by an average validation accuracy of 11%. These results provide a deep learning solution for real-time disaster monitoring systems to mitigate the loss of lives and properties.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/caba0a39d4a17e83508b082158560e13cca6f01f.pdf",
        "venue": "ISPRS Int. J. Geo Inf.",
        "citationCount": 15,
        "score": 7.5,
        "summary": "Rapid damage identification and classification in disastrous situations and natural disasters are crucial for efficiently directing aid and resources. With the development of deep learning techniques and the availability of imagery content on social media platforms, extensive research has focused on damage assessment. Through the use of geospatial data related to such incidents, the visual characteristics of these images can quickly determine the safety situation in the region. However, training accurate disaster classification models has proven to be challenging due to the lack of labeled imagery data in this domain. This paper proposes a disaster classification framework, which combines a set of synthesized diverse disaster images generated using generative adversarial networks (GANs) and domain-specific fine-tuning of a deep convolutional neural network (CNN)-based model. The proposed model utilizes bootstrap aggregating (bagging) to further stabilize the target predictions. Since past work in this domain mainly suffers from limited data resources, a sample dataset that highlights the issue of imbalanced classification of multiple natural disasters was constructed and augmented. Qualitative and quantitative experiments show the validity of the data augmentation method employed in producing a balanced dataset. Further experiments with various evaluation metrics verified the proposed framework’s accuracy and generalization ability across different classes for the task of disaster classification in comparison to other state-of-the-art techniques. Furthermore, the framework outperforms the other models by an average validation accuracy of 11%. These results provide a deep learning solution for real-time disaster monitoring systems to mitigate the loss of lives and properties.",
        "keywords": []
      },
      "file_name": "caba0a39d4a17e83508b082158560e13cca6f01f.pdf"
    },
    {
      "success": true,
      "doc_id": "f7c9ed8a52df64e1253c1aed556313ae",
      "summary": "Zero-shot learning (ZSL) aims to recognize the novel classes which cannot be collected for training a prediction model. Accordingly, generative models [e.g., generative adversarial network (GAN)] are typically used to synthesize the visual samples conditioned by the class semantic vectors and achieve remarkable progress for ZSL. However, existing GAN-based generative ZSL methods are based on hand-crafted models, which cannot adapt to various datasets/scenarios and fails to model instability. To alleviate these challenges, we propose evolutionary GAN search (termed EGANS) to automatically design the generative network with good adaptation and stability, enabling reliable visual feature sample synthesis for advancing ZSL. Specifically, we adopt cooperative dual evolution to conduct a neural architecture search (NAS) for both generator and discriminator under a unified evolutionary adversarial framework. EGANS is learned by two stages: 1) evolution generator architecture search and 2) evolution discriminator architecture search. During the evolution generator architecture search, we adopt a many-to-one adversarial training strategy to evolutionarily search for the optimal generator. Then the optimal generator is further applied to search for the optimal discriminator in the evolution discriminator architecture search with a similar evolution search algorithm. Once the optimal generator and discriminator are searched, we entail them into various generative ZSL baselines for ZSL classification. Extensive experiments show that EGANS consistently improve existing generative ZSL methods on the standard CUB, SUN, AWA2 and FLO datasets. The significant performance gains indicate that the evolutionary NAS explores a virgin field in ZSL.",
      "intriguing_abstract": "Zero-shot learning (ZSL) aims to recognize the novel classes which cannot be collected for training a prediction model. Accordingly, generative models [e.g., generative adversarial network (GAN)] are typically used to synthesize the visual samples conditioned by the class semantic vectors and achieve remarkable progress for ZSL. However, existing GAN-based generative ZSL methods are based on hand-crafted models, which cannot adapt to various datasets/scenarios and fails to model instability. To alleviate these challenges, we propose evolutionary GAN search (termed EGANS) to automatically design the generative network with good adaptation and stability, enabling reliable visual feature sample synthesis for advancing ZSL. Specifically, we adopt cooperative dual evolution to conduct a neural architecture search (NAS) for both generator and discriminator under a unified evolutionary adversarial framework. EGANS is learned by two stages: 1) evolution generator architecture search and 2) evolution discriminator architecture search. During the evolution generator architecture search, we adopt a many-to-one adversarial training strategy to evolutionarily search for the optimal generator. Then the optimal generator is further applied to search for the optimal discriminator in the evolution discriminator architecture search with a similar evolution search algorithm. Once the optimal generator and discriminator are searched, we entail them into various generative ZSL baselines for ZSL classification. Extensive experiments show that EGANS consistently improve existing generative ZSL methods on the standard CUB, SUN, AWA2 and FLO datasets. The significant performance gains indicate that the evolutionary NAS explores a virgin field in ZSL.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/9602146b95175c69ff187a9ee3bfe45fbc01fa1e.pdf",
      "citation_key": "chen2023rrf",
      "metadata": {
        "title": "EGANS: Evolutionary Generative Adversarial Network Search for Zero-Shot Learning",
        "authors": [
          "Shiming Chen",
          "Shuhuang Chen",
          "W. Hou",
          "Weiping Ding",
          "Xinge You"
        ],
        "published_date": "2023",
        "abstract": "Zero-shot learning (ZSL) aims to recognize the novel classes which cannot be collected for training a prediction model. Accordingly, generative models [e.g., generative adversarial network (GAN)] are typically used to synthesize the visual samples conditioned by the class semantic vectors and achieve remarkable progress for ZSL. However, existing GAN-based generative ZSL methods are based on hand-crafted models, which cannot adapt to various datasets/scenarios and fails to model instability. To alleviate these challenges, we propose evolutionary GAN search (termed EGANS) to automatically design the generative network with good adaptation and stability, enabling reliable visual feature sample synthesis for advancing ZSL. Specifically, we adopt cooperative dual evolution to conduct a neural architecture search (NAS) for both generator and discriminator under a unified evolutionary adversarial framework. EGANS is learned by two stages: 1) evolution generator architecture search and 2) evolution discriminator architecture search. During the evolution generator architecture search, we adopt a many-to-one adversarial training strategy to evolutionarily search for the optimal generator. Then the optimal generator is further applied to search for the optimal discriminator in the evolution discriminator architecture search with a similar evolution search algorithm. Once the optimal generator and discriminator are searched, we entail them into various generative ZSL baselines for ZSL classification. Extensive experiments show that EGANS consistently improve existing generative ZSL methods on the standard CUB, SUN, AWA2 and FLO datasets. The significant performance gains indicate that the evolutionary NAS explores a virgin field in ZSL.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/9602146b95175c69ff187a9ee3bfe45fbc01fa1e.pdf",
        "venue": "IEEE Transactions on Evolutionary Computation",
        "citationCount": 15,
        "score": 7.5,
        "summary": "Zero-shot learning (ZSL) aims to recognize the novel classes which cannot be collected for training a prediction model. Accordingly, generative models [e.g., generative adversarial network (GAN)] are typically used to synthesize the visual samples conditioned by the class semantic vectors and achieve remarkable progress for ZSL. However, existing GAN-based generative ZSL methods are based on hand-crafted models, which cannot adapt to various datasets/scenarios and fails to model instability. To alleviate these challenges, we propose evolutionary GAN search (termed EGANS) to automatically design the generative network with good adaptation and stability, enabling reliable visual feature sample synthesis for advancing ZSL. Specifically, we adopt cooperative dual evolution to conduct a neural architecture search (NAS) for both generator and discriminator under a unified evolutionary adversarial framework. EGANS is learned by two stages: 1) evolution generator architecture search and 2) evolution discriminator architecture search. During the evolution generator architecture search, we adopt a many-to-one adversarial training strategy to evolutionarily search for the optimal generator. Then the optimal generator is further applied to search for the optimal discriminator in the evolution discriminator architecture search with a similar evolution search algorithm. Once the optimal generator and discriminator are searched, we entail them into various generative ZSL baselines for ZSL classification. Extensive experiments show that EGANS consistently improve existing generative ZSL methods on the standard CUB, SUN, AWA2 and FLO datasets. The significant performance gains indicate that the evolutionary NAS explores a virgin field in ZSL.",
        "keywords": []
      },
      "file_name": "9602146b95175c69ff187a9ee3bfe45fbc01fa1e.pdf"
    },
    {
      "success": true,
      "doc_id": "6ad6a87bcc148cda614d4d081091d9e0",
      "summary": "Inunderwater imaging, achieving high-quality imagery is essential but challenging due to factors such as wavelength-dependent absorption and complex lighting dynamics. This paper introduces MEvo-GAN, a novel methodology designed to address these challenges by combining generative adversarial networks with genetic algorithms. The key innovation lies in the integration of genetic algorithm principles with multi-scale generator and discriminator structures in Generative Adversarial Networks (GANs). This approach enhances image details and structural integrity while significantly improving training stability. This combination enables more effective exploration and optimization of the solution space, leading to reduced oscillation, mitigated mode collapse, and smoother convergence to high-quality generative outcomes. By analyzing various public datasets in a quantitative and qualitative manner, the results confirm the effectiveness of MEvo-GAN in improving the clarity, color fidelity, and detail accuracy of underwater images. The results of the experiments on the UIEB dataset are remarkable, with MEvo-GAN attaining a Peak Signal-to-Noise Ratio (PSNR) of 21.2758, Structural Similarity Index (SSIM) of 0.8662, and Underwater Color Image Quality Evaluation (UCIQE) of 0.6597.",
      "intriguing_abstract": "Inunderwater imaging, achieving high-quality imagery is essential but challenging due to factors such as wavelength-dependent absorption and complex lighting dynamics. This paper introduces MEvo-GAN, a novel methodology designed to address these challenges by combining generative adversarial networks with genetic algorithms. The key innovation lies in the integration of genetic algorithm principles with multi-scale generator and discriminator structures in Generative Adversarial Networks (GANs). This approach enhances image details and structural integrity while significantly improving training stability. This combination enables more effective exploration and optimization of the solution space, leading to reduced oscillation, mitigated mode collapse, and smoother convergence to high-quality generative outcomes. By analyzing various public datasets in a quantitative and qualitative manner, the results confirm the effectiveness of MEvo-GAN in improving the clarity, color fidelity, and detail accuracy of underwater images. The results of the experiments on the UIEB dataset are remarkable, with MEvo-GAN attaining a Peak Signal-to-Noise Ratio (PSNR) of 21.2758, Structural Similarity Index (SSIM) of 0.8662, and Underwater Color Image Quality Evaluation (UCIQE) of 0.6597.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/88cffe6fdf149250c09ae90498431379dd813d3a.pdf",
      "citation_key": "fu20241mw",
      "metadata": {
        "title": "MEvo-GAN: A Multi-Scale Evolutionary Generative Adversarial Network for Underwater Image Enhancement",
        "authors": [
          "Feiran Fu",
          "Peng Liu",
          "Zhen Shao",
          "Jing Xu",
          "Ming Fang"
        ],
        "published_date": "2024",
        "abstract": "Inunderwater imaging, achieving high-quality imagery is essential but challenging due to factors such as wavelength-dependent absorption and complex lighting dynamics. This paper introduces MEvo-GAN, a novel methodology designed to address these challenges by combining generative adversarial networks with genetic algorithms. The key innovation lies in the integration of genetic algorithm principles with multi-scale generator and discriminator structures in Generative Adversarial Networks (GANs). This approach enhances image details and structural integrity while significantly improving training stability. This combination enables more effective exploration and optimization of the solution space, leading to reduced oscillation, mitigated mode collapse, and smoother convergence to high-quality generative outcomes. By analyzing various public datasets in a quantitative and qualitative manner, the results confirm the effectiveness of MEvo-GAN in improving the clarity, color fidelity, and detail accuracy of underwater images. The results of the experiments on the UIEB dataset are remarkable, with MEvo-GAN attaining a Peak Signal-to-Noise Ratio (PSNR) of 21.2758, Structural Similarity Index (SSIM) of 0.8662, and Underwater Color Image Quality Evaluation (UCIQE) of 0.6597.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/88cffe6fdf149250c09ae90498431379dd813d3a.pdf",
        "venue": "Journal of Marine Science and Engineering",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Inunderwater imaging, achieving high-quality imagery is essential but challenging due to factors such as wavelength-dependent absorption and complex lighting dynamics. This paper introduces MEvo-GAN, a novel methodology designed to address these challenges by combining generative adversarial networks with genetic algorithms. The key innovation lies in the integration of genetic algorithm principles with multi-scale generator and discriminator structures in Generative Adversarial Networks (GANs). This approach enhances image details and structural integrity while significantly improving training stability. This combination enables more effective exploration and optimization of the solution space, leading to reduced oscillation, mitigated mode collapse, and smoother convergence to high-quality generative outcomes. By analyzing various public datasets in a quantitative and qualitative manner, the results confirm the effectiveness of MEvo-GAN in improving the clarity, color fidelity, and detail accuracy of underwater images. The results of the experiments on the UIEB dataset are remarkable, with MEvo-GAN attaining a Peak Signal-to-Noise Ratio (PSNR) of 21.2758, Structural Similarity Index (SSIM) of 0.8662, and Underwater Color Image Quality Evaluation (UCIQE) of 0.6597.",
        "keywords": []
      },
      "file_name": "88cffe6fdf149250c09ae90498431379dd813d3a.pdf"
    },
    {
      "success": true,
      "doc_id": "26b34486022373be3ec1c18a48815fa0",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/3ae101a0a196fc73419fbb5207a0394dbbedf191.pdf",
      "citation_key": "soleymanzadeh202358z",
      "metadata": {
        "title": "Efficient intrusion detection using multi-player generative adversarial networks (GANs): an ensemble-based deep learning architecture",
        "authors": [
          "Raha Soleymanzadeh",
          "R. Kashef"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/3ae101a0a196fc73419fbb5207a0394dbbedf191.pdf",
        "venue": "Neural computing & applications (Print)",
        "citationCount": 13,
        "score": 6.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "3ae101a0a196fc73419fbb5207a0394dbbedf191.pdf"
    },
    {
      "success": true,
      "doc_id": "daaadee5e50d0b89351e276a6f871b18",
      "summary": "Generative adversarial networks (GANs) are a powerful tool for synthesizing realistic images, but they can be difficult to train and are prone to instability and mode collapse. This paper proposes a new model called Identity Generative Adversarial Network (IGAN) that addresses these issues. This model is based on three modifications to the baseline deep convolutional generative adversarial network (DCGAN). The first change is to add a non-linear identity block to the architecture. This will make it easier for the model to fit complex data types and cut down on the time it takes to train. The second change is to smooth out the standard GAN loss function by using a modified loss function and label smoothing. The third and final change is to use minibatch training to let the model use other examples from the same minibatch as side information to improve the quality and variety of generated images. These changes help to stabilize the training process and improve the model’s performance. The performance of the GAN models is compared using the inception score (IS) and the Fréchet inception distance (FID), which are widely used metrics for evaluating the quality and diversity of generated images. The effectiveness of our approach was tested by comparing an IGAN model with other GAN models on the CelebA and stacked MNIST datasets. Results show that IGAN outperforms all the other models, achieving an IS of 13.95 and an FID of 43.71 after traning for 200 epochs. In addition to demonstrating the improvement in the performance of the IGAN, the instabilities, diversity, and fidelity of the models were investigated. The results showed that the IGAN was able to converge to a distribution of the real data more quickly. Furthermore, the experiments revealed that IGAN is capable of producing more stable and high-quality images. This suggests that IGAN is a promising approach for improving the training and performance of GANs and may have a range of applications in image synthesis and other areas.",
      "intriguing_abstract": "Generative adversarial networks (GANs) are a powerful tool for synthesizing realistic images, but they can be difficult to train and are prone to instability and mode collapse. This paper proposes a new model called Identity Generative Adversarial Network (IGAN) that addresses these issues. This model is based on three modifications to the baseline deep convolutional generative adversarial network (DCGAN). The first change is to add a non-linear identity block to the architecture. This will make it easier for the model to fit complex data types and cut down on the time it takes to train. The second change is to smooth out the standard GAN loss function by using a modified loss function and label smoothing. The third and final change is to use minibatch training to let the model use other examples from the same minibatch as side information to improve the quality and variety of generated images. These changes help to stabilize the training process and improve the model’s performance. The performance of the GAN models is compared using the inception score (IS) and the Fréchet inception distance (FID), which are widely used metrics for evaluating the quality and diversity of generated images. The effectiveness of our approach was tested by comparing an IGAN model with other GAN models on the CelebA and stacked MNIST datasets. Results show that IGAN outperforms all the other models, achieving an IS of 13.95 and an FID of 43.71 after traning for 200 epochs. In addition to demonstrating the improvement in the performance of the IGAN, the instabilities, diversity, and fidelity of the models were investigated. The results showed that the IGAN was able to converge to a distribution of the real data more quickly. Furthermore, the experiments revealed that IGAN is capable of producing more stable and high-quality images. This suggests that IGAN is a promising approach for improving the training and performance of GANs and may have a range of applications in image synthesis and other areas.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/94087f564f2fc3760f170c35801df0dc511aecb9.pdf",
      "citation_key": "fathallah20236k5",
      "metadata": {
        "title": "Stabilizing and Improving Training of Generative Adversarial Networks Through Identity Blocks and Modified Loss Function",
        "authors": [
          "Mohamed Fathallah",
          "Mohamed Sakr",
          "Sherif Eletriby"
        ],
        "published_date": "2023",
        "abstract": "Generative adversarial networks (GANs) are a powerful tool for synthesizing realistic images, but they can be difficult to train and are prone to instability and mode collapse. This paper proposes a new model called Identity Generative Adversarial Network (IGAN) that addresses these issues. This model is based on three modifications to the baseline deep convolutional generative adversarial network (DCGAN). The first change is to add a non-linear identity block to the architecture. This will make it easier for the model to fit complex data types and cut down on the time it takes to train. The second change is to smooth out the standard GAN loss function by using a modified loss function and label smoothing. The third and final change is to use minibatch training to let the model use other examples from the same minibatch as side information to improve the quality and variety of generated images. These changes help to stabilize the training process and improve the model’s performance. The performance of the GAN models is compared using the inception score (IS) and the Fréchet inception distance (FID), which are widely used metrics for evaluating the quality and diversity of generated images. The effectiveness of our approach was tested by comparing an IGAN model with other GAN models on the CelebA and stacked MNIST datasets. Results show that IGAN outperforms all the other models, achieving an IS of 13.95 and an FID of 43.71 after traning for 200 epochs. In addition to demonstrating the improvement in the performance of the IGAN, the instabilities, diversity, and fidelity of the models were investigated. The results showed that the IGAN was able to converge to a distribution of the real data more quickly. Furthermore, the experiments revealed that IGAN is capable of producing more stable and high-quality images. This suggests that IGAN is a promising approach for improving the training and performance of GANs and may have a range of applications in image synthesis and other areas.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/94087f564f2fc3760f170c35801df0dc511aecb9.pdf",
        "venue": "IEEE Access",
        "citationCount": 12,
        "score": 6.0,
        "summary": "Generative adversarial networks (GANs) are a powerful tool for synthesizing realistic images, but they can be difficult to train and are prone to instability and mode collapse. This paper proposes a new model called Identity Generative Adversarial Network (IGAN) that addresses these issues. This model is based on three modifications to the baseline deep convolutional generative adversarial network (DCGAN). The first change is to add a non-linear identity block to the architecture. This will make it easier for the model to fit complex data types and cut down on the time it takes to train. The second change is to smooth out the standard GAN loss function by using a modified loss function and label smoothing. The third and final change is to use minibatch training to let the model use other examples from the same minibatch as side information to improve the quality and variety of generated images. These changes help to stabilize the training process and improve the model’s performance. The performance of the GAN models is compared using the inception score (IS) and the Fréchet inception distance (FID), which are widely used metrics for evaluating the quality and diversity of generated images. The effectiveness of our approach was tested by comparing an IGAN model with other GAN models on the CelebA and stacked MNIST datasets. Results show that IGAN outperforms all the other models, achieving an IS of 13.95 and an FID of 43.71 after traning for 200 epochs. In addition to demonstrating the improvement in the performance of the IGAN, the instabilities, diversity, and fidelity of the models were investigated. The results showed that the IGAN was able to converge to a distribution of the real data more quickly. Furthermore, the experiments revealed that IGAN is capable of producing more stable and high-quality images. This suggests that IGAN is a promising approach for improving the training and performance of GANs and may have a range of applications in image synthesis and other areas.",
        "keywords": []
      },
      "file_name": "94087f564f2fc3760f170c35801df0dc511aecb9.pdf"
    },
    {
      "success": true,
      "doc_id": "8ded1341cdf60f3c20e28c11f29cb836",
      "summary": "Generative Adversarial Imitation Learning (GAIL) trains a generative policy to mimic a demonstrator. It uses on-policy Reinforcement Learning (RL) to optimize a reward signal derived from a GAN-like discriminator. A major drawback of GAIL is its training instability - it inherits the complex training dynamics of GANs, and the distribution shift introduced by RL. This can cause oscillations during training, harming its sample efficiency and final policy performance. Recent work has shown that control theory can help with the convergence of a GAN's training. This paper extends this line of work, conducting a control-theoretic analysis of GAIL and deriving a novel controller that not only pushes GAIL to the desired equilibrium but also achieves asymptotic stability in a 'one-step' setting. Based on this, we propose a practical algorithm 'Controlled-GAIL' (C-GAIL). On MuJoCo tasks, our controlled variant is able to speed up the rate of convergence, reduce the range of oscillation and match the expert's distribution more closely both for vanilla GAIL and GAIL-DAC.",
      "intriguing_abstract": "Generative Adversarial Imitation Learning (GAIL) trains a generative policy to mimic a demonstrator. It uses on-policy Reinforcement Learning (RL) to optimize a reward signal derived from a GAN-like discriminator. A major drawback of GAIL is its training instability - it inherits the complex training dynamics of GANs, and the distribution shift introduced by RL. This can cause oscillations during training, harming its sample efficiency and final policy performance. Recent work has shown that control theory can help with the convergence of a GAN's training. This paper extends this line of work, conducting a control-theoretic analysis of GAIL and deriving a novel controller that not only pushes GAIL to the desired equilibrium but also achieves asymptotic stability in a 'one-step' setting. Based on this, we propose a practical algorithm 'Controlled-GAIL' (C-GAIL). On MuJoCo tasks, our controlled variant is able to speed up the rate of convergence, reduce the range of oscillation and match the expert's distribution more closely both for vanilla GAIL and GAIL-DAC.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/bbe8fd30efad3354f6ff523cc26c3d0eb3a21793.pdf",
      "citation_key": "luo2024o1x",
      "metadata": {
        "title": "C-GAIL: Stabilizing Generative Adversarial Imitation Learning with Control Theory",
        "authors": [
          "Tianjiao Luo",
          "Tim Pearce",
          "Huayu Chen",
          "Jianfei Chen",
          "Jun Zhu"
        ],
        "published_date": "2024",
        "abstract": "Generative Adversarial Imitation Learning (GAIL) trains a generative policy to mimic a demonstrator. It uses on-policy Reinforcement Learning (RL) to optimize a reward signal derived from a GAN-like discriminator. A major drawback of GAIL is its training instability - it inherits the complex training dynamics of GANs, and the distribution shift introduced by RL. This can cause oscillations during training, harming its sample efficiency and final policy performance. Recent work has shown that control theory can help with the convergence of a GAN's training. This paper extends this line of work, conducting a control-theoretic analysis of GAIL and deriving a novel controller that not only pushes GAIL to the desired equilibrium but also achieves asymptotic stability in a 'one-step' setting. Based on this, we propose a practical algorithm 'Controlled-GAIL' (C-GAIL). On MuJoCo tasks, our controlled variant is able to speed up the rate of convergence, reduce the range of oscillation and match the expert's distribution more closely both for vanilla GAIL and GAIL-DAC.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/bbe8fd30efad3354f6ff523cc26c3d0eb3a21793.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Generative Adversarial Imitation Learning (GAIL) trains a generative policy to mimic a demonstrator. It uses on-policy Reinforcement Learning (RL) to optimize a reward signal derived from a GAN-like discriminator. A major drawback of GAIL is its training instability - it inherits the complex training dynamics of GANs, and the distribution shift introduced by RL. This can cause oscillations during training, harming its sample efficiency and final policy performance. Recent work has shown that control theory can help with the convergence of a GAN's training. This paper extends this line of work, conducting a control-theoretic analysis of GAIL and deriving a novel controller that not only pushes GAIL to the desired equilibrium but also achieves asymptotic stability in a 'one-step' setting. Based on this, we propose a practical algorithm 'Controlled-GAIL' (C-GAIL). On MuJoCo tasks, our controlled variant is able to speed up the rate of convergence, reduce the range of oscillation and match the expert's distribution more closely both for vanilla GAIL and GAIL-DAC.",
        "keywords": []
      },
      "file_name": "bbe8fd30efad3354f6ff523cc26c3d0eb3a21793.pdf"
    },
    {
      "success": true,
      "doc_id": "b9076d9f9b53be4abe9f856fbb28288b",
      "summary": "In this paper, we propose the Soft Generative Adversarial Network (SoftGAN), a strategy that utilizes a dynamic borderline softening mechanism to train Generative Adversarial Networks. This mechanism aims to solve the mode collapse problem and enhance the training stability of the generated outputs. Within the SoftGAN, the objective of the discriminator is to learn a fuzzy concept of real data with a soft borderline between real and generated data. This objective is achieved by balancing the principles of maximum concept coverage and maximum expected entropy of fuzzy concepts. During the early training stage of the SoftGAN, the principle of maximum expected entropy of fuzzy concepts guides the learning process due to the significant divergence between the generated and real data. However, in the final stage of training, the principle of maximum concept coverage dominates as the divergence between the two distributions decreases. The dynamic borderline softening mechanism of the SoftGAN can be likened to a student (the generator) striving to create realistic images, with the tutor (the discriminator) dynamically guiding the student towards the right direction and motivating effective learning. The tutor gives appropriate encouragement or requirements according to abilities of the student at different stages, so as to promote the student to improve themselves better. Our approach offers both theoretical and practical benefits for improving GAN training. We empirically demonstrate the superiority of our SoftGAN approach in addressing mode collapse issues and generating high-quality outputs compared to existing approaches.",
      "intriguing_abstract": "In this paper, we propose the Soft Generative Adversarial Network (SoftGAN), a strategy that utilizes a dynamic borderline softening mechanism to train Generative Adversarial Networks. This mechanism aims to solve the mode collapse problem and enhance the training stability of the generated outputs. Within the SoftGAN, the objective of the discriminator is to learn a fuzzy concept of real data with a soft borderline between real and generated data. This objective is achieved by balancing the principles of maximum concept coverage and maximum expected entropy of fuzzy concepts. During the early training stage of the SoftGAN, the principle of maximum expected entropy of fuzzy concepts guides the learning process due to the significant divergence between the generated and real data. However, in the final stage of training, the principle of maximum concept coverage dominates as the divergence between the two distributions decreases. The dynamic borderline softening mechanism of the SoftGAN can be likened to a student (the generator) striving to create realistic images, with the tutor (the discriminator) dynamically guiding the student towards the right direction and motivating effective learning. The tutor gives appropriate encouragement or requirements according to abilities of the student at different stages, so as to promote the student to improve themselves better. Our approach offers both theoretical and practical benefits for improving GAN training. We empirically demonstrate the superiority of our SoftGAN approach in addressing mode collapse issues and generating high-quality outputs compared to existing approaches.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/2a4bcdfb38fd8627b5a1e6161b253ae5b980cfeb.pdf",
      "citation_key": "li2024uae",
      "metadata": {
        "title": "Soft Generative Adversarial Network: Combating Mode Collapse in Generative Adversarial Network Training via Dynamic Borderline Softening Mechanism",
        "authors": [
          "Wei Li",
          "Yongchuan Tang"
        ],
        "published_date": "2024",
        "abstract": "In this paper, we propose the Soft Generative Adversarial Network (SoftGAN), a strategy that utilizes a dynamic borderline softening mechanism to train Generative Adversarial Networks. This mechanism aims to solve the mode collapse problem and enhance the training stability of the generated outputs. Within the SoftGAN, the objective of the discriminator is to learn a fuzzy concept of real data with a soft borderline between real and generated data. This objective is achieved by balancing the principles of maximum concept coverage and maximum expected entropy of fuzzy concepts. During the early training stage of the SoftGAN, the principle of maximum expected entropy of fuzzy concepts guides the learning process due to the significant divergence between the generated and real data. However, in the final stage of training, the principle of maximum concept coverage dominates as the divergence between the two distributions decreases. The dynamic borderline softening mechanism of the SoftGAN can be likened to a student (the generator) striving to create realistic images, with the tutor (the discriminator) dynamically guiding the student towards the right direction and motivating effective learning. The tutor gives appropriate encouragement or requirements according to abilities of the student at different stages, so as to promote the student to improve themselves better. Our approach offers both theoretical and practical benefits for improving GAN training. We empirically demonstrate the superiority of our SoftGAN approach in addressing mode collapse issues and generating high-quality outputs compared to existing approaches.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/2a4bcdfb38fd8627b5a1e6161b253ae5b980cfeb.pdf",
        "venue": "Applied Sciences",
        "citationCount": 5,
        "score": 5.0,
        "summary": "In this paper, we propose the Soft Generative Adversarial Network (SoftGAN), a strategy that utilizes a dynamic borderline softening mechanism to train Generative Adversarial Networks. This mechanism aims to solve the mode collapse problem and enhance the training stability of the generated outputs. Within the SoftGAN, the objective of the discriminator is to learn a fuzzy concept of real data with a soft borderline between real and generated data. This objective is achieved by balancing the principles of maximum concept coverage and maximum expected entropy of fuzzy concepts. During the early training stage of the SoftGAN, the principle of maximum expected entropy of fuzzy concepts guides the learning process due to the significant divergence between the generated and real data. However, in the final stage of training, the principle of maximum concept coverage dominates as the divergence between the two distributions decreases. The dynamic borderline softening mechanism of the SoftGAN can be likened to a student (the generator) striving to create realistic images, with the tutor (the discriminator) dynamically guiding the student towards the right direction and motivating effective learning. The tutor gives appropriate encouragement or requirements according to abilities of the student at different stages, so as to promote the student to improve themselves better. Our approach offers both theoretical and practical benefits for improving GAN training. We empirically demonstrate the superiority of our SoftGAN approach in addressing mode collapse issues and generating high-quality outputs compared to existing approaches.",
        "keywords": []
      },
      "file_name": "2a4bcdfb38fd8627b5a1e6161b253ae5b980cfeb.pdf"
    },
    {
      "success": true,
      "doc_id": "2dc8f56e69e277268c401233cb54fa51",
      "summary": "Here's a focused summary of the technical paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the inherent limitations of standalone Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) in generative modeling \\cite{cai2024m9z}.\n    *   VAEs, while having robust probabilistic foundations, often produce outputs that lack sharpness and fine detail \\cite{cai2024m9z}.\n    *   GANs, despite generating high-fidelity images, suffer from significant training challenges, including instability and mode collapse (where the generator fails to capture data diversity) \\cite{cai2024m9z}.\n    *   The problem is important because overcoming these limitations is crucial for enhancing the quality, diversity, and robustness of generated data across various high-impact applications.\n\n*   **Related Work & Positioning**\n    *   **VAEs**: Introduced by Kingma and Welling, VAEs are praised for their probabilistic frameworks and efficient unsupervised latent representation learning, providing a theoretically sound approach to modeling complex data distributions \\cite{cai2024m9z}. Their loss function combines KL divergence for regularization and reconstruction loss \\cite{cai2024m9z}.\n    *   **GANs**: Developed by Goodfellow et al., GANs employ a minimax game between a generator and a discriminator to produce realistic data \\cite{cai2024m9z}. Early GANs lacked resolution, but advancements like Deep Convolutional GANs (DCGAN), Wasserstein GANs (WGAN) for stability, and Progressive GANs for high-resolution generation significantly improved their capabilities \\cite{cai2024m9z}.\n    *   **Limitations of Previous Solutions**: VAEs' outputs often lack sharpness, particularly for high-quality image synthesis. GANs are notorious for training instability and mode collapse, limiting the diversity of generated samples \\cite{cai2024m9z}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical approach is the synergistic integration of VAEs and GANs into hybrid VAE-GAN models \\cite{cai2024m9z}.\n    *   This integration, initially proposed by Larsen et al. (2015), leverages the VAE's encoding efficiency to structure a meaningful latent space, which then feeds into the GAN's generator \\cite{cai2024m9z}.\n    *   The innovation lies in combining the strengths: VAEs provide a structured probabilistic latent space that helps mitigate GAN's mode collapse, while the GAN discriminator pushes the VAE decoder to produce sharper, more realistic outputs, addressing VAE's blurriness \\cite{cai2024m9z}.\n    *   The VAE-GAN framework utilizes a combined loss function: $\\mathcal{L}_{\\mathcal{V}\\mathcal{A}\\mathcal{E}-\\mathcal{G}\\mathcal{A}\\mathcal{N}} = \\mathcal{L}_{prior} + \\mathcal{L}_{llikeDis_l} + \\mathcal{L}_{GAN}$ \\cite{cai2024m9z}. Here, $\\mathcal{L}_{prior}$ is the VAE's KL divergence, $\\mathcal{L}_{GAN}$ is the adversarial loss, and crucially, $\\mathcal{L}_{llikeDis_l}$ replaces the VAE's pixel-wise reconstruction loss with a feature-wise metric derived from the discriminator, enhancing output quality \\cite{cai2024m9z}.\n\n*   **Key Technical Contributions**\n    *   **Novel Integration Strategy**: The paper reviews the VAE-GAN architecture as a significant innovation that combines the structured latent space learning of VAEs with the high-fidelity generation capabilities of GANs \\cite{cai2024m9z}.\n    *   **Enhanced Loss Function**: The use of a combined loss function, particularly replacing the VAE's pixel-wise reconstruction with a feature-wise metric from the GAN discriminator ($\\mathcal{L}_{llikeDis_l}$), is a key technical contribution that drives improved output sharpness and realism \\cite{cai2024m9z}.\n    *   **Mitigation of Limitations**: The hybrid model inherently contributes to mitigating VAE's blurred outputs and GAN's training instability and mode collapse, leading to more robust and diverse generative models \\cite{cai2024m9z}.\n\n*   **Experimental Validation**\n    *   As a review paper, it synthesizes findings from various applications rather than presenting new experiments by the author \\cite{cai2024m9z}.\n    *   **Art and Creative Media**: VAE-GANs have been used to synthesize and manipulate digital images, mimic artistic styles, and create personalized artworks \\cite{cai2024m9z}.\n    *   **Medical Imaging**: Demonstrated capability in generating anatomically accurate synthetic medical images, including high-resolution images of rare tumors, aiding diagnostics and training \\cite{cai2024m9z}.\n    *   **Personalized E-commerce**: A study by Kim and Lee (2023) cited in the paper showed VAE-GANs enabling dynamic visualization of furniture in different room settings, leading to a \"notable 30% increase in user engagement and a 25% rise in sales conversions\" \\cite{cai2024m9z}.\n    *   **Video Game Development**: Applied to generate dynamic game environments, adaptive difficulty levels, and narratives, enhancing player immersion and streamlining content creation \\cite{cai2024m9z}.\n\n*   **Limitations & Scope**\n    *   **Training Stability and Robustness**: Despite improvements, VAE-GANs still face challenges related to GAN's mode collapse and VAE's posterior collapse, requiring ongoing research into innovative training methods and stabilization techniques \\cite{cai2024m9z}.\n    *   **Computational Efficiency**: Training these complex models demands significant computational resources, limiting accessibility for many researchers and developers \\cite{cai2024m9z}. Efforts are needed to develop lightweight models and efficient training algorithms \\cite{cai2024m9z}.\n    *   **Ethical Concerns**: The ability to generate highly realistic data raises serious ethical concerns regarding misuse, such as creating deepfakes, misinformation, and deceptive media, necessitating stringent ethical guidelines and regulatory frameworks \\cite{cai2024m9z}.\n    *   **Scope of Applicability**: While broadly applicable, the paper highlights the need for further expansion into domains like augmented reality, personalized medicine, and autonomous systems \\cite{cai2024m9z}.\n\n*   **Technical Significance**\n    *   The integration of VAEs and GANs significantly advances the technical state-of-the-art in generative modeling by overcoming the individual limitations of each architecture \\cite{cai2024m9z}.\n    *   It enables the generation of outputs that are simultaneously high-fidelity, sharp, and diverse, which was a major challenge for previous models \\cite{cai2024m9z}.\n    *   This hybrid approach has transformative potential across diverse fields, from creative arts and medical diagnostics to e-commerce and gaming, by enabling more realistic and controllable data generation \\cite{cai2024m9z}.\n    *   Future research is poised to enhance training stability (e.g., self-correcting mechanisms), reduce computational costs (e.g., MobileNetV2), and expand applications into new domains like augmented reality and personalized medicine, further democratizing and advancing generative AI \\cite{cai2024m9z}.",
      "intriguing_abstract": "The quest for truly robust and high-fidelity generative models has long been hampered by the inherent limitations of standalone architectures. While Variational Autoencoders (VAEs) offer probabilistic rigor, their outputs often lack sharpness. Conversely, Generative Adversarial Networks (GANs) achieve stunning realism but are plagued by training instability and mode collapse. This paper explores the transformative potential of **VAE-GAN models**, a synergistic hybrid architecture that ingeniously combines their strengths.\n\nOur review highlights a novel integration strategy where the VAE's structured **latent space** mitigates GAN's mode collapse, while a powerful **GAN discriminator** pushes the VAE decoder to generate unprecedentedly sharp and realistic outputs. A key innovation lies in the enhanced combined loss function, crucially replacing the VAE's traditional pixel-wise reconstruction with a **feature-wise metric** derived from the discriminator ($\\mathcal{L}_{llikeDis_l}$). This technical advancement is pivotal for achieving superior output quality. The resulting **VAE-GAN framework** overcomes the long-standing challenges of blurriness and diversity, enabling the creation of simultaneously high-fidelity, sharp, and diverse data. Empirical validations across **medical imaging**, **art generation**, **e-commerce**, and **video game development** demonstrate its profound impact, from generating anatomically accurate synthetic tumors to boosting user engagement. This hybrid approach represents a significant leap forward in **generative AI**, paving the way for more controllable, robust, and impactful applications across diverse high-stakes domains.",
      "keywords": [
        "Generative modeling",
        "Variational Autoencoders (VAEs)",
        "Generative Adversarial Networks (GANs)",
        "Hybrid VAE-GAN models",
        "Structured probabilistic latent space",
        "Combined loss function",
        "Feature-wise reconstruction loss",
        "Mode collapse mitigation",
        "High-fidelity",
        "sharp",
        "diverse data generation",
        "Medical imaging",
        "E-commerce applications",
        "Video game development",
        "Training stability"
      ],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/1e98ac158f0fefdefe9d44d9dc95399bee8ecef6.pdf",
      "citation_key": "cai2024m9z",
      "metadata": {
        "title": "Enhancing capabilities of generative models through VAE-GAN integration: A review",
        "authors": [
          "Dongting Cai"
        ],
        "published_date": "2024",
        "abstract": "Our review explores the integration of Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs), which are pivotal in the realm of generative models. VAEs are renowned for their robust probabilistic foundations and capacity for complex data representation learning, while GANs are celebrated for generating high-fidelity images. Despite their strengths, both models have limitations: VAEs often produce less sharp outputs, and GANs face challenges with training stability. The hybrid VAE-GAN models harness the strengths of both architectures to overcome these limitations, enhancing output quality and diversity. We provide a comprehensive overview of VAEs and GANs technology developments, their integration strategies, and resultant performance improvements. Applications across various fields, such as artistic creation, medical imaging, e-commerce, and video gaming, highlight the transformative potential of these models. However, challenges in model robustness, ethical concerns, and computational demands persist, posing significant hurdles. Future research directions are poised to transform the VAE-GAN landscape significantly. Enhancing training stability remains a priority, with new approaches such as incorporating self-correcting mechanisms into GANs training being tested. Addressing ethical issues is also critical, as policymakers and technologists work together to develop standards that prevent misuse. Moreover, reducing computational costs is fundamental to democratizing access to these technologies. Projects such as the development of MobileNetV2 have made strides in creating more efficient neural network architectures that maintain performance while being less resource-intensive. Further, the exploration of VAE-GAN applications in fields like augmented reality and personalized medicine offers exciting opportunities for growth, as evidenced by recent pilot studies.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/1e98ac158f0fefdefe9d44d9dc95399bee8ecef6.pdf",
        "venue": "Applied and Computational Engineering",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Here's a focused summary of the technical paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the inherent limitations of standalone Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) in generative modeling \\cite{cai2024m9z}.\n    *   VAEs, while having robust probabilistic foundations, often produce outputs that lack sharpness and fine detail \\cite{cai2024m9z}.\n    *   GANs, despite generating high-fidelity images, suffer from significant training challenges, including instability and mode collapse (where the generator fails to capture data diversity) \\cite{cai2024m9z}.\n    *   The problem is important because overcoming these limitations is crucial for enhancing the quality, diversity, and robustness of generated data across various high-impact applications.\n\n*   **Related Work & Positioning**\n    *   **VAEs**: Introduced by Kingma and Welling, VAEs are praised for their probabilistic frameworks and efficient unsupervised latent representation learning, providing a theoretically sound approach to modeling complex data distributions \\cite{cai2024m9z}. Their loss function combines KL divergence for regularization and reconstruction loss \\cite{cai2024m9z}.\n    *   **GANs**: Developed by Goodfellow et al., GANs employ a minimax game between a generator and a discriminator to produce realistic data \\cite{cai2024m9z}. Early GANs lacked resolution, but advancements like Deep Convolutional GANs (DCGAN), Wasserstein GANs (WGAN) for stability, and Progressive GANs for high-resolution generation significantly improved their capabilities \\cite{cai2024m9z}.\n    *   **Limitations of Previous Solutions**: VAEs' outputs often lack sharpness, particularly for high-quality image synthesis. GANs are notorious for training instability and mode collapse, limiting the diversity of generated samples \\cite{cai2024m9z}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical approach is the synergistic integration of VAEs and GANs into hybrid VAE-GAN models \\cite{cai2024m9z}.\n    *   This integration, initially proposed by Larsen et al. (2015), leverages the VAE's encoding efficiency to structure a meaningful latent space, which then feeds into the GAN's generator \\cite{cai2024m9z}.\n    *   The innovation lies in combining the strengths: VAEs provide a structured probabilistic latent space that helps mitigate GAN's mode collapse, while the GAN discriminator pushes the VAE decoder to produce sharper, more realistic outputs, addressing VAE's blurriness \\cite{cai2024m9z}.\n    *   The VAE-GAN framework utilizes a combined loss function: $\\mathcal{L}_{\\mathcal{V}\\mathcal{A}\\mathcal{E}-\\mathcal{G}\\mathcal{A}\\mathcal{N}} = \\mathcal{L}_{prior} + \\mathcal{L}_{llikeDis_l} + \\mathcal{L}_{GAN}$ \\cite{cai2024m9z}. Here, $\\mathcal{L}_{prior}$ is the VAE's KL divergence, $\\mathcal{L}_{GAN}$ is the adversarial loss, and crucially, $\\mathcal{L}_{llikeDis_l}$ replaces the VAE's pixel-wise reconstruction loss with a feature-wise metric derived from the discriminator, enhancing output quality \\cite{cai2024m9z}.\n\n*   **Key Technical Contributions**\n    *   **Novel Integration Strategy**: The paper reviews the VAE-GAN architecture as a significant innovation that combines the structured latent space learning of VAEs with the high-fidelity generation capabilities of GANs \\cite{cai2024m9z}.\n    *   **Enhanced Loss Function**: The use of a combined loss function, particularly replacing the VAE's pixel-wise reconstruction with a feature-wise metric from the GAN discriminator ($\\mathcal{L}_{llikeDis_l}$), is a key technical contribution that drives improved output sharpness and realism \\cite{cai2024m9z}.\n    *   **Mitigation of Limitations**: The hybrid model inherently contributes to mitigating VAE's blurred outputs and GAN's training instability and mode collapse, leading to more robust and diverse generative models \\cite{cai2024m9z}.\n\n*   **Experimental Validation**\n    *   As a review paper, it synthesizes findings from various applications rather than presenting new experiments by the author \\cite{cai2024m9z}.\n    *   **Art and Creative Media**: VAE-GANs have been used to synthesize and manipulate digital images, mimic artistic styles, and create personalized artworks \\cite{cai2024m9z}.\n    *   **Medical Imaging**: Demonstrated capability in generating anatomically accurate synthetic medical images, including high-resolution images of rare tumors, aiding diagnostics and training \\cite{cai2024m9z}.\n    *   **Personalized E-commerce**: A study by Kim and Lee (2023) cited in the paper showed VAE-GANs enabling dynamic visualization of furniture in different room settings, leading to a \"notable 30% increase in user engagement and a 25% rise in sales conversions\" \\cite{cai2024m9z}.\n    *   **Video Game Development**: Applied to generate dynamic game environments, adaptive difficulty levels, and narratives, enhancing player immersion and streamlining content creation \\cite{cai2024m9z}.\n\n*   **Limitations & Scope**\n    *   **Training Stability and Robustness**: Despite improvements, VAE-GANs still face challenges related to GAN's mode collapse and VAE's posterior collapse, requiring ongoing research into innovative training methods and stabilization techniques \\cite{cai2024m9z}.\n    *   **Computational Efficiency**: Training these complex models demands significant computational resources, limiting accessibility for many researchers and developers \\cite{cai2024m9z}. Efforts are needed to develop lightweight models and efficient training algorithms \\cite{cai2024m9z}.\n    *   **Ethical Concerns**: The ability to generate highly realistic data raises serious ethical concerns regarding misuse, such as creating deepfakes, misinformation, and deceptive media, necessitating stringent ethical guidelines and regulatory frameworks \\cite{cai2024m9z}.\n    *   **Scope of Applicability**: While broadly applicable, the paper highlights the need for further expansion into domains like augmented reality, personalized medicine, and autonomous systems \\cite{cai2024m9z}.\n\n*   **Technical Significance**\n    *   The integration of VAEs and GANs significantly advances the technical state-of-the-art in generative modeling by overcoming the individual limitations of each architecture \\cite{cai2024m9z}.\n    *   It enables the generation of outputs that are simultaneously high-fidelity, sharp, and diverse, which was a major challenge for previous models \\cite{cai2024m9z}.\n    *   This hybrid approach has transformative potential across diverse fields, from creative arts and medical diagnostics to e-commerce and gaming, by enabling more realistic and controllable data generation \\cite{cai2024m9z}.\n    *   Future research is poised to enhance training stability (e.g., self-correcting mechanisms), reduce computational costs (e.g., MobileNetV2), and expand applications into new domains like augmented reality and personalized medicine, further democratizing and advancing generative AI \\cite{cai2024m9z}.",
        "keywords": [
          "Generative modeling",
          "Variational Autoencoders (VAEs)",
          "Generative Adversarial Networks (GANs)",
          "Hybrid VAE-GAN models",
          "Structured probabilistic latent space",
          "Combined loss function",
          "Feature-wise reconstruction loss",
          "Mode collapse mitigation",
          "High-fidelity",
          "sharp",
          "diverse data generation",
          "Medical imaging",
          "E-commerce applications",
          "Video game development",
          "Training stability"
        ],
        "paper_type": "the paper type is **survey**.\n\n**reasoning:**\n\n1.  **title:** the title explicitly includes \"a **review**\", which is a direct indicator for a survey paper.\n2.  **abstract:**\n    *   \"our **review explores** the integration...\"\n    *   \"we provide a **comprehensive overview** of vaes and gans technology developments, their integration strategies, and resultant performance improvements.\"\n    *   it discusses existing models (vaes, gans, vae-gans), their strengths, limitations, applications, challenges, and future research directions. this structure is characteristic of a survey paper that synthesizes existing literature.\n3.  **introduction:** it sets the context by introducing vaes and gans, discussing their existing advancements and limitations, which is typical for a paper that will then review their integration.\n\nthese points strongly align with the criteria for a **survey** paper: \"reviews existing literature comprehensively\" and mentions \"survey\", \"review\", \"comprehensive analysis\"."
      },
      "file_name": "1e98ac158f0fefdefe9d44d9dc95399bee8ecef6.pdf"
    },
    {
      "success": true,
      "doc_id": "420b38488e487db1ce397fc65a479301",
      "summary": "Software-piracy continues to be most critical distress, posing grave threats to digital-assets and financial stability. Traditional Intrusion Detection systems (IDS) often battles hard to identify latest piracy attempts owing to their dependence on pre-established patterns. To effectively address this we attempt to suggest innovative approach leveraging DL based Generative Adversarial Networks (GANs) and ML based Isolation Forest (IF) for detecting software piracies. Our proposed GAN-IF based cyber-security model performs its functions by training a Generator network to mimic the behavior of genuine software applications. Discriminator network discriminates between legitimate and pirated software. Isolation Forests assists in detecting anomalies in diverse conditions, including unseen attacks. Integrated training based on DL and ML framework enables efficient learning and adaptation with respect to piracy challenges, making it highly-successful against prior known threats. There are several DL models which are utilized in IDS operations having limitations in terms of robustness, interpretability. Utilizing GAN in the context of cyber-security to combat software-piracy can have noteworthy merits since GANs can precisely identify forged software as they are skilled at generating fake content resembling actual. Training a GAN on legitimate software, helps to learn and identify disparities in pirated versions. Isolation-Forest can detect anomalies in software distribution networks or user behavior with respect to software usage by recognizing abnormal patterns indicating software piracy, like illegal access or sharing of software licenses. Our proposed model combines GANs and Isolation Forests, excels at accurately detecting subtle indicators of software piracy, a capability that traditional methods may fail to recognize. ML-DL integrated model continuously learns and updates its detection capabilities in response to evolving piracy tactics, making it resilient against zero-day attacks, polymorphic malware. Through adversarial training, ml-model minimizes false alarms and focuses only on genuine threats. In our evaluation, we demonstrate the effectiveness of GAN-IF based cyber-security model in detecting software piracy attempts across various scenarios. Results indicate that our approach outperforms traditional solutions in terms of detection accuracy and adaptability.",
      "intriguing_abstract": "Software-piracy continues to be most critical distress, posing grave threats to digital-assets and financial stability. Traditional Intrusion Detection systems (IDS) often battles hard to identify latest piracy attempts owing to their dependence on pre-established patterns. To effectively address this we attempt to suggest innovative approach leveraging DL based Generative Adversarial Networks (GANs) and ML based Isolation Forest (IF) for detecting software piracies. Our proposed GAN-IF based cyber-security model performs its functions by training a Generator network to mimic the behavior of genuine software applications. Discriminator network discriminates between legitimate and pirated software. Isolation Forests assists in detecting anomalies in diverse conditions, including unseen attacks. Integrated training based on DL and ML framework enables efficient learning and adaptation with respect to piracy challenges, making it highly-successful against prior known threats. There are several DL models which are utilized in IDS operations having limitations in terms of robustness, interpretability. Utilizing GAN in the context of cyber-security to combat software-piracy can have noteworthy merits since GANs can precisely identify forged software as they are skilled at generating fake content resembling actual. Training a GAN on legitimate software, helps to learn and identify disparities in pirated versions. Isolation-Forest can detect anomalies in software distribution networks or user behavior with respect to software usage by recognizing abnormal patterns indicating software piracy, like illegal access or sharing of software licenses. Our proposed model combines GANs and Isolation Forests, excels at accurately detecting subtle indicators of software piracy, a capability that traditional methods may fail to recognize. ML-DL integrated model continuously learns and updates its detection capabilities in response to evolving piracy tactics, making it resilient against zero-day attacks, polymorphic malware. Through adversarial training, ml-model minimizes false alarms and focuses only on genuine threats. In our evaluation, we demonstrate the effectiveness of GAN-IF based cyber-security model in detecting software piracy attempts across various scenarios. Results indicate that our approach outperforms traditional solutions in terms of detection accuracy and adaptability.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/6c8cdeb796e34f9bc8c8377079f29f07f44785e4.pdf",
      "citation_key": "u2023m2y",
      "metadata": {
        "title": "Adversarial Defense: A GAN-IF Based Cyber-security Model for Intrusion Detection in Software Piracy",
        "authors": [
          "K. U",
          "T. S",
          "T.V. Nidhin Prabhakar",
          "Jana Selvaganesan",
          "Vishwas H.N."
        ],
        "published_date": "2023",
        "abstract": "Software-piracy continues to be most critical distress, posing grave threats to digital-assets and financial stability. Traditional Intrusion Detection systems (IDS) often battles hard to identify latest piracy attempts owing to their dependence on pre-established patterns. To effectively address this we attempt to suggest innovative approach leveraging DL based Generative Adversarial Networks (GANs) and ML based Isolation Forest (IF) for detecting software piracies. Our proposed GAN-IF based cyber-security model performs its functions by training a Generator network to mimic the behavior of genuine software applications. Discriminator network discriminates between legitimate and pirated software. Isolation Forests assists in detecting anomalies in diverse conditions, including unseen attacks. Integrated training based on DL and ML framework enables efficient learning and adaptation with respect to piracy challenges, making it highly-successful against prior known threats. There are several DL models which are utilized in IDS operations having limitations in terms of robustness, interpretability. Utilizing GAN in the context of cyber-security to combat software-piracy can have noteworthy merits since GANs can precisely identify forged software as they are skilled at generating fake content resembling actual. Training a GAN on legitimate software, helps to learn and identify disparities in pirated versions. Isolation-Forest can detect anomalies in software distribution networks or user behavior with respect to software usage by recognizing abnormal patterns indicating software piracy, like illegal access or sharing of software licenses. Our proposed model combines GANs and Isolation Forests, excels at accurately detecting subtle indicators of software piracy, a capability that traditional methods may fail to recognize. ML-DL integrated model continuously learns and updates its detection capabilities in response to evolving piracy tactics, making it resilient against zero-day attacks, polymorphic malware. Through adversarial training, ml-model minimizes false alarms and focuses only on genuine threats. In our evaluation, we demonstrate the effectiveness of GAN-IF based cyber-security model in detecting software piracy attempts across various scenarios. Results indicate that our approach outperforms traditional solutions in terms of detection accuracy and adaptability.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/6c8cdeb796e34f9bc8c8377079f29f07f44785e4.pdf",
        "venue": "J. Wirel. Mob. Networks Ubiquitous Comput. Dependable Appl.",
        "citationCount": 9,
        "score": 4.5,
        "summary": "Software-piracy continues to be most critical distress, posing grave threats to digital-assets and financial stability. Traditional Intrusion Detection systems (IDS) often battles hard to identify latest piracy attempts owing to their dependence on pre-established patterns. To effectively address this we attempt to suggest innovative approach leveraging DL based Generative Adversarial Networks (GANs) and ML based Isolation Forest (IF) for detecting software piracies. Our proposed GAN-IF based cyber-security model performs its functions by training a Generator network to mimic the behavior of genuine software applications. Discriminator network discriminates between legitimate and pirated software. Isolation Forests assists in detecting anomalies in diverse conditions, including unseen attacks. Integrated training based on DL and ML framework enables efficient learning and adaptation with respect to piracy challenges, making it highly-successful against prior known threats. There are several DL models which are utilized in IDS operations having limitations in terms of robustness, interpretability. Utilizing GAN in the context of cyber-security to combat software-piracy can have noteworthy merits since GANs can precisely identify forged software as they are skilled at generating fake content resembling actual. Training a GAN on legitimate software, helps to learn and identify disparities in pirated versions. Isolation-Forest can detect anomalies in software distribution networks or user behavior with respect to software usage by recognizing abnormal patterns indicating software piracy, like illegal access or sharing of software licenses. Our proposed model combines GANs and Isolation Forests, excels at accurately detecting subtle indicators of software piracy, a capability that traditional methods may fail to recognize. ML-DL integrated model continuously learns and updates its detection capabilities in response to evolving piracy tactics, making it resilient against zero-day attacks, polymorphic malware. Through adversarial training, ml-model minimizes false alarms and focuses only on genuine threats. In our evaluation, we demonstrate the effectiveness of GAN-IF based cyber-security model in detecting software piracy attempts across various scenarios. Results indicate that our approach outperforms traditional solutions in terms of detection accuracy and adaptability.",
        "keywords": []
      },
      "file_name": "6c8cdeb796e34f9bc8c8377079f29f07f44785e4.pdf"
    },
    {
      "success": true,
      "doc_id": "d05169ae1e6c79246ee724fd947d2bfc",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/bfbba9a4faadc7f820c0b21bbbc4d48d06f88623.pdf",
      "citation_key": "liu2023q2q",
      "metadata": {
        "title": "Super-Resolution Reconstruction of CT Images Based on Multi-scale Information Fused Generative Adversarial Networks",
        "authors": [
          "Xiaobao Liu",
          "Shuailin Su",
          "Wenjuan Gu",
          "Tingqiang Yao",
          "Jihong Shen",
          "Yin Mo"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/bfbba9a4faadc7f820c0b21bbbc4d48d06f88623.pdf",
        "venue": "Annals of Biomedical Engineering",
        "citationCount": 9,
        "score": 4.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "bfbba9a4faadc7f820c0b21bbbc4d48d06f88623.pdf"
    },
    {
      "success": true,
      "doc_id": "1419e80d152c4ca8fb065d49bac6f7a5",
      "summary": "As one of the representative models in the field of image generation, generative adversarial networks (GANs) face a significant challenge: how to make the best trade-off between the quality of generated images and training stability. The U-Net based GAN (U-Net GAN), a recently developed approach, can generate high-quality synthetic images by using a U-Net architecture for the discriminator. However, this model may suffer from severe mode collapse. In this study, a stable U-Net GAN (SUGAN) is proposed to mainly solve this problem. First, a gradient normalization module is introduced to the discriminator of U-Net GAN. This module effectively reduces gradient magnitudes, thereby greatly alleviating the problems of gradient instability and overfitting. As a result, the training stability of the GAN model is improved. Additionally, in order to solve the problem of blurred edges of the generated images, a modified residual network is used in the generator. This modification enhances its ability to capture image details, leading to higher-definition generated images. Extensive experiments conducted on several datasets show that the proposed SUGAN significantly improves over the Inception Score (IS) and Fréchet Inception Distance (FID) metrics compared with several state-of-the-art and classic GANs. The training process of our SUGAN is stable, and the quality and diversity of the generated samples are higher. This clearly demonstrates the effectiveness of our approach for image generation tasks. The source code and trained model of our SUGAN have been publicly released.",
      "intriguing_abstract": "As one of the representative models in the field of image generation, generative adversarial networks (GANs) face a significant challenge: how to make the best trade-off between the quality of generated images and training stability. The U-Net based GAN (U-Net GAN), a recently developed approach, can generate high-quality synthetic images by using a U-Net architecture for the discriminator. However, this model may suffer from severe mode collapse. In this study, a stable U-Net GAN (SUGAN) is proposed to mainly solve this problem. First, a gradient normalization module is introduced to the discriminator of U-Net GAN. This module effectively reduces gradient magnitudes, thereby greatly alleviating the problems of gradient instability and overfitting. As a result, the training stability of the GAN model is improved. Additionally, in order to solve the problem of blurred edges of the generated images, a modified residual network is used in the generator. This modification enhances its ability to capture image details, leading to higher-definition generated images. Extensive experiments conducted on several datasets show that the proposed SUGAN significantly improves over the Inception Score (IS) and Fréchet Inception Distance (FID) metrics compared with several state-of-the-art and classic GANs. The training process of our SUGAN is stable, and the quality and diversity of the generated samples are higher. This clearly demonstrates the effectiveness of our approach for image generation tasks. The source code and trained model of our SUGAN have been publicly released.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/0fe35c17baf4a451ed11981ac518b89abf618278.pdf",
      "citation_key": "cheng2023t9b",
      "metadata": {
        "title": "SUGAN: A Stable U-Net Based Generative Adversarial Network",
        "authors": [
          "Shijie Cheng",
          "Lingfeng Wang",
          "M. Zhang",
          "Cheng Zeng",
          "Yan Meng"
        ],
        "published_date": "2023",
        "abstract": "As one of the representative models in the field of image generation, generative adversarial networks (GANs) face a significant challenge: how to make the best trade-off between the quality of generated images and training stability. The U-Net based GAN (U-Net GAN), a recently developed approach, can generate high-quality synthetic images by using a U-Net architecture for the discriminator. However, this model may suffer from severe mode collapse. In this study, a stable U-Net GAN (SUGAN) is proposed to mainly solve this problem. First, a gradient normalization module is introduced to the discriminator of U-Net GAN. This module effectively reduces gradient magnitudes, thereby greatly alleviating the problems of gradient instability and overfitting. As a result, the training stability of the GAN model is improved. Additionally, in order to solve the problem of blurred edges of the generated images, a modified residual network is used in the generator. This modification enhances its ability to capture image details, leading to higher-definition generated images. Extensive experiments conducted on several datasets show that the proposed SUGAN significantly improves over the Inception Score (IS) and Fréchet Inception Distance (FID) metrics compared with several state-of-the-art and classic GANs. The training process of our SUGAN is stable, and the quality and diversity of the generated samples are higher. This clearly demonstrates the effectiveness of our approach for image generation tasks. The source code and trained model of our SUGAN have been publicly released.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/0fe35c17baf4a451ed11981ac518b89abf618278.pdf",
        "venue": "Italian National Conference on Sensors",
        "citationCount": 6,
        "score": 3.0,
        "summary": "As one of the representative models in the field of image generation, generative adversarial networks (GANs) face a significant challenge: how to make the best trade-off between the quality of generated images and training stability. The U-Net based GAN (U-Net GAN), a recently developed approach, can generate high-quality synthetic images by using a U-Net architecture for the discriminator. However, this model may suffer from severe mode collapse. In this study, a stable U-Net GAN (SUGAN) is proposed to mainly solve this problem. First, a gradient normalization module is introduced to the discriminator of U-Net GAN. This module effectively reduces gradient magnitudes, thereby greatly alleviating the problems of gradient instability and overfitting. As a result, the training stability of the GAN model is improved. Additionally, in order to solve the problem of blurred edges of the generated images, a modified residual network is used in the generator. This modification enhances its ability to capture image details, leading to higher-definition generated images. Extensive experiments conducted on several datasets show that the proposed SUGAN significantly improves over the Inception Score (IS) and Fréchet Inception Distance (FID) metrics compared with several state-of-the-art and classic GANs. The training process of our SUGAN is stable, and the quality and diversity of the generated samples are higher. This clearly demonstrates the effectiveness of our approach for image generation tasks. The source code and trained model of our SUGAN have been publicly released.",
        "keywords": []
      },
      "file_name": "0fe35c17baf4a451ed11981ac518b89abf618278.pdf"
    },
    {
      "success": true,
      "doc_id": "706e9bd0d4888e0f7ebbf142162b7044",
      "summary": "With the development of full digitalization, the amount of time series data generated by sensors is ever-increasing; thus, time series outlier detection has become crucial. Moreover, in practice, discovering and flagging anomalies is very time-consuming and expensive. To solve this problem, unsupervised anomaly detection methods have often been used in the past, in which the model is trained with normal data to learn its behavioral patterns. Generative adversarial networks (GANs) can simulate complex and high-dimensional distributions of data and can be used to learn the behavioral patterns of normal data for unsupervised anomaly detection. However, because of the problem of convergence, GANs are difficult to train. Thus, USADs (an unsupervised anomaly detection model) utilize an autoencoder (AE) to undertake the task of the generator and discriminator and enhance the stability during adversarial training by using the AE to alleviate the problem of non-convergence encountered in GANs. Therefore, in this study, we used the USAD’s generative adversarial training architecture combined with convolutional AEs to improve the model’s feature extraction capabilities. In addition, to reduce false-positive outcomes caused by the prominent sharp points in the reconstructed data, we used the exponential weighted moving average method to smooth the reconstruction error, thereby improving the anomaly detection accuracy of the model. Finally, we experimented with real-world time-series data (ECG and 2D gesture) and verified that our approach could improve accuracy. Compared to the best in the comparison method, our model improved by 0.028% in AUROC, 0.233% in AUPRC, and 0.187% in F1 on average.",
      "intriguing_abstract": "With the development of full digitalization, the amount of time series data generated by sensors is ever-increasing; thus, time series outlier detection has become crucial. Moreover, in practice, discovering and flagging anomalies is very time-consuming and expensive. To solve this problem, unsupervised anomaly detection methods have often been used in the past, in which the model is trained with normal data to learn its behavioral patterns. Generative adversarial networks (GANs) can simulate complex and high-dimensional distributions of data and can be used to learn the behavioral patterns of normal data for unsupervised anomaly detection. However, because of the problem of convergence, GANs are difficult to train. Thus, USADs (an unsupervised anomaly detection model) utilize an autoencoder (AE) to undertake the task of the generator and discriminator and enhance the stability during adversarial training by using the AE to alleviate the problem of non-convergence encountered in GANs. Therefore, in this study, we used the USAD’s generative adversarial training architecture combined with convolutional AEs to improve the model’s feature extraction capabilities. In addition, to reduce false-positive outcomes caused by the prominent sharp points in the reconstructed data, we used the exponential weighted moving average method to smooth the reconstruction error, thereby improving the anomaly detection accuracy of the model. Finally, we experimented with real-world time-series data (ECG and 2D gesture) and verified that our approach could improve accuracy. Compared to the best in the comparison method, our model improved by 0.028% in AUROC, 0.233% in AUPRC, and 0.187% in F1 on average.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/44d1adb9f96b87ace6408ba6b9ec31713f74cece.pdf",
      "citation_key": "luo2022rm1",
      "metadata": {
        "title": "Anomaly detection by using a combination of generative adversarial networks and convolutional autoencoders",
        "authors": [
          "Xukang Luo",
          "Ying Jiang",
          "Enqiang Wang",
          "Xinlei Men"
        ],
        "published_date": "2022",
        "abstract": "With the development of full digitalization, the amount of time series data generated by sensors is ever-increasing; thus, time series outlier detection has become crucial. Moreover, in practice, discovering and flagging anomalies is very time-consuming and expensive. To solve this problem, unsupervised anomaly detection methods have often been used in the past, in which the model is trained with normal data to learn its behavioral patterns. Generative adversarial networks (GANs) can simulate complex and high-dimensional distributions of data and can be used to learn the behavioral patterns of normal data for unsupervised anomaly detection. However, because of the problem of convergence, GANs are difficult to train. Thus, USADs (an unsupervised anomaly detection model) utilize an autoencoder (AE) to undertake the task of the generator and discriminator and enhance the stability during adversarial training by using the AE to alleviate the problem of non-convergence encountered in GANs. Therefore, in this study, we used the USAD’s generative adversarial training architecture combined with convolutional AEs to improve the model’s feature extraction capabilities. In addition, to reduce false-positive outcomes caused by the prominent sharp points in the reconstructed data, we used the exponential weighted moving average method to smooth the reconstruction error, thereby improving the anomaly detection accuracy of the model. Finally, we experimented with real-world time-series data (ECG and 2D gesture) and verified that our approach could improve accuracy. Compared to the best in the comparison method, our model improved by 0.028% in AUROC, 0.233% in AUPRC, and 0.187% in F1 on average.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/44d1adb9f96b87ace6408ba6b9ec31713f74cece.pdf",
        "venue": "EURASIP Journal on Advances in Signal Processing",
        "citationCount": 9,
        "score": 3.0,
        "summary": "With the development of full digitalization, the amount of time series data generated by sensors is ever-increasing; thus, time series outlier detection has become crucial. Moreover, in practice, discovering and flagging anomalies is very time-consuming and expensive. To solve this problem, unsupervised anomaly detection methods have often been used in the past, in which the model is trained with normal data to learn its behavioral patterns. Generative adversarial networks (GANs) can simulate complex and high-dimensional distributions of data and can be used to learn the behavioral patterns of normal data for unsupervised anomaly detection. However, because of the problem of convergence, GANs are difficult to train. Thus, USADs (an unsupervised anomaly detection model) utilize an autoencoder (AE) to undertake the task of the generator and discriminator and enhance the stability during adversarial training by using the AE to alleviate the problem of non-convergence encountered in GANs. Therefore, in this study, we used the USAD’s generative adversarial training architecture combined with convolutional AEs to improve the model’s feature extraction capabilities. In addition, to reduce false-positive outcomes caused by the prominent sharp points in the reconstructed data, we used the exponential weighted moving average method to smooth the reconstruction error, thereby improving the anomaly detection accuracy of the model. Finally, we experimented with real-world time-series data (ECG and 2D gesture) and verified that our approach could improve accuracy. Compared to the best in the comparison method, our model improved by 0.028% in AUROC, 0.233% in AUPRC, and 0.187% in F1 on average.",
        "keywords": []
      },
      "file_name": "44d1adb9f96b87ace6408ba6b9ec31713f74cece.pdf"
    },
    {
      "success": true,
      "doc_id": "1155229d9ebc634ab9348130177f9e17",
      "summary": "Considering the problems of the model collapse and the low forecast precision in predicting the financial time series of the generative adversarial networks (GAN), we apply the WGAN-GP model to solve the gradient collapse. Extreme gradient boosting (XGBoost) is used for feature extraction to improve prediction accuracy. Alibaba stock is taken as the research object, using XGBoost to optimize its characteristic factors, and training the optimized characteristic variables with WGAN-GP. We compare the prediction results of WGAN-GP model and classical time series prediction models, long short term memory (LSTM) and gate recurrent unit (GRU). In the experimental stage, root mean square error (RMSE) is chosen as the evaluation index. The results of different models show that the RMSE of WGAN-GP model is the smallest, which are 61.94% and 47.42%, lower than that of LSTM model and GRU model respectively. At the same time, the stock price data of Google and Amazon confirm the stability of WGAN-GP model. WGAN-GP model can obtain higher prediction accuracy than the classical time series prediction model.",
      "intriguing_abstract": "Considering the problems of the model collapse and the low forecast precision in predicting the financial time series of the generative adversarial networks (GAN), we apply the WGAN-GP model to solve the gradient collapse. Extreme gradient boosting (XGBoost) is used for feature extraction to improve prediction accuracy. Alibaba stock is taken as the research object, using XGBoost to optimize its characteristic factors, and training the optimized characteristic variables with WGAN-GP. We compare the prediction results of WGAN-GP model and classical time series prediction models, long short term memory (LSTM) and gate recurrent unit (GRU). In the experimental stage, root mean square error (RMSE) is chosen as the evaluation index. The results of different models show that the RMSE of WGAN-GP model is the smallest, which are 61.94% and 47.42%, lower than that of LSTM model and GRU model respectively. At the same time, the stock price data of Google and Amazon confirm the stability of WGAN-GP model. WGAN-GP model can obtain higher prediction accuracy than the classical time series prediction model.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/a6a3a7a76219defe50741256e40cba5a7132e007.pdf",
      "citation_key": "xu2022ss4",
      "metadata": {
        "title": "Financial Time Series Prediction Based on XGBoost and Generative Adversarial Networks",
        "authors": [
          "Jialing Xu",
          "Jingxing He",
          "Jinqiang Gu",
          "Huayang Wu",
          "Lei Wang",
          "Yongzhen Zhu",
          "Tiejun Wang",
          "Xiaoling He",
          "Zhangyuan Zhou"
        ],
        "published_date": "2022",
        "abstract": "Considering the problems of the model collapse and the low forecast precision in predicting the financial time series of the generative adversarial networks (GAN), we apply the WGAN-GP model to solve the gradient collapse. Extreme gradient boosting (XGBoost) is used for feature extraction to improve prediction accuracy. Alibaba stock is taken as the research object, using XGBoost to optimize its characteristic factors, and training the optimized characteristic variables with WGAN-GP. We compare the prediction results of WGAN-GP model and classical time series prediction models, long short term memory (LSTM) and gate recurrent unit (GRU). In the experimental stage, root mean square error (RMSE) is chosen as the evaluation index. The results of different models show that the RMSE of WGAN-GP model is the smallest, which are 61.94% and 47.42%, lower than that of LSTM model and GRU model respectively. At the same time, the stock price data of Google and Amazon confirm the stability of WGAN-GP model. WGAN-GP model can obtain higher prediction accuracy than the classical time series prediction model.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/a6a3a7a76219defe50741256e40cba5a7132e007.pdf",
        "venue": "International Journal of Circuits, Systems and Signal Processing",
        "citationCount": 8,
        "score": 2.6666666666666665,
        "summary": "Considering the problems of the model collapse and the low forecast precision in predicting the financial time series of the generative adversarial networks (GAN), we apply the WGAN-GP model to solve the gradient collapse. Extreme gradient boosting (XGBoost) is used for feature extraction to improve prediction accuracy. Alibaba stock is taken as the research object, using XGBoost to optimize its characteristic factors, and training the optimized characteristic variables with WGAN-GP. We compare the prediction results of WGAN-GP model and classical time series prediction models, long short term memory (LSTM) and gate recurrent unit (GRU). In the experimental stage, root mean square error (RMSE) is chosen as the evaluation index. The results of different models show that the RMSE of WGAN-GP model is the smallest, which are 61.94% and 47.42%, lower than that of LSTM model and GRU model respectively. At the same time, the stock price data of Google and Amazon confirm the stability of WGAN-GP model. WGAN-GP model can obtain higher prediction accuracy than the classical time series prediction model.",
        "keywords": []
      },
      "file_name": "a6a3a7a76219defe50741256e40cba5a7132e007.pdf"
    },
    {
      "success": true,
      "doc_id": "8668246f93e02fbfb9843b91bd7066b2",
      "summary": "Automatic image annotation (AIA) has been adopted in different applications such as image retrieval and classification. Deep Learning is used in AIA to extract image features and then convert these features into text descriptions and labels. However, conventional AIA models that employ deep learning methods suffer from various shortcomings, such as poor annotation performance. This work proposes an AIA model based on convolutional neural networks (CNNs), generative adversarial networks (GANs), and transfer learning. GANs have attracted a lot of interest because of its ability to generate data without explicitly using probability density. Thus, it has proven its usefulness in image annotation and image augmentation. In this work, an Auxiliary classifier-GAN (ACGAN) has been used, where the discriminator predicts the class of an image rather than taking it as a given input; therefore, the stabilization of the training stage is ensured, and the generation of high-quality images is provided. Transfer learning is also used to enhance the performance of the classification. The proposed model outperforms the best state-of-the-art models in terms of MiAP, F-measure and error rate using ImageClef, ESPGame and IAPR-TC12 datasets.",
      "intriguing_abstract": "Automatic image annotation (AIA) has been adopted in different applications such as image retrieval and classification. Deep Learning is used in AIA to extract image features and then convert these features into text descriptions and labels. However, conventional AIA models that employ deep learning methods suffer from various shortcomings, such as poor annotation performance. This work proposes an AIA model based on convolutional neural networks (CNNs), generative adversarial networks (GANs), and transfer learning. GANs have attracted a lot of interest because of its ability to generate data without explicitly using probability density. Thus, it has proven its usefulness in image annotation and image augmentation. In this work, an Auxiliary classifier-GAN (ACGAN) has been used, where the discriminator predicts the class of an image rather than taking it as a given input; therefore, the stabilization of the training stage is ensured, and the generation of high-quality images is provided. Transfer learning is also used to enhance the performance of the classification. The proposed model outperforms the best state-of-the-art models in terms of MiAP, F-measure and error rate using ImageClef, ESPGame and IAPR-TC12 datasets.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/38ade2fc7490467dffc74bced440363fa7c27c5e.pdf",
      "citation_key": "alshehri2022d1h",
      "metadata": {
        "title": "DeepAIA: An Automatic Image Annotation Model Based on Generative Adversarial Networks and Transfer Learning",
        "authors": [
          "Abeer Alshehri",
          "Mounira Taileb",
          "Reem M. Alotaibi"
        ],
        "published_date": "2022",
        "abstract": "Automatic image annotation (AIA) has been adopted in different applications such as image retrieval and classification. Deep Learning is used in AIA to extract image features and then convert these features into text descriptions and labels. However, conventional AIA models that employ deep learning methods suffer from various shortcomings, such as poor annotation performance. This work proposes an AIA model based on convolutional neural networks (CNNs), generative adversarial networks (GANs), and transfer learning. GANs have attracted a lot of interest because of its ability to generate data without explicitly using probability density. Thus, it has proven its usefulness in image annotation and image augmentation. In this work, an Auxiliary classifier-GAN (ACGAN) has been used, where the discriminator predicts the class of an image rather than taking it as a given input; therefore, the stabilization of the training stage is ensured, and the generation of high-quality images is provided. Transfer learning is also used to enhance the performance of the classification. The proposed model outperforms the best state-of-the-art models in terms of MiAP, F-measure and error rate using ImageClef, ESPGame and IAPR-TC12 datasets.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/38ade2fc7490467dffc74bced440363fa7c27c5e.pdf",
        "venue": "IEEE Access",
        "citationCount": 8,
        "score": 2.6666666666666665,
        "summary": "Automatic image annotation (AIA) has been adopted in different applications such as image retrieval and classification. Deep Learning is used in AIA to extract image features and then convert these features into text descriptions and labels. However, conventional AIA models that employ deep learning methods suffer from various shortcomings, such as poor annotation performance. This work proposes an AIA model based on convolutional neural networks (CNNs), generative adversarial networks (GANs), and transfer learning. GANs have attracted a lot of interest because of its ability to generate data without explicitly using probability density. Thus, it has proven its usefulness in image annotation and image augmentation. In this work, an Auxiliary classifier-GAN (ACGAN) has been used, where the discriminator predicts the class of an image rather than taking it as a given input; therefore, the stabilization of the training stage is ensured, and the generation of high-quality images is provided. Transfer learning is also used to enhance the performance of the classification. The proposed model outperforms the best state-of-the-art models in terms of MiAP, F-measure and error rate using ImageClef, ESPGame and IAPR-TC12 datasets.",
        "keywords": []
      },
      "file_name": "38ade2fc7490467dffc74bced440363fa7c27c5e.pdf"
    },
    {
      "success": true,
      "doc_id": "d80e35fe7bb92a6305f9527cd32e3fd0",
      "summary": "While generative adversarial networks (GANs) have been widely used in research on audio generation, the training of a GAN model is known to be unstable, time consuming, and data inefficient. Among the attempts to ameliorate the training process of GANs, the idea of Projected GAN emerges as an effective solution for GAN-based image generation, establishing the state-of-the-art in different image applications. The core idea is to use a pre-trained classifier to constrain the feature space of the discriminator to stabilize and improve GAN training. This paper investigates whether Projected GAN can similarly improve audio generation, by evaluating the performance of a StyleGAN2-based audio-domain loop generation model with and without using a pre-trained feature space in the discriminator. Moreover, we compare the performance of using a general versus domain-specific classifier as the pre-trained audio classifier. With experiments on both drum loop and synth loop generation, we show that a general audio classifier works better, and that with Projected GAN our loop generation models can converge around 5 times faster without performance degradation.",
      "intriguing_abstract": "While generative adversarial networks (GANs) have been widely used in research on audio generation, the training of a GAN model is known to be unstable, time consuming, and data inefficient. Among the attempts to ameliorate the training process of GANs, the idea of Projected GAN emerges as an effective solution for GAN-based image generation, establishing the state-of-the-art in different image applications. The core idea is to use a pre-trained classifier to constrain the feature space of the discriminator to stabilize and improve GAN training. This paper investigates whether Projected GAN can similarly improve audio generation, by evaluating the performance of a StyleGAN2-based audio-domain loop generation model with and without using a pre-trained feature space in the discriminator. Moreover, we compare the performance of using a general versus domain-specific classifier as the pre-trained audio classifier. With experiments on both drum loop and synth loop generation, we show that a general audio classifier works better, and that with Projected GAN our loop generation models can converge around 5 times faster without performance degradation.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/6a1cbe2ee843f6765d3fd849afa7d654daf118a1.pdf",
      "citation_key": "yeh2022yvr",
      "metadata": {
        "title": "Exploiting Pre-trained Feature Networks for Generative Adversarial Networks in Audio-domain Loop Generation",
        "authors": [
          "Yen-Tung Yeh",
          "Bo-Yu Chen",
          "Yi-Hsuan Yang"
        ],
        "published_date": "2022",
        "abstract": "While generative adversarial networks (GANs) have been widely used in research on audio generation, the training of a GAN model is known to be unstable, time consuming, and data inefficient. Among the attempts to ameliorate the training process of GANs, the idea of Projected GAN emerges as an effective solution for GAN-based image generation, establishing the state-of-the-art in different image applications. The core idea is to use a pre-trained classifier to constrain the feature space of the discriminator to stabilize and improve GAN training. This paper investigates whether Projected GAN can similarly improve audio generation, by evaluating the performance of a StyleGAN2-based audio-domain loop generation model with and without using a pre-trained feature space in the discriminator. Moreover, we compare the performance of using a general versus domain-specific classifier as the pre-trained audio classifier. With experiments on both drum loop and synth loop generation, we show that a general audio classifier works better, and that with Projected GAN our loop generation models can converge around 5 times faster without performance degradation.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/6a1cbe2ee843f6765d3fd849afa7d654daf118a1.pdf",
        "venue": "International Society for Music Information Retrieval Conference",
        "citationCount": 6,
        "score": 2.0,
        "summary": "While generative adversarial networks (GANs) have been widely used in research on audio generation, the training of a GAN model is known to be unstable, time consuming, and data inefficient. Among the attempts to ameliorate the training process of GANs, the idea of Projected GAN emerges as an effective solution for GAN-based image generation, establishing the state-of-the-art in different image applications. The core idea is to use a pre-trained classifier to constrain the feature space of the discriminator to stabilize and improve GAN training. This paper investigates whether Projected GAN can similarly improve audio generation, by evaluating the performance of a StyleGAN2-based audio-domain loop generation model with and without using a pre-trained feature space in the discriminator. Moreover, we compare the performance of using a general versus domain-specific classifier as the pre-trained audio classifier. With experiments on both drum loop and synth loop generation, we show that a general audio classifier works better, and that with Projected GAN our loop generation models can converge around 5 times faster without performance degradation.",
        "keywords": []
      },
      "file_name": "6a1cbe2ee843f6765d3fd849afa7d654daf118a1.pdf"
    },
    {
      "success": true,
      "doc_id": "d94601e70268474577c98493cfa96103",
      "summary": "Generative Adversarial Networks (GANs) are powerful machine learning models capable of generating fully synthetic samples of a desired phenomenon with a high resolution. Despite their success, the training process of a GAN is highly unstable, and typically, it is necessary to implement several accessory heuristics to the networks to reach acceptable convergence of the model. In this paper, we introduce a novel method to analyze the convergence and stability in the training of generative adversarial networks. For this purpose, we propose to decompose the objective function of the adversary min–max game defining a periodic GAN into its Fourier series. By studying the dynamics of the truncated Fourier series for the continuous alternating gradient descend algorithm, we are able to approximate the real flow and to identify the main features of the convergence of GAN. This approach is confirmed empirically by studying the training flow in a 2-parametric GAN, aiming to generate an unknown exponential distribution. As a by-product, we show that convergent orbits in GANs are small perturbations of periodic orbits so the Nash equillibria are spiral attractors. This theoretically justifies the slow and unstable training observed in GANs.",
      "intriguing_abstract": "Generative Adversarial Networks (GANs) are powerful machine learning models capable of generating fully synthetic samples of a desired phenomenon with a high resolution. Despite their success, the training process of a GAN is highly unstable, and typically, it is necessary to implement several accessory heuristics to the networks to reach acceptable convergence of the model. In this paper, we introduce a novel method to analyze the convergence and stability in the training of generative adversarial networks. For this purpose, we propose to decompose the objective function of the adversary min–max game defining a periodic GAN into its Fourier series. By studying the dynamics of the truncated Fourier series for the continuous alternating gradient descend algorithm, we are able to approximate the real flow and to identify the main features of the convergence of GAN. This approach is confirmed empirically by studying the training flow in a 2-parametric GAN, aiming to generate an unknown exponential distribution. As a by-product, we show that convergent orbits in GANs are small perturbations of periodic orbits so the Nash equillibria are spiral attractors. This theoretically justifies the slow and unstable training observed in GANs.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/f92e1f06c6f12a287d82ecf06b555b9fcf0b9d99.pdf",
      "citation_key": "gonzlezprieto20214wh",
      "metadata": {
        "title": "Dynamics of Fourier Modes in Torus Generative Adversarial Networks",
        "authors": [
          "Ángel González-Prieto",
          "Alberto Mozo",
          "Edgar Talavera",
          "Sandra Gómez Canaval"
        ],
        "published_date": "2021",
        "abstract": "Generative Adversarial Networks (GANs) are powerful machine learning models capable of generating fully synthetic samples of a desired phenomenon with a high resolution. Despite their success, the training process of a GAN is highly unstable, and typically, it is necessary to implement several accessory heuristics to the networks to reach acceptable convergence of the model. In this paper, we introduce a novel method to analyze the convergence and stability in the training of generative adversarial networks. For this purpose, we propose to decompose the objective function of the adversary min–max game defining a periodic GAN into its Fourier series. By studying the dynamics of the truncated Fourier series for the continuous alternating gradient descend algorithm, we are able to approximate the real flow and to identify the main features of the convergence of GAN. This approach is confirmed empirically by studying the training flow in a 2-parametric GAN, aiming to generate an unknown exponential distribution. As a by-product, we show that convergent orbits in GANs are small perturbations of periodic orbits so the Nash equillibria are spiral attractors. This theoretically justifies the slow and unstable training observed in GANs.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/f92e1f06c6f12a287d82ecf06b555b9fcf0b9d99.pdf",
        "venue": "Mathematics",
        "citationCount": 7,
        "score": 1.75,
        "summary": "Generative Adversarial Networks (GANs) are powerful machine learning models capable of generating fully synthetic samples of a desired phenomenon with a high resolution. Despite their success, the training process of a GAN is highly unstable, and typically, it is necessary to implement several accessory heuristics to the networks to reach acceptable convergence of the model. In this paper, we introduce a novel method to analyze the convergence and stability in the training of generative adversarial networks. For this purpose, we propose to decompose the objective function of the adversary min–max game defining a periodic GAN into its Fourier series. By studying the dynamics of the truncated Fourier series for the continuous alternating gradient descend algorithm, we are able to approximate the real flow and to identify the main features of the convergence of GAN. This approach is confirmed empirically by studying the training flow in a 2-parametric GAN, aiming to generate an unknown exponential distribution. As a by-product, we show that convergent orbits in GANs are small perturbations of periodic orbits so the Nash equillibria are spiral attractors. This theoretically justifies the slow and unstable training observed in GANs.",
        "keywords": []
      },
      "file_name": "f92e1f06c6f12a287d82ecf06b555b9fcf0b9d99.pdf"
    },
    {
      "success": true,
      "doc_id": "4a6a252759e6af4efd9538fbe8dc8cc6",
      "summary": "In practical application, the research of synthetic aperture radar (SAR) target recognition has fallen into a bottle-neck due to the lack of samples. Data argumentation methods based on generative adversarial networks (GAN) have received widespread attention in solving this type of few-shot sample problem. However, the generated images suffer from various shortcomings, such as lack of diversity, low signal-to-noise ratio, blur, etc. In this article, the VAE-WGANGP is proposed, which combines GAN and variational autoencoder (VAE) to alleviate these shortcomings. The innovations of this paper are as follows: firstly, the generator of GAN is replaced with VAE, which constructs an asymmetric network ensuring the stability of GAN training; secondly, the asymmetric loss function is composed of four parts, including reconstruction loss, divergence loss, adversarial loss, and gradient penalty. In this way, the problem of gradient explosion or gradient disappearance is alleviated. The experimental results with the MSTAR dataset show that the images generated by the proposed model outperform the advanced technology with many similar deep features and achieve significant improvement in the target recognition accuracy rate.",
      "intriguing_abstract": "In practical application, the research of synthetic aperture radar (SAR) target recognition has fallen into a bottle-neck due to the lack of samples. Data argumentation methods based on generative adversarial networks (GAN) have received widespread attention in solving this type of few-shot sample problem. However, the generated images suffer from various shortcomings, such as lack of diversity, low signal-to-noise ratio, blur, etc. In this article, the VAE-WGANGP is proposed, which combines GAN and variational autoencoder (VAE) to alleviate these shortcomings. The innovations of this paper are as follows: firstly, the generator of GAN is replaced with VAE, which constructs an asymmetric network ensuring the stability of GAN training; secondly, the asymmetric loss function is composed of four parts, including reconstruction loss, divergence loss, adversarial loss, and gradient penalty. In this way, the problem of gradient explosion or gradient disappearance is alleviated. The experimental results with the MSTAR dataset show that the images generated by the proposed model outperform the advanced technology with many similar deep features and achieve significant improvement in the target recognition accuracy rate.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/b0f12decb3b54ad0eec46d7c29385d714cb879f0.pdf",
      "citation_key": "huang2022zar",
      "metadata": {
        "title": "Asymmetric Training of Generative Adversarial Network for High Fidelity SAR Image Generation",
        "authors": [
          "Ying Huang",
          "Wenhao Mei",
          "Su Liu",
          "Tangsheng Li"
        ],
        "published_date": "2022",
        "abstract": "In practical application, the research of synthetic aperture radar (SAR) target recognition has fallen into a bottle-neck due to the lack of samples. Data argumentation methods based on generative adversarial networks (GAN) have received widespread attention in solving this type of few-shot sample problem. However, the generated images suffer from various shortcomings, such as lack of diversity, low signal-to-noise ratio, blur, etc. In this article, the VAE-WGANGP is proposed, which combines GAN and variational autoencoder (VAE) to alleviate these shortcomings. The innovations of this paper are as follows: firstly, the generator of GAN is replaced with VAE, which constructs an asymmetric network ensuring the stability of GAN training; secondly, the asymmetric loss function is composed of four parts, including reconstruction loss, divergence loss, adversarial loss, and gradient penalty. In this way, the problem of gradient explosion or gradient disappearance is alleviated. The experimental results with the MSTAR dataset show that the images generated by the proposed model outperform the advanced technology with many similar deep features and achieve significant improvement in the target recognition accuracy rate.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/b0f12decb3b54ad0eec46d7c29385d714cb879f0.pdf",
        "venue": "IEEE International Geoscience and Remote Sensing Symposium",
        "citationCount": 5,
        "score": 1.6666666666666665,
        "summary": "In practical application, the research of synthetic aperture radar (SAR) target recognition has fallen into a bottle-neck due to the lack of samples. Data argumentation methods based on generative adversarial networks (GAN) have received widespread attention in solving this type of few-shot sample problem. However, the generated images suffer from various shortcomings, such as lack of diversity, low signal-to-noise ratio, blur, etc. In this article, the VAE-WGANGP is proposed, which combines GAN and variational autoencoder (VAE) to alleviate these shortcomings. The innovations of this paper are as follows: firstly, the generator of GAN is replaced with VAE, which constructs an asymmetric network ensuring the stability of GAN training; secondly, the asymmetric loss function is composed of four parts, including reconstruction loss, divergence loss, adversarial loss, and gradient penalty. In this way, the problem of gradient explosion or gradient disappearance is alleviated. The experimental results with the MSTAR dataset show that the images generated by the proposed model outperform the advanced technology with many similar deep features and achieve significant improvement in the target recognition accuracy rate.",
        "keywords": []
      },
      "file_name": "b0f12decb3b54ad0eec46d7c29385d714cb879f0.pdf"
    },
    {
      "success": true,
      "doc_id": "35b9830d950dfbf5e38b59b2a0e98ee7",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/a3993af5ab3f48f190a6fcdc2fc711d9f091e19d.pdf",
      "citation_key": "wang2020iia",
      "metadata": {
        "title": "Image classification based on principal component analysis optimized generative adversarial networks",
        "authors": [
          "Chunzhi Wang",
          "Pan Wu",
          "Lingyu Yan",
          "Z. Ye",
          "Hongwe Chen",
          "H. Ling"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/a3993af5ab3f48f190a6fcdc2fc711d9f091e19d.pdf",
        "venue": "Multimedia tools and applications",
        "citationCount": 8,
        "score": 1.6,
        "summary": "",
        "keywords": []
      },
      "file_name": "a3993af5ab3f48f190a6fcdc2fc711d9f091e19d.pdf"
    },
    {
      "success": true,
      "doc_id": "d1d10d32e587cf82b6ffb3ec3bd2f45f",
      "summary": "Text-to-image synthesis is an important and challenging application of computer vision. Many interesting and meaningful text-to-image synthesis models have been put forward. However, most of the works pay attention to the quality of synthesis images, but rarely consider the size of these models. Large models contain many parameters and high delay, which makes it difficult to be deployed on mobile applications. To solve this problem, we propose an efficient architecture CPGAN for text-to-image generative adversarial networks (GAN) based on canonical polyadic decomposition (CPD). It is a general method to design the lightweight architecture of text-to-image GAN. To improve the stability of CPGAN, we introduce conditioning augmentation and the idea of autoencoder during the training process. Experimental results prove that our architecture CPGAN can maintain the quality of generated images and reduce at least 20% parameters and flops.",
      "intriguing_abstract": "Text-to-image synthesis is an important and challenging application of computer vision. Many interesting and meaningful text-to-image synthesis models have been put forward. However, most of the works pay attention to the quality of synthesis images, but rarely consider the size of these models. Large models contain many parameters and high delay, which makes it difficult to be deployed on mobile applications. To solve this problem, we propose an efficient architecture CPGAN for text-to-image generative adversarial networks (GAN) based on canonical polyadic decomposition (CPD). It is a general method to design the lightweight architecture of text-to-image GAN. To improve the stability of CPGAN, we introduce conditioning augmentation and the idea of autoencoder during the training process. Experimental results prove that our architecture CPGAN can maintain the quality of generated images and reduce at least 20% parameters and flops.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/4c9f5b3891705ae5637fec5b57c85dff379bcde9.pdf",
      "citation_key": "ma2021w69",
      "metadata": {
        "title": "CPGAN : An Efficient Architecture Designing for Text-to-Image Generative Adversarial Networks Based on Canonical Polyadic Decomposition",
        "authors": [
          "Ruixin Ma",
          "Junying Lou"
        ],
        "published_date": "2021",
        "abstract": "Text-to-image synthesis is an important and challenging application of computer vision. Many interesting and meaningful text-to-image synthesis models have been put forward. However, most of the works pay attention to the quality of synthesis images, but rarely consider the size of these models. Large models contain many parameters and high delay, which makes it difficult to be deployed on mobile applications. To solve this problem, we propose an efficient architecture CPGAN for text-to-image generative adversarial networks (GAN) based on canonical polyadic decomposition (CPD). It is a general method to design the lightweight architecture of text-to-image GAN. To improve the stability of CPGAN, we introduce conditioning augmentation and the idea of autoencoder during the training process. Experimental results prove that our architecture CPGAN can maintain the quality of generated images and reduce at least 20% parameters and flops.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/4c9f5b3891705ae5637fec5b57c85dff379bcde9.pdf",
        "venue": "Scientific Programming",
        "citationCount": 6,
        "score": 1.5,
        "summary": "Text-to-image synthesis is an important and challenging application of computer vision. Many interesting and meaningful text-to-image synthesis models have been put forward. However, most of the works pay attention to the quality of synthesis images, but rarely consider the size of these models. Large models contain many parameters and high delay, which makes it difficult to be deployed on mobile applications. To solve this problem, we propose an efficient architecture CPGAN for text-to-image generative adversarial networks (GAN) based on canonical polyadic decomposition (CPD). It is a general method to design the lightweight architecture of text-to-image GAN. To improve the stability of CPGAN, we introduce conditioning augmentation and the idea of autoencoder during the training process. Experimental results prove that our architecture CPGAN can maintain the quality of generated images and reduce at least 20% parameters and flops.",
        "keywords": []
      },
      "file_name": "4c9f5b3891705ae5637fec5b57c85dff379bcde9.pdf"
    },
    {
      "success": true,
      "doc_id": "f94bdd13b105ae159a5e689653f2917e",
      "summary": "Popular neural network-based speech enhancement systems operate on the magnitude spectrogram and ignore the phase mismatch between the noisy and clean speech signals. Conditional generative adversarial networks (cGANs) show promise in addressing the phase mismatch problem by directly mapping the raw noisy speech waveform to the underlying clean speech signal. However, stabilizing and training cGAN systems is difficult and they still fall short of the performance achieved by the spectral enhancement approaches. This paper investigates whether different normalization strategies and one-sided label smoothing can further stabilize the cGAN-based speech enhancement model. In addition, we propose incorporating a Gammatone-based auditory filtering layer and a trainable pre-emphasis layer to further improve the performance of the cGAN framework. Simulation results show that the proposed approaches improve the speech enhancement performance of cGAN systems in addition to yielding improved stability and reduced computational effort.",
      "intriguing_abstract": "Popular neural network-based speech enhancement systems operate on the magnitude spectrogram and ignore the phase mismatch between the noisy and clean speech signals. Conditional generative adversarial networks (cGANs) show promise in addressing the phase mismatch problem by directly mapping the raw noisy speech waveform to the underlying clean speech signal. However, stabilizing and training cGAN systems is difficult and they still fall short of the performance achieved by the spectral enhancement approaches. This paper investigates whether different normalization strategies and one-sided label smoothing can further stabilize the cGAN-based speech enhancement model. In addition, we propose incorporating a Gammatone-based auditory filtering layer and a trainable pre-emphasis layer to further improve the performance of the cGAN framework. Simulation results show that the proposed approaches improve the speech enhancement performance of cGAN systems in addition to yielding improved stability and reduced computational effort.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/29bdd183402a94e3cca7531ced412bb427e9285a.pdf",
      "citation_key": "baby2020e5n",
      "metadata": {
        "title": "iSEGAN: Improved Speech Enhancement Generative Adversarial Networks",
        "authors": [
          "Deepak Baby"
        ],
        "published_date": "2020",
        "abstract": "Popular neural network-based speech enhancement systems operate on the magnitude spectrogram and ignore the phase mismatch between the noisy and clean speech signals. Conditional generative adversarial networks (cGANs) show promise in addressing the phase mismatch problem by directly mapping the raw noisy speech waveform to the underlying clean speech signal. However, stabilizing and training cGAN systems is difficult and they still fall short of the performance achieved by the spectral enhancement approaches. This paper investigates whether different normalization strategies and one-sided label smoothing can further stabilize the cGAN-based speech enhancement model. In addition, we propose incorporating a Gammatone-based auditory filtering layer and a trainable pre-emphasis layer to further improve the performance of the cGAN framework. Simulation results show that the proposed approaches improve the speech enhancement performance of cGAN systems in addition to yielding improved stability and reduced computational effort.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/29bdd183402a94e3cca7531ced412bb427e9285a.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 1.4000000000000001,
        "summary": "Popular neural network-based speech enhancement systems operate on the magnitude spectrogram and ignore the phase mismatch between the noisy and clean speech signals. Conditional generative adversarial networks (cGANs) show promise in addressing the phase mismatch problem by directly mapping the raw noisy speech waveform to the underlying clean speech signal. However, stabilizing and training cGAN systems is difficult and they still fall short of the performance achieved by the spectral enhancement approaches. This paper investigates whether different normalization strategies and one-sided label smoothing can further stabilize the cGAN-based speech enhancement model. In addition, we propose incorporating a Gammatone-based auditory filtering layer and a trainable pre-emphasis layer to further improve the performance of the cGAN framework. Simulation results show that the proposed approaches improve the speech enhancement performance of cGAN systems in addition to yielding improved stability and reduced computational effort.",
        "keywords": []
      },
      "file_name": "29bdd183402a94e3cca7531ced412bb427e9285a.pdf"
    },
    {
      "success": true,
      "doc_id": "0213a5293dc0121bb3c5cbf8dbe0b12d",
      "summary": "We propose a stable, parallel approach to train Wasserstein conditional generative adversarial neural networks (W-CGANs) under the constraint of a fixed computational budget. Differently from previous distributed GANs training techniques, our approach avoids inter-process communications, reduces the risk of mode collapse and enhances scalability by using multiple generators, each one of them concurrently trained on a single data label. The use of the Wasserstein metric also reduces the risk of cycling by stabilizing the training of each generator. We illustrate the approach on the CIFAR10, CIFAR100, and ImageNet1k datasets, three standard benchmark image datasets, maintaining the original resolution of the images for each dataset. Performance is assessed in terms of scalability and final accuracy within a limited fixed computational time and computational resources. To measure accuracy, we use the inception score, the Fréchet inception distance, and image quality. An improvement in inception score and Fréchet inception distance is shown in comparison to previous results obtained by performing the parallel approach on deep convolutional conditional generative adversarial neural networks as well as an improvement of image quality of the new images created by the GANs approach. Weak scaling is attained on both datasets using up to 2000 NVIDIA V100 GPUs on the OLCF supercomputer Summit.",
      "intriguing_abstract": "We propose a stable, parallel approach to train Wasserstein conditional generative adversarial neural networks (W-CGANs) under the constraint of a fixed computational budget. Differently from previous distributed GANs training techniques, our approach avoids inter-process communications, reduces the risk of mode collapse and enhances scalability by using multiple generators, each one of them concurrently trained on a single data label. The use of the Wasserstein metric also reduces the risk of cycling by stabilizing the training of each generator. We illustrate the approach on the CIFAR10, CIFAR100, and ImageNet1k datasets, three standard benchmark image datasets, maintaining the original resolution of the images for each dataset. Performance is assessed in terms of scalability and final accuracy within a limited fixed computational time and computational resources. To measure accuracy, we use the inception score, the Fréchet inception distance, and image quality. An improvement in inception score and Fréchet inception distance is shown in comparison to previous results obtained by performing the parallel approach on deep convolutional conditional generative adversarial neural networks as well as an improvement of image quality of the new images created by the GANs approach. Weak scaling is attained on both datasets using up to 2000 NVIDIA V100 GPUs on the OLCF supercomputer Summit.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/bd8b0558c9d72fa09f849768879777f04599f7d4.pdf",
      "citation_key": "pasini2021ta3",
      "metadata": {
        "title": "Stable parallel training of Wasserstein conditional generative adversarial neural networks",
        "authors": [
          "Massimiliano Lupo Pasini",
          "Junqi Yin"
        ],
        "published_date": "2021",
        "abstract": "We propose a stable, parallel approach to train Wasserstein conditional generative adversarial neural networks (W-CGANs) under the constraint of a fixed computational budget. Differently from previous distributed GANs training techniques, our approach avoids inter-process communications, reduces the risk of mode collapse and enhances scalability by using multiple generators, each one of them concurrently trained on a single data label. The use of the Wasserstein metric also reduces the risk of cycling by stabilizing the training of each generator. We illustrate the approach on the CIFAR10, CIFAR100, and ImageNet1k datasets, three standard benchmark image datasets, maintaining the original resolution of the images for each dataset. Performance is assessed in terms of scalability and final accuracy within a limited fixed computational time and computational resources. To measure accuracy, we use the inception score, the Fréchet inception distance, and image quality. An improvement in inception score and Fréchet inception distance is shown in comparison to previous results obtained by performing the parallel approach on deep convolutional conditional generative adversarial neural networks as well as an improvement of image quality of the new images created by the GANs approach. Weak scaling is attained on both datasets using up to 2000 NVIDIA V100 GPUs on the OLCF supercomputer Summit.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/bd8b0558c9d72fa09f849768879777f04599f7d4.pdf",
        "venue": "2021 International Conference on Computational Science and Computational Intelligence (CSCI)",
        "citationCount": 5,
        "score": 1.25,
        "summary": "We propose a stable, parallel approach to train Wasserstein conditional generative adversarial neural networks (W-CGANs) under the constraint of a fixed computational budget. Differently from previous distributed GANs training techniques, our approach avoids inter-process communications, reduces the risk of mode collapse and enhances scalability by using multiple generators, each one of them concurrently trained on a single data label. The use of the Wasserstein metric also reduces the risk of cycling by stabilizing the training of each generator. We illustrate the approach on the CIFAR10, CIFAR100, and ImageNet1k datasets, three standard benchmark image datasets, maintaining the original resolution of the images for each dataset. Performance is assessed in terms of scalability and final accuracy within a limited fixed computational time and computational resources. To measure accuracy, we use the inception score, the Fréchet inception distance, and image quality. An improvement in inception score and Fréchet inception distance is shown in comparison to previous results obtained by performing the parallel approach on deep convolutional conditional generative adversarial neural networks as well as an improvement of image quality of the new images created by the GANs approach. Weak scaling is attained on both datasets using up to 2000 NVIDIA V100 GPUs on the OLCF supercomputer Summit.",
        "keywords": []
      },
      "file_name": "bd8b0558c9d72fa09f849768879777f04599f7d4.pdf"
    },
    {
      "success": true,
      "doc_id": "2118efa30cf24e8f3043727b9573f24c",
      "summary": "Synthetic data are increasingly being recognized for their potential to address serious real-world challenges in various domains. They provide innovative solutions to combat the data scarcity, privacy concerns, and algorithmic biases commonly used in machine learning applications. Synthetic data preserve all underlying patterns and behaviors of the original dataset while altering the actual content. The methods proposed in the literature to generate synthetic data vary from large language models (LLMs), which are pre-trained on gigantic datasets, to generative adversarial networks (GANs) and variational autoencoders (VAEs). This study provides a systematic review of the various techniques proposed in the literature that can be used to generate synthetic data to identify their limitations and suggest potential future research areas. The findings indicate that while these technologies generate synthetic data of specific data types, they still have some drawbacks, such as computational requirements, training stability, and privacy-preserving measures which limit their real-world usability. Addressing these issues will facilitate the broader adoption of synthetic data generation techniques across various disciplines, thereby advancing machine learning and data-driven solutions.",
      "intriguing_abstract": "Synthetic data are increasingly being recognized for their potential to address serious real-world challenges in various domains. They provide innovative solutions to combat the data scarcity, privacy concerns, and algorithmic biases commonly used in machine learning applications. Synthetic data preserve all underlying patterns and behaviors of the original dataset while altering the actual content. The methods proposed in the literature to generate synthetic data vary from large language models (LLMs), which are pre-trained on gigantic datasets, to generative adversarial networks (GANs) and variational autoencoders (VAEs). This study provides a systematic review of the various techniques proposed in the literature that can be used to generate synthetic data to identify their limitations and suggest potential future research areas. The findings indicate that while these technologies generate synthetic data of specific data types, they still have some drawbacks, such as computational requirements, training stability, and privacy-preserving measures which limit their real-world usability. Addressing these issues will facilitate the broader adoption of synthetic data generation techniques across various disciplines, thereby advancing machine learning and data-driven solutions.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/4a1d533193d8e6607c381d231aaea06a5522622a.pdf",
      "citation_key": "goyal2024ufg",
      "metadata": {
        "title": "A Systematic Review of Synthetic Data Generation Techniques Using Generative AI",
        "authors": [
          "Mandeep Goyal",
          "Q. Mahmoud"
        ],
        "published_date": "2024",
        "abstract": "Synthetic data are increasingly being recognized for their potential to address serious real-world challenges in various domains. They provide innovative solutions to combat the data scarcity, privacy concerns, and algorithmic biases commonly used in machine learning applications. Synthetic data preserve all underlying patterns and behaviors of the original dataset while altering the actual content. The methods proposed in the literature to generate synthetic data vary from large language models (LLMs), which are pre-trained on gigantic datasets, to generative adversarial networks (GANs) and variational autoencoders (VAEs). This study provides a systematic review of the various techniques proposed in the literature that can be used to generate synthetic data to identify their limitations and suggest potential future research areas. The findings indicate that while these technologies generate synthetic data of specific data types, they still have some drawbacks, such as computational requirements, training stability, and privacy-preserving measures which limit their real-world usability. Addressing these issues will facilitate the broader adoption of synthetic data generation techniques across various disciplines, thereby advancing machine learning and data-driven solutions.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/4a1d533193d8e6607c381d231aaea06a5522622a.pdf",
        "venue": "Electronics",
        "citationCount": 95,
        "score": 95.0,
        "summary": "Synthetic data are increasingly being recognized for their potential to address serious real-world challenges in various domains. They provide innovative solutions to combat the data scarcity, privacy concerns, and algorithmic biases commonly used in machine learning applications. Synthetic data preserve all underlying patterns and behaviors of the original dataset while altering the actual content. The methods proposed in the literature to generate synthetic data vary from large language models (LLMs), which are pre-trained on gigantic datasets, to generative adversarial networks (GANs) and variational autoencoders (VAEs). This study provides a systematic review of the various techniques proposed in the literature that can be used to generate synthetic data to identify their limitations and suggest potential future research areas. The findings indicate that while these technologies generate synthetic data of specific data types, they still have some drawbacks, such as computational requirements, training stability, and privacy-preserving measures which limit their real-world usability. Addressing these issues will facilitate the broader adoption of synthetic data generation techniques across various disciplines, thereby advancing machine learning and data-driven solutions.",
        "keywords": []
      },
      "file_name": "4a1d533193d8e6607c381d231aaea06a5522622a.pdf"
    },
    {
      "success": true,
      "doc_id": "52790a46a09fb568f75ea864ab74e74e",
      "summary": "Computer network anomaly detection and log analysis, as an important topic in the field of network security, has been a key task to ensure network security and system reliability. First, existing network anomaly detection and log analysis methods are often challenged by high-dimensional data and complex network topologies, resulting in unstable performance and high false-positive rates. In addition, traditional methods are usually difficult to handle time-series data, which is crucial for anomaly detection and log analysis. Therefore, we need a more efficient and accurate method to cope with these problems. To compensate for the shortcomings of current methods, we propose an innovative fusion model that integrates Isolation Forest, GAN (Generative Adversarial Network), and Transformer with each other, and each of them plays a unique role. Isolation Forest is used to quickly identify anomalous data points, and GAN is used to generate synthetic data with the real data distribution characteristics to augment the training dataset, while the Transformer is used for modeling and context extraction on time series data. The synergy of these three components makes our model more accurate and robust in anomaly detection and log analysis tasks. We validate the effectiveness of this fusion model in an extensive experimental evaluation. Experimental results show that our model significantly improves the accuracy of anomaly detection while reducing the false alarm rate, which helps to detect potential network problems in advance. The model also performs well in the log analysis task and is able to quickly identify anomalous behaviors, which helps to improve the stability of the system. The significance of this study is that it introduces advanced deep learning techniques, which work anomaly detection and log analysis.",
      "intriguing_abstract": "Computer network anomaly detection and log analysis, as an important topic in the field of network security, has been a key task to ensure network security and system reliability. First, existing network anomaly detection and log analysis methods are often challenged by high-dimensional data and complex network topologies, resulting in unstable performance and high false-positive rates. In addition, traditional methods are usually difficult to handle time-series data, which is crucial for anomaly detection and log analysis. Therefore, we need a more efficient and accurate method to cope with these problems. To compensate for the shortcomings of current methods, we propose an innovative fusion model that integrates Isolation Forest, GAN (Generative Adversarial Network), and Transformer with each other, and each of them plays a unique role. Isolation Forest is used to quickly identify anomalous data points, and GAN is used to generate synthetic data with the real data distribution characteristics to augment the training dataset, while the Transformer is used for modeling and context extraction on time series data. The synergy of these three components makes our model more accurate and robust in anomaly detection and log analysis tasks. We validate the effectiveness of this fusion model in an extensive experimental evaluation. Experimental results show that our model significantly improves the accuracy of anomaly detection while reducing the false alarm rate, which helps to detect potential network problems in advance. The model also performs well in the log analysis task and is able to quickly identify anomalous behaviors, which helps to improve the stability of the system. The significance of this study is that it introduces advanced deep learning techniques, which work anomaly detection and log analysis.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/9d240b5aa22e310b31d52afd729a1195390da871.pdf",
      "citation_key": "wang2024v83",
      "metadata": {
        "title": "Deep Learning-based Anomaly Detection and Log Analysis for Computer Networks",
        "authors": [
          "Shuzhan Wang",
          "Ruxue Jiang",
          "Zhaoqi Wang",
          "Yan Zhou"
        ],
        "published_date": "2024",
        "abstract": "Computer network anomaly detection and log analysis, as an important topic in the field of network security, has been a key task to ensure network security and system reliability. First, existing network anomaly detection and log analysis methods are often challenged by high-dimensional data and complex network topologies, resulting in unstable performance and high false-positive rates. In addition, traditional methods are usually difficult to handle time-series data, which is crucial for anomaly detection and log analysis. Therefore, we need a more efficient and accurate method to cope with these problems. To compensate for the shortcomings of current methods, we propose an innovative fusion model that integrates Isolation Forest, GAN (Generative Adversarial Network), and Transformer with each other, and each of them plays a unique role. Isolation Forest is used to quickly identify anomalous data points, and GAN is used to generate synthetic data with the real data distribution characteristics to augment the training dataset, while the Transformer is used for modeling and context extraction on time series data. The synergy of these three components makes our model more accurate and robust in anomaly detection and log analysis tasks. We validate the effectiveness of this fusion model in an extensive experimental evaluation. Experimental results show that our model significantly improves the accuracy of anomaly detection while reducing the false alarm rate, which helps to detect potential network problems in advance. The model also performs well in the log analysis task and is able to quickly identify anomalous behaviors, which helps to improve the stability of the system. The significance of this study is that it introduces advanced deep learning techniques, which work anomaly detection and log analysis.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/9d240b5aa22e310b31d52afd729a1195390da871.pdf",
        "venue": "arXiv.org",
        "citationCount": 36,
        "score": 36.0,
        "summary": "Computer network anomaly detection and log analysis, as an important topic in the field of network security, has been a key task to ensure network security and system reliability. First, existing network anomaly detection and log analysis methods are often challenged by high-dimensional data and complex network topologies, resulting in unstable performance and high false-positive rates. In addition, traditional methods are usually difficult to handle time-series data, which is crucial for anomaly detection and log analysis. Therefore, we need a more efficient and accurate method to cope with these problems. To compensate for the shortcomings of current methods, we propose an innovative fusion model that integrates Isolation Forest, GAN (Generative Adversarial Network), and Transformer with each other, and each of them plays a unique role. Isolation Forest is used to quickly identify anomalous data points, and GAN is used to generate synthetic data with the real data distribution characteristics to augment the training dataset, while the Transformer is used for modeling and context extraction on time series data. The synergy of these three components makes our model more accurate and robust in anomaly detection and log analysis tasks. We validate the effectiveness of this fusion model in an extensive experimental evaluation. Experimental results show that our model significantly improves the accuracy of anomaly detection while reducing the false alarm rate, which helps to detect potential network problems in advance. The model also performs well in the log analysis task and is able to quickly identify anomalous behaviors, which helps to improve the stability of the system. The significance of this study is that it introduces advanced deep learning techniques, which work anomaly detection and log analysis.",
        "keywords": []
      },
      "file_name": "9d240b5aa22e310b31d52afd729a1195390da871.pdf"
    },
    {
      "success": true,
      "doc_id": "f17f8cf1901697081704a3fd634c9d28",
      "summary": "The rise of industrial progress has advanced the growth of deep learning (DL)-driven smart fault diagnosis techniques, particularly for condition-based maintenance (CBM). However, the training of these DL methods relies on large dataset, which is unrealistic to collect because fault signal is not practically viable in real case. To address this issue, this article proposes a conditional auxiliary classier cycle-consistent generative adversarial network restrained by Wasserstein distance with gradient penalty (CAC-CycleGAN-WGP). This model can generate superior-quality signals of the minority classes with stability from majority class. In the experimental section, a stacked autoencoder (AE)-based evaluator is proposed to evaluate the quality of these generated sample, and then imbalanced fault diagnosis is conducted at varying balance ratios based on two benchmarked datasets. The outcomes indicate that the proposed approach is adept at generating fault signals, leading to a notable enhancement in fault diagnosis accuracy as the generated samples are added. Additionally, the efficacy of the proposed framework was benchmarked against other commonly employed techniques. Among them, CAC-CycleGAN-WGP stands out with superior performance.",
      "intriguing_abstract": "The rise of industrial progress has advanced the growth of deep learning (DL)-driven smart fault diagnosis techniques, particularly for condition-based maintenance (CBM). However, the training of these DL methods relies on large dataset, which is unrealistic to collect because fault signal is not practically viable in real case. To address this issue, this article proposes a conditional auxiliary classier cycle-consistent generative adversarial network restrained by Wasserstein distance with gradient penalty (CAC-CycleGAN-WGP). This model can generate superior-quality signals of the minority classes with stability from majority class. In the experimental section, a stacked autoencoder (AE)-based evaluator is proposed to evaluate the quality of these generated sample, and then imbalanced fault diagnosis is conducted at varying balance ratios based on two benchmarked datasets. The outcomes indicate that the proposed approach is adept at generating fault signals, leading to a notable enhancement in fault diagnosis accuracy as the generated samples are added. Additionally, the efficacy of the proposed framework was benchmarked against other commonly employed techniques. Among them, CAC-CycleGAN-WGP stands out with superior performance.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/6fba827469a0cd3d090ec9898593445783ab484e.pdf",
      "citation_key": "liao20249ku",
      "metadata": {
        "title": "A Novel Approach for Intelligent Fault Diagnosis in Bearing With Imbalanced Data Based on Cycle-Consistent GAN",
        "authors": [
          "Wenjie Liao",
          "Like Wu",
          "Shihui Xu",
          "Shigeru Fujimura"
        ],
        "published_date": "2024",
        "abstract": "The rise of industrial progress has advanced the growth of deep learning (DL)-driven smart fault diagnosis techniques, particularly for condition-based maintenance (CBM). However, the training of these DL methods relies on large dataset, which is unrealistic to collect because fault signal is not practically viable in real case. To address this issue, this article proposes a conditional auxiliary classier cycle-consistent generative adversarial network restrained by Wasserstein distance with gradient penalty (CAC-CycleGAN-WGP). This model can generate superior-quality signals of the minority classes with stability from majority class. In the experimental section, a stacked autoencoder (AE)-based evaluator is proposed to evaluate the quality of these generated sample, and then imbalanced fault diagnosis is conducted at varying balance ratios based on two benchmarked datasets. The outcomes indicate that the proposed approach is adept at generating fault signals, leading to a notable enhancement in fault diagnosis accuracy as the generated samples are added. Additionally, the efficacy of the proposed framework was benchmarked against other commonly employed techniques. Among them, CAC-CycleGAN-WGP stands out with superior performance.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/6fba827469a0cd3d090ec9898593445783ab484e.pdf",
        "venue": "IEEE Transactions on Instrumentation and Measurement",
        "citationCount": 16,
        "score": 16.0,
        "summary": "The rise of industrial progress has advanced the growth of deep learning (DL)-driven smart fault diagnosis techniques, particularly for condition-based maintenance (CBM). However, the training of these DL methods relies on large dataset, which is unrealistic to collect because fault signal is not practically viable in real case. To address this issue, this article proposes a conditional auxiliary classier cycle-consistent generative adversarial network restrained by Wasserstein distance with gradient penalty (CAC-CycleGAN-WGP). This model can generate superior-quality signals of the minority classes with stability from majority class. In the experimental section, a stacked autoencoder (AE)-based evaluator is proposed to evaluate the quality of these generated sample, and then imbalanced fault diagnosis is conducted at varying balance ratios based on two benchmarked datasets. The outcomes indicate that the proposed approach is adept at generating fault signals, leading to a notable enhancement in fault diagnosis accuracy as the generated samples are added. Additionally, the efficacy of the proposed framework was benchmarked against other commonly employed techniques. Among them, CAC-CycleGAN-WGP stands out with superior performance.",
        "keywords": []
      },
      "file_name": "6fba827469a0cd3d090ec9898593445783ab484e.pdf"
    },
    {
      "success": true,
      "doc_id": "60704a3765badbd05b3f9f9368f4f0ef",
      "summary": "In the field of artificial intelligence, image-generation techniques have been a hotspot for research. Two generative models that have garnered a lot of attention are diffusion models and generative adversarial networks (GANs). This review paper aims to compare and analyze GAN and Diffusion Models in the field of picture generation, as well as to give a thorough discussion of their features, applications, benefits, and drawbacks. Firstly, the work related to the working principle of GAN and diffusion models are introduced, and then their applications and results in image generation are reviewed. By comparing the existing research results, the author find that GAN performs well in generating realistic images but suffers from problems such as pattern collapse and unstable training, while the diffusion model has better stability and controllability. Combining the advantages of the two methods, this paper explores the possible fusion methods and looks forward to the future development direction in the field of image generation. These research results provide important references and insights to further enhance the level and application scope of image generation technology.",
      "intriguing_abstract": "In the field of artificial intelligence, image-generation techniques have been a hotspot for research. Two generative models that have garnered a lot of attention are diffusion models and generative adversarial networks (GANs). This review paper aims to compare and analyze GAN and Diffusion Models in the field of picture generation, as well as to give a thorough discussion of their features, applications, benefits, and drawbacks. Firstly, the work related to the working principle of GAN and diffusion models are introduced, and then their applications and results in image generation are reviewed. By comparing the existing research results, the author find that GAN performs well in generating realistic images but suffers from problems such as pattern collapse and unstable training, while the diffusion model has better stability and controllability. Combining the advantages of the two methods, this paper explores the possible fusion methods and looks forward to the future development direction in the field of image generation. These research results provide important references and insights to further enhance the level and application scope of image generation technology.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/15f5dc4ed2ee1152aea4ee1e042e80f96316377b.pdf",
      "citation_key": "peng2024kkw",
      "metadata": {
        "title": "A Comparative Analysis Between GAN and Diffusion Models in Image Generation",
        "authors": [
          "Yingying Peng"
        ],
        "published_date": "2024",
        "abstract": "In the field of artificial intelligence, image-generation techniques have been a hotspot for research. Two generative models that have garnered a lot of attention are diffusion models and generative adversarial networks (GANs). This review paper aims to compare and analyze GAN and Diffusion Models in the field of picture generation, as well as to give a thorough discussion of their features, applications, benefits, and drawbacks. Firstly, the work related to the working principle of GAN and diffusion models are introduced, and then their applications and results in image generation are reviewed. By comparing the existing research results, the author find that GAN performs well in generating realistic images but suffers from problems such as pattern collapse and unstable training, while the diffusion model has better stability and controllability. Combining the advantages of the two methods, this paper explores the possible fusion methods and looks forward to the future development direction in the field of image generation. These research results provide important references and insights to further enhance the level and application scope of image generation technology.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/15f5dc4ed2ee1152aea4ee1e042e80f96316377b.pdf",
        "venue": "Transactions on Computer Science and Intelligent Systems Research",
        "citationCount": 14,
        "score": 14.0,
        "summary": "In the field of artificial intelligence, image-generation techniques have been a hotspot for research. Two generative models that have garnered a lot of attention are diffusion models and generative adversarial networks (GANs). This review paper aims to compare and analyze GAN and Diffusion Models in the field of picture generation, as well as to give a thorough discussion of their features, applications, benefits, and drawbacks. Firstly, the work related to the working principle of GAN and diffusion models are introduced, and then their applications and results in image generation are reviewed. By comparing the existing research results, the author find that GAN performs well in generating realistic images but suffers from problems such as pattern collapse and unstable training, while the diffusion model has better stability and controllability. Combining the advantages of the two methods, this paper explores the possible fusion methods and looks forward to the future development direction in the field of image generation. These research results provide important references and insights to further enhance the level and application scope of image generation technology.",
        "keywords": []
      },
      "file_name": "15f5dc4ed2ee1152aea4ee1e042e80f96316377b.pdf"
    },
    {
      "success": true,
      "doc_id": "3fbec3d967106cb60aacb1f17d7ff474",
      "summary": "Recently, some works have tried to combine diffusion and Generative Adversarial Networks (GANs) to alleviate the computational cost of the iterative denoising inference in Diffusion Models (DMs). However, existing works in this line suffer from either training instability and mode collapse or subpar one-step generation learning efficiency. To address these issues, we introduce YOSO, a novel generative model designed for rapid, scalable, and high-fidelity one-step image synthesis with high training stability and mode coverage. Specifically, we smooth the adversarial divergence by the denoising generator itself, performing self-cooperative learning. We show that our method can serve as a one-step generation model training from scratch with competitive performance. Moreover, we extend our YOSO to one-step text-to-image generation based on pre-trained models by several effective training techniques (i.e., latent perceptual loss and latent discriminator for efficient training along with the latent DMs; the informative prior initialization (IPI), and the quick adaption stage for fixing the flawed noise scheduler). Experimental results show that YOSO achieves the state-of-the-art one-step generation performance even with Low-Rank Adaptation (LoRA) fine-tuning. In particular, we show that the YOSO-PixArt-$\\alpha$ can generate images in one step trained on 512 resolution, with the capability of adapting to 1024 resolution without extra explicit training, requiring only ~10 A800 days for fine-tuning. Our code is provided at https://github.com/Luo-Yihong/YOSO.",
      "intriguing_abstract": "Recently, some works have tried to combine diffusion and Generative Adversarial Networks (GANs) to alleviate the computational cost of the iterative denoising inference in Diffusion Models (DMs). However, existing works in this line suffer from either training instability and mode collapse or subpar one-step generation learning efficiency. To address these issues, we introduce YOSO, a novel generative model designed for rapid, scalable, and high-fidelity one-step image synthesis with high training stability and mode coverage. Specifically, we smooth the adversarial divergence by the denoising generator itself, performing self-cooperative learning. We show that our method can serve as a one-step generation model training from scratch with competitive performance. Moreover, we extend our YOSO to one-step text-to-image generation based on pre-trained models by several effective training techniques (i.e., latent perceptual loss and latent discriminator for efficient training along with the latent DMs; the informative prior initialization (IPI), and the quick adaption stage for fixing the flawed noise scheduler). Experimental results show that YOSO achieves the state-of-the-art one-step generation performance even with Low-Rank Adaptation (LoRA) fine-tuning. In particular, we show that the YOSO-PixArt-$\\alpha$ can generate images in one step trained on 512 resolution, with the capability of adapting to 1024 resolution without extra explicit training, requiring only ~10 A800 days for fine-tuning. Our code is provided at https://github.com/Luo-Yihong/YOSO.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/3c0b8a5ffcd3402d29d81614595a5dd6a9268072.pdf",
      "citation_key": "luo2024znt",
      "metadata": {
        "title": "You Only Sample Once: Taming One-Step Text-To-Image Synthesis by Self-Cooperative Diffusion GANs",
        "authors": [
          "Yihong Luo",
          "Xiaolong Chen",
          "Tianyang Hu",
          "Jing Tang"
        ],
        "published_date": "2024",
        "abstract": "Recently, some works have tried to combine diffusion and Generative Adversarial Networks (GANs) to alleviate the computational cost of the iterative denoising inference in Diffusion Models (DMs). However, existing works in this line suffer from either training instability and mode collapse or subpar one-step generation learning efficiency. To address these issues, we introduce YOSO, a novel generative model designed for rapid, scalable, and high-fidelity one-step image synthesis with high training stability and mode coverage. Specifically, we smooth the adversarial divergence by the denoising generator itself, performing self-cooperative learning. We show that our method can serve as a one-step generation model training from scratch with competitive performance. Moreover, we extend our YOSO to one-step text-to-image generation based on pre-trained models by several effective training techniques (i.e., latent perceptual loss and latent discriminator for efficient training along with the latent DMs; the informative prior initialization (IPI), and the quick adaption stage for fixing the flawed noise scheduler). Experimental results show that YOSO achieves the state-of-the-art one-step generation performance even with Low-Rank Adaptation (LoRA) fine-tuning. In particular, we show that the YOSO-PixArt-$\\alpha$ can generate images in one step trained on 512 resolution, with the capability of adapting to 1024 resolution without extra explicit training, requiring only ~10 A800 days for fine-tuning. Our code is provided at https://github.com/Luo-Yihong/YOSO.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/3c0b8a5ffcd3402d29d81614595a5dd6a9268072.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 14,
        "score": 14.0,
        "summary": "Recently, some works have tried to combine diffusion and Generative Adversarial Networks (GANs) to alleviate the computational cost of the iterative denoising inference in Diffusion Models (DMs). However, existing works in this line suffer from either training instability and mode collapse or subpar one-step generation learning efficiency. To address these issues, we introduce YOSO, a novel generative model designed for rapid, scalable, and high-fidelity one-step image synthesis with high training stability and mode coverage. Specifically, we smooth the adversarial divergence by the denoising generator itself, performing self-cooperative learning. We show that our method can serve as a one-step generation model training from scratch with competitive performance. Moreover, we extend our YOSO to one-step text-to-image generation based on pre-trained models by several effective training techniques (i.e., latent perceptual loss and latent discriminator for efficient training along with the latent DMs; the informative prior initialization (IPI), and the quick adaption stage for fixing the flawed noise scheduler). Experimental results show that YOSO achieves the state-of-the-art one-step generation performance even with Low-Rank Adaptation (LoRA) fine-tuning. In particular, we show that the YOSO-PixArt-$\\alpha$ can generate images in one step trained on 512 resolution, with the capability of adapting to 1024 resolution without extra explicit training, requiring only ~10 A800 days for fine-tuning. Our code is provided at https://github.com/Luo-Yihong/YOSO.",
        "keywords": []
      },
      "file_name": "3c0b8a5ffcd3402d29d81614595a5dd6a9268072.pdf"
    },
    {
      "success": true,
      "doc_id": "b34c693f73d93e646ef8ecb90fa09d44",
      "summary": "Induction motor serves as the indispensable component of industrial equipment, where the failure of the key part can lead to significant losses, and its intelligent fault diagnosis (IFD) based on supervised deep learning methods requires extensive labeled data. However, acquiring sufficient labeled samples in industrial equipment usually remains a significant challenge. To tackle this problem, a novel semi-supervised IFD framework based on an improved unsupervised generative adversarial network (GAN) and fine-tuning is proposed. Specifically, an enhanced learning strategy is developed by introducing gradient normalization constraint and a hinge loss function into the modified GAN model to stabilize the adversarial training process and improve the discrimination, which can boost the model’s feature distribution learning ability among various unlabeled data categories. The discriminator trained by unlabeled samples is then employed to identify the fault after being fine-tuned with a limited number of labeled samples. Finally, the effectiveness of the proposed framework is verified by two induction motor cases, including mechanical and electrical failures. The results demonstrate that the proposed method has significant superiorities in training stability and identification accuracy under the condition of very few labeled samples.",
      "intriguing_abstract": "Induction motor serves as the indispensable component of industrial equipment, where the failure of the key part can lead to significant losses, and its intelligent fault diagnosis (IFD) based on supervised deep learning methods requires extensive labeled data. However, acquiring sufficient labeled samples in industrial equipment usually remains a significant challenge. To tackle this problem, a novel semi-supervised IFD framework based on an improved unsupervised generative adversarial network (GAN) and fine-tuning is proposed. Specifically, an enhanced learning strategy is developed by introducing gradient normalization constraint and a hinge loss function into the modified GAN model to stabilize the adversarial training process and improve the discrimination, which can boost the model’s feature distribution learning ability among various unlabeled data categories. The discriminator trained by unlabeled samples is then employed to identify the fault after being fine-tuned with a limited number of labeled samples. Finally, the effectiveness of the proposed framework is verified by two induction motor cases, including mechanical and electrical failures. The results demonstrate that the proposed method has significant superiorities in training stability and identification accuracy under the condition of very few labeled samples.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/cee85f664275dc55612b465d89003d946056e02d.pdf",
      "citation_key": "chen2024ajr",
      "metadata": {
        "title": "Unsupervised GAN With Fine-Tuning: A Novel Framework for Induction Motor Fault Diagnosis in Scarcely Labeled Sample Scenarios",
        "authors": [
          "Xin Chen",
          "Zaigang Chen",
          "Shiqian Chen",
          "Liming Wang",
          "Wanming Zhai"
        ],
        "published_date": "2024",
        "abstract": "Induction motor serves as the indispensable component of industrial equipment, where the failure of the key part can lead to significant losses, and its intelligent fault diagnosis (IFD) based on supervised deep learning methods requires extensive labeled data. However, acquiring sufficient labeled samples in industrial equipment usually remains a significant challenge. To tackle this problem, a novel semi-supervised IFD framework based on an improved unsupervised generative adversarial network (GAN) and fine-tuning is proposed. Specifically, an enhanced learning strategy is developed by introducing gradient normalization constraint and a hinge loss function into the modified GAN model to stabilize the adversarial training process and improve the discrimination, which can boost the model’s feature distribution learning ability among various unlabeled data categories. The discriminator trained by unlabeled samples is then employed to identify the fault after being fine-tuned with a limited number of labeled samples. Finally, the effectiveness of the proposed framework is verified by two induction motor cases, including mechanical and electrical failures. The results demonstrate that the proposed method has significant superiorities in training stability and identification accuracy under the condition of very few labeled samples.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/cee85f664275dc55612b465d89003d946056e02d.pdf",
        "venue": "IEEE Transactions on Instrumentation and Measurement",
        "citationCount": 9,
        "score": 9.0,
        "summary": "Induction motor serves as the indispensable component of industrial equipment, where the failure of the key part can lead to significant losses, and its intelligent fault diagnosis (IFD) based on supervised deep learning methods requires extensive labeled data. However, acquiring sufficient labeled samples in industrial equipment usually remains a significant challenge. To tackle this problem, a novel semi-supervised IFD framework based on an improved unsupervised generative adversarial network (GAN) and fine-tuning is proposed. Specifically, an enhanced learning strategy is developed by introducing gradient normalization constraint and a hinge loss function into the modified GAN model to stabilize the adversarial training process and improve the discrimination, which can boost the model’s feature distribution learning ability among various unlabeled data categories. The discriminator trained by unlabeled samples is then employed to identify the fault after being fine-tuned with a limited number of labeled samples. Finally, the effectiveness of the proposed framework is verified by two induction motor cases, including mechanical and electrical failures. The results demonstrate that the proposed method has significant superiorities in training stability and identification accuracy under the condition of very few labeled samples.",
        "keywords": []
      },
      "file_name": "cee85f664275dc55612b465d89003d946056e02d.pdf"
    },
    {
      "success": true,
      "doc_id": "b1fb5b7052a4d85d7522181ed5939d61",
      "summary": "The bearing fault diagnosis based on deep learning algorithms requires a substantial amount of data. However, in practical industrial production, the diagnostic algorithms tend to work ineffectively due to the limitations of samples. Therefore, in this article, we propose an improved method of deep convolutional generative adversarial networks (DCGAN) with discriminator gradient gap regularization (IDIG-GAN), which can effectively solve the problems of unstable training and poor training performance under a small sample dataset. Firstly, the self-attention mechanism is integrated into the DCGAN to capture global information to enhance the generalization capability of the network. Moreover, gradient normalization is applied to the discriminator to address the problem of vanishing gradients in the network. Furthermore, gradient gap regularization is incorporated into the loss function to narrow the gap between the discriminator gradient norms, thereby improving network stability when dealing with small fault datasets. Through training with the improved IDIG-GAN, then the generated samples are used to expand the dataset and construct a fault diagnosis model. By verifying under two bearing datasets, the results demonstrate that the proposed method can generate high-quality samples and effectively enhance the diagnostic capability of the network when working with small datasets.",
      "intriguing_abstract": "The bearing fault diagnosis based on deep learning algorithms requires a substantial amount of data. However, in practical industrial production, the diagnostic algorithms tend to work ineffectively due to the limitations of samples. Therefore, in this article, we propose an improved method of deep convolutional generative adversarial networks (DCGAN) with discriminator gradient gap regularization (IDIG-GAN), which can effectively solve the problems of unstable training and poor training performance under a small sample dataset. Firstly, the self-attention mechanism is integrated into the DCGAN to capture global information to enhance the generalization capability of the network. Moreover, gradient normalization is applied to the discriminator to address the problem of vanishing gradients in the network. Furthermore, gradient gap regularization is incorporated into the loss function to narrow the gap between the discriminator gradient norms, thereby improving network stability when dealing with small fault datasets. Through training with the improved IDIG-GAN, then the generated samples are used to expand the dataset and construct a fault diagnosis model. By verifying under two bearing datasets, the results demonstrate that the proposed method can generate high-quality samples and effectively enhance the diagnostic capability of the network when working with small datasets.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/bfb9ec2a6583e6e86716d12f1273e97626bbe5f0.pdf",
      "citation_key": "song2024htg",
      "metadata": {
        "title": "Rolling bearing fault diagnosis in electric motors based on IDIG-GAN under small sample conditions",
        "authors": [
          "Xiangjin Song",
          "Zhicheng Liu",
          "Zhaowei Wang"
        ],
        "published_date": "2024",
        "abstract": "The bearing fault diagnosis based on deep learning algorithms requires a substantial amount of data. However, in practical industrial production, the diagnostic algorithms tend to work ineffectively due to the limitations of samples. Therefore, in this article, we propose an improved method of deep convolutional generative adversarial networks (DCGAN) with discriminator gradient gap regularization (IDIG-GAN), which can effectively solve the problems of unstable training and poor training performance under a small sample dataset. Firstly, the self-attention mechanism is integrated into the DCGAN to capture global information to enhance the generalization capability of the network. Moreover, gradient normalization is applied to the discriminator to address the problem of vanishing gradients in the network. Furthermore, gradient gap regularization is incorporated into the loss function to narrow the gap between the discriminator gradient norms, thereby improving network stability when dealing with small fault datasets. Through training with the improved IDIG-GAN, then the generated samples are used to expand the dataset and construct a fault diagnosis model. By verifying under two bearing datasets, the results demonstrate that the proposed method can generate high-quality samples and effectively enhance the diagnostic capability of the network when working with small datasets.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/bfb9ec2a6583e6e86716d12f1273e97626bbe5f0.pdf",
        "venue": "Measurement science and technology",
        "citationCount": 8,
        "score": 8.0,
        "summary": "The bearing fault diagnosis based on deep learning algorithms requires a substantial amount of data. However, in practical industrial production, the diagnostic algorithms tend to work ineffectively due to the limitations of samples. Therefore, in this article, we propose an improved method of deep convolutional generative adversarial networks (DCGAN) with discriminator gradient gap regularization (IDIG-GAN), which can effectively solve the problems of unstable training and poor training performance under a small sample dataset. Firstly, the self-attention mechanism is integrated into the DCGAN to capture global information to enhance the generalization capability of the network. Moreover, gradient normalization is applied to the discriminator to address the problem of vanishing gradients in the network. Furthermore, gradient gap regularization is incorporated into the loss function to narrow the gap between the discriminator gradient norms, thereby improving network stability when dealing with small fault datasets. Through training with the improved IDIG-GAN, then the generated samples are used to expand the dataset and construct a fault diagnosis model. By verifying under two bearing datasets, the results demonstrate that the proposed method can generate high-quality samples and effectively enhance the diagnostic capability of the network when working with small datasets.",
        "keywords": []
      },
      "file_name": "bfb9ec2a6583e6e86716d12f1273e97626bbe5f0.pdf"
    },
    {
      "success": true,
      "doc_id": "debfcc44311c9af4caebada44a888b3c",
      "summary": "Fault data under real operating conditions are often difficult to collect, making the number of trained fault data small and out of proportion to normal data. Thus, fault diagnosis symmetry (balance) is compromised. This will result in less effective fault diagnosis methods for cases with a small number of data and data imbalances (S&I). We present an innovative solution to overcome this problem, which is composed of two components: data augmentation and fault diagnosis. In the data augmentation section, the S&I dataset is supplemented with a deep convolutional generative adversarial network based on a gradient penalty and Wasserstein distance (WDCGAN-GP), which solve the problems of the generative adversarial network (GAN) being prone to model collapse and the gradient vanishing during the training time. The addition of self-attention allows for a better identification and generation of sample features. Finally, the addition of spectral normalization can stabilize the training of the model. In the fault diagnosis section, fault diagnosis is performed through a convolutional neural network with coordinate attention (CNN-CA). Our experiments conducted on two bearing fault datasets for comparison demonstrate that the proposed method surpasses other comparative approaches in terms of the quality of data augmentation and the accuracy of fault diagnosis. It effectively addresses S&I fault diagnosis challenges.",
      "intriguing_abstract": "Fault data under real operating conditions are often difficult to collect, making the number of trained fault data small and out of proportion to normal data. Thus, fault diagnosis symmetry (balance) is compromised. This will result in less effective fault diagnosis methods for cases with a small number of data and data imbalances (S&I). We present an innovative solution to overcome this problem, which is composed of two components: data augmentation and fault diagnosis. In the data augmentation section, the S&I dataset is supplemented with a deep convolutional generative adversarial network based on a gradient penalty and Wasserstein distance (WDCGAN-GP), which solve the problems of the generative adversarial network (GAN) being prone to model collapse and the gradient vanishing during the training time. The addition of self-attention allows for a better identification and generation of sample features. Finally, the addition of spectral normalization can stabilize the training of the model. In the fault diagnosis section, fault diagnosis is performed through a convolutional neural network with coordinate attention (CNN-CA). Our experiments conducted on two bearing fault datasets for comparison demonstrate that the proposed method surpasses other comparative approaches in terms of the quality of data augmentation and the accuracy of fault diagnosis. It effectively addresses S&I fault diagnosis challenges.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/70f107e0c675dcac8fd9a4f9a11fd78c1ae8700f.pdf",
      "citation_key": "qin2024a4b",
      "metadata": {
        "title": "Improved Generative Adversarial Network for Bearing Fault Diagnosis with a Small Number of Data and Unbalanced Data",
        "authors": [
          "Zhaohui Qin",
          "Faguo Huang",
          "Jiafang Pan",
          "Junlin Niu",
          "Haihua Qin"
        ],
        "published_date": "2024",
        "abstract": "Fault data under real operating conditions are often difficult to collect, making the number of trained fault data small and out of proportion to normal data. Thus, fault diagnosis symmetry (balance) is compromised. This will result in less effective fault diagnosis methods for cases with a small number of data and data imbalances (S&I). We present an innovative solution to overcome this problem, which is composed of two components: data augmentation and fault diagnosis. In the data augmentation section, the S&I dataset is supplemented with a deep convolutional generative adversarial network based on a gradient penalty and Wasserstein distance (WDCGAN-GP), which solve the problems of the generative adversarial network (GAN) being prone to model collapse and the gradient vanishing during the training time. The addition of self-attention allows for a better identification and generation of sample features. Finally, the addition of spectral normalization can stabilize the training of the model. In the fault diagnosis section, fault diagnosis is performed through a convolutional neural network with coordinate attention (CNN-CA). Our experiments conducted on two bearing fault datasets for comparison demonstrate that the proposed method surpasses other comparative approaches in terms of the quality of data augmentation and the accuracy of fault diagnosis. It effectively addresses S&I fault diagnosis challenges.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/70f107e0c675dcac8fd9a4f9a11fd78c1ae8700f.pdf",
        "venue": "Symmetry",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Fault data under real operating conditions are often difficult to collect, making the number of trained fault data small and out of proportion to normal data. Thus, fault diagnosis symmetry (balance) is compromised. This will result in less effective fault diagnosis methods for cases with a small number of data and data imbalances (S&I). We present an innovative solution to overcome this problem, which is composed of two components: data augmentation and fault diagnosis. In the data augmentation section, the S&I dataset is supplemented with a deep convolutional generative adversarial network based on a gradient penalty and Wasserstein distance (WDCGAN-GP), which solve the problems of the generative adversarial network (GAN) being prone to model collapse and the gradient vanishing during the training time. The addition of self-attention allows for a better identification and generation of sample features. Finally, the addition of spectral normalization can stabilize the training of the model. In the fault diagnosis section, fault diagnosis is performed through a convolutional neural network with coordinate attention (CNN-CA). Our experiments conducted on two bearing fault datasets for comparison demonstrate that the proposed method surpasses other comparative approaches in terms of the quality of data augmentation and the accuracy of fault diagnosis. It effectively addresses S&I fault diagnosis challenges.",
        "keywords": []
      },
      "file_name": "70f107e0c675dcac8fd9a4f9a11fd78c1ae8700f.pdf"
    },
    {
      "success": true,
      "doc_id": "d7c6cf4c9708d8382d89d7c894d2e19a",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Electroencephalography (EEG) signals are frequently contaminated by substantial noise and interference, which can obscure crucial clinically and scientifically relevant features.\n    *   **Importance and Challenge:** Traditional denoising methods (e.g., linear filtering, wavelet thresholding) are often ineffective against nonlinear or time-varying artifacts. The unpredictable nature of real-world signal acquisition environments further degrades fixed or purely data-driven enhancement strategies, making robust and adaptive denoising challenging, especially when preserving subtle signal characteristics is critical \\cite{tibermacine2025pye}.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:** Previous work includes traditional signal processing (linear methods like LMS, nonlinear techniques like wavelet transform), deep neural networks (e.g., auto-encoders), and various GAN-based frameworks. Specific GAN applications include WSE-GAN for wireless signals, WGAN with Temporal-Spatial-Frequency (TSF) loss for EEG, AR-WGAN (WGAN-GP) for artifact removal, and conditional GANs (cGAN) for specific artifact types \\cite{tibermacine2025pye}.\n    *   **Limitations of Previous Solutions:** Traditional methods struggle with nonlinear EEG artifacts. While GANs show promise, there has been limited direct comparison between a standard GAN and a Wasserstein GAN with Gradient Penalty (WGAN-GP) under identical experimental conditions for EEG denoising. Most studies use limited metrics, hindering the evaluation of trade-offs between artifact removal effectiveness and signal fidelity. Practical considerations like computational overhead and performance across diverse EEG datasets also remain underexplored \\cite{tibermacine2025pye}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper explores a Generative Adversarial Network (GAN) framework for enhancing EEG signal quality, specifically comparing a conventional GAN model and a Wasserstein GAN with Gradient Penalty (WGAN-GP) \\cite{tibermacine2025pye}.\n    *   **Novelty/Difference:** The study's primary innovation lies in its systematic, head-to-head comparative analysis of standard GAN and WGAN-GP architectures for EEG denoising within the same experimental setting. This approach aims to elucidate the practical trade-off between aggressive noise suppression and high-fidelity signal reconstruction, providing clear guidelines for selecting the appropriate adversarial framework based on specific application needs \\cite{tibermacine2025pye}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Design and analysis of an adversarial pipeline for EEG noise suppression, demonstrating its capability to handle nonlinear distortions.\n    *   **System Design/Architectural Innovations:** A comprehensive evaluation framework comparing standard GAN and WGAN-GP performance in terms of noise suppression versus detail retention, offering nuanced guidance for their respective use-case scenarios.\n    *   **Theoretical Insights/Analysis:** The study highlights a practical trade-off between aggressive noise suppression and high-fidelity signal reconstruction, delineating how each model's strengths suit different real-world scenarios (e.g., clinical settings requiring subtle neural signal fidelity vs. high-interference environments favoring stronger artifact reduction) \\cite{tibermacine2025pye}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** The models were evaluated on two distinct EEG datasets: a \"healthy\" set (64-channel recordings from 109 volunteers during motor/imagery tasks) and an \"unhealthy\" set (18-channel recordings from 7 individuals with orthopedic impairments). Both datasets underwent comprehensive preprocessing, including band-pass filtering (8–30Hz), channel standardization, and artifact trimming \\cite{tibermacine2025pye}.\n    *   **Key Performance Metrics and Comparison Results:** Model evaluation used quantitative metrics such as signal-to-noise ratio (SNR), peak signal-to-noise ratio (PSNR), correlation coefficient, mutual information, dynamic time warping (DTW) distance, and relative root mean squared error (RRMSE).\n        *   Both adversarial frameworks *outperformed classical wavelet-based thresholding and linear filtering methods*, demonstrating superior adaptability to nonlinear distortions \\cite{tibermacine2025pye}.\n        *   WGAN-GP achieved higher SNR (up to 14.47dB compared to 12.37dB for standard GAN) and exhibited greater training stability (consistently lower RRMSE values) \\cite{tibermacine2025pye}.\n        *   The conventional GAN model excelled in preserving finer signal details, reflected in a PSNR of 19.28dB and a correlation coefficient exceeding 0.90 in several recordings \\cite{tibermacine2025pye}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The study identifies a practical trade-off between aggressive noise suppression (WGAN-GP) and high-fidelity signal reconstruction (standard GAN), implying that neither model is universally optimal for all scenarios.\n    *   **Scope of Applicability:** The findings are applicable to EEG denoising for various applications, from basic neuroscience research to real-time brain-computer interfaces (BCIs) in clinical or consumer-grade settings. The frameworks are suggested to be scalable to next-generation wireless networks and complex electrophysiological datasets \\cite{tibermacine2025pye}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** This work significantly advances the technical state-of-the-art by providing the first direct, systematic comparative analysis of standard GAN and WGAN-GP for EEG denoising under identical conditions. It clarifies their respective strengths and weaknesses, offering practical guidelines for model selection based on specific application requirements (e.g., prioritizing noise suppression vs. signal fidelity) \\cite{tibermacine2025pye}.\n    *   **Potential Impact:** The demonstrated improvements in signal quality underscore the promise of adversarially trained models for robust and dynamic solutions to long-standing challenges in EEG denoising. This can lead to more reliable neuroscience research, enhanced BCI performance, and informed deployment strategies in diverse real-world clinical or high-interference scenarios \\cite{tibermacine2025pye}.",
      "intriguing_abstract": "Electroencephalography (EEG) signals are notoriously susceptible to pervasive noise and nonlinear artifacts, obscuring critical neural information and hindering advancements in neuroscience and Brain-Computer Interfaces (BCIs). Traditional denoising methods often falter against these complex, time-varying distortions. This paper presents a novel, systematic head-to-head comparison of two powerful Generative Adversarial Network (GAN) architectures—a standard GAN and a Wasserstein GAN with Gradient Penalty (WGAN-GP)—for robust EEG signal enhancement.\n\nOur rigorous evaluation across diverse datasets reveals that both adversarial frameworks significantly outperform classical wavelet-based and linear filtering techniques. Crucially, we delineate a practical trade-off: WGAN-GP achieves superior noise suppression (up to 14.47dB SNR) and training stability, while the standard GAN excels in preserving subtle signal fidelity (PSNR 19.28dB). This unprecedented comparative analysis provides clear guidelines for selecting the optimal GAN architecture based on application-specific needs, paving the way for more reliable EEG analysis, enhanced BCI performance, and informed clinical deployment in high-interference environments.",
      "keywords": [
        "EEG denoising",
        "Generative Adversarial Networks (GANs)",
        "Wasserstein GAN with Gradient Penalty (WGAN-GP)",
        "nonlinear EEG artifacts",
        "systematic comparative analysis",
        "noise suppression",
        "signal fidelity",
        "trade-off analysis",
        "adversarial framework",
        "Brain-Computer Interfaces (BCIs)",
        "clinical applications",
        "quantitative performance metrics"
      ],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/3314863efff246ae64bb266dc920ae44afb24674.pdf",
      "citation_key": "tibermacine2025pye",
      "metadata": {
        "title": "Adversarial denoising of EEG signals: a comparative analysis of standard GAN and WGAN-GP approaches",
        "authors": [
          "Imad Eddine Tibermacine",
          "Samuele Russo",
          "Francesco Citeroni",
          "Giuseppe Mancini",
          "Abdelaziz Rabehi",
          "Amal H. Alharbi",
          "El-Sayed M. El-Kenawy",
          "Christian Napoli"
        ],
        "published_date": "2025",
        "abstract": "Introduction Electroencephalography (EEG) signals frequently contain substantial noise and interference, which can obscure clinically and scientifically relevant features. Traditional denoising approaches, such as linear filtering or wavelet thresholding, often struggle with nonlinear or time-varying artifacts. In response, the present study explores a Generative Adversarial Network (GAN) framework to enhance EEG signal quality, focusing on two variants: a conventional GAN model and a Wasserstein GAN with Gradient Penalty (WGAN-GP). Methods Data were obtained from two distinct EEG datasets: a “healthy” set of 64-channel recordings collected during various motor/imagery tasks, and an “unhealthy” set of 18-channel recordings from individuals with orthopedic impairments. Both datasets underwent comprehensive preprocessing, including band-pass filtering (8–30 Hz), channel standardization, and artifact trimming. The training stage involved adversarial learning, in which a generator sought to reconstruct clean EEG signals while a discriminator (or critic in the case of WGAN-GP) attempted to distinguish between real and generated signals. The model evaluation was conducted using quantitative metrics such as signal-to-noise ratio (SNR), peak signal-to-noise ratio (PSNR), correlation coefficient, mutual information, and dynamic time warping (DTW) distance. Results Experimental findings indicate that adversarial learning substantially improves EEG signal fidelity across multiple quantitative metrics. Specifically, WGAN-GP achieved an SNR of up to 14.47 dB (compared to 12.37 dB for the standard GAN) and exhibited greater training stability, as evidenced by consistently lower relative root mean squared error (RRMSE) values. In contrast, the conventional GAN model excelled in preserving finer signal details, reflected in a PSNR of 19.28 dB and a correlation coefficient exceeding 0.90 in several recordings. Both adversarial frameworks outperformed classical wavelet-based thresholding and linear filtering methods, demonstrating superior adaptability to nonlinear distortions and dynamic interference patterns in EEG time-series data. Discussion By systematically comparing standard GAN and WGAN-GP architectures, this study highlights a practical trade-off between aggressive noise suppression and high-fidelity signal reconstruction. The demonstrated improvements in signal quality underscore the promise of adversarially trained models for applications ranging from basic neuroscience research to real-time brain–computer interfaces (BCIs) in clinical or consumer-grade settings. The results further suggest that GAN-based frameworks can be easily scaled to next-generation wireless networks and complex electrophysiological datasets, offering robust and dynamic solutions to long-standing challenges in EEG denoising.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/3314863efff246ae64bb266dc920ae44afb24674.pdf",
        "venue": "Frontiers in Human Neuroscience",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Electroencephalography (EEG) signals are frequently contaminated by substantial noise and interference, which can obscure crucial clinically and scientifically relevant features.\n    *   **Importance and Challenge:** Traditional denoising methods (e.g., linear filtering, wavelet thresholding) are often ineffective against nonlinear or time-varying artifacts. The unpredictable nature of real-world signal acquisition environments further degrades fixed or purely data-driven enhancement strategies, making robust and adaptive denoising challenging, especially when preserving subtle signal characteristics is critical \\cite{tibermacine2025pye}.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:** Previous work includes traditional signal processing (linear methods like LMS, nonlinear techniques like wavelet transform), deep neural networks (e.g., auto-encoders), and various GAN-based frameworks. Specific GAN applications include WSE-GAN for wireless signals, WGAN with Temporal-Spatial-Frequency (TSF) loss for EEG, AR-WGAN (WGAN-GP) for artifact removal, and conditional GANs (cGAN) for specific artifact types \\cite{tibermacine2025pye}.\n    *   **Limitations of Previous Solutions:** Traditional methods struggle with nonlinear EEG artifacts. While GANs show promise, there has been limited direct comparison between a standard GAN and a Wasserstein GAN with Gradient Penalty (WGAN-GP) under identical experimental conditions for EEG denoising. Most studies use limited metrics, hindering the evaluation of trade-offs between artifact removal effectiveness and signal fidelity. Practical considerations like computational overhead and performance across diverse EEG datasets also remain underexplored \\cite{tibermacine2025pye}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper explores a Generative Adversarial Network (GAN) framework for enhancing EEG signal quality, specifically comparing a conventional GAN model and a Wasserstein GAN with Gradient Penalty (WGAN-GP) \\cite{tibermacine2025pye}.\n    *   **Novelty/Difference:** The study's primary innovation lies in its systematic, head-to-head comparative analysis of standard GAN and WGAN-GP architectures for EEG denoising within the same experimental setting. This approach aims to elucidate the practical trade-off between aggressive noise suppression and high-fidelity signal reconstruction, providing clear guidelines for selecting the appropriate adversarial framework based on specific application needs \\cite{tibermacine2025pye}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:** Design and analysis of an adversarial pipeline for EEG noise suppression, demonstrating its capability to handle nonlinear distortions.\n    *   **System Design/Architectural Innovations:** A comprehensive evaluation framework comparing standard GAN and WGAN-GP performance in terms of noise suppression versus detail retention, offering nuanced guidance for their respective use-case scenarios.\n    *   **Theoretical Insights/Analysis:** The study highlights a practical trade-off between aggressive noise suppression and high-fidelity signal reconstruction, delineating how each model's strengths suit different real-world scenarios (e.g., clinical settings requiring subtle neural signal fidelity vs. high-interference environments favoring stronger artifact reduction) \\cite{tibermacine2025pye}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** The models were evaluated on two distinct EEG datasets: a \"healthy\" set (64-channel recordings from 109 volunteers during motor/imagery tasks) and an \"unhealthy\" set (18-channel recordings from 7 individuals with orthopedic impairments). Both datasets underwent comprehensive preprocessing, including band-pass filtering (8–30Hz), channel standardization, and artifact trimming \\cite{tibermacine2025pye}.\n    *   **Key Performance Metrics and Comparison Results:** Model evaluation used quantitative metrics such as signal-to-noise ratio (SNR), peak signal-to-noise ratio (PSNR), correlation coefficient, mutual information, dynamic time warping (DTW) distance, and relative root mean squared error (RRMSE).\n        *   Both adversarial frameworks *outperformed classical wavelet-based thresholding and linear filtering methods*, demonstrating superior adaptability to nonlinear distortions \\cite{tibermacine2025pye}.\n        *   WGAN-GP achieved higher SNR (up to 14.47dB compared to 12.37dB for standard GAN) and exhibited greater training stability (consistently lower RRMSE values) \\cite{tibermacine2025pye}.\n        *   The conventional GAN model excelled in preserving finer signal details, reflected in a PSNR of 19.28dB and a correlation coefficient exceeding 0.90 in several recordings \\cite{tibermacine2025pye}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The study identifies a practical trade-off between aggressive noise suppression (WGAN-GP) and high-fidelity signal reconstruction (standard GAN), implying that neither model is universally optimal for all scenarios.\n    *   **Scope of Applicability:** The findings are applicable to EEG denoising for various applications, from basic neuroscience research to real-time brain-computer interfaces (BCIs) in clinical or consumer-grade settings. The frameworks are suggested to be scalable to next-generation wireless networks and complex electrophysiological datasets \\cite{tibermacine2025pye}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** This work significantly advances the technical state-of-the-art by providing the first direct, systematic comparative analysis of standard GAN and WGAN-GP for EEG denoising under identical conditions. It clarifies their respective strengths and weaknesses, offering practical guidelines for model selection based on specific application requirements (e.g., prioritizing noise suppression vs. signal fidelity) \\cite{tibermacine2025pye}.\n    *   **Potential Impact:** The demonstrated improvements in signal quality underscore the promise of adversarially trained models for robust and dynamic solutions to long-standing challenges in EEG denoising. This can lead to more reliable neuroscience research, enhanced BCI performance, and informed deployment strategies in diverse real-world clinical or high-interference scenarios \\cite{tibermacine2025pye}.",
        "keywords": [
          "EEG denoising",
          "Generative Adversarial Networks (GANs)",
          "Wasserstein GAN with Gradient Penalty (WGAN-GP)",
          "nonlinear EEG artifacts",
          "systematic comparative analysis",
          "noise suppression",
          "signal fidelity",
          "trade-off analysis",
          "adversarial framework",
          "Brain-Computer Interfaces (BCIs)",
          "clinical applications",
          "quantitative performance metrics"
        ],
        "paper_type": "the paper should be classified as **empirical**.\n\nhere's why:\n\n*   **abstract:** mentions \"typeoriginal research,\" which often implies empirical work. the title \"a comparative analysis of standard gan and wgan-gp approaches\" strongly suggests an experimental comparison.\n*   **introduction:**\n    *   \"the present study explores a generative adversarial network (gan) framework...\" - indicates a study being conducted.\n    *   **methods section (within the introduction):** explicitly details the data sources (\"two distinct eeg datasets\"), preprocessing steps, the training stage (\"adversarial learning\"), and the \"model evaluation was conducted using quantitative metrics such as signal-to-noise ratio (snr), peak signal-to-noise ratio (psnr), correlation coefficient, mutual information, and dynamic time warping (dtw) distance.\" these are all hallmarks of a data-driven study with statistical analysis.\n    *   **results section (starts):** \"experimental findings indicate...\" directly points to an empirical study.\n\nwhile the paper deals with \"technical\" methods (gans), its primary focus as described in the abstract and introduction is on conducting a *study* to *compare* and *evaluate* these methods using *data* and *quantitative metrics*, leading to \"experimental findings.\" this aligns perfectly with the definition of an empirical paper."
      },
      "file_name": "3314863efff246ae64bb266dc920ae44afb24674.pdf"
    },
    {
      "success": true,
      "doc_id": "a10881143916c9add8e185cdfe9a49da",
      "summary": "Generative adversarial network (GAN) models can synthesize high-quality audio signals while ensuring fast sample generation. However, they are difficult to train and are prone to several issues including mode collapse and divergence. In this paper, we introduce SpecDiff-GAN, a neural vocoder based on HiFi-GAN, which was initially devised for speech synthesis from mel spectrogram. In our model, the training stability is enhanced by means of a forward diffusion process which consists in injecting noise from a Gaussian distribution to both real and fake samples before inputting them to the discriminator. We further improve the model by exploiting a spectrally-shaped noise distribution with the aim to make the discriminator's task more challenging. We then show the merits of our proposed model for speech and music synthesis on several datasets. Our experiments confirm that our model compares favorably in audio quality and efficiency compared to several baselines.",
      "intriguing_abstract": "Generative adversarial network (GAN) models can synthesize high-quality audio signals while ensuring fast sample generation. However, they are difficult to train and are prone to several issues including mode collapse and divergence. In this paper, we introduce SpecDiff-GAN, a neural vocoder based on HiFi-GAN, which was initially devised for speech synthesis from mel spectrogram. In our model, the training stability is enhanced by means of a forward diffusion process which consists in injecting noise from a Gaussian distribution to both real and fake samples before inputting them to the discriminator. We further improve the model by exploiting a spectrally-shaped noise distribution with the aim to make the discriminator's task more challenging. We then show the merits of our proposed model for speech and music synthesis on several datasets. Our experiments confirm that our model compares favorably in audio quality and efficiency compared to several baselines.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/d3c97a9cae3cee24a66f42c5800e438290b7a8ea.pdf",
      "citation_key": "baoueb2024rlq",
      "metadata": {
        "title": "SpecDiff-GAN: A Spectrally-Shaped Noise Diffusion GAN for Speech and Music Synthesis",
        "authors": [
          "Teysir Baoueb",
          "Haocheng Liu",
          "Mathieu Fontaine",
          "Jonathan Le Roux",
          "G. Richard"
        ],
        "published_date": "2024",
        "abstract": "Generative adversarial network (GAN) models can synthesize high-quality audio signals while ensuring fast sample generation. However, they are difficult to train and are prone to several issues including mode collapse and divergence. In this paper, we introduce SpecDiff-GAN, a neural vocoder based on HiFi-GAN, which was initially devised for speech synthesis from mel spectrogram. In our model, the training stability is enhanced by means of a forward diffusion process which consists in injecting noise from a Gaussian distribution to both real and fake samples before inputting them to the discriminator. We further improve the model by exploiting a spectrally-shaped noise distribution with the aim to make the discriminator's task more challenging. We then show the merits of our proposed model for speech and music synthesis on several datasets. Our experiments confirm that our model compares favorably in audio quality and efficiency compared to several baselines.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/d3c97a9cae3cee24a66f42c5800e438290b7a8ea.pdf",
        "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Generative adversarial network (GAN) models can synthesize high-quality audio signals while ensuring fast sample generation. However, they are difficult to train and are prone to several issues including mode collapse and divergence. In this paper, we introduce SpecDiff-GAN, a neural vocoder based on HiFi-GAN, which was initially devised for speech synthesis from mel spectrogram. In our model, the training stability is enhanced by means of a forward diffusion process which consists in injecting noise from a Gaussian distribution to both real and fake samples before inputting them to the discriminator. We further improve the model by exploiting a spectrally-shaped noise distribution with the aim to make the discriminator's task more challenging. We then show the merits of our proposed model for speech and music synthesis on several datasets. Our experiments confirm that our model compares favorably in audio quality and efficiency compared to several baselines.",
        "keywords": []
      },
      "file_name": "d3c97a9cae3cee24a66f42c5800e438290b7a8ea.pdf"
    },
    {
      "success": true,
      "doc_id": "e2aa6f36ca3c3dea65754be1f8b2fd13",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis: A data-driven approach for the partial reconstruction of individual human molar teeth using generative deep learning \\cite{broll2024edy}\n\n1.  **Research Problem & Motivation**\n    *   The fabrication of fixed dental restorations (FDPs) is challenging due to the complex and unique morphology of individual human dentition and the masticatory system \\cite{broll2024edy}.\n    *   Current methods for designing FDPs, especially for smaller restorations, rely on functional analysis and mechanical/digital articulators, which are prone to errors due to the system's complexity and individuality \\cite{broll2024edy}.\n    *   These errors frequently lead to occlusal discomfort, temporomandibular disorders, FDP failure, and necessitate costly and time-consuming adaptation and reworking during insertion, reducing overall stability \\cite{broll2024edy}.\n\n2.  **Related Work & Positioning**\n    *   Existing commercial software and non-data-driven conventional reconstruction techniques (e.g., Bayesian maximum posterior probability, iterative deformation, morphing standard teeth) still face problems with reconstruction quality \\cite{broll2024edy}.\n    *   While deep learning is widely used for segmentation and classification in dentistry, generative network architectures for tooth generation and reconstruction are less explored \\cite{broll2024edy}.\n    *   Previous data-driven approaches often use 2D depth map representations with conditional GANs (e.g., pix2pix), but these are limited by data loss from undercuts and typically restricted to a small number of teeth (e.g., single or max three teeth) \\cite{broll2024edy}.\n    *   Some 3D data approaches use transformer networks with graph convolutional neural networks or 3D-GANs on point clouds or voxelized points, allowing for entire jaw input, but none have used complete upper and lower jaw data \\cite{broll2024edy}.\n    *   This work differentiates itself by decoupling the 2D-projection, teeth generation, and reconstruction components, allowing for modular updates \\cite{broll2024edy}.\n    *   Unlike prior inlay restoration networks (e.g., Tian et al., 2021), this method trains a generative network without a predefined target, enabling a downstream optimization-based reconstruction for arbitrary inlay geometries without retraining \\cite{broll2024edy}.\n    *   It uses Principal Component Analysis (PCA) for optimal 2D projection, maximizing spatial information, in contrast to optimizing projection parameters based on image entropy \\cite{broll2024edy}.\n\n3.  **Technical Approach & Innovation**\n    *   **Two-step Process**: The method involves a generating component (StyleGAN-2) and a reconstructing component (downstream optimization) \\cite{broll2024edy}.\n    *   **Generative Model**: StyleGAN-2 is trained on 3D mesh files of full dental crown restorations to learn the morphology of teeth and generate diverse occlusal surfaces \\cite{broll2024edy}.\n    *   **Normalized 2D Projection**: A novel 2D projection method is proposed to convert 3D mesh data into 2D images for StyleGAN-2. This involves:\n        *   Normalizing 3D orientation using the body's bounding box to ensure independence from intrinsic coordinate systems \\cite{broll2024edy}.\n        *   Employing Principal Component Analysis (PCA) of the occlusal surface to identify an optimal projection plane that maximizes spatial information retention \\cite{broll2024edy}.\n    *   **Downstream Optimization for Reconstruction**: Partial reconstruction is handled by an optimization process derived from Bayesian Image Reconstruction \\cite{broll2024edy}.\n        *   The fixed, fully trained StyleGAN-2 generator serves as the underlying model \\cite{broll2024edy}.\n        *   An Adam optimizer is used to find the optimal latent vector `w` that generates the missing occlusal surface, guided by the remaining tooth data and a binary mask \\cite{broll2024edy}.\n        *   The initial reconstruction loss is modified for this purpose \\cite{broll2024edy}.\n\n4.  **Key Technical Contributions**\n    *   **Modular Deep Learning Architecture**: Decouples 2D-projection, teeth generation, and reconstruction, simplifying enhancements and allowing state-of-the-art updates to individual components without affecting the entire pipeline \\cite{broll2024edy}.\n    *   **Generalizable Generative Training**: The StyleGAN-2 is trained as a general generative model, not a task-specific completion network, enabling the reconstruction of arbitrary inlay geometries through downstream optimization without time-consuming retraining \\cite{broll2024edy}.\n    *   **PCA-Optimized 2D Projection**: Introduces a novel normalized 2D projection method that leverages PCA to optimally orient 3D tooth meshes, maximizing spatial information captured in the 2D representation for StyleGAN-2 input \\cite{broll2024edy}.\n    *   **Adapted Bayesian Image Reconstruction**: Integrates a modified Bayesian Image Reconstruction method with a pre-trained StyleGAN-2 generator for effective partial tooth surface completion \\cite{broll2024edy}.\n\n5.  **Experimental Validation**\n    *   **Experiments**: The reconstruction capabilities of the trained network were demonstrated on 4 common inlay types \\cite{broll2024edy}.\n    *   **Dataset**: A dataset comprising 92 3D mesh files of full dental crown restorations (single molar crowns) was used, split into 89 for training (mirrored to 178 images) and 3 for testing \\cite{broll2024edy}.\n    *   **Performance Metrics**: Quantitative results were assessed using Root Mean Square Error (RMSE), alongside visual evaluation \\cite{broll2024edy}.\n    *   **Comparison**: The GAN-based restorations were compared against a clinical procedure for CAD inlay fabrication \\cite{broll2024edy}.\n    *   **Key Results**:\n        *   The reconstruction process yielded satisfactory visual and quantitative results, with RMSE ranging from 0.02mm to 0.18mm \\cite{broll2024edy}.\n        *   A group of dentists preferred the GAN-based restorations for 3 out of the 4 inlay geometries when compared to the clinical CAD procedure \\cite{broll2024edy}.\n\n6.  **Limitations & Scope**\n    *   **Dataset Size**: The method was validated on a relatively small dataset of 92 3D mesh files of single molar crowns, which might limit its generalizability to a wider range of tooth types or more complex dental scenarios \\cite{broll2024edy}.\n    *   **2D Representation Constraints**: While improved by PCA, the use of a 2D projection method for 3D data inherently carries limitations regarding the capture of complex 3D structures and potential data loss from undercuts, as acknowledged for other 2D approaches \\cite{broll2024edy}.\n    *   **Specific Tooth Type and Restoration**: The validation focused on molar teeth and 4 common inlay types, and its applicability to other tooth types (e.g., incisors, canines) or more extensive restorations (e.g., full crowns, bridges) is not explicitly demonstrated \\cite{broll2024edy}.\n\n7.  **Technical Significance**\n    *   **Advances Digital Dentistry**: Provides a robust, data-driven approach that significantly improves the partial reconstruction of individual human molar teeth, addressing a critical challenge in the quality and efficiency of dental restorations \\cite{broll2024edy}.\n    *   **Enhanced Flexibility and Generalizability**: The decoupled architecture and the generative training approach enable the method to handle arbitrary inlay geometries without requiring time-consuming retraining, offering greater flexibility than previous task-specific models \\cite{broll2024edy}.\n    *   **High-Quality and Clinically Relevant Results**: Achieves high visual and quantitative accuracy, with a preference from dental professionals over conventional CAD methods, indicating strong potential for clinical adoption and improved patient outcomes \\cite{broll2024edy}.\n    *   **Foundation for Future Research**: The innovative combination of StyleGAN-2 with a PCA-optimized 2D projection and a downstream optimization process establishes a strong methodological framework for future research in generative deep learning for complex 3D dental morphology reconstruction \\cite{broll2024edy}.",
      "intriguing_abstract": "Fabricating precise fixed dental restorations (FDPs) remains a significant challenge, often leading to occlusal discomfort and costly reworks due to the complex, unique morphology of human teeth. Current design methods struggle with reconstruction quality, particularly for partial restorations. We introduce a novel data-driven approach leveraging **generative deep learning** for the partial reconstruction of individual human molar teeth, aiming to revolutionize FDP design.\n\nOur method employs a modular architecture, decoupling 2D-projection, tooth generation, and reconstruction. At its core, a **StyleGAN-2** model is trained on 3D mesh files of full dental crowns, learning diverse occlusal morphologies. Crucially, we propose a **PCA-optimized 2D projection** to convert 3D data, maximizing spatial information for the generator. Partial reconstruction is achieved via a downstream **latent vector optimization** process, adapted from **Bayesian Image Reconstruction**, utilizing the pre-trained StyleGAN-2 to complete arbitrary inlay geometries without retraining.\n\nValidated on common inlay types, our approach yields impressive visual and quantitative results (RMSE 0.02-0.18mm), with dental professionals preferring GAN-based restorations over conventional CAD methods. This work offers a highly flexible and generalizable solution, significantly advancing digital dentistry and paving the way for superior, patient-specific FDPs.",
      "keywords": [
        "Partial reconstruction of human molar teeth",
        "generative deep learning",
        "StyleGAN-2",
        "3D mesh data",
        "PCA-optimized 2D projection",
        "downstream optimization",
        "Bayesian Image Reconstruction",
        "modular deep learning architecture",
        "generalizable generative training",
        "fixed dental restorations (FDPs)",
        "digital dentistry",
        "occlusal surface generation",
        "RMSE",
        "clinical preference"
      ],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/1f76f23c919c9b4503a9a369c11ef303822646cd.pdf",
      "citation_key": "broll2024edy",
      "metadata": {
        "title": "A data-driven approach for the partial reconstruction of individual human molar teeth using generative deep learning",
        "authors": [
          "Alexander Broll",
          "M. Rosentritt",
          "Thomas Schlegl",
          "Markus Goldhacker"
        ],
        "published_date": "2024",
        "abstract": "Background and objective\nDue to the high prevalence of dental caries, fixed dental restorations are regularly required to restore compromised teeth or replace missing teeth while retaining function and aesthetic appearance. The fabrication of dental restorations, however, remains challenging due to the complexity of the human masticatory system as well as the unique morphology of each individual dentition. Adaptation and reworking are frequently required during the insertion of fixed dental prostheses (FDPs), which increase cost and treatment time. This article proposes a data-driven approach for the partial reconstruction of occlusal surfaces based on a data set that comprises 92 3D mesh files of full dental crown restorations.\n\n\nMethods\nA Generative Adversarial Network (GAN) is considered for the given task in view of its ability to represent extensive data sets in an unsupervised manner with a wide variety of applications. Having demonstrated good capabilities in terms of image quality and training stability, StyleGAN-2 has been chosen as the main network for generating the occlusal surfaces. A 2D projection method is proposed in order to generate 2D representations of the provided 3D tooth data set for integration with the StyleGAN architecture. The reconstruction capabilities of the trained network are demonstrated by means of 4 common inlay types using a Bayesian Image Reconstruction method. This involves pre-processing the data in order to extract the necessary information of the tooth preparations required for the used method as well as the modification of the initial reconstruction loss.\n\n\nResults\nThe reconstruction process yields satisfactory visual and quantitative results for all preparations with a root mean square error (RMSE) ranging from 0.02 mm to 0.18 mm. When compared against a clinical procedure for CAD inlay fabrication, the group of dentists preferred the GAN-based restorations for 3 of the total 4 inlay geometries.\n\n\nConclusions\nThis article shows the effectiveness of the StyleGAN architecture with a downstream optimization process for the reconstruction of 4 different inlay geometries. The independence of the reconstruction process and the initial training of the GAN enables the application of the method for arbitrary inlay geometries without time-consuming retraining of the GAN.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/1f76f23c919c9b4503a9a369c11ef303822646cd.pdf",
        "venue": "Frontiers Artif. Intell.",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis: A data-driven approach for the partial reconstruction of individual human molar teeth using generative deep learning \\cite{broll2024edy}\n\n1.  **Research Problem & Motivation**\n    *   The fabrication of fixed dental restorations (FDPs) is challenging due to the complex and unique morphology of individual human dentition and the masticatory system \\cite{broll2024edy}.\n    *   Current methods for designing FDPs, especially for smaller restorations, rely on functional analysis and mechanical/digital articulators, which are prone to errors due to the system's complexity and individuality \\cite{broll2024edy}.\n    *   These errors frequently lead to occlusal discomfort, temporomandibular disorders, FDP failure, and necessitate costly and time-consuming adaptation and reworking during insertion, reducing overall stability \\cite{broll2024edy}.\n\n2.  **Related Work & Positioning**\n    *   Existing commercial software and non-data-driven conventional reconstruction techniques (e.g., Bayesian maximum posterior probability, iterative deformation, morphing standard teeth) still face problems with reconstruction quality \\cite{broll2024edy}.\n    *   While deep learning is widely used for segmentation and classification in dentistry, generative network architectures for tooth generation and reconstruction are less explored \\cite{broll2024edy}.\n    *   Previous data-driven approaches often use 2D depth map representations with conditional GANs (e.g., pix2pix), but these are limited by data loss from undercuts and typically restricted to a small number of teeth (e.g., single or max three teeth) \\cite{broll2024edy}.\n    *   Some 3D data approaches use transformer networks with graph convolutional neural networks or 3D-GANs on point clouds or voxelized points, allowing for entire jaw input, but none have used complete upper and lower jaw data \\cite{broll2024edy}.\n    *   This work differentiates itself by decoupling the 2D-projection, teeth generation, and reconstruction components, allowing for modular updates \\cite{broll2024edy}.\n    *   Unlike prior inlay restoration networks (e.g., Tian et al., 2021), this method trains a generative network without a predefined target, enabling a downstream optimization-based reconstruction for arbitrary inlay geometries without retraining \\cite{broll2024edy}.\n    *   It uses Principal Component Analysis (PCA) for optimal 2D projection, maximizing spatial information, in contrast to optimizing projection parameters based on image entropy \\cite{broll2024edy}.\n\n3.  **Technical Approach & Innovation**\n    *   **Two-step Process**: The method involves a generating component (StyleGAN-2) and a reconstructing component (downstream optimization) \\cite{broll2024edy}.\n    *   **Generative Model**: StyleGAN-2 is trained on 3D mesh files of full dental crown restorations to learn the morphology of teeth and generate diverse occlusal surfaces \\cite{broll2024edy}.\n    *   **Normalized 2D Projection**: A novel 2D projection method is proposed to convert 3D mesh data into 2D images for StyleGAN-2. This involves:\n        *   Normalizing 3D orientation using the body's bounding box to ensure independence from intrinsic coordinate systems \\cite{broll2024edy}.\n        *   Employing Principal Component Analysis (PCA) of the occlusal surface to identify an optimal projection plane that maximizes spatial information retention \\cite{broll2024edy}.\n    *   **Downstream Optimization for Reconstruction**: Partial reconstruction is handled by an optimization process derived from Bayesian Image Reconstruction \\cite{broll2024edy}.\n        *   The fixed, fully trained StyleGAN-2 generator serves as the underlying model \\cite{broll2024edy}.\n        *   An Adam optimizer is used to find the optimal latent vector `w` that generates the missing occlusal surface, guided by the remaining tooth data and a binary mask \\cite{broll2024edy}.\n        *   The initial reconstruction loss is modified for this purpose \\cite{broll2024edy}.\n\n4.  **Key Technical Contributions**\n    *   **Modular Deep Learning Architecture**: Decouples 2D-projection, teeth generation, and reconstruction, simplifying enhancements and allowing state-of-the-art updates to individual components without affecting the entire pipeline \\cite{broll2024edy}.\n    *   **Generalizable Generative Training**: The StyleGAN-2 is trained as a general generative model, not a task-specific completion network, enabling the reconstruction of arbitrary inlay geometries through downstream optimization without time-consuming retraining \\cite{broll2024edy}.\n    *   **PCA-Optimized 2D Projection**: Introduces a novel normalized 2D projection method that leverages PCA to optimally orient 3D tooth meshes, maximizing spatial information captured in the 2D representation for StyleGAN-2 input \\cite{broll2024edy}.\n    *   **Adapted Bayesian Image Reconstruction**: Integrates a modified Bayesian Image Reconstruction method with a pre-trained StyleGAN-2 generator for effective partial tooth surface completion \\cite{broll2024edy}.\n\n5.  **Experimental Validation**\n    *   **Experiments**: The reconstruction capabilities of the trained network were demonstrated on 4 common inlay types \\cite{broll2024edy}.\n    *   **Dataset**: A dataset comprising 92 3D mesh files of full dental crown restorations (single molar crowns) was used, split into 89 for training (mirrored to 178 images) and 3 for testing \\cite{broll2024edy}.\n    *   **Performance Metrics**: Quantitative results were assessed using Root Mean Square Error (RMSE), alongside visual evaluation \\cite{broll2024edy}.\n    *   **Comparison**: The GAN-based restorations were compared against a clinical procedure for CAD inlay fabrication \\cite{broll2024edy}.\n    *   **Key Results**:\n        *   The reconstruction process yielded satisfactory visual and quantitative results, with RMSE ranging from 0.02mm to 0.18mm \\cite{broll2024edy}.\n        *   A group of dentists preferred the GAN-based restorations for 3 out of the 4 inlay geometries when compared to the clinical CAD procedure \\cite{broll2024edy}.\n\n6.  **Limitations & Scope**\n    *   **Dataset Size**: The method was validated on a relatively small dataset of 92 3D mesh files of single molar crowns, which might limit its generalizability to a wider range of tooth types or more complex dental scenarios \\cite{broll2024edy}.\n    *   **2D Representation Constraints**: While improved by PCA, the use of a 2D projection method for 3D data inherently carries limitations regarding the capture of complex 3D structures and potential data loss from undercuts, as acknowledged for other 2D approaches \\cite{broll2024edy}.\n    *   **Specific Tooth Type and Restoration**: The validation focused on molar teeth and 4 common inlay types, and its applicability to other tooth types (e.g., incisors, canines) or more extensive restorations (e.g., full crowns, bridges) is not explicitly demonstrated \\cite{broll2024edy}.\n\n7.  **Technical Significance**\n    *   **Advances Digital Dentistry**: Provides a robust, data-driven approach that significantly improves the partial reconstruction of individual human molar teeth, addressing a critical challenge in the quality and efficiency of dental restorations \\cite{broll2024edy}.\n    *   **Enhanced Flexibility and Generalizability**: The decoupled architecture and the generative training approach enable the method to handle arbitrary inlay geometries without requiring time-consuming retraining, offering greater flexibility than previous task-specific models \\cite{broll2024edy}.\n    *   **High-Quality and Clinically Relevant Results**: Achieves high visual and quantitative accuracy, with a preference from dental professionals over conventional CAD methods, indicating strong potential for clinical adoption and improved patient outcomes \\cite{broll2024edy}.\n    *   **Foundation for Future Research**: The innovative combination of StyleGAN-2 with a PCA-optimized 2D projection and a downstream optimization process establishes a strong methodological framework for future research in generative deep learning for complex 3D dental morphology reconstruction \\cite{broll2024edy}.",
        "keywords": [
          "Partial reconstruction of human molar teeth",
          "generative deep learning",
          "StyleGAN-2",
          "3D mesh data",
          "PCA-optimized 2D projection",
          "downstream optimization",
          "Bayesian Image Reconstruction",
          "modular deep learning architecture",
          "generalizable generative training",
          "fixed dental restorations (FDPs)",
          "digital dentistry",
          "occlusal surface generation",
          "RMSE",
          "clinical preference"
        ],
        "paper_type": "based on the provided title and introduction:\n\n*   **title:** \"a data-driven approach for the partial reconstruction of individual human molar teeth using generative deep learning\"\n    *   keywords: \"data-driven approach,\" \"reconstruction,\" \"generative deep learning.\" these strongly suggest the development and presentation of a new method or system.\n*   **introduction:** discusses a complex problem in dentistry (designing well-fitting fdps, simulating dynamic occlusion) and the current methods (3d scans, cad/cam). it sets the stage for a proposed solution to this technical challenge.\n\nthe combination of a \"data-driven approach\" and \"generative deep learning\" in the title, aimed at solving a specific problem in dental reconstruction, clearly indicates the paper is presenting a new method or system.\n\ntherefore, this paper is best classified as **technical**."
      },
      "file_name": "1f76f23c919c9b4503a9a369c11ef303822646cd.pdf"
    },
    {
      "success": true,
      "doc_id": "790776184d56a949168bfdadac2fd5e3",
      "summary": "In this letter, we provide the target signal enhancement method based on deep learning for weak target detection. First, the proposed method fully considers the nature characteristic of radar complex echoes and exploits the complex-valued neural networks. Then, the architecture of weak target enhancement complex-valued generative adversarial network (WTE-CGAN) is proposed. More specifically, the generator loss function of generative adversarial network (GAN) is modified, which can be used to reflect the difference between the generated target signal by the generator and the label signal. To keep the training stability of the proposed method, a gradient penalty factor is randomly added to every sample, which embodies the loss function of discriminator. Finally, simulation and measured experiments are given to demonstrate the effectiveness of the proposed method compared with other methods, and it has a significant signal enhancement effect on weak targets.",
      "intriguing_abstract": "In this letter, we provide the target signal enhancement method based on deep learning for weak target detection. First, the proposed method fully considers the nature characteristic of radar complex echoes and exploits the complex-valued neural networks. Then, the architecture of weak target enhancement complex-valued generative adversarial network (WTE-CGAN) is proposed. More specifically, the generator loss function of generative adversarial network (GAN) is modified, which can be used to reflect the difference between the generated target signal by the generator and the label signal. To keep the training stability of the proposed method, a gradient penalty factor is randomly added to every sample, which embodies the loss function of discriminator. Finally, simulation and measured experiments are given to demonstrate the effectiveness of the proposed method compared with other methods, and it has a significant signal enhancement effect on weak targets.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/d38bec69a9ebbf9d57de0baf131e711c7c7fbfb4.pdf",
      "citation_key": "wang20245dt",
      "metadata": {
        "title": "WTE-CGAN Based Signal Enhancement for Weak Target Detection",
        "authors": [
          "Yumiao Wang",
          "Chuanfei Zang",
          "Bo Yu",
          "Wenjing Zhao",
          "Xiangrong Wang",
          "Cong'an Xu",
          "Guolong Cui"
        ],
        "published_date": "2024",
        "abstract": "In this letter, we provide the target signal enhancement method based on deep learning for weak target detection. First, the proposed method fully considers the nature characteristic of radar complex echoes and exploits the complex-valued neural networks. Then, the architecture of weak target enhancement complex-valued generative adversarial network (WTE-CGAN) is proposed. More specifically, the generator loss function of generative adversarial network (GAN) is modified, which can be used to reflect the difference between the generated target signal by the generator and the label signal. To keep the training stability of the proposed method, a gradient penalty factor is randomly added to every sample, which embodies the loss function of discriminator. Finally, simulation and measured experiments are given to demonstrate the effectiveness of the proposed method compared with other methods, and it has a significant signal enhancement effect on weak targets.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/d38bec69a9ebbf9d57de0baf131e711c7c7fbfb4.pdf",
        "venue": "IEEE Geoscience and Remote Sensing Letters",
        "citationCount": 5,
        "score": 5.0,
        "summary": "In this letter, we provide the target signal enhancement method based on deep learning for weak target detection. First, the proposed method fully considers the nature characteristic of radar complex echoes and exploits the complex-valued neural networks. Then, the architecture of weak target enhancement complex-valued generative adversarial network (WTE-CGAN) is proposed. More specifically, the generator loss function of generative adversarial network (GAN) is modified, which can be used to reflect the difference between the generated target signal by the generator and the label signal. To keep the training stability of the proposed method, a gradient penalty factor is randomly added to every sample, which embodies the loss function of discriminator. Finally, simulation and measured experiments are given to demonstrate the effectiveness of the proposed method compared with other methods, and it has a significant signal enhancement effect on weak targets.",
        "keywords": []
      },
      "file_name": "d38bec69a9ebbf9d57de0baf131e711c7c7fbfb4.pdf"
    },
    {
      "success": true,
      "doc_id": "8a28282742fc3575dae74d3c0331d2c8",
      "summary": "Generative Adversarial Network (GAN) outperforms its peers in the generative models’ family and is widely used to generate realistic samples in various domains. The basic idea of GAN is a competition between two networks called a generator and discriminator. Throughout the training process of GAN, the two networks face various challenges that affect the quality and diversity of the generated samples of GAN. These challenges include training instability and mode collapse problem. Training instability happens due to the variance of the performance between the generator and discriminator. The mode collapse, on the other hand, happens when the generator is stuck to generate diverse samples. One of the promising techniques that might overcome these issues and increase the networks’ performance is transfer learning between discriminators as same as generators. In this regard, the contribution of this paper is fourfold. First, it proposes a novel approach called Collaborative-GAN based on transfer learning to mitigate the training instability and tackle the mode collapse issues. In the proposed approach, the well-performer network transfers its learned weights to the low-performer ones based on a periodical evaluation during the training process. Second, the paper proposes a novel method to evaluate the discriminators’ performance based on a fuzzy inference system. Third, the paper proposes a method to evaluate the generators’ performance based on a series of detected FID scores that measure the diversity of the generated samples every certain intervals during the training process. We apply the proposed approach on two different architectures of GAN, which we called Single-GAN and Dual-GANs. In Single-GAN, the weights are transferred between the identical networks within the same GAN model. In Dual-GANs, on the other hand, the weights are transferred between identical networks across different GAN models. Thus, the paper introduces two types of transfer learning for GANs; inter and intra-transfer learning based on the paradigm of GAN architecture as a fourth contribution. We validate the proposed approach on three different benchmarks representing CelebA, Cifar-10, and Fashion-Mnist. The experimental results indicate that the proposed approach outperforms the state-of-the-art GAN models in terms of FID metric that measures the generated sample diversity. It is worth noting that the proposed approach achieved remarkable FID scores of 11.44, 24.19, and 11.21 on the Fashion-Mnist, Cifar-10, and CelebA datasets respectively.",
      "intriguing_abstract": "Generative Adversarial Network (GAN) outperforms its peers in the generative models’ family and is widely used to generate realistic samples in various domains. The basic idea of GAN is a competition between two networks called a generator and discriminator. Throughout the training process of GAN, the two networks face various challenges that affect the quality and diversity of the generated samples of GAN. These challenges include training instability and mode collapse problem. Training instability happens due to the variance of the performance between the generator and discriminator. The mode collapse, on the other hand, happens when the generator is stuck to generate diverse samples. One of the promising techniques that might overcome these issues and increase the networks’ performance is transfer learning between discriminators as same as generators. In this regard, the contribution of this paper is fourfold. First, it proposes a novel approach called Collaborative-GAN based on transfer learning to mitigate the training instability and tackle the mode collapse issues. In the proposed approach, the well-performer network transfers its learned weights to the low-performer ones based on a periodical evaluation during the training process. Second, the paper proposes a novel method to evaluate the discriminators’ performance based on a fuzzy inference system. Third, the paper proposes a method to evaluate the generators’ performance based on a series of detected FID scores that measure the diversity of the generated samples every certain intervals during the training process. We apply the proposed approach on two different architectures of GAN, which we called Single-GAN and Dual-GANs. In Single-GAN, the weights are transferred between the identical networks within the same GAN model. In Dual-GANs, on the other hand, the weights are transferred between identical networks across different GAN models. Thus, the paper introduces two types of transfer learning for GANs; inter and intra-transfer learning based on the paradigm of GAN architecture as a fourth contribution. We validate the proposed approach on three different benchmarks representing CelebA, Cifar-10, and Fashion-Mnist. The experimental results indicate that the proposed approach outperforms the state-of-the-art GAN models in terms of FID metric that measures the generated sample diversity. It is worth noting that the proposed approach achieved remarkable FID scores of 11.44, 24.19, and 11.21 on the Fashion-Mnist, Cifar-10, and CelebA datasets respectively.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/f47efc7762b9025ce17fad7a8ffc81c672362851.pdf",
      "citation_key": "megahed2024c23",
      "metadata": {
        "title": "Collaborative-GAN: An Approach for Stabilizing the Training Process of Generative Adversarial Network",
        "authors": [
          "Mohammed Megahed",
          "Ammar Mohammed"
        ],
        "published_date": "2024",
        "abstract": "Generative Adversarial Network (GAN) outperforms its peers in the generative models’ family and is widely used to generate realistic samples in various domains. The basic idea of GAN is a competition between two networks called a generator and discriminator. Throughout the training process of GAN, the two networks face various challenges that affect the quality and diversity of the generated samples of GAN. These challenges include training instability and mode collapse problem. Training instability happens due to the variance of the performance between the generator and discriminator. The mode collapse, on the other hand, happens when the generator is stuck to generate diverse samples. One of the promising techniques that might overcome these issues and increase the networks’ performance is transfer learning between discriminators as same as generators. In this regard, the contribution of this paper is fourfold. First, it proposes a novel approach called Collaborative-GAN based on transfer learning to mitigate the training instability and tackle the mode collapse issues. In the proposed approach, the well-performer network transfers its learned weights to the low-performer ones based on a periodical evaluation during the training process. Second, the paper proposes a novel method to evaluate the discriminators’ performance based on a fuzzy inference system. Third, the paper proposes a method to evaluate the generators’ performance based on a series of detected FID scores that measure the diversity of the generated samples every certain intervals during the training process. We apply the proposed approach on two different architectures of GAN, which we called Single-GAN and Dual-GANs. In Single-GAN, the weights are transferred between the identical networks within the same GAN model. In Dual-GANs, on the other hand, the weights are transferred between identical networks across different GAN models. Thus, the paper introduces two types of transfer learning for GANs; inter and intra-transfer learning based on the paradigm of GAN architecture as a fourth contribution. We validate the proposed approach on three different benchmarks representing CelebA, Cifar-10, and Fashion-Mnist. The experimental results indicate that the proposed approach outperforms the state-of-the-art GAN models in terms of FID metric that measures the generated sample diversity. It is worth noting that the proposed approach achieved remarkable FID scores of 11.44, 24.19, and 11.21 on the Fashion-Mnist, Cifar-10, and CelebA datasets respectively.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/f47efc7762b9025ce17fad7a8ffc81c672362851.pdf",
        "venue": "IEEE Access",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Generative Adversarial Network (GAN) outperforms its peers in the generative models’ family and is widely used to generate realistic samples in various domains. The basic idea of GAN is a competition between two networks called a generator and discriminator. Throughout the training process of GAN, the two networks face various challenges that affect the quality and diversity of the generated samples of GAN. These challenges include training instability and mode collapse problem. Training instability happens due to the variance of the performance between the generator and discriminator. The mode collapse, on the other hand, happens when the generator is stuck to generate diverse samples. One of the promising techniques that might overcome these issues and increase the networks’ performance is transfer learning between discriminators as same as generators. In this regard, the contribution of this paper is fourfold. First, it proposes a novel approach called Collaborative-GAN based on transfer learning to mitigate the training instability and tackle the mode collapse issues. In the proposed approach, the well-performer network transfers its learned weights to the low-performer ones based on a periodical evaluation during the training process. Second, the paper proposes a novel method to evaluate the discriminators’ performance based on a fuzzy inference system. Third, the paper proposes a method to evaluate the generators’ performance based on a series of detected FID scores that measure the diversity of the generated samples every certain intervals during the training process. We apply the proposed approach on two different architectures of GAN, which we called Single-GAN and Dual-GANs. In Single-GAN, the weights are transferred between the identical networks within the same GAN model. In Dual-GANs, on the other hand, the weights are transferred between identical networks across different GAN models. Thus, the paper introduces two types of transfer learning for GANs; inter and intra-transfer learning based on the paradigm of GAN architecture as a fourth contribution. We validate the proposed approach on three different benchmarks representing CelebA, Cifar-10, and Fashion-Mnist. The experimental results indicate that the proposed approach outperforms the state-of-the-art GAN models in terms of FID metric that measures the generated sample diversity. It is worth noting that the proposed approach achieved remarkable FID scores of 11.44, 24.19, and 11.21 on the Fashion-Mnist, Cifar-10, and CelebA datasets respectively.",
        "keywords": []
      },
      "file_name": "f47efc7762b9025ce17fad7a8ffc81c672362851.pdf"
    },
    {
      "success": true,
      "doc_id": "b570e41752ed55fc5029e360dbc6f2ef",
      "summary": "The large‐scale integration of new energy generation into the power transmission network introduces uncertainty and fluctuations, posing a threat to the secure operation of the transmission network. Modelling and analysing the uncertainty in photovoltaic (PV) generation is essential. This paper introduces a method for generating scenarios for centralized PV station based on spectral normalization generative adversarial networks (SNGAN). Data‐driven scenario generation methods, with generative adversarial networks (GAN) as representatives, are commonly used today to analyse the uncertainty of new energy generation. In addressing the uncertainty of centralized PV power generation, this paper introduces SNGAN, makes improvements to the discriminator, enhances training stability, and generates PV power generation scenarios. The incorporation of spectral normalization as a regularization technique contributes to the stability and convergence of the proposed GAN model. The effectiveness of the selected method is validated through comparisons with actual data.",
      "intriguing_abstract": "The large‐scale integration of new energy generation into the power transmission network introduces uncertainty and fluctuations, posing a threat to the secure operation of the transmission network. Modelling and analysing the uncertainty in photovoltaic (PV) generation is essential. This paper introduces a method for generating scenarios for centralized PV station based on spectral normalization generative adversarial networks (SNGAN). Data‐driven scenario generation methods, with generative adversarial networks (GAN) as representatives, are commonly used today to analyse the uncertainty of new energy generation. In addressing the uncertainty of centralized PV power generation, this paper introduces SNGAN, makes improvements to the discriminator, enhances training stability, and generates PV power generation scenarios. The incorporation of spectral normalization as a regularization technique contributes to the stability and convergence of the proposed GAN model. The effectiveness of the selected method is validated through comparisons with actual data.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/6cb4ca76895cf98383a53bb07939cb9a4c6b9391.pdf",
      "citation_key": "zhang2024k8a",
      "metadata": {
        "title": "Spectral normalization generative adversarial networks for photovoltaic power scenario generation",
        "authors": [
          "Xiurong Zhang",
          "Shaoqian Fan",
          "Daoliang Li"
        ],
        "published_date": "2024",
        "abstract": "The large‐scale integration of new energy generation into the power transmission network introduces uncertainty and fluctuations, posing a threat to the secure operation of the transmission network. Modelling and analysing the uncertainty in photovoltaic (PV) generation is essential. This paper introduces a method for generating scenarios for centralized PV station based on spectral normalization generative adversarial networks (SNGAN). Data‐driven scenario generation methods, with generative adversarial networks (GAN) as representatives, are commonly used today to analyse the uncertainty of new energy generation. In addressing the uncertainty of centralized PV power generation, this paper introduces SNGAN, makes improvements to the discriminator, enhances training stability, and generates PV power generation scenarios. The incorporation of spectral normalization as a regularization technique contributes to the stability and convergence of the proposed GAN model. The effectiveness of the selected method is validated through comparisons with actual data.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/6cb4ca76895cf98383a53bb07939cb9a4c6b9391.pdf",
        "venue": "IET Renewable Power Generation",
        "citationCount": 4,
        "score": 4.0,
        "summary": "The large‐scale integration of new energy generation into the power transmission network introduces uncertainty and fluctuations, posing a threat to the secure operation of the transmission network. Modelling and analysing the uncertainty in photovoltaic (PV) generation is essential. This paper introduces a method for generating scenarios for centralized PV station based on spectral normalization generative adversarial networks (SNGAN). Data‐driven scenario generation methods, with generative adversarial networks (GAN) as representatives, are commonly used today to analyse the uncertainty of new energy generation. In addressing the uncertainty of centralized PV power generation, this paper introduces SNGAN, makes improvements to the discriminator, enhances training stability, and generates PV power generation scenarios. The incorporation of spectral normalization as a regularization technique contributes to the stability and convergence of the proposed GAN model. The effectiveness of the selected method is validated through comparisons with actual data.",
        "keywords": []
      },
      "file_name": "6cb4ca76895cf98383a53bb07939cb9a4c6b9391.pdf"
    },
    {
      "success": true,
      "doc_id": "f617dfd85c7d1cb6c4c17471563944de",
      "summary": "Generative Adversarial Networks (GANs) are a major advancement in generative modeling, surpassing traditional machine learning models in tasks such as image generation, super-resolution, and image-to-text translation. A GAN consists of two neural networks: a Generator (G), which creates data from noise or a latent vector, and a Discriminator (D), which determines whether the data is real or generated. These networks train competitively, improving each other iteratively to produce increasingly realistic outputs. However, GANs face challenges like mode collapse, unstable training, and convergence issues, leading to the adoption of strategies such as instance normalization and enhanced loss functions. Future research can focus on improving stability, developing novel loss functions, and applying GANs in unsupervised learning. Performance metrics like Inception Score, Fréchet Inception Distance (FID), and Structural Similarity Index (SSIM) are essential for evaluating and comparing GAN architectures. Additionally, ethical concerns, including the misuse of GANs for deepfakes and synthetic data, underscore the importance of transparency, accountability, and ethical standards in research and deployment. This review provides an accessible introduction to GANs for novice researchers, along with a detailed analysis of their limitations, applications, and future prospects, offering valuable insights and direction for advancing this field.",
      "intriguing_abstract": "Generative Adversarial Networks (GANs) are a major advancement in generative modeling, surpassing traditional machine learning models in tasks such as image generation, super-resolution, and image-to-text translation. A GAN consists of two neural networks: a Generator (G), which creates data from noise or a latent vector, and a Discriminator (D), which determines whether the data is real or generated. These networks train competitively, improving each other iteratively to produce increasingly realistic outputs. However, GANs face challenges like mode collapse, unstable training, and convergence issues, leading to the adoption of strategies such as instance normalization and enhanced loss functions. Future research can focus on improving stability, developing novel loss functions, and applying GANs in unsupervised learning. Performance metrics like Inception Score, Fréchet Inception Distance (FID), and Structural Similarity Index (SSIM) are essential for evaluating and comparing GAN architectures. Additionally, ethical concerns, including the misuse of GANs for deepfakes and synthetic data, underscore the importance of transparency, accountability, and ethical standards in research and deployment. This review provides an accessible introduction to GANs for novice researchers, along with a detailed analysis of their limitations, applications, and future prospects, offering valuable insights and direction for advancing this field.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/50700e326fdedf55245932da52c703f732175f40.pdf",
      "citation_key": "bhat202445j",
      "metadata": {
        "title": "A Review on Comparative Analysis of Generative Adversarial Networks’ Architectures and Applications",
        "authors": [
          "Ranjith Bhat",
          "Raghu Nanjundegowda"
        ],
        "published_date": "2024",
        "abstract": "Generative Adversarial Networks (GANs) are a major advancement in generative modeling, surpassing traditional machine learning models in tasks such as image generation, super-resolution, and image-to-text translation. A GAN consists of two neural networks: a Generator (G), which creates data from noise or a latent vector, and a Discriminator (D), which determines whether the data is real or generated. These networks train competitively, improving each other iteratively to produce increasingly realistic outputs. However, GANs face challenges like mode collapse, unstable training, and convergence issues, leading to the adoption of strategies such as instance normalization and enhanced loss functions. Future research can focus on improving stability, developing novel loss functions, and applying GANs in unsupervised learning. Performance metrics like Inception Score, Fréchet Inception Distance (FID), and Structural Similarity Index (SSIM) are essential for evaluating and comparing GAN architectures. Additionally, ethical concerns, including the misuse of GANs for deepfakes and synthetic data, underscore the importance of transparency, accountability, and ethical standards in research and deployment. This review provides an accessible introduction to GANs for novice researchers, along with a detailed analysis of their limitations, applications, and future prospects, offering valuable insights and direction for advancing this field.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/50700e326fdedf55245932da52c703f732175f40.pdf",
        "venue": "Journal of Robotics and Control (JRC)",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Generative Adversarial Networks (GANs) are a major advancement in generative modeling, surpassing traditional machine learning models in tasks such as image generation, super-resolution, and image-to-text translation. A GAN consists of two neural networks: a Generator (G), which creates data from noise or a latent vector, and a Discriminator (D), which determines whether the data is real or generated. These networks train competitively, improving each other iteratively to produce increasingly realistic outputs. However, GANs face challenges like mode collapse, unstable training, and convergence issues, leading to the adoption of strategies such as instance normalization and enhanced loss functions. Future research can focus on improving stability, developing novel loss functions, and applying GANs in unsupervised learning. Performance metrics like Inception Score, Fréchet Inception Distance (FID), and Structural Similarity Index (SSIM) are essential for evaluating and comparing GAN architectures. Additionally, ethical concerns, including the misuse of GANs for deepfakes and synthetic data, underscore the importance of transparency, accountability, and ethical standards in research and deployment. This review provides an accessible introduction to GANs for novice researchers, along with a detailed analysis of their limitations, applications, and future prospects, offering valuable insights and direction for advancing this field.",
        "keywords": []
      },
      "file_name": "50700e326fdedf55245932da52c703f732175f40.pdf"
    },
    {
      "success": true,
      "doc_id": "3da7e7a882bcb26e3ff5bfafbc9aa07b",
      "summary": "The applications of Generative Adversarial Networks (GANs) are just as diverse as their architectures, problem settings as well as challenges. A key area of research on GANs is anomaly detection where they are most often utilized when only the data of one class is readily available. In this work, we organize, summarize and compare key concepts and challenges of anomaly detection based on GANs. Common problems which have to be investigated to progress the applicability of GANs are identified and discussed. This includes stability and time requirements during training as well as inference, the restriction of the latent space to produce solely data from the normal class distribution, contaminated training data as well as the composition of the resulting anomaly detection score. We discuss the problems using existing work as well as possible (partial) solutions, including related work from similar areas of research such as related generative models or novelty detection. Our findings are also relevant for a variety of closely related generative modeling approaches, such as autoencoders, and are of interest for areas of research tangent to anomaly detection such as image inpainting or image translation.",
      "intriguing_abstract": "The applications of Generative Adversarial Networks (GANs) are just as diverse as their architectures, problem settings as well as challenges. A key area of research on GANs is anomaly detection where they are most often utilized when only the data of one class is readily available. In this work, we organize, summarize and compare key concepts and challenges of anomaly detection based on GANs. Common problems which have to be investigated to progress the applicability of GANs are identified and discussed. This includes stability and time requirements during training as well as inference, the restriction of the latent space to produce solely data from the normal class distribution, contaminated training data as well as the composition of the resulting anomaly detection score. We discuss the problems using existing work as well as possible (partial) solutions, including related work from similar areas of research such as related generative models or novelty detection. Our findings are also relevant for a variety of closely related generative modeling approaches, such as autoencoders, and are of interest for areas of research tangent to anomaly detection such as image inpainting or image translation.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/a40eff8cfc5fad3870fa6be8aa55f314316013af.pdf",
      "citation_key": "ler20248xg",
      "metadata": {
        "title": "Anomaly Detection using Generative Adversarial Networks Reviewing methodological progress and challenges",
        "authors": [
          "Fiete Lüer",
          "Christian Böhm"
        ],
        "published_date": "2024",
        "abstract": "The applications of Generative Adversarial Networks (GANs) are just as diverse as their architectures, problem settings as well as challenges. A key area of research on GANs is anomaly detection where they are most often utilized when only the data of one class is readily available. In this work, we organize, summarize and compare key concepts and challenges of anomaly detection based on GANs. Common problems which have to be investigated to progress the applicability of GANs are identified and discussed. This includes stability and time requirements during training as well as inference, the restriction of the latent space to produce solely data from the normal class distribution, contaminated training data as well as the composition of the resulting anomaly detection score. We discuss the problems using existing work as well as possible (partial) solutions, including related work from similar areas of research such as related generative models or novelty detection. Our findings are also relevant for a variety of closely related generative modeling approaches, such as autoencoders, and are of interest for areas of research tangent to anomaly detection such as image inpainting or image translation.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/a40eff8cfc5fad3870fa6be8aa55f314316013af.pdf",
        "venue": "SIGKDD Explorations",
        "citationCount": 4,
        "score": 4.0,
        "summary": "The applications of Generative Adversarial Networks (GANs) are just as diverse as their architectures, problem settings as well as challenges. A key area of research on GANs is anomaly detection where they are most often utilized when only the data of one class is readily available. In this work, we organize, summarize and compare key concepts and challenges of anomaly detection based on GANs. Common problems which have to be investigated to progress the applicability of GANs are identified and discussed. This includes stability and time requirements during training as well as inference, the restriction of the latent space to produce solely data from the normal class distribution, contaminated training data as well as the composition of the resulting anomaly detection score. We discuss the problems using existing work as well as possible (partial) solutions, including related work from similar areas of research such as related generative models or novelty detection. Our findings are also relevant for a variety of closely related generative modeling approaches, such as autoencoders, and are of interest for areas of research tangent to anomaly detection such as image inpainting or image translation.",
        "keywords": []
      },
      "file_name": "a40eff8cfc5fad3870fa6be8aa55f314316013af.pdf"
    },
    {
      "success": true,
      "doc_id": "863df27e7aef3d238d1c93c24e077912",
      "summary": "Generative Adversarial Networks (GANs) is an important breakthrough in artificial intelligence that uses two neural networks, a generator and a discriminator, that work in an adversarial framework. The generator generates synthetic data, while the discriminator evaluates the authenticity of the data. This dynamic interaction forms a minimax game that produces high-quality synthetic data. Since its introduction in 2014 by Ian Goodfellow, GAN has evolved through various innovative architectures, including Vanilla GAN, Conditional GAN (cGAN), Deep Convolutional GAN (DCGAN), CycleGAN, StyleGAN, Wasserstein GAN (WGAN), and BigGAN. Each of these architectures presents a novel approach to address technical challenges such as training stability, data diversification, and result quality. GANs have been widely applied in various sectors. In healthcare, GANs are used to generate synthetic medical images that support diagnostic development without violating patient privacy. In the media and entertainment industry, GANs facilitate the enhancement of image and video resolution, as well as the creation of realistic content. However, the development of GANs faces challenges such as mode collapse, training instability, and inadequate quality evaluation. In addition to technical challenges, GANs raise ethical issues, such as the misuse of the technology for deepfake creation. Legal regulations, detection tools, and public education are important mitigation measures. Future trends suggest that GANs will be increasingly used in text-to-image synthesis, realistic video generation, and integration with multimodal systems to support cross-disciplinary innovation.",
      "intriguing_abstract": "Generative Adversarial Networks (GANs) is an important breakthrough in artificial intelligence that uses two neural networks, a generator and a discriminator, that work in an adversarial framework. The generator generates synthetic data, while the discriminator evaluates the authenticity of the data. This dynamic interaction forms a minimax game that produces high-quality synthetic data. Since its introduction in 2014 by Ian Goodfellow, GAN has evolved through various innovative architectures, including Vanilla GAN, Conditional GAN (cGAN), Deep Convolutional GAN (DCGAN), CycleGAN, StyleGAN, Wasserstein GAN (WGAN), and BigGAN. Each of these architectures presents a novel approach to address technical challenges such as training stability, data diversification, and result quality. GANs have been widely applied in various sectors. In healthcare, GANs are used to generate synthetic medical images that support diagnostic development without violating patient privacy. In the media and entertainment industry, GANs facilitate the enhancement of image and video resolution, as well as the creation of realistic content. However, the development of GANs faces challenges such as mode collapse, training instability, and inadequate quality evaluation. In addition to technical challenges, GANs raise ethical issues, such as the misuse of the technology for deepfake creation. Legal regulations, detection tools, and public education are important mitigation measures. Future trends suggest that GANs will be increasingly used in text-to-image synthesis, realistic video generation, and integration with multimodal systems to support cross-disciplinary innovation.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/8ac65e097e13c58825e5dd7b83da911cd37a0d33.pdf",
      "citation_key": "purwono2025spz",
      "metadata": {
        "title": "Understanding Generative Adversarial Networks (GANs): A Review",
        "authors": [
          "Purwono Purwono",
          "Annastasya Nabila Elsa Wulandari",
          "Alfian Ma’arif",
          "Wael A. Salah"
        ],
        "published_date": "2025",
        "abstract": "Generative Adversarial Networks (GANs) is an important breakthrough in artificial intelligence that uses two neural networks, a generator and a discriminator, that work in an adversarial framework. The generator generates synthetic data, while the discriminator evaluates the authenticity of the data. This dynamic interaction forms a minimax game that produces high-quality synthetic data. Since its introduction in 2014 by Ian Goodfellow, GAN has evolved through various innovative architectures, including Vanilla GAN, Conditional GAN (cGAN), Deep Convolutional GAN (DCGAN), CycleGAN, StyleGAN, Wasserstein GAN (WGAN), and BigGAN. Each of these architectures presents a novel approach to address technical challenges such as training stability, data diversification, and result quality. GANs have been widely applied in various sectors. In healthcare, GANs are used to generate synthetic medical images that support diagnostic development without violating patient privacy. In the media and entertainment industry, GANs facilitate the enhancement of image and video resolution, as well as the creation of realistic content. However, the development of GANs faces challenges such as mode collapse, training instability, and inadequate quality evaluation. In addition to technical challenges, GANs raise ethical issues, such as the misuse of the technology for deepfake creation. Legal regulations, detection tools, and public education are important mitigation measures. Future trends suggest that GANs will be increasingly used in text-to-image synthesis, realistic video generation, and integration with multimodal systems to support cross-disciplinary innovation.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/8ac65e097e13c58825e5dd7b83da911cd37a0d33.pdf",
        "venue": "Control Systems and Optimization Letters",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Generative Adversarial Networks (GANs) is an important breakthrough in artificial intelligence that uses two neural networks, a generator and a discriminator, that work in an adversarial framework. The generator generates synthetic data, while the discriminator evaluates the authenticity of the data. This dynamic interaction forms a minimax game that produces high-quality synthetic data. Since its introduction in 2014 by Ian Goodfellow, GAN has evolved through various innovative architectures, including Vanilla GAN, Conditional GAN (cGAN), Deep Convolutional GAN (DCGAN), CycleGAN, StyleGAN, Wasserstein GAN (WGAN), and BigGAN. Each of these architectures presents a novel approach to address technical challenges such as training stability, data diversification, and result quality. GANs have been widely applied in various sectors. In healthcare, GANs are used to generate synthetic medical images that support diagnostic development without violating patient privacy. In the media and entertainment industry, GANs facilitate the enhancement of image and video resolution, as well as the creation of realistic content. However, the development of GANs faces challenges such as mode collapse, training instability, and inadequate quality evaluation. In addition to technical challenges, GANs raise ethical issues, such as the misuse of the technology for deepfake creation. Legal regulations, detection tools, and public education are important mitigation measures. Future trends suggest that GANs will be increasingly used in text-to-image synthesis, realistic video generation, and integration with multimodal systems to support cross-disciplinary innovation.",
        "keywords": []
      },
      "file_name": "8ac65e097e13c58825e5dd7b83da911cd37a0d33.pdf"
    },
    {
      "success": true,
      "doc_id": "7019e1acef66762db29e7265bf03a71a",
      "summary": "Generative adversarial networks (GANs) excel in diverse applications such as image enhancement, manipulation, and generating images and videos from text. Yet, training GANs with large datasets remains computationally intensive for standalone systems. Synchronization issues between the generator and discriminator lead to unstable training, poor convergence, vanishing, and exploding gradient challenges. In decentralized environments, standalone GANs struggle with distributed data on client machines. Researchers have turned to federated learning (FL) for distributed-GAN (D-GAN) implementations, but efforts often fall short due to training instability and poor synchronization within GAN components. In this study, we present DRL-GAN, a lightweight Wasserstein conditional distributed relativistic loss-GAN designed to overcome existing limitations. DRL-GAN ensures training stability in the face of nonconvex losses by employing a single global generator on the central server and a discriminator per client. Utilizing Wasserstein-1 for relativistic loss computation between real and fake samples, DRL-GAN effectively addresses issues, such as mode collapses, vanishing, and exploding gradients, accommodating both iid and non-iid private data in clients and fostering strong convergence. The absence of a robust conditional distributed-GAN model serves as another motivation for this work. We provide a comprehensive mathematical formulation of DRL-GAN and validate our claims empirically on CIFAR-10, MNIST, EuroSAT, and LSUN-Bedroom datasets.",
      "intriguing_abstract": "Generative adversarial networks (GANs) excel in diverse applications such as image enhancement, manipulation, and generating images and videos from text. Yet, training GANs with large datasets remains computationally intensive for standalone systems. Synchronization issues between the generator and discriminator lead to unstable training, poor convergence, vanishing, and exploding gradient challenges. In decentralized environments, standalone GANs struggle with distributed data on client machines. Researchers have turned to federated learning (FL) for distributed-GAN (D-GAN) implementations, but efforts often fall short due to training instability and poor synchronization within GAN components. In this study, we present DRL-GAN, a lightweight Wasserstein conditional distributed relativistic loss-GAN designed to overcome existing limitations. DRL-GAN ensures training stability in the face of nonconvex losses by employing a single global generator on the central server and a discriminator per client. Utilizing Wasserstein-1 for relativistic loss computation between real and fake samples, DRL-GAN effectively addresses issues, such as mode collapses, vanishing, and exploding gradients, accommodating both iid and non-iid private data in clients and fostering strong convergence. The absence of a robust conditional distributed-GAN model serves as another motivation for this work. We provide a comprehensive mathematical formulation of DRL-GAN and validate our claims empirically on CIFAR-10, MNIST, EuroSAT, and LSUN-Bedroom datasets.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/31a841e28be8f81f1c83b34edc51b350b9000236.pdf",
      "citation_key": "roy2024k91",
      "metadata": {
        "title": "A Distributed Conditional Wasserstein Deep Convolutional Relativistic Loss Generative Adversarial Network With Improved Convergence",
        "authors": [
          "Arunava Roy",
          "Dipankar Dasgupta"
        ],
        "published_date": "2024",
        "abstract": "Generative adversarial networks (GANs) excel in diverse applications such as image enhancement, manipulation, and generating images and videos from text. Yet, training GANs with large datasets remains computationally intensive for standalone systems. Synchronization issues between the generator and discriminator lead to unstable training, poor convergence, vanishing, and exploding gradient challenges. In decentralized environments, standalone GANs struggle with distributed data on client machines. Researchers have turned to federated learning (FL) for distributed-GAN (D-GAN) implementations, but efforts often fall short due to training instability and poor synchronization within GAN components. In this study, we present DRL-GAN, a lightweight Wasserstein conditional distributed relativistic loss-GAN designed to overcome existing limitations. DRL-GAN ensures training stability in the face of nonconvex losses by employing a single global generator on the central server and a discriminator per client. Utilizing Wasserstein-1 for relativistic loss computation between real and fake samples, DRL-GAN effectively addresses issues, such as mode collapses, vanishing, and exploding gradients, accommodating both iid and non-iid private data in clients and fostering strong convergence. The absence of a robust conditional distributed-GAN model serves as another motivation for this work. We provide a comprehensive mathematical formulation of DRL-GAN and validate our claims empirically on CIFAR-10, MNIST, EuroSAT, and LSUN-Bedroom datasets.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/31a841e28be8f81f1c83b34edc51b350b9000236.pdf",
        "venue": "IEEE Transactions on Artificial Intelligence",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Generative adversarial networks (GANs) excel in diverse applications such as image enhancement, manipulation, and generating images and videos from text. Yet, training GANs with large datasets remains computationally intensive for standalone systems. Synchronization issues between the generator and discriminator lead to unstable training, poor convergence, vanishing, and exploding gradient challenges. In decentralized environments, standalone GANs struggle with distributed data on client machines. Researchers have turned to federated learning (FL) for distributed-GAN (D-GAN) implementations, but efforts often fall short due to training instability and poor synchronization within GAN components. In this study, we present DRL-GAN, a lightweight Wasserstein conditional distributed relativistic loss-GAN designed to overcome existing limitations. DRL-GAN ensures training stability in the face of nonconvex losses by employing a single global generator on the central server and a discriminator per client. Utilizing Wasserstein-1 for relativistic loss computation between real and fake samples, DRL-GAN effectively addresses issues, such as mode collapses, vanishing, and exploding gradients, accommodating both iid and non-iid private data in clients and fostering strong convergence. The absence of a robust conditional distributed-GAN model serves as another motivation for this work. We provide a comprehensive mathematical formulation of DRL-GAN and validate our claims empirically on CIFAR-10, MNIST, EuroSAT, and LSUN-Bedroom datasets.",
        "keywords": []
      },
      "file_name": "31a841e28be8f81f1c83b34edc51b350b9000236.pdf"
    },
    {
      "success": true,
      "doc_id": "a1352360643c97a2e84c68b77f1ce634",
      "summary": "In industrial Internet of Things (IIoT) systems, imbalanced datasets are prevalent because of the relative ease of acquiring normal operational data compared to abnormal or faulty data. An unbalanced distribution of data may lead to a biased learning problem, resulting in performance degradation of deep learning models. Data augmentation approaches based on generative adversarial networks (GAN) have been proposed to mitigate biased learning problems. However, GAN-based approaches constructed solely with convolutional neural networks may be incapable of extracting temporal properties from data. To utilize the temporal properties of data, a novel GAN structure consisting of an embedding network and recurrent neural networks is proposed in this paper. Additionally, in the novel GAN model based on mean-squared error, modified loss and mutual information terms are employed to improve training stability. From simulation results, it is confirmed that classification accuracy can be significantly improved by up to 54% based on the proposed method when compared with conventional fault diagnosis methods.",
      "intriguing_abstract": "In industrial Internet of Things (IIoT) systems, imbalanced datasets are prevalent because of the relative ease of acquiring normal operational data compared to abnormal or faulty data. An unbalanced distribution of data may lead to a biased learning problem, resulting in performance degradation of deep learning models. Data augmentation approaches based on generative adversarial networks (GAN) have been proposed to mitigate biased learning problems. However, GAN-based approaches constructed solely with convolutional neural networks may be incapable of extracting temporal properties from data. To utilize the temporal properties of data, a novel GAN structure consisting of an embedding network and recurrent neural networks is proposed in this paper. Additionally, in the novel GAN model based on mean-squared error, modified loss and mutual information terms are employed to improve training stability. From simulation results, it is confirmed that classification accuracy can be significantly improved by up to 54% based on the proposed method when compared with conventional fault diagnosis methods.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/045884983c01e75cda7d299e0d31530dd4019b69.pdf",
      "citation_key": "seon202526r",
      "metadata": {
        "title": "Least Information Spectral GAN With Time-Series Data Augmentation for Industrial IoT",
        "authors": [
          "Joonho Seon",
          "Seongwoo Lee",
          "Youngghyu Sun",
          "Soohyun Kim",
          "Dong In Kim",
          "Jin Young Kim"
        ],
        "published_date": "2025",
        "abstract": "In industrial Internet of Things (IIoT) systems, imbalanced datasets are prevalent because of the relative ease of acquiring normal operational data compared to abnormal or faulty data. An unbalanced distribution of data may lead to a biased learning problem, resulting in performance degradation of deep learning models. Data augmentation approaches based on generative adversarial networks (GAN) have been proposed to mitigate biased learning problems. However, GAN-based approaches constructed solely with convolutional neural networks may be incapable of extracting temporal properties from data. To utilize the temporal properties of data, a novel GAN structure consisting of an embedding network and recurrent neural networks is proposed in this paper. Additionally, in the novel GAN model based on mean-squared error, modified loss and mutual information terms are employed to improve training stability. From simulation results, it is confirmed that classification accuracy can be significantly improved by up to 54% based on the proposed method when compared with conventional fault diagnosis methods.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/045884983c01e75cda7d299e0d31530dd4019b69.pdf",
        "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence",
        "citationCount": 4,
        "score": 4.0,
        "summary": "In industrial Internet of Things (IIoT) systems, imbalanced datasets are prevalent because of the relative ease of acquiring normal operational data compared to abnormal or faulty data. An unbalanced distribution of data may lead to a biased learning problem, resulting in performance degradation of deep learning models. Data augmentation approaches based on generative adversarial networks (GAN) have been proposed to mitigate biased learning problems. However, GAN-based approaches constructed solely with convolutional neural networks may be incapable of extracting temporal properties from data. To utilize the temporal properties of data, a novel GAN structure consisting of an embedding network and recurrent neural networks is proposed in this paper. Additionally, in the novel GAN model based on mean-squared error, modified loss and mutual information terms are employed to improve training stability. From simulation results, it is confirmed that classification accuracy can be significantly improved by up to 54% based on the proposed method when compared with conventional fault diagnosis methods.",
        "keywords": []
      },
      "file_name": "045884983c01e75cda7d299e0d31530dd4019b69.pdf"
    },
    {
      "success": true,
      "doc_id": "fe96285cf84bb30c9e6bb3a84fa509d9",
      "summary": "Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lips-chitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our theoretical analyses firmly establishes CHAIN's effectiveness in reducing gradients in latent features and weights, improving stability and generalization in GAN training. Empirical evidence supports our theory. CHAIN achieves state-of-the-art results in data-limited scenarios on CIFAR-10/100, ImageNet, five low-shot and seven high-resolution few-shot image datasets.",
      "intriguing_abstract": "Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lips-chitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our theoretical analyses firmly establishes CHAIN's effectiveness in reducing gradients in latent features and weights, improving stability and generalization in GAN training. Empirical evidence supports our theory. CHAIN achieves state-of-the-art results in data-limited scenarios on CIFAR-10/100, ImageNet, five low-shot and seven high-resolution few-shot image datasets.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/a3a910ba06e4d5564ac3763f617f220d8fd4a146.pdf",
      "citation_key": "ni2024y70",
      "metadata": {
        "title": "$\\bigcirc\\!\\!\\!\\!\\bigcirc$ CHAIN: Enhancing Generalization in Data-Efficient GANs via LipsCHitz Continuity ConstrAIned Normalization",
        "authors": [
          "Yao Ni",
          "Piotr Koniusz"
        ],
        "published_date": "2024",
        "abstract": "Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lips-chitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our theoretical analyses firmly establishes CHAIN's effectiveness in reducing gradients in latent features and weights, improving stability and generalization in GAN training. Empirical evidence supports our theory. CHAIN achieves state-of-the-art results in data-limited scenarios on CIFAR-10/100, ImageNet, five low-shot and seven high-resolution few-shot image datasets.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/a3a910ba06e4d5564ac3763f617f220d8fd4a146.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lips-chitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our theoretical analyses firmly establishes CHAIN's effectiveness in reducing gradients in latent features and weights, improving stability and generalization in GAN training. Empirical evidence supports our theory. CHAIN achieves state-of-the-art results in data-limited scenarios on CIFAR-10/100, ImageNet, five low-shot and seven high-resolution few-shot image datasets.",
        "keywords": []
      },
      "file_name": "a3a910ba06e4d5564ac3763f617f220d8fd4a146.pdf"
    },
    {
      "success": true,
      "doc_id": "5d1317d0979289378a28125f2f980b89",
      "summary": "Channel estimation is a challenging task in intelligent reflecting surface (IRS)-assisted communication systems due to the large amount of passive IRS elements. Recently, deep learning (DL) based channel estimation schemes for multiple-input multiple-output (MIMO) communication systems have achieved remarkable success. However, the performance of channel estimation algorithms still needs to be improved. Meanwhile, the loss functions in traditional DL-based methods are not well designed and investigated. In this paper, we propose a generative adversarial network (GAN) variant based channel estimation method to improve the channel estimation accuracy. Specifically, two DL networks are trained adversarially with the received signals as the conditional input to learn an adaptive loss function. Furthermore, the GAN variant can also learn the mapping from the received signals to the real channels. To improve the training stability of GANs, a loss function is proposed to ensure the correct optimization direction of training the generator. To further improve the estimation performance, we investigate the influence of the hyper-parameter of the loss function on the performance of our model. Our extensive simulation results show that the proposed method outperforms traditional DL-based methods and shows great robustness.",
      "intriguing_abstract": "Channel estimation is a challenging task in intelligent reflecting surface (IRS)-assisted communication systems due to the large amount of passive IRS elements. Recently, deep learning (DL) based channel estimation schemes for multiple-input multiple-output (MIMO) communication systems have achieved remarkable success. However, the performance of channel estimation algorithms still needs to be improved. Meanwhile, the loss functions in traditional DL-based methods are not well designed and investigated. In this paper, we propose a generative adversarial network (GAN) variant based channel estimation method to improve the channel estimation accuracy. Specifically, two DL networks are trained adversarially with the received signals as the conditional input to learn an adaptive loss function. Furthermore, the GAN variant can also learn the mapping from the received signals to the real channels. To improve the training stability of GANs, a loss function is proposed to ensure the correct optimization direction of training the generator. To further improve the estimation performance, we investigate the influence of the hyper-parameter of the loss function on the performance of our model. Our extensive simulation results show that the proposed method outperforms traditional DL-based methods and shows great robustness.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/df09f8a4dcc9b7b5698afc56a35e66dc099c2429.pdf",
      "citation_key": "ye2024n41",
      "metadata": {
        "title": "Generative Adversarial Networks-Based Channel Estimation for Intelligent Reflecting Surface Assisted mmWave MIMO Systems",
        "authors": [
          "Ming Ye",
          "Cunhua Pan",
          "Yinfei Xu",
          "Chunguo Li"
        ],
        "published_date": "2024",
        "abstract": "Channel estimation is a challenging task in intelligent reflecting surface (IRS)-assisted communication systems due to the large amount of passive IRS elements. Recently, deep learning (DL) based channel estimation schemes for multiple-input multiple-output (MIMO) communication systems have achieved remarkable success. However, the performance of channel estimation algorithms still needs to be improved. Meanwhile, the loss functions in traditional DL-based methods are not well designed and investigated. In this paper, we propose a generative adversarial network (GAN) variant based channel estimation method to improve the channel estimation accuracy. Specifically, two DL networks are trained adversarially with the received signals as the conditional input to learn an adaptive loss function. Furthermore, the GAN variant can also learn the mapping from the received signals to the real channels. To improve the training stability of GANs, a loss function is proposed to ensure the correct optimization direction of training the generator. To further improve the estimation performance, we investigate the influence of the hyper-parameter of the loss function on the performance of our model. Our extensive simulation results show that the proposed method outperforms traditional DL-based methods and shows great robustness.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/df09f8a4dcc9b7b5698afc56a35e66dc099c2429.pdf",
        "venue": "IEEE Transactions on Cognitive Communications and Networking",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Channel estimation is a challenging task in intelligent reflecting surface (IRS)-assisted communication systems due to the large amount of passive IRS elements. Recently, deep learning (DL) based channel estimation schemes for multiple-input multiple-output (MIMO) communication systems have achieved remarkable success. However, the performance of channel estimation algorithms still needs to be improved. Meanwhile, the loss functions in traditional DL-based methods are not well designed and investigated. In this paper, we propose a generative adversarial network (GAN) variant based channel estimation method to improve the channel estimation accuracy. Specifically, two DL networks are trained adversarially with the received signals as the conditional input to learn an adaptive loss function. Furthermore, the GAN variant can also learn the mapping from the received signals to the real channels. To improve the training stability of GANs, a loss function is proposed to ensure the correct optimization direction of training the generator. To further improve the estimation performance, we investigate the influence of the hyper-parameter of the loss function on the performance of our model. Our extensive simulation results show that the proposed method outperforms traditional DL-based methods and shows great robustness.",
        "keywords": []
      },
      "file_name": "df09f8a4dcc9b7b5698afc56a35e66dc099c2429.pdf"
    },
    {
      "success": true,
      "doc_id": "7e71d4176bacfe45e24541545c199150",
      "summary": "Quantum Generative Adversarial Networks (QGANs) represent a useful development in quantum machine learning, using the particular properties of quantum mechanics to solve the challenges of data analysis and modeling. This paper brings up a general analysis of five QGAN architectures, focusing on their evolution, strengths, weaknesses, and limitations in noisy intermediate-scale quantum (NISQ) devices. Primary methods like Entangling Quantum GAN (EQ-GAN) and Quantum state fidelity (QuGAN) concentrate on stability, convergence, and robust performance on small-scale datasets such as 2 × 2 grayscale images. Intermediate models such as Image Quantum GAN (IQGAN) and Experimental Quantum GAN (EXQGAN) provide new ideas like trainable encoders and patch-based sub-generators that are scalable to 8 × 8 datasets with increasing noise resilience. The most advanced method is Parameterized Quantum Wasserstein GAN (PQWGAN), which uses a hybrid quantum-classical structure to obtain high-resolution image processing for 28 × 28 grayscale datasets while trying to maintain parameter efficiency. This study explores, analyzes, and summarizes critical problems of QGANs, including accuracy, convergence, parameter efficiency, image quality, performance metrics, and training stability under noisy conditions. In addition, developing QGANs can generate and train parameters in quantum approximation optimization algorithms. One of the useful applications of QGAN is generating medical datasets that can generate medical images from limited datasets to train specific medical models for the recognition of diseases.",
      "intriguing_abstract": "Quantum Generative Adversarial Networks (QGANs) represent a useful development in quantum machine learning, using the particular properties of quantum mechanics to solve the challenges of data analysis and modeling. This paper brings up a general analysis of five QGAN architectures, focusing on their evolution, strengths, weaknesses, and limitations in noisy intermediate-scale quantum (NISQ) devices. Primary methods like Entangling Quantum GAN (EQ-GAN) and Quantum state fidelity (QuGAN) concentrate on stability, convergence, and robust performance on small-scale datasets such as 2 × 2 grayscale images. Intermediate models such as Image Quantum GAN (IQGAN) and Experimental Quantum GAN (EXQGAN) provide new ideas like trainable encoders and patch-based sub-generators that are scalable to 8 × 8 datasets with increasing noise resilience. The most advanced method is Parameterized Quantum Wasserstein GAN (PQWGAN), which uses a hybrid quantum-classical structure to obtain high-resolution image processing for 28 × 28 grayscale datasets while trying to maintain parameter efficiency. This study explores, analyzes, and summarizes critical problems of QGANs, including accuracy, convergence, parameter efficiency, image quality, performance metrics, and training stability under noisy conditions. In addition, developing QGANs can generate and train parameters in quantum approximation optimization algorithms. One of the useful applications of QGAN is generating medical datasets that can generate medical images from limited datasets to train specific medical models for the recognition of diseases.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/fdd5b6806c9b45b94ff59840018a155a990f11c9.pdf",
      "citation_key": "pajuhanfard2024ult",
      "metadata": {
        "title": "Survey of Quantum Generative Adversarial Networks (QGAN) to Generate Images",
        "authors": [
          "Mohammadsaleh Pajuhanfard",
          "Rasoul Kiani",
          "Victor S. Sheng"
        ],
        "published_date": "2024",
        "abstract": "Quantum Generative Adversarial Networks (QGANs) represent a useful development in quantum machine learning, using the particular properties of quantum mechanics to solve the challenges of data analysis and modeling. This paper brings up a general analysis of five QGAN architectures, focusing on their evolution, strengths, weaknesses, and limitations in noisy intermediate-scale quantum (NISQ) devices. Primary methods like Entangling Quantum GAN (EQ-GAN) and Quantum state fidelity (QuGAN) concentrate on stability, convergence, and robust performance on small-scale datasets such as 2 × 2 grayscale images. Intermediate models such as Image Quantum GAN (IQGAN) and Experimental Quantum GAN (EXQGAN) provide new ideas like trainable encoders and patch-based sub-generators that are scalable to 8 × 8 datasets with increasing noise resilience. The most advanced method is Parameterized Quantum Wasserstein GAN (PQWGAN), which uses a hybrid quantum-classical structure to obtain high-resolution image processing for 28 × 28 grayscale datasets while trying to maintain parameter efficiency. This study explores, analyzes, and summarizes critical problems of QGANs, including accuracy, convergence, parameter efficiency, image quality, performance metrics, and training stability under noisy conditions. In addition, developing QGANs can generate and train parameters in quantum approximation optimization algorithms. One of the useful applications of QGAN is generating medical datasets that can generate medical images from limited datasets to train specific medical models for the recognition of diseases.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/fdd5b6806c9b45b94ff59840018a155a990f11c9.pdf",
        "venue": "Mathematics",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Quantum Generative Adversarial Networks (QGANs) represent a useful development in quantum machine learning, using the particular properties of quantum mechanics to solve the challenges of data analysis and modeling. This paper brings up a general analysis of five QGAN architectures, focusing on their evolution, strengths, weaknesses, and limitations in noisy intermediate-scale quantum (NISQ) devices. Primary methods like Entangling Quantum GAN (EQ-GAN) and Quantum state fidelity (QuGAN) concentrate on stability, convergence, and robust performance on small-scale datasets such as 2 × 2 grayscale images. Intermediate models such as Image Quantum GAN (IQGAN) and Experimental Quantum GAN (EXQGAN) provide new ideas like trainable encoders and patch-based sub-generators that are scalable to 8 × 8 datasets with increasing noise resilience. The most advanced method is Parameterized Quantum Wasserstein GAN (PQWGAN), which uses a hybrid quantum-classical structure to obtain high-resolution image processing for 28 × 28 grayscale datasets while trying to maintain parameter efficiency. This study explores, analyzes, and summarizes critical problems of QGANs, including accuracy, convergence, parameter efficiency, image quality, performance metrics, and training stability under noisy conditions. In addition, developing QGANs can generate and train parameters in quantum approximation optimization algorithms. One of the useful applications of QGAN is generating medical datasets that can generate medical images from limited datasets to train specific medical models for the recognition of diseases.",
        "keywords": []
      },
      "file_name": "fdd5b6806c9b45b94ff59840018a155a990f11c9.pdf"
    },
    {
      "success": true,
      "doc_id": "a56d2ad26866a1179389aa07d44ef790",
      "summary": "Generating time series data using Generative Adversarial Networks (GANs) presents several prevalent challenges, such as slow convergence, information loss in embedding spaces, instability, and performance variability depending on the se-ries length. To tackle these obstacles, we introduce a robust framework aimed at addressing and mitigating these issues effectively. This advanced framework integrates the benefits of an Autoencoder-generated embedding space with the adversarial training dynamics of GANs. This framework benefits from a time series-based loss function and oversight from a supervisory net-work, both of which capture the stepwise conditional distributions of the data effectively. The generator functions within the latent space, while the discriminator offers essential feedback based on the feature space. Moreover, we introduce an early generation algorithm and an improved neural network architecture to enhance stability and ensure effective generalization across both short and long time series. Through joint training, our framework consistently outperforms existing benchmarks, generating high-quality time series data across a range of real and synthetic datasets with diverse characteristics.",
      "intriguing_abstract": "Generating time series data using Generative Adversarial Networks (GANs) presents several prevalent challenges, such as slow convergence, information loss in embedding spaces, instability, and performance variability depending on the se-ries length. To tackle these obstacles, we introduce a robust framework aimed at addressing and mitigating these issues effectively. This advanced framework integrates the benefits of an Autoencoder-generated embedding space with the adversarial training dynamics of GANs. This framework benefits from a time series-based loss function and oversight from a supervisory net-work, both of which capture the stepwise conditional distributions of the data effectively. The generator functions within the latent space, while the discriminator offers essential feedback based on the feature space. Moreover, we introduce an early generation algorithm and an improved neural network architecture to enhance stability and ensure effective generalization across both short and long time series. Through joint training, our framework consistently outperforms existing benchmarks, generating high-quality time series data across a range of real and synthetic datasets with diverse characteristics.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/d8166043f684461068b59060b968e9eced7b03c4.pdf",
      "citation_key": "eskandarinasab202431h",
      "metadata": {
        "title": "ChronoGAN: Supervised and Embedded Generative Adversarial Networks for Time Series Generation",
        "authors": [
          "MohammadReza EskandariNasab",
          "S. M. Hamdi",
          "S. F. Boubrahimi"
        ],
        "published_date": "2024",
        "abstract": "Generating time series data using Generative Adversarial Networks (GANs) presents several prevalent challenges, such as slow convergence, information loss in embedding spaces, instability, and performance variability depending on the se-ries length. To tackle these obstacles, we introduce a robust framework aimed at addressing and mitigating these issues effectively. This advanced framework integrates the benefits of an Autoencoder-generated embedding space with the adversarial training dynamics of GANs. This framework benefits from a time series-based loss function and oversight from a supervisory net-work, both of which capture the stepwise conditional distributions of the data effectively. The generator functions within the latent space, while the discriminator offers essential feedback based on the feature space. Moreover, we introduce an early generation algorithm and an improved neural network architecture to enhance stability and ensure effective generalization across both short and long time series. Through joint training, our framework consistently outperforms existing benchmarks, generating high-quality time series data across a range of real and synthetic datasets with diverse characteristics.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/d8166043f684461068b59060b968e9eced7b03c4.pdf",
        "venue": "International Conference on Machine Learning and Applications",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Generating time series data using Generative Adversarial Networks (GANs) presents several prevalent challenges, such as slow convergence, information loss in embedding spaces, instability, and performance variability depending on the se-ries length. To tackle these obstacles, we introduce a robust framework aimed at addressing and mitigating these issues effectively. This advanced framework integrates the benefits of an Autoencoder-generated embedding space with the adversarial training dynamics of GANs. This framework benefits from a time series-based loss function and oversight from a supervisory net-work, both of which capture the stepwise conditional distributions of the data effectively. The generator functions within the latent space, while the discriminator offers essential feedback based on the feature space. Moreover, we introduce an early generation algorithm and an improved neural network architecture to enhance stability and ensure effective generalization across both short and long time series. Through joint training, our framework consistently outperforms existing benchmarks, generating high-quality time series data across a range of real and synthetic datasets with diverse characteristics.",
        "keywords": []
      },
      "file_name": "d8166043f684461068b59060b968e9eced7b03c4.pdf"
    },
    {
      "success": true,
      "doc_id": "be905492c657f57d069b9f5ce6507af4",
      "summary": "Current breast cancer diagnosis methods often face limitations such as high cost, time consumption, and inter-observer variability. To address these challenges, this research proposes a novel deep learning framework that leverages generative adversarial networks (GANs) for data augmentation and transfer learning to enhance breast cancer classification using convolutional neural networks (CNNs). The framework uses a two-stage augmentation approach. First, a conditional Wasserstein GAN (cWGAN) generates synthetic breast cancer images based on clinical data, enhancing training stability and enabling targeted feature incorporation. Second, traditional augmentation techniques (e.g., rotation, flipping, cropping) are applied to both original and synthetic images. A multi-scale transfer learning technique is also employed, integrating three pre-trained CNNs (DenseNet-201, NasNetMobile, ResNet-101) with a multi-scale feature enrichment scheme, allowing the model to capture features at various scales. The framework was evaluated on the BreakHis dataset, achieving an accuracy of 99.2% for binary classification and 98.5% for multi-class classification, significantly outperforming existing methods. This framework offers a more efficient, cost-effective, and accurate approach for breast cancer diagnosis. Future work will focus on generalizing the framework to clinical datasets and integrating it into diagnostic workflows.",
      "intriguing_abstract": "Current breast cancer diagnosis methods often face limitations such as high cost, time consumption, and inter-observer variability. To address these challenges, this research proposes a novel deep learning framework that leverages generative adversarial networks (GANs) for data augmentation and transfer learning to enhance breast cancer classification using convolutional neural networks (CNNs). The framework uses a two-stage augmentation approach. First, a conditional Wasserstein GAN (cWGAN) generates synthetic breast cancer images based on clinical data, enhancing training stability and enabling targeted feature incorporation. Second, traditional augmentation techniques (e.g., rotation, flipping, cropping) are applied to both original and synthetic images. A multi-scale transfer learning technique is also employed, integrating three pre-trained CNNs (DenseNet-201, NasNetMobile, ResNet-101) with a multi-scale feature enrichment scheme, allowing the model to capture features at various scales. The framework was evaluated on the BreakHis dataset, achieving an accuracy of 99.2% for binary classification and 98.5% for multi-class classification, significantly outperforming existing methods. This framework offers a more efficient, cost-effective, and accurate approach for breast cancer diagnosis. Future work will focus on generalizing the framework to clinical datasets and integrating it into diagnostic workflows.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/450d90df20a8b050b0e788253d98cf7ac0c14274.pdf",
      "citation_key": "deebani202549r",
      "metadata": {
        "title": "Synergistic transfer learning and adversarial networks for breast cancer diagnosis: benign vs. invasive classification",
        "authors": [
          "Wejdan Deebani",
          "Lubna Aziz",
          "Arshad Aziz",
          "Wael Sh. Basri",
          "Wedad Alawad",
          "S. Althubiti"
        ],
        "published_date": "2025",
        "abstract": "Current breast cancer diagnosis methods often face limitations such as high cost, time consumption, and inter-observer variability. To address these challenges, this research proposes a novel deep learning framework that leverages generative adversarial networks (GANs) for data augmentation and transfer learning to enhance breast cancer classification using convolutional neural networks (CNNs). The framework uses a two-stage augmentation approach. First, a conditional Wasserstein GAN (cWGAN) generates synthetic breast cancer images based on clinical data, enhancing training stability and enabling targeted feature incorporation. Second, traditional augmentation techniques (e.g., rotation, flipping, cropping) are applied to both original and synthetic images. A multi-scale transfer learning technique is also employed, integrating three pre-trained CNNs (DenseNet-201, NasNetMobile, ResNet-101) with a multi-scale feature enrichment scheme, allowing the model to capture features at various scales. The framework was evaluated on the BreakHis dataset, achieving an accuracy of 99.2% for binary classification and 98.5% for multi-class classification, significantly outperforming existing methods. This framework offers a more efficient, cost-effective, and accurate approach for breast cancer diagnosis. Future work will focus on generalizing the framework to clinical datasets and integrating it into diagnostic workflows.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/450d90df20a8b050b0e788253d98cf7ac0c14274.pdf",
        "venue": "Scientific Reports",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Current breast cancer diagnosis methods often face limitations such as high cost, time consumption, and inter-observer variability. To address these challenges, this research proposes a novel deep learning framework that leverages generative adversarial networks (GANs) for data augmentation and transfer learning to enhance breast cancer classification using convolutional neural networks (CNNs). The framework uses a two-stage augmentation approach. First, a conditional Wasserstein GAN (cWGAN) generates synthetic breast cancer images based on clinical data, enhancing training stability and enabling targeted feature incorporation. Second, traditional augmentation techniques (e.g., rotation, flipping, cropping) are applied to both original and synthetic images. A multi-scale transfer learning technique is also employed, integrating three pre-trained CNNs (DenseNet-201, NasNetMobile, ResNet-101) with a multi-scale feature enrichment scheme, allowing the model to capture features at various scales. The framework was evaluated on the BreakHis dataset, achieving an accuracy of 99.2% for binary classification and 98.5% for multi-class classification, significantly outperforming existing methods. This framework offers a more efficient, cost-effective, and accurate approach for breast cancer diagnosis. Future work will focus on generalizing the framework to clinical datasets and integrating it into diagnostic workflows.",
        "keywords": []
      },
      "file_name": "450d90df20a8b050b0e788253d98cf7ac0c14274.pdf"
    },
    {
      "success": true,
      "doc_id": "0a0c9f0c64748af2cf0a04c11b533464",
      "summary": "Deep learning has played a vital role in advancing medical research, particularly in brain tumor segmentation. Despite using numerous deep learning algorithms for this purpose, accurately and reliably segmenting brain tumors remains a significant challenge. Segmentation of precise tumors is essential for the effective treatment of brain diseases. While deep learning offers a range of algorithms for segmentation, they still face limitations when analyzing medical images due to the variations in tumor shape, size, and location. This study proposes a deep learning approach combining a Generative Adversarial Network (GAN) with transfer learning and auto-encoder techniques to enhance brain tumor segmentation. The GAN incorporates a generator and discriminator to generate superior segmentation outcomes. In the generator, we applied downsampling and upsampling for tumor segmentation. In addition, an auto-encoder is applied in which the encoder retains as much information as possible and then the decoder with those encodings reconstructs the image. The transfer learning technique is applied at the bottleneck using the DenseNet model. Combining auto-encoder techniques with transfer learning methodologies in GANs feature learning is enhanced, training time is reduced, and stability is increased. In this work, we enhanced the accuracy of brain tumor segmentation and even achieved better results for tumors having small sizes. We train and evaluate our proposed model using the publicly available BraTS 2021 dataset. The experimental result shows a dice score of 0.94 for the whole tumor, 0.86 for the tumor core, and 0.82 for the enhancing tumor. It is also shown that we achieve 2% to 4% higher accuracy than other methods.",
      "intriguing_abstract": "Deep learning has played a vital role in advancing medical research, particularly in brain tumor segmentation. Despite using numerous deep learning algorithms for this purpose, accurately and reliably segmenting brain tumors remains a significant challenge. Segmentation of precise tumors is essential for the effective treatment of brain diseases. While deep learning offers a range of algorithms for segmentation, they still face limitations when analyzing medical images due to the variations in tumor shape, size, and location. This study proposes a deep learning approach combining a Generative Adversarial Network (GAN) with transfer learning and auto-encoder techniques to enhance brain tumor segmentation. The GAN incorporates a generator and discriminator to generate superior segmentation outcomes. In the generator, we applied downsampling and upsampling for tumor segmentation. In addition, an auto-encoder is applied in which the encoder retains as much information as possible and then the decoder with those encodings reconstructs the image. The transfer learning technique is applied at the bottleneck using the DenseNet model. Combining auto-encoder techniques with transfer learning methodologies in GANs feature learning is enhanced, training time is reduced, and stability is increased. In this work, we enhanced the accuracy of brain tumor segmentation and even achieved better results for tumors having small sizes. We train and evaluate our proposed model using the publicly available BraTS 2021 dataset. The experimental result shows a dice score of 0.94 for the whole tumor, 0.86 for the tumor core, and 0.82 for the enhancing tumor. It is also shown that we achieve 2% to 4% higher accuracy than other methods.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/b8ba0bfe19bafb14a4eb30c2d22c2e8f1a7d1160.pdf",
      "citation_key": "ali2024ks3",
      "metadata": {
        "title": "Brain Tumor Segmentation Using Generative Adversarial Networks",
        "authors": [
          "Abid Ali",
          "Muhammad Sharif",
          "Muhammad Shahzad Faisal",
          "Atif Rizwan",
          "G. Atteia",
          "Maali Alabdulhafith"
        ],
        "published_date": "2024",
        "abstract": "Deep learning has played a vital role in advancing medical research, particularly in brain tumor segmentation. Despite using numerous deep learning algorithms for this purpose, accurately and reliably segmenting brain tumors remains a significant challenge. Segmentation of precise tumors is essential for the effective treatment of brain diseases. While deep learning offers a range of algorithms for segmentation, they still face limitations when analyzing medical images due to the variations in tumor shape, size, and location. This study proposes a deep learning approach combining a Generative Adversarial Network (GAN) with transfer learning and auto-encoder techniques to enhance brain tumor segmentation. The GAN incorporates a generator and discriminator to generate superior segmentation outcomes. In the generator, we applied downsampling and upsampling for tumor segmentation. In addition, an auto-encoder is applied in which the encoder retains as much information as possible and then the decoder with those encodings reconstructs the image. The transfer learning technique is applied at the bottleneck using the DenseNet model. Combining auto-encoder techniques with transfer learning methodologies in GANs feature learning is enhanced, training time is reduced, and stability is increased. In this work, we enhanced the accuracy of brain tumor segmentation and even achieved better results for tumors having small sizes. We train and evaluate our proposed model using the publicly available BraTS 2021 dataset. The experimental result shows a dice score of 0.94 for the whole tumor, 0.86 for the tumor core, and 0.82 for the enhancing tumor. It is also shown that we achieve 2% to 4% higher accuracy than other methods.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/b8ba0bfe19bafb14a4eb30c2d22c2e8f1a7d1160.pdf",
        "venue": "IEEE Access",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Deep learning has played a vital role in advancing medical research, particularly in brain tumor segmentation. Despite using numerous deep learning algorithms for this purpose, accurately and reliably segmenting brain tumors remains a significant challenge. Segmentation of precise tumors is essential for the effective treatment of brain diseases. While deep learning offers a range of algorithms for segmentation, they still face limitations when analyzing medical images due to the variations in tumor shape, size, and location. This study proposes a deep learning approach combining a Generative Adversarial Network (GAN) with transfer learning and auto-encoder techniques to enhance brain tumor segmentation. The GAN incorporates a generator and discriminator to generate superior segmentation outcomes. In the generator, we applied downsampling and upsampling for tumor segmentation. In addition, an auto-encoder is applied in which the encoder retains as much information as possible and then the decoder with those encodings reconstructs the image. The transfer learning technique is applied at the bottleneck using the DenseNet model. Combining auto-encoder techniques with transfer learning methodologies in GANs feature learning is enhanced, training time is reduced, and stability is increased. In this work, we enhanced the accuracy of brain tumor segmentation and even achieved better results for tumors having small sizes. We train and evaluate our proposed model using the publicly available BraTS 2021 dataset. The experimental result shows a dice score of 0.94 for the whole tumor, 0.86 for the tumor core, and 0.82 for the enhancing tumor. It is also shown that we achieve 2% to 4% higher accuracy than other methods.",
        "keywords": []
      },
      "file_name": "b8ba0bfe19bafb14a4eb30c2d22c2e8f1a7d1160.pdf"
    },
    {
      "success": true,
      "doc_id": "dd82efd2e0089a558578d06d0d8c6b35",
      "summary": "In oncology, melanoma is a serious concern, often arising from DNA changes caused mainly by ultraviolet radiation. This cancer is known for its aggressive growth, highlighting the necessity of early detection. Our research introduces a novel deep learning framework for melanoma classification, trained and validated using the extensive SIIM‐ISIC Melanoma Classification Challenge‐ISIC‐2020 dataset. The framework features three dilated convolution layers that extract critical feature vectors for classification. A key aspect of our model is incorporating the Off‐policy Proximal Policy Optimization (Off‐policy PPO) algorithm, which effectively handles data imbalance in the training set by rewarding the accurate classification of underrepresented samples. In this framework, the model is visualized as an agent making a series of decisions, where each sample represents a distinct state. Additionally, a Generative Adversarial Network (GAN) augments training data to improve generalizability, paired with a new regularization technique to stabilize GAN training and prevent mode collapse. The model achieved an F‐measure of 91.836% and a geometric mean of 91.920%, surpassing existing models and demonstrating the model's practical utility in clinical environments. These results demonstrate its potential in enhancing early melanoma detection and informing more accurate treatment approaches, significantly advancing in combating this aggressive cancer.",
      "intriguing_abstract": "In oncology, melanoma is a serious concern, often arising from DNA changes caused mainly by ultraviolet radiation. This cancer is known for its aggressive growth, highlighting the necessity of early detection. Our research introduces a novel deep learning framework for melanoma classification, trained and validated using the extensive SIIM‐ISIC Melanoma Classification Challenge‐ISIC‐2020 dataset. The framework features three dilated convolution layers that extract critical feature vectors for classification. A key aspect of our model is incorporating the Off‐policy Proximal Policy Optimization (Off‐policy PPO) algorithm, which effectively handles data imbalance in the training set by rewarding the accurate classification of underrepresented samples. In this framework, the model is visualized as an agent making a series of decisions, where each sample represents a distinct state. Additionally, a Generative Adversarial Network (GAN) augments training data to improve generalizability, paired with a new regularization technique to stabilize GAN training and prevent mode collapse. The model achieved an F‐measure of 91.836% and a geometric mean of 91.920%, surpassing existing models and demonstrating the model's practical utility in clinical environments. These results demonstrate its potential in enhancing early melanoma detection and informing more accurate treatment approaches, significantly advancing in combating this aggressive cancer.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/316fd1658e4ae59cdbeaf5caa03f46d4d32d616d.pdf",
      "citation_key": "ju2024uai",
      "metadata": {
        "title": "Melanoma classification using generative adversarial network and proximal policy optimization",
        "authors": [
          "Xiangui Ju",
          "Chi-Ho Lin",
          "Suan Lee",
          "Sizheng Wei"
        ],
        "published_date": "2024",
        "abstract": "In oncology, melanoma is a serious concern, often arising from DNA changes caused mainly by ultraviolet radiation. This cancer is known for its aggressive growth, highlighting the necessity of early detection. Our research introduces a novel deep learning framework for melanoma classification, trained and validated using the extensive SIIM‐ISIC Melanoma Classification Challenge‐ISIC‐2020 dataset. The framework features three dilated convolution layers that extract critical feature vectors for classification. A key aspect of our model is incorporating the Off‐policy Proximal Policy Optimization (Off‐policy PPO) algorithm, which effectively handles data imbalance in the training set by rewarding the accurate classification of underrepresented samples. In this framework, the model is visualized as an agent making a series of decisions, where each sample represents a distinct state. Additionally, a Generative Adversarial Network (GAN) augments training data to improve generalizability, paired with a new regularization technique to stabilize GAN training and prevent mode collapse. The model achieved an F‐measure of 91.836% and a geometric mean of 91.920%, surpassing existing models and demonstrating the model's practical utility in clinical environments. These results demonstrate its potential in enhancing early melanoma detection and informing more accurate treatment approaches, significantly advancing in combating this aggressive cancer.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/316fd1658e4ae59cdbeaf5caa03f46d4d32d616d.pdf",
        "venue": "Photochemistry and Photobiology",
        "citationCount": 3,
        "score": 3.0,
        "summary": "In oncology, melanoma is a serious concern, often arising from DNA changes caused mainly by ultraviolet radiation. This cancer is known for its aggressive growth, highlighting the necessity of early detection. Our research introduces a novel deep learning framework for melanoma classification, trained and validated using the extensive SIIM‐ISIC Melanoma Classification Challenge‐ISIC‐2020 dataset. The framework features three dilated convolution layers that extract critical feature vectors for classification. A key aspect of our model is incorporating the Off‐policy Proximal Policy Optimization (Off‐policy PPO) algorithm, which effectively handles data imbalance in the training set by rewarding the accurate classification of underrepresented samples. In this framework, the model is visualized as an agent making a series of decisions, where each sample represents a distinct state. Additionally, a Generative Adversarial Network (GAN) augments training data to improve generalizability, paired with a new regularization technique to stabilize GAN training and prevent mode collapse. The model achieved an F‐measure of 91.836% and a geometric mean of 91.920%, surpassing existing models and demonstrating the model's practical utility in clinical environments. These results demonstrate its potential in enhancing early melanoma detection and informing more accurate treatment approaches, significantly advancing in combating this aggressive cancer.",
        "keywords": []
      },
      "file_name": "316fd1658e4ae59cdbeaf5caa03f46d4d32d616d.pdf"
    },
    {
      "success": true,
      "doc_id": "adf44e7cdfcf46474e622f2a2b9391a1",
      "summary": "Probe-based confocal laser endomicroscopy (pCLE) has a role in characterising tissue intraoperatively to guide tumour resection during surgery. To capture good quality pCLE data which is important for diagnosis, the probe-tissue contact needs to be maintained within a working range of micrometre scale. This can be achieved through micro-surgical robotic manipulation which requires the automatic estimation of the probe-tissue distance. In this paper, we propose a novel deep regression framework composed of the Deep Regression Generative Adversarial Network (DR-GAN) and a Sequence Attention (SA) module. The aim of DR-GAN is to train the network using an enhanced image-based supervision approach. It extents the standard generator by using a well-defined function for image generation, instead of a learnable decoder. Also, DR-GAN uses a novel learnable neural perceptual loss which combines for the first time spatial and frequency domain features. This effectively suppresses the adverse effects of noise in the pCLE data. To incorporate temporal information, we’ve designed the SA module which is a cross-attention module, enhanced with Radial Basis Function based encoding (SA-RBF). Furthermore, to train the regression framework, we designed a multi-step training mechanism. During inference, the trained network is used to generate data representations which are fused along time in the SA-RBF module to boost the regression stability. Our proposed network advances SOTA networks by addressing the challenge of excessive noise in the pCLE data and enhancing regression stability. It outperforms SOTA networks applied on the pCLE Regression dataset (PRD) in terms of accuracy, data quality and stability.",
      "intriguing_abstract": "Probe-based confocal laser endomicroscopy (pCLE) has a role in characterising tissue intraoperatively to guide tumour resection during surgery. To capture good quality pCLE data which is important for diagnosis, the probe-tissue contact needs to be maintained within a working range of micrometre scale. This can be achieved through micro-surgical robotic manipulation which requires the automatic estimation of the probe-tissue distance. In this paper, we propose a novel deep regression framework composed of the Deep Regression Generative Adversarial Network (DR-GAN) and a Sequence Attention (SA) module. The aim of DR-GAN is to train the network using an enhanced image-based supervision approach. It extents the standard generator by using a well-defined function for image generation, instead of a learnable decoder. Also, DR-GAN uses a novel learnable neural perceptual loss which combines for the first time spatial and frequency domain features. This effectively suppresses the adverse effects of noise in the pCLE data. To incorporate temporal information, we’ve designed the SA module which is a cross-attention module, enhanced with Radial Basis Function based encoding (SA-RBF). Furthermore, to train the regression framework, we designed a multi-step training mechanism. During inference, the trained network is used to generate data representations which are fused along time in the SA-RBF module to boost the regression stability. Our proposed network advances SOTA networks by addressing the challenge of excessive noise in the pCLE data and enhancing regression stability. It outperforms SOTA networks applied on the pCLE Regression dataset (PRD) in terms of accuracy, data quality and stability.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/87220aa1684992aba1c48ab72934cfe3a8dd3c59.pdf",
      "citation_key": "xu2024u5a",
      "metadata": {
        "title": "Distance Regression Enhanced With Temporal Information Fusion and Adversarial Training for Robot-Assisted Endomicroscopy",
        "authors": [
          "Chi Xu",
          "Haozheng Xu",
          "S. Giannarou"
        ],
        "published_date": "2024",
        "abstract": "Probe-based confocal laser endomicroscopy (pCLE) has a role in characterising tissue intraoperatively to guide tumour resection during surgery. To capture good quality pCLE data which is important for diagnosis, the probe-tissue contact needs to be maintained within a working range of micrometre scale. This can be achieved through micro-surgical robotic manipulation which requires the automatic estimation of the probe-tissue distance. In this paper, we propose a novel deep regression framework composed of the Deep Regression Generative Adversarial Network (DR-GAN) and a Sequence Attention (SA) module. The aim of DR-GAN is to train the network using an enhanced image-based supervision approach. It extents the standard generator by using a well-defined function for image generation, instead of a learnable decoder. Also, DR-GAN uses a novel learnable neural perceptual loss which combines for the first time spatial and frequency domain features. This effectively suppresses the adverse effects of noise in the pCLE data. To incorporate temporal information, we’ve designed the SA module which is a cross-attention module, enhanced with Radial Basis Function based encoding (SA-RBF). Furthermore, to train the regression framework, we designed a multi-step training mechanism. During inference, the trained network is used to generate data representations which are fused along time in the SA-RBF module to boost the regression stability. Our proposed network advances SOTA networks by addressing the challenge of excessive noise in the pCLE data and enhancing regression stability. It outperforms SOTA networks applied on the pCLE Regression dataset (PRD) in terms of accuracy, data quality and stability.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/87220aa1684992aba1c48ab72934cfe3a8dd3c59.pdf",
        "venue": "IEEE Transactions on Medical Imaging",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Probe-based confocal laser endomicroscopy (pCLE) has a role in characterising tissue intraoperatively to guide tumour resection during surgery. To capture good quality pCLE data which is important for diagnosis, the probe-tissue contact needs to be maintained within a working range of micrometre scale. This can be achieved through micro-surgical robotic manipulation which requires the automatic estimation of the probe-tissue distance. In this paper, we propose a novel deep regression framework composed of the Deep Regression Generative Adversarial Network (DR-GAN) and a Sequence Attention (SA) module. The aim of DR-GAN is to train the network using an enhanced image-based supervision approach. It extents the standard generator by using a well-defined function for image generation, instead of a learnable decoder. Also, DR-GAN uses a novel learnable neural perceptual loss which combines for the first time spatial and frequency domain features. This effectively suppresses the adverse effects of noise in the pCLE data. To incorporate temporal information, we’ve designed the SA module which is a cross-attention module, enhanced with Radial Basis Function based encoding (SA-RBF). Furthermore, to train the regression framework, we designed a multi-step training mechanism. During inference, the trained network is used to generate data representations which are fused along time in the SA-RBF module to boost the regression stability. Our proposed network advances SOTA networks by addressing the challenge of excessive noise in the pCLE data and enhancing regression stability. It outperforms SOTA networks applied on the pCLE Regression dataset (PRD) in terms of accuracy, data quality and stability.",
        "keywords": []
      },
      "file_name": "87220aa1684992aba1c48ab72934cfe3a8dd3c59.pdf"
    },
    {
      "success": true,
      "doc_id": "e843c3ed835ed420ea73cb7194f1b3f3",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis: A Dual GAN with Identity Blocks and Pancreas-Inspired Loss for Renewable Energy Optimization \\cite{elbaz2025wzb}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The limited availability of high-quality datasets for renewable energy applications (e.g., energy prediction, consumption analysis, fault detection) hinders the accuracy and robustness of predictive models. Traditional Generative Adversarial Networks (GANs), while promising for data augmentation, suffer from issues like mode collapse (producing limited output variety), vanishing gradients, and pixel integrity problems, which compromise the quality and diversity of synthetic data.\n    *   **Importance & Challenge:** Accurate analysis of renewable energy imagery is crucial for optimizing production, identifying anomalies, and improving efficiency in sustainable energy systems. The scarcity of labeled data makes it challenging to train machine learning models that generalize well to real-world scenarios, necessitating innovative approaches for data augmentation.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds upon the foundational adversarial training paradigm of GANs and acknowledges advancements in cGANs, Pix2Pix, CycleGAN, StyleGAN, SRGANs, WGANs, SAGANs, ProGANs, and BEGANs, which have improved image synthesis, style control, and training stability.\n    *   **Limitations of Previous Solutions:**\n        *   Many existing GAN models (e.g., Pix2Pix, CycleGAN) rely on large-scale datasets, limiting their applicability in data-scarce domains like renewable energy.\n        *   Challenges persist in the interpretability and controllability of generated outputs, with complex latent spaces hindering user control.\n        *   Training instability remains a significant concern, requiring robust training techniques.\n        *   There is a need for GANs capable of learning from weakly supervised or noisy data, producing diverse outputs, and seamlessly integrating multiple augmentation techniques.\n        *   Previous GAN applications in renewable energy have shown limitations in mitigating mode collapse, handling vanishing gradients, and generating high-quality images with fewer datasets.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces Penca-GAN, a novel GAN architecture designed to generate high-fidelity synthetic images for renewable energy applications. It incorporates three key modifications:\n        1.  **Dual Loss Functions:** A new dual loss function is employed to ensure pixel integrity and promote diversity in augmented images, directly addressing mode collapse and improving synthetic data quality.\n        2.  **Identity Block Integration:** An identity block is integrated into the GAN framework to stabilize training, preserve essential input features, and facilitate smoother gradient flow, thereby enhancing the generator's ability to produce diverse outputs and combat mode collapse.\n        3.  **Pancreas-Inspired Metaheuristic Loss Function:** A novel loss function, inspired by the pancreas's regulatory mechanisms for maintaining homeostasis, dynamically adapts to variations in training data. This function promotes pixel coherence and enhances diversity by acting as a feedback control system, akin to how the pancreas adjusts insulin levels.\n    *   **Novelty:** The Penca-GAN's novelty lies in its holistic integration of these three components. The pancreas-inspired metaheuristic loss function is particularly novel, differing from other biological-inspired optimization techniques by employing a dynamic feedback mechanism for real-time adaptability, focusing on pixel-level integrity and diversity simultaneously, and integrating multiple facets of image generation into a comprehensive framework.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Development of a new pancreas-inspired metaheuristic loss function that dynamically adapts to training data, balancing pixel integrity and diversity.\n        *   Introduction of a novel dual loss function based on the pancreas-inspired metaheuristic.\n    *   **System Design/Architectural Innovations:**\n        *   Integration of an identity block within the GAN architecture to stabilize training, preserve input features, and mitigate mode collapse.\n        *   A novel Penca-GAN architecture that combines these elements to outperform existing GANs in image diversity and quality.\n    *   **Theoretical Insights/Analysis:** The paper draws a unique analogy between the generator's goal of producing diverse, high-quality images and the pancreas's role in maintaining biological homeostasis, providing a novel conceptual framework for loss function design.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were conducted on three distinct renewable energy datasets: SKY images, Solar images, and Wind Turbine images. The generated synthetic images were also used to enhance fault detection capabilities in solar panels and wind turbines.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **Fréchet Inception Distance (FID):** Penca-GAN consistently achieved the lowest FID scores, indicating superior image quality: 164.45 for SKY, 113.54 for Solar, and 109.34 for Wind Turbine.\n        *   **Inception Score (IS):** Penca-GAN attained the highest IS across all datasets, scoring 71.43 for SKY, 87.65 for Solar, and 90.32 for Wind Turbine, demonstrating better image diversity and quality.\n        *   **Fault Detection Accuracy:** The application of Penca-GAN significantly enhanced fault detection capabilities, improving accuracy from 85.92% to 90.04% for solar panels and from 86.06% to 90.43% for wind turbines.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** While the proposed modifications are critical, the paper notes that \"challenges in convergence persist, necessitating careful consideration of their integration.\" This implies that fine-tuning and careful integration of the dual loss and identity block are crucial for optimal performance.\n    *   **Scope of Applicability:** The primary scope is the generation of synthetic imagery for renewable energy applications (SKY, Solar, Wind Turbine images) to address data scarcity and improve downstream tasks like energy prediction and fault detection. The methodology is presented as broadly applicable to image augmentation where data quality and diversity are critical.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** Penca-GAN significantly advances the technical state-of-the-art in GAN-based data augmentation by effectively mitigating mode collapse, improving pixel integrity, and enhancing image diversity through its novel architecture and pancreas-inspired loss function. It demonstrates superior performance compared to other GAN architectures in generating high-fidelity synthetic images.\n    *   **Potential Impact on Future Research:** The robust performance of Penca-GAN in generating high-quality synthetic images has a substantial impact on renewable energy applications, particularly in improving model performance for critical tasks like fault detection and energy prediction. The novel pancreas-inspired metaheuristic loss function opens new avenues for biologically inspired optimization in generative modeling, potentially influencing future research in designing more adaptive and robust GAN architectures for various data-scarce domains. The architecture's ability to improve segmentation and detection processes suggests broader applicability.",
      "intriguing_abstract": "The burgeoning renewable energy sector critically relies on robust data for optimizing systems and detecting faults, yet faces a persistent challenge: data scarcity and the limitations of traditional Generative Adversarial Networks (GANs) in generating diverse, high-fidelity synthetic images. We introduce Penca-GAN, a novel GAN architecture engineered to overcome mode collapse and enhance synthetic data quality for renewable energy applications. Penca-GAN integrates a unique dual loss function ensuring pixel integrity and diversity, alongside an identity block for training stability. Its most groundbreaking innovation is a *pancreas-inspired metaheuristic loss function* that dynamically adapts to training data, mirroring biological homeostasis to balance pixel coherence and output variety. Extensive experiments on SKY, Solar, and Wind Turbine datasets demonstrate Penca-GAN's superior performance, achieving significantly lower Fréchet Inception Distance (FID) and higher Inception Scores (IS) than state-of-the-art GANs. Crucially, synthetic data generated by Penca-GAN boosted fault detection accuracy in solar panels and wind turbines by over 4%. This work not only advances the technical state-of-the-art in GAN-based data augmentation but also offers a powerful tool for accelerating sustainable energy innovation and opens new frontiers in biologically-inspired generative modeling.",
      "keywords": [
        "Penca-GAN",
        "Generative Adversarial Networks (GANs)",
        "Renewable energy data augmentation",
        "Pancreas-inspired metaheuristic loss",
        "Mode collapse mitigation",
        "Identity block integration",
        "Dual loss functions",
        "High-fidelity synthetic images",
        "Fault detection enhancement",
        "Fréchet Inception Distance (FID)",
        "Inception Score (IS)",
        "Training stability",
        "Data scarcity"
      ],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/d2b0b0e427f9518be18156dc6025acc5c7d32d0a.pdf",
      "citation_key": "elbaz2025wzb",
      "metadata": {
        "title": "A dual GAN with identity blocks and pancreas-inspired loss for renewable energy optimization",
        "authors": [
          "Mostafa Elbaz",
          "Wael Said",
          "G. Mahmoud",
          "H. Marie"
        ],
        "published_date": "2025",
        "abstract": "Integrating energy and solar imagery is essential for electrical engineers in renewable energy prediction, consumption analysis, regression modeling, and fault detection applications. A significant challenge in these areas is the limited availability of high-quality datasets, which can hinder the accuracy of the predictive models. To address this issue, this paper proposes leveraging Generative Adversarial Networks (GANs) to generate synthetic samples for training. Despite their potential, traditional GAN face challenges such as mode collapse, vanishing gradients, and pixel integrity issues. This paper introduces a novel architecture, Penca-GAN, which enhances GANs through three key modifications: (1) dual loss functions to ensure pixel integrity and promote diversity in augmented images, effectively mitigating mode collapse and improving the quality of synthetic data; (2) the integration of an identity block to stabilize training, preserving essential input features and facilitating smoother gradient flow; and (3) a pancreas-inspired metaheuristic loss function that dynamically adapts to variations in training data to maintain pixel coherence and diversity. Extensive experiments on three renewable energy datasets—SKY images, Solar images, and Wind Turbine images—demonstrate the effectiveness of the Penca-GAN architecture. Our comparative analysis revealed that Penca-GAN consistently achieved the lowest Fréchet Inception Distance (FID) scores (164.45 for SKY, 113.54 for Solar, and 109.34 for Wind Turbine), indicating superior image quality compared to other architectures. Additionally, it attains the highest Inception Score (IS) across all datasets, scoring 71.43 for SKY, 87.65 for Solar, and 90.32 for Wind Turbine. Furthermore, the application of Penca-GAN significantly enhanced the fault detection capabilities, achieving accuracy improvements from 85.92 to 90.04% for solar panels and from 86.06 to 90.43% for wind turbines. These results underscore Penca-GAN’s robust performance in generating high-fidelity synthetic images, significantly advancing renewable energy applications, and improving model performance in critical tasks such as fault detection and energy prediction.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/d2b0b0e427f9518be18156dc6025acc5c7d32d0a.pdf",
        "venue": "Scientific Reports",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis: A Dual GAN with Identity Blocks and Pancreas-Inspired Loss for Renewable Energy Optimization \\cite{elbaz2025wzb}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The limited availability of high-quality datasets for renewable energy applications (e.g., energy prediction, consumption analysis, fault detection) hinders the accuracy and robustness of predictive models. Traditional Generative Adversarial Networks (GANs), while promising for data augmentation, suffer from issues like mode collapse (producing limited output variety), vanishing gradients, and pixel integrity problems, which compromise the quality and diversity of synthetic data.\n    *   **Importance & Challenge:** Accurate analysis of renewable energy imagery is crucial for optimizing production, identifying anomalies, and improving efficiency in sustainable energy systems. The scarcity of labeled data makes it challenging to train machine learning models that generalize well to real-world scenarios, necessitating innovative approaches for data augmentation.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds upon the foundational adversarial training paradigm of GANs and acknowledges advancements in cGANs, Pix2Pix, CycleGAN, StyleGAN, SRGANs, WGANs, SAGANs, ProGANs, and BEGANs, which have improved image synthesis, style control, and training stability.\n    *   **Limitations of Previous Solutions:**\n        *   Many existing GAN models (e.g., Pix2Pix, CycleGAN) rely on large-scale datasets, limiting their applicability in data-scarce domains like renewable energy.\n        *   Challenges persist in the interpretability and controllability of generated outputs, with complex latent spaces hindering user control.\n        *   Training instability remains a significant concern, requiring robust training techniques.\n        *   There is a need for GANs capable of learning from weakly supervised or noisy data, producing diverse outputs, and seamlessly integrating multiple augmentation techniques.\n        *   Previous GAN applications in renewable energy have shown limitations in mitigating mode collapse, handling vanishing gradients, and generating high-quality images with fewer datasets.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces Penca-GAN, a novel GAN architecture designed to generate high-fidelity synthetic images for renewable energy applications. It incorporates three key modifications:\n        1.  **Dual Loss Functions:** A new dual loss function is employed to ensure pixel integrity and promote diversity in augmented images, directly addressing mode collapse and improving synthetic data quality.\n        2.  **Identity Block Integration:** An identity block is integrated into the GAN framework to stabilize training, preserve essential input features, and facilitate smoother gradient flow, thereby enhancing the generator's ability to produce diverse outputs and combat mode collapse.\n        3.  **Pancreas-Inspired Metaheuristic Loss Function:** A novel loss function, inspired by the pancreas's regulatory mechanisms for maintaining homeostasis, dynamically adapts to variations in training data. This function promotes pixel coherence and enhances diversity by acting as a feedback control system, akin to how the pancreas adjusts insulin levels.\n    *   **Novelty:** The Penca-GAN's novelty lies in its holistic integration of these three components. The pancreas-inspired metaheuristic loss function is particularly novel, differing from other biological-inspired optimization techniques by employing a dynamic feedback mechanism for real-time adaptability, focusing on pixel-level integrity and diversity simultaneously, and integrating multiple facets of image generation into a comprehensive framework.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Development of a new pancreas-inspired metaheuristic loss function that dynamically adapts to training data, balancing pixel integrity and diversity.\n        *   Introduction of a novel dual loss function based on the pancreas-inspired metaheuristic.\n    *   **System Design/Architectural Innovations:**\n        *   Integration of an identity block within the GAN architecture to stabilize training, preserve input features, and mitigate mode collapse.\n        *   A novel Penca-GAN architecture that combines these elements to outperform existing GANs in image diversity and quality.\n    *   **Theoretical Insights/Analysis:** The paper draws a unique analogy between the generator's goal of producing diverse, high-quality images and the pancreas's role in maintaining biological homeostasis, providing a novel conceptual framework for loss function design.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experiments were conducted on three distinct renewable energy datasets: SKY images, Solar images, and Wind Turbine images. The generated synthetic images were also used to enhance fault detection capabilities in solar panels and wind turbines.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **Fréchet Inception Distance (FID):** Penca-GAN consistently achieved the lowest FID scores, indicating superior image quality: 164.45 for SKY, 113.54 for Solar, and 109.34 for Wind Turbine.\n        *   **Inception Score (IS):** Penca-GAN attained the highest IS across all datasets, scoring 71.43 for SKY, 87.65 for Solar, and 90.32 for Wind Turbine, demonstrating better image diversity and quality.\n        *   **Fault Detection Accuracy:** The application of Penca-GAN significantly enhanced fault detection capabilities, improving accuracy from 85.92% to 90.04% for solar panels and from 86.06% to 90.43% for wind turbines.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** While the proposed modifications are critical, the paper notes that \"challenges in convergence persist, necessitating careful consideration of their integration.\" This implies that fine-tuning and careful integration of the dual loss and identity block are crucial for optimal performance.\n    *   **Scope of Applicability:** The primary scope is the generation of synthetic imagery for renewable energy applications (SKY, Solar, Wind Turbine images) to address data scarcity and improve downstream tasks like energy prediction and fault detection. The methodology is presented as broadly applicable to image augmentation where data quality and diversity are critical.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** Penca-GAN significantly advances the technical state-of-the-art in GAN-based data augmentation by effectively mitigating mode collapse, improving pixel integrity, and enhancing image diversity through its novel architecture and pancreas-inspired loss function. It demonstrates superior performance compared to other GAN architectures in generating high-fidelity synthetic images.\n    *   **Potential Impact on Future Research:** The robust performance of Penca-GAN in generating high-quality synthetic images has a substantial impact on renewable energy applications, particularly in improving model performance for critical tasks like fault detection and energy prediction. The novel pancreas-inspired metaheuristic loss function opens new avenues for biologically inspired optimization in generative modeling, potentially influencing future research in designing more adaptive and robust GAN architectures for various data-scarce domains. The architecture's ability to improve segmentation and detection processes suggests broader applicability.",
        "keywords": [
          "Penca-GAN",
          "Generative Adversarial Networks (GANs)",
          "Renewable energy data augmentation",
          "Pancreas-inspired metaheuristic loss",
          "Mode collapse mitigation",
          "Identity block integration",
          "Dual loss functions",
          "High-fidelity synthetic images",
          "Fault detection enhancement",
          "Fréchet Inception Distance (FID)",
          "Inception Score (IS)",
          "Training stability",
          "Data scarcity"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract describes the components and functionality of a \"generator and discriminator\" in a \"dynamic adversarial training process,\" detailing their architecture and purpose.\n*   the introduction explicitly states the \"introduction of a novel loss function inspired by the intelligent behavior of the pancreas,\" and discusses its unique features like \"dynamic adaptation,\" \"focus on integrity and diversity,\" and \"enhanced learning efficiency.\" it uses phrases like \"the proposed pancreas-inspired function\" and \"our approach mimics.\"\n*   the title \"a dual gan with identity blocks and pancreas-inspired loss for renewable energy optimization\" also indicates the development of a new system/method.\n\nthese elements strongly align with the criteria for a **technical** paper, which \"presents new methods, algorithms, or systems\" and uses language like \"propose,\" \"develop,\" \"present,\" \"algorithm,\" \"method,\" and discusses \"technical problem, proposed solution.\"\n\n**classification: technical**"
      },
      "file_name": "d2b0b0e427f9518be18156dc6025acc5c7d32d0a.pdf"
    },
    {
      "success": true,
      "doc_id": "5cabdc43cff27ce44d048bd44779ee03",
      "summary": "With the improvement of intelligence and integration, automatic supervision of large-scale systems is a current challenge in guaranteeing the high-reliability of edge devices. Hence, fast and accurate anomaly detection (AD) has become an urgent need via the edge computing of the industrial Internet of Things (IIoT). For this purpose, this article creatively proposes a dual agents based on two-phase adversarial training strategy [two-phase dual-adversarial agents (2P-DAs)] to perform rapid, stable, and unsupervised AD for large-scale IIoT-edge devices. It integrates the superiorities of deep autoencoder (AEs) and generative adversarial network (GAN), utilizing normal multivariate time-series as inputs, 1-Encoder vs. 2-Decoders architecture as backbone, and two-phase unsupervised adversarial learning to make it isolate anomalies while providing efficient training. On the one hand, this allows the inherent limitations of AEs to be overcome by training a model capable for recognizing nonanomalies and thus performing a good reconstruction. On the other hand, dual structures allow for stability in adversarial training, thereby solving the issues of collapse and nonconvergence encountered in GANs. Two practical industrial data, as cloud and edge data, are used to verify the robustness, inference speed and high detection performance of 2P-DAs in IIoT-edge AD, which demonstrates an impressive performance under multiple evaluation indexes.",
      "intriguing_abstract": "With the improvement of intelligence and integration, automatic supervision of large-scale systems is a current challenge in guaranteeing the high-reliability of edge devices. Hence, fast and accurate anomaly detection (AD) has become an urgent need via the edge computing of the industrial Internet of Things (IIoT). For this purpose, this article creatively proposes a dual agents based on two-phase adversarial training strategy [two-phase dual-adversarial agents (2P-DAs)] to perform rapid, stable, and unsupervised AD for large-scale IIoT-edge devices. It integrates the superiorities of deep autoencoder (AEs) and generative adversarial network (GAN), utilizing normal multivariate time-series as inputs, 1-Encoder vs. 2-Decoders architecture as backbone, and two-phase unsupervised adversarial learning to make it isolate anomalies while providing efficient training. On the one hand, this allows the inherent limitations of AEs to be overcome by training a model capable for recognizing nonanomalies and thus performing a good reconstruction. On the other hand, dual structures allow for stability in adversarial training, thereby solving the issues of collapse and nonconvergence encountered in GANs. Two practical industrial data, as cloud and edge data, are used to verify the robustness, inference speed and high detection performance of 2P-DAs in IIoT-edge AD, which demonstrates an impressive performance under multiple evaluation indexes.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/62433dc4233f5278c4e5cd84c324514fc528e3a0.pdf",
      "citation_key": "chang2024c0a",
      "metadata": {
        "title": "Two-Phase Dual-Adversarial Agents With Multivariate Information for Unsupervised Anomaly Detection of IIoT-Edge Devices",
        "authors": [
          "Yuanhong Chang",
          "Jinglong Chen",
          "Rong Su",
          "Jingsong Xie",
          "Aimin Li"
        ],
        "published_date": "2024",
        "abstract": "With the improvement of intelligence and integration, automatic supervision of large-scale systems is a current challenge in guaranteeing the high-reliability of edge devices. Hence, fast and accurate anomaly detection (AD) has become an urgent need via the edge computing of the industrial Internet of Things (IIoT). For this purpose, this article creatively proposes a dual agents based on two-phase adversarial training strategy [two-phase dual-adversarial agents (2P-DAs)] to perform rapid, stable, and unsupervised AD for large-scale IIoT-edge devices. It integrates the superiorities of deep autoencoder (AEs) and generative adversarial network (GAN), utilizing normal multivariate time-series as inputs, 1-Encoder vs. 2-Decoders architecture as backbone, and two-phase unsupervised adversarial learning to make it isolate anomalies while providing efficient training. On the one hand, this allows the inherent limitations of AEs to be overcome by training a model capable for recognizing nonanomalies and thus performing a good reconstruction. On the other hand, dual structures allow for stability in adversarial training, thereby solving the issues of collapse and nonconvergence encountered in GANs. Two practical industrial data, as cloud and edge data, are used to verify the robustness, inference speed and high detection performance of 2P-DAs in IIoT-edge AD, which demonstrates an impressive performance under multiple evaluation indexes.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/62433dc4233f5278c4e5cd84c324514fc528e3a0.pdf",
        "venue": "IEEE Internet of Things Journal",
        "citationCount": 3,
        "score": 3.0,
        "summary": "With the improvement of intelligence and integration, automatic supervision of large-scale systems is a current challenge in guaranteeing the high-reliability of edge devices. Hence, fast and accurate anomaly detection (AD) has become an urgent need via the edge computing of the industrial Internet of Things (IIoT). For this purpose, this article creatively proposes a dual agents based on two-phase adversarial training strategy [two-phase dual-adversarial agents (2P-DAs)] to perform rapid, stable, and unsupervised AD for large-scale IIoT-edge devices. It integrates the superiorities of deep autoencoder (AEs) and generative adversarial network (GAN), utilizing normal multivariate time-series as inputs, 1-Encoder vs. 2-Decoders architecture as backbone, and two-phase unsupervised adversarial learning to make it isolate anomalies while providing efficient training. On the one hand, this allows the inherent limitations of AEs to be overcome by training a model capable for recognizing nonanomalies and thus performing a good reconstruction. On the other hand, dual structures allow for stability in adversarial training, thereby solving the issues of collapse and nonconvergence encountered in GANs. Two practical industrial data, as cloud and edge data, are used to verify the robustness, inference speed and high detection performance of 2P-DAs in IIoT-edge AD, which demonstrates an impressive performance under multiple evaluation indexes.",
        "keywords": []
      },
      "file_name": "62433dc4233f5278c4e5cd84c324514fc528e3a0.pdf"
    },
    {
      "success": true,
      "doc_id": "d7fdb82ba5852918d3c982e06871e9aa",
      "summary": "This paper presents a novel framework for predicting semiconductor wafer yields using Wafer Acceptance Test (WAT) parameters, addressing the challenge of unbalanced datasets commonly encountered in wafer manufacturing. WAT, essential for monitoring process stability and cost control, and Chip Probing (CP) testing, which assesses the functionality of each Die on a wafer, are integral to ensuring product quality. However, the yield data, indicating the proportion of functional chips post-CP testing, often suffer from data imbalance, skewing machine learning predictions towards the majority class and reducing model accuracy. To overcome this, the study leverages Generative Adversarial Networks (GAN) to generate balanced samples by creating minority class samples for model training. This approach not only mitigates the costs associated with CP testing but also facilitates early fault detection in wafer manufacturing. Through extensive experimentation with real in-dustrial WAT and yield data, the proposed method demonstrates a significant improvement in model performance, enhancing it by approximately 7% and achieving an Area Under the Curve (AUC) of over 0.95. Additionally, the SHapley Additive exPlanations (SHAP) model is employed within the framework to rapidly analyze and identify faults impacting yield, further contributing to the efficiency of semiconductor production processes.",
      "intriguing_abstract": "This paper presents a novel framework for predicting semiconductor wafer yields using Wafer Acceptance Test (WAT) parameters, addressing the challenge of unbalanced datasets commonly encountered in wafer manufacturing. WAT, essential for monitoring process stability and cost control, and Chip Probing (CP) testing, which assesses the functionality of each Die on a wafer, are integral to ensuring product quality. However, the yield data, indicating the proportion of functional chips post-CP testing, often suffer from data imbalance, skewing machine learning predictions towards the majority class and reducing model accuracy. To overcome this, the study leverages Generative Adversarial Networks (GAN) to generate balanced samples by creating minority class samples for model training. This approach not only mitigates the costs associated with CP testing but also facilitates early fault detection in wafer manufacturing. Through extensive experimentation with real in-dustrial WAT and yield data, the proposed method demonstrates a significant improvement in model performance, enhancing it by approximately 7% and achieving an Area Under the Curve (AUC) of over 0.95. Additionally, the SHapley Additive exPlanations (SHAP) model is employed within the framework to rapidly analyze and identify faults impacting yield, further contributing to the efficiency of semiconductor production processes.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/5ff7bda5d2508218fab6c3a3b1f4ea964a59fdcd.pdf",
      "citation_key": "guo2024y0l",
      "metadata": {
        "title": "Enhanced Yield Prediction in Semiconductor Manufacturing: Innovative Strategies for Imbalanced Sample Management and Root Cause Analysis",
        "authors": [
          "Pang Guo",
          "Yining Chen"
        ],
        "published_date": "2024",
        "abstract": "This paper presents a novel framework for predicting semiconductor wafer yields using Wafer Acceptance Test (WAT) parameters, addressing the challenge of unbalanced datasets commonly encountered in wafer manufacturing. WAT, essential for monitoring process stability and cost control, and Chip Probing (CP) testing, which assesses the functionality of each Die on a wafer, are integral to ensuring product quality. However, the yield data, indicating the proportion of functional chips post-CP testing, often suffer from data imbalance, skewing machine learning predictions towards the majority class and reducing model accuracy. To overcome this, the study leverages Generative Adversarial Networks (GAN) to generate balanced samples by creating minority class samples for model training. This approach not only mitigates the costs associated with CP testing but also facilitates early fault detection in wafer manufacturing. Through extensive experimentation with real in-dustrial WAT and yield data, the proposed method demonstrates a significant improvement in model performance, enhancing it by approximately 7% and achieving an Area Under the Curve (AUC) of over 0.95. Additionally, the SHapley Additive exPlanations (SHAP) model is employed within the framework to rapidly analyze and identify faults impacting yield, further contributing to the efficiency of semiconductor production processes.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/5ff7bda5d2508218fab6c3a3b1f4ea964a59fdcd.pdf",
        "venue": "International Symposium on the Physical and Failure Analysis of Integrated Circuits",
        "citationCount": 3,
        "score": 3.0,
        "summary": "This paper presents a novel framework for predicting semiconductor wafer yields using Wafer Acceptance Test (WAT) parameters, addressing the challenge of unbalanced datasets commonly encountered in wafer manufacturing. WAT, essential for monitoring process stability and cost control, and Chip Probing (CP) testing, which assesses the functionality of each Die on a wafer, are integral to ensuring product quality. However, the yield data, indicating the proportion of functional chips post-CP testing, often suffer from data imbalance, skewing machine learning predictions towards the majority class and reducing model accuracy. To overcome this, the study leverages Generative Adversarial Networks (GAN) to generate balanced samples by creating minority class samples for model training. This approach not only mitigates the costs associated with CP testing but also facilitates early fault detection in wafer manufacturing. Through extensive experimentation with real in-dustrial WAT and yield data, the proposed method demonstrates a significant improvement in model performance, enhancing it by approximately 7% and achieving an Area Under the Curve (AUC) of over 0.95. Additionally, the SHapley Additive exPlanations (SHAP) model is employed within the framework to rapidly analyze and identify faults impacting yield, further contributing to the efficiency of semiconductor production processes.",
        "keywords": []
      },
      "file_name": "5ff7bda5d2508218fab6c3a3b1f4ea964a59fdcd.pdf"
    },
    {
      "success": true,
      "doc_id": "492b6db931241e1df259f6a1411c6c5e",
      "summary": "Generative Adversarial Networks (GANs) have achieved remarkable success in various tasks, including image generation, editing, and reconstruction, as well as in unsupervised and representation learning. Despite their impressive capabilities, GANs are often plagued by challenges such as unstable training dynamics and limitations in generating complex patterns. To address these challenges, we propose a novel image style transfer method, named C3GAN, which leverages CycleGAN architecture to achieve consistent and stable transformation of image style. In this context, “image style” refers to the distinct visual characteristics or artistic elements, such as the color schemes, textures, and brushstrokes that define the overall appearance of an image. Our method incorporates cyclic consistency, ensuring that the style transformation remains coherent and visually appealing, thus enhancing the training stability and overcoming the generative limitations of traditional GAN models. Additionally, we have developed a robust and efficient image style transfer system by integrating Flask for web development and MySQL for database management. Our system demonstrates superior performance in transferring complex styles compared to existing model-based approaches. This paper presents the development of a comprehensive image style transfer system based on our advanced C3GAN model, effectively addressing the challenges of GANs and expanding application potential in domains such as artistic creation and cinematic special effects.",
      "intriguing_abstract": "Generative Adversarial Networks (GANs) have achieved remarkable success in various tasks, including image generation, editing, and reconstruction, as well as in unsupervised and representation learning. Despite their impressive capabilities, GANs are often plagued by challenges such as unstable training dynamics and limitations in generating complex patterns. To address these challenges, we propose a novel image style transfer method, named C3GAN, which leverages CycleGAN architecture to achieve consistent and stable transformation of image style. In this context, “image style” refers to the distinct visual characteristics or artistic elements, such as the color schemes, textures, and brushstrokes that define the overall appearance of an image. Our method incorporates cyclic consistency, ensuring that the style transformation remains coherent and visually appealing, thus enhancing the training stability and overcoming the generative limitations of traditional GAN models. Additionally, we have developed a robust and efficient image style transfer system by integrating Flask for web development and MySQL for database management. Our system demonstrates superior performance in transferring complex styles compared to existing model-based approaches. This paper presents the development of a comprehensive image style transfer system based on our advanced C3GAN model, effectively addressing the challenges of GANs and expanding application potential in domains such as artistic creation and cinematic special effects.",
      "keywords": [],
      "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/50374aab2ed51f528fbaba4cd1c1482c45b843b6.pdf",
      "citation_key": "peng2024crk",
      "metadata": {
        "title": "Cyclic Consistent Image Style Transformation: From Model to System",
        "authors": [
          "Jun Peng",
          "Kaiyi Chen",
          "Yuqing Gong",
          "Tianxiang Zhang",
          "Baohua Su"
        ],
        "published_date": "2024",
        "abstract": "Generative Adversarial Networks (GANs) have achieved remarkable success in various tasks, including image generation, editing, and reconstruction, as well as in unsupervised and representation learning. Despite their impressive capabilities, GANs are often plagued by challenges such as unstable training dynamics and limitations in generating complex patterns. To address these challenges, we propose a novel image style transfer method, named C3GAN, which leverages CycleGAN architecture to achieve consistent and stable transformation of image style. In this context, “image style” refers to the distinct visual characteristics or artistic elements, such as the color schemes, textures, and brushstrokes that define the overall appearance of an image. Our method incorporates cyclic consistency, ensuring that the style transformation remains coherent and visually appealing, thus enhancing the training stability and overcoming the generative limitations of traditional GAN models. Additionally, we have developed a robust and efficient image style transfer system by integrating Flask for web development and MySQL for database management. Our system demonstrates superior performance in transferring complex styles compared to existing model-based approaches. This paper presents the development of a comprehensive image style transfer system based on our advanced C3GAN model, effectively addressing the challenges of GANs and expanding application potential in domains such as artistic creation and cinematic special effects.",
        "file_path": "paper_data/Stabilizing_Generative_Adversarial_Networks/info/50374aab2ed51f528fbaba4cd1c1482c45b843b6.pdf",
        "venue": "Applied Sciences",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Generative Adversarial Networks (GANs) have achieved remarkable success in various tasks, including image generation, editing, and reconstruction, as well as in unsupervised and representation learning. Despite their impressive capabilities, GANs are often plagued by challenges such as unstable training dynamics and limitations in generating complex patterns. To address these challenges, we propose a novel image style transfer method, named C3GAN, which leverages CycleGAN architecture to achieve consistent and stable transformation of image style. In this context, “image style” refers to the distinct visual characteristics or artistic elements, such as the color schemes, textures, and brushstrokes that define the overall appearance of an image. Our method incorporates cyclic consistency, ensuring that the style transformation remains coherent and visually appealing, thus enhancing the training stability and overcoming the generative limitations of traditional GAN models. Additionally, we have developed a robust and efficient image style transfer system by integrating Flask for web development and MySQL for database management. Our system demonstrates superior performance in transferring complex styles compared to existing model-based approaches. This paper presents the development of a comprehensive image style transfer system based on our advanced C3GAN model, effectively addressing the challenges of GANs and expanding application potential in domains such as artistic creation and cinematic special effects.",
        "keywords": []
      },
      "file_name": "50374aab2ed51f528fbaba4cd1c1482c45b843b6.pdf"
    }
  ]
}