\subsection*{Bridging 2D GANs with 3D Neural Radiance Fields}

While Generative Adversarial Networks (GANs) have achieved remarkable success in synthesizing photorealistic 2D images, as extensively discussed in Sections 4.3 and 4.4 regarding the StyleGAN family, their inherent lack of explicit 3D understanding limits their utility for applications requiring consistent multi-view generation or controllable 3D scene manipulation. This subsection explores the innovative and rapidly evolving direction of extending the capabilities of these high-fidelity 2D GANs to 3D-aware image synthesis by integrating them with Neural Radiance Fields (NeRFs). This methodological progression addresses the critical challenge of creating controllable 3D content from powerful 2D generative models, leveraging existing 2D strengths for tasks like virtual reality, content creation, and 3D reconstruction. The combination offers the benefits of GAN's high-quality texture generation and disentangled control with NeRF's inherent 3D consistency and novel view synthesis capabilities.

The core limitation of 2D GANs lies in their inability to guarantee geometric consistency across different viewpoints, as their generative process is fundamentally image-centric. Simultaneously, Neural Radiance Fields (NeRFs) \cite{Mildenhall2020} emerged as a powerful paradigm for novel view synthesis, representing 3D scenes as continuous volumetric functions. NeRFs excel at rendering photorealistic and geometrically consistent novel views, but typically require extensive multi-view image datasets for training and lack an intuitive, disentangled latent space for content manipulation akin to StyleGANs. The challenge, therefore, became how to combine the photorealism and latent space control of 2D GANs with the 3D consistency of NeRFs, ideally without requiring explicit 3D supervision.

Early efforts to bridge this gap focused on learning generative models that could produce implicit 3D scene representations from a latent code, which could then be rendered into 2D images. Generative Radiance Fields (GRAF) \cite{Schwarz2020} was among the first to propose a GAN-based approach for learning 3D-aware image synthesis. GRAF trained a GAN to generate parameters for a NeRF-like scene representation, enabling the synthesis of multi-view consistent images from a single latent vector. While a significant conceptual step, GRAF often produced lower-resolution outputs and faced challenges in achieving the same level of disentanglement and photorealism as state-of-the-art 2D GANs. Following this, pi-GAN \cite{Chan2021} further explored implicit neural representations for 3D-aware synthesis, demonstrating improved disentanglement and quality by leveraging a hierarchical latent space and a progressive training scheme. GIRAFFE \cite{Niemeyer2021} advanced this by introducing a compositional scene representation, allowing for the disentanglement of object pose, shape, and appearance, and enabling the generation of scenes with multiple objects and backgrounds, further enhancing controllable 3D-aware synthesis. These initial works laid the groundwork by demonstrating the feasibility of learning 3D-aware generative models from 2D image collections.

A pivotal development in achieving high-fidelity 3D-aware synthesis involved directly integrating the powerful StyleGAN architecture with NeRFs. StyleNeRF \cite{Gu2021} was an early attempt to adapt StyleGAN's generator to produce features for a NeRF, allowing for high-resolution 3D-consistent image generation while leveraging StyleGAN's disentangled latent space for control. It demonstrated that the rich semantic information encoded in StyleGAN's latent space could be effectively transferred to control 3D scene properties.

The most significant breakthrough in this domain, however, came with Efficient Geometry-aware 3D Generative Adversarial Networks (EG3D) \cite{Chan2022}. This landmark paper proposed a highly efficient and high-fidelity method for 3D-aware image synthesis by explicitly leveraging a StyleGAN2 backbone to generate a *tri-plane feature representation*. Instead of generating a full 3D volume, EG3D projects the latent code into three orthogonal 2D feature planes (XY, XZ, YZ). A lightweight neural renderer then queries these tri-planes at specific 3D coordinates and viewing directions to predict color and density, effectively reconstructing the 3D scene. This tri-plane representation is crucial because it factorizes the 3D problem into a more manageable set of 2D operations, significantly reducing computational cost and memory requirements compared to volumetric NeRFs, while retaining 3D consistency. The StyleGAN's W-space directly controls the features within these tri-planes, allowing for precise and disentangled manipulation of 3D geometry and appearance, such as changing facial attributes or expressions in a 3D-consistent manner. EG3D achieved unprecedented levels of photorealism and view consistency for 3D-aware face generation, setting a new state-of-the-art.

The integration of 2D GANs with NeRFs represents a powerful synergy. GANs contribute their ability to generate photorealistic textures and offer a highly disentangled latent space for intuitive control, while NeRFs provide the necessary 3D consistency and novel view synthesis capabilities. This combination has opened new avenues for applications in virtual reality, where consistent 3D environments are paramount, and for content creation pipelines that demand both high visual fidelity and intuitive 3D control. It also facilitates 3D reconstruction from limited 2D inputs by leveraging the strong generative priors encoded within the latent space.

Despite these remarkable advancements, challenges persist. While EG3D significantly improved efficiency, training and inference for these hybrid models remain computationally intensive compared to purely 2D GANs, especially for very high resolutions or complex, diverse scenes beyond specific object categories like faces. Generalization to open-world scenes or highly diverse object classes, where the underlying 3D geometry is more varied, is still an active research area. Achieving perfect geometric accuracy and photorealism across *all* viewpoints, particularly for highly occluded or unseen parts, remains difficult due to the inherent ambiguity of learning 3D from purely 2D data. Furthermore, while disentanglement has improved, the latent space mapping to desirable 3D properties is not always perfectly orthogonal, leading to some entanglement between attributes. Future research directions include improving the robustness and generalizability of these models to more complex and diverse scenes, enhancing the resolution and realism of generated 3D content, and exploring more efficient training and inference mechanisms. Integrating explicit geometric priors or sparse 3D supervision could further improve geometric accuracy, and extending these methods to dynamic 3D scenes or incorporating other generative paradigms like diffusion models for 3D-aware synthesis are promising avenues.