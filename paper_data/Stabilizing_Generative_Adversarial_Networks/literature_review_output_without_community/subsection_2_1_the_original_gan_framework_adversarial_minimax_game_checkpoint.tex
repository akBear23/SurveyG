\subsection{The Original GAN Framework: Adversarial Minimax Game}

The seminal work by \cite{goodfellow2014generative} introduced Generative Adversarial Networks (GANs), a groundbreaking framework that recast generative modeling as a dynamic, zero-sum game between two competing neural networks. This innovative paradigm immediately captured significant attention for its potential to synthesize highly realistic data, particularly images, by learning complex data distributions implicitly \cite{jabbar2020aj0, bhat202445j}. The foundational understanding of this original framework is crucial for appreciating the subsequent extensive research aimed at stabilizing its delicate adversarial balance and overcoming its inherent optimization challenges.

At its core, the original GAN architecture comprises two distinct neural networks: a generator ($G$) and a discriminator ($D$). The generator's primary function is to learn a mapping from a simple prior noise distribution $p_z(z)$ (typically a uniform or Gaussian distribution) to the intricate real data distribution $p_{data}(x)$. Through this mapping, the generator produces synthetic samples, denoted as $G(z)$, that aim to be indistinguishable from authentic data. Conversely, the discriminator acts as a binary classifier, tasked with differentiating between real samples drawn directly from $p_{data}(x)$ and fake samples generated by $G$. This adversarial interplay is conceptualized as a minimax game, where the discriminator strives to maximize its classification accuracy, while the generator simultaneously endeavors to minimize the discriminator's ability to discern between real and fake, thereby producing increasingly convincing synthetic data.

The objective function for this adversarial game is mathematically defined as:
$$ \min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))] $$
During the training process, the discriminator $D$ is optimized to maximize $V(D,G)$. This entails learning to assign high probabilities to real data samples ($D(x) \approx 1$) and low probabilities to generated samples ($D(G(z)) \approx 0$). Concurrently, the generator $G$ is updated to minimize $V(D,G)$, which is equivalent to maximizing $\mathbb{E}_{z \sim p_z(z)}[\log D(G(z))]$. This objective compels $G$ to produce samples $G(z)$ for which $D(G(z))$ approaches $1$, effectively making its outputs appear real to the discriminator. Theoretically, this adversarial game converges to a unique Nash equilibrium. At this equilibrium, the generator perfectly replicates the real data distribution ($p_g = p_{data}$), and the discriminator outputs $0.5$ for all inputs, signifying its inability to distinguish between real and fake samples.

A critical theoretical implication of this original formulation is that, at the global optimum, the objective function corresponds to minimizing the Jensen-Shannon Divergence (JSD) between the real data distribution ($p_{data}$) and the generated data distribution ($p_g$). While JSD is a symmetric and bounded measure of similarity between probability distributions, its properties proved to be a significant source of practical instability in GAN training \cite{jabbar2020aj0}. Specifically, when the supports of $p_{data}$ and $p_g$ are disjoint or have negligible overlap—a common occurrence, especially in high-dimensional data spaces like images—the JSD becomes a constant value. In such scenarios, the gradients of the discriminator with respect to the generator's parameters can vanish, providing little to no meaningful learning signal to the generator. This vanishing gradient problem makes it exceedingly difficult for the generator to learn effectively, particularly during the early stages of training when $p_g$ is far from $p_{data}$.

Beyond vanishing gradients, the delicate balance inherent in the adversarial minimax game often led to training instability and convergence issues \cite{bhat202445j}. The competitive nature meant that if one network became too powerful too quickly, the training process could derail. For instance, an overly strong discriminator could consistently output $0$ or $1$ for generated samples, leading to saturated gradients for the generator. Conversely, a weak discriminator might provide an insufficient learning signal, allowing the generator to produce poor-quality samples without significant penalty. This imbalance could manifest as oscillations in performance or, more critically, as mode collapse, where the generator produces only a limited variety of samples, failing to capture the full diversity of the real data distribution \cite{jabbar2020aj0}. The theoretical analysis of GAN dynamics, such as that by \cite{gonzlezprieto20214wh}, further elucidates why the training process is inherently unstable; they show that convergent orbits in GANs are often small perturbations of periodic orbits, implying that Nash equilibria can act as spiral attractors, which theoretically justifies the observed slow and unstable training.

In summary, the original GAN framework by Goodfellow et al. was a revolutionary contribution, offering a powerful new paradigm for generative modeling. However, its reliance on the Jensen-Shannon Divergence as the underlying objective function, coupled with the inherent competitive dynamics of the minimax game, immediately exposed fundamental challenges. These included the pervasive problem of vanishing gradients when data distributions had non-overlapping supports, leading to training instability and the notorious issue of mode collapse. These initial theoretical and practical difficulties underscored the need for significant advancements in objective functions, architectural designs, and training methodologies, setting the stage for the extensive research that followed to stabilize and enhance generative adversarial models.