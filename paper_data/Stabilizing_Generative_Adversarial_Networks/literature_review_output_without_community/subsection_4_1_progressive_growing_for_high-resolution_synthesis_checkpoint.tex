\subsection*{Progressive Growing for High-Resolution Synthesis}

The generation of high-resolution, photorealistic images has long been a significant challenge for Generative Adversarial Networks (GANs), often hampered by training instability and the computational demands of large models. Early attempts to stabilize GAN training primarily focused on modifying objective functions or regularization techniques to mitigate issues such as mode collapse and vanishing gradients. For instance, \textcite{metz20169ir} introduced Unrolled Generative Adversarial Networks, a method that stabilized training by defining the generator's objective with respect to an unrolled optimization of the discriminator, thereby addressing mode collapse and enhancing the diversity and coverage of the data distribution. Similarly, \textcite{che2016kho} proposed Mode Regularized Generative Adversarial Networks, arguing that the functional shape of discriminators in high-dimensional spaces contributed to instability and mode collapse. Their solution involved introducing regularizers to stabilize training and ensure a more equitable distribution of probability mass across data modes, particularly in the early phases of training. While these methods significantly improved the foundational stability and diversity of GANs, scaling them to generate images at resolutions beyond 256x256 pixels remained a formidable hurdle.

A pivotal methodological breakthrough that fundamentally transformed the landscape of high-resolution image synthesis was the introduction of Progressive Growing of GANs (PGGANs) by \textcite{Karras2018}. This innovative approach directly tackled the challenges of training stability and high-resolution output by gradually increasing the resolution of both the generated images and the discriminator's inputs throughout the training process. Instead of attempting to synthesize high-resolution images from scratch, PGGANs begin training at a very low resolution, typically 4x4 pixels. As training progresses and the network learns to generate stable images at the current resolution, new layers are incrementally added to both the generator and discriminator. These new layers are smoothly "faded in" using a weighted sum with the existing layers, ensuring a continuous and stable transition to higher resolutions.

This progressive growing strategy offers several critical advantages. Firstly, it significantly improves training stability by presenting an easier learning task to the networks at each stage. Learning low-frequency features at coarse resolutions is simpler, and this knowledge is then leveraged and refined as higher-frequency details are introduced with increasing resolution. This hierarchical learning process effectively prevents the common pitfalls of GAN training, such as mode collapse and gradient instability, which are exacerbated when attempting to learn complex, high-dimensional distributions directly. Secondly, PGGANs enabled the synthesis of unprecedentedly photorealistic images, pushing the boundaries to resolutions as high as 1024x1024 pixels. This marked a major leap in image quality and scale, allowing for the creation of visually compelling and diverse outputs that were previously unattainable. Finally, by starting with smaller networks and gradually expanding them, PGGANs also contributed to a reduction in the overall training time required to achieve high-resolution outputs, as the initial stages are computationally less intensive. The success of PGGANs laid a robust foundation for subsequent advancements in high-fidelity image generation, including the StyleGAN series, by demonstrating a scalable and stable training paradigm for complex generative tasks.

In conclusion, the progressive growing methodology introduced by PGGANs represented a paradigm shift in generative modeling, moving beyond earlier regularization and objective function modifications to address stability and resolution through a structured training curriculum. While earlier works like \textcite{metz20169ir} and \textcite{che2016kho} laid crucial groundwork for general GAN stability, PGGANs provided the architectural and training strategy necessary to unlock truly high-resolution, photorealistic synthesis. Despite its profound impact, the computational cost of training PGGANs, especially for extremely high resolutions or diverse datasets, still presented avenues for further optimization, paving the way for future research into more efficient and controllable high-fidelity generative models.