\subsection*{Wasserstein GANs (WGAN) and the Earth Mover's Distance}

The early development of Generative Adversarial Networks (GANs) \cite{Goodfellow2014} was plagued by significant training instabilities, primarily stemming from the objective function's reliance on the Jensen-Shannon (JS) divergence. This divergence often became saturated when the real and generated data distributions were non-overlapping, a common occurrence in high-dimensional data spaces, leading to vanishing gradients for the generator and consequently, a lack of meaningful learning signals. This fundamental issue contributed to notorious problems such as mode collapse, where the generator would produce only a limited subset of the true data distribution's diversity, and overall training fragility. Efforts to mitigate these issues, such as the regularization techniques proposed in \cite{che2016kho} to address mode collapse, highlighted the pressing need for a more robust and stable GAN objective.

A groundbreaking paradigm shift was introduced by Wasserstein GANs (WGAN) \cite{Arjovsky2017}, which fundamentally altered the GAN objective by replacing the problematic JS divergence with the Earth Mover's (Wasserstein-1) distance. This change was pivotal because, unlike f-divergences, the Wasserstein distance provides a smooth and continuous loss landscape, even when the real and generated distributions are disjoint. Consequently, the critic (the WGAN equivalent of the discriminator) could provide meaningful, non-zero gradients to the generator almost everywhere, effectively resolving the critical issues of vanishing gradients and mode collapse that had severely hampered earlier GAN formulations. The continuous nature of the Wasserstein distance ensures that the generator receives a consistent and informative learning signal, allowing it to gradually improve its output and cover the full diversity of the target data distribution.

The theoretical foundation of WGAN relies on the Kantorovich-Rubinstein duality, which allows the Wasserstein-1 distance to be approximated by a neural network, provided that this network satisfies a K-Lipschitz continuity constraint. To enforce this crucial constraint, the initial WGAN formulation \cite{Arjovsky2017} employed a simple yet effective technique: weight clipping. After each gradient update, the weights of the critic network were clipped to a small, fixed range (e.g., $[-c, c]$). This method aimed to keep the critic's gradients bounded, thereby ensuring its Lipschitz continuity and allowing it to reliably estimate the Earth Mover's distance. This represented a significant theoretical and practical advancement, as it provided a more stable training objective and a loss value that directly correlated with the quality of the generated samples, offering a much-needed metric for convergence and model evaluation.

The introduction of WGAN marked a critical turning point in GAN research, providing a robust framework that significantly improved training stability and reduced mode collapse. Its core innovation, the adoption of the Earth Mover's distance, provided a theoretically sound and empirically effective solution to the long-standing problem of unstable GAN training. While the initial weight clipping method for enforcing the Lipschitz constraint, as proposed in \cite{Arjovsky2017}, proved effective in demonstrating the power of the Wasserstein objective, it also presented practical challenges, such as sensitivity to the clipping range and potential for pathological behavior in the critic's capacity. These limitations subsequently spurred further research into more robust and adaptive methods for Lipschitz enforcement, paving the way for advanced regularization techniques like gradient penalties and spectral normalization. Nevertheless, WGAN's foundational contribution has been widely adopted and continues to influence the development of stable generative models, as evidenced by its integration into various applications, such as enhancing channel estimation in wireless communication systems \cite{hu2021yk5, ye2024n41}.