\subsection*{Alternative Objective Functions: LSGAN, EBGAN, BEGAN}

The early development of Generative Adversarial Networks (GANs) was significantly challenged by training instability and issues like mode collapse, prompting researchers to explore innovative objective functions and architectural designs beyond the original minimax game and the subsequent Wasserstein distance framework. This subsection delves into several pivotal approaches that introduced alternative loss functions and discriminator architectures to foster more stable and higher-quality generation.

One notable departure from the standard GAN objective was the introduction of Least Squares Generative Adversarial Networks (LSGANs) by \cite{Mao2017}. Unlike traditional GANs that employ a sigmoid cross-entropy loss, LSGANs replace this with a least squares loss function. This modification fundamentally alters the gradient landscape, providing smoother gradients and mitigating the vanishing gradient problem that often plagued early GANs. Empirically, LSGANs demonstrated improved training stability and generated higher quality images by penalizing samples based on their distance from the decision boundaries, encouraging the generator to produce samples closer to the real data manifold and the discriminator to push fake samples further away. The simplicity and effectiveness of this loss function modification made LSGAN a practical alternative for stable GAN training.

Building on the exploration of novel objective functions, Energy-Based Generative Adversarial Networks (EBGANs) proposed by \cite{Zhao2017} offered a distinct conceptual framework. In EBGANs, the discriminator is re-imagined as an energy function that assigns low energy to real data samples and high energy to generated (fake) samples. The objective is to learn a manifold where real data resides in a low-energy region, while pushing fake samples into higher-energy regions. This is achieved through a margin loss, which explicitly pulls down the energy of real samples and pushes up the energy of fake ones, thereby providing a different perspective on stability and mode collapse reduction. A key architectural innovation in EBGANs is the use of an autoencoder as the discriminator, where the reconstruction error serves as the energy function. This design choice implicitly encourages the discriminator to learn meaningful features of the data distribution.

Further refining the concept of an autoencoder-based discriminator, Boundary Equilibrium Generative Adversarial Networks (BEGANs) were introduced by \cite{Berthelot2017}. BEGANs leverage a similar autoencoder architecture for their discriminator, focusing on matching the distribution of reconstruction errors for real and generated samples rather than the data distributions directly. The core innovation of BEGAN lies in its equilibrium-enforcing loss, which introduces a hyperparameter, $\gamma$, to balance the generator's ability to produce diverse samples and the discriminator's ability to distinguish them. This equilibrium concept dynamically adjusts the training balance between the generator and discriminator, aiming to achieve a state where the average reconstruction error for real data is equal to that for generated data. This mechanism proved highly effective in generating visually appealing and high-quality images, particularly faces, while maintaining training stability.

Collectively, these methods represent a crucial phase in GAN research, moving beyond the initial adversarial loss and Wasserstein distance to explore a diverse array of objective functions and discriminator architectures. LSGAN \cite{Mao2017} offered a straightforward yet impactful modification to the loss function for enhanced stability. EBGAN \cite{Zhao2017} introduced a conceptual shift with its energy-based discriminator and the innovative use of an autoencoder, laying groundwork for subsequent architectural advancements. BEGAN \cite{Berthelot2017} then built upon the autoencoder discriminator, integrating a novel equilibrium mechanism to explicitly target both stability and high-quality image generation. While these approaches significantly advanced the field by providing unique perspectives on achieving stable and high-quality generative models, some, like BEGAN, could exhibit sensitivity to hyperparameter tuning, highlighting the ongoing challenge of universal stability and robustness across diverse applications. Nevertheless, their contributions underscored the breadth of early research into GAN objective design and paved the way for more sophisticated models.