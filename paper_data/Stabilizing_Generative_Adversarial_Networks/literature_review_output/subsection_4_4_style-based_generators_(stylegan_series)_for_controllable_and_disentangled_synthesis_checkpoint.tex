\subsection{Style-Based Generators (StyleGAN Series) for Controllable and Disentangled Synthesis}

The StyleGAN series represents a pivotal advancement in generative adversarial networks (GANs), fundamentally transforming high-fidelity image synthesis by introducing a novel style-based generator architecture that enables unprecedented disentangled control over various image features and achieves state-of-the-art visual quality. This architectural paradigm shift moved beyond merely generating realistic images to providing fine-grained, intuitive control over the synthesis process, addressing a key limitation of earlier GAN models.

The foundational work, StyleGAN \cite{Karras2019}, introduced a generator that departs significantly from traditional architectures by mapping a latent code through a fully connected network into an intermediate latent space, $\mathcal{W}$. This $\mathcal{W}$ space is then used to control the synthesis process at multiple scales through Adaptive Instance Normalization (AdaIN) layers, injecting "styles" into the generator's feature maps. This design, coupled with progressive growing and a constant learned input, allowed StyleGAN to produce highly diverse and photorealistic images, while enabling intuitive manipulation of high-level attributes (e.g., pose, identity) and low-level details (e.g., hair color, freckles) by modifying specific "style" vectors. The disentanglement achieved by this style-based approach was a significant leap forward, offering a more interpretable latent space compared to previous GANs.

Despite its groundbreaking success, the initial StyleGAN architecture exhibited certain visual artifacts, such as "blob" artifacts and phase artifacts, which became more apparent in higher resolutions or during latent space interpolations. StyleGAN2 \cite{Karras2020} meticulously addressed these shortcomings through a series of architectural refinements and improved regularization techniques. Key innovations included redesigning the normalization layers, removing the progressive growing architecture in favor of a single-resolution training approach, and introducing a new regularization method called path length regularization. This regularization technique explicitly encouraged a more disentangled and perceptually smooth latent space by ensuring that a constant-speed movement in the latent space $\mathcal{W}$ results in a constant-speed change in the image space. These modifications significantly reduced artifacts, improved overall image quality, and enhanced the linearity of the latent space, making latent space interpolations more coherent and visually pleasing.

The pursuit of even greater realism and artifact elimination continued with StyleGAN3 \cite{Karras2021}, which tackled a more fundamental issue: aliasing artifacts. While StyleGAN2 produced highly realistic static images, these artifacts became noticeable when generating videos or performing fine-grained geometric transformations, manifesting as "texture sticking" or unnatural movements. StyleGAN3 approached this problem from a signal processing perspective, redesigning the generator to be alias-free. This involved replacing traditional upsampling and downsampling operations with anti-aliasing filters and operating on a continuous-domain representation. By ensuring that all layers respect the Nyquist-Shannon sampling theorem, StyleGAN3 achieved truly alias-free synthesis, resulting in images that appear more natural and coherent, especially in dynamic contexts, pushing the boundaries of perceptual realism to an unprecedented degree.

In conclusion, the StyleGAN series represents a pinnacle in generator design, prioritizing fine-grained control, disentanglement of latent factors, and the systematic elimination of subtle visual artifacts. From StyleGAN's initial introduction of a style-based architecture and AdaIN \cite{Karras2019}, through StyleGAN2's meticulous refinements and path length regularization for artifact reduction \cite{Karras2020}, to StyleGAN3's alias-free architecture addressing fundamental signal processing issues \cite{Karras2021}, each iteration built upon its predecessor to achieve increasingly realistic and controllable image synthesis. While these models have set new benchmarks for image quality and disentanglement, their significant computational cost for training remains a practical challenge, and the inherent complexity of their architectures can make them challenging to adapt for tasks beyond high-fidelity image generation, as noted in comparative analyses of generative models \cite{peng2024kkw}.