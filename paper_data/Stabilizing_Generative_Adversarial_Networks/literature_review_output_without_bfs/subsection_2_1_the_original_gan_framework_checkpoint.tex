\subsection*{The Original GAN Framework}

Generative modeling, the task of learning to produce new samples from the same distribution as a given training dataset, has long been a fundamental challenge in machine learning. Before the advent of Generative Adversarial Networks (GANs), approaches often relied on explicit density estimation or computationally intensive methods. The seminal work by Goodfellow et al. \cite{goodfellow2014generative} introduced a radically different paradigm, framing generative modeling as an adversarial game between two neural networks, which quickly became a cornerstone for unsupervised learning.

The core of the original GAN framework consists of two competing neural networks: a generator (G) and a discriminator (D). The generator, G, is a differentiable function that learns to map samples from a simple prior distribution, typically a uniform or Gaussian latent vector $z$, to the data space, producing synthetic data samples $G(z)$. Its objective is to generate samples that are indistinguishable from real data. Conversely, the discriminator, D, is a binary classifier that takes a data sample as input and outputs a single scalar representing the probability that the sample came from the real training data rather than from the generator. D's goal is to accurately distinguish between real data samples and the fake samples produced by G.

The training process of GANs is conceptualized as a minimax game, where G and D are simultaneously optimized in an adversarial fashion. The discriminator D is trained to maximize the probability of correctly classifying real data samples as real and generated samples as fake. In parallel, the generator G is trained to minimize D's ability to distinguish between real and fake samples, effectively trying to "fool" the discriminator into classifying its generated outputs as real. This dynamic creates a continuous feedback loop: as G improves its ability to generate realistic samples, D must become more sophisticated to detect fakes, and this improved D, in turn, provides a stronger learning signal for G to further refine its generation capabilities.

The original objective function, or value function $V(D, G)$, for this minimax game is defined as:
$$ \min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))] $$
Here, $p_{data}(x)$ represents the true data distribution, and $p_z(z)$ is the prior distribution over the input noise variables for the generator. The discriminator D aims to maximize this value function, while the generator G aims to minimize it. Goodfellow et al. \cite{goodfellow2014generative} theoretically demonstrated that at the optimal point of this adversarial game, the generator's distribution $p_g$ converges to the true data distribution $p_{data}$. At this equilibrium, the optimal discriminator $D^*(x)$ would output $0.5$ for all inputs, indicating that it can no longer differentiate between real and generated samples. The theoretical underpinnings further reveal that this objective function is equivalent to minimizing the Jensen-Shannon divergence between the data distribution and the generator's distribution, up to a constant term.

The initial promise of the original GAN framework was immense. It offered a novel and powerful approach to unsupervised learning, capable of implicitly learning complex, high-dimensional data distributions without requiring explicit density estimation. This opened up new possibilities for generating novel, high-fidelity samples, which was a significant advancement for tasks like image synthesis. However, despite its theoretical elegance and groundbreaking potential, the original GAN framework proved notoriously difficult to train in practice. Issues such as mode collapse, where the generator produces a limited variety of samples, and vanishing gradients, where the discriminator becomes too powerful too quickly and provides no useful learning signal to the generator, were prevalent. These inherent instabilities and training challenges made the original GAN a critical baseline, immediately highlighting the need for subsequent research into stabilization techniques, alternative objective functions, and architectural improvements that would define the next generation of generative models.

\bibliographystyle{plain}
\bibliography{references}