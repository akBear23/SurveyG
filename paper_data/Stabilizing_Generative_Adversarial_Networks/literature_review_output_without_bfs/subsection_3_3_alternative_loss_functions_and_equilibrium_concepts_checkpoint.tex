\subsection*{Alternative Loss Functions and Equilibrium Concepts}

The persistent instability of Generative Adversarial Networks (GANs), often manifested as mode collapse, vanishing gradients, and training oscillations, spurred significant research into objective functions and training dynamics beyond the original minimax game and the subsequent Wasserstein framework. These alternative approaches primarily aim to provide smoother loss landscapes, enforce specific architectural properties, or introduce explicit mechanisms to balance the adversarial training process, thereby improving stability and sample quality.

One of the earliest and most straightforward modifications to the GAN objective was introduced by Least Squares Generative Adversarial Networks (LSGANs) \cite{Mao2017}. LSGANs replace the traditional sigmoid cross-entropy loss with a least squares loss for both the discriminator and the generator. This change yields two primary benefits: first, it provides smoother gradients, particularly for samples far from the decision boundary, mitigating the vanishing gradient problem that plagues sigmoid-based losses when the discriminator becomes too confident. Second, \cite{Mao2017} theoretically demonstrated that minimizing the LSGAN objective corresponds to minimizing the Pearson $\chi^2$ divergence, a measure of statistical distance that offers more stable gradient signals compared to the Jensen-Shannon divergence implicitly minimized by original GANs. This direct mathematical connection to a well-behaved divergence contributed significantly to LSGANs' improved training stability and ability to generate higher quality images.

Moving beyond direct loss function replacements, architectural designs for the discriminator coupled with novel training objectives emerged as a powerful avenue. Energy-based Generative Adversarial Networks (EBGANs) \cite{Zhao2016} conceptualize the discriminator as an energy function. In this framework, the discriminator, often implemented as an autoencoder, learns to assign low energy to real data samples and high energy to generated (fake) data. The generator is then trained to produce samples that minimize this energy, effectively guiding it towards the real data distribution. This approach stabilizes training by focusing on the discriminator's ability to reconstruct real data accurately (low reconstruction error, thus low energy) while failing to reconstruct fake data (high reconstruction error, thus high energy). This mechanism helps mitigate mode collapse by encouraging the generator to produce diverse samples that the autoencoder discriminator cannot easily reconstruct as "real."

Building upon the autoencoder discriminator concept, Boundary Equilibrium Generative Adversarial Networks (BEGAN) \cite{Berthelot2017} further refined this idea by focusing on matching the autoencoder loss distributions for real and generated images. Crucially, BEGAN introduced an explicit *equilibrium concept* through a diversity control parameter ($\gamma$). This parameter dynamically balances the generator and discriminator's training progress by adjusting the weight of the reconstruction loss for generated samples. By controlling this balance, BEGAN allows for a principled trade-off between image diversity and quality. This dynamic adjustment mechanism, based on the ratio of the discriminator's autoencoder loss for real versus fake samples, significantly improved visual quality and training stability for its time, demonstrating the power of an explicit equilibrium strategy to prevent either network from overpowering the other.

The broader theoretical landscape of GAN objective functions can be understood through the lens of *f-divergences*. Many GAN variants implicitly or explicitly minimize different f-divergences, which are a class of statistical distances that generalize measures like Kullback-Leibler (KL) divergence, Jensen-Shannon (JS) divergence, and Pearson $\chi^2$ divergence. For instance, as noted, LSGANs correspond to Pearson $\chi^2$ divergence \cite{Mao2017}. Further exploring this framework, Alpha-GAN \cite{Cai2020AlphaGAN} adopts the alpha divergence as its minimization objective. Alpha divergence offers a flexible generalization of several common divergences, allowing for a tunable parameter ($\alpha$) to balance training stability and generated image quality. Similarly, LeCam-GAN \cite{Tseng2021LeCamGAN} proposes a regularization approach that theoretically connects the regularized loss to the LeCam-divergence, which is found to be more robust under limited training data. This highlights how selecting appropriate f-divergences can fundamentally alter the GAN's learning dynamics and stability properties.

Another prominent alternative loss function that gained widespread adoption is the Hinge Loss. Initially explored in variants like Margin Adaptation for Generative Adversarial Networks (MAGAN) \cite{Wang2017MAGAN}, Hinge Loss provides a clear margin for the discriminator's classification, penalizing it only when its prediction for real samples falls below a positive margin or its prediction for fake samples rises above a negative margin. This loss function is particularly effective because it provides strong, non-saturating gradients when the discriminator is performing poorly but yields zero gradients once the discriminator achieves a sufficient margin. This piecewise linear nature helps stabilize training by preventing the discriminator from becoming overly confident and thus providing useful gradients to the generator for a longer period, making it a robust choice for many high-performance GAN architectures.

Further innovations in objective functions and training dynamics include Relativistic GANs, where the discriminator estimates the probability that *real data is more realistic than fake data*, or vice-versa, rather than just classifying samples as real or fake independently. This relativistic approach, exemplified by methods like DRL-GAN \cite{Roy2024DRLGAN}, can provide more informative gradient signals to the generator, contributing to improved stability and sample quality by directly comparing real and fake distributions. Other approaches focus on explicit regularization of the objective to prevent specific failure modes. For instance, Mode Regularized Generative Adversarial Networks \cite{Che2016ModeRegGAN} introduce regularizers to the objective function to encourage the fair distribution of probability mass across data modes, directly addressing mode collapse. Similarly, Robust Generative Adversarial Networks (RGAN) \cite{Zhang2019RobustGAN} propose a worst-case adversarial training setting, where both generator and discriminator compete within a small Wasserstein ball, aiming to improve generalization and stability by making the networks robust to perturbations. The theoretical underpinnings of such stability improvements, often linking to properties like Lipschitz continuity and the choice of divergence, have been further explored by works like \cite{Chu2020Smoothness}, which provides a principled framework for understanding GAN stability.

In summary, the quest for stable GAN training has led to a rich diversity of alternative loss functions and equilibrium concepts. From the simpler, yet effective, least squares loss \cite{Mao2017} and the energy-based autoencoder discriminator of EBGAN \cite{Zhao2016}, to the explicit equilibrium parameter of BEGAN \cite{Berthelot2017}, these methods offered distinct strategies. The f-divergence framework provides a unifying theoretical lens, explaining how different choices (e.g., Pearson $\chi^2$, Alpha-divergence, LeCam-divergence) impact training dynamics. The widespread adoption of Hinge Loss further underscores the value of non-saturating, margin-based objectives. While these approaches have significantly improved GAN stability and sample quality, they often introduce new hyperparameters (e.g., $\gamma$ in BEGAN, $\alpha$ in Alpha-GAN) that require careful tuning, and their theoretical guarantees can be complex. Nevertheless, these foundational innovations laid crucial groundwork, demonstrating that stability could be achieved not only through gradient regularization (as in WGAN-GP) but also through fundamentally rethinking the adversarial objective and the balance between the generator and discriminator. This paved the way for more sophisticated architectural and training strategy innovations.