\subsection*{Wasserstein GANs: A New Distance Metric}

The initial formulation of Generative Adversarial Networks (GANs) by \textcite{goodfellow2014generative} encountered significant training challenges, primarily stemming from the properties of the Jensen-Shannon (JS) divergence employed in its objective function. When the real and generated data distributions had disjoint or non-overlapping supports—a common occurrence in high-dimensional data spaces, especially during early training stages—the JS divergence became constant. This led to vanishing gradients for the generator, effectively halting its learning process and preventing it from producing diverse samples, a phenomenon known as mode collapse \cite{arjovsky2017ze5, chu2020zbv}. Such instability underscored the urgent need for a more robust and continuous metric to measure the distance between probability distributions.

A pivotal innovation addressing these fundamental limitations was the introduction of Wasserstein GANs (WGANs) by \textcite{arjovsky2017ze5}. This seminal work proposed replacing the JS divergence with the Earth-Mover (EM) distance, also known as the Wasserstein-1 distance, as the metric for quantifying the discrepancy between the real and generated data distributions. The EM distance offers a crucial advantage: it provides a continuous and differentiable (almost everywhere) measure of distance between probability distributions, even when their supports are disjoint \cite{arjovsky2017ze5}. This continuity is paramount because it ensures that the loss function provides meaningful, non-zero gradients to the generator even when its output distribution is far from the real data, thereby directly mitigating the vanishing gradient problem that plagued original GANs. Consequently, WGANs offered a smoother loss landscape, allowing for more stable training and providing more informative gradients that guide the generator towards the true data distribution more effectively, thus contributing to alleviating mode collapse by encouraging broader exploration of the data manifold.

To theoretically guarantee that the discriminator—re-termed "critic" in WGANs—approximates the EM distance, its function must satisfy the 1-Lipschitz continuity condition \cite{arjovsky2017ze5}. This condition implies that the magnitude of the critic's gradients must be bounded by one, ensuring that the critic's output does not change too rapidly with respect to its input. The initial WGAN model enforced this Lipschitz constraint through a simple yet theoretically significant method: weight clipping \cite{arjovsky2017ze5}. This involved clamping the weights of the critic network to a small fixed range (e.g., $[-c, c]$) after each gradient update. From a theoretical standpoint, weight clipping attempts to bound the function space of the discriminator, which has been shown to be crucial for stabilizing training and achieving convergence \cite{chao2021ynq}. By directly limiting the magnitude of the weights, it aims to indirectly control the Lipschitz constant of the critic.

However, despite its theoretical appeal, the practical implementation of weight clipping introduced its own set of significant challenges and limitations. Firstly, the choice of the clipping range $c$ was highly sensitive and often required extensive hyperparameter tuning; an improperly chosen $c$ could severely impact performance. If $c$ was too small, it could drastically restrict the critic's capacity, forcing it to learn an overly simplistic function and preventing it from adequately distinguishing between real and fake samples. This under-capacity could lead to poor gradient signals for the generator. Conversely, if $c$ was too large, the Lipschitz constraint might not be effectively enforced, leading back to training instability \cite{arjovsky2017ze5}. Secondly, weight clipping often resulted in pathological gradient behavior, where gradients tended to concentrate at the boundaries of the clipping range. This phenomenon meant that the critic's weights would frequently be pushed to their maximum or minimum allowed values, leading to a "binary" or "sparse" critic that struggled to provide rich, informative gradients across the entire input space \cite{saqur2018oqp}. Such behavior could hinder the generator's ability to learn complex data distributions effectively, as the critic's feedback became less nuanced. The need for a more robust and principled method to enforce the Lipschitz constraint, without compromising the critic's expressive power or inducing pathological gradient dynamics, became evident.

In conclusion, the advent of Wasserstein GANs marked a profound theoretical shift in the understanding and training of generative adversarial networks. By leveraging the Earth-Mover distance, \textcite{arjovsky2017ze5} provided a theoretically sound framework for stable learning, offering a continuous loss function that yielded informative gradients even in challenging scenarios of disjoint distribution supports. The introduction of the 1-Lipschitz constraint and the initial attempt to enforce it via weight clipping were crucial steps towards stabilizing GAN training. However, the practical deficiencies of weight clipping, including its capacity limitations and pathological gradient behavior, highlighted the urgent need for a more principled and effective method to enforce the Lipschitz constraint without crippling the critic's expressive power. This challenge set the stage for subsequent advancements aimed at refining the stability mechanisms of Wasserstein GANs.