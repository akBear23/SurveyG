\subsection*{Quantitative Metrics for Evaluating GAN Stability and Quality}

The inherent challenges of training Generative Adversarial Networks (GANs), including mode collapse, vanishing gradients, and training instability, necessitate robust quantitative metrics for objective evaluation of their stability, quality, and diversity \cite{jabbar2020aj0, wang2019w53}. These metrics provide a crucial framework for comparing different GAN architectures and assessing the effectiveness of various stabilization techniques.

Early efforts to quantify GAN performance moved beyond subjective visual inspection to more objective measures. The \textbf{Inception Score (IS)} emerged as one of the first widely adopted metrics \cite{salimans2016improved}. It assesses both the quality and diversity of generated images by leveraging a pre-trained Inception-v3 network. High IS values indicate that generated images are both semantically clear (low entropy of predicted class labels) and diverse (high entropy of marginal class probabilities). For instance, \cite{karras2017raw} utilized the Inception Score to demonstrate "unprecedented quality" and "increased variation" in their progressively grown GANs. While groundbreaking, IS has limitations, including its reliance on a classifier pre-trained on ImageNet, which may not generalize well to other datasets, and its sensitivity to image resolution.

To address some of the shortcomings of IS, the \textbf{Fréchet Inception Distance (FID)} was introduced as a more robust and widely accepted metric \cite{heusel2017gans}. FID measures the distance between the feature distributions of real and generated images, extracted from an intermediate layer of a pre-trained Inception-v3 network. By modeling these feature distributions as multivariate Gaussians, FID computes the Fréchet distance between them, providing a single score that correlates well with human perception of image quality and diversity. A lower FID score indicates better quality and diversity. FID has become the de facto standard for evaluating high-fidelity GANs, with architectural innovations like BigGAN \cite{brock2019} and the StyleGAN series \cite{karras2019, karras2020, karras2021} consistently reporting FID scores to demonstrate their state-of-the-art performance. Many stabilization techniques, such as those proposed by \cite{zadorozhnyy20208ft} with adaptive weighted discriminators and \cite{zhang2021ypi} with twin discriminators, explicitly aim to improve IS and FID, showcasing these metrics as key indicators of success.

Beyond overall quality and diversity, metrics for \textbf{mode coverage} are essential to diagnose and mitigate mode collapse, a common GAN failure where the generator produces only a limited subset of the true data distribution. While FID implicitly captures aspects of mode coverage, more explicit measures like precision and recall (though not explicitly detailed in the provided papers, they are widely used in the field) quantify how well the generator captures the full data distribution. The development of GANs like PresGAN \cite{dieng2019rjn} directly addresses mode collapse through an entropy regularizer, with evaluation focusing on mitigating this issue alongside perceptual quality. The ability of a GAN to fit multimodal distributions is also theoretically linked to its Lipschitz constant, highlighting a trade-off between stability and mode coverage \cite{salmona202283g}.

Finally, \textbf{convergence diagnostics} and measures of training stability are critical for assessing the effectiveness of stabilization techniques. While IS and FID evaluate the final output, monitoring training dynamics provides insights into the learning process itself. Techniques like Wasserstein GAN (WGAN) \cite{arjovsky2017ze5} and Least Squares GAN (LSGAN) \cite{mao2017ss0} were developed to provide more stable gradients and smoother loss landscapes, which are observed through less volatile training curves. Improved WGANs with gradient penalty (WGAN-GP) \cite{gulrajani2017improved} further enhanced stability, leading to better final image quality as measured by IS/FID. The work by \cite{xu2019uwg} models GAN training dynamics using control theory to directly improve stability, while \cite{gan202494y} and \cite{megahed2024c23} propose methods with learnable auxiliary modules and collaborative training, respectively, explicitly targeting training stability and evaluating their success through improved FID scores and reduced loss oscillations. The stability of the training process, often measured by the consistency of loss functions and the absence of exploding/vanishing gradients, is a direct diagnostic for the effectiveness of regularization techniques like spectral normalization \cite{miyato2018spectral} or various gradient penalties.

In conclusion, quantitative metrics such as Inception Score and Fréchet Inception Distance, alongside diagnostics for mode coverage and training stability, are indispensable for the rigorous evaluation and advancement of GAN research. They provide an objective basis for comparing the efficacy of different architectures and stabilization methods, guiding the field towards generators capable of producing increasingly high-quality, diverse, and stable outputs. However, ongoing challenges include the reliance on proxy metrics derived from pre-trained classifiers and the need for metrics that better align with nuanced aspects of human perception and specific application requirements.