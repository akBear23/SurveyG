\subsection{Scaling Up: Large-Scale GAN Training}

The ambition to generate highly realistic and diverse images across complex, large-scale datasets, such as ImageNet, represented a formidable challenge for Generative Adversarial Networks (GANs). Achieving this required not only overcoming inherent training instabilities but also developing architectural paradigms capable of managing immense model complexity and computational demands \cite{wang2019w53, purwono2025spz}. The foundational breakthroughs in GAN stability, including the more robust objective functions provided by Wasserstein GAN with Gradient Penalty (WGAN-GP) \cite{Gulrajani2017} and the efficient Lipschitz constraint enforcement of Spectral Normalization (SN) \cite{Miyato2018} (as discussed in Sections 3.2 and 4.1, respectively), laid critical groundwork. Furthermore, the Progressive Growing of GANs (PGGAN) \cite{Karras2018} (Section 4.2) demonstrated that a curriculum learning approach could enable stable synthesis of high-resolution images, setting the stage for models to explore the potential of sheer scale. These advancements collectively provided the necessary stability and architectural insights that allowed researchers to push the boundaries of GAN performance on demanding datasets.

A pivotal moment in this scaling trajectory was the introduction of BigGAN \cite{Brock2019}, which dramatically elevated the state-of-the-art in image fidelity and diversity on the challenging ImageNet dataset. BigGAN demonstrated that with substantial computational resources, large batch sizes, and meticulous engineering, GANs could generate remarkably realistic and varied images across a vast array of categories. This success was not merely a matter of increasing model size; it was a carefully orchestrated combination of architectural innovations and advanced regularization techniques designed to maintain stability and performance at an unprecedented scale \cite{liu2020jt0}.

Architecturally, BigGAN integrated the self-attention mechanism, a concept previously shown to be effective for modeling long-range dependencies in images by Self-Attention Generative Adversarial Networks (SAGAN) \cite{Zhang2019} (Section 4.3). By incorporating self-attention into both the generator and discriminator, BigGAN enabled the networks to capture global coherence and intricate relationships between distant parts of an image, which is crucial for synthesizing complex scenes with consistent structures. This adaptation of self-attention to a larger scale, alongside the use of shared embeddings for class conditioning, allowed BigGAN to leverage information across different classes more effectively, significantly enhancing sample diversity and the model's ability to generate distinct categories of images. The effectiveness of self-attention in improving image generation quality and stability has been further corroborated in subsequent works, such as SARA-GAN, which also utilized self-attention to build long-range dependencies \cite{yuan2020bt6}.

Beyond architectural enhancements, BigGAN introduced several key regularization techniques specifically tailored to maintain stability and enhance performance when operating at such a massive scale. One critical innovation was orthogonal regularization applied to the generator's weight matrices. This technique was introduced to combat the specific training collapse phenomena observed when scaling GANs to very large models and batch sizes, preventing gradient pathologies and encouraging a more stable learning process \cite{Brock2019}. This aligns with broader research efforts in stabilizing GANs through regularization, as explored by works like \cite{che2016kho} and \cite{roth2017eui}. Another significant contribution was the "truncation trick," which provided a practical mechanism to trade off sample fidelity for diversity. By adjusting the sampling range of the latent space, the truncation trick allowed for the generation of exceptionally high-quality, albeit sometimes less diverse, images, offering a valuable control knob for practitioners. Crucially, BigGAN built upon established stability principles, notably incorporating Spectral Normalization \cite{Miyato2018} within its discriminator. This ensured stable training even with the significantly increased model capacity and large batch sizes, demonstrating a synergistic approach where prior stability mechanisms were essential enablers for scaling.

The synergistic integration of these advancements—large batch sizes, self-attention, shared class embeddings, orthogonal regularization, and the truncation trick—allowed BigGAN to achieve unprecedented Inception Scores and Fréchet Inception Distances on ImageNet. This landmark work unequivocally demonstrated that scaling up GANs, when coupled with careful engineering and a deep understanding of training dynamics, was a viable and powerful path to state-of-the-art image synthesis \cite{wiatrak20194ib, jabbar2020aj0}. However, this monumental achievement came with a substantial computational cost, demanding extensive GPU resources and large batch sizes for training. This trade-off between performance and computational expense highlighted a significant challenge for future research: making such high-fidelity generative models more computationally efficient and accessible to a wider range of researchers and applications.