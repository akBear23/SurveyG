\subsection{Inherent Instabilities: Mode Collapse and Training Divergence}

The initial promise of Generative Adversarial Networks (GANs) was significantly hampered by pervasive challenges related to training instability and mode collapse, which severely limited their practical applicability. These fundamental issues stem from the intricate dynamics of the adversarial training process and the limitations of the original GAN objective function.

A primary concern was **mode collapse**, a phenomenon where the generator fails to capture the full diversity of the real data distribution, instead producing only a limited set of samples or variations thereof. This results in a lack of richness and realism in the generated output, as the generator essentially finds a few "safe" modes that fool the discriminator, neglecting to explore the broader data manifold. \cite{che2016kho} explicitly acknowledged this problem, stating that GANs are "prone to miss modes." The authors attributed this behavior to the particular functional shape of trained discriminators in high-dimensional spaces, which could easily lead to training stagnation or misdirection of probability mass towards regions of higher concentration. To counteract this, \cite{che2016kho} proposed introducing several regularization techniques to the objective function, aiming to stabilize training and promote a "fair distribution of probability mass across the modes of the data generating distribution," thereby offering a unified solution to the missing modes problem.

Concurrently, GANs were plagued by **training instability**, characterized by vanishing or exploding gradients, oscillatory behavior, and outright non-convergence. This made GANs notoriously difficult to train, often requiring extensive hyperparameter tuning and specific architectural heuristics. A significant contributor to this instability was the tendency for the discriminator to become overly powerful, leading to saturated gradients for the generator. When the discriminator perfectly distinguishes real from fake samples, the generator receives little to no meaningful gradient signal, hindering its ability to improve. \cite{miyato2018arc} directly addressed this issue, identifying "the instability of its training" as a major challenge in GAN research. To stabilize the training of the discriminator, they introduced Spectral Normalization, a novel weight normalization technique. This method constrains the Lipschitz constant of the discriminator's layers, ensuring that the gradients remain well-behaved and preventing the discriminator from becoming too strong too quickly, which in turn provides a more stable and continuous learning signal for the generator \cite{miyato2018arc}.

These instabilities and the propensity for mode collapse were often attributed to the limitations of the original GAN objective function, which relies on the Jensen-Shannon (JS) divergence. When the real and generated data distributions are non-overlapping in high-dimensional spaces—a common scenario, especially early in training—the JS divergence can become saturated. This saturation leads to vanishing gradients for the generator, effectively paralyzing its learning process and hindering its ability to explore the full data manifold. The efforts by \cite{che2016kho} to regularize the objective to encourage mode coverage, and by \cite{miyato2018arc} to stabilize discriminator training through spectral normalization, represent crucial early steps in mitigating these inherent challenges. While \cite{che2016kho} focused on the symptom of mode collapse by encouraging broader mode exploration, \cite{miyato2018arc} tackled the more fundamental issue of gradient stability within the adversarial game.

In conclusion, the inherent instabilities of early GANs, particularly mode collapse and the challenges of training divergence rooted in the limitations of the JS divergence, presented significant hurdles to their widespread adoption. The pioneering work on regularization techniques \cite{che2016kho} and discriminator stabilization methods like Spectral Normalization \cite{miyato2018arc} laid essential groundwork. These efforts were critical in understanding the complex dynamics of adversarial training and paved the way for subsequent research into more robust objective functions and architectural innovations designed to achieve stable, diverse, and high-quality generative models.