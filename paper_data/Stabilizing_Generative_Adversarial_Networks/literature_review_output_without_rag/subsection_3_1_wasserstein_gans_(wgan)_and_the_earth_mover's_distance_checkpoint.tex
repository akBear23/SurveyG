\subsection*{Wasserstein GANs (WGAN) and the Earth Mover's Distance}

The initial formulation of Generative Adversarial Networks (GANs) \cite{Goodfellow2014} presented a revolutionary framework for generative modeling, yet it was plagued by significant training instabilities, notably vanishing gradients and mode collapse. These issues largely stemmed from the use of the Jensen-Shannon (JS) divergence as the objective function, which provides a saturated gradient when the real and generated data distributions are non-overlapping, a common occurrence in high-dimensional data spaces. This led to a critical need for a more robust and stable training objective.

A groundbreaking paradigm shift was introduced by Wasserstein GANs (WGAN) \cite{Arjovsky2017}, which fundamentally altered the GAN objective by replacing the problematic Jensen-Shannon divergence with the Earth Mover's (Wasserstein-1) distance. This change offered a smoother and more continuous loss landscape, providing more meaningful gradients even when the real and generated distributions were non-overlapping. The theoretical underpinnings of the Wasserstein distance ensure that a continuous and non-zero gradient is available almost everywhere, directly addressing the critical issues of vanishing gradients and mode collapse that hindered earlier GANs. In this formulation, the discriminator was re-termed as a 'critic' to reflect its new role in estimating the Earth Mover's distance rather than performing binary classification. The initial WGAN formulation enforced a K-Lipschitz constraint on this critic via weight clipping, representing a significant theoretical and practical advancement in achieving GAN training stability \cite{Arjovsky2017}.

Prior to WGAN, researchers attempted to mitigate GAN instabilities through various regularization techniques. For instance, \cite{che2016kho} proposed Mode Regularized Generative Adversarial Networks to address mode collapse and training instability by regularizing the objective function. While these efforts provided some improvements, WGAN offered a more fundamental solution by changing the underlying divergence metric itself, thereby providing a theoretically sounder path to stability.

Building upon the foundational requirement of a Lipschitz-constrained critic in WGAN, subsequent research focused on more robust and efficient methods to enforce this constraint. While weight clipping, as initially proposed by \cite{Arjovsky2017}, could lead to undesirable behaviors such as capacity underutilization or exploding gradients, alternative regularization techniques emerged. For example, \cite{miyato2018arc} introduced Spectral Normalization (SN-GAN), a novel weight normalization technique designed to stabilize the training of the discriminator by directly controlling its Lipschitz constant. This method proved to be computationally light and easy to integrate, offering a more effective and stable way to enforce the Lipschitz constraint compared to simple weight clipping, thereby further enhancing the stability of WGAN-based architectures.

The principles introduced by WGAN have had a lasting impact on the field, extending beyond fundamental generative modeling to various practical applications where stable and robust generative models are crucial. For instance, in wireless communications, the inherent stability of WGANs has been leveraged to enhance channel estimation. \cite{hu2021yk5} proposed a GAN-based channel estimation enhancement algorithm that integrates a conditional GAN with an improved Wasserstein GAN to boost training stability and learning ability. Similarly, \cite{ye2024n41} utilized a GAN variant for channel estimation in intelligent reflecting surface (IRS)-assisted mmWave MIMO systems, specifically designing a loss function to ensure the correct optimization direction and improve training stability, directly benefiting from the lessons learned from Wasserstein GANs. These applications underscore the enduring relevance of WGAN's contributions to stable GAN training.

In conclusion, the introduction of Wasserstein GANs marked a pivotal moment in the evolution of generative adversarial networks. By replacing the Jensen-Shannon divergence with the Earth Mover's distance and enforcing a Lipschitz constraint on the critic, WGAN provided a fundamentally more stable and interpretable training objective. This innovation effectively addressed the critical issues of vanishing gradients and mode collapse, transforming GANs from a notoriously unstable concept into a more reliable and scientifically measurable framework. The subsequent refinements in Lipschitz constraint enforcement and the widespread application of WGAN principles in diverse domains highlight its foundational role in advancing the field of generative modeling.