\subsection{Alternative Objective Functions: LSGAN, EBGAN, BEGAN}

The initial formulation of Generative Adversarial Networks (GANs) often suffered from training instabilities, such as vanishing gradients and mode collapse, largely due to the limitations of the Jensen-Shannon divergence used in its objective function. While subsequent work explored the Wasserstein distance to address these issues, a parallel line of research focused on developing alternative objective functions and discriminator architectures to achieve greater stability and enhance sample quality. This subsection delves into prominent examples of these innovative approaches, including Least Squares GANs (LSGAN), Energy-Based GANs (EBGAN), and Boundary Equilibrium Generative Adversarial Networks (BEGAN).

One of the earliest and most straightforward modifications to the GAN objective was proposed by \textcite{Mao2017} with Least Squares Generative Adversarial Networks (LSGANs). Instead of the traditional sigmoid cross-entropy loss, which can lead to vanishing gradients when samples are far from the decision boundary, LSGANs replace it with a least squares loss function. This modification provides smoother gradients and penalizes samples based on their distance from the decision boundaries, encouraging the generator to produce samples closer to the real data manifold and the discriminator to classify them more confidently. Empirically, LSGANs demonstrated improved training stability and the generation of higher-quality images compared to their cross-entropy counterparts, offering a practical and effective alternative to the original GAN loss.

Moving beyond simple loss function modifications, other researchers explored fundamentally different conceptualizations of the discriminator and its objective. \textcite{Zhao2017} introduced Energy-Based Generative Adversarial Networks (EBGANs), which model the discriminator not as a binary classifier, but as an energy function. In this framework, the discriminator learns to assign low energy to real data samples and high energy to generated (fake) samples. The objective function for the discriminator is designed to minimize the energy of real data while maximizing the energy of fake data, typically through a margin-based loss that pushes fake samples away from the real data manifold. Uniquely, EBGANs often employ an autoencoder architecture for the discriminator, where the reconstruction error serves as the energy function, providing a distinct perspective on stabilizing GAN training and mitigating mode collapse by focusing on manifold learning.

Building upon the concept of an autoencoder discriminator, \textcite{Berthelot2017} presented Boundary Equilibrium Generative Adversarial Networks (BEGAN). BEGAN also utilizes an autoencoder for its discriminator, but its key innovation lies in a novel equilibrium-enforcing loss function. This loss aims to balance the generator and discriminator by controlling the diversity of generated samples and their visual quality. It achieves this balance through a hyperparameter, $\gamma$, which dictates the trade-off between the discriminator's ability to reconstruct real images and its ability to reconstruct generated images. By dynamically adjusting this equilibrium, BEGAN can achieve impressive visual quality and mode coverage, often generating high-resolution images with remarkable fidelity. However, the performance of BEGAN can be sensitive to the tuning of its equilibrium hyperparameter, representing a common challenge in balancing complex GAN objectives.

In summary, these diverse methods offered unique perspectives on achieving stability and enhancing sample quality during the early stages of GAN research. LSGANs provided a simpler, yet effective, modification to the core loss function, yielding smoother gradients and improved stability. EBGANs introduced a conceptual shift by framing the discriminator as an energy function, often leveraging autoencoders to learn data manifolds. BEGAN further refined the autoencoder discriminator concept by introducing an equilibrium mechanism, explicitly targeting a balance between generator and discriminator to achieve high-quality image generation. Collectively, these approaches showcased the breadth of early research into GAN objective design, moving beyond the original minimax game and the Wasserstein framework to explore innovative loss functions, energy-based models, and autoencoder-based discriminators, all contributing to the ongoing quest for robust and high-fidelity generative models. Despite their successes, the challenge of universal stability, optimal hyperparameter tuning, and consistent high-quality generation across diverse datasets remained an active area of research.