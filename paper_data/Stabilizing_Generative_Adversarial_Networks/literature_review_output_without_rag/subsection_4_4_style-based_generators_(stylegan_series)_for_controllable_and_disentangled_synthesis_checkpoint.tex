\subsection{Style-Based Generators (StyleGAN Series) for Controllable and Disentangled Synthesis}

The advent of the StyleGAN series marked a revolutionary turning point in high-fidelity image synthesis, fundamentally transforming generator architecture to enable unprecedented disentangled control over various image features and achieve state-of-the-art visual quality. While earlier Generative Adversarial Networks (GANs) often struggled with training stability and mode collapse \cite{che2016kho, peng2024kkw}, the StyleGAN lineage prioritized architectural innovation to address these issues and push the boundaries of perceptual realism and latent space interpretability.

The foundational work, StyleGAN \cite{Karras2019}, introduced a novel style-based generator architecture that decoupled the generation process from the traditional latent vector input. Instead of feeding a latent code directly into the generator, StyleGAN employed a mapping network to transform a latent vector $\mathbf{z}$ into an intermediate latent space $\mathbf{w}$, which then controlled the synthesis process through adaptive instance normalization (AdaIN) layers at multiple resolutions. This architecture, combined with progressive growing, allowed for fine-grained control over different levels of detail, from coarse pose and identity to subtle features like hair color or freckles, significantly improving the disentanglement of latent factors compared to previous models.

Building upon this success, StyleGAN2 \cite{Karras2020} meticulously analyzed and refined the architecture to address several subtle visual artifacts present in its predecessor, such as the "droplet" artifacts and phase artifacts. Key innovations included the removal of progressive growing in favor of a simpler upsampling scheme, a revised normalization technique (demodulation) that avoided an explicit AdaIN, and the introduction of path length regularization (PLR). PLR encouraged a more linear and disentangled latent space by ensuring that a constant-speed change in the latent space $\mathbf{w}$ resulted in a constant-speed change in the generated image, thereby improving the perceptual quality and interpretability of the latent space. These refinements led to a noticeable improvement in image quality and reduced the occurrence of common GAN artifacts.

The pursuit of ultimate realism culminated in StyleGAN3 \cite{Karras2021}, which tackled fundamental signal processing issues, specifically aliasing, that limited the fidelity of generated images, especially when viewed at high resolutions or during latent space interpolations. StyleGAN3 proposed an alias-free architecture by designing the generator to operate on a continuous-domain representation, effectively addressing the discrete sampling artifacts inherent in convolutional networks. This was achieved through the careful application of anti-aliasing filters and a re-evaluation of upsampling and downsampling operations, resulting in images that maintained their quality and detail even under transformations, making them suitable for applications like animation.

The StyleGAN series collectively represents a pinnacle in generator design, prioritizing fine-grained control, disentanglement of latent factors, and the systematic elimination of subtle visual artifacts. Each iteration built upon the last, moving from a novel architectural concept to meticulous refinements and finally to a fundamental signal processing correction. While the series achieved remarkable success in generating photorealistic images with high controllability, the training of such complex models remains computationally intensive, a common challenge for high-fidelity GANs \cite{peng2024kkw}. Despite these computational demands, the StyleGAN series has profoundly influenced the field, setting new benchmarks for generative image quality and latent space manipulation, and continues to be a cornerstone for research into controllable and disentangled synthesis.