{
  "nodes": [
    {
      "id": "991b64748dfeecf026a27030c16fe1743aa20167",
      "title": "From One Point to a Manifold: Knowledge Graph Embedding for Precise Link Prediction",
      "abstract": "Knowledge graph embedding aims at offering a numerical knowledge representation paradigm by transforming the entities and relations into continuous vector space. However, existing methods could not characterize the knowledge graph in a fine degree to make a precise prediction. There are two reasons: being an ill-posed algebraic system and applying an overstrict geometric form. As precise prediction is critical, we propose an manifold-based embedding principle (\\textbf{ManifoldE}) which could be treated as a well-posed algebraic system that expands the position of golden triples from one point in current models to a manifold in ours. Extensive experiments show that the proposed models achieve substantial improvements against the state-of-the-art baselines especially for the precise prediction task, and yet maintain high efficiency.",
      "authors": [
        "Han Xiao",
        "Minlie Huang",
        "Xiaoyan Zhu"
      ],
      "year": 2015,
      "citation_count": 148,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/991b64748dfeecf026a27030c16fe1743aa20167",
      "pdf_link": "",
      "venue": "International Joint Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "cab5194d13c1ce89a96322adaac754b2cb630d87",
      "title": "TeAST: Temporal Knowledge Graph Embedding via Archimedean Spiral Timeline",
      "abstract": "Temporal knowledge graph embedding (TKGE) models are commonly utilized to infer the missing facts and facilitate reasoning and decision-making in temporal knowledge graph based systems. However, existing methods fuse temporal information into entities, potentially leading to the evolution of entity information and limiting the link prediction performance of TKG. Meanwhile, current TKGE models often lack the ability to simultaneously model important relation patterns and provide interpretability, which hinders their effectiveness and potential applications. To address these limitations, we propose a novel TKGE model which encodes Temporal knowledge graph embeddings via Archimedean Spiral Timeline (TeAST), which maps relations onto the corresponding Archimedean spiral timeline and transforms the quadruples completion to 3th-order tensor completion problem. Specifically, the Archimedean spiral timeline ensures that relations that occur simultaneously are placed on the same timeline, and all relations evolve over time. Meanwhile, we present a novel temporal spiral regularizer to make the spiral timeline orderly. In addition, we provide mathematical proofs to demonstrate the ability of TeAST to encode various relation patterns. Experimental results show that our proposed model significantly outperforms existing TKGE methods. Our code is available at https://github.com/IMU-MachineLearningSXD/TeAST.",
      "authors": [
        "Jiang Li",
        "Xiangdong Su",
        "Guanglai Gao"
      ],
      "year": 2023,
      "citation_count": 27,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/cab5194d13c1ce89a96322adaac754b2cb630d87",
      "pdf_link": "",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5dc88d795cbcd01e6e99ba673e91e9024f0c3318",
      "title": "Modality-Aware Negative Sampling for Multi-modal Knowledge Graph Embedding",
      "abstract": "Negative sampling (NS) is widely used in knowledge graph embedding (KGE), which aims to generate negative triples to make a positive-negative contrast during training. However, existing NS methods are unsuitable when multi-modal information is considered in KGE models. They are also inefficient due to their complex design. In this paper, we propose Modality-Aware Negative Sampling (MANS) for multi-modal knowledge graph embedding (MMKGE) to address the mentioned problems. MANS could align structural and visual embeddings for entities in KGs and learn meaningful embeddings to perform better in multi-modal KGE while keeping lightweight and efficient. Empirical results on two benchmarks demonstrate that MANS outperforms existing NS methods. Meanwhile, we make further explorations about MANS to confirm its effectiveness.",
      "authors": [
        "Yichi Zhang",
        "Mingyang Chen",
        "Wen Zhang"
      ],
      "year": 2023,
      "citation_count": 13,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5dc88d795cbcd01e6e99ba673e91e9024f0c3318",
      "pdf_link": "",
      "venue": "IEEE International Joint Conference on Neural Network",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e39afdbd832bd8fd0fb4f4f7df3722dc5f5cab2a",
      "title": "Knowledge Graph Embedding via Graph Attenuated Attention Networks",
      "abstract": "Knowledge graphs contain a wealth of real-world knowledge that can provide strong support for artificial intelligence applications. Much progress has been made in knowledge graph completion, state-of-the-art models are based on graph convolutional neural networks. These models automatically extract features, in combination with the features of the graph model, to generate feature embeddings with a strong expressive ability. However, these methods assign the same weights on the relation path in the knowledge graph and ignore the rich information presented in neighbor nodes, which result in incomplete mining of triple features. To this end, we propose Graph Attenuated Attention networks(GAATs), a novel representation method, which integrates an attenuated attention mechanism to assign different weight in different relation path and acquire the information from the neighborhoods. As a result, entities and relations can be learned in any neighbors. Our empirical research provides insight into the effectiveness of the attenuated attention-based models, and we show significant improvement compared to the state-of-the-art methods on two benchmark datasets WN18RR and FB15k-237.",
      "authors": [
        "Rui Wang",
        "Bicheng Li",
        "Shengwei Hu",
        "W. Du",
        "Min Zhang"
      ],
      "year": 2020,
      "citation_count": 89,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e39afdbd832bd8fd0fb4f4f7df3722dc5f5cab2a",
      "pdf_link": "",
      "venue": "IEEE Access",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "7e5f318bf5b9c986ca82d2d97e11f50d58ee6680",
      "title": "TranSHER: Translating Knowledge Graph Embedding with Hyper-Ellipsoidal Restriction",
      "abstract": "Knowledge graph embedding methods are important for the knowledge graph completion (or link prediction) task.One state-of-the-art method, PairRE, leverages two separate vectors to model complex relations (i.e., 1-to-N, N-to-1, and N-to-N) in knowledge graphs. However, such a method strictly restricts entities on the hyper-ellipsoid surfaces which limits the optimization of entity distribution, leading to suboptimal performance of knowledge graph completion. To address this issue, we propose a novel score function TranSHER, which leverages relation-specific translations between head and tail entities to relax the constraint of hyper-ellipsoid restrictions. By introducing an intuitive and simple relation-specific translation, TranSHER can provide more direct guidance on optimization and capture more semantic characteristics of entities with complex relations. Experimental results show that TranSHER achieves state-of-the-art performance on link prediction and generalizes well to datasets in different domains and scales. Our codes are public available athttps://github.com/yizhilll/TranSHER.",
      "authors": [
        "Yizhi Li",
        "Wei Fan",
        "Chaochun Liu",
        "Chenghua Lin",
        "Jiang Qian"
      ],
      "year": 2022,
      "citation_count": 12,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/7e5f318bf5b9c986ca82d2d97e11f50d58ee6680",
      "pdf_link": "",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "title": "Knowledge Graph Embedding via Dynamic Mapping Matrix",
      "abstract": "Knowledge graphs are useful resources for numerous AI applications, but they are far from completeness. Previous work such as TransE, TransH and TransR/CTransR regard a relation as translation from head entity to tail entity and the CTransR achieves state-of-the-art performance. In this paper, we propose a more fine-grained model named TransD, which is an improvement of TransR/CTransR. In TransD, we use two vectors to represent a named symbol object (entity and relation). The first one represents the meaning of a(n) entity (relation), the other one is used to construct mapping matrix dynamically. Compared with TransR/CTransR, TransD not only considers the diversity of relations, but also entities. TransD has less parameters and has no matrix-vector multiplication operations, which makes it can be applied on large scale graphs. In Experiments, we evaluate our model on two typical tasks including triplets classification and link prediction. Evaluation results show that our approach outperforms state-of-the-art methods.",
      "authors": [
        "Guoliang Ji",
        "Shizhu He",
        "Liheng Xu",
        "Kang Liu",
        "Jun Zhao"
      ],
      "year": 2015,
      "citation_count": 1542,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "pdf_link": "",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e379f7c85441df5d8ddc1565cabf4b4290c22f1f",
      "title": "SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions",
      "abstract": "\n \n Knowledge graph embedding represents entities and relations in knowledge graph as low-dimensional, continuous vectors, and thus enables knowledge graph compatible with machine learning models. Though there have been a variety of models for knowledge graph embedding, most methods merely concentrate on the fact triples, while supplementary textual descriptions of entities and relations have not been fully employed. To this end, this paper proposes the semantic space projection (SSP) model which jointly learns from the symbolic triples and textual descriptions. Our model builds interaction between the two information sources, and employs textual descriptions to discover semantic relevance and offer precise semantic embedding. Extensive experiments show that our method achieves substantial improvements against baselines on the tasks of knowledge graph completion and entity classification.\n \n",
      "authors": [
        "Han Xiao",
        "Minlie Huang",
        "Lian Meng",
        "Xiaoyan Zhu"
      ],
      "year": 2016,
      "citation_count": 191,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e379f7c85441df5d8ddc1565cabf4b4290c22f1f",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d4220644ef94fa4c2e5138a619cfcd86508d2ea1",
      "title": "Confidence-Aware Negative Sampling Method for Noisy Knowledge Graph Embedding",
      "abstract": "Knowledge graph embedding (KGE) can benefit a variety of downstream tasks, such as link prediction and relation extraction, and has therefore quickly gained much attention. However, most conventional embedding models assume that all triple facts share the same confidence without any noise, which is inappropriate. In fact, many noises and conflicts can be brought into a knowledge graph (KG) because of both the automatic construction process and data quality problems. Fortunately, the novel confidence-aware knowledge representation learning (CKRL) framework was proposed, to incorporate triple confidence into translation-based models for KGE. Though effective at detecting noises, with uniform negative sampling methods, and a harsh triple quality function, CKRL could easily cause zero loss problems and false detection issues. To address these problems, we introduce the concept of negative triple confidence and propose a confidence-aware negative sampling method to support the training of CKRL in noisy KGs. We evaluate our model on the knowledge graph completion task. Experimental results demonstrate that the idea of introducing negative triple confidence can greatly facilitate performance improvement in this task, which confirms the capability of our model in noisy knowledge representation learning (NKRL).",
      "authors": [
        "Yingchun Shan",
        "Chenyang Bu",
        "Xiaojian Liu",
        "Shengwei Ji",
        "Lei Li"
      ],
      "year": 2018,
      "citation_count": 30,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d4220644ef94fa4c2e5138a619cfcd86508d2ea1",
      "pdf_link": "",
      "venue": "International Conference on Big Knowledge",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "52eb7f27cdfbf359096b8b5ef56b2c2826beb660",
      "title": "MADE: Multicurvature Adaptive Embedding for Temporal Knowledge Graph Completion",
      "abstract": "Temporal knowledge graphs (TKGs) are receiving increased attention due to their time-dependent properties and the evolving nature of knowledge over time. TKGs typically contain complex geometric structures, such as hierarchical, ring, and chain structures, which can often be mixed together. However, embedding TKGs into Euclidean space, as is typically done with TKG completion (TKGC) models, presents a challenge when dealing with high-dimensional nonlinear data and complex geometric structures. To address this issue, we propose a novel TKGC model called multicurvature adaptive embedding (MADE). MADE models TKGs in multicurvature spaces, including flat Euclidean space (zero curvature), hyperbolic space (negative curvature), and hyperspherical space (positive curvature), to handle multiple geometric structures. We assign different weights to different curvature spaces in a data-driven manner to strengthen the ideal curvature spaces for modeling and weaken the inappropriate ones. Additionally, we introduce the quadruplet distributor (QD) to assist the information interaction in each geometric space. Ultimately, we develop an innovative temporal regularization to enhance the smoothness of timestamp embeddings by strengthening the correlation of neighboring timestamps. Experimental results show that MADE outperforms the existing state-of-the-art TKGC models.",
      "authors": [
        "Jiapu Wang",
        "Boyue Wang",
        "Junbin Gao",
        "Shirui Pan",
        "Tengfei Liu",
        "Baocai Yin",
        "Wen Gao"
      ],
      "year": 2024,
      "citation_count": 8,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/52eb7f27cdfbf359096b8b5ef56b2c2826beb660",
      "pdf_link": "",
      "venue": "IEEE Transactions on Cybernetics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f4e39a4f8fd8f8453372b74fda17047b9860d870",
      "title": "Beyond Triplets: Hyper-Relational Knowledge Graph Embedding for Link Prediction",
      "abstract": "Knowledge Graph (KG) embeddings are a powerful tool for predicting missing links in KGs. Existing techniques typically represent a KG as a set of triplets, where each triplet (h, r, t) links two entities h and t through a relation r, and learn entity/relation embeddings from such triplets while preserving such a structure. However, this triplet representation oversimplifies the complex nature of the data stored in the KG, in particular for hyper-relational facts, where each fact contains not only a base triplet (h, r, t), but also the associated key-value pairs (k, v). Even though a few recent techniques tried to learn from such data by transforming a hyper-relational fact into an n-ary representation (i.e., a set of key-value pairs only without triplets), they result in suboptimal models as they are unaware of the triplet structure, which serves as the fundamental data structure in modern KGs and preserves the essential information for link prediction. To address this issue, we propose HINGE, a hyper-relational KG embedding model, which directly learns from hyper-relational facts in a KG. HINGE captures not only the primary structural information of the KG encoded in the triplets, but also the correlation between each triplet and its associated key-value pairs. Our extensive evaluation shows the superiority of HINGE on various link prediction tasks over KGs. In particular, HINGE consistently outperforms not only the KG embedding methods learning from triplets only (by 0.81-41.45% depending on the link prediction tasks and settings), but also the methods learning from hyper-relational facts using the n-ary representation (by 13.2-84.1%).",
      "authors": [
        "Paolo Rosso",
        "Dingqi Yang",
        "P. Cudré-Mauroux"
      ],
      "year": 2020,
      "citation_count": 136,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f4e39a4f8fd8f8453372b74fda17047b9860d870",
      "pdf_link": "",
      "venue": "The Web Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "69418ff5d4eac106c72130e152b807004e2b979c",
      "title": "Semantically Smooth Knowledge Graph Embedding",
      "abstract": "This paper considers the problem of embedding Knowledge Graphs (KGs) consisting of entities and relations into lowdimensional vector spaces. Most of the existing methods perform this task based solely on observed facts. The only requirement is that the learned embeddings should be compatible within each individual fact. In this paper, aiming at further discovering the intrinsic geometric structure of the embedding space, we propose Semantically Smooth Embedding (SSE). The key idea of SSE is to take full advantage of additional semantic information and enforce the embedding space to be semantically smooth, i.e., entities belonging to the same semantic category will lie close to each other in the embedding space. Two manifold learning algorithms Laplacian Eigenmaps and Locally Linear Embedding are used to model the smoothness assumption. Both are formulated as geometrically based regularization terms to constrain the embedding task. We empirically evaluate SSE in two benchmark tasks of link prediction and triple classification, and achieve significant and consistent improvements over state-of-the-art methods. Furthermore, SSE is a general framework. The smoothness assumption can be imposed to a wide variety of embedding models, and it can also be constructed using other information besides entities’ semantic categories.",
      "authors": [
        "Shu Guo",
        "Quan Wang",
        "Bin Wang",
        "Lihong Wang",
        "Li Guo"
      ],
      "year": 2015,
      "citation_count": 146,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/69418ff5d4eac106c72130e152b807004e2b979c",
      "pdf_link": "",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "21f8ea62da6a4031d85a1ee701dbc3e6847fa6d3",
      "title": "DualDE: Dually Distilling Knowledge Graph Embedding for Faster and Cheaper Reasoning",
      "abstract": "Knowledge Graph Embedding (KGE) is a popular method for KG reasoning and training KGEs with higher dimension are usually preferred since they have better reasoning capability. However, high-dimensional KGEs pose huge challenges to storage and computing resources and are not suitable for resource-limited or time-constrained applications, for which faster and cheaper reasoning is necessary. To address this problem, we propose DualDE, a knowledge distillation method to build low-dimensional student KGE from pre-trained high-dimensional teacher KGE. DualDE considers the dual-influence between the teacher and the student. In DualDE, we propose a soft label evaluation mechanism to adaptively assign different soft label and hard label weights to different triples, and a two-stage distillation approach to improve the student's acceptance of the teacher. Our DualDE is general enough to be applied to various KGEs. Experimental results show that our method can successfully reduce the embedding parameters of a high-dimensional KGE by 7× - 15× and increase the inference speed by 2× - 6× while retaining a high performance. We also experimentally prove the effectiveness of our soft label evaluation mechanism and two-stage distillation approach via ablation study.",
      "authors": [
        "Yushan Zhu",
        "Wen Zhang",
        "Mingyang Chen",
        "Hui Chen",
        "Xu-Xin Cheng",
        "Wei Zhang",
        "Huajun Chen Zhejiang University",
        "Alibaba Group",
        "Cetc Big Data Research Institute"
      ],
      "year": 2020,
      "citation_count": 34,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/21f8ea62da6a4031d85a1ee701dbc3e6847fa6d3",
      "pdf_link": "",
      "venue": "Web Search and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3e3a84bbceba79843ca1105939b2eb438c149e9e",
      "title": "TransMS: Knowledge Graph Embedding for Complex Relations by Multidirectional Semantics",
      "abstract": "Knowledge graph embedding, which projects the symbolic relations and entities onto low-dimension continuous spaces, is essential to knowledge graph completion. Recently, translation-based embedding models (e.g. TransE) have aroused increasing attention for their simplicity and effectiveness. These models attempt to translate semantics from head entities to tail entities with the relations and infer richer facts outside the knowledge graph. In this paper, we propose a novel knowledge graph embedding method named TransMS, which translates and transmits multidirectional semantics: i) the semantics of head/tail entities and relations to tail/head entities with nonlinear functions and ii) the semantics from entities to relations with linear bias vectors. Our model has merely one additional parameter α than TransE for each triplet, which results in its better scalability in large-scale knowledge graph. Experiments show that TransMS achieves substantial improvements against state-of-the-art baselines, especially the Hit@10s of head entity prediction for N-1 relations and tail entity prediction for 1-N relations improved by about 27.1% and 24.8% on FB15K database respectively.",
      "authors": [
        "Shihui Yang",
        "Jidong Tian",
        "Honglun Zhang",
        "Junchi Yan",
        "Hao He",
        "Yaohui Jin"
      ],
      "year": 2019,
      "citation_count": 75,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3e3a84bbceba79843ca1105939b2eb438c149e9e",
      "pdf_link": "",
      "venue": "International Joint Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "933cb8bf1cd50d6d5833a627683327b15db28836",
      "title": "Joint Language Semantic and Structure Embedding for Knowledge Graph Completion",
      "abstract": "The task of completing knowledge triplets has broad downstream applications. Both structural and semantic information plays an important role in knowledge graph completion. Unlike previous approaches that rely on either the structures or semantics of the knowledge graphs, we propose to jointly embed the semantics in the natural language description of the knowledge triplets with their structure information. Our method embeds knowledge graphs for the completion task via fine-tuning pre-trained language models with respect to a probabilistic structured loss, where the forward pass of the language models captures semantics and the loss reconstructs structures. Our extensive experiments on a variety of knowledge graph benchmarks have demonstrated the state-of-the-art performance of our method. We also show that our method can significantly improve the performance in a low-resource regime, thanks to the better use of semantics. The code and datasets are available at https://github.com/pkusjh/LASS.",
      "authors": [
        "Jianhao Shen",
        "Chenguang Wang",
        "Linyuan Gong",
        "Dawn Song"
      ],
      "year": 2022,
      "citation_count": 39,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/933cb8bf1cd50d6d5833a627683327b15db28836",
      "pdf_link": "",
      "venue": "International Conference on Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "15710515bae025372f298570267d234d4a3141cb",
      "title": "Knowledge graph embedding closed under composition",
      "abstract": "Knowledge Graph Embedding (KGE) has attracted increasing attention. Relation patterns, such as symmetry and inversion, have received considerable focus. Among them, composition patterns are particularly important, as they involve nearly all relations in KGs. However, prior KGE approaches often consider relations to be compositional only if they are well-represented in the training data. Consequently, it can lead to performance degradation, especially for under-represented composition patterns. To this end, we propose HolmE, a general form of KGE with its relation embedding space closed under composition, namely that the composition of any two given relation embeddings remains within the embedding space. This property ensures that every relation embedding can compose, or be composed by other relation embeddings. It enhances HolmE’s capability to model under-represented (also called long-tail) composition patterns with limited learning instances. To our best knowledge, our work is pioneering in discussing KGE with this property of being closed under composition. We provide detailed theoretical proof and extensive experiments to demonstrate the notable advantages of HolmE in modelling composition patterns, particularly for long-tail patterns. Our results also highlight HolmE’s effectiveness in extrapolating to unseen relations through composition and its state-of-the-art performance on benchmark datasets.",
      "authors": [
        "Zhuoxun Zheng",
        "Baifan Zhou",
        "Hui Yang",
        "Zhipeng Tan",
        "Zequn Sun",
        "Chunnong Li",
        "A. Waaler",
        "Evgeny Kharlamov",
        "A. Soylu"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/15710515bae025372f298570267d234d4a3141cb",
      "pdf_link": "",
      "venue": "Data mining and knowledge discovery",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e03b8e02ddda86eafb54cafc5c44d231992be95a",
      "title": "Learning Knowledge Graph Embedding With Heterogeneous Relation Attention Networks",
      "abstract": "Knowledge graph (KG) embedding aims to study the embedding representation to retain the inherent structure of KGs. Graph neural networks (GNNs), as an effective graph representation technique, have shown impressive performance in learning graph embedding. However, KGs have an intrinsic property of heterogeneity, which contains various types of entities and relations. How to address complex graph data and aggregate multiple types of semantic information simultaneously is a critical issue. In this article, a novel heterogeneous GNNs framework based on attention mechanism is proposed. Specifically, the neighbor features of an entity are first aggregated under each relation-path. Then the importance of different relation-paths is learned through the relation features. Finally, each relation-path-based features with the learned weight values are aggregated to generate the embedding representation. Thus, the proposed method not only aggregates entity features from different semantic aspects but also allocates appropriate weights to them. This method can capture various types of semantic information and selectively aggregate informative features. The experiment results on three real-world KGs demonstrate superior performance when compared with several state-of-the-art methods.",
      "authors": [
        "Zhifei Li",
        "Hai Liu",
        "Zhaoli Zhang",
        "Tingting Liu",
        "N. Xiong"
      ],
      "year": 2021,
      "citation_count": 246,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e03b8e02ddda86eafb54cafc5c44d231992be95a",
      "pdf_link": "",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f44ee7932aacd054101b00f37d4c26c27630c557",
      "title": "Multi-Scale Dynamic Convolutional Network for Knowledge Graph Embedding",
      "abstract": "Knowledge graphs are large graph-structured knowledge bases with incomplete or partial information. Numerous studies have focused on knowledge graph embedding to identify the embedded representation of entities and relations, thereby predicting missing relations between entities. Previous embedding models primarily regard (subject entity, relation, and object entity) triplet as translational distance or semantic matching in vector space. However, these models only learn a few expressive features and hard to handle complex relations, i.e., 1-to-N, N-to-1, and N-to-N, in knowledge graphs. To overcome these issues, we introduce a multi-scale dynamic convolutional network (M-DCN) model for knowledge graph embedding. This model features topnotch performance and an ability to generate richer and more expressive feature embeddings than its counterparts. The subject entity and relation embeddings in M-DCN are composed in an alternating pattern in the input layer, which helps extract additional feature interactions and increase the expressiveness. Multi-scale filters are generated in the convolution layer to learn different characteristics among input embeddings. Specifically, the weights of these filters are dynamically related to each relation to model complex relations. The performance of M-DCN on the five benchmark datasets is tested via experiments. Results show that the model can effectively handle complex relations and achieve state-of-the-art link prediction results on most evaluation metrics.",
      "authors": [
        "Zhaoli Zhang",
        "Zhifei Li",
        "Hai Liu",
        "N. Xiong"
      ],
      "year": 2020,
      "citation_count": 128,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f44ee7932aacd054101b00f37d4c26c27630c557",
      "pdf_link": "",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "bcffbb40e7922d2a34e752f8faaa4fe99649e21a",
      "title": "Fully Hyperbolic Rotation for Knowledge Graph Embedding",
      "abstract": "Hyperbolic rotation is commonly used to effectively model knowledge graphs and their inherent hierarchies. However, existing hyperbolic rotation models rely on logarithmic and exponential mappings for feature transformation. These models only project data features into hyperbolic space for rotation, limiting their ability to fully exploit the hyperbolic space. To address this problem, we propose a novel fully hyperbolic model designed for knowledge graph embedding. Instead of feature mappings, we define the model directly in hyperbolic space with the Lorentz model. Our model considers each relation in knowledge graphs as a Lorentz rotation from the head entity to the tail entity. We adopt the Lorentzian version distance as the scoring function for measuring the plausibility of triplets. Extensive results on standard knowledge graph completion benchmarks demonstrated that our model achieves competitive results with fewer parameters. In addition, our model get the state-of-the-art performance on datasets of CoDEx-s and CoDEx-m, which are more diverse and challenging than before. Our code is available at https://github.com/llqy123/FHRE.",
      "authors": [
        "Qiuyu Liang",
        "Weihua Wang",
        "F. Bao",
        "Guanglai Gao"
      ],
      "year": 2024,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/bcffbb40e7922d2a34e752f8faaa4fe99649e21a",
      "pdf_link": "",
      "venue": "European Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1620a20881b572b5ffc6f9cb3cf39f6090cee19f",
      "title": "ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding",
      "abstract": "The goal of Knowledge graph embedding (KGE) is to learn how to represent the low dimensional vectors for entities and relations based on the observed triples. The conventional shallow models are limited to their expressiveness. ConvE (Dettmers et al., 2018) takes advantage of CNN and improves the expressive power with parameter efficient operators by increasing the interactions between head and relation embeddings. However, there is no structural information in the embedding space of ConvE, and the performance is still limited by the number of interactions. The recent KBGAT (Nathani et al., 2019) provides another way to learn embeddings by adaptively utilizing structural information. In this paper, we take the benefits of ConvE and KBGAT together and propose a Relation-aware Inception network with joint local-global structural information for knowledge graph Embedding (ReInceptionE). Specifically, we first explore the Inception network to learn query embedding, which aims to further increase the interactions between head and relation embeddings. Then, we propose to use a relation-aware attention mechanism to enrich the query embedding with the local neighborhood and global entity information. Experimental results on both WN18RR and FB15k-237 datasets demonstrate that ReInceptionE achieves competitive performance compared with state-of-the-art methods.",
      "authors": [
        "Zhiwen Xie",
        "Guangyou Zhou",
        "Jin Liu",
        "Xiangji Huang"
      ],
      "year": 2020,
      "citation_count": 63,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1620a20881b572b5ffc6f9cb3cf39f6090cee19f",
      "pdf_link": "",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "44ce738296c3148c6593324773706cdc228614d4",
      "title": "CompoundE: Knowledge Graph Embedding with Translation, Rotation and Scaling Compound Operations",
      "abstract": "Translation, rotation, and scaling are three commonly used geometric manipulation operations in image processing. Besides, some of them are successfully used in developing effective knowledge graph embedding (KGE) models such as TransE and RotatE. Inspired by the synergy, we propose a new KGE model by leveraging all three operations in this work. Since translation, rotation, and scaling operations are cascaded to form a compound one, the new model is named CompoundE. By casting CompoundE in the framework of group theory, we show that quite a few scoring-function-based KGE models are special cases of CompoundE. CompoundE extends the simple distance-based relation to relation-dependent compound operations on head and/or tail entities. To demonstrate the effectiveness of CompoundE, we conduct experiments on three popular KG completion datasets. Experimental results show that CompoundE consistently achieves the state of-the-art performance.",
      "authors": [
        "Xiou Ge",
        "Yun Cheng Wang",
        "Bin Wang",
        "C.-C. Jay Kuo"
      ],
      "year": 2022,
      "citation_count": 11,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/44ce738296c3148c6593324773706cdc228614d4",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c762e198b0239313ee50476021b1939390c4ef9d",
      "title": "Knowledge Graph Embedding",
      "abstract": "A knowledge graph is a graph with entities of different types as nodes and various relations among them as edges. The construction of knowledge graphs in the past decades facilitates many applications, such as link prediction, web search analysis, question answering, and so on. Knowledge graph embedding aims to represent entities and relations in a large-scale knowledge graph as elements in a continuous vector space. Existing methods, for example, TransE, TransH, and TransR, learn the embedding representation by defining a global margin-based loss function over the data. However, the loss function is determined during experiments whose parameters are examined among a closed set of candidates. Moreover, embeddings over two knowledge graphs with different entities and relations share the same set of candidates, ignoring the locality of both graphs. This leads to the limited performance of embedding related applications. In this article, a locally adaptive translation method for knowledge graph embedding, called TransA, is proposed to find the loss function by adaptively determining its margin over different knowledge graphs. Then the convergence of TransA is verified from the aspect of its uniform stability. To make the embedding methods up-to-date when new vertices and edges are added into the knowledge graph, the incremental algorithm for TransA, called iTransA, is proposed by adaptively adjusting the optimal margin over time. Experiments on four benchmark data sets demonstrate the superiority of the proposed method, as compared to the state-of-the-art ones.",
      "authors": [
        "Yantao Jia",
        "Yuanzhuo Wang",
        "Xiaolong Jin",
        "Hailun Lin",
        "Xueqi Cheng"
      ],
      "year": 2017,
      "citation_count": 54,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c762e198b0239313ee50476021b1939390c4ef9d",
      "pdf_link": "",
      "venue": "ACM Transactions on the Web",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "7029ecb5d5fc04f54e1e25e739db2e993fb147c8",
      "title": "SpherE: Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval",
      "abstract": "Knowledge graphs (KGs), which store an extensive number of relational facts (head, relation, tail), serve various applications. While many downstream tasks highly rely on the expressive modeling and predictive embedding of KGs, most of the current KG representation learning methods, where each entity is embedded as a vector in the Euclidean space and each relation is embedded as a transformation, follow an entity ranking protocol. On one hand, such an embedding design cannot capture many-to-many relations. On the other hand, in many retrieval cases, the users wish to get an exact set of answers without any ranking, especially when the results are expected to be precise, e.g., which genes cause an illness. Such scenarios are commonly referred to as \"set retrieval\". This work presents a pioneering study on the KG set retrieval problem. We show that the set retrieval highly depends on expressive modeling of many-to-many relations, and propose a new KG embedding model SpherE to address this problem. SpherE is based on rotational embedding methods, but each entity is embedded as a sphere instead of a vector. While inheriting the high interpretability of rotational-based models, our SpherE can more expressively model one-to-many, many-to-one, and many-to-many relations. Through extensive experiments, we show that our SpherE can well address the set retrieval problem while still having a good predictive ability to infer missing facts. The code is available at https://github.com/Violet24K/SpherE.",
      "authors": [
        "Li",
        "Yuyi Ao",
        "Jingrui He"
      ],
      "year": 2024,
      "citation_count": 14,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/7029ecb5d5fc04f54e1e25e739db2e993fb147c8",
      "pdf_link": "",
      "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3f0d5aa7a637d2c0bb3d768c99cc203430b4481e",
      "title": "A Lightweight Knowledge Graph Embedding Framework for Efficient Inference and Storage",
      "abstract": "Knowledge graphs, which consist of entities and their relations, have become a popular way to store structured knowledge. Knowledge graph embedding (KGE), which derives a representation for each entity and relation, has been widely used to capture the semantics of the information in the knowledge graphs, and has demonstrated great success in many downstream applications, such as the extraction of similar entities in response to a query entity. However, existing KGE methods cannot work well on emerging knowledge graphs that are large-scale due to the constraints in storage and inference efficiency. In this paper, we propose a lightweight KGE model, LightKG, which significantly reduces storage as well as running time needed for inference. Instead of storing a continuous vector for every entity, LightKG only needs to store a few codebooks, each of which contains some codewords that correspond to the representatives among the embeddings, and the indices that correspond to the codeword selections for entities. Hence LightKG can achieve highly efficient storage. The efficiency of the downstream querying process can be significantly boosted too with the proposed LightKG model as the relevance score between the query and an entity can be efficiently calculated via a quick look-up in a table that contains the scores between the query and codewords. The storage and inference efficiency of LightKG is achieved by its novel design. LightKG is an end-to-end framework that automatically infers codebooks and codewords and generates an approximated embedding for each entity. A residual module is included in LightKG to induce the diversity among codebooks, and a continuous function is adopted to approximate codeword selection, which is non-differential. In addition, to further improve the performance of KGE, we propose a novel dynamic negative sampling method based on quantization, which can be applied to the proposed LightKG or other KGE methods. We conduct extensive experiments on five public datasets. The experiments show that LightKG is search and memory efficient with high approximate search accuracy. Also, the dynamic negative sampling can dramatically improve model performance with over 19% improvement on average.",
      "authors": [
        "Haoyu Wang",
        "Yaqing Wang",
        "Defu Lian",
        "Jing Gao"
      ],
      "year": 2021,
      "citation_count": 16,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3f0d5aa7a637d2c0bb3d768c99cc203430b4481e",
      "pdf_link": "",
      "venue": "International Conference on Information and Knowledge Management",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "beade097ff41c62a8d8d29065be0e1339be39f30",
      "title": "NSCaching: Simple and Efficient Negative Sampling for Knowledge Graph Embedding",
      "abstract": "Knowledge graph (KG) embedding is a fundamental problem in data mining research with many real-world applications. It aims to encode the entities and relations in the graph into low dimensional vector space, which can be used for subsequent algorithms. Negative sampling, which samples negative triplets from non-observed ones in the training data, is an important step in KG embedding. Recently, generative adversarial network (GAN), has been introduced in negative sampling. By sampling negative triplets with large scores, these methods avoid the problem of vanishing gradient and thus obtain better performance. However, using GAN makes the original model more complex and harder to train, where reinforcement learning must be used. In this paper, motivated by the observation that negative triplets with large scores are important but rare, we propose to directly keep track of them with cache. However, how to sample from and update the cache are two important questions. We carefully design the solutions, which are not only efficient but also achieve good balance between exploration and exploitation. In this way, our method acts as a \"distilled\" version of previous GAN-based methods, which does not waste training time on additional parameters to fit the full distribution of negative triplets. The extensive experiments show that our method can gain significant improvement on various KG embedding models, and outperform the state-of-the-arts negative sampling methods based on GAN.",
      "authors": [
        "Yongqi Zhang",
        "Quanming Yao",
        "Yingxia Shao",
        "Lei Chen"
      ],
      "year": 2018,
      "citation_count": 127,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/beade097ff41c62a8d8d29065be0e1339be39f30",
      "pdf_link": "",
      "venue": "IEEE International Conference on Data Engineering",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b594b21557395c6a8fa8356249373f8e318c2df2",
      "title": "AutoSF: Searching Scoring Functions for Knowledge Graph Embedding",
      "abstract": "Scoring functions (SFs), which measure the plausibility of triplets in knowledge graph (KG), have become the crux of KG embedding. Lots of SFs, which target at capturing different kinds of relations in KGs, have been designed by humans in recent years. However, as relations can exhibit complex patterns that are hard to infer before training, none of them can consistently perform better than others on existing benchmark data sets. In this paper, inspired by the recent success of automated machine learning (AutoML), we propose to automatically design SFs (AutoSF) for distinct KGs by the AutoML techniques. However, it is non-trivial to explore domain- specific information here to make AutoSF efficient and effective. We firstly identify a unified representation over popularly used SFs, which helps to set up a search space for AutoSF. Then, we propose a greedy algorithm to search in such a space efficiently. The algorithm is further sped up by a filter and a predictor, which can avoid repeatedly training SFs with same expressive ability and help removing bad candidates during the search before model training. Finally, we perform extensive experiments on benchmark data sets. Results on link prediction and triplets classification show that the searched SFs by AutoSF, are KG dependent, new to the literature, and outperform the state-of- the-art SFs designed by humans. 1",
      "authors": [
        "Yongqi Zhang",
        "Quanming Yao",
        "Wenyuan Dai",
        "Lei Chen"
      ],
      "year": 2019,
      "citation_count": 82,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b594b21557395c6a8fa8356249373f8e318c2df2",
      "pdf_link": "",
      "venue": "IEEE International Conference on Data Engineering",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "990334cf76845e2da64d3baa10b0a671e433d4b6",
      "title": "TorusE: Knowledge Graph Embedding on a Lie Group",
      "abstract": "\n \n Knowledge graphs are useful for many artificial intelligence (AI) tasks. However, knowledge graphs often have missing facts. To populate the graphs, knowledge graph embedding models have been developed. Knowledge graph embedding models map entities and relations in a knowledge graph to a vector space and predict unknown triples by scoring candidate triples. TransE is the first translation-based method and it is well known because of its simplicity and efficiency for knowledge graph completion. It employs the principle that the differences between entity embeddings represent their relations. The principle seems very simple, but it can effectively capture the rules of a knowledge graph. However, TransE has a problem with its regularization. TransE forces entity embeddings to be on a sphere in the embedding vector space. This regularization warps the embeddings and makes it difficult for them to fulfill the abovementioned principle. The regularization also affects adversely the accuracies of the link predictions. On the other hand, regularization is important because entity embeddings diverge by negative sampling without it. This paper proposes a novel embedding model, TorusE, to solve the regularization problem. The principle of TransE can be defined on any Lie group. A torus, which is one of the compact Lie groups, can be chosen for the embedding space to avoid regularization. To the best of our knowledge, TorusE is the first model that embeds objects on other than a real or complex vector space, and this paper is the first to formally discuss the problem of regularization of TransE. Our approach outperforms other state-of-the-art approaches such as TransE, DistMult and ComplEx on a standard link prediction task. We show that TorusE is scalable to large-size knowledge graphs and is faster than the original TransE.\n \n",
      "authors": [
        "Takuma Ebisu",
        "R. Ichise"
      ],
      "year": 2017,
      "citation_count": 215,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/990334cf76845e2da64d3baa10b0a671e433d4b6",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "7eece37709dceba5086f48dc43ac1a69d0427486",
      "title": "Joint Knowledge Graph and Large Language Model for Fault Diagnosis and Its Application in Aviation Assembly",
      "abstract": "In complex assembly industry settings, fault localization involves rapidly and accurately identifying the source of a fault and obtaining a troubleshooting solution based on fault symptoms. This study proposes a knowledge-enhanced joint model that incorporates aviation assembly knowledge graph (KG) embedding into large language models (LLMs). This model utilizes graph-structured Big Data within KGs to conduct prefix-tuning of the LLMs. The KGs for prefix-tuning enable an online reconfiguration of the LLMs, which avoids a massive computational load. Through the subgraph embedding learning process, the specialized knowledge of the joint model within the aviation assembly domain, especially in fault localization, is strengthened. In the context of aviation assembly functional testing, the joint model can generate knowledge subgraphs, fuse knowledge through retrieval augmentation, and ultimately provide knowledge-based reasoning responses. In practical industrial scenario experiments, the joint enhancement model demonstrates an accuracy of 98.5% for fault diagnosis and troubleshooting schemes.",
      "authors": [
        "Peifeng Liu",
        "Lu Qian",
        "Xingwei Zhao",
        "Bo Tao"
      ],
      "year": 2024,
      "citation_count": 60,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/7eece37709dceba5086f48dc43ac1a69d0427486",
      "pdf_link": "",
      "venue": "IEEE Transactions on Industrial Informatics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "12cc4b65644a84a16ef7dfe7bdd70172cd38cffd",
      "title": "Multihop Fuzzy Spatiotemporal RDF Knowledge Graph Query via Quaternion Embedding",
      "abstract": "The proliferation of uncertain spatiotemporal data has led to an increasing demand for fuzzy spatiotemporal knowledge modeling in various applications. However, performing multihop query modeling on incomplete fuzzy spatiotemporal knowledge graphs (KGs) poses significant challenges. Recently, embedding-based multihop KG querying approaches have gained attention. Yet, these approaches often overlook KG uncertainty and spatiotemporal sensitivity, resulting in the neglect of fuzzy spatiotemporal information during multihop path reasoning. To address these challenges, we propose an embedding-based multihop query model for fuzzy spatiotemporal KG. We use quaternion to jointly embed spatiotemporal entities, and relations are represented as rotations from spatiotemporal subject to object. We incorporate uncertainty by the scoring function's bias factor, allowing for relaxation embedding. This approach facilitates the learning of a richer representation of fuzzy spatiotemporal KGs in vector space. By exploiting the inherent noncommutative compositional pattern of quaternions, we construct more accurate multihop paths within fuzzy spatiotemporal KGs, thus improving path reasoning performance. To evaluate the effectiveness of our model, we conduct experiments on two fuzzy spatiotemporal KG datasets, focusing on link prediction and path query answering. Results show that our proposed method significantly outperforms several state-of-the-art baselines in terms of performance metrics.",
      "authors": [
        "Hao Ji",
        "Li Yan",
        "Z. Ma"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/12cc4b65644a84a16ef7dfe7bdd70172cd38cffd",
      "pdf_link": "",
      "venue": "IEEE transactions on fuzzy systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "58e1b93b18370433633152cb8825917edc2f16a6",
      "title": "Temporal Knowledge Graph Embedding Model based on Additive Time Series Decomposition",
      "abstract": "Knowledge Graph (KG) embedding has attracted more attention in recent years. Most KG embedding models learn from time-unaware triples. However, the inclusion of temporal information beside triples would further improve the performance of a KGE model. In this regard, we propose ATiSE, a temporal KG embedding model which incorporates time information into entity/relation representations by using Additive Time Series decomposition. Moreover, considering the temporal uncertainty during the evolution of entity/relation representations over time, we map the representations of temporal KGs into the space of multi-dimensional Gaussian distributions. The mean of each entity/relation embedding at a time step shows the current expected position, whereas its covariance (which is temporally stationary) represents its temporal uncertainty. Experimental results show that ATiSE chieves the state-of-the-art on link prediction over four temporal KGs.",
      "authors": [
        "Chengjin Xu",
        "M. Nayyeri",
        "Fouad Alkhoury",
        "Jens Lehmann",
        "H. S. Yazdi"
      ],
      "year": 2019,
      "citation_count": 83,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/58e1b93b18370433633152cb8825917edc2f16a6",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "354fb91810c6d3756600c99ad84d2e6ef4136021",
      "title": "A type-augmented knowledge graph embedding framework for knowledge graph completion",
      "abstract": "Knowledge graphs (KGs) are of great importance to many artificial intelligence applications, but they usually suffer from the incomplete problem. Knowledge graph embedding (KGE), which aims to represent entities and relations in low-dimensional continuous vector spaces, has been proved to be a promising approach for KG completion. Traditional KGE methods only concentrate on structured triples, while paying less attention to the type information of entities. In fact, incorporating entity types into embedding learning could further improve the performance of KG completion. To this end, we propose a universal Type-augmented Knowledge graph Embedding framework (TaKE) which could utilize type features to enhance any traditional KGE models. TaKE automatically captures type features under no explicit type information supervision. And by learning different type representations of each entity, TaKE could distinguish the diversity of types specific to distinct relations. We also design a new type-constrained negative sampling strategy to construct more effective negative samples for the training process. Extensive experiments on four datasets from three real-world KGs (Freebase, WordNet and YAGO) demonstrate the merits of our proposed framework. In particular, combining TaKE with the recent tensor factorization KGE model SimplE can achieve state-of-the-art performance on the KG completion task.",
      "authors": [
        "Peng He",
        "Gang Zhou",
        "Yao Yao",
        "Zhe Wang",
        "Hao Yang"
      ],
      "year": 2023,
      "citation_count": 12,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/354fb91810c6d3756600c99ad84d2e6ef4136021",
      "pdf_link": "",
      "venue": "Scientific Reports",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "af051c87cecca64c2de4ad9110608f7579766653",
      "title": "OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding",
      "abstract": "Semantic embedding has been widely investigated for aligning knowledge graph (KG) entities. Current methods have explored and utilized the graph structure, the entity names and attributes, but ignore the ontology (or ontological schema) which contains critical meta information such as classes and their membership relationships with entities. In this paper, we propose an ontology-guided entity alignment method named OntoEA, where both KGs and their ontologies are jointly embedded, and the class hierarchy and the class disjointness are utilized to avoid false mappings. Extensive experiments on seven public and industrial benchmarks have demonstrated the state-of-the-art performance of OntoEA and the effectiveness of the ontologies.",
      "authors": [
        "Yuejia Xiang",
        "Ziheng Zhang",
        "Jiaoyan Chen",
        "Xi Chen",
        "Zhenxi Lin",
        "Yefeng Zheng"
      ],
      "year": 2021,
      "citation_count": 44,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/af051c87cecca64c2de4ad9110608f7579766653",
      "pdf_link": "",
      "venue": "Findings",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "11e402c699bcb54d57da1a5fdbc57076d7255baf",
      "title": "Multi-view Knowledge Graph Embedding for Entity Alignment",
      "abstract": "We study the problem of embedding-based entity alignment between knowledge graphs (KGs). Previous works mainly focus on the relational structure of entities. Some further incorporate another type of features, such as attributes, for refinement. However, a vast of entity features are still unexplored or not equally treated together, which impairs the accuracy and robustness of embedding-based entity alignment. In this paper, we propose a novel framework that unifies multiple views of entities to learn embeddings for entity alignment. Specifically, we embed entities based on the views of entity names, relations and attributes, with several combination strategies. Furthermore, we design some cross-KG inference methods to enhance the alignment between two KGs. Our experiments on real-world datasets show that the proposed framework significantly outperforms the state-of-the-art embedding-based entity alignment methods. The selected views, cross-KG inference and combination strategies all contribute to the performance improvement.",
      "authors": [
        "Qingheng Zhang",
        "Zequn Sun",
        "Wei Hu",
        "Muhao Chen",
        "Lingbing Guo",
        "Yuzhong Qu"
      ],
      "year": 2019,
      "citation_count": 263,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/11e402c699bcb54d57da1a5fdbc57076d7255baf",
      "pdf_link": "",
      "venue": "International Joint Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "84aa127dc5ca3080385439cb10edc50b5d2c04e4",
      "title": "Knowledge graph embedding methods for entity alignment: experimental review",
      "abstract": "In recent years, we have witnessed the proliferation of knowledge graphs (KG) in various domains, aiming to support applications like question answering, recommendations, etc. A frequent task when integrating knowledge from different KGs is to find which subgraphs refer to the same real-world entity, a task largely known as the Entity Alignment. Recently, embedding methods have been used for entity alignment tasks, that learn a vector-space representation of entities which preserves their similarity in the original KGs. A wide variety of supervised, unsupervised, and semi-supervised methods have been proposed that exploit both factual (attribute based) and structural information (relation based) of entities in the KGs. Still, a quantitative assessment of their strengths and weaknesses in real-world KGs according to different performance metrics and KG characteristics is missing from the literature. In this work, we conduct the first meta-level analysis of popular embedding methods for entity alignment, based on a statistically sound methodology. Our analysis reveals statistically significant correlations of different embedding methods with various meta-features extracted by KGs and rank them in a statistically significant way according to their effectiveness across all real-world KGs of our testbed. Finally, we study interesting trade-offs in terms of methods’ effectiveness and efficiency.",
      "authors": [
        "N. Fanourakis",
        "Vasilis Efthymiou",
        "D. Kotzinos",
        "V. Christophides"
      ],
      "year": 2022,
      "citation_count": 42,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/84aa127dc5ca3080385439cb10edc50b5d2c04e4",
      "pdf_link": "",
      "venue": "Data mining and knowledge discovery",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "19a672bdf29367b7509586a4be27c6843af903b1",
      "title": "Probability Calibration for Knowledge Graph Embedding Models",
      "abstract": "Knowledge graph embedding research has overlooked the problem of probability calibration. We show popular embedding models are indeed uncalibrated. That means probability estimates associated to predicted triples are unreliable. We present a novel method to calibrate a model when ground truth negatives are not available, which is the usual case in knowledge graphs. We propose to use Platt scaling and isotonic regression alongside our method. Experiments on three datasets with ground truth negatives show our contribution leads to well-calibrated models when compared to the gold standard of using negatives. We get significantly better results than the uncalibrated models from all calibration methods. We show isotonic regression offers the best the performance overall, not without trade-offs. We also show that calibrated models reach state-of-the-art accuracy without the need to define relation-specific decision thresholds.",
      "authors": [
        "Pedro Tabacof",
        "Luca Costabello"
      ],
      "year": 2019,
      "citation_count": 47,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/19a672bdf29367b7509586a4be27c6843af903b1",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b1d807fc6b184d757ebdea67acd81132d8298ff6",
      "title": "Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation",
      "abstract": "Learning and development, or L&D, plays an important role in talent management, which aims to improve the knowledge and capabilities of employees through a variety of performance-oriented training activities. Recently, with the rapid development of enterprise management information systems, many research efforts and industrial practices have been devoted to building personalized employee training course recommender systems. Nevertheless, a widespread challenge is how to provide explainable recommendations with the consideration of different learning motivations from talents. To this end, we propose CKGE, a contextualized knowledge graph (KG) embedding approach for developing an explainable training course recommender system. A novel perspective of CKGE is to integrate both the contextualized neighbor semantics and high-order connections as motivation-aware information for learning effective representations of talents and courses. Specifically, in CKGE, for each entity pair (i.e., the talent-course pair), we first construct a meta-graph, including the neighbors of each entity and the meta-paths between entities as motivation-aware information. Then, we develop a novel KG-based Transformer, which can serialize entities and paths in the meta-graph as a sequential input, with the specially designed relational attention and structural encoding mechanisms to better model the global dependence of KG structured data. Meanwhile, the local path mask prediction can effectively reveal the importance of different paths. As a result, CKGE not only can make precise predictions but also can discriminate the saliencies of meta-paths in characterizing corresponding preferences. Extensive experiments on real-world and public datasets clearly validate the effectiveness and interpretability of CKGE compared with state-of-the-art baselines.",
      "authors": [
        "Yang Yang",
        "Chubing Zhang",
        "Xin Song",
        "Zheng Dong",
        "Hengshu Zhu",
        "Wenjie Li"
      ],
      "year": 2023,
      "citation_count": 33,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b1d807fc6b184d757ebdea67acd81132d8298ff6",
      "pdf_link": "",
      "venue": "ACM Trans. Inf. Syst.",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "33f3f53c957c4a8832b1dcb095a4ac967bd89897",
      "title": "A Semantic Enhanced Knowledge Graph Embedding Model With AIGC Designed for Healthcare Prediction",
      "abstract": "AI technology has been often employed to establish knowledge graph embedding (KGE) model, which can be used for link prediction on medical knowledge graph to help medical decision-making and disease prediction. However, traditional knowledge graph completion models usually focus on exploiting simple structural features during the phase of feature learning while neglecting the complex structural feature. Considering AI-generated content (AIGC) has shown great potentials for healthcare electronics (HE), a knowledge graph embedding model with AIGC called SEConv is proposed for medical knowledge graph completion. Firstly, a less resource-consuming model of self-attention mechanism is introduced to generate more expressive embedding representations, which contributes to deploying on resource-limited consumer electronics. Secondly, in order to extract more informative features from the triplets, a multilayer convolutional neural network is adopted to learn deeper structural features. Experiments have been implemented on the medical dataset of UMLS and DBpedia50, and other two benchmark datasets. And the results show that SEConv excels in learning more expressive and discriminative feature representations. Compared with the baseline models, SEConv achieves a substantial improvement, which verifies it can be used for healthcare prediction task and smart healthcare treatments.",
      "authors": [
        "Qingqing Yang",
        "Min He",
        "Zhongwen Li",
        "Tao He",
        "Seunggil Jeon"
      ],
      "year": 2025,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/33f3f53c957c4a8832b1dcb095a4ac967bd89897",
      "pdf_link": "",
      "venue": "IEEE transactions on consumer electronics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "6205f75cb6db1503c94386441ca68c63c9cbd456",
      "title": "CPa-WAC: Constellation Partitioning-based Scalable Weighted Aggregation Composition for Knowledge Graph Embedding",
      "abstract": "Scalability and training time are crucial for any graph neural network model processing a knowledge graph (KG). While partitioning knowledge graphs helps reduce the training time, the prediction accuracy reduces significantly compared to training the model on the whole graph. In this paper, we propose CPa-WAC: a lightweight architecture that incorporates graph convolutional networks and modularity maximization-based constellation partitioning to harness the power of local graph topology. The proposed CPa-WAC method reduces the training time and memory cost of knowledge graph embedding, making the learning model scalable. The results from our experiments on standard databases, such as Wordnet and Freebase, show that by achieving meaningful partitioning, any knowledge graph can be broken down into subgraphs and processed separately to learn embeddings. Furthermore, these learned embeddings can be used for knowledge graph completion, retaining similar performance compared to training a GCN on the whole KG, while speeding up the training process by upto five times. Additionally, the proposed CPa-WAC method outperforms several other state-of-the-art KG in terms of prediction accuracy.",
      "authors": [
        "S. Modak",
        "Aakarsh Malhotra",
        "Sarthak Malik",
        "Anil Surisetty",
        "Esam Abdel-Raheem"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/6205f75cb6db1503c94386441ca68c63c9cbd456",
      "pdf_link": "",
      "venue": "International Joint Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b307e96f59fde63567cd0beb30c9e36d968fad8e",
      "title": "Hyperbolic Hierarchy-Aware Knowledge Graph Embedding for Link Prediction",
      "abstract": "Knowledge graph embedding (KGE) using low-dimensional representations to predict missing information is widely applied in knowledge completion. Existing embedding methods are mostly built on Euclidean space, which are difficult to handle hierarchical structures. Hyperbolic embedding methods have shown the promise of high fidelity and concise representation for hierarchical data. However, the logical patterns in knowledge graphs are not considered well in these methods. To address this problem, we propose a novel KGE model with extended Poincaré Ball and polar coordinate system to capture hierarchical structures. We use the tangent space and exponential transformation to initialize and map the corresponding vectors to the Poincaré Ball in hyperbolic space. To solve the boundary conditions, the boundary is stretched and zoomed by expanding the modulus length in the Poincaré Ball. We optimize our model using polar coordinate and changing operators in the extended Poincaré Ball. Experiments achieve new state-of-the-art results on part of link prediction tasks, which demonstrates the effectiveness of our method.",
      "authors": [
        "Zhe Pan",
        "Peng Wang"
      ],
      "year": 2021,
      "citation_count": 32,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b307e96f59fde63567cd0beb30c9e36d968fad8e",
      "pdf_link": "",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "145fa4ea1567a6b9d981fdea0e183140d99aeb97",
      "title": "Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation",
      "abstract": "Recommender system can provide users with the required information accurately and efficiently, playing a very important role in improving users’ life experience. Although knowledge graph-based recommender system can solve the sparsity and cold start problems faced by traditional recommender system, it cannot handle the cross-domain cold start problem and cannot provide multi-domain recommendations. Therefore, this paper focuses on multi-domain item-item (I2I) recommendation based on cross-domain knowledge graph embedding by analyzing the association between items of the same domain and the interaction between items of diverse domains with the aid of knowledge graph that contains rich information. First, a cross-domain knowledge graph chiasmal embedding approach is proposed to efficiently interact all items in multiple domains. To help achieve both homo-domain embedding and hetero-domain embedding of items, a binding rule is put forward. Second, a multi-domain I2I recommendation method is presented to efficiently recommend items in multiple domains, which is a recommendation method based on link prediction of knowledge graph. Finally, the proposed methods are compared and analyzed with some benchmark methods using two datasets. The experimental results show that the proposed methods achieve better link prediction results and multi-domain recommendation results.",
      "authors": [
        "Jia Liu",
        "Wei Huang",
        "Tianrui Li",
        "Shenggong Ji",
        "Junbo Zhang"
      ],
      "year": 2023,
      "citation_count": 36,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/145fa4ea1567a6b9d981fdea0e183140d99aeb97",
      "pdf_link": "",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e9a13a97b7266ac27dcd7117a99a4fcbadc5fd9c",
      "title": "An Approach to Knowledge Base Completion by a Committee-Based Knowledge Graph Embedding",
      "abstract": "Knowledge bases such as Freebase, YAGO, DBPedia, and Nell contain a number of facts with various entities and relations. Since they store many facts, they are regarded as core resources for many natural language processing tasks. Nevertheless, they are not normally complete and have many missing facts. Such missing facts keep them from being used in diverse applications in spite of their usefulness. Therefore, it is significant to complete knowledge bases. Knowledge graph embedding is one of the promising approaches to completing a knowledge base and thus many variants of knowledge graph embedding have been proposed. It maps all entities and relations in knowledge base onto a low dimensional vector space. Then, candidate facts that are plausible in the space are determined as missing facts. However, any single knowledge graph embedding is insufficient to complete a knowledge base. As a solution to this problem, this paper defines knowledge base completion as a ranking task and proposes a committee-based knowledge graph embedding model for improving the performance of knowledge base completion. Since each knowledge graph embedding has its own idiosyncrasy, we make up a committee of various knowledge graph embeddings to reflect various perspectives. After ranking all candidate facts according to their plausibility computed by the committee, the top-k facts are chosen as missing facts. Our experimental results on two data sets show that the proposed model achieves higher performance than any single knowledge graph embedding and shows robust performances regardless of k. These results prove that the proposed model considers various perspectives in measuring the plausibility of candidate facts.",
      "authors": [
        "S. Choi",
        "Hyun-Je Song",
        "Seong-Bae Park"
      ],
      "year": 2020,
      "citation_count": 8,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e9a13a97b7266ac27dcd7117a99a4fcbadc5fd9c",
      "pdf_link": "",
      "venue": "Applied Sciences",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "title": "Knowledge Graph Embedding by Translating on Hyperplanes",
      "abstract": "\n \n We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.\n \n",
      "authors": [
        "Zhen Wang",
        "Jianwen Zhang",
        "Jianlin Feng",
        "Zheng Chen"
      ],
      "year": 2014,
      "citation_count": 3712,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2a3f862199883ceff5e3c74126f0c80770653e05",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "727183c5cff89a6f2c3b71167ae50c02ca2cacc4",
      "title": "Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding",
      "abstract": "Knowledge graph embedding aims at modeling entities and relations with low-dimensional vectors. Most previous methods require that all entities should be seen during training, which is unpractical for real-world knowledge graphs with new entities emerging on a daily basis. Recent efforts on this issue suggest training a neighborhood aggregator in conjunction with the conventional entity and relation embeddings, which may help embed new entities inductively via their existing neighbors. However, their neighborhood aggregators neglect the unordered and unequal natures of an entity’s neighbors. To this end, we summarize the desired properties that may lead to effective neighborhood aggregators. We also introduce a novel aggregator, namely, Logic Attention Network (LAN), which addresses the properties by aggregating neighbors with both rules- and network-based attention weights. By comparing with conventional aggregators on two knowledge graph completion tasks, we experimentally validate LAN’s superiority in terms of the desired properties.",
      "authors": [
        "Peifeng Wang",
        "Jialong Han",
        "Chenliang Li",
        "Rong Pan"
      ],
      "year": 2018,
      "citation_count": 166,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/727183c5cff89a6f2c3b71167ae50c02ca2cacc4",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "405a7a7464cfe175333d6f04703ac272e00a85b4",
      "title": "Knowledge Graph Embedding with Iterative Guidance from Soft Rules",
      "abstract": "\n \n Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Combining such an embedding model with logic rules has recently attracted increasing attention. Most previous attempts made a one-time injection of logic rules, ignoring the interactive nature between embedding learning and logical inference. And they focused only on hard rules, which always hold with no exception and usually require extensive manual effort to create or validate. In this paper, we propose Rule-Guided Embedding (RUGE), a novel paradigm of KG embedding with iterative guidance from soft rules. RUGE enables an embedding model to learn simultaneously from 1) labeled triples that have been directly observed in a given KG, 2) unlabeled triples whose labels are going to be predicted iteratively, and 3) soft rules with various confidence levels extracted automatically from the KG. In the learning process, RUGE iteratively queries rules to obtain soft labels for unlabeled triples, and integrates such newly labeled triples to update the embedding model. Through this iterative procedure, knowledge embodied in logic rules may be better transferred into the learned embeddings. We evaluate RUGE in link prediction on Freebase and YAGO. Experimental results show that: 1) with rule knowledge injected iteratively, RUGE achieves significant and consistent improvements over state-of-the-art baselines; and 2) despite their uncertainties, automatically extracted soft rules are highly beneficial to KG embedding, even those with moderate confidence levels. The code and data used for this paper can be obtained from https://github.com/iieir-km/RUGE.\n \n",
      "authors": [
        "Shu Guo",
        "Quan Wang",
        "Lihong Wang",
        "Bin Wang",
        "Li Guo"
      ],
      "year": 2017,
      "citation_count": 227,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/405a7a7464cfe175333d6f04703ac272e00a85b4",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4801db5c5cb24a9069f2d264252fa26986ceefa9",
      "title": "Negative Sampling in Knowledge Graph Representation Learning: A Review",
      "abstract": "Knowledge Graph Representation Learning (KGRL), or Knowledge Graph Embedding (KGE), is essential for AI applications such as knowledge construction and information retrieval. These models encode entities and relations into lower-dimensional vectors, supporting tasks like link prediction and recommendation systems. Training KGE models relies on both positive and negative samples for effective learning, but generating high-quality negative samples from existing knowledge graphs is challenging. The quality of these samples significantly impacts the model's accuracy. This comprehensive survey paper systematically reviews various negative sampling (NS) methods and their contributions to the success of KGRL. Their respective advantages and disadvantages are outlined by categorizing existing NS methods into six distinct categories. Moreover, this survey identifies open research questions that serve as potential directions for future investigations. By offering a generalization and alignment of fundamental NS concepts, this survey provides valuable insights for designing effective NS methods in the context of KGRL and serves as a motivating force for further advancements in the field.",
      "authors": [
        "Tiroshan Madushanka",
        "R. Ichise"
      ],
      "year": 2024,
      "citation_count": 12,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4801db5c5cb24a9069f2d264252fa26986ceefa9",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3f170af3566f055e758fa3bdf2bfd3a0e8787e58",
      "title": "TGformer: A Graph Transformer Framework for Knowledge Graph Embedding",
      "abstract": "Knowledge graph embedding is efficient method for reasoning over known facts and inferring missing links. Existing methods are mainly triplet-based or graph-based. Triplet-based approaches learn the embedding of missing entities by a single triple only. They ignore the fact that the knowledge graph is essentially a graph structure. Graph-based methods consider graph structure information but ignore the contextual information of nodes in the knowledge graph, making them unable to discern valuable entity (relation) information. In response to the above limitations, we propose a general graph transformer framework for knowledge graph embedding (TGformer). It is the first to use a graph transformer to build knowledge embeddings with triplet-level and graph-level structural features in the static and temporal knowledge graph. Specifically, a context-level subgraph is constructed for each predicted triplet, which models the relation between triplets with the same entity. Afterward, we design a knowledge graph transformer network (KGTN) to fully explore multi-structural features in knowledge graphs, including triplet-level and graph-level, boosting the model to understand entities (relations) in different contexts. Finally, semantic matching is adopted to select the entity with the highest score. Experimental results on several public knowledge graph datasets show that our method can achieve state-of-the-art performance in link prediction.",
      "authors": [
        "Fobo Shi",
        "Duantengchuan Li",
        "Xiaoguang Wang",
        "Bing Li",
        "Xindong Wu"
      ],
      "year": 2025,
      "citation_count": 12,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3f170af3566f055e758fa3bdf2bfd3a0e8787e58",
      "pdf_link": "",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f",
      "title": "How Does Knowledge Graph Embedding Extrapolate to Unseen Data: a Semantic Evidence View",
      "abstract": "Knowledge Graph Embedding (KGE) aims to learn representations for entities and relations. Most KGE models have gained great success, especially on extrapolation scenarios. Specifically, given an unseen triple (h, r, t), a trained model can still correctly predict t from (h, r, ?), or h from (?, r, t), such extrapolation ability is impressive. However, most existing KGE works focus on the design of delicate triple modeling function, which mainly tells us how to measure the plausibility of observed triples, but offers limited explanation of why the methods can extrapolate to unseen data, and what are the important factors to help KGE extrapolate. Therefore in this work, we attempt to study the KGE extrapolation of two problems: 1. How does KGE extrapolate to unseen data? 2. How to design the KGE model with better extrapolation ability? \nFor the problem 1, we first discuss the impact factors for extrapolation and from relation, entity and triple level respectively, propose three Semantic Evidences (SEs), which can be observed from train set and provide important semantic information for extrapolation. Then we verify the effectiveness of SEs through extensive experiments on several typical KGE methods.\nFor the problem 2, to make better use of the three levels of SE, we propose a novel GNN-based KGE model, called Semantic Evidence aware Graph Neural Network (SE-GNN). In SE-GNN, each level of SE is modeled explicitly by the corresponding neighbor pattern, and merged sufficiently by the multi-layer aggregation, which contributes to obtaining more extrapolative knowledge representation. \nFinally, through extensive experiments on FB15k-237 and WN18RR datasets, we show that SE-GNN achieves state-of-the-art performance on Knowledge Graph Completion task and performs a better extrapolation ability. Our code is available at https://github.com/renli1024/SE-GNN.",
      "authors": [
        "Ren Li",
        "Yanan Cao",
        "Qiannan Zhu",
        "Guanqun Bi",
        "Fang Fang",
        "Yi Liu",
        "Qian Li"
      ],
      "year": 2021,
      "citation_count": 80,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "8b717c4dfb309638307fcc7d2c798b1c20927a3e",
      "title": "Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding",
      "abstract": "Knowledge graphs (KGs) consisting of a large number of triples have become widespread recently, and many knowledge graph embedding (KGE) methods are proposed to embed entities and relations of a KG into continuous vector spaces. Such embedding methods simplify the operations of conducting various in-KG tasks (e.g., link prediction) and out-of-KG tasks (e.g., question answering). They can be viewed as general solutions for representing KGs. However, existing KGE methods are not applicable to inductive settings, where a model trained on source KGs will be tested on target KGs with entities unseen during model training. Existing works focusing on KGs in inductive settings can only solve the inductive relation prediction task. They can not handle other out-of-KG tasks as general as KGE methods since they don't produce embeddings for entities. In this paper, to achieve inductive knowledge graph embedding, we propose a model MorsE, which does not learn embeddings for entities but learns transferable meta-knowledge that can be used to produce entity embeddings. Such meta-knowledge is modeled by entity-independent modules and learned by meta-learning. Experimental results show that our model significantly outperforms corresponding baselines for in-KG and out-of-KG tasks in inductive settings.",
      "authors": [
        "Mingyang Chen",
        "Wen Zhang",
        "Yushan Zhu",
        "Hongting Zhou",
        "Zonggang Yuan",
        "Changliang Xu",
        "Hua-zeng Chen"
      ],
      "year": 2021,
      "citation_count": 67,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/8b717c4dfb309638307fcc7d2c798b1c20927a3e",
      "pdf_link": "",
      "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a6a735f8e218f772e5b9dac411fa4abea87fdb9c",
      "title": "Recurrent knowledge graph embedding for effective recommendation",
      "abstract": "Knowledge graphs (KGs) have proven to be effective to improve recommendation. Existing methods mainly rely on hand-engineered features from KGs (e.g., meta paths), which requires domain knowledge. This paper presents RKGE, a KG embedding approach that automatically learns semantic representations of both entities and paths between entities for characterizing user preferences towards items. Specifically, RKGE employs a novel recurrent network architecture that contains a batch of recurrent networks to model the semantics of paths linking a same entity pair, which are seamlessly fused into recommendation. It further employs a pooling operator to discriminate the saliency of different paths in characterizing user preferences towards items. Extensive validation on real-world datasets shows the superiority of RKGE against state-of-the-art methods. Furthermore, we show that RKGE provides meaningful explanations for recommendation results.",
      "authors": [
        "Zhu Sun",
        "Jie Yang",
        "Jie Zhang",
        "A. Bozzon",
        "Long-Kai Huang",
        "Chi Xu"
      ],
      "year": 2018,
      "citation_count": 332,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a6a735f8e218f772e5b9dac411fa4abea87fdb9c",
      "pdf_link": "",
      "venue": "ACM Conference on Recommender Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "eb14b24b329a6cc80747644616e15491ef49596f",
      "title": "Mixed Geometry Message and Trainable Convolutional Attention Network for Knowledge Graph Completion",
      "abstract": "Knowledge graph completion (KGC) aims to study the embedding representation to solve the incompleteness of knowledge graphs (KGs). Recently, graph convolutional networks (GCNs) and graph attention networks (GATs) have been widely used in KGC tasks by capturing neighbor information of entities. However, Both GCNs and GATs based KGC models have their limitations, and the best method is to analyze the neighbors of each entity (pre-validating), while this process is prohibitively expensive. Furthermore, the representation quality of the embeddings can affect the aggregation of neighbor information (message passing). To address the above limitations, we propose a novel knowledge graph completion model with mixed geometry message and trainable convolutional attention network named MGTCA. Concretely, the mixed geometry message function generates rich neighbor message by integrating spatially information in the hyperbolic space, hypersphere space and Euclidean space jointly. To complete the autonomous switching of graph neural networks (GNNs) and eliminate the necessity of pre-validating the local structure of KGs, a trainable convolutional attention network is proposed by comprising three types of GNNs in one trainable formulation. Furthermore, a mixed geometry scoring function is proposed, which calculates scores of triples by novel prediction function and similarity function based on different geometric spaces. Extensive experiments on three standard datasets confirm the effectiveness of our innovations, and the performance of MGTCA is significantly improved compared to the state-of-the-art approaches.",
      "authors": [
        "Bin Shang",
        "Yinliang Zhao",
        "Jun Liu",
        "Di Wang"
      ],
      "year": 2024,
      "citation_count": 9,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/eb14b24b329a6cc80747644616e15491ef49596f",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "23efe9b99b5f0e79d7dbd4e3bfcf1c2d8b23c1ff",
      "title": "Marie and BERT—A Knowledge Graph Embedding Based Question Answering System for Chemistry",
      "abstract": "This paper presents a novel knowledge graph question answering (KGQA) system for chemistry, which is implemented on hybrid knowledge graph embeddings, aiming to provide fact-oriented information retrieval for chemistry-related research and industrial applications. Unlike other existing designs, the system operates on multiple embedding spaces, which use various embedding methods and queries the embedding spaces in parallel. With the answers returned from multiple embedding spaces, the system leverages a score alignment model to adjust the answer scores and rerank the answers. Further, the system implements an algorithm to derive implicit multihop relations to handle the complexities of deep ontologies and improve multihop question answering. The system also implements a BERT-based bidirectional entity-linking model to enhance the robustness and accuracy of the entity-linking module. The system uses a joint numerical embedding model to efficiently handle numerical filtering questions. Further, it can invoke semantic agents to perform dynamic calculations autonomously. Finally, the KGQA system handles numerous chemical reaction mechanisms using semantic parsing supported by a Linked Data Fragment server. This paper evaluates the accuracy of each module within the KGQA system with a chemistry question data set.",
      "authors": [
        "Xiaochi Zhou",
        "Shaocong Zhang",
        "Mehal Agarwal",
        "J. Akroyd",
        "S. Mosbach",
        "Markus Kraft"
      ],
      "year": 2023,
      "citation_count": 16,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/23efe9b99b5f0e79d7dbd4e3bfcf1c2d8b23c1ff",
      "pdf_link": "",
      "venue": "ACS Omega",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e4e7bc893b6fb4ff8ebbff899be65d96d50ccd1d",
      "title": "A Translation-Based Knowledge Graph Embedding Preserving Logical Property of Relations",
      "abstract": "This paper proposes a novel translation-based knowledge graph embedding that preserves the logical properties of relations such as transitivity and symmetricity. The embedding space generated by existing translation-based embeddings do not represent transitive and symmetric relations precisely, because they ignore the role of entities in triples. Thus, we introduce a role-specific projection which maps an entity to distinct vectors according to its role in a triple. That is, a head entity is projected onto an embedding space by a head projection operator, and a tail entity is projected by a tail projection operator. This idea is applied to TransE, TransR, and TransD to produce lppTransE, lppTransR, and lppTransD, respectively. According to the experimental results on link prediction and triple classification, the proposed logical property preserving embeddings show the state-of-the-art performance at both tasks. These results prove that it is critical to preserve logical properties of relations while embedding knowledge graphs, and the proposed method does it effectively.",
      "authors": [
        "Hee-Geun Yoon",
        "Hyun-Je Song",
        "Seong-Bae Park",
        "Se-Young Park"
      ],
      "year": 2016,
      "citation_count": 49,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e4e7bc893b6fb4ff8ebbff899be65d96d50ccd1d",
      "pdf_link": "",
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "52b167a90a10cde25309e40d7f6e6b5e14ec3261",
      "title": "A survey: knowledge graph entity alignment research based on graph embedding",
      "abstract": "Entity alignment (EA) aims to automatically match entities in different knowledge graphs, which is beneficial to the development of knowledge-driven applications. Representation learning has powerful feature capture capability and it is widely used in the field of natural language processing. Compared with traditional EA methods, EA methods based on representation learning have better performance and efficiency. Hence, we summarize and analyze the representative EA approaches based on representation learning in this paper. We present the problem description and data preprocessing for EA and other related fundamental knowledge. We propose a new EA framework for the latest models, which includes information aggregation module, entity alignment module, and post-alignment module. Based on these three modules, the various technologies are described in detail. In the experimental part, we first explore the effect of EA direction on model performance. Then, we classify the models into different categories in terms of alignment inference strategy, noise filtering strategy, and whether additional information is utilized. To ensure fairness, we perform the comparative analysis of the performance of the models within the categories separately on different datasets. We investigate both unimodal and multimodal EA. Finally, we present future research perspectives based on the shortcomings of existing EA methods.",
      "authors": [
        "Beibei Zhu",
        "Ruolin Wang",
        "Junyi Wang",
        "Fei Shao",
        "Kerun Wang"
      ],
      "year": 2024,
      "citation_count": 9,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/52b167a90a10cde25309e40d7f6e6b5e14ec3261",
      "pdf_link": "",
      "venue": "Artificial Intelligence Review",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "3ac716ac5d47d4420010678fda766ebb5b882ba9",
      "title": "Communication-Efficient Federated Knowledge Graph Embedding with Entity-Wise Top-K Sparsification",
      "abstract": "Federated Knowledge Graphs Embedding learning (FKGE) encounters challenges in communication efficiency stemming from the considerable size of parameters and extensive communication rounds. However, existing FKGE methods only focus on reducing communication rounds by conducting multiple rounds of local training in each communication round, and ignore reducing the size of parameters transmitted within each communication round. To tackle the problem, we first find that universal reduction in embedding precision across all entities during compression can significantly impede convergence speed, underscoring the importance of maintaining embedding precision. We then propose bidirectional communication-efficient FedS based on Entity-Wise Top-K Sparsification strategy. During upload, clients dynamically identify and upload only the Top-K entity embeddings with the greater changes to the server. During download, the server first performs personalized embedding aggregation for each client. It then identifies and transmits the Top-K aggregated embeddings to each client. Besides, an Intermittent Synchronization Mechanism is used by FedS to mitigate negative effect of embedding inconsistency among shared entities of clients caused by heterogeneity of Federated Knowledge Graph. Extensive experiments across three datasets showcase that FedS significantly enhances communication efficiency with negligible (even no) performance degradation.",
      "authors": [
        "Xiaoxiong Zhang",
        "Zhiwei Zeng",
        "Xin Zhou",
        "D. Niyato",
        "Zhiqi Shen"
      ],
      "year": 2024,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/3ac716ac5d47d4420010678fda766ebb5b882ba9",
      "pdf_link": "",
      "venue": "Knowledge-Based Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "8c93f3cecf79bd9f8d021f589d095305e281dd2f",
      "title": "Knowledge Graph Embedding for Link Prediction",
      "abstract": "Knowledge Graphs (KGs) have found many applications in industrial and in academic settings, which in turn, have motivated considerable research efforts towards large-scale information extraction from a variety of sources. Despite such efforts, it is well known that even the largest KGs suffer from incompleteness; Link Prediction (LP) techniques address this issue by identifying missing facts among entities already in the KG. Among the recent LP techniques, those based on KG embeddings have achieved very promising performance in some benchmarks. Despite the fast-growing literature on the subject, insufficient attention has been paid to the effect of the design choices in those methods. Moreover, the standard practice in this area is to report accuracy by aggregating over a large number of test facts in which some entities are vastly more represented than others; this allows LP methods to exhibit good results by just attending to structural properties that include such entities, while ignoring the remaining majority of the KG. This analysis provides a comprehensive comparison of embedding-based LP methods, extending the dimensions of analysis beyond what is commonly available in the literature. We experimentally compare the effectiveness and efficiency of 18 state-of-the-art methods, consider a rule-based baseline, and report detailed analysis over the most popular benchmarks in the literature.",
      "authors": [
        "Andrea Rossi",
        "D. Firmani",
        "Antonio Matinata",
        "P. Merialdo",
        "Denilson Barbosa"
      ],
      "year": 2020,
      "citation_count": 365,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/8c93f3cecf79bd9f8d021f589d095305e281dd2f",
      "pdf_link": "",
      "venue": "ACM Transactions on Knowledge Discovery from Data",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ecc04e9285f016090697a1a8f9e96ce01e94e742",
      "title": "Semi-Supervised Entity Alignment via Knowledge Graph Embedding with Awareness of Degree Difference",
      "abstract": "Entity alignment associates entities in different knowledge graphs if they are semantically same, and has been successfully used in the knowledge graph construction and connection. Most of the recent solutions for entity alignment are based on knowledge graph embedding, which maps knowledge entities in a low-dimension space where entities are connected with the guidance of prior aligned entity pairs. The study in this paper focuses on two important issues that limit the accuracy of current entity alignment solutions: 1) labeled data of priorly aligned entity pairs are difficult and expensive to acquire, whereas abundant of unlabeled data are not used; and 2) knowledge graph embedding is affected by entity's degree difference, which brings challenges to align high frequent and low frequent entities. We propose a semi-supervised entity alignment method (SEA) to leverage both labeled entities and the abundant unlabeled entity information for the alignment. Furthermore, we improve the knowledge graph embedding with awareness of the degree difference by performing the adversarial training. To evaluate our proposed model, we conduct extensive experiments on real-world datasets. The experimental results show that our model consistently outperforms the state-of-the-art methods with significant improvement on alignment accuracy.",
      "authors": [
        "Shichao Pei",
        "Lu Yu",
        "R. Hoehndorf",
        "Xiangliang Zhang"
      ],
      "year": 2019,
      "citation_count": 171,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ecc04e9285f016090697a1a8f9e96ce01e94e742",
      "pdf_link": "",
      "venue": "The Web Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d899e434a7f2eecf33a90053df84cf32842fbca9",
      "title": "Bootstrapping Entity Alignment with Knowledge Graph Embedding",
      "abstract": "Embedding-based entity alignment represents different knowledge graphs (KGs) as low-dimensional embeddings and finds entity alignment by measuring the similarities between entity embeddings. Existing approaches have achieved promising results, however, they are still challenged by the lack of enough prior alignment as labeled training data. In this paper, we propose a bootstrapping approach to embedding-based entity alignment. It iteratively labels likely entity alignment as training data for learning alignment-oriented KG embeddings. Furthermore, it employs an alignment editing method to reduce error accumulation during iterations. Our experiments on real-world datasets showed that the proposed approach significantly outperformed the state-of-the-art embedding-based ones for entity alignment. The proposed alignment-oriented KG embedding, bootstrapping process and alignment editing method all contributed to the performance improvement.",
      "authors": [
        "Zequn Sun",
        "Wei Hu",
        "Qingheng Zhang",
        "Yuzhong Qu"
      ],
      "year": 2018,
      "citation_count": 527,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d899e434a7f2eecf33a90053df84cf32842fbca9",
      "pdf_link": "",
      "venue": "International Joint Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "63836e669416668744c3676a831060e8de3f58a1",
      "title": "HousE: Knowledge Graph Embedding with Householder Parameterization",
      "abstract": "The effectiveness of knowledge graph embedding (KGE) largely depends on the ability to model intrinsic relation patterns and mapping properties. However, existing approaches can only capture some of them with insufficient modeling capacity. In this work, we propose a more powerful KGE framework named HousE, which involves a novel parameterization based on two kinds of Householder transformations: (1) Householder rotations to achieve superior capacity of modeling relation patterns; (2) Householder projections to handle sophisticated relation mapping properties. Theoretically, HousE is capable of modeling crucial relation patterns and mapping properties simultaneously. Besides, HousE is a generalization of existing rotation-based models while extending the rotations to high-dimensional spaces. Empirically, HousE achieves new state-of-the-art performance on five benchmark datasets. Our code is available at https://github.com/anrep/HousE.",
      "authors": [
        "Rui Li",
        "Jianan Zhao",
        "Chaozhuo Li",
        "Di He",
        "Yiqi Wang",
        "Yuming Liu",
        "Hao Sun",
        "Senzhang Wang",
        "Weiwei Deng",
        "Yanming Shen",
        "Xing Xie",
        "Qi Zhang"
      ],
      "year": 2022,
      "citation_count": 53,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/63836e669416668744c3676a831060e8de3f58a1",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "191815e4109ee392b9120b61642c0e859fb662a1",
      "title": "RulE: Knowledge Graph Reasoning with Rule Embedding",
      "abstract": "Knowledge graph (KG) reasoning is an important problem for knowledge graphs. In this paper, we propose a novel and principled framework called \\textbf{RulE} (stands for {Rul}e {E}mbedding) to effectively leverage logical rules to enhance KG reasoning. Unlike knowledge graph embedding (KGE) methods, RulE learns rule embeddings from existing triplets and first-order {rules} by jointly representing \\textbf{entities}, \\textbf{relations} and \\textbf{logical rules} in a unified embedding space. Based on the learned rule embeddings, a confidence score can be calculated for each rule, reflecting its consistency with the observed triplets. This allows us to perform logical rule inference in a soft way, thus alleviating the brittleness of logic. On the other hand, RulE injects prior logical rule information into the embedding space, enriching and regularizing the entity/relation embeddings. This makes KGE alone perform better too. RulE is conceptually simple and empirically effective. We conduct extensive experiments to verify each component of RulE. Results on multiple benchmarks reveal that our model outperforms the majority of existing embedding-based and rule-based approaches.",
      "authors": [
        "Xiaojuan Tang",
        "Song-Chun Zhu",
        "Yitao Liang",
        "Muhan Zhang"
      ],
      "year": 2022,
      "citation_count": 11,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/191815e4109ee392b9120b61642c0e859fb662a1",
      "pdf_link": "",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "552bfaca30af29647c083993fbe406867fc70d4c",
      "title": "TeRo: A Time-aware Knowledge Graph Embedding via Temporal Rotation",
      "abstract": "In the last few years, there has been a surge of interest in learning representations of entities and relations in knowledge graph (KG). However, the recent availability of temporal knowledge graphs (TKGs) that contain time information for each fact created the need for reasoning over time in such TKGs. In this regard, we present a new approach of TKG embedding, TeRo, which defines the temporal evolution of entity embedding as a rotation from the initial time to the current time in the complex vector space. Specially, for facts involving time intervals, each relation is represented as a pair of dual complex embeddings to handle the beginning and the end of the relation, respectively. We show our proposed model overcomes the limitations of the existing KG embedding models and TKG embedding models and has the ability of learning and inferring various relation patterns over time. Experimental results on three different TKGs show that TeRo significantly outperforms existing state-of-the-art models for link prediction. In addition, we analyze the effect of time granularity on link prediction over TKGs, which as far as we know has not been investigated in previous literature.",
      "authors": [
        "Chengjin Xu",
        "M. Nayyeri",
        "Fouad Alkhoury",
        "H. S. Yazdi",
        "Jens Lehmann"
      ],
      "year": 2020,
      "citation_count": 135,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/552bfaca30af29647c083993fbe406867fc70d4c",
      "pdf_link": "",
      "venue": "International Conference on Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2bd20cfec4ad3df0fd9cd87cef3eefe6f3847b83",
      "title": "LibKGE - A knowledge graph embedding library for reproducible research",
      "abstract": "LibKGE ( https://github.com/uma-pi1/kge ) is an open-source PyTorch-based library for training, hyperparameter optimization, and evaluation of knowledge graph embedding models for link prediction. The key goals of LibKGE are to enable reproducible research, to provide a framework for comprehensive experimental studies, and to facilitate analyzing the contributions of individual components of training methods, model architectures, and evaluation methods. LibKGE is highly configurable and every experiment can be fully reproduced with a single configuration file. Individual components are decoupled to the extent possible so that they can be mixed and matched with each other. Implementations in LibKGE aim to be as efficient as possible without leaving the scope of Python/Numpy/PyTorch. A comprehensive logging mechanism and tooling facilitates in-depth analysis. LibKGE provides implementations of common knowledge graph embedding models and training methods, and new ones can be easily added. A comparative study (Ruffinelli et al., 2020) showed that LibKGE reaches competitive to state-of-the-art performance for many models with a modest amount of automatic hyperparameter tuning.",
      "authors": [
        "Samuel Broscheit",
        "Daniel Ruffinelli",
        "Adrian Kochsiek",
        "Patrick Betz",
        "Rainer Gemulla"
      ],
      "year": 2020,
      "citation_count": 86,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2bd20cfec4ad3df0fd9cd87cef3eefe6f3847b83",
      "pdf_link": "",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ce7291c5cd919a97ced6369ca697db9849848688",
      "title": "Learning Dynamic Knowledge Graph Embedding in Evolving Service Ecosystems via Meta-Learning",
      "abstract": "In the context of dynamic service ecosystems, the inability of conventional knowledge graph embedding (KGE) methods to efficiently update incremental knowledge poses a significant challenge for the effectiveness of intelligent web applications. To address the continuous updating challenges of service knowledge, this paper introduces MetaHG, a meta-learning strategy for KGE. Unlike existing meta-learning KGE studies that focus solely on local entity information, MetaHG incorporates both local and potential global structural information from current snapshot’s seen knowledge graphs (KGs) to mitigate issues such as spatial deformation and enhance the representation of unseen entities. Our approach initializes entity embeddings using ‘in’ and ‘out’ relationship matrices and refines them through a hybrid graph neural network (GNN) framework, which includes a GNN layer for local information and a hypergraph neural network (HGNN) layer for potential global information. The meta-learning strategy embedded in MetaHG effectively transfers meta-knowledge for the accurate representation of emerging entities. Extensive experiments are conducted on a self-collected clothing industry service dataset and two publicly available open-source KG datasets. By comparing with several baselines, experiment results demonstrate the superior performance of MetaHG in generating high-quality embeddings for emerging entities and dynamically updating service knowledge.",
      "authors": [
        "Hongliang Sun",
        "Jinlan Liu",
        "Can Wang",
        "Dianbo Sui",
        "Zhiying Tu",
        "Xiaofei Xu"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ce7291c5cd919a97ced6369ca697db9849848688",
      "pdf_link": "",
      "venue": "2024 IEEE International Conference on Web Services (ICWS)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
      "title": "TransG : A Generative Model for Knowledge Graph Embedding",
      "abstract": "Recently, knowledge graph embedding, which projects symbolic entities and relations into continuous vector space, has become a new, hot topic in artificial intelligence. This paper addresses a new issue of multiple relation semantics that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples, and proposes a novel Gaussian mixture model for embedding, TransG. The new model can discover latent semantics for a relation and leverage a mixture of relation component vectors for embedding a fact triple. To the best of our knowledge, this is the first generative model for knowledge graph embedding, which is able to deal with multiple relation semantics. Extensive experiments show that the proposed model achieves substantial improvements against the state-of-the-art baselines.",
      "authors": [
        "Han Xiao",
        "Minlie Huang",
        "Xiaoyan Zhu"
      ],
      "year": 2015,
      "citation_count": 288,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
      "pdf_link": "",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b2d2ad9a458bdcb0523d22be659eb013ca2d3c67",
      "title": "TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation",
      "abstract": "Knowledge graph embedding (KGE) aims to learn continuous vectors of relations and entities in knowledge graph. Recently, transition-based KGE methods have achieved promising performance, where the single relation vector learns to translate head entity to tail entity. However, this scoring pattern is not suitable for complex scenarios where the same entity pair has different relations. Previous models usually focus on the improvement of entity representation for 1-to-N, N-to-1 and N-to-N relations, but ignore the single relation vector. In this paper, we propose a novel transition-based method, TranS, for knowledge graph embedding. The single relation vector in traditional scoring patterns is replaced with synthetic relation representation, which can solve these issues effectively and efficiently. Experiments on a large knowledge graph dataset, ogbl-wikikg2, show that our model achieves state-of-the-art results.",
      "authors": [
        "Xuanyu Zhang",
        "Qing Yang",
        "Dongliang Xu"
      ],
      "year": 2022,
      "citation_count": 22,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b2d2ad9a458bdcb0523d22be659eb013ca2d3c67",
      "pdf_link": "",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0367603c0197ab48eeba29aa6af391584a5077c0",
      "title": "Towards Robust Knowledge Graph Embedding via Multi-Task Reinforcement Learning",
      "abstract": "Nowadays, Knowledge graphs (KGs) have been playing a pivotal role in AI-related applications. Despite the large sizes, existing KGs are far from complete and comprehensive. In order to continuously enrich KGs, automatic knowledge construction and update mechanisms are usually utilized, which inevitably bring in plenty of noise. However, most existing knowledge graph embedding (KGE) methods assume that all the triple facts in KGs are correct, and project both entities and relations into a low-dimensional space without considering noise and knowledge conflicts. This will lead to low-quality and unreliable representations of KGs. To this end, in this paper, we propose a general multi-task reinforcement learning framework, which can greatly alleviate the noisy data problem. In our framework, we exploit reinforcement learning for choosing high-quality knowledge triples while filtering out the noisy ones. Also, in order to take full advantage of the correlations among semantically similar relations, the triple selection processes of similar relations are trained in a collective way with multi-task learning. Moreover, we extend popular KGE models TransE, DistMult, ConvE and RotatE with the proposed framework. Finally, the experimental validation shows that our approach is able to enhance existing KGE models and can provide more robust representations of KGs in noisy scenarios.",
      "authors": [
        "Zhao Zhang",
        "Fuzhen Zhuang",
        "Hengshu Zhu",
        "Chao Li",
        "Hui Xiong",
        "Qing He",
        "Yongjun Xu"
      ],
      "year": 2021,
      "citation_count": 12,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0367603c0197ab48eeba29aa6af391584a5077c0",
      "pdf_link": "",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "780bc77fac1aaf460ba191daa218f3c111119092",
      "title": "IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion",
      "abstract": "Temporal Knowledge Graphs (TKGs) incorporate a temporal dimension, allowing for a precise capture of the evolution of knowledge and reflecting the dynamic nature of the real world. Typically, TKGs contain complex geometric structures, with various geometric structures interwoven. However, existing Temporal Knowledge Graph Completion (TKGC) methods either model TKGs in a single space or neglect the heterogeneity of different curvature spaces, thus constraining their capacity to capture these intricate geometric structures. In this paper, we propose a novel Integrating Multi-curvature shared and specific Embedding (IME) model for TKGC tasks. Concretely, IME models TKGs into multi-curvature spaces, including hyperspherical, hyperbolic, and Euclidean spaces. Subsequently, IME incorporates two key properties, namely space-shared property and space-specific property. The space-shared property facilitates the learning of commonalities across different curvature spaces and alleviates the spatial gap caused by the heterogeneous nature of multi-curvature spaces, while the space-specific property captures characteristic features. Meanwhile, IME proposes an Adjustable Multi-curvature Pooling (AMP) approach to effectively retain important information. Furthermore, IME innovatively designs similarity, difference, and structure loss functions to attain the stated objective. Experimental results clearly demonstrate the superior performance of IME over existing state-of-the-art TKGC models.",
      "authors": [
        "Jiapu Wang",
        "Zheng Cui",
        "Boyue Wang",
        "Shirui Pan",
        "Junbin Gao",
        "Baocai Yin",
        "Wen Gao"
      ],
      "year": 2024,
      "citation_count": 12,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/780bc77fac1aaf460ba191daa218f3c111119092",
      "pdf_link": "",
      "venue": "The Web Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "77dc07c92c37586f94a6f5ac3de103b218931578",
      "title": "TransGate: Knowledge Graph Embedding with Shared Gate Structure",
      "abstract": "Embedding knowledge graphs (KGs) into continuous vector space is an essential problem in knowledge extraction. Current models continue to improve embedding by focusing on discriminating relation-specific information from entities with increasingly complex feature engineering. We noted that they ignored the inherent relevance between relations and tried to learn unique discriminate parameter set for each relation. Thus, these models potentially suffer from high time complexity and large parameters, preventing them from efficiently applying on real-world KGs. In this paper, we follow the thought of parameter sharing to simultaneously learn more expressive features, reduce parameters and avoid complex feature engineering. Based on gate structure from LSTM, we propose a novel model TransGate and develop shared discriminate mechanism, resulting in almost same space complexity as indiscriminate models. Furthermore, to develop a more effective and scalable model, we reconstruct the gate with weight vectors making our method has comparative time complexity against indiscriminate model. We conduct extensive experiments on link prediction and triplets classification. Experiments show that TransGate not only outperforms state-of-art baselines, but also reduces parameters greatly. For example, TransGate outperforms ConvE and RGCN with 6x and 17x fewer parameters, respectively. These results indicate that parameter sharing is a superior way to further optimize embedding and TransGate finds a better trade-off between complexity and expressivity.",
      "authors": [
        "Jun Yuan",
        "Neng Gao",
        "Ji Xiang"
      ],
      "year": 2019,
      "citation_count": 24,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/77dc07c92c37586f94a6f5ac3de103b218931578",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c2c6edc5750a438bddd1217481832d38df6336de",
      "title": "Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding",
      "abstract": "Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First, we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes.",
      "authors": [
        "Yun Tang",
        "Jing Huang",
        "Guangtao Wang",
        "Xiaodong He",
        "Bowen Zhou"
      ],
      "year": 2019,
      "citation_count": 100,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c2c6edc5750a438bddd1217481832d38df6336de",
      "pdf_link": "",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "29052ddd048acb1afa2c42613068b63bb7428a34",
      "title": "Position-Aware Relational Transformer for Knowledge Graph Embedding",
      "abstract": "Although Transformer has achieved success in language and vision tasks, its capacity for knowledge graph (KG) embedding has not been fully exploited. Using the self-attention (SA) mechanism in Transformer to model the subject-relation-object triples in KGs suffers from training inconsistency as SA is invariant to the order of input tokens. As a result, it is unable to distinguish a (real) relation triple from its shuffled (fake) variants (e.g., object-relation-subject) and, thus, fails to capture the correct semantics. To cope with this issue, we propose a novel Transformer architecture, namely, Knowformer, for KG embedding. It incorporates relational compositions in entity representations to explicitly inject semantics and capture the role of an entity based on its position (subject or object) in a relation triple. The relational composition for a subject (or object) entity of a relation triple refers to an operator on the relation and the object (or subject). We borrow ideas from the typical translational and semantic-matching embedding techniques to design relational compositions. We carefully design a residual block to integrate relational compositions into SA and efficiently propagate the composed relational semantics layer by layer. We formally prove that the SA with relational compositions is able to distinguish the entity roles in different positions and correctly capture relational semantics. Extensive experiments and analyses on six benchmark datasets show that Knowformer achieves state-of-the-art performance on both link prediction and entity alignment.",
      "authors": [
        "Guang-pu Li",
        "Zequn Sun",
        "Wei Hu",
        "Gong Cheng",
        "Yuzhong Qu"
      ],
      "year": 2023,
      "citation_count": 19,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/29052ddd048acb1afa2c42613068b63bb7428a34",
      "pdf_link": "",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "06315f8b2633a54b087c6094cdb281f01dd06482",
      "title": "TransET: Knowledge Graph Embedding with Entity Types",
      "abstract": "Knowledge graph embedding aims to embed entities and relations into low-dimensional vector spaces. Most existing methods only focus on triple facts in knowledge graphs. In addition, models based on translation or distance measurement cannot fully represent complex relations. As well-constructed prior knowledge, entity types can be employed to learn the representations of entities and relations. In this paper, we propose a novel knowledge graph embedding model named TransET, which takes advantage of entity types to learn more semantic features. More specifically, circle convolution based on the embeddings of entity and entity types is utilized to map head entity and tail entity to type-specific representations, then translation-based score function is used to learn the presentation triples. We evaluated our model on real-world datasets with two benchmark tasks of link prediction and triple classification. Experimental results demonstrate that it outperforms state-of-the-art models in most cases.",
      "authors": [
        "Peng Wang",
        "Jing Zhou",
        "Yuzhang Liu",
        "Xing-Chun Zhou"
      ],
      "year": 2021,
      "citation_count": 16,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/06315f8b2633a54b087c6094cdb281f01dd06482",
      "pdf_link": "",
      "venue": "Electronics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5515fd5d14ac7b19806294119560a8c74f7fa4b2",
      "title": "Parallel Training of Knowledge Graph Embedding Models: A Comparison of Techniques",
      "abstract": "Knowledge graph embedding (KGE) models represent the entities and relations of a knowledge graph (KG) using dense continuous representations called embeddings. KGE methods have recently gained traction for tasks such as knowledge graph completion and reasoning as well as to provide suitable entity representations for downstream learning tasks. While a large part of the available literature focuses on small KGs, a number of frameworks that are able to train KGE models for large-scale KGs by parallelization across multiple GPUs or machines have recently been proposed. So far, the benefits and drawbacks of the various parallelization techniques have not been studied comprehensively. In this paper, we report on an experimental study in which we presented, re-implemented in a common computational framework, investigated, and improved the available techniques. We found that the evaluation methodologies used in prior work are often not comparable and can be misleading, and that most of currently implemented training methods tend to have a negative impact on embedding quality. We propose a simple but effective variation of the stratification technique used by PyTorch BigGraph for mitigation. Moreover, basic random partitioning can be an effective or even the best-performing choice when combined with suitable sampling techniques. Ultimately, we found that efficient and effective parallel training of large-scale KGE models is indeed achievable but requires a careful choice of techniques.",
      "authors": [
        "Adrian Kochsiek"
      ],
      "year": 2021,
      "citation_count": 30,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5515fd5d14ac7b19806294119560a8c74f7fa4b2",
      "pdf_link": "",
      "venue": "Proceedings of the VLDB Endowment",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4085a5cf49c193fe3d3ff19ff2d696fe20a5a596",
      "title": "Knowledge Graph Embedding with 3D Compound Geometric Transformations",
      "abstract": "The cascade of 2D geometric transformations were exploited to model relations between entities in a knowledge graph (KG), leading to an effective KG embedding (KGE) model, CompoundE. Furthermore, the rotation in the 3D space was proposed as a new KGE model, Rotate3D, by leveraging its non-commutative property. Inspired by CompoundE and Rotate3D, we leverage 3D compound geometric transformations, including translation, rotation, scaling, reflection, and shear and propose a family of KGE models, named CompoundE3D, in this work. CompoundE3D allows multiple design variants to match rich underlying characteristics of a KG. Since each variant has its own advantages on a subset of relations, an ensemble of multiple variants can yield superior performance. The effectiveness and flexibility of CompoundE3D are experimentally verified on four popular link prediction datasets.",
      "authors": [
        "Xiou Ge",
        "Yun Cheng Wang",
        "Bin Wang",
        "C. J. Kuo"
      ],
      "year": 2023,
      "citation_count": 10,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4085a5cf49c193fe3d3ff19ff2d696fe20a5a596",
      "pdf_link": "",
      "venue": "APSIPA Transactions on Signal and Information Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "95c3d25b40f963eb248136555bd9b9e35817cc09",
      "title": "LineaRE: Simple but Powerful Knowledge Graph Embedding for Link Prediction",
      "abstract": "The task of link prediction for knowledge graphs is to predict missing relationships between entities. Knowledge graph embedding, which aims to represent entities and relations of a knowledge graph as low dimensional vectors in a continuous vector space, has achieved promising predictive performance. If an embedding model can cover different types of connectivity patterns and mapping properties of relations as many as possible, it will potentially bring more benefits for link prediction tasks. In this paper, we propose a novel embedding model, namely LineaRE, which is capable of modeling four connectivity patterns (i.e., symmetry, antisymmetry, inversion, and composition) and four mapping properties (i.e., one-to-one, one-to-many, many-to-one, and many-to-many) of relations. Specifically, we regard knowledge graph embedding as a simple linear regression task, where a relation is modeled as a linear function of two low-dimensional vector-presented entities with two weight vectors and a bias vector. Since the vectors are defined in a real number space and the scoring function of the model is linear, our model is simple and scalable to large knowledge graphs. Experimental results on multiple widely used real-world datasets show that the proposed LineaRE model significantly outperforms existing state-of-the-art models for link prediction tasks.",
      "authors": [
        "Yanhui Peng",
        "Jing Zhang"
      ],
      "year": 2020,
      "citation_count": 28,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/95c3d25b40f963eb248136555bd9b9e35817cc09",
      "pdf_link": "",
      "venue": "Industrial Conference on Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2e925a02db26a60ee1cc022f3923e09f3fae7b39",
      "title": "CoKE: Contextualized Knowledge Graph Embedding",
      "abstract": "Knowledge graph embedding, which projects symbolic entities and relations into continuous vector spaces, is gaining increasing attention. Previous methods allow a single static embedding for each entity or relation, ignoring their intrinsic contextual nature, i.e., entities and relations may appear in different graph contexts, and accordingly, exhibit different properties. This work presents Contextualized Knowledge Graph Embedding (CoKE), a novel paradigm that takes into account such contextual nature, and learns dynamic, flexible, and fully contextualized entity and relation embeddings. Two types of graph contexts are studied: edges and paths, both formulated as sequences of entities and relations. CoKE takes a sequence as input and uses a Transformer encoder to obtain contextualized representations. These representations are hence naturally adaptive to the input, capturing contextual meanings of entities and relations therein. Evaluation on a wide variety of public benchmarks verifies the superiority of CoKE in link prediction and path query answering. It performs consistently better than, or at least equally well as current state-of-the-art in almost every case, in particular offering an absolute improvement of 19.7% in H@10 on path query answering. Our code is available at \\url{this https URL}.",
      "authors": [
        "Quan Wang",
        "Pingping Huang",
        "Haifeng Wang",
        "Songtai Dai",
        "Wenbin Jiang",
        "Jing Liu",
        "Yajuan Lyu",
        "Yong Zhu",
        "Hua Wu"
      ],
      "year": 2019,
      "citation_count": 89,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2e925a02db26a60ee1cc022f3923e09f3fae7b39",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "86ac98157da100a529ca65fe6e1da064b0a651e8",
      "title": "Knowledge Graph Embedding with Hierarchical Relation Structure",
      "abstract": "The rapid development of knowledge graphs (KGs), such as Freebase and WordNet, has changed the paradigm for AI-related applications. However, even though these KGs are impressively large, most of them are suffering from incompleteness, which leads to performance degradation of AI applications. Most existing researches are focusing on knowledge graph embedding (KGE) models. Nevertheless, those models simply embed entities and relations into latent vectors without leveraging the rich information from the relation structure. Indeed, relations in KGs conform to a three-layer hierarchical relation structure (HRS), i.e., semantically similar relations can make up relation clusters and some relations can be further split into several fine-grained sub-relations. Relation clusters, relations and sub-relations can fit in the top, the middle and the bottom layer of three-layer HRS respectively. To this end, in this paper, we extend existing KGE models TransE, TransH and DistMult, to learn knowledge representations by leveraging the information from the HRS. Particularly, our approach is capable to extend other KGE models. Finally, the experiment results clearly validate the effectiveness of the proposed approach against baselines.",
      "authors": [
        "Zhao Zhang",
        "Fuzhen Zhuang",
        "Meng Qu",
        "Fen Lin",
        "Qing He"
      ],
      "year": 2018,
      "citation_count": 76,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/86ac98157da100a529ca65fe6e1da064b0a651e8",
      "pdf_link": "",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d7ef14459674b75807cd9be549f1e12d53849ead",
      "title": "Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis",
      "abstract": "Knowledge Graph Embeddings (KGEs) have been intensively explored in recent years due to their promise for a wide range of applications. However, existing studies focus on improving the final model performance without acknowledging the computational cost of the proposed approaches, in terms of execution time and environmental impact. This paper proposes a simple yet effective KGE framework which can reduce the training time and carbon footprint by orders of magnitudes compared with state-of-the-art approaches, while producing competitive performance. We highlight three technical innovations: full batch learning via relational matrices, closed-form Orthogonal Procrustes Analysis for KGEs, and non-negative-sampling training. In addition, as the first KGE method whose entity embeddings also store full relation information, our trained models encode rich semantics and are highly interpretable. Comprehensive experiments and ablation studies involving 13 strong baselines and two standard datasets verify the effectiveness and efficiency of our algorithm.",
      "authors": [
        "Xutan Peng",
        "Guanyi Chen",
        "Chenghua Lin",
        "Mark Stevenson"
      ],
      "year": 2021,
      "citation_count": 16,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d7ef14459674b75807cd9be549f1e12d53849ead",
      "pdf_link": "",
      "venue": "North American Chapter of the Association for Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b30481dd5467a187b7e1a5a2dd326d97cafd95ac",
      "title": "Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding",
      "abstract": "",
      "authors": [
        "Chenyan Xiong",
        "Russell Power",
        "Jamie Callan"
      ],
      "year": 2017,
      "citation_count": 437,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b30481dd5467a187b7e1a5a2dd326d97cafd95ac",
      "pdf_link": "",
      "venue": "The Web Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d1a525c16a53b94200029df1037f2c9c7c244d7b",
      "title": "TransA: An Adaptive Approach for Knowledge Graph Embedding",
      "abstract": "Knowledge representation is a major topic in AI, and many studies attempt to represent entities and relations of knowledge base in a continuous vector space. Among these attempts, translation-based methods build entity and relation vectors by minimizing the translation loss from a head entity to a tail one. In spite of the success of these methods, translation-based methods also suffer from the oversimplified loss metric, and are not competitive enough to model various and complex entities/relations in knowledge bases. To address this issue, we propose \\textbf{TransA}, an adaptive metric approach for embedding, utilizing the metric learning ideas to provide a more flexible embedding method. Experiments are conducted on the benchmark datasets and our proposed method makes significant and consistent improvements over the state-of-the-art baselines.",
      "authors": [
        "Han Xiao",
        "Minlie Huang",
        "Yu Hao",
        "Xiaoyan Zhu"
      ],
      "year": 2015,
      "citation_count": 150,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d1a525c16a53b94200029df1037f2c9c7c244d7b",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "7572aefcd241ec76341addcb2e2e417587cb2e4c",
      "title": "Knowledge Graph Embedding Based Question Answering",
      "abstract": "Question answering over knowledge graph (QA-KG) aims to use facts in the knowledge graph (KG) to answer natural language questions. It helps end users more efficiently and more easily access the substantial and valuable knowledge in the KG, without knowing its data structures. QA-KG is a nontrivial problem since capturing the semantic meaning of natural language is difficult for a machine. Meanwhile, many knowledge graph embedding methods have been proposed. The key idea is to represent each predicate/entity as a low-dimensional vector, such that the relation information in the KG could be preserved. The learned vectors could benefit various applications such as KG completion and recommender systems. In this paper, we explore to use them to handle the QA-KG problem. However, this remains a challenging task since a predicate could be expressed in different ways in natural language questions. Also, the ambiguity of entity names and partial names makes the number of possible answers large. To bridge the gap, we propose an effective Knowledge Embedding based Question Answering (KEQA) framework. We focus on answering the most common types of questions, i.e., simple questions, in which each question could be answered by the machine straightforwardly if its single head entity and single predicate are correctly identified. To answer a simple question, instead of inferring its head entity and predicate directly, KEQA targets at jointly recovering the question's head entity, predicate, and tail entity representations in the KG embedding spaces. Based on a carefully-designed joint distance metric, the three learned vectors' closest fact in the KG is returned as the answer. Experiments on a widely-adopted benchmark demonstrate that the proposed KEQA outperforms the state-of-the-art QA-KG methods.",
      "authors": [
        "Xiao Huang",
        "Jingyuan Zhang",
        "Dingcheng Li",
        "Ping Li"
      ],
      "year": 2019,
      "citation_count": 581,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/7572aefcd241ec76341addcb2e2e417587cb2e4c",
      "pdf_link": "",
      "venue": "Web Search and Data Mining",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "68f34ed64fdf07bb1325097c93576658e061231e",
      "title": "A Survey on Knowledge Graph Embedding: Approaches, Applications and Benchmarks",
      "abstract": "A knowledge graph (KG), also known as a knowledge base, is a particular kind of network structure in which the node indicates entity and the edge represent relation. However, with the explosion of network volume, the problem of data sparsity that causes large-scale KG systems to calculate and manage difficultly has become more significant. For alleviating the issue, knowledge graph embedding is proposed to embed entities and relations in a KG to a low-, dense and continuous feature space, and endow the yield model with abilities of knowledge inference and fusion. In recent years, many researchers have poured much attention in this approach, and we will systematically introduce the existing state-of-the-art approaches and a variety of applications that benefit from these methods in this paper. In addition, we discuss future prospects for the development of techniques and application trends. Specifically, we first introduce the embedding models that only leverage the information of observed triplets in the KG. We illustrate the overall framework and specific idea and compare the advantages and disadvantages of such approaches. Next, we introduce the advanced models that utilize additional semantic information to improve the performance of the original methods. We divide the additional information into two categories, including textual descriptions and relation paths. The extension approaches in each category are described, following the same classification criteria as those defined for the triplet fact-based models. We then describe two experiments for comparing the performance of listed methods and mention some broader domain tasks such as question answering, recommender systems, and so forth. Finally, we collect several hurdles that need to be overcome and provide a few future research directions for knowledge graph embedding.",
      "authors": [
        "Yuanfei Dai",
        "Shiping Wang",
        "N. Xiong",
        "Wenzhong Guo"
      ],
      "year": 2020,
      "citation_count": 242,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/68f34ed64fdf07bb1325097c93576658e061231e",
      "pdf_link": "",
      "venue": "Electronics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "e5c851867af5587466f7cd9c22f8b2c84f8c6b63",
      "title": "Cycle or Minkowski: Which is More Appropriate for Knowledge Graph Embedding?",
      "abstract": "Knowledge graph (KG) embedding aims to encode entities and relations into low-dimensional vector spaces, in turn, can support various machine learning models on KG related tasks with good performance. However, existing methods for knowledge graph embedding fail to consider the influence of the embedding space, which makes them still unsatisfactory in practical applications. In this study, we try to improve the expressiveness of the embedding space from the perspective of the metric. Specifically, we first point out the implications of Minkowski metric used in KG embedding and then make a quantitative analysis. To solve the limitations, we introduce a new metric, named Cycle metric, based on the oscillation property of the periodic function. Furthermore, we find that the function period has a significant influence on the expressiveness of the embedding space. Given a fully trained model, the smaller the period, the better the expressive ability. Finally, to validate the findings, we propose a new model, named CyclE by combining Cycle Metric and the popular KG embeddings models. Comprehensive experimental results show that Cycle is more appropriate than Minkowski for KG embedding.",
      "authors": [
        "Han Yang",
        "Leilei Zhang",
        "Bingning Wang",
        "Ting Yao",
        "Junfei Liu"
      ],
      "year": 2021,
      "citation_count": 6,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/e5c851867af5587466f7cd9c22f8b2c84f8c6b63",
      "pdf_link": "",
      "venue": "International Conference on Information and Knowledge Management",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "6a2f26cece133b0aa52843be0f149a65e78374f7",
      "title": "GeoEntity-type constrained knowledge graph embedding for predicting natural-language spatial relations",
      "abstract": "Abstract Natural-language spatial relations between geographic entities (geoentities) reflect diverse perceptions influenced by factors like location, culture, and linguistic conventions. These relations play a crucial role in supporting geospatial tasks, such as question answering and cognitive reasoning. While prior studies focused on a limited set of human-selected spatial terms and geometric attributes, they often overlooked essential semantic attributes. To overcome this limitation, we developed a Spatial Relation-based Knowledge Graph Embedding framework, SR-KGE, with new KG fusion functions to predict spatial relation terms among distinct geoentities. This method not only considers graph structures and the diversity of natural language expressions in the embedding and learning process, but also incorporates geoentity types as a constraint to capture spatial and semantic relations more accurately. Our experiments on two knowledge graph datasets, one small-scale and one large-scale, have both shown its superior performance in spatial relation inference compared to popular KGE models, including TransE, RotatE, and HAKE. We hope our research will advance the classic study of natural language described spatial relations in a more automated and intelligent way.",
      "authors": [
        "Lei Hu",
        "Wenwen Li",
        "Jun Xu",
        "Yunqiang Zhu"
      ],
      "year": 2024,
      "citation_count": 5,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/6a2f26cece133b0aa52843be0f149a65e78374f7",
      "pdf_link": "",
      "venue": "International Journal of Geographical Information Science",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "040fe47af8f4870bf681f34861c42b3ea46d76cf",
      "title": "Message Function Search for Knowledge Graph Embedding",
      "abstract": "Recently, many promising embedding models have been proposed to embed knowledge graphs (KGs) and their more general forms, such as n-ary relational data (NRD) and hyper-relational KG (HKG). To promote the data adaptability and performance of embedding models, KG searching methods propose to search for suitable models for a given KG data set. But they are restricted to a single KG form, and the searched models are restricted to a single type of embedding model. To tackle such issues, we propose to build a search space for the message function in graph neural networks (GNNs). However, it is a non-trivial task. Existing message function designs fix the structures and operators, which makes them difficult to handle different KG forms and data sets. Therefore, we first design a novel message function space, which enables both structures and operators to be searched for the given KG form (including KG, NRD, and HKG) and data. The proposed space can flexibly take different KG forms as inputs and is expressive to search for different types of embedding models. Especially, some existing message function designs and some classic KG embedding models can be instantiated as special cases of our space. We empirically show that the searched message functions are data-dependent, and can achieve leading performance on benchmark KGs, NRD, and HKGs.",
      "authors": [
        "Shimin Di",
        "Lei Chen"
      ],
      "year": 2023,
      "citation_count": 9,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/040fe47af8f4870bf681f34861c42b3ea46d76cf",
      "pdf_link": "",
      "venue": "The Web Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "bbb89d88ad5b8279709ff089d3c00cd2750cd26b",
      "title": "Efficient Non-Sampling Knowledge Graph Embedding",
      "abstract": "Knowledge Graph (KG) is a flexible structure that is able to describe the complex relationship between data entities. Currently, most KG embedding models are trained based on negative sampling, i.e., the model aims to maximize some similarity of the connected entities in the KG, while minimizing the similarity of the sampled disconnected entities. Negative sampling helps to reduce the time complexity of model learning by only considering a subset of negative instances, which may fail to deliver stable model performance due to the uncertainty in the sampling procedure. To avoid such deficiency, we propose a new framework for KG embedding—Efficient Non-Sampling Knowledge Graph Embedding (NS-KGE). The basic idea is to consider all of the negative instances in the KG for model learning, and thus to avoid negative sampling. The framework can be applied to square-loss based knowledge graph embedding models or models whose loss can be converted to a square loss. A natural side-effect of this non-sampling strategy is the increased computational complexity of model learning. To solve the problem, we leverage mathematical derivations to reduce the complexity of non-sampling loss function, which eventually provides us both better efficiency and better accuracy in KG embedding compared with existing models. Experiments on benchmark datasets show that our NS-KGE framework can achieve a better performance on efficiency and accuracy over traditional negative sampling based models, and that the framework is applicable to a large class of knowledge graph embedding models.",
      "authors": [
        "Zelong Li",
        "Jianchao Ji",
        "Zuohui Fu",
        "Yingqiang Ge",
        "Shuyuan Xu",
        "Chong Chen",
        "Yongfeng Zhang"
      ],
      "year": 2021,
      "citation_count": 40,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/bbb89d88ad5b8279709ff089d3c00cd2750cd26b",
      "pdf_link": "",
      "venue": "The Web Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "4e52607397a96fb2104a99c570c9cec29c9ca519",
      "title": "ChronoR: Rotation Based Temporal Knowledge Graph Embedding",
      "abstract": "Despite the importance and abundance of temporal knowledge graphs, most of the current research has been focused on reasoning on static graphs. In this paper, we study the challenging problem of inference over temporal knowledge graphs. In particular, the task of temporal link prediction. In general, this is a difficult task due to data non-stationarity, data heterogeneity, and its complex temporal dependencies. \nWe propose Chronological Rotation embedding (ChronoR), a novel model for learning representations for entities, relations, and time. Learning dense representations is frequently used as an efficient and versatile method to perform reasoning on knowledge graphs. The proposed model learns a k-dimensional rotation transformation parametrized by relation and time, such that after each fact's head entity is transformed using the rotation, it falls near its corresponding tail entity. By using high dimensional rotation as its transformation operator, ChronoR captures rich interaction between the temporal and multi-relational characteristics of a Temporal Knowledge Graph. Experimentally, we show that ChronoR is able to outperform many of the state-of-the-art methods on the benchmark datasets for temporal knowledge graph link prediction.",
      "authors": [
        "A. Sadeghian",
        "Mohammadreza Armandpour",
        "Anthony Colas",
        "D. Wang"
      ],
      "year": 2021,
      "citation_count": 119,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/4e52607397a96fb2104a99c570c9cec29c9ca519",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "83a46afaeb520abcd9b0138507a253f6d4d8bff7",
      "title": "Rot-Pro: Modeling Transitivity by Projection in Knowledge Graph Embedding",
      "abstract": "Knowledge graph embedding models learn the representations of entities and relations in the knowledge graphs for predicting missing links (relations) between entities. Their effectiveness are deeply affected by the ability of modeling and inferring different relation patterns such as symmetry, asymmetry, inversion, composition and transitivity. Although existing models are already able to model many of these relations patterns, transitivity, a very common relation pattern, is still not been fully supported. In this paper, we first theoretically show that the transitive relations can be modeled with projections. We then propose the Rot-Pro model which combines the projection and relational rotation together. We prove that Rot-Pro can infer all the above relation patterns. Experimental results show that the proposed Rot-Pro model effectively learns the transitivity pattern and achieves the state-of-the-art results on the link prediction task in the datasets containing transitive relations.",
      "authors": [
        "Tengwei Song",
        "Jie Luo",
        "Lei Huang"
      ],
      "year": 2021,
      "citation_count": 40,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/83a46afaeb520abcd9b0138507a253f6d4d8bff7",
      "pdf_link": "",
      "venue": "Neural Information Processing Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "fda63b289d4c0c332f88975994114fb61b514ced",
      "title": "Molecular-evaluated and explainable drug repurposing for COVID-19 using ensemble knowledge graph embedding",
      "abstract": "The search for an effective drug is still urgent for COVID-19 as no drug with proven clinical efficacy is available. Finding the new purpose of an approved or investigational drug, known as drug repurposing, has become increasingly popular in recent years. We propose here a new drug repurposing approach for COVID-19, based on knowledge graph (KG) embeddings. Our approach learns “ensemble embeddings” of entities and relations in a COVID-19 centric KG, in order to get a better latent representation of the graph elements. Ensemble KG-embeddings are subsequently used in a deep neural network trained for discovering potential drugs for COVID-19. Compared to related works, we retrieve more in-trial drugs among our top-ranked predictions, thus giving greater confidence in our prediction for out-of-trial drugs. For the first time to our knowledge, molecular docking is then used to evaluate the predictions obtained from drug repurposing using KG embedding. We show that Fosinopril is a potential ligand for the SARS-CoV-2 nsp13 target. We also provide explanations of our predictions thanks to rules extracted from the KG and instanciated by KG-derived explanatory paths. Molecular evaluation and explanatory paths bring reliability to our results and constitute new complementary and reusable methods for assessing KG-based drug repurposing.",
      "authors": [
        "M. Islam",
        "Diego Amaya-Ramirez",
        "B. Maigret",
        "M. Devignes",
        "Sabeur Aridhi",
        "Malika Smaïl-Tabbone"
      ],
      "year": 2023,
      "citation_count": 15,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/fda63b289d4c0c332f88975994114fb61b514ced",
      "pdf_link": "",
      "venue": "Scientific Reports",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "bcdb8914550df02bfe1f69348c9830d775f6590a",
      "title": "Knowledge Graph Embedding with Atrous Convolution and Residual Learning",
      "abstract": "Knowledge graph embedding is an important task and it will benefit lots of downstream applications. Currently, deep neural networks based methods achieve state-of-the-art performance. However, most of these existing methods are very complex and need much time for training and inference. To address this issue, we propose a simple but effective atrous convolution based knowledge graph embedding method. Compared with existing state-of-the-art methods, our method has following main characteristics. First, it effectively increases feature interactions by using atrous convolutions. Second, to address the original information forgotten issue and vanishing/exploding gradient issue, it uses the residual learning method. Third, it has simpler structure but much higher parameter efficiency. We evaluate our method on six benchmark datasets with different evaluation metrics. Extensive experiments show that our model is very effective. On these diverse datasets, it achieves better results than the compared state-of-the-art methods on most of evaluation metrics. The source codes of our model could be found at https://github.com/neukg/AcrE.",
      "authors": [
        "Feiliang Ren",
        "Jucheng Li",
        "Huihui Zhang",
        "Shilei Liu",
        "Bochao Li",
        "Ruicheng Ming",
        "Yujia Bai"
      ],
      "year": 2020,
      "citation_count": 37,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/bcdb8914550df02bfe1f69348c9830d775f6590a",
      "pdf_link": "",
      "venue": "International Conference on Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "0364e17da01358e2705524cd781ef8cc928256f5",
      "title": "Tensor Decomposition-Based Temporal Knowledge Graph Embedding",
      "abstract": "In order to meet the problems caused by sparse data and computational efficiency, knowledge graph (KG) is adopted to represent the semantic information of entities and relations as dense and low-dimensional vectors. While conventional KG representation methods mainly focuse on static data. These methods fail to deal with data that evolves with time which may only be valid for a certain period of time. To accommodate this problem, a temporal KG embedding model based on tensor decomposition is proposed in this paper, which regards the fact set in the KG as a fourth-order tensor including head entities, relations, tail entities and time dimensions. This method can be further generalized to other static KG embedding based on tensor decomposition. With experiments on temporal datasets extracted from real-world KG, extensive experiment results show that our approach outperforms state-of-the-art methods of KG embedding.",
      "authors": [
        "Lifan Lin",
        "Kun She"
      ],
      "year": 2020,
      "citation_count": 18,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/0364e17da01358e2705524cd781ef8cc928256f5",
      "pdf_link": "",
      "venue": "IEEE International Conference on Tools with Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "title": "Knowledge Graph Embedding: An Overview",
      "abstract": "Many mathematical models have been leveraged to design embeddings for representing Knowledge Graph (KG) entities and relations for link prediction and many downstream tasks. These mathematically-inspired models are not only highly scalable for inference in large KGs, but also have many explainable advantages in modeling different relation patterns that can be validated through both formal proofs and empirical results. In this paper, we make a comprehensive overview of the current state of research in KG completion. In particular, we focus on two main branches of KG embedding (KGE) design: 1) distance-based methods and 2) semantic matching-based methods. We discover the connections between recently proposed models and present an underlying trend that might help researchers invent novel and more effective models. Next, we delve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D affine operations, respectively. They encompass a broad spectrum of techniques including distance-based and semantic-based methods. We will also discuss an emerging approach for KG completion which leverages pre-trained language models (PLMs) and textual descriptions of entities and relations and offer insights into the integration of KGE embedding methods with PLMs for KG completion.",
      "authors": [
        "Xiou Ge",
        "Yun Cheng Wang",
        "Bin Wang",
        "C.-C. Jay Kuo"
      ],
      "year": 2023,
      "citation_count": 27,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f2b924e69735fb7fd6fd95c6a032954480862029",
      "pdf_link": "",
      "venue": "APSIPA Transactions on Signal and Information Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "f470e11faa6200026cf39e248510070c078e509a",
      "title": "A Survey on Knowledge Graph Embedding",
      "abstract": "Knowledge graph (KG) is used to represent the relationships between different concepts in the real world. It is a special network in which nodes represent entities and edges represent relationships. KGs can intuitively model the connections between facts, but in many applications, there are certain limitations in directly using symbolic logic to represent knowledge in KGs and perform calculations, making it difficult to achieve expected results in downstream tasks. Meanwhile, with the explosive growth of Internet capacity, the traditional KG structure faces the problems of computational inefficiency and management difficulties. To alleviate these problems, Knowledge graph embedding (KGE) is proposed to improve the computational efficiency by embedding entities and relations in the KG into a low-dimensional, dense and continuous vector space, and thus the solution of some problems in the knowledge graph is transformed into vector operations. Moreover, KGE also can be used as a pre-trained model which is more beneficial to downstream applications, such as applications based on deep learning. In this paper, we classify KGE into three categories, namely translational distance models, semantic matching models and neural network based models. The advantages and disadvantages of different embedding methods are compared, while the main applications of KGE are summarized. Some current challenges of KGE are summarized, as well as some views on the future research directions of KGE.",
      "authors": [
        "Qi Yan",
        "Jiaxin Fan",
        "Mohan Li",
        "Guanqun Qu",
        "Yang Xiao"
      ],
      "year": 2022,
      "citation_count": 24,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/f470e11faa6200026cf39e248510070c078e509a",
      "pdf_link": "",
      "venue": "International Conference on Data Science in Cyberspace",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d9802a67b326fe89bbd761c261937ee1e4d4d674",
      "title": "Link Prediction with Attention Applied on Multiple Knowledge Graph Embedding Models",
      "abstract": "Predicting missing links between entities in a knowledge graph is a fundamental task to deal with the incompleteness of data on the Web. Knowledge graph embeddings map nodes into a vector space to predict new links, scoring them according to geometric criteria. Relations in the graph may follow patterns that can be learned, e.g., some relations might be symmetric and others might be hierarchical. However, the learning capability of different embedding models varies for each pattern and, so far, no single model can learn all patterns equally well. In this paper, we combine the query representations from several models in a unified one to incorporate patterns that are independently captured by each model. Our combination uses attention to select the most suitable model to answer each query. The models are also mapped onto a non-Euclidean manifold, the Poincaré ball, to capture structural patterns, such as hierarchies, besides relational patterns, such as symmetry. We prove that our combination provides a higher expressiveness and inference power than each model on its own. As a result, the combined model can learn relational and structural patterns. We conduct extensive experimental analysis with various link prediction benchmarks showing that the combined model outperforms individual models, including state-of-the-art approaches.",
      "authors": [
        "Cosimo Gregucci",
        "M. Nayyeri",
        "D. Hern'andez",
        "Steffen Staab"
      ],
      "year": 2023,
      "citation_count": 26,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d9802a67b326fe89bbd761c261937ee1e4d4d674",
      "pdf_link": "",
      "venue": "The Web Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "33d469c6d9fc09b59522d91b7696b15dc60a9a93",
      "title": "Knowledge Graph Embedding Compression",
      "abstract": "Knowledge graph (KG) representation learning techniques that learn continuous embeddings of entities and relations in the KG have become popular in many AI applications. With a large KG, the embeddings consume a large amount of storage and memory. This is problematic and prohibits the deployment of these techniques in many real world settings. Thus, we propose an approach that compresses the KG embedding layer by representing each entity in the KG as a vector of discrete codes and then composes the embeddings from these codes. The approach can be trained end-to-end with simple modifications to any existing KG embedding technique. We evaluate the approach on various standard KG embedding evaluations and show that it achieves 50-1000x compression of embeddings with a minor loss in performance. The compressed embeddings also retain the ability to perform various reasoning tasks such as KG inference.",
      "authors": [
        "Mrinmaya Sachan"
      ],
      "year": 2020,
      "citation_count": 30,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/33d469c6d9fc09b59522d91b7696b15dc60a9a93",
      "pdf_link": "",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4",
      "title": "Rotate3D: Representing Relations as Rotations in Three-Dimensional Space for Knowledge Graph Embedding",
      "abstract": "Knowledge graph embedding, which aims to learn low-dimensional embeddings of entities and relations, plays a vital role in a wide range of applications. It is crucial for knowledge graph embedding models to model and infer various relation patterns, such as symmetry/antisymmetry, inversion, and composition. However, most existing methods fail to model the non-commutative composition pattern, which is essential, especially for multi-hop reasoning. To address this issue, we propose a new model called Rotate3D, which maps entities to the three-dimensional space and defines relations as rotations from head entities to tail entities. By using the non-commutative composition property of rotations in the three-dimensional space, Rotate3D can naturally preserve the order of the composition of relations. Experiments show that Rotate3D outperforms existing state-of-the-art models for link prediction and path query answering. Further case studies demonstrate that Rotate3D can effectively capture various relation patterns with a marked improvement in modeling the composition pattern.",
      "authors": [
        "Chang Gao",
        "Chengjie Sun",
        "Lili Shan",
        "Lei Lin",
        "Mingjiang Wang"
      ],
      "year": 2020,
      "citation_count": 59,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4",
      "pdf_link": "",
      "venue": "International Conference on Information and Knowledge Management",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "658702b2fa647ae7eaf1255058105da9eefe6f52",
      "title": "Assessing the effects of hyperparameters on knowledge graph embedding quality",
      "abstract": "Embedding knowledge graphs into low-dimensional spaces is a popular method for applying approaches, such as link prediction or node classification, to these databases. This embedding process is very costly in terms of both computational time and space. Part of the reason for this is the optimisation of hyperparameters, which involves repeatedly sampling, by random, guided, or brute-force selection, from a large hyperparameter space and testing the resulting embeddings for their quality. However, not all hyperparameters in this search space will be equally important. In fact, with prior knowledge of the relative importance of the hyperparameters, some could be eliminated from the search altogether without significantly impacting the overall quality of the outputted embeddings. To this end, we ran a Sobol sensitivity analysis to evaluate the effects of tuning different hyperparameters on the variance of embedding quality. This was achieved by performing thousands of embedding trials, each time measuring the quality of embeddings produced by different hyperparameter configurations. We regressed the embedding quality on those hyperparameter configurations, using this model to generate Sobol sensitivity indices for each of the hyperparameters. By evaluating the correlation between Sobol indices, we find substantial variability in the hyperparameter sensitivities between knowledge graphs with differing dataset characteristics as the probable cause of these inconsistencies. As an additional contribution of this work we identify several relations in the UMLS knowledge graph that may cause data leakage via inverse relations, and derive and present UMLS-43, a leakage-robust variant of that graph.",
      "authors": [
        "Oliver Lloyd",
        "Yi Liu",
        "T. Gaunt"
      ],
      "year": 2022,
      "citation_count": 7,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/658702b2fa647ae7eaf1255058105da9eefe6f52",
      "pdf_link": "",
      "venue": "Journal of Big Data",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "b3f0cdc217a3d192d2671e44913542903c94105b",
      "title": "TARGAT: A Time-Aware Relational Graph Attention Model for Temporal Knowledge Graph Embedding",
      "abstract": "Temporal knowledge graph embedding (TKGE) aims to learn the embedding of entities and relations in a temporal knowledge graph (TKG). Although the previous graph neural networks (GNN) based models have achieved promising results, they cannot directly capture the interactions of multi-facts at different timestamps. To address the above limitation, we propose a time-aware relational graph attention model (TARGAT), which takes the multi-facts at different timestamps as a unified graph. First, we develop a relational generator to dynamically generate a series of time-aware relational message transformation matrices, which jointly models the relations and the timestamp information into a unified way. Then, we apply the generated message transformation matrices to project the neighborhood features into different time-aware spaces and aggregate these neighborhood features to explicitly capture the interactions of multi-facts. Finally, a temporal transformer classifier is applied to learn the representation of the query quadruples and predict the missing entities. The experimental results show that our TARGAT model beats the GNN-based models by a large margin and achieves new state-of-the-art results on four popular benchmark datasets.",
      "authors": [
        "Zhiwen Xie",
        "Runjie Zhu",
        "Jin Liu",
        "Guangyou Zhou",
        "J. Huang"
      ],
      "year": 2023,
      "citation_count": 19,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/b3f0cdc217a3d192d2671e44913542903c94105b",
      "pdf_link": "",
      "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "33a7b7abf006d22de24c1471e6f6c93842a497b6",
      "title": "GE2: A General and Efficient Knowledge Graph Embedding Learning System",
      "abstract": "Graph embedding learning computes an embedding vector for each node in a graph and finds many applications in areas such as social networks, e-commerce, and medicine. We observe that existing graph embedding systems (e.g., PBG, DGL-KE, and Marius) have long CPU time and high CPU-GPU communication overhead, especially when using multiple GPUs. Moreover, it is cumbersome to implement negative sampling algorithms on them, which have many variants and are crucial for model quality. We propose a new system called GE2, which achieves both generality and efficiency for graph embedding learning. In particular, we propose a general execution model that encompasses various negative sampling algorithms. Based on the execution model, we design a user-friendly API that allows users to easily express negative sampling algorithms. To support efficient training, we offload operations from CPU to GPU to enjoy high parallelism and reduce CPU time. We also design COVER, which, to our knowledge, is the first algorithm to manage data swap between CPU and multiple GPUs for small communication costs. Extensive experimental results show that, comparing with the state-of-the-art graph embedding systems, GE2 trains consistently faster across different models and datasets, where the speedup is usually over 2x and can be up to 7.5x.",
      "authors": [
        "Chenguang Zheng",
        "Guanxian Jiang",
        "Xiao Yan",
        "Peiqi Yin",
        "Qihui Zhou",
        "James Cheng"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/33a7b7abf006d22de24c1471e6f6c93842a497b6",
      "pdf_link": "",
      "venue": "Proc. ACM Manag. Data",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "398978c84ca8dab093d0b7fa73c6d380f5fa914c",
      "title": "MQuinE: a Cure for “Z-paradox” in Knowledge Graph Embedding",
      "abstract": "Knowledge graph embedding (KGE) models achieved state-of-the-art results on many knowledge graph tasks including link prediction and information retrieval. Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called Z-paradox. Motivated by the existence of Z-paradox, we propose a new KGE model called MQuinE that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification. Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20% accuracy drop on some challenging test samples. Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction tasks.",
      "authors": [
        "Yang Liu",
        "Huang Fang",
        "Yunfeng Cai",
        "Mingming Sun"
      ],
      "year": 2024,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/398978c84ca8dab093d0b7fa73c6d380f5fa914c",
      "pdf_link": "",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "8f096071a09701012c9c279aee2a88143a295935",
      "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space",
      "abstract": "We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.",
      "authors": [
        "Zhiqing Sun",
        "Zhihong Deng",
        "Jian-Yun Nie",
        "Jian Tang"
      ],
      "year": 2018,
      "citation_count": 2251,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/8f096071a09701012c9c279aee2a88143a295935",
      "pdf_link": "",
      "venue": "International Conference on Learning Representations",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a166957ec488cd20e61360d630568b3b81af3397",
      "title": "Multimodal reasoning based on knowledge graph embedding for specific diseases",
      "abstract": "Abstract Motivation Knowledge Graph (KG) is becoming increasingly important in the biomedical field. Deriving new and reliable knowledge from existing knowledge by KG embedding technology is a cutting-edge method. Some add a variety of additional information to aid reasoning, namely multimodal reasoning. However, few works based on the existing biomedical KGs are focused on specific diseases. Results This work develops a construction and multimodal reasoning process of Specific Disease Knowledge Graphs (SDKGs). We construct SDKG-11, a SDKG set including five cancers, six non-cancer diseases, a combined Cancer5 and a combined Diseases11, aiming to discover new reliable knowledge and provide universal pre-trained knowledge for that specific disease field. SDKG-11 is obtained through original triplet extraction, standard entity set construction, entity linking and relation linking. We implement multimodal reasoning by reverse-hyperplane projection for SDKGs based on structure, category and description embeddings. Multimodal reasoning improves pre-existing models on all SDKGs using entity prediction task as the evaluation protocol. We verify the model’s reliability in discovering new knowledge by manually proofreading predicted drug–gene, gene–disease and disease–drug pairs. Using embedding results as initialization parameters for the biomolecular interaction classification, we demonstrate the universality of embedding models. Availability and implementation The constructed SDKG-11 and the implementation by TensorFlow are available from https://github.com/ZhuChaoY/SDKG-11. Supplementary information Supplementary data are available at Bioinformatics online.",
      "authors": [
        "Chaoyu Zhu",
        "Zhihao Yang",
        "Xiaoqiong Xia",
        "Nan Li",
        "Fan Zhong",
        "Lei Liu"
      ],
      "year": 2022,
      "citation_count": 29,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a166957ec488cd20e61360d630568b3b81af3397",
      "pdf_link": "",
      "venue": "Bioinform.",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d3c287ff061f295ddf8dc3cb02a6f39e301cae3b",
      "title": "Differentiating Concepts and Instances for Knowledge Graph Embedding",
      "abstract": "Concepts, which represent a group of different instances sharing common properties, are essential information in knowledge representation. Most conventional knowledge embedding methods encode both entities (concepts and instances) and relations as vectors in a low dimensional semantic space equally, ignoring the difference between concepts and instances. In this paper, we propose a novel knowledge graph embedding model named TransC by differentiating concepts and instances. Specifically, TransC encodes each concept in knowledge graph as a sphere and each instance as a vector in the same semantic space. We use the relative positions to model the relations between concepts and instances (i.e.,instanceOf), and the relations between concepts and sub-concepts (i.e., subClassOf). We evaluate our model on both link prediction and triple classification tasks on the dataset based on YAGO. Experimental results show that TransC outperforms state-of-the-art methods, and captures the semantic transitivity for instanceOf and subClassOf relation. Our codes and datasets can be obtained from https://github.com/davidlvxin/TransC.",
      "authors": [
        "Xin Lv",
        "Lei Hou",
        "Juan-Zi Li",
        "Zhiyuan Liu"
      ],
      "year": 2018,
      "citation_count": 91,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d3c287ff061f295ddf8dc3cb02a6f39e301cae3b",
      "pdf_link": "",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "a905a690ec350b1aeb5fcfd7f2ff0f5e1663b3a0",
      "title": "Knowledge Graph Embedding Preserving Soft Logical Regularity",
      "abstract": "Embedding knowledge graphs (KGs) into continuous vector spaces is currently an active research area. Soft rules, despite their uncertainty, are highly beneficial to KG embedding. However, they have not been studied enough in recent methods. A major challenge here is how to devise a principled framework, which efficiently and effectively integrates such soft logical information into embedding models. This paper proposes a highly scalable and effective method for preserving soft logical regularities by imposing soft rule constraints on relation representations. Specifically, we first represent relations as bilinear forms and map entity representations into a non-negative and bounded space. Then we derive a rule-based regularization that merely enforces relation representations to satisfy constraints introduced by soft rules. The proposed method has the following advantages: 1) it regularizes relations directly with the complexity of rule learning independent of entity set size, improving scalability; 2) it imposes prior logical information upon the structure of the embedding space, and would be beneficial to knowledge reasoning. Evaluation in link prediction on Freebase and DBpedia shows the effectiveness of our approach over many competitive baselines. Code and datasets are available at https://github.com/StudyGroup-lab/SLRE.",
      "authors": [
        "Shu Guo",
        "Lin Li",
        "Zhen Hui",
        "Lingshuai Meng",
        "Bingnan Ma",
        "Wei Liu",
        "Lihong Wang",
        "Haibin Zhai",
        "Hong Zhang"
      ],
      "year": 2020,
      "citation_count": 14,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/a905a690ec350b1aeb5fcfd7f2ff0f5e1663b3a0",
      "pdf_link": "",
      "venue": "International Conference on Information and Knowledge Management",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "acc855d74431537b98de5185e065e4eacbab7b26",
      "title": "Bringing Light Into the Dark: A Large-Scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework",
      "abstract": "The heterogeneity in recently published knowledge graph embedding models’ implementations, training, and evaluation has made fair and thorough comparisons difficult. To assess the reproducibility of previously published results, we re-implemented and evaluated 21 models in the PyKEEN software package. In this paper, we outline which results could be reproduced with their reported hyper-parameters, which could only be reproduced with alternate hyper-parameters, and which could not be reproduced at all, as well as provide insight as to why this might be the case. We then performed a large-scale benchmarking on four datasets with several thousands of experiments and 24,804 GPU hours of computation time. We present insights gained as to best practices, best configurations for each model, and where improvements could be made over previously published best configurations. Our results highlight that the combination of model architecture, training approach, loss function, and the explicit modeling of inverse relations is crucial for a model’s performance and is not only determined by its architecture. We provide evidence that several architectures can obtain results competitive to the state of the art when configured carefully. We have made all code, experimental configurations, results, and analyses available at https://github.com/pykeen/pykeen and https://github.com/pykeen/benchmarking.",
      "authors": [
        "Mehdi Ali",
        "M. Berrendorf",
        "Charles Tapley Hoyt",
        "Laurent Vermue",
        "Mikhail Galkin",
        "Sahand Sharifzadeh",
        "Asja Fischer",
        "Volker Tresp",
        "Jens Lehmann"
      ],
      "year": 2020,
      "citation_count": 134,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/acc855d74431537b98de5185e065e4eacbab7b26",
      "pdf_link": "",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "29eb99518d16ccf8ac306d92f4a6377ae109d9be",
      "title": "DisenKGAT: Knowledge Graph Embedding with Disentangled Graph Attention Network",
      "abstract": "Knowledge graph completion (KGC) has become a focus of attention across deep learning community owing to its excellent contribution to numerous downstream tasks. Although recently have witnessed a surge of work on KGC, they are still insufficient to accurately capture complex relations, since they adopt the single and static representations. In this work, we propose a novel Disentangled Knowledge Graph Attention Network (DisenKGAT) for KGC, which leverages both micro-disentanglement and macro-disentanglement to exploit representations behind Knowledge graphs (KGs). To achieve micro-disentanglement, we put forward a novel relation-aware aggregation to learn diverse component representation. For macro-disentanglement, we leverage mutual information as a regularization to enhance independence. With the assistance of disentanglement, our model is able to generate adaptive representations in terms of the given scenario. Besides, our work has strong robustness and flexibility to adapt to various score functions. Extensive experiments on public benchmark datasets have been conducted to validate the superiority of DisenKGAT over existing methods in terms of both accuracy and explainability.",
      "authors": [
        "Junkang Wu",
        "Wentao Shi",
        "Xuezhi Cao",
        "Jiawei Chen",
        "Wenqiang Lei",
        "Fuzheng Zhang",
        "Wei Wu",
        "Xiangnan He"
      ],
      "year": 2021,
      "citation_count": 73,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/29eb99518d16ccf8ac306d92f4a6377ae109d9be",
      "pdf_link": "",
      "venue": "International Conference on Information and Knowledge Management",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "title": "Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces",
      "abstract": "Knowledge graph embedding (KGE) is an increasingly popular technique that aims to represent entities and relations of knowledge graphs into low-dimensional semantic spaces for a wide spectrum of applications such as link prediction, knowledge reasoning and knowledge completion. In this article, we provide a systematic review of existing KGE techniques based on representation spaces. Particularly, we build a fine-grained classification to categorise the models based on three mathematical perspectives of the representation spaces: (1) algebraic perspective, (2) geometric perspective and (3) analytical perspective. We introduce the rigorous definitions of fundamental mathematical spaces before diving into KGE models and their mathematical properties. We further discuss different KGE methods over the three categories, as well as summarise how spatial advantages work over different embedding needs. By collating the experimental results from downstream tasks, we also explore the advantages of mathematical space in different scenarios and the reasons behind them. We further state some promising research directions from a representation space perspective, with which we hope to inspire researchers to design their KGE models as well as their related applications with more consideration of their mathematical space properties.",
      "authors": [
        "Jiahang Cao",
        "Jinyuan Fang",
        "Zaiqiao Meng",
        "Shangsong Liang"
      ],
      "year": 2022,
      "citation_count": 101,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "pdf_link": "",
      "venue": "ACM Computing Surveys",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "1f20378d2820fdf1c1bb09ce22f739ab77b14e82",
      "title": "A Survey of Knowledge Graph Embedding and Their Applications",
      "abstract": "Knowledge Graph embedding provides a versatile technique for representing knowledge. These techniques can be used in a variety of applications such as completion of knowledge graph to predict missing information, recommender systems, question answering, query expansion, etc. The information embedded in Knowledge graph though being structured is challenging to consume in a real-world application. Knowledge graph embedding enables the real-world application to consume information to improve performance. Knowledge graph embedding is an active research area. Most of the embedding methods focus on structure-based information. Recent research has extended the boundary to include text-based information and image-based information in entity embedding. Efforts have been made to enhance the representation with context information. This paper introduces growth in the field of KG embedding from simple translation-based models to enrichment-based models. This paper includes the utility of the Knowledge graph in real-world applications.",
      "authors": [
        "Shivani Choudhary",
        "Tarun Luthra",
        "Ashima Mittal",
        "Rajat Singh"
      ],
      "year": 2021,
      "citation_count": 54,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/1f20378d2820fdf1c1bb09ce22f739ab77b14e82",
      "pdf_link": "",
      "venue": "arXiv.org",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "bb3e135757bfb82c4de202c807c9e381caecb623",
      "title": "Convolutional Neural Network-Based Entity-Specific Common Feature Aggregation for Knowledge Graph Embedding Learning",
      "abstract": "Deep learning models present impressive capability for automatic feature extraction, where common features-based aggregation have demonstrated valuable potential in improving the model performance on text classification, sentiment analysis, etc. However, leveraging entity-specific common feature aggregation for enhancing knowledge graph representation learning has not been fully explored yet, though diverse strategies in knowledge graph embedding models have been developed in recent years. This paper proposes an innovative Convolutional Neural Network-based Entity-specific Common Feature Aggregation strategy named CNN-ECFA. Besides, a new universal framework based on the CNN-ECFA strategy is introduced for knowledge graph embedding learning. Experiments are conducted on publicly-available standard datasets for a link prediction task including WN18RR, YAGO3-10 and NELL-995. Results show that the CNN-ECFA strategy outperforms the state-of-the-art feature projection strategies with average improvements of 0.6% and 0.7% of MRR and Hits@1 on all the datasets, demonstrating our CNN-ECFA strategy is more effective for knowledge graph embedding learning. In addition, our universal framework significantly outperforms a generalized relation learning framework on WN18RR and NELL-995 with average improvements of 1.7% and 1.9% on MRR and Hits@1. The source code is publicly available at https://github.com/peterhu95/ConvE-CNN-ECFA.",
      "authors": [
        "Kairong Hu",
        "Xiaozhi Zhu",
        "Hai Liu",
        "Yingying Qu",
        "Fu Lee Wang",
        "Tianyong Hao"
      ],
      "year": 2024,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/bb3e135757bfb82c4de202c807c9e381caecb623",
      "pdf_link": "",
      "venue": "IEEE transactions on consumer electronics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "6a86594566fc9fa2e92afb6f0229d63a45fe25e6",
      "title": "Poisoning Attack on Federated Knowledge Graph Embedding",
      "abstract": "Federated Knowledge Graph Embedding (FKGE) is an emerging collaborative learning technique for deriving expressive representations (i.e., embeddings) from client-maintained distributed knowledge graphs (KGs). However, poisoning attacks in FKGE, which lead to biased decisions by downstream applications, remain unexplored. This paper is the first work to systematize the risks of FKGE poisoning attacks, from which we develop a novel framework for poisoning attacks that force the victim client to predict specific false facts. Unlike centralized KGEs, FKGE maintains KGs locally, making direct injection of poisoned data challenging. Instead, attackers must create poisoned data without access to the victim's KG and inject it indirectly through FKGE aggregation. Specifically, to create poisoned data, the attacker first infers the targeted relations in the victim's local KG via a new KG component inference attack. Then, to accurately mislead the victim's embeddings via aggregation, the attacker locally trains a shadow model using the poisoned data and uses an optimized dynamic poisoning scheme to adjust the model and generate progressive poisoned updates. Our experimental results demonstrate the attack's effectiveness, achieving a remarkable success rate on various KGE models (e.g., 100% on TransE with WN18RR) while keeping the original task's performance nearly unchanged.",
      "authors": [
        "Enyuan Zhou",
        "Song Guo",
        "Zhixiu Ma",
        "Zicong Hong",
        "Tao Guo",
        "Peiran Dong"
      ],
      "year": 2024,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/6a86594566fc9fa2e92afb6f0229d63a45fe25e6",
      "pdf_link": "",
      "venue": "The Web Conference",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "d605a7628b2a7ff8ce04fc27111626e2d734cab4",
      "title": "Embedding knowledge graph of patent metadata to measure knowledge proximity",
      "abstract": "Knowledge proximity refers to the strength of association between any two entities in a structural form that embodies certain aspects of a knowledge base. In this work, we operationalize knowledge proximity within the context of the US Patent Database (knowledge base) using a knowledge graph (structural form) named “PatNet” built using patent metadata, including citations, inventors, assignees, and domain classifications. We train various graph embedding models using PatNet to obtain the embeddings of entities and relations. The cosine similarity between the corresponding (or transformed) embeddings of entities denotes the knowledge proximity between these. We compare the embedding models in terms of their performances in predicting target entities and explaining domain expansion profiles of inventors and assignees. We then apply the embeddings of the best‐preferred model to associate homogeneous (e.g., patent–patent) and heterogeneous (e.g., inventor–assignee) pairs of entities.",
      "authors": [
        "Guangtong Li",
        "L. Siddharth",
        "Jianxi Luo"
      ],
      "year": 2022,
      "citation_count": 8,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/d605a7628b2a7ff8ce04fc27111626e2d734cab4",
      "pdf_link": "",
      "venue": "J. Assoc. Inf. Sci. Technol.",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "10d949dee482aeea1cab8b42c326d0dbf0505de3",
      "title": "Entity-Agnostic Representation Learning for Parameter-Efficient Knowledge Graph Embedding",
      "abstract": "We propose an entity-agnostic representation learning method for handling the problem of inefficient parameter storage costs brought by embedding knowledge graphs. Conventional knowledge graph embedding methods map elements in a knowledge graph, including entities and relations, into continuous vector spaces by assigning them one or multiple specific embeddings (i.e., vector representations). Thus the number of embedding parameters increases linearly as the growth of knowledge graphs. In our proposed model, Entity-Agnostic Representation Learning (EARL), we only learn the embeddings for a small set of entities and refer to them as reserved entities. To obtain the embeddings for the full set of entities, we encode their distinguishable information from their connected relations, k-nearest reserved entities, and multi-hop neighbors. We learn universal and entity-agnostic encoders for transforming distinguishable information into entity embeddings. This approach allows our proposed EARL to have a static, efficient, and lower parameter count than conventional knowledge graph embedding methods. Experimental results show that EARL uses fewer parameters and performs better on link prediction tasks than baselines, reflecting its parameter efficiency.",
      "authors": [
        "Mingyang Chen",
        "Wen Zhang",
        "Zhen Yao",
        "Yushan Zhu",
        "Yang Gao",
        "Jeff Z. Pan",
        "Hua-zeng Chen"
      ],
      "year": 2023,
      "citation_count": 14,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/10d949dee482aeea1cab8b42c326d0dbf0505de3",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "eae107f7eeed756dfc996c47bc3faf381d36fd94",
      "title": "Fast and Continual Knowledge Graph Embedding via Incremental LoRA",
      "abstract": "Continual Knowledge Graph Embedding (CKGE) aims to efficiently learn new knowledge and simultaneously preserve old knowledge. Dominant approaches primarily focus on alleviating catastrophic forgetting of old knowledge but neglect efficient learning for the emergence of new knowledge. However, in real-world scenarios, knowledge graphs (KGs) are continuously growing, which brings a significant challenge to fine-tuning KGE models efficiently. To address this issue, we propose a fast CKGE framework (FastKGE), incorporating an incremental low-rank adapter (IncLoRA) mechanism to efficiently acquire new knowledge while preserving old knowledge. Specifically, to mitigate catastrophic forgetting, FastKGE isolates and allocates new knowledge to specific layers based on the fine-grained influence between old and new KGs. Subsequently, to accelerate fine-tuning, FastKGE devises an efficient IncLoRA mechanism, which embeds the specific layers into incremental low-rank adapters with fewer training parameters. Moreover, IncLoRA introduces adaptive rank allocation, which makes the LoRA aware of the importance of entities and adjusts its rank scale adaptively. We conduct experiments on four public datasets and two new datasets with a larger initial scale. Experimental results demonstrate that FastKGE can reduce training time by 34%-49% while still achieving competitive link prediction performance against state-of-the-art models on four public datasets (average MRR score of 21.0% vs. 21.1%). Meanwhile, on two newly constructed datasets, FastKGE saves 51%-68% training time and improves link prediction performance by 1.5%.",
      "authors": [
        "Jiajun Liu",
        "Wenjun Ke",
        "Peng Wang",
        "Jiahao Wang",
        "Jinhua Gao",
        "Ziyu Shang",
        "Guozheng Li",
        "Zijie Xu",
        "Ke Ji",
        "Yining Li"
      ],
      "year": 2024,
      "citation_count": 11,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/eae107f7eeed756dfc996c47bc3faf381d36fd94",
      "pdf_link": "",
      "venue": "International Joint Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c64433657869ecdaaa7988a029eabfe774d3ac47",
      "title": "Contextualized Quaternion Embedding Towards Polysemy in Knowledge Graph for Link Prediction",
      "abstract": "To meet the challenge of incompleteness within Knowledge Graphs, Knowledge Graph Embedding (KGE) has emerged as the fundamental methodology for predicting the missing link (Link Prediction), by mapping entities and relations as low-dimensional vectors in continuous space. However, current KGE models often struggle with the polysemy issue, where entities exhibit different semantic characteristics depending on the relations in which they participate. Such limitation stems from weak interactions between entities and their relation contexts, leading to low expressiveness in modeling complex structures and resulting in inaccurate predictions. To address this, we propose Contextualized Quaternion Embedding (ConQuatE), a model that enhances the representation learning of entities across multiple semantic dimensions by leveraging quaternion rotation to capture diverse relational contexts. In specific, ConQuatE incorporates contextual cues from various connected relations to enrich the original entity representations. Notably, this is achieved through efficient vector transformations in quaternion space, without any extra information required other than original triples. Experimental results demonstrate that our model outperforms state-of-the-art models for Link Prediction on four widely recognized datasets: FB15k-237, WN18RR, FB15k, and WN18.",
      "authors": [
        "Jie Chen",
        "Yinlong Wang",
        "Shu Zhao",
        "Peng Zhou",
        "Yanping Zhang"
      ],
      "year": 2025,
      "citation_count": 3,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c64433657869ecdaaa7988a029eabfe774d3ac47",
      "pdf_link": "",
      "venue": "ACM Trans. Asian Low Resour. Lang. Inf. Process.",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "322aa32b2a409d2e135dbb14736d9aeb497f1c52",
      "title": "Improving Knowledge Graph Embedding Using Simple Constraints",
      "abstract": "Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Early works performed this task via simple models developed over KG triples. Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond triples. This paper, by contrast, investigates the potential of using very simple constraints to improve KG embedding. We examine non-negativity constraints on entity representations and approximate entailment constraints on relation representations. The former help to learn compact and interpretable representations for entities. The latter further encode regularities of logical entailment between relations into their distributed representations. These constraints impose prior beliefs upon the structure of the embedding space, without negative impacts on efficiency or scalability. Evaluation on WordNet, Freebase, and DBpedia shows that our approach is simple yet surprisingly effective, significantly and consistently outperforming competitive baselines. The constraints imposed indeed improve model interpretability, leading to a substantially increased structuring of the embedding space. Code and data are available at https://github.com/iieir-km/ComplEx-NNE_AER.",
      "authors": [
        "Boyang Ding",
        "Quan Wang",
        "Bin Wang",
        "Li Guo"
      ],
      "year": 2018,
      "citation_count": 134,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/322aa32b2a409d2e135dbb14736d9aeb497f1c52",
      "pdf_link": "",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "8fef3f8bb8bcd254898b5d24f3d78beab09e99d4",
      "title": "Understanding Negative Sampling in Knowledge Graph Embedding",
      "abstract": "Knowledge graph embedding (KGE) is to project entities and relations of a knowledge graph (KG) into a low-dimensional vector space, which has made steady progress in recent years. Conventional KGE methods, especially translational distance-based models, are trained through discriminating positive samples from negative ones. Most KGs store only positive samples for space efficiency. Negative sampling thus plays a crucial role in encoding triples of a KG. The quality of generated negative samples has a direct impact on the performance of learnt knowledge representation in a myriad of downstream tasks, such as recommendation, link prediction and node classification. We summarize current negative sampling approaches in KGE into three categories, static distribution-based, dynamic distribution-based and custom cluster-based respectively. Based on this categorization we discuss the most prevalent existing approaches and their characteristics. It is a hope that this review can provide some guidelines for new thoughts about negative sampling in KGE.",
      "authors": [
        "Jing Qian",
        "Gangmin Li",
        "Katie Atkinson",
        "Yong Yue"
      ],
      "year": 2021,
      "citation_count": 10,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/8fef3f8bb8bcd254898b5d24f3d78beab09e99d4",
      "pdf_link": "",
      "venue": "International Journal of Artificial Intelligence & Applications",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c180564160d0788a82df203f9e5f61380d9846aa",
      "title": "Weighted Knowledge Graph Embedding",
      "abstract": "Knowledge graph embedding (KGE) aims to project both entities and relations in a knowledge graph (KG) into low-dimensional vectors. Indeed, existing KGs suffer from the data imbalance issue, i.e., entities and relations conform to a long-tail distribution, only a small portion of entities and relations occur frequently, while the vast majority of entities and relations only have a few training samples. Existing KGE methods assign equal weights to each entity and relation during the training process. Under this setting, long-tail entities and relations are not fully trained during training, leading to unreliable representations. In this paper, we propose WeightE, which attends differentially to different entities and relations. Specifically, WeightE is able to endow lower weights to frequent entities and relations, and higher weights to infrequent ones. In such manner, WeightE is capable of increasing the weights of long-tail entities and relations, and learning better representations for them. In particular, WeightE tailors bilevel optimization for the KGE task, where the inner level aims to learn reliable entity and relation embeddings, and the outer level attempts to assign appropriate weights for each entity and relation. Moreover, it is worth noting that our technique of applying weights to different entities and relations is general and flexible, which can be applied to a number of existing KGE models. Finally, we extensively validate the superiority of WeightE against various state-of-the-art baselines.",
      "authors": [
        "Zhao Zhang",
        "Zhanpeng Guan",
        "Fuwei Zhang",
        "Fuzhen Zhuang",
        "Zhulin An",
        "Fei Wang",
        "Yongjun Xu"
      ],
      "year": 2023,
      "citation_count": 8,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c180564160d0788a82df203f9e5f61380d9846aa",
      "pdf_link": "",
      "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "efea0197c956e981e98c4d2532fa720c58954492",
      "title": "FSTRE: Fuzzy Spatiotemporal RDF Knowledge Graph Embedding Using Uncertain Dynamic Vector Projection and Rotation",
      "abstract": "Knowledge graphs (KGs) use resource description framework (RDF) triples to model various crisp and static resources in the world. Meanwhile, knowledge embedded into vector space can imply more meanings. Much real-world information, however, is often uncertain and dynamic. Existing KG embedding (KGE) models are insufficient to deal with uncertain dynamic knowledge in vector spaces. To overcome this drawback, this article concentrates on an embedding module for the distributed representation of uncertain dynamic knowledge and proposes a strongly adaptive fuzzy spatiotemporal RDF embedding model (FSTRE). Specifically, we first propose a fine-grained fuzzy spatiotemporal RDF model, which provides the underlying representation framework for FSTRE. Then, within the complex vector space, spatial and temporal information is embedded by projection and rotation, respectively. Fine-grained fuzziness penetrates each element of the spatiotemporal five-tuples by a modal length of the anisotropic vectors. By using geometric operations as its transformation operator, FSTRE can capture the rich interaction between crisp and static knowledge and fuzzy spatiotemporal knowledge. We performed an experimental evaluation of FSTRE based on the built fuzzy spatiotemporal KG. It was shown that our FSTRE model is superior to state-of-the-art methods and can handle complex fuzzy spatiotemporal knowledge.",
      "authors": [
        "Hao Ji",
        "Li Yan",
        "Z. Ma"
      ],
      "year": 2024,
      "citation_count": 10,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/efea0197c956e981e98c4d2532fa720c58954492",
      "pdf_link": "",
      "venue": "IEEE transactions on fuzzy systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "ecb80d1e5507e163be4a6757b00c8809a2de4863",
      "title": "Knowledge Graph Embedding Based on Multi-View Clustering Framework",
      "abstract": "Knowledge representation is one of the critical problems in knowledge engineering and artificial intelligence, while knowledge embedding as a knowledge representation methodology indicates entities and relations in knowledge graph as low-dimensional, continuous vectors. In this way, knowledge graph is compatible with numerical machine learning models. Major knowledge embedding methods employ geometric translation to design score function, which is weak-semantic for natural language processing. To overcome this disadvantage, in this paper, we propose our model based on multi-view clustering framework, which could generate semantic representations of knowledge elements (i.e., entities/relations). With our semantic model, we also present an empowered solution to entity retrieval with entity description. Extensive experiments show that our model achieves substantial improvements against baselines on the task of knowledge graph completion, triple classification, entity classification, and entity retrieval.",
      "authors": [
        "Han Xiao",
        "Yidong Chen",
        "X. Shi"
      ],
      "year": 2019,
      "citation_count": 41,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/ecb80d1e5507e163be4a6757b00c8809a2de4863",
      "pdf_link": "",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "c075a84356b529464df2e06a02bf9b524a815152",
      "title": "Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization",
      "abstract": "Recent advances in knowledge graph embedding (KGE) rely on Euclidean/hyperbolic orthogonal relation transformations to model intrinsic logical patterns and topological structures. However, existing approaches are confined to rigid relational orthogonalization with restricted dimension and homogeneous geometry, leading to deficient modeling capability. In this work, we move beyond these approaches in terms of both dimension and geometry by introducing a powerful framework named GoldE, which features a universal orthogonal parameterization based on a generalized form of Householder reflection. Such parameterization can naturally achieve dimensional extension and geometric unification with theoretical guarantees, enabling our framework to simultaneously capture crucial logical patterns and inherent topological heterogeneity of knowledge graphs. Empirically, GoldE achieves state-of-the-art performance on three standard benchmarks. Codes are available at https://github.com/xxrep/GoldE.",
      "authors": [
        "Rui Li",
        "Chaozhuo Li",
        "Yanming Shen",
        "Zeyu Zhang",
        "Xu Chen"
      ],
      "year": 2024,
      "citation_count": 4,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/c075a84356b529464df2e06a02bf9b524a815152",
      "pdf_link": "",
      "venue": "International Conference on Machine Learning",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "5b5b3face4be1cf131d0cb9c40ae5adcd0c16408",
      "title": "Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph",
      "abstract": "Federated Knowledge Graph Embedding (FKGE) has recently garnered considerable interest due to its capacity to extract expressive representations from distributed knowledge graphs, while concurrently safeguarding the privacy of individual clients. Existing FKGE methods typically harness the arithmetic mean of entity embeddings from all clients as the global supplementary knowledge, and learn a replica of global consensus entities embeddings for each client. However, these methods usually neglect the inherent semantic disparities among distinct clients. This oversight not only results in the globally shared complementary knowledge being inundated with too much noise when tailored to a specific client, but also instigates a discrepancy between local and global optimization objectives. Consequently, the quality of the learned embeddings is compromised. To address this, we propose Personalized Federated knowledge graph Embedding with client-wise relation Graph (PFedEG), a novel approach that employs a client-wise relation graph to learn personalized embeddings by discerning the semantic relevance of embeddings from other clients. Specifically, PFedEG learns personalized supplementary knowledge for each client by amalgamating entity embedding from its neighboring clients based on their\"affinity\"on the client-wise relation graph. Each client then conducts personalized embedding learning based on its local triples and personalized supplementary knowledge. We conduct extensive experiments on four benchmark datasets to evaluate our method against state-of-the-art models and results demonstrate the superiority of our method.",
      "authors": [
        "Xiaoxiong Zhang",
        "Zhiwei Zeng",
        "Xin Zhou",
        "D. Niyato",
        "Zhiqi Shen"
      ],
      "year": 2024,
      "citation_count": 5,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/5b5b3face4be1cf131d0cb9c40ae5adcd0c16408",
      "pdf_link": "",
      "venue": "Applied intelligence (Boston)",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "abea782b5d0bdb4cd90ec42f672711613e71e43e",
      "title": "Locally Adaptive Translation for Knowledge Graph Embedding",
      "abstract": "\n \n Knowledge graph embedding aims to represent entities and relations in a large-scale knowledge graph as elements in a continuous vector space. Existing methods, e.g., TransE and TransH, learn embedding representation by defining a global margin-based loss function over the data. However, the optimal loss function is determined during experiments whose parameters are examined among a closed set of candidates. Moreover, embeddings over two knowledge graphs with different entities and relations share the same set of candidate loss functions, ignoring the locality of both graphs. This leads to the limited performance of embedding related applications. In this paper, we propose a locally adaptive translation method for knowledge graph embedding, called TransA, to find the optimal loss function by adaptively determining its margin over different knowledge graphs. Experiments on two benchmark data sets demonstrate the superiority of the proposed method, as compared to the-state-of-the-art ones.\n \n",
      "authors": [
        "Yantao Jia",
        "Yuanzhuo Wang",
        "Hailun Lin",
        "Xiaolong Jin",
        "Xueqi Cheng"
      ],
      "year": 2015,
      "citation_count": 92,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/abea782b5d0bdb4cd90ec42f672711613e71e43e",
      "pdf_link": "",
      "venue": "AAAI Conference on Artificial Intelligence",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "9c510e24b5edc5720440b695d7bd0636b52f4f66",
      "title": "A Review of Knowledge Graph Embedding Methods of TransE, TransH and TransR for Missing Links",
      "abstract": "Knowledge representation and reasoning require knowledge graph embedding as it is crucial in the area. It involves mapping entities and relationships from a knowledge graph into vectors of lower dimensions that are continuous in nature. This encoding enables machine learning algorithms to effectively reason and make predictions on graph-structured data. This review article offers an overview and critical analysis specifically about the methods of knowledge graph embedding which are TransE, TransH, and TransR. The key concepts, methodologies, strengths, and limitations of these methods, along with examining their applications and experiments conducted by existing researchers have been studied. The motivation to conduct this study is to review the well-known and most applied knowledge embedding methods and compare the features of those methods so that a comprehensive resource for researchers and practitioners interested in delving into knowledge graph embedding techniques is delivered.",
      "authors": [
        "S. M. Asmara",
        "N. A. Sahabudin",
        "Nor Syahidatul Nadiah Ismail",
        "I. A. Sabri"
      ],
      "year": 2023,
      "citation_count": 6,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/9c510e24b5edc5720440b695d7bd0636b52f4f66",
      "pdf_link": "",
      "venue": "International Conference on Software Engineering and Computer Systems",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "83d58bc46b7adb92d8750da52313f060b10f201d",
      "title": "HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding",
      "abstract": "Knowledge Graph (KG) embedding has emerged as an active area of research resulting in the development of several KG embedding methods. Relational facts in KG often show temporal dynamics, e.g., the fact (Cristiano_Ronaldo, playsFor, Manchester_United) is valid only from 2003 to 2009. Most of the existing KG embedding methods ignore this temporal dimension while learning embeddings of the KG elements. In this paper, we propose HyTE, a temporally aware KG embedding method which explicitly incorporates time in the entity-relation space by associating each timestamp with a corresponding hyperplane. HyTE not only performs KG inference using temporal guidance, but also predicts temporal scopes for relational facts with missing time annotations. Through extensive experimentation on temporal datasets extracted from real-world KGs, we demonstrate the effectiveness of our model over both traditional as well as temporal KG embedding methods.",
      "authors": [
        "S. Dasgupta",
        "Swayambhu Nath Ray",
        "P. Talukdar"
      ],
      "year": 2018,
      "citation_count": 387,
      "layer": 1,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/83d58bc46b7adb92d8750da52313f060b10f201d",
      "pdf_link": "",
      "venue": "Conference on Empirical Methods in Natural Language Processing",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "2a25540e3ce0baba56ee71da7ca938f0264f790d",
      "title": "Biological applications of knowledge graph embedding models",
      "abstract": "Complex biological systems are traditionally modelled as graphs of interconnected biological entities. These graphs, i.e. biological knowledge graphs, are then processed using graph exploratory approaches to perform different types of analytical and predictive tasks. Despite the high predictive accuracy of these approaches, they have limited scalability due to their dependency on time-consuming path exploratory procedures. In recent years, owing to the rapid advances of computational technologies, new approaches for modelling graphs and mining them with high accuracy and scalability have emerged. These approaches, i.e. knowledge graph embedding (KGE) models, operate by learning low-rank vector representations of graph nodes and edges that preserve the graph's inherent structure. These approaches were used to analyse knowledge graphs from different domains where they showed superior performance and accuracy compared to previous graph exploratory approaches. In this work, we study this class of models in the context of biological knowledge graphs and their different applications. We then show how KGE models can be a natural fit for representing complex biological knowledge modelled as graphs. We also discuss their predictive and analytical capabilities in different biology applications. In this regard, we present two example case studies that demonstrate the capabilities of KGE models: prediction of drug-target interactions and polypharmacy side effects. Finally, we analyse different practical considerations for KGEs, and we discuss possible opportunities and challenges related to adopting them for modelling biological systems.",
      "authors": [
        "Sameh K. Mohamed",
        "A. Nounu",
        "V. Nováček"
      ],
      "year": 2020,
      "citation_count": 118,
      "layer": 2,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/2a25540e3ce0baba56ee71da7ca938f0264f790d",
      "pdf_link": "",
      "venue": "Briefings Bioinform.",
      "paper_type": "",
      "keywords": []
    },
    {
      "id": "40479fd70115e545d21c01853aad56e6922280ac",
      "title": "Integrating Entity Attributes for Error-Aware Knowledge Graph Embedding",
      "abstract": "Knowledge graphs (KGs) can structurally organize large-scale information in the form of triples and significantly support many real-world applications. While most KG embedding algorithms hold the assumption that all triples are correct, considerable errors were inevitably injected during the construction process. It is urgent to develop effective error-aware KG embedding, since errors in KGs would lead to significant performance degradation in downstream applications. To this end, we propose a novel framework named Attributed Error-aware Knowledge Embedding (AEKE). It leverages the semantics contained in entity attributes to guide the KG embedding model learning against the impact of erroneous triples. We design two triple-level hypergraphs to model the topological structures of the KG and its attributes, respectively. The confidence score of each triple is jointly calculated based on self-contradictory within the triple, consistency between local and global structures, and homogeneity between structures and attributes. We leverage confidence scores to adaptively update the weighted aggregation in the multi-view graph learning framework and margin loss in KG embedding, such that potential errors will contribute little to KG learning. Experiments on three real-world KGs demonstrate that AEKE outperforms state-of-the-art KG embedding and error detection algorithms.",
      "authors": [
        "Qinggang Zhang",
        "Junnan Dong",
        "Qiaoyu Tan",
        "Xiao Huang"
      ],
      "year": 2024,
      "citation_count": 20,
      "layer": 3,
      "new_direction": 0,
      "url": "https://www.semanticscholar.org/paper/40479fd70115e545d21c01853aad56e6922280ac",
      "pdf_link": "",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "paper_type": "",
      "keywords": []
    }
  ],
  "edges": [
    {
      "source": "991b64748dfeecf026a27030c16fe1743aa20167",
      "target": "eb14b24b329a6cc80747644616e15491ef49596f",
      "weight": 0.25424645515090283
    },
    {
      "source": "991b64748dfeecf026a27030c16fe1743aa20167",
      "target": "4801db5c5cb24a9069f2d264252fa26986ceefa9",
      "weight": 0.05085254044741015
    },
    {
      "source": "991b64748dfeecf026a27030c16fe1743aa20167",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.2652555000507747
    },
    {
      "source": "991b64748dfeecf026a27030c16fe1743aa20167",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.3131770387705501
    },
    {
      "source": "991b64748dfeecf026a27030c16fe1743aa20167",
      "target": "7e5f318bf5b9c986ca82d2d97e11f50d58ee6680",
      "weight": 0.24714479992375776
    },
    {
      "source": "991b64748dfeecf026a27030c16fe1743aa20167",
      "target": "e5c851867af5587466f7cd9c22f8b2c84f8c6b63",
      "weight": 0.2605833239570413
    },
    {
      "source": "991b64748dfeecf026a27030c16fe1743aa20167",
      "target": "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4",
      "weight": 0.2628967104364369
    },
    {
      "source": "991b64748dfeecf026a27030c16fe1743aa20167",
      "target": "e9a13a97b7266ac27dcd7117a99a4fcbadc5fd9c",
      "weight": 0.2554043762102771
    },
    {
      "source": "991b64748dfeecf026a27030c16fe1743aa20167",
      "target": "ecb80d1e5507e163be4a6757b00c8809a2de4863",
      "weight": 0.3492526193423934
    },
    {
      "source": "991b64748dfeecf026a27030c16fe1743aa20167",
      "target": "77dc07c92c37586f94a6f5ac3de103b218931578",
      "weight": 0.2641313907835404
    },
    {
      "source": "991b64748dfeecf026a27030c16fe1743aa20167",
      "target": "beade097ff41c62a8d8d29065be0e1339be39f30",
      "weight": 0.2391720559388711
    },
    {
      "source": "991b64748dfeecf026a27030c16fe1743aa20167",
      "target": "d3c287ff061f295ddf8dc3cb02a6f39e301cae3b",
      "weight": 0.2363799673825321
    },
    {
      "source": "991b64748dfeecf026a27030c16fe1743aa20167",
      "target": "d4220644ef94fa4c2e5138a619cfcd86508d2ea1",
      "weight": 0.2405740303618806
    },
    {
      "source": "991b64748dfeecf026a27030c16fe1743aa20167",
      "target": "e379f7c85441df5d8ddc1565cabf4b4290c22f1f",
      "weight": 0.35150900929889023
    },
    {
      "source": "991b64748dfeecf026a27030c16fe1743aa20167",
      "target": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
      "weight": 0.28435025206508435
    },
    {
      "source": "cab5194d13c1ce89a96322adaac754b2cb630d87",
      "target": "3f170af3566f055e758fa3bdf2bfd3a0e8787e58",
      "weight": 0.29841334069443276
    },
    {
      "source": "cab5194d13c1ce89a96322adaac754b2cb630d87",
      "target": "52eb7f27cdfbf359096b8b5ef56b2c2826beb660",
      "weight": 0.25975745730402755
    },
    {
      "source": "e39afdbd832bd8fd0fb4f4f7df3722dc5f5cab2a",
      "target": "933cb8bf1cd50d6d5833a627683327b15db28836",
      "weight": 0.2763534014273878
    },
    {
      "source": "e39afdbd832bd8fd0fb4f4f7df3722dc5f5cab2a",
      "target": "8fef3f8bb8bcd254898b5d24f3d78beab09e99d4",
      "weight": 0.258847908167691
    },
    {
      "source": "e39afdbd832bd8fd0fb4f4f7df3722dc5f5cab2a",
      "target": "acc855d74431537b98de5185e065e4eacbab7b26",
      "weight": 0.2457560890864685
    },
    {
      "source": "7e5f318bf5b9c986ca82d2d97e11f50d58ee6680",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.28662910318005697
    },
    {
      "source": "7e5f318bf5b9c986ca82d2d97e11f50d58ee6680",
      "target": "4085a5cf49c193fe3d3ff19ff2d696fe20a5a596",
      "weight": 0.2514966778008394
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "33f3f53c957c4a8832b1dcb095a4ac967bd89897",
      "weight": 0.24874617605982494
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "6a2f26cece133b0aa52843be0f149a65e78374f7",
      "weight": 0.26667031106060063
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "6205f75cb6db1503c94386441ca68c63c9cbd456",
      "weight": 0.238307487801331
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "5b5b3face4be1cf131d0cb9c40ae5adcd0c16408",
      "weight": 0.23978162635547198
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "c075a84356b529464df2e06a02bf9b524a815152",
      "weight": 0.2382874001632011
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "4801db5c5cb24a9069f2d264252fa26986ceefa9",
      "weight": 0.032667421267461666
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "bb3e135757bfb82c4de202c807c9e381caecb623",
      "weight": 0.24631978556082104
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.24833650029459137
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "9c510e24b5edc5720440b695d7bd0636b52f4f66",
      "weight": 0.30190406157352045
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "145fa4ea1567a6b9d981fdea0e183140d99aeb97",
      "weight": 0.21764229758298348
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "4085a5cf49c193fe3d3ff19ff2d696fe20a5a596",
      "weight": 0.24383995970961725
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "fda63b289d4c0c332f88975994114fb61b514ced",
      "weight": 0.22328424463616206
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.2503066765206887
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "933cb8bf1cd50d6d5833a627683327b15db28836",
      "weight": 0.26406585117560477
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "44ce738296c3148c6593324773706cdc228614d4",
      "weight": 0.3046127227614393
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "f470e11faa6200026cf39e248510070c078e509a",
      "weight": 0.26586763165893906
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "b2d2ad9a458bdcb0523d22be659eb013ca2d3c67",
      "weight": 0.3533871534473263
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "63836e669416668744c3676a831060e8de3f58a1",
      "weight": 0.272485403204486
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "8b717c4dfb309638307fcc7d2c798b1c20927a3e",
      "weight": 0.276786645056463
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "83a46afaeb520abcd9b0138507a253f6d4d8bff7",
      "weight": 0.25747713318715093
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "e5c851867af5587466f7cd9c22f8b2c84f8c6b63",
      "weight": 0.24519281326429024
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "29eb99518d16ccf8ac306d92f4a6377ae109d9be",
      "weight": 0.2388381654767487
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "1f20378d2820fdf1c1bb09ce22f739ab77b14e82",
      "weight": 0.23596582006820663
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "06315f8b2633a54b087c6094cdb281f01dd06482",
      "weight": 0.3660611356633835
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "d7ef14459674b75807cd9be549f1e12d53849ead",
      "weight": 0.2405471254385804
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "4e52607397a96fb2104a99c570c9cec29c9ca519",
      "weight": 0.06855598725397567
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "8fef3f8bb8bcd254898b5d24f3d78beab09e99d4",
      "weight": 0.22153033482071427
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "bcdb8914550df02bfe1f69348c9830d775f6590a",
      "weight": 0.2726343559431885
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "552bfaca30af29647c083993fbe406867fc70d4c",
      "weight": 0.28061965548252316
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "21f8ea62da6a4031d85a1ee701dbc3e6847fa6d3",
      "weight": 0.23969700221378226
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "33d469c6d9fc09b59522d91b7696b15dc60a9a93",
      "weight": 0.2646969380116414
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "1620a20881b572b5ffc6f9cb3cf39f6090cee19f",
      "weight": 0.27322378566064526
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "acc855d74431537b98de5185e065e4eacbab7b26",
      "weight": 0.24270298869080667
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "68f34ed64fdf07bb1325097c93576658e061231e",
      "weight": 0.2719743581477731
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "95c3d25b40f963eb248136555bd9b9e35817cc09",
      "weight": 0.33005199066471047
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "f4e39a4f8fd8f8453372b74fda17047b9860d870",
      "weight": 0.25740089866259996
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "e9a13a97b7266ac27dcd7117a99a4fcbadc5fd9c",
      "weight": 0.24002394068573196
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "58e1b93b18370433633152cb8825917edc2f16a6",
      "weight": 0.2842107208898936
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "c2c6edc5750a438bddd1217481832d38df6336de",
      "weight": 0.2608492037477704
    },
    {
      "source": "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "target": "2e925a02db26a60ee1cc022f3923e09f3fae7b39",
      "weight": 0.26160398706783944
    },
    {
      "source": "e379f7c85441df5d8ddc1565cabf4b4290c22f1f",
      "target": "354fb91810c6d3756600c99ad84d2e6ef4136021",
      "weight": 0.32167171975572645
    },
    {
      "source": "e379f7c85441df5d8ddc1565cabf4b4290c22f1f",
      "target": "a166957ec488cd20e61360d630568b3b81af3397",
      "weight": 0.28380137909817726
    },
    {
      "source": "e379f7c85441df5d8ddc1565cabf4b4290c22f1f",
      "target": "bcdb8914550df02bfe1f69348c9830d775f6590a",
      "weight": 0.2659540255321348
    },
    {
      "source": "e379f7c85441df5d8ddc1565cabf4b4290c22f1f",
      "target": "a905a690ec350b1aeb5fcfd7f2ff0f5e1663b3a0",
      "weight": 0.2769675576874428
    },
    {
      "source": "e379f7c85441df5d8ddc1565cabf4b4290c22f1f",
      "target": "68f34ed64fdf07bb1325097c93576658e061231e",
      "weight": 0.3836588911608553
    },
    {
      "source": "e379f7c85441df5d8ddc1565cabf4b4290c22f1f",
      "target": "ecb80d1e5507e163be4a6757b00c8809a2de4863",
      "weight": 0.4938792424765124
    },
    {
      "source": "e379f7c85441df5d8ddc1565cabf4b4290c22f1f",
      "target": "7572aefcd241ec76341addcb2e2e417587cb2e4c",
      "weight": 0.2882362469598732
    },
    {
      "source": "e379f7c85441df5d8ddc1565cabf4b4290c22f1f",
      "target": "322aa32b2a409d2e135dbb14736d9aeb497f1c52",
      "weight": 0.2683196923803404
    },
    {
      "source": "e379f7c85441df5d8ddc1565cabf4b4290c22f1f",
      "target": "405a7a7464cfe175333d6f04703ac272e00a85b4",
      "weight": 0.2768416942294473
    },
    {
      "source": "e379f7c85441df5d8ddc1565cabf4b4290c22f1f",
      "target": "86ac98157da100a529ca65fe6e1da064b0a651e8",
      "weight": 0.2795522925070324
    },
    {
      "source": "d4220644ef94fa4c2e5138a619cfcd86508d2ea1",
      "target": "4801db5c5cb24a9069f2d264252fa26986ceefa9",
      "weight": 0.11805374136627708
    },
    {
      "source": "d4220644ef94fa4c2e5138a619cfcd86508d2ea1",
      "target": "8fef3f8bb8bcd254898b5d24f3d78beab09e99d4",
      "weight": 0.3881575549022566
    },
    {
      "source": "f4e39a4f8fd8f8453372b74fda17047b9860d870",
      "target": "040fe47af8f4870bf681f34861c42b3ea46d76cf",
      "weight": 0.3638125962452927
    },
    {
      "source": "69418ff5d4eac106c72130e152b807004e2b979c",
      "target": "06315f8b2633a54b087c6094cdb281f01dd06482",
      "weight": 0.34173455975818917
    },
    {
      "source": "69418ff5d4eac106c72130e152b807004e2b979c",
      "target": "8fef3f8bb8bcd254898b5d24f3d78beab09e99d4",
      "weight": 0.2641218313370439
    },
    {
      "source": "69418ff5d4eac106c72130e152b807004e2b979c",
      "target": "a905a690ec350b1aeb5fcfd7f2ff0f5e1663b3a0",
      "weight": 0.269917868533725
    },
    {
      "source": "69418ff5d4eac106c72130e152b807004e2b979c",
      "target": "68f34ed64fdf07bb1325097c93576658e061231e",
      "weight": 0.33008918442941665
    },
    {
      "source": "69418ff5d4eac106c72130e152b807004e2b979c",
      "target": "ecb80d1e5507e163be4a6757b00c8809a2de4863",
      "weight": 0.3442419641424756
    },
    {
      "source": "69418ff5d4eac106c72130e152b807004e2b979c",
      "target": "322aa32b2a409d2e135dbb14736d9aeb497f1c52",
      "weight": 0.27865906061192514
    },
    {
      "source": "69418ff5d4eac106c72130e152b807004e2b979c",
      "target": "405a7a7464cfe175333d6f04703ac272e00a85b4",
      "weight": 0.26455133037519485
    },
    {
      "source": "69418ff5d4eac106c72130e152b807004e2b979c",
      "target": "e4e7bc893b6fb4ff8ebbff899be65d96d50ccd1d",
      "weight": 0.2854435666057808
    },
    {
      "source": "69418ff5d4eac106c72130e152b807004e2b979c",
      "target": "e379f7c85441df5d8ddc1565cabf4b4290c22f1f",
      "weight": 0.339999649490383
    },
    {
      "source": "69418ff5d4eac106c72130e152b807004e2b979c",
      "target": "991b64748dfeecf026a27030c16fe1743aa20167",
      "weight": 0.3050516118481146
    },
    {
      "source": "69418ff5d4eac106c72130e152b807004e2b979c",
      "target": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
      "weight": 0.2732744316769664
    },
    {
      "source": "69418ff5d4eac106c72130e152b807004e2b979c",
      "target": "d1a525c16a53b94200029df1037f2c9c7c244d7b",
      "weight": 0.2897966497694927
    },
    {
      "source": "21f8ea62da6a4031d85a1ee701dbc3e6847fa6d3",
      "target": "3ac716ac5d47d4420010678fda766ebb5b882ba9",
      "weight": 0.23065625434187254
    },
    {
      "source": "21f8ea62da6a4031d85a1ee701dbc3e6847fa6d3",
      "target": "10d949dee482aeea1cab8b42c326d0dbf0505de3",
      "weight": 0.24083698599074785
    },
    {
      "source": "21f8ea62da6a4031d85a1ee701dbc3e6847fa6d3",
      "target": "8b717c4dfb309638307fcc7d2c798b1c20927a3e",
      "weight": 0.2590631099686988
    },
    {
      "source": "3e3a84bbceba79843ca1105939b2eb438c149e9e",
      "target": "4801db5c5cb24a9069f2d264252fa26986ceefa9",
      "weight": 0.04939244425779427
    },
    {
      "source": "3e3a84bbceba79843ca1105939b2eb438c149e9e",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.29196393505910967
    },
    {
      "source": "3e3a84bbceba79843ca1105939b2eb438c149e9e",
      "target": "83a46afaeb520abcd9b0138507a253f6d4d8bff7",
      "weight": 0.30109944099435315
    },
    {
      "source": "3e3a84bbceba79843ca1105939b2eb438c149e9e",
      "target": "a905a690ec350b1aeb5fcfd7f2ff0f5e1663b3a0",
      "weight": 0.27404511157128575
    },
    {
      "source": "3e3a84bbceba79843ca1105939b2eb438c149e9e",
      "target": "1620a20881b572b5ffc6f9cb3cf39f6090cee19f",
      "weight": 0.32315545599969336
    },
    {
      "source": "e03b8e02ddda86eafb54cafc5c44d231992be95a",
      "target": "29052ddd048acb1afa2c42613068b63bb7428a34",
      "weight": 0.29306374202133273
    },
    {
      "source": "e03b8e02ddda86eafb54cafc5c44d231992be95a",
      "target": "f470e11faa6200026cf39e248510070c078e509a",
      "weight": 0.2958116877058049
    },
    {
      "source": "f44ee7932aacd054101b00f37d4c26c27630c557",
      "target": "3ac716ac5d47d4420010678fda766ebb5b882ba9",
      "weight": 0.24429758721019612
    },
    {
      "source": "f44ee7932aacd054101b00f37d4c26c27630c557",
      "target": "52eb7f27cdfbf359096b8b5ef56b2c2826beb660",
      "weight": 0.2686330668433962
    },
    {
      "source": "f44ee7932aacd054101b00f37d4c26c27630c557",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.28956408587672205
    },
    {
      "source": "f44ee7932aacd054101b00f37d4c26c27630c557",
      "target": "d605a7628b2a7ff8ce04fc27111626e2d734cab4",
      "weight": 0.2610757592260391
    },
    {
      "source": "f44ee7932aacd054101b00f37d4c26c27630c557",
      "target": "44ce738296c3148c6593324773706cdc228614d4",
      "weight": 0.2460291910152943
    },
    {
      "source": "1620a20881b572b5ffc6f9cb3cf39f6090cee19f",
      "target": "bb3e135757bfb82c4de202c807c9e381caecb623",
      "weight": 0.30414795080168244
    },
    {
      "source": "1620a20881b572b5ffc6f9cb3cf39f6090cee19f",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.24796547996674445
    },
    {
      "source": "1620a20881b572b5ffc6f9cb3cf39f6090cee19f",
      "target": "29eb99518d16ccf8ac306d92f4a6377ae109d9be",
      "weight": 0.24687982741538594
    },
    {
      "source": "1620a20881b572b5ffc6f9cb3cf39f6090cee19f",
      "target": "b3f0cdc217a3d192d2671e44913542903c94105b",
      "weight": 0.365782555349044
    },
    {
      "source": "44ce738296c3148c6593324773706cdc228614d4",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.3970250121080662
    },
    {
      "source": "44ce738296c3148c6593324773706cdc228614d4",
      "target": "4085a5cf49c193fe3d3ff19ff2d696fe20a5a596",
      "weight": 0.5380384652964441
    },
    {
      "source": "44ce738296c3148c6593324773706cdc228614d4",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.2697247966114037
    },
    {
      "source": "c762e198b0239313ee50476021b1939390c4ef9d",
      "target": "e9a13a97b7266ac27dcd7117a99a4fcbadc5fd9c",
      "weight": 0.32461825686159906
    },
    {
      "source": "c762e198b0239313ee50476021b1939390c4ef9d",
      "target": "7572aefcd241ec76341addcb2e2e417587cb2e4c",
      "weight": 0.29192656800361916
    },
    {
      "source": "c762e198b0239313ee50476021b1939390c4ef9d",
      "target": "322aa32b2a409d2e135dbb14736d9aeb497f1c52",
      "weight": 0.23920923178168998
    },
    {
      "source": "c762e198b0239313ee50476021b1939390c4ef9d",
      "target": "991b64748dfeecf026a27030c16fe1743aa20167",
      "weight": 0.28377130382929716
    },
    {
      "source": "3f0d5aa7a637d2c0bb3d768c99cc203430b4481e",
      "target": "10d949dee482aeea1cab8b42c326d0dbf0505de3",
      "weight": 0.2982424367491977
    },
    {
      "source": "3f0d5aa7a637d2c0bb3d768c99cc203430b4481e",
      "target": "658702b2fa647ae7eaf1255058105da9eefe6f52",
      "weight": 0.2637946238385208
    },
    {
      "source": "beade097ff41c62a8d8d29065be0e1339be39f30",
      "target": "4801db5c5cb24a9069f2d264252fa26986ceefa9",
      "weight": 0.1554032917654959
    },
    {
      "source": "beade097ff41c62a8d8d29065be0e1339be39f30",
      "target": "5dc88d795cbcd01e6e99ba673e91e9024f0c3318",
      "weight": 0.38121937762994573
    },
    {
      "source": "beade097ff41c62a8d8d29065be0e1339be39f30",
      "target": "bbb89d88ad5b8279709ff089d3c00cd2750cd26b",
      "weight": 0.531029546625406
    },
    {
      "source": "beade097ff41c62a8d8d29065be0e1339be39f30",
      "target": "8fef3f8bb8bcd254898b5d24f3d78beab09e99d4",
      "weight": 0.48418198245876387
    },
    {
      "source": "beade097ff41c62a8d8d29065be0e1339be39f30",
      "target": "95c3d25b40f963eb248136555bd9b9e35817cc09",
      "weight": 0.26997810213795215
    },
    {
      "source": "beade097ff41c62a8d8d29065be0e1339be39f30",
      "target": "b594b21557395c6a8fa8356249373f8e318c2df2",
      "weight": 0.29184514773844367
    },
    {
      "source": "b594b21557395c6a8fa8356249373f8e318c2df2",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.2710956024449865
    },
    {
      "source": "b594b21557395c6a8fa8356249373f8e318c2df2",
      "target": "c180564160d0788a82df203f9e5f61380d9846aa",
      "weight": 0.24802770424064502
    },
    {
      "source": "b594b21557395c6a8fa8356249373f8e318c2df2",
      "target": "040fe47af8f4870bf681f34861c42b3ea46d76cf",
      "weight": 0.3559371417110722
    },
    {
      "source": "b594b21557395c6a8fa8356249373f8e318c2df2",
      "target": "4085a5cf49c193fe3d3ff19ff2d696fe20a5a596",
      "weight": 0.2475878992399597
    },
    {
      "source": "b594b21557395c6a8fa8356249373f8e318c2df2",
      "target": "44ce738296c3148c6593324773706cdc228614d4",
      "weight": 0.2422833586916405
    },
    {
      "source": "b594b21557395c6a8fa8356249373f8e318c2df2",
      "target": "b2d2ad9a458bdcb0523d22be659eb013ca2d3c67",
      "weight": 0.25309173616729574
    },
    {
      "source": "b594b21557395c6a8fa8356249373f8e318c2df2",
      "target": "8c93f3cecf79bd9f8d021f589d095305e281dd2f",
      "weight": 0.2590255109514908
    },
    {
      "source": "990334cf76845e2da64d3baa10b0a671e433d4b6",
      "target": "33f3f53c957c4a8832b1dcb095a4ac967bd89897",
      "weight": 0.2581010551170816
    },
    {
      "source": "990334cf76845e2da64d3baa10b0a671e433d4b6",
      "target": "3f170af3566f055e758fa3bdf2bfd3a0e8787e58",
      "weight": 0.3053395811481123
    },
    {
      "source": "990334cf76845e2da64d3baa10b0a671e433d4b6",
      "target": "7029ecb5d5fc04f54e1e25e739db2e993fb147c8",
      "weight": 0.31621215194664065
    },
    {
      "source": "990334cf76845e2da64d3baa10b0a671e433d4b6",
      "target": "4801db5c5cb24a9069f2d264252fa26986ceefa9",
      "weight": 0.05148464881413549
    },
    {
      "source": "990334cf76845e2da64d3baa10b0a671e433d4b6",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.264586334880461
    },
    {
      "source": "990334cf76845e2da64d3baa10b0a671e433d4b6",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.25915002771758333
    },
    {
      "source": "990334cf76845e2da64d3baa10b0a671e433d4b6",
      "target": "e5c851867af5587466f7cd9c22f8b2c84f8c6b63",
      "weight": 0.26265731347277993
    },
    {
      "source": "990334cf76845e2da64d3baa10b0a671e433d4b6",
      "target": "f4e39a4f8fd8f8453372b74fda17047b9860d870",
      "weight": 0.24315970737194215
    },
    {
      "source": "990334cf76845e2da64d3baa10b0a671e433d4b6",
      "target": "8c93f3cecf79bd9f8d021f589d095305e281dd2f",
      "weight": 0.24595052859965755
    },
    {
      "source": "990334cf76845e2da64d3baa10b0a671e433d4b6",
      "target": "19a672bdf29367b7509586a4be27c6843af903b1",
      "weight": 0.2509146535288317
    },
    {
      "source": "990334cf76845e2da64d3baa10b0a671e433d4b6",
      "target": "2e925a02db26a60ee1cc022f3923e09f3fae7b39",
      "weight": 0.24960387416653834
    },
    {
      "source": "990334cf76845e2da64d3baa10b0a671e433d4b6",
      "target": "8f096071a09701012c9c279aee2a88143a295935",
      "weight": 0.3197882769910867
    },
    {
      "source": "58e1b93b18370433633152cb8825917edc2f16a6",
      "target": "4e52607397a96fb2104a99c570c9cec29c9ca519",
      "weight": 0.346588692324046
    },
    {
      "source": "58e1b93b18370433633152cb8825917edc2f16a6",
      "target": "552bfaca30af29647c083993fbe406867fc70d4c",
      "weight": 0.5298531238877588
    },
    {
      "source": "58e1b93b18370433633152cb8825917edc2f16a6",
      "target": "b3f0cdc217a3d192d2671e44913542903c94105b",
      "weight": 0.4214839247986502
    },
    {
      "source": "354fb91810c6d3756600c99ad84d2e6ef4136021",
      "target": "4801db5c5cb24a9069f2d264252fa26986ceefa9",
      "weight": 0.11756491190882094
    },
    {
      "source": "af051c87cecca64c2de4ad9110608f7579766653",
      "target": "52b167a90a10cde25309e40d7f6e6b5e14ec3261",
      "weight": 0.26793994851201064
    },
    {
      "source": "11e402c699bcb54d57da1a5fdbc57076d7255baf",
      "target": "52b167a90a10cde25309e40d7f6e6b5e14ec3261",
      "weight": 0.35829818802528735
    },
    {
      "source": "11e402c699bcb54d57da1a5fdbc57076d7255baf",
      "target": "f470e11faa6200026cf39e248510070c078e509a",
      "weight": 0.3111638130526341
    },
    {
      "source": "11e402c699bcb54d57da1a5fdbc57076d7255baf",
      "target": "84aa127dc5ca3080385439cb10edc50b5d2c04e4",
      "weight": 0.4583162724106743
    },
    {
      "source": "11e402c699bcb54d57da1a5fdbc57076d7255baf",
      "target": "af051c87cecca64c2de4ad9110608f7579766653",
      "weight": 0.36124895955445624
    },
    {
      "source": "19a672bdf29367b7509586a4be27c6843af903b1",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.23291589681274133
    },
    {
      "source": "b307e96f59fde63567cd0beb30c9e36d968fad8e",
      "target": "bcffbb40e7922d2a34e752f8faaa4fe99649e21a",
      "weight": 0.3536359705383138
    },
    {
      "source": "b307e96f59fde63567cd0beb30c9e36d968fad8e",
      "target": "eae107f7eeed756dfc996c47bc3faf381d36fd94",
      "weight": 0.2341639591914416
    },
    {
      "source": "b307e96f59fde63567cd0beb30c9e36d968fad8e",
      "target": "eb14b24b329a6cc80747644616e15491ef49596f",
      "weight": 0.2683098458387655
    },
    {
      "source": "b307e96f59fde63567cd0beb30c9e36d968fad8e",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.25042828429961744
    },
    {
      "source": "b307e96f59fde63567cd0beb30c9e36d968fad8e",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.2690702022699787
    },
    {
      "source": "e9a13a97b7266ac27dcd7117a99a4fcbadc5fd9c",
      "target": "06315f8b2633a54b087c6094cdb281f01dd06482",
      "weight": 0.3256870221681932
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "c64433657869ecdaaa7988a029eabfe774d3ac47",
      "weight": 0.3020071876099465
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "33f3f53c957c4a8832b1dcb095a4ac967bd89897",
      "weight": 0.23987083545519663
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "bcffbb40e7922d2a34e752f8faaa4fe99649e21a",
      "weight": 0.24208675118160228
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "6a2f26cece133b0aa52843be0f149a65e78374f7",
      "weight": 0.29104833005913744
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "52b167a90a10cde25309e40d7f6e6b5e14ec3261",
      "weight": 0.22716655292050897
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "6205f75cb6db1503c94386441ca68c63c9cbd456",
      "weight": 0.30427100050927697
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "ce7291c5cd919a97ced6369ca697db9849848688",
      "weight": 0.22328323585489557
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "15710515bae025372f298570267d234d4a3141cb",
      "weight": 0.27571899218975027
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "c075a84356b529464df2e06a02bf9b524a815152",
      "weight": 0.2211888937899971
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "6a86594566fc9fa2e92afb6f0229d63a45fe25e6",
      "weight": 0.2416785642351446
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "7029ecb5d5fc04f54e1e25e739db2e993fb147c8",
      "weight": 0.2683267966910162
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "40479fd70115e545d21c01853aad56e6922280ac",
      "weight": 0.23784228185895046
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "eb14b24b329a6cc80747644616e15491ef49596f",
      "weight": 0.23603357911131115
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "4801db5c5cb24a9069f2d264252fa26986ceefa9",
      "weight": 0.06904685499457007
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "bb3e135757bfb82c4de202c807c9e381caecb623",
      "weight": 0.24733342968577152
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.2591196973791742
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "23efe9b99b5f0e79d7dbd4e3bfcf1c2d8b23c1ff",
      "weight": 0.028383707011421677
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "9c510e24b5edc5720440b695d7bd0636b52f4f66",
      "weight": 0.3374377464332672
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "b1d807fc6b184d757ebdea67acd81132d8298ff6",
      "weight": 0.23838121286329697
    },
    {
      "source": "2a3f862199883ceff5e3c74126f0c80770653e05",
      "target": "354fb91810c6d3756600c99ad84d2e6ef4136021",
      "weight": 0.3094748193600325
    },
    {
      "source": "727183c5cff89a6f2c3b71167ae50c02ca2cacc4",
      "target": "ce7291c5cd919a97ced6369ca697db9849848688",
      "weight": 0.2843552620797866
    },
    {
      "source": "727183c5cff89a6f2c3b71167ae50c02ca2cacc4",
      "target": "15710515bae025372f298570267d234d4a3141cb",
      "weight": 0.23750293651155982
    },
    {
      "source": "727183c5cff89a6f2c3b71167ae50c02ca2cacc4",
      "target": "8b717c4dfb309638307fcc7d2c798b1c20927a3e",
      "weight": 0.2684396215054627
    },
    {
      "source": "405a7a7464cfe175333d6f04703ac272e00a85b4",
      "target": "40479fd70115e545d21c01853aad56e6922280ac",
      "weight": 0.332805275909577
    },
    {
      "source": "405a7a7464cfe175333d6f04703ac272e00a85b4",
      "target": "4085a5cf49c193fe3d3ff19ff2d696fe20a5a596",
      "weight": 0.2293730688093522
    },
    {
      "source": "405a7a7464cfe175333d6f04703ac272e00a85b4",
      "target": "191815e4109ee392b9120b61642c0e859fb662a1",
      "weight": 0.4579571386967395
    },
    {
      "source": "405a7a7464cfe175333d6f04703ac272e00a85b4",
      "target": "7e5f318bf5b9c986ca82d2d97e11f50d58ee6680",
      "weight": 0.2372296842961813
    },
    {
      "source": "405a7a7464cfe175333d6f04703ac272e00a85b4",
      "target": "0367603c0197ab48eeba29aa6af391584a5077c0",
      "weight": 0.2502928563453929
    },
    {
      "source": "405a7a7464cfe175333d6f04703ac272e00a85b4",
      "target": "8fef3f8bb8bcd254898b5d24f3d78beab09e99d4",
      "weight": 0.23585539661454968
    },
    {
      "source": "405a7a7464cfe175333d6f04703ac272e00a85b4",
      "target": "bcdb8914550df02bfe1f69348c9830d775f6590a",
      "weight": 0.2517275898454622
    },
    {
      "source": "405a7a7464cfe175333d6f04703ac272e00a85b4",
      "target": "a905a690ec350b1aeb5fcfd7f2ff0f5e1663b3a0",
      "weight": 0.5131065491706202
    },
    {
      "source": "405a7a7464cfe175333d6f04703ac272e00a85b4",
      "target": "8c93f3cecf79bd9f8d021f589d095305e281dd2f",
      "weight": 0.25377013132324866
    },
    {
      "source": "405a7a7464cfe175333d6f04703ac272e00a85b4",
      "target": "77dc07c92c37586f94a6f5ac3de103b218931578",
      "weight": 0.2417941200067197
    },
    {
      "source": "405a7a7464cfe175333d6f04703ac272e00a85b4",
      "target": "322aa32b2a409d2e135dbb14736d9aeb497f1c52",
      "weight": 0.3162852846518064
    },
    {
      "source": "405a7a7464cfe175333d6f04703ac272e00a85b4",
      "target": "83d58bc46b7adb92d8750da52313f060b10f201d",
      "weight": 0.2996279568801221
    },
    {
      "source": "0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f",
      "target": "3f170af3566f055e758fa3bdf2bfd3a0e8787e58",
      "weight": 0.31482442136147243
    },
    {
      "source": "0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f",
      "target": "6205f75cb6db1503c94386441ca68c63c9cbd456",
      "weight": 0.2294039090029207
    },
    {
      "source": "0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f",
      "target": "15710515bae025372f298570267d234d4a3141cb",
      "weight": 0.2962991927804293
    },
    {
      "source": "0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f",
      "target": "eb14b24b329a6cc80747644616e15491ef49596f",
      "weight": 0.25973675914679667
    },
    {
      "source": "8b717c4dfb309638307fcc7d2c798b1c20927a3e",
      "target": "ce7291c5cd919a97ced6369ca697db9849848688",
      "weight": 0.3780609070456483
    },
    {
      "source": "8b717c4dfb309638307fcc7d2c798b1c20927a3e",
      "target": "15710515bae025372f298570267d234d4a3141cb",
      "weight": 0.2796250310270385
    },
    {
      "source": "8b717c4dfb309638307fcc7d2c798b1c20927a3e",
      "target": "6a86594566fc9fa2e92afb6f0229d63a45fe25e6",
      "weight": 0.26798126628685903
    },
    {
      "source": "8b717c4dfb309638307fcc7d2c798b1c20927a3e",
      "target": "10d949dee482aeea1cab8b42c326d0dbf0505de3",
      "weight": 0.33291608248061444
    },
    {
      "source": "a6a735f8e218f772e5b9dac411fa4abea87fdb9c",
      "target": "33a7b7abf006d22de24c1471e6f6c93842a497b6",
      "weight": 0.23087613509971544
    },
    {
      "source": "a6a735f8e218f772e5b9dac411fa4abea87fdb9c",
      "target": "b1d807fc6b184d757ebdea67acd81132d8298ff6",
      "weight": 0.4026040138800704
    },
    {
      "source": "a6a735f8e218f772e5b9dac411fa4abea87fdb9c",
      "target": "145fa4ea1567a6b9d981fdea0e183140d99aeb97",
      "weight": 0.3739583561341215
    },
    {
      "source": "e4e7bc893b6fb4ff8ebbff899be65d96d50ccd1d",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.258720878307688
    },
    {
      "source": "8c93f3cecf79bd9f8d021f589d095305e281dd2f",
      "target": "15710515bae025372f298570267d234d4a3141cb",
      "weight": 0.25306871287635163
    },
    {
      "source": "8c93f3cecf79bd9f8d021f589d095305e281dd2f",
      "target": "7029ecb5d5fc04f54e1e25e739db2e993fb147c8",
      "weight": 0.2780226084538487
    },
    {
      "source": "8c93f3cecf79bd9f8d021f589d095305e281dd2f",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.31071811644292163
    },
    {
      "source": "8c93f3cecf79bd9f8d021f589d095305e281dd2f",
      "target": "c180564160d0788a82df203f9e5f61380d9846aa",
      "weight": 0.2530863799555542
    },
    {
      "source": "8c93f3cecf79bd9f8d021f589d095305e281dd2f",
      "target": "040fe47af8f4870bf681f34861c42b3ea46d76cf",
      "weight": 0.258200411030728
    },
    {
      "source": "8c93f3cecf79bd9f8d021f589d095305e281dd2f",
      "target": "fda63b289d4c0c332f88975994114fb61b514ced",
      "weight": 0.25394157361902814
    },
    {
      "source": "8c93f3cecf79bd9f8d021f589d095305e281dd2f",
      "target": "658702b2fa647ae7eaf1255058105da9eefe6f52",
      "weight": 0.23174139014241385
    },
    {
      "source": "8c93f3cecf79bd9f8d021f589d095305e281dd2f",
      "target": "5515fd5d14ac7b19806294119560a8c74f7fa4b2",
      "weight": 0.3339275127390873
    },
    {
      "source": "8c93f3cecf79bd9f8d021f589d095305e281dd2f",
      "target": "1f20378d2820fdf1c1bb09ce22f739ab77b14e82",
      "weight": 0.33728053538038516
    },
    {
      "source": "8c93f3cecf79bd9f8d021f589d095305e281dd2f",
      "target": "acc855d74431537b98de5185e065e4eacbab7b26",
      "weight": 0.22008410449874669
    },
    {
      "source": "ecc04e9285f016090697a1a8f9e96ce01e94e742",
      "target": "52b167a90a10cde25309e40d7f6e6b5e14ec3261",
      "weight": 0.34808732412235144
    },
    {
      "source": "ecc04e9285f016090697a1a8f9e96ce01e94e742",
      "target": "af051c87cecca64c2de4ad9110608f7579766653",
      "weight": 0.32393699513650276
    },
    {
      "source": "d899e434a7f2eecf33a90053df84cf32842fbca9",
      "target": "52b167a90a10cde25309e40d7f6e6b5e14ec3261",
      "weight": 0.4044062579551959
    },
    {
      "source": "d899e434a7f2eecf33a90053df84cf32842fbca9",
      "target": "33a7b7abf006d22de24c1471e6f6c93842a497b6",
      "weight": 0.24133029088950295
    },
    {
      "source": "d899e434a7f2eecf33a90053df84cf32842fbca9",
      "target": "4801db5c5cb24a9069f2d264252fa26986ceefa9",
      "weight": 0.07372315952484926
    },
    {
      "source": "d899e434a7f2eecf33a90053df84cf32842fbca9",
      "target": "29052ddd048acb1afa2c42613068b63bb7428a34",
      "weight": 0.29331824932034783
    },
    {
      "source": "d899e434a7f2eecf33a90053df84cf32842fbca9",
      "target": "f470e11faa6200026cf39e248510070c078e509a",
      "weight": 0.24992575527061703
    },
    {
      "source": "d899e434a7f2eecf33a90053df84cf32842fbca9",
      "target": "84aa127dc5ca3080385439cb10edc50b5d2c04e4",
      "weight": 0.4573255200850945
    },
    {
      "source": "d899e434a7f2eecf33a90053df84cf32842fbca9",
      "target": "af051c87cecca64c2de4ad9110608f7579766653",
      "weight": 0.32558988705670544
    },
    {
      "source": "d899e434a7f2eecf33a90053df84cf32842fbca9",
      "target": "95c3d25b40f963eb248136555bd9b9e35817cc09",
      "weight": 0.26576965347756276
    },
    {
      "source": "d899e434a7f2eecf33a90053df84cf32842fbca9",
      "target": "11e402c699bcb54d57da1a5fdbc57076d7255baf",
      "weight": 0.6438840725452295
    },
    {
      "source": "d899e434a7f2eecf33a90053df84cf32842fbca9",
      "target": "ecc04e9285f016090697a1a8f9e96ce01e94e742",
      "weight": 0.6521651571381267
    },
    {
      "source": "63836e669416668744c3676a831060e8de3f58a1",
      "target": "c64433657869ecdaaa7988a029eabfe774d3ac47",
      "weight": 0.29005739599474756
    },
    {
      "source": "63836e669416668744c3676a831060e8de3f58a1",
      "target": "3f170af3566f055e758fa3bdf2bfd3a0e8787e58",
      "weight": 0.2513910115747011
    },
    {
      "source": "63836e669416668744c3676a831060e8de3f58a1",
      "target": "7029ecb5d5fc04f54e1e25e739db2e993fb147c8",
      "weight": 0.2761653436309457
    },
    {
      "source": "63836e669416668744c3676a831060e8de3f58a1",
      "target": "398978c84ca8dab093d0b7fa73c6d380f5fa914c",
      "weight": 0.27761715654823255
    },
    {
      "source": "63836e669416668744c3676a831060e8de3f58a1",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.2621259013191505
    },
    {
      "source": "63836e669416668744c3676a831060e8de3f58a1",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.2862762818138107
    },
    {
      "source": "552bfaca30af29647c083993fbe406867fc70d4c",
      "target": "52eb7f27cdfbf359096b8b5ef56b2c2826beb660",
      "weight": 0.4519261797245216
    },
    {
      "source": "552bfaca30af29647c083993fbe406867fc70d4c",
      "target": "12cc4b65644a84a16ef7dfe7bdd70172cd38cffd",
      "weight": 0.24504441769300717
    },
    {
      "source": "552bfaca30af29647c083993fbe406867fc70d4c",
      "target": "780bc77fac1aaf460ba191daa218f3c111119092",
      "weight": 0.3684120744643672
    },
    {
      "source": "552bfaca30af29647c083993fbe406867fc70d4c",
      "target": "efea0197c956e981e98c4d2532fa720c58954492",
      "weight": 0.2667400751856804
    },
    {
      "source": "552bfaca30af29647c083993fbe406867fc70d4c",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.3026662798616889
    },
    {
      "source": "552bfaca30af29647c083993fbe406867fc70d4c",
      "target": "354fb91810c6d3756600c99ad84d2e6ef4136021",
      "weight": 0.27688613717567784
    },
    {
      "source": "552bfaca30af29647c083993fbe406867fc70d4c",
      "target": "4e52607397a96fb2104a99c570c9cec29c9ca519",
      "weight": 0.22639781105226953
    },
    {
      "source": "552bfaca30af29647c083993fbe406867fc70d4c",
      "target": "b3f0cdc217a3d192d2671e44913542903c94105b",
      "weight": 0.3826862532705062
    },
    {
      "source": "552bfaca30af29647c083993fbe406867fc70d4c",
      "target": "cab5194d13c1ce89a96322adaac754b2cb630d87",
      "weight": 0.35992963712836445
    },
    {
      "source": "2bd20cfec4ad3df0fd9cd87cef3eefe6f3847b83",
      "target": "658702b2fa647ae7eaf1255058105da9eefe6f52",
      "weight": 0.2722942303568272
    },
    {
      "source": "2bd20cfec4ad3df0fd9cd87cef3eefe6f3847b83",
      "target": "5515fd5d14ac7b19806294119560a8c74f7fa4b2",
      "weight": 0.28376732835798535
    },
    {
      "source": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
      "target": "eb14b24b329a6cc80747644616e15491ef49596f",
      "weight": 0.2546703227911508
    },
    {
      "source": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
      "target": "4801db5c5cb24a9069f2d264252fa26986ceefa9",
      "weight": 0.02695832020685435
    },
    {
      "source": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
      "target": "933cb8bf1cd50d6d5833a627683327b15db28836",
      "weight": 0.355051326330784
    },
    {
      "source": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
      "target": "f470e11faa6200026cf39e248510070c078e509a",
      "weight": 0.26643555069526886
    },
    {
      "source": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
      "target": "8fef3f8bb8bcd254898b5d24f3d78beab09e99d4",
      "weight": 0.2414550200871337
    },
    {
      "source": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
      "target": "bcdb8914550df02bfe1f69348c9830d775f6590a",
      "weight": 0.27740859664535356
    },
    {
      "source": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
      "target": "68f34ed64fdf07bb1325097c93576658e061231e",
      "weight": 0.29306453546650496
    },
    {
      "source": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
      "target": "2e925a02db26a60ee1cc022f3923e09f3fae7b39",
      "weight": 0.3068741414464411
    },
    {
      "source": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
      "target": "ecb80d1e5507e163be4a6757b00c8809a2de4863",
      "weight": 0.3790319614688389
    },
    {
      "source": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
      "target": "d3c287ff061f295ddf8dc3cb02a6f39e301cae3b",
      "weight": 0.26493308557472306
    },
    {
      "source": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
      "target": "322aa32b2a409d2e135dbb14736d9aeb497f1c52",
      "weight": 0.27152785635993126
    },
    {
      "source": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
      "target": "990334cf76845e2da64d3baa10b0a671e433d4b6",
      "weight": 0.09698012735169305
    },
    {
      "source": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
      "target": "e379f7c85441df5d8ddc1565cabf4b4290c22f1f",
      "weight": 0.39256951419637853
    },
    {
      "source": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
      "target": "86ac98157da100a529ca65fe6e1da064b0a651e8",
      "weight": 0.2995293178725007
    },
    {
      "source": "b2d2ad9a458bdcb0523d22be659eb013ca2d3c67",
      "target": "c64433657869ecdaaa7988a029eabfe774d3ac47",
      "weight": 0.32712521587790966
    },
    {
      "source": "b2d2ad9a458bdcb0523d22be659eb013ca2d3c67",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.3170609911044818
    },
    {
      "source": "0367603c0197ab48eeba29aa6af391584a5077c0",
      "target": "40479fd70115e545d21c01853aad56e6922280ac",
      "weight": 0.36450939923077275
    },
    {
      "source": "0367603c0197ab48eeba29aa6af391584a5077c0",
      "target": "c180564160d0788a82df203f9e5f61380d9846aa",
      "weight": 0.3383298197821253
    },
    {
      "source": "780bc77fac1aaf460ba191daa218f3c111119092",
      "target": "52eb7f27cdfbf359096b8b5ef56b2c2826beb660",
      "weight": 0.725101612079176
    },
    {
      "source": "77dc07c92c37586f94a6f5ac3de103b218931578",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.262702525258258
    },
    {
      "source": "c2c6edc5750a438bddd1217481832d38df6336de",
      "target": "c64433657869ecdaaa7988a029eabfe774d3ac47",
      "weight": 0.3360955755157428
    },
    {
      "source": "c2c6edc5750a438bddd1217481832d38df6336de",
      "target": "398978c84ca8dab093d0b7fa73c6d380f5fa914c",
      "weight": 0.28661442434202977
    },
    {
      "source": "c2c6edc5750a438bddd1217481832d38df6336de",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.32240454595099116
    },
    {
      "source": "c2c6edc5750a438bddd1217481832d38df6336de",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.24766821022524416
    },
    {
      "source": "c2c6edc5750a438bddd1217481832d38df6336de",
      "target": "83a46afaeb520abcd9b0138507a253f6d4d8bff7",
      "weight": 0.30760932198383395
    },
    {
      "source": "c2c6edc5750a438bddd1217481832d38df6336de",
      "target": "d7ef14459674b75807cd9be549f1e12d53849ead",
      "weight": 0.248034126254158
    },
    {
      "source": "29052ddd048acb1afa2c42613068b63bb7428a34",
      "target": "3f170af3566f055e758fa3bdf2bfd3a0e8787e58",
      "weight": 0.3263621543397076
    },
    {
      "source": "29052ddd048acb1afa2c42613068b63bb7428a34",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.22332748381320086
    },
    {
      "source": "06315f8b2633a54b087c6094cdb281f01dd06482",
      "target": "9c510e24b5edc5720440b695d7bd0636b52f4f66",
      "weight": 0.26901116760132493
    },
    {
      "source": "5515fd5d14ac7b19806294119560a8c74f7fa4b2",
      "target": "6205f75cb6db1503c94386441ca68c63c9cbd456",
      "weight": 0.3065705789429809
    },
    {
      "source": "5515fd5d14ac7b19806294119560a8c74f7fa4b2",
      "target": "33a7b7abf006d22de24c1471e6f6c93842a497b6",
      "weight": 0.2997829043405809
    },
    {
      "source": "4085a5cf49c193fe3d3ff19ff2d696fe20a5a596",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.44283321818466936
    },
    {
      "source": "4085a5cf49c193fe3d3ff19ff2d696fe20a5a596",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.30115997847373444
    },
    {
      "source": "95c3d25b40f963eb248136555bd9b9e35817cc09",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.29129329269154997
    },
    {
      "source": "95c3d25b40f963eb248136555bd9b9e35817cc09",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.3183491239386469
    },
    {
      "source": "95c3d25b40f963eb248136555bd9b9e35817cc09",
      "target": "44ce738296c3148c6593324773706cdc228614d4",
      "weight": 0.2700841892521262
    },
    {
      "source": "2e925a02db26a60ee1cc022f3923e09f3fae7b39",
      "target": "3f170af3566f055e758fa3bdf2bfd3a0e8787e58",
      "weight": 0.3546050618724099
    },
    {
      "source": "2e925a02db26a60ee1cc022f3923e09f3fae7b39",
      "target": "b1d807fc6b184d757ebdea67acd81132d8298ff6",
      "weight": 0.3355029950221071
    },
    {
      "source": "2e925a02db26a60ee1cc022f3923e09f3fae7b39",
      "target": "29052ddd048acb1afa2c42613068b63bb7428a34",
      "weight": 0.26899213903400665
    },
    {
      "source": "2e925a02db26a60ee1cc022f3923e09f3fae7b39",
      "target": "29eb99518d16ccf8ac306d92f4a6377ae109d9be",
      "weight": 0.2738504000792026
    },
    {
      "source": "2e925a02db26a60ee1cc022f3923e09f3fae7b39",
      "target": "bcdb8914550df02bfe1f69348c9830d775f6590a",
      "weight": 0.2557248707698821
    },
    {
      "source": "2e925a02db26a60ee1cc022f3923e09f3fae7b39",
      "target": "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4",
      "weight": 0.3212615587998787
    },
    {
      "source": "2e925a02db26a60ee1cc022f3923e09f3fae7b39",
      "target": "a905a690ec350b1aeb5fcfd7f2ff0f5e1663b3a0",
      "weight": 0.2768243353941314
    },
    {
      "source": "2e925a02db26a60ee1cc022f3923e09f3fae7b39",
      "target": "1620a20881b572b5ffc6f9cb3cf39f6090cee19f",
      "weight": 0.29234649945420255
    },
    {
      "source": "86ac98157da100a529ca65fe6e1da064b0a651e8",
      "target": "c180564160d0788a82df203f9e5f61380d9846aa",
      "weight": 0.35947906490954235
    },
    {
      "source": "86ac98157da100a529ca65fe6e1da064b0a651e8",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.3093980166173601
    },
    {
      "source": "86ac98157da100a529ca65fe6e1da064b0a651e8",
      "target": "933cb8bf1cd50d6d5833a627683327b15db28836",
      "weight": 0.2695211552901678
    },
    {
      "source": "86ac98157da100a529ca65fe6e1da064b0a651e8",
      "target": "0367603c0197ab48eeba29aa6af391584a5077c0",
      "weight": 0.42705257897741355
    },
    {
      "source": "86ac98157da100a529ca65fe6e1da064b0a651e8",
      "target": "06315f8b2633a54b087c6094cdb281f01dd06482",
      "weight": 0.2992133974633536
    },
    {
      "source": "86ac98157da100a529ca65fe6e1da064b0a651e8",
      "target": "19a672bdf29367b7509586a4be27c6843af903b1",
      "weight": 0.247114310141689
    },
    {
      "source": "d7ef14459674b75807cd9be549f1e12d53849ead",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.2558048075143851
    },
    {
      "source": "d7ef14459674b75807cd9be549f1e12d53849ead",
      "target": "658702b2fa647ae7eaf1255058105da9eefe6f52",
      "weight": 0.2509434704933313
    },
    {
      "source": "d7ef14459674b75807cd9be549f1e12d53849ead",
      "target": "0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f",
      "weight": 0.2716412606367112
    },
    {
      "source": "d1a525c16a53b94200029df1037f2c9c7c244d7b",
      "target": "4801db5c5cb24a9069f2d264252fa26986ceefa9",
      "weight": 0.06428145039599678
    },
    {
      "source": "d1a525c16a53b94200029df1037f2c9c7c244d7b",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.2643313424025866
    },
    {
      "source": "d1a525c16a53b94200029df1037f2c9c7c244d7b",
      "target": "f470e11faa6200026cf39e248510070c078e509a",
      "weight": 0.2753392501564677
    },
    {
      "source": "d1a525c16a53b94200029df1037f2c9c7c244d7b",
      "target": "63836e669416668744c3676a831060e8de3f58a1",
      "weight": 0.2317119050238084
    },
    {
      "source": "d1a525c16a53b94200029df1037f2c9c7c244d7b",
      "target": "83a46afaeb520abcd9b0138507a253f6d4d8bff7",
      "weight": 0.2550480740381287
    },
    {
      "source": "d1a525c16a53b94200029df1037f2c9c7c244d7b",
      "target": "68f34ed64fdf07bb1325097c93576658e061231e",
      "weight": 0.29730139233362973
    },
    {
      "source": "d1a525c16a53b94200029df1037f2c9c7c244d7b",
      "target": "ecb80d1e5507e163be4a6757b00c8809a2de4863",
      "weight": 0.38780000083980615
    },
    {
      "source": "d1a525c16a53b94200029df1037f2c9c7c244d7b",
      "target": "e379f7c85441df5d8ddc1565cabf4b4290c22f1f",
      "weight": 0.30313091929829356
    },
    {
      "source": "d1a525c16a53b94200029df1037f2c9c7c244d7b",
      "target": "e39afdbd832bd8fd0fb4f4f7df3722dc5f5cab2a",
      "weight": 0.281083000810226
    },
    {
      "source": "7572aefcd241ec76341addcb2e2e417587cb2e4c",
      "target": "40479fd70115e545d21c01853aad56e6922280ac",
      "weight": 0.3250550078837972
    },
    {
      "source": "7572aefcd241ec76341addcb2e2e417587cb2e4c",
      "target": "4801db5c5cb24a9069f2d264252fa26986ceefa9",
      "weight": 0.06274564328956832
    },
    {
      "source": "7572aefcd241ec76341addcb2e2e417587cb2e4c",
      "target": "398978c84ca8dab093d0b7fa73c6d380f5fa914c",
      "weight": 0.22118563492307425
    },
    {
      "source": "7572aefcd241ec76341addcb2e2e417587cb2e4c",
      "target": "23efe9b99b5f0e79d7dbd4e3bfcf1c2d8b23c1ff",
      "weight": 0.1891493468559792
    },
    {
      "source": "7572aefcd241ec76341addcb2e2e417587cb2e4c",
      "target": "d605a7628b2a7ff8ce04fc27111626e2d734cab4",
      "weight": 0.2140497243490116
    },
    {
      "source": "7572aefcd241ec76341addcb2e2e417587cb2e4c",
      "target": "b2d2ad9a458bdcb0523d22be659eb013ca2d3c67",
      "weight": 0.3527137296847629
    },
    {
      "source": "7572aefcd241ec76341addcb2e2e417587cb2e4c",
      "target": "a166957ec488cd20e61360d630568b3b81af3397",
      "weight": 0.2440931443847929
    },
    {
      "source": "7572aefcd241ec76341addcb2e2e417587cb2e4c",
      "target": "3f0d5aa7a637d2c0bb3d768c99cc203430b4481e",
      "weight": 0.25048085500110606
    },
    {
      "source": "7572aefcd241ec76341addcb2e2e417587cb2e4c",
      "target": "bbb89d88ad5b8279709ff089d3c00cd2750cd26b",
      "weight": 0.3065568540719531
    },
    {
      "source": "7572aefcd241ec76341addcb2e2e417587cb2e4c",
      "target": "8fef3f8bb8bcd254898b5d24f3d78beab09e99d4",
      "weight": 0.25526875584572506
    },
    {
      "source": "7572aefcd241ec76341addcb2e2e417587cb2e4c",
      "target": "95c3d25b40f963eb248136555bd9b9e35817cc09",
      "weight": 0.2995415599681344
    },
    {
      "source": "68f34ed64fdf07bb1325097c93576658e061231e",
      "target": "5b5b3face4be1cf131d0cb9c40ae5adcd0c16408",
      "weight": 0.2520442406091951
    },
    {
      "source": "68f34ed64fdf07bb1325097c93576658e061231e",
      "target": "f470e11faa6200026cf39e248510070c078e509a",
      "weight": 0.38855240381975975
    },
    {
      "source": "e5c851867af5587466f7cd9c22f8b2c84f8c6b63",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.301938650309191
    },
    {
      "source": "bbb89d88ad5b8279709ff089d3c00cd2750cd26b",
      "target": "4801db5c5cb24a9069f2d264252fa26986ceefa9",
      "weight": 0.20530637370504992
    },
    {
      "source": "bbb89d88ad5b8279709ff089d3c00cd2750cd26b",
      "target": "5dc88d795cbcd01e6e99ba673e91e9024f0c3318",
      "weight": 0.46035448180340155
    },
    {
      "source": "bbb89d88ad5b8279709ff089d3c00cd2750cd26b",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.29408592065793854
    },
    {
      "source": "bbb89d88ad5b8279709ff089d3c00cd2750cd26b",
      "target": "658702b2fa647ae7eaf1255058105da9eefe6f52",
      "weight": 0.252048199509535
    },
    {
      "source": "4e52607397a96fb2104a99c570c9cec29c9ca519",
      "target": "52eb7f27cdfbf359096b8b5ef56b2c2826beb660",
      "weight": 0.2989461138655178
    },
    {
      "source": "4e52607397a96fb2104a99c570c9cec29c9ca519",
      "target": "780bc77fac1aaf460ba191daa218f3c111119092",
      "weight": 0.30476261720288833
    },
    {
      "source": "4e52607397a96fb2104a99c570c9cec29c9ca519",
      "target": "b3f0cdc217a3d192d2671e44913542903c94105b",
      "weight": 0.4092449879178138
    },
    {
      "source": "4e52607397a96fb2104a99c570c9cec29c9ca519",
      "target": "cab5194d13c1ce89a96322adaac754b2cb630d87",
      "weight": 0.3727046008422459
    },
    {
      "source": "83a46afaeb520abcd9b0138507a253f6d4d8bff7",
      "target": "eb14b24b329a6cc80747644616e15491ef49596f",
      "weight": 0.22486963347870914
    },
    {
      "source": "83a46afaeb520abcd9b0138507a253f6d4d8bff7",
      "target": "4085a5cf49c193fe3d3ff19ff2d696fe20a5a596",
      "weight": 0.2616212071154877
    },
    {
      "source": "83a46afaeb520abcd9b0138507a253f6d4d8bff7",
      "target": "63836e669416668744c3676a831060e8de3f58a1",
      "weight": 0.32969060242037407
    },
    {
      "source": "bcdb8914550df02bfe1f69348c9830d775f6590a",
      "target": "33f3f53c957c4a8832b1dcb095a4ac967bd89897",
      "weight": 0.2751866786678822
    },
    {
      "source": "bcdb8914550df02bfe1f69348c9830d775f6590a",
      "target": "bb3e135757bfb82c4de202c807c9e381caecb623",
      "weight": 0.3021841158783513
    },
    {
      "source": "bcdb8914550df02bfe1f69348c9830d775f6590a",
      "target": "4085a5cf49c193fe3d3ff19ff2d696fe20a5a596",
      "weight": 0.23618005894525013
    },
    {
      "source": "bcdb8914550df02bfe1f69348c9830d775f6590a",
      "target": "29eb99518d16ccf8ac306d92f4a6377ae109d9be",
      "weight": 0.26031039033164377
    },
    {
      "source": "0364e17da01358e2705524cd781ef8cc928256f5",
      "target": "52eb7f27cdfbf359096b8b5ef56b2c2826beb660",
      "weight": 0.2657646414528848
    },
    {
      "source": "f470e11faa6200026cf39e248510070c078e509a",
      "target": "84aa127dc5ca3080385439cb10edc50b5d2c04e4",
      "weight": 0.3260216009557524
    },
    {
      "source": "d9802a67b326fe89bbd761c261937ee1e4d4d674",
      "target": "15710515bae025372f298570267d234d4a3141cb",
      "weight": 0.36870055201767216
    },
    {
      "source": "d9802a67b326fe89bbd761c261937ee1e4d4d674",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.25715678844100553
    },
    {
      "source": "33d469c6d9fc09b59522d91b7696b15dc60a9a93",
      "target": "10d949dee482aeea1cab8b42c326d0dbf0505de3",
      "weight": 0.32073749643263905
    },
    {
      "source": "33d469c6d9fc09b59522d91b7696b15dc60a9a93",
      "target": "3f0d5aa7a637d2c0bb3d768c99cc203430b4481e",
      "weight": 0.29649107624530996
    },
    {
      "source": "33d469c6d9fc09b59522d91b7696b15dc60a9a93",
      "target": "21f8ea62da6a4031d85a1ee701dbc3e6847fa6d3",
      "weight": 0.2820264956136658
    },
    {
      "source": "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4",
      "target": "c64433657869ecdaaa7988a029eabfe774d3ac47",
      "weight": 0.31621211852110737
    },
    {
      "source": "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4",
      "target": "bcffbb40e7922d2a34e752f8faaa4fe99649e21a",
      "weight": 0.26854938588044436
    },
    {
      "source": "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4",
      "target": "15710515bae025372f298570267d234d4a3141cb",
      "weight": 0.5586934566660169
    },
    {
      "source": "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4",
      "target": "c075a84356b529464df2e06a02bf9b524a815152",
      "weight": 0.27029713678410994
    },
    {
      "source": "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4",
      "target": "12cc4b65644a84a16ef7dfe7bdd70172cd38cffd",
      "weight": 0.28304485437483656
    },
    {
      "source": "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4",
      "target": "7029ecb5d5fc04f54e1e25e739db2e993fb147c8",
      "weight": 0.2735277418233002
    },
    {
      "source": "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.26407975069316636
    },
    {
      "source": "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4",
      "target": "c180564160d0788a82df203f9e5f61380d9846aa",
      "weight": 0.3152715803341878
    },
    {
      "source": "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4",
      "target": "4085a5cf49c193fe3d3ff19ff2d696fe20a5a596",
      "weight": 0.4111916725533108
    },
    {
      "source": "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.2719425480449132
    },
    {
      "source": "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4",
      "target": "44ce738296c3148c6593324773706cdc228614d4",
      "weight": 0.2475813024038246
    },
    {
      "source": "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4",
      "target": "63836e669416668744c3676a831060e8de3f58a1",
      "weight": 0.33049888826748086
    },
    {
      "source": "d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4",
      "target": "0367603c0197ab48eeba29aa6af391584a5077c0",
      "weight": 0.2830268401769382
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "c64433657869ecdaaa7988a029eabfe774d3ac47",
      "weight": 0.3390153273814118
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "33f3f53c957c4a8832b1dcb095a4ac967bd89897",
      "weight": 0.27413431578590325
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "bcffbb40e7922d2a34e752f8faaa4fe99649e21a",
      "weight": 0.34936645806927646
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "6a2f26cece133b0aa52843be0f149a65e78374f7",
      "weight": 0.2940485288337309
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "eae107f7eeed756dfc996c47bc3faf381d36fd94",
      "weight": 0.26666739263220596
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "ce7291c5cd919a97ced6369ca697db9849848688",
      "weight": 0.258995322825573
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "3ac716ac5d47d4420010678fda766ebb5b882ba9",
      "weight": 0.2413296701999304
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "5b5b3face4be1cf131d0cb9c40ae5adcd0c16408",
      "weight": 0.2588390943660376
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "33a7b7abf006d22de24c1471e6f6c93842a497b6",
      "weight": 0.29862479173753786
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "52eb7f27cdfbf359096b8b5ef56b2c2826beb660",
      "weight": 0.26447513611832807
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "c075a84356b529464df2e06a02bf9b524a815152",
      "weight": 0.26920055569818335
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "6a86594566fc9fa2e92afb6f0229d63a45fe25e6",
      "weight": 0.25157451420018867
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "12cc4b65644a84a16ef7dfe7bdd70172cd38cffd",
      "weight": 0.23977522316053138
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "7029ecb5d5fc04f54e1e25e739db2e993fb147c8",
      "weight": 0.30912050004071046
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "780bc77fac1aaf460ba191daa218f3c111119092",
      "weight": 0.25199153797770446
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "eb14b24b329a6cc80747644616e15491ef49596f",
      "weight": 0.24624151404548136
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "4801db5c5cb24a9069f2d264252fa26986ceefa9",
      "weight": 0.09837674865573379
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "398978c84ca8dab093d0b7fa73c6d380f5fa914c",
      "weight": 0.3342702604361537
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "efea0197c956e981e98c4d2532fa720c58954492",
      "weight": 0.2653599902776624
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.2949508486674381
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "b1d807fc6b184d757ebdea67acd81132d8298ff6",
      "weight": 0.25403515928875164
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "354fb91810c6d3756600c99ad84d2e6ef4136021",
      "weight": 0.2896194021549021
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "c180564160d0788a82df203f9e5f61380d9846aa",
      "weight": 0.3174665051232254
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "040fe47af8f4870bf681f34861c42b3ea46d76cf",
      "weight": 0.2475545747464733
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "5dc88d795cbcd01e6e99ba673e91e9024f0c3318",
      "weight": 0.26482523386325096
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "29052ddd048acb1afa2c42613068b63bb7428a34",
      "weight": 0.2992039753547341
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "4085a5cf49c193fe3d3ff19ff2d696fe20a5a596",
      "weight": 0.2881065644135048
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "fda63b289d4c0c332f88975994114fb61b514ced",
      "weight": 0.22754802754231346
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "d9802a67b326fe89bbd761c261937ee1e4d4d674",
      "weight": 0.4570668180583018
    },
    {
      "source": "8f096071a09701012c9c279aee2a88143a295935",
      "target": "10d949dee482aeea1cab8b42c326d0dbf0505de3",
      "weight": 0.3016521167642264
    },
    {
      "source": "d3c287ff061f295ddf8dc3cb02a6f39e301cae3b",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.2532066199357059
    },
    {
      "source": "d3c287ff061f295ddf8dc3cb02a6f39e301cae3b",
      "target": "354fb91810c6d3756600c99ad84d2e6ef4136021",
      "weight": 0.25247133371331903
    },
    {
      "source": "d3c287ff061f295ddf8dc3cb02a6f39e301cae3b",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.26740632157451977
    },
    {
      "source": "d3c287ff061f295ddf8dc3cb02a6f39e301cae3b",
      "target": "af051c87cecca64c2de4ad9110608f7579766653",
      "weight": 0.23268494573853965
    },
    {
      "source": "d3c287ff061f295ddf8dc3cb02a6f39e301cae3b",
      "target": "19a672bdf29367b7509586a4be27c6843af903b1",
      "weight": 0.22192904774353273
    },
    {
      "source": "a905a690ec350b1aeb5fcfd7f2ff0f5e1663b3a0",
      "target": "191815e4109ee392b9120b61642c0e859fb662a1",
      "weight": 0.5681374439743611
    },
    {
      "source": "acc855d74431537b98de5185e065e4eacbab7b26",
      "target": "5515fd5d14ac7b19806294119560a8c74f7fa4b2",
      "weight": 0.2954651990561541
    },
    {
      "source": "acc855d74431537b98de5185e065e4eacbab7b26",
      "target": "2bd20cfec4ad3df0fd9cd87cef3eefe6f3847b83",
      "weight": 0.30926697470413445
    },
    {
      "source": "29eb99518d16ccf8ac306d92f4a6377ae109d9be",
      "target": "3f170af3566f055e758fa3bdf2bfd3a0e8787e58",
      "weight": 0.26758835878747594
    },
    {
      "source": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "target": "c64433657869ecdaaa7988a029eabfe774d3ac47",
      "weight": 0.30747310920932336
    },
    {
      "source": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.4113898779832257
    },
    {
      "source": "1f20378d2820fdf1c1bb09ce22f739ab77b14e82",
      "target": "5b5b3face4be1cf131d0cb9c40ae5adcd0c16408",
      "weight": 0.2601085125571448
    },
    {
      "source": "1f20378d2820fdf1c1bb09ce22f739ab77b14e82",
      "target": "9c510e24b5edc5720440b695d7bd0636b52f4f66",
      "weight": 0.34703944044760526
    },
    {
      "source": "1f20378d2820fdf1c1bb09ce22f739ab77b14e82",
      "target": "f470e11faa6200026cf39e248510070c078e509a",
      "weight": 0.3640964211860577
    },
    {
      "source": "1f20378d2820fdf1c1bb09ce22f739ab77b14e82",
      "target": "84aa127dc5ca3080385439cb10edc50b5d2c04e4",
      "weight": 0.3784983644879323
    },
    {
      "source": "10d949dee482aeea1cab8b42c326d0dbf0505de3",
      "target": "eae107f7eeed756dfc996c47bc3faf381d36fd94",
      "weight": 0.2573799134324746
    },
    {
      "source": "10d949dee482aeea1cab8b42c326d0dbf0505de3",
      "target": "40479fd70115e545d21c01853aad56e6922280ac",
      "weight": 0.24058484283033982
    },
    {
      "source": "322aa32b2a409d2e135dbb14736d9aeb497f1c52",
      "target": "4085a5cf49c193fe3d3ff19ff2d696fe20a5a596",
      "weight": 0.24575574712142054
    },
    {
      "source": "322aa32b2a409d2e135dbb14736d9aeb497f1c52",
      "target": "191815e4109ee392b9120b61642c0e859fb662a1",
      "weight": 0.2637001589115587
    },
    {
      "source": "322aa32b2a409d2e135dbb14736d9aeb497f1c52",
      "target": "7e5f318bf5b9c986ca82d2d97e11f50d58ee6680",
      "weight": 0.26301860253287845
    },
    {
      "source": "322aa32b2a409d2e135dbb14736d9aeb497f1c52",
      "target": "bcdb8914550df02bfe1f69348c9830d775f6590a",
      "weight": 0.28357848833001037
    },
    {
      "source": "322aa32b2a409d2e135dbb14736d9aeb497f1c52",
      "target": "a905a690ec350b1aeb5fcfd7f2ff0f5e1663b3a0",
      "weight": 0.4887301980264149
    },
    {
      "source": "322aa32b2a409d2e135dbb14736d9aeb497f1c52",
      "target": "2e925a02db26a60ee1cc022f3923e09f3fae7b39",
      "weight": 0.2771704547492588
    },
    {
      "source": "5b5b3face4be1cf131d0cb9c40ae5adcd0c16408",
      "target": "3ac716ac5d47d4420010678fda766ebb5b882ba9",
      "weight": 0.45133838437969015
    },
    {
      "source": "abea782b5d0bdb4cd90ec42f672711613e71e43e",
      "target": "f2b924e69735fb7fd6fd95c6a032954480862029",
      "weight": 0.25476452122919646
    },
    {
      "source": "abea782b5d0bdb4cd90ec42f672711613e71e43e",
      "target": "68f34ed64fdf07bb1325097c93576658e061231e",
      "weight": 0.28555931807981216
    },
    {
      "source": "abea782b5d0bdb4cd90ec42f672711613e71e43e",
      "target": "c762e198b0239313ee50476021b1939390c4ef9d",
      "weight": 0.9748770750591362
    },
    {
      "source": "83d58bc46b7adb92d8750da52313f060b10f201d",
      "target": "52eb7f27cdfbf359096b8b5ef56b2c2826beb660",
      "weight": 0.2893593591921406
    },
    {
      "source": "83d58bc46b7adb92d8750da52313f060b10f201d",
      "target": "780bc77fac1aaf460ba191daa218f3c111119092",
      "weight": 0.29679615872987203
    },
    {
      "source": "83d58bc46b7adb92d8750da52313f060b10f201d",
      "target": "efea0197c956e981e98c4d2532fa720c58954492",
      "weight": 0.2753732012556299
    },
    {
      "source": "83d58bc46b7adb92d8750da52313f060b10f201d",
      "target": "4e52607397a96fb2104a99c570c9cec29c9ca519",
      "weight": 0.30500012019312406
    },
    {
      "source": "83d58bc46b7adb92d8750da52313f060b10f201d",
      "target": "0364e17da01358e2705524cd781ef8cc928256f5",
      "weight": 0.5256030120900915
    },
    {
      "source": "83d58bc46b7adb92d8750da52313f060b10f201d",
      "target": "552bfaca30af29647c083993fbe406867fc70d4c",
      "weight": 0.4080257583024984
    },
    {
      "source": "83d58bc46b7adb92d8750da52313f060b10f201d",
      "target": "58e1b93b18370433633152cb8825917edc2f16a6",
      "weight": 0.5939723299976261
    },
    {
      "source": "83d58bc46b7adb92d8750da52313f060b10f201d",
      "target": "b3f0cdc217a3d192d2671e44913542903c94105b",
      "weight": 0.43271051421792317
    },
    {
      "source": "2a25540e3ce0baba56ee71da7ca938f0264f790d",
      "target": "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "weight": 0.3174083116700204
    },
    {
      "source": "2a25540e3ce0baba56ee71da7ca938f0264f790d",
      "target": "d605a7628b2a7ff8ce04fc27111626e2d734cab4",
      "weight": 0.24085465038576084
    },
    {
      "source": "2a25540e3ce0baba56ee71da7ca938f0264f790d",
      "target": "a166957ec488cd20e61360d630568b3b81af3397",
      "weight": 0.23598107064803192
    }
  ]
}