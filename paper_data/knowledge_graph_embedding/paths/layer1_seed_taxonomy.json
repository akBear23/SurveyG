{
  "18bd7cd489874ed9976b4f87a6a558f9533316e0": {
    "seed_title": "Knowledge Graph Embedding via Dynamic Mapping Matrix",
    "summary": "This citation path reveals a dynamic and highly specialized evolution in Knowledge Graph Embedding (KGE) research, driven by the continuous pursuit of more expressive, robust, and efficient models for increasingly complex scenarios.\n\n**1. Methodological Evolution:**\nThe methodological evolution in this path demonstrates a significant and rapid shift from foundational vector embeddings towards sophisticated geometric and neural network-based approaches, often combining multiple paradigms. Key innovations include the widespread adoption of multi-curvature spaces (hyperbolic, hyperspherical, Euclidean) in Papers 2 (MADE), 3 (IME), and 14 (MGTCA) to capture complex geometric structures, particularly in Temporal Knowledge Graphs (TKGs) and for general Knowledge Graph Completion (KGC). Furthermore, Papers 1 (ConQuatE) and 5 (Multihop Fuzzy Spatiotemporal RDF KG Query) introduce quaternion embeddings, utilizing their inherent rotational properties to model nuanced challenges like polysemy and fuzzy spatiotemporal data, while advanced neural architectures (CNNs in Papers 9 (CNN-ECFA), 10 (SEConv); Graph Transformers in Paper 13 (TGformer)) are integrated for enhanced feature aggregation and contextual understanding, alongside meta-learning (Paper 6, MetaHG) and specialized strategies for Federated KGE (Papers 8 (Poisoning Attack), 15 (FedS), 16 (PFedEG)).\n\n**2. Knowledge Progression:**\nThis path addresses the fundamental problem of enhancing KGE expressiveness and applicability to increasingly complex and real-world scenarios, systematically building upon previous limitations. Paper 1 (ConQuatE) tackles the polysemy issue, where entities have context-dependent meanings, by proposing contextualized quaternion embeddings to enrich representations through quaternion rotation and contextual cues, improving upon models with weak entity-relation interactions; concurrently, Papers 2 (MADE) and 3 (IME) address the challenge of modeling diverse geometric structures within Temporal Knowledge Graphs (TKGs) by proposing adaptive and integrated multi-curvature embedding strategies, moving beyond single-space limitations. Papers 4 (FSTRE) and 5 (Multihop Fuzzy Spatiotemporal RDF KG Query) further extend KGE to fuzzy and uncertain spatiotemporal knowledge, incorporating dynamic vector projection, rotation, and quaternion-based reasoning to handle real-world uncertainty and multihop path queries, a significant leap from static, crisp KGs. Concurrently, Paper 6 (MetaHG) enhances KGE for dynamic service ecosystems via meta-learning, Paper 7 (HolmE) ensures the relation embedding space is closed under composition, significantly improving the modeling of under-represented compositional patterns, and Papers 8, 15, and 16 collectively address critical emerging challenges in Federated KGE, focusing on systematizing poisoning attacks, improving communication efficiency through entity-wise sparsification, and enabling personalized embeddings via client-wise relation graphs, respectively.\n\n**3. Temporal Context:**\nThe striking aspect of this citation path is that all papers are published in 2024 or 2025, indicating an extremely rapid and contemporary acceleration of research in highly advanced KGE topics. This concentrated publication period highlights a vibrant and highly active research front, with researchers quickly building upon and diversifying recent theoretical and practical advancements in areas like multi-curvature spaces, quaternion embeddings, and federated learning, rather than addressing foundational KGE problems.\n\n**4. Synthesis:**\nCollectively, these works narrate a continuous and rapid evolution in KGE, moving from foundational embedding concepts to highly specialized and robust models designed for complex, dynamic, and distributed environments. The unified narrative is a relentless quest to overcome the inherent limitations of traditional KGE by embracing richer mathematical spaces, dynamic learning paradigms, and distributed architectures to accurately represent and reason over increasingly complex, uncertain, and evolving knowledge. This collective contribution significantly expands the \"knowledge graph embedding\" toolkit, enabling more expressive, efficient, and secure applications across diverse and challenging real-world domains.",
    "path": [
      "18bd7cd489874ed9976b4f87a6a558f9533316e0",
      "06315f8b2633a54b087c6094cdb281f01dd06482",
      "b2d2ad9a458bdcb0523d22be659eb013ca2d3c67",
      "95c3d25b40f963eb248136555bd9b9e35817cc09",
      "44ce738296c3148c6593324773706cdc228614d4",
      "9c510e24b5edc5720440b695d7bd0636b52f4f66",
      "58e1b93b18370433633152cb8825917edc2f16a6",
      "552bfaca30af29647c083993fbe406867fc70d4c",
      "8b717c4dfb309638307fcc7d2c798b1c20927a3e",
      "1620a20881b572b5ffc6f9cb3cf39f6090cee19f",
      "bcdb8914550df02bfe1f69348c9830d775f6590a",
      "63836e669416668744c3676a831060e8de3f58a1",
      "f470e11faa6200026cf39e248510070c078e509a",
      "33d469c6d9fc09b59522d91b7696b15dc60a9a93",
      "933cb8bf1cd50d6d5833a627683327b15db28836",
      "2e925a02db26a60ee1cc022f3923e09f3fae7b39",
      "c2c6edc5750a438bddd1217481832d38df6336de",
      "83a46afaeb520abcd9b0138507a253f6d4d8bff7",
      "f4e39a4f8fd8f8453372b74fda17047b9860d870",
      "f2b924e69735fb7fd6fd95c6a032954480862029",
      "e5c851867af5587466f7cd9c22f8b2c84f8c6b63",
      "4085a5cf49c193fe3d3ff19ff2d696fe20a5a596",
      "acc855d74431537b98de5185e065e4eacbab7b26",
      "d7ef14459674b75807cd9be549f1e12d53849ead",
      "e9a13a97b7266ac27dcd7117a99a4fcbadc5fd9c",
      "21f8ea62da6a4031d85a1ee701dbc3e6847fa6d3",
      "29eb99518d16ccf8ac306d92f4a6377ae109d9be",
      "1f20378d2820fdf1c1bb09ce22f739ab77b14e82",
      "fda63b289d4c0c332f88975994114fb61b514ced",
      "8fef3f8bb8bcd254898b5d24f3d78beab09e99d4",
      "145fa4ea1567a6b9d981fdea0e183140d99aeb97",
      "4e52607397a96fb2104a99c570c9cec29c9ca519",
      "c64433657869ecdaaa7988a029eabfe774d3ac47",
      "52eb7f27cdfbf359096b8b5ef56b2c2826beb660",
      "780bc77fac1aaf460ba191daa218f3c111119092",
      "efea0197c956e981e98c4d2532fa720c58954492",
      "12cc4b65644a84a16ef7dfe7bdd70172cd38cffd",
      "ce7291c5cd919a97ced6369ca697db9849848688",
      "15710515bae025372f298570267d234d4a3141cb",
      "6a86594566fc9fa2e92afb6f0229d63a45fe25e6",
      "bb3e135757bfb82c4de202c807c9e381caecb623",
      "33f3f53c957c4a8832b1dcb095a4ac967bd89897",
      "398978c84ca8dab093d0b7fa73c6d380f5fa914c",
      "7029ecb5d5fc04f54e1e25e739db2e993fb147c8",
      "3f170af3566f055e758fa3bdf2bfd3a0e8787e58",
      "eb14b24b329a6cc80747644616e15491ef49596f",
      "3ac716ac5d47d4420010678fda766ebb5b882ba9",
      "5b5b3face4be1cf131d0cb9c40ae5adcd0c16408"
    ],
    "layer1_papers": [
      {
        "title": "Knowledge Graph Embedding via Dynamic Mapping Matrix",
        "abstract": "Knowledge graphs are useful resources for numerous AI applications, but they are far from completeness. Previous work such as TransE, TransH and TransR/CTransR regard a relation as translation from head entity to tail entity and the CTransR achieves state-of-the-art performance. In this paper, we propose a more fine-grained model named TransD, which is an improvement of TransR/CTransR. In TransD, we use two vectors to represent a named symbol object (entity and relation). The first one represents the meaning of a(n) entity (relation), the other one is used to construct mapping matrix dynamically. Compared with TransR/CTransR, TransD not only considers the diversity of relations, but also entities. TransD has less parameters and has no matrix-vector multiplication operations, which makes it can be applied on large scale graphs. In Experiments, we evaluate our model on two typical tasks including triplets classification and link prediction. Evaluation results show that our approach outperforms state-of-the-art methods.",
        "summary": "",
        "year": 2015
      }
    ],
    "layer2_papers": [
      {
        "title": "TransET: Knowledge Graph Embedding with Entity Types",
        "abstract": "Knowledge graph embedding aims to embed entities and relations into low-dimensional vector spaces. Most existing methods only focus on triple facts in knowledge graphs. In addition, models based on translation or distance measurement cannot fully represent complex relations. As well-constructed prior knowledge, entity types can be employed to learn the representations of entities and relations. In this paper, we propose a novel knowledge graph embedding model named TransET, which takes advantage of entity types to learn more semantic features. More specifically, circle convolution based on the embeddings of entity and entity types is utilized to map head entity and tail entity to type-specific representations, then translation-based score function is used to learn the presentation triples. We evaluated our model on real-world datasets with two benchmark tasks of link prediction and triple classification. Experimental results demonstrate that it outperforms state-of-the-art models in most cases.",
        "year": 2021
      },
      {
        "title": "TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation",
        "abstract": "Knowledge graph embedding (KGE) aims to learn continuous vectors of relations and entities in knowledge graph. Recently, transition-based KGE methods have achieved promising performance, where the single relation vector learns to translate head entity to tail entity. However, this scoring pattern is not suitable for complex scenarios where the same entity pair has different relations. Previous models usually focus on the improvement of entity representation for 1-to-N, N-to-1 and N-to-N relations, but ignore the single relation vector. In this paper, we propose a novel transition-based method, TranS, for knowledge graph embedding. The single relation vector in traditional scoring patterns is replaced with synthetic relation representation, which can solve these issues effectively and efficiently. Experiments on a large knowledge graph dataset, ogbl-wikikg2, show that our model achieves state-of-the-art results.",
        "year": 2022
      },
      {
        "title": "LineaRE: Simple but Powerful Knowledge Graph Embedding for Link Prediction",
        "abstract": "The task of link prediction for knowledge graphs is to predict missing relationships between entities. Knowledge graph embedding, which aims to represent entities and relations of a knowledge graph as low dimensional vectors in a continuous vector space, has achieved promising predictive performance. If an embedding model can cover different types of connectivity patterns and mapping properties of relations as many as possible, it will potentially bring more benefits for link prediction tasks. In this paper, we propose a novel embedding model, namely LineaRE, which is capable of modeling four connectivity patterns (i.e., symmetry, antisymmetry, inversion, and composition) and four mapping properties (i.e., one-to-one, one-to-many, many-to-one, and many-to-many) of relations. Specifically, we regard knowledge graph embedding as a simple linear regression task, where a relation is modeled as a linear function of two low-dimensional vector-presented entities with two weight vectors and a bias vector. Since the vectors are defined in a real number space and the scoring function of the model is linear, our model is simple and scalable to large knowledge graphs. Experimental results on multiple widely used real-world datasets show that the proposed LineaRE model significantly outperforms existing state-of-the-art models for link prediction tasks.",
        "year": 2020
      },
      {
        "title": "CompoundE: Knowledge Graph Embedding with Translation, Rotation and Scaling Compound Operations",
        "abstract": "Translation, rotation, and scaling are three commonly used geometric manipulation operations in image processing. Besides, some of them are successfully used in developing effective knowledge graph embedding (KGE) models such as TransE and RotatE. Inspired by the synergy, we propose a new KGE model by leveraging all three operations in this work. Since translation, rotation, and scaling operations are cascaded to form a compound one, the new model is named CompoundE. By casting CompoundE in the framework of group theory, we show that quite a few scoring-function-based KGE models are special cases of CompoundE. CompoundE extends the simple distance-based relation to relation-dependent compound operations on head and/or tail entities. To demonstrate the effectiveness of CompoundE, we conduct experiments on three popular KG completion datasets. Experimental results show that CompoundE consistently achieves the state of-the-art performance.",
        "year": 2022
      },
      {
        "title": "A Review of Knowledge Graph Embedding Methods of TransE, TransH and TransR for Missing Links",
        "abstract": "Knowledge representation and reasoning require knowledge graph embedding as it is crucial in the area. It involves mapping entities and relationships from a knowledge graph into vectors of lower dimensions that are continuous in nature. This encoding enables machine learning algorithms to effectively reason and make predictions on graph-structured data. This review article offers an overview and critical analysis specifically about the methods of knowledge graph embedding which are TransE, TransH, and TransR. The key concepts, methodologies, strengths, and limitations of these methods, along with examining their applications and experiments conducted by existing researchers have been studied. The motivation to conduct this study is to review the well-known and most applied knowledge embedding methods and compare the features of those methods so that a comprehensive resource for researchers and practitioners interested in delving into knowledge graph embedding techniques is delivered.",
        "year": 2023
      },
      {
        "title": "Temporal Knowledge Graph Embedding Model based on Additive Time Series Decomposition",
        "abstract": "Knowledge Graph (KG) embedding has attracted more attention in recent years. Most KG embedding models learn from time-unaware triples. However, the inclusion of temporal information beside triples would further improve the performance of a KGE model. In this regard, we propose ATiSE, a temporal KG embedding model which incorporates time information into entity/relation representations by using Additive Time Series decomposition. Moreover, considering the temporal uncertainty during the evolution of entity/relation representations over time, we map the representations of temporal KGs into the space of multi-dimensional Gaussian distributions. The mean of each entity/relation embedding at a time step shows the current expected position, whereas its covariance (which is temporally stationary) represents its temporal uncertainty. Experimental results show that ATiSE chieves the state-of-the-art on link prediction over four temporal KGs.",
        "year": 2019
      },
      {
        "title": "TeRo: A Time-aware Knowledge Graph Embedding via Temporal Rotation",
        "abstract": "In the last few years, there has been a surge of interest in learning representations of entities and relations in knowledge graph (KG). However, the recent availability of temporal knowledge graphs (TKGs) that contain time information for each fact created the need for reasoning over time in such TKGs. In this regard, we present a new approach of TKG embedding, TeRo, which defines the temporal evolution of entity embedding as a rotation from the initial time to the current time in the complex vector space. Specially, for facts involving time intervals, each relation is represented as a pair of dual complex embeddings to handle the beginning and the end of the relation, respectively. We show our proposed model overcomes the limitations of the existing KG embedding models and TKG embedding models and has the ability of learning and inferring various relation patterns over time. Experimental results on three different TKGs show that TeRo significantly outperforms existing state-of-the-art models for link prediction. In addition, we analyze the effect of time granularity on link prediction over TKGs, which as far as we know has not been investigated in previous literature.",
        "year": 2020
      },
      {
        "title": "Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding",
        "abstract": "Knowledge graphs (KGs) consisting of a large number of triples have become widespread recently, and many knowledge graph embedding (KGE) methods are proposed to embed entities and relations of a KG into continuous vector spaces. Such embedding methods simplify the operations of conducting various in-KG tasks (e.g., link prediction) and out-of-KG tasks (e.g., question answering). They can be viewed as general solutions for representing KGs. However, existing KGE methods are not applicable to inductive settings, where a model trained on source KGs will be tested on target KGs with entities unseen during model training. Existing works focusing on KGs in inductive settings can only solve the inductive relation prediction task. They can not handle other out-of-KG tasks as general as KGE methods since they don't produce embeddings for entities. In this paper, to achieve inductive knowledge graph embedding, we propose a model MorsE, which does not learn embeddings for entities but learns transferable meta-knowledge that can be used to produce entity embeddings. Such meta-knowledge is modeled by entity-independent modules and learned by meta-learning. Experimental results show that our model significantly outperforms corresponding baselines for in-KG and out-of-KG tasks in inductive settings.",
        "year": 2021
      },
      {
        "title": "ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding",
        "abstract": "The goal of Knowledge graph embedding (KGE) is to learn how to represent the low dimensional vectors for entities and relations based on the observed triples. The conventional shallow models are limited to their expressiveness. ConvE (Dettmers et al., 2018) takes advantage of CNN and improves the expressive power with parameter efficient operators by increasing the interactions between head and relation embeddings. However, there is no structural information in the embedding space of ConvE, and the performance is still limited by the number of interactions. The recent KBGAT (Nathani et al., 2019) provides another way to learn embeddings by adaptively utilizing structural information. In this paper, we take the benefits of ConvE and KBGAT together and propose a Relation-aware Inception network with joint local-global structural information for knowledge graph Embedding (ReInceptionE). Specifically, we first explore the Inception network to learn query embedding, which aims to further increase the interactions between head and relation embeddings. Then, we propose to use a relation-aware attention mechanism to enrich the query embedding with the local neighborhood and global entity information. Experimental results on both WN18RR and FB15k-237 datasets demonstrate that ReInceptionE achieves competitive performance compared with state-of-the-art methods.",
        "year": 2020
      },
      {
        "title": "Knowledge Graph Embedding with Atrous Convolution and Residual Learning",
        "abstract": "Knowledge graph embedding is an important task and it will benefit lots of downstream applications. Currently, deep neural networks based methods achieve state-of-the-art performance. However, most of these existing methods are very complex and need much time for training and inference. To address this issue, we propose a simple but effective atrous convolution based knowledge graph embedding method. Compared with existing state-of-the-art methods, our method has following main characteristics. First, it effectively increases feature interactions by using atrous convolutions. Second, to address the original information forgotten issue and vanishing/exploding gradient issue, it uses the residual learning method. Third, it has simpler structure but much higher parameter efficiency. We evaluate our method on six benchmark datasets with different evaluation metrics. Extensive experiments show that our model is very effective. On these diverse datasets, it achieves better results than the compared state-of-the-art methods on most of evaluation metrics. The source codes of our model could be found at https://github.com/neukg/AcrE.",
        "year": 2020
      },
      {
        "title": "HousE: Knowledge Graph Embedding with Householder Parameterization",
        "abstract": "The effectiveness of knowledge graph embedding (KGE) largely depends on the ability to model intrinsic relation patterns and mapping properties. However, existing approaches can only capture some of them with insufficient modeling capacity. In this work, we propose a more powerful KGE framework named HousE, which involves a novel parameterization based on two kinds of Householder transformations: (1) Householder rotations to achieve superior capacity of modeling relation patterns; (2) Householder projections to handle sophisticated relation mapping properties. Theoretically, HousE is capable of modeling crucial relation patterns and mapping properties simultaneously. Besides, HousE is a generalization of existing rotation-based models while extending the rotations to high-dimensional spaces. Empirically, HousE achieves new state-of-the-art performance on five benchmark datasets. Our code is available at https://github.com/anrep/HousE.",
        "year": 2022
      },
      {
        "title": "A Survey on Knowledge Graph Embedding",
        "abstract": "Knowledge graph (KG) is used to represent the relationships between different concepts in the real world. It is a special network in which nodes represent entities and edges represent relationships. KGs can intuitively model the connections between facts, but in many applications, there are certain limitations in directly using symbolic logic to represent knowledge in KGs and perform calculations, making it difficult to achieve expected results in downstream tasks. Meanwhile, with the explosive growth of Internet capacity, the traditional KG structure faces the problems of computational inefficiency and management difficulties. To alleviate these problems, Knowledge graph embedding (KGE) is proposed to improve the computational efficiency by embedding entities and relations in the KG into a low-dimensional, dense and continuous vector space, and thus the solution of some problems in the knowledge graph is transformed into vector operations. Moreover, KGE also can be used as a pre-trained model which is more beneficial to downstream applications, such as applications based on deep learning. In this paper, we classify KGE into three categories, namely translational distance models, semantic matching models and neural network based models. The advantages and disadvantages of different embedding methods are compared, while the main applications of KGE are summarized. Some current challenges of KGE are summarized, as well as some views on the future research directions of KGE.",
        "year": 2022
      },
      {
        "title": "Knowledge Graph Embedding Compression",
        "abstract": "Knowledge graph (KG) representation learning techniques that learn continuous embeddings of entities and relations in the KG have become popular in many AI applications. With a large KG, the embeddings consume a large amount of storage and memory. This is problematic and prohibits the deployment of these techniques in many real world settings. Thus, we propose an approach that compresses the KG embedding layer by representing each entity in the KG as a vector of discrete codes and then composes the embeddings from these codes. The approach can be trained end-to-end with simple modifications to any existing KG embedding technique. We evaluate the approach on various standard KG embedding evaluations and show that it achieves 50-1000x compression of embeddings with a minor loss in performance. The compressed embeddings also retain the ability to perform various reasoning tasks such as KG inference.",
        "year": 2020
      },
      {
        "title": "Joint Language Semantic and Structure Embedding for Knowledge Graph Completion",
        "abstract": "The task of completing knowledge triplets has broad downstream applications. Both structural and semantic information plays an important role in knowledge graph completion. Unlike previous approaches that rely on either the structures or semantics of the knowledge graphs, we propose to jointly embed the semantics in the natural language description of the knowledge triplets with their structure information. Our method embeds knowledge graphs for the completion task via fine-tuning pre-trained language models with respect to a probabilistic structured loss, where the forward pass of the language models captures semantics and the loss reconstructs structures. Our extensive experiments on a variety of knowledge graph benchmarks have demonstrated the state-of-the-art performance of our method. We also show that our method can significantly improve the performance in a low-resource regime, thanks to the better use of semantics. The code and datasets are available at https://github.com/pkusjh/LASS.",
        "year": 2022
      },
      {
        "title": "CoKE: Contextualized Knowledge Graph Embedding",
        "abstract": "Knowledge graph embedding, which projects symbolic entities and relations into continuous vector spaces, is gaining increasing attention. Previous methods allow a single static embedding for each entity or relation, ignoring their intrinsic contextual nature, i.e., entities and relations may appear in different graph contexts, and accordingly, exhibit different properties. This work presents Contextualized Knowledge Graph Embedding (CoKE), a novel paradigm that takes into account such contextual nature, and learns dynamic, flexible, and fully contextualized entity and relation embeddings. Two types of graph contexts are studied: edges and paths, both formulated as sequences of entities and relations. CoKE takes a sequence as input and uses a Transformer encoder to obtain contextualized representations. These representations are hence naturally adaptive to the input, capturing contextual meanings of entities and relations therein. Evaluation on a wide variety of public benchmarks verifies the superiority of CoKE in link prediction and path query answering. It performs consistently better than, or at least equally well as current state-of-the-art in almost every case, in particular offering an absolute improvement of 19.7% in H@10 on path query answering. Our code is available at \\url{this https URL}.",
        "year": 2019
      },
      {
        "title": "Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding",
        "abstract": "Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE. However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict. In this work, we propose a novel distance-based approach for knowledge graph link prediction. First, we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations. The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity. Second, the graph context is integrated into distance scoring functions directly. Specifically, graph context is explicitly modeled via two directed context representations. Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively. The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases. Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes.",
        "year": 2019
      },
      {
        "title": "Rot-Pro: Modeling Transitivity by Projection in Knowledge Graph Embedding",
        "abstract": "Knowledge graph embedding models learn the representations of entities and relations in the knowledge graphs for predicting missing links (relations) between entities. Their effectiveness are deeply affected by the ability of modeling and inferring different relation patterns such as symmetry, asymmetry, inversion, composition and transitivity. Although existing models are already able to model many of these relations patterns, transitivity, a very common relation pattern, is still not been fully supported. In this paper, we first theoretically show that the transitive relations can be modeled with projections. We then propose the Rot-Pro model which combines the projection and relational rotation together. We prove that Rot-Pro can infer all the above relation patterns. Experimental results show that the proposed Rot-Pro model effectively learns the transitivity pattern and achieves the state-of-the-art results on the link prediction task in the datasets containing transitive relations.",
        "year": 2021
      },
      {
        "title": "Beyond Triplets: Hyper-Relational Knowledge Graph Embedding for Link Prediction",
        "abstract": "Knowledge Graph (KG) embeddings are a powerful tool for predicting missing links in KGs. Existing techniques typically represent a KG as a set of triplets, where each triplet (h, r, t) links two entities h and t through a relation r, and learn entity/relation embeddings from such triplets while preserving such a structure. However, this triplet representation oversimplifies the complex nature of the data stored in the KG, in particular for hyper-relational facts, where each fact contains not only a base triplet (h, r, t), but also the associated key-value pairs (k, v). Even though a few recent techniques tried to learn from such data by transforming a hyper-relational fact into an n-ary representation (i.e., a set of key-value pairs only without triplets), they result in suboptimal models as they are unaware of the triplet structure, which serves as the fundamental data structure in modern KGs and preserves the essential information for link prediction. To address this issue, we propose HINGE, a hyper-relational KG embedding model, which directly learns from hyper-relational facts in a KG. HINGE captures not only the primary structural information of the KG encoded in the triplets, but also the correlation between each triplet and its associated key-value pairs. Our extensive evaluation shows the superiority of HINGE on various link prediction tasks over KGs. In particular, HINGE consistently outperforms not only the KG embedding methods learning from triplets only (by 0.81-41.45% depending on the link prediction tasks and settings), but also the methods learning from hyper-relational facts using the n-ary representation (by 13.2-84.1%).",
        "year": 2020
      },
      {
        "title": "Knowledge Graph Embedding: An Overview",
        "abstract": "Many mathematical models have been leveraged to design embeddings for representing Knowledge Graph (KG) entities and relations for link prediction and many downstream tasks. These mathematically-inspired models are not only highly scalable for inference in large KGs, but also have many explainable advantages in modeling different relation patterns that can be validated through both formal proofs and empirical results. In this paper, we make a comprehensive overview of the current state of research in KG completion. In particular, we focus on two main branches of KG embedding (KGE) design: 1) distance-based methods and 2) semantic matching-based methods. We discover the connections between recently proposed models and present an underlying trend that might help researchers invent novel and more effective models. Next, we delve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D affine operations, respectively. They encompass a broad spectrum of techniques including distance-based and semantic-based methods. We will also discuss an emerging approach for KG completion which leverages pre-trained language models (PLMs) and textual descriptions of entities and relations and offer insights into the integration of KGE embedding methods with PLMs for KG completion.",
        "year": 2023
      },
      {
        "title": "Cycle or Minkowski: Which is More Appropriate for Knowledge Graph Embedding?",
        "abstract": "Knowledge graph (KG) embedding aims to encode entities and relations into low-dimensional vector spaces, in turn, can support various machine learning models on KG related tasks with good performance. However, existing methods for knowledge graph embedding fail to consider the influence of the embedding space, which makes them still unsatisfactory in practical applications. In this study, we try to improve the expressiveness of the embedding space from the perspective of the metric. Specifically, we first point out the implications of Minkowski metric used in KG embedding and then make a quantitative analysis. To solve the limitations, we introduce a new metric, named Cycle metric, based on the oscillation property of the periodic function. Furthermore, we find that the function period has a significant influence on the expressiveness of the embedding space. Given a fully trained model, the smaller the period, the better the expressive ability. Finally, to validate the findings, we propose a new model, named CyclE by combining Cycle Metric and the popular KG embeddings models. Comprehensive experimental results show that Cycle is more appropriate than Minkowski for KG embedding.",
        "year": 2021
      },
      {
        "title": "Knowledge Graph Embedding with 3D Compound Geometric Transformations",
        "abstract": "The cascade of 2D geometric transformations were exploited to model relations between entities in a knowledge graph (KG), leading to an effective KG embedding (KGE) model, CompoundE. Furthermore, the rotation in the 3D space was proposed as a new KGE model, Rotate3D, by leveraging its non-commutative property. Inspired by CompoundE and Rotate3D, we leverage 3D compound geometric transformations, including translation, rotation, scaling, reflection, and shear and propose a family of KGE models, named CompoundE3D, in this work. CompoundE3D allows multiple design variants to match rich underlying characteristics of a KG. Since each variant has its own advantages on a subset of relations, an ensemble of multiple variants can yield superior performance. The effectiveness and flexibility of CompoundE3D are experimentally verified on four popular link prediction datasets.",
        "year": 2023
      },
      {
        "title": "Bringing Light Into the Dark: A Large-Scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework",
        "abstract": "The heterogeneity in recently published knowledge graph embedding models’ implementations, training, and evaluation has made fair and thorough comparisons difficult. To assess the reproducibility of previously published results, we re-implemented and evaluated 21 models in the PyKEEN software package. In this paper, we outline which results could be reproduced with their reported hyper-parameters, which could only be reproduced with alternate hyper-parameters, and which could not be reproduced at all, as well as provide insight as to why this might be the case. We then performed a large-scale benchmarking on four datasets with several thousands of experiments and 24,804 GPU hours of computation time. We present insights gained as to best practices, best configurations for each model, and where improvements could be made over previously published best configurations. Our results highlight that the combination of model architecture, training approach, loss function, and the explicit modeling of inverse relations is crucial for a model’s performance and is not only determined by its architecture. We provide evidence that several architectures can obtain results competitive to the state of the art when configured carefully. We have made all code, experimental configurations, results, and analyses available at https://github.com/pykeen/pykeen and https://github.com/pykeen/benchmarking.",
        "year": 2020
      },
      {
        "title": "Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis",
        "abstract": "Knowledge Graph Embeddings (KGEs) have been intensively explored in recent years due to their promise for a wide range of applications. However, existing studies focus on improving the final model performance without acknowledging the computational cost of the proposed approaches, in terms of execution time and environmental impact. This paper proposes a simple yet effective KGE framework which can reduce the training time and carbon footprint by orders of magnitudes compared with state-of-the-art approaches, while producing competitive performance. We highlight three technical innovations: full batch learning via relational matrices, closed-form Orthogonal Procrustes Analysis for KGEs, and non-negative-sampling training. In addition, as the first KGE method whose entity embeddings also store full relation information, our trained models encode rich semantics and are highly interpretable. Comprehensive experiments and ablation studies involving 13 strong baselines and two standard datasets verify the effectiveness and efficiency of our algorithm.",
        "year": 2021
      },
      {
        "title": "An Approach to Knowledge Base Completion by a Committee-Based Knowledge Graph Embedding",
        "abstract": "Knowledge bases such as Freebase, YAGO, DBPedia, and Nell contain a number of facts with various entities and relations. Since they store many facts, they are regarded as core resources for many natural language processing tasks. Nevertheless, they are not normally complete and have many missing facts. Such missing facts keep them from being used in diverse applications in spite of their usefulness. Therefore, it is significant to complete knowledge bases. Knowledge graph embedding is one of the promising approaches to completing a knowledge base and thus many variants of knowledge graph embedding have been proposed. It maps all entities and relations in knowledge base onto a low dimensional vector space. Then, candidate facts that are plausible in the space are determined as missing facts. However, any single knowledge graph embedding is insufficient to complete a knowledge base. As a solution to this problem, this paper defines knowledge base completion as a ranking task and proposes a committee-based knowledge graph embedding model for improving the performance of knowledge base completion. Since each knowledge graph embedding has its own idiosyncrasy, we make up a committee of various knowledge graph embeddings to reflect various perspectives. After ranking all candidate facts according to their plausibility computed by the committee, the top-k facts are chosen as missing facts. Our experimental results on two data sets show that the proposed model achieves higher performance than any single knowledge graph embedding and shows robust performances regardless of k. These results prove that the proposed model considers various perspectives in measuring the plausibility of candidate facts.",
        "year": 2020
      },
      {
        "title": "DualDE: Dually Distilling Knowledge Graph Embedding for Faster and Cheaper Reasoning",
        "abstract": "Knowledge Graph Embedding (KGE) is a popular method for KG reasoning and training KGEs with higher dimension are usually preferred since they have better reasoning capability. However, high-dimensional KGEs pose huge challenges to storage and computing resources and are not suitable for resource-limited or time-constrained applications, for which faster and cheaper reasoning is necessary. To address this problem, we propose DualDE, a knowledge distillation method to build low-dimensional student KGE from pre-trained high-dimensional teacher KGE. DualDE considers the dual-influence between the teacher and the student. In DualDE, we propose a soft label evaluation mechanism to adaptively assign different soft label and hard label weights to different triples, and a two-stage distillation approach to improve the student's acceptance of the teacher. Our DualDE is general enough to be applied to various KGEs. Experimental results show that our method can successfully reduce the embedding parameters of a high-dimensional KGE by 7× - 15× and increase the inference speed by 2× - 6× while retaining a high performance. We also experimentally prove the effectiveness of our soft label evaluation mechanism and two-stage distillation approach via ablation study.",
        "year": 2020
      },
      {
        "title": "DisenKGAT: Knowledge Graph Embedding with Disentangled Graph Attention Network",
        "abstract": "Knowledge graph completion (KGC) has become a focus of attention across deep learning community owing to its excellent contribution to numerous downstream tasks. Although recently have witnessed a surge of work on KGC, they are still insufficient to accurately capture complex relations, since they adopt the single and static representations. In this work, we propose a novel Disentangled Knowledge Graph Attention Network (DisenKGAT) for KGC, which leverages both micro-disentanglement and macro-disentanglement to exploit representations behind Knowledge graphs (KGs). To achieve micro-disentanglement, we put forward a novel relation-aware aggregation to learn diverse component representation. For macro-disentanglement, we leverage mutual information as a regularization to enhance independence. With the assistance of disentanglement, our model is able to generate adaptive representations in terms of the given scenario. Besides, our work has strong robustness and flexibility to adapt to various score functions. Extensive experiments on public benchmark datasets have been conducted to validate the superiority of DisenKGAT over existing methods in terms of both accuracy and explainability.",
        "year": 2021
      },
      {
        "title": "A Survey of Knowledge Graph Embedding and Their Applications",
        "abstract": "Knowledge Graph embedding provides a versatile technique for representing knowledge. These techniques can be used in a variety of applications such as completion of knowledge graph to predict missing information, recommender systems, question answering, query expansion, etc. The information embedded in Knowledge graph though being structured is challenging to consume in a real-world application. Knowledge graph embedding enables the real-world application to consume information to improve performance. Knowledge graph embedding is an active research area. Most of the embedding methods focus on structure-based information. Recent research has extended the boundary to include text-based information and image-based information in entity embedding. Efforts have been made to enhance the representation with context information. This paper introduces growth in the field of KG embedding from simple translation-based models to enrichment-based models. This paper includes the utility of the Knowledge graph in real-world applications.",
        "year": 2021
      },
      {
        "title": "Molecular-evaluated and explainable drug repurposing for COVID-19 using ensemble knowledge graph embedding",
        "abstract": "The search for an effective drug is still urgent for COVID-19 as no drug with proven clinical efficacy is available. Finding the new purpose of an approved or investigational drug, known as drug repurposing, has become increasingly popular in recent years. We propose here a new drug repurposing approach for COVID-19, based on knowledge graph (KG) embeddings. Our approach learns “ensemble embeddings” of entities and relations in a COVID-19 centric KG, in order to get a better latent representation of the graph elements. Ensemble KG-embeddings are subsequently used in a deep neural network trained for discovering potential drugs for COVID-19. Compared to related works, we retrieve more in-trial drugs among our top-ranked predictions, thus giving greater confidence in our prediction for out-of-trial drugs. For the first time to our knowledge, molecular docking is then used to evaluate the predictions obtained from drug repurposing using KG embedding. We show that Fosinopril is a potential ligand for the SARS-CoV-2 nsp13 target. We also provide explanations of our predictions thanks to rules extracted from the KG and instanciated by KG-derived explanatory paths. Molecular evaluation and explanatory paths bring reliability to our results and constitute new complementary and reusable methods for assessing KG-based drug repurposing.",
        "year": 2023
      },
      {
        "title": "Understanding Negative Sampling in Knowledge Graph Embedding",
        "abstract": "Knowledge graph embedding (KGE) is to project entities and relations of a knowledge graph (KG) into a low-dimensional vector space, which has made steady progress in recent years. Conventional KGE methods, especially translational distance-based models, are trained through discriminating positive samples from negative ones. Most KGs store only positive samples for space efficiency. Negative sampling thus plays a crucial role in encoding triples of a KG. The quality of generated negative samples has a direct impact on the performance of learnt knowledge representation in a myriad of downstream tasks, such as recommendation, link prediction and node classification. We summarize current negative sampling approaches in KGE into three categories, static distribution-based, dynamic distribution-based and custom cluster-based respectively. Based on this categorization we discuss the most prevalent existing approaches and their characteristics. It is a hope that this review can provide some guidelines for new thoughts about negative sampling in KGE.",
        "year": 2021
      },
      {
        "title": "Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation",
        "abstract": "Recommender system can provide users with the required information accurately and efficiently, playing a very important role in improving users’ life experience. Although knowledge graph-based recommender system can solve the sparsity and cold start problems faced by traditional recommender system, it cannot handle the cross-domain cold start problem and cannot provide multi-domain recommendations. Therefore, this paper focuses on multi-domain item-item (I2I) recommendation based on cross-domain knowledge graph embedding by analyzing the association between items of the same domain and the interaction between items of diverse domains with the aid of knowledge graph that contains rich information. First, a cross-domain knowledge graph chiasmal embedding approach is proposed to efficiently interact all items in multiple domains. To help achieve both homo-domain embedding and hetero-domain embedding of items, a binding rule is put forward. Second, a multi-domain I2I recommendation method is presented to efficiently recommend items in multiple domains, which is a recommendation method based on link prediction of knowledge graph. Finally, the proposed methods are compared and analyzed with some benchmark methods using two datasets. The experimental results show that the proposed methods achieve better link prediction results and multi-domain recommendation results.",
        "year": 2023
      },
      {
        "title": "ChronoR: Rotation Based Temporal Knowledge Graph Embedding",
        "abstract": "Despite the importance and abundance of temporal knowledge graphs, most of the current research has been focused on reasoning on static graphs. In this paper, we study the challenging problem of inference over temporal knowledge graphs. In particular, the task of temporal link prediction. In general, this is a difficult task due to data non-stationarity, data heterogeneity, and its complex temporal dependencies. \nWe propose Chronological Rotation embedding (ChronoR), a novel model for learning representations for entities, relations, and time. Learning dense representations is frequently used as an efficient and versatile method to perform reasoning on knowledge graphs. The proposed model learns a k-dimensional rotation transformation parametrized by relation and time, such that after each fact's head entity is transformed using the rotation, it falls near its corresponding tail entity. By using high dimensional rotation as its transformation operator, ChronoR captures rich interaction between the temporal and multi-relational characteristics of a Temporal Knowledge Graph. Experimentally, we show that ChronoR is able to outperform many of the state-of-the-art methods on the benchmark datasets for temporal knowledge graph link prediction.",
        "year": 2021
      }
    ],
    "layer3_papers": [
      {
        "title": "Contextualized Quaternion Embedding Towards Polysemy in Knowledge Graph for Link Prediction",
        "abstract": "To meet the challenge of incompleteness within Knowledge Graphs, Knowledge Graph Embedding (KGE) has emerged as the fundamental methodology for predicting the missing link (Link Prediction), by mapping entities and relations as low-dimensional vectors in continuous space. However, current KGE models often struggle with the polysemy issue, where entities exhibit different semantic characteristics depending on the relations in which they participate. Such limitation stems from weak interactions between entities and their relation contexts, leading to low expressiveness in modeling complex structures and resulting in inaccurate predictions. To address this, we propose Contextualized Quaternion Embedding (ConQuatE), a model that enhances the representation learning of entities across multiple semantic dimensions by leveraging quaternion rotation to capture diverse relational contexts. In specific, ConQuatE incorporates contextual cues from various connected relations to enrich the original entity representations. Notably, this is achieved through efficient vector transformations in quaternion space, without any extra information required other than original triples. Experimental results demonstrate that our model outperforms state-of-the-art models for Link Prediction on four widely recognized datasets: FB15k-237, WN18RR, FB15k, and WN18.",
        "year": 2025
      },
      {
        "title": "MADE: Multicurvature Adaptive Embedding for Temporal Knowledge Graph Completion",
        "abstract": "Temporal knowledge graphs (TKGs) are receiving increased attention due to their time-dependent properties and the evolving nature of knowledge over time. TKGs typically contain complex geometric structures, such as hierarchical, ring, and chain structures, which can often be mixed together. However, embedding TKGs into Euclidean space, as is typically done with TKG completion (TKGC) models, presents a challenge when dealing with high-dimensional nonlinear data and complex geometric structures. To address this issue, we propose a novel TKGC model called multicurvature adaptive embedding (MADE). MADE models TKGs in multicurvature spaces, including flat Euclidean space (zero curvature), hyperbolic space (negative curvature), and hyperspherical space (positive curvature), to handle multiple geometric structures. We assign different weights to different curvature spaces in a data-driven manner to strengthen the ideal curvature spaces for modeling and weaken the inappropriate ones. Additionally, we introduce the quadruplet distributor (QD) to assist the information interaction in each geometric space. Ultimately, we develop an innovative temporal regularization to enhance the smoothness of timestamp embeddings by strengthening the correlation of neighboring timestamps. Experimental results show that MADE outperforms the existing state-of-the-art TKGC models.",
        "year": 2024
      },
      {
        "title": "IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion",
        "abstract": "Temporal Knowledge Graphs (TKGs) incorporate a temporal dimension, allowing for a precise capture of the evolution of knowledge and reflecting the dynamic nature of the real world. Typically, TKGs contain complex geometric structures, with various geometric structures interwoven. However, existing Temporal Knowledge Graph Completion (TKGC) methods either model TKGs in a single space or neglect the heterogeneity of different curvature spaces, thus constraining their capacity to capture these intricate geometric structures. In this paper, we propose a novel Integrating Multi-curvature shared and specific Embedding (IME) model for TKGC tasks. Concretely, IME models TKGs into multi-curvature spaces, including hyperspherical, hyperbolic, and Euclidean spaces. Subsequently, IME incorporates two key properties, namely space-shared property and space-specific property. The space-shared property facilitates the learning of commonalities across different curvature spaces and alleviates the spatial gap caused by the heterogeneous nature of multi-curvature spaces, while the space-specific property captures characteristic features. Meanwhile, IME proposes an Adjustable Multi-curvature Pooling (AMP) approach to effectively retain important information. Furthermore, IME innovatively designs similarity, difference, and structure loss functions to attain the stated objective. Experimental results clearly demonstrate the superior performance of IME over existing state-of-the-art TKGC models.",
        "year": 2024
      },
      {
        "title": "FSTRE: Fuzzy Spatiotemporal RDF Knowledge Graph Embedding Using Uncertain Dynamic Vector Projection and Rotation",
        "abstract": "Knowledge graphs (KGs) use resource description framework (RDF) triples to model various crisp and static resources in the world. Meanwhile, knowledge embedded into vector space can imply more meanings. Much real-world information, however, is often uncertain and dynamic. Existing KG embedding (KGE) models are insufficient to deal with uncertain dynamic knowledge in vector spaces. To overcome this drawback, this article concentrates on an embedding module for the distributed representation of uncertain dynamic knowledge and proposes a strongly adaptive fuzzy spatiotemporal RDF embedding model (FSTRE). Specifically, we first propose a fine-grained fuzzy spatiotemporal RDF model, which provides the underlying representation framework for FSTRE. Then, within the complex vector space, spatial and temporal information is embedded by projection and rotation, respectively. Fine-grained fuzziness penetrates each element of the spatiotemporal five-tuples by a modal length of the anisotropic vectors. By using geometric operations as its transformation operator, FSTRE can capture the rich interaction between crisp and static knowledge and fuzzy spatiotemporal knowledge. We performed an experimental evaluation of FSTRE based on the built fuzzy spatiotemporal KG. It was shown that our FSTRE model is superior to state-of-the-art methods and can handle complex fuzzy spatiotemporal knowledge.",
        "year": 2024
      },
      {
        "title": "Multihop Fuzzy Spatiotemporal RDF Knowledge Graph Query via Quaternion Embedding",
        "abstract": "The proliferation of uncertain spatiotemporal data has led to an increasing demand for fuzzy spatiotemporal knowledge modeling in various applications. However, performing multihop query modeling on incomplete fuzzy spatiotemporal knowledge graphs (KGs) poses significant challenges. Recently, embedding-based multihop KG querying approaches have gained attention. Yet, these approaches often overlook KG uncertainty and spatiotemporal sensitivity, resulting in the neglect of fuzzy spatiotemporal information during multihop path reasoning. To address these challenges, we propose an embedding-based multihop query model for fuzzy spatiotemporal KG. We use quaternion to jointly embed spatiotemporal entities, and relations are represented as rotations from spatiotemporal subject to object. We incorporate uncertainty by the scoring function's bias factor, allowing for relaxation embedding. This approach facilitates the learning of a richer representation of fuzzy spatiotemporal KGs in vector space. By exploiting the inherent noncommutative compositional pattern of quaternions, we construct more accurate multihop paths within fuzzy spatiotemporal KGs, thus improving path reasoning performance. To evaluate the effectiveness of our model, we conduct experiments on two fuzzy spatiotemporal KG datasets, focusing on link prediction and path query answering. Results show that our proposed method significantly outperforms several state-of-the-art baselines in terms of performance metrics.",
        "year": 2024
      },
      {
        "title": "Learning Dynamic Knowledge Graph Embedding in Evolving Service Ecosystems via Meta-Learning",
        "abstract": "In the context of dynamic service ecosystems, the inability of conventional knowledge graph embedding (KGE) methods to efficiently update incremental knowledge poses a significant challenge for the effectiveness of intelligent web applications. To address the continuous updating challenges of service knowledge, this paper introduces MetaHG, a meta-learning strategy for KGE. Unlike existing meta-learning KGE studies that focus solely on local entity information, MetaHG incorporates both local and potential global structural information from current snapshot’s seen knowledge graphs (KGs) to mitigate issues such as spatial deformation and enhance the representation of unseen entities. Our approach initializes entity embeddings using ‘in’ and ‘out’ relationship matrices and refines them through a hybrid graph neural network (GNN) framework, which includes a GNN layer for local information and a hypergraph neural network (HGNN) layer for potential global information. The meta-learning strategy embedded in MetaHG effectively transfers meta-knowledge for the accurate representation of emerging entities. Extensive experiments are conducted on a self-collected clothing industry service dataset and two publicly available open-source KG datasets. By comparing with several baselines, experiment results demonstrate the superior performance of MetaHG in generating high-quality embeddings for emerging entities and dynamically updating service knowledge.",
        "year": 2024
      },
      {
        "title": "Knowledge graph embedding closed under composition",
        "abstract": "Knowledge Graph Embedding (KGE) has attracted increasing attention. Relation patterns, such as symmetry and inversion, have received considerable focus. Among them, composition patterns are particularly important, as they involve nearly all relations in KGs. However, prior KGE approaches often consider relations to be compositional only if they are well-represented in the training data. Consequently, it can lead to performance degradation, especially for under-represented composition patterns. To this end, we propose HolmE, a general form of KGE with its relation embedding space closed under composition, namely that the composition of any two given relation embeddings remains within the embedding space. This property ensures that every relation embedding can compose, or be composed by other relation embeddings. It enhances HolmE’s capability to model under-represented (also called long-tail) composition patterns with limited learning instances. To our best knowledge, our work is pioneering in discussing KGE with this property of being closed under composition. We provide detailed theoretical proof and extensive experiments to demonstrate the notable advantages of HolmE in modelling composition patterns, particularly for long-tail patterns. Our results also highlight HolmE’s effectiveness in extrapolating to unseen relations through composition and its state-of-the-art performance on benchmark datasets.",
        "year": 2024
      },
      {
        "title": "Poisoning Attack on Federated Knowledge Graph Embedding",
        "abstract": "Federated Knowledge Graph Embedding (FKGE) is an emerging collaborative learning technique for deriving expressive representations (i.e., embeddings) from client-maintained distributed knowledge graphs (KGs). However, poisoning attacks in FKGE, which lead to biased decisions by downstream applications, remain unexplored. This paper is the first work to systematize the risks of FKGE poisoning attacks, from which we develop a novel framework for poisoning attacks that force the victim client to predict specific false facts. Unlike centralized KGEs, FKGE maintains KGs locally, making direct injection of poisoned data challenging. Instead, attackers must create poisoned data without access to the victim's KG and inject it indirectly through FKGE aggregation. Specifically, to create poisoned data, the attacker first infers the targeted relations in the victim's local KG via a new KG component inference attack. Then, to accurately mislead the victim's embeddings via aggregation, the attacker locally trains a shadow model using the poisoned data and uses an optimized dynamic poisoning scheme to adjust the model and generate progressive poisoned updates. Our experimental results demonstrate the attack's effectiveness, achieving a remarkable success rate on various KGE models (e.g., 100% on TransE with WN18RR) while keeping the original task's performance nearly unchanged.",
        "year": 2024
      },
      {
        "title": "Convolutional Neural Network-Based Entity-Specific Common Feature Aggregation for Knowledge Graph Embedding Learning",
        "abstract": "Deep learning models present impressive capability for automatic feature extraction, where common features-based aggregation have demonstrated valuable potential in improving the model performance on text classification, sentiment analysis, etc. However, leveraging entity-specific common feature aggregation for enhancing knowledge graph representation learning has not been fully explored yet, though diverse strategies in knowledge graph embedding models have been developed in recent years. This paper proposes an innovative Convolutional Neural Network-based Entity-specific Common Feature Aggregation strategy named CNN-ECFA. Besides, a new universal framework based on the CNN-ECFA strategy is introduced for knowledge graph embedding learning. Experiments are conducted on publicly-available standard datasets for a link prediction task including WN18RR, YAGO3-10 and NELL-995. Results show that the CNN-ECFA strategy outperforms the state-of-the-art feature projection strategies with average improvements of 0.6% and 0.7% of MRR and Hits@1 on all the datasets, demonstrating our CNN-ECFA strategy is more effective for knowledge graph embedding learning. In addition, our universal framework significantly outperforms a generalized relation learning framework on WN18RR and NELL-995 with average improvements of 1.7% and 1.9% on MRR and Hits@1. The source code is publicly available at https://github.com/peterhu95/ConvE-CNN-ECFA.",
        "year": 2024
      },
      {
        "title": "A Semantic Enhanced Knowledge Graph Embedding Model With AIGC Designed for Healthcare Prediction",
        "abstract": "AI technology has been often employed to establish knowledge graph embedding (KGE) model, which can be used for link prediction on medical knowledge graph to help medical decision-making and disease prediction. However, traditional knowledge graph completion models usually focus on exploiting simple structural features during the phase of feature learning while neglecting the complex structural feature. Considering AI-generated content (AIGC) has shown great potentials for healthcare electronics (HE), a knowledge graph embedding model with AIGC called SEConv is proposed for medical knowledge graph completion. Firstly, a less resource-consuming model of self-attention mechanism is introduced to generate more expressive embedding representations, which contributes to deploying on resource-limited consumer electronics. Secondly, in order to extract more informative features from the triplets, a multilayer convolutional neural network is adopted to learn deeper structural features. Experiments have been implemented on the medical dataset of UMLS and DBpedia50, and other two benchmark datasets. And the results show that SEConv excels in learning more expressive and discriminative feature representations. Compared with the baseline models, SEConv achieves a substantial improvement, which verifies it can be used for healthcare prediction task and smart healthcare treatments.",
        "year": 2025
      },
      {
        "title": "MQuinE: a Cure for “Z-paradox” in Knowledge Graph Embedding",
        "abstract": "Knowledge graph embedding (KGE) models achieved state-of-the-art results on many knowledge graph tasks including link prediction and information retrieval. Despite the superior performance of KGE models in practice, we discover a deficiency in the expressiveness of some popular existing KGE models called Z-paradox. Motivated by the existence of Z-paradox, we propose a new KGE model called MQuinE that does not suffer from Z-paradox while preserves strong expressiveness to model various relation patterns including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations with theoretical justification. Experiments on real-world knowledge bases indicate that Z-paradox indeed degrades the performance of existing KGE models, and can cause more than 20% accuracy drop on some challenging test samples. Our experiments further demonstrate that MQuinE can mitigate the negative impact of Z-paradox and outperform existing KGE models by a visible margin on link prediction tasks.",
        "year": 2024
      },
      {
        "title": "SpherE: Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval",
        "abstract": "Knowledge graphs (KGs), which store an extensive number of relational facts (head, relation, tail), serve various applications. While many downstream tasks highly rely on the expressive modeling and predictive embedding of KGs, most of the current KG representation learning methods, where each entity is embedded as a vector in the Euclidean space and each relation is embedded as a transformation, follow an entity ranking protocol. On one hand, such an embedding design cannot capture many-to-many relations. On the other hand, in many retrieval cases, the users wish to get an exact set of answers without any ranking, especially when the results are expected to be precise, e.g., which genes cause an illness. Such scenarios are commonly referred to as \"set retrieval\". This work presents a pioneering study on the KG set retrieval problem. We show that the set retrieval highly depends on expressive modeling of many-to-many relations, and propose a new KG embedding model SpherE to address this problem. SpherE is based on rotational embedding methods, but each entity is embedded as a sphere instead of a vector. While inheriting the high interpretability of rotational-based models, our SpherE can more expressively model one-to-many, many-to-one, and many-to-many relations. Through extensive experiments, we show that our SpherE can well address the set retrieval problem while still having a good predictive ability to infer missing facts. The code is available at https://github.com/Violet24K/SpherE.",
        "year": 2024
      },
      {
        "title": "TGformer: A Graph Transformer Framework for Knowledge Graph Embedding",
        "abstract": "Knowledge graph embedding is efficient method for reasoning over known facts and inferring missing links. Existing methods are mainly triplet-based or graph-based. Triplet-based approaches learn the embedding of missing entities by a single triple only. They ignore the fact that the knowledge graph is essentially a graph structure. Graph-based methods consider graph structure information but ignore the contextual information of nodes in the knowledge graph, making them unable to discern valuable entity (relation) information. In response to the above limitations, we propose a general graph transformer framework for knowledge graph embedding (TGformer). It is the first to use a graph transformer to build knowledge embeddings with triplet-level and graph-level structural features in the static and temporal knowledge graph. Specifically, a context-level subgraph is constructed for each predicted triplet, which models the relation between triplets with the same entity. Afterward, we design a knowledge graph transformer network (KGTN) to fully explore multi-structural features in knowledge graphs, including triplet-level and graph-level, boosting the model to understand entities (relations) in different contexts. Finally, semantic matching is adopted to select the entity with the highest score. Experimental results on several public knowledge graph datasets show that our method can achieve state-of-the-art performance in link prediction.",
        "year": 2025
      },
      {
        "title": "Mixed Geometry Message and Trainable Convolutional Attention Network for Knowledge Graph Completion",
        "abstract": "Knowledge graph completion (KGC) aims to study the embedding representation to solve the incompleteness of knowledge graphs (KGs). Recently, graph convolutional networks (GCNs) and graph attention networks (GATs) have been widely used in KGC tasks by capturing neighbor information of entities. However, Both GCNs and GATs based KGC models have their limitations, and the best method is to analyze the neighbors of each entity (pre-validating), while this process is prohibitively expensive. Furthermore, the representation quality of the embeddings can affect the aggregation of neighbor information (message passing). To address the above limitations, we propose a novel knowledge graph completion model with mixed geometry message and trainable convolutional attention network named MGTCA. Concretely, the mixed geometry message function generates rich neighbor message by integrating spatially information in the hyperbolic space, hypersphere space and Euclidean space jointly. To complete the autonomous switching of graph neural networks (GNNs) and eliminate the necessity of pre-validating the local structure of KGs, a trainable convolutional attention network is proposed by comprising three types of GNNs in one trainable formulation. Furthermore, a mixed geometry scoring function is proposed, which calculates scores of triples by novel prediction function and similarity function based on different geometric spaces. Extensive experiments on three standard datasets confirm the effectiveness of our innovations, and the performance of MGTCA is significantly improved compared to the state-of-the-art approaches.",
        "year": 2024
      },
      {
        "title": "Communication-Efficient Federated Knowledge Graph Embedding with Entity-Wise Top-K Sparsification",
        "abstract": "Federated Knowledge Graphs Embedding learning (FKGE) encounters challenges in communication efficiency stemming from the considerable size of parameters and extensive communication rounds. However, existing FKGE methods only focus on reducing communication rounds by conducting multiple rounds of local training in each communication round, and ignore reducing the size of parameters transmitted within each communication round. To tackle the problem, we first find that universal reduction in embedding precision across all entities during compression can significantly impede convergence speed, underscoring the importance of maintaining embedding precision. We then propose bidirectional communication-efficient FedS based on Entity-Wise Top-K Sparsification strategy. During upload, clients dynamically identify and upload only the Top-K entity embeddings with the greater changes to the server. During download, the server first performs personalized embedding aggregation for each client. It then identifies and transmits the Top-K aggregated embeddings to each client. Besides, an Intermittent Synchronization Mechanism is used by FedS to mitigate negative effect of embedding inconsistency among shared entities of clients caused by heterogeneity of Federated Knowledge Graph. Extensive experiments across three datasets showcase that FedS significantly enhances communication efficiency with negligible (even no) performance degradation.",
        "year": 2024
      },
      {
        "title": "Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph",
        "abstract": "Federated Knowledge Graph Embedding (FKGE) has recently garnered considerable interest due to its capacity to extract expressive representations from distributed knowledge graphs, while concurrently safeguarding the privacy of individual clients. Existing FKGE methods typically harness the arithmetic mean of entity embeddings from all clients as the global supplementary knowledge, and learn a replica of global consensus entities embeddings for each client. However, these methods usually neglect the inherent semantic disparities among distinct clients. This oversight not only results in the globally shared complementary knowledge being inundated with too much noise when tailored to a specific client, but also instigates a discrepancy between local and global optimization objectives. Consequently, the quality of the learned embeddings is compromised. To address this, we propose Personalized Federated knowledge graph Embedding with client-wise relation Graph (PFedEG), a novel approach that employs a client-wise relation graph to learn personalized embeddings by discerning the semantic relevance of embeddings from other clients. Specifically, PFedEG learns personalized supplementary knowledge for each client by amalgamating entity embedding from its neighboring clients based on their\"affinity\"on the client-wise relation graph. Each client then conducts personalized embedding learning based on its local triples and personalized supplementary knowledge. We conduct extensive experiments on four benchmark datasets to evaluate our method against state-of-the-art models and results demonstrate the superiority of our method.",
        "year": 2024
      }
    ],
    "layer2_summary": "This citation path reveals a dynamic evolution in Knowledge Graph Embedding (KGE) research, driven by the continuous pursuit of more expressive, robust, and efficient models.\n\n1.  **Methodological Evolution:**\n    The methodological evolution begins with foundational translation-based models (highlighted by Paper 5 and 12), which represent relations as simple translations between entity embeddings. This quickly progressed to more sophisticated geometric transformations, with Paper 3 (LineaRE) modeling relations as linear functions, Paper 16 extending RotatE with orthogonal transforms, Paper 4 (CompoundE) introducing compound translation, rotation, and scaling operations, and Paper 11 (HousE) leveraging Householder transformations for superior capacity. Concurrently, neural network architectures emerged, with Paper 15 (CoKE) utilizing Transformers for contextualized embeddings, and Papers 9 (ReInceptionE) and 10 (AcrE) employing CNNs with inception and atrous convolutions, respectively, to enhance feature interactions. Later, the field diversified to incorporate auxiliary information, such as entity types (Paper 1, TransET) and language semantics (Paper 14, Joint Language Semantic), and to address specific challenges like temporal dynamics (Paper 6, ATiSE; Paper 7, TeRo) and inductive settings (Paper 8, MorsE).\n\n2.  **Knowledge Progression:**\n    The core problem addressed is the accurate and comprehensive representation of entities and relations in low-dimensional vector spaces for tasks like link prediction. Early translation models, while foundational, struggled with complex relation patterns (e.g., 1-to-N, N-to-1, N-to-N, symmetry, transitivity). This limitation spurred innovations like Paper 3 (LineaRE) and Paper 16 (Orthogonal Relation Transforms) to explicitly model various relation patterns. Paper 4 (CompoundE) and Paper 11 (HousE) further advanced this by proposing generalized geometric operations, with HousE being a generalization of rotation-based models. Paper 17 (Rot-Pro) specifically tackled the challenge of modeling transitivity, a common but often unsupported pattern. The need for richer context led to models like Paper 15 (CoKE), which learns dynamic, contextualized embeddings, and Paper 1 (TransET), which leverages entity types. Paper 18 (HINGE) addresses the oversimplification of hyper-relational facts, moving \"Beyond Triplets.\" Furthermore, Paper 8 (MorsE) introduced meta-knowledge transfer to overcome the limitation of existing KGE methods in inductive settings with unseen entities. Efficiency concerns were addressed by Paper 13 (Knowledge Graph Embedding Compression), while Paper 20 (Cycle or Minkowski) explored the influence of embedding space metrics, leading to new insights into expressiveness.\n\n3.  **Temporal Context:**\n    The publication timing shows a rapid acceleration and diversification of KGE research, particularly from 2019 to 2022. Early papers in 2019-2020 (e.g., Paper 6, 7, 15, 16) began exploring temporal aspects, neural architectures, and graph context. The period of 2020-2022 saw a surge in sophisticated geometric models (Paper 3, 4, 11), inductive capabilities (Paper 8), hyper-relational KGE (Paper 18), and theoretical explorations of embedding spaces (Paper 20). The field continues to evolve, with recent papers (2022-2023) including comprehensive surveys (Paper 5, 12, 19) and advanced models like Paper 2 (TranS) and Paper 14 (Joint Language Semantic), indicating a maturing yet still highly active research area.\n\n4.  **Synthesis:**\n    Collectively, these works narrate a continuous quest to enhance the expressiveness and applicability of KGE models. The progression moves from basic representation learning to sophisticated methods capable of capturing complex relation patterns, temporal dynamics, contextual nuances, and inductive generalization. The collective contribution to \"knowledge graph embedding\" is a robust and diverse toolkit of methodologies, ranging from advanced geometric transformations and neural architectures to the integration of auxiliary information and theoretical considerations of embedding spaces, all aimed at improving the accuracy, efficiency, and real-world utility of knowledge graph reasoning."
  },
  "68f34ed64fdf07bb1325097c93576658e061231e": {
    "seed_title": "A Survey on Knowledge Graph Embedding: Approaches, Applications and Benchmarks",
    "summary": "This analysis focuses on the evolution of \"knowledge graph embedding\" as described within the provided survey paper, treating the survey itself as a comprehensive overview of the field's development up to its publication year.\n\n---\n\n### Analysis of Knowledge Graph Embedding Research Evolution\n\n**1. Methodological Evolution**\nThe field of Knowledge Graph Embedding (KGE) has undergone significant methodological shifts, moving from direct symbolic logic representations, which faced computational and management limitations, to embedding entities and relations into low-dimensional vector spaces. Paper 1 (2022) categorizes these embedding methods into three main types: translational distance models, semantic matching models, and neural network-based models. This progression highlights an evolution from simpler geometric or algebraic approaches, which model relationships based on distances or scores in the embedding space, towards more sophisticated, data-driven techniques leveraging deep learning architectures to capture complex semantic patterns.\n\n**2. Knowledge Progression**\nThe primary problem addressed by KGE is the inherent limitations of directly using symbolic logic for Knowledge Graphs (KGs), specifically computational inefficiency, management difficulties with growing data, and challenges in achieving expected results in downstream tasks. Paper 1 (2022) explains that KGE addresses these by transforming symbolic knowledge into dense, continuous vector representations, thereby converting complex KG problems into more efficient vector operations. This progression within KGE, as outlined by the survey, moves from foundational embedding techniques to more advanced neural network models, continually striving to capture richer semantics and improve performance. The key new capabilities emerging from this evolution include enhanced computational efficiency, improved scalability for large KGs, and the ability for KGE models to serve as powerful pre-trained components for various downstream applications, particularly those leveraging deep learning.\n\n**3. Temporal Context**\nThe publication of Paper 1 (2022) as a comprehensive survey indicates a mature yet still evolving field, with its development closely intertwined with broader technological and theoretical advances. The abstract's mention of the \"explosive growth of Internet capacity\" and the utility of KGE for \"applications based on deep learning\" directly links the rise of KGE to the era of big data and the rapid advancements in deep learning. This suggests a period of significant acceleration in KGE research, particularly as neural network-based methods gained prominence, offering new avenues for modeling complex relationships within KGs.\n\n**4. Synthesis**\nThe unified narrative connecting the works summarized by Paper 1 (2022) is the strategic shift from explicit, symbolic knowledge representation to implicit, dense vector representations to overcome the practical limitations of traditional KGs. The collective contribution of the research summarized in this survey is the establishment of a robust and diverse set of embedding techniques that make KGs more computationally tractable, scalable, and adaptable for integration with modern machine learning paradigms. This transformation has unlocked the potential of KGs for a wide array of downstream applications, solidifying KGE as a critical subfield in knowledge representation and artificial intelligence.",
    "path": [
      "68f34ed64fdf07bb1325097c93576658e061231e",
      "f470e11faa6200026cf39e248510070c078e509a"
    ],
    "layer1_papers": [
      {
        "title": "A Survey on Knowledge Graph Embedding: Approaches, Applications and Benchmarks",
        "abstract": "A knowledge graph (KG), also known as a knowledge base, is a particular kind of network structure in which the node indicates entity and the edge represent relation. However, with the explosion of network volume, the problem of data sparsity that causes large-scale KG systems to calculate and manage difficultly has become more significant. For alleviating the issue, knowledge graph embedding is proposed to embed entities and relations in a KG to a low-, dense and continuous feature space, and endow the yield model with abilities of knowledge inference and fusion. In recent years, many researchers have poured much attention in this approach, and we will systematically introduce the existing state-of-the-art approaches and a variety of applications that benefit from these methods in this paper. In addition, we discuss future prospects for the development of techniques and application trends. Specifically, we first introduce the embedding models that only leverage the information of observed triplets in the KG. We illustrate the overall framework and specific idea and compare the advantages and disadvantages of such approaches. Next, we introduce the advanced models that utilize additional semantic information to improve the performance of the original methods. We divide the additional information into two categories, including textual descriptions and relation paths. The extension approaches in each category are described, following the same classification criteria as those defined for the triplet fact-based models. We then describe two experiments for comparing the performance of listed methods and mention some broader domain tasks such as question answering, recommender systems, and so forth. Finally, we collect several hurdles that need to be overcome and provide a few future research directions for knowledge graph embedding.",
        "summary": "",
        "year": 2020
      }
    ],
    "layer2_papers": [
      {
        "title": "A Survey on Knowledge Graph Embedding",
        "abstract": "Knowledge graph (KG) is used to represent the relationships between different concepts in the real world. It is a special network in which nodes represent entities and edges represent relationships. KGs can intuitively model the connections between facts, but in many applications, there are certain limitations in directly using symbolic logic to represent knowledge in KGs and perform calculations, making it difficult to achieve expected results in downstream tasks. Meanwhile, with the explosive growth of Internet capacity, the traditional KG structure faces the problems of computational inefficiency and management difficulties. To alleviate these problems, Knowledge graph embedding (KGE) is proposed to improve the computational efficiency by embedding entities and relations in the KG into a low-dimensional, dense and continuous vector space, and thus the solution of some problems in the knowledge graph is transformed into vector operations. Moreover, KGE also can be used as a pre-trained model which is more beneficial to downstream applications, such as applications based on deep learning. In this paper, we classify KGE into three categories, namely translational distance models, semantic matching models and neural network based models. The advantages and disadvantages of different embedding methods are compared, while the main applications of KGE are summarized. Some current challenges of KGE are summarized, as well as some views on the future research directions of KGE.",
        "year": 2022
      }
    ],
    "layer3_papers": [],
    "layer2_summary": "**Note on Provided Data:** The prompt indicated a \"CITATION PATH 2 papers\" but only provided one paper. Therefore, this analysis will focus on the provided paper (referred to as \"Paper 1\") as a survey that describes the evolution of the \"knowledge graph embedding\" field up to its publication year, rather than analyzing the evolution *between* two distinct papers.\n\n---\n\n**Analysis of \"A Survey on Knowledge Graph Embedding\" (2022)**\n\n1.  **Methodological Evolution:**\n    Paper 1 identifies a clear methodological evolution within Knowledge Graph Embedding (KGE), categorizing methods into translational distance models, semantic matching models, and neural network based models. This progression reflects a shift from simpler geometric transformations and similarity functions towards more complex, data-driven architectures, particularly leveraging deep learning for richer and more nuanced representations. The inclusion of neural network-based models highlights a significant innovation in the field, moving beyond traditional statistical or geometric approaches.\n\n2.  **Knowledge Progression:**\n    Paper 1 highlights that KGE emerged to address fundamental problems with traditional KGs, specifically the limitations of directly using symbolic logic for computation and the challenges of computational inefficiency and management difficulties with growing data. KGE builds upon these limitations by transforming symbolic knowledge into a low-dimensional, dense, and continuous vector space, enabling efficient vector operations and serving as a pre-trained model for downstream applications. This approach offers new capabilities in handling large-scale KGs and integrating knowledge into machine learning tasks, overcoming the rigidity and scalability issues of purely symbolic representations.\n\n3.  **Temporal Context:**\n    Published in 2022, Paper 1 provides a contemporary overview of KGE, reflecting the field's maturity and its increasing integration with deep learning advancements. The emergence of \"neural network based models\" as a distinct category underscores the significant impact of deep learning on KGE research, aligning with broader trends in AI during the late 2010s and early 2020s, indicating a period of accelerated development driven by new computational paradigms.\n\n4.  **Synthesis:**\n    Paper 1 offers a unified narrative of KGE's development, framing it as a crucial paradigm shift from symbolic to distributed representations for knowledge graphs. Its collective contribution is to synthesize the diverse landscape of KGE methods, applications, and challenges, providing a structured understanding of how the field has evolved to make knowledge more computationally tractable and applicable in modern AI systems. This survey consolidates the foundational work and recent advancements, setting a benchmark for future research directions in knowledge graph embedding."
  },
  "85064a4b1b96863af4fccff9ad34ce484945ad7b": {
    "seed_title": "Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces",
    "summary": "This analysis focuses on \"Paper 1: Knowledge Graph Embedding: An Overview\" (2023), interpreting its content as a synthesis of the field's evolution, given that it is the sole paper provided in the \"citation path.\"\n\n1.  **Methodological Evolution:**\n    Paper 1, \"Knowledge Graph Embedding: An Overview\" (2023), meticulously synthesizes the methodological evolution in Knowledge Graph Embedding (KGE) research. It delineates a clear progression from foundational mathematically-inspired models, which prioritize scalability and explainability, towards increasingly sophisticated designs. The paper primarily categorizes these into distance-based methods, which model relations as transformations in an embedding space, and semantic matching-based methods, which evaluate the plausibility of triples through scoring functions. A significant innovation highlighted is the development of models like CompoundE and CompoundE3D, which draw inspiration from 2D and 3D affine operations. These models represent a methodological advancement by encompassing and generalizing aspects of both prior distance-based and semantic matching approaches, demonstrating a trend towards more unified and expressive frameworks. Crucially, Paper 1 also identifies an emerging, transformative shift towards integrating pre-trained language models (PLMs) with KGE, leveraging rich textual descriptions of entities and relations to enhance embedding quality and address the limitations of purely structural methods.\n\n2.  **Knowledge Progression:**\n    The central problem driving KGE research, as articulated by Paper 1, is the effective representation of Knowledge Graph (KG) entities and relations for tasks like link prediction and KG completion. The progression of knowledge in the field has been marked by continuous efforts to overcome the limitations of earlier models. Initial methods focused on capturing basic relational patterns, but subsequent work, implicitly reviewed by Paper 1, sought to model more complex and diverse relation types. The introduction of CompoundE and CompoundE3D exemplifies this, offering more nuanced ways to represent relational semantics through affine transformations, thereby building upon the representational power of prior distance and semantic matching models. Furthermore, the integration of PLMs represents a significant leap in capability, allowing KGE models to incorporate external, rich textual context. This addresses a key limitation of traditional KGE, which often struggles with sparse KGs or entities lacking sufficient structural connections, by providing a powerful mechanism to infer meaning from textual descriptions, leading to more robust and context-aware embeddings.\n\n3.  **Temporal Context:**\n    Published in 2023, Paper 1 offers a highly contemporary perspective on the KGE landscape, directly reflecting the rapid technological and theoretical advancements of the preceding years. Its comprehensive overview captures the field at a point of significant acceleration, particularly evident in the discussion of PLM integration. This timing underscores how the maturity of large language models has profoundly influenced KGE research, enabling novel hybrid approaches that were not feasible a decade prior. The paper effectively bridges the gap between established KGE paradigms and the cutting-edge intersection with natural language processing, highlighting a period of intense innovation.\n\n4.  **Synthesis:**\n    The unified narrative connecting the diverse works reviewed in Paper 1 is the relentless pursuit of more effective, expressive, and robust representations for knowledge graphs, evolving from purely structural mathematical models to sophisticated hybrid approaches. The collective contribution of the research synthesized by Paper 1 is the establishment of a multifaceted and dynamic framework for KGE. This framework is characterized by a rich array of modeling techniques, a deep understanding of relation patterns, and an increasing emphasis on leveraging multi-modal information, particularly textual context, to significantly enhance KG completion, link prediction, and various downstream tasks, thereby pushing the boundaries of automated knowledge reasoning.",
    "path": [
      "85064a4b1b96863af4fccff9ad34ce484945ad7b",
      "f2b924e69735fb7fd6fd95c6a032954480862029"
    ],
    "layer1_papers": [
      {
        "title": "Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces",
        "abstract": "Knowledge graph embedding (KGE) is an increasingly popular technique that aims to represent entities and relations of knowledge graphs into low-dimensional semantic spaces for a wide spectrum of applications such as link prediction, knowledge reasoning and knowledge completion. In this article, we provide a systematic review of existing KGE techniques based on representation spaces. Particularly, we build a fine-grained classification to categorise the models based on three mathematical perspectives of the representation spaces: (1) algebraic perspective, (2) geometric perspective and (3) analytical perspective. We introduce the rigorous definitions of fundamental mathematical spaces before diving into KGE models and their mathematical properties. We further discuss different KGE methods over the three categories, as well as summarise how spatial advantages work over different embedding needs. By collating the experimental results from downstream tasks, we also explore the advantages of mathematical space in different scenarios and the reasons behind them. We further state some promising research directions from a representation space perspective, with which we hope to inspire researchers to design their KGE models as well as their related applications with more consideration of their mathematical space properties.",
        "summary": "",
        "year": 2022
      }
    ],
    "layer2_papers": [
      {
        "title": "Knowledge Graph Embedding: An Overview",
        "abstract": "Many mathematical models have been leveraged to design embeddings for representing Knowledge Graph (KG) entities and relations for link prediction and many downstream tasks. These mathematically-inspired models are not only highly scalable for inference in large KGs, but also have many explainable advantages in modeling different relation patterns that can be validated through both formal proofs and empirical results. In this paper, we make a comprehensive overview of the current state of research in KG completion. In particular, we focus on two main branches of KG embedding (KGE) design: 1) distance-based methods and 2) semantic matching-based methods. We discover the connections between recently proposed models and present an underlying trend that might help researchers invent novel and more effective models. Next, we delve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D affine operations, respectively. They encompass a broad spectrum of techniques including distance-based and semantic-based methods. We will also discuss an emerging approach for KG completion which leverages pre-trained language models (PLMs) and textual descriptions of entities and relations and offer insights into the integration of KGE embedding methods with PLMs for KG completion.",
        "year": 2023
      }
    ],
    "layer3_papers": [],
    "layer2_summary": "This analysis interprets the provided single paper as the culmination of a \"citation path,\" with its content reflecting a comprehensive overview of prior, foundational work in Knowledge Graph Embedding (KGE). The analysis therefore infers the characteristics of earlier KGE research that \"Paper 1: Knowledge Graph Embedding: An Overview\" (2023) synthesizes and builds upon.\n\n---\n\n### Analysis of Citation Path\n\n**1. Methodological Evolution**\nThe methodological evolution in this path progresses from the development of distinct, mathematically-inspired models to a meta-analysis and integration of these diverse techniques. Earlier KGE research (implicitly foundational to Paper 1) focused on establishing two primary paradigms: \"distance-based methods\" and \"semantic matching-based methods,\" each designed to model specific relation patterns within Knowledge Graphs (KGs). Paper 1 (2023) marks a significant shift by moving beyond individual model design to discover connections and underlying trends across these foundational approaches. Crucially, it highlights the emergence of more encompassing models like CompoundE and CompoundE3D, which generalize a broad spectrum of techniques, and, most notably, the integration of KGE with pre-trained language models (PLMs), signaling a major methodological shift towards hybrid, multi-modal embedding strategies that leverage both structural and textual information.\n\n**2. Knowledge Progression**\nThe knowledge progression addresses the initial problem of effectively representing KG entities and relations for tasks like link prediction, aiming for scalable and explainable models. Earlier KGE research laid the groundwork by developing methods to capture the structural semantics of KGs. Paper 1 (2023) builds upon this by tackling the challenge of synthesizing a rapidly diversifying field, offering a comprehensive overview that unifies disparate techniques by revealing their underlying connections and common principles. It introduces advanced models such as CompoundE and CompoundE3D, which offer more sophisticated geometric interpretations and encompass a wider range of existing methods. Furthermore, Paper 1 significantly expands the scope of KGE by discussing the integration of PLMs, enabling new capabilities to leverage rich textual descriptions of entities and relations, thereby moving beyond the limitations of purely structural embeddings and enhancing KG completion with external semantic knowledge.\n\n**3. Temporal Context**\nThe foundational KGE methods (implicitly reviewed by Paper 1) likely emerged and matured in the decade preceding 2023, establishing the core paradigms of distance-based and semantic matching approaches. Paper 1's publication in 2023 is highly timely, reflecting a period of significant consolidation in KGE research and the rapid acceleration of AI, particularly with the transformative rise of large PLMs in the late 2010s and early 2020s. This temporal positioning allows Paper 1 to provide a critical overview that not only synthesizes established KGE paradigms but also immediately incorporates the cutting-edge integration with PLMs, highlighting a rapid acceleration in methodological convergence and the adoption of new technological advances.\n\n**4. Synthesis**\nThis path illustrates a unified narrative of KGE research, evolving from the initial development of specialized mathematical models for representing KG structures to a sophisticated understanding of their interconnections and, ultimately, their powerful integration with advanced language models. The collective contribution is the establishment of a robust and increasingly comprehensive framework for Knowledge Graph Embedding. This framework has progressed from foundational structural methods to a mature, hybrid approach capable of leveraging both graph topology and rich textual semantics, significantly advancing the capabilities of KGs for various AI tasks such as link prediction, knowledge base completion, and question answering."
  },
  "83d58bc46b7adb92d8750da52313f060b10f201d": {
    "seed_title": "HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding",
    "summary": "This citation path illustrates a rapid and sophisticated evolution in knowledge graph embedding, particularly focusing on temporal, spatial, and uncertain aspects.\n\n### 1. Methodological Evolution\nThe methodological evolution in these papers demonstrates a significant shift from traditional Euclidean embeddings to more sophisticated geometric and algebraic spaces for capturing complex knowledge graph properties. Papers 1 (MADE) and 2 (IME) showcase an evolution in modeling complex geometric structures within Temporal Knowledge Graphs (TKGs) by moving to multi-curvature spaces (Euclidean, hyperbolic, hyperspherical). Paper 1 introduces adaptive weighting and a quadruplet distributor for these spaces, while Paper 2 refines this by integrating space-shared and space-specific properties with an adjustable pooling mechanism to better handle the inherent heterogeneity of multi-curvature spaces. Concurrently, Papers 3 (FSTRE) and 4 evolve Knowledge Graph Embedding (KGE) to address uncertainty and spatiotemporal dynamics, with Paper 3 using projection and rotation in complex vector space with fine-grained fuzziness, and Paper 4 leveraging quaternion embeddings and their non-commutative properties for multihop querying in fuzzy spatiotemporal KGs.\n\n### 2. Knowledge Progression\nThese works collectively address the critical problems of effectively modeling complex geometric structures in Temporal Knowledge Graphs (TKGs) and embedding/reasoning over uncertain, dynamic, fuzzy spatiotemporal knowledge. Papers 1 and 2 tackle the limitation of existing Temporal Knowledge Graph Completion (TKGC) methods that struggle with high-dimensional nonlinear data and mixed geometric structures by proposing adaptive multi-curvature spaces. Paper 2 (IME) specifically builds on Paper 1's multi-curvature idea by addressing the neglect of heterogeneity across different curvature spaces, introducing mechanisms to integrate shared and specific features for a more comprehensive representation. In parallel, Paper 3 (FSTRE) overcomes the insufficiency of prior KGE models for uncertain and dynamic knowledge by introducing a fuzzy spatiotemporal RDF embedding framework. Paper 4 then extends this by addressing the significant challenge of multihop querying on *incomplete* fuzzy spatiotemporal KGs, where previous embedding-based approaches overlooked uncertainty and spatiotemporal sensitivity during reasoning. These advancements yield new capabilities, enabling more nuanced modeling of TKG structures, including adaptive curvature selection, and robust methods for representing and reasoning with uncertainty and spatiotemporal dynamics, particularly for complex tasks like multihop path inference.\n\n### 3. Temporal Context\nThe publication of all four papers in 2024 highlights a contemporary and rapid acceleration of research interest in advanced geometric and algebraic approaches for handling complex temporal, spatial, and uncertain aspects within knowledge graph embeddings. This concentrated activity suggests a current frontier in KGE research, pushing beyond traditional Euclidean spaces and crisp data representations to address real-world data complexities.\n\n### 4. Synthesis\nThis citation path presents a unified narrative of progressively sophisticated approaches that extend knowledge graph embeddings beyond static, crisp, and Euclidean representations to encompass the full complexity of real-world knowledge, including dynamic temporal evolution, diverse geometric structures, spatial information, and inherent uncertainty. The collective contribution is a significant advancement in \"knowledge graph embedding,\" developing sophisticated models that leverage multi-curvature geometries (Papers 1 & 2) and complex algebraic structures like quaternions (Paper 4), alongside fuzzy logic (Papers 3 & 4), to accurately model, complete, and reason over highly intricate, evolving, and uncertain knowledge graphs.",
    "path": [
      "83d58bc46b7adb92d8750da52313f060b10f201d",
      "58e1b93b18370433633152cb8825917edc2f16a6",
      "0364e17da01358e2705524cd781ef8cc928256f5",
      "b3f0cdc217a3d192d2671e44913542903c94105b",
      "552bfaca30af29647c083993fbe406867fc70d4c",
      "4e52607397a96fb2104a99c570c9cec29c9ca519",
      "52eb7f27cdfbf359096b8b5ef56b2c2826beb660",
      "780bc77fac1aaf460ba191daa218f3c111119092",
      "efea0197c956e981e98c4d2532fa720c58954492",
      "12cc4b65644a84a16ef7dfe7bdd70172cd38cffd"
    ],
    "layer1_papers": [
      {
        "title": "HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding",
        "abstract": "Knowledge Graph (KG) embedding has emerged as an active area of research resulting in the development of several KG embedding methods. Relational facts in KG often show temporal dynamics, e.g., the fact (Cristiano_Ronaldo, playsFor, Manchester_United) is valid only from 2003 to 2009. Most of the existing KG embedding methods ignore this temporal dimension while learning embeddings of the KG elements. In this paper, we propose HyTE, a temporally aware KG embedding method which explicitly incorporates time in the entity-relation space by associating each timestamp with a corresponding hyperplane. HyTE not only performs KG inference using temporal guidance, but also predicts temporal scopes for relational facts with missing time annotations. Through extensive experimentation on temporal datasets extracted from real-world KGs, we demonstrate the effectiveness of our model over both traditional as well as temporal KG embedding methods.",
        "summary": "",
        "year": 2018
      }
    ],
    "layer2_papers": [
      {
        "title": "Temporal Knowledge Graph Embedding Model based on Additive Time Series Decomposition",
        "abstract": "Knowledge Graph (KG) embedding has attracted more attention in recent years. Most KG embedding models learn from time-unaware triples. However, the inclusion of temporal information beside triples would further improve the performance of a KGE model. In this regard, we propose ATiSE, a temporal KG embedding model which incorporates time information into entity/relation representations by using Additive Time Series decomposition. Moreover, considering the temporal uncertainty during the evolution of entity/relation representations over time, we map the representations of temporal KGs into the space of multi-dimensional Gaussian distributions. The mean of each entity/relation embedding at a time step shows the current expected position, whereas its covariance (which is temporally stationary) represents its temporal uncertainty. Experimental results show that ATiSE chieves the state-of-the-art on link prediction over four temporal KGs.",
        "year": 2019
      },
      {
        "title": "Tensor Decomposition-Based Temporal Knowledge Graph Embedding",
        "abstract": "In order to meet the problems caused by sparse data and computational efficiency, knowledge graph (KG) is adopted to represent the semantic information of entities and relations as dense and low-dimensional vectors. While conventional KG representation methods mainly focuse on static data. These methods fail to deal with data that evolves with time which may only be valid for a certain period of time. To accommodate this problem, a temporal KG embedding model based on tensor decomposition is proposed in this paper, which regards the fact set in the KG as a fourth-order tensor including head entities, relations, tail entities and time dimensions. This method can be further generalized to other static KG embedding based on tensor decomposition. With experiments on temporal datasets extracted from real-world KG, extensive experiment results show that our approach outperforms state-of-the-art methods of KG embedding.",
        "year": 2020
      },
      {
        "title": "TARGAT: A Time-Aware Relational Graph Attention Model for Temporal Knowledge Graph Embedding",
        "abstract": "Temporal knowledge graph embedding (TKGE) aims to learn the embedding of entities and relations in a temporal knowledge graph (TKG). Although the previous graph neural networks (GNN) based models have achieved promising results, they cannot directly capture the interactions of multi-facts at different timestamps. To address the above limitation, we propose a time-aware relational graph attention model (TARGAT), which takes the multi-facts at different timestamps as a unified graph. First, we develop a relational generator to dynamically generate a series of time-aware relational message transformation matrices, which jointly models the relations and the timestamp information into a unified way. Then, we apply the generated message transformation matrices to project the neighborhood features into different time-aware spaces and aggregate these neighborhood features to explicitly capture the interactions of multi-facts. Finally, a temporal transformer classifier is applied to learn the representation of the query quadruples and predict the missing entities. The experimental results show that our TARGAT model beats the GNN-based models by a large margin and achieves new state-of-the-art results on four popular benchmark datasets.",
        "year": 2023
      },
      {
        "title": "TeRo: A Time-aware Knowledge Graph Embedding via Temporal Rotation",
        "abstract": "In the last few years, there has been a surge of interest in learning representations of entities and relations in knowledge graph (KG). However, the recent availability of temporal knowledge graphs (TKGs) that contain time information for each fact created the need for reasoning over time in such TKGs. In this regard, we present a new approach of TKG embedding, TeRo, which defines the temporal evolution of entity embedding as a rotation from the initial time to the current time in the complex vector space. Specially, for facts involving time intervals, each relation is represented as a pair of dual complex embeddings to handle the beginning and the end of the relation, respectively. We show our proposed model overcomes the limitations of the existing KG embedding models and TKG embedding models and has the ability of learning and inferring various relation patterns over time. Experimental results on three different TKGs show that TeRo significantly outperforms existing state-of-the-art models for link prediction. In addition, we analyze the effect of time granularity on link prediction over TKGs, which as far as we know has not been investigated in previous literature.",
        "year": 2020
      },
      {
        "title": "ChronoR: Rotation Based Temporal Knowledge Graph Embedding",
        "abstract": "Despite the importance and abundance of temporal knowledge graphs, most of the current research has been focused on reasoning on static graphs. In this paper, we study the challenging problem of inference over temporal knowledge graphs. In particular, the task of temporal link prediction. In general, this is a difficult task due to data non-stationarity, data heterogeneity, and its complex temporal dependencies. \nWe propose Chronological Rotation embedding (ChronoR), a novel model for learning representations for entities, relations, and time. Learning dense representations is frequently used as an efficient and versatile method to perform reasoning on knowledge graphs. The proposed model learns a k-dimensional rotation transformation parametrized by relation and time, such that after each fact's head entity is transformed using the rotation, it falls near its corresponding tail entity. By using high dimensional rotation as its transformation operator, ChronoR captures rich interaction between the temporal and multi-relational characteristics of a Temporal Knowledge Graph. Experimentally, we show that ChronoR is able to outperform many of the state-of-the-art methods on the benchmark datasets for temporal knowledge graph link prediction.",
        "year": 2021
      }
    ],
    "layer3_papers": [
      {
        "title": "MADE: Multicurvature Adaptive Embedding for Temporal Knowledge Graph Completion",
        "abstract": "Temporal knowledge graphs (TKGs) are receiving increased attention due to their time-dependent properties and the evolving nature of knowledge over time. TKGs typically contain complex geometric structures, such as hierarchical, ring, and chain structures, which can often be mixed together. However, embedding TKGs into Euclidean space, as is typically done with TKG completion (TKGC) models, presents a challenge when dealing with high-dimensional nonlinear data and complex geometric structures. To address this issue, we propose a novel TKGC model called multicurvature adaptive embedding (MADE). MADE models TKGs in multicurvature spaces, including flat Euclidean space (zero curvature), hyperbolic space (negative curvature), and hyperspherical space (positive curvature), to handle multiple geometric structures. We assign different weights to different curvature spaces in a data-driven manner to strengthen the ideal curvature spaces for modeling and weaken the inappropriate ones. Additionally, we introduce the quadruplet distributor (QD) to assist the information interaction in each geometric space. Ultimately, we develop an innovative temporal regularization to enhance the smoothness of timestamp embeddings by strengthening the correlation of neighboring timestamps. Experimental results show that MADE outperforms the existing state-of-the-art TKGC models.",
        "year": 2024
      },
      {
        "title": "IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion",
        "abstract": "Temporal Knowledge Graphs (TKGs) incorporate a temporal dimension, allowing for a precise capture of the evolution of knowledge and reflecting the dynamic nature of the real world. Typically, TKGs contain complex geometric structures, with various geometric structures interwoven. However, existing Temporal Knowledge Graph Completion (TKGC) methods either model TKGs in a single space or neglect the heterogeneity of different curvature spaces, thus constraining their capacity to capture these intricate geometric structures. In this paper, we propose a novel Integrating Multi-curvature shared and specific Embedding (IME) model for TKGC tasks. Concretely, IME models TKGs into multi-curvature spaces, including hyperspherical, hyperbolic, and Euclidean spaces. Subsequently, IME incorporates two key properties, namely space-shared property and space-specific property. The space-shared property facilitates the learning of commonalities across different curvature spaces and alleviates the spatial gap caused by the heterogeneous nature of multi-curvature spaces, while the space-specific property captures characteristic features. Meanwhile, IME proposes an Adjustable Multi-curvature Pooling (AMP) approach to effectively retain important information. Furthermore, IME innovatively designs similarity, difference, and structure loss functions to attain the stated objective. Experimental results clearly demonstrate the superior performance of IME over existing state-of-the-art TKGC models.",
        "year": 2024
      },
      {
        "title": "FSTRE: Fuzzy Spatiotemporal RDF Knowledge Graph Embedding Using Uncertain Dynamic Vector Projection and Rotation",
        "abstract": "Knowledge graphs (KGs) use resource description framework (RDF) triples to model various crisp and static resources in the world. Meanwhile, knowledge embedded into vector space can imply more meanings. Much real-world information, however, is often uncertain and dynamic. Existing KG embedding (KGE) models are insufficient to deal with uncertain dynamic knowledge in vector spaces. To overcome this drawback, this article concentrates on an embedding module for the distributed representation of uncertain dynamic knowledge and proposes a strongly adaptive fuzzy spatiotemporal RDF embedding model (FSTRE). Specifically, we first propose a fine-grained fuzzy spatiotemporal RDF model, which provides the underlying representation framework for FSTRE. Then, within the complex vector space, spatial and temporal information is embedded by projection and rotation, respectively. Fine-grained fuzziness penetrates each element of the spatiotemporal five-tuples by a modal length of the anisotropic vectors. By using geometric operations as its transformation operator, FSTRE can capture the rich interaction between crisp and static knowledge and fuzzy spatiotemporal knowledge. We performed an experimental evaluation of FSTRE based on the built fuzzy spatiotemporal KG. It was shown that our FSTRE model is superior to state-of-the-art methods and can handle complex fuzzy spatiotemporal knowledge.",
        "year": 2024
      },
      {
        "title": "Multihop Fuzzy Spatiotemporal RDF Knowledge Graph Query via Quaternion Embedding",
        "abstract": "The proliferation of uncertain spatiotemporal data has led to an increasing demand for fuzzy spatiotemporal knowledge modeling in various applications. However, performing multihop query modeling on incomplete fuzzy spatiotemporal knowledge graphs (KGs) poses significant challenges. Recently, embedding-based multihop KG querying approaches have gained attention. Yet, these approaches often overlook KG uncertainty and spatiotemporal sensitivity, resulting in the neglect of fuzzy spatiotemporal information during multihop path reasoning. To address these challenges, we propose an embedding-based multihop query model for fuzzy spatiotemporal KG. We use quaternion to jointly embed spatiotemporal entities, and relations are represented as rotations from spatiotemporal subject to object. We incorporate uncertainty by the scoring function's bias factor, allowing for relaxation embedding. This approach facilitates the learning of a richer representation of fuzzy spatiotemporal KGs in vector space. By exploiting the inherent noncommutative compositional pattern of quaternions, we construct more accurate multihop paths within fuzzy spatiotemporal KGs, thus improving path reasoning performance. To evaluate the effectiveness of our model, we conduct experiments on two fuzzy spatiotemporal KG datasets, focusing on link prediction and path query answering. Results show that our proposed method significantly outperforms several state-of-the-art baselines in terms of performance metrics.",
        "year": 2024
      }
    ],
    "layer2_summary": "This citation path illustrates a rapid and sophisticated evolution in temporal knowledge graph embedding (TKGE), moving from initial attempts to incorporate time to advanced models capable of capturing complex temporal dynamics and interactions.\n\n1.  **Methodological Evolution:**\n    The methodological evolution demonstrates a clear shift from incorporating time as an additional feature or dimension to more dynamic and sophisticated modeling of temporal evolution and interactions. Early approaches like Paper 1 (2019) used additive time series decomposition and Gaussian distributions to model temporal uncertainty, while Paper 2 (2020) generalized tensor decomposition by adding a time dimension for efficiency. This progressed to dynamic transformation models, specifically rotation-based embeddings (Paper 4, 2020; Paper 5, 2021), culminating in advanced Graph Neural Networks with time-aware attention mechanisms (Paper 3, 2023) for complex multi-fact interactions.\n\n2.  **Knowledge Progression:**\n    The core problem addressed is extending static KGE to effectively model and reason over Temporal Knowledge Graphs (TKGs), moving beyond \"time-unaware triples.\" Paper 1 (2019) and Paper 2 (2020) initially tackled this by incorporating time via decomposition and tensor methods, respectively, addressing uncertainty and efficiency. Paper 4 (2020) and Paper 5 (2021) then introduced and refined rotation-based models to capture the dynamic temporal evolution of entities and relations, overcoming limitations in modeling complex patterns. Most recently, Paper 3 (2023) advanced the field by enabling GNNs to explicitly capture multi-fact interactions across different timestamps through a time-aware attention mechanism, leading to more comprehensive temporal dependency modeling.\n\n3.  **Temporal Context:**\n    The publication years (2019-2023) reveal a rapid acceleration in TKG embedding research, particularly between 2020 and 2021, indicating a surge of interest and innovation. This period aligns with the increasing availability of large-scale temporal datasets and the maturation of deep learning techniques, such as complex embeddings and Graph Neural Networks, which are being adapted and specialized for temporal reasoning.\n\n4.  **Synthesis:**\n    This citation path presents a unified narrative of progressively sophisticated approaches to integrating and reasoning with temporal information in knowledge graph embeddings. The collective contribution is a robust and diverse set of models that have moved from basic temporal inclusion to advanced dynamic modeling, including uncertainty quantification, efficient tensor-based representations, elegant rotation transformations, and powerful graph attention mechanisms. These works collectively push the boundaries of \"knowledge graph embedding\" by enabling more accurate and nuanced understanding of evolving real-world knowledge."
  },
  "8f096071a09701012c9c279aee2a88143a295935": {
    "seed_title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space",
    "summary": "This collection of papers from 2024-2025 demonstrates a continued and intensified focus on enhancing the robustness, adaptability, and fundamental expressiveness of Knowledge Graph Embedding (KGE) models, building directly on the rapid advancements seen in 2023.\n\n1.  **Methodological Evolution**\n    The methodological evolution in this path continues to push the boundaries of KGE design. Paper 1 (TGformer) advances the integration of advanced neural architectures by introducing the first graph transformer framework for KGE, explicitly combining triplet-level and graph-level features, building on the 2023 trend of adapting Transformers to KGs. Paper 2 (HolmE) introduces a novel architectural constraint by proposing a KGE with its relation embedding space *closed under composition*, a foundational design principle to inherently model complex relational patterns. Paper 3 (FastKGE) innovates in efficiency for continual learning by incorporating an incremental low-rank adapter (IncLoRA) mechanism, allowing for faster acquisition of new knowledge with fewer parameters. Paper 4 (AEKE) leverages multi-view graph learning and hypergraphs, integrating entity attributes and confidence scores to create error-aware embeddings, moving beyond purely structural information. Finally, Paper 5 provides a systematic review of negative sampling, highlighting the critical role and categorization of a core training component.\n\n2.  **Knowledge Progression**\n    This path addresses several critical limitations, extending the advancements from 2023. Paper 2 directly tackles the challenge of *under-represented composition patterns* in KGs, a limitation where prior KGEs often failed, by ensuring the relation embedding space is closed under composition, thereby enhancing relational expressiveness. Paper 3 addresses the dual problem in Continual KGE: not only mitigating catastrophic forgetting but also enabling *efficient learning for the emergence of new knowledge* in dynamic KGs, a crucial aspect for real-world applications. Paper 4 confronts the often-neglected issue of *erroneous triples* in KGs, which can significantly degrade performance, by integrating entity attributes to guide error-aware learning. Paper 1 (TGformer) improves upon existing graph-based methods by explicitly considering the *contextual information of nodes* and multi-structural features, addressing the limitation of previous approaches that might ignore valuable entity/relation context. Paper 5, while a review, provides foundational knowledge by systematically outlining the impact and categories of *negative sampling methods*, a critical component for the accuracy and success of all KGE models.\n\n3.  **Temporal Context**\n    With four papers published in 2024 and one in 2025, this path indicates a sustained and accelerated pace of innovation in KGE research. This immediate follow-up to the intense activity of 2023 demonstrates that the field is rapidly building upon recent breakthroughs, particularly in areas like Transformer adaptation (Paper 1), continual learning (Paper 3), and robustness (Paper 4). The 2025 publication further suggests that these trends are actively being refined and pushed forward, with no notable gaps but rather a continuous surge in research output.\n\n4.  **Synthesis**\n    The unified narrative connecting these works is the pursuit of **more robust, adaptive, and inherently capable KGE models** that can effectively handle the complexities, dynamics, and imperfections of real-world knowledge graphs. Collectively, these papers significantly advance the state-of-the-art by addressing fundamental challenges in compositional reasoning, efficient continual learning, error resilience, and comprehensive contextual modeling, while also providing a critical review of core training mechanisms, thereby pushing KGE towards greater practical utility and theoretical soundness.",
    "path": [
      "8f096071a09701012c9c279aee2a88143a295935",
      "d9802a67b326fe89bbd761c261937ee1e4d4d674",
      "c180564160d0788a82df203f9e5f61380d9846aa",
      "10d949dee482aeea1cab8b42c326d0dbf0505de3",
      "29052ddd048acb1afa2c42613068b63bb7428a34",
      "f2b924e69735fb7fd6fd95c6a032954480862029",
      "354fb91810c6d3756600c99ad84d2e6ef4136021",
      "4085a5cf49c193fe3d3ff19ff2d696fe20a5a596",
      "5dc88d795cbcd01e6e99ba673e91e9024f0c3318",
      "b1d807fc6b184d757ebdea67acd81132d8298ff6",
      "040fe47af8f4870bf681f34861c42b3ea46d76cf",
      "fda63b289d4c0c332f88975994114fb61b514ced",
      "15710515bae025372f298570267d234d4a3141cb",
      "eae107f7eeed756dfc996c47bc3faf381d36fd94",
      "40479fd70115e545d21c01853aad56e6922280ac",
      "3f170af3566f055e758fa3bdf2bfd3a0e8787e58",
      "4801db5c5cb24a9069f2d264252fa26986ceefa9"
    ],
    "layer1_papers": [
      {
        "title": "RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space",
        "abstract": "We study the problem of learning representations of entities and relations in knowledge graphs for predicting missing links. The success of such a task heavily relies on the ability of modeling and inferring the patterns of (or between) the relations. In this paper, we present a new approach for knowledge graph embedding called RotatE, which is able to model and infer various relation patterns including: symmetry/antisymmetry, inversion, and composition. Specifically, the RotatE model defines each relation as a rotation from the source entity to the target entity in the complex vector space. In addition, we propose a novel self-adversarial negative sampling technique for efficiently and effectively training the RotatE model. Experimental results on multiple benchmark knowledge graphs show that the proposed RotatE model is not only scalable, but also able to infer and model various relation patterns and significantly outperform existing state-of-the-art models for link prediction.",
        "summary": "",
        "year": 2018
      }
    ],
    "layer2_papers": [
      {
        "title": "Link Prediction with Attention Applied on Multiple Knowledge Graph Embedding Models",
        "abstract": "Predicting missing links between entities in a knowledge graph is a fundamental task to deal with the incompleteness of data on the Web. Knowledge graph embeddings map nodes into a vector space to predict new links, scoring them according to geometric criteria. Relations in the graph may follow patterns that can be learned, e.g., some relations might be symmetric and others might be hierarchical. However, the learning capability of different embedding models varies for each pattern and, so far, no single model can learn all patterns equally well. In this paper, we combine the query representations from several models in a unified one to incorporate patterns that are independently captured by each model. Our combination uses attention to select the most suitable model to answer each query. The models are also mapped onto a non-Euclidean manifold, the Poincaré ball, to capture structural patterns, such as hierarchies, besides relational patterns, such as symmetry. We prove that our combination provides a higher expressiveness and inference power than each model on its own. As a result, the combined model can learn relational and structural patterns. We conduct extensive experimental analysis with various link prediction benchmarks showing that the combined model outperforms individual models, including state-of-the-art approaches.",
        "year": 2023
      },
      {
        "title": "Weighted Knowledge Graph Embedding",
        "abstract": "Knowledge graph embedding (KGE) aims to project both entities and relations in a knowledge graph (KG) into low-dimensional vectors. Indeed, existing KGs suffer from the data imbalance issue, i.e., entities and relations conform to a long-tail distribution, only a small portion of entities and relations occur frequently, while the vast majority of entities and relations only have a few training samples. Existing KGE methods assign equal weights to each entity and relation during the training process. Under this setting, long-tail entities and relations are not fully trained during training, leading to unreliable representations. In this paper, we propose WeightE, which attends differentially to different entities and relations. Specifically, WeightE is able to endow lower weights to frequent entities and relations, and higher weights to infrequent ones. In such manner, WeightE is capable of increasing the weights of long-tail entities and relations, and learning better representations for them. In particular, WeightE tailors bilevel optimization for the KGE task, where the inner level aims to learn reliable entity and relation embeddings, and the outer level attempts to assign appropriate weights for each entity and relation. Moreover, it is worth noting that our technique of applying weights to different entities and relations is general and flexible, which can be applied to a number of existing KGE models. Finally, we extensively validate the superiority of WeightE against various state-of-the-art baselines.",
        "year": 2023
      },
      {
        "title": "Entity-Agnostic Representation Learning for Parameter-Efficient Knowledge Graph Embedding",
        "abstract": "We propose an entity-agnostic representation learning method for handling the problem of inefficient parameter storage costs brought by embedding knowledge graphs. Conventional knowledge graph embedding methods map elements in a knowledge graph, including entities and relations, into continuous vector spaces by assigning them one or multiple specific embeddings (i.e., vector representations). Thus the number of embedding parameters increases linearly as the growth of knowledge graphs. In our proposed model, Entity-Agnostic Representation Learning (EARL), we only learn the embeddings for a small set of entities and refer to them as reserved entities. To obtain the embeddings for the full set of entities, we encode their distinguishable information from their connected relations, k-nearest reserved entities, and multi-hop neighbors. We learn universal and entity-agnostic encoders for transforming distinguishable information into entity embeddings. This approach allows our proposed EARL to have a static, efficient, and lower parameter count than conventional knowledge graph embedding methods. Experimental results show that EARL uses fewer parameters and performs better on link prediction tasks than baselines, reflecting its parameter efficiency.",
        "year": 2023
      },
      {
        "title": "Position-Aware Relational Transformer for Knowledge Graph Embedding",
        "abstract": "Although Transformer has achieved success in language and vision tasks, its capacity for knowledge graph (KG) embedding has not been fully exploited. Using the self-attention (SA) mechanism in Transformer to model the subject-relation-object triples in KGs suffers from training inconsistency as SA is invariant to the order of input tokens. As a result, it is unable to distinguish a (real) relation triple from its shuffled (fake) variants (e.g., object-relation-subject) and, thus, fails to capture the correct semantics. To cope with this issue, we propose a novel Transformer architecture, namely, Knowformer, for KG embedding. It incorporates relational compositions in entity representations to explicitly inject semantics and capture the role of an entity based on its position (subject or object) in a relation triple. The relational composition for a subject (or object) entity of a relation triple refers to an operator on the relation and the object (or subject). We borrow ideas from the typical translational and semantic-matching embedding techniques to design relational compositions. We carefully design a residual block to integrate relational compositions into SA and efficiently propagate the composed relational semantics layer by layer. We formally prove that the SA with relational compositions is able to distinguish the entity roles in different positions and correctly capture relational semantics. Extensive experiments and analyses on six benchmark datasets show that Knowformer achieves state-of-the-art performance on both link prediction and entity alignment.",
        "year": 2023
      },
      {
        "title": "Knowledge Graph Embedding: An Overview",
        "abstract": "Many mathematical models have been leveraged to design embeddings for representing Knowledge Graph (KG) entities and relations for link prediction and many downstream tasks. These mathematically-inspired models are not only highly scalable for inference in large KGs, but also have many explainable advantages in modeling different relation patterns that can be validated through both formal proofs and empirical results. In this paper, we make a comprehensive overview of the current state of research in KG completion. In particular, we focus on two main branches of KG embedding (KGE) design: 1) distance-based methods and 2) semantic matching-based methods. We discover the connections between recently proposed models and present an underlying trend that might help researchers invent novel and more effective models. Next, we delve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D affine operations, respectively. They encompass a broad spectrum of techniques including distance-based and semantic-based methods. We will also discuss an emerging approach for KG completion which leverages pre-trained language models (PLMs) and textual descriptions of entities and relations and offer insights into the integration of KGE embedding methods with PLMs for KG completion.",
        "year": 2023
      },
      {
        "title": "A type-augmented knowledge graph embedding framework for knowledge graph completion",
        "abstract": "Knowledge graphs (KGs) are of great importance to many artificial intelligence applications, but they usually suffer from the incomplete problem. Knowledge graph embedding (KGE), which aims to represent entities and relations in low-dimensional continuous vector spaces, has been proved to be a promising approach for KG completion. Traditional KGE methods only concentrate on structured triples, while paying less attention to the type information of entities. In fact, incorporating entity types into embedding learning could further improve the performance of KG completion. To this end, we propose a universal Type-augmented Knowledge graph Embedding framework (TaKE) which could utilize type features to enhance any traditional KGE models. TaKE automatically captures type features under no explicit type information supervision. And by learning different type representations of each entity, TaKE could distinguish the diversity of types specific to distinct relations. We also design a new type-constrained negative sampling strategy to construct more effective negative samples for the training process. Extensive experiments on four datasets from three real-world KGs (Freebase, WordNet and YAGO) demonstrate the merits of our proposed framework. In particular, combining TaKE with the recent tensor factorization KGE model SimplE can achieve state-of-the-art performance on the KG completion task.",
        "year": 2023
      },
      {
        "title": "Knowledge Graph Embedding with 3D Compound Geometric Transformations",
        "abstract": "The cascade of 2D geometric transformations were exploited to model relations between entities in a knowledge graph (KG), leading to an effective KG embedding (KGE) model, CompoundE. Furthermore, the rotation in the 3D space was proposed as a new KGE model, Rotate3D, by leveraging its non-commutative property. Inspired by CompoundE and Rotate3D, we leverage 3D compound geometric transformations, including translation, rotation, scaling, reflection, and shear and propose a family of KGE models, named CompoundE3D, in this work. CompoundE3D allows multiple design variants to match rich underlying characteristics of a KG. Since each variant has its own advantages on a subset of relations, an ensemble of multiple variants can yield superior performance. The effectiveness and flexibility of CompoundE3D are experimentally verified on four popular link prediction datasets.",
        "year": 2023
      },
      {
        "title": "Modality-Aware Negative Sampling for Multi-modal Knowledge Graph Embedding",
        "abstract": "Negative sampling (NS) is widely used in knowledge graph embedding (KGE), which aims to generate negative triples to make a positive-negative contrast during training. However, existing NS methods are unsuitable when multi-modal information is considered in KGE models. They are also inefficient due to their complex design. In this paper, we propose Modality-Aware Negative Sampling (MANS) for multi-modal knowledge graph embedding (MMKGE) to address the mentioned problems. MANS could align structural and visual embeddings for entities in KGs and learn meaningful embeddings to perform better in multi-modal KGE while keeping lightweight and efficient. Empirical results on two benchmarks demonstrate that MANS outperforms existing NS methods. Meanwhile, we make further explorations about MANS to confirm its effectiveness.",
        "year": 2023
      },
      {
        "title": "Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation",
        "abstract": "Learning and development, or L&D, plays an important role in talent management, which aims to improve the knowledge and capabilities of employees through a variety of performance-oriented training activities. Recently, with the rapid development of enterprise management information systems, many research efforts and industrial practices have been devoted to building personalized employee training course recommender systems. Nevertheless, a widespread challenge is how to provide explainable recommendations with the consideration of different learning motivations from talents. To this end, we propose CKGE, a contextualized knowledge graph (KG) embedding approach for developing an explainable training course recommender system. A novel perspective of CKGE is to integrate both the contextualized neighbor semantics and high-order connections as motivation-aware information for learning effective representations of talents and courses. Specifically, in CKGE, for each entity pair (i.e., the talent-course pair), we first construct a meta-graph, including the neighbors of each entity and the meta-paths between entities as motivation-aware information. Then, we develop a novel KG-based Transformer, which can serialize entities and paths in the meta-graph as a sequential input, with the specially designed relational attention and structural encoding mechanisms to better model the global dependence of KG structured data. Meanwhile, the local path mask prediction can effectively reveal the importance of different paths. As a result, CKGE not only can make precise predictions but also can discriminate the saliencies of meta-paths in characterizing corresponding preferences. Extensive experiments on real-world and public datasets clearly validate the effectiveness and interpretability of CKGE compared with state-of-the-art baselines.",
        "year": 2023
      },
      {
        "title": "Message Function Search for Knowledge Graph Embedding",
        "abstract": "Recently, many promising embedding models have been proposed to embed knowledge graphs (KGs) and their more general forms, such as n-ary relational data (NRD) and hyper-relational KG (HKG). To promote the data adaptability and performance of embedding models, KG searching methods propose to search for suitable models for a given KG data set. But they are restricted to a single KG form, and the searched models are restricted to a single type of embedding model. To tackle such issues, we propose to build a search space for the message function in graph neural networks (GNNs). However, it is a non-trivial task. Existing message function designs fix the structures and operators, which makes them difficult to handle different KG forms and data sets. Therefore, we first design a novel message function space, which enables both structures and operators to be searched for the given KG form (including KG, NRD, and HKG) and data. The proposed space can flexibly take different KG forms as inputs and is expressive to search for different types of embedding models. Especially, some existing message function designs and some classic KG embedding models can be instantiated as special cases of our space. We empirically show that the searched message functions are data-dependent, and can achieve leading performance on benchmark KGs, NRD, and HKGs.",
        "year": 2023
      },
      {
        "title": "Molecular-evaluated and explainable drug repurposing for COVID-19 using ensemble knowledge graph embedding",
        "abstract": "The search for an effective drug is still urgent for COVID-19 as no drug with proven clinical efficacy is available. Finding the new purpose of an approved or investigational drug, known as drug repurposing, has become increasingly popular in recent years. We propose here a new drug repurposing approach for COVID-19, based on knowledge graph (KG) embeddings. Our approach learns “ensemble embeddings” of entities and relations in a COVID-19 centric KG, in order to get a better latent representation of the graph elements. Ensemble KG-embeddings are subsequently used in a deep neural network trained for discovering potential drugs for COVID-19. Compared to related works, we retrieve more in-trial drugs among our top-ranked predictions, thus giving greater confidence in our prediction for out-of-trial drugs. For the first time to our knowledge, molecular docking is then used to evaluate the predictions obtained from drug repurposing using KG embedding. We show that Fosinopril is a potential ligand for the SARS-CoV-2 nsp13 target. We also provide explanations of our predictions thanks to rules extracted from the KG and instanciated by KG-derived explanatory paths. Molecular evaluation and explanatory paths bring reliability to our results and constitute new complementary and reusable methods for assessing KG-based drug repurposing.",
        "year": 2023
      }
    ],
    "layer3_papers": [
      {
        "title": "Knowledge graph embedding closed under composition",
        "abstract": "Knowledge Graph Embedding (KGE) has attracted increasing attention. Relation patterns, such as symmetry and inversion, have received considerable focus. Among them, composition patterns are particularly important, as they involve nearly all relations in KGs. However, prior KGE approaches often consider relations to be compositional only if they are well-represented in the training data. Consequently, it can lead to performance degradation, especially for under-represented composition patterns. To this end, we propose HolmE, a general form of KGE with its relation embedding space closed under composition, namely that the composition of any two given relation embeddings remains within the embedding space. This property ensures that every relation embedding can compose, or be composed by other relation embeddings. It enhances HolmE’s capability to model under-represented (also called long-tail) composition patterns with limited learning instances. To our best knowledge, our work is pioneering in discussing KGE with this property of being closed under composition. We provide detailed theoretical proof and extensive experiments to demonstrate the notable advantages of HolmE in modelling composition patterns, particularly for long-tail patterns. Our results also highlight HolmE’s effectiveness in extrapolating to unseen relations through composition and its state-of-the-art performance on benchmark datasets.",
        "year": 2024
      },
      {
        "title": "Fast and Continual Knowledge Graph Embedding via Incremental LoRA",
        "abstract": "Continual Knowledge Graph Embedding (CKGE) aims to efficiently learn new knowledge and simultaneously preserve old knowledge. Dominant approaches primarily focus on alleviating catastrophic forgetting of old knowledge but neglect efficient learning for the emergence of new knowledge. However, in real-world scenarios, knowledge graphs (KGs) are continuously growing, which brings a significant challenge to fine-tuning KGE models efficiently. To address this issue, we propose a fast CKGE framework (FastKGE), incorporating an incremental low-rank adapter (IncLoRA) mechanism to efficiently acquire new knowledge while preserving old knowledge. Specifically, to mitigate catastrophic forgetting, FastKGE isolates and allocates new knowledge to specific layers based on the fine-grained influence between old and new KGs. Subsequently, to accelerate fine-tuning, FastKGE devises an efficient IncLoRA mechanism, which embeds the specific layers into incremental low-rank adapters with fewer training parameters. Moreover, IncLoRA introduces adaptive rank allocation, which makes the LoRA aware of the importance of entities and adjusts its rank scale adaptively. We conduct experiments on four public datasets and two new datasets with a larger initial scale. Experimental results demonstrate that FastKGE can reduce training time by 34%-49% while still achieving competitive link prediction performance against state-of-the-art models on four public datasets (average MRR score of 21.0% vs. 21.1%). Meanwhile, on two newly constructed datasets, FastKGE saves 51%-68% training time and improves link prediction performance by 1.5%.",
        "year": 2024
      },
      {
        "title": "Integrating Entity Attributes for Error-Aware Knowledge Graph Embedding",
        "abstract": "Knowledge graphs (KGs) can structurally organize large-scale information in the form of triples and significantly support many real-world applications. While most KG embedding algorithms hold the assumption that all triples are correct, considerable errors were inevitably injected during the construction process. It is urgent to develop effective error-aware KG embedding, since errors in KGs would lead to significant performance degradation in downstream applications. To this end, we propose a novel framework named Attributed Error-aware Knowledge Embedding (AEKE). It leverages the semantics contained in entity attributes to guide the KG embedding model learning against the impact of erroneous triples. We design two triple-level hypergraphs to model the topological structures of the KG and its attributes, respectively. The confidence score of each triple is jointly calculated based on self-contradictory within the triple, consistency between local and global structures, and homogeneity between structures and attributes. We leverage confidence scores to adaptively update the weighted aggregation in the multi-view graph learning framework and margin loss in KG embedding, such that potential errors will contribute little to KG learning. Experiments on three real-world KGs demonstrate that AEKE outperforms state-of-the-art KG embedding and error detection algorithms.",
        "year": 2024
      },
      {
        "title": "TGformer: A Graph Transformer Framework for Knowledge Graph Embedding",
        "abstract": "Knowledge graph embedding is efficient method for reasoning over known facts and inferring missing links. Existing methods are mainly triplet-based or graph-based. Triplet-based approaches learn the embedding of missing entities by a single triple only. They ignore the fact that the knowledge graph is essentially a graph structure. Graph-based methods consider graph structure information but ignore the contextual information of nodes in the knowledge graph, making them unable to discern valuable entity (relation) information. In response to the above limitations, we propose a general graph transformer framework for knowledge graph embedding (TGformer). It is the first to use a graph transformer to build knowledge embeddings with triplet-level and graph-level structural features in the static and temporal knowledge graph. Specifically, a context-level subgraph is constructed for each predicted triplet, which models the relation between triplets with the same entity. Afterward, we design a knowledge graph transformer network (KGTN) to fully explore multi-structural features in knowledge graphs, including triplet-level and graph-level, boosting the model to understand entities (relations) in different contexts. Finally, semantic matching is adopted to select the entity with the highest score. Experimental results on several public knowledge graph datasets show that our method can achieve state-of-the-art performance in link prediction.",
        "year": 2025
      },
      {
        "title": "Negative Sampling in Knowledge Graph Representation Learning: A Review",
        "abstract": "Knowledge Graph Representation Learning (KGRL), or Knowledge Graph Embedding (KGE), is essential for AI applications such as knowledge construction and information retrieval. These models encode entities and relations into lower-dimensional vectors, supporting tasks like link prediction and recommendation systems. Training KGE models relies on both positive and negative samples for effective learning, but generating high-quality negative samples from existing knowledge graphs is challenging. The quality of these samples significantly impacts the model's accuracy. This comprehensive survey paper systematically reviews various negative sampling (NS) methods and their contributions to the success of KGRL. Their respective advantages and disadvantages are outlined by categorizing existing NS methods into six distinct categories. Moreover, this survey identifies open research questions that serve as potential directions for future investigations. By offering a generalization and alignment of fundamental NS concepts, this survey provides valuable insights for designing effective NS methods in the context of KGRL and serves as a motivating force for further advancements in the field.",
        "year": 2024
      }
    ],
    "layer2_summary": "This collection of 2023 papers reveals a dynamic and rapidly evolving landscape in knowledge graph embedding (KGE) research, characterized by a concerted effort to enhance model expressiveness, efficiency, and real-world applicability.\n\n**1. Methodological Evolution**\nThe papers showcase a rapid methodological evolution within KGE, moving beyond single-model approaches to more sophisticated, adaptive, and context-aware techniques. A key shift is the move towards **ensemble and adaptive model combinations**, exemplified by Paper 1, which uses attention to select the most suitable KGE model for each query, and Paper 7, which proposes an ensemble of 3D compound geometric transformation variants. There's also a significant trend in **adapting advanced neural architectures like Transformers** (Paper 4, Paper 9) to KGs, requiring novel mechanisms to handle graph structure and relational semantics, such as position-aware relational compositions in Paper 4. Furthermore, methods are becoming more **data-aware and resource-efficient**, incorporating techniques like differential weighting for long-tail entities (Paper 2) and entity-agnostic representations for parameter reduction (Paper 3). The field is also exploring **automated model design**, as seen in Paper 10's message function search for GNNs, moving towards more flexible and data-adaptive architectures.\n\n**2. Knowledge Progression**\nThese works collectively address several critical limitations of traditional KGE. The fundamental problem of **KGE incompleteness and limited expressiveness** is tackled by combining models (Paper 1), which proves higher inference power, and exploring richer geometric transformations (Paper 7), which allows for modeling a broader spectrum of relational patterns. The **data imbalance issue** in KGs, where long-tail entities are poorly represented, is addressed by Paper 2's WeightE, which assigns differential weights during training. Paper 3 confronts the **parameter explosion problem** in large KGs by proposing entity-agnostic learning (EARL), significantly reducing storage costs while maintaining performance. The challenge of **adapting sequence models like Transformers to non-sequential graph data** is resolved by Paper 4 (Knowformer), which introduces position-aware relational compositions to correctly capture relational semantics. Furthermore, the integration of **auxiliary information** like entity types (Paper 6's TaKE framework) and multi-modal data (Paper 8's MANS for multi-modal KGE) enhances embedding quality. Paper 10 pushes the boundaries by **automating the design of message functions** for GNN-based KGE, improving adaptability across diverse KG forms (KG, NRD, HKG). Finally, new capabilities emerge in **explainable AI**, with Paper 9 providing explainable recommendations through contextualized KGE and Paper 11 offering molecular-evaluated and explainable drug repurposing, demonstrating KGE's utility in complex, real-world applications.\n\n**3. Temporal Context**\nAll twelve papers were published in 2023, indicating a highly active and rapidly advancing research front in knowledge graph embedding. This simultaneous publication reflects a period of intense innovation, where researchers are concurrently exploring diverse avenues to overcome KGE limitations, driven by the increasing demand for robust and scalable KG applications. The lack of significant temporal gaps suggests a collective, accelerated push to refine existing models, integrate new architectural paradigms (like Transformers and GNNs), and address practical challenges such as data imbalance, parameter efficiency, and explainability, rather than a sequential build-up over many years.\n\n**4. Synthesis**\nThe unified narrative connecting these works is the pursuit of more **robust, expressive, efficient, and interpretable** knowledge graph embeddings. Collectively, they move beyond basic embedding functions to sophisticated frameworks that can adapt to diverse data characteristics (e.g., long-tail, multi-modal), leverage advanced neural architectures (e.g., Transformers, GNNs), and provide explainable insights for real-world applications. Their collective contribution lies in significantly advancing the state-of-the-art in KGE by addressing its core limitations and expanding its applicability, paving the way for more powerful and trustworthy AI systems capable of handling the complexity and scale of real-world knowledge."
  },
  "2a3f862199883ceff5e3c74126f0c80770653e05": {
    "seed_title": "Knowledge Graph Embedding by Translating on Hyperplanes",
    "summary": "This citation path reveals a rapid and multifaceted evolution within the field of knowledge graph embedding (KGE), extending from foundational advancements in 2023 to a deeper focus on core training mechanisms in 2024.\n\n1.  **Methodological Evolution:**\n    The methodological evolution in 2023 shifted from foundational distance-based models (Paper 1's review of TransE/H/R) towards increasingly sophisticated, context-aware, and hybrid approaches, integrating entity types (Paper 2's TaKE), semantic matching, affine operations, and Pre-trained Language Models (PLMs) (Paper 3). Further advancements included contextualized KGEs with meta-graphs and KG-based Transformers for explainability (Paper 4), culminating in robust hybrid systems for domain-specific applications (Paper 5). Building on this, Paper 6 (2024) represents a methodological refinement by systematically reviewing and categorizing various negative sampling (NS) methods, which are crucial *training techniques* that underpin the effectiveness of all KGE models, including the advanced architectures explored in 2023. This indicates a maturation where foundational training components are rigorously analyzed and optimized, even as model architectures become more complex.\n\n2.  **Knowledge Progression:**\n    The 2023 papers collectively addressed knowledge graph incompleteness and expanded into complex downstream tasks. Paper 1 reviewed foundational link prediction, while Paper 2 improved upon this by incorporating entity type information to enhance KG completion. Paper 3 identified trends leveraging rich textual semantics to overcome structural data limitations, and Paper 4 tackled explainable recommendations by integrating contextualized neighbor semantics. Paper 5 then applied KGEs to complex domain-specific problems like chemistry question answering. Paper 6 (2024) progresses this knowledge by addressing a critical, overarching challenge that impacts the accuracy and robustness of *all* KGE models, including those developed in 2023: the generation of high-quality negative samples. It doesn't build on a specific model's limitation but rather a fundamental training bottleneck, offering a deeper understanding of a core component necessary for effective KGE learning and providing insights for designing more robust NS methods.\n\n3.  **Temporal Context:**\n    The simultaneous publication of five papers in 2023 highlighted a highly active and rapidly evolving research landscape, consolidating foundational concepts while exploring advanced techniques and applications. The publication of Paper 6 in 2024, immediately following this intense period, indicates a continued and deepening focus within the field. It suggests that while new architectures and applications are being rapidly explored, the community is also dedicating significant effort to consolidating and optimizing the *underlying training mechanisms* that enable these advanced models to function effectively, showcasing an acceleration not just in breadth but also in the foundational depth of KGE research.\n\n4.  **Synthesis:**\n    The unified narrative connecting these works illustrates a dynamic evolution in knowledge graph embedding, moving from foundational vector space representations towards increasingly sophisticated, context-aware, and semantically rich methods. The collective contribution now encompasses not only innovative architectures and applications (2023 papers) but also a critical understanding and systematic review of fundamental training components, specifically negative sampling (Paper 6, 2024). This progression highlights that the effectiveness of advanced KGEs is profoundly dependent on the quality of their training data, underscoring a holistic approach to KGE research that seeks both architectural innovation and robust, accurate training methodologies to tackle complex real-world challenges effectively.",
    "path": [
      "2a3f862199883ceff5e3c74126f0c80770653e05",
      "9c510e24b5edc5720440b695d7bd0636b52f4f66",
      "354fb91810c6d3756600c99ad84d2e6ef4136021",
      "f2b924e69735fb7fd6fd95c6a032954480862029",
      "b1d807fc6b184d757ebdea67acd81132d8298ff6",
      "23efe9b99b5f0e79d7dbd4e3bfcf1c2d8b23c1ff",
      "4801db5c5cb24a9069f2d264252fa26986ceefa9"
    ],
    "layer1_papers": [
      {
        "title": "Knowledge Graph Embedding by Translating on Hyperplanes",
        "abstract": "\n \n We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.\n \n",
        "summary": "",
        "year": 2014
      }
    ],
    "layer2_papers": [
      {
        "title": "A Review of Knowledge Graph Embedding Methods of TransE, TransH and TransR for Missing Links",
        "abstract": "Knowledge representation and reasoning require knowledge graph embedding as it is crucial in the area. It involves mapping entities and relationships from a knowledge graph into vectors of lower dimensions that are continuous in nature. This encoding enables machine learning algorithms to effectively reason and make predictions on graph-structured data. This review article offers an overview and critical analysis specifically about the methods of knowledge graph embedding which are TransE, TransH, and TransR. The key concepts, methodologies, strengths, and limitations of these methods, along with examining their applications and experiments conducted by existing researchers have been studied. The motivation to conduct this study is to review the well-known and most applied knowledge embedding methods and compare the features of those methods so that a comprehensive resource for researchers and practitioners interested in delving into knowledge graph embedding techniques is delivered.",
        "year": 2023
      },
      {
        "title": "A type-augmented knowledge graph embedding framework for knowledge graph completion",
        "abstract": "Knowledge graphs (KGs) are of great importance to many artificial intelligence applications, but they usually suffer from the incomplete problem. Knowledge graph embedding (KGE), which aims to represent entities and relations in low-dimensional continuous vector spaces, has been proved to be a promising approach for KG completion. Traditional KGE methods only concentrate on structured triples, while paying less attention to the type information of entities. In fact, incorporating entity types into embedding learning could further improve the performance of KG completion. To this end, we propose a universal Type-augmented Knowledge graph Embedding framework (TaKE) which could utilize type features to enhance any traditional KGE models. TaKE automatically captures type features under no explicit type information supervision. And by learning different type representations of each entity, TaKE could distinguish the diversity of types specific to distinct relations. We also design a new type-constrained negative sampling strategy to construct more effective negative samples for the training process. Extensive experiments on four datasets from three real-world KGs (Freebase, WordNet and YAGO) demonstrate the merits of our proposed framework. In particular, combining TaKE with the recent tensor factorization KGE model SimplE can achieve state-of-the-art performance on the KG completion task.",
        "year": 2023
      },
      {
        "title": "Knowledge Graph Embedding: An Overview",
        "abstract": "Many mathematical models have been leveraged to design embeddings for representing Knowledge Graph (KG) entities and relations for link prediction and many downstream tasks. These mathematically-inspired models are not only highly scalable for inference in large KGs, but also have many explainable advantages in modeling different relation patterns that can be validated through both formal proofs and empirical results. In this paper, we make a comprehensive overview of the current state of research in KG completion. In particular, we focus on two main branches of KG embedding (KGE) design: 1) distance-based methods and 2) semantic matching-based methods. We discover the connections between recently proposed models and present an underlying trend that might help researchers invent novel and more effective models. Next, we delve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D affine operations, respectively. They encompass a broad spectrum of techniques including distance-based and semantic-based methods. We will also discuss an emerging approach for KG completion which leverages pre-trained language models (PLMs) and textual descriptions of entities and relations and offer insights into the integration of KGE embedding methods with PLMs for KG completion.",
        "year": 2023
      },
      {
        "title": "Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation",
        "abstract": "Learning and development, or L&D, plays an important role in talent management, which aims to improve the knowledge and capabilities of employees through a variety of performance-oriented training activities. Recently, with the rapid development of enterprise management information systems, many research efforts and industrial practices have been devoted to building personalized employee training course recommender systems. Nevertheless, a widespread challenge is how to provide explainable recommendations with the consideration of different learning motivations from talents. To this end, we propose CKGE, a contextualized knowledge graph (KG) embedding approach for developing an explainable training course recommender system. A novel perspective of CKGE is to integrate both the contextualized neighbor semantics and high-order connections as motivation-aware information for learning effective representations of talents and courses. Specifically, in CKGE, for each entity pair (i.e., the talent-course pair), we first construct a meta-graph, including the neighbors of each entity and the meta-paths between entities as motivation-aware information. Then, we develop a novel KG-based Transformer, which can serialize entities and paths in the meta-graph as a sequential input, with the specially designed relational attention and structural encoding mechanisms to better model the global dependence of KG structured data. Meanwhile, the local path mask prediction can effectively reveal the importance of different paths. As a result, CKGE not only can make precise predictions but also can discriminate the saliencies of meta-paths in characterizing corresponding preferences. Extensive experiments on real-world and public datasets clearly validate the effectiveness and interpretability of CKGE compared with state-of-the-art baselines.",
        "year": 2023
      },
      {
        "title": "Marie and BERT—A Knowledge Graph Embedding Based Question Answering System for Chemistry",
        "abstract": "This paper presents a novel knowledge graph question answering (KGQA) system for chemistry, which is implemented on hybrid knowledge graph embeddings, aiming to provide fact-oriented information retrieval for chemistry-related research and industrial applications. Unlike other existing designs, the system operates on multiple embedding spaces, which use various embedding methods and queries the embedding spaces in parallel. With the answers returned from multiple embedding spaces, the system leverages a score alignment model to adjust the answer scores and rerank the answers. Further, the system implements an algorithm to derive implicit multihop relations to handle the complexities of deep ontologies and improve multihop question answering. The system also implements a BERT-based bidirectional entity-linking model to enhance the robustness and accuracy of the entity-linking module. The system uses a joint numerical embedding model to efficiently handle numerical filtering questions. Further, it can invoke semantic agents to perform dynamic calculations autonomously. Finally, the KGQA system handles numerous chemical reaction mechanisms using semantic parsing supported by a Linked Data Fragment server. This paper evaluates the accuracy of each module within the KGQA system with a chemistry question data set.",
        "year": 2023
      }
    ],
    "layer3_papers": [
      {
        "title": "Negative Sampling in Knowledge Graph Representation Learning: A Review",
        "abstract": "Knowledge Graph Representation Learning (KGRL), or Knowledge Graph Embedding (KGE), is essential for AI applications such as knowledge construction and information retrieval. These models encode entities and relations into lower-dimensional vectors, supporting tasks like link prediction and recommendation systems. Training KGE models relies on both positive and negative samples for effective learning, but generating high-quality negative samples from existing knowledge graphs is challenging. The quality of these samples significantly impacts the model's accuracy. This comprehensive survey paper systematically reviews various negative sampling (NS) methods and their contributions to the success of KGRL. Their respective advantages and disadvantages are outlined by categorizing existing NS methods into six distinct categories. Moreover, this survey identifies open research questions that serve as potential directions for future investigations. By offering a generalization and alignment of fundamental NS concepts, this survey provides valuable insights for designing effective NS methods in the context of KGRL and serves as a motivating force for further advancements in the field.",
        "year": 2024
      }
    ],
    "layer2_summary": "This citation path reveals a rapid and multifaceted evolution within the field of knowledge graph embedding (KGE) in 2023.\n\n1.  **Methodological Evolution:**\n    The methodological evolution shifts from foundational distance-based models (Paper 1's review of TransE/H/R) towards increasingly sophisticated, context-aware, and hybrid approaches. Paper 2 innovates by integrating entity type information into KGEs (TaKE) to enhance traditional models, while Paper 3 highlights the emergence of semantic matching, affine operations like CompoundE/3D, and the crucial integration of Pre-trained Language Models (PLMs) for leveraging richer textual semantics. Further advancements include Paper 4's contextualized KGE (CKGE) which employs meta-graphs and KG-based Transformers for explainability, culminating in Paper 5's robust hybrid system that combines multiple KGEs and BERT for domain-specific applications.\n\n2.  **Knowledge Progression:**\n    These papers collectively address knowledge graph incompleteness and expand into complex downstream tasks, building on prior limitations. Paper 1 reviews foundational link prediction methods, while Paper 2 improves upon these by explicitly incorporating entity type information to enhance KG completion and distinguish type diversity specific to relations. Paper 3 further progresses by identifying trends like PLM integration, leveraging rich textual semantics to overcome the limitation of KGEs relying solely on structural data. Subsequently, Paper 4 tackles the novel challenge of explainable recommendations by integrating contextualized neighbor semantics and high-order connections, providing interpretable results beyond mere prediction. This culminates in Paper 5, which applies KGEs to a complex domain-specific problem—chemistry question answering—by developing a robust hybrid system for multi-hop relations, numerical filtering, and improved entity linking, demonstrating advanced practical utility.\n\n3.  **Temporal Context:**\n    All five papers were published in 2023, indicating a highly active and rapidly evolving research landscape in knowledge graph embedding. This simultaneous publication of review articles (Paper 1, Paper 3), novel methodological contributions (Paper 2, Paper 4), and application-focused work (Paper 5) highlights the field's maturity, where foundational concepts are being consolidated while advanced techniques and applications are concurrently explored, often integrating state-of-the-art deep learning architectures like Transformers and BERT.\n\n4.  **Synthesis:**\n    The unified narrative connecting these works illustrates a dynamic evolution in knowledge graph embedding, moving from foundational vector space representations towards increasingly sophisticated, context-aware, and semantically rich methods. The collective contribution showcases a progression from addressing basic link prediction to incorporating richer information like entity types and textual descriptions, ultimately enabling explainable AI and robust, domain-specific applications. This path underscores KGEs' versatility and their continuous refinement through advanced neural architectures to tackle complex real-world challenges effectively."
  },
  "7572aefcd241ec76341addcb2e2e417587cb2e4c": {
    "seed_title": "Knowledge Graph Embedding Based Question Answering",
    "summary": "This citation path illustrates a progression from foundational training considerations in Knowledge Graph Embedding (KGE) to advanced model architectures addressing complex semantic challenges.\n\n1.  **Methodological Evolution:**\n    Paper 2 (2024) provides a comprehensive review of negative sampling (NS) methods, a critical methodological component for effectively training KGE models, categorizing existing approaches and outlining their advantages. This foundational understanding of training mechanisms then informs the development of more sophisticated embedding architectures. Paper 1 (2025) introduces ConQuatE, a novel methodological approach that leverages quaternion rotation and contextual cues to enhance entity representations, specifically addressing the polysemy issue by capturing diverse relational contexts without requiring extra information.\n\n2.  **Knowledge Progression:**\n    Paper 2 addresses the fundamental challenge of generating high-quality negative samples, which is crucial for the accurate training of KGE models and significantly impacts their performance. Building upon the general framework of KGE (which implicitly relies on effective training strategies like those surveyed in Paper 2), Paper 1 tackles a specific representational limitation: polysemy, where entities exhibit varying semantics based on their relational context. It identifies that current KGE models suffer from weak entity-relation interactions and low expressiveness. ConQuatE (Paper 1) emerges as a new capability, enhancing representation learning by incorporating contextual cues through efficient quaternion transformations, thereby improving link prediction accuracy by better modeling complex semantic structures.\n\n3.  **Temporal Context:**\n    The publication years, 2024 and 2025, indicate a rapid and forward-looking evolution in the field. Paper 2's comprehensive review of negative sampling in 2024 suggests a maturation of this core training aspect, while Paper 1's novel model in 2025 demonstrates an immediate progression to addressing complex representational challenges, highlighting an active and accelerating research trajectory.\n\n4.  **Synthesis:**\n    Collectively, these works narrate a unified progression from optimizing the fundamental training processes of KGE models to enhancing their representational power for complex semantic phenomena. Paper 2 provides essential insights into a critical training component (negative sampling), which underpins the success of KGE models. Paper 1 then builds on this established foundation by introducing a sophisticated embedding architecture (ConQuatE) to overcome the limitations of polysemy, thereby significantly advancing the expressiveness and accuracy of knowledge graph embedding for tasks like link prediction.",
    "path": [
      "7572aefcd241ec76341addcb2e2e417587cb2e4c",
      "b2d2ad9a458bdcb0523d22be659eb013ca2d3c67",
      "bbb89d88ad5b8279709ff089d3c00cd2750cd26b",
      "95c3d25b40f963eb248136555bd9b9e35817cc09",
      "8fef3f8bb8bcd254898b5d24f3d78beab09e99d4",
      "3f0d5aa7a637d2c0bb3d768c99cc203430b4481e",
      "a166957ec488cd20e61360d630568b3b81af3397",
      "d605a7628b2a7ff8ce04fc27111626e2d734cab4",
      "23efe9b99b5f0e79d7dbd4e3bfcf1c2d8b23c1ff",
      "c64433657869ecdaaa7988a029eabfe774d3ac47",
      "4801db5c5cb24a9069f2d264252fa26986ceefa9"
    ],
    "layer1_papers": [
      {
        "title": "Knowledge Graph Embedding Based Question Answering",
        "abstract": "Question answering over knowledge graph (QA-KG) aims to use facts in the knowledge graph (KG) to answer natural language questions. It helps end users more efficiently and more easily access the substantial and valuable knowledge in the KG, without knowing its data structures. QA-KG is a nontrivial problem since capturing the semantic meaning of natural language is difficult for a machine. Meanwhile, many knowledge graph embedding methods have been proposed. The key idea is to represent each predicate/entity as a low-dimensional vector, such that the relation information in the KG could be preserved. The learned vectors could benefit various applications such as KG completion and recommender systems. In this paper, we explore to use them to handle the QA-KG problem. However, this remains a challenging task since a predicate could be expressed in different ways in natural language questions. Also, the ambiguity of entity names and partial names makes the number of possible answers large. To bridge the gap, we propose an effective Knowledge Embedding based Question Answering (KEQA) framework. We focus on answering the most common types of questions, i.e., simple questions, in which each question could be answered by the machine straightforwardly if its single head entity and single predicate are correctly identified. To answer a simple question, instead of inferring its head entity and predicate directly, KEQA targets at jointly recovering the question's head entity, predicate, and tail entity representations in the KG embedding spaces. Based on a carefully-designed joint distance metric, the three learned vectors' closest fact in the KG is returned as the answer. Experiments on a widely-adopted benchmark demonstrate that the proposed KEQA outperforms the state-of-the-art QA-KG methods.",
        "summary": "",
        "year": 2019
      }
    ],
    "layer2_papers": [
      {
        "title": "TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation",
        "abstract": "Knowledge graph embedding (KGE) aims to learn continuous vectors of relations and entities in knowledge graph. Recently, transition-based KGE methods have achieved promising performance, where the single relation vector learns to translate head entity to tail entity. However, this scoring pattern is not suitable for complex scenarios where the same entity pair has different relations. Previous models usually focus on the improvement of entity representation for 1-to-N, N-to-1 and N-to-N relations, but ignore the single relation vector. In this paper, we propose a novel transition-based method, TranS, for knowledge graph embedding. The single relation vector in traditional scoring patterns is replaced with synthetic relation representation, which can solve these issues effectively and efficiently. Experiments on a large knowledge graph dataset, ogbl-wikikg2, show that our model achieves state-of-the-art results.",
        "year": 2022
      },
      {
        "title": "Efficient Non-Sampling Knowledge Graph Embedding",
        "abstract": "Knowledge Graph (KG) is a flexible structure that is able to describe the complex relationship between data entities. Currently, most KG embedding models are trained based on negative sampling, i.e., the model aims to maximize some similarity of the connected entities in the KG, while minimizing the similarity of the sampled disconnected entities. Negative sampling helps to reduce the time complexity of model learning by only considering a subset of negative instances, which may fail to deliver stable model performance due to the uncertainty in the sampling procedure. To avoid such deficiency, we propose a new framework for KG embedding—Efficient Non-Sampling Knowledge Graph Embedding (NS-KGE). The basic idea is to consider all of the negative instances in the KG for model learning, and thus to avoid negative sampling. The framework can be applied to square-loss based knowledge graph embedding models or models whose loss can be converted to a square loss. A natural side-effect of this non-sampling strategy is the increased computational complexity of model learning. To solve the problem, we leverage mathematical derivations to reduce the complexity of non-sampling loss function, which eventually provides us both better efficiency and better accuracy in KG embedding compared with existing models. Experiments on benchmark datasets show that our NS-KGE framework can achieve a better performance on efficiency and accuracy over traditional negative sampling based models, and that the framework is applicable to a large class of knowledge graph embedding models.",
        "year": 2021
      },
      {
        "title": "LineaRE: Simple but Powerful Knowledge Graph Embedding for Link Prediction",
        "abstract": "The task of link prediction for knowledge graphs is to predict missing relationships between entities. Knowledge graph embedding, which aims to represent entities and relations of a knowledge graph as low dimensional vectors in a continuous vector space, has achieved promising predictive performance. If an embedding model can cover different types of connectivity patterns and mapping properties of relations as many as possible, it will potentially bring more benefits for link prediction tasks. In this paper, we propose a novel embedding model, namely LineaRE, which is capable of modeling four connectivity patterns (i.e., symmetry, antisymmetry, inversion, and composition) and four mapping properties (i.e., one-to-one, one-to-many, many-to-one, and many-to-many) of relations. Specifically, we regard knowledge graph embedding as a simple linear regression task, where a relation is modeled as a linear function of two low-dimensional vector-presented entities with two weight vectors and a bias vector. Since the vectors are defined in a real number space and the scoring function of the model is linear, our model is simple and scalable to large knowledge graphs. Experimental results on multiple widely used real-world datasets show that the proposed LineaRE model significantly outperforms existing state-of-the-art models for link prediction tasks.",
        "year": 2020
      },
      {
        "title": "Understanding Negative Sampling in Knowledge Graph Embedding",
        "abstract": "Knowledge graph embedding (KGE) is to project entities and relations of a knowledge graph (KG) into a low-dimensional vector space, which has made steady progress in recent years. Conventional KGE methods, especially translational distance-based models, are trained through discriminating positive samples from negative ones. Most KGs store only positive samples for space efficiency. Negative sampling thus plays a crucial role in encoding triples of a KG. The quality of generated negative samples has a direct impact on the performance of learnt knowledge representation in a myriad of downstream tasks, such as recommendation, link prediction and node classification. We summarize current negative sampling approaches in KGE into three categories, static distribution-based, dynamic distribution-based and custom cluster-based respectively. Based on this categorization we discuss the most prevalent existing approaches and their characteristics. It is a hope that this review can provide some guidelines for new thoughts about negative sampling in KGE.",
        "year": 2021
      },
      {
        "title": "A Lightweight Knowledge Graph Embedding Framework for Efficient Inference and Storage",
        "abstract": "Knowledge graphs, which consist of entities and their relations, have become a popular way to store structured knowledge. Knowledge graph embedding (KGE), which derives a representation for each entity and relation, has been widely used to capture the semantics of the information in the knowledge graphs, and has demonstrated great success in many downstream applications, such as the extraction of similar entities in response to a query entity. However, existing KGE methods cannot work well on emerging knowledge graphs that are large-scale due to the constraints in storage and inference efficiency. In this paper, we propose a lightweight KGE model, LightKG, which significantly reduces storage as well as running time needed for inference. Instead of storing a continuous vector for every entity, LightKG only needs to store a few codebooks, each of which contains some codewords that correspond to the representatives among the embeddings, and the indices that correspond to the codeword selections for entities. Hence LightKG can achieve highly efficient storage. The efficiency of the downstream querying process can be significantly boosted too with the proposed LightKG model as the relevance score between the query and an entity can be efficiently calculated via a quick look-up in a table that contains the scores between the query and codewords. The storage and inference efficiency of LightKG is achieved by its novel design. LightKG is an end-to-end framework that automatically infers codebooks and codewords and generates an approximated embedding for each entity. A residual module is included in LightKG to induce the diversity among codebooks, and a continuous function is adopted to approximate codeword selection, which is non-differential. In addition, to further improve the performance of KGE, we propose a novel dynamic negative sampling method based on quantization, which can be applied to the proposed LightKG or other KGE methods. We conduct extensive experiments on five public datasets. The experiments show that LightKG is search and memory efficient with high approximate search accuracy. Also, the dynamic negative sampling can dramatically improve model performance with over 19% improvement on average.",
        "year": 2021
      },
      {
        "title": "Multimodal reasoning based on knowledge graph embedding for specific diseases",
        "abstract": "Abstract Motivation Knowledge Graph (KG) is becoming increasingly important in the biomedical field. Deriving new and reliable knowledge from existing knowledge by KG embedding technology is a cutting-edge method. Some add a variety of additional information to aid reasoning, namely multimodal reasoning. However, few works based on the existing biomedical KGs are focused on specific diseases. Results This work develops a construction and multimodal reasoning process of Specific Disease Knowledge Graphs (SDKGs). We construct SDKG-11, a SDKG set including five cancers, six non-cancer diseases, a combined Cancer5 and a combined Diseases11, aiming to discover new reliable knowledge and provide universal pre-trained knowledge for that specific disease field. SDKG-11 is obtained through original triplet extraction, standard entity set construction, entity linking and relation linking. We implement multimodal reasoning by reverse-hyperplane projection for SDKGs based on structure, category and description embeddings. Multimodal reasoning improves pre-existing models on all SDKGs using entity prediction task as the evaluation protocol. We verify the model’s reliability in discovering new knowledge by manually proofreading predicted drug–gene, gene–disease and disease–drug pairs. Using embedding results as initialization parameters for the biomolecular interaction classification, we demonstrate the universality of embedding models. Availability and implementation The constructed SDKG-11 and the implementation by TensorFlow are available from https://github.com/ZhuChaoY/SDKG-11. Supplementary information Supplementary data are available at Bioinformatics online.",
        "year": 2022
      },
      {
        "title": "Embedding knowledge graph of patent metadata to measure knowledge proximity",
        "abstract": "Knowledge proximity refers to the strength of association between any two entities in a structural form that embodies certain aspects of a knowledge base. In this work, we operationalize knowledge proximity within the context of the US Patent Database (knowledge base) using a knowledge graph (structural form) named “PatNet” built using patent metadata, including citations, inventors, assignees, and domain classifications. We train various graph embedding models using PatNet to obtain the embeddings of entities and relations. The cosine similarity between the corresponding (or transformed) embeddings of entities denotes the knowledge proximity between these. We compare the embedding models in terms of their performances in predicting target entities and explaining domain expansion profiles of inventors and assignees. We then apply the embeddings of the best‐preferred model to associate homogeneous (e.g., patent–patent) and heterogeneous (e.g., inventor–assignee) pairs of entities.",
        "year": 2022
      },
      {
        "title": "Marie and BERT—A Knowledge Graph Embedding Based Question Answering System for Chemistry",
        "abstract": "This paper presents a novel knowledge graph question answering (KGQA) system for chemistry, which is implemented on hybrid knowledge graph embeddings, aiming to provide fact-oriented information retrieval for chemistry-related research and industrial applications. Unlike other existing designs, the system operates on multiple embedding spaces, which use various embedding methods and queries the embedding spaces in parallel. With the answers returned from multiple embedding spaces, the system leverages a score alignment model to adjust the answer scores and rerank the answers. Further, the system implements an algorithm to derive implicit multihop relations to handle the complexities of deep ontologies and improve multihop question answering. The system also implements a BERT-based bidirectional entity-linking model to enhance the robustness and accuracy of the entity-linking module. The system uses a joint numerical embedding model to efficiently handle numerical filtering questions. Further, it can invoke semantic agents to perform dynamic calculations autonomously. Finally, the KGQA system handles numerous chemical reaction mechanisms using semantic parsing supported by a Linked Data Fragment server. This paper evaluates the accuracy of each module within the KGQA system with a chemistry question data set.",
        "year": 2023
      }
    ],
    "layer3_papers": [
      {
        "title": "Contextualized Quaternion Embedding Towards Polysemy in Knowledge Graph for Link Prediction",
        "abstract": "To meet the challenge of incompleteness within Knowledge Graphs, Knowledge Graph Embedding (KGE) has emerged as the fundamental methodology for predicting the missing link (Link Prediction), by mapping entities and relations as low-dimensional vectors in continuous space. However, current KGE models often struggle with the polysemy issue, where entities exhibit different semantic characteristics depending on the relations in which they participate. Such limitation stems from weak interactions between entities and their relation contexts, leading to low expressiveness in modeling complex structures and resulting in inaccurate predictions. To address this, we propose Contextualized Quaternion Embedding (ConQuatE), a model that enhances the representation learning of entities across multiple semantic dimensions by leveraging quaternion rotation to capture diverse relational contexts. In specific, ConQuatE incorporates contextual cues from various connected relations to enrich the original entity representations. Notably, this is achieved through efficient vector transformations in quaternion space, without any extra information required other than original triples. Experimental results demonstrate that our model outperforms state-of-the-art models for Link Prediction on four widely recognized datasets: FB15k-237, WN18RR, FB15k, and WN18.",
        "year": 2025
      },
      {
        "title": "Negative Sampling in Knowledge Graph Representation Learning: A Review",
        "abstract": "Knowledge Graph Representation Learning (KGRL), or Knowledge Graph Embedding (KGE), is essential for AI applications such as knowledge construction and information retrieval. These models encode entities and relations into lower-dimensional vectors, supporting tasks like link prediction and recommendation systems. Training KGE models relies on both positive and negative samples for effective learning, but generating high-quality negative samples from existing knowledge graphs is challenging. The quality of these samples significantly impacts the model's accuracy. This comprehensive survey paper systematically reviews various negative sampling (NS) methods and their contributions to the success of KGRL. Their respective advantages and disadvantages are outlined by categorizing existing NS methods into six distinct categories. Moreover, this survey identifies open research questions that serve as potential directions for future investigations. By offering a generalization and alignment of fundamental NS concepts, this survey provides valuable insights for designing effective NS methods in the context of KGRL and serves as a motivating force for further advancements in the field.",
        "year": 2024
      }
    ],
    "layer2_summary": "This citation path illustrates the dynamic evolution of research in knowledge graph embedding (KGE), moving from foundational model improvements to addressing practical challenges and expanding into complex, domain-specific applications.\n\n1.  **Methodological Evolution:**\n    The methodological evolution demonstrates a progression from foundational models addressing diverse relation properties to sophisticated approaches tackling efficiency, representation complexity, and integration with other AI techniques. LineaRE (2020) establishes an early framework using simple linear regression to robustly model various relation patterns and mapping properties. Subsequent work then diverges into optimizing training (NS-KGE, 2021, by avoiding negative sampling; LightKG, 2021, by proposing dynamic negative sampling) and refining entity/relation representations (TranS, 2022, with synthetic relation representation for complex scenarios). The later papers, Multimodal reasoning (2022) and Marie and BERT (2023), showcase a significant shift towards multimodal and hybrid embedding strategies, integrating structural, categorical, descriptive, and textual information, often leveraging advanced NLP models like BERT.\n\n2.  **Knowledge Progression:**\n    This path demonstrates a clear progression from foundational KGE modeling to addressing practical challenges and expanding application capabilities. LineaRE (2020) initially tackles the problem of comprehensively modeling diverse relation patterns and mapping properties for improved link prediction. The critical role of negative sampling is then elucidated by Understanding Negative Sampling (2021), leading to NS-KGE (2021) which overcomes the instability of sampling by considering all negative instances, and LightKG (2021) which proposes a dynamic, quantization-based negative sampling for efficiency and performance. LightKG (2021) further addresses the scalability issue by introducing a lightweight, codebook-based framework for efficient storage and inference on large KGs. TranS (2022) refines transition-based methods by solving the limitation of single relation vectors in complex scenarios with synthetic relation representation. The later papers, Multimodal reasoning (2022) and Embedding patent metadata (2022), showcase KGE's utility in specialized domains (biomedical, patent analysis) and introduce multimodal reasoning capabilities by integrating various data types. Finally, Marie and BERT (2023) represents a significant leap, building on KGE foundations to develop a sophisticated Question Answering system for chemistry, demonstrating new capabilities in handling complex queries, multi-hop relations, and integrating advanced NLP (BERT) for robust entity linking.\n\n3.  **Temporal Context:**\n    The publication years, spanning 2020 to 2023, reveal a rapid and continuous evolution in knowledge graph embedding research. An acceleration is particularly noticeable in 2021 with multiple papers (NS-KGE, Understanding Negative Sampling, LightKG) focusing on critical training aspects like negative sampling and efficiency. This quickly transitions in 2022 to refinements in core embedding models (TranS) and the emergence of domain-specific applications and multimodal approaches (Multimodal reasoning, Embedding patent metadata), culminating in 2023 with sophisticated KGE-powered AI systems (Marie and BERT) that integrate state-of-the-art NLP.\n\n4.  **Synthesis:**\n    Collectively, these works narrate the maturation of knowledge graph embedding from foundational model development to addressing critical practical challenges and expanding into complex, real-world applications. The unified narrative shows a progression from ensuring comprehensive relation modeling (LineaRE, 2020) and optimizing training efficiency (NS-KGE, 2021; LightKG, 2021) to refining representation for complex scenarios (TranS, 2022) and integrating multimodal information (Multimodal reasoning, 2022). The collective contribution is the establishment of KGE as a versatile and robust technology, capable of not only accurately representing knowledge but also serving as a core component in advanced AI systems like domain-specific Question Answering (Marie and BERT, 2023), thereby significantly broadening its impact and utility."
  },
  "a6a735f8e218f772e5b9dac411fa4abea87fdb9c": {
    "seed_title": "Recurrent knowledge graph embedding for effective recommendation",
    "summary": "This citation path illustrates a rapid evolution in knowledge graph embedding (KGE) for recommender systems, moving from addressing fundamental challenges like cross-domain recommendations to incorporating advanced features like explainability and contextualization.\n\n1.  **Methodological Evolution:**\n    The methodological evolution shows a clear progression from general cross-domain interaction modeling to highly contextualized and explainable representations. Paper 2 introduces a \"cross-domain knowledge graph chiasmal embedding\" approach with a \"binding rule\" to efficiently interact items across multiple domains, primarily focusing on link prediction for multi-domain item-item recommendations. Building upon the general capability of KGE to model complex relationships, Paper 1 significantly advances this by proposing CKGE, a \"contextualized knowledge graph embedding\" method. This involves constructing \"meta-graphs\" with \"contextualized neighbor semantics\" and \"high-order connections\" as \"motivation-aware information,\" then processing these with a novel \"KG-based Transformer\" equipped with \"relational attention\" and \"structural encoding,\" alongside \"local path mask prediction\" for explainability.\n\n2.  **Knowledge Progression:**\n    Paper 2 addresses the critical problems of cross-domain cold start and providing multi-domain recommendations, which traditional recommender systems struggle with due to sparsity. It builds upon the general utility of KGE by extending it to efficiently model associations and interactions between items *across diverse domains*, enabling multi-domain item-item recommendations. Paper 1 then builds upon the established capability of KGE to handle complex relationships by tackling the more advanced challenge of providing *explainable* recommendations that consider different *learning motivations* in talent training. It moves beyond mere prediction to offer new capabilities: not only precise recommendations but also the ability to \"discriminate the saliencies of meta-paths,\" effectively revealing *why* a particular recommendation is made and the importance of different motivational factors.\n\n3.  **Temporal Context:**\n    Both papers being published in 2023 highlights a period of intense and rapid innovation in the field of KGE for recommender systems. This simultaneous publication suggests an acceleration in research, driven by the increasing demand for more sophisticated, robust, and interpretable recommendation solutions. The adoption of advanced architectures like the \"KG-based Transformer\" in Paper 1 also reflects the integration of state-of-the-art deep learning techniques into KGE research.\n\n4.  **Synthesis:**\n    These works collectively narrate an evolution of KGE from a powerful tool for integrating information across disparate domains to a refined mechanism for generating highly personalized, context-aware, and transparent recommendations. Their collective contribution to \"knowledge graph embedding\" is pushing the boundaries beyond basic link prediction and entity representation towards sophisticated modeling of complex, multi-faceted user preferences and item characteristics, with a growing emphasis on interpretability and practical applicability in real-world scenarios.",
    "path": [
      "a6a735f8e218f772e5b9dac411fa4abea87fdb9c",
      "b1d807fc6b184d757ebdea67acd81132d8298ff6",
      "145fa4ea1567a6b9d981fdea0e183140d99aeb97"
    ],
    "layer1_papers": [
      {
        "title": "Recurrent knowledge graph embedding for effective recommendation",
        "abstract": "Knowledge graphs (KGs) have proven to be effective to improve recommendation. Existing methods mainly rely on hand-engineered features from KGs (e.g., meta paths), which requires domain knowledge. This paper presents RKGE, a KG embedding approach that automatically learns semantic representations of both entities and paths between entities for characterizing user preferences towards items. Specifically, RKGE employs a novel recurrent network architecture that contains a batch of recurrent networks to model the semantics of paths linking a same entity pair, which are seamlessly fused into recommendation. It further employs a pooling operator to discriminate the saliency of different paths in characterizing user preferences towards items. Extensive validation on real-world datasets shows the superiority of RKGE against state-of-the-art methods. Furthermore, we show that RKGE provides meaningful explanations for recommendation results.",
        "summary": "",
        "year": 2018
      }
    ],
    "layer2_papers": [
      {
        "title": "Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation",
        "abstract": "Learning and development, or L&D, plays an important role in talent management, which aims to improve the knowledge and capabilities of employees through a variety of performance-oriented training activities. Recently, with the rapid development of enterprise management information systems, many research efforts and industrial practices have been devoted to building personalized employee training course recommender systems. Nevertheless, a widespread challenge is how to provide explainable recommendations with the consideration of different learning motivations from talents. To this end, we propose CKGE, a contextualized knowledge graph (KG) embedding approach for developing an explainable training course recommender system. A novel perspective of CKGE is to integrate both the contextualized neighbor semantics and high-order connections as motivation-aware information for learning effective representations of talents and courses. Specifically, in CKGE, for each entity pair (i.e., the talent-course pair), we first construct a meta-graph, including the neighbors of each entity and the meta-paths between entities as motivation-aware information. Then, we develop a novel KG-based Transformer, which can serialize entities and paths in the meta-graph as a sequential input, with the specially designed relational attention and structural encoding mechanisms to better model the global dependence of KG structured data. Meanwhile, the local path mask prediction can effectively reveal the importance of different paths. As a result, CKGE not only can make precise predictions but also can discriminate the saliencies of meta-paths in characterizing corresponding preferences. Extensive experiments on real-world and public datasets clearly validate the effectiveness and interpretability of CKGE compared with state-of-the-art baselines.",
        "year": 2023
      },
      {
        "title": "Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation",
        "abstract": "Recommender system can provide users with the required information accurately and efficiently, playing a very important role in improving users’ life experience. Although knowledge graph-based recommender system can solve the sparsity and cold start problems faced by traditional recommender system, it cannot handle the cross-domain cold start problem and cannot provide multi-domain recommendations. Therefore, this paper focuses on multi-domain item-item (I2I) recommendation based on cross-domain knowledge graph embedding by analyzing the association between items of the same domain and the interaction between items of diverse domains with the aid of knowledge graph that contains rich information. First, a cross-domain knowledge graph chiasmal embedding approach is proposed to efficiently interact all items in multiple domains. To help achieve both homo-domain embedding and hetero-domain embedding of items, a binding rule is put forward. Second, a multi-domain I2I recommendation method is presented to efficiently recommend items in multiple domains, which is a recommendation method based on link prediction of knowledge graph. Finally, the proposed methods are compared and analyzed with some benchmark methods using two datasets. The experimental results show that the proposed methods achieve better link prediction results and multi-domain recommendation results.",
        "year": 2023
      }
    ],
    "layer3_papers": [],
    "layer2_summary": "This citation path showcases the rapid and diverse evolution of knowledge graph embedding (KGE) techniques applied to recommender systems, both published in 2023.\n\n1.  **Methodological Evolution:**\n    The methodological evolution demonstrates a shift towards more sophisticated and context-aware KGE architectures. Paper 2 introduces \"cross-domain knowledge graph chiasmal embedding\" with a \"binding rule\" to explicitly model and interact items across multiple domains, a structural innovation for multi-domain challenges. Paper 1, in contrast, advances KGE by proposing CKGE, a \"contextualized knowledge graph embedding\" approach that integrates \"contextualized neighbor semantics and high-order connections\" as motivation-aware information. Crucially, Paper 1 employs a novel \"KG-based Transformer\" with \"relational attention and structural encoding mechanisms,\" representing a significant leap towards leveraging advanced deep learning architectures for capturing global dependencies and local path importance within KGs, moving beyond simpler embedding functions.\n\n2.  **Knowledge Progression:**\n    The progression of knowledge addresses increasingly complex challenges in recommender systems using KGE. Paper 2 tackles the limitations of traditional KG-based recommenders in handling the \"cross-domain cold start problem\" and providing \"multi-domain recommendations,\" a crucial capability for modern platforms. It builds upon the foundational idea of KGE for recommendation by extending it to explicitly model and embed items across diverse domains, enabling multi-domain item-item (I2I) recommendations. Paper 1 then addresses the critical need for \"explainable recommendations\" and the consideration of \"different learning motivations\" in talent training. It builds on the general utility of KGE by introducing a mechanism (\"local path mask prediction\") to not only make precise predictions but also \"discriminate the saliencies of meta-paths,\" thereby providing interpretability and new insights into user preferences, a capability largely absent in Paper 2's focus on cross-domain accuracy.\n\n3.  **Temporal Context:**\n    Both papers being published in 2023 highlights a concurrent and accelerated period of innovation in KGE for recommender systems. This timing reflects the maturity of KGE research and its increasing application to complex, real-world problems. The adoption of Transformer architectures in Paper 1 aligns with broader technological advances in deep learning, demonstrating how state-of-the-art neural network designs are being adapted to structured graph data for enhanced performance and interpretability.\n\n4.  **Synthesis:**\n    These works collectively illustrate a unified narrative of extending knowledge graph embedding beyond basic single-domain link prediction to address critical real-world challenges in recommender systems. Paper 2 expands the scope of KGE to handle the complexities of multi-domain interactions, while Paper 1 pushes the frontier towards explainability and context-awareness within specific application domains. Together, they contribute to the field of \"knowledge graph embedding\" by demonstrating its increasing versatility and sophistication in developing more robust, interpretable, and domain-adaptive recommendation systems."
  },
  "d899e434a7f2eecf33a90053df84cf32842fbca9": {
    "seed_title": "Bootstrapping Entity Alignment with Knowledge Graph Embedding",
    "summary": "This citation path, extending the previous context, illustrates a dynamic evolution in knowledge graph embedding (KGE) research, moving from foundational task-specific refinements to sophisticated neural architectures and comprehensive meta-analyses, now incorporating specialized surveys and advanced graph transformer frameworks.\n\n1.  **Methodological Evolution:**\n    The methodological evolution continues its trajectory from refining KGEs for specific tasks and integrating external information (Paper 3) to the adoption of advanced neural architectures like the Transformer (Paper 4). Paper 7 (2024) contributes a meta-level methodological framework by proposing a new three-module structure (information aggregation, entity alignment, and post-alignment) for organizing and analyzing entity alignment (EA) methods based on representation learning, providing a structured lens for future research. Building directly on the Transformer's success, Paper 8 (2025) introduces a significant innovation with TGformer, the *first* framework to use a graph transformer for KGE, explicitly modeling both triplet-level and graph-level structural features in static and *temporal* knowledge graphs, a notable expansion of scope.\n\n2.  **Knowledge Progression:**\n    The progression of knowledge continues to address limitations and introduce new capabilities. Paper 7 (2024) addresses the need for a comprehensive, up-to-date understanding of EA methods based on representation learning, building upon general surveys (Paper 6) by offering a task-specific consolidation, categorization, and identification of future research directions. Paper 8 (2025) tackles critical limitations of existing KGE methods: triplet-based approaches that ignore graph structure, and graph-based methods that overlook contextual information of nodes. It introduces the novel capability of integrating multi-structural features (triplet-level and graph-level) and extends KGE to the challenging domain of *temporal* knowledge graphs, a capability not explicitly addressed by previous papers in this path, thereby significantly enhancing the model's ability to understand entities and relations in diverse contexts.\n\n3.  **Temporal Context:**\n    The papers, now spanning from 2019 to 2025, demonstrate a sustained and accelerating pace of innovation in KGE research. The 2024 survey (Paper 7) signifies a continued effort to consolidate and categorize knowledge within specific sub-domains like EA, even *after* the initial wave of advanced model development (e.g., Paper 4 in 2023). This consolidation quickly precedes, and likely informs, the next wave of architectural breakthroughs, as evidenced by Paper 8 (2025) which pushes the state-of-the-art with a novel graph transformer framework, highlighting a rapid, almost yearly, integration of cutting-edge deep learning techniques into KGE.\n\n4.  **Synthesis:**\n    This extended collection of works forms a unified narrative of continuously striving for more accurate, robust, and expressive KGEs. The collective contribution to \"knowledge graph embedding\" is a comprehensive advancement in its capabilities for tasks like entity alignment and link prediction, achieved through a dual approach: systematic consolidation and analysis of existing methods (Paper 7) to identify gaps, and the relentless pursuit of more powerful and comprehensive models (Paper 8) that leverage advanced neural architectures to capture increasingly complex structural and contextual information, including temporal dynamics.",
    "path": [
      "d899e434a7f2eecf33a90053df84cf32842fbca9",
      "ecc04e9285f016090697a1a8f9e96ce01e94e742",
      "84aa127dc5ca3080385439cb10edc50b5d2c04e4",
      "af051c87cecca64c2de4ad9110608f7579766653",
      "29052ddd048acb1afa2c42613068b63bb7428a34",
      "95c3d25b40f963eb248136555bd9b9e35817cc09",
      "f470e11faa6200026cf39e248510070c078e509a",
      "52b167a90a10cde25309e40d7f6e6b5e14ec3261",
      "3f170af3566f055e758fa3bdf2bfd3a0e8787e58"
    ],
    "layer1_papers": [
      {
        "title": "Bootstrapping Entity Alignment with Knowledge Graph Embedding",
        "abstract": "Embedding-based entity alignment represents different knowledge graphs (KGs) as low-dimensional embeddings and finds entity alignment by measuring the similarities between entity embeddings. Existing approaches have achieved promising results, however, they are still challenged by the lack of enough prior alignment as labeled training data. In this paper, we propose a bootstrapping approach to embedding-based entity alignment. It iteratively labels likely entity alignment as training data for learning alignment-oriented KG embeddings. Furthermore, it employs an alignment editing method to reduce error accumulation during iterations. Our experiments on real-world datasets showed that the proposed approach significantly outperformed the state-of-the-art embedding-based ones for entity alignment. The proposed alignment-oriented KG embedding, bootstrapping process and alignment editing method all contributed to the performance improvement.",
        "summary": "",
        "year": 2018
      }
    ],
    "layer2_papers": [
      {
        "title": "Semi-Supervised Entity Alignment via Knowledge Graph Embedding with Awareness of Degree Difference",
        "abstract": "Entity alignment associates entities in different knowledge graphs if they are semantically same, and has been successfully used in the knowledge graph construction and connection. Most of the recent solutions for entity alignment are based on knowledge graph embedding, which maps knowledge entities in a low-dimension space where entities are connected with the guidance of prior aligned entity pairs. The study in this paper focuses on two important issues that limit the accuracy of current entity alignment solutions: 1) labeled data of priorly aligned entity pairs are difficult and expensive to acquire, whereas abundant of unlabeled data are not used; and 2) knowledge graph embedding is affected by entity's degree difference, which brings challenges to align high frequent and low frequent entities. We propose a semi-supervised entity alignment method (SEA) to leverage both labeled entities and the abundant unlabeled entity information for the alignment. Furthermore, we improve the knowledge graph embedding with awareness of the degree difference by performing the adversarial training. To evaluate our proposed model, we conduct extensive experiments on real-world datasets. The experimental results show that our model consistently outperforms the state-of-the-art methods with significant improvement on alignment accuracy.",
        "year": 2019
      },
      {
        "title": "Knowledge graph embedding methods for entity alignment: experimental review",
        "abstract": "In recent years, we have witnessed the proliferation of knowledge graphs (KG) in various domains, aiming to support applications like question answering, recommendations, etc. A frequent task when integrating knowledge from different KGs is to find which subgraphs refer to the same real-world entity, a task largely known as the Entity Alignment. Recently, embedding methods have been used for entity alignment tasks, that learn a vector-space representation of entities which preserves their similarity in the original KGs. A wide variety of supervised, unsupervised, and semi-supervised methods have been proposed that exploit both factual (attribute based) and structural information (relation based) of entities in the KGs. Still, a quantitative assessment of their strengths and weaknesses in real-world KGs according to different performance metrics and KG characteristics is missing from the literature. In this work, we conduct the first meta-level analysis of popular embedding methods for entity alignment, based on a statistically sound methodology. Our analysis reveals statistically significant correlations of different embedding methods with various meta-features extracted by KGs and rank them in a statistically significant way according to their effectiveness across all real-world KGs of our testbed. Finally, we study interesting trade-offs in terms of methods’ effectiveness and efficiency.",
        "year": 2022
      },
      {
        "title": "OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding",
        "abstract": "Semantic embedding has been widely investigated for aligning knowledge graph (KG) entities. Current methods have explored and utilized the graph structure, the entity names and attributes, but ignore the ontology (or ontological schema) which contains critical meta information such as classes and their membership relationships with entities. In this paper, we propose an ontology-guided entity alignment method named OntoEA, where both KGs and their ontologies are jointly embedded, and the class hierarchy and the class disjointness are utilized to avoid false mappings. Extensive experiments on seven public and industrial benchmarks have demonstrated the state-of-the-art performance of OntoEA and the effectiveness of the ontologies.",
        "year": 2021
      },
      {
        "title": "Position-Aware Relational Transformer for Knowledge Graph Embedding",
        "abstract": "Although Transformer has achieved success in language and vision tasks, its capacity for knowledge graph (KG) embedding has not been fully exploited. Using the self-attention (SA) mechanism in Transformer to model the subject-relation-object triples in KGs suffers from training inconsistency as SA is invariant to the order of input tokens. As a result, it is unable to distinguish a (real) relation triple from its shuffled (fake) variants (e.g., object-relation-subject) and, thus, fails to capture the correct semantics. To cope with this issue, we propose a novel Transformer architecture, namely, Knowformer, for KG embedding. It incorporates relational compositions in entity representations to explicitly inject semantics and capture the role of an entity based on its position (subject or object) in a relation triple. The relational composition for a subject (or object) entity of a relation triple refers to an operator on the relation and the object (or subject). We borrow ideas from the typical translational and semantic-matching embedding techniques to design relational compositions. We carefully design a residual block to integrate relational compositions into SA and efficiently propagate the composed relational semantics layer by layer. We formally prove that the SA with relational compositions is able to distinguish the entity roles in different positions and correctly capture relational semantics. Extensive experiments and analyses on six benchmark datasets show that Knowformer achieves state-of-the-art performance on both link prediction and entity alignment.",
        "year": 2023
      },
      {
        "title": "LineaRE: Simple but Powerful Knowledge Graph Embedding for Link Prediction",
        "abstract": "The task of link prediction for knowledge graphs is to predict missing relationships between entities. Knowledge graph embedding, which aims to represent entities and relations of a knowledge graph as low dimensional vectors in a continuous vector space, has achieved promising predictive performance. If an embedding model can cover different types of connectivity patterns and mapping properties of relations as many as possible, it will potentially bring more benefits for link prediction tasks. In this paper, we propose a novel embedding model, namely LineaRE, which is capable of modeling four connectivity patterns (i.e., symmetry, antisymmetry, inversion, and composition) and four mapping properties (i.e., one-to-one, one-to-many, many-to-one, and many-to-many) of relations. Specifically, we regard knowledge graph embedding as a simple linear regression task, where a relation is modeled as a linear function of two low-dimensional vector-presented entities with two weight vectors and a bias vector. Since the vectors are defined in a real number space and the scoring function of the model is linear, our model is simple and scalable to large knowledge graphs. Experimental results on multiple widely used real-world datasets show that the proposed LineaRE model significantly outperforms existing state-of-the-art models for link prediction tasks.",
        "year": 2020
      },
      {
        "title": "A Survey on Knowledge Graph Embedding",
        "abstract": "Knowledge graph (KG) is used to represent the relationships between different concepts in the real world. It is a special network in which nodes represent entities and edges represent relationships. KGs can intuitively model the connections between facts, but in many applications, there are certain limitations in directly using symbolic logic to represent knowledge in KGs and perform calculations, making it difficult to achieve expected results in downstream tasks. Meanwhile, with the explosive growth of Internet capacity, the traditional KG structure faces the problems of computational inefficiency and management difficulties. To alleviate these problems, Knowledge graph embedding (KGE) is proposed to improve the computational efficiency by embedding entities and relations in the KG into a low-dimensional, dense and continuous vector space, and thus the solution of some problems in the knowledge graph is transformed into vector operations. Moreover, KGE also can be used as a pre-trained model which is more beneficial to downstream applications, such as applications based on deep learning. In this paper, we classify KGE into three categories, namely translational distance models, semantic matching models and neural network based models. The advantages and disadvantages of different embedding methods are compared, while the main applications of KGE are summarized. Some current challenges of KGE are summarized, as well as some views on the future research directions of KGE.",
        "year": 2022
      }
    ],
    "layer3_papers": [
      {
        "title": "A survey: knowledge graph entity alignment research based on graph embedding",
        "abstract": "Entity alignment (EA) aims to automatically match entities in different knowledge graphs, which is beneficial to the development of knowledge-driven applications. Representation learning has powerful feature capture capability and it is widely used in the field of natural language processing. Compared with traditional EA methods, EA methods based on representation learning have better performance and efficiency. Hence, we summarize and analyze the representative EA approaches based on representation learning in this paper. We present the problem description and data preprocessing for EA and other related fundamental knowledge. We propose a new EA framework for the latest models, which includes information aggregation module, entity alignment module, and post-alignment module. Based on these three modules, the various technologies are described in detail. In the experimental part, we first explore the effect of EA direction on model performance. Then, we classify the models into different categories in terms of alignment inference strategy, noise filtering strategy, and whether additional information is utilized. To ensure fairness, we perform the comparative analysis of the performance of the models within the categories separately on different datasets. We investigate both unimodal and multimodal EA. Finally, we present future research perspectives based on the shortcomings of existing EA methods.",
        "year": 2024
      },
      {
        "title": "TGformer: A Graph Transformer Framework for Knowledge Graph Embedding",
        "abstract": "Knowledge graph embedding is efficient method for reasoning over known facts and inferring missing links. Existing methods are mainly triplet-based or graph-based. Triplet-based approaches learn the embedding of missing entities by a single triple only. They ignore the fact that the knowledge graph is essentially a graph structure. Graph-based methods consider graph structure information but ignore the contextual information of nodes in the knowledge graph, making them unable to discern valuable entity (relation) information. In response to the above limitations, we propose a general graph transformer framework for knowledge graph embedding (TGformer). It is the first to use a graph transformer to build knowledge embeddings with triplet-level and graph-level structural features in the static and temporal knowledge graph. Specifically, a context-level subgraph is constructed for each predicted triplet, which models the relation between triplets with the same entity. Afterward, we design a knowledge graph transformer network (KGTN) to fully explore multi-structural features in knowledge graphs, including triplet-level and graph-level, boosting the model to understand entities (relations) in different contexts. Finally, semantic matching is adopted to select the entity with the highest score. Experimental results on several public knowledge graph datasets show that our method can achieve state-of-the-art performance in link prediction.",
        "year": 2025
      }
    ],
    "layer2_summary": "This citation path illustrates a dynamic evolution in knowledge graph embedding (KGE) research, moving from foundational task-specific refinements to sophisticated neural architectures and comprehensive meta-analyses.\n\n1.  **Methodological Evolution:**\n    The methodological evolution begins with refining traditional KGEs for specific tasks. Paper 1 (2019) introduces semi-supervised learning and adversarial training to address data scarcity and degree bias in entity alignment (EA). Paper 5 (2020) focuses on explicit modeling of diverse relation connectivity patterns and mapping properties using a simple linear regression approach for link prediction (LP). A significant shift occurs with Paper 3 (2021), which integrates external ontological schema into a joint embedding framework for EA, moving beyond purely structural or attribute-based embeddings. The most recent methodological leap is seen in Paper 4 (2023), which adapts the powerful Transformer architecture for KGE, specifically addressing its order invariance limitation through position-aware relational compositions. The surveys (Paper 6, 2022) and meta-review (Paper 2, 2022) categorize and assess these diverse methods, including translational, semantic matching, and neural network models.\n\n2.  **Knowledge Progression:**\n    The progression of knowledge addresses limitations and introduces new capabilities. Paper 1 tackles the problems of expensive labeled data and the impact of entity degree difference on EA accuracy, proposing semi-supervised learning and adversarial training. Building on KGE for link prediction, Paper 5 addresses the limitation of simpler models by explicitly modeling various relation connectivity patterns and mapping properties, enhancing predictive performance. Paper 3 identifies and remedies the oversight of ontological schema in EA, leveraging class hierarchy and disjointness to improve alignment accuracy and avoid false mappings, a capability not explored by earlier EA methods like Paper 1. Paper 2, an experimental review, addresses the missing quantitative assessment of existing EA embedding methods (including those like Paper 1 and Paper 3), providing crucial insights into their strengths and weaknesses across real-world KGs. Paper 6 offers a comprehensive survey, consolidating the field's understanding, classifying methods, and outlining challenges, thereby setting the stage for future advancements. Finally, Paper 4 pushes the state-of-the-art by successfully adapting Transformers for KGE, overcoming the challenge of self-attention's order invariance to capture correct relational semantics and entity roles, leading to improved performance in both LP and EA.\n\n3.  **Temporal Context:**\n    The papers, spanning from 2019 to 2023, demonstrate a rapid acceleration in KGE research. Early works (Paper 1, 2019; Paper 5, 2020) focus on refining specific KGE tasks and embedding properties. The year 2022 marks a period of consolidation and assessment, with two significant review papers (Paper 2, Paper 6) reflecting the field's maturity and diversification. This consolidation quickly precedes the adoption of advanced neural architectures, as evidenced by Paper 4 (2023) leveraging Transformers, highlighting a swift integration of cutting-edge deep learning techniques into KGE.\n\n4.  **Synthesis:**\n    This collection of works forms a unified narrative of continuously striving for more accurate, robust, and expressive KGEs. The collective contribution to \"knowledge graph embedding\" is a comprehensive advancement in its capabilities for tasks like entity alignment and link prediction. This is achieved through integrating diverse information sources (e.g., ontology in Paper 3), improving learning paradigms (e.g., semi-supervised and adversarial training in Paper 1), and adopting cutting-edge neural architectures (e.g., Transformers in Paper 4), all while systematically reviewing and understanding the evolving landscape (Paper 2, Paper 6)."
  },
  "8c93f3cecf79bd9f8d021f589d095305e281dd2f": {
    "seed_title": "Knowledge Graph Embedding for Link Prediction",
    "summary": "This latest set of papers, all published in 2024, demonstrates a significant evolution in knowledge graph embedding (KGE) research, moving towards addressing critical practical challenges related to scalability, efficiency, and distributed, privacy-preserving learning.\n\n1.  **Methodological Evolution**:\n    The methodological evolution in KGE continues to advance from foundational model development to sophisticated, system-level, and personalized approaches. Building on previous work that introduced parallelization (Paper 2) and integrated advanced AI techniques like PLMs (Paper 3), the new papers introduce specialized methodologies. Paper A (PFedEG) pioneers *personalized federated KGE*, employing a client-wise relation graph to learn personalized supplementary knowledge, a departure from the global consensus approach. Paper B (CPa-WAC) refines *scalable KGE* by proposing a novel constellation partitioning-based method for GNNs, which maintains prediction accuracy while significantly reducing training time. Concurrently, Paper C (GE2) introduces a *general and efficient system architecture* for KGE training, focusing on optimizing CPU-GPU communication and providing a flexible API for negative sampling, representing a shift towards foundational system-level improvements.\n\n2.  **Knowledge Progression**:\n    This path addresses the pressing problems of deploying KGE in complex, real-world scenarios, building directly on the field's established need for scalability and efficiency. While Paper 2 tackled scalability through parallel training, Paper B (CPa-WAC) specifically addresses the *accuracy-scalability trade-off* for GNN-based KGE by introducing a partitioning strategy that preserves local topology, enabling large KGs to be processed efficiently without significant performance degradation. Paper A (PFedEG) tackles the emerging challenge of *semantic heterogeneity and privacy* in distributed KGE, a limitation of existing federated KGE methods that neglect client-specific semantic disparities. It introduces the new capability of learning personalized embeddings in a federated setting, moving beyond a \"one-size-fits-all\" global knowledge. Furthermore, Paper C (GE2) addresses the *fundamental system-level inefficiencies* and lack of generality in existing KGE training platforms, which were bottlenecks for rapid model development and deployment. It provides a new capability for faster, more flexible, and robust KGE training through optimized execution models and data management.\n\n3.  **Temporal Context**:\n    The publication of all three papers in 2024 signifies a *continued and intensified acceleration* in KGE research, immediately following the rapid advancements observed from 2020-2023. This concentrated period indicates a maturing field that is quickly moving to address the practical deployment challenges and refine existing solutions, demonstrating a rapid response to the growing demand for efficient, scalable, and privacy-aware KGE.\n\n4.  **Synthesis**:\n    This collection of works collectively narrates the evolution of KGE from a focus on core model development and evaluation to a mature field deeply engaged with the practicalities of real-world deployment. The unified narrative highlights a progression towards making KGE models more *scalable, efficient, and adaptable* to distributed and privacy-sensitive environments. The collective contribution is a set of advanced methodologies and system-level innovations that enable KGE to be applied more effectively in complex, large-scale, and privacy-constrained settings, thereby expanding its utility and impact across various domains.",
    "path": [
      "8c93f3cecf79bd9f8d021f589d095305e281dd2f",
      "1f20378d2820fdf1c1bb09ce22f739ab77b14e82",
      "5515fd5d14ac7b19806294119560a8c74f7fa4b2",
      "f2b924e69735fb7fd6fd95c6a032954480862029",
      "040fe47af8f4870bf681f34861c42b3ea46d76cf",
      "fda63b289d4c0c332f88975994114fb61b514ced",
      "c180564160d0788a82df203f9e5f61380d9846aa",
      "658702b2fa647ae7eaf1255058105da9eefe6f52",
      "acc855d74431537b98de5185e065e4eacbab7b26",
      "5b5b3face4be1cf131d0cb9c40ae5adcd0c16408",
      "6205f75cb6db1503c94386441ca68c63c9cbd456",
      "33a7b7abf006d22de24c1471e6f6c93842a497b6"
    ],
    "layer1_papers": [
      {
        "title": "Knowledge Graph Embedding for Link Prediction",
        "abstract": "Knowledge Graphs (KGs) have found many applications in industrial and in academic settings, which in turn, have motivated considerable research efforts towards large-scale information extraction from a variety of sources. Despite such efforts, it is well known that even the largest KGs suffer from incompleteness; Link Prediction (LP) techniques address this issue by identifying missing facts among entities already in the KG. Among the recent LP techniques, those based on KG embeddings have achieved very promising performance in some benchmarks. Despite the fast-growing literature on the subject, insufficient attention has been paid to the effect of the design choices in those methods. Moreover, the standard practice in this area is to report accuracy by aggregating over a large number of test facts in which some entities are vastly more represented than others; this allows LP methods to exhibit good results by just attending to structural properties that include such entities, while ignoring the remaining majority of the KG. This analysis provides a comprehensive comparison of embedding-based LP methods, extending the dimensions of analysis beyond what is commonly available in the literature. We experimentally compare the effectiveness and efficiency of 18 state-of-the-art methods, consider a rule-based baseline, and report detailed analysis over the most popular benchmarks in the literature.",
        "summary": "",
        "year": 2020
      }
    ],
    "layer2_papers": [
      {
        "title": "A Survey of Knowledge Graph Embedding and Their Applications",
        "abstract": "Knowledge Graph embedding provides a versatile technique for representing knowledge. These techniques can be used in a variety of applications such as completion of knowledge graph to predict missing information, recommender systems, question answering, query expansion, etc. The information embedded in Knowledge graph though being structured is challenging to consume in a real-world application. Knowledge graph embedding enables the real-world application to consume information to improve performance. Knowledge graph embedding is an active research area. Most of the embedding methods focus on structure-based information. Recent research has extended the boundary to include text-based information and image-based information in entity embedding. Efforts have been made to enhance the representation with context information. This paper introduces growth in the field of KG embedding from simple translation-based models to enrichment-based models. This paper includes the utility of the Knowledge graph in real-world applications.",
        "year": 2021
      },
      {
        "title": "Parallel Training of Knowledge Graph Embedding Models: A Comparison of Techniques",
        "abstract": "Knowledge graph embedding (KGE) models represent the entities and relations of a knowledge graph (KG) using dense continuous representations called embeddings. KGE methods have recently gained traction for tasks such as knowledge graph completion and reasoning as well as to provide suitable entity representations for downstream learning tasks. While a large part of the available literature focuses on small KGs, a number of frameworks that are able to train KGE models for large-scale KGs by parallelization across multiple GPUs or machines have recently been proposed. So far, the benefits and drawbacks of the various parallelization techniques have not been studied comprehensively. In this paper, we report on an experimental study in which we presented, re-implemented in a common computational framework, investigated, and improved the available techniques. We found that the evaluation methodologies used in prior work are often not comparable and can be misleading, and that most of currently implemented training methods tend to have a negative impact on embedding quality. We propose a simple but effective variation of the stratification technique used by PyTorch BigGraph for mitigation. Moreover, basic random partitioning can be an effective or even the best-performing choice when combined with suitable sampling techniques. Ultimately, we found that efficient and effective parallel training of large-scale KGE models is indeed achievable but requires a careful choice of techniques.",
        "year": 2021
      },
      {
        "title": "Knowledge Graph Embedding: An Overview",
        "abstract": "Many mathematical models have been leveraged to design embeddings for representing Knowledge Graph (KG) entities and relations for link prediction and many downstream tasks. These mathematically-inspired models are not only highly scalable for inference in large KGs, but also have many explainable advantages in modeling different relation patterns that can be validated through both formal proofs and empirical results. In this paper, we make a comprehensive overview of the current state of research in KG completion. In particular, we focus on two main branches of KG embedding (KGE) design: 1) distance-based methods and 2) semantic matching-based methods. We discover the connections between recently proposed models and present an underlying trend that might help researchers invent novel and more effective models. Next, we delve into CompoundE and CompoundE3D, which draw inspiration from 2D and 3D affine operations, respectively. They encompass a broad spectrum of techniques including distance-based and semantic-based methods. We will also discuss an emerging approach for KG completion which leverages pre-trained language models (PLMs) and textual descriptions of entities and relations and offer insights into the integration of KGE embedding methods with PLMs for KG completion.",
        "year": 2023
      },
      {
        "title": "Message Function Search for Knowledge Graph Embedding",
        "abstract": "Recently, many promising embedding models have been proposed to embed knowledge graphs (KGs) and their more general forms, such as n-ary relational data (NRD) and hyper-relational KG (HKG). To promote the data adaptability and performance of embedding models, KG searching methods propose to search for suitable models for a given KG data set. But they are restricted to a single KG form, and the searched models are restricted to a single type of embedding model. To tackle such issues, we propose to build a search space for the message function in graph neural networks (GNNs). However, it is a non-trivial task. Existing message function designs fix the structures and operators, which makes them difficult to handle different KG forms and data sets. Therefore, we first design a novel message function space, which enables both structures and operators to be searched for the given KG form (including KG, NRD, and HKG) and data. The proposed space can flexibly take different KG forms as inputs and is expressive to search for different types of embedding models. Especially, some existing message function designs and some classic KG embedding models can be instantiated as special cases of our space. We empirically show that the searched message functions are data-dependent, and can achieve leading performance on benchmark KGs, NRD, and HKGs.",
        "year": 2023
      },
      {
        "title": "Molecular-evaluated and explainable drug repurposing for COVID-19 using ensemble knowledge graph embedding",
        "abstract": "The search for an effective drug is still urgent for COVID-19 as no drug with proven clinical efficacy is available. Finding the new purpose of an approved or investigational drug, known as drug repurposing, has become increasingly popular in recent years. We propose here a new drug repurposing approach for COVID-19, based on knowledge graph (KG) embeddings. Our approach learns “ensemble embeddings” of entities and relations in a COVID-19 centric KG, in order to get a better latent representation of the graph elements. Ensemble KG-embeddings are subsequently used in a deep neural network trained for discovering potential drugs for COVID-19. Compared to related works, we retrieve more in-trial drugs among our top-ranked predictions, thus giving greater confidence in our prediction for out-of-trial drugs. For the first time to our knowledge, molecular docking is then used to evaluate the predictions obtained from drug repurposing using KG embedding. We show that Fosinopril is a potential ligand for the SARS-CoV-2 nsp13 target. We also provide explanations of our predictions thanks to rules extracted from the KG and instanciated by KG-derived explanatory paths. Molecular evaluation and explanatory paths bring reliability to our results and constitute new complementary and reusable methods for assessing KG-based drug repurposing.",
        "year": 2023
      },
      {
        "title": "Weighted Knowledge Graph Embedding",
        "abstract": "Knowledge graph embedding (KGE) aims to project both entities and relations in a knowledge graph (KG) into low-dimensional vectors. Indeed, existing KGs suffer from the data imbalance issue, i.e., entities and relations conform to a long-tail distribution, only a small portion of entities and relations occur frequently, while the vast majority of entities and relations only have a few training samples. Existing KGE methods assign equal weights to each entity and relation during the training process. Under this setting, long-tail entities and relations are not fully trained during training, leading to unreliable representations. In this paper, we propose WeightE, which attends differentially to different entities and relations. Specifically, WeightE is able to endow lower weights to frequent entities and relations, and higher weights to infrequent ones. In such manner, WeightE is capable of increasing the weights of long-tail entities and relations, and learning better representations for them. In particular, WeightE tailors bilevel optimization for the KGE task, where the inner level aims to learn reliable entity and relation embeddings, and the outer level attempts to assign appropriate weights for each entity and relation. Moreover, it is worth noting that our technique of applying weights to different entities and relations is general and flexible, which can be applied to a number of existing KGE models. Finally, we extensively validate the superiority of WeightE against various state-of-the-art baselines.",
        "year": 2023
      },
      {
        "title": "Assessing the effects of hyperparameters on knowledge graph embedding quality",
        "abstract": "Embedding knowledge graphs into low-dimensional spaces is a popular method for applying approaches, such as link prediction or node classification, to these databases. This embedding process is very costly in terms of both computational time and space. Part of the reason for this is the optimisation of hyperparameters, which involves repeatedly sampling, by random, guided, or brute-force selection, from a large hyperparameter space and testing the resulting embeddings for their quality. However, not all hyperparameters in this search space will be equally important. In fact, with prior knowledge of the relative importance of the hyperparameters, some could be eliminated from the search altogether without significantly impacting the overall quality of the outputted embeddings. To this end, we ran a Sobol sensitivity analysis to evaluate the effects of tuning different hyperparameters on the variance of embedding quality. This was achieved by performing thousands of embedding trials, each time measuring the quality of embeddings produced by different hyperparameter configurations. We regressed the embedding quality on those hyperparameter configurations, using this model to generate Sobol sensitivity indices for each of the hyperparameters. By evaluating the correlation between Sobol indices, we find substantial variability in the hyperparameter sensitivities between knowledge graphs with differing dataset characteristics as the probable cause of these inconsistencies. As an additional contribution of this work we identify several relations in the UMLS knowledge graph that may cause data leakage via inverse relations, and derive and present UMLS-43, a leakage-robust variant of that graph.",
        "year": 2022
      },
      {
        "title": "Bringing Light Into the Dark: A Large-Scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework",
        "abstract": "The heterogeneity in recently published knowledge graph embedding models’ implementations, training, and evaluation has made fair and thorough comparisons difficult. To assess the reproducibility of previously published results, we re-implemented and evaluated 21 models in the PyKEEN software package. In this paper, we outline which results could be reproduced with their reported hyper-parameters, which could only be reproduced with alternate hyper-parameters, and which could not be reproduced at all, as well as provide insight as to why this might be the case. We then performed a large-scale benchmarking on four datasets with several thousands of experiments and 24,804 GPU hours of computation time. We present insights gained as to best practices, best configurations for each model, and where improvements could be made over previously published best configurations. Our results highlight that the combination of model architecture, training approach, loss function, and the explicit modeling of inverse relations is crucial for a model’s performance and is not only determined by its architecture. We provide evidence that several architectures can obtain results competitive to the state of the art when configured carefully. We have made all code, experimental configurations, results, and analyses available at https://github.com/pykeen/pykeen and https://github.com/pykeen/benchmarking.",
        "year": 2020
      }
    ],
    "layer3_papers": [
      {
        "title": "Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph",
        "abstract": "Federated Knowledge Graph Embedding (FKGE) has recently garnered considerable interest due to its capacity to extract expressive representations from distributed knowledge graphs, while concurrently safeguarding the privacy of individual clients. Existing FKGE methods typically harness the arithmetic mean of entity embeddings from all clients as the global supplementary knowledge, and learn a replica of global consensus entities embeddings for each client. However, these methods usually neglect the inherent semantic disparities among distinct clients. This oversight not only results in the globally shared complementary knowledge being inundated with too much noise when tailored to a specific client, but also instigates a discrepancy between local and global optimization objectives. Consequently, the quality of the learned embeddings is compromised. To address this, we propose Personalized Federated knowledge graph Embedding with client-wise relation Graph (PFedEG), a novel approach that employs a client-wise relation graph to learn personalized embeddings by discerning the semantic relevance of embeddings from other clients. Specifically, PFedEG learns personalized supplementary knowledge for each client by amalgamating entity embedding from its neighboring clients based on their\"affinity\"on the client-wise relation graph. Each client then conducts personalized embedding learning based on its local triples and personalized supplementary knowledge. We conduct extensive experiments on four benchmark datasets to evaluate our method against state-of-the-art models and results demonstrate the superiority of our method.",
        "year": 2024
      },
      {
        "title": "CPa-WAC: Constellation Partitioning-based Scalable Weighted Aggregation Composition for Knowledge Graph Embedding",
        "abstract": "Scalability and training time are crucial for any graph neural network model processing a knowledge graph (KG). While partitioning knowledge graphs helps reduce the training time, the prediction accuracy reduces significantly compared to training the model on the whole graph. In this paper, we propose CPa-WAC: a lightweight architecture that incorporates graph convolutional networks and modularity maximization-based constellation partitioning to harness the power of local graph topology. The proposed CPa-WAC method reduces the training time and memory cost of knowledge graph embedding, making the learning model scalable. The results from our experiments on standard databases, such as Wordnet and Freebase, show that by achieving meaningful partitioning, any knowledge graph can be broken down into subgraphs and processed separately to learn embeddings. Furthermore, these learned embeddings can be used for knowledge graph completion, retaining similar performance compared to training a GCN on the whole KG, while speeding up the training process by upto five times. Additionally, the proposed CPa-WAC method outperforms several other state-of-the-art KG in terms of prediction accuracy.",
        "year": 2024
      },
      {
        "title": "GE2: A General and Efficient Knowledge Graph Embedding Learning System",
        "abstract": "Graph embedding learning computes an embedding vector for each node in a graph and finds many applications in areas such as social networks, e-commerce, and medicine. We observe that existing graph embedding systems (e.g., PBG, DGL-KE, and Marius) have long CPU time and high CPU-GPU communication overhead, especially when using multiple GPUs. Moreover, it is cumbersome to implement negative sampling algorithms on them, which have many variants and are crucial for model quality. We propose a new system called GE2, which achieves both generality and efficiency for graph embedding learning. In particular, we propose a general execution model that encompasses various negative sampling algorithms. Based on the execution model, we design a user-friendly API that allows users to easily express negative sampling algorithms. To support efficient training, we offload operations from CPU to GPU to enjoy high parallelism and reduce CPU time. We also design COVER, which, to our knowledge, is the first algorithm to manage data swap between CPU and multiple GPUs for small communication costs. Extensive experimental results show that, comparing with the state-of-the-art graph embedding systems, GE2 trains consistently faster across different models and datasets, where the speedup is usually over 2x and can be up to 7.5x.",
        "year": 2024
      }
    ],
    "layer2_summary": "This citation path illustrates a dynamic evolution in knowledge graph embedding (KGE) research, moving from foundational model development and evaluation to addressing practical challenges, enhancing model adaptability, and integrating with advanced AI for real-world applications.\n\n1.  **Methodological Evolution**:\n    The methodological evolution in KGE progresses from foundational structural models to sophisticated, adaptive, and context-aware approaches. Initially, KGE focused on simple translation-based and distance/semantic matching methods for representing entities and relations (Paper 1, 3). A significant shift occurred with the establishment of unified evaluation frameworks (Paper 8), which enabled rigorous comparison and reproducibility, highlighting the importance of training approaches and hyperparameter configurations (Paper 7). Subsequent innovations include parallelization techniques for large-scale KGs (Paper 2), the integration of pre-trained language models (PLMs) for richer, text-aware embeddings (Paper 3), and the development of search-based methods for automatically discovering optimal GNN message functions adaptable to various KG forms (Paper 4). More recently, methods like `WeightE` (Paper 6) address data imbalance through bilevel optimization, while ensemble KGE (Paper 5) demonstrates advanced application-specific model combinations.\n\n2.  **Knowledge Progression**:\n    The core problem addressed is effectively representing knowledge graph entities and relations in low-dimensional vectors for tasks like link prediction and downstream applications (Paper 1, 3). Early limitations included a lack of standardized evaluation, which Paper 8 addressed by providing a unified framework (PyKEEN) and large-scale benchmarking, revealing reproducibility issues and optimal configurations. This foundational work enabled subsequent advancements to tackle practical challenges: Paper 2 improved scalability for large KGs through parallel training, while Paper 7 investigated the costly and variable impact of hyperparameter tuning on embedding quality. To overcome model rigidity and data-specific performance, Paper 4 introduced message function search, allowing KGE models to adapt to diverse KG forms (NRD, HKG). The issue of data imbalance, where infrequent entities/relations receive unreliable representations, was tackled by Paper 6 with `WeightE`, which differentially weights training samples. Furthermore, Paper 3 highlighted the emerging capability of integrating KGE with PLMs for enhanced representations, leading to more context-rich embeddings. Finally, Paper 5 showcased a new capability: applying ensemble KGE for complex real-world problems like drug repurposing, integrating molecular evaluation and providing explainable paths, thus moving beyond basic link prediction to validated, interpretable applications.\n\n3.  **Temporal Context**:\n    The publication timing reveals a rapid acceleration in KGE research, particularly from 2020 to 2023. Paper 8 (2020) provided a critical early foundation by establishing a unified evaluation framework, which was essential for the field's subsequent rigorous development. The years 2021-2023 saw a surge in specialized advancements, with papers addressing scalability (Paper 2, 2021), hyperparameter optimization (Paper 7, 2022), and sophisticated model design (Paper 4, 6, 2023), alongside the integration of cutting-edge AI techniques like PLMs (Paper 3, 2023) and complex real-world applications (Paper 5, 2023). This concentrated period indicates a maturing field building rapidly on established evaluation practices.\n\n4.  **Synthesis**:\n    This collection of works collectively narrates the evolution of knowledge graph embedding from a nascent field focused on structural representation to a mature, robust, and highly applicable research area. The unified narrative highlights a progression from foundational model development and the critical need for standardized evaluation (Paper 8) to addressing practical challenges like scalability (Paper 2) and optimization (Paper 7). The field then advanced to developing more adaptive and robust embedding techniques (Paper 4, 6) and integrating with external knowledge sources like PLMs (Paper 3). The collective contribution is a sophisticated framework for leveraging structured knowledge, enabling not only improved predictive performance but also explainable and validated solutions for complex real-world problems, as exemplified by drug repurposing (Paper 5)."
  },
  "11e402c699bcb54d57da1a5fdbc57076d7255baf": {
    "seed_title": "Multi-view Knowledge Graph Embedding for Entity Alignment",
    "summary": "This analysis focuses on the single paper provided under \"CITATION PATH 1 papers,\" a survey published in 2024. While the prompt typically implies a sequence of papers to analyze evolution, this analysis will interpret the provided survey paper as a meta-level contribution that *describes* and *synthesizes* the evolution of knowledge graph embedding (KGE) research specifically for entity alignment (EA). It consolidates past advancements and charts future directions, thereby reflecting the field's progression.\n\n1.  **Methodological Evolution:**\n    The 2024 survey paper, \"A survey: knowledge graph entity alignment research based on graph embedding,\" highlights a significant methodological shift in Entity Alignment (EA) from \"traditional EA methods\" to approaches based on \"representation learning,\" particularly \"graph embedding.\" This marks an evolution from potentially rule-based or statistical methods to data-driven, distributed representations. Furthermore, the survey proposes a novel EA framework comprising \"information aggregation,\" \"entity alignment,\" and \"post-alignment modules\" to categorize and analyze the *latest models* within this representation learning paradigm, indicating an ongoing refinement and structuring of these advanced methods.\n\n2.  **Knowledge Progression:**\n    The core problem addressed is Entity Alignment (EA) across different knowledge graphs, crucial for \"knowledge-driven applications.\" This paper implicitly builds upon the limitations of earlier, \"traditional EA methods\" by emphasizing the \"better performance and efficiency\" offered by representation learning-based approaches. It then progresses knowledge by providing a comprehensive summary and analysis of these advanced methods, classifying them by \"alignment inference strategy,\" \"noise filtering strategy,\" and \"utilization of additional information.\" New insights emerge from its \"comparative analysis of the performance of the models within the categories\" on various datasets, including both \"unimodal and multimodal EA,\" offering a structured understanding of their strengths and weaknesses. The paper further advances knowledge by identifying \"shortcomings of existing EA methods\" and presenting \"future research perspectives.\"\n\n3.  **Temporal Context:**\n    The publication of this comprehensive survey in 2024 signifies a mature phase in the research area of KGE-based EA. It indicates that sufficient advancements have accumulated over preceding years to warrant a systematic review, categorization, and comparative analysis. The focus on \"latest models\" and \"future research perspectives\" suggests that while the field has achieved significant progress, it remains highly active and dynamic, with ongoing theoretical and technological developments driving continuous innovation.\n\n4.  **Synthesis:**\n    This 2024 survey paper serves as a crucial meta-contribution to the field of \"knowledge graph embedding\" for entity alignment. It synthesizes the journey from foundational KGE concepts to their specialized application in EA, systematically organizing and evaluating the diverse methods that have emerged. Its collective contribution is to provide a structured understanding of the current landscape, identify key performance drivers, and articulate future research challenges, thereby guiding both new researchers and seasoned practitioners in navigating and advancing this complex and vital area of knowledge representation.",
    "path": [
      "11e402c699bcb54d57da1a5fdbc57076d7255baf",
      "84aa127dc5ca3080385439cb10edc50b5d2c04e4",
      "af051c87cecca64c2de4ad9110608f7579766653",
      "f470e11faa6200026cf39e248510070c078e509a",
      "52b167a90a10cde25309e40d7f6e6b5e14ec3261"
    ],
    "layer1_papers": [
      {
        "title": "Multi-view Knowledge Graph Embedding for Entity Alignment",
        "abstract": "We study the problem of embedding-based entity alignment between knowledge graphs (KGs). Previous works mainly focus on the relational structure of entities. Some further incorporate another type of features, such as attributes, for refinement. However, a vast of entity features are still unexplored or not equally treated together, which impairs the accuracy and robustness of embedding-based entity alignment. In this paper, we propose a novel framework that unifies multiple views of entities to learn embeddings for entity alignment. Specifically, we embed entities based on the views of entity names, relations and attributes, with several combination strategies. Furthermore, we design some cross-KG inference methods to enhance the alignment between two KGs. Our experiments on real-world datasets show that the proposed framework significantly outperforms the state-of-the-art embedding-based entity alignment methods. The selected views, cross-KG inference and combination strategies all contribute to the performance improvement.",
        "summary": "",
        "year": 2019
      }
    ],
    "layer2_papers": [
      {
        "title": "Knowledge graph embedding methods for entity alignment: experimental review",
        "abstract": "In recent years, we have witnessed the proliferation of knowledge graphs (KG) in various domains, aiming to support applications like question answering, recommendations, etc. A frequent task when integrating knowledge from different KGs is to find which subgraphs refer to the same real-world entity, a task largely known as the Entity Alignment. Recently, embedding methods have been used for entity alignment tasks, that learn a vector-space representation of entities which preserves their similarity in the original KGs. A wide variety of supervised, unsupervised, and semi-supervised methods have been proposed that exploit both factual (attribute based) and structural information (relation based) of entities in the KGs. Still, a quantitative assessment of their strengths and weaknesses in real-world KGs according to different performance metrics and KG characteristics is missing from the literature. In this work, we conduct the first meta-level analysis of popular embedding methods for entity alignment, based on a statistically sound methodology. Our analysis reveals statistically significant correlations of different embedding methods with various meta-features extracted by KGs and rank them in a statistically significant way according to their effectiveness across all real-world KGs of our testbed. Finally, we study interesting trade-offs in terms of methods’ effectiveness and efficiency.",
        "year": 2022
      },
      {
        "title": "OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding",
        "abstract": "Semantic embedding has been widely investigated for aligning knowledge graph (KG) entities. Current methods have explored and utilized the graph structure, the entity names and attributes, but ignore the ontology (or ontological schema) which contains critical meta information such as classes and their membership relationships with entities. In this paper, we propose an ontology-guided entity alignment method named OntoEA, where both KGs and their ontologies are jointly embedded, and the class hierarchy and the class disjointness are utilized to avoid false mappings. Extensive experiments on seven public and industrial benchmarks have demonstrated the state-of-the-art performance of OntoEA and the effectiveness of the ontologies.",
        "year": 2021
      },
      {
        "title": "A Survey on Knowledge Graph Embedding",
        "abstract": "Knowledge graph (KG) is used to represent the relationships between different concepts in the real world. It is a special network in which nodes represent entities and edges represent relationships. KGs can intuitively model the connections between facts, but in many applications, there are certain limitations in directly using symbolic logic to represent knowledge in KGs and perform calculations, making it difficult to achieve expected results in downstream tasks. Meanwhile, with the explosive growth of Internet capacity, the traditional KG structure faces the problems of computational inefficiency and management difficulties. To alleviate these problems, Knowledge graph embedding (KGE) is proposed to improve the computational efficiency by embedding entities and relations in the KG into a low-dimensional, dense and continuous vector space, and thus the solution of some problems in the knowledge graph is transformed into vector operations. Moreover, KGE also can be used as a pre-trained model which is more beneficial to downstream applications, such as applications based on deep learning. In this paper, we classify KGE into three categories, namely translational distance models, semantic matching models and neural network based models. The advantages and disadvantages of different embedding methods are compared, while the main applications of KGE are summarized. Some current challenges of KGE are summarized, as well as some views on the future research directions of KGE.",
        "year": 2022
      }
    ],
    "layer3_papers": [
      {
        "title": "A survey: knowledge graph entity alignment research based on graph embedding",
        "abstract": "Entity alignment (EA) aims to automatically match entities in different knowledge graphs, which is beneficial to the development of knowledge-driven applications. Representation learning has powerful feature capture capability and it is widely used in the field of natural language processing. Compared with traditional EA methods, EA methods based on representation learning have better performance and efficiency. Hence, we summarize and analyze the representative EA approaches based on representation learning in this paper. We present the problem description and data preprocessing for EA and other related fundamental knowledge. We propose a new EA framework for the latest models, which includes information aggregation module, entity alignment module, and post-alignment module. Based on these three modules, the various technologies are described in detail. In the experimental part, we first explore the effect of EA direction on model performance. Then, we classify the models into different categories in terms of alignment inference strategy, noise filtering strategy, and whether additional information is utilized. To ensure fairness, we perform the comparative analysis of the performance of the models within the categories separately on different datasets. We investigate both unimodal and multimodal EA. Finally, we present future research perspectives based on the shortcomings of existing EA methods.",
        "year": 2024
      }
    ],
    "layer2_summary": "This analysis is based on the three papers provided, despite the prompt indicating \"4 papers.\" For a coherent analysis of how research \"builds upon previous work,\" these papers are interpreted in a logical conceptual progression: a foundational survey, followed by a specific method, and then a meta-review of that application area.\n\n1.  **Methodological Evolution:**\n    The methodological evolution begins with Paper 3 (2022), which surveys foundational Knowledge Graph Embedding (KGE) methods, categorizing them into translational distance, semantic matching, and neural network-based models. Paper 2 (2021), OntoEA, introduces a significant innovation by moving beyond traditional graph structure and attributes to jointly embed KGs and their *ontologies*, leveraging class hierarchy and disjointness constraints. This represents a shift towards incorporating richer semantic meta-information. Finally, Paper 1 (2022) marks a methodological shift from proposing new methods to conducting a meta-level, statistically sound experimental review of diverse KGE methods specifically for entity alignment, correlating their performance with KG characteristics.\n\n2.  **Knowledge Progression:**\n    Paper 3 addresses the fundamental problem of representing symbolic KGs in a low-dimensional vector space to overcome computational inefficiencies and enable downstream tasks. Building upon this, Paper 2 tackles the specific downstream problem of Entity Alignment (EA), addressing the limitation that prior KGE-based EA methods largely ignored critical ontological schema information. It introduces a novel approach to integrate this meta-information, leading to more accurate alignments. Paper 1 then addresses the gap of a missing quantitative assessment for the proliferation of KGE methods in EA, providing a comprehensive meta-analysis to understand their strengths, weaknesses, and trade-offs across various real-world KGs, thereby offering new insights into method selection and performance drivers.\n\n3.  **Temporal Context:**\n    The close publication years of these papers (Paper 2 in 2021, Papers 1 and 3 in 2022) highlight a period of rapid and concurrent advancement in knowledge graph embedding research. This temporal proximity suggests that while new methods like OntoEA (Paper 2) were being developed, the field was simultaneously maturing enough for comprehensive surveys (Paper 3) and application-specific meta-analyses (Paper 1) to be conducted, reflecting a dynamic and fast-evolving research landscape.\n\n4.  **Synthesis:**\n    These works collectively narrate the journey of \"knowledge graph embedding\" from foundational theoretical models to their specialized application and rigorous empirical validation. The path moves from establishing the core principles of KGE (Paper 3), to innovating a specific application (Entity Alignment) by incorporating richer semantic information (Paper 2), and finally to a systematic, data-driven evaluation of these application-specific methods (Paper 1). Their collective contribution underscores the increasing sophistication in KGE, the importance of leveraging diverse knowledge sources, and the growing demand for robust, empirically validated solutions in real-world KG applications."
  }
}