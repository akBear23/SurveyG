File: paper_data/knowledge_graph_embedding/18101998fb57704b79eb4c4c37891144ede8f8b9.pdf
Created: 2025-10-04T23:49:41.604441
Keywords: 
==================================================
INTRIGUING ABSTRACT:
==================================================
Knowledge graphs contain rich relational structures of the world, and thus complement data-driven knowledge discovery from heterogeneous data. Relational inference between distant entities in large-scale knowledge graphs demands fast relation-specific algebraic manipulations. One of the most effective methods is to embed symbolic relations and entities into continuous spaces, where relations are approximately linear translation between projected images of entities in the relation space. However, state-of-art relation projection methods such as TransR, TransD or TransSparse do not model the correlation between relations, and thus are not scalable to complex knowledge graphs with thousands of relations, both in term of computational demand and statistical robustness. To this end we introduce TransF, a novel translation-based method which mitigates the burden of relation projection by explicitly modeling the basis subspaces of projection matrices. As a result, TransF is far more light weight than the existing projection methods, and is robust when facing a high number of relations. Experimental results on canonical link prediction and triples classification tasks show that our proposed model outperforms competing rivals by a large margin and achieves state-of-the-art performance. Especially, TransF improves by 9% (5%) on the head/tail entity prediction task with N-to-l (l-to-N) over the best performing translation-based method.

==================================================
FULL SUMMARY:
==================================================
Knowledge graphs contain rich relational structures of the world, and thus complement data-driven knowledge discovery from heterogeneous data. Relational inference between distant entities in large-scale knowledge graphs demands fast relation-specific algebraic manipulations. One of the most effective methods is to embed symbolic relations and entities into continuous spaces, where relations are approximately linear translation between projected images of entities in the relation space. However, state-of-art relation projection methods such as TransR, TransD or TransSparse do not model the correlation between relations, and thus are not scalable to complex knowledge graphs with thousands of relations, both in term of computational demand and statistical robustness. To this end we introduce TransF, a novel translation-based method which mitigates the burden of relation projection by explicitly modeling the basis subspaces of projection matrices. As a result, TransF is far more light weight than the existing projection methods, and is robust when facing a high number of relations. Experimental results on canonical link prediction and triples classification tasks show that our proposed model outperforms competing rivals by a large margin and achieves state-of-the-art performance. Especially, TransF improves by 9% (5%) on the head/tail entity prediction task with N-to-l (l-to-N) over the best performing translation-based method.