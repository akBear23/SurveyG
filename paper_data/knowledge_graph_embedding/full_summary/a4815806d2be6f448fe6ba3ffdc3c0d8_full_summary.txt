File: paper_data/knowledge_graph_embedding/ba691084e8088787bca7ae7bc5d963041e1aa03b.pdf
Created: 2025-10-01T23:14:35.556855
Keywords: Selective pre-translation strategies, Multilingual Large Language Models (LLMs), Prompt components, Language resource levels, Experimental design, Low-resource language performance gains, Full pre-translation, Direct inference, Extractive tasks, Translation quality impact, Association Rule Learning (ARL), Optimal prompt translation strategies, Practical guidelines
==================================================
INTRIGUING ABSTRACT:
==================================================
Unlocking the full potential of multilingual Large Language Models (LLMs) in diverse linguistic landscapes remains a critical challenge. This study systematically dismantles conventional wisdom, revealing that neither full pre-translation nor direct inference is optimal. We introduce and empirically validate **selective pre-translation**, a novel prompt engineering strategy that strategically translates only specific prompt components (instruction, context, examples, output). Across 35 languages (including high and low-resource), four diverse tasks (QA, NLI, NER, Summarization), and four leading LLMs (GPT-3.5-turbo, Mistral-8x7B, Gemini-1.0-pro, bloomz-7b1-mt), selective pre-translation consistently outperformed full pre-translation (92% of languages) and direct inference (90%). Crucially, low-resource languages experienced dramatic gains, averaging 65% greater improvement than high-resource languages, with some seeing over 200% performance boosts. Our findings, supported by Association Rule Learning, demonstrate that optimal strategies are highly nuanced, varying significantly with prompt component, language resource level, and task type, effectively mitigating the negative impacts of suboptimal translation quality. This work provides essential, data-driven guidelines for robust multilingual LLM deployment, fundamentally reshaping how we approach cross-lingual prompt design.

==================================================
FULL SUMMARY:
==================================================
Here's a focused summary of the empirical study for a literature review:

1.  **Research Questions & Hypotheses**
    *   The study investigated the optimal setup for selective pre-translation strategies in multilingual Large Language Models (LLMs) across diverse languages and tasks.
    *   It hypothesized that selective pre-translation would outperform full pre-translation and direct inference, and that optimal strategies would vary based on prompt component, language resource level, and task type.

2.  **Study Design & Methodology**
    *   An experimental design systematically assessed 24 distinct prompt configurations by varying the language (English or source) of four prompt components: instruction, context, examples, and output.
    *   Evaluations were conducted across 35 languages, four tasks (QA, NLI, NER, Summarization), six datasets, and three generative LLMs (GPT-3.5-turbo, Mistral-8x7B, Gemini-1.0-pro) plus one multilingual LLM (bloomz-7b1-mt).

3.  **Data & Participants**
    *   The study utilized 35 languages, categorized into high, medium, low, and unrepresented resource levels based on GPT-3 training data distribution.
    *   Datasets included XNLI, XQuAD, IndicQA, WikiANN, MasakhaNER, and XL-Sum, with a sample of 250 examples per language from test or validation sets.

4.  **Key Empirical Findings**
    *   Selective pre-translation consistently outperformed both full pre-translation (92% of languages) and direct inference (90% of languages) across tasks and languages \cite{mondshine2025}.
    *   Low-resource languages showed substantial performance gains (e.g., >200% for Malayalam and Telugu) with selective pre-translation, with an average improvement 65% greater than high-resource languages over complete pre-translation.
    *   For extractive tasks (QA, NER), models were often agnostic to context language in high-resource settings but preferred source language context in low-resource settings.
    *   Translation quality significantly impacted model performance, and selective pre-translation effectively mitigated negative effects of suboptimal translations, particularly for lower-resourced languages.

5.  **Statistical Analysis**
    *   Empirical results were analyzed using correlation analysis (Point-biserial correlation, Ï„) to assess relationships between individual component language selection and model performance.
    *   Association Rule Learning (ARL) with the Apriori algorithm was employed to identify optimal combined translation decisions and non-linear relationships.
    *   Performance gaps were calculated as the average difference between configuration pairs differing in only one component to quantify impact. Significance levels were indicated by *p < 0.05 and **p < 0.01.

6.  **Validity & Limitations**
    *   The study used GPT-3's training data distribution as a proxy for language resource levels due to the lack of public data for other LLMs, which might not perfectly reflect the actual training data of the models used.
    *   The reliance on the Google Translate API for all translations could introduce a consistent bias or limitation in translation quality, especially for low-resource languages.

7.  **Empirical Contribution**
    *   This work systematically establishes the efficacy of selective pre-translation strategies, providing empirical evidence that modular prompt translation significantly enhances multilingual LLM performance.
    *   It offers practical guidelines for choosing optimal prompt translation strategies based on language resource levels and task types, advancing robust LLM use in diverse multilingual settings.