\section*{Inherent Limitations and Unresolved Tensions}

Despite significant advancements in knowledge graph embedding (KGE) paradigms, the field continues to grapple with inherent limitations and persistent unresolved tensions that hinder the realization of truly intelligent and robust knowledge systems. These challenges primarily revolve around the difficulty in handling open-world assumptions, the limitations in performing complex logical reasoning, the persistent gap between expressiveness and interpretability, and the trade-offs between model complexity, computational efficiency, and the ability to capture highly diverse and uncertain relational patterns.

A fundamental tension in KGE lies in its implicit reliance on a closed-world assumption, where unobserved facts are often treated as false during training, particularly through negative sampling strategies. While early translational models like TransH \cite{wang2014} and TransD \cite{ji2015} significantly improved the modeling of complex relation types (e.g., 1-N, N-1, N-N) by introducing relation-specific hyperplanes or dynamic mapping matrices, their core mechanisms for link prediction still operate within a binary truth framework. This contrasts sharply with real-world knowledge graphs (KGs), which are inherently incomplete and where an unobserved fact is merely unknown, not necessarily false. The challenge of performing complex, multi-hop logical reasoning, a hallmark of symbolic AI, also remains largely unresolved by current KGE methods. Even models designed to capture compositional patterns, such as RotatE (a special case of HolmE \cite{zheng2024}), which formalizes the property of being "closed under composition" and provides a unifying framework for several geometric models, primarily focus on learning statistical patterns of composition rather than enabling explicit, arbitrary logical deductions over multiple hops. Similarly, neural architectures like the Multi-Scale Dynamic Convolutional Network (M-DCN) \cite{zhang2020} or ReInceptionE \cite{xie2020}, which leverage advanced CNNs and attention mechanisms to extract richer, multi-hop contextual information, excel at statistical pattern matching for link prediction but struggle with the explicit, rule-based inference capabilities of symbolic systems. While efforts to model uncertainty and fuzziness, as seen in the quaternion embedding approach for fuzzy spatiotemporal RDF KGs \cite{ji2024}, move beyond binary truth values, their "multihop query" capabilities are more akin to pathfinding in an uncertain context rather than complex logical reasoning. Even the integration of hyper-relational facts by HINGE \cite{rosso2020}, which captures richer information beyond simple triplets, does not inherently equip models with robust logical inference capabilities.

Another persistent tension lies in the trade-off between the expressiveness of KGE models and their interpretability. Highly expressive geometric models, such as CompoundE \cite{ge2022}, which combines translation, rotation, and scaling operations, or HousE \cite{li2022}, which uses Householder parameterization for high-dimensional rotations and invertible projections to model all relation patterns, achieve superior performance but often at the cost of transparency. Their intricate mathematical operations make it challenging to directly interpret *why* a specific prediction is made or to understand the semantic meaning of individual embedding dimensions. Similarly, advanced neural architectures like TGformer \cite{shi2025}, a Graph Transformer framework designed to capture multi-structural and contextual features, or SEConv \cite{yang2025}, which uses self-attention and multilayer CNNs for deeper structural features in healthcare KGs, often operate as "black boxes." While some models attempt to bridge this gap, their interpretability is often limited. SpherE \cite{li2024}, which embeds entities as spheres with an interpretable radius reflecting entity "universality" and is designed for set retrieval, offers a step towards more intuitive embeddings. Similarly, Contextualized Knowledge Graph Embedding (CKGE) \cite{yang2023} for explainable recommendations uses meta-graphs and local path mask prediction to provide post-hoc explanations, but these are often derived features rather than inherent properties of the embedding space itself.

The pursuit of increased expressiveness also frequently leads to greater model complexity and computational cost, creating a significant tension with efficiency and scalability. For large-scale KGs, this becomes a critical bottleneck. While models like TransMS \cite{yang2019} aim for parameter efficiency by modeling multidirectional semantics, many state-of-the-art neural and geometric models are computationally intensive. To address this, approaches like DualDE \cite{zhu2020} employ knowledge distillation to achieve faster and cheaper reasoning, and LightKG \cite{wang2021} utilizes codebooks for efficient storage and inference. However, these efficiency gains often come with inherent approximations or potential sacrifices in expressiveness. For large-scale Graph Neural Network (GNN)-based KGEs, CPa-WAC \cite{modak2024} introduces constellation partitioning to enable scalability, but managing partitioned embeddings for global inference remains a complex challenge. Furthermore, capturing highly diverse, uncertain, and dynamic relational patterns adds another layer of complexity. Hyperbolic embedding models, such as the Hyperbolic Hierarchy-Aware KGE \cite{pan2021} and Fully Hyperbolic Rotation (FHRE) \cite{liang2024}, are adept at modeling hierarchical structures due to their negative curvature. FHRE \cite{liang2024} specifically aims to operate entirely within hyperbolic space to reduce mapping overhead. However, specialized geometric spaces may not universally capture all types of relational patterns, leading to a trade-off between tailored expressiveness and general applicability. The challenge of adapting model architectures to specific KG properties, as highlighted by AutoSF \cite{zhang2019} which automatically searches for optimal scoring functions, underscores the difficulty in designing a single, universally effective model for diverse and evolving KGs. Moreover, handling dynamic and evolving KGs, addressed by MetaHG \cite{sun2024} (meta-learning for incremental updates) and MorsE \cite{chen2021} (meta-knowledge transfer for inductive KGE), introduces complexities in maintaining consistency and generalization over time.

In conclusion, current KGE paradigms face fundamental limitations in moving beyond closed-world assumptions, performing robust multi-hop logical reasoning, and reconciling expressiveness with interpretability. The inherent trade-offs between model complexity, computational efficiency, and the ability to capture highly diverse, uncertain, and dynamic relational patterns necessitate fundamental breakthroughs. Future research needs to explore hybrid symbolic-neural approaches that combine the strengths of both paradigms, develop inherently interpretable embedding spaces, and design adaptive, resource-efficient models that can truly operate under open-world assumptions while performing complex logical inference.