\section*{2. Foundational Concepts and Early Geometric Models}
The advent of Knowledge Graph Embedding (KGE) marked a pivotal shift in how knowledge graphs (KGs) are processed and leveraged in artificial intelligence. Moving beyond the limitations of purely symbolic representations, KGE transforms entities and relations into continuous, low-dimensional vector spaces, known as embeddings \cite{dai2020, cao2022}. This paradigm shift addresses the inherent challenges of symbolic methods, such as computational inefficiency, difficulties in handling KG incompleteness, and the inability to seamlessly integrate with statistical machine learning models \cite{ge2023}. The core idea is to learn vector representations such that the structural and semantic properties of the KG are preserved, allowing for efficient computation of relationships and inference of new facts. This section establishes these foundational concepts, delving into the pioneering translational models that laid the groundwork for KGE, specifically TransE, and its early extensions like TransH and TransD. These models introduced crucial geometric innovations, such as hyperplanes and dynamic mapping matrices, to enhance the modeling of diverse relational patterns. While significantly improving expressiveness and scalability over their symbolic predecessors, these early geometric approaches also revealed inherent limitations in capturing highly complex or compositional relations, thereby setting the stage for subsequent architectural innovations in the field. This initial phase of KGE research initiated a "methodological evolution" from rigid symbolic logic to flexible, dense vector representations, fundamentally changing how knowledge is understood and processed \cite{yan2022}.

\subsection*{2.1. Basic Principles of Knowledge Graph Embedding}
At its heart, Knowledge Graph Embedding (KGE) is predicated on the principle of representing the discrete, symbolic elements of a knowledge graph—entities and relations—as continuous vectors within a low-dimensional embedding space \cite{dai2020, cao2022}. This transformation is not merely a data conversion; it is a strategic move to imbue symbolic knowledge with statistical properties, enabling machine learning models to reason over complex relational data more effectively. The primary motivation for KGE stems from the inherent drawbacks of traditional symbolic methods, which struggle with the vast scale and inherent incompleteness of real-world KGs. Symbolic systems often face computational bottlenecks when performing inference over millions or billions of facts, and their binary true/false logic is ill-equipped to handle the nuanced uncertainties and missing information prevalent in KGs. Furthermore, the discrete nature of symbolic representations makes them incompatible with the continuous vector inputs required by most modern deep learning architectures, hindering their integration into advanced AI systems \cite{ge2023}.

KGE addresses these issues by learning embeddings such that a scoring function, typically a distance-based or similarity-based measure, can quantify the plausibility of a given triple $(h, r, t)$, where $h$ is the head entity, $r$ is the relation, and $t$ is the tail entity. A valid triple should yield a high score (or low distance), while an invalid one should yield a low score (or high distance). This approach allows for efficient, approximate reasoning through vector arithmetic, replacing computationally intensive logical inference with simple algebraic operations in the embedding space \cite{rossi2020}. The continuous nature of these embeddings also provides a natural mechanism for handling KG incompleteness; by learning generalized patterns from observed facts, KGE models can predict missing links, a critical task known as link prediction \cite{rossi2020}. This "knowledge progression" from discrete symbols to dense vectors has been instrumental in making KGs amenable to scalable processing and integration into a wide array of AI applications, from recommendation systems \cite{yang2023} to entity alignment \cite{sun2018}. The quality of these embeddings, however, is highly dependent on various factors, including the choice of scoring function, optimization strategy, and crucial training mechanisms like negative sampling \cite{madushanka2024, qian2021}, whose impact on embedding quality can be substantial and dataset-dependent \cite{lloyd2022}.

\subsection*{2.2. Translational Models and Their Extensions}
The early development of KGE was significantly shaped by translational models, which conceptualized relations as translation operations between entity embeddings in a continuous vector space. The pioneering work in this area was \textbf{TransE} (Translating Embeddings), which posited that if a triple $(h, r, t)$ is true, then the embedding of the head entity $h$ plus the embedding of the relation $r$ should be approximately equal to the embedding of the tail entity $t$, i.e., $h + r \approx t$ \cite{jia2015}. This simple yet powerful geometric intuition allowed for a straightforward scoring function, typically based on L1 or L2 distance, to measure the plausibility of a triple. TransE's strengths lay in its remarkable simplicity, computational efficiency, and scalability, making it a foundational model for large-scale KGs. However, its core geometric assumption—that relations are simple translations—inherently limited its expressiveness. Specifically, TransE struggled with complex relation patterns such as one-to-many, many-to-one, and many-to-many relationships. For instance, if an entity $h$ has multiple tail entities $t_1, t_2$ for the same relation $r$ (e.g., `(person, bornIn, city1)` and `(person, bornIn, city2)`), TransE would attempt to map $h+r$ to both $t_1$ and $t_2$ simultaneously, forcing $t_1$ and $t_2$ to be close in the embedding space, which is often semantically incorrect. This "knowledge progression" bottleneck necessitated further innovation.

To address TransE's limitations, \textbf{TransH} (Translating on Hyperplanes) was introduced \cite{wang2014}. TransH refined the translational assumption by modeling each relation $r$ as a hyperplane, along with a translation vector specific to that relation. Instead of directly translating entity embeddings, TransH projects the head and tail entity embeddings ($h, t$) onto the relation-specific hyperplane, resulting in projected embeddings $h_{\perp}$ and $t_{\perp}$. The translational rule then becomes $h_{\perp} + r \approx t_{\perp}$ \cite{wang2014}. This mechanism allows an entity to have different representations (projections) for different relations, effectively mitigating the issues with one-to-many and many-to-one relations. For example, `(Obama, bornIn, Hawaii)` and `(Obama, presidentOf, USA)` can be modeled without forcing `Hawaii` and `USA` to be close, as Obama's embedding is projected differently for `bornIn` and `presidentOf`. While TransH significantly improved expressiveness, its reliance on a single relation vector for translation on the hyperplane could still be insufficient for highly diverse or nuanced relations.

Building upon these ideas, \textbf{TransD} (Knowledge Graph Embedding via Dynamic Mapping Matrix) further enhanced the modeling capacity \cite{ji2015}. TransD introduced a more fine-grained approach by representing each entity and relation with *two* vectors: one for its meaning and another for constructing a dynamic mapping matrix. This dynamic matrix is then used to project entities into a relation-specific space. Unlike TransH, which uses a fixed hyperplane and a single translation vector, TransD's dynamic mapping matrices allow for more flexible and adaptive transformations, considering the diversity of *both* entities and relations. This innovation addressed some of the rigidities of earlier models, leading to improved performance in tasks like link prediction and triplet classification \cite{ji2015}. However, despite these advancements, TransD, like its predecessors, still operated within a fundamentally translational framework. While more expressive, it still faced challenges in capturing highly complex, compositional, or hierarchical relations that require more than simple vector additions or projections, setting the stage for further architectural innovations beyond purely geometric transformations.

\subsection*{2.3. Early Geometric Innovations: Hyperplanes and Manifolds}
The early geometric innovations in KGE, particularly with TransH and TransD, represented a crucial "methodological evolution" beyond the simplistic vector translation of TransE, aiming to capture more intricate relational patterns. \textbf{Hyperplanes}, as introduced by TransH \cite{wang2014}, were a significant step. The core idea was to allow entities to manifest different "aspects" or "roles" depending on the specific relation they participate in. Instead of a single, static entity embedding, TransH projects entity embeddings onto a relation-specific hyperplane. This geometric transformation $h \to h_{\perp}$ and $t \to t_{\perp}$ enables the model to handle one-to-many and many-to-one relations more effectively. For instance, an entity "Michael Jordan" can be projected differently when associated with "playsFor" (e.g., Chicago Bulls) versus "hasNationality" (e.g., USA), preventing the model from erroneously forcing "Chicago Bulls" and "USA" to be semantically close. This mechanism directly addressed TransE's limitation where a single entity vector struggled to satisfy multiple, potentially conflicting, relational facts. The hyperplane essentially serves as a context-dependent subspace, allowing for more flexible entity representations without drastically increasing parameter count.

Further enhancing this concept, \textbf{dynamic mapping matrices} were a key innovation in TransD \cite{ji2015}. While TransH used a fixed hyperplane and a single translation vector for each relation, TransD recognized that both entities and relations exhibit diverse characteristics. To capture this, TransD assigns each entity and relation two vectors: one for its meaning and another for constructing a dynamic projection matrix. This matrix is then used to project entities into a relation-specific space, allowing for more adaptive and fine-grained transformations than a fixed hyperplane. This dynamic projection mechanism enables TransD to consider the diversity of *both* entities and relations, leading to a more expressive model with fewer parameters than some contemporary alternatives like TransR/CTransR, which used dense, relation-specific projection matrices \cite{ji2015}.

Despite these clever geometric innovations, these early models shared fundamental limitations rooted in their reliance on Euclidean space and simple geometric operations. They struggled to capture highly complex, compositional, or hierarchical relations (e.g., `parentOf` and `grandparentOf`, or logical inferences like `A is a part of B` and `B is a part of C` implies `A is a part of C`). The geometric constraints of translation and projection inherently limit their capacity to model intricate logical patterns or infer complex rules. For instance, relations like "is-a" or "part-of" often imply hierarchical structures that are poorly represented by simple vector additions or projections in flat Euclidean space. This limitation fueled an "arms race" for expressiveness, pushing researchers towards more sophisticated architectures. Furthermore, the performance of these models was, and still is, heavily influenced by training methodologies, particularly the generation of high-quality negative samples \cite{madushanka2024, shan2018}, and the careful tuning of hyperparameters \cite{lloyd2022}. These inherent geometric and training-related limitations of early translational models paved the way for the exploration of non-Euclidean spaces (e.g., hyperbolic embeddings) and neural network-based architectures, which promised greater capacity to model the multifaceted and complex nature of knowledge graphs.