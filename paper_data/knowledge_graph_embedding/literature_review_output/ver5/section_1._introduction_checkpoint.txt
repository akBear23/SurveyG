\section*{1. Introduction}
Knowledge Graphs (KGs) have emerged as a foundational paradigm for representing structured information, offering a rich, interconnected view of entities and their relationships within various domains \cite{dai2020, ge2023, yan2022}. By organizing data into a network of nodes (entities) and edges (relations), KGs provide a human-interpretable and machine-readable framework that underpins a vast array of Artificial Intelligence (AI) applications, from semantic search and question answering \cite{huang2019, zhou2023} to recommender systems \cite{sun2018, yang2023} and scientific discovery \cite{mohamed2020, islam2023}. The increasing complexity and sheer volume of information available, particularly with the "explosion of Internet capacity" \cite{dai2020}, have underscored the critical need for efficient and scalable methods to process, analyze, and reason over these intricate knowledge structures. However, traditional symbolic representations of KGs, while offering high interpretability and precision, inherently suffer from significant limitations, including computational inefficiency, difficulties in handling data incompleteness, and challenges in integrating with statistical machine learning models.

To overcome these intrinsic challenges, Knowledge Graph Embedding (KGE) has become a crucial technique, revolutionizing how KGs are utilized in AI. KGE methods transform symbolic entities and relations into continuous, low-dimensional vector spaces, often referred to as embeddings \cite{dai2020, cao2022}. This transformation is not merely a change in data format; it represents a fundamental shift in how knowledge is represented and processed, enabling machine learning models to efficiently operate and reason over complex relational data. The evolution of KGE, as highlighted by various surveys \cite{dai2020, cao2022, ge2023}, marks a significant "knowledge progression" from rigid symbolic logic to flexible, dense vector representations, thereby converting complex KG problems into more tractable vector operations. This introduction will delve into the significance of KGs, define the pivotal role of KGE, elaborate on how it addresses the limitations of symbolic representations, and finally, outline the comprehensive scope and structure of this literature review.

\subsection*{1.1. Background: Knowledge Graphs and Their Significance}
Knowledge Graphs serve as powerful semantic networks, meticulously structuring real-world information into a graph-based format where entities (e.g., people, places, concepts) are nodes and relations (e.g., "bornIn," "hasPart") are directed edges between them. This explicit representation of facts, often in the form of (head entity, relation, tail entity) triplets, provides a rich, interpretable, and machine-actionable knowledge base \cite{dai2020}. The significance of KGs is manifold, extending across numerous AI domains. They provide the backbone for enhancing search engine capabilities, enabling more intelligent and context-aware responses \cite{huang2019}. In natural language processing, KGs facilitate semantic understanding, improving tasks such as question answering by grounding linguistic queries in structured knowledge \cite{zhou2023}. Furthermore, KGs are instrumental in personalized recommender systems, where they can model complex user-item interactions and provide explainable recommendations by leveraging rich relational paths \cite{sun2018, yang2023}.

Despite their immense potential, the direct utilization of symbolic KGs presents formidable challenges. The sheer scale of real-world KGs, often comprising millions or billions of entities and relations, leads to significant computational inefficiency when performing reasoning or inference tasks using traditional symbolic logic \cite{dai2020}. Symbolic methods struggle with the inherent incompleteness of KGs, as real-world knowledge is rarely exhaustive, and the absence of a fact often cannot be definitively interpreted as its falsehood. This "data sparsity" problem \cite{dai2020} makes it difficult for symbolic systems to generalize or infer new knowledge without explicit rules. Moreover, the rigid, discrete nature of symbolic representations makes them incompatible with the continuous vector spaces favored by modern machine learning algorithms, hindering seamless integration and leveraging the power of deep learning for complex pattern recognition and prediction. The development of KGE directly addresses these fundamental limitations, paving the way for more scalable, robust, and intelligent knowledge systems.

\subsection*{1.2. The Role of Knowledge Graph Embedding}
Knowledge Graph Embedding (KGE) is a transformative technique designed to bridge the gap between symbolic knowledge representation and the statistical power of machine learning. At its core, KGE aims to represent entities and relations as low-dimensional, dense, and continuous vectors (embeddings) in a multi-dimensional space \cite{dai2020, cao2022}. This transformation is crucial because it converts the discrete, sparse, and often high-dimensional symbolic data of KGs into a format that is readily processable by various machine learning models. The primary motivation for KGE stems from the inherent limitations of symbolic representations, which, as discussed, include computational inefficiency, difficulty in handling incompleteness, and challenges in integrating with modern deep learning architectures.

By embedding entities and relations into a continuous vector space, KGE models can capture latent semantic relationships and structural patterns that are difficult to discern from raw symbolic data. For instance, the "methodological evolution" in KGE has seen a progression from simpler geometric or algebraic approaches, like translational distance models (e.g., TransE \cite{jia2015}), to more sophisticated neural network-based models \cite{wu2021, shi2025}. This evolution directly addresses the computational inefficiency of symbolic KGs by enabling fast, approximate reasoning through vector arithmetic, replacing complex logical inference with efficient distance or scoring functions in the embedding space \cite{dai2020}. Furthermore, KGE models inherently handle incompleteness by learning representations that can generalize from observed facts to predict missing links, a task known as link prediction \cite{rossi2020}. The continuous nature of embeddings allows for a nuanced understanding of similarity and relatedness, moving beyond the binary true/false logic of symbolic systems. This "knowledge progression" has also led to KGE models being able to capture more complex aspects of KGs, such as temporal dynamics \cite{dasgupta2018, xu2019, xie2023, wang2024}, multi-faceted entity meanings through disentangled representations \cite{wu2021}, and even parameter-efficient embeddings for large-scale deployment \cite{chen2023}. The ability to transform symbolic knowledge into dense vectors has made KGE a cornerstone for integrating KGs into advanced AI applications, providing a powerful pre-trained component for tasks requiring efficient processing and reasoning over complex relational data.

\subsection*{1.3. Scope and Organization of the Review}
This literature review aims to provide a comprehensive overview of Knowledge Graph Embedding (KGE) research, systematically exploring its foundational principles, diverse methodologies, and significant advancements. The review is structured to guide the reader through the evolution of KGE, from its early conceptualizations to the cutting-edge techniques that address contemporary challenges in AI. We will begin by categorizing KGE models based on their underlying mathematical and architectural principles, including translational distance models \cite{jia2015, wang2014, xiao2015}, semantic matching models, and neural network-based approaches such as those leveraging Graph Neural Networks \cite{wu2021, wang2020} and attention mechanisms \cite{wu2021, xie2020}.

A critical analysis will be conducted for each category, evaluating their methodological strengths and weaknesses, identifying the specific types of relational patterns they are best suited to capture, and discussing their inherent trade-offs in terms of expressiveness, computational complexity, and scalability. We will delve into advanced topics such as temporal KGE, examining how models like HyTE \cite{dasgupta2018} and ATiSE \cite{xu2019} integrate time-varying information to capture the dynamic nature of knowledge. The review will also address crucial practical considerations, including parameter efficiency \cite{chen2023, sachan2020}, robustness against noise and adversarial attacks \cite{shan2018, zhang2021, zhou2024}, and the impact of negative sampling strategies on model performance \cite{madushanka2024, qian2021}. Furthermore, we will explore the application of KGE across various downstream AI tasks, such as entity alignment \cite{sun2018, zhang2019, xiang2021}, question answering \cite{huang2019, zhou2023}, and recommender systems \cite{sun2018, yang2023}. Finally, the review will highlight current limitations, open challenges, and promising future research directions in the field of KGE, aiming to provide a holistic understanding of this rapidly evolving and impactful area of AI research.