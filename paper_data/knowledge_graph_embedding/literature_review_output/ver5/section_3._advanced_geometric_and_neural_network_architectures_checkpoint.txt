\section{Advanced Geometric and Neural Network Architectures}
The limitations of foundational translational models, such as TransE, TransH, and TransD, in capturing complex relational patterns like symmetry, antisymmetry, inversion, and composition, as well as hierarchical structures and nuanced semantic dependencies, spurred a significant "methodological evolution" in Knowledge Graph Embedding (KGE) research \cite{ge2023, cao2022}. This phase marks a decisive shift from simple Euclidean vector operations to more sophisticated geometric spaces and the integration of powerful deep learning architectures. The driving force behind this "knowledge progression" is a relentless pursuit of greater expressiveness, enabling KGE models to accurately represent and reason over the multifaceted and dynamic nature of real-world knowledge graphs. This section delves into these advanced architectures, exploring rotational models that leverage complex spaces, multi-curvature embeddings for hierarchical data, the integration of Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs) for enhanced feature learning, and the emergence of Transformer-based and quaternion embeddings for tackling polysemy and spatiotemporal reasoning. While these advancements offer superior expressiveness and address critical limitations, they often introduce increased computational complexity and higher parameter counts, posing significant scalability challenges that researchers continue to grapple with \cite{chen2023, lloyd2022}. The rapid pace of innovation in this domain is evident, with a concentrated surge of research, particularly in 2024 and 2025, pushing the boundaries of what KGE models can achieve \cite{chen2025, shi2025, wang2024, ji2024}.

\subsection{Rotational, Spherical, and Multi-Curvature Embeddings}
Moving beyond translational operations, a significant advancement in KGE involved representing relations as rotations, particularly in complex vector spaces. \textbf{RotatE} \cite{sun2018} pioneered this approach by defining each relation as a rotation from the head entity to the tail entity in a complex vector space. This elegant geometric interpretation inherently captures various relational patterns: symmetry (rotation by $\pi$), antisymmetry (rotation by $-\pi$), inversion (rotation by $\pi$ followed by a reflection), and composition (sequential rotations). For example, if $h \circ r \approx t$, then $t \circ r^{-1} \approx h$, where $r^{-1}$ is the inverse rotation. This property is crucial for logical reasoning and inferring missing links based on existing paths. Building on this, \textbf{Rotate3D} \cite{gao2020} extended the concept to three-dimensional space, further enhancing the capacity to model complex relational patterns. While rotational models significantly improved expressiveness for these specific patterns, they still operate within a flat Euclidean-like space, which can be suboptimal for modeling hierarchical or highly structured data.

This limitation led to the exploration of \textbf{multi-curvature spaces}, recognizing that Euclidean space's constant zero curvature is often inadequate for representing the inherent hierarchical and tree-like structures prevalent in knowledge graphs. Hyperbolic spaces, with their constant negative curvature, are particularly well-suited for embedding hierarchies, as distances grow exponentially, allowing for efficient representation of many nodes with few dimensions \cite{pan2021, liang2024}. Conversely, hyperspherical spaces, with constant positive curvature, can model cyclic or periodic relationships \cite{li2024}. The integration of these diverse geometries represents a significant "methodological evolution." Recent works like \textbf{MADE} (Multicurvature Adaptive Embedding) \cite{wang2024} and \textbf{IME} (Integrating Multi-curvature Shared and Specific Embedding) \cite{wang2024} explicitly address the challenge of modeling diverse geometric structures within Temporal Knowledge Graphs (TKGs). MADE introduces adaptive weighting and a quadruplet distributor across Euclidean, hyperbolic, and hyperspherical spaces, while IME refines this by integrating space-shared and space-specific properties with an adjustable pooling mechanism, acknowledging the inherent heterogeneity of multi-curvature spaces. This adaptive approach allows models to select the most appropriate geometry for different parts of the graph or different types of relations, overcoming the "one-size-fits-all" limitation of earlier models. However, the theoretical challenge of optimally combining these spaces and the practical challenge of selecting appropriate curvatures for different data subsets remain active research areas. The increased complexity of working with non-Euclidean geometries also introduces computational overhead and can complicate model training and interpretation.

\subsection{Convolutional and Graph Neural Network Approaches}
The integration of neural network architectures marked another pivotal "methodological evolution" in KGE, moving beyond purely geometric transformations to leverage the powerful feature extraction capabilities of deep learning. \textbf{Convolutional Neural Networks (CNNs)} were initially adapted to KGE for their ability to capture local features and patterns within the embedding space. Models like \cite{ren2020} and \textbf{CNN-ECFA} (CNN-based Entity-Specific Common Feature Aggregation) \cite{hu2024} apply convolutional filters to the concatenated embeddings of entities and relations, treating them as images or sequences. This allows the model to learn complex interaction patterns between components of a triple, going beyond simple additive or multiplicative interactions. \textbf{Multi-Scale Dynamic Convolutional Network} \cite{zhang2020} further enhances this by employing convolutions at various scales, capturing both fine-grained and broader interaction patterns. The strength of CNNs lies in their parameter sharing and hierarchical feature learning, which can be more expressive than simple scoring functions. However, CNNs typically operate on fixed-size inputs, necessitating strategies like padding or reshaping for variable-length triples, and they do not inherently leverage the graph structure beyond local triplet patterns.

To directly address the graph structure, \textbf{Graph Neural Networks (GNNs)} emerged as a natural fit for KGE, representing a significant leap in "knowledge progression" towards more contextual and structure-aware embeddings. GNNs operate by iteratively aggregating information from an entity's neighbors, effectively propagating messages across the graph to enrich entity representations with local and multi-hop structural context. Models like \textbf{DisenKGAT} (Disentangled Knowledge Graph Attention Network) \cite{wu2021} utilize graph attention mechanisms to learn disentangled entity representations, capturing multiple facets of an entity and their context-dependent relevance to different relations. This addresses the limitation of static entity representations by allowing entities to exhibit distinct meanings in different contexts, thereby improving the modeling of complex relations (e.g., one-to-many). \textbf{TARGAT} (Time-Aware Relational Graph Attention Model) \cite{xie2023} further extends GNNs to temporal KGs, incorporating time-aware attention mechanisms to capture evolving relational patterns. While GNNs offer superior contextual awareness and feature learning, they face challenges such as scalability to very large graphs (due to message passing over many neighbors), the "over-smoothing" problem where representations of distant nodes become indistinguishable, and the computational cost associated with iterative aggregation. Furthermore, the effectiveness of GNNs often relies on the quality of the graph structure itself, and they can struggle with noisy or incomplete graphs unless specifically designed for robustness \cite{zhang2024}.

\subsection{Transformer-based and Quaternion Embeddings}
The latest wave of innovation in KGE has seen the adaptation of \textbf{Transformer architectures} and the exploration of \textbf{quaternion embeddings}, pushing the boundaries of expressiveness, particularly for polysemy and spatiotemporal reasoning. Transformers, renowned for their self-attention mechanism, excel at capturing global dependencies and long-range interactions, making them highly suitable for modeling complex contextual information in KGs. \textbf{TGformer} \cite{shi2025} represents a significant "methodological evolution" by introducing the first graph transformer framework for KGE, explicitly combining triplet-level and graph-level features. This allows TGformer to overcome the limitations of purely triplet-based models (which ignore graph structure) and traditional graph-based methods (which might overlook contextual information), thereby enhancing the model's ability to understand entities and relations in diverse contexts, including temporal KGs. Similarly, \textbf{Position-Aware Relational Transformer} \cite{li2023} leverages the Transformer's power to integrate positional information within relational paths, further enriching contextual understanding. \textbf{CKGE} (Contextualized Knowledge Graph Embedding) \cite{yang2023} also employs a novel KG-based Transformer with relational attention and structural encoding to capture global dependencies and provide explainable recommendations, demonstrating the versatility of this architecture. However, Transformers are notoriously parameter-heavy and computationally intensive, requiring substantial data and resources for effective training, which can exacerbate scalability challenges for massive KGs \cite{chen2023}.

Concurrently, \textbf{quaternion embeddings} have emerged as a powerful algebraic tool for modeling multi-faceted semantic information, particularly polysemy and complex spatiotemporal dynamics. Quaternions, as extensions of complex numbers, offer a richer algebraic structure with non-commutative multiplication, which can naturally represent rotations in 3D or 4D space. This property makes them highly effective for capturing complex patterns like polysemy, where an entity's meaning depends on its context. For example, \textbf{ConQuatE} (Contextualized Quaternion Embedding) \cite{chen2025} proposes contextualized quaternion embeddings to enrich entity representations through quaternion rotation and contextual cues, directly addressing the polysemy issue where entities have context-dependent meanings. This approach improves upon models with weak entity-relation interactions by allowing for more nuanced and context-aware representations. Furthermore, quaternions are adept at modeling spatiotemporal data due to their inherent rotational properties. \textbf{Multihop Fuzzy Spatiotemporal RDF Knowledge Graph Query via Quaternion Embedding} \cite{ji2024} leverages quaternions to handle fuzzy and uncertain spatiotemporal knowledge, incorporating dynamic vector projection, rotation, and quaternion-based reasoning to address real-world uncertainty and multihop path queries. This represents a significant "knowledge progression" from static, crisp KGs to dynamic, uncertain ones. While offering superior expressiveness for these specific tasks, quaternion embeddings introduce a higher mathematical complexity, which can make model design and interpretation more challenging compared to simpler vector-based models. Their computational efficiency, especially for large-scale KGs, also requires careful consideration.

Collectively, the advancements in rotational, multi-curvature, CNN, GNN, Transformer, and quaternion embeddings narrate a continuous and rapid evolution in KGE. This "unified narrative" is a relentless quest to overcome the inherent limitations of traditional KGE by embracing richer mathematical spaces, dynamic learning paradigms, and distributed architectures to accurately represent and reason over increasingly complex, uncertain, and evolving knowledge. These sophisticated models offer superior expressiveness, enabling more nuanced modeling of relational patterns, hierarchical structures, contextual dependencies, polysemy, and spatiotemporal dynamics. However, this enhanced capability comes with a trade-off: increased computational complexity, higher parameter counts, and greater data requirements, which pose significant scalability challenges for real-world applications \cite{chen2023, lloyd2022}. The current research landscape, particularly highlighted by the surge of papers in 2024 and 2025, indicates a vibrant and highly active front, where researchers are rapidly building upon and diversifying recent theoretical and practical advancements to develop more robust, adaptive, and inherently capable KGE models for complex, dynamic, and imperfect environments.