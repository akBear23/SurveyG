\section*{4. Enriching KGE: Context, Rules, and Multi-modality}
The foundational approaches to Knowledge Graph Embedding (KGE), primarily focused on representing entities and relations as vectors or matrices in a low-dimensional space, often operate under the simplifying assumption that the (head, relation, tail) triplet provides sufficient information for learning robust representations \cite{ge2023, cao2022}. However, real-world knowledge graphs (KGs) are inherently complex, characterized by data sparsity, polysemy, weak semantics, and the need for logical consistency. This has driven a significant "methodological evolution" and "knowledge progression" in KGE research, moving beyond purely structural information to integrate richer contextual cues, explicit logical rules, and diverse modalities \cite{ge2023}. The motivation is to overcome limitations such as limited expressiveness, poor generalization to unseen entities, and the inability to perform complex reasoning. By leveraging auxiliary data like entity types and attributes, incorporating logical constraints, and integrating textual descriptions or pre-trained language models (PLMs), KGE models can ground their embeddings in a more comprehensive understanding of the real world, leading to more semantic, robust, and interpretable representations. This section explores these advanced paradigms, highlighting how they address the inherent challenges of KG incompleteness and semantic ambiguity, while also critically examining their trade-offs and remaining limitations.

\subsection*{4.1. Incorporating Auxiliary Information and Entity Types}
Traditional KGE models often treat entities and relations as atomic symbols, learning their embeddings solely from their observed triplet patterns. This approach struggles with data sparsity, where entities with few connections lack sufficient context, and weak semantics, as the models cannot leverage higher-level conceptual information. To address this, a significant "methodological evolution" has involved incorporating auxiliary information, particularly entity types and attributes, to provide more semantic and robust representations \cite{ge2023}.

Approaches like \cite{lv2018} differentiate between concepts and instances, learning distinct embeddings for them to better capture their hierarchical nature and improve representation quality. Similarly, \textbf{TransET} \cite{wang2021} explicitly integrates entity types into the embedding process, typically by associating each entity with one or more types and modifying the scoring function or embedding projection based on these types. \textbf{TaKE} (Type-augmented Knowledge Graph Embedding) \cite{he2023} further refines this by proposing a type-augmented framework for knowledge graph completion, demonstrating that leveraging type information significantly enhances the accuracy of link prediction, especially for entities with sparse connections. These models often concatenate type embeddings with entity embeddings or use type-specific projection matrices to transform entity representations, thereby injecting semantic constraints and shared properties among entities of the same type. For instance, \cite{hu2024} introduces GeoEntity-type constrained KGE, specifically for predicting natural-language spatial relations, showcasing how domain-specific type constraints can be vital for specialized tasks.

While incorporating entity types offers substantial benefits, particularly in improving the semantic coherence of embeddings and aiding in knowledge graph completion, it also introduces challenges. The quality and completeness of type information are crucial; noisy or incomplete type assignments can degrade performance. Furthermore, most models assume a flat type hierarchy or require explicit modeling of type hierarchies, which can be complex. The approach by \cite{zhang2024} addresses a critical limitation by integrating entity attributes for *error-aware* KGE, acknowledging that auxiliary information itself can be imperfect. This highlights a general methodological limitation: while auxiliary data enriches embeddings, its inherent quality and the robustness of its integration mechanism are paramount. The increased parameter space required for type embeddings can also add to computational overhead, a trade-off for enhanced semantic richness. Despite these challenges, the consistent improvement in performance across various benchmarks demonstrates that auxiliary information, especially entity types, is indispensable for developing more semantically robust KGE models.

\subsection*{4.2. Rule-based and Constraint-driven Embeddings}
Beyond explicit auxiliary features, injecting logical rules and constraints into the KGE training process represents a powerful strategy to enhance logical consistency, improve reasoning capabilities, and address data sparsity by inferring missing facts \cite{ge2023}. This approach shifts from purely data-driven learning to knowledge-guided learning, embodying a significant "knowledge progression" towards more intelligent KGE systems.

Early methods, such as \cite{ding2018}, demonstrated that even simple constraints, like symmetry or transitivity, when incorporated into the loss function, can significantly improve embedding quality. These constraints act as regularization terms, guiding the embedding space to reflect known logical properties. \textbf{TransH} \cite{wang2014}, for instance, models relations as translations on hyperplanes, which inherently helps in distinguishing different mapping properties (e.g., one-to-many, many-to-one) that are often associated with logical rules. Building on this, \cite{yoon2016} proposed a translation-based KGE that explicitly preserves logical properties of relations.

More sophisticated approaches, like \cite{guo2017}, introduced iterative guidance from *soft rules*. Instead of hard constraints that might be too rigid for noisy KGs, soft rules allow for some flexibility while still encouraging logical consistency. This method iteratively refines embeddings by using the predictions of logical rules to generate additional training signals. \textbf{RulE} \cite{tang2022} takes this a step further by directly learning *rule embeddings*, allowing the model to reason with and apply rules within the embedding space itself. This is particularly effective for complex logical patterns, such as Horn clauses. \cite{guo2020} also focuses on preserving soft logical regularity, demonstrating the importance of balancing strict adherence to rules with the inherent noise and incompleteness of real-world KGs. A recent advancement, \cite{zheng2024}, explores KGE models that are "closed under composition," meaning if two relations compose to form a third, the embeddings reflect this property, which is fundamental for multi-hop reasoning.

The primary strength of rule-based KGE lies in its ability to inject prior knowledge, enforce consistency, and enable more robust reasoning, especially in scenarios with sparse data where rules can help infer new triples. However, this paradigm faces several challenges. Rule extraction can be a labor-intensive and error-prone process, requiring domain expertise. Hard constraints can be overly restrictive for noisy KGs, potentially leading to suboptimal embeddings if the rules themselves are imperfect. Soft rules offer more flexibility but might not always enforce strong logical consistency. Furthermore, handling conflicting rules or determining the optimal "softness" of a constraint remains an active research area. The "Z-paradox" discussed in \cite{liu2024} (though applied to a different context) highlights the general difficulty in enforcing desired structural properties in embedding spaces. Despite these complexities, the integration of logical rules represents a crucial step towards making KGE models more reliable and capable of symbolic reasoning.

\subsection*{4.3. Multi-modal and Language Model Integration}
The most transformative shift in KGE, particularly evident in recent years, is the integration of multi-modal information, especially textual descriptions and pre-trained language models (PLMs) \cite{ge2023}. This approach addresses the fundamental limitations of purely structural KGE models: their inability to capture the rich semantics often expressed in natural language and their struggle with entities lacking sufficient structural connections (data sparsity). By grounding embeddings in richer real-world context, multi-modal KGE significantly enhances semantic understanding and model performance.

Early efforts, such as \textbf{SSP} (Semantic Space Projection) \cite{xiao2016}, leveraged textual descriptions by projecting them into the same embedding space as structural embeddings, often through simple concatenation or shared encoders. This allowed entities to borrow semantic information from their associated text, improving representations for entities with limited structural data. A more sophisticated approach is seen in \cite{shen2022}, which proposes joint language semantic and structure embedding for knowledge graph completion, demonstrating how simultaneously learning from both modalities can lead to more comprehensive and accurate representations.

The advent of powerful PLMs like BERT has revolutionized this field. As highlighted by \cite{ge2023}, the integration of PLMs with KGE is an "emerging, transformative shift." PLMs, pre-trained on vast text corpora, possess a deep understanding of natural language semantics, which can be transferred to KGE. Models now often use PLMs to generate rich contextualized embeddings for entity and relation descriptions, which are then fused with structural embeddings. For instance, \textbf{Marie and BERT} \cite{zhou2023} developed a KGE-based question answering system for chemistry that leverages BERT to enhance semantic understanding, demonstrating the utility of PLMs in domain-specific applications. Similarly, \cite{yang2025} proposes a semantic enhanced KGE model with AIGC (AI-Generated Content) for healthcare prediction, showcasing how generative language models can contribute to enriching KGE.

A key advantage of PLM integration is its ability to address data sparsity and cold-start problems, as entities with few structural connections can still derive meaningful representations from their textual descriptions. This also improves the model's ability to handle polysemy, where an entity's meaning depends on its context, as PLMs excel at contextualizing word meanings. However, this integration is not without challenges. The fusion of heterogeneous information (structural vs. textual) remains a complex task, requiring sophisticated attention mechanisms or gating units to balance their contributions effectively. The computational cost of training and inference with large PLMs is substantial, posing scalability issues for massive KGs \cite{chen2023}. Furthermore, \cite{zhang2023} addresses the need for *modality-aware negative sampling* for multi-modal KGE, indicating that naive negative sampling strategies can be suboptimal when dealing with diverse data types. Despite these computational and architectural complexities, the superior semantic richness and performance gains offered by multi-modal and PLM-integrated KGE models make them a crucial direction for future research, particularly in applications requiring deep semantic understanding and robustness in data-scarce environments.