\subsection{Question Answering and Recommendation Systems}

Knowledge Graph Embeddings (KGEs) serve as a foundational technology for advancing intelligent systems in natural language understanding (NLU) for question answering (QA) and personalized recommendation systems (RS). By learning rich, low-dimensional vector representations of entities and relations within knowledge graphs (KGs), KGEs bridge the gap between unstructured natural language and structured knowledge, and enable the discovery of intricate user-item interactions that often elude traditional, sparse data models.

In the realm of Question Answering over Knowledge Graphs (QA-KG), KGEs provide a critical mechanism for mapping complex natural language queries into relevant KG components, facilitating accurate and efficient answer retrieval. Early approaches, such as the Knowledge Embedding based Question Answering (KEQA) framework \cite{huang2019}, focused on "simple questions" answerable by a single head entity and predicate. KEQA innovated by jointly recovering the question's head entity, predicate, and tail entity representations within the KG embedding space, utilizing a carefully designed joint distance metric to retrieve the closest fact. While effective for single-hop queries, this highlighted the need for more sophisticated mechanisms to handle the inherent complexities of natural language and multi-hop reasoning. To address the challenge of integrating rich textual semantics with KG structures, the Joint Language Semantic and Structure Embedding (LASS) model \cite{shen2022} fine-tunes pre-trained language models (LMs) using a probabilistic structured loss. LASS effectively leverages the forward pass of LMs to capture semantics from textual descriptions while simultaneously reconstructing KG structures through backpropagation, thereby enabling joint learning of both types of information. This integration is vital for accurately understanding natural language questions and mapping them to relevant KG components, especially for multi-hop queries where intermediate entities and relations must be inferred. More recently, the synergy between KGEs and large language models (LLMs) has been explored to enhance QA capabilities. KGEs provide a structured, factual grounding for LLMs, mitigating issues like hallucination and enabling more precise, knowledge-based reasoning. Approaches often involve incorporating KGEs into LLMs through techniques like prefix-tuning or retrieval augmentation, where graph-structured data from KGs is used to reconfigure LLMs or retrieve relevant subgraphs, strengthening specialized knowledge within a domain and providing more accurate responses \cite{liu2024q3q}.

For Recommendation Systems (RS), KGEs offer a powerful paradigm to learn semantic representations of users, items, and their interactions, effectively addressing the limitations of traditional collaborative filtering, such as data sparsity and cold-start problems \cite{liu2019e1u}. By embedding KG entities and relations, KGEs can capture rich contextual information and infer implicit relationships, leading to more effective and often explainable personalized suggestions.

The application of KGEs in recommendation systems can be broadly categorized into several paradigms:

Firstly, **embedding-based collaborative filtering** methods directly leverage KGEs to enrich user and item representations. These approaches often treat the recommendation task as a link prediction problem within an extended knowledge graph that includes user-item interactions. For instance, KGECF \cite{zhang2020wou} proposes a neural network that models user-item interactions as an interaction knowledge graph, learning vector representations through the RotatE KGE model. This converts collaborative filtering into link prediction, demonstrating state-of-the-art performance across diverse datasets. Similarly, other factorization-based KGE models like HolE and DistMult have been applied to build semantic-based recommenders, showing effectiveness in extracting semantics for explainable recommendations and alleviating cold-start issues \cite{kartheek2021aj7}.

Secondly, **path-based methods** explicitly exploit the rich relational paths between entities in a KG to infer user preferences and item similarities. The Recurrent Knowledge Graph Embedding (RKGE) model \cite{sun2018} exemplifies this by automatically learning semantic representations for both entities and the paths connecting them within a KG. RKGE employs a novel recurrent network architecture to model the semantics of multiple paths linking the same entity pair, fusing these learned path semantics into the recommendation process. Furthermore, it incorporates a pooling operator to discriminate and leverage the saliency (importance) of different paths, offering not only effective personalized suggestions but also meaningful explanations for its recommendations. This move towards explainable recommendations, driven by interpretable path semantics, represents a significant step beyond opaque black-box models.

Thirdly, the current state-of-the-art in KG-based recommendation is largely dominated by **Graph Neural Network (GNN) based approaches**. GNNs, through their message passing and aggregation mechanisms, are particularly adept at capturing higher-order, implicit interactions and structural information within KGs, which path-based methods might struggle to fully leverage due to computational complexity or reliance on predefined meta-paths. Models like KGCN (Knowledge Graph Convolutional Network) and KGAT (Knowledge Graph Attention Network) have demonstrated superior performance by iteratively aggregating neighborhood information to learn more expressive entity and user embeddings. A recent example, IDGCN \cite{pham20243mh}, proposes a knowledge graph embedding with a Graph Convolution Network for context-aware recommendation systems. It considers all user and item-based relationships to detect intricate connections, outperforming baseline approaches in personalized recommendations by effectively modeling the relationships between entities.

Finally, the drive for **explainable recommendation systems** has seen significant advancements, moving beyond simple path interpretations. While RKGE offered initial steps towards explainability through path saliency \cite{sun2018}, more sophisticated models now integrate contextualized KGEs with advanced neural architectures. For instance, CKGE (Contextualized Knowledge Graph Embedding) \cite{yang2023} proposes an explainable training course recommender system that integrates motivation-aware information by constructing specific meta-graphs for talent-course pairs. It employs a novel KG-based Transformer with relational attention and structural encoding, and crucially, a local path mask prediction mechanism that quantifies and highlights the saliency of meta-paths, providing explicit, motivation-driven explanations for recommendations.

Despite these advancements, several challenges remain. For QA, accurately handling highly complex, multi-hop questions that require intricate reasoning over diverse relation types, and ensuring robust entity and relation linking in noisy or evolving KGs, continues to be an active research area. The dynamic nature of KGs also poses a challenge for maintaining up-to-date embeddings for both QA and RS, requiring models that can adapt to temporal changes efficiently. For recommendation systems, integrating real-time user feedback and dynamic item attributes into KGE models, as well as developing more sophisticated mechanisms for cold-start scenarios that do not rely on extensive side information, are crucial future directions. The ongoing exploration of synergistic approaches combining KGEs with large language models, particularly for complex reasoning and personalized content generation, holds significant promise for the next generation of intelligent QA and recommendation systems, demanding KGEs that are not only expressive but also scalable, interpretable, and robust.