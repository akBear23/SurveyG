\subsection{Rotational and Complex Space Models}

Moving beyond simple linear translations, a significant advancement in Knowledge Graph Embedding (KGE) models involves leveraging more sophisticated mathematical operations, particularly rotations in complex or higher-dimensional spaces, to capture richer relational semantics. These approaches demonstrate enhanced expressiveness by employing intricate geometric transformations.

An early and influential model in this paradigm is ComplEx \cite{Trouillon_2016}. ComplEx extends the idea of semantic matching by employing complex-valued embeddings for both entities and relations. It defines a scoring function based on a Hermitian dot product, which naturally allows for the effective capture of both symmetric and antisymmetric relations without requiring explicit constraints or additional parameters. This algebraic approach provided a powerful alternative to earlier translational models, offering a more elegant and efficient solution for these specific relational patterns compared to more parameter-heavy tensor factorization methods like RESCAL. However, while adept at symmetry and antisymmetry, ComplEx did not inherently capture compositional patterns as elegantly as later models.

Addressing the limitations of previous models in simultaneously capturing diverse relational patterns, RotatE \cite{sun2018} emerged as a pivotal model. RotatE maps entities and relations into a complex vector space, interpreting each relation as an element-wise rotation from the head entity to the tail entity. Specifically, for a valid triple $(h, r, t)$, RotatE posits that $t \approx h \circ r$, where $\circ$ denotes the Hadamard (element-wise) product and the modulus of each component of the relation embedding $r$ is constrained to 1. This elegant formulation, inspired by Euler's identity, allows RotatE to simultaneously model symmetric, antisymmetric, and compositional patterns, as well as inversion, within a unified framework. For instance, a relation $r$ can be symmetric if its rotation is its own inverse, antisymmetric if its rotation is distinct from its inverse, and compositional if the rotation of $r_1$ followed by $r_2$ results in $r_3$. This capability represented a significant leap, as prior state-of-the-art models often excelled at one or two patterns but struggled to capture all three concurrently. RotatE also introduced self-adversarial negative sampling, an efficient training technique that generates more informative negative samples based on the current model's scores, further improving its performance.

The development of RotatE, building upon the foundations laid by ComplEx and generalizing translational models, showcased the power of moving beyond simple linear transformations. By embedding entities and relations in complex spaces and defining relations as rotations, these models gained a substantial increase in expressiveness. This allowed for a more nuanced understanding and prediction of complex relational dynamics within knowledge graphs, marking a critical step towards more intricate geometric transformations for KGE. While highly effective, these models still rely on predefined geometric operations, which, despite their sophistication, might not fully capture highly complex or implicit semantic patterns compared to more advanced neural approaches that learn features directly from data.