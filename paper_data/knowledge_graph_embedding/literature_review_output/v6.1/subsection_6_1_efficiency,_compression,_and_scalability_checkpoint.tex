\subsection{Efficiency, Compression, and Scalability}
The increasing scale of real-world knowledge graphs (KGs) and the computational demands of complex Knowledge Graph Embedding (KGE) models have made efficiency, compression, and scalability paramount concerns. This subsection examines a range of techniques designed to reduce memory footprint, accelerate training and inference, and enable the deployment of KGE models in resource-constrained environments and for massive knowledge bases. The focus has shifted from merely achieving high accuracy to ensuring that KGEs are practically deployable, as highlighted by the community's emphasis on addressing practical challenges and system-level optimization.

One prominent approach to enhancing efficiency is **knowledge distillation**, where a smaller, more efficient student model is trained to mimic the behavior of a larger, high-performing teacher model. \cite{zhu2020} introduced DualDE, a method that dually distills knowledge from a high-dimensional teacher KGE to a low-dimensional student. DualDE employs a soft label evaluation mechanism to adaptively weight soft and hard labels and a two-stage distillation process, enabling significant parameter reduction (7-15x) and inference speedup (2-6x) with minimal performance degradation. This technique offers a generalizable solution across various KGE architectures, allowing for the deployment of faster and cheaper reasoning models.

Complementing distillation, **embedding compression** directly addresses the memory footprint of KGEs. \cite{sachan2020} proposed a method to compress the embedding layer by representing each entity as a vector of discrete codes, composing embeddings from these codes. This approach achieved massive compression ratios (50-1000x) with only a minor loss in performance, demonstrating the feasibility of drastically reducing storage requirements. Building on this, LightKG \cite{wang2021} introduced a lightweight framework that stores only a few codebooks and indices, rather than full continuous vectors for every entity. This design not only reduces storage but also boosts inference efficiency through quick look-ups. LightKG further innovates with a dynamic negative sampling method based on quantization, which can be applied to other KGE methods for performance improvement. While both \cite{sachan2020} and \cite{wang2021} tackle storage, LightKG provides a more integrated solution for both storage and inference, coupled with a novel sampling strategy, illustrating a progression towards holistic efficiency. A common limitation, however, is the inherent trade-off: aggressive compression, while vital for scalability, can sometimes lead to a performance drop, which these methods strive to minimize.

**Parameter-efficient learning** offers another avenue for reducing resource consumption, particularly for growing KGs. Entity-Agnostic Representation Learning (EARL) \cite{chen2023} proposes a novel method that learns embeddings only for a small set of "reserved entities." Embeddings for other entities are then derived from their context using entity-agnostic encoders, which transform distinguishable information from connected relations, k-nearest reserved entities, and multi-hop neighbors. This approach results in a static and lower parameter count, decoupling the growth of the KG from the linear increase in embedding parameters, a crucial benefit for large and evolving knowledge bases.

Beyond model-specific compression, **novel algorithms and optimized system designs** are critical for large-scale KGE training. \cite{peng2021} introduced a highly efficient KGE learning framework using Orthogonal Procrustes Analysis. By formulating KGE as a closed-form solution, their method enables full-batch learning and non-negative sampling, leading to orders of magnitude reduction in training time and carbon footprint while maintaining competitive performance. This represents a paradigm shift from iterative optimization to a mathematically elegant, direct solution. In contrast to model-specific algorithmic innovations, GE2 \cite{zheng2024} focuses on system-level optimization for KGE training. It proposes a general and efficient learning system that offloads operations from CPU to GPU for high parallelism and introduces COVER, a novel algorithm for managing data swap between CPU and multiple GPUs with minimal communication costs. GE2's general execution model and user-friendly API for negative sampling address fundamental system bottlenecks, achieving significant speedups (over 2x, up to 7.5x) across various models and datasets, thereby accelerating research and development.

For Graph Neural Network (GNN)-based KGEs, which are often computationally intensive, **graph partitioning strategies** are essential for scalability. CPa-WAC \cite{modak2024} addresses this by employing modularity maximization-based constellation partitioning. This method breaks down large KGs into subgraphs that can be processed separately, reducing memory and training time while crucially aiming to retain prediction accuracy. CPa-WAC demonstrates that meaningful partitioning can enable efficient learning on subgraphs, with performance comparable to training on the entire KG, and up to five times faster training. This approach directly tackles the accuracy-scalability trade-off inherent in processing large graphs with GNNs.

Collectively, these innovations represent a concerted effort to overcome the practical bottlenecks of KGE, making them deployable in resource-constrained environments and for handling ever-growing knowledge bases. The progression from model-level compression and distillation \cite{zhu2020, sachan2020, wang2021} to parameter-efficient learning \cite{chen2023}, novel training algorithms \cite{peng2021}, and sophisticated system-level optimizations \cite{zheng2024, modak2024} reflects a maturing field. This evolution, as noted in the development directions, moves towards specialized, system-level, and scalable approaches, addressing the accuracy-scalability trade-off and fundamental system inefficiencies. While each method offers distinct advantages, the overarching challenge remains balancing the gains in efficiency and scalability with the potential for minor performance degradation, a trade-off that researchers continue to navigate through increasingly sophisticated designs.