\subsection{Transformer-based KGE Models}
The advent of Transformer architectures, initially lauded for their success in natural language processing, has ushered in a new era for Knowledge Graph Embedding (KGE) by offering unparalleled capabilities in capturing long-range dependencies and contextualized representations. This paradigm shift moves beyond the limitations of static embeddings and local feature interactions, enabling models to discern more nuanced semantic relationships within complex knowledge graphs.

Early adaptations, such as CoKE (Contextualized Knowledge Graph Embedding) \cite{wang2019}, pioneered the application of Transformer encoders to KGE. CoKE innovatively treats knowledge graph paths and edges as sequences of entities and relations, allowing the self-attention mechanism to learn dynamic, context-dependent embeddings. Unlike foundational models like TransE \cite{bordes2013} or RotatE \cite{sun2018}, which assign a single static vector to each entity and relation, CoKE's approach enables entities and relations to exhibit different properties based on their specific graph context. This ability to capture intrinsic contextual variations significantly enhances expressiveness, particularly for tasks requiring a deep understanding of relational paths. However, a fundamental challenge arises from the inherent order-invariance of the vanilla self-attention mechanism, which struggles to distinguish between a valid (head, relation, tail) triplet and its semantically incorrect permutations (e.g., (tail, relation, head)). This limitation prevents vanilla Transformers from accurately capturing the directed nature of relational semantics in KGs.

To address this critical issue, Knowformer \cite{li2023} introduced a Position-Aware Relational Transformer specifically designed for KGE. Knowformer overcomes the order-invariance problem by explicitly injecting relational compositions into entity representations, thereby capturing the role of an entity (subject or object) based on its position within a triple. This methodological innovation ensures that the self-attention mechanism correctly distinguishes entity roles and captures the precise relational semantics, a crucial aspect for accurate link prediction and entity alignment. The formal proof provided by \cite{li2023} underscores its theoretical soundness, demonstrating its ability to differentiate between valid and shuffled triplet variants.

More recently, comprehensive graph transformer frameworks have emerged that more deeply integrate graph structures. TGformer \cite{shi2025} represents a significant advancement, being presented as the first general graph transformer framework for KGE that explicitly models both triplet-level and graph-level structural features. As detailed in its technical overview \cite{shi2025}, TGformer moves beyond simply treating KGs as sequences by constructing a context-level subgraph for each predicted triplet, thereby capturing inter-triplet relationships based on shared entities. It employs a Knowledge Graph Transformer Network (KGTN) designed to comprehensively explore multi-structural features and leverage the contextual information of nodes more effectively, discerning valuable entity and relation information that might be overlooked by purely triplet-based or even some graph-based methods. Furthermore, TGformer extends its capabilities to temporal knowledge graphs, addressing the dynamic nature of real-world knowledge, a challenge that many earlier models, including those from the "Foundational KGE Models" section like TorusE \cite{ebisu2017} or CyclE \cite{yang2021}, do not explicitly tackle.

Transformer-based KGE models offer distinct advantages over other deep learning architectures. While Convolutional Neural Networks (CNNs) like AcrE \cite{ren2020} and ReInceptionE \cite{xie2020} excel at extracting local features and modeling interactions within fixed-size receptive fields, and Graph Neural Networks (GNNs) such as DisenKGAT \cite{wu2021} effectively aggregate neighborhood information, Transformers' self-attention mechanism allows for direct modeling of global dependencies across the entire graph or relevant subgraphs. This enables them to capture long-range semantic relationships and complex contextual information more effectively, pushing the state-of-the-art in KGE performance. For instance, \cite{li2023} demonstrates that Knowformer achieves state-of-the-art results on both link prediction and entity alignment tasks, showcasing the power of position-aware attention.

However, the enhanced expressiveness of Transformer-based models often comes with trade-offs. Their increased computational complexity and higher parameter counts can pose scalability challenges, especially for extremely large knowledge graphs, contrasting with the parameter efficiency often sought in earlier models like TransD \cite{ji2015}. While general KGE training optimizations, such as efficient negative sampling strategies, optimized loss functions, and careful hyperparameter tuning, are crucial for all models \cite{kge_training_components_general}, they become particularly salient for the resource-intensive Transformer architectures. Moreover, while models like CoKE \cite{wang2019} and Knowformer \cite{li2023} significantly improve contextual understanding, the interpretability of their complex attention patterns remains an area for further research. Despite these challenges, the innovative application of Transformers to graph structures, as seen in models like TGformer \cite{shi2025}, signifies a crucial step towards developing more robust, adaptive, and inherently capable KGE models that can effectively handle the complexities and multi-structural features of real-world knowledge graphs. This continuous methodological evolution underscores the field's commitment to leveraging cutting-edge deep learning techniques to advance knowledge representation.