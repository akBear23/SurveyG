\subsection{Core Translational Models and Extensions}
The advent of knowledge graph embedding (KGE) marked a significant shift from purely symbolic knowledge representation to continuous vector spaces, offering enhanced efficiency and expressiveness for various AI tasks. Pioneering this paradigm were the translational models, with \textit{TransE} \cite{bordes2013} establishing the foundational principle where the embedding of a head entity ($h$) plus the relation vector ($r$) should approximate the embedding of the tail entity ($t$), i.e., $h+r \approx t$. This elegant simplicity, however, struggled with complex relational patterns such as one-to-many, many-to-one, and many-to-many relations \cite{wang2014, lin2015}. In such scenarios, a single relation vector could not adequately distinguish between multiple valid tail entities for a given head, or vice-versa, leading to a collapse of entity embeddings and reduced discriminative power, as the model would be forced to place multiple distinct entities close to each other in the embedding space \cite{wang2014}. This limitation meant that \textit{TransE} often failed to capture the nuanced semantics where an entity might participate in a relation in different "roles" or contexts.

To address these limitations, \textit{TransH} introduced a significant refinement by modeling relations as translations on relation-specific hyperplanes \cite{wang2014}. Instead of a single relation vector directly translating entities in the global embedding space, \textit{TransH} projects entity embeddings onto a hyperplane orthogonal to the relation vector, and then performs the translation within that specific hyperplane. This mechanism allows a single entity to have different representations when involved in different relations, effectively mitigating the issues of one-to-many and many-to-one relations. For instance, an entity like "Barack Obama" can be projected differently when involved in relations like "place\_of\_birth" (to Hawaii) versus "spouse" (to Michelle Obama), preventing the embedding of "Barack Obama" from being forced to be close to both "Hawaii" and "Michelle Obama" simultaneously in the original space. While \textit{TransH} offered a substantial improvement in expressiveness over \textit{TransE} with comparable computational complexity, it still faced challenges in handling the diversity of entities, as the projection for a relation was fixed regardless of the specific entities involved \cite{ji2015}. Furthermore, \textit{TransH} also introduced a novel Bernoulli negative sampling strategy, which leverages relation mapping properties (tails per head, heads per tail) to construct more effective negative examples during training, thereby reducing the generation of false negative labels and improving learning robustness \cite{wang2014}.

Building upon \textit{TransH}, \textit{TransR} \cite{lin2015} introduced a more sophisticated approach by mapping entities from the entity space to a relation-specific space before performing the translation. In \textit{TransR}, each relation $r$ is associated with a relation vector $r$ and a dedicated projection matrix $M_r$. For a triplet $(h, r, t)$, the head entity $h$ and tail entity $t$ are first projected into the relation space using $h_r = h M_r$ and $t_r = t M_r$, and then the translational assumption $h_r + r \approx t_r$ is applied within this relation-specific space. This mechanism allows entities to have different representations in different relation spaces, which significantly improves the model's ability to capture the nuanced semantics of diverse relations and entities compared to \textit{TransE} and \textit{TransH} \cite{lin2015}. By using relation-specific projection matrices, \textit{TransR} could better capture the contextual roles of entities. However, a major drawback of \textit{TransR} was its high computational cost and large number of parameters, as each relation required its own dense projection matrix. This made it less scalable for knowledge graphs with a vast number of relations \cite{ji2015}. Moreover, while it allowed for relation-specific projections, these projections were still fixed for all entities within a given relation, potentially limiting its flexibility in capturing fine-grained entity diversity. An extension, \textit{TransHR}, further explored this direction by transforming hyper-relations into individual vectors, demonstrating the continuous effort to adapt projection-based models to more complex data structures \cite{zhang2018transhr}.

Addressing the computational and flexibility issues of \textit{TransR}, \textit{TransD} \cite{ji2015} further refined the translational paradigm by introducing dynamic mapping matrices. Unlike \textit{TransR}'s fixed relation-specific projection matrices, \textit{TransD} constructs projection matrices dynamically from *both* entity and relation vectors. Specifically, each entity $e$ and relation $r$ is represented by two vectors: one for its meaning ($e, r$) and another for constructing its dynamic mapping matrix ($e_p, r_p$). The projection matrices are then formed as outer products of these auxiliary vectors, allowing for highly flexible, entity-specific projections. This dynamic approach enables \textit{TransD} to better capture the diversity of entities within a relation, as the projection adapts to the specific head or tail entity involved. Crucially, \textit{TransD} achieves this enhanced expressiveness while maintaining parameter efficiency by avoiding large, dense matrix storage; the projection matrices are implicitly defined, making it more scalable for large knowledge graphs compared to \textit{TransR} \cite{ji2015}.

Collectively, these pioneering translational models—\textit{TransE}, \textit{TransH}, \textit{TransR}, and \textit{TransD}—established a fundamental paradigm for KGE. They represent a crucial intellectual trajectory, systematically addressing limitations of prior models to enhance expressiveness and efficiency. From \textit{TransE}'s foundational simplicity, which laid the groundwork, to \textit{TransH}'s hyperplane projections, \textit{TransR}'s relation-specific spaces, and \textit{TransD}'s dynamic, entity-aware projections, each model built upon its predecessors to better capture complex relational patterns like one-to-many and many-to-one mappings \cite{wang2014, lin2015, ji2015}. These works significantly advanced the field by offering a robust framework for tasks like link prediction, moving beyond purely symbolic methods to continuous representations \cite{bordes2013}. While subsequent research would introduce alternative geometric operations, such as rotations in complex spaces with models like \textit{RotatE} \cite{sun2018}, the translational family laid the essential groundwork for efficiently representing relational knowledge. Their contributions in conceptualizing relations as transformations in embedding spaces continue to influence modern KGE research, forming a crucial stepping stone towards more sophisticated and context-aware embedding techniques.