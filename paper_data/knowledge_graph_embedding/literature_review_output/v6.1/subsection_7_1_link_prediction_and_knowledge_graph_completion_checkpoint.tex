\subsection{Link Prediction and Knowledge Graph Completion}
Link prediction and knowledge graph completion (KGC) represent the foundational and most widely studied applications of Knowledge Graph Embedding (KGE), serving as critical benchmarks for evaluating the efficacy of various KGE models. These tasks fundamentally involve inferring unobserved facts, typically in the form of $(h, r, t)$ triples, or completing partially observed ones within a knowledge graph structure. The continuous drive to improve accuracy and robustly handle complex relational patterns in these tasks underpins the utility of KGE for numerous downstream AI applications, making knowledge graphs more complete, robust, and informative.

The evolution of KGE models for link prediction commenced with foundational geometric approaches, primarily translational models (detailed in Section 2.1). TransE \cite{bordes2013}, a pioneering method, established the paradigm of representing relations as translations in an embedding space. While efficient, its limitations in modeling complex relational patterns such as one-to-many, many-to-one, and many-to-many relationships quickly became apparent. Subsequent models like TransH \cite{wang2014} addressed this by projecting entities onto relation-specific hyperplanes, allowing for more nuanced representations without a substantial increase in computational complexity. TransD \cite{ji2015} further refined this by introducing dynamic mapping matrices, enabling entity-specific projections that adapt to different relations, thereby improving expressiveness and scalability. Despite these advancements, translational models often struggled with intricate logical patterns, prompting the exploration of more sophisticated geometric transformations \cite{asmara2023}.

A significant advancement in capturing complex relational patterns, including symmetry, antisymmetry, inversion, and composition, emerged with rotational and complex space embeddings (discussed in Section 2.2). RotatE \cite{sun2018} defined relations as rotations in complex vector spaces, inherently allowing it to model these patterns more effectively than its translational predecessors. This approach demonstrated superior performance in link prediction, particularly for relations exhibiting these properties. Further innovations, such as HousE \cite{li2022}, leveraged Householder transformations, combining rotations and reflections for enhanced modeling capacity. CompoundE \cite{ge2022} and CompoundE3D \cite{ge2023} generalized this by cascading translation, rotation, and scaling operations, aiming for a richer set of transformations to capture diverse relational semantics. The choice of embedding space itself also evolved, with models like TorusE \cite{ebisu2017} exploring Lie groups to circumvent regularization issues, and CyclE \cite{yang2021} investigating the impact of different metrics on expressiveness for KGC. More recently, MQuinE \cite{liu2024} identified and addressed a "Z-paradox" in existing KGE models, where theoretical deficiencies in representing specific relational patterns (e.g., composition) could degrade link prediction performance, proposing a solution for stronger expressiveness.

Beyond geometric transformations, deep learning architectures have profoundly impacted KGE for link prediction (as explored in Section 3). Convolutional Neural Networks (CNNs) have been employed to extract local features and model intricate interactions between entity and relation embeddings. AcrE \cite{ren2020} and ReInceptionE \cite{xie2020} utilized advanced convolutional techniques to capture local-global structural information for improved link prediction. More recent CNN-based approaches like CNN-ECFA \cite{hu2024} and SEConv \cite{yang2025} continue to refine feature aggregation for enhanced performance. Graph Neural Networks (GNNs) and attention mechanisms (Section 3.2) also play a crucial role, with models like DisenKGAT \cite{wu2021} leveraging disentangled graph attention networks to learn diverse component representations, thereby boosting accuracy and explainability for KGC. Transformer-based models (Section 3.3), such as CoKE \cite{wang2019}, Knowformer \cite{li2023}, and TGformer \cite{shi2025}, adapt self-attention mechanisms to capture long-range dependencies and contextualized representations, pushing the state-of-the-art in modeling complex contextual information for link prediction. While these deep learning models often achieve higher accuracy by learning non-linear patterns, they typically incur increased computational complexity and may offer reduced interpretability compared to simpler geometric models.

A persistent challenge in link prediction is the inherent incompleteness and sparsity of knowledge graphs, alongside the need to capture complex relational patterns that no single model can perfectly address. To overcome the limitations of individual KGE models, advanced strategies focus on integrating multiple model strengths. For instance, \cite{gregucci2023} proposes a novel framework that combines query representations from diverse KGE models using an attention mechanism within a spherical geometric framework. This allows the system to dynamically select the most suitable model's representation for a given query, effectively leveraging the distinct geometric transformations of different KGE models to capture a broader range of relational and structural patterns, thereby outperforming individual models in link prediction. Furthermore, real-world knowledge graphs often contain weighted facts, necessitating models capable of handling such nuances. The framework proposed by \cite{wang2022} extends deterministic KGE models to learn embeddings for weighted knowledge graphs, introducing "weighted link prediction" as a crucial evaluation task for these scenarios. For dynamic and evolving KGs, temporal KGE models (Section 5.1) are specifically applied to enable time-aware link prediction, inferring facts that hold true at specific timestamps. Similarly, for unseen entities, inductive KGE methods (Section 5.2) are crucial for generating embeddings and performing link prediction without full retraining.

The reliability of link prediction and KGC heavily depends on robust evaluation and benchmarking. Standard metrics like Mean Reciprocal Rank (MRR) and Hits@k are widely used, but the consistency and reproducibility of experimental results remain a concern \cite{rossi2020, ruffinelli2020}. To address this, frameworks like LibKGE \cite{broscheit2020} and TorchKGE \cite{torchkge} provide standardized, modular, and efficient platforms for training, hyperparameter optimization, and evaluation of KGE models specifically for link prediction. These tools are vital for fostering reproducible research and enabling fair comparisons across different models. Comparative studies, such as that by \cite{ruffinelli2020}, have also highlighted the relationship between KGE models designed for link prediction and those for other data mining tasks, demonstrating that their effectiveness can often translate across applications. The practical deployment of KGE for link prediction also necessitates efficiency and scalability, especially for large graphs. While detailed solutions are in Section 6.1, the need for scalable training, such as that for GNN-based KGE models \cite{cpa-wac_2023}, is paramount for real-world link prediction tasks.

Despite significant progress, challenges persist, including handling long-tail entities, further mitigating data sparsity, and developing more robust evaluation benchmarks that account for diverse KG characteristics. The continuous trade-off between model expressiveness, computational efficiency, and interpretability remains a central tension. Nonetheless, ongoing research in link prediction and KGC, spanning from refining geometric transformations to leveraging sophisticated deep learning architectures and integrating multi-model strategies, continues to make knowledge graphs more complete, robust, and ultimately more valuable for a wide array of AI applications.