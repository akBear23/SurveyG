Knowledge Graphs (KGs) represent a fundamental paradigm for organizing and structuring world knowledge, serving as a cornerstone for numerous Artificial Intelligence (AI) applications. At their core, KGs are structured networks comprising entities (real-world objects, concepts, or events) and relations (the types of connections between these entities) \cite{ge2023, dai2020}. This structure is typically represented as a collection of triples in the form of (head entity, relation, tail entity), such as (\textit{Barack Obama}, \textit{bornIn}, \textit{Honolulu}). This formalization allows for a machine-readable representation of facts, enabling systems to understand and reason over complex information.

The historical development of KGs can be traced back to early symbolic AI efforts, evolving from semantic networks and expert systems in the 1970s and 80s \cite{sowa1991, brachman1979}. These predecessors aimed to represent knowledge explicitly through nodes and links, often relying on hand-crafted rules and ontologies. However, they faced significant limitations, primarily in terms of scalability, flexibility, and the immense human effort required for knowledge engineering \cite{russell2010}. The vision of the Semantic Web in the early 2000s, championed by Tim Berners-Lee, sought to extend the World Wide Web with machine-readable metadata, providing a framework for sharing and reusing data across applications \cite{berners2001semantic}. This initiative, coupled with the "explosive growth of Internet capacity" \cite{ge2023}, spurred the creation of large-scale, open-domain KGs. Prominent examples include Freebase \cite{bollacker2008freebase}, DBpedia \cite{auer2007dbpedia}, Wikidata \cite{vrandecic2014wikidata}, and YAGO \cite{suchanek2007yago, choudhary2021}. These KGs aggregate vast amounts of factual information from diverse sources, such as Wikipedia and other structured databases, providing a rich, interconnected web of knowledge that underpins many intelligent systems today. For instance, they power semantic search engines, enhance question answering systems \cite{huang2019, choudhary2021, additional_paper_4_key}, and improve recommender systems \cite{sun2018, choudhary2021, additional_paper_6_key}. Furthermore, the construction of domain-specific KGs, often from heterogeneous and mixed-quality resources, highlights their adaptability and the ongoing need for robust representation \cite{additional_paper_5_key}.

Despite their immense value, symbolic KGs inherently present several challenges that limit their full potential and motivate the need for alternative representations. Firstly, their discrete and sparse nature leads to significant \textbf{computational inefficiency} when dealing with large-scale graphs \cite{ge2023}. Reasoning over explicit symbolic rules, especially for multi-hop inference, often involves combinatorial search spaces, becoming computationally intractable as the graph size and depth of reasoning grow \cite{russell2010}. This makes real-time inference on massive KGs impractical.

Secondly, KGs are almost always \textbf{incomplete}, meaning many true facts are simply missing \cite{ge2023, choudhary2021}. Symbolic methods struggle profoundly with this incompleteness, as they rely on explicit connections and rules; inferring missing links without predefined logical axioms or extensive domain knowledge is difficult. This limitation is particularly evident when trying to capture nuanced semantic similarities or discover implicit relationships, which are not explicitly encoded. While advanced symbolic methods, such as probabilistic logic programming, have been developed to handle uncertainty and incompleteness, they often introduce their own complexities in terms of inference algorithms and knowledge acquisition \cite{raedt2008probabilistic}. Furthermore, existing fully expressive symbolic models often struggle to provably respect background taxonomic information, such as subclass and subproperty hierarchies, which are crucial for consistent knowledge representation \cite{additional_paper_2_key}.

Moreover, symbolic representations often struggle with the \textbf{diversity and complexity of relations}. Relations can exhibit various patterns such as one-to-many, many-to-one, N-to-N, symmetry, antisymmetry, inversion, and composition \cite{wang2014, sun2018}. While some symbolic systems can model these through explicit logical constraints, doing so for every relation type across a massive KG is a monumental and brittle task, often leading to difficult-to-maintain systems. The problem of \textbf{polysemy}, where entities or relations can have different meanings depending on context, is also challenging for purely symbolic representations \cite{choudhary2021}. A single symbolic identifier for an entity cannot easily capture its multiple facets or roles without extensive disambiguation rules, which themselves require significant knowledge engineering.

These inherent challenges—computational inefficiency arising from discrete representations, pervasive incompleteness and difficulty in inferring missing facts, the struggle to model complex relation patterns and taxonomic hierarchies, and the lack of semantic nuance—collectively underscore the limitations of purely symbolic knowledge representation. They highlight a critical need for more advanced, robust, and scalable techniques that can effectively manage, reason with, and integrate large-scale knowledge bases into modern AI systems. This motivation directly paved the way for the development of Knowledge Graph Embedding (KGE) techniques, which aim to transform symbolic knowledge into dense, continuous vector spaces, thereby enabling more efficient computation, robust inference, and seamless integration with contemporary machine learning paradigms \cite{cao2022, choudhary2021}. KGE models, as explored in subsequent sections, offer a generalized framework for reasoning about knowledge graph information, including composite relations, without special training \cite{additional_paper_1_key}.