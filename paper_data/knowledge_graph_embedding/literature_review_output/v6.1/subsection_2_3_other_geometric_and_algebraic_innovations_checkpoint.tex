\subsection{Other Geometric and Algebraic Innovations}
Beyond the foundational translational and rotational paradigms, the field of Knowledge Graph Embedding (KGE) has seen a continuous exploration of diverse geometric spaces and advanced algebraic transformations to enhance model expressiveness, theoretical soundness, and robustness. This pursuit aims to circumvent inherent limitations of simpler models, such as regularization problems or insufficient capacity to capture intricate relational patterns \cite{ge2023, cao2022}.

One significant line of innovation involves embedding entities and relations within non-Euclidean spaces, particularly Lie groups. \cite{ebisu2017} introduced \texttt{TorusE}, a pioneering model that embeds objects on a torus, a compact Lie group, to address the regularization issues prevalent in early translation-based models like \texttt{TransE}. \texttt{TransE} often forces entity embeddings onto a sphere in Euclidean space, which can warp representations and hinder their ability to accurately fulfill the translational principle. By leveraging the inherent properties of a torus, \texttt{TorusE} naturally avoids the need for explicit regularization, leading to more stable and accurate embeddings. This approach highlights a fundamental shift in thinking about the underlying geometry of the embedding space, moving beyond standard vector spaces to exploit the mathematical properties of more complex manifolds. While \texttt{TorusE} demonstrated superior performance and scalability, its adoption of a non-Euclidean space introduces a different set of mathematical complexities compared to Euclidean models, potentially affecting ease of implementation or interpretability for researchers less familiar with Lie group theory.

Complementing the exploration of novel embedding spaces, other works have critically examined the choice of metric within these spaces. \cite{yang2021} investigated the impact of the metric itself, proposing the "Cycle metric" as an alternative to the widely used Minkowski metric in their model, \texttt{CyclE}. They argued that the oscillation property of periodic functions, which underpins the Cycle metric, offers superior expressiveness for KGE. Their quantitative analysis demonstrated that a smaller function period correlates with better expressive ability, suggesting that the fundamental way distances and relationships are measured significantly influences model performance. While \texttt{TorusE} changes the *shape* of the embedding space, \texttt{CyclE} focuses on the *rules* governing distances within a space, both aiming to refine the mathematical foundations for better representation. The theoretical justification for \texttt{CyclE}'s metric choice is compelling, but its practical benefits are contingent on the specific characteristics of the knowledge graph and the types of relations it contains.

Further advancements have come from employing more sophisticated algebraic transformations. \cite{li2022} introduced \texttt{HousE}, a powerful KGE framework that utilizes Householder parameterization. This involves two types of Householder transformations: rotations for superior capacity in modeling relation patterns and projections for handling complex relation mapping properties. Theoretically, \texttt{HousE} is capable of simultaneously capturing crucial relation patterns (e.g., symmetry, antisymmetry, inversion, composition) and mapping properties (e.g., 1-to-N, N-to-1, N-to-N), and generalizes existing rotation-based models to higher-dimensional spaces. This represents a significant leap from simpler operations, offering a richer set of transformations to encode complex semantic interactions. Similarly, \cite{ge2022} and \cite{ge2023} proposed \texttt{CompoundE} and \texttt{CompoundE3D}, respectively, which leverage compound geometric operations combining translation, rotation, and scaling (and even reflection and shear in \texttt{CompoundE3D}). These models are designed to be highly generalizable, encompassing many existing scoring-function-based KGE models as special cases. The strength of these compound operations lies in their ability to model a broader spectrum of relational patterns through a cascade of transformations, reflecting the multifaceted nature of real-world relations. However, the increased complexity of these compound operations can lead to higher computational costs and a larger parameter space, posing scalability challenges for extremely large knowledge graphs.

The continuous effort to refine mathematical foundations is also evident in models like \texttt{TranS} \cite{zhang2022}, which introduces synthetic relation representations within a transition-based framework to better handle complex scenarios where the same entity pair might have different relations. This addresses a limitation of single-vector relation representations by allowing for more nuanced and context-dependent relation modeling. Furthermore, the development of \texttt{MQuinE} \cite{liu2024} highlights the ongoing theoretical scrutiny of KGE models, as it identifies and provides a "cure" for the "Z-paradox"â€”a deficiency in the expressiveness of some popular existing KGE models that can lead to significant accuracy drops on challenging test samples. This work underscores the importance of theoretical rigor in ensuring the fundamental soundness of KGE representations. Even earlier, \cite{ji2015}'s \texttt{TransD} represented an initial step in refining mapping matrices dynamically, demonstrating an early awareness of the need for more flexible transformations beyond simple vector additions.

In summary, this diverse array of geometric and algebraic innovations, ranging from embedding on Lie groups \cite{ebisu2017} and exploring novel metrics \cite{yang2021} to employing advanced Householder parameterizations \cite{li2022} and compound transformations \cite{ge2022, ge2023}, collectively showcases the field's relentless pursuit of more expressive, theoretically sound, and robust KGE models. While these advancements often introduce increased model complexity, they are crucial for pushing the boundaries of what KGE can achieve in capturing the intricate and multifaceted nature of knowledge.