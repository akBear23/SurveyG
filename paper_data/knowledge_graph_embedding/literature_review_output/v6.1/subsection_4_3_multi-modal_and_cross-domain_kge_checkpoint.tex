\subsection*{Multi-modal and Cross-domain KGE}

The inherent incompleteness and sparsity of knowledge graphs (KGs) often limit the effectiveness of purely structural embedding models. To address these challenges and enrich knowledge representations, a significant research direction has emerged in multi-modal and cross-domain knowledge graph embedding (KGE). These approaches integrate diverse information sources, such as textual descriptions, visual features, or data from disparate domains, to provide more comprehensive, semantically rich, and robust embeddings. This integration leverages complementary information, leading to a deeper understanding of entities and relations that goes beyond the symbolic triplet structure.

Early efforts in multi-modal KGE primarily focused on leveraging textual descriptions associated with entities and relations. For instance, \cite{xiao2016} proposed \textit{SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions}. SSP jointly learns from symbolic triples and textual descriptions, projecting information into a semantic space where text is used to discover semantic relevance. This approach aimed to overcome the "weak-semantic" nature of purely geometric models by grounding embeddings in natural language semantics, thereby providing more precise representations. While SSP marked an important step, its text embedding capabilities were limited by the NLP techniques available at the time.

More recent advancements have capitalized on the power of pre-trained language models (PLMs) to achieve more sophisticated integration of textual semantics. \cite{shen2022} introduced a method for \textit{Joint Language Semantic and Structure Embedding for Knowledge Graph Completion}. This approach fine-tunes PLMs with a probabilistic structured loss, effectively capturing rich semantics from natural language descriptions while simultaneously reconstructing structural information. This represents a significant leap from earlier text-integration methods like SSP, as it leverages the deep contextual understanding of modern PLMs. A key strength of this joint learning paradigm is its ability to significantly improve performance, particularly in low-resource settings where structural information is scarce, by injecting robust semantic cues. However, the computational cost associated with fine-tuning large PLMs can be substantial, posing scalability challenges compared to simpler, non-contextualized models.

Beyond textual descriptions, multi-modal KGE extends to integrating other modalities and domain-specific knowledge. For instance, \cite{zhu2022} demonstrated \textit{Multimodal reasoning based on knowledge graph embedding for specific diseases}. This work constructs Specific Disease Knowledge Graphs (SDKGs) and employs multimodal reasoning using reverse-hyperplane projection, integrating structural, category, and description embeddings. This application in the biomedical domain highlights how combining different modalities can lead to the discovery of new, reliable knowledge, such as drug-gene or gene-disease associations. The strength here lies in its practical utility for specialized fields, where diverse data types (e.g., clinical notes, biological pathways, patient data) are crucial. However, the generalizability of such highly specialized models to other domains without significant re-engineering remains a challenge, and the availability and quality of multimodal data in specific domains can be a limiting factor.

Cross-domain KGE, while related, focuses on integrating knowledge across different, often heterogeneous, domains to address challenges like data sparsity and cold-start problems in applications such as recommendation systems. \cite{liu2023} proposed a \textit{Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation}. This approach aims to efficiently model associations and interactions between items across multiple domains by introducing a "binding rule" to facilitate both homo-domain and hetero-domain item embeddings. This directly addresses the cross-domain cold start problem and enables multi-domain recommendations, which traditional single-domain recommenders struggle with. The methodological strength lies in its ability to leverage knowledge from richer domains to inform recommendations in sparser ones, thereby enriching item representations and improving prediction accuracy. However, the complexity of designing effective "binding rules" that accurately capture nuanced cross-domain interactions without introducing negative transfer remains a critical challenge.

The evolution in this area demonstrates a clear trajectory: from initial attempts to project text into embedding spaces \cite{xiao2016} to sophisticated joint learning frameworks that leverage advanced language models \cite{shen2022}, and further to specialized multimodal reasoning for domain-specific knowledge discovery \cite{zhu2022} and cross-domain applications like recommendation \cite{liu2023}. While these approaches significantly enrich representations and mitigate data sparsity, they introduce new complexities. Methodological limitations often include the difficulty of effectively aligning heterogeneous modalities in a shared embedding space, the increased computational burden of processing and fusing diverse data types, and the challenge of maintaining interpretability as models become more complex. The trade-off between enhanced accuracy and expressiveness versus computational cost and data requirements is particularly pronounced in multi-modal and cross-domain KGE. Future research must continue to explore more efficient fusion mechanisms and robust evaluation protocols for these increasingly complex models.