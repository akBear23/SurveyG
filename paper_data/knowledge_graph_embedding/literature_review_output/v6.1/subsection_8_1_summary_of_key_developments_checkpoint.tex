\subsection{Summary of Key Developments}
The intellectual trajectory of Knowledge Graph Embedding (KGE) research has undergone a profound evolution, tracing a path from early geometric and algebraic models to sophisticated deep learning architectures. This continuous progression underscores the field's commitment to transforming static, symbolic knowledge into dynamic, actionable insights for diverse AI systems, serving both data mining tasks and link prediction \cite{additional_paper_2}.

Initially, the foundation of KGE was laid by geometric and algebraic models, which sought to represent entities and relations in continuous vector spaces. Early translation-based models, such as TransE, TransH \cite{wang2014}, and TransD \cite{ji2015}, established the core paradigm by modeling relations as vector translations. While efficient, these foundational approaches often struggled with capturing complex relational patterns like symmetry, antisymmetry, and composition, leading to limited expressiveness and difficulties in distinguishing fine-grained semantics. A significant advancement came with models like RotatE \cite{sun2018}, which introduced rotations in complex vector spaces to elegantly capture these intricate patterns, moving beyond the inherent limitations of simpler translational approaches. This concept was further extended to 3D spaces in Rotate3D \cite{gao2020}. The continuous drive to refine these mathematical underpinnings is evident in innovations like HousE \cite{li2022}, which employed Householder transformations, and CompoundE \cite{ge2022} (and its 3D variant \cite{ge2023}), which generalized by combining translation, rotation, and scaling. Critically, the field also addressed fundamental theoretical issues to enhance model robustness and expressiveness. For instance, TorusE \cite{ebisu2017} resolved a long-standing flaw in TransE's regularization strategy by embedding entities and relations on a compact Lie group (a torus). This innovative approach eliminated the conflict between the translation principle and regularization, leading to more accurate and less warped embeddings. Further theoretical depth was introduced by HolmE \cite{zheng2024}, which proposed a Riemannian KGE model specifically designed to be "closed under composition." This novel property ensures that the composition of any two relation embeddings remains within the embedding space, enabling robust modeling of complex, under-represented composition patterns and theoretically unifying models like TransE and RotatE as special cases. These developments highlight a persistent effort to develop theoretically sound and highly expressive representations, moving beyond simple distance-based scoring functions to capture richer relational semantics. The ability to extend deterministic KGE models to handle weighted knowledge graphs, as demonstrated by frameworks like WeExt \cite{additional_paper_3}, further showcases the adaptability of these foundational paradigms to richer data types.

A pivotal shift occurred with the integration of deep learning architectures, which enabled KGE models to automatically extract complex features and capture non-linear patterns, often surpassing the performance of purely geometric models. Convolutional Neural Networks (CNNs) were adapted to model intricate feature interactions, as seen in AcrE \cite{ren2020}, ReInceptionE \cite{xie2020}, and M-DCN \cite{zhang2020}, excelling at capturing local patterns and non-linear feature interactions for improved link prediction. Graph Neural Networks (GNNs) and attention mechanisms, exemplified by GAATs \cite{wang2020} and DisenKGAT \cite{wu2021}, became instrumental in leveraging the graph's topology and neighborhood context to learn richer, context-dependent, and disentangled representations, which are crucial for multi-hop reasoning and understanding structural dependencies. More recently, Transformer architectures, such as CoKE \cite{wang2019} and Knowformer \cite{li2023}, have been innovatively applied to capture long-range dependencies and contextualized embeddings, treating KGs as sequences or integrating graph structures into self-attention mechanisms. While CNNs are highly effective for local feature aggregation, GNNs leverage the graph structure for broader contextual understanding, and Transformers excel at modeling global dependencies, each architecture offers distinct advantages but also introduces increased computational complexity and demands for larger datasets, necessitating careful trade-offs between expressiveness and efficiency.

Beyond structural and architectural innovations, significant efforts have focused on enriching KGE models with auxiliary information and enhancing their robustness and practical utility. This includes incorporating explicit entity types \cite{wang2021} and attributes \cite{zhang2024_ae_ke} to provide richer semantic guidance, addressing data sparsity and improving discriminative power by grounding embeddings in more comprehensive context. The integration of logical rules and constraints, from enforcing semantic smoothness \cite{guo2015} to iteratively guiding embeddings with soft rules \cite{guo2017, guo2020, tang2022}, has been crucial for injecting prior knowledge, ensuring consistency, and improving reasoning capabilities, thereby enhancing interpretability and aligning learned representations with human-understandable patterns. Concurrently, the field has dedicated substantial research to optimizing the training process and improving model reliability, recognizing that the choice of training components, such as loss functions, hyperparameters, and negative sampling strategies, can have a substantial impact on model efficiency and accuracy, often as much as the architectural design itself \cite{additional_paper_1}. This includes developing sophisticated negative sampling strategies \cite{shan2018, zhang2018, qian2021}, which evolved from uniform random sampling to more adversarial or confidence-aware methods to better approximate true negative distributions and mitigate false negatives. Non-sampling alternatives \cite{li2021} and weighted training for imbalanced data \cite{zhang2023} have also emerged as vital techniques. Probability calibration \cite{tabacof2019} and reinforcement learning-based noise filtering \cite{zhang2021} further ensure the trustworthiness and robustness of KGE predictions in real-world, often noisy, knowledge graphs.

The increasing focus on dynamic, inductive, and multi-modal approaches reflects the critical challenges posed by real-world knowledge graphs, which are rarely static or confined to a single modality. Temporal KGE models, such as HyTE \cite{kazemi2018} and TeRo \cite{han2020}, have emerged to capture the evolving nature of facts, moving beyond static representations to model time explicitly through various transformations and geometric spaces. Inductive KGE, enabling models to embed unseen entities, progressed from neighborhood aggregation \cite{wang2018} to advanced meta-learning strategies \cite{chen2021}, addressing the challenge of continuously growing knowledge bases. Continual KGE further addresses efficient model updates and catastrophic forgetting in these dynamic environments. Furthermore, the rise of Federated KGE tackled privacy concerns in distributed settings, leading to breakthroughs in communication efficiency \cite{zhang2024_fkge} and personalization \cite{zhang2024_personalized_fkge}, crucial for leveraging decentralized knowledge without compromising sensitive information. Simultaneously, multi-modal KGE has gained traction, integrating textual descriptions \cite{xiao2016, shen2022} and other modalities to overcome data sparsity and enrich semantic understanding, often leveraging powerful pre-trained language models for richer context. The drive for efficiency and scalability has also led to techniques like knowledge distillation \cite{zhu2020}, embedding compression \cite{sachan2020, wang2021_lightkg}, and parameter-efficient learning \cite{chen2023}, ensuring that KGE models are not only powerful but also practical for large-scale deployment.

Finally, the field has increasingly emphasized rigorous evaluation and reproducibility, recognizing their critical importance for scientific progress and real-world deployment. Early research often suffered from inconsistent benchmarking and hyperparameter tuning, leading to a reproducibility crisis where reported results were difficult to verify and compare fairly \cite{ali2020}. This led to the development of unified frameworks and libraries like PyKEEN \cite{ali2020_pykeen} and LibKGE \cite{thulke2021} to ensure fair comparisons, standardize evaluation metrics, and promote reliable scientific progress. These efforts highlight the maturity of the field, moving towards higher standards of empirical validation and transparency, crucial for translating theoretical advancements into reliable and trustworthy real-world applications.

In summary, the field of KGE has undergone a remarkable transformation, evolving from simple geometric models to complex, hybrid architectures that integrate deep learning, logical reasoning, and multi-modal information. This continuous pursuit of enhanced expressiveness, efficiency, and robustness, coupled with a growing emphasis on dynamic, inductive, and privacy-preserving capabilities, has significantly advanced the ability to transform symbolic knowledge into actionable insights, making KGE an indispensable component for modern AI systems across diverse applications.