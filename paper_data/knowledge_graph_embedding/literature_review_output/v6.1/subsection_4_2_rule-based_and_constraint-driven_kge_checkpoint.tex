\subsection{Rule-based and Constraint-driven KGE}
While purely data-driven knowledge graph embedding (KGE) models excel at capturing statistical patterns from observed triples, they often struggle with interpretability, reasoning capabilities, and ensuring semantic consistency, particularly in sparse or noisy knowledge graphs. To address these limitations, a significant line of research focuses on integrating logical rules and explicit constraints directly into the KGE learning process. These rule-based and constraint-driven approaches aim to inject prior knowledge, guide the embedding space to adhere to logical patterns, and enhance the interpretability of learned representations by aligning them with human-understandable rules \cite{dai2020}. This emphasis on leveraging logical knowledge leads to more robust and semantically coherent embeddings.

Early efforts in this domain focused on enforcing semantic smoothness within the embedding space. For instance, \cite{guo2015} proposed Semantically Smooth Embedding (SSE), which leverages additional semantic information, such as entity categories, to ensure that entities belonging to the same semantic group are embedded closely together. By formulating manifold learning algorithms like Laplacian Eigenmaps as regularization terms, SSE guides the embedding process beyond mere factual compatibility, aiming to discover intrinsic geometric structures. While effective in introducing semantic guidance, SSE relies on pre-defined semantic categories rather than explicit logical rules, and its effectiveness is contingent on the quality and availability of such categorical information.

A pivotal advancement in integrating explicit logical knowledge came with the introduction of methods that leverage soft rules. Traditional logic rules are often hard, meaning they must hold without exception, making them difficult to extract automatically and prone to brittleness in real-world, noisy KGs. Recognizing this, \cite{guo2017} introduced Rule-Guided Embedding (RUGE), a novel paradigm that iteratively integrates soft rules (rules associated with confidence levels) into the embedding learning process. RUGE enables an embedding model to learn simultaneously from observed triples, unlabeled triples (whose labels are iteratively predicted), and automatically extracted soft rules. This iterative guidance allows for a more robust transfer of logical knowledge into the embeddings, demonstrating that even uncertain, automatically extracted rules can significantly improve performance in tasks like link prediction. This approach marked a crucial departure from one-time rule injection, acknowledging the interactive nature between embedding learning and logical inference.

Beyond complex rule mining, even simple structural constraints can yield substantial benefits. \cite{ding2018} explored the potential of using straightforward constraints, such as non-negativity on entity representations and approximate entailment on relation representations. These constraints impose prior beliefs about the structure of the embedding space, leading to more compact and interpretable entity representations and encoding logical entailment regularities between relations. Crucially, these simple constraints improve model interpretability and structure the embedding space without introducing significant computational complexity or scalability issues. However, while effective for basic structural properties, they may not capture the full richness of complex logical patterns that explicit rules can express.

Further refining the integration of soft rules, \cite{guo2020} proposed a highly scalable method for preserving soft logical regularities by imposing rule constraints directly on relation representations. By representing relations as bilinear forms and mapping entities into a non-negative, bounded space, their method derived a rule-based regularization that primarily constrains relation representations. This design significantly improves scalability, as the complexity of rule learning becomes independent of the entity set size, a critical advantage for large-scale KGs. This work builds upon the foundation laid by RUGE, focusing on practical deployment by optimizing for scalability while maintaining the benefits of soft logical guidance.

The most sophisticated integration of logical rules into KGE is exemplified by \cite{tang2022} with RulE (Rule Embedding). RulE proposes a principled framework that learns rule embeddings jointly with entity and relation embeddings in a unified space. This allows for soft logical inference, where a confidence score can be calculated for each rule based on its consistency with observed triples, thereby alleviating the brittleness of hard logic. Furthermore, RulE uses these learned rule embeddings to regularize and enrich the entity and relation embeddings, deeply intertwining logical reasoning with embedding learning. This unified approach not only enhances reasoning capabilities but also makes the KGE models more robust and semantically coherent by ensuring adherence to logical patterns.

Despite the significant advancements, challenges persist. A primary limitation across many rule-based approaches is the efficient and accurate acquisition of high-quality rules. While RUGE and \cite{guo2020} leverage automatically extracted soft rules, the process of rule mining itself can be complex and prone to noise. Balancing the strict adherence to rules with the flexibility to capture exceptions or novel patterns not covered by existing rules remains a delicate trade-off. Over-constraining the embedding space with too many or overly strict rules can limit the model's ability to learn nuanced, data-driven patterns. Moreover, while some methods like \cite{guo2020} address scalability for rule integration, handling a vast number of complex, multi-hop rules efficiently in very large KGs continues to be an area of active research. The theoretical gaps also include a more formal understanding of how "softness" in rules translates into continuous embedding spaces and how to optimally combine symbolic logic with sub-symbolic representations. Nonetheless, the trajectory of this research subgroup clearly demonstrates the immense value of integrating logical knowledge to overcome the inherent limitations of purely data-driven KGE models, paving the way for more intelligent and interpretable AI systems.