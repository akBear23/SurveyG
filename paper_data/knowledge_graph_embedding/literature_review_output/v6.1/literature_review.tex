\documentclass[12pt,a4paper]{article}
    \usepackage[utf8]{inputenc}
    \usepackage[T1]{fontenc}
    \usepackage{amsmath,amsfonts,amssymb}
    \usepackage{graphicx}
    \usepackage[margin=2.5cm]{geometry}
    \usepackage{setspace}
    \usepackage{natbib}
    \usepackage{url}
    \usepackage{hyperref}
    \usepackage{booktabs}
    \usepackage{longtable}
    \usepackage{array}
    \usepackage{multirow}
    \usepackage{wrapfig}
    \usepackage{float}
    \usepackage{colortbl}
    \usepackage{pdflscape}
    \usepackage{tabu}
    \usepackage{threeparttable}
    \usepackage{threeparttablex}
    \usepackage[normalem]{ulem}
    \usepackage{makecell}
    \usepackage{xcolor}

    % Set line spacing
    \doublespacing

    % Configure hyperref
    \hypersetup{
        colorlinks=true,
        linkcolor=blue,
        filecolor=magenta,      
        urlcolor=cyan,
        citecolor=red,
    }

    % Title and author information
    \title{A Comprehensive Literature Review with Self-Reflection}
    \author{Literature Review}
    \date{\today}

    \begin{document}

    \maketitle

    % Abstract (optional)
    \begin{abstract}
    This literature review provides a comprehensive analysis of recent research in the field. The review synthesizes findings from 377 research papers, identifying key themes, methodological approaches, and future research directions.
    \end{abstract}

    \newpage
    \tableofcontents
    \newpage

    \label{sec:introduction}

\label{sec:introduction}

\section{Introduction}
Knowledge representation has undergone a significant evolution, progressing from early semantic networks to the sophisticated, large-scale knowledge graphs (KGs) that now underpin many intelligent systems. While KGs like Freebase, DBpedia, and Wikidata offer a powerful means to organize and structure world knowledge symbolically, they inherently face challenges such as data sparsity, incompleteness, and computational inefficiencies in capturing nuanced semantic similarities and performing scalable reasoning. These limitations have historically hindered their seamless integration with modern machine learning paradigms.

This review introduces Knowledge Graph Embeddings (KGE) as a pivotal paradigm shift designed to overcome these obstacles. KGE transforms discrete, symbolic entities and relations into continuous, low-dimensional vector spaces, thereby converting complex symbolic problems into efficient vector operations. This transformation makes knowledge machine-understandable and actionable for diverse artificial intelligence tasks. This introductory section establishes the foundational context for KGE, beginning with an exploration of the historical evolution of knowledge representation and the fundamental structure of knowledge graphs. We then delve into the core motivations driving the development of embedding methods, highlighting how they address the shortcomings of traditional symbolic approaches by enabling scalability, facilitating semantic reasoning, and enhancing performance in tasks such as link prediction, entity alignment, and question answering. The subsequent subsections will delineate the comprehensive scope and organizational structure of this review, guiding the reader through the intricate landscape of KGE research, from foundational geometric models to advanced deep learning architectures, practical considerations, and real-world applications. By providing this essential background, this introduction aims to illuminate the significance and trajectory of KGE within the broader field of artificial intelligence, underscoring its transformative role in converting complex knowledge into a format amenable to advanced computational analysis.

\subsection{Background: Knowledge Graphs}
\label{sec:1\_1\_background:\_knowledge\_graphs}

Knowledge Graphs (KGs) represent a fundamental paradigm for organizing and structuring world knowledge, serving as a cornerstone for numerous Artificial Intelligence (AI) applications. At their core, KGs are structured networks comprising entities (real-world objects, concepts, or events) and relations (the types of connections between these entities) \cite{ge2023, dai2020}. This structure is typically represented as a collection of triples in the form of (head entity, relation, tail entity), such as (\textit{Barack Obama}, \textit{bornIn}, \textit{Honolulu}). This formalization allows for a machine-readable representation of facts, enabling systems to understand and reason over complex information.

The historical development of KGs can be traced back to early symbolic AI efforts, evolving from semantic networks and expert systems in the 1970s and 80s \cite{sowa1991, brachman1979}. These predecessors aimed to represent knowledge explicitly through nodes and links, often relying on hand-crafted rules and ontologies. However, they faced significant limitations, primarily in terms of scalability, flexibility, and the immense human effort required for knowledge engineering \cite{russell2010}. The vision of the Semantic Web in the early 2000s, championed by Tim Berners-Lee, sought to extend the World Wide Web with machine-readable metadata, providing a framework for sharing and reusing data across applications \cite{berners2001semantic}. This initiative, coupled with the "explosive growth of Internet capacity" \cite{ge2023}, spurred the creation of large-scale, open-domain KGs. Prominent examples include Freebase \cite{bollacker2008freebase}, DBpedia \cite{auer2007dbpedia}, Wikidata \cite{vrandecic2014wikidata}, and YAGO \cite{suchanek2007yago, choudhary2021}. These KGs aggregate vast amounts of factual information from diverse sources, such as Wikipedia and other structured databases, providing a rich, interconnected web of knowledge that underpins many intelligent systems today. For instance, they power semantic search engines, enhance question answering systems \cite{huang2019, choudhary2021, additional\_paper\_4\_key}, and improve recommender systems \cite{sun2018, choudhary2021, additional\_paper\_6\_key}. Furthermore, the construction of domain-specific KGs, often from heterogeneous and mixed-quality resources, highlights their adaptability and the ongoing need for robust representation \cite{additional\_paper\_5\_key}.

Despite their immense value, symbolic KGs inherently present several challenges that limit their full potential and motivate the need for alternative representations. Firstly, their discrete and sparse nature leads to significant \textbf{computational inefficiency} when dealing with large-scale graphs \cite{ge2023}. Reasoning over explicit symbolic rules, especially for multi-hop inference, often involves combinatorial search spaces, becoming computationally intractable as the graph size and depth of reasoning grow \cite{russell2010}. This makes real-time inference on massive KGs impractical.

Secondly, KGs are almost always \textbf{incomplete}, meaning many true facts are simply missing \cite{ge2023, choudhary2021}. Symbolic methods struggle profoundly with this incompleteness, as they rely on explicit connections and rules; inferring missing links without predefined logical axioms or extensive domain knowledge is difficult. This limitation is particularly evident when trying to capture nuanced semantic similarities or discover implicit relationships, which are not explicitly encoded. While advanced symbolic methods, such as probabilistic logic programming, have been developed to handle uncertainty and incompleteness, they often introduce their own complexities in terms of inference algorithms and knowledge acquisition \cite{raedt2008probabilistic}. Furthermore, existing fully expressive symbolic models often struggle to provably respect background taxonomic information, such as subclass and subproperty hierarchies, which are crucial for consistent knowledge representation \cite{additional\_paper\_2\_key}.

Moreover, symbolic representations often struggle with the \textbf{diversity and complexity of relations}. Relations can exhibit various patterns such as one-to-many, many-to-one, N-to-N, symmetry, antisymmetry, inversion, and composition \cite{wang2014, sun2018}. While some symbolic systems can model these through explicit logical constraints, doing so for every relation type across a massive KG is a monumental and brittle task, often leading to difficult-to-maintain systems. The problem of \textbf{polysemy}, where entities or relations can have different meanings depending on context, is also challenging for purely symbolic representations \cite{choudhary2021}. A single symbolic identifier for an entity cannot easily capture its multiple facets or roles without extensive disambiguation rules, which themselves require significant knowledge engineering.

These inherent challenges—computational inefficiency arising from discrete representations, pervasive incompleteness and difficulty in inferring missing facts, the struggle to model complex relation patterns and taxonomic hierarchies, and the lack of semantic nuance—collectively underscore the limitations of purely symbolic knowledge representation. They highlight a critical need for more advanced, robust, and scalable techniques that can effectively manage, reason with, and integrate large-scale knowledge bases into modern AI systems. This motivation directly paved the way for the development of Knowledge Graph Embedding (KGE) techniques, which aim to transform symbolic knowledge into dense, continuous vector spaces, thereby enabling more efficient computation, robust inference, and seamless integration with contemporary machine learning paradigms \cite{cao2022, choudhary2021}. KGE models, as explored in subsequent sections, offer a generalized framework for reasoning about knowledge graph information, including composite relations, without special training \cite{additional\_paper\_1\_key}.
\subsection{Motivation for Knowledge Graph Embedding}
\label{sec:1\_2\_motivation\_for\_knowledge\_graph\_embedding}

The increasing prevalence of large-scale knowledge graphs (KGs) has established them as critical assets for artificial intelligence, providing structured repositories of factual information. However, the inherent characteristics of symbolic, discrete representations within traditional KGs pose significant limitations, which fundamentally motivate the development of Knowledge Graph Embedding (KGE) techniques. KGE transforms symbolic entities and relations into continuous, low-dimensional vector spaces, thereby addressing these challenges and unlocking new capabilities for AI systems \cite{dai2020, cao2022, yan2022}.

A primary limitation of sparse, symbolic representations is their inherent inability to capture nuanced semantic similarities and intricate relational patterns. Traditional KGs operate on exact matches and predefined logical rules, making it difficult to infer implicit relationships or understand subtle degrees of relatedness. For example, while a symbolic KG can explicitly state "Paris is the capital of France" and "Berlin is the capital of Germany," it struggles to intrinsically recognize that Paris and Berlin are both major European capitals sharing similar functional roles, or that "is the capital of" and "is located in" are semantically related but distinct concepts. KGE models overcome this by embedding entities and relations as vectors, allowing semantic similarity to be computed through vector space metrics (e.g., cosine similarity, Euclidean distance) \cite{cao2022}. This continuous representation enables a richer, more flexible understanding of knowledge, moving beyond rigid, binary facts. This paradigm shift aligns with the distributional hypothesis prevalent in natural language processing, where the meaning of an entity is derived from its context and relationships within the embedding space \cite{dai2020}. Furthermore, KGE models, particularly those considering contextual connectivity patterns, can capture implicit relationships across different triples, leading to more accurate and comprehensive embeddings \cite{additional\_paper\_6, yan2022}.

Another critical barrier for symbolic KGs is the computational inefficiency in large-scale reasoning and management. As KGs expand to billions of triples, performing complex symbolic inference or even simple path-based queries can become computationally prohibitive due to the combinatorial explosion of potential paths and rules. KGEs fundamentally address this by converting complex symbolic problems into efficient vector operations. Pioneering translational models like TransE \cite{bordes2013}, TransH \cite{wang2014}, and TransD \cite{ji2015} exemplify this by representing relations as translations in embedding spaces, thereby reducing tasks like link prediction to computationally inexpensive vector additions and distance calculations. This shift to vector arithmetic facilitates unparalleled scalability, a crucial requirement for real-world applications dealing with massive KGs \cite{yan2022}. The continuous drive for practical scalability is further evidenced by works focusing on embedding compression, such as those by \cite{sachan2020} and LightKG \cite{wang2021}, which drastically reduce storage and inference costs. Similarly, knowledge distillation techniques, as seen in DualDE \cite{zhu2020}, and optimized system designs like GE2 \cite{zheng2024}, aim to create faster and more resource-efficient KGEs. Frameworks like LightCAKE \cite{additional\_paper\_2} further enhance efficiency by explicitly modeling graph context without introducing redundant trainable parameters, demonstrating that sophisticated context-awareness can be achieved with minimal computational overhead.

The inherent incompleteness of real-world KGs presents a significant challenge for symbolic systems, which struggle to infer missing links without explicit, often incomplete, rules. KGEs provide a powerful mechanism for addressing this, primarily through link prediction and knowledge graph completion tasks \cite{rossi2020, additional\_paper\_4}. By learning latent patterns from existing triples, KGE models can predict the plausibility of unobserved facts, effectively "filling in the blanks" and making KGs more robust and useful. This capability extends to predicting new triples, even incorporating background taxonomic information like subclasses and subproperties, as demonstrated by models that minimally modify existing methods to inject such knowledge \cite{additional\_paper\_5}.

While KGEs offer substantial advantages in terms of scalability and semantic representation, this transformation from symbolic to continuous space is not without its trade-offs. The dense vector representations, while powerful for computation, often come at the cost of the inherent interpretability and logical precision characteristic of symbolic systems. The "black box" nature of many embedding models can obscure the underlying reasoning process, making it challenging to understand \textit{why} a particular prediction was made. This limitation has, in turn, motivated significant subsequent research into developing more interpretable KGE models and integrating explicit logical rules or explainable AI techniques to bridge the gap between continuous representations and human-understandable reasoning \cite{additional\_paper\_3}.

Beyond overcoming these limitations, KGEs offer several proactive advantages, fundamentally transforming how KGs interact with modern AI. They facilitate seamless integration with contemporary machine learning pipelines, especially deep learning models. As KGEs produce dense vector representations, they serve as powerful feature inputs for various downstream AI tasks that rely on neural networks, such as natural language processing, computer vision, and recommender systems \cite{dai2020, yan2022}. This compatibility has been a major driver of KGE adoption, enabling the development of hybrid AI systems that combine the structured knowledge of KGs with the pattern recognition capabilities of deep learning. KGE models are increasingly viewed as effective pre-trained components, particularly beneficial for diverse downstream deep learning applications \cite{yan2022}.

This strategic shift has proven transformative for a wide array of AI tasks, providing an encoding for data mining and prediction \cite{additional\_paper\_4}:
\begin{itemize}
    \item \textbf{Knowledge Graph Completion and Link Prediction}: A core application, enabling KGs to become more complete and informative by inferring missing facts \cite{rossi2020, additional\_paper\_4}.
    \item \textbf{Entity Alignment}: Crucial for integrating heterogeneous KGs by identifying equivalent entities across different datasets, leveraging embeddings to find semantic correspondences \cite{sun2018, zhang2019}.
    \item \textbf{Question Answering (QA)}: KGEs bridge the gap between natural language questions and structured knowledge, allowing systems to interpret queries and retrieve relevant facts from KGs, making knowledge more accessible \cite{huang2019, zhou2023}.
    \item \textbf{Recommendation Systems}: KGEs enhance personalized recommendations by modeling user-item interactions and item properties within a knowledge graph, often providing explainable suggestions \cite{sun2018, yang2023}.
    \item \textbf{Complex Reasoning}: KGEs contribute to complex reasoning by providing a structured, embeddable representation that can be combined with explainable AI techniques for more robust and transparent inference \cite{additional\_paper\_3}.
\end{itemize}

In summary, the motivation for knowledge graph embedding is multifaceted, stemming from the imperative to overcome the inherent limitations of sparse, symbolic knowledge representations while simultaneously enhancing their utility and interoperability within modern AI ecosystems. By converting complex symbolic problems into efficient vector operations, KGEs enable scalability, capture nuanced semantics, handle incompleteness, and integrate smoothly with advanced machine learning paradigms. This strategic shift from explicit, symbolic knowledge representation to implicit, dense vector representations has been pivotal in unlocking the full potential of knowledge graphs, solidifying KGE as a critical and continuously evolving subfield in knowledge representation and artificial intelligence.
\subsection{Scope and Structure of the Review}
\label{sec:1\_3\_scope\_\_and\_\_structure\_of\_the\_review}

This literature review offers a comprehensive and systematic exploration of Knowledge Graph Embedding (KGE), charting the field's intellectual trajectory from its foundational theoretical underpinnings to its most advanced architectural innovations and diverse real-world applications. KGE models are designed to transform sparse, symbolic knowledge into dense, continuous vector representations, thereby enabling more efficient computation and sophisticated reasoning within AI systems \cite{choudhary2021, layer\_1, community\_1}. This transformation serves dual purposes: providing effective encodings for various data mining tasks and facilitating robust link prediction within knowledge graphs \cite{kge\_purposes}. Our review's scope is deliberately broad, encompassing the evolution of KGE models, their practical considerations, and their diverse utility across domains. We adopt a pedagogical progression, beginning with core concepts and gradually advancing to cutting-edge developments, ensuring a coherent narrative that illuminates the motivations behind successive innovations and the continuous efforts to overcome inherent limitations. This structured approach provides a clear roadmap for understanding the complex and rapidly evolving landscape of KGE research, as highlighted by numerous comprehensive surveys in the field \cite{layer\_1, community\_1, community\_2, community\_3}.

The review commences by establishing the foundational context of knowledge graphs and the compelling motivations for their embedding into continuous vector spaces, addressing the inherent limitations of symbolic representations in terms of scalability, semantic richness, and handling incompleteness \cite{layer\_1, community\_1}. This sets the stage for Section 2, "Foundational KGE Models and Geometric Paradigms," which delves into the bedrock of the field. This section examines early geometric and algebraic models that laid the groundwork for representing entities and relations in continuous spaces \cite{community\_0, community\_1}. While these pioneering approaches significantly advanced the field by offering improved efficiency over purely symbolic methods, they often faced limitations in capturing complex relational patterns such as symmetry, antisymmetry, composition, or the nuances of weighted relationships \cite{weext\_weighted\_kge}. This critical evaluation of their expressiveness and computational trade-offs highlights the impetus for more sophisticated mathematical formulations.

Building upon these foundations, Section 3, "Deep Learning Architectures for Knowledge Graph Embedding," explores the significant paradigm shift towards leveraging advanced deep learning models. This section details how Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and Transformer models have been adapted to learn more expressive and context-aware representations \cite{community\_0, community\_2}. Unlike purely geometric models that rely on predefined transformations, these architectures automatically extract intricate structural patterns and model complex, non-linear interactions, pushing the boundaries of KGE performance. However, this enhanced expressiveness often introduces increased computational demands and requires larger datasets for effective training, presenting a new set of practical challenges that the field continues to address.

Recognizing that purely structural information can be insufficient for comprehensive knowledge representation, Section 4, "Enriching KGE: Auxiliary Information, Rules, and Multi-modality," investigates methods that integrate diverse external knowledge sources and logical constraints \cite{community\_0, community\_3}. This includes leveraging entity types and attributes, incorporating explicit logical rules, and fusing multi-modal data (e.g., text, images). This critical development addresses data sparsity, enhances semantic understanding, and improves reasoning capabilities, moving KGE models towards more robust and interpretable representations in complex, real-world scenarios. This integration of heterogeneous information is crucial for overcoming the limitations of relying solely on the graph's topological structure.

The review then addresses the dynamic, evolving, and often distributed nature of real-world knowledge graphs in Section 5, "Dynamic, Inductive, and Distributed KGE." This section covers methods for handling temporal changes, learning embeddings for unseen entities (inductive learning), and efficiently updating models with new facts (continual learning) \cite{community\_1, community\_4}. Furthermore, it explores emerging federated and privacy-preserving KGE approaches that enable collaborative learning across decentralized data sources \cite{community\_1, 8c93f3cecf79bd9f8d021f589d095305e281dd2f}. These advancements are crucial for making KGE models adaptable, scalable, and secure in operational environments, moving beyond static and centralized assumptions to meet the demands of modern knowledge management systems.

Section 6, "Practical Considerations: Efficiency, Robustness, and Evaluation," shifts focus to the critical practical challenges in deploying and evaluating KGE models. Drawing insights from comprehensive comparative analyses and surveys \cite{madushanka2024}, this section examines strategies for improving computational efficiency, reducing memory footprint, and ensuring scalability for massive knowledge graphs \cite{community\_1, community\_6}. It delves into methods for enhancing model robustness against data imperfections and optimizing training processes, recognizing that factors beyond scoring functions, such as loss functions, hyperparameters, and negative sampling strategies, significantly impact model efficiency and accuracy \cite{kge\_training\_components}. A significant part is dedicated to the importance of rigorous evaluation, standardized benchmarking, and reproducibility, which are essential for translating theoretical advancements into reliable and trustworthy real-world applications \cite{community\_0, community\_6}. The varied performance reported across benchmarks underscores the need for robust evaluation protocols and transparent reporting practices.

Finally, Section 7, "Applications and Real-World Impact of KGE," showcases the diverse and significant real-world utility of KGE across various AI applications. This includes fundamental tasks like link prediction and knowledge graph completion \cite{community\_0, community\_1}, as well as more complex applications such as entity alignment \cite{layer\_1, community\_5}, question answering, and recommender systems \cite{layer\_1, community\_3}. This section demonstrates how KGE models are leveraged to solve complex problems, not only for traditional tasks but also for advanced data representation and analysis, enabling semantic queries and data exploration based on learned semantic structures \cite{kge\_semantic\_structures}. This illustrates their transformative potential in different industries and highlights the tangible benefits of embedding techniques in modern AI systems. The review concludes in Section 8 with a summary of key developments, a discussion of persistent open challenges, theoretical gaps, and emerging trends, including ethical considerations, charting a course for future research and development in the field \cite{community\_1, community\_6}. This structured approach provides a comprehensive roadmap for understanding the complex and rapidly evolving landscape of KGE research.


\label{sec:foundational_kge_models_and_geometric_paradigms}

\section{Foundational KGE Models and Geometric Paradigms}
\label{sec:foundational\_kge\_models\_\_and\_\_geometric\_paradigms}

Building upon the motivation for Knowledge Graph Embeddings (KGE) to overcome the limitations of sparse symbolic representations, this section delves into the foundational models that first translated entities and relations into continuous vector spaces. It explores the bedrock of KGE research, focusing on early and influential models that laid the theoretical and practical groundwork for representing knowledge in a machine-understandable, continuous format. Central to these pioneering efforts are geometric and algebraic paradigms, where relations are conceptualized as transformations or interactions within these learned embedding spaces. The journey began with simpler translational models, such as TransE and its extensions like TransH, TransR, and TransD, which effectively modeled relations as vector translations from head to tail entities \cite{bordes2013, wang2014, ji2015}. These initial breakthroughs significantly improved upon purely symbolic methods by offering enhanced efficiency and a basic level of semantic expressiveness, establishing a fundamental paradigm for KGE \cite{community\_0, community\_1}. However, the inherent limitations of simple translations in capturing complex relational patterns, such as symmetry, antisymmetry, inversion, or composition, quickly became apparent. This spurred the development of more sophisticated approaches, leading to the emergence of rotational and complex space embeddings. Models like RotatE, which define relations as rotations in complex vector spaces, demonstrated superior capabilities in modeling these intricate logical patterns, moving beyond the constraints of linear transformations \cite{sun2019}. Further innovations explored diverse geometric and algebraic structures, including embeddings on Lie groups, different metric choices, and advanced transformations like Householder parameterization, continuously refining the mathematical foundations of KGE for greater expressiveness and theoretical soundness \cite{community\_2}. Collectively, these foundational geometric and algebraic models not only formed the initial theoretical and practical basis for KGE but also highlighted the continuous quest to capture diverse relational semantics, thereby setting the stage for subsequent advancements in the field.

\subsection{Core Translational Models and Extensions}
\label{sec:2\_1\_core\_translational\_models\_\_and\_\_extensions}

The advent of knowledge graph embedding (KGE) marked a significant shift from purely symbolic knowledge representation to continuous vector spaces, offering enhanced efficiency and expressiveness for various AI tasks. Pioneering this paradigm were the translational models, with \textit{TransE} \cite{bordes2013} establishing the foundational principle where the embedding of a head entity ($h$) plus the relation vector ($r$) should approximate the embedding of the tail entity ($t$), i.e., $h+r \approx t$. This elegant simplicity, however, struggled with complex relational patterns such as one-to-many, many-to-one, and many-to-many relations \cite{wang2014, lin2015}. In such scenarios, a single relation vector could not adequately distinguish between multiple valid tail entities for a given head, or vice-versa, leading to a collapse of entity embeddings and reduced discriminative power, as the model would be forced to place multiple distinct entities close to each other in the embedding space \cite{wang2014}. This limitation meant that \textit{TransE} often failed to capture the nuanced semantics where an entity might participate in a relation in different "roles" or contexts.

To address these limitations, \textit{TransH} introduced a significant refinement by modeling relations as translations on relation-specific hyperplanes \cite{wang2014}. Instead of a single relation vector directly translating entities in the global embedding space, \textit{TransH} projects entity embeddings onto a hyperplane orthogonal to the relation vector, and then performs the translation within that specific hyperplane. This mechanism allows a single entity to have different representations when involved in different relations, effectively mitigating the issues of one-to-many and many-to-one relations. For instance, an entity like "Barack Obama" can be projected differently when involved in relations like "place\\_of\\_birth" (to Hawaii) versus "spouse" (to Michelle Obama), preventing the embedding of "Barack Obama" from being forced to be close to both "Hawaii" and "Michelle Obama" simultaneously in the original space. While \textit{TransH} offered a substantial improvement in expressiveness over \textit{TransE} with comparable computational complexity, it still faced challenges in handling the diversity of entities, as the projection for a relation was fixed regardless of the specific entities involved \cite{ji2015}. Furthermore, \textit{TransH} also introduced a novel Bernoulli negative sampling strategy, which leverages relation mapping properties (tails per head, heads per tail) to construct more effective negative examples during training, thereby reducing the generation of false negative labels and improving learning robustness \cite{wang2014}.

Building upon \textit{TransH}, \textit{TransR} \cite{lin2015} introduced a more sophisticated approach by mapping entities from the entity space to a relation-specific space before performing the translation. In \textit{TransR}, each relation $r$ is associated with a relation vector $r$ and a dedicated projection matrix $M\_r$. For a triplet $(h, r, t)$, the head entity $h$ and tail entity $t$ are first projected into the relation space using $h\_r = h M\_r$ and $t\_r = t M\_r$, and then the translational assumption $h\_r + r \approx t\_r$ is applied within this relation-specific space. This mechanism allows entities to have different representations in different relation spaces, which significantly improves the model's ability to capture the nuanced semantics of diverse relations and entities compared to \textit{TransE} and \textit{TransH} \cite{lin2015}. By using relation-specific projection matrices, \textit{TransR} could better capture the contextual roles of entities. However, a major drawback of \textit{TransR} was its high computational cost and large number of parameters, as each relation required its own dense projection matrix. This made it less scalable for knowledge graphs with a vast number of relations \cite{ji2015}. Moreover, while it allowed for relation-specific projections, these projections were still fixed for all entities within a given relation, potentially limiting its flexibility in capturing fine-grained entity diversity. An extension, \textit{TransHR}, further explored this direction by transforming hyper-relations into individual vectors, demonstrating the continuous effort to adapt projection-based models to more complex data structures \cite{zhang2018transhr}.

Addressing the computational and flexibility issues of \textit{TransR}, \textit{TransD} \cite{ji2015} further refined the translational paradigm by introducing dynamic mapping matrices. Unlike \textit{TransR}'s fixed relation-specific projection matrices, \textit{TransD} constructs projection matrices dynamically from \textit{both} entity and relation vectors. Specifically, each entity $e$ and relation $r$ is represented by two vectors: one for its meaning ($e, r$) and another for constructing its dynamic mapping matrix ($e\_p, r\_p$). The projection matrices are then formed as outer products of these auxiliary vectors, allowing for highly flexible, entity-specific projections. This dynamic approach enables \textit{TransD} to better capture the diversity of entities within a relation, as the projection adapts to the specific head or tail entity involved. Crucially, \textit{TransD} achieves this enhanced expressiveness while maintaining parameter efficiency by avoiding large, dense matrix storage; the projection matrices are implicitly defined, making it more scalable for large knowledge graphs compared to \textit{TransR} \cite{ji2015}.

Collectively, these pioneering translational models—\textit{TransE}, \textit{TransH}, \textit{TransR}, and \textit{TransD}—established a fundamental paradigm for KGE. They represent a crucial intellectual trajectory, systematically addressing limitations of prior models to enhance expressiveness and efficiency. From \textit{TransE}'s foundational simplicity, which laid the groundwork, to \textit{TransH}'s hyperplane projections, \textit{TransR}'s relation-specific spaces, and \textit{TransD}'s dynamic, entity-aware projections, each model built upon its predecessors to better capture complex relational patterns like one-to-many and many-to-one mappings \cite{wang2014, lin2015, ji2015}. These works significantly advanced the field by offering a robust framework for tasks like link prediction, moving beyond purely symbolic methods to continuous representations \cite{bordes2013}. While subsequent research would introduce alternative geometric operations, such as rotations in complex spaces with models like \textit{RotatE} \cite{sun2018}, the translational family laid the essential groundwork for efficiently representing relational knowledge. Their contributions in conceptualizing relations as transformations in embedding spaces continue to influence modern KGE research, forming a crucial stepping stone towards more sophisticated and context-aware embedding techniques.
\subsection{Rotational and Complex Space Embeddings}
\label{sec:2\_2\_rotational\_\_and\_\_complex\_space\_embeddings}

While translational models like TransE \cite{wang2014} and its extensions such as TransH \cite{wang2014} and TransD \cite{ji2015} offered significant advancements in knowledge graph embedding (KGE), their inherent limitations in capturing complex relational patterns, particularly those involving symmetry, antisymmetry, inversion, and composition, became apparent. These simpler geometric operations often struggled to differentiate between various relation types or to model multi-hop reasoning effectively. This limitation spurred the development of a new class of KGE models that leverage rotations in complex or higher-dimensional spaces, offering more nuanced and powerful transformations.

A pivotal contribution in this area is RotatE \cite{sun2018}, which defines each relation as a rotation from the head entity to the tail entity in a complex vector space. This elegant formulation inherently allows RotatE to model and infer a rich set of relational patterns. For instance, symmetric relations can be represented by a rotation of 0 or $\pi$ (or multiples thereof), antisymmetric relations by a non-zero rotation, and inverse relations by a rotation that is the negative of the original. Crucially, composition of relations (e.g., $r\_1 \circ r\_2 = r\_3$) naturally translates to the composition of rotations in the complex plane, making it highly effective for complex logical patterns. As highlighted in the thematic taxonomy, RotatE represents a significant step within the "Advanced Geometric and Temporal Models" group, moving beyond the simpler translational assumptions to capture richer semantics \cite{dasgupta2018}. Its success demonstrated that complex number spaces could provide a more expressive embedding environment without drastically increasing computational overhead compared to some projection-based methods.

Building upon the success of complex-space rotations, researchers explored extensions to higher-dimensional spaces. Rotate3D \cite{gao2020} maps entities into a three-dimensional Euclidean space and models relations as rotations within this space. A key advantage of Rotate3D lies in its ability to capture non-commutative composition, which is essential for accurate multi-hop reasoning where the order of relations matters. While RotatE effectively models composition in 2D complex space, Rotate3D's exploration of 3D rotations provides a richer algebraic structure, allowing for more intricate transformations. However, the increased dimensionality and complexity of 3D rotations can introduce challenges in optimization and parameter efficiency, a common trade-off between expressiveness and computational cost that many KGE models face \cite{sachan2020, wang2021}.

Further pushing the boundaries of algebraic structures, Contextualized Quaternion Embedding (ConQuatE) \cite{chen2025} leverages quaternions, a four-dimensional extension of complex numbers. Quaternions offer even greater expressive power for rotations and can capture more diverse relational contexts, specifically addressing the challenge of polysemy in knowledge graphs. Polysemy, where an entity can have different semantic characteristics depending on the relation it participates in, is a significant limitation for models that assign a single, static vector to each entity. ConQuatE enriches entity representations by incorporating contextual cues from various connected relations through efficient quaternion transformations, without requiring extra information beyond original triples. This represents a forward-looking development, as indicated by its 2025 publication, showcasing the continuous evolution towards more sophisticated algebraic structures to handle nuanced semantic problems.

These rotational and complex space embeddings are part of a broader trend within the "Geometric and Algebraic KGE Models for Complex Relations" subgroup, which continuously seeks to enhance model expressiveness \cite{cao2022, ge2023}. Other notable innovations include HousE \cite{li2022}, which employs Householder parameterization (a type of reflection and rotation) to achieve superior capacity in modeling relation patterns and mapping properties, and CompoundE \cite{ge2022} and CompoundE3D \cite{ge2023}, which generalize by combining translation, rotation, and scaling operations. These models collectively demonstrate a progression towards more intricate, cascaded geometric manipulations. The recent GoldE \cite{li2024} further generalizes orthogonal parameterization, aiming for a unified framework that captures both logical patterns and topological heterogeneity, extending the capabilities of these rotational approaches.

Despite their strengths, these models are not without limitations. While RotatE and its extensions significantly improve performance on complex patterns, they can still struggle with specific theoretical deficiencies. For instance, MQuinE \cite{liu2024} identifies a "Z-paradox" in some popular KGE models, where expressiveness is degraded, and proposes a new model to mitigate this, suggesting that even advanced geometric models may have subtle theoretical gaps. Furthermore, while models like Fully Hyperbolic Rotation \cite{liang2024} aim to fully exploit non-Euclidean spaces for hierarchical structures, they still need to demonstrate consistent superiority across all relation patterns, as some complex operations in hyperbolic space can be more involved. SpherE \cite{li2024} offers another innovative extension by representing entities as spheres within a rotational framework, specifically targeting many-to-many relations and set retrieval, indicating a diversification of geometric primitives to address specific challenges. The experimental setups for these models often focus on link prediction, which, while standard, may not fully capture the nuances of all the complex patterns they aim to model, especially for tasks like set retrieval or multi-hop logical reasoning, highlighting a potential gap in comprehensive evaluation.
\subsection{Other Geometric and Algebraic Innovations}
\label{sec:2\_3\_other\_geometric\_\_and\_\_algebraic\_innovations}

Beyond the foundational translational and rotational paradigms, the field of Knowledge Graph Embedding (KGE) has seen a continuous exploration of diverse geometric spaces and advanced algebraic transformations to enhance model expressiveness, theoretical soundness, and robustness. This pursuit aims to circumvent inherent limitations of simpler models, such as regularization problems or insufficient capacity to capture intricate relational patterns \cite{ge2023, cao2022}.

One significant line of innovation involves embedding entities and relations within non-Euclidean spaces, particularly Lie groups. \cite{ebisu2017} introduced \texttt{TorusE}, a pioneering model that embeds objects on a torus, a compact Lie group, to address the regularization issues prevalent in early translation-based models like \texttt{TransE}. \texttt{TransE} often forces entity embeddings onto a sphere in Euclidean space, which can warp representations and hinder their ability to accurately fulfill the translational principle. By leveraging the inherent properties of a torus, \texttt{TorusE} naturally avoids the need for explicit regularization, leading to more stable and accurate embeddings. This approach highlights a fundamental shift in thinking about the underlying geometry of the embedding space, moving beyond standard vector spaces to exploit the mathematical properties of more complex manifolds. While \texttt{TorusE} demonstrated superior performance and scalability, its adoption of a non-Euclidean space introduces a different set of mathematical complexities compared to Euclidean models, potentially affecting ease of implementation or interpretability for researchers less familiar with Lie group theory.

Complementing the exploration of novel embedding spaces, other works have critically examined the choice of metric within these spaces. \cite{yang2021} investigated the impact of the metric itself, proposing the "Cycle metric" as an alternative to the widely used Minkowski metric in their model, \texttt{CyclE}. They argued that the oscillation property of periodic functions, which underpins the Cycle metric, offers superior expressiveness for KGE. Their quantitative analysis demonstrated that a smaller function period correlates with better expressive ability, suggesting that the fundamental way distances and relationships are measured significantly influences model performance. While \texttt{TorusE} changes the \textit{shape} of the embedding space, \texttt{CyclE} focuses on the \textit{rules} governing distances within a space, both aiming to refine the mathematical foundations for better representation. The theoretical justification for \texttt{CyclE}'s metric choice is compelling, but its practical benefits are contingent on the specific characteristics of the knowledge graph and the types of relations it contains.

Further advancements have come from employing more sophisticated algebraic transformations. \cite{li2022} introduced \texttt{HousE}, a powerful KGE framework that utilizes Householder parameterization. This involves two types of Householder transformations: rotations for superior capacity in modeling relation patterns and projections for handling complex relation mapping properties. Theoretically, \texttt{HousE} is capable of simultaneously capturing crucial relation patterns (e.g., symmetry, antisymmetry, inversion, composition) and mapping properties (e.g., 1-to-N, N-to-1, N-to-N), and generalizes existing rotation-based models to higher-dimensional spaces. This represents a significant leap from simpler operations, offering a richer set of transformations to encode complex semantic interactions. Similarly, \cite{ge2022} and \cite{ge2023} proposed \texttt{CompoundE} and \texttt{CompoundE3D}, respectively, which leverage compound geometric operations combining translation, rotation, and scaling (and even reflection and shear in \texttt{CompoundE3D}). These models are designed to be highly generalizable, encompassing many existing scoring-function-based KGE models as special cases. The strength of these compound operations lies in their ability to model a broader spectrum of relational patterns through a cascade of transformations, reflecting the multifaceted nature of real-world relations. However, the increased complexity of these compound operations can lead to higher computational costs and a larger parameter space, posing scalability challenges for extremely large knowledge graphs.

The continuous effort to refine mathematical foundations is also evident in models like \texttt{TranS} \cite{zhang2022}, which introduces synthetic relation representations within a transition-based framework to better handle complex scenarios where the same entity pair might have different relations. This addresses a limitation of single-vector relation representations by allowing for more nuanced and context-dependent relation modeling. Furthermore, the development of \texttt{MQuinE} \cite{liu2024} highlights the ongoing theoretical scrutiny of KGE models, as it identifies and provides a "cure" for the "Z-paradox"—a deficiency in the expressiveness of some popular existing KGE models that can lead to significant accuracy drops on challenging test samples. This work underscores the importance of theoretical rigor in ensuring the fundamental soundness of KGE representations. Even earlier, \cite{ji2015}'s \texttt{TransD} represented an initial step in refining mapping matrices dynamically, demonstrating an early awareness of the need for more flexible transformations beyond simple vector additions.

In summary, this diverse array of geometric and algebraic innovations, ranging from embedding on Lie groups \cite{ebisu2017} and exploring novel metrics \cite{yang2021} to employing advanced Householder parameterizations \cite{li2022} and compound transformations \cite{ge2022, ge2023}, collectively showcases the field's relentless pursuit of more expressive, theoretically sound, and robust KGE models. While these advancements often introduce increased model complexity, they are crucial for pushing the boundaries of what KGE can achieve in capturing the intricate and multifaceted nature of knowledge.


\label{sec:deep_learning_architectures_for_knowledge_graph_embedding}

\section{Deep Learning Architectures for Knowledge Graph Embedding}
\label{sec:deep\_learning\_architectures\_for\_knowledge\_graph\_embedding}

Building upon the foundational geometric and algebraic paradigms discussed in the previous section, which primarily modeled relations as transformations in continuous vector spaces, this section marks a significant paradigm shift in Knowledge Graph Embedding (KGE) research. While models like TransE and RotatE provided crucial initial frameworks for representing entities and relations \cite{bordes2013, sun2018}, their inherent limitations in capturing highly intricate, non-linear, and hierarchical structural patterns within knowledge graphs became increasingly apparent. The emergence of advanced deep learning architectures has revolutionized KGE, enabling the learning of far more expressive and context-aware representations directly from data \cite{community\_0, community\_1, community\_2}.

This section delves into how Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and Transformer models have been innovatively adapted to address these challenges. CNNs, initially renowned for image processing, are explored for their ability to extract local features and model intricate interactions between entity and relation embeddings, automatically discovering complex patterns that geometric models struggle with. Subsequently, we examine GNNs, which are inherently suited for graph-structured data. Through message passing and aggregation mechanisms, GNNs effectively capture rich structural information and neighborhood context, moving beyond simple triplet-based interactions to leverage the graph's full topology and relational paths. Finally, the discussion extends to Transformer-based KGE models, which harness powerful self-attention mechanisms to capture long-range dependencies and contextualized representations, pushing the state-of-the-art in modeling complex contextual information and multi-structural features \cite{8f096071a09701012c9c279aee2a88143a295935, d899e434a7f2eecf33a90053df84cf32842fbca9}. By detailing these architectural advancements, this section highlights how deep learning has pushed the boundaries of KGE performance, enabling the automatic extraction of features and the modeling of complex, non-linear relationships, thereby providing a more nuanced and powerful understanding of knowledge graphs.

\subsection{Convolutional Neural Networks (CNNs) for KGE}
\label{sec:3\_1\_convolutional\_neural\_networks\_(cnns)\_for\_kge}

Convolutional Neural Networks (CNNs) have emerged as a powerful deep learning paradigm for Knowledge Graph Embedding (KGE), offering a distinct advantage in extracting local features and modeling intricate interactions between entity and relation embeddings. Unlike traditional geometric models that rely on predefined transformations, CNNs automatically learn complex, non-linear feature patterns, thereby enhancing the expressiveness of KGE models \cite{cao2022, ge2023}. This approach often treats the concatenated embeddings of head entities and relations as a 2D input, applying convolutional filters to capture diverse interaction patterns.

Early applications of CNNs in KGE, such as AcrE \cite{ren2020}, leveraged atrous convolutions and residual learning to effectively increase feature interactions while maintaining parameter efficiency. AcrE demonstrated that by using dilated convolutions, models could capture a wider receptive field without increasing computational cost, addressing the challenge of modeling complex relation patterns with a simpler yet effective architecture. Similarly, the Multi-Scale Dynamic Convolutional Network (M-DCN) \cite{zhang2020} introduced multi-scale dynamic filters to generate richer and more expressive feature embeddings. M-DCN specifically tackled complex relation types, including 1-to-N, N-to-1, and N-to-N, by dynamically adapting filter weights to each relation, thereby enabling a more nuanced understanding of diverse relational semantics. This dynamic filtering mechanism allows the model to learn relation-specific feature extractors, a significant improvement over static filters.

Further advancements integrated more sophisticated neural network components. ReInceptionE \cite{xie2020} combined the power of Inception networks with attention mechanisms to capture both local and global structural information. By employing an Inception network, ReInceptionE increased the interactions between head and relation embeddings, while its relation-aware attention mechanism enriched query embeddings with local neighborhood and global entity context. This hybrid approach showcased the potential of CNNs to not only learn intricate feature interactions but also to incorporate broader graph context, moving beyond purely local pattern recognition. This represents a critical step in bridging the gap between local feature extraction and global structural understanding, which is often a limitation of purely convolutional approaches.

More recent works continue to refine CNN-based KGE. CNN-ECFA \cite{hu2024} proposed a Convolutional Neural Network-based Entity-specific Common Feature Aggregation strategy, demonstrating that aggregating entity-specific common features can significantly improve knowledge graph representation learning. This model focuses on enhancing the feature projection strategies, achieving notable improvements in link prediction tasks by learning more effective representations. Similarly, SEConv \cite{yang2025} introduced a semantic-enhanced KGE model, employing a multi-layer convolutional neural network alongside a self-attention mechanism. SEConv's design aims to extract deeper structural features and generate more expressive embedding representations, particularly highlighting its potential for application in resource-limited consumer electronics and specialized domains like healthcare prediction. The integration of self-attention in SEConv further addresses the limitation of CNNs in capturing long-range dependencies, a common challenge for models relying solely on fixed-size convolutional kernels.

Overall, CNN-based KGE models demonstrate a clear intellectual trajectory towards learning intricate, non-linear feature interactions for improved link prediction. Their strength lies in their ability to automatically discover complex patterns from raw embedding inputs, often achieving state-of-the-art results on benchmark datasets. However, this expressiveness often comes with trade-offs. Compared to simpler geometric models like TransE or RotatE, CNN-based models typically have a higher computational cost and a larger number of parameters, which can impact scalability on extremely large knowledge graphs \cite{ge2023}. Furthermore, while they excel at local feature extraction, their capacity to capture global graph structures or complex logical reasoning patterns might be limited without explicit architectural additions like attention mechanisms or graph neural network components \cite{wu2021, li2023}. The interpretability of the learned features within deep CNN layers also remains a challenge, making it difficult to fully understand \textit{why} certain predictions are made, a common critique across deep learning architectures as noted in the broader analysis of deep learning KGE models. Despite these challenges, the continuous refinement of CNN architectures, including dynamic filtering, multi-scale processing, and attention integration, underscores their enduring relevance and potential for advancing KGE research.
\subsection{Graph Neural Networks (GNNs) and Attention Mechanisms}
\label{sec:3\_2\_graph\_neural\_networks\_(gnns)\_\_and\_\_attention\_mechanisms}

The evolution of Knowledge Graph Embedding (KGE) has seen a significant shift from purely geometric and algebraic models to sophisticated deep learning architectures, with Graph Neural Networks (GNNs) and attention mechanisms emerging as particularly powerful tools. GNNs, through their inherent message passing and aggregation frameworks, are uniquely suited to capture the rich structural information and neighborhood context within knowledge graphs, moving beyond the limitations of simple triplet-based interactions. This capability is crucial for understanding complex relational patterns that span multiple hops and involve diverse entities.

Early forays into leveraging graph structures for KGE, particularly for inductive capabilities, can be seen in models like Logic Attention-based Neighborhood Aggregation \cite{wang2018}. This approach introduced the Logic Attention Network (LAN) to aggregate information from an entity's neighbors, enabling the embedding of unseen entities by considering both rule-based and network-based attention weights. This marked a significant step towards inductive KGE, addressing the challenge of dynamically evolving knowledge graphs where new entities frequently emerge \cite{chen2021, sun2024}. However, these early methods often faced limitations in fully capturing the complex, multi-faceted nature of relations and entity interactions, sometimes relying on simplistic aggregation functions.

The integration of explicit attention mechanisms has further enhanced GNNs in KGE, allowing models to dynamically weigh the importance of different neighbors and relational paths. Graph Attenuated Attention Networks (GAATs) \cite{wang2020}, for instance, incorporate an attenuated attention mechanism to assign varying weights to different relation paths and acquire information from neighborhoods. This allows entities and relations to be learned in a more nuanced, context-dependent manner, overcoming the limitation of assigning uniform weights to all neighbors or paths. While GAATs improve feature extraction, their attention mechanism might still be constrained by a single, aggregated representation.

A more advanced approach is seen in DisenKGAT \cite{wu2021}, which proposes a novel Disentangled Graph Attention Network for KGE. DisenKGAT addresses the challenge of accurately capturing complex relations by leveraging both micro-disentanglement (relation-aware aggregation for diverse component representations) and macro-disentanglement (mutual information regularization to enhance independence). This disentangled approach allows the model to generate more diverse and adaptive representations, which is critical for handling the polysemy and multifaceted nature of real-world relations. The disentanglement not only improves accuracy but also offers enhanced explainability, a growing demand in complex AI systems. Compared to simpler attention models, DisenKGAT's explicit disentanglement aims to prevent a single, monolithic representation from dominating, thereby capturing a broader spectrum of relational semantics.

Beyond specific attention mechanisms, the broader application of GNNs in KGE continues to evolve. Researchers are exploring how to optimize the fundamental message functions within GNNs to improve data adaptability and performance across various KG forms, including n-ary and hyper-relational data \cite{di2023}. This "message function search" aims to automatically discover optimal GNN architectures tailored to specific datasets, addressing the challenge of designing effective GNNs manually. Furthermore, understanding \textit{how} GNN-based KGE models extrapolate to unseen data is a critical area of research. Models like SE-GNN \cite{li2021} explicitly model and merge "Semantic Evidences" at relation, entity, and triple levels through multi-layer aggregation, demonstrating how theoretically informed GNN designs can lead to better extrapolation abilities.

Despite their significant strengths in capturing complex structural information and enabling inductive learning, GNN-based KGE models are not without limitations. Their computational complexity can be substantial, especially with deep GNN layers or intricate attention mechanisms, posing scalability challenges for extremely large knowledge graphs. This is a common trade-off between model expressiveness and computational efficiency, a problem that other research areas in KGE, such as efficiency and compression \cite{modak2024}, actively address. Furthermore, while attention mechanisms provide a degree of interpretability by highlighting important paths or neighbors, the overall "black-box" nature of deep GNNs can still make it challenging to fully understand \textit{why} a particular embedding or prediction is made. The ongoing research into message function search \cite{di2023} and understanding extrapolation \cite{li2021} represents efforts to mitigate these limitations by designing more adaptive, efficient, and interpretable GNN architectures for KGE.
\subsection{Transformer-based KGE Models}
\label{sec:3\_3\_transformer-based\_kge\_models}

The advent of Transformer architectures, initially lauded for their success in natural language processing, has ushered in a new era for Knowledge Graph Embedding (KGE) by offering unparalleled capabilities in capturing long-range dependencies and contextualized representations. This paradigm shift moves beyond the limitations of static embeddings and local feature interactions, enabling models to discern more nuanced semantic relationships within complex knowledge graphs.

Early adaptations, such as CoKE (Contextualized Knowledge Graph Embedding) \cite{wang2019}, pioneered the application of Transformer encoders to KGE. CoKE innovatively treats knowledge graph paths and edges as sequences of entities and relations, allowing the self-attention mechanism to learn dynamic, context-dependent embeddings. Unlike foundational models like TransE \cite{bordes2013} or RotatE \cite{sun2018}, which assign a single static vector to each entity and relation, CoKE's approach enables entities and relations to exhibit different properties based on their specific graph context. This ability to capture intrinsic contextual variations significantly enhances expressiveness, particularly for tasks requiring a deep understanding of relational paths. However, a fundamental challenge arises from the inherent order-invariance of the vanilla self-attention mechanism, which struggles to distinguish between a valid (head, relation, tail) triplet and its semantically incorrect permutations (e.g., (tail, relation, head)). This limitation prevents vanilla Transformers from accurately capturing the directed nature of relational semantics in KGs.

To address this critical issue, Knowformer \cite{li2023} introduced a Position-Aware Relational Transformer specifically designed for KGE. Knowformer overcomes the order-invariance problem by explicitly injecting relational compositions into entity representations, thereby capturing the role of an entity (subject or object) based on its position within a triple. This methodological innovation ensures that the self-attention mechanism correctly distinguishes entity roles and captures the precise relational semantics, a crucial aspect for accurate link prediction and entity alignment. The formal proof provided by \cite{li2023} underscores its theoretical soundness, demonstrating its ability to differentiate between valid and shuffled triplet variants.

More recently, comprehensive graph transformer frameworks have emerged that more deeply integrate graph structures. TGformer \cite{shi2025} represents a significant advancement, being presented as the first general graph transformer framework for KGE that explicitly models both triplet-level and graph-level structural features. As detailed in its technical overview \cite{shi2025}, TGformer moves beyond simply treating KGs as sequences by constructing a context-level subgraph for each predicted triplet, thereby capturing inter-triplet relationships based on shared entities. It employs a Knowledge Graph Transformer Network (KGTN) designed to comprehensively explore multi-structural features and leverage the contextual information of nodes more effectively, discerning valuable entity and relation information that might be overlooked by purely triplet-based or even some graph-based methods. Furthermore, TGformer extends its capabilities to temporal knowledge graphs, addressing the dynamic nature of real-world knowledge, a challenge that many earlier models, including those from the "Foundational KGE Models" section like TorusE \cite{ebisu2017} or CyclE \cite{yang2021}, do not explicitly tackle.

Transformer-based KGE models offer distinct advantages over other deep learning architectures. While Convolutional Neural Networks (CNNs) like AcrE \cite{ren2020} and ReInceptionE \cite{xie2020} excel at extracting local features and modeling interactions within fixed-size receptive fields, and Graph Neural Networks (GNNs) such as DisenKGAT \cite{wu2021} effectively aggregate neighborhood information, Transformers' self-attention mechanism allows for direct modeling of global dependencies across the entire graph or relevant subgraphs. This enables them to capture long-range semantic relationships and complex contextual information more effectively, pushing the state-of-the-art in KGE performance. For instance, \cite{li2023} demonstrates that Knowformer achieves state-of-the-art results on both link prediction and entity alignment tasks, showcasing the power of position-aware attention.

However, the enhanced expressiveness of Transformer-based models often comes with trade-offs. Their increased computational complexity and higher parameter counts can pose scalability challenges, especially for extremely large knowledge graphs, contrasting with the parameter efficiency often sought in earlier models like TransD \cite{ji2015}. While general KGE training optimizations, such as efficient negative sampling strategies, optimized loss functions, and careful hyperparameter tuning, are crucial for all models \cite{kge\_training\_components\_general}, they become particularly salient for the resource-intensive Transformer architectures. Moreover, while models like CoKE \cite{wang2019} and Knowformer \cite{li2023} significantly improve contextual understanding, the interpretability of their complex attention patterns remains an area for further research. Despite these challenges, the innovative application of Transformers to graph structures, as seen in models like TGformer \cite{shi2025}, signifies a crucial step towards developing more robust, adaptive, and inherently capable KGE models that can effectively handle the complexities and multi-structural features of real-world knowledge graphs. This continuous methodological evolution underscores the field's commitment to leveraging cutting-edge deep learning techniques to advance knowledge representation.


\label{sec:enriching_kge:_auxiliary_information,_rules,_and_multi-modality}

\label{sec:enriching\_kge:\_auxiliary\_information,\_rules,\_\_and\_\_multi-modality}

\section{Enriching KGE: Auxiliary Information, Rules, and Multi-modality}
\label{sec:enriching\_kge:\_auxiliary\_information,\_rules,\_and\_multi-modality}

While the preceding section demonstrated how advanced deep learning architectures have significantly enhanced Knowledge Graph Embedding (KGE) by capturing intricate structural patterns and complex interactions within knowledge graphs, a purely structural perspective often falls short in real-world scenarios. Even sophisticated Graph Neural Networks and Transformers, while powerful, can be limited by inherent data sparsity, struggle with explicit logical reasoning, or overlook rich semantic context available beyond the graph's topology. This section therefore shifts focus to advanced KGE approaches that move beyond purely structural information, exploring how diverse external knowledge sources and logical constraints can be integrated to create more robust, semantically rich, and interpretable embeddings.

We delve into three primary avenues for enrichment. First, we examine the incorporation of auxiliary information, such as entity types and attributes, which provides crucial semantic guidance to overcome data sparsity and enhance the discriminative power of embeddings \cite{community\_0, community\_1}. Second, the integration of explicit logical rules and constraints is explored, demonstrating how prior knowledge can improve reasoning capabilities, ensure consistency, and align learned representations with human-understandable patterns \cite{community\_3}. Finally, we investigate multi-modal and cross-domain KGE models that leverage complementary information from various modalities like text and images. This multi-modal fusion not only addresses data incompleteness but also deepens semantic understanding, enabling more comprehensive and nuanced knowledge representation \cite{community\_0, community\_3, a6a735f8e218f772e5b9dac411fa4abea87fdb9c}. By detailing these innovative strategies, this section highlights the critical advancements in making KGE models more robust, interpretable, and capable of handling the complexities of real-world knowledge, ultimately providing a more holistic understanding of entities and relations.

\subsection{Incorporating Auxiliary Information (Types, Attributes)}
\label{sec:4\_1\_incorporating\_auxiliary\_information\_(types,\_attributes)}

The effectiveness of Knowledge Graph Embedding (KGE) models, while significant, can be inherently limited by their reliance solely on the structural information of (head, relation, tail) triplets. Real-world knowledge graphs (KGs) are often incomplete, noisy, and contain rich semantic information beyond simple relational facts. To address these challenges, a crucial direction in KGE research involves incorporating auxiliary semantic information, such as entity types and attributes, to generate more discriminative, robust, and semantically grounded representations. This approach moves beyond the purely structural paradigm, enriching the embedding process with external, well-structured knowledge.

One prominent avenue for incorporating auxiliary information is the utilization of entity types. Entity types provide valuable semantic guidance, allowing models to infer plausible relations and constrain the embedding space. \cite{wang2021} introduced \textbf{TransET}, a novel KGE model that leverages entity types to learn more semantic features. TransET employs a circle convolution mechanism based on entity and entity type embeddings to map head and tail entities into type-specific representations, which are then used with a translation-based scoring function. This approach explicitly models the interaction between an entity and its type, enhancing the semantic richness of the embeddings. Building on this, \cite{he2023} proposed \textbf{TaKE}, a universal Type-augmented Knowledge graph Embedding framework. TaKE is designed to enhance any traditional KGE model by automatically capturing type features without explicit supervision and learning relation-specific type representations. A key innovation in TaKE is its type-constrained negative sampling strategy, which generates more effective negative samples by considering type consistency, thereby improving the training process. While TransET integrates types through a specific convolutional operation, TaKE offers a more general framework and refines the crucial negative sampling step, making it broadly applicable. A common limitation for both, however, is their dependence on the quality and completeness of the entity type hierarchy; if type information is sparse or inaccurate, the benefits may diminish.

Beyond explicit types, richer entity attributes offer a more granular level of auxiliary information, particularly useful for enhancing model robustness against noisy data. \cite{zhang2024} introduced \textbf{AEKE} (Attributed Error-aware Knowledge Embedding), a framework designed to guide KGE learning against the impact of erroneous triples by leveraging entity attributes. AEKE constructs two triple-level hypergraphs, one for the KG's topological structure and another for its attributes. It then calculates a confidence score for each triple based on self-contradiction, consistency between local/global structures, and homogeneity between structures and attributes. These confidence scores adaptively weigh the contributions of potentially erroneous triples during training, making the model more resilient to noise. This approach is particularly valuable as real-world KGs are rarely pristine, and errors can significantly degrade downstream application performance. A critical trade-off here is the reliance on the availability and quality of detailed entity attributes; if attributes are missing or themselves noisy, AEKE's ability to discern erroneous triples might be compromised.

The scope of auxiliary information also extends beyond simple entity types or attributes to more complex, hyper-relational facts. Traditional KGE models typically represent KGs as collections of (head, relation, tail) triplets, which can oversimplify the complex nature of real-world data where facts often come with associated key-value pairs (e.g., (person, born\\_in, city, date=1990)). \cite{rosso2020} addressed this by proposing \textbf{HINGE}, a hyper-relational KGE model that directly learns from such enriched facts. HINGE captures not only the primary structural information encoded in triplets but also the correlation between each triplet and its associated key-value pairs. This allows HINGE to model richer data semantics, significantly outperforming triplet-only methods and even those that transform hyper-relational facts into n-ary representations without considering the underlying triplet structure. The strength of HINGE lies in its ability to process more comprehensive factual representations, but its applicability is contingent on the availability of KGs that explicitly store hyper-relational information.

Furthermore, domain-specific auxiliary information can be particularly powerful. For instance, \cite{hu2024} developed \textbf{SR-KGE}, a GeoEntity-type constrained KGE framework for predicting natural-language spatial relations. This model integrates geoentity types as a constraint to more accurately capture spatial and semantic relations between geographic entities, considering both graph structures and semantic attributes. This exemplifies how specialized auxiliary knowledge, tailored to a specific domain, can significantly enhance the precision of KGE models for niche tasks.

In summary, the integration of auxiliary information, whether through entity types \cite{wang2021, he2023}, attributes \cite{zhang2024}, or hyper-relational facts \cite{rosso2020}, represents a crucial advancement in KGE. These methods collectively enhance KGE by grounding embeddings in richer context, leading to more semantic, discriminative, and robust representations. They address the inherent limitations of purely structural models by providing additional signals that help overcome data incompleteness and noise. While these approaches often introduce increased model complexity and rely on the availability and quality of external data, the trade-off is generally favorable, yielding significant performance improvements, particularly for complex real-world KGs. This direction highlights a shift towards more holistic knowledge representation, where KGE models are not just learning from graph topology but also from the rich semantic metadata that accompanies it.
\subsection{Rule-based and Constraint-driven KGE}
\label{sec:4\_2\_rule-based\_\_and\_\_constraint-driven\_kge}

While purely data-driven knowledge graph embedding (KGE) models excel at capturing statistical patterns from observed triples, they often struggle with interpretability, reasoning capabilities, and ensuring semantic consistency, particularly in sparse or noisy knowledge graphs. To address these limitations, a significant line of research focuses on integrating logical rules and explicit constraints directly into the KGE learning process. These rule-based and constraint-driven approaches aim to inject prior knowledge, guide the embedding space to adhere to logical patterns, and enhance the interpretability of learned representations by aligning them with human-understandable rules \cite{dai2020}. This emphasis on leveraging logical knowledge leads to more robust and semantically coherent embeddings.

Early efforts in this domain focused on enforcing semantic smoothness within the embedding space. For instance, \cite{guo2015} proposed Semantically Smooth Embedding (SSE), which leverages additional semantic information, such as entity categories, to ensure that entities belonging to the same semantic group are embedded closely together. By formulating manifold learning algorithms like Laplacian Eigenmaps as regularization terms, SSE guides the embedding process beyond mere factual compatibility, aiming to discover intrinsic geometric structures. While effective in introducing semantic guidance, SSE relies on pre-defined semantic categories rather than explicit logical rules, and its effectiveness is contingent on the quality and availability of such categorical information.

A pivotal advancement in integrating explicit logical knowledge came with the introduction of methods that leverage soft rules. Traditional logic rules are often hard, meaning they must hold without exception, making them difficult to extract automatically and prone to brittleness in real-world, noisy KGs. Recognizing this, \cite{guo2017} introduced Rule-Guided Embedding (RUGE), a novel paradigm that iteratively integrates soft rules (rules associated with confidence levels) into the embedding learning process. RUGE enables an embedding model to learn simultaneously from observed triples, unlabeled triples (whose labels are iteratively predicted), and automatically extracted soft rules. This iterative guidance allows for a more robust transfer of logical knowledge into the embeddings, demonstrating that even uncertain, automatically extracted rules can significantly improve performance in tasks like link prediction. This approach marked a crucial departure from one-time rule injection, acknowledging the interactive nature between embedding learning and logical inference.

Beyond complex rule mining, even simple structural constraints can yield substantial benefits. \cite{ding2018} explored the potential of using straightforward constraints, such as non-negativity on entity representations and approximate entailment on relation representations. These constraints impose prior beliefs about the structure of the embedding space, leading to more compact and interpretable entity representations and encoding logical entailment regularities between relations. Crucially, these simple constraints improve model interpretability and structure the embedding space without introducing significant computational complexity or scalability issues. However, while effective for basic structural properties, they may not capture the full richness of complex logical patterns that explicit rules can express.

Further refining the integration of soft rules, \cite{guo2020} proposed a highly scalable method for preserving soft logical regularities by imposing rule constraints directly on relation representations. By representing relations as bilinear forms and mapping entities into a non-negative, bounded space, their method derived a rule-based regularization that primarily constrains relation representations. This design significantly improves scalability, as the complexity of rule learning becomes independent of the entity set size, a critical advantage for large-scale KGs. This work builds upon the foundation laid by RUGE, focusing on practical deployment by optimizing for scalability while maintaining the benefits of soft logical guidance.

The most sophisticated integration of logical rules into KGE is exemplified by \cite{tang2022} with RulE (Rule Embedding). RulE proposes a principled framework that learns rule embeddings jointly with entity and relation embeddings in a unified space. This allows for soft logical inference, where a confidence score can be calculated for each rule based on its consistency with observed triples, thereby alleviating the brittleness of hard logic. Furthermore, RulE uses these learned rule embeddings to regularize and enrich the entity and relation embeddings, deeply intertwining logical reasoning with embedding learning. This unified approach not only enhances reasoning capabilities but also makes the KGE models more robust and semantically coherent by ensuring adherence to logical patterns.

Despite the significant advancements, challenges persist. A primary limitation across many rule-based approaches is the efficient and accurate acquisition of high-quality rules. While RUGE and \cite{guo2020} leverage automatically extracted soft rules, the process of rule mining itself can be complex and prone to noise. Balancing the strict adherence to rules with the flexibility to capture exceptions or novel patterns not covered by existing rules remains a delicate trade-off. Over-constraining the embedding space with too many or overly strict rules can limit the model's ability to learn nuanced, data-driven patterns. Moreover, while some methods like \cite{guo2020} address scalability for rule integration, handling a vast number of complex, multi-hop rules efficiently in very large KGs continues to be an area of active research. The theoretical gaps also include a more formal understanding of how "softness" in rules translates into continuous embedding spaces and how to optimally combine symbolic logic with sub-symbolic representations. Nonetheless, the trajectory of this research subgroup clearly demonstrates the immense value of integrating logical knowledge to overcome the inherent limitations of purely data-driven KGE models, paving the way for more intelligent and interpretable AI systems.
\subsection{Multi-modal and Cross-domain KGE}
\label{sec:4\_3\_multi-modal\_\_and\_\_cross-domain\_kge}

The inherent incompleteness and sparsity of knowledge graphs (KGs) often limit the effectiveness of purely structural embedding models. To address these challenges and enrich knowledge representations, a significant research direction has emerged in multi-modal and cross-domain knowledge graph embedding (KGE). These approaches integrate diverse information sources, such as textual descriptions, visual features, or data from disparate domains, to provide more comprehensive, semantically rich, and robust embeddings. This integration leverages complementary information, leading to a deeper understanding of entities and relations that goes beyond the symbolic triplet structure.

Early efforts in multi-modal KGE primarily focused on leveraging textual descriptions associated with entities and relations. For instance, \cite{xiao2016} proposed \textit{SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions}. SSP jointly learns from symbolic triples and textual descriptions, projecting information into a semantic space where text is used to discover semantic relevance. This approach aimed to overcome the "weak-semantic" nature of purely geometric models by grounding embeddings in natural language semantics, thereby providing more precise representations. While SSP marked an important step, its text embedding capabilities were limited by the NLP techniques available at the time.

More recent advancements have capitalized on the power of pre-trained language models (PLMs) to achieve more sophisticated integration of textual semantics. \cite{shen2022} introduced a method for \textit{Joint Language Semantic and Structure Embedding for Knowledge Graph Completion}. This approach fine-tunes PLMs with a probabilistic structured loss, effectively capturing rich semantics from natural language descriptions while simultaneously reconstructing structural information. This represents a significant leap from earlier text-integration methods like SSP, as it leverages the deep contextual understanding of modern PLMs. A key strength of this joint learning paradigm is its ability to significantly improve performance, particularly in low-resource settings where structural information is scarce, by injecting robust semantic cues. However, the computational cost associated with fine-tuning large PLMs can be substantial, posing scalability challenges compared to simpler, non-contextualized models.

Beyond textual descriptions, multi-modal KGE extends to integrating other modalities and domain-specific knowledge. For instance, \cite{zhu2022} demonstrated \textit{Multimodal reasoning based on knowledge graph embedding for specific diseases}. This work constructs Specific Disease Knowledge Graphs (SDKGs) and employs multimodal reasoning using reverse-hyperplane projection, integrating structural, category, and description embeddings. This application in the biomedical domain highlights how combining different modalities can lead to the discovery of new, reliable knowledge, such as drug-gene or gene-disease associations. The strength here lies in its practical utility for specialized fields, where diverse data types (e.g., clinical notes, biological pathways, patient data) are crucial. However, the generalizability of such highly specialized models to other domains without significant re-engineering remains a challenge, and the availability and quality of multimodal data in specific domains can be a limiting factor.

Cross-domain KGE, while related, focuses on integrating knowledge across different, often heterogeneous, domains to address challenges like data sparsity and cold-start problems in applications such as recommendation systems. \cite{liu2023} proposed a \textit{Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation}. This approach aims to efficiently model associations and interactions between items across multiple domains by introducing a "binding rule" to facilitate both homo-domain and hetero-domain item embeddings. This directly addresses the cross-domain cold start problem and enables multi-domain recommendations, which traditional single-domain recommenders struggle with. The methodological strength lies in its ability to leverage knowledge from richer domains to inform recommendations in sparser ones, thereby enriching item representations and improving prediction accuracy. However, the complexity of designing effective "binding rules" that accurately capture nuanced cross-domain interactions without introducing negative transfer remains a critical challenge.

The evolution in this area demonstrates a clear trajectory: from initial attempts to project text into embedding spaces \cite{xiao2016} to sophisticated joint learning frameworks that leverage advanced language models \cite{shen2022}, and further to specialized multimodal reasoning for domain-specific knowledge discovery \cite{zhu2022} and cross-domain applications like recommendation \cite{liu2023}. While these approaches significantly enrich representations and mitigate data sparsity, they introduce new complexities. Methodological limitations often include the difficulty of effectively aligning heterogeneous modalities in a shared embedding space, the increased computational burden of processing and fusing diverse data types, and the challenge of maintaining interpretability as models become more complex. The trade-off between enhanced accuracy and expressiveness versus computational cost and data requirements is particularly pronounced in multi-modal and cross-domain KGE. Future research must continue to explore more efficient fusion mechanisms and robust evaluation protocols for these increasingly complex models.


\label{sec:dynamic,_inductive,_and_distributed_kge}

\label{sec:dynamic,\_inductive,\_\_and\_\_distributed\_kge}

\section{Dynamic, Inductive, and Distributed KGE}
\label{sec:dynamic\_inductive\_and\_distributed\_kge}

While the preceding section demonstrated how integrating auxiliary information, logical rules, and multi-modal data enriches Knowledge Graph Embeddings (KGE) by enhancing their semantic depth and robustness, these advancements largely operate within a static and centralized paradigm. However, real-world knowledge graphs are inherently dynamic, constantly evolving with new facts, entities, and relations, and are often distributed across various sources. This section therefore shifts our focus to address these critical challenges, moving beyond static and centralized assumptions to explore KGE models that are adaptable, scalable, and secure in complex operational environments.

We delve into three crucial dimensions that define modern KGE research. First, we examine \textit{Temporal Knowledge Graph Embedding (TKGE)}, which explicitly models the temporal dynamics of knowledge, capturing how entities and relations evolve over time \cite{community\_4, 83d58bc46b7adb92d8750da52313f060b10f201d}. This is vital for understanding the fluidity of real-world knowledge and enabling reasoning over historical and future events. Second, the section explores \textit{Inductive and Continual KGE}, addressing the challenge of learning embeddings for unseen entities and efficiently updating models with new facts without requiring full retraining \cite{community\_1, 8f096071a09701012c9c279aee2a88143a295935}. These methods are paramount for ensuring KGE models can adapt to the ever-growing nature of knowledge bases and mitigate issues like catastrophic forgetting. Finally, we investigate \textit{Federated and Privacy-Preserving KGE}, which enables collaborative learning across distributed knowledge sources while safeguarding sensitive data \cite{community\_1, 8c93f3cecf79bd9f8d021f589d095305e281dd2f}. This emerging area is crucial for deploying KGE in privacy-sensitive, decentralized settings. Collectively, these advancements are essential for making KGE models truly operational, capable of meeting the demands of modern, dynamic, and distributed knowledge management systems.

\subsection{Temporal Knowledge Graph Embedding (TKGE)}
\label{sec:5\_1\_temporal\_knowledge\_graph\_embedding\_(tkge)}

Temporal Knowledge Graph Embedding (TKGE) models are specifically designed to capture the dynamic evolution of facts within knowledge graphs, moving beyond static representations to understand how entities and relations change over time. This field is crucial for tasks requiring reasoning over time, predicting future events, and understanding the fluidity of real-world knowledge. The methodological evolution in TKGE has progressed from explicitly incorporating time as a distinct dimension to leveraging sophisticated geometric transformations and multi-curvature spaces, increasingly addressing the complexities of dynamic, spatiotemporal, and even fuzzy knowledge.

Early approaches to TKGE focused on explicitly modeling time within the embedding space. \cite{dasgupta2018} introduced HyTE, a hyperplane-based method that associates each timestamp with a corresponding hyperplane. This allows for temporal guidance during knowledge graph inference and the prediction of temporal scopes for facts with missing time annotations. While intuitive and an important early step, HyTE's hyperplane approach might struggle with highly complex, non-linear temporal dependencies inherent in rapidly evolving knowledge graphs, as it relies on a relatively simple geometric interpretation of time. Extending this, \cite{lin2020} proposed a tensor decomposition-based model that treats the entire fact set as a fourth-order tensor (head, relation, tail, time). This provides a robust mathematical framework for handling temporal data by generalizing static tensor-based KGEs. However, the computational cost associated with higher-order tensor operations can be a significant limitation, especially for very dense temporal datasets. \cite{xu2019} introduced ATiSE, which incorporates time using additive time series decomposition, mapping representations into multi-dimensional Gaussian distributions where the mean denotes the expected position and covariance captures temporal uncertainty. This probabilistic view is valuable for real-world noisy data, but the complexity of time series decomposition can also be computationally demanding. More recently, \cite{li2023} presented TeAST, a novel model that maps relations onto an Archimedean spiral timeline, transforming the quadruple completion problem into a 3rd-order tensor completion task. TeAST explicitly aims for interpretability by ensuring relations evolve orderly along the spiral, a distinct advantage over more abstract temporal representations.

A significant advancement in TKGE involves leveraging geometric transformations to model temporal dynamics. \cite{xu2020} proposed TeRo, which defines the temporal evolution of entity embeddings as rotations in a complex vector space. For facts involving time intervals, TeRo uses dual complex embeddings for the beginning and end of relations, effectively capturing dynamic interactions. Building on this, \cite{sadeghian2021} introduced ChronoR, employing k-dimensional rotation transformations parametrized by both relation and time to transform a head entity near its tail. Both TeRo and ChronoR demonstrate strong performance in temporal link prediction, but the interpretability of complex rotations in high-dimensional spaces can be challenging, and the computational complexity of learning these transformations can be substantial, particularly for large-scale KGs. More recent works have extended these geometric transformations to address additional complexities. \cite{ji2024} (FSTRE) utilizes projection and rotation in a complex vector space to embed spatial and temporal information, introducing fine-grained fuzziness through modal lengths of anisotropic vectors. This represents a crucial step towards handling uncertain and dynamic knowledge, reflecting the inherent messiness of real-world data. Further, \cite{ji2024} (Quaternion Embedding) leverages quaternions to jointly embed spatiotemporal entities, representing relations as rotations and exploiting the non-commutative compositional pattern of quaternions for multihop path reasoning and uncertainty modeling. While powerful, the increased algebraic complexity of quaternions can lead to higher model intricacy and training demands.

The latest frontier in TKGE research, particularly in 2024, has seen the emergence of multi-curvature space embeddings to address the limitations of single-space models in capturing intricate TKG structures. \cite{wang2024} (MADE) proposes modeling TKGs in multi-curvature spaces, including Euclidean, hyperbolic, and hyperspherical geometries. MADE introduces an adaptive weighting mechanism to assign different weights to these spaces in a data-driven manner, strengthening ideal spaces and weakening inappropriate ones, along with a temporal regularization for timestamp smoothness. Similarly, \cite{wang2024} (IME) integrates "space-shared" properties to learn commonalities across spaces and "space-specific" properties to capture characteristic features, also proposing an Adjustable Multi-curvature Pooling (AMP) approach. Both MADE and IME demonstrate state-of-the-art results by acknowledging that TKGs often contain interwoven hierarchical, ring, and chain structures that no single curvature space can optimally capture. A common limitation for these multi-curvature models is the increased complexity of optimizing embeddings across potentially disparate geometric spaces, and the computational overhead associated with managing these different curvatures. The interpretability of embeddings in such hybrid spaces also presents a significant challenge, as the theoretical advantages must translate into practical, understandable insights.

In summary, TKGE research has evolved from explicit, structured temporal modeling to sophisticated geometric transformations and adaptive multi-curvature embeddings. The field consistently grapples with the trade-off between increased model expressiveness (e.g., handling fuzziness, spatiotemporal data, multi-curvature geometries) and the associated computational complexity and interpretability challenges. While models like HyTE provided foundational temporal awareness, newer approaches like MADE and the quaternion-based embeddings push the boundaries by offering more nuanced and robust representations for the complex, dynamic, and often uncertain nature of real-world knowledge graphs. The ongoing challenge lies in developing scalable, efficient, and interpretable models that can seamlessly integrate all these facets to enable comprehensive reasoning over evolving knowledge.
\subsection{Inductive and Continual KGE}
\label{sec:5\_2\_inductive\_\_and\_\_continual\_kge}

Real-world knowledge bases are inherently dynamic, constantly evolving with the emergence of new entities, relations, and facts. Traditional Knowledge Graph Embedding (KGE) models are predominantly transductive, meaning they can only generate embeddings for entities observed during training. This limitation necessitates expensive full retraining whenever new information arises, rendering them impractical for dynamic environments. To address this, research has focused on inductive KGE, which can embed unseen entities, and continual KGE, which efficiently updates models with new facts while preserving previously learned knowledge, thereby mitigating catastrophic forgetting \cite{liu2024}. These methods are crucial for adapting KGE models to the ever-growing nature of knowledge graphs and ensuring their scalability.

Early efforts in inductive KGE focused on leveraging graph neural network (GNN) principles, particularly neighborhood aggregation. \cite{wang2018} introduced the Logic Attention Network (LAN) for inductive KGE. LAN aggregates information from an entity's neighbors, using both rules- and network-based attention weights to account for the unordered and unequal nature of neighbors. This approach allows for the generation of embeddings for unseen entities by dynamically composing representations from their local graph context. While pioneering, a fundamental limitation of aggregation-based methods like LAN is their reliance on the existence of sufficient neighbors for new entities. Truly novel entities with sparse connections may still pose a challenge, as their embeddings would be poorly informed. Moreover, the inductive capability here is primarily about generating embeddings for \textit{new entities} within an existing graph structure, rather than continually updating the model with \textit{new facts} about existing or new entities over time.

A more sophisticated paradigm for inductive and dynamic KGE has emerged through meta-learning. Meta-learning aims to learn transferable meta-knowledge that can be quickly adapted to new tasks or entities. \cite{chen2021} proposed MorsE, a meta-learning approach for inductive KGE that does not learn direct entity embeddings but rather transferable, entity-independent meta-knowledge. This meta-knowledge is then used to produce embeddings for new entities, enabling a more general inductive capability compared to direct aggregation. MorsE demonstrates superior performance on both in-KG and out-of-KG tasks in inductive settings, highlighting the power of learning "how to learn" embeddings. However, its focus is primarily on the initial embedding of new entities rather than the continuous, incremental updates required for dynamic KGs.

Extending meta-learning to truly dynamic KGE, \cite{sun2024} introduced MetaHG for evolving service ecosystems. This work addresses the continuous updating challenges of service knowledge by incorporating both local (GNN) and potential global (Hypergraph Neural Network, HGNN) structural information from current knowledge graph snapshots. MetaHG refines entity embeddings using a hybrid GNN framework and leverages meta-learning to transfer meta-knowledge for accurate representation of emerging entities. This approach is particularly valuable for scenarios where entities and relations frequently appear and disappear, such as in service ecosystems. A key strength of MetaHG is its attempt to mitigate spatial deformation issues by considering global structural information, which can be a limitation for purely local aggregation methods. However, the complexity of managing hybrid GNNs and hypergraphs, along with the meta-learning process, can introduce computational overheads, and its evaluation on domain-specific datasets might limit direct generalizability to broader KGE benchmarks.

For continual KGE, the primary challenges are efficiently acquiring new knowledge and mitigating catastrophic forgetting—the tendency of models to forget previously learned information when updated with new data. \cite{liu2024} proposed FastKGE, a framework incorporating an incremental low-rank adapter (IncLoRA) mechanism. FastKGE addresses the dual problem of efficient new knowledge acquisition and catastrophic forgetting by isolating and allocating new knowledge to specific layers based on fine-grained influence between old and new KGs. The IncLoRA mechanism then embeds these specific layers into low-rank adapters, significantly reducing the number of trainable parameters during fine-tuning. This approach also features adaptive rank allocation, making LoRA aware of entity importance. FastKGE demonstrates substantial reductions in training time (34-68\\%) while maintaining competitive or even improved link prediction performance, especially on larger, newly constructed datasets. This parameter-efficient adaptation technique offers a compelling trade-off between model performance and computational cost, which is critical for large-scale, dynamic KGs. Compared to general parameter-efficient methods like Entity-Agnostic Representation Learning (EARL) \cite{chen2023}, which focuses on reducing the static parameter count, IncLoRA is specifically designed for the \textit{dynamic update} process, making it highly relevant for continual learning.

In summary, the field of inductive and continual KGE has evolved from foundational neighborhood aggregation methods \cite{wang2018} to more sophisticated meta-learning strategies \cite{chen2021, sun2024} and parameter-efficient adaptation techniques \cite{liu2024}. While aggregation methods provide basic inductive capabilities, they often struggle with truly novel entities or dynamic updates. Meta-learning offers a more generalized approach to transfer knowledge, but its application to continuous updates across diverse KGs is still an active research area. Parameter-efficient methods like IncLoRA represent a significant step towards practical continual learning by balancing the acquisition of new knowledge with the retention of old, mitigating catastrophic forgetting, and ensuring scalability. A persistent challenge across these approaches remains the development of robust evaluation methodologies that accurately reflect performance in dynamic settings, as current benchmarks often fall short of capturing the complexities of real-world evolving knowledge graphs. Furthermore, balancing the expressiveness required for complex relational patterns (e.g., those addressed by \cite{zheng2024}) with the efficiency and adaptability of inductive/continual models remains a key theoretical and practical gap.
\subsection{Federated and Privacy-Preserving KGE}
\label{sec:5\_3\_federated\_\_and\_\_privacy-preserving\_kge}

The proliferation of sensitive data across distributed sources and the escalating demand for privacy-aware AI systems have positioned Federated Learning (FL) as a critical paradigm for Knowledge Graph Embedding (KGE). Federated KGE (FKGE) enables collaborative training of KGE models across multiple clients, each possessing a local knowledge graph (KG), without requiring the centralization of raw, sensitive data \cite{mcmahan2017communication, kairouz2021advances}. This distributed approach is vital for leveraging decentralized knowledge while adhering to stringent privacy regulations. However, extending KGE into the federated setting introduces a unique set of challenges encompassing communication efficiency, personalization for diverse client data, and robust security and privacy guarantees.

A paramount challenge in FKGE is the substantial communication overhead. KGE models often involve large embedding matrices, and the iterative nature of FL requires frequent exchanges of model updates between clients and a central server, leading to significant bandwidth consumption and latency. While general FL techniques often mitigate this by increasing local training epochs, this does not intrinsically reduce the size of parameters transmitted per round. To address this, Zhang et al. \cite{zhang2024\_comm} proposed FedS, a bidirectional communication-efficient framework for FKGE. FedS employs Entity-Wise Top-K Sparsification, where clients dynamically identify and upload only the Top-K entity embeddings exhibiting the most significant changes. Similarly, the server performs personalized aggregation and transmits only the Top-K aggregated embeddings back to each client. FedS further incorporates an Intermittent Synchronization Mechanism to alleviate embedding inconsistency arising from client data heterogeneity. While FedS significantly enhances communication efficiency with minimal performance degradation on various datasets, it inherently involves a trade-off: aggressive sparsification, while reducing communication, might impact the global model's convergence speed or the precision of less frequently updated embeddings. This mirrors broader challenges in KGE compression efforts, where methods like LightKG \cite{wang2021} or those discussed by Sachan et al. \cite{sachan2020} also balance compression ratios with performance retention, often at the cost of some model expressiveness or training stability. Other communication-efficient FL strategies, such as quantization or gradient compression, could also be explored in the FKGE context, offering different trade-offs between compression rate and information loss.

Beyond communication, the semantic disparities inherent in client-specific KGs pose a significant hurdle for FKGE. A "one-size-fits-all" global model, typically derived from a simple arithmetic mean of client embeddings, often fails to adequately capture the unique characteristics and preferences of individual clients. This can lead to a global model that is "inundated with too much noise" when applied locally, compromising embedding quality \cite{zhang2024\_pers}. To tackle this, Zhang et al. \cite{zhang2024\_pers} introduced Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph (PFedEG). This novel approach moves beyond a universal global knowledge by learning personalized supplementary knowledge for each client. PFedEG constructs a "client-wise relation graph" to discern the semantic relevance of embeddings from other clients, enabling each client to amalgamate entity embeddings from its "neighboring" clients based on their affinity. This personalized aggregation, followed by local personalized embedding learning, significantly improves embedding quality by aligning local and global optimization objectives. This personalization is crucial for FKGE's utility in diverse real-world applications, akin to how inductive KGE methods \cite{wang2018, chen2021} adapt to unseen entities by leveraging local context or meta-knowledge transfer, but here applied to client-specific semantic contexts. Alternative personalization strategies in general FL, such as meta-learning based approaches or local fine-tuning of a global model, could also be adapted for FKGE, offering different ways to balance global consistency with local utility.

While FKGE offers inherent privacy benefits by keeping raw data local, it is not immune to security and privacy vulnerabilities. Distributed training environments can be susceptible to malicious actors, particularly through poisoning attacks. Zhou et al. \cite{zhou2024} provided the first systematic exploration of poisoning attacks in FKGE, developing a framework to force victim clients to predict specific false facts. Unlike centralized KGEs, where attackers might directly inject poisoned data, FKGE's local data retention makes direct injection challenging. Instead, attackers in this framework infer targeted relations in the victim's local KG via a novel KG component inference attack, then create poisoned data without direct access to the victim's KG. They inject this poisoned data indirectly through FKGE aggregation by locally training a shadow model and using an optimized dynamic poisoning scheme to generate progressive poisoned updates. Experimental results demonstrate alarming effectiveness, achieving high success rates on various KGE models (e.g., 100\\% on TransE with WN18RR) with minimal impact on the original task's performance.

Beyond poisoning, privacy threats in FKGE also extend to inference attacks, where adversaries attempt to reconstruct sensitive information from shared model updates. Li et al. \cite{li2024\_privacy} conducted a holistic study on privacy threats in FKGE, proposing three new inference attacks that successfully infer the existence of KG triples from victim clients' updates. To counter these threats, they introduced DP-Flames, a novel differentially private FKGE framework that incorporates private selection. DP-Flames offers a better privacy-utility trade-off by exploiting the entity-binding sparse gradient property of FKGE and provides a tight privacy accountant. It also includes an adaptive privacy budget allocation policy to dynamically adjust the defense magnitude throughout training. This work highlights that privacy-preserving distributed training does not inherently guarantee security against sophisticated malicious actors or prevent all forms of data leakage. Other privacy-enhancing technologies, such as Secure Multi-Party Computation (SMC) \cite{bogdanov2008secure} or homomorphic encryption, could also be integrated with FKGE to provide stronger cryptographic privacy guarantees, albeit often at a higher computational cost.

Collectively, these works underscore the multifaceted challenges and emerging solutions in FKGE research. The pursuit of communication efficiency \cite{zhang2024\_comm}, while vital for scalability, often involves a trade-off with the precision of updates, potentially impacting convergence speed or final model quality. Similarly, personalization \cite{zhang2024\_pers}, while enhancing client-specific utility, adds complexity in managing diverse models and ensuring global consistency. A significant theoretical and practical gap remains in developing robust, comprehensive defense mechanisms against both poisoning \cite{zhou2024} and inference attacks \cite{li2024\_privacy}, which is crucial for the trustworthy deployment of FKGE systems. The rapid acceleration of research in this domain, as evidenced by these contemporary papers, reflects the field's urgent response to making KGE practical, secure, and effective in distributed and privacy-sensitive environments. Future work must focus on integrating various privacy-preserving techniques, developing more adaptive and robust defense strategies, and rigorously evaluating their impact on both utility and privacy guarantees.


\label{sec:practical_considerations:_efficiency,_robustness,_and_evaluation}

\label{sec:practical\_considerations:\_efficiency,\_robustness,\_\_and\_\_evaluation}

\section{Practical Considerations: Efficiency, Robustness, and Evaluation}
\label{sec:practical\_considerations:\_efficiency,\_robustness,\_and\_evaluation}

While the preceding sections have explored the foundational models, advanced architectures, and sophisticated mechanisms for handling dynamic, inductive, and distributed knowledge graphs, the successful deployment of Knowledge Graph Embedding (KGE) models in real-world applications hinges on addressing critical practical considerations. This section shifts focus from theoretical expressiveness and adaptability to the operational viability of KGEs, tackling the pervasive challenges that arise when moving from academic benchmarks to industrial-scale systems. We first delve into strategies for enhancing computational efficiency, reducing memory footprint, and ensuring scalability for increasingly massive knowledge graphs, which are paramount for practical utility and widespread adoption. This includes techniques like embedding compression, parameter-efficient learning, and optimized system designs that enable KGEs to operate within resource constraints \cite{community\_1, community\_6}. Furthermore, we examine methods designed to bolster model robustness against inherent data imperfections, such as noise, incompleteness, and class imbalance, and to optimize the intricate training processes required for complex KGE architectures, including advanced negative sampling strategies \cite{community\_2, community\_3}. A significant and often overlooked dimension is the rigorous evaluation, standardized benchmarking, and the imperative for reproducibility in KGE research. These aspects are crucial for fostering scientific progress, enabling fair comparisons, identifying biases, and ultimately ensuring that theoretical advancements translate into reliable, trustworthy, and deployable KGE solutions in diverse real-world scenarios \cite{community\_0, community\_6}.

\subsection{Efficiency, Compression, and Scalability}
\label{sec:6\_1\_efficiency,\_compression,\_\_and\_\_scalability}

The increasing scale of real-world knowledge graphs (KGs) and the computational demands of complex Knowledge Graph Embedding (KGE) models have made efficiency, compression, and scalability paramount concerns. This subsection examines a range of techniques designed to reduce memory footprint, accelerate training and inference, and enable the deployment of KGE models in resource-constrained environments and for massive knowledge bases. The focus has shifted from merely achieving high accuracy to ensuring that KGEs are practically deployable, as highlighted by the community's emphasis on addressing practical challenges and system-level optimization.

One prominent approach to enhancing efficiency is \textbf{knowledge distillation}, where a smaller, more efficient student model is trained to mimic the behavior of a larger, high-performing teacher model. \cite{zhu2020} introduced DualDE, a method that dually distills knowledge from a high-dimensional teacher KGE to a low-dimensional student. DualDE employs a soft label evaluation mechanism to adaptively weight soft and hard labels and a two-stage distillation process, enabling significant parameter reduction (7-15x) and inference speedup (2-6x) with minimal performance degradation. This technique offers a generalizable solution across various KGE architectures, allowing for the deployment of faster and cheaper reasoning models.

Complementing distillation, \textbf{embedding compression} directly addresses the memory footprint of KGEs. \cite{sachan2020} proposed a method to compress the embedding layer by representing each entity as a vector of discrete codes, composing embeddings from these codes. This approach achieved massive compression ratios (50-1000x) with only a minor loss in performance, demonstrating the feasibility of drastically reducing storage requirements. Building on this, LightKG \cite{wang2021} introduced a lightweight framework that stores only a few codebooks and indices, rather than full continuous vectors for every entity. This design not only reduces storage but also boosts inference efficiency through quick look-ups. LightKG further innovates with a dynamic negative sampling method based on quantization, which can be applied to other KGE methods for performance improvement. While both \cite{sachan2020} and \cite{wang2021} tackle storage, LightKG provides a more integrated solution for both storage and inference, coupled with a novel sampling strategy, illustrating a progression towards holistic efficiency. A common limitation, however, is the inherent trade-off: aggressive compression, while vital for scalability, can sometimes lead to a performance drop, which these methods strive to minimize.

\textbf{Parameter-efficient learning} offers another avenue for reducing resource consumption, particularly for growing KGs. Entity-Agnostic Representation Learning (EARL) \cite{chen2023} proposes a novel method that learns embeddings only for a small set of "reserved entities." Embeddings for other entities are then derived from their context using entity-agnostic encoders, which transform distinguishable information from connected relations, k-nearest reserved entities, and multi-hop neighbors. This approach results in a static and lower parameter count, decoupling the growth of the KG from the linear increase in embedding parameters, a crucial benefit for large and evolving knowledge bases.

Beyond model-specific compression, \textbf{novel algorithms and optimized system designs} are critical for large-scale KGE training. \cite{peng2021} introduced a highly efficient KGE learning framework using Orthogonal Procrustes Analysis. By formulating KGE as a closed-form solution, their method enables full-batch learning and non-negative sampling, leading to orders of magnitude reduction in training time and carbon footprint while maintaining competitive performance. This represents a paradigm shift from iterative optimization to a mathematically elegant, direct solution. In contrast to model-specific algorithmic innovations, GE2 \cite{zheng2024} focuses on system-level optimization for KGE training. It proposes a general and efficient learning system that offloads operations from CPU to GPU for high parallelism and introduces COVER, a novel algorithm for managing data swap between CPU and multiple GPUs with minimal communication costs. GE2's general execution model and user-friendly API for negative sampling address fundamental system bottlenecks, achieving significant speedups (over 2x, up to 7.5x) across various models and datasets, thereby accelerating research and development.

For Graph Neural Network (GNN)-based KGEs, which are often computationally intensive, \textbf{graph partitioning strategies} are essential for scalability. CPa-WAC \cite{modak2024} addresses this by employing modularity maximization-based constellation partitioning. This method breaks down large KGs into subgraphs that can be processed separately, reducing memory and training time while crucially aiming to retain prediction accuracy. CPa-WAC demonstrates that meaningful partitioning can enable efficient learning on subgraphs, with performance comparable to training on the entire KG, and up to five times faster training. This approach directly tackles the accuracy-scalability trade-off inherent in processing large graphs with GNNs.

Collectively, these innovations represent a concerted effort to overcome the practical bottlenecks of KGE, making them deployable in resource-constrained environments and for handling ever-growing knowledge bases. The progression from model-level compression and distillation \cite{zhu2020, sachan2020, wang2021} to parameter-efficient learning \cite{chen2023}, novel training algorithms \cite{peng2021}, and sophisticated system-level optimizations \cite{zheng2024, modak2024} reflects a maturing field. This evolution, as noted in the development directions, moves towards specialized, system-level, and scalable approaches, addressing the accuracy-scalability trade-off and fundamental system inefficiencies. While each method offers distinct advantages, the overarching challenge remains balancing the gains in efficiency and scalability with the potential for minor performance degradation, a trade-off that researchers continue to navigate through increasingly sophisticated designs.
\subsection{Robustness and Training Optimization}
\label{sec:6\_2\_robustness\_\_and\_\_training\_optimization}

The effectiveness of Knowledge Graph Embedding (KGE) models in real-world applications is profoundly influenced by their robustness to data imperfections and the efficiency of their training processes. Knowledge graphs (KGs) are inherently noisy, incomplete, and often exhibit imbalanced data distributions, where a small fraction of entities and relations are frequent while the majority are long-tail \cite{zhang2023}. Addressing these challenges is crucial for ensuring that KGE models learn accurate, reliable representations and generalize well to unseen data. This subsection delves into methodologies designed to enhance KGE robustness and optimize training, with a particular focus on probability calibration, noise filtering, weighted training, and the critical role of negative sampling.

A fundamental aspect of model reliability is the trustworthiness of its predictions. While KGE models often achieve high accuracy in link prediction, their associated probability estimates can be uncalibrated, meaning that a predicted probability of 0.8 might not truly correspond to an 80\\% chance of correctness. \cite{tabacof2019} highlights this overlooked problem, demonstrating that popular embedding models indeed produce uncalibrated probabilities. They propose a novel calibration method, applicable even when ground truth negatives are scarce, by employing techniques like Platt scaling and isotonic regression. This post-processing step significantly improves the reliability of KGE outputs, a vital consideration for high-stakes applications, though it does not address the underlying representational limitations of the KGE model itself. Isotonic regression, in particular, showed strong performance, albeit with potential trade-offs in computational complexity compared to simpler methods like Platt scaling.

To combat the pervasive issue of noisy data, which arises from automatic KG construction and updates, \cite{zhang2021} introduced a multi-task reinforcement learning (RL) framework. This framework aims to filter out noisy triples during training, thereby guiding the KGE model to learn more robust representations. By exploiting correlations among semantically similar relations through multi-task learning, the RL agent collectively selects high-quality triples. While effective in enhancing existing KGE models like TransE, DistMult, ConvE, and RotatE in noisy scenarios, this approach introduces additional complexity to the training process due to the RL component. The trade-off lies between the improved robustness gained from cleaner training data and the increased computational overhead and tuning effort required for the RL agent.

Another significant challenge in KGE training is data imbalance, where entities and relations follow a long-tail distribution. Traditional KGE methods often assign equal weights to all triples, leading to under-trained representations for infrequent entities and relations. To address this, \cite{zhang2023} proposed Weighted Knowledge Graph Embedding (WeightE), which employs a bilevel optimization scheme. The inner level focuses on learning reliable embeddings, while the outer level dynamically assigns higher weights to infrequent entities and relations and lower weights to frequent ones. This differential weighting ensures that long-tail components receive adequate attention during training, leading to more balanced and reliable representations across the entire KG. The flexibility of WeightE allows its application to various existing KGE models, offering a general solution to a common practical problem. However, the effectiveness of bilevel optimization can depend on careful hyperparameter tuning for the weighting mechanism.

A crucial and often complex aspect of KGE training optimization revolves around negative sampling. Since KGs typically store only positive facts, generating "negative" (false) triples is essential for contrastive learning, but the quality and efficiency of this process profoundly impact model performance \cite{qian2021, madushanka2024}. The challenge stems from the unknown true distribution of negative facts, making heuristic sampling strategies prone to generating "false negatives" (actual positive triples mistakenly labeled as negative) or "easy negatives" (obviously false triples that provide little learning signal).

Several strategies have emerged to refine negative sampling:
\begin{itemize}
    \item \textbf{Confidence-Aware Sampling:} In noisy KGs, uniform negative sampling can exacerbate issues. \cite{shan2018} introduced a confidence-aware negative sampling method that incorporates negative triple confidence. This approach aims to improve training in noisy environments by assigning a confidence score to negative triples, thereby mitigating the zero-loss and false detection problems associated with uniform sampling. However, the accuracy of this confidence estimation is critical; a flawed estimation could inadvertently introduce new biases or false negatives.
    \item \textbf{Caching Strategies:} To efficiently identify and leverage "hard" negative samples (those that are plausible but incorrect, providing strong learning signals), \cite{zhang2018} proposed NSCaching. Inspired by the observation that hard negatives are crucial but rare, NSCaching uses a cache to track and sample these challenging triples. This method aims to distill the benefits of more complex Generative Adversarial Network (GAN)-based negative sampling techniques into a simpler, more efficient framework, offering a good balance between exploration and exploitation of negative samples without the added complexity of GAN training.
    \item \textbf{Non-Sampling Approaches:} A more radical departure from traditional methods is the "Efficient Non-Sampling Knowledge Graph Embedding" (NS-KGE) framework by \cite{li2021}. This approach avoids negative sampling entirely by considering all negative instances. While this theoretically eliminates the uncertainty inherent in sampling, it significantly increases computational complexity. \cite{li2021} addresses this by leveraging mathematical derivations to reduce the complexity of the non-sampling loss function, aiming for more stable and accurate performance. The applicability of this method is primarily limited to square-loss based KGE models or those whose loss can be converted to a square loss.
    \item \textbf{Modality-Aware Negative Sampling:} As KGE increasingly integrates multi-modal information, negative sampling strategies must adapt. \cite{zhang2023} introduced Modality-Aware Negative Sampling (MANS) for multi-modal KGE. MANS aligns structural and visual embeddings for entities, generating meaningful embeddings for multi-modal KGE while remaining lightweight and efficient. This highlights that the definition of a "good" negative sample becomes more intricate when diverse data modalities are involved, requiring specialized sampling techniques.
    \item \textbf{Comprehensive Reviews:} The importance and diversity of negative sampling methods are underscored by systematic reviews such as \cite{qian2021} and \cite{madushanka2024}. These surveys categorize existing approaches (e.g., static, dynamic, custom cluster-based) and provide valuable insights into their advantages, disadvantages, and open research questions, guiding future advancements in this critical area.
\end{itemize}

In summary, the advancements in robustness and training optimization represent a crucial intellectual trajectory in KGE research, moving beyond purely architectural innovations to address the practical challenges of real-world data. While models like \cite{zhang2021} and \cite{zhang2023} enhance robustness against noise and imbalance, respectively, \cite{tabacof2019} ensures the reliability of model outputs. Simultaneously, the continuous refinement of negative sampling, from confidence-aware strategies \cite{shan2018} and caching \cite{zhang2018} to non-sampling paradigms \cite{li2021} and modality-aware adaptations \cite{zhang2023}, demonstrates a deep commitment to optimizing the fundamental learning process. These efforts collectively ensure that KGE models can learn accurate and generalizable representations even from imperfect data, leading to more reliable predictions and broader applicability across diverse AI tasks. The ongoing tension lies in balancing the increased model and training complexity with the gains in robustness and predictive reliability.
\subsection{Evaluation, Benchmarking, and Reproducibility}
\label{sec:6\_3\_evaluation,\_benchmarking,\_\_and\_\_reproducibility}

The rapid proliferation of Knowledge Graph Embedding (KGE) models necessitates a strong emphasis on rigorous evaluation, standardized benchmarking, and reproducibility to ensure scientific integrity and foster reliable progress. Without these pillars, comparing novel approaches fairly, identifying true advancements, and deploying trustworthy models becomes exceedingly challenging. The KGE community has increasingly recognized this, leading to the development of unified frameworks and large-scale comparative studies that expose critical issues in current research practices.

A significant step towards enhancing reproducibility and facilitating comprehensive experimental studies is the introduction of standardized libraries. \cite{broscheit2020} presented \texttt{LibKGE}, an open-source PyTorch-based library designed for training, hyperparameter optimization, and evaluation of KGE models. Its key strengths lie in its configurability and the decoupling of individual components, allowing researchers to mix and match elements of training methods, model architectures, and evaluation techniques. This modularity is crucial for isolating the impact of specific design choices and ensuring that experiments can be fully reproduced using a single configuration file.

Complementing such frameworks, large-scale comparative studies have been instrumental in shedding light on the state of KGE research. \cite{ali2020}, in their seminal work "Bringing Light Into the Dark," undertook a massive re-implementation and evaluation of 21 KGE models within the unified \texttt{PyKEEN} framework. Their findings were stark: a significant number of previously published results could not be reproduced with their reported hyperparameters, often requiring extensive re-tuning or proving entirely irreproducible. This highlights a critical methodological limitation in the field, where inconsistent implementations, varying training protocols, and undisclosed hyperparameter choices hinder fair comparisons and obscure genuine progress. The study further provides invaluable insights into best practices, optimal configurations, and the crucial interplay between model architecture, training approach, loss function, and the explicit modeling of inverse relations. It underscores that performance is not solely dictated by architectural novelty but by a holistic and carefully configured experimental setup.

Further exposing biases in evaluation, \cite{rossi2020} conducted a comprehensive comparison of 18 state-of-the-art KGE methods for link prediction. Their analysis critically examined the effect of various design choices and, more importantly, highlighted biases inherent in standard evaluation practices. They found that many benchmarks suffer from an over-representation of certain entities in test sets, allowing models to achieve seemingly high scores by focusing on these frequently occurring entities while neglecting the vast majority of the knowledge graph. This phenomenon can lead to an inflated sense of model generalizability, as models might perform well on "easy" cases without truly capturing complex relational patterns across the entire graph. Such findings reveal a fundamental flaw in how experimental setups can inadvertently affect the generalizability of reported results, making it difficult to discern which models genuinely extrapolate well to unseen, diverse data.

The impact of hyperparameters on KGE quality is another critical aspect directly affecting reproducibility and model reliability. \cite{lloyd2022} conducted a Sobol sensitivity analysis to quantify the importance of different hyperparameters, revealing substantial variability in their effects across different knowledge graphs. This implies that hyperparameter configurations optimal for one dataset may be suboptimal or even detrimental for another, making universal "best practices" elusive. More alarmingly, their work identified potential data leakage via inverse relations in the widely used UMLS-43 benchmark, leading to the derivation of a leakage-robust variant. Such discoveries are crucial as data leakage can artificially inflate model performance, leading to unrealistic assumptions about a model's true capabilities and undermining the validity of comparisons. This underscores the need for continuous scrutiny of benchmark datasets themselves, not just the models evaluated on them.

Collectively, these studies underscore the urgent need for more robust practices in KGE research. The heterogeneity of implementations, the challenges in reproducing reported results, the biases in standard evaluation metrics, and the profound impact of hyperparameter choices all contribute to a landscape where fair comparisons and reliable scientific progress are often compromised. Moving forward, the field must embrace higher standards of empirical validation and transparency, leveraging unified frameworks like \texttt{LibKGE} and \texttt{PyKEEN}, adopting more rigorous and unbiased evaluation protocols, and thoroughly analyzing the sensitivity of models to hyperparameter variations. This shift is essential not only for advancing the theoretical understanding of KGE but also for ensuring the trustworthy and effective deployment of KGE models in real-world applications.


\label{sec:applications_and_real-world_impact_of_kge}

\label{sec:applications\_\_and\_\_real-world\_impact\_of\_kge}

\section{Applications and Real-World Impact of KGE}
\label{sec:applications\_and\_real-world\_impact\_of\_kge}

Having explored the foundational models, advanced architectures, and critical practical considerations—including efficiency, robustness, and rigorous evaluation—that underpin the development of effective Knowledge Graph Embedding (KGE) models, this section shifts focus from theoretical advancements and operational viability to the tangible outcomes and transformative potential of KGE in diverse real-world scenarios. It serves as a crucial demonstration of how the sophisticated embedding techniques discussed throughout this review are not merely academic constructs but powerful tools leveraged to address complex challenges across various artificial intelligence applications. We delve into the significant impact of KGE in core tasks such as link prediction and knowledge graph completion, where embeddings facilitate the inference of missing facts and enhance the completeness of knowledge bases \cite{layer\_1, community\_0}. Furthermore, this section highlights KGE's pivotal role in entity alignment, enabling the seamless integration of heterogeneous information from disparate knowledge sources \cite{community\_5}. Beyond these foundational tasks, we examine how KGE models empower more intelligent systems, from enhancing question answering over structured knowledge to personalizing recommender systems, bridging the gap between natural language understanding and structured data \cite{community\_1, community\_3}. Finally, we explore a range of domain-specific applications, illustrating KGE's practical utility in specialized industries, often emphasizing the growing demand for explainable and verifiable insights \cite{community\_2, community\_6}. This section collectively underscores the broad applicability and profound real-world benefits that KGE brings to modern AI systems, showcasing its capacity to unlock deeper insights and drive innovation across numerous fields.

\subsection{Link Prediction and Knowledge Graph Completion}
\label{sec:7\_1\_link\_prediction\_\_and\_\_knowledge\_graph\_completion}

Link prediction and knowledge graph completion (KGC) represent the foundational and most widely studied applications of Knowledge Graph Embedding (KGE), serving as critical benchmarks for evaluating the efficacy of various KGE models. These tasks fundamentally involve inferring unobserved facts, typically in the form of $(h, r, t)$ triples, or completing partially observed ones within a knowledge graph structure. The continuous drive to improve accuracy and robustly handle complex relational patterns in these tasks underpins the utility of KGE for numerous downstream AI applications, making knowledge graphs more complete, robust, and informative.

The evolution of KGE models for link prediction commenced with foundational geometric approaches, primarily translational models (detailed in Section 2.1). TransE \cite{bordes2013}, a pioneering method, established the paradigm of representing relations as translations in an embedding space. While efficient, its limitations in modeling complex relational patterns such as one-to-many, many-to-one, and many-to-many relationships quickly became apparent. Subsequent models like TransH \cite{wang2014} addressed this by projecting entities onto relation-specific hyperplanes, allowing for more nuanced representations without a substantial increase in computational complexity. TransD \cite{ji2015} further refined this by introducing dynamic mapping matrices, enabling entity-specific projections that adapt to different relations, thereby improving expressiveness and scalability. Despite these advancements, translational models often struggled with intricate logical patterns, prompting the exploration of more sophisticated geometric transformations \cite{asmara2023}.

A significant advancement in capturing complex relational patterns, including symmetry, antisymmetry, inversion, and composition, emerged with rotational and complex space embeddings (discussed in Section 2.2). RotatE \cite{sun2018} defined relations as rotations in complex vector spaces, inherently allowing it to model these patterns more effectively than its translational predecessors. This approach demonstrated superior performance in link prediction, particularly for relations exhibiting these properties. Further innovations, such as HousE \cite{li2022}, leveraged Householder transformations, combining rotations and reflections for enhanced modeling capacity. CompoundE \cite{ge2022} and CompoundE3D \cite{ge2023} generalized this by cascading translation, rotation, and scaling operations, aiming for a richer set of transformations to capture diverse relational semantics. The choice of embedding space itself also evolved, with models like TorusE \cite{ebisu2017} exploring Lie groups to circumvent regularization issues, and CyclE \cite{yang2021} investigating the impact of different metrics on expressiveness for KGC. More recently, MQuinE \cite{liu2024} identified and addressed a "Z-paradox" in existing KGE models, where theoretical deficiencies in representing specific relational patterns (e.g., composition) could degrade link prediction performance, proposing a solution for stronger expressiveness.

Beyond geometric transformations, deep learning architectures have profoundly impacted KGE for link prediction (as explored in Section 3). Convolutional Neural Networks (CNNs) have been employed to extract local features and model intricate interactions between entity and relation embeddings. AcrE \cite{ren2020} and ReInceptionE \cite{xie2020} utilized advanced convolutional techniques to capture local-global structural information for improved link prediction. More recent CNN-based approaches like CNN-ECFA \cite{hu2024} and SEConv \cite{yang2025} continue to refine feature aggregation for enhanced performance. Graph Neural Networks (GNNs) and attention mechanisms (Section 3.2) also play a crucial role, with models like DisenKGAT \cite{wu2021} leveraging disentangled graph attention networks to learn diverse component representations, thereby boosting accuracy and explainability for KGC. Transformer-based models (Section 3.3), such as CoKE \cite{wang2019}, Knowformer \cite{li2023}, and TGformer \cite{shi2025}, adapt self-attention mechanisms to capture long-range dependencies and contextualized representations, pushing the state-of-the-art in modeling complex contextual information for link prediction. While these deep learning models often achieve higher accuracy by learning non-linear patterns, they typically incur increased computational complexity and may offer reduced interpretability compared to simpler geometric models.

A persistent challenge in link prediction is the inherent incompleteness and sparsity of knowledge graphs, alongside the need to capture complex relational patterns that no single model can perfectly address. To overcome the limitations of individual KGE models, advanced strategies focus on integrating multiple model strengths. For instance, \cite{gregucci2023} proposes a novel framework that combines query representations from diverse KGE models using an attention mechanism within a spherical geometric framework. This allows the system to dynamically select the most suitable model's representation for a given query, effectively leveraging the distinct geometric transformations of different KGE models to capture a broader range of relational and structural patterns, thereby outperforming individual models in link prediction. Furthermore, real-world knowledge graphs often contain weighted facts, necessitating models capable of handling such nuances. The framework proposed by \cite{wang2022} extends deterministic KGE models to learn embeddings for weighted knowledge graphs, introducing "weighted link prediction" as a crucial evaluation task for these scenarios. For dynamic and evolving KGs, temporal KGE models (Section 5.1) are specifically applied to enable time-aware link prediction, inferring facts that hold true at specific timestamps. Similarly, for unseen entities, inductive KGE methods (Section 5.2) are crucial for generating embeddings and performing link prediction without full retraining.

The reliability of link prediction and KGC heavily depends on robust evaluation and benchmarking. Standard metrics like Mean Reciprocal Rank (MRR) and Hits@k are widely used, but the consistency and reproducibility of experimental results remain a concern \cite{rossi2020, ruffinelli2020}. To address this, frameworks like LibKGE \cite{broscheit2020} and TorchKGE \cite{torchkge} provide standardized, modular, and efficient platforms for training, hyperparameter optimization, and evaluation of KGE models specifically for link prediction. These tools are vital for fostering reproducible research and enabling fair comparisons across different models. Comparative studies, such as that by \cite{ruffinelli2020}, have also highlighted the relationship between KGE models designed for link prediction and those for other data mining tasks, demonstrating that their effectiveness can often translate across applications. The practical deployment of KGE for link prediction also necessitates efficiency and scalability, especially for large graphs. While detailed solutions are in Section 6.1, the need for scalable training, such as that for GNN-based KGE models \cite{cpa-wac\_2023}, is paramount for real-world link prediction tasks.

Despite significant progress, challenges persist, including handling long-tail entities, further mitigating data sparsity, and developing more robust evaluation benchmarks that account for diverse KG characteristics. The continuous trade-off between model expressiveness, computational efficiency, and interpretability remains a central tension. Nonetheless, ongoing research in link prediction and KGC, spanning from refining geometric transformations to leveraging sophisticated deep learning architectures and integrating multi-model strategies, continues to make knowledge graphs more complete, robust, and ultimately more valuable for a wide array of AI applications.
\subsection{Entity Alignment}
\label{sec:7\_2\_entity\_alignment}

\subsubsection*{Entity Alignment}
Entity alignment (EA) is a critical task in knowledge graph (KG) integration, aiming to identify equivalent entities across different, often heterogeneous, knowledge graphs. This process is fundamental for building comprehensive knowledge bases, enriching existing KGs, and enabling sophisticated cross-KG reasoning \cite{dai2020, choudhary2021}. Knowledge Graph Embeddings (KGEs) have emerged as a powerful, data-driven paradigm for EA, transforming the challenge of finding semantic correspondences between disparate knowledge structures into a problem of measuring vector space similarity \cite{yan2022, cao2022}. By representing entities and relations in low-dimensional, continuous vector spaces, KGEs can capture intricate semantic relationships that are difficult to discern from sparse symbolic representations alone.

Early applications of KGEs to EA often adapted foundational translational models, where entities from different KGs are projected into a shared embedding space, and alignment is determined by the proximity of their embeddings. However, a significant challenge for these embedding-based approaches is the inherent scarcity of labeled training data (i.e., pre-aligned entity pairs) \cite{sun2018}. To mitigate this, \cite{sun2018} proposed a \textbf{bootstrapping entity alignment} method. This approach iteratively labels likely entity alignments as training data, thereby expanding the initial seed set and refining the alignment-oriented KG embeddings. While effective in leveraging unlabeled data, a critical limitation of bootstrapping is the potential for error accumulation, where incorrectly labeled alignments in early iterations can propagate and degrade performance in subsequent steps, despite the inclusion of alignment editing methods \cite{sun2018}. This highlights a trade-off between leveraging more data and maintaining high precision in the iterative process.

Further addressing the data scarcity and robustness issues, \textbf{semi-supervised entity alignment} methods have gained prominence. \cite{pei2019} introduced a semi-supervised EA method (SEA) that not only utilizes both labeled and abundant unlabeled entity information but also incorporates an awareness of degree difference in entities. This is crucial because KGEs can be disproportionately affected by entities with vastly different degrees (number of connections), leading to less stable embeddings for low-degree entities or over-emphasized representations for high-degree ones. By performing adversarial training, SEA aims to make the embeddings more robust to these structural imbalances, thereby improving alignment accuracy, particularly for challenging cases involving entities with sparse connections. However, the effectiveness of such adversarial training can be sensitive to hyperparameter tuning and the specific characteristics of the KGs being aligned, potentially limiting its generalizability without careful adaptation.

To capture a more holistic view of entities and enhance alignment accuracy, \textbf{multi-view knowledge graph embedding} frameworks have been developed. \cite{zhang2019} proposed a novel framework that unifies multiple entity features, including entity names, relational structures, and attributes, to learn more robust embeddings for alignment. Traditional KGE-based EA often focused predominantly on relational structure, potentially overlooking rich semantic information embedded in entity names or descriptive attributes. By integrating these diverse views, the multi-view approach aims to overcome the limitations of single-view methods, which might suffer from data sparsity in one view or fail to capture complementary information. The challenge, however, lies in effectively combining these heterogeneous features, as different views may have varying levels of reliability or conflicting signals, requiring sophisticated combination strategies and cross-KG inference methods \cite{zhang2019}. The methodological limitation here is often the heuristic nature of combining these views, lacking a theoretically grounded optimal weighting.

Moving beyond structural and attribute information, approaches like \textbf{OntoEA} \cite{xiang2021} further integrate ontological information to guide entity alignment. Ontologies provide critical meta-information, such as class hierarchies and disjointness axioms, which can significantly enhance semantic consistency and prevent false mappings. By jointly embedding both KGs and their associated ontologies, OntoEA leverages these higher-level semantic constraints to achieve more accurate and logically sound alignments. This represents a significant step towards incorporating richer, human-defined knowledge into the embedding process, addressing a theoretical gap where purely data-driven KGEs might miss explicit logical relationships. However, the reliance on well-defined and consistent ontologies can be a practical constraint, as such resources are not universally available or perfectly aligned across all KGs, which limits its applicability in scenarios with less structured metadata.

Collectively, these methods demonstrate how KGEs provide a powerful, data-driven approach to integrate heterogeneous knowledge sources. The evolution from basic KGE adaptations to sophisticated bootstrapping, semi-supervised, multi-view, and ontology-guided frameworks reflects a continuous effort to address practical limitations like data scarcity and the need for richer semantic understanding \cite{zhu2024}. While KGEs offer significant advantages in scalability and efficiency over traditional symbolic methods, their generalizability across diverse KG characteristics (e.g., density, domain specificity, noise levels) remains an active area of research, as highlighted by experimental reviews \cite{fanourakis2022}. The trade-off often involves balancing the complexity of integrating multiple information sources with the computational cost and the availability of high-quality auxiliary data. Future directions in EA are likely to focus on more robust integration of diverse information, including temporal dynamics and multimodal data, and developing more sophisticated post-alignment modules to refine results and handle uncertainty \cite{zhu2024}.
\subsection{Question Answering and Recommendation Systems}
\label{sec:7\_3\_question\_answering\_\_and\_\_recommendation\_systems}

Knowledge Graph Embeddings (KGEs) have proven instrumental in bridging the semantic gap between natural language and structured knowledge, thereby enabling more intelligent, personalized, and interpretable user interactions in diverse application contexts, particularly in Question Answering (QA) over knowledge graphs and Recommender Systems. The utility of KGE in these domains stems from its ability to represent complex relational data in a continuous vector space, which can then be seamlessly integrated with advanced natural language processing (NLP) models or used for sophisticated preference modeling, addressing challenges like data sparsity and semantic ambiguity.

In the realm of Question Answering over Knowledge Graphs (QA-KG), KGEs facilitate the understanding of natural language queries by mapping entities and relations within the question to their corresponding representations in the knowledge graph. Early frameworks, such as Knowledge Embedding based Question Answering (KEQA) by \cite{huang2019}, demonstrated this utility by jointly recovering the head entity, predicate, and tail entity representations from a natural language question within the KG embedding space. KEQA, by focusing on simple, single-fact questions, effectively addressed predicate variability and entity ambiguity through a carefully designed joint distance metric. However, KEQA's primary focus on simple queries limits its applicability to more complex, multi-hop reasoning or numerical filtering, a common challenge for KGE-only QA systems that struggle to capture deeper logical patterns without external guidance. Extending this line of KGE-centric approaches, \cite{kge\_feq} introduced KGE-FEQ for Factoid Entity Questions (FEQ), leveraging a textual knowledge graph to encode textual relationships between entities. This model employs a two-step process: retrieving relevant triples based on semantic similarities to the question, and then using a KGE approach to position the answer entity's embedding close to the question entity's embedding. KGE-FEQ's success highlights the effectiveness of integrating textual context directly into the embedding process for specific query types, outperforming state-of-the-art baselines for FEQs. While powerful for factoid questions, such methods still face limitations when confronted with the full spectrum of natural language complexity, including complex logical operations or multi-hop inference.

To overcome these limitations, more advanced QA systems integrate KGEs with powerful NLP models, forming hybrid architectures. For instance, \textit{Marie and BERT} \cite{zhou2023} presents a comprehensive KGQA system specifically for chemistry, showcasing a hybrid approach. This system leverages multiple KGE spaces to capture diverse relational semantics, while a BERT-based entity-linking model enhances the robustness and accuracy of identifying entities in natural language queries. Crucially, \textit{Marie and BERT} also incorporates mechanisms for deriving implicit multi-hop relations and efficient numerical filtering, alongside the ability to invoke semantic agents for dynamic calculations. This integration of KGE with state-of-the-art NLP (BERT) and domain-specific reasoning agents highlights an evolutionary trend in KGE applications, moving from purely embedding-based solutions to sophisticated hybrid architectures that can handle the intricacies of deep ontologies and diverse query types in specialized fields. Similarly, GETT-QA \cite{gett\_qa} leverages the T5 text-to-text pre-trained language model to generate simplified SPARQL queries, which are then grounded to KG entity and relation IDs. A key innovation in GETT-QA is its ability to learn truncated KGEs for entities, which are then used for finer disambiguation during the grounding step, demonstrating that PLMs can effectively integrate and utilize KGE information without explicit architectural changes. While such hybrid systems achieve high accuracy, their generalizability to other domains might be limited, and the complexity of integrating heterogeneous components can pose significant engineering challenges, particularly in balancing the strengths of symbolic KGs with the flexibility of neural models.

For recommender systems, KGEs offer a powerful mechanism to model user preferences and item characteristics by capturing rich relational information within knowledge graphs. Traditional recommender systems often struggle with data sparsity and providing transparent suggestions. Recurrent Knowledge Graph Embedding (RKGE) \cite{sun2018} was an early significant contribution, employing a novel recurrent network architecture to automatically learn semantic representations of paths between entities. By fusing these path semantics into the recommendation process and using a pooling operator to discriminate path saliency, RKGE not only improved recommendation accuracy but also provided meaningful explanations based on the importance of different paths. This marked a crucial step towards interpretable recommendations, aligning with the broader goal of explainable AI (as discussed in Section 7.4).

Building upon this foundation, more recent works have introduced contextualized and explainable approaches. Contextualized Knowledge Graph Embedding (CKGE) for Explainable Talent Training Course Recommendation \cite{yang2023} exemplifies this advancement. CKGE constructs "meta-graphs" for talent-course pairs, integrating contextualized neighbor semantics and high-order connections as "motivation-aware information." It then processes these with a novel KG-based Transformer, equipped with relational attention and structural encoding, and uses local path mask prediction to reveal the importance of different paths. This approach not only delivers precise recommendations but also discriminates the saliencies of meta-paths, offering fine-grained, motivation-aware explanations. This represents a significant progression from RKGE, moving from general path-based explanations to deeper contextualization and the integration of modern deep learning architectures like Transformers. Beyond these, the field has seen a significant surge in Graph Neural Network (GNN)-based recommender systems that inherently leverage KGEs or learn embeddings directly from KGs to capture complex, multi-hop user-item interactions and provide rich contextual information (as explored in Section 3.2). These GNN-KGE hybrid models often excel in addressing data sparsity and cold-start problems by propagating information across the graph structure. While CKGE and GNN-based models enhance both precision and explainability, the increased architectural complexity might introduce computational overhead, a trade-off often observed in the pursuit of higher expressiveness, as highlighted in the discussions on "Efficiency, Compression, and Scalability" (Section 6.1) within the broader KGE landscape.

In summary, KGEs are pivotal in transforming how users interact with information. For QA, they evolve from foundational frameworks like KEQA \cite{huang2019} and KGE-FEQ \cite{kge\_feq} to sophisticated hybrid systems such as \textit{Marie and BERT} \cite{zhou2023} and GETT-QA \cite{gett\_qa}, which integrate KGE with NLP to tackle domain-specific complexities and enhance disambiguation. For recommendation, the progression from RKGE \cite{sun2018} to CKGE \cite{yang2023} and the widespread adoption of GNN-KGE models demonstrate a clear trajectory towards more personalized, contextualized, and explainable suggestions. This evolution underscores KGE's critical role in bridging the gap between natural language and structured knowledge, enabling more intelligent, personalized, and interpretable user interactions across diverse application contexts.
\subsection{Domain-Specific Applications and Explainability}
\label{sec:7\_4\_domain-specific\_applications\_\_and\_\_explainability}

The utility of Knowledge Graph Embeddings (KGEs) extends far beyond generic link prediction benchmarks, finding critical applications in specialized, high-stakes domains where verifiable insights and interpretability are paramount. This subsection highlights the application of KGE in fields such as biological systems, patent metadata analysis, and drug repurposing, emphasizing how these models are tailored, validated with domain-specific metrics, and increasingly designed for explainability to build trust and provide actionable intelligence.

In the realm of biological and biomedical systems, KGE models offer powerful tools for understanding complex interactions. \cite{mohamed2020} provides a comprehensive review of KGE applications in this domain, showcasing their predictive and analytical capabilities for tasks like drug-target interactions and polypharmacy side effects. The authors emphasize that KGE models are a natural fit for representing intricate biological knowledge, which is inherently graph-structured. Building upon this, \cite{zhu2022} demonstrates the construction and multimodal reasoning capabilities of Specific Disease Knowledge Graphs (SDKGs). Their work integrates structural, category, and description embeddings to discover new, reliable knowledge for specific diseases, such as various cancers. This multimodal approach, while enhancing predictive power, also implicitly demands a higher degree of confidence in its outputs, given the critical nature of health-related predictions. However, while these works highlight the \textit{application} of KGEs, the explicit mechanisms for \textit{explaining} individual predictions often remain a challenge, as the interpretability of the underlying embedding space is not always transparent.

A compelling example of KGE in a high-stakes domain is drug repurposing for diseases like COVID-19. \cite{islam2023} proposes an ensemble KGE approach combined with a deep neural network to identify potential drug candidates. Crucially, this work moves beyond conventional KGE evaluation metrics by integrating \textit{molecular docking} to assess the binding affinity of predicted drugs to viral targets. This provides a tangible, domain-specific validation, directly addressing the need for verifiable solutions in medical research. Furthermore, \cite{islam2023} places a strong emphasis on explainability, generating insights through rules extracted from the knowledge graph and instantiated by explanatory paths. This dual focus on molecular validation and interpretability is vital for building trust and providing actionable insights to researchers, moving beyond a black-box prediction to reveal \textit{why} a particular drug might be effective. While the specific ensemble and DNN architecture might be highly tuned for the COVID-19 task, the methodology of integrating domain-specific validation and explanation is broadly applicable and sets a high standard for future KGE applications in medicine.

Beyond biology, KGEs are also applied to analyze complex socio-economic datasets. \cite{li2022} utilizes KGE models to embed patent metadata, thereby operationalizing and measuring "knowledge proximity" within the US Patent Database. By training models on entities like patents, inventors, and assignees, they demonstrate how KGEs can associate homogeneous and heterogeneous entities and explain domain expansion profiles. This application provides valuable insights into innovation ecosystems, where understanding connections and trends is crucial for policy-making and strategic planning. While the embeddings facilitate the \textit{explanation} of observed phenomena, the inherent explainability of the KGE model itself is often a secondary consideration, with the emphasis placed on the utility of the derived proximity measures.

The growing demand for interpretable KGE models in these high-stakes fields reflects a broader shift in AI research. Generic performance metrics, such as Hit@10 or MRR, are insufficient when decisions impact human health or significant economic outcomes. This necessitates a move towards solutions that are not only performant but also transparent and verifiable. Rule-based KGE models, like those explored by \cite{guo2017} (RUGE) and \cite{tang2022} (RulE), inherently offer a degree of explainability by aligning learned embeddings with human-understandable logical rules. These approaches inject prior knowledge and enforce semantic consistency, allowing for a more traceable inference process. Even models primarily focused on efficiency, such as \cite{peng2021}, acknowledge that their design can lead to "highly interpretable" entity embeddings. Furthermore, research into understanding \textit{how} KGE models extrapolate to unseen data, as investigated by \cite{li2021} through "Semantic Evidences," contributes to building a more transparent foundation for KGE predictions.

In summary, the application of KGE in specialized domains like drug discovery and patent analysis underscores a critical evolution in the field. The focus has expanded from merely achieving high accuracy on general benchmarks to delivering verifiable, transparent, and actionable solutions tailored to specific industry problems. This transition often involves the integration of domain-specific evaluation metrics (e.g., molecular docking) and a concerted effort to enhance model explainability, bridging the gap between abstract embedding spaces and concrete, trustworthy insights. The ongoing challenge lies in balancing the expressiveness and performance of complex KGE models with the imperative for clear, human-understandable explanations in sensitive application contexts.


\label{sec:conclusion_and_future_directions}

\label{sec:conclusion\_\_and\_\_future\_directions}

\section{Conclusion and Future Directions}
\label{sec:conclusion\_\_and\_\_future\_directions}

This concluding section synthesizes the extensive journey of knowledge graph embedding (KGE) research, reflecting on the intellectual trajectory from its foundational geometric and algebraic origins to the sophisticated deep learning architectures and application-driven solutions explored throughout this review. Building upon the diverse real-world applications showcased in the preceding section \ref{sec:applications\_and\_real-world\_impact\_of\_kge}, we first provide a concise summary of the key developments that have shaped the field, highlighting the continuous advancements in expressiveness, efficiency, and robustness across various KGE paradigms \cite{community\_0, community\_1}. Despite remarkable progress, the field still grapples with persistent open challenges and theoretical gaps, such as balancing model complexity with interpretability, achieving true inductive generalization, ensuring scalability for massive and dynamic knowledge graphs, and developing more rigorous evaluation methodologies \cite{community\_3, community\_6}. These limitations underscore critical areas ripe for future investigation. Furthermore, we delve into emerging trends that are poised to redefine the landscape of KGE, notably the increasing synergy with large language models for enhanced semantic understanding \cite{85064a4b1b96863af4fccff9ad34ce484945ad7b}, advancements in adaptive and multi-curvature embeddings, and the growing importance of federated and privacy-preserving KGE. Crucially, this section also addresses the paramount ethical considerations inherent in KGE technologies, including potential biases in learned representations and the imperative for responsible and transparent AI systems \cite{68f34ed64fdf07bb1325097c93576658e061231e}. By charting a course for future research and development, this forward-looking perspective aims to inspire novel inquiries and guide the responsible advancement of KGE technologies, ensuring their continued impact and beneficial integration into intelligent systems.

\subsection{Summary of Key Developments}
\label{sec:8\_1\_summary\_of\_key\_developments}

The intellectual trajectory of Knowledge Graph Embedding (KGE) research has undergone a profound evolution, tracing a path from early geometric and algebraic models to sophisticated deep learning architectures. This continuous progression underscores the field's commitment to transforming static, symbolic knowledge into dynamic, actionable insights for diverse AI systems, serving both data mining tasks and link prediction \cite{additional\_paper\_2}.

Initially, the foundation of KGE was laid by geometric and algebraic models, which sought to represent entities and relations in continuous vector spaces. Early translation-based models, such as TransE, TransH \cite{wang2014}, and TransD \cite{ji2015}, established the core paradigm by modeling relations as vector translations. While efficient, these foundational approaches often struggled with capturing complex relational patterns like symmetry, antisymmetry, and composition, leading to limited expressiveness and difficulties in distinguishing fine-grained semantics. A significant advancement came with models like RotatE \cite{sun2018}, which introduced rotations in complex vector spaces to elegantly capture these intricate patterns, moving beyond the inherent limitations of simpler translational approaches. This concept was further extended to 3D spaces in Rotate3D \cite{gao2020}. The continuous drive to refine these mathematical underpinnings is evident in innovations like HousE \cite{li2022}, which employed Householder transformations, and CompoundE \cite{ge2022} (and its 3D variant \cite{ge2023}), which generalized by combining translation, rotation, and scaling. Critically, the field also addressed fundamental theoretical issues to enhance model robustness and expressiveness. For instance, TorusE \cite{ebisu2017} resolved a long-standing flaw in TransE's regularization strategy by embedding entities and relations on a compact Lie group (a torus). This innovative approach eliminated the conflict between the translation principle and regularization, leading to more accurate and less warped embeddings. Further theoretical depth was introduced by HolmE \cite{zheng2024}, which proposed a Riemannian KGE model specifically designed to be "closed under composition." This novel property ensures that the composition of any two relation embeddings remains within the embedding space, enabling robust modeling of complex, under-represented composition patterns and theoretically unifying models like TransE and RotatE as special cases. These developments highlight a persistent effort to develop theoretically sound and highly expressive representations, moving beyond simple distance-based scoring functions to capture richer relational semantics. The ability to extend deterministic KGE models to handle weighted knowledge graphs, as demonstrated by frameworks like WeExt \cite{additional\_paper\_3}, further showcases the adaptability of these foundational paradigms to richer data types.

A pivotal shift occurred with the integration of deep learning architectures, which enabled KGE models to automatically extract complex features and capture non-linear patterns, often surpassing the performance of purely geometric models. Convolutional Neural Networks (CNNs) were adapted to model intricate feature interactions, as seen in AcrE \cite{ren2020}, ReInceptionE \cite{xie2020}, and M-DCN \cite{zhang2020}, excelling at capturing local patterns and non-linear feature interactions for improved link prediction. Graph Neural Networks (GNNs) and attention mechanisms, exemplified by GAATs \cite{wang2020} and DisenKGAT \cite{wu2021}, became instrumental in leveraging the graph's topology and neighborhood context to learn richer, context-dependent, and disentangled representations, which are crucial for multi-hop reasoning and understanding structural dependencies. More recently, Transformer architectures, such as CoKE \cite{wang2019} and Knowformer \cite{li2023}, have been innovatively applied to capture long-range dependencies and contextualized embeddings, treating KGs as sequences or integrating graph structures into self-attention mechanisms. While CNNs are highly effective for local feature aggregation, GNNs leverage the graph structure for broader contextual understanding, and Transformers excel at modeling global dependencies, each architecture offers distinct advantages but also introduces increased computational complexity and demands for larger datasets, necessitating careful trade-offs between expressiveness and efficiency.

Beyond structural and architectural innovations, significant efforts have focused on enriching KGE models with auxiliary information and enhancing their robustness and practical utility. This includes incorporating explicit entity types \cite{wang2021} and attributes \cite{zhang2024\_ae\_ke} to provide richer semantic guidance, addressing data sparsity and improving discriminative power by grounding embeddings in more comprehensive context. The integration of logical rules and constraints, from enforcing semantic smoothness \cite{guo2015} to iteratively guiding embeddings with soft rules \cite{guo2017, guo2020, tang2022}, has been crucial for injecting prior knowledge, ensuring consistency, and improving reasoning capabilities, thereby enhancing interpretability and aligning learned representations with human-understandable patterns. Concurrently, the field has dedicated substantial research to optimizing the training process and improving model reliability, recognizing that the choice of training components, such as loss functions, hyperparameters, and negative sampling strategies, can have a substantial impact on model efficiency and accuracy, often as much as the architectural design itself \cite{additional\_paper\_1}. This includes developing sophisticated negative sampling strategies \cite{shan2018, zhang2018, qian2021}, which evolved from uniform random sampling to more adversarial or confidence-aware methods to better approximate true negative distributions and mitigate false negatives. Non-sampling alternatives \cite{li2021} and weighted training for imbalanced data \cite{zhang2023} have also emerged as vital techniques. Probability calibration \cite{tabacof2019} and reinforcement learning-based noise filtering \cite{zhang2021} further ensure the trustworthiness and robustness of KGE predictions in real-world, often noisy, knowledge graphs.

The increasing focus on dynamic, inductive, and multi-modal approaches reflects the critical challenges posed by real-world knowledge graphs, which are rarely static or confined to a single modality. Temporal KGE models, such as HyTE \cite{kazemi2018} and TeRo \cite{han2020}, have emerged to capture the evolving nature of facts, moving beyond static representations to model time explicitly through various transformations and geometric spaces. Inductive KGE, enabling models to embed unseen entities, progressed from neighborhood aggregation \cite{wang2018} to advanced meta-learning strategies \cite{chen2021}, addressing the challenge of continuously growing knowledge bases. Continual KGE further addresses efficient model updates and catastrophic forgetting in these dynamic environments. Furthermore, the rise of Federated KGE tackled privacy concerns in distributed settings, leading to breakthroughs in communication efficiency \cite{zhang2024\_fkge} and personalization \cite{zhang2024\_personalized\_fkge}, crucial for leveraging decentralized knowledge without compromising sensitive information. Simultaneously, multi-modal KGE has gained traction, integrating textual descriptions \cite{xiao2016, shen2022} and other modalities to overcome data sparsity and enrich semantic understanding, often leveraging powerful pre-trained language models for richer context. The drive for efficiency and scalability has also led to techniques like knowledge distillation \cite{zhu2020}, embedding compression \cite{sachan2020, wang2021\_lightkg}, and parameter-efficient learning \cite{chen2023}, ensuring that KGE models are not only powerful but also practical for large-scale deployment.

Finally, the field has increasingly emphasized rigorous evaluation and reproducibility, recognizing their critical importance for scientific progress and real-world deployment. Early research often suffered from inconsistent benchmarking and hyperparameter tuning, leading to a reproducibility crisis where reported results were difficult to verify and compare fairly \cite{ali2020}. This led to the development of unified frameworks and libraries like PyKEEN \cite{ali2020\_pykeen} and LibKGE \cite{thulke2021} to ensure fair comparisons, standardize evaluation metrics, and promote reliable scientific progress. These efforts highlight the maturity of the field, moving towards higher standards of empirical validation and transparency, crucial for translating theoretical advancements into reliable and trustworthy real-world applications.

In summary, the field of KGE has undergone a remarkable transformation, evolving from simple geometric models to complex, hybrid architectures that integrate deep learning, logical reasoning, and multi-modal information. This continuous pursuit of enhanced expressiveness, efficiency, and robustness, coupled with a growing emphasis on dynamic, inductive, and privacy-preserving capabilities, has significantly advanced the ability to transform symbolic knowledge into actionable insights, making KGE an indispensable component for modern AI systems across diverse applications.
\subsection{Open Challenges and Theoretical Gaps}
\label{sec:8\_2\_open\_challenges\_\_and\_\_theoretical\_gaps}

Despite significant advancements in Knowledge Graph Embedding (KGE) research, several critical open challenges and theoretical gaps persist, representing fertile ground for future investigation. These issues often stem from inherent trade-offs between desirable model properties or the fundamental limitations of current methodologies.

One of the most enduring challenges is \textbf{balancing model expressiveness with computational complexity} \cite{zheng2024, li2024}. While sophisticated models, including those leveraging complex geometric spaces like hyperbolic embeddings \cite{liang2024, shang2024} or advanced deep learning architectures, offer superior expressiveness for capturing intricate relational patterns (e.g., composition, hierarchy), they often incur substantial computational costs for training and inference. For instance, models like HolmE \cite{zheng2024} and SpherE \cite{li2024} push the boundaries of expressiveness for compositional patterns and set retrieval, respectively, but their increased complexity can limit scalability. Efforts to mitigate this, such as knowledge distillation \cite{zhu2020}, embedding compression \cite{sachan2020, wang2021}, and parameter-efficient learning \cite{chen2023}, often come with a "minor loss in performance," as noted in analyses of efficiency-focused methods. While systems like GE2 \cite{zheng2024} and methods like Orthogonal Procrustes Analysis \cite{peng2021} offer impressive speedups, a universally efficient yet highly expressive KGE paradigm remains elusive, particularly for heterogeneous and dynamic graphs.

Another significant gap lies in \textbf{ensuring the interpretability of complex deep learning models}. As KGE increasingly adopts Graph Neural Networks (GNNs) and Transformer architectures, the "black box" nature of these models becomes a major concern, especially in high-stakes applications. While some works, such as Contextualized KGE for recommendation \cite{yang2023} and molecular-evaluated drug repurposing \cite{islam2023}, strive for explainability by identifying important paths or providing domain-specific validation, the explanations often remain at a high level or are specific to the model's internal mechanisms rather than intuitive for human users. Simpler geometric models might offer higher interpretability, but at the cost of expressiveness. The theoretical challenge is to design models that are both highly expressive and inherently interpretable, rather than relying on post-hoc explanation techniques.

The \textbf{efficient extraction and integration of high-quality rules} into KGE models also presents a persistent hurdle. Rule-based methods, such as RUGE \cite{guo2017} and RulE \cite{tang2022}, have demonstrated the value of injecting prior logical knowledge to enhance reasoning and consistency. However, the practical bottleneck lies in the automated discovery of high-quality, non-trivial rules from large, noisy KGs, and then effectively balancing adherence to these rules with the flexibility to capture exceptions. Current approaches often rely on separately extracted soft rules, whose quality can be uncertain, or on manually curated hard rules, which are scarce and labor-intensive. A theoretical framework for seamlessly and robustly learning and integrating rules directly from data, without extensive manual intervention or pre-processing, is still needed.

A fundamental theoretical issue revolves around \textbf{resolving problems related to the 'true' negative distribution in training}. Since knowledge graphs are inherently incomplete, the absence of a triple does not necessarily imply its falsehood. This makes negative sampling, a cornerstone of contrastive KGE training, a heuristic process. As highlighted by surveys \cite{qian2021, madushanka2024}, even sophisticated sampling methods like confidence-aware sampling \cite{shan2018} or caching strategies \cite{zhang2018} are approximations. While "Efficient Non-Sampling Knowledge Graph Embedding" \cite{li2021} attempts to circumvent sampling entirely, it introduces its own computational complexities. The theoretical gap lies in developing a robust training paradigm that does not rely on arbitrary negative sampling or can effectively model the uncertainty of missing facts, moving beyond the binary true/false assumption.

Furthermore, there is a pressing \textbf{need for more robust and unbiased evaluation metrics}. Existing benchmarks and evaluation protocols have been shown to suffer from reproducibility issues \cite{ali2020, broscheit2020}, biases due to entity over-representation \cite{rossi2020}, and vulnerabilities to data leakage \cite{lloyd2022}. The reliance on generic link prediction metrics often fails to capture the nuances of specific applications or the model's ability to generalize to different types of entities or relations. The development of domain-specific evaluation metrics, as demonstrated in drug repurposing \cite{islam2023}, points towards a future where evaluation is more tailored and verifiable, yet a generalized framework for such robust, context-aware evaluation is still nascent.

The \textbf{challenges of scalability for extremely large and dynamic knowledge graphs} remain paramount. While efforts in efficiency \cite{peng2021, zheng2024} and parallelization \cite{kochsiek2021, modak2024} have made strides, truly massive and continuously evolving KGs still pose significant hurdles. For dynamic KGE, balancing the efficiency of updates with the quality of embeddings for new knowledge and preventing catastrophic forgetting of old knowledge is a complex trade-off \cite{liu2024, sun2024}. The theoretical underpinnings for efficient, real-time updates and seamless integration of new information without compromising the integrity of the learned representation are still being developed.

Finally, the \textbf{development of truly generalizable inductive models} is a critical open challenge. Most KGE models are transductive, struggling to embed unseen entities without full retraining. While inductive approaches like neighborhood aggregation \cite{wang2018} and meta-learning \cite{chen2021} have emerged, they often rely on the presence of existing neighbors or transferable meta-knowledge. The ability to generalize to completely novel entities or subgraphs with minimal or no shared structural patterns remains a significant theoretical and practical barrier. Understanding \textit{how} KGE models extrapolate to unseen data, as explored by \cite{li2021}, is a step in the right direction, but designing models that can robustly and universally achieve this inductive capability is a frontier of KGE research.
\subsection{Emerging Trends and Ethical Considerations}
\label{sec:8\_3\_emerging\_trends\_\_and\_\_ethical\_considerations}

The landscape of Knowledge Graph Embedding (KGE) is continuously evolving, driven by both technological advancements and an increasing awareness of societal impact. As the field matures, several key trends are emerging that promise to redefine KGE capabilities, while crucial ethical considerations simultaneously demand rigorous attention to ensure responsible development and deployment. These intertwined aspects will undoubtedly shape the next generation of KGE research, balancing innovation with accountability.

One of the most significant emerging trends is the deeper integration of KGE with pre-trained language models (PLMs) for richer semantic understanding. While traditional KGE models primarily leverage structural information \cite{dai2020, cao2022}, PLMs offer a powerful mechanism to incorporate rich textual descriptions of entities and relations, thereby addressing the limitations of purely structural methods, especially in sparse knowledge graphs or for entities lacking sufficient connections. This integration is highlighted as a transformative shift in the field's evolution, enabling KGE models to infer meaning from external textual context \cite{analysis\_kge\_evolution}. Although specific PLM-KGE hybrid models are not extensively detailed in the provided papers, the general trend towards leveraging auxiliary information, such as entity types \cite{he2023} and attributes \cite{zhang2024}, underscores the growing recognition that structural data alone is often insufficient. The challenge lies in effectively fusing the dense, contextualized representations from PLMs with the structured, relational patterns learned by KGEs, potentially leading to more robust and context-aware embeddings, but also introducing complexity and opacity.

Another prominent trend involves the development of more adaptive multi-curvature embeddings. Traditional KGE models often rely on Euclidean spaces, which struggle to efficiently represent hierarchical or complex topological structures inherent in many knowledge graphs \cite{pan2021}. Hyperbolic spaces, with their inherent negative curvature, have shown promise for embedding hierarchical data with high fidelity and fewer dimensions \cite{liang2024, pan2021}. However, not all knowledge graph structures are purely hierarchical, leading to the emergence of mixed-geometry approaches. For instance, \cite{shang2024} proposes a model that integrates messages and scoring functions from hyperbolic, hypersphere, and Euclidean spaces, allowing for adaptive modeling of diverse local structures. This represents a critical methodological advancement, moving beyond the limitations of single-geometry spaces by enabling models to dynamically select the most appropriate geometric space for different parts of a knowledge graph. While these models offer enhanced expressiveness, they often introduce increased computational complexity in managing multiple geometric spaces and defining operations within them, posing a trade-off between representational power and computational efficiency.

Furthermore, advancements in federated and privacy-preserving KGE are gaining traction, driven by increasing concerns over data privacy and the prevalence of distributed knowledge sources. Federated Learning (FL) allows multiple clients to collaboratively train a shared KGE model without directly sharing their local knowledge graphs. This paradigm introduces unique challenges, such as communication efficiency, which \cite{zhang2024} addresses through entity-wise Top-K sparsification, transmitting only the most significant embedding changes. Personalized Federated KGE \cite{zhang2024} further tackles semantic disparities among clients by learning personalized supplementary knowledge, moving beyond a universal global model. However, privacy-preserving distributed training does not inherently guarantee security. \cite{zhou2024} critically exposes this by demonstrating poisoning attacks on federated KGE, where malicious clients can force victim clients to predict false facts. This highlights a crucial trade-off: while FL enhances privacy by decentralizing data, it also opens new attack vectors, necessitating robust defense mechanisms and careful consideration of trust in distributed environments.

Alongside these technological trends, crucial ethical considerations are becoming paramount. Firstly, potential biases in learned representations are a significant concern. KGE models learn from existing knowledge graphs, which may inherently contain biases reflecting societal stereotypes, historical inaccuracies, or skewed data collection practices. If not addressed, KGEs can perpetuate and even amplify these biases, leading to unfair or discriminatory outcomes in downstream applications. While the provided papers do not explicitly detail methods for KGE bias detection or mitigation, the broader field's growing emphasis on rigorous evaluation and understanding hyperparameter effects \cite{lloyd2022, rossi2020} indicates an increasing awareness of factors influencing model fairness and robustness. Future research must develop specific techniques to identify, quantify, and mitigate representational biases within KGEs, ensuring that the knowledge encoded is equitable and just.

Secondly, the responsible use of KGE in sensitive applications demands careful attention. As KGEs are increasingly deployed in high-stakes domains like healthcare, finance, and talent management, the consequences of erroneous or biased predictions can be severe. For instance, in drug repurposing for COVID-19, \cite{islam2023} emphasizes the need for "molecular evaluation and explanatory paths" to bring reliability and actionable insights to KGE-based predictions. This highlights that in sensitive applications, relying solely on standard KGE metrics is insufficient; domain-specific validation and expert oversight are imperative. The imperative is to move beyond mere predictive accuracy to ensure that KGE systems are trustworthy, verifiable, and align with ethical guidelines for their specific application contexts.

Finally, the imperative for transparent and explainable AI systems is a driving force in KGE research. As KGE models become more complex, their decision-making processes can become opaque, hindering user trust and accountability. The demand for explainability is evident in applications like recommender systems, where models like CKGE \cite{yang2023} aim to provide "explainable recommendations" by revealing the importance of different paths, and RKGE \cite{sun2018} offers "meaningful explanations." Similarly, \cite{islam2023} provides explanations for drug repurposing predictions through extracted rules and explanatory paths. Even in core model design, efforts like SpherE \cite{li2024} aim for high interpretability. The challenge lies in developing KGE models that are not only performant but can also articulate \textit{why} a particular prediction or embedding was made in a human-understandable manner. This requires a shift in model design towards inherent interpretability or the development of robust post-hoc explanation techniques, ensuring that KGE systems are not just black boxes but transparent tools that foster understanding and trust.

In conclusion, the future of KGE research is poised at the intersection of advanced technological innovation and critical ethical considerations. The deeper integration with PLMs, the development of adaptive multi-curvature embeddings, and the advancements in federated and privacy-preserving KGE represent exciting frontiers for enhancing KGE capabilities. Simultaneously, addressing potential biases, ensuring responsible deployment in sensitive applications, and prioritizing transparency and explainability are non-negotiable imperatives. These emerging trends and ethical concerns will collectively guide the next generation of KGE research, ensuring that technological progress is harmonized with societal responsibility.


\newpage
\section*{References}
\addcontentsline{toc}{section}{References}

\begin{thebibliography}{377}

\bibitem{sun2018}
Zequn Sun, Wei Hu, Qingheng Zhang, et al. (2018). \textit{Bootstrapping Entity Alignment with Knowledge Graph Embedding}. International Joint Conference on Artificial Intelligence.

\bibitem{dasgupta2018}
S. Dasgupta, Swayambhu Nath Ray, and P. Talukdar (2018). \textit{HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{chen2023}
Mingyang Chen, Wen Zhang, Zhen Yao, et al. (2023). \textit{Entity-Agnostic Representation Learning for Parameter-Efficient Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{yang2023}
Yang Yang, Chubing Zhang, Xin Song, et al. (2023). \textit{Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation}. ACM Trans. Inf. Syst..

\bibitem{jia2015}
Yantao Jia, Yuanzhuo Wang, Hailun Lin, et al. (2015). \textit{Locally Adaptive Translation for Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{lloyd2022}
Oliver Lloyd, Yi Liu, and T. Gaunt (2022). \textit{Assessing the effects of hyperparameters on knowledge graph embedding quality}. Journal of Big Data.

\bibitem{wu2021}
Junkang Wu, Wentao Shi, Xuezhi Cao, et al. (2021). \textit{DisenKGAT: Knowledge Graph Embedding with Disentangled Graph Attention Network}. International Conference on Information and Knowledge Management.

\bibitem{xu2019}
Chengjin Xu, M. Nayyeri, Fouad Alkhoury, et al. (2019). \textit{Temporal Knowledge Graph Embedding Model based on Additive Time Series Decomposition}. arXiv.org.

\bibitem{shan2018}
Yingchun Shan, Chenyang Bu, Xiaojian Liu, et al. (2018). \textit{Confidence-Aware Negative Sampling Method for Noisy Knowledge Graph Embedding}. International Conference on Big Knowledge.

\bibitem{zheng2024}
Zhuoxun Zheng, Baifan Zhou, Hui Yang, et al. (2024). \textit{Knowledge graph embedding closed under composition}. Data mining and knowledge discovery.

\bibitem{he2023}
Peng He, Gang Zhou, Yao Yao, et al. (2023). \textit{A type-augmented knowledge graph embedding framework for knowledge graph completion}. Scientific Reports.

\bibitem{xiao2015}
Han Xiao, Minlie Huang, and Xiaoyan Zhu (2015). \textit{TransG : A Generative Model for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{guo2017}
Shu Guo, Quan Wang, Lihong Wang, et al. (2017). \textit{Knowledge Graph Embedding with Iterative Guidance from Soft Rules}. AAAI Conference on Artificial Intelligence.

\bibitem{chen2021}
Mingyang Chen, Wen Zhang, Yushan Zhu, et al. (2021). \textit{Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{li2023}
Guang-pu Li, Zequn Sun, Wei Hu, et al. (2023). \textit{Position-Aware Relational Transformer for Knowledge Graph Embedding}. IEEE Transactions on Neural Networks and Learning Systems.

\bibitem{zhou2023}
Xiaochi Zhou, Shaocong Zhang, Mehal Agarwal, et al. (2023). \textit{Marie and BERT—A Knowledge Graph Embedding Based Question Answering System for Chemistry}. ACS Omega.

\bibitem{xiang2021}
Yuejia Xiang, Ziheng Zhang, Jiaoyan Chen, et al. (2021). \textit{OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding}. Findings.

\bibitem{cao2022}
Jiahang Cao, Jinyuan Fang, Zaiqiao Meng, et al. (2022). \textit{Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces}. ACM Computing Surveys.

\bibitem{wang2021}
Peng Wang, Jing Zhou, Yuzhang Liu, et al. (2021). \textit{TransET: Knowledge Graph Embedding with Entity Types}. Electronics.

\bibitem{guo2020}
Shu Guo, Lin Li, Zhen Hui, et al. (2020). \textit{Knowledge Graph Embedding Preserving Soft Logical Regularity}. International Conference on Information and Knowledge Management.

\bibitem{zhang2024}
Xiaoxiong Zhang, Zhiwei Zeng, Xin Zhou, et al. (2024). \textit{Communication-Efficient Federated Knowledge Graph Embedding with Entity-Wise Top-K Sparsification}. Knowledge-Based Systems.

\bibitem{shen2022}
Jianhao Shen, Chenguang Wang, Linyuan Gong, et al. (2022). \textit{Joint Language Semantic and Structure Embedding for Knowledge Graph Completion}. International Conference on Computational Linguistics.

\bibitem{hu2024}
Kairong Hu, Xiaozhi Zhu, Hai Liu, et al. (2024). \textit{Convolutional Neural Network-Based Entity-Specific Common Feature Aggregation for Knowledge Graph Embedding Learning}. IEEE transactions on consumer electronics.

\bibitem{liu2024}
Yang Liu, Huang Fang, Yunfeng Cai, et al. (2024). \textit{MQuinE: a Cure for “Z-paradox” in Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhang2019}
Yongqi Zhang, Quanming Yao, Wenyuan Dai, et al. (2019). \textit{AutoSF: Searching Scoring Functions for Knowledge Graph Embedding}. IEEE International Conference on Data Engineering.

\bibitem{yang2019}
Shihui Yang, Jidong Tian, Honglun Zhang, et al. (2019). \textit{TransMS: Knowledge Graph Embedding for Complex Relations by Multidirectional Semantics}. International Joint Conference on Artificial Intelligence.

\bibitem{xie2023}
Zhiwen Xie, Runjie Zhu, Jin Liu, et al. (2023). \textit{TARGAT: A Time-Aware Relational Graph Attention Model for Temporal Knowledge Graph Embedding}. IEEE/ACM Transactions on Audio Speech and Language Processing.

\bibitem{wang2024}
Jiapu Wang, Boyue Wang, Junbin Gao, et al. (2024). \textit{MADE: Multicurvature Adaptive Embedding for Temporal Knowledge Graph Completion}. IEEE Transactions on Cybernetics.

\bibitem{xiao2019}
Han Xiao, Yidong Chen, and X. Shi (2019). \textit{Knowledge Graph Embedding Based on Multi-View Clustering Framework}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{sachan2020}
Mrinmaya Sachan (2020). \textit{Knowledge Graph Embedding Compression}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{madushanka2024}
Tiroshan Madushanka, and R. Ichise (2024). \textit{Negative Sampling in Knowledge Graph Representation Learning: A Review}. arXiv.org.

\bibitem{zhu2022}
Chaoyu Zhu, Zhihao Yang, Xiaoqiong Xia, et al. (2022). \textit{Multimodal reasoning based on knowledge graph embedding for specific diseases}. Bioinform..

\bibitem{liang2024}
Qiuyu Liang, Weihua Wang, F. Bao, et al. (2024). \textit{Fully Hyperbolic Rotation for Knowledge Graph Embedding}. European Conference on Artificial Intelligence.

\bibitem{li2024}
Li, Yuyi Ao, and Jingrui He (2024). \textit{SpherE: Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{ebisu2017}
Takuma Ebisu, and R. Ichise (2017). \textit{TorusE: Knowledge Graph Embedding on a Lie Group}. AAAI Conference on Artificial Intelligence.

\bibitem{zhang2021}
Zhao Zhang, Fuzhen Zhuang, Hengshu Zhu, et al. (2021). \textit{Towards Robust Knowledge Graph Embedding via Multi-Task Reinforcement Learning}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{huang2019}
Xiao Huang, Jingyuan Zhang, Dingcheng Li, et al. (2019). \textit{Knowledge Graph Embedding Based Question Answering}. Web Search and Data Mining.

\bibitem{tang2019}
Yun Tang, Jing Huang, Guangtao Wang, et al. (2019). \textit{Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{sun2018}
Zhu Sun, Jie Yang, Jie Zhang, et al. (2018). \textit{Recurrent knowledge graph embedding for effective recommendation}. ACM Conference on Recommender Systems.

\bibitem{ge2023}
Xiou Ge, Yun Cheng Wang, Bin Wang, et al. (2023). \textit{Knowledge Graph Embedding: An Overview}. APSIPA Transactions on Signal and Information Processing.

\bibitem{wang2020}
Rui Wang, Bicheng Li, Shengwei Hu, et al. (2020). \textit{Knowledge Graph Embedding via Graph Attenuated Attention Networks}. IEEE Access.

\bibitem{li2022}
Rui Li, Jianan Zhao, Chaozhuo Li, et al. (2022). \textit{HousE: Knowledge Graph Embedding with Householder Parameterization}. International Conference on Machine Learning.

\bibitem{zhang2019}
Qingheng Zhang, Zequn Sun, Wei Hu, et al. (2019). \textit{Multi-view Knowledge Graph Embedding for Entity Alignment}. International Joint Conference on Artificial Intelligence.

\bibitem{tang2022}
Xiaojuan Tang, Song-Chun Zhu, Yitao Liang, et al. (2022). \textit{RulE: Knowledge Graph Reasoning with Rule Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{lv2018}
Xin Lv, Lei Hou, Juan-Zi Li, et al. (2018). \textit{Differentiating Concepts and Instances for Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{chen2025}
Jie Chen, Yinlong Wang, Shu Zhao, et al. (2025). \textit{Contextualized Quaternion Embedding Towards Polysemy in Knowledge Graph for Link Prediction}. ACM Trans. Asian Low Resour. Lang. Inf. Process..

\bibitem{qian2021}
Jing Qian, Gangmin Li, Katie Atkinson, et al. (2021). \textit{Understanding Negative Sampling in Knowledge Graph Embedding}. International Journal of Artificial Intelligence & Applications.

\bibitem{dai2020}
Yuanfei Dai, Shiping Wang, N. Xiong, et al. (2020). \textit{A Survey on Knowledge Graph Embedding: Approaches, Applications and Benchmarks}. Electronics.

\bibitem{ji2024}
Hao Ji, Li Yan, and Z. Ma (2024). \textit{FSTRE: Fuzzy Spatiotemporal RDF Knowledge Graph Embedding Using Uncertain Dynamic Vector Projection and Rotation}. IEEE transactions on fuzzy systems.

\bibitem{yan2022}
Qi Yan, Jiaxin Fan, Mohan Li, et al. (2022). \textit{A Survey on Knowledge Graph Embedding}. International Conference on Data Science in Cyberspace.

\bibitem{zhang2023}
Yichi Zhang, Mingyang Chen, and Wen Zhang (2023). \textit{Modality-Aware Negative Sampling for Multi-modal Knowledge Graph Embedding}. IEEE International Joint Conference on Neural Network.

\bibitem{li2021}
Ren Li, Yanan Cao, Qiannan Zhu, et al. (2021). \textit{How Does Knowledge Graph Embedding Extrapolate to Unseen Data: a Semantic Evidence View}. AAAI Conference on Artificial Intelligence.

\bibitem{yang2025}
Qingqing Yang, Min He, Zhongwen Li, et al. (2025). \textit{A Semantic Enhanced Knowledge Graph Embedding Model With AIGC Designed for Healthcare Prediction}. IEEE transactions on consumer electronics.

\bibitem{wang2019}
Quan Wang, Pingping Huang, Haifeng Wang, et al. (2019). \textit{CoKE: Contextualized Knowledge Graph Embedding}. arXiv.org.

\bibitem{di2023}
Shimin Di, and Lei Chen (2023). \textit{Message Function Search for Knowledge Graph Embedding}. The Web Conference.

\bibitem{jia2017}
Yantao Jia, Yuanzhuo Wang, Xiaolong Jin, et al. (2017). \textit{Knowledge Graph Embedding}. ACM Transactions on the Web.

\bibitem{choudhary2021}
Shivani Choudhary, Tarun Luthra, Ashima Mittal, et al. (2021). \textit{A Survey of Knowledge Graph Embedding and Their Applications}. arXiv.org.

\bibitem{xiao2015}
Han Xiao, Minlie Huang, and Xiaoyan Zhu (2015). \textit{From One Point to a Manifold: Knowledge Graph Embedding for Precise Link Prediction}. International Joint Conference on Artificial Intelligence.

\bibitem{hu2024}
Lei Hu, Wenwen Li, Jun Xu, et al. (2024). \textit{GeoEntity-type constrained knowledge graph embedding for predicting natural-language spatial relations}. International Journal of Geographical Information Science.

\bibitem{wang2014}
Zhen Wang, Jianwen Zhang, Jianlin Feng, et al. (2014). \textit{Knowledge Graph Embedding by Translating on Hyperplanes}. AAAI Conference on Artificial Intelligence.

\bibitem{zhu2020}
Yushan Zhu, Wen Zhang, Mingyang Chen, et al. (2020). \textit{DualDE: Dually Distilling Knowledge Graph Embedding for Faster and Cheaper Reasoning}. Web Search and Data Mining.

\bibitem{ali2020}
Mehdi Ali, M. Berrendorf, Charles Tapley Hoyt, et al. (2020). \textit{Bringing Light Into the Dark: A Large-Scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{mohamed2020}
Sameh K. Mohamed, A. Nounu, and V. Nováček (2020). \textit{Biological applications of knowledge graph embedding models}. Briefings Bioinform..

\bibitem{gao2020}
Chang Gao, Chengjie Sun, Lili Shan, et al. (2020). \textit{Rotate3D: Representing Relations as Rotations in Three-Dimensional Space for Knowledge Graph Embedding}. International Conference on Information and Knowledge Management.

\bibitem{peng2021}
Xutan Peng, Guanyi Chen, Chenghua Lin, et al. (2021). \textit{Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis}. North American Chapter of the Association for Computational Linguistics.

\bibitem{shi2025}
Fobo Shi, Duantengchuan Li, Xiaoguang Wang, et al. (2025). \textit{TGformer: A Graph Transformer Framework for Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{zhang2024}
Xiaoxiong Zhang, Zhiwei Zeng, Xin Zhou, et al. (2024). \textit{Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph}. Applied intelligence (Boston).

\bibitem{rosso2020}
Paolo Rosso, Dingqi Yang, and P. Cudré-Mauroux (2020). \textit{Beyond Triplets: Hyper-Relational Knowledge Graph Embedding for Link Prediction}. The Web Conference.

\bibitem{zhou2024}
Enyuan Zhou, Song Guo, Zhixiu Ma, et al. (2024). \textit{Poisoning Attack on Federated Knowledge Graph Embedding}. The Web Conference.

\bibitem{xie2020}
Zhiwen Xie, Guangyou Zhou, Jin Liu, et al. (2020). \textit{ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{song2021}
Tengwei Song, Jie Luo, and Lei Huang (2021). \textit{Rot-Pro: Modeling Transitivity by Projection in Knowledge Graph Embedding}. Neural Information Processing Systems.

\bibitem{zhang2020}
Zhaoli Zhang, Zhifei Li, Hai Liu, et al. (2020). \textit{Multi-Scale Dynamic Convolutional Network for Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{ge2022}
Xiou Ge, Yun Cheng Wang, Bin Wang, et al. (2022). \textit{CompoundE: Knowledge Graph Embedding with Translation, Rotation and Scaling Compound Operations}. arXiv.org.

\bibitem{ren2020}
Feiliang Ren, Jucheng Li, Huihui Zhang, et al. (2020). \textit{Knowledge Graph Embedding with Atrous Convolution and Residual Learning}. International Conference on Computational Linguistics.

\bibitem{yuan2019}
Jun Yuan, Neng Gao, and Ji Xiang (2019). \textit{TransGate: Knowledge Graph Embedding with Shared Gate Structure}. AAAI Conference on Artificial Intelligence.

\bibitem{xiao2015}
Han Xiao, Minlie Huang, Yu Hao, et al. (2015). \textit{TransA: An Adaptive Approach for Knowledge Graph Embedding}. arXiv.org.

\bibitem{sun2018}
Zhiqing Sun, Zhihong Deng, Jian-Yun Nie, et al. (2018). \textit{RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space}. International Conference on Learning Representations.

\bibitem{ji2015}
Guoliang Ji, Shizhu He, Liheng Xu, et al. (2015). \textit{Knowledge Graph Embedding via Dynamic Mapping Matrix}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{lin2020}
Lifan Lin, and Kun She (2020). \textit{Tensor Decomposition-Based Temporal Knowledge Graph Embedding}. IEEE International Conference on Tools with Artificial Intelligence.

\bibitem{islam2023}
M. Islam, Diego Amaya-Ramirez, B. Maigret, et al. (2023). \textit{Molecular-evaluated and explainable drug repurposing for COVID-19 using ensemble knowledge graph embedding}. Scientific Reports.

\bibitem{wang2021}
Haoyu Wang, Yaqing Wang, Defu Lian, et al. (2021). \textit{A Lightweight Knowledge Graph Embedding Framework for Efficient Inference and Storage}. International Conference on Information and Knowledge Management.

\bibitem{broscheit2020}
Samuel Broscheit, Daniel Ruffinelli, Adrian Kochsiek, et al. (2020). \textit{LibKGE - A knowledge graph embedding library for reproducible research}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{fanourakis2022}
N. Fanourakis, Vasilis Efthymiou, D. Kotzinos, et al. (2022). \textit{Knowledge graph embedding methods for entity alignment: experimental review}. Data mining and knowledge discovery.

\bibitem{wang2018}
Peifeng Wang, Jialong Han, Chenliang Li, et al. (2018). \textit{Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{tabacof2019}
Pedro Tabacof, and Luca Costabello (2019). \textit{Probability Calibration for Knowledge Graph Embedding Models}. International Conference on Learning Representations.

\bibitem{pei2019}
Shichao Pei, Lu Yu, R. Hoehndorf, et al. (2019). \textit{Semi-Supervised Entity Alignment via Knowledge Graph Embedding with Awareness of Degree Difference}. The Web Conference.

\bibitem{zhang2018}
Yongqi Zhang, Quanming Yao, Yingxia Shao, et al. (2018). \textit{NSCaching: Simple and Efficient Negative Sampling for Knowledge Graph Embedding}. IEEE International Conference on Data Engineering.

\bibitem{li2021}
Zelong Li, Jianchao Ji, Zuohui Fu, et al. (2021). \textit{Efficient Non-Sampling Knowledge Graph Embedding}. The Web Conference.

\bibitem{li2022}
Guangtong Li, L. Siddharth, and Jianxi Luo (2022). \textit{Embedding knowledge graph of patent metadata to measure knowledge proximity}. J. Assoc. Inf. Sci. Technol..

\bibitem{ding2018}
Boyang Ding, Quan Wang, Bin Wang, et al. (2018). \textit{Improving Knowledge Graph Embedding Using Simple Constraints}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{zhang2022}
Xuanyu Zhang, Qing Yang, and Dongliang Xu (2022). \textit{TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{sun2024}
Hongliang Sun, Jinlan Liu, Can Wang, et al. (2024). \textit{Learning Dynamic Knowledge Graph Embedding in Evolving Service Ecosystems via Meta-Learning}. 2024 IEEE International Conference on Web Services (ICWS).

\bibitem{wang2024}
Jiapu Wang, Zheng Cui, Boyue Wang, et al. (2024). \textit{IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion}. The Web Conference.

\bibitem{modak2024}
S. Modak, Aakarsh Malhotra, Sarthak Malik, et al. (2024). \textit{CPa-WAC: Constellation Partitioning-based Scalable Weighted Aggregation Composition for Knowledge Graph Embedding}. International Joint Conference on Artificial Intelligence.

\bibitem{xiao2016}
Han Xiao, Minlie Huang, Lian Meng, et al. (2016). \textit{SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions}. AAAI Conference on Artificial Intelligence.

\bibitem{zhang2023}
Zhao Zhang, Zhanpeng Guan, Fuwei Zhang, et al. (2023). \textit{Weighted Knowledge Graph Embedding}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{guo2015}
Shu Guo, Quan Wang, Bin Wang, et al. (2015). \textit{Semantically Smooth Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{xu2020}
Chengjin Xu, M. Nayyeri, Fouad Alkhoury, et al. (2020). \textit{TeRo: A Time-aware Knowledge Graph Embedding via Temporal Rotation}. International Conference on Computational Linguistics.

\bibitem{zheng2024}
Chenguang Zheng, Guanxian Jiang, Xiao Yan, et al. (2024). \textit{GE2: A General and Efficient Knowledge Graph Embedding Learning System}. Proc. ACM Manag. Data.

\bibitem{zhang2018}
Zhao Zhang, Fuzhen Zhuang, Meng Qu, et al. (2018). \textit{Knowledge Graph Embedding with Hierarchical Relation Structure}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhu2024}
Beibei Zhu, Ruolin Wang, Junyi Wang, et al. (2024). \textit{A survey: knowledge graph entity alignment research based on graph embedding}. Artificial Intelligence Review.

\bibitem{liu2023}
Jia Liu, Wei Huang, Tianrui Li, et al. (2023). \textit{Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{choi2020}
S. Choi, Hyun-Je Song, and Seong-Bae Park (2020). \textit{An Approach to Knowledge Base Completion by a Committee-Based Knowledge Graph Embedding}. Applied Sciences.

\bibitem{ge2023}
Xiou Ge, Yun Cheng Wang, Bin Wang, et al. (2023). \textit{Knowledge Graph Embedding with 3D Compound Geometric Transformations}. APSIPA Transactions on Signal and Information Processing.

\bibitem{sadeghian2021}
A. Sadeghian, Mohammadreza Armandpour, Anthony Colas, et al. (2021). \textit{ChronoR: Rotation Based Temporal Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{liu2024}
Jiajun Liu, Wenjun Ke, Peng Wang, et al. (2024). \textit{Fast and Continual Knowledge Graph Embedding via Incremental LoRA}. International Joint Conference on Artificial Intelligence.

\bibitem{li2022}
Yizhi Li, Wei Fan, Chaochun Liu, et al. (2022). \textit{TranSHER: Translating Knowledge Graph Embedding with Hyper-Ellipsoidal Restriction}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{rossi2020}
Andrea Rossi, D. Firmani, Antonio Matinata, et al. (2020). \textit{Knowledge Graph Embedding for Link Prediction}. ACM Transactions on Knowledge Discovery from Data.

\bibitem{li2023}
Jiang Li, Xiangdong Su, and Guanglai Gao (2023). \textit{TeAST: Temporal Knowledge Graph Embedding via Archimedean Spiral Timeline}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{peng2020}
Yanhui Peng, and Jing Zhang (2020). \textit{LineaRE: Simple but Powerful Knowledge Graph Embedding for Link Prediction}. Industrial Conference on Data Mining.

\bibitem{ji2024}
Hao Ji, Li Yan, and Z. Ma (2024). \textit{Multihop Fuzzy Spatiotemporal RDF Knowledge Graph Query via Quaternion Embedding}. IEEE transactions on fuzzy systems.

\bibitem{zhang2024}
Qinggang Zhang, Junnan Dong, Qiaoyu Tan, et al. (2024). \textit{Integrating Entity Attributes for Error-Aware Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{kochsiek2021}
Adrian Kochsiek (2021). \textit{Parallel Training of Knowledge Graph Embedding Models: A Comparison of Techniques}. Proceedings of the VLDB Endowment.

\bibitem{yang2021}
Han Yang, Leilei Zhang, Bingning Wang, et al. (2021). \textit{Cycle or Minkowski: Which is More Appropriate for Knowledge Graph Embedding?}. International Conference on Information and Knowledge Management.

\bibitem{shang2024}
Bin Shang, Yinliang Zhao, Jun Liu, et al. (2024). \textit{Mixed Geometry Message and Trainable Convolutional Attention Network for Knowledge Graph Completion}. AAAI Conference on Artificial Intelligence.

\bibitem{asmara2023}
S. M. Asmara, N. A. Sahabudin, Nor Syahidatul Nadiah Ismail, et al. (2023). \textit{A Review of Knowledge Graph Embedding Methods of TransE, TransH and TransR for Missing Links}. International Conference on Software Engineering and Computer Systems.

\bibitem{gregucci2023}
Cosimo Gregucci, M. Nayyeri, D. Hern'andez, et al. (2023). \textit{Link Prediction with Attention Applied on Multiple Knowledge Graph Embedding Models}. The Web Conference.

\bibitem{pan2021}
Zhe Pan, and Peng Wang (2021). \textit{Hyperbolic Hierarchy-Aware Knowledge Graph Embedding for Link Prediction}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{yoon2016}
Hee-Geun Yoon, Hyun-Je Song, Seong-Bae Park, et al. (2016). \textit{A Translation-Based Knowledge Graph Embedding Preserving Logical Property of Relations}. North American Chapter of the Association for Computational Linguistics.

\bibitem{li2024}
Rui Li, Chaozhuo Li, Yanming Shen, et al. (2024). \textit{Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization}. International Conference on Machine Learning.

\bibitem{xiong2017zqu}
Chenyan Xiong, Russell Power, and Jamie Callan (2017). \textit{Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding}. The Web Conference.

\bibitem{gong2020b2k}
Fan Gong, Meng Wang, Haofen Wang, et al. (2020). \textit{SMR: Medical Knowledge Graph Embedding for Safe Medicine Recommendation}. Big Data Research.

\bibitem{zhou2022ehi}
Bin Zhou, Xingwang Shen, Yuqian Lu, et al. (2022). \textit{Semantic-aware event link reasoning over industrial knowledge graph embedding time series data}. International Journal of Production Research.

\bibitem{le2022ji8}
Thanh-Binh Le, N. Le, and H. Le (2022). \textit{Knowledge graph embedding by relational rotation and complex convolution for link prediction}. Expert systems with applications.

\bibitem{zhou2022vgb}
Zhehui Zhou, Can Wang, Yan Feng, et al. (2022). \textit{JointE: Jointly utilizing 1D and 2D convolution for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{xu2019t6b}
Da Xu, Chuanwei Ruan, Evren Körpeoglu, et al. (2019). \textit{Product Knowledge Graph Embedding for E-commerce}. Web Search and Data Mining.

\bibitem{mezni20218ml}
Haithem Mezni, D. Benslimane, and Ladjel Bellatreche (2021). \textit{Context-Aware Service Recommendation Based on Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{do2021mw0}
P. Do, and Truong H. V. Phan (2021). \textit{Developing a BERT based triple classification model using knowledge graph embedding for question answering system}. Applied intelligence (Boston).

\bibitem{mai2020ei3}
Gengchen Mai, K. Janowicz, Ling Cai, et al. (2020). \textit{SE‐KGE: A location‐aware Knowledge Graph Embedding model for Geographic Question Answering and Spatial Semantic Lifting}. Trans. GIS.

\bibitem{zhang2022eab}
Jiarui Zhang, Jian Huang, Jialong Gao, et al. (2022). \textit{Knowledge graph embedding by logical-default attention graph convolution neural network for link prediction}. Information Sciences.

\bibitem{sosa2019ih0}
Daniel N. Sosa, Alexander Derry, Margaret Guo, et al. (2019). \textit{A Literature-Based Knowledge Graph Embedding Method for Identifying Drug Repurposing Opportunities in Rare Diseases}. bioRxiv.

\bibitem{guan2019pr4}
Niannian Guan, Dandan Song, and L. Liao (2019). \textit{Knowledge graph embedding with concepts}. Knowledge-Based Systems.

\bibitem{fan2014g7s}
M. Fan, Qiang Zhou, E. Chang, et al. (2014). \textit{Transition-based Knowledge Graph Embedding with Relational Mapping Properties}. Pacific Asia Conference on Language, Information and Computation.

\bibitem{zhang20190zu}
Hengtong Zhang, T. Zheng, Jing Gao, et al. (2019). \textit{Data Poisoning Attack against Knowledge Graph Embedding}. International Joint Conference on Artificial Intelligence.

\bibitem{chen2022mxn}
Qi Chen, Wei Wang, Kaizhu Huang, et al. (2022). \textit{Zero-Shot Text Classification via Knowledge Graph Embedding for Social Media Data}. IEEE Internet of Things Journal.

\bibitem{wang2022hwx}
Xin Wang, Shengfei Lyu, Xiangyu Wang, et al. (2022). \textit{Temporal knowledge graph embedding via sparse transfer matrix}. Information Sciences.

\bibitem{chen20226e4}
Mingyang Chen, Wen Zhang, Zonggang Yuan, et al. (2022). \textit{Federated knowledge graph completion via embedding-contrastive learning}. Knowledge-Based Systems.

\bibitem{abusalih2020gdu}
Bilal Abu-Salih, Marwan Al-Tawil, Ibrahim Aljarah, et al. (2020). \textit{Relational Learning Analysis of Social Politics using Knowledge Graph Embedding}. Data mining and knowledge discovery.

\bibitem{fang2022wp6}
Haichuan Fang, Youwei Wang, Zhen Tian, et al. (2022). \textit{Learning knowledge graph embedding with a dual-attention embedding network}. Expert systems with applications.

\bibitem{elebi2019bzc}
R. Çelebi, Hüseyin Uyar, Erkan Yasar, et al. (2019). \textit{Evaluation of knowledge graph embedding approaches for drug-drug interaction prediction in realistic settings}. BMC Bioinformatics.

\bibitem{sha2019i3a}
Xiao Sha, Zhu Sun, and Jie Zhang (2019). \textit{Hierarchical attentive knowledge graph embedding for personalized recommendation}. Electronic Commerce Research and Applications.

\bibitem{li2021ro5}
Zhifei Li, Hai Liu, Zhaoli Zhang, et al. (2021). \textit{Recalibration convolutional networks for learning interaction knowledge graph embedding}. Neurocomputing.

\bibitem{xiao20151fj}
Han Xiao, Minlie Huang, Yu Hao, et al. (2015). \textit{TransG : A Generative Mixture Model for Knowledge Graph Embedding}. arXiv.org.

\bibitem{zhang2021wg7}
Fei Zhang, Bo Sun, Xiaolin Diao, et al. (2021). \textit{Prediction of adverse drug reactions based on knowledge graph embedding}. BMC Medical Informatics and Decision Making.

\bibitem{wang20186zs}
Guanying Wang, Wen Zhang, Ruoxu Wang, et al. (2018). \textit{Label-Free Distant Supervision for Relation Extraction via Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{li2021x10}
Xinyu Li, P. Zheng, Jinsong Bao, et al. (2021). \textit{Achieving cognitive mass personalization via the self-X cognitive manufacturing network: An industrial-knowledge-graph- and graph-embedding-enabled pathway}. Engineering.

\bibitem{wang202110w}
Xin Wang, Xiao Liu, Jin Liu, et al. (2021). \textit{A novel knowledge graph embedding based API recommendation method for Mashup development}. World wide web (Bussum).

\bibitem{gutirrezbasulto2018oi0}
Víctor Gutiérrez-Basulto, and S. Schockaert (2018). \textit{From Knowledge Graph Embedding to Ontology Embedding? An Analysis of the Compatibility between Vector Space Representations and Rules}. International Conference on Principles of Knowledge Representation and Reasoning.

\bibitem{portisch20221rd}
Jan Portisch, Nicolas Heist, and Heiko Paulheim (2022). \textit{Knowledge graph embedding for data mining vs. knowledge graph embedding for link prediction - two sides of the same coin?}. Semantic Web.

\bibitem{zhang2022muu}
Fuwei Zhang, Zhao Zhang, Xiang Ao, et al. (2022). \textit{Along the Time: Timeline-traced Embedding for Temporal Knowledge Graph Completion}. International Conference on Information and Knowledge Management.

\bibitem{feng2016dp7}
Jun Feng, Minlie Huang, Mingdong Wang, et al. (2016). \textit{Knowledge Graph Embedding by Flexible Translation}. International Conference on Principles of Knowledge Representation and Reasoning.

\bibitem{liu2021wqa}
Jia Liu, Tianrui Li, Shenggong Ji, et al. (2021). \textit{Urban Flow Pattern Mining Based on Multi-Source Heterogeneous Data Fusion and Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{sang2019gjl}
Shengtian Sang, Zhihao Yang, Xiaoxia Liu, et al. (2019). \textit{GrEDeL: A Knowledge Graph Embedding Based Method for Drug Discovery From Biomedical Literatures}. IEEE Access.

\bibitem{wang2017yjq}
M. Wang, Mengyue Liu, Jun Liu, et al. (2017). \textit{Safe Medicine Recommendation via Medical Knowledge Graph Embedding}. arXiv.org.

\bibitem{jiang20219xl}
Dan Jiang, Ronggui Wang, Juan Yang, et al. (2021). \textit{Kernel multi-attention neural network for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{liu2022fu5}
Yang Liu, Zequn Sun, Guang-pu Li, et al. (2022). \textit{I Know What You Do Not Know: Knowledge Graph Embedding via Co-distillation Learning}. International Conference on Information and Knowledge Management.

\bibitem{khan202236g}
Nasrullah Khan, Zongmin Ma, Aman Ullah, et al. (2022). \textit{Similarity attributed knowledge graph embedding enhancement for item recommendation}. Information Sciences.

\bibitem{mezni2021ezn}
Haithem Mezni (2021). \textit{Temporal Knowledge Graph Embedding for Effective Service Recommendation}. IEEE Transactions on Services Computing.

\bibitem{zhang2021wix}
Qianjin Zhang, Ronggui Wang, Juan Yang, et al. (2021). \textit{Structural context-based knowledge graph embedding for link prediction}. Neurocomputing.

\bibitem{huang2021u42}
Xuqian Huang, Jiuyang Tang, Zhen Tan, et al. (2021). \textit{Knowledge graph embedding by relational and entity rotation}. Knowledge-Based Systems.

\bibitem{pavlovic2022qte}
Aleksandar Pavlovic, and Emanuel Sallinger (2022). \textit{ExpressivE: A Spatio-Functional Embedding For Knowledge Graph Completion}. International Conference on Learning Representations.

\bibitem{wang20213kg}
Shensi Wang, Kun Fu, Xian Sun, et al. (2021). \textit{Hierarchical-aware relation rotational knowledge graph embedding for link prediction}. Neurocomputing.

\bibitem{zhang2019rlm}
Shuai Zhang, Yi Tay, Lina Yao, et al. (2019). \textit{Quaternion Knowledge Graph Embedding}. arXiv.org.

\bibitem{mai20195rp}
Gengchen Mai, Bo Yan, K. Janowicz, et al. (2019). \textit{Relaxing Unanswerable Geographic Questions Using A Spatially Explicit Knowledge Graph Embedding Model}. Agile Conference.

\bibitem{han2018tzc}
Zhuobing Han, Xiaohong Li, Hongtao Liu, et al. (2018). \textit{DeepWeak: Reasoning common software weaknesses via knowledge graph embedding}. IEEE International Conference on Software Analysis, Evolution, and Reengineering.

\bibitem{wang2022fvx}
Feiyang Wang, Zhongbao Zhang, Li Sun, et al. (2022). \textit{DiriE: Knowledge Graph Embedding with Dirichlet Distribution}. The Web Conference.

\bibitem{ferrari2022r82}
Ilaria Ferrari, Giacomo Frisoni, Paolo Italiani, et al. (2022). \textit{Comprehensive Analysis of Knowledge Graph Embedding Techniques Benchmarked on Link Prediction}. Electronics.

\bibitem{fu2022df2}
Guirong Fu, Zhao Meng, Zhen Han, et al. (2022). \textit{TempCaps: A Capsule Network-based Embedding Model for Temporal Knowledge Graph Completion}. SPNLP.

\bibitem{wu2018c4b}
Yanrong Wu, and Zhichun Wang (2018). \textit{Knowledge Graph Embedding with Numeric Attributes of Entities}. Rep4NLP@ACL.

\bibitem{zhang202121t}
Qianjin Zhang, Ronggui Wang, Juan Yang, et al. (2021). \textit{Knowledge graph embedding by reflection transformation}. Knowledge-Based Systems.

\bibitem{mohamed2019meq}
Sameh K. Mohamed, V. Nováček, P. Vandenbussche, et al. (2019). \textit{Loss Functions in Knowledge Graph Embedding Models}. DL4KG@ESWC.

\bibitem{xin2022dam}
Kexuan Xin, Zequn Sun, Wen Hua, et al. (2022). \textit{Large-scale Entity Alignment via Knowledge Graph Merging, Partitioning and Embedding}. International Conference on Information and Knowledge Management.

\bibitem{nie20195gc}
Binling Nie, and Shouqian Sun (2019). \textit{Knowledge graph embedding via reasoning over entities, relations, and text}. Future generations computer systems.

\bibitem{liu2018kvd}
Yang Liu, Qingguo Zeng, Huanrui Yang, et al. (2018). \textit{Stock Price Movement Prediction from Financial News with Deep Learning and Knowledge Graph Embedding}. Pacific Rim Knowledge Acquisition Workshop.

\bibitem{ni2020ruj}
Chien-Chun Ni, Kin Sum Liu, and Nicolas Torzec (2020). \textit{Layered Graph Embedding for Entity Recommendation using Wikipedia in the Yahoo! Knowledge Graph}. The Web Conference.

\bibitem{li20215pu}
Chen Li, Xutan Peng, Yuhang Niu, et al. (2021). \textit{Learning graph attention-aware knowledge graph embedding}. Neurocomputing.

\bibitem{yu2019qgs}
S. Yu, Sujit Rokka Chhetri, A. Canedo, et al. (2019). \textit{Pykg2vec: A Python Library for Knowledge Graph Embedding}. Journal of machine learning research.

\bibitem{fatemi2018e6v}
Bahare Fatemi, Siamak Ravanbakhsh, and D. Poole (2018). \textit{Improved Knowledge Graph Embedding using Background Taxonomic Information}. AAAI Conference on Artificial Intelligence.

\bibitem{chen2021i5t}
Zhuo Chen, Mi-Yen Yeh, and Tei-Wei Kuo (2021). \textit{PASSLEAF: A Pool-bAsed Semi-Supervised LEArning Framework for Uncertain Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{dong2022c6z}
Sicong Dong, Xupeng Miao, Peng Liu, et al. (2022). \textit{HET-KG: Communication-Efficient Knowledge Graph Embedding Training via Hotness-Aware Cache}. IEEE International Conference on Data Engineering.

\bibitem{lu20206x1}
Fengyuan Lu, Peijin Cong, and Xinli Huang (2020). \textit{Utilizing Textual Information in Knowledge Graph Embedding: A Survey of Methods and Applications}. IEEE Access.

\bibitem{li2022nr8}
Weidong Li, Rong Peng, and Zhi Li (2022). \textit{Improving knowledge graph completion via increasing embedding interactions}. Applied intelligence (Boston).

\bibitem{luo2015df2}
Yuanfei Luo, Quan Wang, Bin Wang, et al. (2015). \textit{Context-Dependent Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhou20216m0}
Xiaohan Zhou, Yunhui Yi, and Geng Jia (2021). \textit{Path-RotatE: Knowledge Graph Embedding by Relational Rotation of Path in Complex Space}. International Conference on Innovative Computing and Cloud Computing.

\bibitem{zhao202095o}
Feng Zhao, Haoran Sun, Langjunqing Jin, et al. (2020). \textit{Structure-augmented knowledge graph embedding for sparse data with rule learning}. Computer Communications.

\bibitem{jia201870f}
Yantao Jia, Yuanzhuo Wang, Xiaolong Jin, et al. (2018). \textit{Path-specific knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{mai2018u0h}
Gengchen Mai, K. Janowicz, and Bo Yan (2018). \textit{Combining Text Embedding and Knowledge Graph Embedding Techniques for Academic Search Engines}. Semdeep/NLIWoD@ISWC.

\bibitem{li201949n}
Dingcheng Li, Siamak Zamani, Jingyuan Zhang, et al. (2019). \textit{Integration of Knowledge Graph Embedding Into Topic Modeling with Hierarchical Dirichlet Process}. North American Chapter of the Association for Computational Linguistics.

\bibitem{tang2020ufr}
Xiaoli Tang, Rui Yuan, Qianyu Li, et al. (2020). \textit{Timespan-Aware Dynamic Knowledge Graph Embedding by Incorporating Temporal Evolution}. IEEE Access.

\bibitem{guo2022qtv}
Lingbing Guo, Qiang Zhang, Zequn Sun, et al. (2022). \textit{Understanding and Improving Knowledge Graph Embedding for Entity Alignment}. International Conference on Machine Learning.

\bibitem{jiang202235y}
Dan Jiang, Ronggui Wang, Lixia Xue, et al. (2022). \textit{Multiview feature augmented neural network for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{liu201918i}
Yu Liu, Wen Hua, Kexuan Xin, et al. (2019). \textit{Context-Aware Temporal Knowledge Graph Embedding}. WISE.

\bibitem{zhang2020s4x}
Qianjin Zhang, Ronggui Wang, Juan Yang, et al. (2020). \textit{Knowledge graph embedding by translating in time domain space for link prediction}. Knowledge-Based Systems.

\bibitem{chang20179yf}
Liang Chang, Manli Zhu, T. Gu, et al. (2017). \textit{Knowledge Graph Embedding by Dynamic Translation}. IEEE Access.

\bibitem{lee2022hr9}
Yeon-Chang Lee, and Sang-Wook Kim (2022). \textit{THOR: Self-Supervised Temporal Knowledge Graph Embedding via Three-Tower Graph Convolutional Networks}. Industrial Conference on Data Mining.

\bibitem{zhang2022fpm}
Yongqi Zhang, Zhanke Zhou, Quanming Yao, et al. (2022). \textit{Efficient Hyper-parameter Search for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{liu2019e1u}
Chang Liu, Lun Li, Xiaolu Yao, et al. (2019). \textit{A Survey of Recommendation Algorithms Based on Knowledge Graph Embedding}. 2019 IEEE International Conference on Computer Science and Educational Informatization (CSEI).

\bibitem{song2021fnl}
Wei Song, Jingjin Guo, Ruiji Fu, et al. (2021). \textit{A Knowledge Graph Embedding Approach for Metaphor Processing}. IEEE/ACM Transactions on Audio Speech and Language Processing.

\bibitem{gradgyenge2017xdy}
László Grad-Gyenge, A. Kiss, and P. Filzmoser (2017). \textit{Graph Embedding Based Recommendation Techniques on the Knowledge Graph}. User Modeling, Adaptation, and Personalization.

\bibitem{zhou20218bt}
Xiaofei Zhou, Lingfeng Niu, Qiannan Zhu, et al. (2021). \textit{Knowledge Graph Embedding by Double Limit Scoring Loss}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{chen20210ah}
Yao Chen, Jiangang Liu, Zhe Zhang, et al. (2021). \textit{MöbiusE: Knowledge Graph Embedding on Möbius Ring}. Knowledge-Based Systems.

\bibitem{zhang2020i7j}
Yongqi Zhang, Quanming Yao, and Lei Chen (2020). \textit{Interstellar: Searching Recurrent Architecture for Knowledge Graph Embedding}. Neural Information Processing Systems.

\bibitem{boschin2020ki4}
Armand Boschin (2020). \textit{TorchKGE: Knowledge Graph Embedding in Python and PyTorch}. arXiv.org.

\bibitem{wang20199fe}
P. Wang, D. Dou, Fangzhao Wu, et al. (2019). \textit{Logic Rules Powered Knowledge Graph Embedding}. arXiv.org.

\bibitem{myklebust201941l}
E. B. Myklebust, Ernesto Jiménez-Ruiz, Jiaoyan Chen, et al. (2019). \textit{Knowledge Graph Embedding for Ecotoxicological Effect Prediction}. International Workshop on the Semantic Web.

\bibitem{kartheek2021aj7}
Miriyala Kartheek, and G. Sajeev (2021). \textit{Building Semantic Based Recommender System Using Knowledge Graph Embedding}. International Conference on Intelligent Information Processing.

\bibitem{sha2019plw}
Xiao Sha, Zhu Sun, and Jie Zhang (2019). \textit{Attentive Knowledge Graph Embedding for Personalized Recommendation}. arXiv.org.

\bibitem{lu2020x6y}
Haonan Lu, and Hailin Hu (2020). \textit{DensE: An Enhanced Non-Abelian Group Representation for Knowledge Graph Embedding}. arXiv.org.

\bibitem{zhang2020c15}
Siheng Zhang, Zhengya Sun, and Wensheng Zhang (2020). \textit{Improve the translational distance models for knowledge graph embedding}. Journal of Intelligence and Information Systems.

\bibitem{li2020ek4}
Mingda Li, Zhengya Sun, Siheng Zhang, et al. (2020). \textit{Enhancing Knowledge Graph Embedding with Relational Constraints}. 2020 IEEE International Conference on Knowledge Graph (ICKG).

\bibitem{li2020he5}
Jian Li, Zhuoming Xu, Yan Tang, et al. (2020). \textit{Deep Hybrid Knowledge Graph Embedding for Top-N Recommendation}. Web Information System and Application Conference.

\bibitem{kim2020zu3}
Kuekyeng Kim, Yuna Hur, Gyeongmin Kim, et al. (2020). \textit{GREG: A Global Level Relation Extraction with Knowledge Graph Embedding}. Applied Sciences.

\bibitem{zhu2018l0u}
Jizhao Zhu, Yantao Jia, Jun Xu, et al. (2018). \textit{Modeling the Correlations of Relations for Knowledge Graph Embedding}. Journal of Computational Science and Technology.

\bibitem{do20184o2}
Kien Do, T. Tran, and S. Venkatesh (2018). \textit{Knowledge Graph Embedding with Multiple Relation Projections}. International Conference on Pattern Recognition.

\bibitem{ma20194ua}
Yunpu Ma, Volker Tresp, Liming Zhao, et al. (2019). \textit{Variational Quantum Circuit Model for Knowledge Graph Embedding}. Advanced Quantum Technologies.

\bibitem{zhang2020wou}
Yuhang Zhang, Jun Wang, and Jie Luo (2020). \textit{Knowledge Graph Embedding Based Collaborative Filtering}. IEEE Access.

\bibitem{zhang2019hs5}
Wen Zhang, Shumin Deng, Han Wang, et al. (2019). \textit{XTransE: Explainable Knowledge Graph Embedding for Link Prediction with Lifestyles in e-Commerce}. Joint International Conference of Semantic Technology.

\bibitem{wang20198d2}
Zhihao Wang, and Xin Li (2019). \textit{Hybrid-TE: Hybrid Translation-Based Temporal Knowledge Graph Embedding}. IEEE International Conference on Tools with Artificial Intelligence.

\bibitem{tran20195x3}
Hung Nghiep Tran, and A. Takasu (2019). \textit{Analyzing Knowledge Graph Embedding Methods from a Multi-Embedding Interaction Perspective}. EDBT/ICDT Workshops.

\bibitem{xiong2018fof}
Shengwu Xiong, Weitao Huang, and P. Duan (2018). \textit{Knowledge Graph Embedding via Relation Paths and Dynamic Mapping Matrix}. ER Workshops.

\bibitem{radstok2021yup}
Wessel Radstok, M. Chekol, and M. Schäfer (2021). \textit{Are Knowledge Graph Embedding Models Biased, or Is it the Data That They Are Trained on?}. Wikidata@ISWC.

\bibitem{zhao2020o6z}
Ling Zhao, Hanhan Deng, L. Qiu, et al. (2020). \textit{Urban Multi-Source Spatio-Temporal Data Analysis Aware Knowledge Graph Embedding}. Symmetry.

\bibitem{zhang20182ey}
Maoyuan Zhang, Qi Wang, Wukui Xu, et al. (2018). \textit{Discriminative Path-Based Knowledge Graph Embedding for Precise Link Prediction}. European Conference on Information Retrieval.

\bibitem{jia20207dd}
Ningning Jia, Xiang Cheng, and Sen Su (2020). \textit{Improving Knowledge Graph Embedding Using Locally and Globally Attentive Relation Paths}. European Conference on Information Retrieval.

\bibitem{zhu2019ir6}
Qiannan Zhu, Xiaofei Zhou, P. Zhang, et al. (2019). \textit{A neural translating general hyperplane for knowledge graph embedding}. Journal of Computer Science.

\bibitem{wang2021dgy}
Shen Wang, Xiaokai Wei, C. D. Santos, et al. (2021). \textit{Knowledge Graph Representation via Hierarchical Hyperbolic Neural Graph Embedding}. 2021 IEEE International Conference on Big Data (Big Data).

\bibitem{ning20219et}
Zhiyuan Ning, Ziyue Qiao, Hao Dong, et al. (2021). \textit{LightCAKE: A Lightweight Framework for Context-Aware Knowledge Graph Embedding}. Pacific-Asia Conference on Knowledge Discovery and Data Mining.

\bibitem{sheikh20213qq}
Nasrullah Sheikh, Xiao Qin, B. Reinwald, et al. (2021). \textit{Knowledge Graph Embedding using Graph Convolutional Networks with Relation-Aware Attention}. arXiv.org.

\bibitem{rim2021s9a}
Wiem Ben Rim, Carolin (Haas) Lawrence, Kiril Gashteovski, et al. (2021). \textit{Behavioral Testing of Knowledge Graph Embedding Models for Link Prediction}. Conference on Automated Knowledge Base Construction.

\bibitem{zhang20179i2}
Chunhong Zhang, Miao Zhou, Xiao Han, et al. (2017). \textit{Knowledge Graph Embedding for Hyper-Relational Data}. Unpublished manuscript.

\bibitem{elebi20182bd}
R. Çelebi, Erkan Yasar, Hüseyin Uyar, et al. (2018). \textit{Evaluation of knowledge graph embedding approaches for drug-drug interaction prediction using Linked Open Data}. Workshop on Semantic Web Applications and Tools for Life Sciences.

\bibitem{garofalo20185g9}
Martina Garofalo, Maria Angela Pellegrino, Abdulrahman Altabba, et al. (2018). \textit{Leveraging Knowledge Graph Embedding Techniques for Industry 4.0 Use Cases}. arXiv.org.

\bibitem{wang201825m}
Kai Wang, Yu Liu, Xiujuan Xu, et al. (2018). \textit{Knowledge Graph Embedding with Entity Neighbors and Deep Memory Network}. arXiv.org.

\bibitem{chung2021u2l}
Chanyoung Chung, and Joyce Jiyoung Whang (2021). \textit{Knowledge Graph Embedding via Metagraph Learning}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{tran2019j42}
Hung Nghiep Tran, and A. Takasu (2019). \textit{Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space}. International Conference on Theory and Practice of Digital Libraries.

\bibitem{shi2017m2h}
Jun Shi, Huan Gao, G. Qi, et al. (2017). \textit{Knowledge Graph Embedding with Triple Context}. International Conference on Information and Knowledge Management.

\bibitem{zhang2017ixt}
Wen Zhang (2017). \textit{Knowledge Graph Embedding with Diversity of Structures}. The Web Conference.

\bibitem{zhu20196p1}
Ming-Yi Zhu, De-sheng Zhen, Ran Tao, et al. (2019). \textit{Top-N Collaborative Filtering Recommendation Algorithm Based on Knowledge Graph Embedding}. International Conference on Knowledge Management in Organizations.

\bibitem{kertkeidkachorn2019dkn}
Natthawut Kertkeidkachorn, Xin Liu, and R. Ichise (2019). \textit{GTransE: Generalizing Translation-Based Model on Uncertain Knowledge Graph Embedding}. JSAI.

\bibitem{zhu2019zqy}
Jia Zhu, Zetao Zheng, Min Yang, et al. (2019). \textit{A semi-supervised model for knowledge graph embedding}. Data mining and knowledge discovery.

\bibitem{zhang20193g2}
Hengtong Zhang, T. Zheng, Jing Gao, et al. (2019). \textit{Towards Data Poisoning Attack against Knowledge Graph Embedding}. arXiv.org.

\bibitem{liu2019fcs}
Wenqiang Liu, Hongyun Cai, Xu Cheng, et al. (2019). \textit{Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding}. Knowledge-Based Systems.

\bibitem{kanojia20171in}
Vibhor Kanojia, Hideyuki Maeda, Riku Togashi, et al. (2017). \textit{Enhancing Knowledge Graph Embedding with Probabilistic Negative Sampling}. The Web Conference.

\bibitem{gao2018di0}
Huan Gao, Jun Shi, G. Qi, et al. (2018). \textit{Triple Context-Based Knowledge Graph Embedding}. IEEE Access.

\bibitem{mai2018egi}
Gengchen Mai, K. Janowicz, and Bo Yan (2018). \textit{Support and Centrality: Learning Weights for Knowledge Graph Embedding Models}. International Conference Knowledge Engineering and Knowledge Management.

\bibitem{xiao2016bb9}
Han Xiao, Minlie Huang, and Xiaoyan Zhu (2016). \textit{Knowledge Semantic Representation: A Generative Model for Interpretable Knowledge Graph Embedding}. arXiv.org.

\bibitem{liu2024q3q}
Peifeng Liu, Lu Qian, Xingwei Zhao, et al. (2024). \textit{Joint Knowledge Graph and Large Language Model for Fault Diagnosis and Its Application in Aviation Assembly}. IEEE Transactions on Industrial Informatics.

\bibitem{zhang2024cjl}
Jin-cheng Zhang, A. Zain, Kai Zhou, et al. (2024). \textit{A review of recommender systems based on knowledge graph embedding}. Expert systems with applications.

\bibitem{su2023v6e}
Xiao-Rui Su, Zhuhong You, Deshuang Huang, et al. (2023). \textit{Biomedical Knowledge Graph Embedding With Capsule Network for Multi-Label Drug-Drug Interaction Prediction}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{zhu2023bfj}
Xiangrong Zhu, Guang-pu Li, and Wei Hu (2023). \textit{Heterogeneous Federated Knowledge Graph Embedding Learning and Unlearning}. The Web Conference.

\bibitem{liu2024to0}
Jiajun Liu, Wenjun Ke, Peng Wang, et al. (2024). \textit{Towards Continual Knowledge Graph Embedding via Incremental Distillation}. AAAI Conference on Artificial Intelligence.

\bibitem{wang2024vgj}
Wei Wang, Xiaoxuan Shen, Baolin Yi, et al. (2024). \textit{Knowledge-aware fine-grained attention networks with refined knowledge graph embedding for personalized recommendation}. Expert systems with applications.

\bibitem{li2024920}
Duantengchuan Li, Tao Xia, Jing Wang, et al. (2024). \textit{SDFormer: A shallow-to-deep feature interaction for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{lee202380l}
Jaejun Lee, Chanyoung Chung, and Joyce Jiyoung Whang (2023). \textit{InGram: Inductive Knowledge Graph Embedding via Relation Graphs}. International Conference on Machine Learning.

\bibitem{shokrzadeh2023twj}
Zeinab Shokrzadeh, M. Feizi-Derakhshi, M. Balafar, et al. (2023). \textit{Knowledge graph-based recommendation system enhanced by neural collaborative filtering and knowledge graph embedding}. Ain Shams Engineering Journal.

\bibitem{gao2023086}
Weibo Gao, Hao Wang, Qi Liu, et al. (2023). \textit{Leveraging Transferable Knowledge Concept Graph Embedding for Cold-Start Cognitive Diagnosis}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{li2024sgp}
Yufeng Li, Wenchao Zhao, Bo Dang, et al. (2024). \textit{Research on Adverse Drug Reaction Prediction Model Combining Knowledge Graph Embedding and Deep Learning}. 2024 4th International Conference on Machine Learning and Intelligent Systems Engineering (MLISE).

\bibitem{xue2023qi7}
Zengcan Xue, Zhao Zhang, Hai Liu, et al. (2023). \textit{Learning knowledge graph embedding with multi-granularity relational augmentation network}. Expert systems with applications.

\bibitem{duan2024d3f}
Pengbo Duan, Kuo Yang, Xin Su, et al. (2024). \textit{HTINet2: herb–target prediction via knowledge graph embedding and residual-like graph neural network}. Briefings Bioinform..

\bibitem{chen20246rm}
Zhen Chen, Dalin Zhang, Shanshan Feng, et al. (2024). \textit{KGTS: Contrastive Trajectory Similarity Learning over Prompt Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{zhu2022o32}
Jia Zhu, Changqin Huang, and P. D. Meo (2022). \textit{DFMKE: A dual fusion multi-modal knowledge graph embedding framework for entity alignment}. Information Fusion.

\bibitem{mitropoulou20235t0}
Katerina Mitropoulou, Panagiotis C. Kokkinos, P. Soumplis, et al. (2023). \textit{Anomaly Detection in Cloud Computing using Knowledge Graph Embedding and Machine Learning Mechanisms}. Journal of Grid Computing.

\bibitem{shomer2023imo}
Harry Shomer, Wei Jin, Wentao Wang, et al. (2023). \textit{Toward Degree Bias in Embedding-Based Knowledge Graph Completion}. The Web Conference.

\bibitem{wang202490m}
Mingjie Wang, Zijie Li, Jun Wang, et al. (2024). \textit{TracKGE: Transformer with Relation-pattern Adaptive Contrastive Learning for Knowledge Graph Embedding}. Knowledge-Based Systems.

\bibitem{li2024bl5}
Zhifei Li, Wei Huang, Xuchao Gong, et al. (2024). \textit{Decoupled semantic graph neural network for knowledge graph embedding}. Neurocomputing.

\bibitem{li2024y2a}
Mingqi Li, Wenming Ma, and Zihao Chu (2024). \textit{KGIE: Knowledge graph convolutional network for recommender system with interactive embedding}. Knowledge-Based Systems.

\bibitem{jia2023krv}
Yan Jia, Mengqi Lin, Yechen Wang, et al. (2023). \textit{Extrapolation over temporal knowledge graph via hyperbolic embedding}. CAAI Transactions on Intelligence Technology.

\bibitem{huang2023grx}
Wei Huang, Jia Liu, Tianrui Li, et al. (2023). \textit{FedCKE: Cross-Domain Knowledge Graph Embedding in Federated Learning}. IEEE Transactions on Big Data.

\bibitem{wang2023s70}
Ruoxin Wang, and C. F. Cheung (2023). \textit{Knowledge graph embedding learning system for defect diagnosis in additive manufacturing}. Computers in industry (Print).

\bibitem{hou20237gt}
Xiangning Hou, Ruizhe Ma, Li Yan, et al. (2023). \textit{T-GAE: A Timespan-aware Graph Attention-based Embedding Model for Temporal Knowledge Graph Completion}. Information Sciences.

\bibitem{jiang2023opm}
Dan Jiang, Ronggui Wang, Lixia Xue, et al. (2023). \textit{Multisource hierarchical neural network for knowledge graph embedding}. Expert systems with applications.

\bibitem{lu2022bwo}
H. Lu, Hailin Hu, and Xiaodong Lin (2022). \textit{DensE: An enhanced non-commutative representation for knowledge graph embedding with adaptive semantic hierarchy}. Neurocomputing.

\bibitem{djeddi2023g71}
W. Djeddi, Khalil Hermi, S. Yahia, et al. (2023). \textit{Advancing drug–target interaction prediction: a comprehensive graph-based approach integrating knowledge graph embedding and ProtBert pretraining}. BMC Bioinformatics.

\bibitem{zhang20243iw}
Yuchao Zhang, Xiangjie Kong, Zhehui Shen, et al. (2024). \textit{A survey on temporal knowledge graph embedding: Models and applications}. Knowledge-Based Systems.

\bibitem{le2023hjy}
Thanh-Binh Le, Huy Tran, and H. Le (2023). \textit{Knowledge graph embedding with the special orthogonal group in quaternion space for link prediction}. Knowledge-Based Systems.

\bibitem{yao2023y12}
Zhen Yao, Wen Zhang, Mingyang Chen, et al. (2023). \textit{Analogical Inference Enhanced Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{li2023y5q}
Zhipeng Li, Shanshan Feng, Jun Shi, et al. (2023). \textit{Future Event Prediction Based on Temporal Knowledge Graph Embedding}. Computer systems science and engineering.

\bibitem{yang2022j7z}
Shihan Yang, Weiya Zhang, R. Tang, et al. (2022). \textit{Approximate inferring with confidence predicting based on uncertain knowledge graph embedding}. Information Sciences.

\bibitem{banerjee2023fdi}
Debayan Banerjee, Pranav Ajit Nair, Ricardo Usbeck, et al. (2023). \textit{GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering}. Extended Semantic Web Conference.

\bibitem{hu20230kr}
Yuke Hu, Wei Liang, Ruofan Wu, et al. (2023). \textit{Quantifying and Defending against Privacy Threats on Federated Knowledge Graph Embedding}. The Web Conference.

\bibitem{li2023wgg}
Daiyi Li, Li Yan, Xiaowen Zhang, et al. (2023). \textit{EventKGE: Event knowledge graph embedding with event causal transfer}. Knowledge-Based Systems.

\bibitem{hao2022cl4}
Xinkun Hao, Qingfeng Chen, Haiming Pan, et al. (2022). \textit{Enhancing drug–drug interaction prediction by three-way decision and knowledge graph embedding}. Granular Computing.

\bibitem{khan20222j1}
Nasrullah Khan, Z. Ma, Li Yan, et al. (2022). \textit{Hashing-based semantic relevance attributed knowledge graph embedding enhancement for deep probabilistic recommendation}. Applied intelligence (Boston).

\bibitem{le2022ybl}
Thanh-Binh Le, Ngoc Huynh, and Bac Le (2022). \textit{Knowledge graph embedding by projection and rotation on hyperplanes for link prediction}. Applied intelligence (Boston).

\bibitem{liang202338l}
Shuang Liang (2023). \textit{Knowledge Graph Embedding Based on Graph Neural Network}. IEEE International Conference on Data Engineering.

\bibitem{khan2022ipv}
Nasrullah Khan, Zongmin Ma, Aman Ullah, et al. (2022). \textit{DCA-IoMT: Knowledge-Graph-Embedding-Enhanced Deep Collaborative Alert Recommendation Against COVID-19}. IEEE Transactions on Industrial Informatics.

\bibitem{he2022e37}
Peng He, Gang Zhou, Mengli Zhang, et al. (2022). \textit{Improving temporal knowledge graph embedding using tensor factorization}. Applied intelligence (Boston).

\bibitem{shen2022d5j}
Linshan Shen, Rongbo He, and Shaobin Huang (2022). \textit{Entity alignment with adaptive margin learning knowledge graph embedding}. Data & Knowledge Engineering.

\bibitem{di20210ib}
Shimin Di, Quanming Yao, Yongqi Zhang, et al. (2021). \textit{Efficient Relation-aware Scoring Function Search for Knowledge Graph Embedding}. IEEE International Conference on Data Engineering.

\bibitem{niu2020uyy}
Guanglin Niu, Bo Li, Yongfei Zhang, et al. (2020). \textit{AutoETER: Automated Entity Type Representation with Relation-Aware Attention for Knowledge Graph Embedding}. Findings.

\bibitem{nie2023ejz}
H. Nie, Xiangguo Zhao, Xin Bi, et al. (2023). \textit{Correlation embedding learning with dynamic semantic enhanced sampling for knowledge graph completion}. World wide web (Bussum).

\bibitem{li2022du0}
Jiayi Li, and Yujiu Yang (2022). \textit{STaR: Knowledge Graph Embedding by Scaling, Translation and Rotation}. Autonomous Infrastructure, Management and Security.

\bibitem{daruna2022dmk}
A. Daruna, Devleena Das, and S. Chernova (2022). \textit{Explainable Knowledge Graph Embedding: Inference Reconciliation for Knowledge Inferences Supporting Robot Actions}. IEEE/RJS International Conference on Intelligent RObots and Systems.

\bibitem{zhou20210ma}
Xing-Chun Zhou, Peng Wang, Qi Luo, et al. (2021). \textit{Multi-hop Knowledge Graph Reasoning Based on Hyperbolic Knowledge Graph Embedding and Reinforcement Learning}. IJCKG.

\bibitem{kun202384f}
Kong Wei Kun, Xin Liu, Teeradaj Racharak, et al. (2023). \textit{WeExt: A Framework of Extending Deterministic Knowledge Graph Embedding Models for Embedding Weighted Knowledge Graphs}. IEEE Access.

\bibitem{dong2022taz}
Yao Dong, Lei Wang, Ji Xiang, et al. (2022). \textit{RotateCT: Knowledge Graph Embedding by Rotation and Coordinate Transformation in Complex Space}. International Conference on Computational Linguistics.

\bibitem{kamigaito20218jz}
Hidetaka Kamigaito, and Katsuhiko Hayashi (2021). \textit{Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{krause2022th0}
Franziska Krause (2022). \textit{Dynamic Knowledge Graph Embeddings via Local Embedding Reconstructions}. Extended Semantic Web Conference.

\bibitem{zhang20213h6}
Zhao Zhang, Fuzhen Zhuang, Meng Qu, et al. (2021). \textit{Knowledge graph embedding with shared latent semantic units}. Neural Networks.

\bibitem{li2021tm6}
Guang-pu Li, Zequn Sun, Lei Qian, et al. (2021). \textit{Rule-based data augmentation for knowledge graph embedding}. AI Open.

\bibitem{wang2020au0}
Kai Wang, Yu Liu, Xiujuan Xu, et al. (2020). \textit{Enhancing knowledge graph embedding by composite neighbors for link prediction}. Computing.

\bibitem{wei20215a7}
Yuyang Wei, Wei Chen, Zhixu Li, et al. (2021). \textit{Incremental Update of Knowledge Graph Embedding by Rotating on Hyperplanes}. 2021 IEEE International Conference on Web Services (ICWS).

\bibitem{zhang2021rjh}
Yongqi Zhang, Quanming Yao, and Lei Chen (2021). \textit{Simple and automated negative sampling for knowledge graph embedding}. The VLDB journal.

\bibitem{sheikh202245c}
Nasrullah Sheikh, Xiao Qin, B. Reinwald, et al. (2022). \textit{Scaling knowledge graph embedding models for link prediction}. EuroMLSys@EuroSys.

\bibitem{ren2021muc}
Chao Ren, Le Zhang, Lintao Fang, et al. (2021). \textit{Ontological Concept Structure Aware Knowledge Transfer for Inductive Knowledge Graph Embedding}. IEEE International Joint Conference on Neural Network.

\bibitem{eyharabide2021wx4}
Victoria Eyharabide, I. E. I. Bekkouch, and Nicolae Dragoș Constantin (2021). \textit{Knowledge Graph Embedding-Based Domain Adaptation for Musical Instrument Recognition}. De Computis.

\bibitem{hong2020hyg}
Y. Hong, Chenyang Bu, and Tingting Jiang (2020). \textit{Rule-enhanced Noisy Knowledge Graph Embedding via Low-quality Error Detection}. 2020 IEEE International Conference on Knowledge Graph (ICKG).

\bibitem{huang2020sqc}
Yan Huang, Haili Sun, Xu Ke, et al. (2020). \textit{CoRelatE: Learning the correlation in multi-fold relations for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{kurokawa2021f4f}
M. Kurokawa (2021). \textit{Explainable Knowledge Reasoning Framework Using Multiple Knowledge Graph Embedding}. IJCKG.

\bibitem{mohamed2021dwg}
Sameh K. Mohamed, Emir Muñoz, and V. Nováček (2021). \textit{On Training Knowledge Graph Embedding Models}. Inf..

\bibitem{gebhart2021gtp}
Thomas Gebhart, J. Hansen, and Paul Schrater (2021). \textit{Knowledge Sheaves: A Sheaf-Theoretic Framework for Knowledge Graph Embedding}. International Conference on Artificial Intelligence and Statistics.

\bibitem{deng2024643}
Weibin Deng, Yiteng Zhang, Hong Yu, et al. (2024). \textit{Knowledge graph embedding based on dynamic adaptive atrous convolution and attention mechanism for link prediction}. Information Processing & Management.

\bibitem{liu2024zr9}
Jin Liu, Hao Du, R. Guo, et al. (2024). \textit{MMGK: Multimodality Multiview Graph Representations and Knowledge Embedding for Mild Cognitive Impairment Diagnosis}. IEEE Transactions on Computational Social Systems.

\bibitem{zhang2024zmq}
Chengcheng Zhang, Tianyi Zang, and Tianyi Zhao (2024). \textit{KGE-UNIT: toward the unification of molecular interactions prediction based on knowledge graph and multi-task learning on drug discovery}. Briefings Bioinform..

\bibitem{he2024vks}
Mingsheng He, Lin Zhu, and Luyi Bai (2024). \textit{ConvTKG: A query-aware convolutional neural network-based embedding model for temporal knowledge graph completion}. Neurocomputing.

\bibitem{zhang2024fy0}
Dong Zhang, Zhe Rong, Chengyuan Xue, et al. (2024). \textit{SimRE: Simple contrastive learning with soft logical rule for knowledge graph embedding}. Information Sciences.

\bibitem{zhang2024ivc}
Dong Zhang, Wenlong Feng, Zonghang Wu, et al. (2024). \textit{CDRGN-SDE: Cross-Dimensional Recurrent Graph Network with neural Stochastic Differential Equation for temporal knowledge graph embedding}. Expert systems with applications.

\bibitem{jing2024nxw}
Yanzhen Jing, Guanghui Zhou, Chao Zhang, et al. (2024). \textit{XMKR: Explainable manufacturing knowledge recommendation for collaborative design with graph embedding learning}. Advanced Engineering Informatics.

\bibitem{jiang2024zlc}
Pengcheng Jiang, Lang Cao, Cao Xiao, et al. (2024). \textit{KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge}. Neural Information Processing Systems.

\bibitem{han2024u0t}
Zhulin Han, and Jian Wang (2024). \textit{Knowledge enhanced graph inference network based entity-relation extraction and knowledge graph construction for industrial domain}. Frontiers of Engineering Management.

\bibitem{quan2024o2a}
Huafeng Quan, Yiting Li, Dashuai Liu, et al. (2024). \textit{Protection of Guizhou Miao batik culture based on knowledge graph and deep learning}. Heritage Science.

\bibitem{liu2024tc2}
Bufan Liu, Chun-Hsien Chen, and Zuoxu Wang (2024). \textit{A multi-hierarchical aggregation-based graph convolutional network for industrial knowledge graph embedding towards cognitive intelligent manufacturing}. Journal of manufacturing systems.

\bibitem{hello2024hgf}
Nour Hello, P. Lorenzo, and E. Strinati (2024). \textit{Semantic Communication Enhanced by Knowledge Graph Representation Learning}. International Workshop on Signal Processing Advances in Wireless Communications.

\bibitem{li2024z0e}
Jinpeng Li, Hang Yu, Xiangfeng Luo, et al. (2024). \textit{COSIGN: Contextual Facts Guided Generation for Knowledge Graph Completion}. North American Chapter of the Association for Computational Linguistics.

\bibitem{yan2024joa}
Qun Yan, Juan Zhao, Linfu Xue, et al. (2024). \textit{Mineral Prospectivity Mapping Based on Spatial Feature Classification with Geological Map Knowledge Graph Embedding: Case Study of Gold Ore Prediction at Wulonggou, Qinghai Province (Western China)}. Natural Resources Research.

\bibitem{liu2024tn0}
Jhih-Chen Liu, Chiao-Ting Chen, Chi Lee, et al. (2024). \textit{Evolving Knowledge Graph Representation Learning with Multiple Attention Strategies for Citation Recommendation System}. ACM Transactions on Intelligent Systems and Technology.

\bibitem{wang20245dw}
Chuanghui Wang, Yunqing Yang, Jinshuai Song, et al. (2024). \textit{Research Progresses and Applications of Knowledge Graph Embedding Technique in Chemistry}. Journal of Chemical Information and Modeling.

\bibitem{long2024soi}
Xiao Long, Liansheng Zhuang, Aodi Li, et al. (2024). \textit{KGDM: A Diffusion Model to Capture Multiple Relation Semantics for Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{zhou2024ayq}
Qihui Zhou, Peiqi Yin, Xiao Yan, et al. (2024). \textit{Atom: An Efficient Query Serving System for Embedding-based Knowledge Graph Reasoning with Operator-level Batching}. Proc. ACM Manag. Data.

\bibitem{huang2024t19}
Chen Huang, Deshan Chen, Tengze Fan, et al. (2024). \textit{Incorporating environmental knowledge embedding and spatial-temporal graph attention networks for inland vessel traffic flow prediction}. Engineering applications of artificial intelligence.

\bibitem{lu2024fsd}
Ming Lu, Yancong Li, Jiangxiao Zhang, et al. (2024). \textit{Deep hyperbolic convolutional model for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{liu2024yar}
Qi Liu, Qinghua Zhang, Fan Zhao, et al. (2024). \textit{Uncertain knowledge graph embedding: an effective method combining multi-relation and multi-path}. Frontiers Comput. Sci..

\bibitem{khan20242y2}
Nasrullah Khan, Zongmin Ma, Ruizhe Ma, et al. (2024). \textit{Continual knowledge graph embedding enhancement for joint interaction-based next click recommendation}. Knowledge-Based Systems.

\bibitem{xue2025ee8}
Zengcan Xue, Zhaoli Zhang, Hai Liu, et al. (2025). \textit{MHRN: A multi-perspective hierarchical relation network for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{long20248vt}
Xiao Long, Liansheng Zhuang, Aodi Li, et al. (2024). \textit{Fact Embedding through Diffusion Model for Knowledge Graph Completion}. The Web Conference.

\bibitem{huang20240su}
Chen Huang, Fei Yu, Zhiguo Wan, et al. (2024). \textit{Knowledge graph confidence-aware embedding for recommendation}. Neural Networks.

\bibitem{wang2024nej}
Yuzhuo Wang, Hongzhi Wang, Xianglong Liu, et al. (2024). \textit{GFedKG: GNN-based federated embedding model for knowledge graph completion}. Knowledge-Based Systems.

\bibitem{wang2024c8z}
Xinyan Wang, Kuo Yang, Ting Jia, et al. (2024). \textit{KDGene: knowledge graph completion for disease gene prediction using interactional tensor decomposition}. Briefings Bioinform..

\bibitem{liu2024x0k}
Yuhan Liu, Zelin Cao, Xing Gao, et al. (2024). \textit{Bridging the Space Gap: Unifying Geometry Knowledge Graph Embedding with Optimal Transport}. The Web Conference.

\bibitem{li2024uio}
Yongfang Li, and Chunhua Zhu (2024). \textit{TransE-MTP: A New Representation Learning Method for Knowledge Graph Embedding with Multi-Translation Principles and TransE}. Electronics.

\bibitem{zhang2024z78}
Qianjin Zhang, and Yandan Xu (2024). \textit{Knowledge graph embedding with inverse function representation for link prediction}. Engineering applications of artificial intelligence.

\bibitem{wang2024534}
Hao Wang, Dandan Song, Zhijing Wu, et al. (2024). \textit{A collaborative learning framework for knowledge graph embedding and reasoning}. Knowledge-Based Systems.

\bibitem{ni202438q}
Shengkun Ni, Xiangtai Kong, Yingying Zhang, et al. (2024). \textit{Identifying compound-protein interactions with knowledge graph embedding of perturbation transcriptomics}. Cell Genomics.

\bibitem{nie202499i}
Jixuan Nie, Xia Hou, Wenfeng Song, et al. (2024). \textit{Knowledge Graph Efficient Construction: Embedding Chain-of-Thought into LLMs}. VLDB Workshops.

\bibitem{wang2024d52}
Jingchao Wang, Weimin Li, Fangfang Liu, et al. (2024). \textit{ConeE: Global and local context-enhanced embedding for inductive knowledge graph completion}. Expert systems with applications.

\bibitem{mao2024v2s}
Yuren Mao, Yu Hao, Xin Cao, et al. (2024). \textit{Dynamic Graph Embedding via Meta-Learning}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{jafarzadeh202468v}
Parastoo Jafarzadeh, F. Ensan, Mahdiyar Ali Akbar Alavi, et al. (2024). \textit{A Knowledge Graph Embedding Model for Answering Factoid Entity Questions}. ACM Trans. Inf. Syst..

\bibitem{wang2024dea}
Yalin Wang, Yubin Peng, and Jingyu Guo (2024). \textit{Enhancing knowledge graph embedding with structure and semantic features}. Applied intelligence (Boston).

\bibitem{lu202436n}
Yuhuan Lu, Weijian Yu, Xin Jing, et al. (2024). \textit{HyperCL: A Contrastive Learning Framework for Hyper-Relational Knowledge Graph Embedding with Hierarchical Ontology}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{han2024gaq}
Yadan Han, Guangquan Lu, Shichao Zhang, et al. (2024). \textit{A Temporal Knowledge Graph Embedding Model Based on Variable Translation}. Tsinghua Science and Technology.

\bibitem{liu2024jz8}
Bingchen Liu, Shifu Hou, Weiyi Zhong, et al. (2024). \textit{Enhancing Temporal Knowledge Graph Alignment in News Domain With Box Embedding}. IEEE Transactions on Computational Social Systems.

\bibitem{he2024y6o}
Yunjie He, Daniel Hernández, M. Nayyeri, et al. (2024). \textit{Generating $SROI^-$ Ontologies via Knowledge Graph Query Embedding Learning}. Unpublished manuscript.

\bibitem{fang20243a4}
Yan Fang, Xiaodong Liu, Wei Lu, et al. (2024). \textit{Knowledge graph completion with low-dimensional gated hierarchical hyperbolic embedding}. Knowledge-Based Systems.

\bibitem{zhang2024h9k}
Mingtao Zhang, Guoli Yang, Yi Liu, et al. (2024). \textit{Knowledge graph accuracy evaluation: an LLM-enhanced embedding approach}. International Journal of Data Science and Analysis.

\bibitem{li2024wyh}
Yicong Li, Yu Yang, Jiannong Cao, et al. (2024). \textit{Toward Structure Fairness in Dynamic Graph Embedding: A Trend-aware Dual Debiasing Approach}. Knowledge Discovery and Data Mining.

\bibitem{dong2024ijo}
Dibo Dong, Shangwei Wang, Qiaoying Guo, et al. (2024). \textit{Short-Term Marine Wind Speed Forecasting Based on Dynamic Graph Embedding and Spatiotemporal Information}. Journal of Marine Science and Engineering.

\bibitem{wang20246c7}
Tao Wang, Bo Shen, Jinglin Zhang, et al. (2024). \textit{Knowledge Graph Embedding via Triplet Component Interactions}. Neural Processing Letters.

\bibitem{zhang2024yjo}
Pengfei Zhang, Xiaoxue Zhang, Yang Fang, et al. (2024). \textit{Knowledge Graph Embedding for Hierarchical Entities Based on Auto-Embedding Size}. Mathematics.

\bibitem{liang20247wv}
K. Liang, Yue Liu, Hao Li, et al. (2024). \textit{Clustering then Propagation: Select Better Anchors for Knowledge Graph Embedding}. Neural Information Processing Systems.

\bibitem{liu2024t05}
Qi Liu, Yuanyuan Jin, Xuefei Cao, et al. (2024). \textit{An Entity Ontology-Based Knowledge Graph Embedding Approach to News Credibility Assessment}. IEEE Transactions on Computational Social Systems.

\bibitem{pham20243mh}
H. V. Pham, Trung Tuan Nguyen, Luu Minh Tuan, et al. (2024). \textit{IDGCN: A Proposed Knowledge Graph Embedding With Graph Convolution Network For Context-Aware Recommendation Systems}. Journal of Organizational Computing and Electronic Commerce.

\bibitem{li2024gar}
Yu Li, Zhu-Hong You, Shu-Min Wang, et al. (2024). \textit{Attention-Based Learning for Predicting Drug-Drug Interactions in Knowledge Graph Embedding Based on Multisource Fusion Information}. International Journal of Intelligent Systems.

\bibitem{li2024nje}
Nan Li, Zhihao Yang, Jian Wang, et al. (2024). \textit{Drug–target interaction prediction using knowledge graph embedding}. iScience.

\bibitem{bao20249xp}
Liming Bao, Yan Wang, Xiaoyu Song, et al. (2024). \textit{HGCGE: hyperbolic graph convolutional networks-based knowledge graph embedding for link prediction}. Knowledge and Information Systems.

\bibitem{xu2024fto}
Guoshun Xu, Guozheng Rao, Li Zhang, et al. (2024). \textit{Entity-relation aggregation mechanism graph neural network for knowledge graph embedding}. Applied intelligence (Boston).

\bibitem{liang2024z0q}
Qiuyu Liang, Weihua Wang, Jie Yu, et al. (2024). \textit{Effective Knowledge Graph Embedding with Quaternion Convolutional Networks}. Natural Language Processing and Chinese Computing.

\bibitem{liu2024ixy}
Jie Liu, Lizheng Zu, Yunbin Yan, et al. (2024). \textit{Multi-Filter soft shrinkage network for knowledge graph embedding}. Expert systems with applications.

\bibitem{dong2025l9k}
Jie Dong, Cuiping Chen, Chi Zhang, et al. (2025). \textit{Knowledge Graph Embedding With Graph Convolutional Network and Bidirectional Gated Recurrent Unit for Fault Diagnosis of Industrial Processes}. IEEE Sensors Journal.

\bibitem{zhang2025ebv}
Sensen Zhang, Xun Liang, Simin Niu, et al. (2025). \textit{Integrating Large Language Models and Möbius Group Transformations for Temporal Knowledge Graph Embedding on the Riemann Sphere}. AAAI Conference on Artificial Intelligence.

\bibitem{liu20242zm}
Xinyue Liu, Jianan Zhang, Chi Ma, et al. (2024). \textit{Temporal Knowledge Graph Reasoning with Dynamic Hypergraph Embedding}. International Conference on Language Resources and Evaluation.

\bibitem{yang2024lwa}
Ruiyi Yang, Flora D. Salim, and Hao Xue (2024). \textit{SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding}. The Web Conference.

\bibitem{li20246qx}
Bo Li, Haowei Quan, Jiawei Wang, et al. (2024). \textit{Neural Library Recommendation by Embedding Project-Library Knowledge Graph}. IEEE Transactions on Software Engineering.

\bibitem{liu2024mji}
Xiaojian Liu, Xinwei Guo, and Wen Gu (2024). \textit{SecKG2vec: A novel security knowledge graph relational reasoning method based on semantic and structural fusion embedding}. Computers & security.

\bibitem{chen2024efo}
Bin Chen, Hongyi Li, Di Zhao, et al. (2024). \textit{Quality assessment of cyber threat intelligence knowledge graph based on adaptive joining of embedding model}. Complex &amp; Intelligent Systems.

\bibitem{chen2024uld}
Deng Chen, Weiwei Zhang, and Zuohua Ding (2024). \textit{Embedding dynamic graph attention mechanism into Clinical Knowledge Graph for enhanced diagnostic accuracy}. Expert systems with applications.

\bibitem{wang2017zm5}
Quan Wang, Zhendong Mao, Bin Wang, et al. (2017). \textit{Knowledge Graph Embedding: A Survey of Approaches and Applications}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{li2021qr0}
Zhifei Li, Hai Liu, Zhaoli Zhang, et al. (2021). \textit{Learning Knowledge Graph Embedding With Heterogeneous Relation Attention Networks}. IEEE Transactions on Neural Networks and Learning Systems.

\end{thebibliography}

\end{document}