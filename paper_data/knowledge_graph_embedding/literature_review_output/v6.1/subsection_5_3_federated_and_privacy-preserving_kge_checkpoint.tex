\subsection{Federated and Privacy-Preserving KGE}
The proliferation of sensitive data across distributed sources and the escalating demand for privacy-aware AI systems have positioned Federated Learning (FL) as a critical paradigm for Knowledge Graph Embedding (KGE). Federated KGE (FKGE) enables collaborative training of KGE models across multiple clients, each possessing a local knowledge graph (KG), without requiring the centralization of raw, sensitive data \cite{mcmahan2017communication, kairouz2021advances}. This distributed approach is vital for leveraging decentralized knowledge while adhering to stringent privacy regulations. However, extending KGE into the federated setting introduces a unique set of challenges encompassing communication efficiency, personalization for diverse client data, and robust security and privacy guarantees.

A paramount challenge in FKGE is the substantial communication overhead. KGE models often involve large embedding matrices, and the iterative nature of FL requires frequent exchanges of model updates between clients and a central server, leading to significant bandwidth consumption and latency. While general FL techniques often mitigate this by increasing local training epochs, this does not intrinsically reduce the size of parameters transmitted per round. To address this, Zhang et al. \cite{zhang2024_comm} proposed FedS, a bidirectional communication-efficient framework for FKGE. FedS employs Entity-Wise Top-K Sparsification, where clients dynamically identify and upload only the Top-K entity embeddings exhibiting the most significant changes. Similarly, the server performs personalized aggregation and transmits only the Top-K aggregated embeddings back to each client. FedS further incorporates an Intermittent Synchronization Mechanism to alleviate embedding inconsistency arising from client data heterogeneity. While FedS significantly enhances communication efficiency with minimal performance degradation on various datasets, it inherently involves a trade-off: aggressive sparsification, while reducing communication, might impact the global model's convergence speed or the precision of less frequently updated embeddings. This mirrors broader challenges in KGE compression efforts, where methods like LightKG \cite{wang2021} or those discussed by Sachan et al. \cite{sachan2020} also balance compression ratios with performance retention, often at the cost of some model expressiveness or training stability. Other communication-efficient FL strategies, such as quantization or gradient compression, could also be explored in the FKGE context, offering different trade-offs between compression rate and information loss.

Beyond communication, the semantic disparities inherent in client-specific KGs pose a significant hurdle for FKGE. A "one-size-fits-all" global model, typically derived from a simple arithmetic mean of client embeddings, often fails to adequately capture the unique characteristics and preferences of individual clients. This can lead to a global model that is "inundated with too much noise" when applied locally, compromising embedding quality \cite{zhang2024_pers}. To tackle this, Zhang et al. \cite{zhang2024_pers} introduced Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph (PFedEG). This novel approach moves beyond a universal global knowledge by learning personalized supplementary knowledge for each client. PFedEG constructs a "client-wise relation graph" to discern the semantic relevance of embeddings from other clients, enabling each client to amalgamate entity embeddings from its "neighboring" clients based on their affinity. This personalized aggregation, followed by local personalized embedding learning, significantly improves embedding quality by aligning local and global optimization objectives. This personalization is crucial for FKGE's utility in diverse real-world applications, akin to how inductive KGE methods \cite{wang2018, chen2021} adapt to unseen entities by leveraging local context or meta-knowledge transfer, but here applied to client-specific semantic contexts. Alternative personalization strategies in general FL, such as meta-learning based approaches or local fine-tuning of a global model, could also be adapted for FKGE, offering different ways to balance global consistency with local utility.

While FKGE offers inherent privacy benefits by keeping raw data local, it is not immune to security and privacy vulnerabilities. Distributed training environments can be susceptible to malicious actors, particularly through poisoning attacks. Zhou et al. \cite{zhou2024} provided the first systematic exploration of poisoning attacks in FKGE, developing a framework to force victim clients to predict specific false facts. Unlike centralized KGEs, where attackers might directly inject poisoned data, FKGE's local data retention makes direct injection challenging. Instead, attackers in this framework infer targeted relations in the victim's local KG via a novel KG component inference attack, then create poisoned data without direct access to the victim's KG. They inject this poisoned data indirectly through FKGE aggregation by locally training a shadow model and using an optimized dynamic poisoning scheme to generate progressive poisoned updates. Experimental results demonstrate alarming effectiveness, achieving high success rates on various KGE models (e.g., 100\% on TransE with WN18RR) with minimal impact on the original task's performance.

Beyond poisoning, privacy threats in FKGE also extend to inference attacks, where adversaries attempt to reconstruct sensitive information from shared model updates. Li et al. \cite{li2024_privacy} conducted a holistic study on privacy threats in FKGE, proposing three new inference attacks that successfully infer the existence of KG triples from victim clients' updates. To counter these threats, they introduced DP-Flames, a novel differentially private FKGE framework that incorporates private selection. DP-Flames offers a better privacy-utility trade-off by exploiting the entity-binding sparse gradient property of FKGE and provides a tight privacy accountant. It also includes an adaptive privacy budget allocation policy to dynamically adjust the defense magnitude throughout training. This work highlights that privacy-preserving distributed training does not inherently guarantee security against sophisticated malicious actors or prevent all forms of data leakage. Other privacy-enhancing technologies, such as Secure Multi-Party Computation (SMC) \cite{bogdanov2008secure} or homomorphic encryption, could also be integrated with FKGE to provide stronger cryptographic privacy guarantees, albeit often at a higher computational cost.

Collectively, these works underscore the multifaceted challenges and emerging solutions in FKGE research. The pursuit of communication efficiency \cite{zhang2024_comm}, while vital for scalability, often involves a trade-off with the precision of updates, potentially impacting convergence speed or final model quality. Similarly, personalization \cite{zhang2024_pers}, while enhancing client-specific utility, adds complexity in managing diverse models and ensuring global consistency. A significant theoretical and practical gap remains in developing robust, comprehensive defense mechanisms against both poisoning \cite{zhou2024} and inference attacks \cite{li2024_privacy}, which is crucial for the trustworthy deployment of FKGE systems. The rapid acceleration of research in this domain, as evidenced by these contemporary papers, reflects the field's urgent response to making KGE practical, secure, and effective in distributed and privacy-sensitive environments. Future work must focus on integrating various privacy-preserving techniques, developing more adaptive and robust defense strategies, and rigorously evaluating their impact on both utility and privacy guarantees.