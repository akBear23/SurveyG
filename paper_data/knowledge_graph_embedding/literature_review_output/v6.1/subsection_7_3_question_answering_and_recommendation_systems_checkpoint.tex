\subsection{Question Answering and Recommendation Systems}
Knowledge Graph Embeddings (KGEs) have proven instrumental in bridging the semantic gap between natural language and structured knowledge, thereby enabling more intelligent, personalized, and interpretable user interactions in diverse application contexts, particularly in Question Answering (QA) over knowledge graphs and Recommender Systems. The utility of KGE in these domains stems from its ability to represent complex relational data in a continuous vector space, which can then be seamlessly integrated with advanced natural language processing (NLP) models or used for sophisticated preference modeling, addressing challenges like data sparsity and semantic ambiguity.

In the realm of Question Answering over Knowledge Graphs (QA-KG), KGEs facilitate the understanding of natural language queries by mapping entities and relations within the question to their corresponding representations in the knowledge graph. Early frameworks, such as Knowledge Embedding based Question Answering (KEQA) by \cite{huang2019}, demonstrated this utility by jointly recovering the head entity, predicate, and tail entity representations from a natural language question within the KG embedding space. KEQA, by focusing on simple, single-fact questions, effectively addressed predicate variability and entity ambiguity through a carefully designed joint distance metric. However, KEQA's primary focus on simple queries limits its applicability to more complex, multi-hop reasoning or numerical filtering, a common challenge for KGE-only QA systems that struggle to capture deeper logical patterns without external guidance. Extending this line of KGE-centric approaches, \cite{kge_feq} introduced KGE-FEQ for Factoid Entity Questions (FEQ), leveraging a textual knowledge graph to encode textual relationships between entities. This model employs a two-step process: retrieving relevant triples based on semantic similarities to the question, and then using a KGE approach to position the answer entity's embedding close to the question entity's embedding. KGE-FEQ's success highlights the effectiveness of integrating textual context directly into the embedding process for specific query types, outperforming state-of-the-art baselines for FEQs. While powerful for factoid questions, such methods still face limitations when confronted with the full spectrum of natural language complexity, including complex logical operations or multi-hop inference.

To overcome these limitations, more advanced QA systems integrate KGEs with powerful NLP models, forming hybrid architectures. For instance, \textit{Marie and BERT} \cite{zhou2023} presents a comprehensive KGQA system specifically for chemistry, showcasing a hybrid approach. This system leverages multiple KGE spaces to capture diverse relational semantics, while a BERT-based entity-linking model enhances the robustness and accuracy of identifying entities in natural language queries. Crucially, \textit{Marie and BERT} also incorporates mechanisms for deriving implicit multi-hop relations and efficient numerical filtering, alongside the ability to invoke semantic agents for dynamic calculations. This integration of KGE with state-of-the-art NLP (BERT) and domain-specific reasoning agents highlights an evolutionary trend in KGE applications, moving from purely embedding-based solutions to sophisticated hybrid architectures that can handle the intricacies of deep ontologies and diverse query types in specialized fields. Similarly, GETT-QA \cite{gett_qa} leverages the T5 text-to-text pre-trained language model to generate simplified SPARQL queries, which are then grounded to KG entity and relation IDs. A key innovation in GETT-QA is its ability to learn truncated KGEs for entities, which are then used for finer disambiguation during the grounding step, demonstrating that PLMs can effectively integrate and utilize KGE information without explicit architectural changes. While such hybrid systems achieve high accuracy, their generalizability to other domains might be limited, and the complexity of integrating heterogeneous components can pose significant engineering challenges, particularly in balancing the strengths of symbolic KGs with the flexibility of neural models.

For recommender systems, KGEs offer a powerful mechanism to model user preferences and item characteristics by capturing rich relational information within knowledge graphs. Traditional recommender systems often struggle with data sparsity and providing transparent suggestions. Recurrent Knowledge Graph Embedding (RKGE) \cite{sun2018} was an early significant contribution, employing a novel recurrent network architecture to automatically learn semantic representations of paths between entities. By fusing these path semantics into the recommendation process and using a pooling operator to discriminate path saliency, RKGE not only improved recommendation accuracy but also provided meaningful explanations based on the importance of different paths. This marked a crucial step towards interpretable recommendations, aligning with the broader goal of explainable AI (as discussed in Section 7.4).

Building upon this foundation, more recent works have introduced contextualized and explainable approaches. Contextualized Knowledge Graph Embedding (CKGE) for Explainable Talent Training Course Recommendation \cite{yang2023} exemplifies this advancement. CKGE constructs "meta-graphs" for talent-course pairs, integrating contextualized neighbor semantics and high-order connections as "motivation-aware information." It then processes these with a novel KG-based Transformer, equipped with relational attention and structural encoding, and uses local path mask prediction to reveal the importance of different paths. This approach not only delivers precise recommendations but also discriminates the saliencies of meta-paths, offering fine-grained, motivation-aware explanations. This represents a significant progression from RKGE, moving from general path-based explanations to deeper contextualization and the integration of modern deep learning architectures like Transformers. Beyond these, the field has seen a significant surge in Graph Neural Network (GNN)-based recommender systems that inherently leverage KGEs or learn embeddings directly from KGs to capture complex, multi-hop user-item interactions and provide rich contextual information (as explored in Section 3.2). These GNN-KGE hybrid models often excel in addressing data sparsity and cold-start problems by propagating information across the graph structure. While CKGE and GNN-based models enhance both precision and explainability, the increased architectural complexity might introduce computational overhead, a trade-off often observed in the pursuit of higher expressiveness, as highlighted in the discussions on "Efficiency, Compression, and Scalability" (Section 6.1) within the broader KGE landscape.

In summary, KGEs are pivotal in transforming how users interact with information. For QA, they evolve from foundational frameworks like KEQA \cite{huang2019} and KGE-FEQ \cite{kge_feq} to sophisticated hybrid systems such as \textit{Marie and BERT} \cite{zhou2023} and GETT-QA \cite{gett_qa}, which integrate KGE with NLP to tackle domain-specific complexities and enhance disambiguation. For recommendation, the progression from RKGE \cite{sun2018} to CKGE \cite{yang2023} and the widespread adoption of GNN-KGE models demonstrate a clear trajectory towards more personalized, contextualized, and explainable suggestions. This evolution underscores KGE's critical role in bridging the gap between natural language and structured knowledge, enabling more intelligent, personalized, and interpretable user interactions across diverse application contexts.