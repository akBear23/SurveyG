\subsection{Incorporating Auxiliary Information (Types, Attributes)}
The effectiveness of Knowledge Graph Embedding (KGE) models, while significant, can be inherently limited by their reliance solely on the structural information of (head, relation, tail) triplets. Real-world knowledge graphs (KGs) are often incomplete, noisy, and contain rich semantic information beyond simple relational facts. To address these challenges, a crucial direction in KGE research involves incorporating auxiliary semantic information, such as entity types and attributes, to generate more discriminative, robust, and semantically grounded representations. This approach moves beyond the purely structural paradigm, enriching the embedding process with external, well-structured knowledge.

One prominent avenue for incorporating auxiliary information is the utilization of entity types. Entity types provide valuable semantic guidance, allowing models to infer plausible relations and constrain the embedding space. \cite{wang2021} introduced \textbf{TransET}, a novel KGE model that leverages entity types to learn more semantic features. TransET employs a circle convolution mechanism based on entity and entity type embeddings to map head and tail entities into type-specific representations, which are then used with a translation-based scoring function. This approach explicitly models the interaction between an entity and its type, enhancing the semantic richness of the embeddings. Building on this, \cite{he2023} proposed \textbf{TaKE}, a universal Type-augmented Knowledge graph Embedding framework. TaKE is designed to enhance any traditional KGE model by automatically capturing type features without explicit supervision and learning relation-specific type representations. A key innovation in TaKE is its type-constrained negative sampling strategy, which generates more effective negative samples by considering type consistency, thereby improving the training process. While TransET integrates types through a specific convolutional operation, TaKE offers a more general framework and refines the crucial negative sampling step, making it broadly applicable. A common limitation for both, however, is their dependence on the quality and completeness of the entity type hierarchy; if type information is sparse or inaccurate, the benefits may diminish.

Beyond explicit types, richer entity attributes offer a more granular level of auxiliary information, particularly useful for enhancing model robustness against noisy data. \cite{zhang2024} introduced \textbf{AEKE} (Attributed Error-aware Knowledge Embedding), a framework designed to guide KGE learning against the impact of erroneous triples by leveraging entity attributes. AEKE constructs two triple-level hypergraphs, one for the KG's topological structure and another for its attributes. It then calculates a confidence score for each triple based on self-contradiction, consistency between local/global structures, and homogeneity between structures and attributes. These confidence scores adaptively weigh the contributions of potentially erroneous triples during training, making the model more resilient to noise. This approach is particularly valuable as real-world KGs are rarely pristine, and errors can significantly degrade downstream application performance. A critical trade-off here is the reliance on the availability and quality of detailed entity attributes; if attributes are missing or themselves noisy, AEKE's ability to discern erroneous triples might be compromised.

The scope of auxiliary information also extends beyond simple entity types or attributes to more complex, hyper-relational facts. Traditional KGE models typically represent KGs as collections of (head, relation, tail) triplets, which can oversimplify the complex nature of real-world data where facts often come with associated key-value pairs (e.g., (person, born\_in, city, date=1990)). \cite{rosso2020} addressed this by proposing \textbf{HINGE}, a hyper-relational KGE model that directly learns from such enriched facts. HINGE captures not only the primary structural information encoded in triplets but also the correlation between each triplet and its associated key-value pairs. This allows HINGE to model richer data semantics, significantly outperforming triplet-only methods and even those that transform hyper-relational facts into n-ary representations without considering the underlying triplet structure. The strength of HINGE lies in its ability to process more comprehensive factual representations, but its applicability is contingent on the availability of KGs that explicitly store hyper-relational information.

Furthermore, domain-specific auxiliary information can be particularly powerful. For instance, \cite{hu2024} developed \textbf{SR-KGE}, a GeoEntity-type constrained KGE framework for predicting natural-language spatial relations. This model integrates geoentity types as a constraint to more accurately capture spatial and semantic relations between geographic entities, considering both graph structures and semantic attributes. This exemplifies how specialized auxiliary knowledge, tailored to a specific domain, can significantly enhance the precision of KGE models for niche tasks.

In summary, the integration of auxiliary information, whether through entity types \cite{wang2021, he2023}, attributes \cite{zhang2024}, or hyper-relational facts \cite{rosso2020}, represents a crucial advancement in KGE. These methods collectively enhance KGE by grounding embeddings in richer context, leading to more semantic, discriminative, and robust representations. They address the inherent limitations of purely structural models by providing additional signals that help overcome data incompleteness and noise. While these approaches often introduce increased model complexity and rely on the availability and quality of external data, the trade-off is generally favorable, yielding significant performance improvements, particularly for complex real-world KGs. This direction highlights a shift towards more holistic knowledge representation, where KGE models are not just learning from graph topology but also from the rich semantic metadata that accompanies it.