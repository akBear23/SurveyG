\subsection{Inductive and Continual KGE}
Real-world knowledge bases are inherently dynamic, constantly evolving with the emergence of new entities, relations, and facts. Traditional Knowledge Graph Embedding (KGE) models are predominantly transductive, meaning they can only generate embeddings for entities observed during training. This limitation necessitates expensive full retraining whenever new information arises, rendering them impractical for dynamic environments. To address this, research has focused on inductive KGE, which can embed unseen entities, and continual KGE, which efficiently updates models with new facts while preserving previously learned knowledge, thereby mitigating catastrophic forgetting \cite{liu2024}. These methods are crucial for adapting KGE models to the ever-growing nature of knowledge graphs and ensuring their scalability.

Early efforts in inductive KGE focused on leveraging graph neural network (GNN) principles, particularly neighborhood aggregation. \cite{wang2018} introduced the Logic Attention Network (LAN) for inductive KGE. LAN aggregates information from an entity's neighbors, using both rules- and network-based attention weights to account for the unordered and unequal nature of neighbors. This approach allows for the generation of embeddings for unseen entities by dynamically composing representations from their local graph context. While pioneering, a fundamental limitation of aggregation-based methods like LAN is their reliance on the existence of sufficient neighbors for new entities. Truly novel entities with sparse connections may still pose a challenge, as their embeddings would be poorly informed. Moreover, the inductive capability here is primarily about generating embeddings for *new entities* within an existing graph structure, rather than continually updating the model with *new facts* about existing or new entities over time.

A more sophisticated paradigm for inductive and dynamic KGE has emerged through meta-learning. Meta-learning aims to learn transferable meta-knowledge that can be quickly adapted to new tasks or entities. \cite{chen2021} proposed MorsE, a meta-learning approach for inductive KGE that does not learn direct entity embeddings but rather transferable, entity-independent meta-knowledge. This meta-knowledge is then used to produce embeddings for new entities, enabling a more general inductive capability compared to direct aggregation. MorsE demonstrates superior performance on both in-KG and out-of-KG tasks in inductive settings, highlighting the power of learning "how to learn" embeddings. However, its focus is primarily on the initial embedding of new entities rather than the continuous, incremental updates required for dynamic KGs.

Extending meta-learning to truly dynamic KGE, \cite{sun2024} introduced MetaHG for evolving service ecosystems. This work addresses the continuous updating challenges of service knowledge by incorporating both local (GNN) and potential global (Hypergraph Neural Network, HGNN) structural information from current knowledge graph snapshots. MetaHG refines entity embeddings using a hybrid GNN framework and leverages meta-learning to transfer meta-knowledge for accurate representation of emerging entities. This approach is particularly valuable for scenarios where entities and relations frequently appear and disappear, such as in service ecosystems. A key strength of MetaHG is its attempt to mitigate spatial deformation issues by considering global structural information, which can be a limitation for purely local aggregation methods. However, the complexity of managing hybrid GNNs and hypergraphs, along with the meta-learning process, can introduce computational overheads, and its evaluation on domain-specific datasets might limit direct generalizability to broader KGE benchmarks.

For continual KGE, the primary challenges are efficiently acquiring new knowledge and mitigating catastrophic forgettingâ€”the tendency of models to forget previously learned information when updated with new data. \cite{liu2024} proposed FastKGE, a framework incorporating an incremental low-rank adapter (IncLoRA) mechanism. FastKGE addresses the dual problem of efficient new knowledge acquisition and catastrophic forgetting by isolating and allocating new knowledge to specific layers based on fine-grained influence between old and new KGs. The IncLoRA mechanism then embeds these specific layers into low-rank adapters, significantly reducing the number of trainable parameters during fine-tuning. This approach also features adaptive rank allocation, making LoRA aware of entity importance. FastKGE demonstrates substantial reductions in training time (34-68\%) while maintaining competitive or even improved link prediction performance, especially on larger, newly constructed datasets. This parameter-efficient adaptation technique offers a compelling trade-off between model performance and computational cost, which is critical for large-scale, dynamic KGs. Compared to general parameter-efficient methods like Entity-Agnostic Representation Learning (EARL) \cite{chen2023}, which focuses on reducing the static parameter count, IncLoRA is specifically designed for the *dynamic update* process, making it highly relevant for continual learning.

In summary, the field of inductive and continual KGE has evolved from foundational neighborhood aggregation methods \cite{wang2018} to more sophisticated meta-learning strategies \cite{chen2021, sun2024} and parameter-efficient adaptation techniques \cite{liu2024}. While aggregation methods provide basic inductive capabilities, they often struggle with truly novel entities or dynamic updates. Meta-learning offers a more generalized approach to transfer knowledge, but its application to continuous updates across diverse KGs is still an active research area. Parameter-efficient methods like IncLoRA represent a significant step towards practical continual learning by balancing the acquisition of new knowledge with the retention of old, mitigating catastrophic forgetting, and ensuring scalability. A persistent challenge across these approaches remains the development of robust evaluation methodologies that accurately reflect performance in dynamic settings, as current benchmarks often fall short of capturing the complexities of real-world evolving knowledge graphs. Furthermore, balancing the expressiveness required for complex relational patterns (e.g., those addressed by \cite{zheng2024}) with the efficiency and adaptability of inductive/continual models remains a key theoretical and practical gap.