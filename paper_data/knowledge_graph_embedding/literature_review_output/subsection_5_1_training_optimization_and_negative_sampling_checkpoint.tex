\subsection{Training Optimization and Negative Sampling}

Optimizing the training process and effectively generating negative examples are paramount for the stability, efficiency, and accuracy of knowledge graph embedding (KGE) models. The inherent incompleteness of knowledge graphs necessitates strategies to distinguish true facts from false ones, typically achieved through contrasting positive triples with synthetically generated negative triples. This section delves into the evolution of negative sampling techniques, adaptive loss functions, and other optimization strategies that enhance KGE model performance.

The critical role of negative sampling (NS) in KGE training is underscored by comprehensive reviews such as \cite{madushanka2024}, which systematically categorize and analyze various NS methods, highlighting their advantages, disadvantages, and open challenges. Early improvements over uniform random sampling focused on generating more informative negative examples. For instance, \cite{sun2018} introduced self-adversarial negative sampling within the RotatE model, which samples negative triples based on the current model's scores, making them more challenging and providing stronger gradient signals. Building on this, \cite{shan2018} proposed a confidence-aware negative sampling method specifically designed for noisy knowledge graphs, leveraging the concept of "negative triple confidence" to support robust training of confidence-aware KGE models like CKRL, mitigating issues like zero loss and false detection.

To address the efficiency and quality limitations of dynamic negative sampling, \cite{zhang2018} introduced NSCaching, a simple yet effective cache-based approach. NSCaching maintains and samples from a small, dynamically updated cache of "hard" negative triplets, effectively tackling the vanishing gradient problem without the complexity, instability, or computational overhead associated with Generative Adversarial Network (GAN)-based methods. Extending the concept of negative sampling to more complex data structures, \cite{zhang2023} developed Modality-Aware Negative Sampling (MANS) for multi-modal KGE. MANS innovates by performing modal-level negative sampling (e.g., only sampling negative visual embeddings), which explicitly facilitates modality alignment between heterogeneous embeddings, a crucial aspect often overlooked by traditional entity-level sampling. Furthermore, domain-specific negative sampling strategies have emerged, such as the type-constrained negative sampling proposed by \cite{he2023} within the TaKE framework, which leverages implicit type features to construct more effective negative samples without requiring explicit type supervision.

While negative sampling remains a cornerstone, the field has also explored non-sampling frameworks to overcome its inherent instability and suboptimal accuracy. \cite{li2021} presented the Efficient Non-Sampling Knowledge Graph Embedding (NS-KGE) framework, which mathematically re-derives and re-organizes the non-sampling square loss function. This innovation enables KGE models to be trained efficiently on all positive and negative instances, eliminating the need for sampling and leading to more stable and accurate embeddings without incurring prohibitive computational or space complexity.

Beyond negative sampling, other optimization techniques focus on refining loss functions and enhancing model robustness. The adaptive nature of training parameters is crucial; for example, \cite{jia2017} introduced TransA, an adaptive translation method that dynamically determines the margin of the loss function based on the local characteristics of different knowledge graphs. This adaptive margin improves embedding performance and facilitates incremental learning for evolving KGs. Similarly, \cite{xiao2015} (also named TransA) proposed an adaptive approach that replaces the oversimplified Euclidean distance with an adaptive Mahalanobis distance. This allows for flexible elliptical equipotential surfaces and suppresses noise by weighting different dimensions of the loss vector, thereby better modeling complex relation topologies.

Robustness to noisy data, a pervasive issue in real-world KGs, has also driven significant optimization efforts. \cite{zhang2021} developed a multi-task reinforcement learning (MTRL) framework for robust KGE, where RL agents are trained to make hard decisions to filter out noisy triples. This framework, combined with multi-task learning for similar relations, provides a powerful mechanism for data cleansing and robust representation learning. Furthermore, ensuring the reliability of KGE model predictions is vital for downstream applications. \cite{tabacof2019} addressed the problem of probability calibration in KGE models, demonstrating that KGE predictions are often uncalibrated. They proposed calibration techniques, including a novel heuristic for handling synthetic negatives, to ensure that predicted probabilities accurately reflect true confidence, thereby simplifying triple classification and enhancing trustworthiness.

In conclusion, the optimization of KGE training has evolved from basic negative sampling to sophisticated, context-aware, and even non-sampling methodologies. This progression reflects a continuous drive to enhance stability, efficiency, and accuracy, alongside a growing emphasis on robustness to noise and the reliability of predictions. Despite these advancements, challenges remain in designing NS methods that are universally optimal, integrating diverse contextual information seamlessly, and developing truly adaptive and self-correcting training paradigms, as highlighted by ongoing research in the field \cite{madushanka2024}.