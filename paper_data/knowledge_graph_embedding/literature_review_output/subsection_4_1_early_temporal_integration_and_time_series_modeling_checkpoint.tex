\subsection{Foundational Approaches to Temporal Knowledge Graph Embeddings}
The transition from static to dynamic Knowledge Graph Embeddings (KGEs) marks a pivotal advancement, acknowledging that real-world facts are inherently time-dependent and evolve. This foundational phase saw the emergence of distinct paradigms to explicitly integrate the temporal dimension, laying the groundwork for more sophisticated temporal reasoning. These early efforts primarily explored geometric, statistical, and algebraic approaches to capture the fluidity of knowledge.

One of the pioneering geometric approaches is HyTE \cite{dasgupta2018}, which introduced a novel mechanism for temporal integration. HyTE associates each timestamp with a unique hyperplane in the embedding space. This allows for a temporally-guided inference, where the plausibility of a triple $(h, r, t)$ at a specific time $T$ is determined by its proximity to the hyperplane corresponding to $T$. A key strength of HyTE lies in its ability to predict temporal scopes for facts with missing time annotations, addressing a critical challenge in incomplete knowledge graphs. However, HyTE's approach has inherent limitations. The assumption of a distinct hyperplane for every timestamp can become computationally expensive and parameter-heavy for continuous time or very long temporal spans. Moreover, its linear projections might struggle to capture complex, non-linear, or periodic temporal patterns. Building on HyTE, Hybrid-TE \cite{wang20198d2} sought to enhance its capabilities by combining it with TransD and projecting a triplet to *all* time-specific hyperplanes on which it is temporally valid. This extension aimed to improve precision for multi-relational facts, demonstrating an early attempt to refine the geometric temporal modeling paradigm.

Addressing the deterministic nature of HyTE's temporal representations, ATiSE \cite{xu2019} introduced a more sophisticated statistical approach. ATiSE conceptualizes entity and relation evolution not as fixed points or hyperplanes, but as multi-dimensional additive time series. Crucially, it represents these evolving embeddings as Gaussian distributions, explicitly capturing temporal uncertainty through their covariance matrices. This novel connection between relational processes and time series analysis allows ATiSE to model the probabilistic nature of temporal facts. While offering a richer representation of uncertainty, ATiSE faces challenges related to the computational cost of learning and storing covariance matrices for all entities and relations, especially in high-dimensional spaces. Its reliance on explicit time series modeling assumptions might also limit its flexibility in capturing highly irregular or sparse temporal events.

Complementing these geometric and statistical models, tensor decomposition methods emerged as a powerful algebraic framework for temporal modeling. This paradigm inherently integrates time by conceptualizing knowledge graph facts as a fourth-order tensor $\mathcal{X} \in \mathbb{R}^{|\mathcal{E}| \times |\mathcal{R}| \times |\mathcal{E}| \times |\mathcal{T}|}$, where the dimensions correspond to head entities, relations, tail entities, and time. Early works in this area, such as TTransE \cite{leblay2018ttranse} and TA-TransE/TA-DistMult \cite{garcia2018learning}, extended static tensor factorization models to incorporate the temporal dimension. TTransE, for instance, augments the translational scoring function of TransE by adding a time embedding that influences the triple's plausibility. Similarly, TA-TransE and TA-DistMult learn time-aware relation embeddings or project static embeddings into time-specific spaces, effectively treating time as an additional context for relations. By applying various tensor decomposition techniques (e.g., CP decomposition, Tucker decomposition) to this four-dimensional structure, these methods directly embed the temporal dimension alongside other components, offering a robust way to learn temporal representations. More recent works like \cite{lin2020} continue to explore and refine tensor decomposition for temporal KGEs, often focusing on efficiency or specific decomposition strategies. However, a common limitation of tensor decomposition methods is their susceptibility to data sparsity, as real-world temporal KGs often have many missing entries in the 4D tensor. Furthermore, the computational complexity of decomposing high-order tensors can be substantial, and these methods often treat time as discrete indices, which may not adequately capture continuous temporal dynamics or fine-grained temporal interactions without further modifications.

These foundational models collectively demonstrated the feasibility and necessity of temporal integration in KGEs, employing diverse strategies from geometric interpretations and statistical time series analysis to multi-dimensional tensor representations. Each paradigm offered unique strengths in modeling specific aspects of temporal knowledge, from discrete temporal validity to probabilistic evolution. However, a common challenge across these early methods lies in their potential struggle with highly complex, non-linear temporal patterns, the scalability issues associated with explicit temporal indexing or high-order tensor operations, and the limited capacity to model continuous time or capture long-range temporal dependencies efficiently. These limitations subsequently motivated the development of more advanced models, particularly those leveraging more expressive geometric spaces and neural architectures, capable of handling intricate temporal dynamics and geometric complexities.