\subsection{Scalability and Parallelization Techniques}

Scaling knowledge graph embedding (KGE) models to handle massive, real-world knowledge graphs (KGs) presents a crucial challenge, demanding innovative techniques to manage computational and memory costs while accelerating training times. Early efforts focused on optimizing model architectures for inherent efficiency. For instance, \textcite{ji2015} introduced TransD, a model that uses a dual-vector representation for entities and relations to dynamically construct mapping matrices, significantly reducing parameter count and avoiding computationally intensive matrix-vector multiplications, thereby enhancing scalability. Similarly, \textcite{yang2019} proposed TransMS, which models multidirectional semantics with remarkable parameter efficiency, adding only a single scalar parameter per triplet, making it highly scalable for large KGs. \textcite{ebisu2017} addressed TransE's regularization conflict by embedding on a compact Lie group (torus), leading to a regularization-free and more efficient model with lower computational complexity. More recently, models like CompoundE \textcite{ge2022} and HousE \textcite{li2022} have achieved superior expressiveness while maintaining efficiency; CompoundE leverages compound geometric operations to achieve state-of-the-art performance on large KGs with fewer parameters, while HousE employs Householder parameterization for high-dimensional rotations and invertible projections, optimizing matrix-vector multiplications into efficient vector operations.

As KGE models grew in complexity, the need for more direct computational and memory optimizations became apparent. Knowledge distillation emerged as a powerful technique to reduce model footprint. DualDE \textcite{zhu2020} dually distills knowledge from a high-dimensional teacher KGE to a low-dimensional student, achieving 7x-15x parameter reduction and 2x-6x inference speedup with minimal accuracy loss by adaptively weighting soft labels and employing a two-stage distillation process. Complementing this, LightKG \textcite{wang2021} introduced a lightweight framework that uses codebooks and codewords for efficient storage and inference, significantly reducing memory footprint while maintaining high approximate search accuracy. This framework also incorporates a residual module for codebook diversity and a continuous function to approximate non-differentiable codeword selection. In the context of neural architectures, SEConv \textcite{yang2025} integrates a less resource-consuming self-attention mechanism with a multilayer convolutional neural network to learn deeper structural features, designed for efficient deployment on resource-limited consumer electronics.

For truly massive KGs, parallelization and distributed training techniques are indispensable. \textcite{kochsiek2021} conducted a comprehensive empirical comparison of various parallelization techniques for KGE models, demonstrating that even simple random partitioning with suitable sampling can be highly effective, and proposing a stratification variation that mitigates negative impacts on embedding quality. Building on this, CPa-WAC \textcite{modak2024} tackled the scalability of GNN-based KGE models, which are notoriously resource-intensive. It introduced Constellation Partitioning (CPa), a topology-preserving algorithm using Louvain/Leiden clustering with hierarchical merging to divide KGs into manageable subgraphs, and a novel Global Decoder framework to effectively combine cluster-specific embeddings for global inference. This approach achieved up to a five-fold reduction in training time without sacrificing prediction accuracy.

Beyond model and partitioning strategies, efficient system designs and data management are critical for minimizing communication overhead and maximizing throughput. GE2 \textcite{zheng2024} presented a general and efficient KGE learning system that offloads computationally intensive operations from CPU to GPU for enhanced parallelism and proposes the novel COVER algorithm for efficient multi-GPU data swap, achieving substantial training speedups (2x to 7.5x) by minimizing communication costs. In distributed and privacy-sensitive environments, Federated KGE (FKGE) introduces unique challenges. FedS \textcite{zhang2024} addressed communication efficiency in FKGE by proposing an Entity-Wise Top-K Sparsification strategy. This method dynamically identifies and transmits only the most changed entity embeddings, coupled with an intermittent synchronization mechanism to handle embedding inconsistency, significantly reducing communication overhead with negligible performance degradation.

In conclusion, the evolution of KGE scalability techniques has progressed from initial architectural optimizations and parameter reduction to sophisticated distributed training frameworks and highly efficient system designs. While significant strides have been made in managing computational and memory costs, particularly for GNN-based models and federated learning, challenges persist in balancing accuracy with efficiency in highly dynamic, heterogeneous, and extremely large-scale distributed environments. Future research will likely focus on more adaptive resource allocation, novel communication-efficient algorithms for diverse distributed settings, and integrating hardware-aware optimizations to further push the boundaries of KGE scalability.