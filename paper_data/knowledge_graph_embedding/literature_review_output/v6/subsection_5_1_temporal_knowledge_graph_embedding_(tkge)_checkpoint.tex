\subsection{Temporal Knowledge Graph Embedding (TKGE)}
The dynamic nature of real-world knowledge graphs (KGs), where facts evolve, appear, and disappear over time, necessitates embedding models capable of capturing these temporal dynamics. Moving beyond the static representations discussed in previous sections, Temporal Knowledge Graph Embedding (TKGE) models are specifically designed to represent entities and relations as they change through time, enabling reasoning over temporal sequences and understanding the evolution of facts. This subsection details the progression of TKGE, from early explicit temporal modeling to advanced geometric transformations and multi-curvature spaces, addressing the complexities of dynamic, spatiotemporal, and fuzzy knowledge.

The evolution of TKGE research can be broadly categorized into three main methodological families: (1) approaches that explicitly structure and model time, often through tensor representations or time series analysis; (2) methods leveraging geometric transformations like rotations to capture temporal evolution; and (3) advanced models employing multi-curvature spaces or Graph Neural Networks (GNNs) for more intricate temporal and spatiotemporal dynamics.

\subsubsection*{Explicit Temporal Modeling and Tensor/Time Series Approaches}
Early efforts in TKGE focused on explicitly integrating time into the embedding space. \cite{dasgupta2018} introduced \textbf{HyTE} (Hyperplane-based Temporally aware Knowledge Graph Embedding), a pioneering model that addresses the problem of static KGEs ignoring temporal validity. HyTE's core innovation is to associate each timestamp with a corresponding hyperplane in the entity-relation embedding space. This allows for temporal guidance during KG inference and the prediction of temporal scopes for facts with missing time annotations. HyTE succeeds under conditions where temporal information can be discretely assigned to hyperplanes and offers a more expressive temporal representation than purely static models. However, its theoretical limitation lies in its potential inability to capture highly complex, non-linear temporal dependencies or continuous temporal evolution, as hyperplanes might oversimplify intricate temporal patterns. Practically, managing and learning distinct hyperplanes for a vast number of timestamps can introduce computational overhead, especially for fine-grained temporal data.

Building on the concept of explicit temporal integration, \cite{lin2020} proposed a \textbf{Tensor Decomposition-Based Temporal Knowledge Graph Embedding} model. This approach tackles the problem of sparse and time-evolving data by regarding the entire fact set as a fourth-order tensor (head, relation, tail, time). The core innovation is to apply tensor decomposition techniques (e.g., CP or TuckER decomposition extended to four dimensions) to learn dense, low-dimensional vectors for entities, relations, and time. This method claims to generalize well to other static tensor-based KGEs and demonstrates superior performance on temporal datasets. It succeeds in scenarios where temporal data can be naturally represented as a discrete dimension within a tensor. A theoretical limitation is that tensor decomposition often assumes a fixed dimensionality for time, which might not be ideal for continuously evolving or irregularly sampled temporal data. Practical limitations include the high computational cost and memory requirements associated with higher-order tensor operations, particularly for very large and dense temporal KGs, which can hinder scalability.

Addressing the challenge of temporal uncertainty and continuous evolution, \cite{xu2019} introduced \textbf{ATiSE} (Temporal Knowledge Graph Embedding Model based on Additive Time Series Decomposition). ATiSE's innovation lies in incorporating time using additive time series decomposition and mapping representations into multi-dimensional Gaussian distributions. The mean of each entity/relation embedding at a time step represents its expected position, while its covariance captures temporal uncertainty. This probabilistic approach claims to achieve state-of-the-art results on link prediction by accounting for the inherent fuzziness of temporal evolution. ATiSE excels when temporal uncertainty is a significant factor, providing a more robust representation than deterministic models. However, its theoretical limitation is the assumption of additive time series decomposition, which might not hold for all complex, non-linear temporal dynamics. Practically, learning and managing Gaussian distributions for all entities and relations across time can be computationally intensive, and the interpretability of learned covariances might be challenging.

More recently, \cite{li2023} presented \textbf{TeAST} (Temporal Knowledge Graph Embedding via Archimedean Spiral Timeline), which offers a novel way to model temporal relations. TeAST addresses the limitations of existing methods that fuse temporal information into entities, potentially leading to entity information evolution and limiting link prediction performance. Its core innovation is mapping relations onto an Archimedean spiral timeline, transforming the quadruple completion problem into a 3rd-order tensor completion problem. The spiral timeline ensures that simultaneously occurring relations are placed on the same timeline, and all relations evolve orderly over time with a spiral regularizer. TeAST claims to encode various relation patterns and provide interpretability. It succeeds in scenarios where a continuous, orderly temporal progression can be assumed. A theoretical limitation is that the Archimedean spiral might impose a specific, potentially restrictive, structure on temporal evolution that may not generalize to all types of temporal dynamics. Practically, the complexity of implementing and optimizing the spiral timeline and its regularizer adds to the model's intricacy.

\subsubsection*{Geometric Transformation-based Temporal Embeddings}
A significant paradigm shift in TKGE involves leveraging geometric transformations, particularly rotations, to model temporal dynamics. \cite{xu2020} proposed \textbf{TeRo} (A Time-aware Knowledge Graph Embedding via Temporal Rotation), which defines the temporal evolution of entity embeddings as a rotation from an initial time to the current time in a complex vector space. TeRo's innovation lies in using dual complex embeddings for relations involving time intervals (beginning and end), allowing it to capture rich interaction between temporal and multi-relational characteristics. TeRo claims to overcome limitations of existing models and infer various relation patterns over time, outperforming state-of-the-art models for temporal link prediction. It performs well on datasets with clear temporal sequences and where relational patterns like symmetry and inversion are prominent. However, the interpretability of complex rotations in high-dimensional spaces can be a theoretical limitation, and the computational cost of learning these transformations can be high.

Building on TeRo's insights, \cite{sadeghian2021} introduced \textbf{ChronoR} (Rotation Based Temporal Knowledge Graph Embedding). ChronoR extends the concept by learning a k-dimensional rotation transformation, parameterized by both relation and time, to transform a head entity to fall near its tail entity. This approach, using high-dimensional rotation, aims to capture richer interactions between temporal and multi-relational characteristics. ChronoR demonstrates superior performance on benchmark datasets for temporal link prediction. While both TeRo and ChronoR focus on rotation, ChronoR's use of k-dimensional rotations offers potentially greater flexibility in modeling complex interactions. A common tension across these rotational models is balancing the expressiveness gained from complex rotations with the increased computational complexity and potential difficulty in interpreting the learned transformations.

More recently, the geometric transformation paradigm has been extended to address additional complexities like fuzziness and spatiotemporal dimensions. \cite{ji2024} presented \textbf{FSTRE} (Fuzzy Spatiotemporal RDF Knowledge Graph Embedding Using Uncertain Dynamic Vector Projection and Rotation). FSTRE addresses the problem of existing KGE models being insufficient for uncertain and dynamic knowledge. Its core innovation is to embed spatial and temporal information by projection and rotation within a complex vector space, while introducing fine-grained fuzziness through modal lengths of anisotropic vectors. This allows FSTRE to capture rich interactions between crisp/static and fuzzy spatiotemporal knowledge. FSTRE succeeds in scenarios where both spatial and temporal dynamics are present, along with inherent uncertainty. A theoretical limitation is the complexity of simultaneously modeling fuzziness, spatial, and temporal dimensions within a single geometric framework, which might lead to intricate optimization landscapes.

Further advancing this, \cite{ji2024} proposed a \textbf{Multihop Fuzzy Spatiotemporal RDF Knowledge Graph Query via Quaternion Embedding}. This model tackles the significant challenge of multihop querying on *incomplete* fuzzy spatiotemporal KGs, where previous embedding-based approaches overlooked uncertainty and spatiotemporal sensitivity during reasoning. Its innovation lies in using quaternions to jointly embed spatiotemporal entities and represent relations as rotations from subject to object. Uncertainty is incorporated via the scoring function's bias factor, and the non-commutative compositional pattern of quaternions is exploited for more accurate multihop path reasoning. This approach offers a powerful algebraic structure for representing rotations and compositions, beneficial for complex path queries. However, the algebraic complexity of quaternions can lead to increased computational demands and potentially reduced interpretability compared to simpler vector spaces.

\subsubsection*{Multi-Curvature Space Embeddings and GNN-based Approaches}
Recent advancements in TKGE have pushed towards even more sophisticated geometric and architectural solutions. \cite{wang2024} introduced \textbf{MADE} (Multicurvature Adaptive Embedding for Temporal Knowledge Graph Completion), which addresses the challenge of embedding TKGs with complex, interwoven geometric structures (hierarchical, ring, chain) that are poorly captured by single Euclidean spaces. MADE's core innovation is modeling TKGs in multi-curvature spaces (Euclidean, hyperbolic, hyperspherical) with an adaptive weighting mechanism. This mechanism assigns different weights to curvature spaces in a data-driven manner, strengthening ideal spaces and weakening inappropriate ones. It also includes a quadruplet distributor for information interaction and an innovative temporal regularization for timestamp smoothness. MADE claims to outperform existing state-of-the-art TKGC models by adaptively leveraging the strengths of different geometries. It succeeds when TKGs exhibit diverse underlying geometric patterns. A theoretical limitation is the increased complexity of optimizing embeddings across multiple, potentially disparate, geometric spaces. Practically, the computational overhead of managing different curvatures and the adaptive weighting mechanism can be substantial.

Concurrently, \cite{wang2024} proposed \textbf{IME} (Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion), which also models TKGs in multi-curvature spaces. IME addresses the problem that existing TKGC methods often neglect the heterogeneity of different curvature spaces. Its innovation lies in incorporating "space-shared" properties to learn commonalities across spaces and alleviate spatial gaps, and "space-specific" properties to capture characteristic features. IME also introduces an Adjustable Multi-curvature Pooling (AMP) approach and designs specific similarity, difference, and structure loss functions. IME, like MADE, achieves state-of-the-art results by leveraging the power of hybrid geometries. While both MADE and IME utilize multi-curvature spaces, MADE's adaptive weighting offers a more dynamic selection of geometries, whereas IME focuses on explicit shared/specific properties and tailored loss functions. A common practical limitation for both is the increased complexity in model design and training, making them potentially harder to implement and optimize than simpler models. The interpretability of embeddings in such hybrid spaces also remains a significant challenge.

Beyond geometric spaces, Graph Neural Networks (GNNs) have also been adapted for temporal KGE. \cite{xie2023} presented \textbf{TARGAT} (A Time-Aware Relational Graph Attention Model for Temporal Knowledge Graph Embedding). TARGAT addresses the limitation of previous GNN-based models that struggle to directly capture multi-fact interactions at different timestamps. Its core innovation is a relational generator that dynamically produces time-aware relational message transformation matrices, which jointly model relations and timestamp information. These matrices project neighborhood features into different time-aware spaces for aggregation, followed by a temporal transformer classifier for query quadruples. TARGAT claims to significantly outperform prior GNN-based models and achieve state-of-the-art results on benchmark datasets. It succeeds in capturing complex, localized interactions within the graph structure over time. A practical limitation is the scalability of GNNs to extremely large and dense TKGs, as message passing can be computationally intensive. Furthermore, while it captures interactions, the interpretability of the learned time-aware transformations might still be a challenge compared to more geometrically intuitive models.

\subsubsection*{Comparative Framework and Evolution}
The intellectual trajectory of TKGE research demonstrates a clear evolution from explicitly modeling time as an additional dimension to integrating it deeply within complex geometric and algebraic structures. Early models like \textbf{HyTE} \cite{dasgupta2018} and \textbf{Tensor Decomposition} \cite{lin2020} established the feasibility of temporal awareness, treating time as a distinct, often discrete, variable. This laid the groundwork for more sophisticated approaches. \textbf{ATiSE} \cite{xu2019} and \textbf{TeAST} \cite{li2023} then introduced probabilistic and structured timeline representations, respectively, to better handle temporal uncertainty and continuous evolution, moving beyond simple discrete timestamps.

A significant shift occurred with the advent of \textbf{geometric transformation-based models} like \textbf{TeRo} \cite{xu2020} and \textbf{ChronoR} \cite{sadeghian2021}. These models evolved from the static rotational KGEs (e.g., RotatE \cite{sun2018} from Section 2.2) by parameterizing rotations with time, allowing entities and relations to evolve dynamically. This family excels at capturing complex relational patterns (symmetry, inversion, composition) as they change over time. However, these models often assume crisp temporal facts. The most recent advancements, such as \textbf{FSTRE} \cite{ji2024} and the \textbf{Quaternion Embedding} approach \cite{ji2024}, further extend this by incorporating fuzziness and spatiotemporal dimensions, acknowledging the inherent uncertainty and geographical context of real-world knowledge. This exemplifies a broader theme in KGE research: the move from simplified, crisp data assumptions to models robust enough for noisy, multi-faceted real-world data.

The latest frontier, represented by \textbf{MADE} \cite{wang2024} and \textbf{IME} \cite{wang2024}, addresses a fundamental geometric limitation. While earlier models focused on *how* entities change over time, these multi-curvature models tackle *where* they are best represented. They recognize that a single Euclidean space is insufficient for the diverse geometric structures (hierarchical, chain-like, ring-like) present in dynamic KGs. By adaptively combining hyperbolic, hyperspherical, and Euclidean spaces, they aim to capture these intricate structures more accurately, pushing the boundaries of representational capacity. This directly addresses the limitations of single-space embeddings, a recurring critique in KGE research as highlighted in \cite{cao2022}.

\textbf{TARGAT} \cite{xie2023}, on the other hand, represents a divergence in methodology, leveraging the power of GNNs. While many KGE models operate on graph data, TARGAT explicitly uses attention-based message passing to model multi-fact interactions across timestamps. This contrasts with the more mathematically constrained geometric approaches, offering a data-driven way to learn complex temporal dependencies.

A critical tension across TKGE models lies in the trade-off between \textit{expressiveness} and \textit{computational complexity}. Models like MADE and IME, while highly expressive due to their multi-curvature nature, introduce significant optimization challenges and computational overhead. Similarly, quaternion-based embeddings, while powerful for composition and spatiotemporal modeling, are algebraically more complex than simpler vector operations. This echoes the efficiency concerns raised in Subsection 6.1, where models like DualDE \cite{zhu2020} and LightKG \cite{wang2021} aim to mitigate the parameter explosion in static KGEs; TKGE models face similar, if not greater, challenges due to the added temporal dimension.

Another unstated assumption in many TKGE models is the availability of precise timestamp information. While HyTE can predict missing time annotations, many models rely on well-defined discrete or continuous timestamps. This assumption breaks down in scenarios with fuzzy or uncertain temporal boundaries, which FSTRE \cite{ji2024} and the Quaternion Embedding \cite{ji2024} explicitly address. The field is steadily moving towards models that can handle such ambiguities, reflecting a broader trend towards robustness against data imperfections, as discussed in Subsection 6.2.

In conclusion, TKGE research has rapidly evolved from basic temporal awareness to highly sophisticated models that integrate complex geometries, algebraic structures, and deep learning architectures. The progression shows a clear drive to move beyond static, crisp, and Euclidean representations to encompass the full fluidity, uncertainty, and multi-faceted nature of real-world knowledge. The ongoing challenge remains to balance this increasing expressiveness with computational efficiency, interpretability, and robust handling of diverse temporal data characteristics.