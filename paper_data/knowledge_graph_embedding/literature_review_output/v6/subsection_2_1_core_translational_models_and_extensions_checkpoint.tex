\section*{Core Translational Models and Extensions}

The foundational era of Knowledge Graph Embedding (KGE) research was significantly shaped by a family of models that conceptualized relations as translational operations within a continuous vector space. These pioneering translational models, notably TransE, TransH, TransR, and TransD, established a fundamental paradigm for representing symbolic knowledge in a machine-understandable format. They offered enhanced efficiency and expressiveness over purely symbolic methods, marking a crucial step in transforming knowledge graph analysis and laying the groundwork for subsequent geometric and deep learning advancements \cite{asmara2023}.

The seminal work, \textbf{TransE} (Translating Embeddings) \cite{bordes2013}, introduced the intuitive idea that if a triple $(h, r, t)$ exists in a knowledge graph, then the embedding of the head entity ($\mathbf{h}$) plus the embedding of the relation ($\mathbf{r}$) should approximately equal the embedding of the tail entity ($\mathbf{t}$), i.e., $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$. This simple vector addition in Euclidean space, typically minimized using a scoring function like $f(h,r,t) = ||\mathbf{h} + \mathbf{r} - \mathbf{t}||_{L1/L2}$, offered remarkable efficiency and a straightforward mechanism for link prediction. However, TransE suffered from a critical theoretical limitation: its inherent inability to effectively model complex relation patterns such as one-to-many, many-to-one, and many-to-many relations. This limitation arises because TransE represents each entity and relation as a single point or vector in the same embedding space. For instance, if a relation `(country, hasCapital, city)` is one-to-many (e.g., `(France, hasCapital, Paris)` and `(France, hasCapital, Lyon)` if considering historical capitals), TransE would force the embeddings of `Paris` and `Lyon` to be very close to each other relative to `France` and `hasCapital`, which is semantically incorrect and leads to a "blurring" of entity representations. This issue, as highlighted by \cite{wang2014}, motivated the development of more sophisticated translational models.

Building directly on TransE's insights, \cite{wang2014} introduced \textbf{TransH} (Translating on Hyperplanes) to address the challenge of modeling one-to-many and many-to-one relations more effectively. The core innovation of TransH lies in representing each relation not as a single vector in the entity embedding space, but as a hyperplane ($\mathbf{w}_r$) along with a translation vector ($\mathbf{r}$) on that hyperplane. For a given triple $(h, r, t)$, TransH first projects the head and tail entity embeddings onto the relation-specific hyperplane: $\mathbf{h}_{\perp} = \mathbf{h} - \mathbf{w}_r^\top \mathbf{h} \mathbf{w}_r$ and $\mathbf{t}_{\perp} = \mathbf{t} - \mathbf{w}_r^\top \mathbf{t} \mathbf{w}_r$. The translational assumption then applies to these projected embeddings: $||\mathbf{h}_{\perp} + \mathbf{r} - \mathbf{t}_{\perp}||_{L1/L2}$. Unlike TransE, which uses a single embedding for each entity regardless of the relation, TransH allows a single entity to have different representations (projections) when involved in different relations. This mechanism enables the model to handle multiple tail entities for a single head (one-to-many) or multiple head entities for a single tail (many-to-one) without forcing their original embeddings to be identical. TransH successfully claims to solve the problem of preserving diverse mapping properties of relations with almost the same model complexity as TransE, achieving significant improvements in predictive accuracy on benchmark datasets like WordNet and Freebase \cite{wang2014}. Its practical limitation is a slight increase in parameters compared to TransE due to the additional hyperplane normal vector for each relation, but it maintains competitive scalability.

Following TransH, models like \textbf{TransR} \cite{lin2015} and CTransR further explored the idea of relation-specific transformations by introducing relation-specific projection matrices. These models generally map entities from the entity embedding space into a relation-specific space *before* applying the translation. For instance, TransR projects $\mathbf{h}$ and $\mathbf{t}$ into a relation-specific space using a matrix $\mathbf{M}_r$, then applies the translation: $||\mathbf{h}\mathbf{M}_r + \mathbf{r} - \mathbf{t}\mathbf{M}_r||_{L1/L2}$. This approach aimed to capture more fine-grained relational semantics than TransH's hyperplane projection. However, a significant practical limitation of TransR/CTransR was the high parameter count. Each relation required its own dense projection matrix $\mathbf{M}_r$, leading to a parameter complexity of $O(N_r \cdot d_e \cdot d_r)$ where $N_r$ is the number of relations, $d_e$ is entity embedding dimension, and $d_r$ is relation embedding dimension. This quadratic growth in parameters with respect to embedding dimensions hindered scalability for large knowledge graphs, making them less practical for real-world applications with many relations.

This parameter explosion limitation was directly addressed by \cite{ji2015} with the introduction of \textbf{TransD} (Knowledge Graph Embedding via Dynamic Mapping Matrix). TransD claims to solve the problem of high parameter count in TransR/CTransR while considering the diversity of both relations and entities. Its core innovation is the use of dynamic mapping matrices. Instead of a static, dense projection matrix for each relation, TransD assigns two vectors to each entity ($\mathbf{e}, \mathbf{e}_p$) and relation ($\mathbf{r}, \mathbf{r}_p$). The first vector represents the entity's or relation's meaning, while the second is used to dynamically construct a mapping matrix for projection. Specifically, the projection of an entity $\mathbf{h}$ for relation $\mathbf{r}$ is given by $\mathbf{h}_{\perp r} = \mathbf{h} + \mathbf{h}_p \mathbf{r}_p^\top \mathbf{h}$, and similarly for $\mathbf{t}$. The scoring function then follows the translational principle: $||\mathbf{h}_{\perp r} + \mathbf{r} - \mathbf{t}_{\perp r}||_{L1/L2}$. In contrast to TransR's fixed, dense matrices, TransD's dynamic, entity-specific projection mechanism allows for more fine-grained transformations while significantly reducing the number of parameters to $O(N_e \cdot d_e + N_r \cdot d_r)$, as it avoids storing large, dense projection matrices. This makes TransD a more scalable solution for large-scale graphs, demonstrating superior performance on triplet classification and link prediction tasks \cite{ji2015}. Its practical advantage lies in its parameter efficiency and absence of computationally intensive matrix-vector multiplication operations during projection, making it more applicable to large KGs. However, like other translational models, its theoretical limitations persist in fully capturing highly complex logical patterns such as composition or inversion, which later rotational models would specifically target.

Further extensions to the translational paradigm include models like \textbf{TransE-MTP} (TransE with Multi-Translation Principles) \cite{additional_paper_1}. This model directly addresses TransE's struggle with complex relations by proposing *multiple translation principles* tailored for different relation types (e.g., one-to-one, one-to-many, many-to-one, many-to-many). Unlike TransH and TransD, which modify the projection mechanism, TransE-MTP modifies the core translational rule itself based on the identified relation type. It combines these MTPs with the original TransE framework, allowing for multiple optimization objectives during training on complex relations. This approach claims to provide superior prediction performance over both TransE and TransH, demonstrating its effectiveness at link prediction and triplet classification on Freebase and WordNet datasets \cite{additional_paper_1}. This illustrates a pattern in translational model evolution: the initial simplicity of TransE led to subsequent models either introducing relation-specific projections (TransH, TransR, TransD) or adapting the translational principle itself (TransE-MTP) to better handle the diversity of real-world relational patterns.

In summary, the evolution of core translational models showcases a continuous effort to balance efficiency with expressiveness. Early work like TransE established the foundational idea of relations as translations, prioritizing computational efficiency. Mid-period models such as TransH and TransD systematically addressed TransE's limitations regarding complex relation patterns (one-to-many, many-to-one) by introducing relation-specific projections, first via hyperplanes and then through dynamic, parameter-efficient mapping matrices. This progression from TransE to TransH and then to TransD \cite{asmara2023} illustrates a clear causal relationship: the limitations of simpler models (e.g., TransE's inability to distinguish multiple tail entities for a single head) directly led to innovations in subsequent models (e.g., TransH's hyperplanes, TransD's dynamic matrices) to overcome these specific challenges. TransR, while innovative in its use of projection matrices, highlighted the trade-off between expressiveness and parameter count, a challenge that TransD successfully mitigated. The introduction of TransE-MTP further refined this by adapting the translational principle itself to relation types. This intellectual trajectory exemplifies the field's shift from basic vector operations to more nuanced geometric transformations, all while striving for models that are both effective and scalable for real-world knowledge graphs.