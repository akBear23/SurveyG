\subsection{Multi-modal and Cross-domain KGE}
Building on the discussion of integrating logical rules and structural constraints, which enrich Knowledge Graph Embeddings (KGEs) with explicit consistency and reasoning capabilities, this subsection explores a complementary paradigm: leveraging multi-modal and cross-domain information. While rule-based methods inject logical structure, multi-modal and cross-domain KGEs aim to enrich representations with diverse semantic content from external sources, such as textual descriptions, visual features, or data from heterogeneous domains. This approach is crucial for addressing data sparsity, enhancing semantic understanding, and enabling more comprehensive knowledge comprehension by leveraging complementary information that is often absent in the sparse, symbolic graph structure alone \cite{dai2020}.

The integration of multi-modal and cross-domain data into KGE models represents a significant step towards grounding abstract knowledge in real-world observations and connecting disparate information silos. This paradigm moves beyond the limitations of relying solely on graph structure, which can be incomplete or lack fine-grained semantic detail, by drawing on the richness of natural language, visual cues, or domain-specific auxiliary data. Three main categories of approaches emerge:
\begin{enumerate}
    \item \textbf{Text-Enhanced KGE}: Models that integrate textual descriptions of entities and relations to enrich their semantic representations.
    \item \textbf{Cross-Domain KGE}: Approaches designed to learn embeddings that capture relationships and interactions across different knowledge domains, often for specific applications like recommendation.
    \item \textbf{Domain-Specific Multi-modal Reasoning}: Models that combine various data modalities within a specialized domain to facilitate complex reasoning and knowledge discovery.
\end{enumerate}

\subsubsection*{Text-Enhanced Knowledge Graph Embedding}
The most common form of multi-modal integration in KGE involves leveraging textual descriptions associated with entities and relations. These descriptions provide rich semantic context that can resolve ambiguities and enrich embeddings, especially for sparse entities.

\textbf{SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions} \cite{xiao2016} is an early and influential work that addresses the "weak-semantic" nature of purely geometric KGE models, which often struggle to capture nuanced meanings from symbolic triples alone.
\begin{itemize}
    \item \textbf{Problem Solved}: SSP aims to overcome the semantic limitations of traditional KGEs by jointly learning from symbolic triples and textual descriptions, thereby providing more precise semantic embeddings. It addresses the challenge of integrating heterogeneous data sources (structured triples and unstructured text).
    \item \textbf{Core Innovation}: SSP proposes a semantic space projection model that builds an interaction between symbolic triples and textual descriptions. It uses textual descriptions to discover semantic relevance and project information into a shared semantic space. This allows the model to leverage the rich contextual information present in text to refine entity and relation representations.
    \item \textbf{Conditions for Success}: SSP demonstrates substantial improvements in knowledge graph completion and entity classification on benchmark datasets. Its success is contingent on the availability and quality of textual descriptions for entities and relations. The model performs well when text provides complementary semantic information that is not easily inferable from the graph structure.
    \item \textbf{Theoretical Limitations}: While effective, SSP's projection mechanism might be limited in capturing highly complex, multi-faceted semantic relationships compared to more advanced language models. The interaction between symbolic and textual information is modeled through a relatively simple projection, which might not fully exploit deep semantic patterns.
    \item \textbf{Practical Limitations}: The quality of the textual descriptions is paramount; noisy or irrelevant text can degrade performance. The model's reliance on pre-computed text embeddings (e.g., from word2vec) means its semantic understanding is bounded by the capabilities of those embeddings.
    \item \textbf{Comparison to Alternatives}: Compared to purely structural KGEs (e.g., \cite{ji2015} TransD, \cite{ebisu2017} TorusE, \cite{yang2021} CyclE), SSP provides a significant semantic boost by grounding embeddings in natural language. It represents an early step in multi-modal KGE, preceding the widespread adoption of large pre-trained language models (PLMs).
\end{itemize}

More recently, the advent of powerful pre-trained language models has revolutionized text-enhanced KGE. \textbf{Joint Language Semantic and Structure Embedding for Knowledge Graph Completion} \cite{shen2022} exemplifies this by integrating deep language semantics with structural information.
\begin{itemize}
    \item \textbf{Problem Solved}: This work addresses the need for more robust KGEs, particularly in low-resource settings, by effectively combining the rich semantics from natural language descriptions with the structural information of knowledge graphs. It aims to overcome data sparsity by leveraging external textual context.
    \item \textbf{Core Innovation}: The method fine-tunes pre-trained language models (PLMs) with a probabilistic structured loss. The forward pass of the PLM captures deep semantics from natural language descriptions of knowledge triplets, while the structured loss ensures the reconstruction of graph structure. This joint learning paradigm allows for a synergistic capture of both semantic and structural cues.
    \item \textbf{Conditions for Success}: The model achieves state-of-the-art performance on various knowledge graph benchmarks, showing significant improvements, especially in low-resource regimes. Its success relies on the powerful semantic understanding capabilities of PLMs and the ability to effectively fine-tune them for the KGE task.
    \item \textbf{Theoretical Limitations}: The computational cost and memory requirements of fine-tuning large PLMs can be substantial, limiting scalability for extremely large KGs or resource-constrained environments. The effectiveness is also tied to the quality of the PLM and the relevance of its pre-training data to the KG domain.
    \item \textbf{Practical Limitations}: Requires significant computational resources (GPUs) for training. The model's performance can be sensitive to hyperparameter tuning for PLM fine-tuning.
    \item \textbf{Comparison to Alternatives}: This approach represents a significant advancement over earlier text-enhanced methods like SSP \cite{xiao2016}. While SSP uses simpler text embeddings and projection, \cite{shen2022} leverages the deep, contextualized semantic understanding of PLMs, leading to superior performance, particularly in capturing nuanced meanings and handling data sparsity. This reflects the broader field's shift towards integrating advanced deep learning models for semantic enrichment.
\end{itemize}

\subsubsection*{Cross-Domain and Domain-Specific Multi-modal Reasoning}
Beyond general text enhancement, KGEs are increasingly adapted to integrate diverse data types across domains or within specialized fields, enabling complex applications.

\textbf{Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation} \cite{liu2023} addresses critical challenges in recommender systems, namely cross-domain cold start and the provision of multi-domain recommendations, which traditional systems often struggle with due to data sparsity and domain heterogeneity.
\begin{itemize}
    \item \textbf{Problem Solved}: This paper aims to efficiently model associations and interactions between items across diverse domains to enable multi-domain item-item (I2I) recommendations. It tackles the challenge of integrating information from different domains to enrich item representations.
    \item \textbf{Core Innovation}: A "cross-domain knowledge graph chiasmal embedding" approach is proposed, which efficiently interacts all items in multiple domains. To achieve both homo-domain (within-domain) and hetero-domain (cross-domain) embeddings, a novel "binding rule" is introduced. This allows the model to learn shared and domain-specific representations simultaneously, facilitating link prediction for I2I recommendations.
    \item \textbf{Conditions for Success}: The method achieves better link prediction and multi-domain recommendation results on two benchmark datasets. Its success depends on the existence of a knowledge graph that contains rich information about items and their associations, both within and across domains. The "binding rule" is crucial for effectively managing the interaction between different domains.
    \item \textbf{Theoretical Limitations}: The effectiveness of the chiasmal embedding and binding rule is predicated on the assumption that meaningful cross-domain links or shared entities exist within the knowledge graph. If domains are too disparate with few bridging connections, the cross-domain interaction might be limited. The model's focus on I2I recommendation might not directly translate to other cross-domain tasks.
    \item \textbf{Practical Limitations}: Constructing a comprehensive cross-domain knowledge graph can be a significant data engineering challenge. The model's performance is sensitive to the quality and density of cross-domain links.
    \item \textbf{Comparison to Alternatives}: Unlike general KGEs that focus on single-domain link prediction, this work specifically targets the complex multi-domain recommendation problem. It offers a more integrated approach than simply training separate KGEs for each domain, by explicitly modeling cross-domain interactions through its chiasmal embedding and binding rule.
\end{itemize}

In specialized domains, multi-modal KGE can drive knowledge discovery. \textbf{Multimodal reasoning based on knowledge graph embedding for specific diseases} \cite{zhu2022} showcases this in the biomedical field.
\begin{itemize}
    \item \textbf{Problem Solved}: This work addresses the challenge of discovering new and reliable knowledge from existing biomedical KGs, particularly for specific diseases, by leveraging multimodal reasoning. It aims to provide universal pre-trained knowledge for specific disease fields.
    \item \textbf{Core Innovation}: The paper develops a process for constructing Specific Disease Knowledge Graphs (SDKGs) and implements multimodal reasoning using reverse-hyperplane projection. This approach integrates structural, category, and description embeddings, allowing for a richer representation of biomedical entities and relations. The combination of these modalities leads to the discovery of new, reliable drug-gene, gene-disease, and disease-drug pairs.
    \item \textbf{Conditions for Success}: Multimodal reasoning improves pre-existing models on all SDKGs using entity prediction tasks. The model's reliability in discovering new knowledge is verified through manual proofreading. Its success is highly dependent on the availability of structured biomedical data, entity categories, and textual descriptions.
    \item \textbf{Theoretical Limitations}: The reverse-hyperplane projection, while effective, might have limitations in capturing highly complex, non-linear interactions across diverse modalities compared to more advanced neural fusion techniques. The generalizability of the SDKG construction process to vastly different disease areas might require adaptation.
    \item \textbf{Practical Limitations}: The construction of high-quality SDKGs, including entity linking and relation linking, is a labor-intensive and domain-specific task. The manual proofreading for validation, while robust, is not scalable for large-scale knowledge discovery.
    \item \textbf{Comparison to Alternatives}: This model stands out by explicitly focusing on domain-specific multimodal reasoning, contrasting with general-purpose KGEs. It moves beyond purely structural or text-only approaches by integrating multiple distinct modalities (structure, category, description) to achieve more comprehensive knowledge discovery in a high-stakes domain.
\end{itemize}

\subsubsection*{Comparative Framework and Evolution}
The evolution of multi-modal and cross-domain KGE demonstrates a clear trajectory from augmenting structural embeddings with simple textual features to leveraging sophisticated language models and integrating diverse data types for complex, application-specific reasoning (Table \ref{tab:multimodal_kge_comparison}). Early methods like SSP \cite{xiao2016} recognized the value of text as an auxiliary modality. The field then progressed to more powerful joint learning frameworks, exemplified by \cite{shen2022}, which harness the deep semantic understanding of pre-trained language models. Concurrently, the application scope expanded to address challenges like cross-domain recommendation \cite{liu2023} and domain-specific knowledge discovery \cite{zhu2022}, showcasing the versatility of multi-modal KGE.

\begin{table}[htbp]
    \centering
    \caption{Comparison of Multi-modal and Cross-domain KGE Approaches}
    \label{tab:multimodal_kge_comparison}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|l|l|l|l|l|}
        \hline
        \textbf{Model Family} & \textbf{Key Papers} & \textbf{Primary Modalities/Data} & \textbf{Core Mechanism} & \textbf{Problem Addressed} & \textbf{Key Advantage/Limitation} \\
        \hline
        \textbf{Text-Enhanced (Projection)} & SSP \cite{xiao2016} & Structured triples, text descriptions & Semantic space projection, interaction between sources & Weak-semantic KGEs, basic text integration & \textbf{Advantage}: Semantic enrichment. \textbf{Limitation}: Simpler text embeddings, limited deep semantic capture. \\
        \hline
        \textbf{Text-Enhanced (PLM-based)} & \cite{shen2022} & Structured triples, natural language descriptions & Fine-tuning PLMs with probabilistic structured loss & Data sparsity, deep semantic capture, low-resource settings & \textbf{Advantage}: State-of-the-art semantics, robust to sparsity. \textbf{Limitation}: High computational cost, PLM dependency. \\
        \hline
        \textbf{Cross-Domain KGE} & \cite{liu2023} & Multi-domain item KGs & Chiasmal embedding, binding rule for homo/hetero-domain interaction & Cross-domain cold start, multi-domain recommendation & \textbf{Advantage}: Handles domain heterogeneity. \textbf{Limitation}: Requires cross-domain links, specific to I2I rec. \\
        \hline
        \textbf{Domain-Specific Multi-modal} & \cite{zhu2022} & Structure, category, description embeddings (biomedical) & Reverse-hyperplane projection, multimodal fusion & Knowledge discovery in specific diseases & \textbf{Advantage}: Deep domain understanding, new knowledge discovery. \textbf{Limitation}: Domain-specific, data engineering overhead. \\
        \hline
    \end{tabular}
    }
\end{table}

\subsubsection*{Connections Across Papers and Broader Themes}
The progression from \textbf{SSP} \cite{xiao2016} to \textbf{Joint Language Semantic and Structure Embedding} \cite{shen2022} vividly illustrates the rapid advancement in leveraging textual information. While SSP uses a projection mechanism to integrate text embeddings, \cite{shen2022} capitalizes on the deep contextual understanding offered by pre-trained language models, fine-tuning them to simultaneously capture language semantics and structural consistency. This shift reflects a broader trend in AI towards integrating powerful, pre-trained foundation models into specialized tasks, moving from simpler feature engineering to complex, end-to-end learning.

These multi-modal and cross-domain approaches offer a distinct and complementary strategy to the rule-based and constraint-driven KGEs discussed in the previous subsection. While rule-based methods (e.g., \cite{guo2017} RUGE, \cite{tang2022} RulE) inject *logical consistency* and *reasoning capabilities* by enforcing explicit patterns, multi-modal methods like \cite{xiao2016} SSP and \cite{shen2022} provide *semantic richness* and *contextual understanding* by drawing from diverse external data sources. For instance, a rule might state "if A is a parent of B, then B is a child of A," but a textual description might explain *what* A and B are, adding depth to their representations. This highlights that both logical and semantic enrichments are vital for comprehensive knowledge understanding.

Furthermore, the work on \textbf{Cross-Domain KGE Chiasmal Embedding} \cite{liu2023} and \textbf{Multimodal reasoning for specific diseases} \cite{zhu2022} showcases the versatility and practical impact of KGE in complex real-world scenarios. \cite{liu2023} addresses the pragmatic problem of recommendation across disparate domains, which is a common challenge in e-commerce and content platforms. \cite{zhu2022}, on the other hand, applies multimodal KGE for scientific discovery in a high-stakes domain, demonstrating how integrating structural, categorical, and descriptive information can lead to verifiable new knowledge. Both papers exemplify how KGE moves beyond generic link prediction to solve specific, high-value problems by adapting to the inherent heterogeneity and richness of real-world data.

\subsubsection*{Patterns and Tensions}
A recurring pattern across these multi-modal and cross-domain approaches is their ability to significantly mitigate the **data sparsity problem**, a pervasive limitation of purely structural KGEs. By leveraging external information, even sparsely connected entities or relations can acquire rich representations. However, this comes with a critical tension: the **complexity of fusing heterogeneous information**. Effectively combining textual, structural, categorical, and potentially visual data requires sophisticated joint learning frameworks, which often increase model complexity and computational cost. For instance, fine-tuning PLMs in \cite{shen2022} is computationally intensive, contrasting with the relative efficiency of simpler translational models.

Another unstated assumption in many multi-modal KGEs is the **quality and alignment of the auxiliary data**. If textual descriptions are noisy, outdated, or misaligned with the symbolic facts, they can introduce errors rather than improvements. Similarly, for cross-domain KGEs, the existence of meaningful bridging entities or relations is crucial; without them, the "chiasmal embedding" of \cite{liu2023} might struggle to find relevant connections. This highlights a practical limitation: the performance of these models is often bounded by the quality of the *least reliable* modality or domain.

The field also grapples with the **interpretability** of fused multi-modal representations. While individual modalities might be interpretable (e.g., text descriptions), understanding *how* different modalities interact and contribute to a final embedding can be challenging, especially with deep learning fusion mechanisms. This is a persistent challenge that connects to broader themes of explainable AI, particularly relevant in high-stakes applications like biomedical knowledge discovery \cite{zhu2022}.

In conclusion, multi-modal and cross-domain KGEs represent a powerful evolution in knowledge representation, moving beyond the confines of symbolic graph structures to embrace the richness of diverse information sources. From projecting text into semantic spaces \cite{xiao2016} to fine-tuning large language models for joint semantic and structural understanding \cite{shen2022}, and from enabling cross-domain recommendations \cite{liu2023} to facilitating multimodal reasoning for specific diseases \cite{zhu2022}, these approaches consistently demonstrate improved expressiveness and robustness. While challenges remain in data quality, fusion complexity, and interpretability, the trajectory of this research points towards increasingly comprehensive and context-aware knowledge understanding, crucial for the next generation of intelligent systems.