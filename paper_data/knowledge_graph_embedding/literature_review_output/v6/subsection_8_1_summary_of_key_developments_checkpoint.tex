\subsection*{Summary of Key Developments}
The field of knowledge graph embedding (KGE) has undergone a remarkable evolution, transforming the static, symbolic representation of knowledge into dynamic, continuous vector spaces amenable to machine learning. This progression has been driven by a continuous quest for enhanced expressiveness, efficiency, and robustness, culminating in sophisticated models capable of extracting actionable insights for diverse AI applications \cite{yan2022, choudhary2021}.

The initial wave of KGE research was dominated by \textit{foundational geometric and algebraic models}, primarily translation-based approaches. Pioneering models like TransE \cite{bordes2013}, TransH \cite{wang2014}, TransR \cite{lin2015}, and TransD \cite{ji2015} established the paradigm of representing relations as transformations between entity embeddings. These early models addressed the challenge of efficiently capturing relational semantics by modeling relations as simple translations in Euclidean space. TransE, for instance, represented relations as vectors $h + r \approx t$. However, its simple additive nature struggled with complex mapping properties (e.g., one-to-many, many-to-one, one-to-one, many-to-many) because a single relation vector could not adequately distinguish between multiple tail entities for a given head, or vice-versa \cite{wang2014}. To mitigate this, TransH \cite{wang2014} innovated by projecting entities onto relation-specific hyperplanes, allowing for more flexible representations of entities under different relations. Similarly, TransD \cite{ji2015} introduced dynamic mapping matrices to account for the diversity of both relations and entities, reducing parameters compared to its predecessors. While efficient and scalable, a fundamental limitation of these translational models lay in their inability to fully capture intricate logical patterns like symmetry, antisymmetry, inversion, and composition, or to adequately represent polysemous relations, primarily because their linear transformations lacked the necessary algebraic properties \cite{xiao2015}. This limitation spurred the development of \textit{rotational and complex space embeddings}. In contrast to the additive nature of translational models, RotatE \cite{sun2018} modeled relations as rotations in complex vector spaces, demonstrating superior capability in inferring these complex patterns due to the inherent multiplicative properties of complex numbers. Building on this, Rotate3D \cite{gao2020} extended rotations to three-dimensional space, leveraging non-commutative properties for multi-hop reasoning. Further algebraic innovations included HousE \cite{li2022} with Householder transformations and CompoundE \cite{ge2022} and CompoundE3D \cite{ge2023}, which combined translation, rotation, and scaling operations, aiming for a more generalized and expressive framework. Other geometric approaches, such as TorusE \cite{ebrahimi2020} and CyclE \cite{ebrahimi2020}, explored embedding entities on Lie groups or using different metric choices to enhance expressiveness and circumvent regularization problems. These models continuously pushed the boundary of expressiveness by exploring richer mathematical operations and embedding spaces, with recent theoretical advancements ensuring properties like closure under composition \cite{zheng2024} and resolving theoretical deficiencies like the "Z-paradox" in existing models \cite{liu2024}. The practical limitation of these geometric models often involves a trade-off between increased expressiveness and higher computational complexity or parameter count, making them more challenging to scale than their simpler predecessors.

A significant paradigm shift occurred with the adoption of \textit{deep learning architectures} for KGE, which addressed the problem of capturing highly complex, hierarchical, and non-linear relational patterns that simpler geometric models often missed. Convolutional Neural Networks (CNNs) were introduced to extract local features and model intricate interactions between entity and relation embeddings. Models like AcrE \cite{ren2020} utilized atrous convolutions and residual learning for efficient feature interactions, specifically designed to overcome the "reduced feature resolution" and "information forgotten" issues prevalent in standard deep CNNs, while maintaining parameter efficiency. In contrast, ReInceptionE \cite{xie2020} integrated an Inception network with attention for joint local-global structural information. More advanced CNN-based approaches like M-DCN \cite{zhang2020}, CNN-ECFA \cite{hu2024}, and SEConv \cite{yang2025} further refined multi-scale feature aggregation and entity-specific common feature learning, demonstrating CNNs' power in capturing non-linear patterns. PConvKB \cite{PConvKB} further enhanced CNN-based KGE by incorporating relation paths, both locally via attention and globally via a novel DIPF measure, distinguishing itself from models that only utilize triple information. Graph Neural Networks (GNNs) and attention mechanisms also became central, leveraging the graph's topology to learn context-dependent embeddings. Early work like Logic Attention-based Neighborhood Aggregation \cite{wang2018} provided inductive capabilities by aggregating neighbor information. DisenKGAT \cite{wu2021} introduced disentangled graph attention networks for more diverse and independent component representations, while GAATs \cite{wang2020} incorporated graph-attenuated attention to better extract features from neighbor nodes and relation paths. However, a key limitation of many GNN-based KGC models is their "data dependence," where performance is sensitive to local graph structures, and their message functions primarily operate in Euclidean space, failing to capture richer intrinsic structural information \cite{shang2024}. To address this, MGTCA \cite{shang2024} proposed a Mixed Geometry Message Function (MGMF) integrating hyperbolic, hypersphere, and Euclidean spaces, alongside a Trainable Convolutional Attention Network (TCAN) that adaptively switches between GNN types (GCN, GAT, KGCAT) to overcome data dependence. The emergence of Transformer architectures further revolutionized KGE, with models like CoKE \cite{wang2019}, Knowformer \cite{li2023}, and TGformer \cite{shi2025} adapting self-attention mechanisms to capture long-range dependencies and contextualized representations. While these deep learning models achieve state-of-the-art accuracy by automatically extracting features and flexibly modeling interactions, their core practical limitations include increased computational cost, higher parameter counts, and reduced interpretability compared to their geometric counterparts.

Beyond purely structural modeling, the field has increasingly focused on \textit{enriching KGE with auxiliary information, rules, and multi-modality}. This addresses the inherent incompleteness and semantic ambiguity of raw knowledge graphs. Approaches like TransET \cite{wang2021} and TaKE \cite{he2023} incorporated entity type information to provide semantic guidance, leading to more discriminative embeddings for KG completion. Similarly, AEKE \cite{zhang2024} leveraged entity attributes to make KGE models robust against noisy or erroneous triples, a critical practical concern, while HINGE \cite{chen2020} extended beyond triplets to hyper-relational facts. The integration of \textit{logical rules and constraints} also gained prominence, with models like RUGE \cite{guo2017} and RulE \cite{tang2022} iteratively guiding embeddings with soft rules to enforce consistency and enhance reasoning capabilities. These methods solve the problem of injecting prior knowledge and ensuring logical coherence, moving beyond purely data-driven learning. However, a critical assumption here is the availability and quality of such explicit rules, which can be challenging to extract or define for large, dynamic KGs. Furthermore, \textit{multi-modal and cross-domain KGE} emerged to tackle data sparsity and enrich semantic understanding. SSP \cite{xiao2016} projected textual descriptions into semantic space, while more recent works like "Joint Language Semantic and Structure Embedding" \cite{shen2022} fine-tuned pre-trained language models to jointly learn from text and structure. Cross-Domain Knowledge Graph Chiasmal Embedding \cite{liu2023} and multimodal reasoning for specific diseases \cite{zhu2022} exemplify the application of multi-modal KGE to complex, specialized domains. These approaches collectively aim for a more comprehensive and nuanced knowledge representation by leveraging diverse, complementary information sources. A common practical limitation across all these enrichment strategies is the availability and quality of such auxiliary or multi-modal data, and the inherent complexity of effectively fusing heterogeneous information without introducing new biases or noise.

The field has also made significant strides in addressing \textit{practical considerations} such as efficiency, robustness, and adaptability to dynamic environments. To counter the computational and storage demands of large KGs, techniques like knowledge distillation (DualDE \cite{zhu2020}), embedding compression \cite{sachan2020, wang2021}, and parameter-efficient learning (EARL \cite{chen2023}) have been developed. System-level optimizations like GE2 \cite{zheng2024} further accelerate KGE training. For instance, DualDE reduces model size and inference time by distilling knowledge from a larger teacher model, whereas LightKG focuses on compressing the embedding vectors themselves. Robustness against noisy data and imbalanced distributions has been tackled through methods like confidence-aware negative sampling \cite{shan2018}, reinforcement learning-based noise filtering \cite{zhang2021}, and weighted training schemes \cite{zhang2023}. The critical role of negative sampling in KGE training has been extensively studied, leading to sophisticated strategies \cite{zhang2018, qian2021} and even non-sampling approaches \cite{li2021} to improve accuracy and efficiency. The dynamic nature of real-world KGs spurred research into \textit{inductive and continual KGE}. Approaches like Logic Attention-based Neighborhood Aggregation \cite{wang2018} and meta-learning strategies \cite{chen2021, sun2024} enable models to embed unseen entities and adapt to new facts efficiently. These continual learning models, however, face the persistent challenge of catastrophic forgetting, where acquiring new knowledge can degrade performance on previously learned information; techniques like incremental LoRA \cite{liu2024} aim to mitigate this by parameter-efficient adaptation. Finally, the growing concern for data privacy led to the emergence of \textit{Federated KGE (FKGE)}, with solutions addressing communication efficiency \cite{zhang2024}, personalization for diverse client data \cite{zhang2024}, and security vulnerabilities like poisoning attacks \cite{zhou2024}. While FKGE offers significant privacy benefits, it introduces a trade-off with communication overhead and the complexity of aggregating disparate local models. A comprehensive empirical study by \cite{ali2020} further highlighted that reported KGE performance is highly sensitive to hyperparameter tuning and training configurations, often revealing reproducibility failures. This critique underscores the need for robust training optimization and standardized evaluation practices, as architectural innovation alone does not guarantee reliable real-world performance.

In summary, the narrative arc of KGE research traces a path from simple, elegant geometric models to highly sophisticated deep learning architectures, continuously integrating richer semantic context, logical constraints, and multi-modal information. This evolution reflects a persistent effort to enhance expressiveness, efficiency, and robustness, while increasingly focusing on adaptability to dynamic, inductive, and distributed knowledge environments. The significant strides made in transforming symbolic knowledge into actionable insights have positioned KGE as a cornerstone for modern AI systems, enabling tasks from link prediction and question answering to personalized recommendation and scientific discovery.