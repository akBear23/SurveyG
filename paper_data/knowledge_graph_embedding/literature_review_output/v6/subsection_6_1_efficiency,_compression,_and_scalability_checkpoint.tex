\subsection{Efficiency, Compression, and Scalability}
As Knowledge Graph Embedding (KGE) models have grown in expressiveness and complexity, as discussed in Sections 2 and 3, the practical challenges of their deployment have become increasingly prominent. The sheer scale of real-world knowledge graphs necessitates techniques that enhance efficiency, reduce memory footprint, and ensure scalability during both training and inference. This subsection examines key innovations addressing these practical bottlenecks, ranging from model-agnostic system optimizations to model-specific compression and parameter-efficient learning strategies.

A primary driver for this research area is the inherent trade-off between model capacity (and thus, performance) and computational resource requirements. Early KGE models, while efficient, often lacked expressiveness. Modern, highly expressive models, particularly those leveraging deep learning architectures, tend to be parameter-heavy and computationally intensive. This tension has spurred the development of several distinct, yet complementary, methodological families:
\begin{enumerate}
    \item \textbf{Knowledge Distillation and Embedding Compression:} These approaches aim to reduce the size of the embedding layer and accelerate inference by either training a smaller model to mimic a larger one or by encoding embeddings more compactly.
    \item \textbf{Parameter-Efficient Learning:} This family focuses on designing KGE models that inherently require fewer parameters, especially for entities, thus mitigating the linear growth of parameters with graph size.
    \item \textbf{Algorithmic and System-Level Optimizations:} These methods tackle the computational cost of training, either through novel, inherently efficient algorithms or by optimizing the underlying system infrastructure for parallel and distributed processing.
\end{enumerate}

\subsubsection*{Knowledge Distillation and Embedding Compression}
One prominent strategy to achieve efficiency is \textit{knowledge distillation}, where a smaller, "student" KGE model is trained to replicate the behavior of a larger, more complex "teacher" model. \cite{zhu2020} introduced DualDE, a dually distilling method designed to create low-dimensional student KGEs from high-dimensional pre-trained teachers. The core innovation of DualDE lies in its dual-influence consideration between teacher and student, employing a soft label evaluation mechanism to adaptively weigh soft and hard labels for different triples, alongside a two-stage distillation process. This approach claims significant parameter reduction (7-15x) and inference speedup (2-6x) while retaining high performance, making it suitable for resource-limited applications. A theoretical limitation of distillation is that the student model's performance is inherently bounded by the teacher's capabilities, and the distillation process itself can be complex to tune. Practically, the "minor loss in performance" often associated with compression, as noted in other works, can still be a critical factor in highly sensitive downstream tasks.

Complementary to distillation are direct \textit{embedding compression} techniques. \cite{sachan2020} proposed an approach to compress the KGE layer by representing each entity as a vector of discrete codes, from which embeddings are composed. This method achieves massive compression (50-1000x) with only a minor loss in performance, demonstrating that the information content of embeddings can be significantly condensed. Building on this, \cite{wang2021} introduced LightKG, a lightweight framework for efficient inference and storage. LightKG's innovation lies in storing only a few codebooks and indices, drastically reducing storage requirements. It boosts inference efficiency through quick look-ups and incorporates a novel dynamic negative sampling method based on quantization. While \cite{sachan2020} focuses on the compression mechanism, LightKG provides a more comprehensive framework, integrating residual modules for codebook diversity and a continuous function to approximate non-differentiable codeword selection. Both methods address the problem of large memory footprints, but LightKG specifically optimizes for both storage and inference speed, making it highly practical for real-time applications. However, the reliance on discrete codes or codebooks introduces a quantization error, which is the theoretical limitation that leads to the "minor loss in performance" compared to full-precision embeddings.

\subsubsection*{Parameter-Efficient Learning}
Moving beyond post-hoc compression, \textit{parameter-efficient learning} aims to design KGE models that are inherently less parameter-intensive. \cite{chen2023} proposed Entity-Agnostic Representation Learning (EARL) to tackle the inefficient parameter storage costs of conventional KGE methods, where the number of embedding parameters grows linearly with the number of entities. EARL's core innovation is to learn embeddings only for a small set of "reserved entities" and derive the embeddings for all other entities from their context using universal, entity-agnostic encoders. This approach results in a static and lower parameter count, making it particularly beneficial for massive and continuously growing KGs. EARL addresses a fundamental problem in KGE scalability by decoupling entity representations from direct parameter storage. Its success hinges on the assumption that sufficient distinguishable information can be extracted from an entity's context to form a high-quality embedding, which might be a practical limitation for very sparse entities or those with limited contextual information. Compared to distillation or compression, EARL offers a more fundamental solution to parameter explosion by rethinking how embeddings are learned, rather than how they are stored or approximated.

\subsubsection*{Algorithmic and System-Level Optimizations}
Beyond reducing model size, significant efforts have been directed towards accelerating the training process itself. \cite{peng2021} introduced a "Highly Efficient Knowledge Graph Embedding Learning" framework that significantly reduces training time and carbon footprint by orders of magnitude. Their core innovation is a closed-form solution using Orthogonal Procrustes Analysis, enabling full-batch learning and non-negative sampling. This represents a paradigm shift from iterative optimization methods, offering inherent efficiency. A key theoretical advantage is that their entity embeddings also store full relation information, suggesting improved interpretability. However, the applicability of such closed-form solutions might be limited to specific KGE model architectures, potentially not generalizing to the full complexity of GNN-based or Transformer-based models discussed in Section 3.

For Graph Neural Network (GNN)-based KGEs, which are increasingly popular for their expressiveness, scalability remains a challenge. \cite{modak2024} proposed CPa-WAC (Constellation Partitioning-based Scalable Weighted Aggregation Composition) to address the training time and memory cost of GNNs on large KGs. The innovation lies in using modularity maximization-based constellation partitioning to break down KGs into subgraphs, which are then processed separately. This method aims to reduce memory and training time while crucially retaining prediction accuracy by preserving local graph topology. CPa-WAC demonstrates that efficient partitioning strategies can enable GNNs to scale to larger KGs, achieving up to five times faster training while maintaining performance comparable to training on the whole graph. A practical limitation is the complexity of effectively aggregating information from partitioned subgraphs without losing critical global context, a common challenge in graph partitioning.

Finally, \textit{system-level optimizations} provide a general infrastructure for efficient KGE training. \cite{zheng2024} introduced GE2, a "General and Efficient Knowledge Graph Embedding Learning System." GE2 tackles the long CPU times and high CPU-GPU communication overhead observed in existing graph embedding systems (e.g., PBG, DGL-KE, Marius). Its core innovations include a general execution model for various negative sampling algorithms, a user-friendly API, offloading CPU operations to GPU for higher parallelism, and the novel COVER algorithm for efficient data swap between CPU and multiple GPUs. GE2 is not a KGE model itself but a foundational system that can accelerate the training of various KGE models across different datasets, achieving speedups of over 2x and up to 7.5x. This work highlights that system-level engineering is as crucial as algorithmic innovation for practical KGE deployment. GE2's generality allows it to potentially accelerate the training of models like CPa-WAC \cite{modak2024} or even the efficient KGE learning from \cite{peng2021}, representing a convergent research direction where optimized algorithms benefit from optimized infrastructure.

\subsubsection*{Synthesis and Tensions}
The research in efficiency, compression, and scalability reveals a clear intellectual trajectory. Early KGE models often prioritized expressiveness, leading to large memory footprints and long training times. This necessitated the development of post-hoc solutions like knowledge distillation \cite{zhu2020} and embedding compression \cite{sachan2020, wang2021} to make models deployable. These methods inherently involve a trade-off: they achieve significant resource savings but often with a "minor loss in performance," as highlighted by \cite{sachan2020}.

More recent work has shifted towards designing KGE models that are *inherently* efficient, such as parameter-efficient learning with EARL \cite{chen2023}, which fundamentally rethinks entity representation to avoid linear parameter growth. Simultaneously, the field has seen a push for radically more efficient training algorithms, exemplified by the closed-form solution of \cite{peng2021}, which offers orders of magnitude speedup by leveraging mathematical properties. This contrasts with engineering solutions like graph partitioning \cite{modak2024} that optimize existing GNN architectures for scalability.

A critical tension across these approaches is the balance between efficiency and the ability to capture complex relational patterns. While compression and parameter efficiency reduce resource demands, they must ensure that the core expressiveness of the KGE is not unduly compromised. Furthermore, the generalizability of these efficiency techniques is an ongoing challenge. For example, while GE2 \cite{zheng2024} offers a general system for accelerating training, its impact is tied to the underlying KGE model and hardware. Similarly, CPa-WAC \cite{modak2024} is tailored for GNN-based KGEs, and its partitioning strategy might not be directly applicable to all KGE architectures. The unstated assumption in many of these works is that the "minor" performance degradation from compression or partitioning is acceptable for the gains in efficiency, an assumption that may not hold in all high-stakes applications. These innovations are crucial for overcoming the practical bottlenecks of KGE, making them deployable in resource-constrained environments and for handling ever-growing knowledge bases, thereby bridging the gap between theoretical advancements and real-world utility.