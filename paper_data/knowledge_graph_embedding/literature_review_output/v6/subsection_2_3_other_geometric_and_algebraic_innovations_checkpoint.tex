\subsection*{Other Geometric and Algebraic Innovations}

Building upon the foundational translational models and the specific rotational and complex space paradigms discussed in preceding subsections, the field of Knowledge Graph Embedding (KGE) has continuously explored a broader spectrum of geometric and algebraic innovations. This subsection delves into models that challenge conventional Euclidean embedding spaces, introduce novel metric choices, or employ advanced, often compound, transformations beyond simple vector addition or pure rotation. The overarching goal is to refine the mathematical foundations of KGE, enabling more nuanced representations of entities and relations, particularly for capturing complex logical patterns and addressing inherent limitations of simpler geometric models. As highlighted by \cite{cao2022}, the choice of representation space and its mathematical properties profoundly influences a model's capacity to capture diverse KG structures, often dictating its ability to model hierarchical, compositional, or context-dependent semantics.

A significant paradigm shift from standard Euclidean vector spaces is observed in models that embed entities on Lie groups or non-Euclidean manifolds, which offer inherent advantages for specific data structures. Unlike Euclidean spaces, which struggle with certain topological properties like hierarchies or bounded representations, these alternative geometries can naturally accommodate them.

**Non-Euclidean Geometries and Manifold Embeddings:**

One pioneering work in this direction is \textbf{TorusE} \cite{ebisu2017}, which embeds entities on a Lie group, specifically a torus (a compact manifold). TorusE claims to solve the regularization problem inherent in TransE-like models, where explicit regularization (e.g., constraining embeddings to a unit sphere) is often needed to prevent divergence during negative sampling. This regularization can inadvertently warp embeddings in Euclidean space, hindering their ability to accurately fulfill the translational principle ($h+r \approx t$). TorusE's core innovation is that the torus itself is a bounded space, naturally preventing embeddings from diverging while preserving the translational property. Empirically, TorusE demonstrated superior performance over TransE, DistMult, and ComplEx on link prediction tasks, and notably, was faster than the original TransE, showcasing that non-Euclidean spaces can offer both theoretical advantages and practical efficiency \cite{ebisu2017}. However, while it elegantly solves the regularization issue, TorusE, being translation-based, still inherits the expressiveness limitations of TransE for complex relation patterns like symmetry or composition, which rotational models (e.g., RotatE \cite{sun2018} from Section 2.2) are better equipped to handle. Its core assumption of relations as simple translations on a torus, while regularization-friendly, might be too restrictive for the rich semantics found in many KGs, failing to capture the diverse transformations relations can represent.

Further exploring non-Euclidean geometries, hyperbolic spaces have gained traction due to their natural ability to model hierarchical structures prevalent in KGs. Unlike Euclidean space, which expands uniformly, hyperbolic space expands exponentially, allowing for more efficient embedding of tree-like or hierarchical data with fewer dimensions. Early hyperbolic KGE models often adopted a "hybrid" approach, mapping data between hyperbolic and tangent Euclidean spaces for transformations, which introduces computational overhead and numerical instability. \textbf{Fully Hyperbolic Rotation (FHRE)} \cite{liang2024} addresses this by defining the KGE model entirely within hyperbolic space, specifically the Lorentz model. FHRE's core innovation is to treat each relation as a Lorentz rotation, transforming head entity embeddings to tail entity embeddings directly within hyperbolic space, eliminating iterative logarithmic and exponential mappings during training. This direct operation, leveraging the Lorentz rotation theorem, leads to a more stable and efficient model for hierarchical data. FHRE achieved state-of-the-art performance on challenging datasets like CoDEx-s and CoDEx-m, demonstrating its effectiveness and generalization ability, particularly for KGs with pronounced hierarchical patterns \cite{liang2024}. A key limitation of FHRE, despite its efficiency, is its reliance on the Lorentz model, which might not be universally optimal for all types of hierarchical structures or logical patterns, and its "pure rotation" assumption might be too restrictive for relations that involve more complex transformations beyond simple hierarchical shifts.

In contrast to FHRE's fully hyperbolic design, \textbf{Unified Geometry Knowledge Graph Embedding (UniGE)} \cite{UniGE_Anonymous} takes a different tack by seamlessly integrating KGE in *both* Euclidean and hyperbolic geometric spaces. UniGE proposes an embedding alignment method and fusion strategy using optimal transport techniques and the Wasserstein barycenter method. This hybrid approach aims to capture the diverse geometric data structures (chains and hierarchies) within KGs that might exceed the capacity of a single embedding space. UniGE's theoretical analysis suggests a more robust error bound, and it empirically outperforms state-of-the-art methods on benchmark datasets, indicating that a judicious combination of geometries can be more expressive than relying solely on one \cite{UniGE_Anonymous}. The trade-off for UniGE's enhanced expressiveness is the increased complexity of managing and aligning embeddings across different geometric spaces compared to FHRE's simpler, fully hyperbolic operations. While FHRE prioritizes computational stability and directness within a single hyperbolic geometry, UniGE sacrifices some of that simplicity for broader applicability across diverse KG structures.

Another hyperbolic approach, proposed by \cite{pan2021}, leverages an extended Poincaré Ball and a polar coordinate system within hyperbolic space to capture both hierarchical structures and logical patterns. This model addresses the boundary conditions of the Poincaré Ball by expanding the modulus length, demonstrating that even within hyperbolic geometry, there are diverse ways to construct embedding spaces, each with its own advantages for specific KG characteristics. Similarly, \textbf{Hierarchical Hyperbolic Neural Graph Embedding (H2E)} \cite{H2E_Anonymous} also employs a hyperbolic polar embedding space with a dual-embedding (modulus and phase parts) to explicitly model inter-level and intra-level hierarchies. H2E further integrates an attentional neural context aggregation to enhance its ability to preserve hierarchical relations. Both \cite{pan2021} and H2E highlight the potential of polar coordinates in hyperbolic space for hierarchical modeling, but their optimization complexity, especially in managing boundary conditions or dual embeddings, can be a practical challenge, potentially leading to slower convergence or sensitivity to hyperparameters.

Further advancing the use of non-Euclidean geometries, \textbf{HolmE} \cite{zheng2024} introduces a general form of Riemannian KGE, specifically leveraging hyperbolic space, with the novel property of being "closed under composition." This means that the composition of any two relation embeddings remains within the same embedding space, aligning with the theoretical nature of real-world relations and enabling robust modeling of composition patterns regardless of their representation in training data. Unlike prior KGEs that often implicitly assume relations are non-compositional if their patterns are not well-represented, HolmE explicitly designs its relation embedding space to ensure this closure. HolmE extends Möbius addition to product spaces for computational efficiency and theoretically proves that prominent KGE models like TransE \cite{bordes2013} and RotatE \cite{sun2018} (discussed in Section 2.2) are special cases of its framework, offering a unifying perspective. Empirically, HolmE demonstrates notable advantages in modeling composition patterns, especially for long-tail patterns, and effectively extrapolates to unseen relations \cite{zheng2024}. However, its focus on composition patterns, while crucial, might mean less emphasis on other complex patterns like symmetry or inversion, which RotatE, for instance, explicitly targets. Its theoretical elegance comes with the inherent complexities of Riemannian geometry, which can be challenging for practitioners.

Beyond the choice of embedding space, the underlying metric used to measure distances and define relationships also profoundly impacts model expressiveness. \textbf{CyclE} \cite{yang2021} addresses this by questioning the ubiquitous use of the Minkowski metric in KGE. The paper argues that existing methods often overlook the implications of the embedding space's metric, leading to suboptimal performance. CyclE's core innovation is the introduction of a "Cycle metric," which is based on the oscillation property of periodic functions. It posits that a smaller function period in this metric leads to better expressive ability. By combining the Cycle metric with popular KGE models, CyclE empirically demonstrated that its proposed metric is more appropriate than the Minkowski metric for KGE, leading to improved expressiveness \cite{yang2021}. This work highlights a fundamental, yet often unstated, assumption in KGE research: the choice of distance function. While CyclE offers a novel metric, its success is still contingent on the underlying KGE model's ability to leverage this metric effectively. Its practical limitation lies in the need for careful tuning of the function period, which can be a complex hyperparameter, and the theoretical justification for why a "cycle" metric is universally superior to Minkowski for all KG patterns is not fully explored, potentially limiting its generalizability.

Another innovative approach to geometric representation is exemplified by models that move beyond representing entities as single points. \textbf{ManifoldE} \cite{xiao2015} proposes a manifold-based embedding principle that replaces the over-strict point-wise translation principle ($h+r \approx t$) with a manifold-wise principle. This model claims to solve the problem of *precise link prediction* by addressing the ill-posed algebraic system and over-rigid geometric forms of traditional KGEs. Instead of mapping true triples to a single point, ManifoldE maps them to a *manifold* (e.g., a high-dimensional sphere or hyperplane) defined by the head entity and relation. This provides a more flexible geometric representation, especially for complex relations like one-to-many or many-to-many. ManifoldE significantly improved HITS@1 scores on FB15K for N-N relations (e.g., 53.0\% for head prediction vs. TransE's 18.1\%), demonstrating its effectiveness for precise link prediction while maintaining efficiency comparable to TransE \cite{xiao2015}. This contrasts sharply with TransE's assumption of a single target point, which breaks down for complex relations. However, the choice and parameterization of the manifold itself can be complex, and its expressiveness is tied to how well the chosen manifold (e.g., sphere, hyperplane) aligns with the intrinsic geometry of the relations.

Similarly, \textbf{SpherE} \cite{li2024} extends rotational embeddings by representing entities as spheres instead of vectors, specifically targeting many-to-many relations and enabling set retrieval. While ManifoldE uses manifolds to define the *target* of a relation, SpherE represents the *entities themselves* as spheres, aiming to capture semantic "spread" rather than a precise target. This novel entity representation, while inheriting the interpretability of rotational models, offers a unique way to handle set-based queries, a problem often overlooked by traditional KGEs. In a different vein, \textbf{TransC} \cite{lv2018} introduces a novel geometric representation for entities by differentiating between concepts and instances. It encodes concepts as spheres and instances as vectors in the same semantic space. This allows for a more intuitive and effective modeling of `instanceOf` and `subClassOf` relations, demonstrating how varying geometric primitives for different entity types can enhance semantic capture. This approach requires explicit type information, which might not always be available in all KGs, limiting its generalizability compared to models like SpherE that learn representations without such external metadata.

**Generalized and Compound Transformations:**

Beyond these fundamental changes to space, metric, and entity representation, other models introduce specialized algebraic or linear transformations to refine relational modeling, often within conventional Euclidean spaces. The intellectual trajectory in this domain also demonstrates a clear progression towards unifying and generalizing transformations. For instance, \textbf{HousE} (Knowledge Graph Embedding with Householder Parameterization) \cite{li2022} proposes a more powerful KGE framework based on Householder transformations. Householder transformations can represent both reflections (a type of rotation) and projections, allowing HousE to achieve superior capacity for modeling relation patterns and handling sophisticated relation mapping properties simultaneously. HousE generalizes existing rotation-based models by extending rotations to high-dimensional real spaces, achieving state-of-the-art performance on several benchmarks \cite{li2022}. Its core innovation is the use of Householder matrices, which are orthogonal and symmetric, providing a theoretically sound way to perform rotations and reflections. Building on this, \textbf{GoldE} (Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization) \cite{li2024} introduces a framework based on a generalized form of Householder reflection. This universal orthogonal parameterization allows for dimensional extension and geometric unification, enabling the framework to simultaneously capture crucial logical patterns and inherent topological heterogeneity. GoldE represents a culmination of efforts to create a unified and highly expressive framework for orthogonal transformations, demonstrating state-of-the-art performance across benchmarks \cite{li2024}. GoldE's strength lies in its ability to generalize HousE, offering a more comprehensive and flexible approach to orthogonal transformations, but at the cost of increased mathematical abstraction.

Another approach, \textbf{CompoundE} \cite{ge2022} and its 3D extension \textbf{CompoundE3D} \cite{ge2023}, takes a more generalized view by combining translation, rotation, and scaling operations. These models argue that a cascade of these fundamental geometric manipulations can capture a broader spectrum of relational semantics. CompoundE demonstrates that many existing scoring-function-based KGE models can be seen as special cases of its framework, highlighting a convergent research direction towards unifying different geometric operations. This approach offers high expressiveness by explicitly modeling relations as a sequence of multiple transformations, providing greater flexibility than models relying on a single type of transformation like RotatE (Section 2.2). However, this comes with a trade-off of increased model complexity and optimization challenges due to the compound nature of operations and the higher number of parameters to learn, potentially impacting scalability compared to more parameter-efficient models like TransD \cite{ji2015} (Section 2.1).

The concept of modeling transitivity, a specific type of compositional pattern, is addressed by \textbf{Rot-Pro} (Modeling Transitivity by Projection in Knowledge Graph Embedding) \cite{song2021}. Rot-Pro theoretically shows that transitive relations can be modeled with projections and combines this insight with relational rotation. It proves that this combination can infer all common relation patterns, including transitivity, and achieves state-of-the-art results on datasets with transitive relations \cite{song2021}. This work builds on the rotational paradigm by adding a projection component to specifically target transitivity, a pattern that even pure rotational models might not perfectly capture. This highlights a pattern of augmenting core geometric operations with additional transformations to target specific complex patterns, demonstrating that a combination of transformations can be more effective than a single one for certain logical properties.

**Novel Metrics and Refined Algebraic Models (within Euclidean/Real space):**

More recently, \textbf{MQuinE} \cite{liu2024} identifies and resolves a theoretical deficiency, termed "Z-paradox," in some popular KGE models, which can degrade performance, especially on challenging test samples. MQuinE proposes a new KGE model that avoids this paradox while preserving strong expressiveness for various relation patterns, including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations. This highlights a critical analysis of the theoretical underpinnings of existing models, pushing for more robust and theoretically sound designs. MQuinE's contribution is less about a novel geometric space or transformation, and more about refining the underlying mathematical formulation to avoid pitfalls, making it a general algebraic innovation that enhances reliability.

Finally, some models focus on refining transformations within the conventional Euclidean space, often prioritizing efficiency or specific relational nuances:
\begin{itemize}
    \item \textbf{LineaRE} \cite{peng2020}: This model simplifies KGE to a linear regression task, modeling relations as a linear function of two low-dimensional entity vectors with weight and bias vectors. Despite its simplicity, LineaRE claims to cover four connectivity patterns (symmetry, antisymmetry, inversion, composition) and four mapping properties (1-to-1, 1-to-N, N-to-1, N-to-N), demonstrating that powerful expressiveness can sometimes be achieved with straightforward linear algebra. Its practical advantage is scalability to large KGs due to its linear nature, making it a strong contender for efficiency-critical applications. However, its linearity might limit its ability to capture highly non-linear semantic relationships compared to models like CompoundE \cite{ge2022} or those leveraging deep learning (Section 3), which can model more intricate interactions.
    \item \textbf{TransMS} \cite{yang2019}: An extension of translation-based models, TransMS introduces "multidirectional semantics" by translating and transmitting semantics from head/tail entities and relations using non-linear functions and linear bias vectors. This moves beyond simple vector addition by incorporating richer, non-linear interactions, improving performance on N-1 and 1-N relations. Unlike the purely linear transformations in LineaRE \cite{peng2020}, TransMS leverages non-linearity to capture more complex relational dynamics while maintaining a relatively low computational footprint compared to matrix-based approaches. However, its non-linear functions are still relatively simple compared to the complex algebraic structures of rotational or hyperbolic models, potentially limiting its capacity for extremely nuanced semantics.
    \item \textbf{TranS} \cite{zhang2022}: A transition-based method that addresses complex scenarios where the same entity pair can have different relations. Its core innovation is replacing the single relation vector in traditional scoring patterns with a "synthetic relation representation," enabling it to handle diverse relations more effectively and efficiently. This contrasts with models that assign a single, fixed embedding to each relation, offering greater flexibility for polysemous relations. While it addresses polysemy, it does so by increasing the representational overhead for relations, which can impact scalability for KGs with many distinct relation instances, especially when compared to more parameter-efficient models like TransD \cite{ji2015} (Section 2.1).
    \item \textbf{lppTransE/R/D} \cite{yoon2016}: An early innovation that introduces "role-specific projections" to preserve the logical properties of relations (e.g., transitivity, symmetricity) within translation-based embeddings. By projecting head and tail entities using distinct operators, these models aim to overcome the limitations of undifferentiated entity roles in earlier TransE variants. This represents an early attempt to inject more semantic nuance into translational models, increasing parameter count compared to base TransE but offering more expressive power for specific logical patterns. This approach, while effective for its time, is less sophisticated than later generalized transformations like HousE \cite{li2022}, which offer a broader range of geometric operations through more advanced matrix parameterizations.
\end{itemize}

\begin{table}[htbp]
    \centering
    \caption{Comparative Overview of Diverse Geometric and Algebraic KGE Innovations}
    \label{tab:other_geometric_models}
    \begin{tabularx}{\textwidth}{|l|X|X|X|X|}
        \hline
        \textbf{Model} & \textbf{Embedding Space/Metric} & \textbf{Core Operation/Mechanism} & \textbf{Key Patterns/Problems Addressed} & \textbf{Theoretical/Practical Considerations} \\
        \hline
        \textbf{TorusE} \cite{ebisu2017} & Lie Group (Torus) & Translation on manifold & TransE regularization problem, embedding divergence & Solves regularization elegantly, maintains translation, but may lack expressiveness for complex patterns beyond translation. \\
        \hline
        \textbf{FHRE} \cite{liang2024} & Lorentz Hyperbolic Space & Fully Hyperbolic Rotation & Hierarchical structures, instability of hybrid hyperbolic models & Direct hyperbolic operation, stable, efficient for hierarchies, specific to Lorentz model, "pure rotation" assumption can be restrictive. \\
        \hline
        \textbf{Pan et al. 2021} \cite{pan2021} & Extended Poincaré Ball (Hyperbolic) & Polar coordinates, boundary handling & Hierarchical structures, logical patterns, Poincaré boundary issues & Novel hyperbolic space design, effective for hierarchies, optimization complexity due to boundary conditions. \\
        \hline
        \textbf{H2E} \cite{H2E_Anonymous} & Hyperbolic Polar Space & Dual-embedding (modulus/phase), attentional context & Inter/intra-level hierarchies, rich relational context & Enhanced hierarchical modeling, complex dual-embedding, attention adds computational cost. \\
        \hline
        \textbf{UniGE} \cite{UniGE_Anonymous} & Hybrid (Euclidean + Hyperbolic) & Embedding alignment (Optimal Transport) & Diverse geometric data (chains, hierarchies), single-space limitations & Unifies geometries, robust error bound, complex fusion strategy, higher computational cost due to alignment. \\
        \hline
        \textbf{HolmE} \cite{zheng2024} & Riemannian (Hyperbolic) & Closed under composition (Möbius addition) & Composition patterns (especially long-tail), unifying TransE/RotatE & Novel theoretical property, robust for composition, but Riemannian geometry adds complexity. \\
        \hline
        \textbf{ManifoldE} \cite{xiao2015} & Manifold (Sphere/Hyperplane) & Manifold-based principle & Precise link prediction, ill-posed algebraic systems, over-strict point-wise models & Flexible geometric representation, improves HITS@1, efficient, but manifold choice impacts performance and complexity. \\
        \hline
        \textbf{SpherE} \cite{li2024} & Spherical Space & Rotational (entity as sphere) & Many-to-many relations, Set Retrieval & Novel entity representation, good for set-based queries, interpretability. \\
        \hline
        \textbf{TransC} \cite{lv2018} & Real Vector Space (Spheres/Vectors) & Differentiating concepts (spheres) and instances (vectors) & `instanceOf`, `subClassOf` relations, ontological distinctions & Innovative entity representation, intuitive for hierarchies, requires explicit type information, limiting generalizability. \\
        \hline
        \textbf{HousE} \cite{li2022} & Real Vector Space & Householder Transformations (Rotations & Projections) & General relation patterns, mapping properties, higher-dimensional rotations & Generalizes rotations, strong modeling capacity, increased complexity. \\
        \hline
        \textbf{GoldE} \cite{li2024} & Real Vector Space & Universal Orthogonal Parameterization & Generalizes Householder, dimensional extension, geometric unification & Unifies orthogonal transformations, high expressiveness, abstract, potentially complex. \\
        \hline
        \textbf{CompoundE} \cite{ge2022} & Real Vector Space & Cascaded Translation, Rotation, Scaling & General relation patterns, diverse mapping properties & Unifies multiple operations, high expressiveness, potentially complex optimization and higher parameter count. \\
        \hline
        \textbf{Rot-Pro} \cite{song2021} & Real Vector Space & Projection + Rotation & Transitivity, common relation patterns & Combines transformations for specific patterns, effective for transitivity, but adds complexity. \\
        \hline
        \textbf{CyclE} \cite{yang2021} & Cycle Metric (vs. Minkowski) & Metric choice (oscillation property) & Expressiveness limitations of Minkowski metric & Fundamental metric innovation, depends on base KGE, period tuning can be complex, not universally superior. \\
        \hline
        \textbf{MQuinE} \cite{liu2024} & Real Vector Space & Refined Scoring Function & Z-paradox, diverse relation patterns & Addresses theoretical deficiency, robust, general expressiveness, improves reliability. \\
        \hline
        \textbf{LineaRE} \cite{peng2020} & Real Vector Space & Linear Regression & Symmetry, inversion, composition, all mapping properties & Simple, scalable, surprisingly effective for broad patterns, but linearity can be limiting for highly non-linear semantics. \\
        \hline
        \textbf{TransMS} \cite{yang2019} & Real Vector Space & Multidirectional semantics (non-linear functions, bias) & Complex relations (N-1, 1-N) beyond simple translation & Refines translation with non-linearity, better scalability than matrix-based, but less expressive than full rotations. \\
        \hline
        \textbf{TranS} \cite{zhang2022} & Real Vector Space & Synthetic Relation Representation & Complex scenarios with same entity pair having different relations & Efficient for diverse relations, addresses single relation vector limitation, but adds representational overhead, impacting scalability. \\
        \hline
        \textbf{lppTransE/R/D} \cite{yoon2016} & Real Vector Space & Role-specific Projections & Preserving logical properties (transitivity, symmetricity) & Early attempt to add semantic nuance to translation, increases parameters compared to base TransE, less sophisticated than later generalizations. \\
        \hline
    \end{tabularx}
\end{table}

This diverse set of models highlights several patterns and tensions in KGE research. There is a continuous effort to move beyond simple vector operations towards more mathematically sophisticated transformations and embedding spaces. For instance, TorusE \cite{ebisu2017}, FHRE \cite{liang2024}, UniGE \cite{UniGE_Anonymous}, \cite{pan2021}, H2E \cite{H2E_Anonymous}, and HolmE \cite{zheng2024} all challenge the fundamental assumptions about the embedding space, demonstrating that non-Euclidean geometries or hybrid spaces can offer significant advantages for specific data structures like hierarchies, unlike purely Euclidean models that struggle with such representations. This pattern suggests a growing recognition that the intrinsic geometry of KGs is often non-Euclidean, and a single Euclidean space may be an oversimplification. Similarly, CyclE \cite{yang2021} and ManifoldE \cite{xiao2015} re-evaluate the underlying metric and the geometric primitive for entities, respectively, showing that even foundational choices can be refined for improved performance or theoretical soundness. This contrasts with models like TransMS \cite{yang2019} and LineaRE \cite{peng2020}, which primarily refine the scoring function within a conventional Euclidean space, showing that significant gains can still be made through clever algebraic manipulations without resorting to exotic geometries.

A critical tension exists between model expressiveness and computational complexity. While models like UniGE, with its hybrid geometry and optimal transport, offer high expressiveness for diverse KG structures, they incur higher computational costs and parameter counts due to the complexity of managing and aligning embeddings across different spaces. This stands in stark contrast to the parameter efficiency of TransD \cite{ji2015} (discussed in Section 2.1) or the linear simplicity of LineaRE \cite{peng2020}. The field implicitly assumes that increased mathematical complexity will yield better results, but this is often at the expense of scalability to extremely large KGs or interpretability. For example, while FHRE offers stability and efficiency for hierarchical data by operating fully in hyperbolic space, its reliance on the Lorentz model might limit its applicability to KGs where other geometric properties are more dominant, or where the "pure rotation" assumption is too restrictive. The evaluation of these models primarily relies on link prediction metrics, which, while standard, may not fully capture the nuances of all the complex patterns they claim to model, especially for tasks like precise link prediction (ManifoldE) or concept-instance differentiation (TransC). The generalizability of these models to extremely large and sparse KGs, where the computational overhead of complex transformations can become prohibitive, remains an ongoing challenge. Furthermore, many papers, while demonstrating empirical improvements, often lack a thorough critique of their own assumptions, such as the universal applicability of a single geometric primitive (e.g., spheres in SpherE) or the optimality of a chosen metric across all relation types (CyclE). This methodological oversight can lead to an overestimation of a model's robustness across diverse KG characteristics.

In summary, this subsection highlights the continuous and multifaceted effort to refine the mathematical foundations of KGE. From exploring non-Euclidean spaces and hybrid geometries to introducing novel metrics and specialized linear/algebraic transformations, researchers are pushing the boundaries of how entities and relations can be represented and interacted with algebraically. This trajectory demonstrates a clear evolution from simpler, often restrictive, geometric assumptions towards more flexible, powerful, and theoretically sound mathematical frameworks, albeit with persistent trade-offs in complexity, scalability, and the need for specialized solutions for specific relational patterns. The progression from basic translational models to sophisticated generalized transformations like CompoundE underscores a drive towards unified frameworks that can encapsulate a wider array of relational semantics.