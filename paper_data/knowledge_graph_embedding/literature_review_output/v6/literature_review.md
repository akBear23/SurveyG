# A Comprehensive Literature Review with Self-Reflection

**Generated on:** 2025-10-05T23:24:58.652139
**Papers analyzed:** 377

## Papers Included:
1. d899e434a7f2eecf33a90053df84cf32842fbca9.pdf [sun2018]
2. 83d58bc46b7adb92d8750da52313f060b10f201d.pdf [dasgupta2018]
3. 10d949dee482aeea1cab8b42c326d0dbf0505de3.pdf [chen2023]
4. b1d807fc6b184d757ebdea67acd81132d8298ff6.pdf [yang2023]
5. abea782b5d0bdb4cd90ec42f672711613e71e43e.pdf [jia2015]
6. 658702b2fa647ae7eaf1255058105da9eefe6f52.pdf [lloyd2022]
7. 29eb99518d16ccf8ac306d92f4a6377ae109d9be.pdf [wu2021]
8. 58e1b93b18370433633152cb8825917edc2f16a6.pdf [xu2019]
9. d4220644ef94fa4c2e5138a619cfcd86508d2ea1.pdf [shan2018]
10. 15710515bae025372f298570267d234d4a3141cb.pdf [zheng2024]
11. 354fb91810c6d3756600c99ad84d2e6ef4136021.pdf [he2023]
12. 67cab3bafc8fa9e1ae3ff89791ad43c81441d271.pdf [xiao2015]
13. 405a7a7464cfe175333d6f04703ac272e00a85b4.pdf [guo2017]
14. 8b717c4dfb309638307fcc7d2c798b1c20927a3e.pdf [chen2021]
15. 29052ddd048acb1afa2c42613068b63bb7428a34.pdf [li2023]
16. 23efe9b99b5f0e79d7dbd4e3bfcf1c2d8b23c1ff.pdf [zhou2023]
17. af051c87cecca64c2de4ad9110608f7579766653.pdf [xiang2021]
18. 85064a4b1b96863af4fccff9ad34ce484945ad7b.pdf [cao2022]
19. 06315f8b2633a54b087c6094cdb281f01dd06482.pdf [wang2021]
20. a905a690ec350b1aeb5fcfd7f2ff0f5e1663b3a0.pdf [guo2020]
21. 3ac716ac5d47d4420010678fda766ebb5b882ba9.pdf [zhang2024]
22. 933cb8bf1cd50d6d5833a627683327b15db28836.pdf [shen2022]
23. bb3e135757bfb82c4de202c807c9e381caecb623.pdf [hu2024]
24. 398978c84ca8dab093d0b7fa73c6d380f5fa914c.pdf [liu2024]
25. b594b21557395c6a8fa8356249373f8e318c2df2.pdf [zhang2019]
26. 3e3a84bbceba79843ca1105939b2eb438c149e9e.pdf [yang2019]
27. b3f0cdc217a3d192d2671e44913542903c94105b.pdf [xie2023]
28. 52eb7f27cdfbf359096b8b5ef56b2c2826beb660.pdf [wang2024]
29. ecb80d1e5507e163be4a6757b00c8809a2de4863.pdf [xiao2019]
30. 33d469c6d9fc09b59522d91b7696b15dc60a9a93.pdf [sachan2020]
31. 4801db5c5cb24a9069f2d264252fa26986ceefa9.pdf [madushanka2024]
32. a166957ec488cd20e61360d630568b3b81af3397.pdf [zhu2022]
33. bcffbb40e7922d2a34e752f8faaa4fe99649e21a.pdf [liang2024]
34. 7029ecb5d5fc04f54e1e25e739db2e993fb147c8.pdf [li2024]
35. 990334cf76845e2da64d3baa10b0a671e433d4b6.pdf [ebisu2017]
36. 0367603c0197ab48eeba29aa6af391584a5077c0.pdf [zhang2021]
37. 7572aefcd241ec76341addcb2e2e417587cb2e4c.pdf [huang2019]
38. c2c6edc5750a438bddd1217481832d38df6336de.pdf [tang2019]
39. a6a735f8e218f772e5b9dac411fa4abea87fdb9c.pdf [sun2018]
40. f2b924e69735fb7fd6fd95c6a032954480862029.pdf [ge2023]
41. e39afdbd832bd8fd0fb4f4f7df3722dc5f5cab2a.pdf [wang2020]
42. 63836e669416668744c3676a831060e8de3f58a1.pdf [li2022]
43. 11e402c699bcb54d57da1a5fdbc57076d7255baf.pdf [zhang2019]
44. 191815e4109ee392b9120b61642c0e859fb662a1.pdf [tang2022]
45. d3c287ff061f295ddf8dc3cb02a6f39e301cae3b.pdf [lv2018]
46. c64433657869ecdaaa7988a029eabfe774d3ac47.pdf [chen2025]
47. 8fef3f8bb8bcd254898b5d24f3d78beab09e99d4.pdf [qian2021]
48. 68f34ed64fdf07bb1325097c93576658e061231e.pdf [dai2020]
49. efea0197c956e981e98c4d2532fa720c58954492.pdf [ji2024]
50. f470e11faa6200026cf39e248510070c078e509a.pdf [yan2022]
51. 5dc88d795cbcd01e6e99ba673e91e9024f0c3318.pdf [zhang2023]
52. 0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f.pdf [li2021]
53. 33f3f53c957c4a8832b1dcb095a4ac967bd89897.pdf [yang2025]
54. 2e925a02db26a60ee1cc022f3923e09f3fae7b39.pdf [wang2019]
55. 040fe47af8f4870bf681f34861c42b3ea46d76cf.pdf [di2023]
56. c762e198b0239313ee50476021b1939390c4ef9d.pdf [jia2017]
57. 1f20378d2820fdf1c1bb09ce22f739ab77b14e82.pdf [choudhary2021]
58. 991b64748dfeecf026a27030c16fe1743aa20167.pdf [xiao2015]
59. 6a2f26cece133b0aa52843be0f149a65e78374f7.pdf [hu2024]
60. 2a3f862199883ceff5e3c74126f0c80770653e05.pdf [wang2014]
61. 21f8ea62da6a4031d85a1ee701dbc3e6847fa6d3.pdf [zhu2020]
62. acc855d74431537b98de5185e065e4eacbab7b26.pdf [ali2020]
63. 2a25540e3ce0baba56ee71da7ca938f0264f790d.pdf [mohamed2020]
64. d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4.pdf [gao2020]
65. d7ef14459674b75807cd9be549f1e12d53849ead.pdf [peng2021]
66. 3f170af3566f055e758fa3bdf2bfd3a0e8787e58.pdf [shi2025]
67. 5b5b3face4be1cf131d0cb9c40ae5adcd0c16408.pdf [zhang2024]
68. f4e39a4f8fd8f8453372b74fda17047b9860d870.pdf [rosso2020]
69. 6a86594566fc9fa2e92afb6f0229d63a45fe25e6.pdf [zhou2024]
70. 1620a20881b572b5ffc6f9cb3cf39f6090cee19f.pdf [xie2020]
71. 83a46afaeb520abcd9b0138507a253f6d4d8bff7.pdf [song2021]
72. f44ee7932aacd054101b00f37d4c26c27630c557.pdf [zhang2020]
73. 44ce738296c3148c6593324773706cdc228614d4.pdf [ge2022]
74. bcdb8914550df02bfe1f69348c9830d775f6590a.pdf [ren2020]
75. 77dc07c92c37586f94a6f5ac3de103b218931578.pdf [yuan2019]
76. d1a525c16a53b94200029df1037f2c9c7c244d7b.pdf [xiao2015]
77. 8f096071a09701012c9c279aee2a88143a295935.pdf [sun2018]
78. 18bd7cd489874ed9976b4f87a6a558f9533316e0.pdf [ji2015]
79. 0364e17da01358e2705524cd781ef8cc928256f5.pdf [lin2020]
80. fda63b289d4c0c332f88975994114fb61b514ced.pdf [islam2023]
81. 3f0d5aa7a637d2c0bb3d768c99cc203430b4481e.pdf [wang2021]
82. 2bd20cfec4ad3df0fd9cd87cef3eefe6f3847b83.pdf [broscheit2020]
83. 84aa127dc5ca3080385439cb10edc50b5d2c04e4.pdf [fanourakis2022]
84. 727183c5cff89a6f2c3b71167ae50c02ca2cacc4.pdf [wang2018]
85. 19a672bdf29367b7509586a4be27c6843af903b1.pdf [tabacof2019]
86. ecc04e9285f016090697a1a8f9e96ce01e94e742.pdf [pei2019]
87. beade097ff41c62a8d8d29065be0e1339be39f30.pdf [zhang2018]
88. bbb89d88ad5b8279709ff089d3c00cd2750cd26b.pdf [li2021]
89. d605a7628b2a7ff8ce04fc27111626e2d734cab4.pdf [li2022]
90. 322aa32b2a409d2e135dbb14736d9aeb497f1c52.pdf [ding2018]
91. b2d2ad9a458bdcb0523d22be659eb013ca2d3c67.pdf [zhang2022]
92. ce7291c5cd919a97ced6369ca697db9849848688.pdf [sun2024]
93. 780bc77fac1aaf460ba191daa218f3c111119092.pdf [wang2024]
94. 6205f75cb6db1503c94386441ca68c63c9cbd456.pdf [modak2024]
95. e379f7c85441df5d8ddc1565cabf4b4290c22f1f.pdf [xiao2016]
96. c180564160d0788a82df203f9e5f61380d9846aa.pdf [zhang2023]
97. 69418ff5d4eac106c72130e152b807004e2b979c.pdf [guo2015]
98. 552bfaca30af29647c083993fbe406867fc70d4c.pdf [xu2020]
99. 33a7b7abf006d22de24c1471e6f6c93842a497b6.pdf [zheng2024]
100. 86ac98157da100a529ca65fe6e1da064b0a651e8.pdf [zhang2018]
101. 52b167a90a10cde25309e40d7f6e6b5e14ec3261.pdf [zhu2024]
102. 145fa4ea1567a6b9d981fdea0e183140d99aeb97.pdf [liu2023]
103. e9a13a97b7266ac27dcd7117a99a4fcbadc5fd9c.pdf [choi2020]
104. 4085a5cf49c193fe3d3ff19ff2d696fe20a5a596.pdf [ge2023]
105. 4e52607397a96fb2104a99c570c9cec29c9ca519.pdf [sadeghian2021]
106. eae107f7eeed756dfc996c47bc3faf381d36fd94.pdf [liu2024]
107. 7e5f318bf5b9c986ca82d2d97e11f50d58ee6680.pdf [li2022]
108. 8c93f3cecf79bd9f8d021f589d095305e281dd2f.pdf [rossi2020]
109. cab5194d13c1ce89a96322adaac754b2cb630d87.pdf [li2023]
110. 95c3d25b40f963eb248136555bd9b9e35817cc09.pdf [peng2020]
111. 12cc4b65644a84a16ef7dfe7bdd70172cd38cffd.pdf [ji2024]
112. 40479fd70115e545d21c01853aad56e6922280ac.pdf [zhang2024]
113. 5515fd5d14ac7b19806294119560a8c74f7fa4b2.pdf [kochsiek2021]
114. e5c851867af5587466f7cd9c22f8b2c84f8c6b63.pdf [yang2021]
115. eb14b24b329a6cc80747644616e15491ef49596f.pdf [shang2024]
116. 9c510e24b5edc5720440b695d7bd0636b52f4f66.pdf [asmara2023]
117. d9802a67b326fe89bbd761c261937ee1e4d4d674.pdf [gregucci2023]
118. b307e96f59fde63567cd0beb30c9e36d968fad8e.pdf [pan2021]
119. e4e7bc893b6fb4ff8ebbff899be65d96d50ccd1d.pdf [yoon2016]
120. c075a84356b529464df2e06a02bf9b524a815152.pdf [li2024]
121. b30481dd5467a187b7e1a5a2dd326d97cafd95ac.pdf [xiong2017zqu]
122. 2930168f3be575781939a57f4bb92e6b29c33b08.pdf [gong2020b2k]
123. da60d33d007681743d939861ae24f4cdac15667e.pdf [zhou2022ehi]
124. bb65c0898647c57c87a72e80d97a53576e3034ca.pdf [le2022ji8]
125. c03965d00865074ae66d0324c7145bf59aec73e6.pdf [zhou2022vgb]
126. 4b0e3d0721ea9324e9950b3bb98d917da8acb222.pdf [xu2019t6b]
127. 8df10fa4eca07dbb5fe2fe2ecc1e546cb8a8c947.pdf [mezni20218ml]
128. d6cc2a58df29d3e3fe4c55902880908dde32ee60.pdf [do2021mw0]
129. a57af41c3845a6d15ffbe5bd278e971ca9b8124a.pdf [mai2020ei3]
130. 8f255a7df12c8ec1b2d7c73c473882eacd8059d2.pdf [zhang2022eab]
131. 23ae48cdb8b7985e5a32fc79b6aae0de3230fe4f.pdf [sosa2019ih0]
132. 87ccb0d6c3e9f6367cd753538f4e906838cea8c2.pdf [guan2019pr4]
133. 0dddf37145689e5f2899f8081d9971882e6ff1e9.pdf [fan2014g7s]
134. 4be29e1cd866ab31f83f03723e2f307cdc1faab0.pdf [zhang20190zu]
135. 2a81032e5bb4b29f6e1423b6083b9a04bb54b605.pdf [chen2022mxn]
136. c88055688c4cd1e4a97da8601e90adbc0acdbd1e.pdf [wang2022hwx]
137. d97ec8a07cea1a18edf0a20981aad7e3dfe351e6.pdf [chen20226e4]
138. 389935511c395526817cf4ae62dae8913845ebdf.pdf [abusalih2020gdu]
139. ba524aa0ae24971b56eef6e92491de07d097a233.pdf [fang2022wp6]
140. a264af122f9f2ea5df46c030beb8ec0c25d6e907.pdf [elebi2019bzc]
141. 90450fe686c0fa645a1954950adffc5b2401e4b7.pdf [sha2019i3a]
142. 2257eb642e9ecae24f455a58dc807ee2a843081f.pdf [li2021ro5]
143. d77de3a4ddfa62f8105c0591fd41e549edcfd95f.pdf [xiao20151fj]
144. 52457f574780c53c68ad645fcdc86e2492b5074a.pdf [zhang2021wg7]
145. ac79b551ca16f98c1c3a5592c22d8093a492c4f3.pdf [wang20186zs]
146. 0abee37fe165b86753b306ffcc59a77e89de0599.pdf [li2021x10]
147. 512177d6b1e643b49b1d5ab1ad389666750144a9.pdf [wang202110w]
148. 60347869db7d1940958ee465b3010b3a612bf791.pdf [gutirrezbasulto2018oi0]
149. 9f7731d72e2aa251d2994eb1729c22aa78d0f718.pdf [portisch20221rd]
150. c7d3a1e82d4d7f6f1b6cffae049e930d0d3f487a.pdf [zhang2022muu]
151. 4ac5f7ad786fbee89b04023383a4fbe095ccc779.pdf [feng2016dp7]
152. 9fc2fd3d53a04d082edc80bafa470a66acdebb14.pdf [liu2021wqa]
153. 747dff7b9cd0d6feb16c340b684b1923034e8777.pdf [sang2019gjl]
154. 3e76e90180fc8300ecdeb5b543015cc68e0fd249.pdf [wang2017yjq]
155. 547dfe2a9d6a1bb1023f2208fb31f3a0671bf9ca.pdf [jiang20219xl]
156. 39eb51ae87c168ad4339214de6b91e2e2fdcfaa1.pdf [liu2022fu5]
157. fee5ac3604ccdefee2b65275fed47503234099e2.pdf [khan202236g]
158. 154fac5040865b4d74cf5a2cad39381c134a8b7d.pdf [mezni2021ezn]
159. 543497b1e551ad6473ddb9aa46697db28bccd3f5.pdf [zhang2021wix]
160. 6cc55dec26f5c078c6872d612c1561b1646d459a.pdf [huang2021u42]
161. ee5ceab9fa5f3bad231469923a03ad16184b51b9.pdf [pavlovic2022qte]
162. 3705cfe0d7dab8881518cb932f2465ca432d3f24.pdf [wang20213kg]
163. 882d6fe22a093ff95a8106a215bca37603ada710.pdf [zhang2019rlm]
164. 92ef8ff6715733697ca915c65cb18b160a764da6.pdf [mai20195rp]
165. a0ca7d39296d8d31dbbf300f58e7e375fb879492.pdf [han2018tzc]
166. 9155e1340e9263cf042d144681acccfc0c9d194b.pdf [wang2022fvx]
167. b5167990eda7d48f1a70a1fcb900ed5d46c40985.pdf [ferrari2022r82]
168. 0a8faa6c0e6dc9f743e96f276239d02d8839aca2.pdf [fu2022df2]
169. 71245f9d9ba0317f78151698dc1ddba7583a3afd.pdf [wu2018c4b]
170. f0499c2123e17106039e8e772878aad073ccf916.pdf [zhang202121t]
171. 2bdb9985208a7c7805676029300e3ba648125bd1.pdf [mohamed2019meq]
172. 7ccb05062f9ea7179532fd3355cf984b0102cfc5.pdf [xin2022dam]
173. c8214cac9c841f7b295a78c5bf71b6ed37c40eec.pdf [nie20195gc]
174. dab87bce4ac8c6033f5836f575b57c4a665b4f49.pdf [liu2018kvd]
175. 7ae22798887ff4e19033a8028007e1780b53ba8c.pdf [ni2020ruj]
176. 01c1e7830031b25410ed70965d239ac439a6fb68.pdf [li20215pu]
177. 021cbcd59c0438ac8a50c511be7634b0c00a1b89.pdf [yu2019qgs]
178. f211a2123e28d60cd8cdc05449c3cb7da2610b0a.pdf [fatemi2018e6v]
179. 3646e2947827c0a9314443e5cbb15575fafaf4ba.pdf [chen2021i5t]
180. 67c03d7a477059dc20faa02e3b45ca7055433615.pdf [dong2022c6z]
181. 91d8e1339eddee3217a6897cebdeb526b4bb1f72.pdf [lu20206x1]
182. b1464e3f0c82e21e23dfd9bc28e423856754b3d6.pdf [li2022nr8]
183. 57a7804d4e4e57de9a5c096ce7ea3e50d2c86f0f.pdf [luo2015df2]
184. 678dacdf029becac1116f345520f8e4afff5a873.pdf [zhou20216m0]
185. 1a25c8afacb6d36d4d8635eb9e3f8b8cf2e2122c.pdf [zhao202095o]
186. 60ad3ce0492a004020ff55653a51d6bfc457f12d.pdf [jia201870f]
187. 434b32d34b5d21071fc78a081741757f263c14ae.pdf [mai2018u0h]
188. 4a96636d1fc92221f2232d2d74be6e303cd0642a.pdf [li201949n]
189. 9c17d3f1837ae9f10f57c0b07c8288137d84026b.pdf [tang2020ufr]
190. e740a9aa753fcc926857ef4b90c1f91dd086e08d.pdf [guo2022qtv]
191. 315b239040f73063076014fdfabcc621b2719d83.pdf [jiang202235y]
192. 96b1f6fb6e904a674aef5cd32efee3edfa1c8ee2.pdf [liu201918i]
193. 5d6b4c5e48ec0585facea96a746bcbf7225d424c.pdf [zhang2020s4x]
194. 441f124d48662d6bd4f8e3190633371aa1b034eb.pdf [chang20179yf]
195. 5f9ea28be0d3bb9a73d62512190a772b10e92db0.pdf [lee2022hr9]
196. 836d1d1c94f0fd0713c77b86ce136fffd059dbc0.pdf [zhang2022fpm]
197. 0639efde0d9351bf5466235a492dbe9175f9cd5f.pdf [liu2019e1u]
198. 00529345e4a604674477f8a1dc1333114883b8d9.pdf [song2021fnl]
199. f0d5351c76448e28626177ece5ce97715087a0f9.pdf [gradgyenge2017xdy]
200. 9866a21c0ada20b62b28b3722c975595be819e24.pdf [zhou20218bt]
201. 50e7017c7768b7b2f5215a35539db1490ddc37ab.pdf [chen20210ah]
202. 95a501bfe4b09323e6e178edd64dc24a6935c23f.pdf [zhang2020i7j]
203. 46b5198a535dfcaf1cc7d57d471ad9ec050e46cf.pdf [boschin2020ki4]
204. cda7a1bdce2bfa77c2d463b631ff84b69ce3c9ed.pdf [wang20199fe]
205. f76a6e8f059820667af53edbd42d33fc4bca85fd.pdf [myklebust201941l]
206. 40667a731593a44d4e2f9391f1d14f368321b751.pdf [kartheek2021aj7]
207. 6bf53a97f5a3f5b0375f4702cbec28d8e9ab61c0.pdf [sha2019plw]
208. 4ae2631fb5e99cb64ff7d6e7ed3a1e6b0bedd269.pdf [lu2020x6y]
209. d76b3bf29366b4f0902ea145a3f7c020a35f084f.pdf [zhang2020c15]
210. 151c9bb547306d66ba252be7c20e35f711e9f330.pdf [li2020ek4]
211. c0827be29366be4b8cfa0dfbef4ead3f7b08f562.pdf [li2020he5]
212. 2d38cdaf2e232b5d1cb1dce388aa0fe75babcf29.pdf [kim2020zu3]
213. d6508e8825a6a1281ad415de47a2f108d98df87d.pdf [zhu2018l0u]
214. 18101998fb57704b79eb4c4c37891144ede8f8b9.pdf [do20184o2]
215. 23830bb104b25103162ec9f9f463624d9a434194.pdf [ma20194ua]
216. 77e23cd2437c6afb16082793badbb02842442e13.pdf [zhang2020wou]
217. 92351a799555df8d49465c2d4959118030339cc0.pdf [zhang2019hs5]
218. 6de535eb1b0024887227f7987e6eb22478af2a95.pdf [wang20198d2]
219. be7b102315ce70a7e01eb87c1140dd6850148e8b.pdf [tran20195x3]
220. 5b6a24ea3ffdccb14ce0267a815845c62ef026c9.pdf [xiong2018fof]
221. 75f7e3459e53fa0775c941cb703f049797851ef0.pdf [radstok2021yup]
222. 3ea066e35fdd45162a7fa94e995abe0beeceb532.pdf [zhao2020o6z]
223. c7a630751e45e3a74691bd0fc0880b4bf87be101.pdf [zhang20182ey]
224. a2a7f85d2ba28750725c4956eb14d53f6a90f003.pdf [jia20207dd]
225. bb0613ea0d39e35901aa0018de40deaf35cbbd5d.pdf [zhu2019ir6]
226. 509fa029989e89a4b82dd01ab75734aed937d684.pdf [wang2021dgy]
227. 4f2cc26b689cdac36ceb2037338eac65e7e5a193.pdf [ning20219et]
228. 7bb4cd36de648ca44cc390fe886ee70a4b2ad1ac.pdf [sheikh20213qq]
229. 93db6077c12cc83ea165a0d8851efb69b0055f3a.pdf [rim2021s9a]
230. 2f700be8a387101411a84199adfe30636e331752.pdf [zhang20179i2]
231. 2dba03d338170bde4e965909230256086bafa9f8.pdf [elebi20182bd]
232. c2648a294ef2fc299e1dd959bc1f92973f9c9ebc.pdf [garofalo20185g9]
233. 62c50e300ee87b185401ce27323bbb3f5262fdff.pdf [wang201825m]
234. 66f19b09f644578f808e69f38d3e76f8b972f813.pdf [chung2021u2l]
235. 9b68475f787be0999e7d09456003e664d37c2155.pdf [tran2019j42]
236. f0ac0c2f82886700dc7e7a178d597d33deebfc88.pdf [shi2017m2h]
237. a5aeca7ef265b27ff6d9ea08873c9499632b6439.pdf [zhang2017ixt]
238. 8412cc4dd7c8d309d7a84573637d4daaad8d33b5.pdf [zhu20196p1]
239. 8be21591c29d68d99e89a71fc7755f09f5eed3a1.pdf [kertkeidkachorn2019dkn]
240. 6493e6d563282fcb65029162a71cd2cb8168765b.pdf [zhu2019zqy]
241. d5eabc89e2346411134569a603e63a143d1d6552.pdf [zhang20193g2]
242. 89cf9719b97e69f5bb7d715d5a16609676c14e86.pdf [liu2019fcs]
243. 1c1b5fd282d3a1fe03a671f7d13092d49cb31139.pdf [kanojia20171in]
244. 7f7137d3e1de7e0e801c27d5e8b963dfd6d94eb4.pdf [gao2018di0]
245. 49899fd94cd272914f7d1e81b0915058c25bb665.pdf [mai2018egi]
246. e64557514ab856d22ddbb34bc23ffb7085d5d6b0.pdf [xiao2016bb9]
247. 7eece37709dceba5086f48dc43ac1a69d0427486.pdf [liu2024q3q]
248. 83424a4fea2e75311632059914bf358bc045435f.pdf [zhang2024cjl]
249. 3f8b13ede9f4d3a770ec8b4771b6036b9f603bfa.pdf [su2023v6e]
250. ac0c9afa9c19f0700d903e00a92e83e41587add3.pdf [zhu2023bfj]
251. f42d060fb530a11daecd90695211c01a5c264f8d.pdf [liu2024to0]
252. 7aca91d068d20d3389b28b8277ebc3d488be459f.pdf [wang2024vgj]
253. fa07384402f5c9d5b789edf7667bbcc555f381e3.pdf [li2024920]
254. 48c2e0d87b84efca7f11462bbdac1be1177e2433.pdf [lee202380l]
255. 51c18009b2c566d7cddc934b2cf9a1bca813f58f.pdf [shokrzadeh2023twj]
256. 5cbf9bc26b3d0471cb37c3f4a931990b1260d82d.pdf [gao2023086]
257. 4383242be5bdfb30ffa84e58cc252acfb58d4878.pdf [li2024sgp]
258. f26d45a806d1f1319f37eb41b8aa87d768a1d656.pdf [xue2023qi7]
259. 7b569aecc97f5fe57ce19ca0670a6b1bc62c7f7c.pdf [duan2024d3f]
260. 8bd3e0c1b6a68a1068da83003335ac01f1af8dcf.pdf [chen20246rm]
261. e83b693a44ec32ddfb084d13138e8d7ebc85a7c3.pdf [zhu2022o32]
262. f284977aa917be0ff15b835b538294b827135d19.pdf [mitropoulou20235t0]
263. f3fa1ef467c996b30242124a298b5b9d031e9ed5.pdf [shomer2023imo]
264. 61ef322fba87ccfd36c004afc875542a290fe879.pdf [wang202490m]
265. 5bef4d28d12dd578ce8a971d88d2779ec01c7ec5.pdf [li2024bl5]
266. c441b2833db8bd96b4ad133679a68f79d464ef59.pdf [li2024y2a]
267. edfbe0b62b9f628858d05b64bd830cf9b0a1ab74.pdf [jia2023krv]
268. 88e700e9fd6c14f3aa4502176a60512ca4020e35.pdf [huang2023grx]
269. 942541df1b97a9d1e46059c7c2d11503adc51c4c.pdf [wang2023s70]
270. abc424e17642df01e0e056427250526bc624f762.pdf [hou20237gt]
271. 825d7339eadadd2baf962f7d3c8fe7dc0cdc9819.pdf [jiang2023opm]
272. b6839f89a59132f0e62011a218ec229a27ffff6b.pdf [lu2022bwo]
273. 59116a07dbdb3cdeebb20085fdfde8b899de8f6a.pdf [djeddi2023g71]
274. 3cab78074e79122fd28cd76f37fd8805e8e4fc31.pdf [zhang20243iw]
275. ed21098804490b98899bcb7195084983ce69ed6c.pdf [le2023hjy]
276. 354b651dbc3ba2af4c3785ccbecd3df0585d30b2.pdf [yao2023y12]
277. c620d157f5f999d698f0da86fb91d267ad8ded5c.pdf [li2023y5q]
278. dc949e502e35307753a1acbcdf937f0cd866e63b.pdf [yang2022j7z]
279. a64167fcaa7a487575c6479510e57795afc9974e.pdf [banerjee2023fdi]
280. f9a575349133b2d4bf512cfb7754fca6d13b0a81.pdf [hu20230kr]
281. 5f850f1f522f959e2d3dcad263d05b0fdbb187c3.pdf [li2023wgg]
282. 4c68ee32d3db73d4d05803c1b3f2f4b929a88b78.pdf [hao2022cl4]
283. 2ac47be80b02a3ff1b87c46cf2b8c27e739c2873.pdf [khan20222j1]
284. b5aedc0464d04aa3fed7e59a141d9be7ee18c217.pdf [le2022ybl]
285. 463c7e28be13eb02620ad7e29b562bf6e5014ba2.pdf [liang202338l]
286. 7009fd9eb533df6882644a1c8e1019dc034b9cc5.pdf [khan2022ipv]
287. e186e5000174ea70729c90d465e60279c5f88646.pdf [he2022e37]
288. 70dc4c1ec4cda0a7c88751fb9a6b0c648e48e11f.pdf [shen2022d5j]
289. 5a8c6890e524b708dc262d3f456c985e8a46d7d1.pdf [di20210ib]
290. 86631a005e1a88a66926ac0c364ed0101a02b7e7.pdf [niu2020uyy]
291. 92b9aeabaaac0f20f66c5a68fbb4fc268c5eaae5.pdf [nie2023ejz]
292. ce494973ceefe5ac011f7e9879843530395fa9db.pdf [li2022du0]
293. 25edfb99d3b8377a11433cf7be2bcd9f8bfbdb87.pdf [daruna2022dmk]
294. 709a128e752414c973613814ddc2509f2abe092f.pdf [zhou20210ma]
295. 18fd8982051fc1de652a9882c2c52db11bca646b.pdf [kun202384f]
296. a7f0b4776d3df11cf0d0e72785c3035cc744726c.pdf [dong2022taz]
297. e2783f8aa4c61443760a8754cd6d88165d50b213.pdf [kamigaito20218jz]
298. 77fedfa533871c6c4218285493f725d5df4e74e5.pdf [krause2022th0]
299. 695ef4cf57b4fd0c7ec17a6e10dffade51f38179.pdf [zhang20213h6]
300. 90d5e74b18d03f733c6086418bfe9b20bb6a0a69.pdf [li2021tm6]
301. c495b2780accfbb53a932181e3c9fd957d16895d.pdf [wang2020au0]
302. 85bfec413860c072529ab8399676ab4b072f2e34.pdf [wei20215a7]
303. a89f61021e5382912aaeb3f69a6d8a6265787af4.pdf [zhang2021rjh]
304. b3cbbc1f34a20c22853f3dd347fd635b2e414fd5.pdf [sheikh202245c]
305. df7265b4652b21bc690497b3967a708d811ddd23.pdf [ren2021muc]
306. f6182d5c14c6047d197f1af842862653a13238f2.pdf [eyharabide2021wx4]
307. 082856e9b36fac60b9b9400abffaff0e74552fe1.pdf [hong2020hyg]
308. b25744d3c5d93e49b1906991dc8b5426ea2cf51d.pdf [huang2020sqc]
309. 18bcad2521cbe8df9d84b1adff1dd57c72c68a9d.pdf [kurokawa2021f4f]
310. bdd6c1a6695e3d201b70f4a913ffc758b74216e7.pdf [mohamed2021dwg]
311. e93565f447a42b158df27ba75385f5e2fc30dde7.pdf [gebhart2021gtp]
312. cf436f34ca6aabe1971c3531d465ecaa3d480d68.pdf [deng2024643]
313. 76016197d7d4f2213a4ace29988c93285793e154.pdf [liu2024zr9]
314. 9730f484b84074c1d61c154211ea06cc6ff20940.pdf [zhang2024zmq]
315. 10c388fa25dd6f07707a414946e5b7a674e7155b.pdf [he2024vks]
316. 7e6a50b70223dc00c712a17537fb7e23f8fd5ad4.pdf [zhang2024fy0]
317. ae58ebc99f67eed0de7f4ba2ca6f7ceb9ab056fb.pdf [zhang2024ivc]
318. 6ad02ad36e7a2c7d72d1a0b15ffc61dae2be1d7a.pdf [jing2024nxw]
319. 75ba0b92bcf095e7cd1544425f1818fed195f83f.pdf [jiang2024zlc]
320. 905d27e361c50da406439bdac25807dd38258fd8.pdf [han2024u0t]
321. b2646d9ee88c3dd6822b039a38c9604932aaaf47.pdf [quan2024o2a]
322. c7666fbaa49da21c465dbfabcf5fdd768b8c7b9e.pdf [liu2024tc2]
323. f1b7682df472a88fbaac3e6049f638ecec6937e7.pdf [hello2024hgf]
324. d66622beef468f7b934a5bf601cb8a3fcefe78f3.pdf [li2024z0e]
325. 20486c2fb358730ee99ae39b5e0a88d7b39ca720.pdf [yan2024joa]
326. b49f6029d681ac286ab929238f5aef5f352767c8.pdf [liu2024tn0]
327. c5a19440511a741edd1581d41d37d3e9b7088186.pdf [wang20245dw]
328. 822ad7c33316202a2511d300c6b8a263b758ad1a.pdf [long2024soi]
329. ba61c59abb560ff47a8dd780c8ccffb0af5e14c2.pdf [zhou2024ayq]
330. b3c340aa22bcd183c41836ef7265d656f741911f.pdf [huang2024t19]
331. 7c82aa0ae4b4e027a2df8afe9bbeccf88368c62f.pdf [lu2024fsd]
332. 0d9a788260e3abff4794d79f72b2b5ab2fb5abe5.pdf [liu2024yar]
333. 6cba788eea4fdb3bd0d1db4ecdd8a70040b81e62.pdf [khan20242y2]
334. 6c195ec2d5a491ffca9ab893968c4d44a6d0ce7d.pdf [xue2025ee8]
335. 37b274eb6fa68dede9f4aaad6dec1e2ea56095ce.pdf [long20248vt]
336. 9be88067bd7351b36bb0c698f5559ced3918a1d5.pdf [huang20240su]
337. e0d17f8b2fffff6c5eaf3f13bc45126196ddd128.pdf [wang2024nej]
338. a4b6e13efa80bedf8e588ac69f91fdaecc8e5077.pdf [wang2024c8z]
339. ccb6674576de48f8cfd99374c3b737a94dc3cb98.pdf [liu2024x0k]
340. 75b5c716e2b20b92a2a0f49674b7411a469a5575.pdf [li2024uio]
341. 8ff387296878f23632a588076823b160673866ab.pdf [zhang2024z78]
342. 6a66b459955959c4b8a67bd298ed291506923b7a.pdf [wang2024534]
343. 6b69c8848a1cc50ed8775beb483c71cfc314c66b.pdf [ni202438q]
344. d57e01d80c7f0f86b5e3f096b193ab9210e9095f.pdf [nie202499i]
345. a9bfb9ab236553768782f2b90a69c5625f033186.pdf [wang2024d52]
346. 6903aea3553a449257388580028e0bddf119d021.pdf [mao2024v2s]
347. 767d56fe80f7681b97943a8bff39f0b580e4acd8.pdf [jafarzadeh202468v]
348. 9e7799ef313143aa9c0669a7d1918fcfd5d21359.pdf [wang2024dea]
349. 563b3d57927b688e59322dbbfc973e5f1b269584.pdf [lu202436n]
350. 984c18fa61b10b6d1c34affc98f27ca8344d4224.pdf [han2024gaq]
351. 4a0048f1942a68e7c39adac43588d1604af26fc7.pdf [liu2024jz8]
352. 49dfd47177fa3aeab8a6bea82a77ec8bdb93bf1e.pdf [he2024y6o]
353. 2a5c888b2df4fd8c49aef46ee065422b00b178c0.pdf [fang20243a4]
354. 48c07506022634f332b410fb59dca9f61f89b032.pdf [zhang2024h9k]
355. 575af1587dea578d48eb27f45f008203565d9170.pdf [li2024wyh]
356. 7bd50842503e23e6479447b98912ac482ef43adc.pdf [dong2024ijo]
357. 4f0e1d5c77d463b136b594c891c4686fde7a1b12.pdf [wang20246c7]
358. c3861a930a65e8d9ee7ab9f0a6ee71e0e59df7ed.pdf [zhang2024yjo]
359. 217a4712feae7d7590d813d23e88f5fbb4f2c37f.pdf [liang20247wv]
360. cf696a919b8476a4d74b8b726e919812a2f05779.pdf [liu2024t05]
361. 91d5aa3d43237ec60266563ec6e8079f86532cfa.pdf [pham20243mh]
362. 58480444670ff933fe644563f7e2948a79503442.pdf [li2024gar]
363. 9b836b4764d4f6947ac684fd4ba3e8c3597d95bd.pdf [li2024nje]
364. bd0e8d6db97111686d02b51134f87439f8f1acfa.pdf [bao20249xp]
365. bea79d59ab3d203d06c88ebf67ac47cb34adeaa7.pdf [xu2024fto]
366. 241904795d94dcb1946ad46c9184c59899783af1.pdf [liang2024z0q]
367. 55dab161c25d1dd04fbeecdeca085274bfe8463f.pdf [liu2024ixy]
368. 3ff6b617cd839c9d85cb7b58aa6ad56e95b6cf69.pdf [dong2025l9k]
369. 9560ca767022020ccf414a2a8514f25b89f78cb3.pdf [zhang2025ebv]
370. d5c8dcc8f5c87c269780c7011a355b9202858847.pdf [liu20242zm]
371. a77b3c5f532e61af63a9d95e671ce02d8065ee24.pdf [yang2024lwa]
372. 2d12d1cec23e1c26c65de52100db70d91ca90035.pdf [li20246qx]
373. 4b1d0cf2b99aec85cdedceaef88c3a074de79832.pdf [liu2024mji]
374. 0845cea58467d372eb296fa1f184ecabe02be18b.pdf [chen2024efo]
375. 6a9caace1919b0e7bb247f0ecb585068c1ec4ff8.pdf [chen2024uld]
376. 30321b036607a7936221235ea8ec7cf7c1627100.pdf [wang2017zm5]
377. e03b8e02ddda86eafb54cafc5c44d231992be95a.pdf [li2021qr0]

## Literature Review

### Introduction

\section{Introduction}
\label{sec:introduction}

The effective representation and utilization of knowledge have long been central to the advancement of artificial intelligence. Knowledge Graphs (KGs) have emerged as a powerful paradigm for organizing vast amounts of structured information, representing real-world entities and their intricate relationships in a human-readable, symbolic format. However, despite their expressiveness, traditional symbolic KGs inherently face challenges such as data sparsity, computational inefficiency in large-scale reasoning, and difficulty in capturing nuanced semantic similarities or handling incompleteness. These limitations hinder their seamless integration with data-driven machine learning models.

To overcome these hurdles, Knowledge Graph Embedding (KGE) methods have revolutionized the field by transforming discrete, symbolic knowledge into continuous, low-dimensional vector representations. This paradigm shift enables entities and relations to be represented as points or vectors in a continuous space, where semantic relationships are captured through geometric or algebraic operations. Such embeddings provide a machine-understandable format, facilitating scalability, efficient computation, and the ability to infer latent relationships, thereby making KGs more actionable for a wide array of AI tasks, including link prediction, entity alignment, and question answering.

This introductory section lays the foundational context for understanding the significance and trajectory of KGE research. We begin by tracing the evolution of knowledge representation, introducing the fundamental concepts and inherent challenges of knowledge graphs. Subsequently, we delve into the core motivations driving the development of KGE methods, explaining how they address the limitations of symbolic representations and enable new capabilities for AI systems. Finally, this section delineates the comprehensive scope and organizational structure of this literature review, providing a roadmap for the detailed discussions that follow on foundational models, advanced architectures, practical considerations, and diverse applications of KGE.

\subsection{Background: Knowledge Graphs}
\label{sec:1_1_background:_knowledge_graphs}

Knowledge Graphs (KGs) have emerged as a pivotal technology for organizing and representing real-world knowledge in a structured, machine-readable format. Fundamentally, a knowledge graph is a directed graph composed of entities (nodes) and relations (edges) that connect them [ge2023, dai2020]. Each piece of information within a KG is typically represented as a triple (head entity, relation, tail entity), often denoted as $(h, r, t)$. For instance, the triple (Barack Obama, bornIn, Hawaii) explicitly states that "Barack Obama was born in Hawaii." This structured representation allows for unambiguous storage and retrieval of facts, forming a rich network of interconnected information [cao2022].

The conceptual roots of knowledge graphs can be traced back to early efforts in Artificial Intelligence to represent knowledge symbolically. Semantic networks, developed in the 1960s and 1970s, were among the first attempts to model knowledge as a graph of concepts and relationships. These early networks, while foundational, often suffered from ambiguity, a lack of formal semantics, and limited scalability, making complex reasoning difficult. The subsequent rise of expert systems in the 1980s, which relied on explicit rules and symbolic logic, further highlighted the challenges of managing large, intricate knowledge bases.

The advent of the Semantic Web in the early 2000s marked a significant step towards formalizing knowledge representation on a global scale. Technologies like Resource Description Framework (RDF) and Web Ontology Language (OWL) provided standardized frameworks for defining ontologies and expressing knowledge with formal semantics, aiming to make web content machine-understandable. This era saw the emergence of early large-scale KGs like DBpedia, which systematically extracted structured information from Wikipedia infoboxes, effectively bridging the gap between unstructured text and structured knowledge [dai2020]. DBpedia's success demonstrated the potential of automatically constructing KGs from existing data sources, laying the groundwork for more ambitious projects.

Modern knowledge graphs, such as Freebase and Wikidata, represent the culmination of these historical developments, leveraging massive datasets and advanced computational techniques. Freebase, a collaborative knowledge base initially developed by Metaweb and later acquired by Google, played a crucial role in organizing a vast array of general-purpose facts, significantly contributing to Google's own proprietary Knowledge Graph that powers its search engine [ge2023]. Wikidata, an open and collaborative knowledge base maintained by the Wikimedia Foundation, has become a central hub for structured data across various Wikimedia projects and beyond. Its multilingual nature and community-driven approach underscore its role in democratizing access to structured world knowledge and enabling a wide range of intelligent systems, from virtual assistants to sophisticated data analytics platforms. These modern KGs are characterized by their immense scale, dynamic nature, and their ability to integrate information from diverse sources, making them indispensable resources for contemporary AI applications.

Despite their power in organizing information, symbolic knowledge graphs inherently face several significant challenges that limit their utility, particularly in the context of modern data-driven AI.

1.  \textbf{Computational Inefficiency and Scalability}: Reasoning directly with symbolic representations, especially on large-scale KGs containing billions of facts, is computationally intensive. Tasks like inferring new facts through logical rules or answering complex queries often involve combinatorial search problems, leading to prohibitive computational costs and poor scalability [ge2023, dai2020]. For instance, traditional rule-based inference engines struggle with the sheer volume of facts in KGs like Wikidata, where deriving multi-hop relations can quickly become intractable. This inefficiency makes real-time applications challenging and limits the depth of reasoning that can be performed.

2.  \textbf{Sparsity and Incompleteness}: Real-world knowledge graphs are notoriously incomplete. Even the largest KGs, such as Freebase or DBpedia, contain only a fraction of all possible facts about entities and relations. Symbolic methods struggle to generalize from observed facts to infer missing ones, often requiring explicit rules or complete data to make deductions. This "cold-start" problem, where new entities or relations lack sufficient explicit connections, is a major bottleneck [dai2020]. For example, if a KG lacks the explicit triple (Albert Einstein, studiedAt, ETH Zurich), a purely symbolic system might fail to infer this, even if it has related facts. This limitation contrasts sharply with human ability to infer and generalize from partial information.

3.  \textbf{Difficulty in Capturing Nuanced Semantics and Similarity}: Symbolic representations are discrete and rigid. They treat each entity and relation as a distinct, atomic symbol, making it difficult to capture subtle semantic similarities or relationships between them. For instance, "car" and "automobile" are semantically very close, but a symbolic KG would treat them as entirely separate entities unless explicitly linked by an equivalence relation. Similarly, understanding that "fatherOf" is the inverse of "childOf" requires explicit logical rules, rather than being inherently captured by the representation itself. This rigidity hinders tasks that rely on semantic proximity, such as recommendation systems or natural language understanding, where nuanced meanings are paramount.

4.  \textbf{Incompatibility with Machine Learning}: Perhaps the most critical limitation in the era of deep learning is the inherent incompatibility of symbolic representations with numerical machine learning models. Modern AI algorithms, particularly neural networks, operate on continuous vector spaces. Directly feeding discrete symbols into these models is inefficient and often requires extensive feature engineering, which is labor-intensive and prone to error. This disconnect prevents KGs from being seamlessly integrated into powerful, end-to-end machine learning pipelines.

These collective challenges of symbolic KGs---computational inefficiency, pervasive incompleteness, difficulty in capturing nuanced semantics, and incompatibility with modern machine learning paradigms---collectively motivate the need for more advanced representation techniques. This is precisely where Knowledge Graph Embedding (KGE) emerges as a transformative solution. KGE aims to address these limitations by transforming entities and relations from their sparse, discrete symbolic forms into dense, continuous, low-dimensional vector representations, often referred to as embeddings, in a continuous vector space [cao2022, ge2023]. By embedding knowledge into a continuous space, KGE models enable several critical advantages: capturing semantic similarity, enhancing computational efficiency, handling incompleteness through generalization, and facilitating seamless integration with machine learning models. This paradigm shift from explicit symbolic reasoning to implicit, distributed representations in vector spaces has unlocked the full potential of knowledge graphs, making them more accessible, actionable, and powerful for a diverse range of intelligent systems.
\subsection{Motivation for Knowledge Graph Embedding}
\label{sec:1_2_motivation_for_knowledge_graph_embedding}

Building upon the discussion in the previous subsection regarding the inherent limitations of symbolic Knowledge Graphs (KGs), the core motivation for Knowledge Graph Embedding (KGE) emerges as a transformative paradigm shift in knowledge representation. Traditional symbolic KGs, while powerful for explicit fact storage, face significant hurdles that impede their utility in modern AI applications. These limitations primarily stem from their sparse, discrete nature, which struggles with nuanced semantics, computational efficiency, and handling incompleteness [dai2020, cao2022]. KGE directly addresses these challenges by projecting entities and relations into continuous, low-dimensional vector spaces, thereby converting complex symbolic problems into efficient vector operations and making KGs more accessible and actionable for AI [dai2020].

One of the most critical limitations of symbolic representations is their **inability to capture nuanced semantic similarities**. Each entity and relation is treated as an atomic, distinct symbol, meaning that semantic proximity (e.g., "car" and "automobile" being synonyms) is not inherently encoded unless explicitly defined by an equivalence relation. This rigidity hinders tasks that rely on understanding subtle semantic relationships, such as recommendation systems, natural language understanding, or information retrieval, where a query might use different but semantically similar terms. KGE overcomes this by representing entities and relations as dense vectors, where semantic similarity is naturally captured by vector proximity in the embedding space. For instance, entities with similar meanings or roles will have vectors that are close to each other, allowing for generalization and inference based on continuous semantic gradients rather than discrete matches [cao2022].

Furthermore, **computational inefficiency in large-scale reasoning** is a major bottleneck for symbolic KGs. Reasoning over billions of facts using traditional rule-based or logical inference engines often involves combinatorial search, leading to prohibitive computational costs and poor scalability. This makes real-time applications, such as dynamic question answering or complex multi-hop inference, extremely challenging. KGE models, by contrast, transform these symbolic operations into efficient vector arithmetic. For example, a relation can be modeled as a translation operation in the embedding space, where $(h, r, t)$ is plausible if the vector of $h$ plus the vector of $r$ is close to the vector of $t$ (as in TransE). This converts a search over discrete symbols into a distance calculation in a continuous space, which is highly optimized for modern hardware and machine learning frameworks, significantly boosting computational efficiency and scalability for large KGs [dai2020].

The pervasive **difficulty in handling incompleteness** is another fundamental problem that KGE aims to solve. Real-world KGs are inherently incomplete, containing only a fraction of all possible facts. Symbolic methods struggle to generalize from observed facts to infer missing ones, often requiring explicit rules or complete data to make deductions. This "cold-start" problem for new entities or relations is a significant barrier. KGE, however, leverages patterns learned from existing facts in the embedding space to predict missing links. By observing that certain entity pairs are connected by specific relation vectors, the model can infer the likelihood of unobserved triples. For example, early translational models like TransH [wang2014] and TransD [ji2015] were specifically designed to better handle complex relation types (e.g., one-to-many, many-to-one) that TransE struggled with, thereby improving the ability to infer missing facts in diverse scenarios. This generalization capability is crucial for knowledge graph completion, a primary application of KGE [rossi2020].

The paradigm shift to continuous vector representations also facilitates **seamless integration with modern machine learning pipelines**. Modern AI algorithms, particularly deep neural networks, are designed to operate on continuous numerical inputs. Symbolic representations are inherently incompatible, requiring extensive and often lossy feature engineering to be used with these models. KGE provides a natural bridge, transforming discrete symbolic knowledge into a format directly consumable by advanced machine learning models. This integration allows KGs to serve as rich, structured knowledge sources for tasks that benefit from both symbolic structure and statistical learning, such as natural language processing, computer vision, and recommendation systems.

This transformation into dense vector spaces provides a powerful mechanism for a multitude of downstream AI tasks:
\begin{itemize}
    \item \textbf{Link Prediction}: KGEs are fundamentally designed for link prediction, where the goal is to infer missing relations between existing entities or to identify new entities for a given relation. Models like RotatE [sun2018] exemplify this, defining relations as rotations in complex vector spaces to effectively model and infer complex relation patterns like symmetry, antisymmetry, inversion, and composition, significantly outperforming earlier translational models.
    \item \textbf{Entity Alignment}: This task involves identifying equivalent entities across different, often heterogeneous, KGs. By embedding entities from multiple KGs into a shared vector space, entity alignment becomes a problem of finding nearest neighbors in that space. Approaches like bootstrapping entity alignment [sun2018] and multi-view KGE for entity alignment [zhang2019] leverage embedding similarities to iteratively discover and refine alignments, demonstrating superior performance over symbolic matching methods.
    \item \textbf{Question Answering (QA)}: KGEs enhance QA systems by enabling the conversion of natural language queries and KG facts into a common embedding space, facilitating efficient matching and retrieval of answers. The Knowledge Embedding based Question Answering (KEQA) framework [huang2019] jointly recovers entity, predicate, and tail entity representations in the KG embedding spaces, allowing for direct matching with KG facts. More sophisticated systems, such as Marie and BERT for chemistry QA [zhou2023], integrate hybrid KGEs with advanced NLP models to handle complex queries and deep ontologies.
    \item \textbf{Recommendation Systems}: KGEs can model user-item interactions and item properties within a KG, providing rich contextual information for personalized recommendations. Recurrent KGE (RKGE) [sun2018] uses a recurrent network to learn semantic representations of paths between entities, characterizing user preferences and even providing meaningful explanations for recommendations. Similarly, contextualized KGE (CKGE) [yang2023] integrates neighbor semantics and high-order connections for explainable talent training course recommendations.
\end{itemize}

In essence, the motivation for KGE is to overcome the inherent limitations of sparse, symbolic knowledge representation by providing a scalable, semantically rich, and computationally efficient alternative. Early work, such as TransH [wang2014], began to address the limitations of simpler models by introducing hyperplanes to better capture diverse relation properties, a step towards more expressive embeddings. This evolution from discrete symbols to continuous vectors represents a fundamental paradigm shift, making KGs not just repositories of facts, but dynamic, actionable components within modern AI systems. While early KGE models sometimes faced a trade-off between expressiveness and computational efficiency, the field has continuously evolved, with models like RotatE [sun2018] demonstrating how sophisticated geometric operations can capture complex logical patterns while maintaining scalability. The collective efforts in KGE research have thus transformed KGs from static, hard-to-manage data structures into versatile, machine-understandable knowledge sources, crucial for the advancement of artificial intelligence.
\subsection{Scope and Structure of the Review}
\label{sec:1_3_scope__and__structure_of_the_review}


Building upon the preceding discussion regarding the fundamental motivations for Knowledge Graph Embedding (KGE), this comprehensive literature review systematically delineates the intellectual landscape of the field, tracing its evolution from foundational theoretical underpinnings to cutting-edge advancements and diverse real-world applications. The scope of this review is intentionally broad, encompassing the primary paradigms and significant innovations that have shaped KGE research. We begin by examining the core conceptual models that first enabled the representation of entities and relations in continuous vector spaces, subsequently progressing to advanced architectural designs, critical practical considerations, and the wide array of applications that leverage KGE for enhanced artificial intelligence capabilities. This pedagogical progression ensures a coherent narrative, illustrating how the field has continuously evolved to address the inherent complexities and limitations of knowledge representation.

The review's comprehensive scope is structured to guide the reader through a logical progression of KGE research. Initially, we delve into the **foundational models** that established the geometric and algebraic paradigms for embedding knowledge graphs. Early translational models, such as TransH [wang2014] and TransD [ji2015], are critically analyzed for their core innovations in handling complex relational patterns like one-to-many and many-to-one, and their trade-offs between expressiveness and computational efficiency. These models, while efficient, often struggled with intricate relational semantics, a limitation that spurred the development of more sophisticated geometric approaches. This initial phase of research, as highlighted by surveys like [dai2020] and [cao2022], laid the groundwork by converting symbolic knowledge into machine-understandable vector operations, addressing the sparsity and computational inefficiency inherent in traditional KGs.

The narrative then progresses to **advanced architectures**, particularly those leveraging deep learning, which represent a significant paradigm shift in KGE. This includes the integration of Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and Transformer models. These architectures, unlike their geometric predecessors, excel at automatically learning complex, non-linear feature interactions and capturing intricate structural patterns directly from the graph topology. For instance, while RotatE [sun2018] introduced rotations in complex space to model symmetry and inversion more effectively than translational models, deep learning models offer even greater capacity for capturing hierarchical and contextual relationships. This evolution reflects the field's response to the need for more expressive and context-aware representations, moving beyond simple triplet-based interactions to leverage broader graph structures.

Beyond core model development, the review critically examines **practical considerations** essential for the real-world deployment of KGE. This includes strategies for improving computational efficiency, managing memory footprint, and ensuring scalability for increasingly massive knowledge graphs. The field has seen a continuous tension between model expressiveness and practical feasibility; highly expressive models often incur significant computational costs, limiting their applicability to large-scale KGs. This section also addresses methods for enhancing model robustness against noisy data, optimizing training processes, and the crucial aspects of rigorous evaluation, benchmarking, and reproducibility. As noted by [rossi2020], inconsistent reporting practices and evaluation biases can obscure true model performance, underscoring the importance of standardized methodologies. For example, methods like HyTE [dasgupta2018], while innovative in incorporating temporal dynamics, introduce additional complexity that must be managed for scalability.

Finally, the review explores the **diverse applications** of KGE, demonstrating their transformative impact across various AI domains. This includes fundamental tasks like link prediction and knowledge graph completion, where KGE models infer missing facts by leveraging learned patterns. The utility extends to more complex applications such as entity alignment across heterogeneous KGs, exemplified by bootstrapping approaches [sun2018] and multi-view frameworks [zhang2019] that leverage embedding similarities. Furthermore, KGEs have significantly enhanced question answering systems, with frameworks like KEQA [huang2019] demonstrating how jointly learned entity and predicate representations can improve answer retrieval. In recommender systems, recurrent KGEs (RKGE) [sun2018] have been shown to effectively characterize user preferences and provide explainable recommendations. These applications collectively illustrate the practical value of KGE in making knowledge graphs actionable and intelligent components within modern AI systems.

The organizational structure of this review is designed to provide a clear roadmap through this complex landscape:
\begin{enumerate}
    \item \textbf{Section 1: Introduction} establishes the foundational context, motivations, and the overarching scope and structure of the review.
    \item \textbf{Section 2: Foundational KGE Models and Geometric Paradigms} delves into the theoretical underpinnings, examining early translational models (e.g., TransH [wang2014], TransD [ji2015]) and more advanced geometric approaches like RotatE [sun2018]. This section highlights the initial breakthroughs in representing symbolic knowledge in continuous spaces and their inherent capabilities and limitations.
    \item \textbf{Section 3: Deep Learning Architectures for Knowledge Graph Embedding} explores the integration of advanced neural network architectures (CNNs, GNNs, Transformers), which represent a shift towards learning more expressive and context-aware representations, often overcoming the limitations of purely geometric models in capturing complex, non-linear patterns.
    \item \textbf{Section 4: Enriching KGE: Auxiliary Information, Rules, and Multi-modality} discusses methods that augment KGE with external knowledge, such as entity types, attributes, logical rules, and multi-modal data. This addresses the challenge of data sparsity and enhances semantic understanding, moving beyond purely structural information.
    \item \textbf{Section 5: Dynamic, Inductive, and Distributed KGE} focuses on adapting KGE models to the evolving, incomplete, and decentralized nature of real-world knowledge graphs, covering temporal KGE (e.g., HyTE [dasgupta2018]), inductive learning for unseen entities, and privacy-preserving federated approaches. This section addresses the critical need for models that can operate in dynamic, real-world environments.
    \item \textbf{Section 6: Practical Considerations: Efficiency, Robustness, and Evaluation} addresses the crucial aspects of deploying KGE models, including scalability, robustness against noise, and the importance of rigorous evaluation and reproducibility in research, as emphasized by comparative analyses like [rossi2020].
    \item \textbf{Section 7: Applications and Real-World Impact of KGE} showcases the practical utility of KGE across diverse domains, including link prediction, entity alignment [sun2018, zhang2019], question answering [huang2019], and recommender systems [sun2018]. This section bridges the gap between theoretical advancements and tangible societal benefits.
    \item \textbf{Section 8: Conclusion and Future Directions} synthesizes the key findings, identifies persistent open challenges, and outlines emerging trends and ethical considerations that will shape the future trajectory of KGE research, drawing insights from comprehensive surveys [dai2020, cao2022].
\end{enumerate}
This structured approach ensures that readers gain a holistic understanding of KGE, from its theoretical underpinnings to its real-world impact and future potential, providing a comprehensive roadmap for navigating this rapidly evolving field.


### Foundational KGE Models and Geometric Paradigms

\label{sec:foundational_kge_models__and__geometric_paradigms}

\section{Foundational KGE Models and Geometric Paradigms}
\label{sec:2_foundational_kge_models_and_geometric_paradigms}

This section marks a pivotal point in our review, transitioning from the foundational motivations for Knowledge Graph Embedding (KGE) to the initial, groundbreaking methodologies that transformed symbolic knowledge into continuous vector representations. Building upon the understanding that traditional Knowledge Graphs (KGs) struggle with sparsity, computational inefficiency, and capturing nuanced semantics, this section delves into the bedrock of KGE research: the foundational geometric and algebraic paradigms. These early and influential models laid the groundwork for representing entities and relations in continuous vector spaces, establishing the core principles for how relational patterns could be encoded and inferred.

The evolution began with simple yet powerful translational models, such as TransE and its crucial extensions like TransH [wang2014] and TransD [ji2015]. These models conceptualized relations as direct translations between head and tail entities in an embedding space, offering significant advancements in efficiency and the ability to infer missing links. However, their limitations in capturing complex relational patterns, such as symmetry, antisymmetry, and composition, quickly became apparent. This challenge spurred the development of more sophisticated geometric approaches.

Consequently, the field progressed to models that leveraged rotations and complex-valued spaces, exemplified by RotatE [sun2018]. These innovations allowed for the elegant modeling of intricate logical patterns through algebraic transformations, significantly enhancing expressiveness beyond simple translations. Further explorations extended to multi-dimensional geometric approaches, including embeddings on Lie groups, different metric choices, and advanced transformations like Householder parameterization. This continuous refinement of mathematical foundations, exploring diverse embedding spaces and transformation mechanisms, was driven by the imperative to capture richer semantic interactions and improve the theoretical soundness and robustness of KGE models. Ultimately, this section traces the critical journey from basic vector operations to complex geometric transformations, forming the theoretical and practical basis for all subsequent advancements in KGE.

\subsection{Core Translational Models and Extensions}
\label{sec:2_1_core_translational_models__and__extensions}


The foundational era of Knowledge Graph Embedding (KGE) research was significantly shaped by a family of models that conceptualized relations as translational operations within a continuous vector space. These pioneering translational models, notably TransE, TransH, TransR, and TransD, established a fundamental paradigm for representing symbolic knowledge in a machine-understandable format. They offered enhanced efficiency and expressiveness over purely symbolic methods, marking a crucial step in transforming knowledge graph analysis and laying the groundwork for subsequent geometric and deep learning advancements [asmara2023].

The seminal work, \textbf{TransE} (Translating Embeddings) [bordes2013], introduced the intuitive idea that if a triple $(h, r, t)$ exists in a knowledge graph, then the embedding of the head entity ($\mathbf{h}$) plus the embedding of the relation ($\mathbf{r}$) should approximately equal the embedding of the tail entity ($\mathbf{t}$), i.e., $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$. This simple vector addition in Euclidean space, typically minimized using a scoring function like $f(h,r,t) = ||\mathbf{h} + \mathbf{r} - \mathbf{t}||_{L1/L2}$, offered remarkable efficiency and a straightforward mechanism for link prediction. However, TransE suffered from a critical theoretical limitation: its inherent inability to effectively model complex relation patterns such as one-to-many, many-to-one, and many-to-many relations. This limitation arises because TransE represents each entity and relation as a single point or vector in the same embedding space. For instance, if a relation `(country, hasCapital, city)` is one-to-many (e.g., `(France, hasCapital, Paris)` and `(France, hasCapital, Lyon)` if considering historical capitals), TransE would force the embeddings of `Paris` and `Lyon` to be very close to each other relative to `France` and `hasCapital`, which is semantically incorrect and leads to a "blurring" of entity representations. This issue, as highlighted by [wang2014], motivated the development of more sophisticated translational models.

Building directly on TransE's insights, [wang2014] introduced \textbf{TransH} (Translating on Hyperplanes) to address the challenge of modeling one-to-many and many-to-one relations more effectively. The core innovation of TransH lies in representing each relation not as a single vector in the entity embedding space, but as a hyperplane ($\mathbf{w}_r$) along with a translation vector ($\mathbf{r}$) on that hyperplane. For a given triple $(h, r, t)$, TransH first projects the head and tail entity embeddings onto the relation-specific hyperplane: $\mathbf{h}_{\perp} = \mathbf{h} - \mathbf{w}_r^\top \mathbf{h} \mathbf{w}_r$ and $\mathbf{t}_{\perp} = \mathbf{t} - \mathbf{w}_r^\top \mathbf{t} \mathbf{w}_r$. The translational assumption then applies to these projected embeddings: $||\mathbf{h}_{\perp} + \mathbf{r} - \mathbf{t}_{\perp}||_{L1/L2}$. Unlike TransE, which uses a single embedding for each entity regardless of the relation, TransH allows a single entity to have different representations (projections) when involved in different relations. This mechanism enables the model to handle multiple tail entities for a single head (one-to-many) or multiple head entities for a single tail (many-to-one) without forcing their original embeddings to be identical. TransH successfully claims to solve the problem of preserving diverse mapping properties of relations with almost the same model complexity as TransE, achieving significant improvements in predictive accuracy on benchmark datasets like WordNet and Freebase [wang2014]. Its practical limitation is a slight increase in parameters compared to TransE due to the additional hyperplane normal vector for each relation, but it maintains competitive scalability.

Following TransH, models like \textbf{TransR} [lin2015] and CTransR further explored the idea of relation-specific transformations by introducing relation-specific projection matrices. These models generally map entities from the entity embedding space into a relation-specific space *before* applying the translation. For instance, TransR projects $\mathbf{h}$ and $\mathbf{t}$ into a relation-specific space using a matrix $\mathbf{M}_r$, then applies the translation: $||\mathbf{h}\mathbf{M}_r + \mathbf{r} - \mathbf{t}\mathbf{M}_r||_{L1/L2}$. This approach aimed to capture more fine-grained relational semantics than TransH's hyperplane projection. However, a significant practical limitation of TransR/CTransR was the high parameter count. Each relation required its own dense projection matrix $\mathbf{M}_r$, leading to a parameter complexity of $O(N_r \cdot d_e \cdot d_r)$ where $N_r$ is the number of relations, $d_e$ is entity embedding dimension, and $d_r$ is relation embedding dimension. This quadratic growth in parameters with respect to embedding dimensions hindered scalability for large knowledge graphs, making them less practical for real-world applications with many relations.

This parameter explosion limitation was directly addressed by [ji2015] with the introduction of \textbf{TransD} (Knowledge Graph Embedding via Dynamic Mapping Matrix). TransD claims to solve the problem of high parameter count in TransR/CTransR while considering the diversity of both relations and entities. Its core innovation is the use of dynamic mapping matrices. Instead of a static, dense projection matrix for each relation, TransD assigns two vectors to each entity ($\mathbf{e}, \mathbf{e}_p$) and relation ($\mathbf{r}, \mathbf{r}_p$). The first vector represents the entity's or relation's meaning, while the second is used to dynamically construct a mapping matrix for projection. Specifically, the projection of an entity $\mathbf{h}$ for relation $\mathbf{r}$ is given by $\mathbf{h}_{\perp r} = \mathbf{h} + \mathbf{h}_p \mathbf{r}_p^\top \mathbf{h}$, and similarly for $\mathbf{t}$. The scoring function then follows the translational principle: $||\mathbf{h}_{\perp r} + \mathbf{r} - \mathbf{t}_{\perp r}||_{L1/L2}$. In contrast to TransR's fixed, dense matrices, TransD's dynamic, entity-specific projection mechanism allows for more fine-grained transformations while significantly reducing the number of parameters to $O(N_e \cdot d_e + N_r \cdot d_r)$, as it avoids storing large, dense projection matrices. This makes TransD a more scalable solution for large-scale graphs, demonstrating superior performance on triplet classification and link prediction tasks [ji2015]. Its practical advantage lies in its parameter efficiency and absence of computationally intensive matrix-vector multiplication operations during projection, making it more applicable to large KGs. However, like other translational models, its theoretical limitations persist in fully capturing highly complex logical patterns such as composition or inversion, which later rotational models would specifically target.

Further extensions to the translational paradigm include models like \textbf{TransE-MTP} (TransE with Multi-Translation Principles) [additional_paper_1]. This model directly addresses TransE's struggle with complex relations by proposing *multiple translation principles* tailored for different relation types (e.g., one-to-one, one-to-many, many-to-one, many-to-many). Unlike TransH and TransD, which modify the projection mechanism, TransE-MTP modifies the core translational rule itself based on the identified relation type. It combines these MTPs with the original TransE framework, allowing for multiple optimization objectives during training on complex relations. This approach claims to provide superior prediction performance over both TransE and TransH, demonstrating its effectiveness at link prediction and triplet classification on Freebase and WordNet datasets [additional_paper_1]. This illustrates a pattern in translational model evolution: the initial simplicity of TransE led to subsequent models either introducing relation-specific projections (TransH, TransR, TransD) or adapting the translational principle itself (TransE-MTP) to better handle the diversity of real-world relational patterns.

In summary, the evolution of core translational models showcases a continuous effort to balance efficiency with expressiveness. Early work like TransE established the foundational idea of relations as translations, prioritizing computational efficiency. Mid-period models such as TransH and TransD systematically addressed TransE's limitations regarding complex relation patterns (one-to-many, many-to-one) by introducing relation-specific projections, first via hyperplanes and then through dynamic, parameter-efficient mapping matrices. This progression from TransE to TransH and then to TransD [asmara2023] illustrates a clear causal relationship: the limitations of simpler models (e.g., TransE's inability to distinguish multiple tail entities for a single head) directly led to innovations in subsequent models (e.g., TransH's hyperplanes, TransD's dynamic matrices) to overcome these specific challenges. TransR, while innovative in its use of projection matrices, highlighted the trade-off between expressiveness and parameter count, a challenge that TransD successfully mitigated. The introduction of TransE-MTP further refined this by adapting the translational principle itself to relation types. This intellectual trajectory exemplifies the field's shift from basic vector operations to more nuanced geometric transformations, all while striving for models that are both effective and scalable for real-world knowledge graphs.
\subsection{Rotational and Complex Space Embeddings}
\label{sec:2_2_rotational__and__complex_space_embeddings}


Building upon the foundational translational models discussed previously, which primarily focused on vector addition in real Euclidean spaces, a significant paradigm shift in Knowledge Graph Embedding (KGE) research emerged with the introduction of rotational and complex space embeddings. These models address the inherent limitations of translational approaches, particularly their struggle to capture richer relational semantics such as symmetry, antisymmetry, inversion, and composition [sun2018]. Translational models like TransE [wang2014] inherently assume that relations are simple displacements, making it difficult to distinguish between $h \xrightarrow{r} t$ and $t \xrightarrow{r} h$ for symmetric relations, or to model $h \xrightarrow{r_1} i \xrightarrow{r_2} t$ as equivalent to $h \xrightarrow{r_3} t$ for compositional relations. Rotational models, by contrast, leverage the algebraic properties of complex numbers and higher-dimensional rotations to offer more nuanced and powerful transformations.

The seminal work in this area is \textbf{RotatE} (Knowledge Graph Embedding by Relational Rotation in Complex Space) [sun2018]. RotatE claims to solve the problem of effectively modeling and inferring various complex relation patterns, including symmetry ($r \iff r^{-1}$), antisymmetry ($r \implies \neg r^{-1}$), inversion ($r \iff r'^{-1}$), and composition ($r_1 \circ r_2 \implies r_3$). Its core innovation lies in defining each entity as a vector in a complex vector space ($\mathbb{C}^k$) and each relation as an element-wise rotation from the head entity to the tail entity. Specifically, for a triple $(h, r, t)$, RotatE models the relation as $\mathbf{h} \circ \mathbf{r} \approx \mathbf{t}$, where $\mathbf{h}, \mathbf{r}, \mathbf{t} \in \mathbb{C}^k$ and $\circ$ denotes the Hadamard (element-wise) product. The relation vector $\mathbf{r}$ is constrained to have moduli of 1 for each dimension (i.e., $|r_i|=1$), ensuring it acts as a pure rotation. The scoring function is typically $f(h,r,t) = ||\mathbf{h} \circ \mathbf{r} - \mathbf{t}||_{L1/L2}$. This elegant formulation inherently captures the desired patterns:
\begin{itemize}
    \item \textbf{Symmetry}: If $r$ is symmetric, then $\mathbf{r}$ should be close to a vector of all 1s (no rotation), or $\mathbf{r}^2 \approx \mathbf{1}$.
    \item \textbf{Antisymmetry}: If $r$ is antisymmetric, then $\mathbf{r}$ should be far from a vector of all 1s.
    \item \textbf{Inversion}: If $r'$ is the inverse of $r$, then $\mathbf{r}' \approx \mathbf{r}^{-1}$ (element-wise conjugate).
    \item \textbf{Composition}: If $r_1 \circ r_2 \implies r_3$, then $\mathbf{r}_1 \circ \mathbf{r}_2 \approx \mathbf{r}_3$.
\end{itemize}
RotatE succeeds under conditions where KGs exhibit a rich variety of these logical patterns. Its theoretical advantage stems from the algebraic properties of complex numbers, which naturally encode rotations and their compositions. Empirically, RotatE significantly outperformed existing state-of-the-art models for link prediction on benchmark datasets like FB15k-237 and WN18RR, demonstrating its effectiveness [sun2018]. A practical limitation of RotatE, while more expressive than TransE, is its increased computational cost due to complex number arithmetic, and it might still struggle with very high-dimensional or extremely sparse relations where the "pure rotation" assumption might be too restrictive.

The success of RotatE spurred further research into higher-dimensional rotational embeddings. \textbf{Rotate3D} [gao2020] extends the concept of relations as rotations from 2D complex space to 3D Euclidean space. This model maps entities to 3D vectors and defines relations as 3D rotations, leveraging the non-commutative property of 3D rotations. This non-commutativity is particularly beneficial for modeling non-commutative composition patterns, which are essential for multi-hop reasoning where the order of relations matters. Rotate3D claims to solve the problem of capturing non-commutative composition, which RotatE, despite its compositional capabilities, might not fully exploit due to the element-wise nature of its complex rotations. The core innovation is using rotation matrices in 3D space, where $\mathbf{R}_r \in SO(3)$ is a 3D rotation matrix for relation $r$. The scoring function is $f(h,r,t) = ||\mathbf{R}_r \mathbf{h} - \mathbf{t}||_{L1/L2}$. Rotate3D demonstrated improved performance on link prediction and path query answering, especially for multi-hop reasoning tasks, compared to RotatE [gao2020]. Its theoretical limitation is that 3D rotations, while powerful, are still a specific type of transformation and may not capture all possible complex relational dynamics. Practically, managing 3D rotation matrices might introduce more parameters or computational complexity than element-wise complex rotations, though it offers a more direct geometric interpretation for certain patterns.

Moving beyond 3D Euclidean space, the use of quaternions offers an even richer algebraic structure for modeling rotations and addressing challenges like polysemy. \textbf{ConQuatE} (Contextualized Quaternion Embedding Towards Polysemy in Knowledge Graph for Link Prediction) [chen2025] is a recent model that leverages quaternion rotations. Quaternions, which extend complex numbers to four dimensions (one real, three imaginary components), can represent 3D rotations more compactly and efficiently than 3x3 matrices. ConQuatE claims to address the polysemy issue in KGs, where entities can exhibit different semantic characteristics depending on the relations they participate in. The core innovation is to incorporate contextual cues from various connected relations to enrich entity representations, which are then transformed using quaternion rotations. For a triple $(h, r, t)$, entities and relations are embedded as quaternions, and the relation $r$ acts as a quaternion rotation from $h$ to $t$. The multiple imaginary components of quaternions allow for a more expressive representation of diverse relational contexts, enabling the model to differentiate between different "senses" of an entity based on the relation. ConQuatE's theoretical advantage is its ability to capture more intricate interactions and context-dependent semantics due to the higher-dimensional algebraic properties of quaternions. Empirically, it outperforms state-of-the-art models for link prediction on datasets like FB15k-237 and WN18RR, particularly in scenarios where polysemy is prevalent [chen2025]. A practical limitation is the increased complexity of quaternion arithmetic, which can be computationally more demanding than complex number operations.

\begin{table}[htbp]
    \centering
    \caption{Comparative Overview of Rotational and Complex Space KGE Models}
    \label{tab:rotational_models}
    \begin{tabularx}{\textwidth}{|l|X|X|X|X|}
        \hline
        \textbf{Model} & \textbf{Embedding Space} & \textbf{Core Operation} & \textbf{Key Patterns/Problems Addressed} & \textbf{Theoretical/Practical Considerations} \\
        \hline
        \textbf{RotatE} [sun2018] & Complex Vector Space ($\mathbb{C}^k$) & Element-wise Rotation (Hadamard product) & Symmetry, Antisymmetry, Inversion, Composition & Elegant mathematical formulation, good expressiveness, moderate computational cost. \\
        \hline
        \textbf{Rotate3D} [gao2020] & 3D Euclidean Space & 3D Rotation Matrix & Non-commutative Composition, Multi-hop Reasoning & Direct geometric interpretation, higher parameter count for matrices, captures order in composition. \\
        \hline
        \textbf{ConQuatE} [chen2025] & Quaternion Space & Quaternion Rotation & Polysemy, Diverse Relational Contexts & Richer algebraic structure, compact 3D rotation, higher computational complexity than complex numbers. \\
        \hline
        \textbf{HousE} [li2022] & Real Vector Space & Householder Transformations (Rotations & Projections) & General relation patterns, mapping properties, higher-dimensional rotations & Generalizes rotations, strong modeling capacity, increased complexity. \\
        \hline
        \textbf{CompoundE} [ge2022] & Real Vector Space & Cascaded Translation, Rotation, Scaling & General relation patterns, diverse mapping properties & Unifies multiple operations, high expressiveness, potentially complex optimization. \\
        \hline
        \textbf{SpherE} [li2024] & Spherical Space & Rotational (entity as sphere) & Many-to-many relations, Set Retrieval & Novel entity representation, good for set-based queries, interpretability. \\
        \hline
    \end{tabularx}
\end{table}

The intellectual trajectory in this domain demonstrates a clear progression from simpler translational models to increasingly complex algebraic structures. While translational models like TransH [wang2014] and TransD [ji2015] introduced relation-specific projections to handle one-to-many/many-to-one relations, they fundamentally relied on vector addition, limiting their ability to model patterns that require more sophisticated transformations. RotatE [sun2018] directly addressed this by introducing rotations in complex space, demonstrating that the algebraic properties of complex numbers are inherently better suited for patterns like inversion and composition. This exemplifies the field's shift from a purely geometric interpretation to leveraging richer algebraic structures.

Further innovations in this family include \textbf{HousE} (Knowledge Graph Embedding with Householder Parameterization) [li2022], which proposes a more powerful KGE framework based on Householder transformations. Householder transformations can represent both reflections (a type of rotation) and projections, allowing HousE to achieve superior capacity for modeling relation patterns and handling sophisticated relation mapping properties simultaneously. HousE generalizes existing rotation-based models by extending rotations to high-dimensional real spaces, achieving state-of-the-art performance on several benchmarks [li2022]. Its core innovation is the use of Householder matrices, which are orthogonal and symmetric, providing a theoretically sound way to perform rotations and reflections.

Another approach, \textbf{CompoundE} [ge2022] and its 3D extension \textbf{CompoundE3D} [ge2023], takes a more generalized view by combining translation, rotation, and scaling operations. These models argue that a cascade of these fundamental geometric manipulations can capture a broader spectrum of relational semantics. CompoundE demonstrates that many existing scoring-function-based KGE models can be seen as special cases of its framework, highlighting a convergent research direction towards unifying different geometric operations. This approach offers high expressiveness but introduces a trade-off with increased model complexity and optimization challenges due to the compound nature of operations.

The concept of modeling transitivity, a specific type of compositional pattern, is addressed by \textbf{Rot-Pro} (Modeling Transitivity by Projection in Knowledge Graph Embedding) [song2021]. Rot-Pro theoretically shows that transitive relations can be modeled with projections and combines this insight with relational rotation. It proves that this combination can infer all common relation patterns, including transitivity, and achieves state-of-the-art results on datasets with transitive relations [song2021]. This work builds on the rotational paradigm by adding a projection component to specifically target transitivity, a pattern that even pure rotational models might not perfectly capture.

More recently, \textbf{MQuinE} [liu2024] identifies and resolves a theoretical deficiency, termed "Z-paradox," in some popular KGE models, which can degrade performance, especially on challenging test samples. MQuinE proposes a new KGE model that avoids this paradox while preserving strong expressiveness for various relation patterns, including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations. This highlights a critical analysis of the theoretical underpinnings of existing models, pushing for more robust and theoretically sound designs. Similarly, \textbf{SpherE} [li2024] extends rotational embeddings by representing entities as spheres instead of vectors, specifically targeting many-to-many relations and enabling set retrieval. This novel entity representation, while inheriting the interpretability of rotational models, offers a unique way to handle set-based queries, a problem often overlooked by traditional KGEs.

The field also sees efforts to generalize orthogonal transformations. \textbf{GoldE} (Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization) [li2024] introduces a framework based on a generalized form of Householder reflection. This universal orthogonal parameterization allows for dimensional extension and geometric unification, enabling the framework to simultaneously capture crucial logical patterns and inherent topological heterogeneity. GoldE represents a culmination of efforts to create a unified and highly expressive framework for orthogonal transformations, demonstrating state-of-the-art performance across benchmarks [li2024].

A critical tension across these models is the trade-off between increased expressiveness and computational complexity. While models like RotatE [sun2018] offer a good balance, extensions to 3D rotations or quaternions, or the use of Householder transformations [li2022], often come with higher parameter counts or more involved arithmetic operations. For instance, the parameter efficiency of TransD [ji2015] (from the previous subsection) is a practical advantage that more complex rotational models sometimes sacrifice. Furthermore, the assumption that relations can be perfectly modeled by a single type of geometric transformation (e.g., pure rotation) might be an oversimplification for the vast diversity of real-world relations. The evaluation of these models primarily relies on link prediction metrics, which, while standard, may not fully capture the nuances of all the complex patterns they claim to model, especially for tasks like set retrieval (SpherE) or complex logical reasoning. The generalizability of these models to extremely large and sparse KGs, where the computational overhead of complex transformations can become prohibitive, remains an ongoing challenge. Despite these limitations, the shift to rotational and complex/higher-dimensional space embeddings marks a profound advancement, offering more nuanced and powerful transformations that move beyond the limitations of simpler translational approaches, thereby significantly enriching the semantic modeling capabilities of KGEs.
\subsection{Other Geometric and Algebraic Innovations}
\label{sec:2_3_other_geometric__and__algebraic_innovations}


Building upon the foundational translational models and the specific rotational and complex space paradigms discussed in preceding subsections, the field of Knowledge Graph Embedding (KGE) has continuously explored a broader spectrum of geometric and algebraic innovations. This subsection delves into models that challenge conventional Euclidean embedding spaces, introduce novel metric choices, or employ advanced, often compound, transformations beyond simple vector addition or pure rotation. The overarching goal is to refine the mathematical foundations of KGE, enabling more nuanced representations of entities and relations, particularly for capturing complex logical patterns and addressing inherent limitations of simpler geometric models. As highlighted by [cao2022], the choice of representation space and its mathematical properties profoundly influences a model's capacity to capture diverse KG structures, often dictating its ability to model hierarchical, compositional, or context-dependent semantics.

A significant paradigm shift from standard Euclidean vector spaces is observed in models that embed entities on Lie groups or non-Euclidean manifolds, which offer inherent advantages for specific data structures. Unlike Euclidean spaces, which struggle with certain topological properties like hierarchies or bounded representations, these alternative geometries can naturally accommodate them.

**Non-Euclidean Geometries and Manifold Embeddings:**

One pioneering work in this direction is \textbf{TorusE} [ebisu2017], which embeds entities on a Lie group, specifically a torus (a compact manifold). TorusE claims to solve the regularization problem inherent in TransE-like models, where explicit regularization (e.g., constraining embeddings to a unit sphere) is often needed to prevent divergence during negative sampling. This regularization can inadvertently warp embeddings in Euclidean space, hindering their ability to accurately fulfill the translational principle ($h+r \approx t$). TorusE's core innovation is that the torus itself is a bounded space, naturally preventing embeddings from diverging while preserving the translational property. Empirically, TorusE demonstrated superior performance over TransE, DistMult, and ComplEx on link prediction tasks, and notably, was faster than the original TransE, showcasing that non-Euclidean spaces can offer both theoretical advantages and practical efficiency [ebisu2017]. However, while it elegantly solves the regularization issue, TorusE, being translation-based, still inherits the expressiveness limitations of TransE for complex relation patterns like symmetry or composition, which rotational models (e.g., RotatE [sun2018] from Section 2.2) are better equipped to handle. Its core assumption of relations as simple translations on a torus, while regularization-friendly, might be too restrictive for the rich semantics found in many KGs, failing to capture the diverse transformations relations can represent.

Further exploring non-Euclidean geometries, hyperbolic spaces have gained traction due to their natural ability to model hierarchical structures prevalent in KGs. Unlike Euclidean space, which expands uniformly, hyperbolic space expands exponentially, allowing for more efficient embedding of tree-like or hierarchical data with fewer dimensions. Early hyperbolic KGE models often adopted a "hybrid" approach, mapping data between hyperbolic and tangent Euclidean spaces for transformations, which introduces computational overhead and numerical instability. \textbf{Fully Hyperbolic Rotation (FHRE)} [liang2024] addresses this by defining the KGE model entirely within hyperbolic space, specifically the Lorentz model. FHRE's core innovation is to treat each relation as a Lorentz rotation, transforming head entity embeddings to tail entity embeddings directly within hyperbolic space, eliminating iterative logarithmic and exponential mappings during training. This direct operation, leveraging the Lorentz rotation theorem, leads to a more stable and efficient model for hierarchical data. FHRE achieved state-of-the-art performance on challenging datasets like CoDEx-s and CoDEx-m, demonstrating its effectiveness and generalization ability, particularly for KGs with pronounced hierarchical patterns [liang2024]. A key limitation of FHRE, despite its efficiency, is its reliance on the Lorentz model, which might not be universally optimal for all types of hierarchical structures or logical patterns, and its "pure rotation" assumption might be too restrictive for relations that involve more complex transformations beyond simple hierarchical shifts.

In contrast to FHRE's fully hyperbolic design, \textbf{Unified Geometry Knowledge Graph Embedding (UniGE)} [UniGE_Anonymous] takes a different tack by seamlessly integrating KGE in *both* Euclidean and hyperbolic geometric spaces. UniGE proposes an embedding alignment method and fusion strategy using optimal transport techniques and the Wasserstein barycenter method. This hybrid approach aims to capture the diverse geometric data structures (chains and hierarchies) within KGs that might exceed the capacity of a single embedding space. UniGE's theoretical analysis suggests a more robust error bound, and it empirically outperforms state-of-the-art methods on benchmark datasets, indicating that a judicious combination of geometries can be more expressive than relying solely on one [UniGE_Anonymous]. The trade-off for UniGE's enhanced expressiveness is the increased complexity of managing and aligning embeddings across different geometric spaces compared to FHRE's simpler, fully hyperbolic operations. While FHRE prioritizes computational stability and directness within a single hyperbolic geometry, UniGE sacrifices some of that simplicity for broader applicability across diverse KG structures.

Another hyperbolic approach, proposed by [pan2021], leverages an extended Poincar Ball and a polar coordinate system within hyperbolic space to capture both hierarchical structures and logical patterns. This model addresses the boundary conditions of the Poincar Ball by expanding the modulus length, demonstrating that even within hyperbolic geometry, there are diverse ways to construct embedding spaces, each with its own advantages for specific KG characteristics. Similarly, \textbf{Hierarchical Hyperbolic Neural Graph Embedding (H2E)} [H2E_Anonymous] also employs a hyperbolic polar embedding space with a dual-embedding (modulus and phase parts) to explicitly model inter-level and intra-level hierarchies. H2E further integrates an attentional neural context aggregation to enhance its ability to preserve hierarchical relations. Both [pan2021] and H2E highlight the potential of polar coordinates in hyperbolic space for hierarchical modeling, but their optimization complexity, especially in managing boundary conditions or dual embeddings, can be a practical challenge, potentially leading to slower convergence or sensitivity to hyperparameters.

Further advancing the use of non-Euclidean geometries, \textbf{HolmE} [zheng2024] introduces a general form of Riemannian KGE, specifically leveraging hyperbolic space, with the novel property of being "closed under composition." This means that the composition of any two relation embeddings remains within the same embedding space, aligning with the theoretical nature of real-world relations and enabling robust modeling of composition patterns regardless of their representation in training data. Unlike prior KGEs that often implicitly assume relations are non-compositional if their patterns are not well-represented, HolmE explicitly designs its relation embedding space to ensure this closure. HolmE extends Mbius addition to product spaces for computational efficiency and theoretically proves that prominent KGE models like TransE [bordes2013] and RotatE [sun2018] (discussed in Section 2.2) are special cases of its framework, offering a unifying perspective. Empirically, HolmE demonstrates notable advantages in modeling composition patterns, especially for long-tail patterns, and effectively extrapolates to unseen relations [zheng2024]. However, its focus on composition patterns, while crucial, might mean less emphasis on other complex patterns like symmetry or inversion, which RotatE, for instance, explicitly targets. Its theoretical elegance comes with the inherent complexities of Riemannian geometry, which can be challenging for practitioners.

Beyond the choice of embedding space, the underlying metric used to measure distances and define relationships also profoundly impacts model expressiveness. \textbf{CyclE} [yang2021] addresses this by questioning the ubiquitous use of the Minkowski metric in KGE. The paper argues that existing methods often overlook the implications of the embedding space's metric, leading to suboptimal performance. CyclE's core innovation is the introduction of a "Cycle metric," which is based on the oscillation property of periodic functions. It posits that a smaller function period in this metric leads to better expressive ability. By combining the Cycle metric with popular KGE models, CyclE empirically demonstrated that its proposed metric is more appropriate than the Minkowski metric for KGE, leading to improved expressiveness [yang2021]. This work highlights a fundamental, yet often unstated, assumption in KGE research: the choice of distance function. While CyclE offers a novel metric, its success is still contingent on the underlying KGE model's ability to leverage this metric effectively. Its practical limitation lies in the need for careful tuning of the function period, which can be a complex hyperparameter, and the theoretical justification for why a "cycle" metric is universally superior to Minkowski for all KG patterns is not fully explored, potentially limiting its generalizability.

Another innovative approach to geometric representation is exemplified by models that move beyond representing entities as single points. \textbf{ManifoldE} [xiao2015] proposes a manifold-based embedding principle that replaces the over-strict point-wise translation principle ($h+r \approx t$) with a manifold-wise principle. This model claims to solve the problem of *precise link prediction* by addressing the ill-posed algebraic system and over-rigid geometric forms of traditional KGEs. Instead of mapping true triples to a single point, ManifoldE maps them to a *manifold* (e.g., a high-dimensional sphere or hyperplane) defined by the head entity and relation. This provides a more flexible geometric representation, especially for complex relations like one-to-many or many-to-many. ManifoldE significantly improved HITS@1 scores on FB15K for N-N relations (e.g., 53.0\% for head prediction vs. TransE's 18.1\%), demonstrating its effectiveness for precise link prediction while maintaining efficiency comparable to TransE [xiao2015]. This contrasts sharply with TransE's assumption of a single target point, which breaks down for complex relations. However, the choice and parameterization of the manifold itself can be complex, and its expressiveness is tied to how well the chosen manifold (e.g., sphere, hyperplane) aligns with the intrinsic geometry of the relations.

Similarly, \textbf{SpherE} [li2024] extends rotational embeddings by representing entities as spheres instead of vectors, specifically targeting many-to-many relations and enabling set retrieval. While ManifoldE uses manifolds to define the *target* of a relation, SpherE represents the *entities themselves* as spheres, aiming to capture semantic "spread" rather than a precise target. This novel entity representation, while inheriting the interpretability of rotational models, offers a unique way to handle set-based queries, a problem often overlooked by traditional KGEs. In a different vein, \textbf{TransC} [lv2018] introduces a novel geometric representation for entities by differentiating between concepts and instances. It encodes concepts as spheres and instances as vectors in the same semantic space. This allows for a more intuitive and effective modeling of `instanceOf` and `subClassOf` relations, demonstrating how varying geometric primitives for different entity types can enhance semantic capture. This approach requires explicit type information, which might not always be available in all KGs, limiting its generalizability compared to models like SpherE that learn representations without such external metadata.

**Generalized and Compound Transformations:**

Beyond these fundamental changes to space, metric, and entity representation, other models introduce specialized algebraic or linear transformations to refine relational modeling, often within conventional Euclidean spaces. The intellectual trajectory in this domain also demonstrates a clear progression towards unifying and generalizing transformations. For instance, \textbf{HousE} (Knowledge Graph Embedding with Householder Parameterization) [li2022] proposes a more powerful KGE framework based on Householder transformations. Householder transformations can represent both reflections (a type of rotation) and projections, allowing HousE to achieve superior capacity for modeling relation patterns and handling sophisticated relation mapping properties simultaneously. HousE generalizes existing rotation-based models by extending rotations to high-dimensional real spaces, achieving state-of-the-art performance on several benchmarks [li2022]. Its core innovation is the use of Householder matrices, which are orthogonal and symmetric, providing a theoretically sound way to perform rotations and reflections. Building on this, \textbf{GoldE} (Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization) [li2024] introduces a framework based on a generalized form of Householder reflection. This universal orthogonal parameterization allows for dimensional extension and geometric unification, enabling the framework to simultaneously capture crucial logical patterns and inherent topological heterogeneity. GoldE represents a culmination of efforts to create a unified and highly expressive framework for orthogonal transformations, demonstrating state-of-the-art performance across benchmarks [li2024]. GoldE's strength lies in its ability to generalize HousE, offering a more comprehensive and flexible approach to orthogonal transformations, but at the cost of increased mathematical abstraction.

Another approach, \textbf{CompoundE} [ge2022] and its 3D extension \textbf{CompoundE3D} [ge2023], takes a more generalized view by combining translation, rotation, and scaling operations. These models argue that a cascade of these fundamental geometric manipulations can capture a broader spectrum of relational semantics. CompoundE demonstrates that many existing scoring-function-based KGE models can be seen as special cases of its framework, highlighting a convergent research direction towards unifying different geometric operations. This approach offers high expressiveness by explicitly modeling relations as a sequence of multiple transformations, providing greater flexibility than models relying on a single type of transformation like RotatE (Section 2.2). However, this comes with a trade-off of increased model complexity and optimization challenges due to the compound nature of operations and the higher number of parameters to learn, potentially impacting scalability compared to more parameter-efficient models like TransD [ji2015] (Section 2.1).

The concept of modeling transitivity, a specific type of compositional pattern, is addressed by \textbf{Rot-Pro} (Modeling Transitivity by Projection in Knowledge Graph Embedding) [song2021]. Rot-Pro theoretically shows that transitive relations can be modeled with projections and combines this insight with relational rotation. It proves that this combination can infer all common relation patterns, including transitivity, and achieves state-of-the-art results on datasets with transitive relations [song2021]. This work builds on the rotational paradigm by adding a projection component to specifically target transitivity, a pattern that even pure rotational models might not perfectly capture. This highlights a pattern of augmenting core geometric operations with additional transformations to target specific complex patterns, demonstrating that a combination of transformations can be more effective than a single one for certain logical properties.

**Novel Metrics and Refined Algebraic Models (within Euclidean/Real space):**

More recently, \textbf{MQuinE} [liu2024] identifies and resolves a theoretical deficiency, termed "Z-paradox," in some popular KGE models, which can degrade performance, especially on challenging test samples. MQuinE proposes a new KGE model that avoids this paradox while preserving strong expressiveness for various relation patterns, including symmetric/asymmetric, inverse, 1-N/N-1/N-N, and composition relations. This highlights a critical analysis of the theoretical underpinnings of existing models, pushing for more robust and theoretically sound designs. MQuinE's contribution is less about a novel geometric space or transformation, and more about refining the underlying mathematical formulation to avoid pitfalls, making it a general algebraic innovation that enhances reliability.

Finally, some models focus on refining transformations within the conventional Euclidean space, often prioritizing efficiency or specific relational nuances:
\begin{itemize}
    \item \textbf{LineaRE} [peng2020]: This model simplifies KGE to a linear regression task, modeling relations as a linear function of two low-dimensional entity vectors with weight and bias vectors. Despite its simplicity, LineaRE claims to cover four connectivity patterns (symmetry, antisymmetry, inversion, composition) and four mapping properties (1-to-1, 1-to-N, N-to-1, N-to-N), demonstrating that powerful expressiveness can sometimes be achieved with straightforward linear algebra. Its practical advantage is scalability to large KGs due to its linear nature, making it a strong contender for efficiency-critical applications. However, its linearity might limit its ability to capture highly non-linear semantic relationships compared to models like CompoundE [ge2022] or those leveraging deep learning (Section 3), which can model more intricate interactions.
    \item \textbf{TransMS} [yang2019]: An extension of translation-based models, TransMS introduces "multidirectional semantics" by translating and transmitting semantics from head/tail entities and relations using non-linear functions and linear bias vectors. This moves beyond simple vector addition by incorporating richer, non-linear interactions, improving performance on N-1 and 1-N relations. Unlike the purely linear transformations in LineaRE [peng2020], TransMS leverages non-linearity to capture more complex relational dynamics while maintaining a relatively low computational footprint compared to matrix-based approaches. However, its non-linear functions are still relatively simple compared to the complex algebraic structures of rotational or hyperbolic models, potentially limiting its capacity for extremely nuanced semantics.
    \item \textbf{TranS} [zhang2022]: A transition-based method that addresses complex scenarios where the same entity pair can have different relations. Its core innovation is replacing the single relation vector in traditional scoring patterns with a "synthetic relation representation," enabling it to handle diverse relations more effectively and efficiently. This contrasts with models that assign a single, fixed embedding to each relation, offering greater flexibility for polysemous relations. While it addresses polysemy, it does so by increasing the representational overhead for relations, which can impact scalability for KGs with many distinct relation instances, especially when compared to more parameter-efficient models like TransD [ji2015] (Section 2.1).
    \item \textbf{lppTransE/R/D} [yoon2016]: An early innovation that introduces "role-specific projections" to preserve the logical properties of relations (e.g., transitivity, symmetricity) within translation-based embeddings. By projecting head and tail entities using distinct operators, these models aim to overcome the limitations of undifferentiated entity roles in earlier TransE variants. This represents an early attempt to inject more semantic nuance into translational models, increasing parameter count compared to base TransE but offering more expressive power for specific logical patterns. This approach, while effective for its time, is less sophisticated than later generalized transformations like HousE [li2022], which offer a broader range of geometric operations through more advanced matrix parameterizations.
\end{itemize}

\begin{table}[htbp]
    \centering
    \caption{Comparative Overview of Diverse Geometric and Algebraic KGE Innovations}
    \label{tab:other_geometric_models}
    \begin{tabularx}{\textwidth}{|l|X|X|X|X|}
        \hline
        \textbf{Model} & \textbf{Embedding Space/Metric} & \textbf{Core Operation/Mechanism} & \textbf{Key Patterns/Problems Addressed} & \textbf{Theoretical/Practical Considerations} \\
        \hline
        \textbf{TorusE} [ebisu2017] & Lie Group (Torus) & Translation on manifold & TransE regularization problem, embedding divergence & Solves regularization elegantly, maintains translation, but may lack expressiveness for complex patterns beyond translation. \\
        \hline
        \textbf{FHRE} [liang2024] & Lorentz Hyperbolic Space & Fully Hyperbolic Rotation & Hierarchical structures, instability of hybrid hyperbolic models & Direct hyperbolic operation, stable, efficient for hierarchies, specific to Lorentz model, "pure rotation" assumption can be restrictive. \\
        \hline
        \textbf{Pan et al. 2021} [pan2021] & Extended Poincar Ball (Hyperbolic) & Polar coordinates, boundary handling & Hierarchical structures, logical patterns, Poincar boundary issues & Novel hyperbolic space design, effective for hierarchies, optimization complexity due to boundary conditions. \\
        \hline
        \textbf{H2E} [H2E_Anonymous] & Hyperbolic Polar Space & Dual-embedding (modulus/phase), attentional context & Inter/intra-level hierarchies, rich relational context & Enhanced hierarchical modeling, complex dual-embedding, attention adds computational cost. \\
        \hline
        \textbf{UniGE} [UniGE_Anonymous] & Hybrid (Euclidean + Hyperbolic) & Embedding alignment (Optimal Transport) & Diverse geometric data (chains, hierarchies), single-space limitations & Unifies geometries, robust error bound, complex fusion strategy, higher computational cost due to alignment. \\
        \hline
        \textbf{HolmE} [zheng2024] & Riemannian (Hyperbolic) & Closed under composition (Mbius addition) & Composition patterns (especially long-tail), unifying TransE/RotatE & Novel theoretical property, robust for composition, but Riemannian geometry adds complexity. \\
        \hline
        \textbf{ManifoldE} [xiao2015] & Manifold (Sphere/Hyperplane) & Manifold-based principle & Precise link prediction, ill-posed algebraic systems, over-strict point-wise models & Flexible geometric representation, improves HITS@1, efficient, but manifold choice impacts performance and complexity. \\
        \hline
        \textbf{SpherE} [li2024] & Spherical Space & Rotational (entity as sphere) & Many-to-many relations, Set Retrieval & Novel entity representation, good for set-based queries, interpretability. \\
        \hline
        \textbf{TransC} [lv2018] & Real Vector Space (Spheres/Vectors) & Differentiating concepts (spheres) and instances (vectors) & `instanceOf`, `subClassOf` relations, ontological distinctions & Innovative entity representation, intuitive for hierarchies, requires explicit type information, limiting generalizability. \\
        \hline
        \textbf{HousE} [li2022] & Real Vector Space & Householder Transformations (Rotations & Projections) & General relation patterns, mapping properties, higher-dimensional rotations & Generalizes rotations, strong modeling capacity, increased complexity. \\
        \hline
        \textbf{GoldE} [li2024] & Real Vector Space & Universal Orthogonal Parameterization & Generalizes Householder, dimensional extension, geometric unification & Unifies orthogonal transformations, high expressiveness, abstract, potentially complex. \\
        \hline
        \textbf{CompoundE} [ge2022] & Real Vector Space & Cascaded Translation, Rotation, Scaling & General relation patterns, diverse mapping properties & Unifies multiple operations, high expressiveness, potentially complex optimization and higher parameter count. \\
        \hline
        \textbf{Rot-Pro} [song2021] & Real Vector Space & Projection + Rotation & Transitivity, common relation patterns & Combines transformations for specific patterns, effective for transitivity, but adds complexity. \\
        \hline
        \textbf{CyclE} [yang2021] & Cycle Metric (vs. Minkowski) & Metric choice (oscillation property) & Expressiveness limitations of Minkowski metric & Fundamental metric innovation, depends on base KGE, period tuning can be complex, not universally superior. \\
        \hline
        \textbf{MQuinE} [liu2024] & Real Vector Space & Refined Scoring Function & Z-paradox, diverse relation patterns & Addresses theoretical deficiency, robust, general expressiveness, improves reliability. \\
        \hline
        \textbf{LineaRE} [peng2020] & Real Vector Space & Linear Regression & Symmetry, inversion, composition, all mapping properties & Simple, scalable, surprisingly effective for broad patterns, but linearity can be limiting for highly non-linear semantics. \\
        \hline
        \textbf{TransMS} [yang2019] & Real Vector Space & Multidirectional semantics (non-linear functions, bias) & Complex relations (N-1, 1-N) beyond simple translation & Refines translation with non-linearity, better scalability than matrix-based, but less expressive than full rotations. \\
        \hline
        \textbf{TranS} [zhang2022] & Real Vector Space & Synthetic Relation Representation & Complex scenarios with same entity pair having different relations & Efficient for diverse relations, addresses single relation vector limitation, but adds representational overhead, impacting scalability. \\
        \hline
        \textbf{lppTransE/R/D} [yoon2016] & Real Vector Space & Role-specific Projections & Preserving logical properties (transitivity, symmetricity) & Early attempt to add semantic nuance to translation, increases parameters compared to base TransE, less sophisticated than later generalizations. \\
        \hline
    \end{tabularx}
\end{table}

This diverse set of models highlights several patterns and tensions in KGE research. There is a continuous effort to move beyond simple vector operations towards more mathematically sophisticated transformations and embedding spaces. For instance, TorusE [ebisu2017], FHRE [liang2024], UniGE [UniGE_Anonymous], [pan2021], H2E [H2E_Anonymous], and HolmE [zheng2024] all challenge the fundamental assumptions about the embedding space, demonstrating that non-Euclidean geometries or hybrid spaces can offer significant advantages for specific data structures like hierarchies, unlike purely Euclidean models that struggle with such representations. This pattern suggests a growing recognition that the intrinsic geometry of KGs is often non-Euclidean, and a single Euclidean space may be an oversimplification. Similarly, CyclE [yang2021] and ManifoldE [xiao2015] re-evaluate the underlying metric and the geometric primitive for entities, respectively, showing that even foundational choices can be refined for improved performance or theoretical soundness. This contrasts with models like TransMS [yang2019] and LineaRE [peng2020], which primarily refine the scoring function within a conventional Euclidean space, showing that significant gains can still be made through clever algebraic manipulations without resorting to exotic geometries.

A critical tension exists between model expressiveness and computational complexity. While models like UniGE, with its hybrid geometry and optimal transport, offer high expressiveness for diverse KG structures, they incur higher computational costs and parameter counts due to the complexity of managing and aligning embeddings across different spaces. This stands in stark contrast to the parameter efficiency of TransD [ji2015] (discussed in Section 2.1) or the linear simplicity of LineaRE [peng2020]. The field implicitly assumes that increased mathematical complexity will yield better results, but this is often at the expense of scalability to extremely large KGs or interpretability. For example, while FHRE offers stability and efficiency for hierarchical data by operating fully in hyperbolic space, its reliance on the Lorentz model might limit its applicability to KGs where other geometric properties are more dominant, or where the "pure rotation" assumption is too restrictive. The evaluation of these models primarily relies on link prediction metrics, which, while standard, may not fully capture the nuances of all the complex patterns they claim to model, especially for tasks like precise link prediction (ManifoldE) or concept-instance differentiation (TransC). The generalizability of these models to extremely large and sparse KGs, where the computational overhead of complex transformations can become prohibitive, remains an ongoing challenge. Furthermore, many papers, while demonstrating empirical improvements, often lack a thorough critique of their own assumptions, such as the universal applicability of a single geometric primitive (e.g., spheres in SpherE) or the optimality of a chosen metric across all relation types (CyclE). This methodological oversight can lead to an overestimation of a model's robustness across diverse KG characteristics.

In summary, this subsection highlights the continuous and multifaceted effort to refine the mathematical foundations of KGE. From exploring non-Euclidean spaces and hybrid geometries to introducing novel metrics and specialized linear/algebraic transformations, researchers are pushing the boundaries of how entities and relations can be represented and interacted with algebraically. This trajectory demonstrates a clear evolution from simpler, often restrictive, geometric assumptions towards more flexible, powerful, and theoretically sound mathematical frameworks, albeit with persistent trade-offs in complexity, scalability, and the need for specialized solutions for specific relational patterns. The progression from basic translational models to sophisticated generalized transformations like CompoundE underscores a drive towards unified frameworks that can encapsulate a wider array of relational semantics.


### Deep Learning Architectures for Knowledge Graph Embedding

\label{sec:deep_learning_architectures_for_knowledge_graph_embedding}

\section{Deep Learning Architectures for Knowledge Graph Embedding}
\label{sec:deep_learning_architectures_for_knowledge_graph_embedding}

While foundational geometric and algebraic models, discussed in the preceding section, laid crucial groundwork for representing entities and relations in continuous vector spaces, the field of Knowledge Graph Embedding (KGE) has witnessed a significant paradigm shift towards leveraging advanced deep learning architectures. This evolution directly addresses inherent limitations of purely geometric approaches, particularly their reliance on predefined transformation rules and their challenges in automatically extracting intricate, non-linear features from complex graph structures [community_0]. Deep learning models offer unparalleled capacity to learn more expressive, context-aware, and hierarchical representations directly from knowledge graph data, pushing the boundaries of KGE performance beyond the constraints of fixed geometric transformations.

This section delves into how Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and Transformer models have been innovatively adapted to capture the rich structural patterns and complex interactions within knowledge graphs. We will explore how CNNs excel at extracting local features and modeling intricate interactions between entity and relation embeddings, providing a powerful mechanism for discovering hidden patterns. Subsequently, we examine GNNs, which, through their message-passing and aggregation mechanisms, effectively capture structural information and neighborhood context, enabling the learning of richer, context-dependent embeddings that leverage the graph's topology. Finally, we discuss the emergence of Transformer architectures, which utilize self-attention to capture long-range dependencies and contextualized semantics, pushing the state-of-the-art in KGE by moving beyond localized triplet interactions [community_1, community_2]. By enabling automatic feature learning and modeling of highly non-linear relationships, these deep learning paradigms have significantly advanced the state-of-the-art in KGE, paving the way for more robust, scalable, and powerful knowledge reasoning systems.

\subsection{Convolutional Neural Networks (CNNs) for KGE}
\label{sec:3_1_convolutional_neural_networks_(cnns)_for_kge}

Convolutional Neural Networks (CNNs) have emerged as a powerful paradigm in Knowledge Graph Embedding (KGE), offering a distinct advantage over purely geometric models by automatically extracting intricate local features and modeling complex, non-linear interactions between entity and relation embeddings. This approach marks a significant shift from predefined mathematical transformations, enabling models to discover patterns directly from data. As discussed in previous subsections, while translational and rotational models [ji2015, ebisu2017, gao2020, li2022, ge2022, ge2023, liu2024, chen2025] provide elegant mathematical frameworks for specific relation types, they often struggle with the sheer diversity and complexity of real-world relational patterns, particularly N-to-1, 1-to-N, and N-to-N mappings. CNNs address this by treating concatenated entity and relation embeddings as a 2D input, over which convolutional filters can slide to learn hierarchical feature representations.

A core innovation of CNN-based KGE models lies in their ability to capture rich feature interactions through shared weights and local receptive fields. Unlike simple scoring functions that rely on dot products or Euclidean distances, CNNs can learn multiple interaction patterns simultaneously. For instance, the pioneering work of ConvE (not directly cited but foundational to many CNN KGEs) demonstrated how reshaping the concatenated head entity and relation embeddings into a 2D matrix and applying convolutional filters could significantly enhance expressiveness. This mechanism allows the model to identify specific patterns that indicate the plausibility of a triplet $(h, r, t)$, moving beyond global transformations to nuanced local feature detection.

One notable approach, \textbf{AcrE} (Knowledge Graph Embedding with Atrous Convolution and Residual Learning) [ren2020], aims to address the computational complexity and potential information loss in deep neural network-based KGE models. AcrE's core innovation lies in its use of *atrous convolutions* and *residual learning*. Atrous convolutions enable the model to effectively enlarge its receptive field without increasing the number of parameters or losing resolution, allowing it to capture broader contextual information around the entity-relation pair. This is crucial for understanding relations that might depend on slightly more distant features within the concatenated embedding. Furthermore, the integration of residual learning helps in mitigating the vanishing/exploding gradient problem and ensures that original information is not forgotten as features are processed through deeper layers. AcrE claims to be simpler and more parameter-efficient than existing state-of-the-art methods [ren2020], achieving superior results on diverse datasets. However, while atrous convolutions expand the receptive field, they still operate locally on the input, which might theoretically limit their ability to capture extremely long-range, non-contiguous dependencies compared to self-attention mechanisms found in Transformers [wang2019, li2023, shi2025]. Practically, while parameter-efficient, the convolutional operations themselves can still be computationally intensive depending on the filter configurations and input dimensions.

Building on the idea of learning richer feature embeddings, the \textbf{Multi-Scale Dynamic Convolutional Network (M-DCN)} [zhang2020] was proposed to specifically tackle the challenge of handling complex relation types (1-to-N, N-to-1, N-to-N) that simpler models often struggle with. M-DCN's core innovation is its use of *multi-scale dynamic filters*. Instead of fixed filters, M-DCN generates filter weights that are dynamically related to each specific relation. This allows the model to adapt its feature extraction process based on the unique characteristics of the relation in question, making it highly flexible and expressive for diverse relational patterns. Additionally, M-DCN composes subject entity and relation embeddings in an alternating pattern in the input layer to extract more feature interactions. This dynamic and multi-scale approach enables M-DCN to achieve state-of-the-art link prediction results on several benchmark datasets [zhang2020]. The theoretical limitation of M-DCN, despite its power, lies in the increased complexity and potential opacity introduced by dynamic filter generation; understanding the precise rationale behind a filter's adaptation for a given relation can be challenging. Practically, the dynamic nature and multi-scale operations likely incur higher computational costs and parameter counts compared to more straightforward CNN architectures.

Further advancing the capabilities of CNNs, \textbf{ReInceptionE} (Relation-Aware Inception Network with Joint Local-Global Structural Information) [xie2020] addresses the limitation of earlier CNN models, such as ConvE, which often lack explicit structural information in their embedding space. ReInceptionE's core innovation is a two-pronged approach: first, it leverages the *Inception network* architecture (popular in computer vision for its ability to capture features at multiple scales) to learn richer query embeddings, thereby increasing the number and diversity of interactions between head and relation embeddings. Second, and crucially, it integrates a *relation-aware attention mechanism* to enrich these query embeddings with *joint local neighborhood and global entity information*. This attention mechanism allows ReInceptionE to dynamically weigh the importance of neighboring entities and relations, effectively incorporating broader graph context into the CNN's local feature extraction process. This directly addresses the critique that CNNs, by their nature, are primarily local feature extractors. ReInceptionE demonstrates competitive performance on datasets like WN18RR and FB15k-237, which are known for their complex relational structures [xie2020]. However, the theoretical limitation arises from the heuristic combination of Inception modules and attention; while effective, the precise interaction and contribution of each component can be difficult to disentangle, potentially leading to a more complex "black-box" model. From a practical standpoint, the architectural depth and complexity of Inception networks combined with attention mechanisms generally lead to increased computational demands and a larger number of parameters compared to simpler KGE models.

More recent works continue to refine CNN-based KGE. \textbf{CNN-ECFA} (Convolutional Neural Network-Based Entity-Specific Common Feature Aggregation) [hu2024] focuses on an underexplored area: leveraging entity-specific common feature aggregation to enhance knowledge graph representation learning. Its innovation is a novel CNN-based strategy that aggregates common features specific to entities, which has proven valuable in other deep learning tasks like text classification. This approach aims to provide more discriminative and robust entity representations by focusing on shared, yet entity-specific, characteristics. CNN-ECFA outperforms state-of-the-art feature projection strategies on datasets like WN18RR, YAGO3-10, and NELL-995, demonstrating average improvements in MRR and Hits@1 [hu2024]. The theoretical underpinning of "common features" requires careful definition in the KGE context, and its effectiveness might depend on the inherent commonalities present in the dataset. Practically, the aggregation strategy, while beneficial, could introduce additional computational overhead during training.

Similarly, \textbf{SEConv} (A Semantic Enhanced Knowledge Graph Embedding Model With AIGC Designed for Healthcare Prediction) [yang2025] addresses the neglect of complex structural features in traditional KGE models, particularly in the context of healthcare prediction. SEConv's core innovation combines a *less resource-consuming self-attention mechanism* with a *multilayer convolutional neural network*. The self-attention component generates more expressive embedding representations, while the multi-layer CNN is adopted to learn deeper structural features from triplets. This dual approach aims for both efficiency (crucial for resource-limited consumer electronics in healthcare) and enhanced expressiveness. SEConv demonstrates substantial improvements on medical datasets like UMLS and DBpedia50, validating its utility for healthcare prediction tasks [yang2025]. A theoretical limitation is ensuring the synergistic rather than redundant operation of self-attention and multi-layer CNNs. Practically, while claiming "less resource-consuming," deep neural networks inherently demand significant computational resources, and application-specific models like SEConv require rigorous validation for generalizability beyond their target domain.

\begin{table}[htbp]
    \centering
    \caption{Comparative Framework of CNN-based KGE Models}
    \label{tab:cnn_kge_comparison}
    \begin{tabularx}{\textwidth}{|l|X|X|X|X|}
        \hline
        \textbf{Model} & \textbf{Core Innovation} & \textbf{Problem Solved} & \textbf{Key Advantages} & \textbf{Limitations} \\
        \hline
        \textbf{AcrE [ren2020]} & Atrous convolutions + Residual learning & Complexity, training time, feature interaction & Increased receptive field, parameter efficiency, mitigates gradient issues & Still local, potential for long-range dependency struggle \\
        \hline
        \textbf{M-DCN [zhang2020]} & Multi-scale dynamic filters, alternating composition & Complex relations (1-to-N, N-to-N), limited expressiveness & Relation-adaptive feature extraction, rich interactions & Increased complexity, potential opacity of dynamic filters, higher computational cost \\
        \hline
        \textbf{ReInceptionE [xie2020]} & Inception network + Relation-aware attention & Lack of structural info, limited interactions in ConvE-like models & Joint local-global structural info, multi-scale feature learning & High complexity (Inception + Attention), increased parameters/computation, interpretability challenges \\
        \hline
        \textbf{CNN-ECFA [hu2024]} & Entity-specific common feature aggregation & Underexplored entity-specific feature learning & Enhanced entity representations, improved link prediction & Definition of "common features" can be vague, potential computational overhead \\
        \hline
        \textbf{SEConv [yang2025]} & Multi-layer CNN + Resource-efficient self-attention & Neglect of complex structural features, healthcare prediction & Deeper structural features, expressive embeddings, efficiency for specific applications & "Less resource-consuming" is relative, application-specific generalizability \\
        \hline
    \end{tabularx}
\end{table}

The evolution of CNN-based KGE models demonstrates a clear intellectual trajectory. Early CNN applications in KGE primarily focused on leveraging their local feature extraction capabilities to move beyond the fixed operations of geometric models. AcrE [ren2020] then introduced atrous convolutions to expand the receptive field while maintaining efficiency, marking a step towards capturing broader context. M-DCN [zhang2020] pushed this further by introducing *dynamic filters*, allowing the CNN to adapt its feature learning to specific relation types, thereby addressing the challenge of diverse relation patterns. ReInceptionE [xie2020] represents a significant convergence, integrating attention mechanisms with CNNs to explicitly incorporate *global structural information* alongside local features, directly addressing the limitation of CNNs being purely local. This exemplifies the field's shift from purely local pattern recognition to a more holistic understanding of graph structure. More recent works like CNN-ECFA [hu2024] and SEConv [yang2025] refine these techniques, focusing on specific feature aggregation strategies or application-driven efficiency, indicating a maturation and specialization of CNN applications in KGE.

A critical tension across these models is the trade-off between model expressiveness and computational cost. While CNNs are powerful in learning intricate, non-linear feature interactions, the introduction of multi-scale filters, dynamic mechanisms, and attention layers (as seen in M-DCN [zhang2020] and ReInceptionE [xie2020]) inevitably increases parameter counts and computational demands. This can pose scalability challenges for extremely large knowledge graphs, a concern that is partially addressed by models like AcrE [ren2020] focusing on parameter efficiency or SEConv [yang2025] aiming for "less resource-consuming" designs. Furthermore, most CNN-based KGE models implicitly assume that concatenating entity and relation embeddings and applying convolutions is the optimal way to represent their interaction. However, the precise spatial arrangement of these input embeddings (e.g., $(h, r)$ vs. $(r, h)$) and the optimal way to represent their interaction for convolutional filters are often unstated assumptions that can significantly impact performance. While these models achieve state-of-the-art results in link prediction, the interpretability of *why* a specific convolutional filter learns a particular pattern for a relation remains a challenge, often making them more "black-box" compared to the mathematically transparent geometric models discussed in Section 2. This lack of interpretability can be a practical limitation, especially in high-stakes applications like healthcare, as highlighted by SEConv [yang2025].

In summary, CNNs have profoundly impacted KGE by enabling the automatic discovery of complex, non-linear patterns that are difficult to model with fixed geometric transformations. From early efforts to extract local features to advanced architectures incorporating dynamic filters and attention mechanisms, CNNs have consistently pushed the boundaries of link prediction performance. However, the field continues to balance the pursuit of higher expressiveness with the practical constraints of computational efficiency, scalability, and model interpretability, paving the way for further innovations in this promising area.
\subsection{Graph Neural Networks (GNNs) and Attention Mechanisms}
\label{sec:3_2_graph_neural_networks_(gnns)__and__attention_mechanisms}

Building upon the local feature extraction capabilities of Convolutional Neural Networks (CNNs) discussed in the previous subsection [ren2020, xie2020], Graph Neural Networks (GNNs) offer a more direct and inherently graph-aware paradigm for Knowledge Graph Embedding (KGE). While CNNs operate on flattened or reshaped embeddings to capture local interactions, GNNs are specifically designed to explicitly leverage the topological structure of knowledge graphs through iterative \textit{message passing} and \textit{aggregation} mechanisms [hamilton2017]. This allows them to effectively capture structural information, neighborhood context, and relational paths, which are crucial for understanding complex relational patterns that extend beyond simple triplet interactions. The integration of attention mechanisms further refines this process by allowing models to dynamically weigh the importance of different neighbors or relational paths, leading to richer, context-dependent embeddings.

One of the foundational GNN architectures adapted for KGE is \textbf{Relational Graph Convolutional Networks (R-GCN)} [schlichtkrull2018]. Unlike generic GCNs that typically operate on homogeneous graphs, R-GCN specifically addresses the multi-relational nature of knowledge graphs. Its core innovation lies in using \textit{relation-specific weight matrices} for message passing, allowing it to learn distinct transformations for each relation type. For a given entity, R-GCN aggregates messages from its neighbors, where each message is transformed by a weight matrix corresponding to the relation connecting the neighbor to the central entity. This mechanism enables R-GCN to capture rich structural context and relation-specific patterns, directly addressing the challenge of modeling diverse relation types that simpler geometric models often struggle with. R-GCN demonstrated significant improvements in link prediction and entity classification tasks on benchmark datasets like FB15k-237 and WN18RR [schlichtkrull2018]. However, a significant theoretical limitation of R-GCN is its high parameter count, which grows quadratically with the embedding dimension and linearly with the number of relation types. This can lead to scalability issues for KGs with many unique relations, as the model becomes prone to overfitting and computationally expensive. Furthermore, like many GNNs, R-GCN can suffer from the \textit{over-smoothing problem} in deeper layers, where repeated aggregation can cause entity embeddings to become indistinguishable, limiting its capacity to capture long-range dependencies effectively [li2018].

Building on R-GCN's success while addressing its parameter efficiency, \textbf{Compositional Graph Neural Networks (CompGCN)} [vashishth2020] introduced a more elegant way to handle relation-specific transformations. CompGCN's core innovation is to integrate \textit{compositional operators} (such as DistMult, ComplEx, or RotatE) directly into the GNN message-passing framework. Instead of learning a separate weight matrix for each relation as in R-GCN, CompGCN represents relations as operations that transform entity embeddings during message aggregation. This allows it to model diverse relational patterns (e.g., symmetry, antisymmetry, inversion, composition) with significantly fewer parameters, as the relation embeddings themselves participate in the compositional operation. CompGCN effectively solves the parameter explosion problem of R-GCN while simultaneously enhancing the model's expressiveness by leveraging the strengths of established KGE scoring functions. It achieves state-of-the-art results on various link prediction benchmarks [vashishth2020]. However, similar to R-GCN, CompGCN is still susceptible to the over-smoothing problem, especially in very deep architectures, as the aggregation process inherently blends information from neighbors. Practically, the choice of compositional operator can impact its expressiveness and the interpretability of the learned interactions, making it a more complex "black-box" model compared to simpler geometric approaches.

A critical challenge for GNNs, and KGE models in general, is \textit{inductive learning}the ability to embed new, unseen entities or relations without retraining the entire model. While R-GCN and CompGCN are primarily transductive, \textbf{Logic Attention Based Neighborhood Aggregation} [wang2018] offered an early foray into inductive capabilities. Its core innovation is the \textit{Logic Attention Network (LAN)}, designed to aggregate information from an entity's neighbors using both rules-based and network-based attention weights. This mechanism allows it to generate embeddings for new entities based on their existing neighborhood structure, a significant advantage over transductive models that require all entities to be present during training. Unlike R-GCN or CompGCN which focus on learning fixed transformations for known relations, LAN dynamically weighs the importance of neighbors, explicitly addressing the unordered and unequal natures of an entity's neighbors. However, its theoretical limitation lies in its reliance on existing neighbors; truly isolated new entities would still pose a challenge, as the model has no information to aggregate. Practically, the complexity of learning and applying dual attention weights can add computational overhead, especially for dense neighborhoods.

Further refining the use of attention for structural information, \textbf{Knowledge Graph Embedding via Graph Attenuated Attention Networks (GAATs)} [wang2020] aims to overcome the limitation of existing graph convolutional network-based models that often assign uniform weights to relation paths or incompletely mine triple features. GAATs' core innovation is an \textit{attenuated attention mechanism} that assigns different weights to different relation paths and effectively acquires information from diverse neighborhoods. This allows entities and relations to be learned from any neighbors, capturing more nuanced contextual information. In contrast to the more general relation-specific transformations of R-GCN or the compositional operations of CompGCN, GAATs specifically focus on dynamically emphasizing more relevant parts of the neighborhood through attention, leading to significant improvements on benchmarks like WN18RR and FB15k-237 [wang2020]. A theoretical limitation, shared with other GNNs, is the potential for over-smoothing in deeper layers, where repeated aggregation can make entity embeddings indistinguishable. Practically, the attention mechanism adds computational complexity, and the "attenuated" aspect requires careful design to avoid simply ignoring less prominent but potentially relevant paths, which could lead to information loss.

Pushing the boundaries of representation learning, \textbf{DisenKGAT} (Knowledge Graph Embedding with Disentangled Graph Attention Network) [wu2021] tackles the challenge of accurately capturing complex relations that are often oversimplified by single, static entity and relation representations. DisenKGAT's core innovation lies in its use of a \textit{disentangled graph attention network}, leveraging both \textit{micro-disentanglement} and \textit{macro-disentanglement}. Micro-disentanglement is achieved through a novel relation-aware aggregation mechanism that learns diverse component representations for entities and relations. Macro-disentanglement, on the other hand, employs mutual information as a regularization term to enhance the independence of these learned components. This allows DisenKGAT to generate adaptive and diverse representations based on the given scenario, thereby improving its ability to model complex, polysemous relations. Unlike R-GCN and CompGCN which learn a single, albeit complex, representation for each entity/relation, DisenKGAT explicitly aims to break down these representations into independent semantic components. DisenKGAT claims to offer superior accuracy and explainability, outperforming existing methods on public benchmark datasets [wu2021]. The theoretical limitation of disentanglement is ensuring that the learned components are truly independent and semantically meaningful, rather than just arbitrary splits. This often requires careful design of the disentanglement objective and can be sensitive to hyperparameters. Practically, the disentanglement process and mutual information regularization add significant complexity and hyperparameter tuning challenges, potentially increasing training time and requiring more sophisticated optimization compared to GAATs.

More recent advancements continue to refine GNN-based KGE, particularly in addressing scalability and inductive capabilities for dynamic KGs. \textbf{InGram} (INductive knowledge GRAph eMbedding) [chen2021] directly addresses the limitation of many inductive KGE methods, including Logic Attention [wang2018], which often assume new entities but not new relations. InGram's core innovation is its ability to generate embeddings for *both new relations and new entities* at inference time. It achieves this by defining a "relation graph" that captures affinities between relations and then uses an attention mechanism to aggregate neighboring embeddings based on both the original knowledge graph and this relation graph. This provides a more comprehensive inductive capability, crucial for real-world KGs where new relations frequently emerge. While Logic Attention provides inductive entity embeddings, InGram extends this to relations, offering a more complete solution for dynamic KGs. However, the complexity of constructing and leveraging the relation graph, especially for very large and sparse relation sets, can introduce practical challenges and computational overhead.

For large-scale KGs, computational efficiency remains a paramount concern. \textbf{CPa-WAC} (Constellation Partitioning-based Scalable Weighted Aggregation Composition) [modak2024] proposes a lightweight GNN architecture that combines graph convolutional networks with a novel partitioning strategy. Its innovation lies in a three-stage approach: (1) \textit{Constellation Partitioning (CPa)} uses modularity maximization (e.g., Louvain clustering) to partition the KG into topological clusters, minimizing cross-partition edges without relying on node/edge attributes. (2) \textit{Weighted Aggregation Composition (WAC) Convolution} is an improved compositional GCN with attention layers for local embedding learning within each partition. (3) A \textit{Global Decoder (GD)} framework then effectively merges these cluster-specific embeddings for global inference. This approach directly tackles the high computational and memory costs of GNNs like R-GCN and CompGCN for large KGs. CPa-WAC demonstrates a significant reduction in training time (up to five times faster) while maintaining comparable prediction accuracy to training on the whole graph [modak2024]. This highlights a critical trade-off: achieving scalability through partitioning without sacrificing the global structural information that GNNs are designed to capture. The theoretical challenge lies in ensuring that the partitioning process does not destroy crucial long-range dependencies, and the Global Decoder effectively reconstructs the global context. Practically, the complexity of the partitioning and merging framework adds architectural overhead.

\begin{table}[htbp]
    \centering
    \caption{Comparative Framework of GNN-based KGE Models with Attention}
    \label{tab:gnn_attention_kge_comparison}
    \begin{tabularx}{\textwidth}{|l|X|X|X|X|}
        \hline
        \textbf{Model} & \textbf{Core Innovation} & \textbf{Problem Solved} & \textbf{Key Advantages} & \textbf{Limitations} \\
        \hline
        \textbf{R-GCN [schlichtkrull2018]} & Relation-specific weight matrices for message passing & Multi-relational graphs, structural context & Explicit relation modeling, foundational GNN for KGE & High parameter count, prone to over-smoothing \\
        \hline
        \textbf{CompGCN [vashishth2020]} & Compositional operators in GNN message passing & Parameter explosion (R-GCN), complex relation patterns & Parameter efficiency, models diverse relation patterns & Over-smoothing, interpretability of compositional ops \\
        \hline
        \textbf{Logic Attention [wang2018]} & Logic Attention Network (LAN) for neighborhood aggregation & Inductive KGE for unseen entities & Inductive capability for entities, explicit attention & Relies on existing neighbors, dual attention complexity \\
        \hline
        \textbf{GAATs [wang2020]} & Graph Attenuated Attention Mechanism & Incomplete feature mining, uniform path weighting & Context-sensitive path weighting, nuanced neighborhood info & Computational cost, potential over-smoothing, careful attenuation design \\
        \hline
        \textbf{DisenKGAT [wu2021]} & Disentangled Graph Attention Network (micro/macro disentanglement) & Static/single representations, complex relations & Diverse, adaptive, independent component representations & High complexity, hyperparameter sensitivity, training time \\
        \hline
        \textbf{InGram [chen2021]} & Inductive embedding for new entities *and* relations & Limited inductive scope (entities only) & Full inductive capability (entities + relations), robust for dynamic KGs & Relation graph complexity, potential sparsity issues \\
        \hline
        \textbf{CPa-WAC [modak2024]} & Constellation Partitioning + Weighted Aggregation Comp. + Global Decoder & Scalability of GNNs for large KGs, long training times & Scalability (up to 5x faster), maintains accuracy, topology-aware partitioning & Architectural complexity, potential info loss across partitions \\
        \hline
    \end{tabularx}
\end{table}

The evolution of GNNs in KGE demonstrates a clear intellectual trajectory from basic neighborhood aggregation to highly sophisticated, context-aware, and disentangled representations, while increasingly addressing scalability and inductive capabilities. Early foundational work like R-GCN [schlichtkrull2018] and CompGCN [vashishth2020] established the power of GNNs for capturing structural context in multi-relational graphs, moving beyond the fixed operations of geometric models (Section 2) and the local-only views of early CNNs (Subsection 3.1). These models primarily operate in a transductive setting. The field then progressed to tackle the critical challenge of inductive learning, with Logic Attention [wang2018] demonstrating early success for new entities, and InGram [chen2021] extending this to new relations, a more comprehensive solution for dynamic KGs. Simultaneously, the refinement of attention mechanisms, from the explicit weighting in Logic Attention to the nuanced "attenuated" approach of GAATs [wang2020], and the disentangled representations of DisenKGAT [wu2021], exemplifies a shift towards understanding the multifaceted semantics embedded within graph structures rather than just connectivity. This pattern of increasing expressiveness, however, often comes at the cost of computational complexity, leading to innovations like CPa-WAC [modak2024] which explicitly addresses the scalability of GNNs for large KGs through partitioning.

A critical tension across these GNN-based models is the trade-off between increased expressiveness and computational complexity. While attention mechanisms and disentanglement (as seen in GAATs [wang2020] and DisenKGAT [wu2021]) significantly enhance the models' ability to capture intricate patterns, they invariably lead to higher parameter counts and longer training times compared to simpler models. For instance, DisenKGAT's mutual information regularization adds significant optimization challenges, potentially increasing training time by a substantial factor compared to a straightforward R-GCN. This can pose practical scalability challenges for extremely large knowledge graphs, a concern that models in Section 6.1 aim to address, and which CPa-WAC [modak2024] directly confronts by reducing training time by up to five times through partitioning. Furthermore, GNNs, by their iterative message-passing nature, implicitly assume that local neighborhood information can be effectively propagated and aggregated to capture global graph structures. However, this assumption can break down in very deep GNNs due to the \textit{over-smoothing problem} [li2018], where entity embeddings become indistinguishable after many layers, limiting their capacity to capture long-range dependencies effectively. While attention mechanisms mitigate this by selectively focusing on relevant neighbors, they do not entirely eliminate the underlying issue. The interpretability of these complex attention mechanisms and disentangled components also remains a challenge; while DisenKGAT [wu2021] claims explainability, the precise rationale behind the learned components can still be opaque to human understanding, a common limitation for deep learning models. The systematic search for optimal GNN message functions, as explored in [di2023], further highlights the complexity and non-triviality of designing effective GNN architectures, often requiring automated machine learning techniques like those for hyperparameter tuning [xu2022].

In conclusion, GNNs, particularly when augmented with attention mechanisms, have profoundly advanced KGE by directly leveraging the graph's topology and relational paths. They have moved beyond simple triplet-based interactions to learn richer, context-dependent, and even disentangled representations. This paradigm shift has enabled significant improvements in tasks like link prediction and, crucially, in handling inductive settings for both entities and relations. However, the ongoing challenge lies in balancing the increasing architectural complexity and computational demands with the need for scalability, robustness, and transparent interpretability in real-world applications.
\subsection{Transformer-based KGE Models}
\label{sec:3_3_transformer-based_kge_models}

Following the advancements in Graph Neural Networks (GNNs) and attention mechanisms, which excel at aggregating local neighborhood information [wu2021, wang2020], a new paradigm has emerged in Knowledge Graph Embedding (KGE) research: the adaptation of Transformer architectures. Originally designed for sequence modeling in natural language processing [vaswani2017attention], Transformers, with their powerful self-attention mechanisms, offer a distinct advantage by inherently capturing long-range dependencies and global contextual information, which can be challenging for traditional GNNs that rely on iterative local message passing [li2018]. This subsection explores how these architectures are innovatively applied to knowledge graphs, treating them either as sequences or by integrating graph structures directly into the Transformer framework, thereby enhancing expressiveness for complex contextual and multi-structural features.

The initial adaptation of Transformers to KGE, exemplified by \textbf{CoKE: Contextualized Knowledge Graph Embedding} [wang2019], marked a significant shift from static entity and relation embeddings. CoKE's core innovation lies in recognizing the intrinsic contextual nature of entities and relations within a knowledge graph. It claims to solve the problem of traditional KGE methods that assign a single, static embedding to each entity or relation, thereby ignoring their dynamic properties across different graph contexts. CoKE formulates both edges and paths within the KG as sequences of entities and relations. A Transformer encoder then processes these sequences to obtain dynamic, flexible, and fully contextualized representations. This mechanism allows CoKE to capture contextual meanings that are adaptive to the input sequence, offering an absolute improvement of 19.7\% in Hits@10 on path query answering compared to previous state-of-the-art models [wang2019].

CoKE succeeds under conditions where contextual information, particularly along paths, is crucial for disambiguation and accurate prediction. Its theoretical limitation, however, stems from the inherent challenge of linearizing a graph structure into sequences. While effective for paths, this approach might implicitly lose some explicit graph-level structural information, such as cycles or broader neighborhood connectivity, that GNNs are specifically designed to capture. Practically, processing very long paths can lead to increased computational cost and memory usage, a common challenge for Transformer models. Compared to GNNs like DisenKGAT [wu2021] which focus on disentangling local graph features, CoKE aims for broader contextualization by viewing KGs as sequences, offering a different perspective on capturing rich semantics.

A critical challenge in applying vanilla Transformers to KGs, particularly to triplets, is their inherent \textit{order invariance}. The self-attention mechanism treats input tokens as an unordered set, making it unable to distinguish between a valid triplet (head, relation, tail) and its shuffled, semantically incorrect variants (e.g., tail, relation, head). \textbf{Knowformer: Position-Aware Relational Transformer for Knowledge Graph Embedding} [li2023] directly addresses this fundamental limitation. Its core innovation is a novel Transformer architecture that incorporates \textit{relational compositions} into entity representations, explicitly injecting semantics and capturing the role of an entity based on its position (subject or object) within a relation triple. This is achieved by defining relational compositions as operators on the relation and the object (or subject) and integrating them into the self-attention mechanism via a carefully designed residual block. Knowformer claims to solve the training inconsistency problem of order-invariant Transformers and formally proves its ability to distinguish entity roles and capture correct relational semantics. It achieves state-of-the-art performance on both link prediction and entity alignment across six benchmark datasets [li2023].

Knowformer succeeds under conditions where precise understanding of entity roles within a triplet is paramount. Its theoretical limitation is that while it effectively addresses the order invariance for triplets, it primarily operates at the triplet level, potentially not fully capturing the global graph structure or multi-hop dependencies as comprehensively as a dedicated graph-aware Transformer might. Practically, the design of relational compositions and their integration into self-attention adds complexity to the model architecture and training. Compared to CoKE [wang2019], which implicitly handles order by framing paths as sequences, Knowformer directly confronts and resolves the order-invariance issue for individual triplets, making it more robust for fundamental KGE tasks.

Pushing the boundaries further, \textbf{TGformer: A Graph Transformer Framework for Knowledge Graph Embedding} [shi2025] represents a significant step towards a more comprehensive integration of Transformer capabilities with graph structures. TGformer's core innovation is being the \textit{first} framework to use a graph transformer to build knowledge embeddings that incorporate both \textit{triplet-level} and \textit{graph-level structural features} in both static and temporal knowledge graphs. It claims to solve the limitations of existing methods: triplet-based approaches that ignore the essential graph structure, and graph-based methods (like many GNNs discussed in the previous subsection) that overlook the contextual information of nodes. TGformer constructs a context-level subgraph for each predicted triplet and then employs a Knowledge Graph Transformer Network (KGTN) to fully explore multi-structural features. This allows the model to understand entities and relations in different contexts, boosting performance in link prediction. Experimental results on several public datasets show that TGformer achieves state-of-the-art performance [shi2025].

TGformer succeeds by leveraging the global receptive field of Transformers to integrate diverse structural information, making it particularly effective for KGs where both local triplet patterns and broader graph context are important. Its theoretical limitation lies in the increased complexity of integrating multiple levels of structural information and handling temporal dynamics, which can lead to higher computational demands and parameter counts. Practically, the construction of context-level subgraphs and the design of the KGTN require careful engineering and can be resource-intensive, especially for very large and dense KGs. TGformer represents a convergence, building on the contextualization ideas of CoKE [wang2019] and the structural awareness of GNNs, while extending to temporal KGs, a domain often addressed by specialized models (see Section 5.1).

\begin{table}[htbp]
    \centering
    \caption{Comparative Framework of Transformer-based KGE Models}
    \label{tab:transformer_kge_comparison}
    \begin{tabularx}{\textwidth}{|l|X|X|X|X|X|}
        \hline
        \textbf{Model} & \textbf{KG Representation} & \textbf{Core Innovation} & \textbf{Problem Addressed} & \textbf{Key Advantages} & \textbf{Limitations} \\
        \hline
        \textbf{CoKE [wang2019]} & Edges/Paths as sequences & Transformer encoder on linearized sequences & Static embeddings, ignoring context & Captures dynamic, contextualized meanings along paths & Potential loss of explicit graph structure, computational for long paths \\
        \hline
        \textbf{Knowformer [li2023]} & Triplet-level (h,r,t) & Position-Aware Relational Transformer with relational compositions & Transformer's order invariance for triplets & Distinguishes entity roles, robust for triplet semantics & Primarily triplet-focused, complex relational composition design \\
        \hline
        \textbf{TGformer [shi2025]} & Triplet-level + Graph-level (static & Graph Transformer framework for multi-structural features & Triplet-based ignore graph, graph-based ignore node context & Integrates global/local, static/temporal features, state-of-the-art & High complexity, computational cost, resource-intensive \\
        \hline
    \end{tabularx}
\end{table}

The intellectual trajectory of Transformer-based KGE models demonstrates a clear evolution from adapting sequence models to explicitly integrating graph structures. Early work like CoKE [wang2019] assumed that KGs could be effectively represented as sequences (paths or edges), leveraging the Transformer's strength in contextualization. This approach, while innovative, implicitly makes the assumption that the linear ordering of elements in a path is sufficient to capture all relevant graph semantics, which may not hold for complex, non-linear graph structures. This tension between sequence-centric and graph-centric views is a recurring theme. Knowformer [li2023] then addressed a fundamental limitation of applying vanilla Transformers directly to triplets, the order invariance, by introducing position-aware mechanisms. This marked a shift towards explicitly making Transformers "graph-aware" at a granular level. Finally, TGformer [shi2025] represents a more comprehensive synthesis, moving beyond mere sequence adaptation or triplet-level fixes to a full graph transformer framework that explicitly models both local (triplet-level) and global (graph-level) structural features, and even extends to temporal dynamics. This progression exemplifies the field's shift from simply applying powerful neural architectures to carefully adapting and innovating them to respect the unique structural properties of knowledge graphs.

A critical pattern across these models is the trade-off between the simplicity of adaptation and the richness of graph structure capture. CoKE's sequence-based approach is relatively straightforward but might sacrifice explicit graph topology. Knowformer's focus on triplet-level order invariance is crucial but remains largely localized. TGformer, while offering the most comprehensive integration, introduces significant architectural and computational complexity. This reflects a broader tension in KGE research: how to leverage the immense power of general-purpose deep learning models while respecting the specific, often irregular, structure of knowledge graphs. Furthermore, while these Transformer-based models achieve state-of-the-art performance, their increased complexity often comes at the cost of interpretability, making it harder to understand *why* a particular prediction is made, a limitation shared with many deep learning approaches. This issue is particularly relevant for high-stakes applications discussed in Section 7.4. The reliance on large datasets for pre-training or fine-tuning is another practical limitation, as Transformers are notoriously data-hungry, which can be a challenge for smaller or domain-specific KGs.

In conclusion, Transformer-based KGE models have revolutionized the field by introducing powerful self-attention mechanisms to capture long-range dependencies and highly contextualized representations. From CoKE's sequence-based contextualization [wang2019] to Knowformer's position-aware relational modeling [li2023] and TGformer's comprehensive graph transformer framework [shi2025], these models have significantly pushed the state-of-the-art in KGE performance. They demonstrate an innovative application of architectures originally designed for sequence modeling to graph structures, effectively capturing global and local semantic relationships. The ongoing challenge lies in balancing their immense expressive power with computational efficiency, scalability, and interpretability, particularly as KGs continue to grow in size and complexity.


### Enriching KGE: Auxiliary Information, Rules, and Multi-modality

\label{sec:enriching_kge:_auxiliary_information,_rules,__and__multi-modality}

\section{Enriching KGE: Auxiliary Information, Rules, and Multi-modality}
\label{sec:enriching_kge_auxiliary_information_rules_and_multi-modality}

While the preceding section demonstrated the profound impact of deep learning architectures, such as CNNs, GNNs, and Transformers, in extracting intricate structural patterns and learning context-aware representations directly from knowledge graph topology, real-world knowledge graphs often present challenges that purely structural models cannot fully address. These include inherent data sparsity, the need for deeper semantic understanding beyond connectivity, and the demand for explicit logical reasoning capabilities. To overcome these limitations and move towards a more comprehensive and nuanced representation of knowledge, this section explores advanced Knowledge Graph Embedding (KGE) approaches that integrate diverse external knowledge sources and logical constraints, thereby enriching the learned embeddings.

This section delves into three primary avenues for KGE enrichment. Firstly, we examine methods that incorporate auxiliary information, such as entity types, attributes, and hierarchical structures, to provide richer semantic context and guide the embedding process, making models more robust to incompleteness [community_0, community_1]. Secondly, we discuss the integration of explicit logical rules and constraints, which inject prior knowledge into the learning process, enhancing reasoning capabilities and improving the interpretability and consistency of the learned embeddings [community_3]. Finally, we explore multi-modal and cross-domain KGE techniques that leverage complementary information from various sources, including textual descriptions and visual features, to alleviate data sparsity and enable a more holistic understanding of entities and relations [community_3, a6a735f8e218f772e5b9dac411fa4abea87fdb9c]. By moving beyond solely structural information, these enriched KGE models aim to achieve superior performance, enhance semantic understanding, improve reasoning, and foster greater robustness and interpretability in complex, real-world scenarios.

\subsection{Incorporating Auxiliary Information (Types, Attributes)}
\label{sec:4_1_incorporating_auxiliary_information_(types,_attributes)}

The effectiveness of Knowledge Graph Embedding (KGE) models, as discussed in previous sections, largely stems from their ability to capture structural patterns within the graph. However, relying solely on the triplet structure (head, relation, tail) can oversimplify the rich semantics inherent in real-world knowledge graphs, leading to limitations in expressiveness, robustness, and interpretability, especially when dealing with incomplete or noisy data. This subsection delves into advanced KGE approaches that enhance representations by integrating auxiliary semantic information, such as entity types, attributes, and hyper-relational facts, thereby grounding embeddings in richer context and providing more semantic guidance.

The integration of auxiliary information into KGE models marks a significant paradigm shift from purely structural learning towards semantically enriched representations. Early KGE models, like TransE and its extensions (as reviewed in [asmara2023]), primarily focused on geometric transformations in vector spaces. While effective for capturing basic relational patterns, they often struggled with the nuances of entity roles or the reliability of facts. The evolution towards incorporating auxiliary data directly addresses these limitations, aiming to produce more discriminative and robust embeddings.

Three major paradigms emerge in leveraging auxiliary information:
\begin{enumerate}
    \item \textbf{Entity Type Integration}: Models that explicitly use entity type hierarchies or categorical labels to refine entity and relation embeddings.
    \item \textbf{Entity Attribute Integration}: Approaches that incorporate descriptive attributes of entities to enrich their representations, often to enhance robustness against noise.
    \item \textbf{Hyper-relational Fact Modeling}: Methods that extend beyond simple triplets to capture more complex, n-ary relationships.
\end{enumerate}

\subsubsection*{Entity Type Integration}
The core problem addressed by entity type integration is the limited semantic guidance in traditional KGEs, which often treat all entities as abstract nodes without distinguishing their inherent categories or roles. This can lead to less discriminative embeddings and difficulties in handling complex relational patterns where entity types impose strong constraints.

\textbf{TransET} [wang2021] is a pioneering framework that leverages entity types to learn more semantic features.
\begin{itemize}
    \item \textbf{Problem Solved}: It addresses the lack of semantic guidance in traditional KGEs by incorporating well-constructed prior knowledge in the form of entity types, which helps in distinguishing entity roles and refining relation representations.
    \item \textbf{Core Innovation}: TransET utilizes a circle convolution based on the embeddings of entities and entity types to map head and tail entities to type-specific representations. This allows for a more nuanced understanding of how relations interact with entities of specific types. A translation-based score function then learns from these type-specific representations.
    \item \textbf{Conditions for Success}: TransET succeeds when reliable and sufficiently granular entity type information is available. Its performance is validated on real-world datasets for link prediction and triple classification, demonstrating superiority over state-of-the-art models in most cases.
    \item \textbf{Theoretical Limitations}: While effective, TransET's reliance on explicit type information means its performance can degrade if type data is sparse, noisy, or unavailable. The circle convolution, while innovative, might not capture all complex interactions between entity and type embeddings.
    \item \textbf{Practical Limitations}: The need for pre-defined and well-structured entity types can be a practical bottleneck, as many KGs might lack comprehensive type annotations. The complexity of integrating type embeddings with entity embeddings also adds to the model's parameter count.
    \item \textbf{Comparison to Alternatives}: Compared to purely structural models like TransE [asmara2023] or RotatE [sun2018], TransET provides a more semantically grounded representation. It offers a more direct way to inject prior knowledge than some deep learning models that might implicitly learn type-like features but without explicit guidance.

\end{itemize}

Building on this, \textbf{TaKE} [he2023] proposes a universal type-augmented KGE framework for knowledge graph completion.
\begin{itemize}
    \item \textbf{Problem Solved}: TaKE aims to further improve KG completion by addressing the limitation of traditional KGE methods that primarily focus on structured triples, neglecting valuable entity type information. It seeks to provide more semantic guidance for embedding learning.
    \item \textbf{Core Innovation}: TaKE automatically captures type features without explicit type supervision, which is a significant advancement over methods requiring pre-defined types. It learns different type representations for each entity, allowing it to distinguish the diversity of types specific to distinct relations. Furthermore, it introduces a novel type-constrained negative sampling strategy to construct more effective negative samples during training. This is crucial as negative sampling quality (as reviewed in [gregucci2023]) directly impacts KGE performance.
    \item \textbf{Conditions for Success}: TaKE demonstrates its merits on four datasets from Freebase, WordNet, and YAGO, achieving state-of-the-art performance when combined with models like SimplE. Its ability to automatically capture type features makes it more robust to KGs with implicit or less structured type information.
    \item \textbf{Theoretical Limitations}: While it mitigates the need for explicit type supervision, the quality of automatically captured type features might still depend on the richness of the existing graph structure. The effectiveness of type-constrained negative sampling relies on the assumption that type information provides meaningful constraints for valid triples.
    \item \textbf{Practical Limitations}: The framework's universality means its performance can still be influenced by the underlying KGE model it augments. The computational overhead of automatically capturing type features and implementing type-constrained negative sampling might be higher than simpler methods.
    \item \textbf{Comparison to Alternatives}: TaKE represents an evolution from TransET by reducing the reliance on explicit type supervision and introducing a more sophisticated negative sampling strategy. It offers a more flexible way to incorporate type information compared to models that hardcode type constraints.
\end{itemize}

A domain-specific application of type-constrained KGE is seen in \textbf{GeoEntity-type constrained KGE} [hu2024].
\begin{itemize}
    \item \textbf{Problem Solved}: This approach addresses the inaccurate prediction of natural-language spatial relations between geographic entities (geoentities) by prior studies that often overlook essential semantic attributes.
    \item \textbf{Core Innovation}: The SR-KGE framework integrates geoentity types as a constraint to capture spatial and semantic relations more accurately. It fuses graph structures and semantic attributes, considering the diversity of natural language expressions in the embedding process.
    \item \textbf{Conditions for Success}: It shows superior performance on both small-scale and large-scale knowledge graph datasets for spatial relation inference, outperforming general KGE models like TransE, RotatE, and HAKE. Its success is tied to the availability and relevance of geoentity type information for spatial reasoning.
    \item \textbf{Theoretical Limitations}: The domain-specific nature of this model means its innovations might not directly generalize to other types of relations (e.g., temporal, social) where geoentity types are irrelevant.
    \item \textbf{Practical Limitations}: Requires specialized geoentity type annotations, which might not be readily available in all KGs.
    \item \textbf{Comparison to Alternatives}: Unlike general KGEs, SR-KGE is tailored to a specific, complex relational task, demonstrating how auxiliary information can be crucial for domain-specific accuracy.
\end{itemize}

\subsubsection*{Entity Attribute Integration}
Beyond categorical types, entities often possess rich descriptive attributes. Integrating these attributes can provide a deeper understanding of entities, particularly useful for handling noisy data.

\textbf{AEKE: Attributed Error-aware Knowledge Embedding} [zhang2024] is a notable framework that leverages entity attributes to guide error-aware embedding learning.
\begin{itemize}
    \item \textbf{Problem Solved}: AEKE tackles the critical issue of erroneous triples inevitably injected during KG construction. Most KGE algorithms assume triples are correct, leading to significant performance degradation in downstream applications when errors are present.
    \item \textbf{Core Innovation}: AEKE leverages the semantics contained in entity attributes to guide the KGE model against the impact of erroneous triples. It designs two triple-level hypergraphs to model the topological structures of the KG and its attributes. A confidence score for each triple is jointly calculated based on self-contradiction within the triple, consistency between local and global structures, and homogeneity between structures and attributes. These confidence scores adaptively update the weighted aggregation in a multi-view graph learning framework and the margin loss in KGE, ensuring that potential errors contribute minimally to learning.
    \item \textbf{Conditions for Success}: AEKE demonstrates superior performance over state-of-the-art KGE and error detection algorithms on three real-world KGs. Its success hinges on the availability of meaningful entity attributes and the effectiveness of its confidence scoring mechanism in identifying erroneous triples.
    \item \textbf{Theoretical Limitations}: The reliance on entity attributes means that KGs with sparse or low-quality attributes might not fully benefit. The complexity of constructing and reasoning with triple-level hypergraphs and multi-view learning adds to the theoretical burden.
    \item \textbf{Practical Limitations}: The computational cost of building and processing two hypergraphs and calculating confidence scores for every triple can be substantial, especially for very large KGs. Data requirements for comprehensive attributes can also be high.
    \item \textbf{Comparison to Alternatives}: AEKE directly addresses data quality issues, a limitation often overlooked by KGEs that assume clean data. Unlike methods that only detect errors, AEKE integrates error awareness directly into the embedding learning process, making it more robust.
\end{itemize}

\subsubsection*{Hyper-relational Fact Modeling}
Traditional KGE models are predominantly triplet-centric. However, many real-world facts are hyper-relational, involving more than two entities or additional key-value pairs that qualify the main triplet.

\textbf{HINGE: Hyper-Relational Knowledge Graph Embedding} [rosso2020] extends KGE beyond triplets to directly model hyper-relational facts.
\begin{itemize}
    \item \textbf{Problem Solved}: HINGE addresses the oversimplification of complex hyper-relational facts by triplet-only KGE models. These models fail to capture additional key-value pairs associated with a base triplet, leading to suboptimal representations.
    \item \textbf{Core Innovation}: HINGE directly learns from hyper-relational facts, which include a base triplet (h, r, t) and associated key-value pairs (k, v). It captures both the primary structural information of the KG (triplets) and the correlation between each triplet and its associated key-value pairs. This moves beyond n-ary representations that discard the fundamental triplet structure.
    \item \textbf{Conditions for Success}: Extensive evaluation shows HINGE's superiority on various link prediction tasks, outperforming both triplet-only methods (by 0.81-41.45\%) and n-ary representation methods (by 13.2-84.1\%). Its success relies on the availability of hyper-relational data.
    \item \textbf{Theoretical Limitations}: Modeling hyper-relational facts inherently increases the complexity of the data structure and the embedding space. Sparsity in key-value pairs for certain relations could limit its effectiveness.
    \item \textbf{Practical Limitations}: The need for KGs to be structured with hyper-relational facts is a prerequisite, which might not be the case for all datasets. The increased dimensionality and complexity of the model can lead to higher computational costs during training and inference.
    \item \textbf{Comparison to Alternatives}: HINGE offers a more comprehensive approach than methods that either ignore auxiliary information or transform hyper-relational facts into less informative n-ary representations. It provides a richer context for each fact, leading to more accurate predictions.
\end{itemize}

\subsubsection*{Comparative Framework and Evolution}
The evolution of KGEs incorporating auxiliary information can be summarized as a progressive movement from relying solely on structural patterns to leveraging richer semantic context (Table \ref{tab:auxiliary_info_kge_comparison}). Early work assumed that basic triplet structures were sufficient, but mid-period research recognized the limitations of this assumption, particularly for complex relations and noisy data. Recent work, as exemplified by these papers, directly addresses these limitations by integrating diverse forms of external knowledge.

\begin{table}[htbp]
    \centering
    \caption{Comparison of KGE Models Incorporating Auxiliary Information}
    \label{tab:auxiliary_info_kge_comparison}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|l|l|l|l|l|}
        \hline
        \textbf{Model Family} & \textbf{Key Papers} & \textbf{Auxiliary Info Type} & \textbf{Core Innovation} & \textbf{Problem Addressed} & \textbf{Key Advantage/Limitation} \\
        \hline
        \textbf{Entity Type} & TransET [wang2021], TaKE [he2023], GeoEntity-type [hu2024] & Entity Types (categorical) & Type-specific representations, type-constrained negative sampling, automatic type feature capture & Limited semantic guidance, entity role ambiguity, domain-specific relations & \textbf{Advantage}: More discriminative, semantic embeddings. \textbf{Limitation}: Reliance on type quality/availability. \\
        \hline
        \textbf{Entity Attribute} & AEKE [zhang2024] & Entity Attributes (descriptive) & Multi-view hypergraphs, confidence scores for error-aware learning & Performance degradation from erroneous triples & \textbf{Advantage}: Robustness to noisy data. \textbf{Limitation}: Computational cost, attribute quality dependence. \\
        \hline
        \textbf{Hyper-relational} & HINGE [rosso2020] & Key-value pairs (contextual qualifiers) & Direct learning from hyper-relational facts & Oversimplification by triplet-only models & \textbf{Advantage}: Captures richer fact semantics. \textbf{Limitation}: Increased data complexity, sparsity of qualifiers. \\
        \hline
    \end{tabular}
    }
\end{table}

\subsubsection*{Connections Across Papers and Broader Themes}
[wang2021]'s \textbf{TransET} and [he2023]'s \textbf{TaKE} both exemplify the field's shift towards leveraging well-structured prior knowledge to enhance KGEs. While TransET requires explicit entity types, TaKE builds on this by innovating with automatic type feature capture and a type-constrained negative sampling strategy, making it more adaptable to KGs with less explicit type annotations. This shows a clear progression in reducing the manual effort required for auxiliary information integration. The success of [hu2024]'s \textbf{GeoEntity-type constrained KGE} further reinforces this, demonstrating that even domain-specific type information can significantly boost performance for specialized tasks, highlighting the general utility of type-based semantic guidance.

The work by [zhang2024] on \textbf{AEKE} addresses a different, but equally critical, challenge: the robustness of KGEs to noisy data. While TransET and TaKE focus on enriching semantic understanding for completion, AEKE leverages entity attributes to directly guide error-aware embedding learning. This highlights a broader tension in KGE research: whether to focus on improving expressiveness for ideal data or enhancing robustness for imperfect real-world data. AEKE's multi-view hypergraph and confidence scoring mechanism represent a sophisticated approach to mitigate the impact of errors, a problem that many KGEs implicitly assume away.

\textbf{HINGE} [rosso2020] offers a distinct yet complementary approach by extending the fundamental data structure itself. While TransET/TaKE enrich the *interpretation* of entities and relations within triplets, HINGE acknowledges that some facts are inherently *more complex than triplets*. This exemplifies the field's shift from a rigid triplet-centric paradigm to one that accommodates the intrinsic complexity of real-world knowledge. HINGE's ability to directly learn from hyper-relational facts provides a richer context for each fact, which can be seen as a form of auxiliary information that is integral to the fact itself, rather than external.

\subsubsection*{Patterns and Tensions}
A recurring trade-off across these approaches is the balance between the richness of auxiliary information and its availability or complexity. While more semantic context generally leads to better performance, acquiring high-quality entity types, attributes, or hyper-relational facts can be challenging. Many KGs are incomplete not only in their structural triples but also in their auxiliary metadata. This leads to a tension between models that require explicit, well-structured auxiliary data (like TransET) and those that can infer or adapt to less complete information (like TaKE's automatic type feature capture).

Another tension lies in the computational cost. Integrating auxiliary information, especially through complex mechanisms like multi-view hypergraphs in AEKE or direct hyper-relational modeling in HINGE, often increases model complexity and computational demands. This can contrast with the need for efficient and scalable KGEs, as discussed in Section 6.1.

The field implicitly assumes that auxiliary information, when available, is largely reliable. However, just as structural triples can be erroneous, so too can entity types or attributes. AEKE directly questions this assumption by building an error-aware framework, suggesting a future direction where the quality of auxiliary information itself is critically assessed and integrated into the learning process.

In conclusion, incorporating auxiliary information represents a crucial advancement in KGE, moving beyond purely structural patterns to leverage richer semantic context. By integrating entity types, attributes, and hyper-relational facts, models like TransET [wang2021], TaKE [he2023], GeoEntity-type constrained KGE [hu2024], AEKE [zhang2024], and HINGE [rosso2020] provide more semantic, discriminative, and robust representations. These approaches collectively demonstrate that external, well-structured knowledge is indispensable for enhancing KGE performance, particularly in dealing with incomplete or noisy knowledge graphs, by grounding embeddings in a more comprehensive understanding of the underlying knowledge.
\subsection{Rule-based and Constraint-driven KGE}
\label{sec:4_2_rule-based__and__constraint-driven_kge}

Building on the previous discussion of incorporating auxiliary semantic information like entity types and attributes, this subsection shifts focus to a more explicit form of prior knowledge: logical rules and structural constraints. While auxiliary information enriches the semantic content of individual entities and relations, rule-based and constraint-driven approaches aim to inject broader logical patterns and consistency requirements directly into the Knowledge Graph Embedding (KGE) learning process. This is crucial because purely data-driven KGE models, despite their success in capturing statistical patterns, often struggle with logical reasoning, ensuring semantic consistency, and providing interpretable explanations for their predictions [dai2020]. By making embeddings adhere to logical patterns, these methods enhance reasoning capabilities, improve interpretability, and lead to more robust and semantically coherent representations, addressing the limitations of models that solely rely on observed factual triples.

The integration of logical rules and constraints into KGE models represents a significant advancement, moving beyond simple geometric transformations or deep learning architectures to explicitly guide the embedding space. This paradigm aims to bridge the gap between symbolic logic and distributed representations, leveraging the strengths of both. Three primary families of approaches can be identified:
\begin{enumerate}
    \item \textbf{Semantic Smoothness and Simple Structural Constraints}: Early methods that impose general consistency or smoothness assumptions on the embedding space.
    \item \textbf{Iterative Soft Rule Guidance}: Approaches that dynamically integrate uncertain logical rules into the training process, allowing for continuous refinement.
    \item \textbf{Principled Rule Embedding and Regularization}: More sophisticated frameworks that jointly learn rule representations alongside entity and relation embeddings, or use rules as direct regularizers.
\end{enumerate}

\subsubsection*{Semantic Smoothness and Simple Structural Constraints}
The initial attempts to inject prior knowledge into KGE focused on enforcing general properties of the embedding space, rather than complex logical rules. These methods aimed to ensure that the learned representations respected basic semantic or structural consistencies.

\textbf{Semantically Smooth Knowledge Graph Embedding (SSE)} [guo2015] addresses the problem of KGEs primarily relying on observed facts, which can lead to embedding spaces that lack intrinsic geometric structure or semantic coherence.
\begin{itemize}
    \item \textbf{Core Innovation}: SSE proposes to enforce a "semantically smooth" embedding space where entities belonging to the same semantic category are encouraged to lie close to each other. It achieves this by formulating manifold learning algorithms, specifically Laplacian Eigenmaps and Locally Linear Embedding, as geometrically based regularization terms. These terms constrain the embedding task, guiding the optimization towards a smoother, more semantically organized space.
    \item \textbf{Conditions for Success}: SSE demonstrates significant and consistent improvements in link prediction and triple classification on benchmark datasets. Its success is contingent on the availability of reliable semantic category information for entities, which serves as the basis for defining "smoothness."
    \item \textbf{Theoretical Limitations}: The effectiveness of SSE heavily relies on the quality and completeness of entity categorization. If categories are noisy or too coarse, the "smoothness" constraint might not provide meaningful guidance. Moreover, manifold learning, while powerful, can be computationally intensive for very large graphs.
    \item \textbf{Practical Limitations}: The requirement for additional semantic category information might not always be met in real-world KGs, necessitating pre-processing or external data integration. The regularization terms add complexity to the optimization problem.
    \item \textbf{Comparison to Alternatives}: Compared to purely structural models, SSE provides a more semantically grounded embedding. It differs from later rule-based methods by imposing a general geometric constraint rather than specific logical patterns, making it less expressive for complex reasoning but potentially more robust to rule sparsity.
\end{itemize}

Building on the idea of simple constraints, \textbf{Improving Knowledge Graph Embedding Using Simple Constraints} [ding2018] investigates the potential of straightforward constraints to enhance KGEs, contrasting with the trend of designing increasingly complex scoring models.
\begin{itemize}
    \item \textbf{Problem Solved}: This work addresses the need for more interpretable and structured embedding spaces, which are often lacking in complex KGE models. It aims to improve performance without significantly increasing model complexity.
    \item \textbf{Core Innovation}: The paper examines two types of simple constraints: non-negativity constraints on entity representations and approximate entailment constraints on relation representations. Non-negativity helps learn compact and interpretable entity vectors, while approximate entailment encodes logical regularities between relations (e.g., if A implies B, then their embeddings should reflect this). These constraints are imposed as prior beliefs on the structure of the embedding space.
    \item \textbf{Conditions for Success}: Evaluated on WordNet, Freebase, and DBpedia, the approach shows significant and consistent improvements over competitive baselines. Its success lies in the simplicity and intuitive nature of the constraints, which are generally applicable and do not require complex rule mining.
    \item \textbf{Theoretical Limitations}: While simple constraints improve interpretability and structure, they might not be expressive enough to capture highly complex logical patterns or multi-hop reasoning. The "approximate" nature of entailment means it's a soft guidance, not a strict logical enforcement.
    \item \textbf{Practical Limitations}: The benefits are tied to the underlying KGE model being constrained; these constraints are augmentations, not standalone KGEs. Their impact might be less pronounced than more sophisticated rule-integration methods for tasks requiring deep logical inference.
    \item \textbf{Comparison to Alternatives}: Unlike SSE [guo2015] which uses manifold learning for smoothness, [ding2018] employs direct mathematical constraints, offering a more lightweight approach. It provides a foundational step towards rule integration, demonstrating that even basic logical principles can significantly improve embedding quality, without the overhead of explicit rule mining required by later methods like RUGE [guo2017].
\end{itemize}

\subsubsection*{Iterative Soft Rule Guidance}
A more direct and powerful approach involves integrating logical rules, especially "soft rules" (rules with confidence levels), into the embedding process. This often involves iterative mechanisms to refine both embeddings and rule applications.

\textbf{Knowledge Graph Embedding with Iterative Guidance from Soft Rules (RUGE)} [guo2017] tackles the challenge of integrating logical rules, particularly soft rules, into KGE models. Previous attempts often made a one-time injection of hard rules, ignoring the interactive nature of embedding learning and logical inference, and the abundance of uncertain, automatically extracted rules.
\begin{itemize}
    \item \textbf{Problem Solved}: RUGE addresses the limitations of one-time rule injection and the exclusive focus on hard, exception-less rules. It aims to effectively leverage automatically extracted soft rules to enhance KGE performance and transfer knowledge more deeply.
    \item \textbf{Core Innovation}: RUGE proposes a novel paradigm that iteratively guides embedding learning using soft rules. It learns simultaneously from labeled triples, unlabeled triples (whose labels are predicted iteratively), and soft rules with varying confidence levels. In each iteration, rules are queried to obtain soft labels for unlabeled triples, which are then integrated to update the embedding model. This iterative procedure allows knowledge from rules to be progressively transferred into the embeddings.
    \item \textbf{Conditions for Success}: Evaluated on link prediction tasks on Freebase and YAGO, RUGE achieves significant and consistent improvements over state-of-the-art baselines. Its success is particularly notable because it effectively leverages automatically extracted soft rules, even those with moderate confidence levels, demonstrating robustness to rule uncertainty.
    \item \textbf{Theoretical Limitations}: The iterative nature, while powerful, can be computationally more expensive than one-time rule injection. The quality of the soft rules themselves is paramount; noisy or contradictory rules could potentially mislead the embedding process. The framework's performance is also tied to the underlying KGE model it augments.
    \item \textbf{Practical Limitations}: Efficiently extracting high-quality soft rules from KGs remains a practical challenge. Balancing the influence of observed triples versus rule-derived soft labels requires careful hyperparameter tuning.
    \item \textbf{Comparison to Alternatives}: RUGE represents a significant evolution from earlier constraint-based methods like SSE [guo2015] and [ding2018] by directly integrating explicit logical rules, rather than general smoothness or structural properties. It moves beyond hard rules, which are scarce and difficult to validate, to embrace the more abundant but uncertain soft rules, making it more applicable to real-world KGs.
\end{itemize}

Further refining the integration of soft rules, \textbf{Knowledge Graph Embedding Preserving Soft Logical Regularity} [guo2020] focuses on improving scalability and efficiency by imposing rule constraints directly on relation representations.
\begin{itemize}
    \item \textbf{Problem Solved}: This paper addresses the challenge of efficiently and effectively integrating soft logical information, particularly the scalability issues that arise when rule learning complexity depends on the entity set size.
    \item \textbf{Core Innovation}: The method represents relations as bilinear forms and maps entity representations into a non-negative and bounded space. Crucially, it derives a rule-based regularization term that *merely enforces relation representations* to satisfy constraints introduced by soft rules. This design makes the complexity of rule learning independent of the entity set size, significantly improving scalability.
    \item \textbf{Conditions for Success}: The approach demonstrates effectiveness in link prediction on Freebase and DBpedia, outperforming many competitive baselines. Its scalability makes it particularly suitable for large KGs where entity-dependent rule learning would be prohibitive.
    \item \textbf{Theoretical Limitations}: While improving scalability, the approach might lose some granularity by focusing solely on relation representations, potentially overlooking entity-specific nuances that rules might imply. The non-negative and bounded space for entities might impose restrictions that are not always ideal for all KGE models.
    \item \textbf{Practical Limitations}: The method still relies on the availability of soft rules, and the process of extracting and validating these rules remains a prerequisite. The specific representation of relations as bilinear forms might not be universally compatible with all KGE architectures.
    \item \textbf{Comparison to Alternatives}: This work builds upon the principles of soft rule integration established by RUGE [guo2017] but offers a more scalable solution by decoupling rule learning complexity from the number of entities. It provides a more direct and efficient way to regularize relation embeddings based on logical patterns, a critical improvement for large-scale applications.
\end{itemize}

\subsubsection*{Principled Rule Embedding and Regularization}
The most recent advancements in this area go beyond simply using rules as regularization, instead learning explicit embeddings for rules themselves, allowing for more sophisticated logical inference and deeper integration.

\textbf{RulE: Knowledge Graph Reasoning with Rule Embedding} [tang2022] introduces a novel and principled framework for enhancing KG reasoning by effectively leveraging logical rules.
\begin{itemize}
    \item \textbf{Problem Solved}: RulE addresses the brittleness of traditional logic-based reasoning and the limited reasoning capabilities of purely KGE models. It aims to deeply intertwine logical rules with embedding learning in a unified space.
    \item \textbf{Core Innovation}: Unlike previous KGE methods, RulE learns *rule embeddings* from existing triplets and first-order rules by jointly representing entities, relations, and logical rules in a unified embedding space. This allows for a confidence score to be calculated for each rule, enabling soft logical inference. Concurrently, the learned rule embeddings inject prior logical information into the embedding space, enriching and regularizing entity/relation embeddings.
    \item \textbf{Conditions for Success}: Extensive experiments on multiple benchmarks reveal that RulE outperforms the majority of existing embedding-based and rule-based approaches. Its success lies in its ability to perform soft logical inference and use rule embeddings to regularize and enrich entity/relation embeddings, leading to more robust and accurate reasoning.
    \item \textbf{Theoretical Limitations}: The joint learning of entity, relation, and rule embeddings can significantly increase model complexity and the number of parameters, potentially leading to higher computational costs. The quality of rule embeddings is highly dependent on the quality and completeness of the input rules.
    \item \textbf{Practical Limitations}: The framework requires a robust mechanism for rule extraction and representation. The computational overhead during training, especially for very large rule sets, could be substantial, although the paper emphasizes its conceptual simplicity.
    \item \textbf{Comparison to Alternatives}: RulE represents a sophisticated evolution from earlier rule-integration methods like RUGE [guo2017] and [guo2020]. While RUGE iteratively guides embeddings with soft rules and [guo2020] regularizes relation embeddings, RulE takes a more fundamental step by learning *embeddings for the rules themselves*. This allows for a more principled and unified approach to soft logical inference and regularization, making it a powerful framework for deep integration of logic and embeddings.
\end{itemize}

\subsubsection*{Comparative Framework and Evolution}
The evolution of rule-based and constraint-driven KGEs demonstrates a clear progression from general consistency enforcement to explicit, iterative, and finally, deeply integrated logical reasoning (Table \ref{tab:rule_based_kge_comparison}). Early work, like [guo2015] and [ding2018], recognized the need for structure beyond raw triples but relied on simpler, often global, constraints. The mid-period, exemplified by [guo2017], embraced the uncertainty of real-world rules by introducing iterative guidance from soft rules. More recent work, such as [guo2020] and [tang2022], pushes the boundaries by making rule integration more scalable and by learning explicit representations for rules themselves, enabling more nuanced logical inference.

\begin{table}[htbp]
    \centering
    \caption{Comparison of Rule-based and Constraint-driven KGE Approaches}
    \label{tab:rule_based_kge_comparison}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|l|l|l|l|l|}
        \hline
        \textbf{Model Family} & \textbf{Key Papers} & \textbf{Type of Logic/Constraint} & \textbf{Core Mechanism} & \textbf{Problem Addressed} & \textbf{Key Advantage/Limitation} \\
        \hline
        \textbf{General Consistency} & SSE [guo2015], [ding2018] & Semantic smoothness, non-negativity, approximate entailment & Manifold learning regularization, direct mathematical constraints & Lack of intrinsic geometric structure, interpretability, basic consistency & \textbf{Advantage}: Simple, generalizable. \textbf{Limitation}: Limited expressiveness for complex logic. \\
        \hline
        \textbf{Iterative Soft Rule Guidance} & RUGE [guo2017] & Soft logical rules (with confidence) & Iterative guidance, soft label generation & One-time rule injection, reliance on hard rules, knowledge transfer & \textbf{Advantage}: Leverages uncertain rules, deep knowledge transfer. \textbf{Limitation}: Computational cost, rule quality dependence. \\
        \hline
        \textbf{Principled Rule Embedding} & [guo2020], RulE [tang2022] & Soft logical rules, first-order rules & Rule-based regularization on relations, joint learning of rule embeddings & Scalability of rule learning, brittleness of logic, unified reasoning & \textbf{Advantage}: Scalable, unified reasoning, explicit rule representation. \textbf{Limitation}: Increased model complexity, rule extraction overhead. \\
        \hline
    \end{tabular}
    }
\end{table}

\subsubsection*{Connections Across Papers and Broader Themes}
The progression from [guo2015]'s \textbf{SSE} to [guo2017]'s \textbf{RUGE} highlights a fundamental shift in how "prior knowledge" is conceptualized and integrated. While SSE focuses on a global "semantic smoothness" constraint, RUGE moves to explicit, local logical rules, demonstrating the field's increasing ambition to inject more precise and actionable knowledge. This transition reflects a broader theme in AI: moving from implicitly learning patterns to explicitly encoding human-understandable knowledge.

[ding2018]'s work on \textbf{simple constraints} can be seen as a precursor to more complex rule integration. By showing that even basic non-negativity and approximate entailment can improve KGEs, it laid the groundwork for the idea that *any* form of logical guidance, no matter how simple, is beneficial. This contrasts with the purely data-driven models discussed in foundational KGE sections (e.g., [xiao2015] TransA, [jia2017] TransA, [xiao2015] ManifoldE, [xiao2015] TransG), which often lack such explicit semantic or logical structuring.

The work by [guo2020] on \textbf{preserving soft logical regularity} directly addresses a practical limitation of iterative rule guidance like RUGE [guo2017]: scalability. By making rule learning complexity independent of entity set size, it allows the benefits of soft rule integration to extend to much larger knowledge graphs, which is critical for real-world applications. This also connects to the broader theme of efficiency and scalability discussed in Section 6.1, demonstrating how methodological innovations can address practical bottlenecks.

Finally, \textbf{RulE} [tang2022] represents the culmination of this line of research by proposing a truly unified framework that learns embeddings for rules themselves. This goes beyond using rules as mere regularizers or guides; it treats rules as first-class citizens in the embedding space. This deep integration allows for more nuanced soft logical inference and directly contributes to the interpretability of KGEs, a persistent challenge in the field.

\subsubsection*{Patterns and Tensions}
A recurring tension in this area is the trade-off between the strictness of logical adherence and the flexibility to capture exceptions or uncertainties. Hard rules are precise but brittle and scarce; soft rules are more abundant and robust to noise but introduce ambiguity. Methods like RUGE [guo2017] and RulE [tang2022] explicitly embrace soft rules, acknowledging that real-world knowledge is often uncertain.

Another significant challenge is the **acquisition and quality of rules**. While these methods demonstrate the value of rules, they often assume the availability of a set of rules, whether manually curated or automatically extracted. The process of extracting high-quality, non-redundant, and consistent rules from large KGs is a research area in itself and remains a practical bottleneck. The field implicitly assumes that the provided rules are largely correct or that the models can robustly handle some level of noise within them.

The **scalability** of rule integration is another critical pattern. Early methods might struggle with large numbers of rules or entities. [guo2020] directly addresses this by designing a mechanism where rule learning complexity is independent of the entity set size, showcasing a methodological trend towards optimizing for real-world graph scales.

In conclusion, rule-based and constraint-driven KGE approaches offer a powerful means to inject prior logical knowledge into embedding models, addressing key limitations of purely data-driven methods. From enforcing semantic smoothness [guo2015] and simple structural constraints [ding2018] to iteratively guiding embeddings with soft rules [guo2017] and learning explicit rule embeddings [tang2022], these methods collectively enhance reasoning capabilities, improve interpretability, and ensure semantic consistency. While challenges remain in rule acquisition and computational scalability, the trajectory of this research demonstrates a clear commitment to building more robust, semantically coherent, and logically sound knowledge representations.
\subsection{Multi-modal and Cross-domain KGE}
\label{sec:4_3_multi-modal__and__cross-domain_kge}

Building on the discussion of integrating logical rules and structural constraints, which enrich Knowledge Graph Embeddings (KGEs) with explicit consistency and reasoning capabilities, this subsection explores a complementary paradigm: leveraging multi-modal and cross-domain information. While rule-based methods inject logical structure, multi-modal and cross-domain KGEs aim to enrich representations with diverse semantic content from external sources, such as textual descriptions, visual features, or data from heterogeneous domains. This approach is crucial for addressing data sparsity, enhancing semantic understanding, and enabling more comprehensive knowledge comprehension by leveraging complementary information that is often absent in the sparse, symbolic graph structure alone [dai2020].

The integration of multi-modal and cross-domain data into KGE models represents a significant step towards grounding abstract knowledge in real-world observations and connecting disparate information silos. This paradigm moves beyond the limitations of relying solely on graph structure, which can be incomplete or lack fine-grained semantic detail, by drawing on the richness of natural language, visual cues, or domain-specific auxiliary data. Three main categories of approaches emerge:
\begin{enumerate}
    \item \textbf{Text-Enhanced KGE}: Models that integrate textual descriptions of entities and relations to enrich their semantic representations.
    \item \textbf{Cross-Domain KGE}: Approaches designed to learn embeddings that capture relationships and interactions across different knowledge domains, often for specific applications like recommendation.
    \item \textbf{Domain-Specific Multi-modal Reasoning}: Models that combine various data modalities within a specialized domain to facilitate complex reasoning and knowledge discovery.
\end{enumerate}

\subsubsection*{Text-Enhanced Knowledge Graph Embedding}
The most common form of multi-modal integration in KGE involves leveraging textual descriptions associated with entities and relations. These descriptions provide rich semantic context that can resolve ambiguities and enrich embeddings, especially for sparse entities.

\textbf{SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions} [xiao2016] is an early and influential work that addresses the "weak-semantic" nature of purely geometric KGE models, which often struggle to capture nuanced meanings from symbolic triples alone.
\begin{itemize}
    \item \textbf{Problem Solved}: SSP aims to overcome the semantic limitations of traditional KGEs by jointly learning from symbolic triples and textual descriptions, thereby providing more precise semantic embeddings. It addresses the challenge of integrating heterogeneous data sources (structured triples and unstructured text).
    \item \textbf{Core Innovation}: SSP proposes a semantic space projection model that builds an interaction between symbolic triples and textual descriptions. It uses textual descriptions to discover semantic relevance and project information into a shared semantic space. This allows the model to leverage the rich contextual information present in text to refine entity and relation representations.
    \item \textbf{Conditions for Success}: SSP demonstrates substantial improvements in knowledge graph completion and entity classification on benchmark datasets. Its success is contingent on the availability and quality of textual descriptions for entities and relations. The model performs well when text provides complementary semantic information that is not easily inferable from the graph structure.
    \item \textbf{Theoretical Limitations}: While effective, SSP's projection mechanism might be limited in capturing highly complex, multi-faceted semantic relationships compared to more advanced language models. The interaction between symbolic and textual information is modeled through a relatively simple projection, which might not fully exploit deep semantic patterns.
    \item \textbf{Practical Limitations}: The quality of the textual descriptions is paramount; noisy or irrelevant text can degrade performance. The model's reliance on pre-computed text embeddings (e.g., from word2vec) means its semantic understanding is bounded by the capabilities of those embeddings.
    \item \textbf{Comparison to Alternatives}: Compared to purely structural KGEs (e.g., [ji2015] TransD, [ebisu2017] TorusE, [yang2021] CyclE), SSP provides a significant semantic boost by grounding embeddings in natural language. It represents an early step in multi-modal KGE, preceding the widespread adoption of large pre-trained language models (PLMs).
\end{itemize}

More recently, the advent of powerful pre-trained language models has revolutionized text-enhanced KGE. \textbf{Joint Language Semantic and Structure Embedding for Knowledge Graph Completion} [shen2022] exemplifies this by integrating deep language semantics with structural information.
\begin{itemize}
    \item \textbf{Problem Solved}: This work addresses the need for more robust KGEs, particularly in low-resource settings, by effectively combining the rich semantics from natural language descriptions with the structural information of knowledge graphs. It aims to overcome data sparsity by leveraging external textual context.
    \item \textbf{Core Innovation}: The method fine-tunes pre-trained language models (PLMs) with a probabilistic structured loss. The forward pass of the PLM captures deep semantics from natural language descriptions of knowledge triplets, while the structured loss ensures the reconstruction of graph structure. This joint learning paradigm allows for a synergistic capture of both semantic and structural cues.
    \item \textbf{Conditions for Success}: The model achieves state-of-the-art performance on various knowledge graph benchmarks, showing significant improvements, especially in low-resource regimes. Its success relies on the powerful semantic understanding capabilities of PLMs and the ability to effectively fine-tune them for the KGE task.
    \item \textbf{Theoretical Limitations}: The computational cost and memory requirements of fine-tuning large PLMs can be substantial, limiting scalability for extremely large KGs or resource-constrained environments. The effectiveness is also tied to the quality of the PLM and the relevance of its pre-training data to the KG domain.
    \item \textbf{Practical Limitations}: Requires significant computational resources (GPUs) for training. The model's performance can be sensitive to hyperparameter tuning for PLM fine-tuning.
    \item \textbf{Comparison to Alternatives}: This approach represents a significant advancement over earlier text-enhanced methods like SSP [xiao2016]. While SSP uses simpler text embeddings and projection, [shen2022] leverages the deep, contextualized semantic understanding of PLMs, leading to superior performance, particularly in capturing nuanced meanings and handling data sparsity. This reflects the broader field's shift towards integrating advanced deep learning models for semantic enrichment.
\end{itemize}

\subsubsection*{Cross-Domain and Domain-Specific Multi-modal Reasoning}
Beyond general text enhancement, KGEs are increasingly adapted to integrate diverse data types across domains or within specialized fields, enabling complex applications.

\textbf{Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation} [liu2023] addresses critical challenges in recommender systems, namely cross-domain cold start and the provision of multi-domain recommendations, which traditional systems often struggle with due to data sparsity and domain heterogeneity.
\begin{itemize}
    \item \textbf{Problem Solved}: This paper aims to efficiently model associations and interactions between items across diverse domains to enable multi-domain item-item (I2I) recommendations. It tackles the challenge of integrating information from different domains to enrich item representations.
    \item \textbf{Core Innovation}: A "cross-domain knowledge graph chiasmal embedding" approach is proposed, which efficiently interacts all items in multiple domains. To achieve both homo-domain (within-domain) and hetero-domain (cross-domain) embeddings, a novel "binding rule" is introduced. This allows the model to learn shared and domain-specific representations simultaneously, facilitating link prediction for I2I recommendations.
    \item \textbf{Conditions for Success}: The method achieves better link prediction and multi-domain recommendation results on two benchmark datasets. Its success depends on the existence of a knowledge graph that contains rich information about items and their associations, both within and across domains. The "binding rule" is crucial for effectively managing the interaction between different domains.
    \item \textbf{Theoretical Limitations}: The effectiveness of the chiasmal embedding and binding rule is predicated on the assumption that meaningful cross-domain links or shared entities exist within the knowledge graph. If domains are too disparate with few bridging connections, the cross-domain interaction might be limited. The model's focus on I2I recommendation might not directly translate to other cross-domain tasks.
    \item \textbf{Practical Limitations}: Constructing a comprehensive cross-domain knowledge graph can be a significant data engineering challenge. The model's performance is sensitive to the quality and density of cross-domain links.
    \item \textbf{Comparison to Alternatives}: Unlike general KGEs that focus on single-domain link prediction, this work specifically targets the complex multi-domain recommendation problem. It offers a more integrated approach than simply training separate KGEs for each domain, by explicitly modeling cross-domain interactions through its chiasmal embedding and binding rule.
\end{itemize}

In specialized domains, multi-modal KGE can drive knowledge discovery. \textbf{Multimodal reasoning based on knowledge graph embedding for specific diseases} [zhu2022] showcases this in the biomedical field.
\begin{itemize}
    \item \textbf{Problem Solved}: This work addresses the challenge of discovering new and reliable knowledge from existing biomedical KGs, particularly for specific diseases, by leveraging multimodal reasoning. It aims to provide universal pre-trained knowledge for specific disease fields.
    \item \textbf{Core Innovation}: The paper develops a process for constructing Specific Disease Knowledge Graphs (SDKGs) and implements multimodal reasoning using reverse-hyperplane projection. This approach integrates structural, category, and description embeddings, allowing for a richer representation of biomedical entities and relations. The combination of these modalities leads to the discovery of new, reliable drug-gene, gene-disease, and disease-drug pairs.
    \item \textbf{Conditions for Success}: Multimodal reasoning improves pre-existing models on all SDKGs using entity prediction tasks. The model's reliability in discovering new knowledge is verified through manual proofreading. Its success is highly dependent on the availability of structured biomedical data, entity categories, and textual descriptions.
    \item \textbf{Theoretical Limitations}: The reverse-hyperplane projection, while effective, might have limitations in capturing highly complex, non-linear interactions across diverse modalities compared to more advanced neural fusion techniques. The generalizability of the SDKG construction process to vastly different disease areas might require adaptation.
    \item \textbf{Practical Limitations}: The construction of high-quality SDKGs, including entity linking and relation linking, is a labor-intensive and domain-specific task. The manual proofreading for validation, while robust, is not scalable for large-scale knowledge discovery.
    \item \textbf{Comparison to Alternatives}: This model stands out by explicitly focusing on domain-specific multimodal reasoning, contrasting with general-purpose KGEs. It moves beyond purely structural or text-only approaches by integrating multiple distinct modalities (structure, category, description) to achieve more comprehensive knowledge discovery in a high-stakes domain.
\end{itemize}

\subsubsection*{Comparative Framework and Evolution}
The evolution of multi-modal and cross-domain KGE demonstrates a clear trajectory from augmenting structural embeddings with simple textual features to leveraging sophisticated language models and integrating diverse data types for complex, application-specific reasoning (Table \ref{tab:multimodal_kge_comparison}). Early methods like SSP [xiao2016] recognized the value of text as an auxiliary modality. The field then progressed to more powerful joint learning frameworks, exemplified by [shen2022], which harness the deep semantic understanding of pre-trained language models. Concurrently, the application scope expanded to address challenges like cross-domain recommendation [liu2023] and domain-specific knowledge discovery [zhu2022], showcasing the versatility of multi-modal KGE.

\begin{table}[htbp]
    \centering
    \caption{Comparison of Multi-modal and Cross-domain KGE Approaches}
    \label{tab:multimodal_kge_comparison}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|l|l|l|l|l|l|}
        \hline
        \textbf{Model Family} & \textbf{Key Papers} & \textbf{Primary Modalities/Data} & \textbf{Core Mechanism} & \textbf{Problem Addressed} & \textbf{Key Advantage/Limitation} \\
        \hline
        \textbf{Text-Enhanced (Projection)} & SSP [xiao2016] & Structured triples, text descriptions & Semantic space projection, interaction between sources & Weak-semantic KGEs, basic text integration & \textbf{Advantage}: Semantic enrichment. \textbf{Limitation}: Simpler text embeddings, limited deep semantic capture. \\
        \hline
        \textbf{Text-Enhanced (PLM-based)} & [shen2022] & Structured triples, natural language descriptions & Fine-tuning PLMs with probabilistic structured loss & Data sparsity, deep semantic capture, low-resource settings & \textbf{Advantage}: State-of-the-art semantics, robust to sparsity. \textbf{Limitation}: High computational cost, PLM dependency. \\
        \hline
        \textbf{Cross-Domain KGE} & [liu2023] & Multi-domain item KGs & Chiasmal embedding, binding rule for homo/hetero-domain interaction & Cross-domain cold start, multi-domain recommendation & \textbf{Advantage}: Handles domain heterogeneity. \textbf{Limitation}: Requires cross-domain links, specific to I2I rec. \\
        \hline
        \textbf{Domain-Specific Multi-modal} & [zhu2022] & Structure, category, description embeddings (biomedical) & Reverse-hyperplane projection, multimodal fusion & Knowledge discovery in specific diseases & \textbf{Advantage}: Deep domain understanding, new knowledge discovery. \textbf{Limitation}: Domain-specific, data engineering overhead. \\
        \hline
    \end{tabular}
    }
\end{table}

\subsubsection*{Connections Across Papers and Broader Themes}
The progression from \textbf{SSP} [xiao2016] to \textbf{Joint Language Semantic and Structure Embedding} [shen2022] vividly illustrates the rapid advancement in leveraging textual information. While SSP uses a projection mechanism to integrate text embeddings, [shen2022] capitalizes on the deep contextual understanding offered by pre-trained language models, fine-tuning them to simultaneously capture language semantics and structural consistency. This shift reflects a broader trend in AI towards integrating powerful, pre-trained foundation models into specialized tasks, moving from simpler feature engineering to complex, end-to-end learning.

These multi-modal and cross-domain approaches offer a distinct and complementary strategy to the rule-based and constraint-driven KGEs discussed in the previous subsection. While rule-based methods (e.g., [guo2017] RUGE, [tang2022] RulE) inject *logical consistency* and *reasoning capabilities* by enforcing explicit patterns, multi-modal methods like [xiao2016] SSP and [shen2022] provide *semantic richness* and *contextual understanding* by drawing from diverse external data sources. For instance, a rule might state "if A is a parent of B, then B is a child of A," but a textual description might explain *what* A and B are, adding depth to their representations. This highlights that both logical and semantic enrichments are vital for comprehensive knowledge understanding.

Furthermore, the work on \textbf{Cross-Domain KGE Chiasmal Embedding} [liu2023] and \textbf{Multimodal reasoning for specific diseases} [zhu2022] showcases the versatility and practical impact of KGE in complex real-world scenarios. [liu2023] addresses the pragmatic problem of recommendation across disparate domains, which is a common challenge in e-commerce and content platforms. [zhu2022], on the other hand, applies multimodal KGE for scientific discovery in a high-stakes domain, demonstrating how integrating structural, categorical, and descriptive information can lead to verifiable new knowledge. Both papers exemplify how KGE moves beyond generic link prediction to solve specific, high-value problems by adapting to the inherent heterogeneity and richness of real-world data.

\subsubsection*{Patterns and Tensions}
A recurring pattern across these multi-modal and cross-domain approaches is their ability to significantly mitigate the **data sparsity problem**, a pervasive limitation of purely structural KGEs. By leveraging external information, even sparsely connected entities or relations can acquire rich representations. However, this comes with a critical tension: the **complexity of fusing heterogeneous information**. Effectively combining textual, structural, categorical, and potentially visual data requires sophisticated joint learning frameworks, which often increase model complexity and computational cost. For instance, fine-tuning PLMs in [shen2022] is computationally intensive, contrasting with the relative efficiency of simpler translational models.

Another unstated assumption in many multi-modal KGEs is the **quality and alignment of the auxiliary data**. If textual descriptions are noisy, outdated, or misaligned with the symbolic facts, they can introduce errors rather than improvements. Similarly, for cross-domain KGEs, the existence of meaningful bridging entities or relations is crucial; without them, the "chiasmal embedding" of [liu2023] might struggle to find relevant connections. This highlights a practical limitation: the performance of these models is often bounded by the quality of the *least reliable* modality or domain.

The field also grapples with the **interpretability** of fused multi-modal representations. While individual modalities might be interpretable (e.g., text descriptions), understanding *how* different modalities interact and contribute to a final embedding can be challenging, especially with deep learning fusion mechanisms. This is a persistent challenge that connects to broader themes of explainable AI, particularly relevant in high-stakes applications like biomedical knowledge discovery [zhu2022].

In conclusion, multi-modal and cross-domain KGEs represent a powerful evolution in knowledge representation, moving beyond the confines of symbolic graph structures to embrace the richness of diverse information sources. From projecting text into semantic spaces [xiao2016] to fine-tuning large language models for joint semantic and structural understanding [shen2022], and from enabling cross-domain recommendations [liu2023] to facilitating multimodal reasoning for specific diseases [zhu2022], these approaches consistently demonstrate improved expressiveness and robustness. While challenges remain in data quality, fusion complexity, and interpretability, the trajectory of this research points towards increasingly comprehensive and context-aware knowledge understanding, crucial for the next generation of intelligent systems.


### Dynamic, Inductive, and Distributed KGE

\label{sec:dynamic,_inductive,__and__distributed_kge}

\section{Dynamic, Inductive, and Distributed KGE}
\label{sec:dynamic_inductive_and_distributed_kge}

The preceding section, "Enriching KGE: Auxiliary Information, Rules, and Multi-modality," demonstrated how integrating diverse external information and explicit logical constraints can significantly enhance the expressiveness and reasoning capabilities of Knowledge Graph Embeddings (KGEs). However, these advancements largely operated under the implicit assumption of a centralized, static, or quasi-static knowledge graph. In stark contrast, real-world knowledge graphs are inherently dynamic, constantly evolving with new facts, entities, and relations, often distributed across various data sources, and requiring continuous updates. This dynamic nature introduces critical challenges that necessitate a fundamental shift in KGE model design.

This section, "Dynamic, Inductive, and Distributed KGE," addresses these pivotal challenges, moving beyond static and centralized assumptions to explore how KGE models can be made adaptable, scalable, and secure in complex, operational environments. We delve into three interconnected areas. Firstly, we examine \textit{Temporal Knowledge Graph Embedding (TKGE)}, which focuses on modeling the temporal dynamics of facts and entities, capturing their evolution and validity over time [community_1, community_4]. Secondly, we explore \textit{Inductive and Continual KGE}, tackling the crucial problem of learning embeddings for unseen entities and relations (inductive learning) and efficiently updating models with new information without full retraining (continual learning), thereby enabling KGEs to adapt to the ever-growing nature of knowledge bases [community_1, 8f096071a09701012c9c279aee2a88143a295935]. Finally, we investigate \textit{Federated and Privacy-Preserving KGE}, addressing the complexities of collaborative learning across distributed and privacy-sensitive data sources, ensuring secure and efficient knowledge sharing in decentralized settings [community_1, 8c93f3cecf79bd9f8d021f589d095305e281dd2f]. The overarching goal is to equip KGE models with the necessary mechanisms to thrive in the fluid, decentralized, and privacy-conscious landscape of modern knowledge management systems.

\subsection{Temporal Knowledge Graph Embedding (TKGE)}
\label{sec:5_1_temporal_knowledge_graph_embedding_(tkge)}

The dynamic nature of real-world knowledge graphs (KGs), where facts evolve, appear, and disappear over time, necessitates embedding models capable of capturing these temporal dynamics. Moving beyond the static representations discussed in previous sections, Temporal Knowledge Graph Embedding (TKGE) models are specifically designed to represent entities and relations as they change through time, enabling reasoning over temporal sequences and understanding the evolution of facts. This subsection details the progression of TKGE, from early explicit temporal modeling to advanced geometric transformations and multi-curvature spaces, addressing the complexities of dynamic, spatiotemporal, and fuzzy knowledge.

The evolution of TKGE research can be broadly categorized into three main methodological families: (1) approaches that explicitly structure and model time, often through tensor representations or time series analysis; (2) methods leveraging geometric transformations like rotations to capture temporal evolution; and (3) advanced models employing multi-curvature spaces or Graph Neural Networks (GNNs) for more intricate temporal and spatiotemporal dynamics.

\subsubsection*{Explicit Temporal Modeling and Tensor/Time Series Approaches}
Early efforts in TKGE focused on explicitly integrating time into the embedding space. [dasgupta2018] introduced \textbf{HyTE} (Hyperplane-based Temporally aware Knowledge Graph Embedding), a pioneering model that addresses the problem of static KGEs ignoring temporal validity. HyTE's core innovation is to associate each timestamp with a corresponding hyperplane in the entity-relation embedding space. This allows for temporal guidance during KG inference and the prediction of temporal scopes for facts with missing time annotations. HyTE succeeds under conditions where temporal information can be discretely assigned to hyperplanes and offers a more expressive temporal representation than purely static models. However, its theoretical limitation lies in its potential inability to capture highly complex, non-linear temporal dependencies or continuous temporal evolution, as hyperplanes might oversimplify intricate temporal patterns. Practically, managing and learning distinct hyperplanes for a vast number of timestamps can introduce computational overhead, especially for fine-grained temporal data.

Building on the concept of explicit temporal integration, [lin2020] proposed a \textbf{Tensor Decomposition-Based Temporal Knowledge Graph Embedding} model. This approach tackles the problem of sparse and time-evolving data by regarding the entire fact set as a fourth-order tensor (head, relation, tail, time). The core innovation is to apply tensor decomposition techniques (e.g., CP or TuckER decomposition extended to four dimensions) to learn dense, low-dimensional vectors for entities, relations, and time. This method claims to generalize well to other static tensor-based KGEs and demonstrates superior performance on temporal datasets. It succeeds in scenarios where temporal data can be naturally represented as a discrete dimension within a tensor. A theoretical limitation is that tensor decomposition often assumes a fixed dimensionality for time, which might not be ideal for continuously evolving or irregularly sampled temporal data. Practical limitations include the high computational cost and memory requirements associated with higher-order tensor operations, particularly for very large and dense temporal KGs, which can hinder scalability.

Addressing the challenge of temporal uncertainty and continuous evolution, [xu2019] introduced \textbf{ATiSE} (Temporal Knowledge Graph Embedding Model based on Additive Time Series Decomposition). ATiSE's innovation lies in incorporating time using additive time series decomposition and mapping representations into multi-dimensional Gaussian distributions. The mean of each entity/relation embedding at a time step represents its expected position, while its covariance captures temporal uncertainty. This probabilistic approach claims to achieve state-of-the-art results on link prediction by accounting for the inherent fuzziness of temporal evolution. ATiSE excels when temporal uncertainty is a significant factor, providing a more robust representation than deterministic models. However, its theoretical limitation is the assumption of additive time series decomposition, which might not hold for all complex, non-linear temporal dynamics. Practically, learning and managing Gaussian distributions for all entities and relations across time can be computationally intensive, and the interpretability of learned covariances might be challenging.

More recently, [li2023] presented \textbf{TeAST} (Temporal Knowledge Graph Embedding via Archimedean Spiral Timeline), which offers a novel way to model temporal relations. TeAST addresses the limitations of existing methods that fuse temporal information into entities, potentially leading to entity information evolution and limiting link prediction performance. Its core innovation is mapping relations onto an Archimedean spiral timeline, transforming the quadruple completion problem into a 3rd-order tensor completion problem. The spiral timeline ensures that simultaneously occurring relations are placed on the same timeline, and all relations evolve orderly over time with a spiral regularizer. TeAST claims to encode various relation patterns and provide interpretability. It succeeds in scenarios where a continuous, orderly temporal progression can be assumed. A theoretical limitation is that the Archimedean spiral might impose a specific, potentially restrictive, structure on temporal evolution that may not generalize to all types of temporal dynamics. Practically, the complexity of implementing and optimizing the spiral timeline and its regularizer adds to the model's intricacy.

\subsubsection*{Geometric Transformation-based Temporal Embeddings}
A significant paradigm shift in TKGE involves leveraging geometric transformations, particularly rotations, to model temporal dynamics. [xu2020] proposed \textbf{TeRo} (A Time-aware Knowledge Graph Embedding via Temporal Rotation), which defines the temporal evolution of entity embeddings as a rotation from an initial time to the current time in a complex vector space. TeRo's innovation lies in using dual complex embeddings for relations involving time intervals (beginning and end), allowing it to capture rich interaction between temporal and multi-relational characteristics. TeRo claims to overcome limitations of existing models and infer various relation patterns over time, outperforming state-of-the-art models for temporal link prediction. It performs well on datasets with clear temporal sequences and where relational patterns like symmetry and inversion are prominent. However, the interpretability of complex rotations in high-dimensional spaces can be a theoretical limitation, and the computational cost of learning these transformations can be high.

Building on TeRo's insights, [sadeghian2021] introduced \textbf{ChronoR} (Rotation Based Temporal Knowledge Graph Embedding). ChronoR extends the concept by learning a k-dimensional rotation transformation, parameterized by both relation and time, to transform a head entity to fall near its tail entity. This approach, using high-dimensional rotation, aims to capture richer interactions between temporal and multi-relational characteristics. ChronoR demonstrates superior performance on benchmark datasets for temporal link prediction. While both TeRo and ChronoR focus on rotation, ChronoR's use of k-dimensional rotations offers potentially greater flexibility in modeling complex interactions. A common tension across these rotational models is balancing the expressiveness gained from complex rotations with the increased computational complexity and potential difficulty in interpreting the learned transformations.

More recently, the geometric transformation paradigm has been extended to address additional complexities like fuzziness and spatiotemporal dimensions. [ji2024] presented \textbf{FSTRE} (Fuzzy Spatiotemporal RDF Knowledge Graph Embedding Using Uncertain Dynamic Vector Projection and Rotation). FSTRE addresses the problem of existing KGE models being insufficient for uncertain and dynamic knowledge. Its core innovation is to embed spatial and temporal information by projection and rotation within a complex vector space, while introducing fine-grained fuzziness through modal lengths of anisotropic vectors. This allows FSTRE to capture rich interactions between crisp/static and fuzzy spatiotemporal knowledge. FSTRE succeeds in scenarios where both spatial and temporal dynamics are present, along with inherent uncertainty. A theoretical limitation is the complexity of simultaneously modeling fuzziness, spatial, and temporal dimensions within a single geometric framework, which might lead to intricate optimization landscapes.

Further advancing this, [ji2024] proposed a \textbf{Multihop Fuzzy Spatiotemporal RDF Knowledge Graph Query via Quaternion Embedding}. This model tackles the significant challenge of multihop querying on *incomplete* fuzzy spatiotemporal KGs, where previous embedding-based approaches overlooked uncertainty and spatiotemporal sensitivity during reasoning. Its innovation lies in using quaternions to jointly embed spatiotemporal entities and represent relations as rotations from subject to object. Uncertainty is incorporated via the scoring function's bias factor, and the non-commutative compositional pattern of quaternions is exploited for more accurate multihop path reasoning. This approach offers a powerful algebraic structure for representing rotations and compositions, beneficial for complex path queries. However, the algebraic complexity of quaternions can lead to increased computational demands and potentially reduced interpretability compared to simpler vector spaces.

\subsubsection*{Multi-Curvature Space Embeddings and GNN-based Approaches}
Recent advancements in TKGE have pushed towards even more sophisticated geometric and architectural solutions. [wang2024] introduced \textbf{MADE} (Multicurvature Adaptive Embedding for Temporal Knowledge Graph Completion), which addresses the challenge of embedding TKGs with complex, interwoven geometric structures (hierarchical, ring, chain) that are poorly captured by single Euclidean spaces. MADE's core innovation is modeling TKGs in multi-curvature spaces (Euclidean, hyperbolic, hyperspherical) with an adaptive weighting mechanism. This mechanism assigns different weights to curvature spaces in a data-driven manner, strengthening ideal spaces and weakening inappropriate ones. It also includes a quadruplet distributor for information interaction and an innovative temporal regularization for timestamp smoothness. MADE claims to outperform existing state-of-the-art TKGC models by adaptively leveraging the strengths of different geometries. It succeeds when TKGs exhibit diverse underlying geometric patterns. A theoretical limitation is the increased complexity of optimizing embeddings across multiple, potentially disparate, geometric spaces. Practically, the computational overhead of managing different curvatures and the adaptive weighting mechanism can be substantial.

Concurrently, [wang2024] proposed \textbf{IME} (Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion), which also models TKGs in multi-curvature spaces. IME addresses the problem that existing TKGC methods often neglect the heterogeneity of different curvature spaces. Its innovation lies in incorporating "space-shared" properties to learn commonalities across spaces and alleviate spatial gaps, and "space-specific" properties to capture characteristic features. IME also introduces an Adjustable Multi-curvature Pooling (AMP) approach and designs specific similarity, difference, and structure loss functions. IME, like MADE, achieves state-of-the-art results by leveraging the power of hybrid geometries. While both MADE and IME utilize multi-curvature spaces, MADE's adaptive weighting offers a more dynamic selection of geometries, whereas IME focuses on explicit shared/specific properties and tailored loss functions. A common practical limitation for both is the increased complexity in model design and training, making them potentially harder to implement and optimize than simpler models. The interpretability of embeddings in such hybrid spaces also remains a significant challenge.

Beyond geometric spaces, Graph Neural Networks (GNNs) have also been adapted for temporal KGE. [xie2023] presented \textbf{TARGAT} (A Time-Aware Relational Graph Attention Model for Temporal Knowledge Graph Embedding). TARGAT addresses the limitation of previous GNN-based models that struggle to directly capture multi-fact interactions at different timestamps. Its core innovation is a relational generator that dynamically produces time-aware relational message transformation matrices, which jointly model relations and timestamp information. These matrices project neighborhood features into different time-aware spaces for aggregation, followed by a temporal transformer classifier for query quadruples. TARGAT claims to significantly outperform prior GNN-based models and achieve state-of-the-art results on benchmark datasets. It succeeds in capturing complex, localized interactions within the graph structure over time. A practical limitation is the scalability of GNNs to extremely large and dense TKGs, as message passing can be computationally intensive. Furthermore, while it captures interactions, the interpretability of the learned time-aware transformations might still be a challenge compared to more geometrically intuitive models.

\subsubsection*{Comparative Framework and Evolution}
The intellectual trajectory of TKGE research demonstrates a clear evolution from explicitly modeling time as an additional dimension to integrating it deeply within complex geometric and algebraic structures. Early models like \textbf{HyTE} [dasgupta2018] and \textbf{Tensor Decomposition} [lin2020] established the feasibility of temporal awareness, treating time as a distinct, often discrete, variable. This laid the groundwork for more sophisticated approaches. \textbf{ATiSE} [xu2019] and \textbf{TeAST} [li2023] then introduced probabilistic and structured timeline representations, respectively, to better handle temporal uncertainty and continuous evolution, moving beyond simple discrete timestamps.

A significant shift occurred with the advent of \textbf{geometric transformation-based models} like \textbf{TeRo} [xu2020] and \textbf{ChronoR} [sadeghian2021]. These models evolved from the static rotational KGEs (e.g., RotatE [sun2018] from Section 2.2) by parameterizing rotations with time, allowing entities and relations to evolve dynamically. This family excels at capturing complex relational patterns (symmetry, inversion, composition) as they change over time. However, these models often assume crisp temporal facts. The most recent advancements, such as \textbf{FSTRE} [ji2024] and the \textbf{Quaternion Embedding} approach [ji2024], further extend this by incorporating fuzziness and spatiotemporal dimensions, acknowledging the inherent uncertainty and geographical context of real-world knowledge. This exemplifies a broader theme in KGE research: the move from simplified, crisp data assumptions to models robust enough for noisy, multi-faceted real-world data.

The latest frontier, represented by \textbf{MADE} [wang2024] and \textbf{IME} [wang2024], addresses a fundamental geometric limitation. While earlier models focused on *how* entities change over time, these multi-curvature models tackle *where* they are best represented. They recognize that a single Euclidean space is insufficient for the diverse geometric structures (hierarchical, chain-like, ring-like) present in dynamic KGs. By adaptively combining hyperbolic, hyperspherical, and Euclidean spaces, they aim to capture these intricate structures more accurately, pushing the boundaries of representational capacity. This directly addresses the limitations of single-space embeddings, a recurring critique in KGE research as highlighted in [cao2022].

\textbf{TARGAT} [xie2023], on the other hand, represents a divergence in methodology, leveraging the power of GNNs. While many KGE models operate on graph data, TARGAT explicitly uses attention-based message passing to model multi-fact interactions across timestamps. This contrasts with the more mathematically constrained geometric approaches, offering a data-driven way to learn complex temporal dependencies.

A critical tension across TKGE models lies in the trade-off between \textit{expressiveness} and \textit{computational complexity}. Models like MADE and IME, while highly expressive due to their multi-curvature nature, introduce significant optimization challenges and computational overhead. Similarly, quaternion-based embeddings, while powerful for composition and spatiotemporal modeling, are algebraically more complex than simpler vector operations. This echoes the efficiency concerns raised in Subsection 6.1, where models like DualDE [zhu2020] and LightKG [wang2021] aim to mitigate the parameter explosion in static KGEs; TKGE models face similar, if not greater, challenges due to the added temporal dimension.

Another unstated assumption in many TKGE models is the availability of precise timestamp information. While HyTE can predict missing time annotations, many models rely on well-defined discrete or continuous timestamps. This assumption breaks down in scenarios with fuzzy or uncertain temporal boundaries, which FSTRE [ji2024] and the Quaternion Embedding [ji2024] explicitly address. The field is steadily moving towards models that can handle such ambiguities, reflecting a broader trend towards robustness against data imperfections, as discussed in Subsection 6.2.

In conclusion, TKGE research has rapidly evolved from basic temporal awareness to highly sophisticated models that integrate complex geometries, algebraic structures, and deep learning architectures. The progression shows a clear drive to move beyond static, crisp, and Euclidean representations to encompass the full fluidity, uncertainty, and multi-faceted nature of real-world knowledge. The ongoing challenge remains to balance this increasing expressiveness with computational efficiency, interpretability, and robust handling of diverse temporal data characteristics.
\subsection{Inductive and Continual KGE}
\label{sec:5_2_inductive__and__continual_kge}

The previous discussion on Temporal Knowledge Graph Embedding (TKGE) highlighted the necessity of adapting KGE models to evolving facts and time-dependent relationships. Building upon this, the challenge extends to handling the emergence of entirely new entities and efficiently updating models with novel information without full retraining. This subsection delves into \textit{Inductive} and \textit{Continual Knowledge Graph Embedding} (KGE) methods, which are crucial for adapting KGE models to the dynamic and ever-growing nature of real-world knowledge bases. Inductive KGE focuses on embedding unseen entities, while continual KGE aims to efficiently acquire new knowledge and retain previously learned information, mitigating catastrophic forgetting and ensuring scalability.

The research in this area can be broadly categorized into three evolving methodological families: (1) neighborhood aggregation-based approaches, which leverage local graph structures for inductive inference; (2) meta-learning strategies, designed to learn transferable knowledge for new entities or dynamic updates; and (3) parameter-efficient adaptation techniques, which enable incremental learning by selectively updating model parameters.

\begin{table}[htbp]
\centering
\caption{Comparative Framework for Inductive and Continual KGE Approaches}
\label{tab:inductive_continual_kge}
\begin{tabularx}{\textwidth}{|p{0.18\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.15\textwidth}|}
\hline
\textbf{Dimension} & \textbf{Neighborhood Aggregation} ([wang2018]) & \textbf{Meta-Learning (Inductive)} ([chen2021]) & \textbf{Meta-Learning (Dynamic/Continual)} ([sun2024]) & \textbf{Parameter-Efficient Adaptation} ([liu2024]) \\
\hline
\textbf{Primary Problem} & Inductive KGE (unseen entities) & Inductive KGE (general unseen entities, entity embeddings) & Dynamic KGE (emerging entities in evolving KGs) & Continual KGE (new knowledge, mitigate forgetting, efficiency) \\
\hline
\textbf{Core Innovation} & Logic Attention Network (LAN) for neighbor aggregation with rules- and network-based attention. & MorsE: Learns entity-independent meta-knowledge via meta-learning to *produce* entity embeddings. & MetaHG: Meta-learning with hybrid GNN (local + global) for emerging entities. & FastKGE: Incremental LoRA (IncLoRA) for efficient, isolated new knowledge acquisition and forgetting mitigation. \\
\hline
\textbf{Conditions for Success} & New entities have existing, informative neighbors. & Diverse source KGs for meta-training; generalizable meta-knowledge. & KGs with both local and global structural patterns; dynamic nature. & Large pre-trained KGE models; new knowledge can be localized. \\
\hline
\textbf{Theoretical Limitations} & Struggles with cold-start (isolated) entities or sparse neighborhoods. Limited by rule expressiveness. & Transferability depends on source-target KG similarity; struggles with highly novel entities. & Complexity of balancing local/global info; hybrid GNNs can be intricate. & Assumption that new knowledge can be "isolated" might not always hold; low-rank approximation limits expressiveness. \\
\hline
\textbf{Practical Limitations} & Performance sensitive to neighborhood quality/density; scalability for very large neighborhoods. & High computational cost of meta-learning; complex setup. & High computational cost of meta-learning + complex GNNs; tuning. & Requires careful design of layer isolation; small overhead from LoRA. \\
\hline
\textbf{Comparison to Alternatives} & Early inductive, simpler but less general than meta-learning. & More general inductive than aggregation, but computationally heavier. & Specialized meta-learning for dynamic KGs, integrates GNNs. & Addresses both efficiency and forgetting; adapts existing models rather than learning from scratch. \\
\hline
\end{tabularx}
\end{table}

\subsubsection*{Neighborhood Aggregation for Inductive KGE}
Early efforts to tackle the inductive KGE problem focused on leveraging the local neighborhood structure of new entities. Traditional KGE models, being transductive, require all entities to be present during training, making them impractical for real-world KGs where new entities constantly emerge. [wang2018] addressed this by proposing \textbf{Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding}. The core problem it claims to solve is enabling KGE models to embed unseen entities by learning a generalized representation function. Its core innovation is the \textit{Logic Attention Network (LAN)}, which aggregates information from an entity's neighbors using both rules-based and network-based attention weights. This mechanism allows the model to dynamically compute an embedding for a new entity based on its existing connections, rather than requiring a pre-learned, static vector.

LAN succeeds under conditions where new entities have a sufficiently rich and informative neighborhood structure in the graph. The rules-based attention component can also leverage explicit logical patterns, enhancing interpretability. However, a significant theoretical limitation is its reliance on the presence of neighbors; truly cold-start entities (those with no or very few connections) would still pose a challenge, as there would be insufficient information to aggregate. Practically, the performance of LAN is highly sensitive to the quality and density of the neighborhood information. Furthermore, scaling neighborhood aggregation to extremely large graphs with vast neighborhoods can introduce computational overhead, as the aggregation process might become expensive. Compared to later meta-learning approaches, LAN offers a simpler, more direct inductive mechanism but is less generalizable to entities with entirely novel structural patterns.

\subsubsection*{Meta-Learning for Inductive and Dynamic KGE}
Building on the need for more generalized inductive capabilities, meta-learning emerged as a powerful paradigm. Instead of learning entity-specific embeddings, meta-learning aims to learn \textit{transferable meta-knowledge} that can be rapidly adapted to new tasks or entities.

[chen2021] introduced \textbf{Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding} (MorsE), directly addressing the limitation of existing inductive KGE methods that could only solve inductive relation prediction, not produce embeddings for unseen entities. The core innovation of MorsE is to learn entity-independent modules that encapsulate meta-knowledge, which can then be used to generate embeddings for new entities. This is achieved through a meta-learning framework, where the model learns how to learn, rather than learning specific entity representations. MorsE claims to significantly outperform baselines for both in-KG and out-of-KG tasks in inductive settings. It succeeds when there is sufficient diversity in the source KGs used for meta-training, allowing the model to learn truly generalizable meta-knowledge. A theoretical limitation is that the quality of meta-knowledge transfer is inherently dependent on the similarity between the meta-training KGs and the target KGs; highly novel entities or graph structures might still be challenging. Practically, meta-learning frameworks are often computationally expensive and complex to set up and train, requiring careful design of meta-tasks and optimization strategies. MorsE represents a significant step beyond neighborhood aggregation by offering a more general mechanism for cold-start entity embedding, as it doesn't strictly rely on existing neighbors but on learned transferable knowledge.

Extending meta-learning to dynamic and evolving KGs, [sun2024] proposed \textbf{MetaHG} for \textbf{Learning Dynamic Knowledge Graph Embedding in Evolving Service Ecosystems}. This work specifically tackles the challenge of efficiently updating incremental knowledge and representing emerging entities in dynamic service ecosystems, where conventional KGE methods often struggle with spatial deformation. MetaHG's core innovation is a meta-learning strategy that incorporates both local (via a GNN layer) and potential global (via a Hypergraph Neural Network, HGNN, layer) structural information from current snapshot KGs. It initializes entity embeddings using 'in' and 'out' relationship matrices and refines them through this hybrid GNN framework. This approach aims to enhance the representation quality of unseen entities by leveraging richer structural context. MetaHG succeeds particularly well in dynamic KGs that exhibit both localized and higher-order global structural patterns, such as those found in complex service ecosystems. A theoretical limitation is the increased complexity of balancing and optimizing the contributions from both local GNN and global HGNN layers, which might lead to intricate optimization landscapes. Practically, the combination of meta-learning with a hybrid GNN framework can result in high computational costs and requires extensive hyperparameter tuning. MetaHG specializes the meta-learning paradigm for dynamic updates and emerging entities, integrating advanced GNNs to capture richer structural context, a more nuanced problem than the general inductive entity embedding addressed by [chen2021].

\subsubsection*{Parameter-Efficient Adaptation for Continual KGE}
While meta-learning offers solutions for inductive capabilities and dynamic updates, the challenge of \textit{continual learning} in KGE involves not only acquiring new knowledge efficiently but also mitigating \textit{catastrophic forgetting} of previously learned information. This is a critical tension in dynamic KGE, as models must balance plasticity (learning new facts) with stability (retaining old facts).

[liu2024] addressed this dual problem with \textbf{Fast and Continual Knowledge Graph Embedding via Incremental LoRA} (FastKGE). The problem it claims to solve is the inefficiency of fine-tuning KGE models for new knowledge acquisition and the persistent issue of catastrophic forgetting. Its core innovation is the \textit{Incremental LoRA (IncLoRA)} mechanism, inspired by parameter-efficient fine-tuning techniques from large language models. FastKGE isolates and allocates new knowledge to specific layers based on the fine-grained influence between old and new KGs. It then devises an efficient IncLoRA mechanism, which embeds these specific layers into low-rank adapters, significantly reducing the number of trainable parameters during fine-tuning. Furthermore, IncLoRA introduces adaptive rank allocation, making the LoRA aware of entity importance. FastKGE demonstrates significant reductions in training time (34-49\% on public datasets, 51-68\% on larger datasets) while maintaining competitive link prediction performance. It succeeds particularly well for large pre-trained KGE models where new knowledge can be effectively localized to specific parts of the network. A theoretical limitation is the underlying assumption that new knowledge can indeed be "isolated" to specific layers without significantly impacting the representations of old knowledge, which might not hold for highly intertwined or foundational new facts. The low-rank approximation, while efficient, could also theoretically limit the expressiveness for very complex new relational patterns. Practically, designing the influence mechanism for layer isolation requires careful consideration, and while LoRA is parameter-efficient, it still adds a small computational overhead. FastKGE directly addresses the efficiency concerns raised in Subsection 6.1, offering a parameter-efficient solution specifically for continual updates, a distinct approach from the more general meta-learning strategies.

\subsubsection*{Comparative Framework and Evolution}
The intellectual trajectory in Inductive and Continual KGE demonstrates a clear evolution from basic inductive capabilities to more sophisticated and efficient dynamic learning paradigms, as summarized in Table \ref{tab:inductive_continual_kge}. Early work, exemplified by [wang2018]'s Logic Attention, established the feasibility of inductive learning through neighborhood aggregation. This approach, while effective for entities with existing connections, implicitly assumes the availability of such neighbors and struggles with truly cold-start scenarios.

The field then progressed to more generalized inductive capabilities through meta-learning. [chen2021]'s MorsE represents a significant leap by learning transferable meta-knowledge to *produce* entity embeddings, thereby overcoming the reliance on direct neighborhood aggregation and enabling embedding for a broader range of unseen entities. This addresses a fundamental limitation of earlier methods by shifting from direct entity embedding to learning a meta-function. Building on this, [sun2024]'s MetaHG further refines meta-learning for *dynamic* KGs, specializing it for emerging entities in evolving service ecosystems and integrating hybrid GNNs to capture richer structural context. This highlights a trend towards tailoring general learning paradigms to specific challenges in dynamic environments.

The most recent advancements, such as [liu2024]'s FastKGE, tackle the critical practical challenge of \textit{efficiency} in continual learning, a concern that meta-learning approaches, despite their power, often face due to their computational complexity. FastKGE's IncLoRA mechanism directly addresses the dual problem of efficient new knowledge acquisition and catastrophic forgetting, a recurring tension in continual learning research. This approach leverages insights from parameter-efficient fine-tuning, demonstrating a convergence of techniques from different AI domains (e.g., large language models) to solve KGE challenges.

A critical tension across this subgroup is balancing the \textit{generality} of inductive capabilities (e.g., embedding any unseen entity) with \textit{computational efficiency} and the effective mitigation of \textit{catastrophic forgetting}. While meta-learning approaches like MorsE and MetaHG offer strong inductive power, they can be computationally intensive. FastKGE, on the other hand, prioritizes efficiency and forgetting mitigation for continual updates, potentially at the cost of the full inductive generality offered by meta-learning for entirely novel entities. This echoes the broader efficiency concerns discussed in Subsection 6.1, where models like [zhu2020]'s DualDE and [wang2021]'s LightKG aim to reduce parameter counts and inference times in static settings; FastKGE extends this focus to the dynamic update scenario.

Another unstated assumption in many inductive KGE models is the presence of sufficient training data for meta-learning or a rich enough neighborhood for aggregation. This assumption can break down in extremely sparse or rapidly changing real-world KGs. The evaluation of these models often requires specialized dynamic datasets, which can be less standardized than static link prediction benchmarks, making direct comparisons challenging and potentially limiting the generalizability of reported performance gains. The field is actively working towards more robust and universally applicable solutions for these complex, real-world scenarios.
\subsection{Federated and Privacy-Preserving KGE}
\label{sec:5_3_federated__and__privacy-preserving_kge}

The previous subsection on Inductive and Continual KGE underscored the necessity for KGE models to adapt to dynamic and evolving knowledge. Extending this, the increasing demand for privacy-aware AI systems, coupled with the distributed nature of real-world knowledge, has propelled research into \textit{Federated and Privacy-Preserving Knowledge Graph Embedding} (FKGE). This paradigm allows multiple clients to collaboratively train a shared KGE model without directly sharing their sensitive local knowledge graphs, thereby addressing critical privacy concerns. However, this distributed setting introduces unique challenges related to communication efficiency, personalization for diverse client data, and security vulnerabilities, which are not typically encountered in centralized KGE training. Furthermore, specific privacy-preserving mechanisms like differential privacy and secure multi-party computation become crucial to bolster the inherent privacy of federated learning.

\begin{table}[htbp]
\centering
\caption{Comparative Framework for Federated and Privacy-Preserving KGE Approaches}
\label{tab:fkge_framework}
\begin{tabularx}{\textwidth}{|p{0.15\textwidth}|p{0.16\textwidth}|p{0.16\textwidth}|p{0.16\textwidth}|p{0.16\textwidth}|p{0.16\textwidth}|}
\hline
\textbf{Dimension} & \textbf{Communication Efficiency} ([zhang2024_feds]) & \textbf{Personalization} ([zhang2024_pfedeg]) & \textbf{Security (Poisoning Attack)} ([zhou2024]) & \textbf{Privacy-Preserving Defense (DP)} ([dpflames]) & \textbf{Cross-Domain Alignment} ([fedcke]) \\
\hline
\textbf{Primary Problem} & High communication costs in FL due to large parameters and extensive communication rounds. & Semantic disparities and heterogeneity among clients; "one-size-fits-all" global model. & Vulnerability to malicious clients injecting poisoned data to manipulate predictions. & Quantifying and defending against privacy threats (e.g., inference attacks) in FKGE. & Fusing/embedding cross-domain KGs securely without sharing raw data, especially for entity/relation alignment. \\
\hline
\textbf{Core Innovation} & FedS: Bidirectional Entity-Wise Top-K Sparsification and Intermittent Synchronization Mechanism. & PFedEG: Client-wise relation graph to learn personalized supplementary knowledge from "neighboring" clients. & Systematizes poisoning attacks; KG component inference + dynamic poisoning scheme. & DP-Flames: Differentially private FKGE with private selection and adaptive budget allocation, exploiting entity-binding sparse gradients. & FedCKE: Encrypted entity/relation alignment using vertical FL, followed by parameter-secure aggregation in horizontal FL. \\
\hline
\textbf{Conditions for Success} & KGE models where changes in entity embeddings are sparse or can be effectively approximated by top-K changes. & Clients have discernible "affinity" or semantic overlap, allowing meaningful knowledge transfer. & Attacker can infer target relations and craft poisoned data; aggregation mechanism is susceptible. & Acceptable privacy-utility trade-off; sparse gradient property of KGE holds. & Existence of shared entities/relations across domains; secure encryption/alignment protocols. \\
\hline
\textbf{Theoretical Limitations} & Potential loss of precision for less significant embedding changes; risk of embedding inconsistency. & Defining "affinity" robustly; potential for personalized models to diverge too much from global consensus. & Assumes attacker has sufficient control over local training/updates; privacy mechanisms might hinder detection. & Noise addition inherently reduces utility; tight privacy accounting is complex; may not fully protect against all attack vectors. & Complexity of encrypted alignment; potential for information leakage during alignment or aggregation if not perfectly secure. \\
\hline
\textbf{Practical Limitations} & Requires careful tuning of K (top-K); overhead of sparsification/desparsification. & Increased complexity in managing client-wise graphs; computational cost of personalized aggregation. & No direct defense mechanism proposed; identifying malicious clients is hard in practice. & Tuning privacy budget (epsilon) is critical; computational overhead of DP mechanisms. & High computational cost for encryption/decryption; scalability for very large cross-domain KGs. \\
\hline
\textbf{Comparison to Alternatives} & Focuses on parameter size reduction per round, unlike general FL methods reducing rounds (e.g., multiple local epochs). & Moves beyond global model averaging, offering client-specific adaptations, unlike FedE or FedEC. & First systematic analysis of poisoning in FKGE, highlighting a novel attack vector compared to centralized KGE. & First holistic study of privacy threats and defense (DP) for FKGE, offering a better privacy-utility trade-off than naive DP. & Addresses cross-domain fusion with privacy, unlike general FKGE that assumes shared schema or simple aggregation. \\
\hline
\end{tabularx}
\end{table}

\subsubsection*{Communication-Efficient Federated KGE}
The distributed nature of Federated Learning inherently leads to significant communication overhead, especially when dealing with large models like KGEs that involve numerous entity and relation embeddings. This challenge is exacerbated by the need for multiple communication rounds between clients and a central server. While general Federated Learning (FL) research has explored techniques like increasing local training epochs to reduce communication rounds, these often overlook the substantial size of parameters transmitted in each round.

[zhang2024_feds] (FedS) directly addresses this by proposing \textbf{Communication-Efficient Federated Knowledge Graph Embedding with Entity-Wise Top-K Sparsification}. The problem it claims to solve is the high communication cost in FKGE, which hinders its scalability and deployment. Its core innovation lies in a bidirectional communication-efficient strategy. During the upload phase, clients identify and transmit only the Top-K entity embeddings that have undergone the most significant changes, rather than sending the entire model. Similarly, during download, the server performs personalized embedding aggregation for each client and then transmits only the Top-K aggregated embeddings back. This mechanism is complemented by an Intermittent Synchronization Mechanism designed to mitigate the negative effects of embedding inconsistency among shared entities, a common issue arising from the heterogeneity of federated KGs.

FedS succeeds under conditions where the most significant updates to entity embeddings can be captured by a sparse subset, allowing for substantial data reduction without critical information loss. Quantitative evidence from experiments on three datasets demonstrates that FedS significantly enhances communication efficiency with negligible, or even no, performance degradation. For instance, it can reduce communication by a factor of 50-100x while maintaining competitive link prediction accuracy. However, a theoretical limitation is the potential for precision loss for less significant, yet cumulatively important, embedding changes that fall outside the Top-K. This occurs because the Top-K selection implicitly assumes that smaller changes are less critical, an assumption that might not hold if many small changes collectively contribute to a significant shift in representation. Practically, determining the optimal value of K (the number of top entities to transmit) is crucial and dataset-dependent, requiring careful tuning. An overly aggressive K could lead to slower convergence or suboptimal model quality. Compared to general FL communication reduction methods that primarily focus on reducing the number of communication rounds (e.g., by increasing local computation), FedS specifically targets the *size* of the transmitted parameters within each round, offering a complementary approach. This aligns with broader efficiency concerns in KGE, similar to how [zhu2020]'s DualDE uses knowledge distillation or [sachan2020]'s Knowledge Graph Embedding Compression (LightKG) reduce parameter size in centralized settings, but FedS adapts this principle to the distributed FL environment.

\subsubsection*{Personalized Federated KGE and Cross-Domain Alignment}
A fundamental tension in Federated Learning is between learning a robust global model and accommodating the inherent data heterogeneity across diverse clients. For FKGE, this translates to semantic disparities, where different clients might have distinct knowledge patterns or even conflicting facts. Traditional federated averaging, which aims for a universal global model, often overlooks these client-specific semantic nuances, potentially leading to a "one-size-fits-all" model that performs suboptimally for individual clients.

[zhang2024_pfedeg] (PFedEG) addresses this by proposing \textbf{Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph}. The problem it claims to solve is the compromise in embedding quality due to semantic disparities among clients and the limitations of a universal global model. Its core innovation is to move beyond a global consensus by learning personalized supplementary knowledge for each client. This is achieved through a novel "client-wise relation graph," which captures the semantic relevance or "affinity" between clients. PFedEG learns personalized supplementary knowledge for each client by amalgamating entity embeddings from its "neighboring" clients (based on their affinity) rather than simply averaging all client contributions. Each client then conducts personalized embedding learning using its local triples and this personalized supplementary knowledge.

PFedEG succeeds when there are meaningful semantic relationships or overlaps between client KGs that can be captured by the client-wise relation graph, allowing for beneficial knowledge transfer. Extensive experiments on four benchmark datasets demonstrate its superiority over state-of-the-art models, indicating that personalized aggregation significantly improves embedding quality. A theoretical limitation lies in the robust definition and dynamic adaptation of "affinity" within the client-wise relation graph; if affinity is poorly defined or static, it might not accurately reflect evolving semantic relationships, potentially leading to suboptimal personalization or even negative transfer. This is because the effectiveness of knowledge transfer is highly sensitive to the semantic alignment between source and target clients. Practically, managing and updating the client-wise relation graph adds complexity to the FKGE setup, and the computational cost of personalized aggregation can be higher than simple federated averaging. This approach directly contrasts with the implicit assumption of a homogeneous global model in many FL setups, offering a more nuanced way to handle non-IID data distributions. This challenge is also recognized in general inductive KGE where meta-learning approaches like [chen2021]'s MorsE aim to learn transferable knowledge across diverse tasks, but PFedEG specializes this for the federated context.

Extending the notion of personalization and heterogeneity to distinct knowledge domains, [fedcke] proposes \textbf{Cross-domain Knowledge Graph Embedding in Federated Learning (FedCKE)}. This work tackles the challenge of securely interrelating, fusing, and embedding KGs from different domains without sharing raw data. Unlike PFedEG which focuses on semantic disparities within a broadly shared schema, FedCKE explicitly addresses cross-domain scenarios where entity/relation alignment is a primary hurdle. Its core innovation is an inter-domain encrypted entity/relation alignment method, leveraging encrypted sample alignment from vertical federated learning. This allows clients to identify shared entities/relations without revealing their full KG structures. Subsequently, a parameter-secure aggregation method from horizontal federated learning is used on the server to combine embeddings. FedCKE is particularly successful when there are common entities or relations that can serve as anchors for alignment across disparate domains. A theoretical limitation is the inherent complexity and potential computational overhead of cryptographic operations for alignment, which could limit scalability for very large KGs or a high number of clients. Practically, the reliance on encrypted alignment adds significant computational cost, making it more resource-intensive than simpler federated averaging methods. FedCKE thus offers a more robust solution for privacy-preserving *cross-domain* knowledge fusion, a problem distinct from but related to the semantic personalization addressed by PFedEG. Both methods highlight a pattern of adapting general FL principles (vertical FL for alignment, horizontal FL for aggregation) to the specific needs of KGE, demonstrating a convergence of techniques.

\subsubsection*{Security Vulnerabilities and Privacy-Preserving Defenses}
While Federated Learning is designed to preserve data privacy by keeping raw data local, it does not inherently guarantee security against malicious actors. A critical, yet often unexplored, vulnerability in FKGE is the threat of poisoning attacks, where a malicious client injects carefully crafted data to manipulate the global model's behavior or force specific false predictions.

[zhou2024] systematically explores this by presenting \textbf{Poisoning Attack on Federated Knowledge Graph Embedding}. The problem it claims to solve is the lack of understanding and systematization of poisoning attacks in FKGE, which can lead to biased decisions in downstream applications. Its core innovation is a novel framework for poisoning attacks that forces victim clients to predict specific false facts. Unlike centralized KGEs where attackers might directly inject poisoned triples, FKGE's distributed nature makes this challenging. Instead, the attacker in [zhou2024]'s framework first infers targeted relations in the victim's local KG via a "KG component inference attack." Then, to accurately mislead the victim's embeddings through the federated aggregation process, the attacker locally trains a shadow model using the poisoned data and employs an optimized dynamic poisoning scheme to adjust its model updates.

This attack framework demonstrates remarkable effectiveness, achieving success rates as high as 100\% on models like TransE with WN18RR, while keeping the original task's performance nearly unchanged. This success is contingent on the attacker's ability to accurately infer target relations and craft poisoned data that can subtly influence the global model through aggregation. A significant theoretical limitation is that the paper identifies a problem without offering a direct defense mechanism, though it lays crucial groundwork for future robust FKGE designs. Practically, implementing such an attack requires sophisticated knowledge of the victim's KG structure and the FKGE aggregation mechanism. The implicit assumption here is that the federated server or other clients cannot easily detect or filter these malicious updates, highlighting a critical gap in current FKGE security protocols. This work serves as a stark reminder that privacy-preserving distributed training, while beneficial, introduces new attack surfaces that require dedicated security research, much like the broader field of KGE needs robustness against noisy data, as explored by [zhang2024_pfedeg]'s AEKE framework in centralized settings.

To counter such privacy threats and provide stronger guarantees, research has turned to mechanisms like Differential Privacy (DP). [dpflames] conducts the first holistic study of privacy threats on FKGE from both attack and defense perspectives, proposing \textbf{DP-Flames}. The problem it claims to solve is quantifying and defending against inference attacks that reveal the existence of KG triples from victim clients. Its core innovation is a novel differentially private FKGE with private selection, which offers a better privacy-utility trade-off. DP-Flames exploits the entity-binding sparse gradient property of FKGE and incorporates state-of-the-art private selection techniques, coupled with an adaptive privacy budget allocation policy. This allows it to dynamically adjust defense magnitude during training. DP-Flames demonstrates its effectiveness by successfully mitigating inference attacks, reducing their success rate significantly (e.g., from 80-90\% to 10-20\% on average) with only a modest utility decrease. Unlike [zhou2024] which focuses on attack vectors, DP-Flames directly provides a defense mechanism. A theoretical limitation of DP is the inherent trade-off between privacy and utility; adding noise to achieve privacy guarantees inevitably reduces the accuracy of the learned model. The challenge lies in finding the optimal balance, which is often complex to prove theoretically for complex models. Practically, tuning the privacy budget (epsilon) is critical and highly sensitive to the desired level of privacy and acceptable utility loss. DP-Flames represents a significant step towards building truly privacy-preserving FKGE systems, moving beyond the inherent privacy of FL to offer quantifiable privacy guarantees against inference attacks. This contrasts sharply with the attack-focused work of [zhou2024], highlighting the two sides of the security coin in FKGE.

\subsubsection*{Patterns, Tensions, and Future Directions}
The emerging field of Federated and Privacy-Preserving KGE is characterized by a critical tension between three key dimensions: \textit{privacy}, \textit{utility} (model performance and personalization), and \textit{efficiency/security}. Solutions like FedS [zhang2024_feds] strive for communication efficiency, often by making trade-offs in the precision of transmitted information via Top-K sparsification, which could theoretically impact model utility if critical but small changes are discarded. PFedEG [zhang2024_pfedeg] pushes for personalization to enhance utility by tailoring knowledge transfer, but this adds complexity and potentially increases communication overhead for personalized aggregation. Similarly, FedCKE [fedcke] enables cross-domain knowledge fusion with privacy, but at the cost of increased computational complexity due to encryption.

A recurring pattern is the adaptation of techniques from general FL or other AI domains to the specific challenges of KGE. For instance, sparsification for communication efficiency in FedS draws inspiration from general FL compression, while personalization strategies in PFedEG echo meta-learning's goal of transferability for diverse tasks, as seen in [sun2024]'s MetaHG for dynamic KGE. The use of vertical FL for alignment in FedCKE further exemplifies this cross-domain technique transfer. The poisoning attack demonstrated by [zhou2024] reveals that the privacy guarantees of FL do not inherently translate to security, highlighting a fundamental vulnerability that could severely compromise model utility and trustworthiness. This systematically challenges the unstated assumption across much of the constructive FKGE research (i.e., not attack-focused) that clients are benign. In response, DP-Flames [dpflames] introduces differential privacy as a robust defense mechanism, directly addressing the security gap exposed by poisoning attacks. This highlights a clear evolutionary trajectory: from initial focus on efficiency and basic privacy (FedS), to enhanced utility through personalization (PFedEG) and cross-domain integration (FedCKE), and finally to explicit security and quantifiable privacy guarantees (DP-Flames countering [zhou2024]'s attacks).

This field faces several open challenges. The generalizability of current FKGE solutions is an area requiring further scrutiny. While methods show promise on benchmark datasets, their performance in highly heterogeneous, real-world distributed KGs with varying data quality and client capabilities remains to be fully evaluated. The trade-off between privacy guarantees (e.g., epsilon in DP) and model utility needs more sophisticated theoretical and empirical analysis, especially for complex KGE models. Furthermore, the field needs to move beyond simple link prediction metrics to assess the impact of FKGE on downstream applications, similar to the broader KGE field's shift towards application-specific evaluations discussed in Subsection 7.3.

In conclusion, Federated and Privacy-Preserving KGE represents a crucial frontier for leveraging decentralized knowledge while respecting privacy. The initial solutions address communication efficiency and personalization, but the identified security vulnerabilities and the need for stronger privacy guarantees underscore the need for a holistic approach that integrates robust defense mechanisms. Future research must focus on developing FKGE frameworks that can simultaneously optimize for privacy, utility, efficiency, and security, ensuring that collaborative KGE models are not only effective but also trustworthy and resilient in real-world, privacy-sensitive environments. This will likely involve more sophisticated privacy-preserving techniques beyond simple non-sharing of raw data, and adaptive personalization strategies that dynamically adjust to evolving client needs and data distributions.


### Practical Considerations: Efficiency, Robustness, and Evaluation

\label{sec:practical_considerations:_efficiency,_robustness,__and__evaluation}

\section{Practical Considerations: Efficiency, Robustness, and Evaluation}
\label{sec:practical_considerations:_efficiency,_robustness,_and_evaluation}

While preceding sections have explored the theoretical foundations, advanced architectures, and adaptive capabilities for dynamic and distributed Knowledge Graph Embeddings (KGEs), their true utility hinges on practical viability in real-world scenarios. This section shifts focus from theoretical advancements to the critical operational challenges encountered when deploying and evaluating KGE models. It addresses the indispensable need for models that are not only expressive but also computationally efficient, resilient to data imperfections, and rigorously validated.

The first major theme, \textit{Efficiency, Compression, and Scalability}, delves into strategies for optimizing KGE models to handle the immense scale of modern knowledge graphs. This includes techniques for reducing computational cost, minimizing memory footprint, and enhancing training and inference speed, which are paramount for large-scale deployments [community_1, community_6, 8c93f3cecf79bd9f8d021f589d095305e281dd2f]. Following this, \textit{Robustness and Training Optimization} explores methods to fortify KGE models against the inherent noise and incompleteness of real-world data. This encompasses strategies for improving data quality, optimizing negative sampling, and making training processes more stable and effective, ensuring reliable performance even under challenging conditions [community_2, community_3, 2a3f862199883ceff5e3c74126f0c80770653e05]. Finally, \textit{Evaluation, Benchmarking, and Reproducibility} underscores the paramount importance of standardized metrics, fair comparisons, and transparent research practices. This subsection highlights efforts to establish unified frameworks, conduct large-scale comparative studies, and promote reproducibility, which are fundamental for fostering trustworthy scientific progress and ensuring the reliable application of KGE models in diverse domains [community_0, community_6]. Collectively, these practical considerations are vital for bridging the gap between academic research and the successful implementation of KGE technologies in industry and real-world AI systems.

\subsection{Efficiency, Compression, and Scalability}
\label{sec:6_1_efficiency,_compression,__and__scalability}

As Knowledge Graph Embedding (KGE) models have grown in expressiveness and complexity, as discussed in Sections 2 and 3, the practical challenges of their deployment have become increasingly prominent. The sheer scale of real-world knowledge graphs necessitates techniques that enhance efficiency, reduce memory footprint, and ensure scalability during both training and inference. This subsection examines key innovations addressing these practical bottlenecks, ranging from model-agnostic system optimizations to model-specific compression and parameter-efficient learning strategies.

A primary driver for this research area is the inherent trade-off between model capacity (and thus, performance) and computational resource requirements. Early KGE models, while efficient, often lacked expressiveness. Modern, highly expressive models, particularly those leveraging deep learning architectures, tend to be parameter-heavy and computationally intensive. This tension has spurred the development of several distinct, yet complementary, methodological families:
\begin{enumerate}
    \item \textbf{Knowledge Distillation and Embedding Compression:} These approaches aim to reduce the size of the embedding layer and accelerate inference by either training a smaller model to mimic a larger one or by encoding embeddings more compactly.
    \item \textbf{Parameter-Efficient Learning:} This family focuses on designing KGE models that inherently require fewer parameters, especially for entities, thus mitigating the linear growth of parameters with graph size.
    \item \textbf{Algorithmic and System-Level Optimizations:} These methods tackle the computational cost of training, either through novel, inherently efficient algorithms or by optimizing the underlying system infrastructure for parallel and distributed processing.
\end{enumerate}

\subsubsection*{Knowledge Distillation and Embedding Compression}
One prominent strategy to achieve efficiency is \textit{knowledge distillation}, where a smaller, "student" KGE model is trained to replicate the behavior of a larger, more complex "teacher" model. [zhu2020] introduced DualDE, a dually distilling method designed to create low-dimensional student KGEs from high-dimensional pre-trained teachers. The core innovation of DualDE lies in its dual-influence consideration between teacher and student, employing a soft label evaluation mechanism to adaptively weigh soft and hard labels for different triples, alongside a two-stage distillation process. This approach claims significant parameter reduction (7-15x) and inference speedup (2-6x) while retaining high performance, making it suitable for resource-limited applications. A theoretical limitation of distillation is that the student model's performance is inherently bounded by the teacher's capabilities, and the distillation process itself can be complex to tune. Practically, the "minor loss in performance" often associated with compression, as noted in other works, can still be a critical factor in highly sensitive downstream tasks.

Complementary to distillation are direct \textit{embedding compression} techniques. [sachan2020] proposed an approach to compress the KGE layer by representing each entity as a vector of discrete codes, from which embeddings are composed. This method achieves massive compression (50-1000x) with only a minor loss in performance, demonstrating that the information content of embeddings can be significantly condensed. Building on this, [wang2021] introduced LightKG, a lightweight framework for efficient inference and storage. LightKG's innovation lies in storing only a few codebooks and indices, drastically reducing storage requirements. It boosts inference efficiency through quick look-ups and incorporates a novel dynamic negative sampling method based on quantization. While [sachan2020] focuses on the compression mechanism, LightKG provides a more comprehensive framework, integrating residual modules for codebook diversity and a continuous function to approximate non-differentiable codeword selection. Both methods address the problem of large memory footprints, but LightKG specifically optimizes for both storage and inference speed, making it highly practical for real-time applications. However, the reliance on discrete codes or codebooks introduces a quantization error, which is the theoretical limitation that leads to the "minor loss in performance" compared to full-precision embeddings.

\subsubsection*{Parameter-Efficient Learning}
Moving beyond post-hoc compression, \textit{parameter-efficient learning} aims to design KGE models that are inherently less parameter-intensive. [chen2023] proposed Entity-Agnostic Representation Learning (EARL) to tackle the inefficient parameter storage costs of conventional KGE methods, where the number of embedding parameters grows linearly with the number of entities. EARL's core innovation is to learn embeddings only for a small set of "reserved entities" and derive the embeddings for all other entities from their context using universal, entity-agnostic encoders. This approach results in a static and lower parameter count, making it particularly beneficial for massive and continuously growing KGs. EARL addresses a fundamental problem in KGE scalability by decoupling entity representations from direct parameter storage. Its success hinges on the assumption that sufficient distinguishable information can be extracted from an entity's context to form a high-quality embedding, which might be a practical limitation for very sparse entities or those with limited contextual information. Compared to distillation or compression, EARL offers a more fundamental solution to parameter explosion by rethinking how embeddings are learned, rather than how they are stored or approximated.

\subsubsection*{Algorithmic and System-Level Optimizations}
Beyond reducing model size, significant efforts have been directed towards accelerating the training process itself. [peng2021] introduced a "Highly Efficient Knowledge Graph Embedding Learning" framework that significantly reduces training time and carbon footprint by orders of magnitude. Their core innovation is a closed-form solution using Orthogonal Procrustes Analysis, enabling full-batch learning and non-negative sampling. This represents a paradigm shift from iterative optimization methods, offering inherent efficiency. A key theoretical advantage is that their entity embeddings also store full relation information, suggesting improved interpretability. However, the applicability of such closed-form solutions might be limited to specific KGE model architectures, potentially not generalizing to the full complexity of GNN-based or Transformer-based models discussed in Section 3.

For Graph Neural Network (GNN)-based KGEs, which are increasingly popular for their expressiveness, scalability remains a challenge. [modak2024] proposed CPa-WAC (Constellation Partitioning-based Scalable Weighted Aggregation Composition) to address the training time and memory cost of GNNs on large KGs. The innovation lies in using modularity maximization-based constellation partitioning to break down KGs into subgraphs, which are then processed separately. This method aims to reduce memory and training time while crucially retaining prediction accuracy by preserving local graph topology. CPa-WAC demonstrates that efficient partitioning strategies can enable GNNs to scale to larger KGs, achieving up to five times faster training while maintaining performance comparable to training on the whole graph. A practical limitation is the complexity of effectively aggregating information from partitioned subgraphs without losing critical global context, a common challenge in graph partitioning.

Finally, \textit{system-level optimizations} provide a general infrastructure for efficient KGE training. [zheng2024] introduced GE2, a "General and Efficient Knowledge Graph Embedding Learning System." GE2 tackles the long CPU times and high CPU-GPU communication overhead observed in existing graph embedding systems (e.g., PBG, DGL-KE, Marius). Its core innovations include a general execution model for various negative sampling algorithms, a user-friendly API, offloading CPU operations to GPU for higher parallelism, and the novel COVER algorithm for efficient data swap between CPU and multiple GPUs. GE2 is not a KGE model itself but a foundational system that can accelerate the training of various KGE models across different datasets, achieving speedups of over 2x and up to 7.5x. This work highlights that system-level engineering is as crucial as algorithmic innovation for practical KGE deployment. GE2's generality allows it to potentially accelerate the training of models like CPa-WAC [modak2024] or even the efficient KGE learning from [peng2021], representing a convergent research direction where optimized algorithms benefit from optimized infrastructure.

\subsubsection*{Synthesis and Tensions}
The research in efficiency, compression, and scalability reveals a clear intellectual trajectory. Early KGE models often prioritized expressiveness, leading to large memory footprints and long training times. This necessitated the development of post-hoc solutions like knowledge distillation [zhu2020] and embedding compression [sachan2020, wang2021] to make models deployable. These methods inherently involve a trade-off: they achieve significant resource savings but often with a "minor loss in performance," as highlighted by [sachan2020].

More recent work has shifted towards designing KGE models that are *inherently* efficient, such as parameter-efficient learning with EARL [chen2023], which fundamentally rethinks entity representation to avoid linear parameter growth. Simultaneously, the field has seen a push for radically more efficient training algorithms, exemplified by the closed-form solution of [peng2021], which offers orders of magnitude speedup by leveraging mathematical properties. This contrasts with engineering solutions like graph partitioning [modak2024] that optimize existing GNN architectures for scalability.

A critical tension across these approaches is the balance between efficiency and the ability to capture complex relational patterns. While compression and parameter efficiency reduce resource demands, they must ensure that the core expressiveness of the KGE is not unduly compromised. Furthermore, the generalizability of these efficiency techniques is an ongoing challenge. For example, while GE2 [zheng2024] offers a general system for accelerating training, its impact is tied to the underlying KGE model and hardware. Similarly, CPa-WAC [modak2024] is tailored for GNN-based KGEs, and its partitioning strategy might not be directly applicable to all KGE architectures. The unstated assumption in many of these works is that the "minor" performance degradation from compression or partitioning is acceptable for the gains in efficiency, an assumption that may not hold in all high-stakes applications. These innovations are crucial for overcoming the practical bottlenecks of KGE, making them deployable in resource-constrained environments and for handling ever-growing knowledge bases, thereby bridging the gap between theoretical advancements and real-world utility.
\subsection{Robustness and Training Optimization}
\label{sec:6_2_robustness__and__training_optimization}

While the preceding sections have explored the evolution of Knowledge Graph Embedding (KGE) models towards greater expressiveness (Section 2) and architectural sophistication (Section 3), and the critical need for efficiency and scalability (Subsection 6.1), the practical utility of these models hinges significantly on their robustness against real-world data imperfections and the optimization of their training processes. This subsection delves into methodologies that ensure KGE models learn accurate and reliable representations even from noisy, incomplete, or imbalanced data, leading to more trustworthy predictions and better generalization. The focus here is on enhancing the resilience of KGE models and refining the learning mechanisms that underpin their performance.

\subsubsection*{Probability Calibration for Reliable Predictions}
A critical, yet often overlooked, aspect of KGE model reliability is the calibration of their probability estimates. Many KGE models output raw scores that indicate plausibility, but these scores are frequently uncalibrated, meaning a score of, for instance, 0.8 does not reliably correspond to an 80\% chance of a triple being true. This unreliability can be detrimental in high-stakes applications requiring precise confidence levels. [tabacof2019] explicitly addresses this problem, demonstrating that popular embedding models are indeed uncalibrated.

\textbf{Problem Solved:} [tabacof2019] aims to make the probability estimates associated with predicted triples reliable, especially in scenarios where ground truth negatives, crucial for direct calibration, are scarce.
\textbf{Core Innovation:} The paper proposes a novel method to calibrate KGE models post-training, even without readily available ground truth negatives. It leverages established techniques like Platt scaling and isotonic regression, adapting them for the KGE context.
\textbf{Conditions for Success:} The method succeeds when the underlying KGE model provides a reasonably good ranking of triples, even if its raw scores are uncalibrated. It is a model-agnostic post-processing step, applicable to a wide range of KGE architectures.
\textbf{Theoretical Limitations:} Calibration is a post-hoc adjustment; it does not enhance the intrinsic discriminative power of the KGE model itself. Its effectiveness is bounded by the quality of the raw scores provided by the base model. Furthermore, the absence of true negatives for calibration introduces its own set of challenges, requiring careful proxy selection.
\textbf{Practical Limitations:} While effective, applying calibration methods like isotonic regression can be computationally intensive and may require a dedicated validation set for optimal performance, which might still be challenging to obtain with reliable negative samples.
\textbf{Comparison to Alternatives:} Unlike methods that directly improve embedding quality or filter noisy inputs, calibration focuses solely on the interpretability and trustworthiness of the *output scores*. It complements other robustness techniques by ensuring that the model's confidence in its predictions is well-aligned with reality. This addresses a different facet of robustness compared to the input-focused methods discussed next.
\textbf{Implications:} This work highlights that traditional KGE evaluation metrics (e.g., Hit@K, MRR) do not fully capture the reliability of predictions, advocating for a more comprehensive assessment of model trustworthiness.

\subsubsection*{Robustness Against Noisy Data and Imbalance}
Real-world knowledge graphs are often constructed automatically, inevitably introducing noise and conflicts. Most KGE models, however, implicitly assume clean data, leading to suboptimal or unreliable representations when trained on imperfect KGs. Concurrently, KGs frequently exhibit long-tail distributions, where a small fraction of entities and relations are highly frequent, while the vast majority are rare, leading to imbalanced training.

\textbf{Noise Filtering via Reinforcement Learning:}
[zhang2021] addresses the challenge of noisy KGs by proposing a novel framework.
\textbf{Problem Solved:} This method tackles the degradation of KGE performance due to noisy triples and knowledge conflicts, which are prevalent in automatically constructed KGs.
\textbf{Core Innovation:} It introduces a multi-task reinforcement learning (RL) framework where an RL agent learns to dynamically select high-quality triples and filter out noisy ones during the training process. To enhance efficiency and leverage semantic similarities, the triple selection processes for semantically similar relations are trained collectively using multi-task learning.
\textbf{Conditions for Success:} The approach is particularly effective in scenarios where KGs are known to contain a significant amount of noise. It is general enough to be extended to popular KGE models like TransE, DistMult, ConvE, and RotatE.
\textbf{Theoretical Limitations:} RL-based training can be complex, requiring careful design of reward functions and state representations. The inherent uncertainty of identifying "true" noise in a large KG makes the optimal filtering strategy difficult to guarantee.
\textbf{Practical Limitations:} Integrating an RL agent significantly increases the complexity and computational cost of the training pipeline compared to standard KGE models. Hyperparameter tuning for the RL component can also be challenging and time-consuming.
\textbf{Comparison to Alternatives:} This approach represents a proactive, *in-training* mechanism for robustness, directly cleaning the input data stream. This contrasts with post-hoc calibration [tabacof2019] or reactive negative sampling strategies, offering a more fundamental solution to input data quality issues.
\textbf{Implications:} This work signals a shift towards more adaptive and intelligent training paradigms that actively manage data quality, rather than passively accepting noisy inputs.

\textbf{Weighted Training for Imbalanced Data:}
[zhang2023] addresses the pervasive data imbalance issue in KGs, where entities and relations follow a long-tail distribution.
\textbf{Problem Solved:} Existing KGE methods often assign equal weights to all entities and relations during training, leading to unreliable representations for infrequent (long-tail) elements due to insufficient training exposure.
\textbf{Core Innovation:} The proposed method, WeightE, employs a bilevel optimization scheme. The inner level focuses on learning reliable entity and relation embeddings, while the outer level dynamically assigns differential weights to entities and relations. It prioritizes infrequent entities and relations by endowing them with higher weights, ensuring they receive adequate training.
\textbf{Conditions for Success:} WeightE is particularly beneficial for KGs with pronounced long-tail distributions. Its weighting technique is designed to be general and flexible, applicable to a number of existing KGE models.
\textbf{Theoretical Limitations:} Bilevel optimization, while powerful, can be computationally more demanding than single-level optimization. The convergence properties and global optimality guarantees can be complex to establish, and the optimal weighting strategy might vary significantly across different KGs.
\textbf{Practical Limitations:} The added complexity of bilevel optimization can increase training time and resource consumption. Careful tuning of the outer-level optimization parameters is crucial for effective performance.
\textbf{Comparison to Alternatives:} WeightE tackles a different aspect of robustness: ensuring fair and effective learning across the entire spectrum of data frequency. This complements noise filtering by addressing data distribution issues and differs from calibration, which focuses on output reliability. It is a general training optimization technique that can be integrated with various KGE architectures.
\textbf{Implications:} This highlights the importance of considering data distribution characteristics in KGE training, moving beyond uniform treatment to ensure high-quality representations for all knowledge elements, which is vital for comprehensive knowledge reasoning.

\subsubsection*{The Crucial Role of Negative Sampling}
The training of KGE models typically relies on contrastive learning, which necessitates discriminating between positive (true) and negative (false) triples. Since KGs predominantly store only positive facts, the generation of effective negative samples is a critical and non-trivial aspect of the training process, profoundly impacting model accuracy and robustness. The evolution of negative sampling (NS) methods reflects a continuous effort to overcome the challenges of generating high-quality, informative negative examples.

\begin{table}[htbp]
    \centering
    \caption{Comparative Framework for Negative Sampling Strategies}
    \label{tab:negative_sampling_comparison}
    \begin{tabularx}{\textwidth}{|l|X|X|X|X|}
        \hline
        \textbf{Feature} & \textbf{Confidence-Aware NS [shan2018]} & \textbf{NSCaching [zhang2018]} & \textbf{Non-Sampling (NS-KGE) [li2021]} & \textbf{Modality-Aware NS [zhang2023_modality]} \\
        \hline
        \textbf{Problem Solved} & Training in noisy KGs, avoiding zero loss/false detection from uniform NS. & Efficiently finding "hard" negatives, avoiding complexity of GAN-based methods. & Instability and uncertainty inherent in all sampling procedures, considering all negatives. & Adapting NS for multi-modal KGEs, aligning structural and visual embeddings. \\
        \hline
        \textbf{Core Innovation} & Incorporating negative triple confidence to guide sampling in noisy environments. & Cache-based tracking and sampling of "hard" negative triplets. & Mathematical derivation to consider all negative instances without explicit sampling. & Aligning structural and visual embeddings during negative sample generation. \\
        \hline
        \textbf{Conditions for Success} & Noisy KGs where uniform sampling leads to issues; requires confidence estimation. & Scenarios where "hard" negatives are crucial for learning but are rare. & KGE models with square-loss or convertible loss functions; requires efficient mathematical optimization. & Multi-modal KGs where diverse data modalities need semantic alignment. \\
        \hline
        \textbf{Theoretical Limitations} & Relies on accurate confidence estimation, which can be challenging and noisy itself. & Still a heuristic sampling method; cache management introduces its own complexities. & Limited to specific loss functions; complexity reduction is often an approximation or specific to certain model forms. & Increased complexity due to modality fusion; optimal cross-modal alignment is difficult to define. \\
        \hline
        \textbf{Practical Limitations} & Adds complexity to the confidence estimation process and its integration into training. & Requires careful management of cache size and balancing exploration-exploitation. & Can have high initial computational cost and memory footprint without clever optimization. & Requires multi-modal data availability; complex fusion and sampling design. \\
        \hline
        \textbf{Comparison to Alternatives} & More robust than uniform NS in noisy settings; less complex than GAN-based methods. & Simpler and more efficient than GAN-based NS while achieving similar benefits of hard negative mining. & Offers more stable and accurate performance by removing sampling uncertainty, but with higher initial complexity. & Extends NS to a new domain; addresses multi-modal specific challenges, unlike purely structural NS. \\
        \hline
    \end{tabularx}
\end{table}

\textbf{Evolution of Negative Sampling Strategies:}
Early KGE models often relied on uniform random negative sampling, which, while simple, frequently generated "easy" negatives that provided little learning signal or "false negatives" that were actually true but unobserved. This led to issues like vanishing gradients or incorrect learning.

\textbf{Heuristic-based Negative Sampling:}
Mid-period research introduced more sophisticated heuristic-based sampling. [shan2018] proposed a \textit{confidence-aware negative sampling method} to address the problem of noisy KGs.
\textbf{Core Innovation:} It introduces the concept of negative triple confidence, allowing the training process to better handle noisy environments where uniform sampling might lead to zero loss or false detection. By assigning confidence scores to negative samples, the model can learn more effectively from them.
\textbf{Comparison:} This method improves upon uniform sampling by making the negative generation process more intelligent and robust to input noise, complementing the noise filtering of [zhang2021] by focusing on the negative side of the learning objective.

Building on the observation that "hard" negative triplets (those that are plausible but false) are crucial for effective learning, [zhang2018] introduced \textit{NSCaching}.
\textbf{Core Innovation:} NSCaching efficiently tracks and samples these hard negatives using a cache. This approach aims to distill the benefits of more complex Generative Adversarial Network (GAN)-based negative sampling methods into a simpler, more efficient framework, avoiding the additional parameters and training complexities associated with GANs.
\textbf{Comparison:} NSCaching offers a practical and efficient alternative to GAN-based methods, achieving comparable performance gains by focusing on the most informative negative samples without the overhead of training a generative model.

\textbf{Non-Sampling Approaches and Comprehensive Reviews:}
A more radical departure from traditional negative sampling is presented by [li2021] with \textit{Efficient Non-Sampling Knowledge Graph Embedding (NS-KGE)}.
\textbf{Core Innovation:} This method proposes to avoid negative sampling entirely by considering all negative instances in the KG for model learning. It leverages mathematical derivations to reduce the computational complexity of this "non-sampling" loss function, aiming for more stable and accurate performance by removing the inherent uncertainty and suboptimality of sampling.
\textbf{Comparison:} NS-KGE fundamentally challenges the assumption that sampling is necessary, offering a theoretically more sound approach by considering the full negative space. This contrasts sharply with all sampling-based methods, including those of [shan2018] and [zhang2018], which are inherently heuristic. While it promises greater stability, its applicability is currently limited to square-loss based KGE models or those whose loss functions can be converted to a square loss.

As KGE models integrate more diverse data, negative sampling strategies also need adaptation. [zhang2023_modality] introduces \textit{Modality-Aware Negative Sampling} for multi-modal KGE.
\textbf{Core Innovation:} This method recognizes that when incorporating multi-modal information (e.g., text, images), negative sampling must align structural and visual embeddings for entities. It ensures that negative samples are meaningful across all modalities, leading to better-aligned and more robust multi-modal embeddings.
\textbf{Comparison:} This extends the negative sampling problem to a new dimension, highlighting that as KGE models become more complex (as discussed in Subsection 4.3), their training components must also evolve to handle the increased complexity.

The critical importance of negative sampling is further underscored by comprehensive reviews. [qian2021] and [madushanka2024] provide systematic categorizations and analyses of existing negative sampling approaches.
\textbf{Implications:} These surveys consolidate knowledge, categorize methods into distinct families (e.g., static, dynamic, custom cluster-based), and identify open research questions, guiding future advancements in this vital area. They highlight that the "true" negative distribution remains unknown, making the design of optimal NS methods an ongoing heuristic challenge and a fundamental theoretical limitation that no current method fully resolves.

\textbf{Patterns and Tensions in Training Optimization:}
The research in robustness and training optimization reveals several key patterns and tensions. A recurring trade-off exists between the \textit{simplicity/efficiency} of a training method and its \textit{robustness/accuracy}. For instance, while uniform negative sampling is simple, it often yields suboptimal results in noisy or imbalanced settings, necessitating more complex solutions like confidence-aware sampling [shan2018] or NSCaching [zhang2018]. The emergence of non-sampling approaches [li2021] represents a paradigm shift, challenging the long-standing assumption that negative sampling is an unavoidable necessity, but this comes with its own computational complexities that require clever mathematical derivations.

Another tension lies between \textit{proactive} and \textit{reactive} robustness. Methods like RL-based noise filtering [zhang2021] proactively clean the input data during training, while probability calibration [tabacof2019] reactively adjusts the model's output scores. Both contribute to overall trustworthiness but address different stages of the KGE pipeline. The field implicitly assumes that the benefits of these complex optimizations outweigh their increased computational cost and implementation complexity, a trade-off that is not always explicitly quantified across diverse application scenarios.

Collectively, these advancements in robustness and training optimization are vital for ensuring that KGE models learn accurate representations even from noisy or incomplete data, leading to more reliable predictions and better generalization. They bridge the gap between theoretically powerful KGE architectures and their practical deployment in real-world, imperfect knowledge environments.
\subsection{Evaluation, Benchmarking, and Reproducibility}
\label{sec:6_3_evaluation,_benchmarking,__and__reproducibility}

While the preceding subsections have explored the evolution of Knowledge Graph Embedding (KGE) models towards greater expressiveness (Section 2), architectural sophistication (Section 3), and practical considerations like efficiency and robustness (Subsections 6.1 and 6.2), the scientific integrity and real-world trustworthiness of these advancements critically depend on rigorous evaluation, standardized benchmarking, and the reproducibility of reported results. This subsection highlights the field's introspection into these crucial aspects, discussing the development of unified frameworks, large-scale comparative studies that expose biases and failures, and analyses of hyperparameter effects, all aimed at moving KGE research towards higher standards of empirical validation and transparency.

The rapid proliferation of KGE models, each often with unique implementations, training procedures, and evaluation protocols, created a significant challenge for fair comparison and hindered cumulative scientific progress. This heterogeneity led to a "reproducibility crisis," where published results were often difficult or impossible to replicate, undermining the reliability of reported performance gains.

\subsubsection*{Unified Frameworks for Reproducible Research}
To address the burgeoning reproducibility crisis, the community recognized the need for standardized tools and environments. This led to the development of unified libraries designed to facilitate consistent experimentation.

\textbf{LibKGE [broscheit2020]:}
\begin{itemize}
    \item \textbf{Context:} The lack of a standardized, configurable, and open-source environment for KGE research hindered reproducible experimentation and systematic comparative studies.
    \item \textbf{Core Innovation:} [broscheit2020] introduced LibKGE, an open-source PyTorch-based library for training, hyperparameter optimization (HPO), and evaluation of KGE models. Its key mechanisms include decoupled components, comprehensive logging, and the ability to fully reproduce any experiment from a single configuration file. This design allows researchers to mix and match various training methods, model architectures, and evaluation strategies.
    \item \textbf{Conditions for Success:} LibKGE succeeds by providing a flexible and efficient platform for KGE experimentation, particularly beneficial for researchers familiar with PyTorch. It is designed to be as efficient as possible within the Python/Numpy/PyTorch ecosystem.
    \item \textbf{Theoretical Limitations:} While LibKGE offers a robust framework, it is fundamentally a tool. It does not inherently solve the underlying challenges of hyperparameter sensitivity or evaluation biases; rather, it provides the infrastructure to *study* and *mitigate* these issues. Its effectiveness is contingent on the user's diligence in configuration and analysis.
    \item \textbf{Practical Limitations:} Despite its configurability, extensive hyperparameter tuning within LibKGE can still be computationally intensive, as highlighted by [lloyd2022]. Users must also invest time in understanding its architecture to fully leverage its capabilities.
    \item \textbf{Comparison to Alternatives:} Compared to ad-hoc, custom implementations prevalent in early KGE research, LibKGE offers a significant leap in standardization and efficiency. It serves as a foundational platform for systematic research, complementing large-scale empirical studies like [ali2020] by providing the means to conduct them.
    \item \textbf{Implications:} LibKGE represents a crucial step towards fostering a more rigorous and transparent research environment in KGE, enabling researchers to build upon each other's work with greater confidence.
\end{itemize}

\subsubsection*{Large-Scale Benchmarking and Uncovering Evaluation Biases}
Beyond providing tools, empirical studies were necessary to quantify the extent of the reproducibility problem and identify biases in common evaluation practices.

\textbf{Bringing Light Into the Dark [ali2020]:}
\begin{itemize}
    \item \textbf{Context:} Motivated by the widespread difficulty in reproducing published KGE results, this work aimed to empirically assess the state of reproducibility.
    \item \textbf{Problem Solved:} The paper addressed the problem of heterogeneous implementations and inconsistent evaluation protocols that made fair and thorough comparisons of KGE models challenging.
    \item \textbf{Core Innovation:} [ali2020] conducted a massive re-implementation and evaluation effort, re-implementing 21 KGE models within the PyKEEN software package (a unified framework similar to LibKGE). They performed a large-scale benchmarking study involving thousands of experiments and 24,804 GPU hours of computation.
    \item \textbf{Evidence:} The study explicitly detailed which results could be reproduced with reported hyperparameters, which required alternate hyperparameters, and which could not be reproduced at all. It provided concrete insights into best practices and optimal configurations, demonstrating that the combination of model architecture, training approach, loss function, and explicit modeling of inverse relations is crucial for performance. They showed that several architectures could achieve state-of-the-art results when carefully configured.
    \item \textbf{Limitations:} Re-implementations, despite best efforts, might not perfectly capture the nuances of original authors' intentions. The sheer scale of HPO means that finding a global optimum is still a heuristic search. The study is retrospective, identifying problems that have already impacted prior research.
    \item \textbf{Comparison to Alternatives:} Unlike individual model papers, this is a critical meta-study that systematically evaluates a broad spectrum of existing models. It provides empirical evidence for the necessity of unified frameworks like LibKGE and directly informs the community about the pitfalls of current research practices.
    \item \textbf{Implications:} This seminal work exposed widespread reproducibility failures, prompting a critical self-reflection within the KGE community and advocating for higher standards of reporting and experimentation. It underscored the paramount importance of hyperparameter tuning and detailed training configurations.
\end{itemize}

\textbf{Knowledge Graph Embedding for Link Prediction [rossi2020]:}
\begin{itemize}
    \item \textbf{Context:} Despite the rapid growth of KGE literature, insufficient attention had been paid to the effect of design choices and potential biases in standard evaluation methodologies.
    \item \textbf{Problem Solved:} This paper critically examined the common practice of reporting accuracy by aggregating over all test facts, which can be misleading due to the vastly uneven representation of entities, allowing models to achieve good scores by focusing only on frequently occurring entities.
    \item \textbf{Core Innovation:} [rossi2020] provided a comprehensive comparison of 18 state-of-the-art KGE methods, extending the dimensions of analysis beyond commonly reported metrics. Their detailed analysis over popular benchmarks systematically exposed biases in standard evaluation practices, particularly the over-representation of certain entities.
    \item \textbf{Evidence:} The study presented detailed experimental results demonstrating how standard metrics could be skewed, highlighting that performance might not reflect true generalization across the entire KG.
    \item \textbf{Limitations:} While identifying critical biases, the paper primarily critiques existing practices rather than proposing a universally accepted, unbiased metric. Its findings are based on existing benchmarks, which themselves might have inherent structural biases.
    \item \textbf{Comparison to Alternatives:} Similar to [ali2020] in its comparative nature, [rossi2020] distinguishes itself by focusing more on the *validity* and *fairness* of evaluation metrics themselves, rather than just the reproducibility of reported numbers. It directly questions the interpretability of aggregated performance scores.
    \item \textbf{Implications:} This work issued a crucial call for more nuanced and less biased evaluation methodologies, urging researchers to move beyond simple aggregated metrics to understand model performance across different parts of the knowledge graph, especially for long-tail entities and relations.
\end{itemize}

\subsubsection*{Assessing the Effects of Hyperparameters}
The findings from large-scale benchmarking efforts underscored the critical role of hyperparameters. Understanding their impact became essential for efficient research and reliable model deployment.

\textbf{Assessing the effects of hyperparameters on knowledge graph embedding quality [lloyd2022]:}
\begin{itemize}
    \item \textbf{Context:} Hyperparameter optimization is a computationally expensive and time-consuming process, as implicitly highlighted by the extensive HPO required in [ali2020]. Not all hyperparameters contribute equally to model quality.
    \item \textbf{Problem Solved:} This paper addressed the lack of understanding regarding the relative importance of hyperparameters on KGE quality, which leads to inefficient HPO. It also identified and mitigated data leakage issues in common benchmarks.
    \item \textbf{Core Innovation:} [lloyd2022] employed Sobol sensitivity analysis to quantify the importance of different hyperparameters by performing thousands of embedding trials. This allowed them to determine which hyperparameters significantly impact embedding quality variance. Crucially, they identified several relations in the UMLS knowledge graph that cause data leakage via inverse relations and derived UMLS-43, a leakage-robust variant of that graph.
    \item \textbf{Evidence:} The study found substantial variability in hyperparameter sensitivities across different knowledge graphs, suggesting that optimal configurations are often dataset-dependent. Their analysis revealed that some hyperparameters could be eliminated from search without significantly impacting quality, leading to more efficient HPO. The identification and correction of data leakage in UMLS-43 provided a more reliable benchmark.
    \item \textbf{Limitations:} Sobol sensitivity analysis, while powerful, can still be computationally intensive for very large hyperparameter spaces. The findings on hyperparameter importance are specific to the KGE models and datasets studied and may not universally generalize.
    \item \textbf{Comparison to Alternatives:} This paper provides a *mechanistic and quantitative understanding* of *why* reproducibility is challenging, directly complementing the empirical observations of [ali2020]. It moves beyond simply identifying reproducibility failures to providing tools and insights for more efficient and robust hyperparameter tuning. It also addresses a fundamental issue of dataset quality, which [rossi2020] touched upon regarding evaluation biases.
    \item \textbf{Implications:} This work guides more efficient hyperparameter optimization by allowing researchers to focus on the most impactful parameters. More significantly, it highlights the critical need for meticulous dataset curation and validation to prevent data leakage, which can artificially inflate reported performance and undermine scientific claims.
\end{itemize}

\begin{table}[htbp]
    \centering
    \caption{Comparative Framework for Key Contributions to KGE Evaluation and Reproducibility}
    \label{tab:evaluation_reproducibility_comparison}
    \begin{tabularx}{\textwidth}{|l|X|X|X|X|}
        \hline
        \textbf{Aspect} & \textbf{LibKGE [broscheit2020]} & \textbf{Bringing Light Into the Dark [ali2020]} & \textbf{KGE for Link Prediction [rossi2020]} & \textbf{Assessing Hyperparameters [lloyd2022]} \\
        \hline
        \textbf{Primary Goal} & Enable reproducible KGE research via a unified library. & Empirically assess reproducibility and benchmark models under unified conditions. & Comprehensive comparison of methods, exposing evaluation biases. & Quantify hyperparameter importance and identify dataset issues (leakage). \\
        \hline
        \textbf{Core Mechanism} & PyTorch library with configurable components, logging, single config file. & Re-implementation of 21 models in PyKEEN, large-scale HPO (24,804 GPU hours). & Extensive experimental comparison of 18 SOTA methods, detailed analysis of design choices. & Sobol sensitivity analysis on thousands of trials; identified and corrected data leakage. \\
        \hline
        \textbf{Key Finding(s)} & Provides a flexible framework for KGE experimentation. & Widespread reproducibility failures; HPs are crucial; careful configuration matters more than architecture alone. & Standard evaluation metrics biased by entity over-representation, leading to misleading performance. & HP sensitivity varies greatly by KG; identified data leakage in UMLS-43, proposing a robust variant. \\
        \hline
        \textbf{Contribution Type} & \textbf{Tool/Framework} for research. & \textbf{Empirical Study/Benchmarking} of existing models. & \textbf{Critical Analysis/Benchmarking} of evaluation practices. & \textbf{Methodological Analysis} of HPO and dataset quality. \\
        \hline
        \textbf{Impact on Field} & Fosters reproducibility, facilitates systematic studies. & Exposed reproducibility crisis, pushed for higher standards, provided best practices. & Called for more nuanced, less biased evaluation metrics and interpretation. & Improved HPO efficiency, highlighted dataset integrity issues for reliable benchmarking. \\
        \hline
    \end{tabularx}
\end{table}

\subsubsection*{Patterns, Tensions, and Unstated Assumptions}
The collective body of work in evaluation, benchmarking, and reproducibility reveals several critical patterns and tensions within KGE research. A significant pattern is the field's maturation from solely developing new models to critically examining *how* research is conducted and evaluated. This shift towards meta-research is a hallmark of a maturing scientific discipline.

A recurring tension exists between \textit{achieving state-of-the-art performance} and ensuring \textit{reproducibility and transparency}. As [ali2020] empirically demonstrated, many published results were difficult to reproduce, often due to undisclosed or highly specific hyperparameter configurations. This complexity is exacerbated by the increasingly sophisticated deep learning architectures discussed in Section 3. The efficiency concerns raised in Subsection 6.1 are directly intertwined with this, as extensive hyperparameter tuning, if not managed efficiently, can be prohibitively costly [lloyd2022].

Another tension lies between \textit{generalizability} and \textit{dataset-specific tuning}. The optimal hyperparameters and even the presence of evaluation biases are often highly dependent on the characteristics of the specific knowledge graph [lloyd2022, rossi2020]. This implies that a model performing exceptionally well on one benchmark might not generalize effectively to another, challenging the notion of universally superior KGE models.

Furthermore, these papers systematically question several unstated assumptions prevalent in earlier KGE research. Many papers implicitly assumed that standard metrics like Mean Reciprocal Rank (MRR) and Hits@K on common benchmarks were sufficient indicators of real-world utility. However, [rossi2020] directly challenged this by showing how these metrics can be biased by entity over-representation. Similarly, the implicit assumption that published results are easily reproducible was empirically disproven by [ali2020], highlighting the need for detailed reporting and open-source implementations. Even the integrity of benchmark datasets, often taken for granted, was questioned by [lloyd2022], revealing data leakage that could artificially inflate performance. The choice of negative sampling strategies (as discussed in Subsection 6.2) also represents a critical hyperparameter and design choice that profoundly impacts reproducibility and model quality, further emphasizing the interconnectedness of these practical considerations.

\subsubsection*{Conclusion}
The emphasis on evaluation, benchmarking, and reproducibility marks a crucial phase in KGE research, moving the field towards higher standards of empirical validation and transparency. The development of unified frameworks like LibKGE [broscheit2020], coupled with large-scale comparative studies [ali2020, rossi2020] and detailed analyses of hyperparameter effects [lloyd2022], has exposed significant challenges in reproducibility and evaluation fairness. These efforts are indispensable for ensuring that KGE models are not only powerful but also reliable, comparable, and ultimately trustworthy for deployment in critical applications. The ongoing trajectory demands continued vigilance in experimental design, meticulous reporting, and the development of more robust, unbiased evaluation methodologies to foster reliable scientific progress.


### Applications and Real-World Impact of KGE

\label{sec:applications__and__real-world_impact_of_kge}

\section{Applications and Real-World Impact of KGE}
\label{sec:applications_and_real-world_impact_of_kge}

Having thoroughly explored the foundational models, advanced deep learning architectures, methods for enriching embeddings with auxiliary information, and critical practical considerations concerning efficiency, robustness, and evaluation in previous sections, this part of the review shifts its focus from the "how" to the "what for." This section showcases the profound and diverse real-world impact of Knowledge Graph Embedding (KGE) across a myriad of artificial intelligence applications. It moves beyond theoretical advancements and model intricacies to demonstrate how KGE models are actively leveraged to solve complex, real-world problems, transforming symbolic knowledge into actionable insights for intelligent systems [layer_1, community_1, community_3].

The utility of KGE is most evident in its ability to enhance core AI tasks that rely on structured knowledge. We will delve into how KGE underpins crucial applications such as link prediction and knowledge graph completion, where embeddings facilitate the inference of missing facts and the enrichment of existing knowledge bases [community_0, community_2]. Furthermore, the section will explore entity alignment, a vital task for integrating disparate knowledge sources by identifying equivalent entities across different graphs [community_5, d899e434a7f2eecf33a90053df84cf32842fbca9]. Beyond these foundational tasks, KGE significantly boosts the performance of more interactive systems like question answering, enabling machines to comprehend and respond to natural language queries over vast knowledge repositories, and recommender systems, providing personalized and context-aware suggestions [a6a735f8e218f772e5b9dac411fa4abea87fdb9c]. Finally, we highlight domain-specific applications, illustrating the transformative potential of KGE in specialized fields such as biology, medicine, and intellectual property analysis, often emphasizing the growing demand for explainable and interpretable KGE solutions in high-stakes environments [community_2, community_6]. Collectively, this section underscores the tangible benefits and broad applicability of embedding techniques, positioning KGE as a cornerstone technology in modern AI.

\subsection{Link Prediction and Knowledge Graph Completion}
\label{sec:7_1_link_prediction__and__knowledge_graph_completion}

The fundamental applications of Knowledge Graph Embedding (KGE) are link prediction (LP) and knowledge graph completion (KGC), which involve inferring missing facts within the graph structure. These tasks are paramount for enhancing the completeness and utility of knowledge graphs, making them more robust and informative for downstream AI systems [dai2020, ge2023]. LP typically focuses on predicting a missing head entity ($h$), relation ($r$), or tail entity ($t$) in an incomplete triple $(h, r, t)$, while KGC broadly encompasses filling in these missing links to expand the knowledge base. The continuous effort in this domain aims to improve accuracy and handle complex relational patterns, forming the bedrock for many other applications.

\subsubsection*{Foundational Geometric Approaches}
Early KGE models primarily leveraged geometric transformations to represent entities and relations, offering a computationally efficient way to model relational patterns for LP/KGC.
\begin{itemize}
    \item \textbf{Translational Models}: Models like \textsf{TransE} (as a baseline for [wang2014, ji2015]) represent relations as translations in the embedding space, where $h+r \approx t$. While simple and efficient, \textsf{TransE} struggles with complex relation patterns such as one-to-many, many-to-one, and many-to-many. \textsf{TransH} [wang2014] addressed this by projecting entities onto a relation-specific hyperplane before translation, allowing a single entity to have different representations for different relations. This significantly improved handling of complex mappings with comparable efficiency to \textsf{TransE}. Building on this, \textsf{TransD} [ji2015] further refined the approach by using dynamic mapping matrices constructed from entity and relation vectors, enabling more fine-grained, entity-specific projections and reducing parameters compared to prior projection-based methods. \textsf{TransA} [jia2015, jia2017] introduced an adaptive margin for the loss function, allowing for more flexible and context-aware embeddings, which was a crucial step towards locally adaptive learning.
    \item \textbf{Rotational and Compound Models}: A significant advancement came with models like \textsf{RotatE} [sun2018], which defined relations as rotations in complex vector spaces. This innovation inherently allowed it to model and infer complex relation patterns like symmetry, antisymmetry, inversion, and composition more effectively than translational models. \textsf{Rotate3D} [gao2020] extended this to three-dimensional space, leveraging the non-commutative property of 3D rotations for multi-hop reasoning. More broadly, \textsf{CompoundE} [ge2022] and \textsf{CompoundE3D} [ge2023] generalized these concepts by combining translation, rotation, and scaling operations, demonstrating that a richer set of cascaded geometric manipulations can lead to superior performance by capturing diverse underlying characteristics of a KG. \textsf{HousE} [li2022] introduced Householder transformations (rotations and projections) to achieve superior capacity for modeling relation patterns and mapping properties simultaneously, generalizing existing rotation-based models to high-dimensional spaces. \textsf{MQuinE} [liu2024] identified and resolved a "Z-paradox" deficiency in some popular KGE models, ensuring stronger expressiveness for various relation patterns including symmetric/asymmetric, inverse, and compositional relations.
\end{itemize}

\textbf{Comparative Framework: Geometric KGE Models for LP/KGC}
\begin{tabular}{|p{0.15\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.15\textwidth}|}
\hline
\textbf{Method Family} & \textbf{Core Innovation} & \textbf{Problem Solved (LP/KGC)} & \textbf{Conditions for Success} & \textbf{Limitations} \\
\hline
\textsf{Translational} (\textsf{TransH} [wang2014], \textsf{TransD} [ji2015]) & Project entities onto relation-specific hyperplanes/dynamic matrices. & Better handling of 1-to-N, N-to-1 relations. & Sufficient training data for relation-specific projections. & Still struggles with complex logical patterns (composition, inversion) compared to rotational models. \\
\hline
\textsf{Rotational} (\textsf{RotatE} [sun2018], \textsf{Rotate3D} [gao2020]) & Relations as rotations in complex/3D space. & Captures symmetry, antisymmetry, inversion, composition. & Effective for KGs with rich logical patterns. & May not fully exploit theoretical underpinnings of composition (addressed by \textsf{HolmE} [zheng2024]). \\
\hline
\textsf{Compound} (\textsf{CompoundE} [ge2022], \textsf{HousE} [li2022]) & Combination of translation, rotation, scaling; Householder transforms. & Generalized modeling of diverse relation patterns and mapping properties. & KGs with heterogeneous relation types and complex structures. & Increased computational complexity and parameter count. \\
\hline
\end{tabular}

The evolution from simple translational models to complex rotational and compound operations highlights a continuous effort to enhance the expressiveness of KGEs for LP/KGC. While \textsf{TransH} [wang2014] and \textsf{TransD} [ji2015] significantly improved upon basic translation by addressing mapping properties, they often fell short in capturing complex logical patterns like composition. This gap was notably addressed by \textsf{RotatE} [sun2018], which demonstrated superior performance in modeling such patterns through complex space rotations. However, even \textsf{RotatE} might not fully exploit the theoretical underpinnings of composition, a limitation that \textsf{HolmE} [zheng2024] explicitly aims to overcome by ensuring its relation embedding space is closed under composition, offering stronger theoretical guarantees for handling under-represented patterns. This progression exemplifies the field's shift towards more mathematically robust and expressive models. A common practical limitation across these geometric models is the trade-off between increased expressiveness and computational cost, though many strive for competitive efficiency.

\subsubsection*{Deep Learning Architectures for LP/KGC}
The advent of deep learning brought a paradigm shift, enabling KGE models to learn more intricate and context-aware representations for LP/KGC.
\begin{itemize}
    \item \textbf{Convolutional Neural Networks (CNNs)}: CNNs have been adapted to extract local features and model interactions between entity and relation embeddings. \textsf{AcrE} [ren2020] used atrous convolutions and residual learning to increase feature interactions and address gradient issues, achieving high parameter efficiency. \textsf{ReInceptionE} [xie2020] further explored Inception networks and relation-aware attention to capture joint local-global structural information, enhancing interactions between head and relation embeddings. More recently, \textsf{CNN-ECFA} [hu2024] proposed an entity-specific common feature aggregation strategy, demonstrating superior performance by leveraging CNNs for richer feature extraction. \textsf{SEConv} [yang2025] (designed for healthcare prediction) also employs multi-layer CNNs for deeper structural features, highlighting their utility in specialized domains. These models excel at learning complex, non-linear feature interactions, often outperforming simpler geometric models in accuracy.
    \item \textbf{Graph Neural Networks (GNNs) and Attention Mechanisms}: GNNs, through message passing and aggregation, are naturally suited to capture structural information and neighborhood context. \textsf{DisenKGAT} [wu2021] introduced a disentangled graph attention network to learn diverse component representations, enhancing independence and adaptability to various score functions. \textsf{GAATs} [wang2020] incorporated an attenuated attention mechanism to assign different weights to relation paths and acquire information from neighborhoods, leading to more complete mining of triple features. For inductive KGE, \textsf{Logic Attention Based Neighborhood Aggregation} [wang2018] used a Logic Attention Network (LAN) to aggregate information from an entity's neighbors, allowing for embeddings of unseen entities.
    \item \textbf{Transformer-based KGE Models}: Transformers leverage self-attention mechanisms to capture long-range dependencies and contextualized representations. \textsf{CoKE} [wang2019] treated KGs as sequences of entities and relations, using a Transformer encoder to obtain dynamic, contextualized representations. \textsf{Knowformer} [li2023] addressed the order-invariance issue of standard Transformers by incorporating relational compositions and position-aware mechanisms, proving effective for both LP and entity alignment. \textsf{TGformer} [shi2025] proposed a general graph Transformer framework that integrates triplet-level and graph-level structural features, boosting the model's ability to understand entities and relations in different contexts.
\end{itemize}

\textbf{Comparative Framework: Deep Learning KGE Models for LP/KGC}
\begin{tabular}{|p{0.15\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.15\textwidth}|}
\hline
\textbf{Method Family} & \textbf{Core Innovation} & \textbf{Problem Solved (LP/KGC)} & \textbf{Conditions for Success} & \textbf{Limitations} \\
\hline
\textsf{CNN-based} (\textsf{AcrE} [ren2020], \textsf{ReInceptionE} [xie2020]) & Multi-scale filters, residual learning, inception networks for feature extraction. & Captures intricate non-linear feature interactions for complex relation types (1-to-N, N-to-1). & Requires sufficient data to learn complex features; benefits from diverse relation types. & Increased computational complexity and parameter count compared to simpler geometric models. \\
\hline
\textsf{GNN-based} (\textsf{DisenKGAT} [wu2021], \textsf{GAATs} [wang2020]) & Message passing, attention mechanisms, disentangled representations. & Leverages graph topology and neighborhood context for richer, context-dependent embeddings. & KGs with rich local structural information and diverse relational paths. & Can be computationally intensive for very large graphs; disentanglement adds complexity. \\
\hline
\textsf{Transformer-based} (\textsf{CoKE} [wang2019], \textsf{Knowformer} [li2023]) & Self-attention, contextualized representations, position-aware mechanisms. & Captures long-range dependencies and contextual meanings of entities/relations. & Benefits from KGs that can be linearized or where global context is crucial. & High computational cost for self-attention; order-invariance issue needs explicit handling (\textsf{Knowformer}). \\
\hline
\end{tabular}

Deep learning architectures address the limitations of foundational geometric models by automatically extracting complex features and capturing intricate structural and contextual information. For instance, while \textsf{RotatE} [sun2018] effectively models compositional relations geometrically, \textsf{CoKE} [wang2019] offers a more flexible, data-driven approach by using Transformers to learn contextualized representations, which can capture more nuanced semantic dependencies beyond simple geometric transformations. This exemplifies the field's shift towards leveraging powerful neural networks for enhanced expressiveness, albeit often at the cost of increased computational resources and reduced interpretability. The tension between model expressiveness and computational efficiency is particularly pronounced here, with models like \textsf{AcrE} [ren2020] attempting to balance this through parameter-efficient designs.

\subsubsection*{Leveraging Auxiliary Information and Rules for LP/KGC}
Beyond structural patterns, integrating auxiliary information and logical rules significantly enhances KGE for LP/KGC, especially in incomplete or noisy KGs.
\begin{itemize}
    \item \textbf{Auxiliary Information}: Models like \textsf{TransET} [wang2021] and \textsf{TaKE} [he2023] incorporate entity type features to learn more semantic representations, guiding the embedding process and improving KG completion. \textsf{TaKE} specifically provides a universal framework that can augment any traditional KGE model. \textsf{HINGE} [rosso2020] moved "beyond triplets" by directly learning from hyper-relational facts (triplets with associated key-value pairs), capturing richer data semantics that traditional triplet-based models overlook. \textsf{GeoEntity-type constrained KGE} [hu2024] applied this principle to a specific domain, using geoentity types as constraints to predict natural-language spatial relations more accurately. These methods leverage well-structured prior knowledge to produce more semantic and robust embeddings.
    \item \textbf{Rule-based and Constraint-driven KGE}: This family injects explicit logical rules or constraints to ensure semantic consistency and improve reasoning. \textsf{Semantically Smooth KGE} [guo2015] enforced a "semantically smooth" embedding space where entities of the same category lie close, using manifold learning as regularization. \textsf{RUGE} [guo2017] introduced an iterative guidance mechanism from soft rules, allowing KGE models to learn simultaneously from labeled triples, unlabeled triples, and automatically extracted soft rules. This was a crucial step beyond one-time rule injection. \textsf{Improving Knowledge Graph Embedding Using Simple Constraints} [ding2018] showed that even basic constraints like non-negativity and approximate entailment can significantly improve interpretability and structure. More recently, \textsf{Knowledge Graph Embedding Preserving Soft Logical Regularity} [guo2020] imposed soft rule constraints directly on relation representations, improving scalability. \textsf{RulE} [tang2022] learned rule embeddings jointly with entity and relation embeddings, enabling soft logical inference and using rule embeddings to regularize and enrich the overall embedding space.
\end{itemize}
\textbf{Comparative Framework: Auxiliary Information & Rule-based KGE for LP/KGC}
\begin{tabular}{|p{0.15\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.15\textwidth}|}
\hline
\textbf{Method Family} & \textbf{Core Innovation} & \textbf{Problem Solved (LP/KGC)} & \textbf{Conditions for Success} & \textbf{Limitations} \\
\hline
\textsf{Type/Attribute Augmented} (\textsf{TransET} [wang2021], \textsf{TaKE} [he2023]) & Integrates entity types/attributes into embedding process. & Enhances semantic understanding, especially for incomplete KGs; improves robustness to noise. & Requires availability and quality of auxiliary information. & Benefits diminish if auxiliary data is sparse or noisy. \\
\hline
\textsf{Rule-based/Constraint-driven} (\textsf{RUGE} [guo2017], \textsf{RulE} [tang2022]) & Incorporates logical rules (soft/hard) as regularization or joint learning. & Enforces consistency, improves reasoning, enhances interpretability. & Availability of high-quality rules (manual or extracted). & Rule extraction can be complex; balancing rule adherence with flexibility for exceptions. \\
\hline
\end{tabular}

These approaches address a critical assumption in many purely structural KGE models: that all necessary information is contained within the triples. \textsf{TransET} [wang2021] and \textsf{TaKE} [he2023] demonstrate that leveraging entity types provides crucial semantic guidance, leading to more discriminative embeddings. This complements the structural learning of models like \textsf{TransD} [ji2015] by adding an orthogonal dimension of knowledge. Similarly, rule-based methods like \textsf{RUGE} [guo2017] and \textsf{RulE} [tang2022] address the limitation of purely data-driven models by injecting logical consistency, which is vital for robust reasoning in LP/KGC. While \textsf{TransH} [wang2014] improves mapping properties, it doesn't inherently enforce logical consistency; rule-based methods fill this gap. A practical limitation is the reliance on the availability and quality of auxiliary information or rules, which may not always be complete or accurate.

\subsubsection*{Addressing Dynamic and Complex Structures for LP/KGC}
Real-world KGs are dynamic and often exhibit complex, hierarchical structures that Euclidean spaces struggle to represent efficiently.
\begin{itemize}
    \item \textbf{Hyperbolic Embeddings}: Hyperbolic spaces, with their inherent negative curvature, are naturally suited for embedding hierarchical data with high fidelity and fewer dimensions. \textsf{Hyperbolic Hierarchy-Aware KGE} [pan2021] explicitly targeted this by extending the Poincar Ball model to capture hierarchical structures more effectively. \textsf{Fully Hyperbolic Rotation} [liang2024] pushed for a more native hyperbolic formulation, defining the entire model directly in hyperbolic space using the Lorentz model, emphasizing hyperbolic rotation for relations. This offers superior representation for hierarchical KGs compared to Euclidean counterparts. \textsf{Mixed Geometry Message and Trainable Convolutional Attention Network} [shang2024] proposed a versatile approach by integrating messages and scoring functions from hyperbolic, hypersphere, and Euclidean spaces, allowing for adaptive modeling of diverse local structures, acknowledging that not all KG structures are purely hierarchical.
    \item \textbf{Temporal KGE (TKGE)}: While a dedicated section (5.1) covers TKGE, it's crucial to note its role in LP/KGC for dynamic KGs. \textsf{HyTE} [dasgupta2018] explicitly incorporates time by associating each timestamp with a hyperplane, enabling temporally aware inference and prediction of temporal scopes for facts. This is essential for predicting links that are valid only for specific time periods.
\end{itemize}
The tension here lies between the theoretical advantages of non-Euclidean geometries for certain graph structures (e.g., hierarchies) and the increased computational complexity of operations within these spaces. While \textsf{Fully Hyperbolic Rotation} [liang2024] claims competitive results with fewer parameters, the overall training and inference can still be more involved than in Euclidean space.

\subsubsection*{Efficiency and Robustness in LP/KGC}
Practical deployment of KGE for LP/KGC necessitates addressing efficiency and robustness concerns.
\begin{itemize}
    \item \textbf{Negative Sampling and Training Optimization}: Training KGE models relies heavily on negative sampling to discriminate positive from negative triples. \textsf{NSCaching} [zhang2018] proposed an efficient method to track and sample "hard" negative triplets using a cache, distilling the benefits of complex GAN-based sampling into a simpler framework. \textsf{Efficient Non-Sampling KGE} [li2021] took a radical approach by considering all negative instances and leveraging mathematical derivations to reduce computational complexity, aiming for more stable and accurate performance by removing sampling uncertainty. \textsf{Confidence-Aware Negative Sampling} [shan2018] addressed noise in KGs by introducing negative triple confidence. \textsf{Modality-Aware Negative Sampling} [zhang2023] extended this to multi-modal KGE, adapting sampling for diverse data modalities. These methods are crucial for optimizing the training process, which directly impacts the accuracy and efficiency of LP/KGC.
    \item \textbf{Compression and Parameter Efficiency}: For large-scale KGs, embedding size and parameter count are critical. \textsf{DualDE} [zhu2020] used knowledge distillation to build low-dimensional student KGEs from high-dimensional teachers, reducing parameters by 7-15x and increasing inference speed by 2-6x with minor performance loss. \textsf{Knowledge Graph Embedding Compression} [sachan2020] represented entities with discrete codes, achieving 50-1000x compression. \textsf{LightKG} [wang2021] introduced a framework that stores only codebooks and indices, drastically reducing storage and boosting inference. \textsf{EARL} [chen2023] achieved parameter efficiency by learning embeddings for only a small set of "reserved entities" and deriving others from context using entity-agnostic encoders.
\end{itemize}
The tension between achieving high accuracy in LP/KGC and maintaining computational efficiency is a recurring theme. While models like \textsf{RotatE} [sun2018] achieve state-of-the-art accuracy, their parameter count can be substantial. Methods like \textsf{DualDE} [zhu2020] and \textsf{EARL} [chen2023] directly address this by reducing model size and computational cost, making KGE more deployable in resource-constrained environments. However, this often comes with a "minor loss in performance" [sachan2020], which might be unacceptable in high-stakes applications. The choice of negative sampling strategy, as reviewed by [qian2021] and [madushanka2024], is also critical; an inefficient or poor sampling method can severely degrade LP/KGC performance, even for highly expressive models.

In conclusion, LP/KGC remains the central benchmark for KGE models, driving innovations from foundational geometric transformations to advanced deep learning architectures and sophisticated strategies for incorporating auxiliary information and managing dynamic data. The field continuously navigates trade-offs between model expressiveness, computational efficiency, and robustness, with recent efforts increasingly focusing on making these models more adaptable and reliable for real-world applications.
\subsection{Entity Alignment}
\label{sec:7_2_entity_alignment}

Building upon the foundational capabilities of Knowledge Graph Embedding (KGE) for tasks like link prediction and knowledge graph completion, KGEs have emerged as a powerful paradigm for Entity Alignment (EA). Entity alignment is the critical task of identifying equivalent entities across different knowledge graphs (KGs), a process essential for integrating heterogeneous knowledge sources, enriching existing KGs, and enabling comprehensive cross-KG reasoning [zhu2024, dai2020]. The core idea is to embed entities from multiple KGs into a shared, low-dimensional vector space, where semantically equivalent entities are expected to be close to each other. This transforms the symbolic matching problem into a geometric proximity search, offering a data-driven approach to overcome the challenges of schema heterogeneity and data incompleteness inherent in real-world KGs.

The application of KGE to entity alignment has evolved significantly, addressing key challenges such as the scarcity of labeled alignment data, the integration of diverse entity features, and the leveraging of meta-information. Early approaches often adapted general KGE models (e.g., \textsf{TransE}, \textsf{TransH} [wang2014], \textsf{TransD} [ji2015]) by training them on a small set of pre-aligned entities and then using distance metrics (e.g., Euclidean or cosine similarity) in the embedding space to find new alignments. However, these basic adaptations often struggled with the inherent differences in structure and semantics between KGs, as well as the practical limitation of requiring substantial initial alignment seeds. This motivated the development of more specialized KGE-based EA methods, which can be broadly categorized into iterative/self-training, multi-feature integration, and schema-aware approaches.

\subsubsection*{Iterative and Semi-Supervised Alignment}
A significant challenge in entity alignment is the scarcity of labeled training data, as manual annotation of equivalent entities across large KGs is prohibitively expensive. This limitation is particularly acute when dealing with KGs from different domains or languages.
\begin{itemize}
    \item \textbf{Bootstrapping Entity Alignment}: To mitigate the reliance on extensive prior alignment, \textsf{Bootstrapping Entity Alignment with Knowledge Graph Embedding} [sun2018] proposed an iterative approach.
    \begin{enumerate}
        \item \textbf{Problem Solved}: Addresses the lack of sufficient prior alignment as labeled training data for embedding-based EA.
        \item \textbf{Core Innovation}: It iteratively labels likely entity alignments as training data, progressively expanding the set of known alignments. Furthermore, it incorporates an "alignment editing method" to reduce error accumulation, a critical concern in self-training loops.
        \item \textbf{Mechanism}: The process typically starts with a small set of seed alignments. A KGE model is trained using these seeds, and then entity similarities in the learned embedding space are used to identify new, highly confident alignment candidates. These candidates are added to the training set, and the process repeats. The alignment editing method acts as a filter, removing potentially erroneous self-labeled alignments to maintain data quality.
        \item \textbf{Conditions for Success}: This method succeeds when the initial seed alignments are of high quality and the KGE model can effectively generalize from them. The alignment editing mechanism is crucial for its robustness, preventing the propagation of errors.
        \item \textbf{Theoretical Limitations}: The primary theoretical limitation is the potential for error accumulation. If the initial KGE model or the confidence threshold for new alignments is not robust, incorrect alignments can be introduced and reinforced, leading to a drift in the embedding space.
        \item \textbf{Practical Limitations}: The performance is highly sensitive to the quality of the initial seed alignments and the effectiveness of the error reduction strategy. It can also be computationally intensive due to multiple retraining iterations of the KGE model.
    \end{enumerate}
    \item \textbf{Semi-Supervised Entity Alignment}: Building on the need to leverage unlabeled data, \textsf{Semi-Supervised Entity Alignment via Knowledge Graph Embedding with Awareness of Degree Difference} (\textsf{SEA}) [pei2019] further refined the approach.
    \begin{enumerate}
        \item \textbf{Problem Solved}: Addresses two key issues: the difficulty of acquiring labeled data (similar to bootstrapping) and the challenge posed by entity degree differences (high-frequency vs. low-frequency entities) which can affect KGE accuracy.
        \item \textbf{Core Innovation}: It employs a semi-supervised learning framework to leverage both labeled and abundant unlabeled entity information. Crucially, it improves the KGE with awareness of degree difference by performing adversarial training, aiming to make embeddings more robust to varying entity frequencies.
        \item \textbf{Mechanism}: Labeled entity pairs are used to initialize KGEs. For unlabeled entities, a self-training mechanism generates pseudo-labels based on embedding similarity. Adversarial training is introduced to encourage the KGE model to learn representations that are less sensitive to an entity's degree, thereby preventing high-degree entities from dominating the embedding space and improving alignment for low-degree entities.
        \item \textbf{Conditions for Success}: Requires a robust semi-supervised learning framework and effective adversarial training to balance the influence of entity degrees. It performs well when there's a significant amount of unlabeled data that can be reliably pseudo-labeled.
        \item \textbf{Theoretical Limitations}: Adversarial training can be notoriously difficult to stabilize and tune, potentially leading to training instabilities or suboptimal embeddings if not carefully managed. The effectiveness of mitigating degree bias might also vary across different KG structures.
        \item \textbf{Practical Limitations}: Increased complexity due to the adversarial component. The quality of pseudo-labels, especially in early iterations, can still impact overall performance.
    \end{enumerate}
\end{itemize}

\subsubsection*{Multi-view and Ontology-guided Alignment}
Beyond structural information, entities in KGs possess rich auxiliary features that, if leveraged effectively, can significantly enhance alignment accuracy.
\begin{itemize}
    \item \textbf{Multi-view Knowledge Graph Embedding}: While early KGEs for EA primarily focused on relational structures, \textsf{Multi-view Knowledge Graph Embedding for Entity Alignment} [zhang2019] recognized the untapped potential of diverse entity features.
    \begin{enumerate}
        \item \textbf{Problem Solved}: Addresses the limitation of previous methods that primarily focused on relational structure, sometimes incorporating attributes, but often overlooking or not equally treating a vast array of other entity features (e.g., entity names, textual descriptions). This oversight impairs accuracy and robustness.
        \item \textbf{Core Innovation}: Proposes a novel framework that unifies multiple views of entities (entity names, relations, and attributes) to learn more comprehensive embeddings for entity alignment.
        \item \textbf{Mechanism}: The framework learns separate embeddings for each view (e.g., using character embeddings for names, structural KGE for relations, and attribute value embeddings). These view-specific embeddings are then combined using various strategies (e.g., concatenation, weighted sum, or attention mechanisms). Cross-KG inference methods are designed to further enhance alignment by propagating alignment signals between the KGs.
        \item \textbf{Conditions for Success}: This approach thrives when multiple views provide complementary and non-redundant information. The effectiveness hinges on robust methods for learning embeddings from each view and intelligent strategies for combining them.
        \item \textbf{Theoretical Limitations}: The optimal strategy for combining heterogeneous view embeddings is often heuristic and dataset-dependent, lacking a universal theoretical guarantee. The interplay between different views can be complex, and simple concatenation might not fully capture nuanced interactions.
        \item \textbf{Practical Limitations}: Increased computational cost due to learning and combining multiple embedding spaces. Requires careful feature engineering and selection for each view. The quality of each view's data (e.g., completeness of attributes, clarity of names) directly impacts performance.
    \end{enumerate}
    \item \textbf{Ontology-guided Entity Alignment}: Recognizing that KGs often come with explicit schema or ontological information, \textsf{OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding} [xiang2021] proposed to integrate this meta-knowledge.
    \begin{enumerate}
        \item \textbf{Problem Solved}: Addresses the oversight of existing methods that ignore ontological schemas, which contain critical meta-information such as classes, their hierarchies, and disjointness relationships. This meta-information can significantly constrain and guide the alignment process.
        \item \textbf{Core Innovation}: Jointly embeds both KGs and their associated ontologies, explicitly utilizing class hierarchy (e.g., `subClassOf` relations) and class disjointness (e.g., `disjointWith` axioms) to avoid false mappings and guide the embedding learning.
        \item \textbf{Mechanism}: \textsf{OntoEA} extends a base KGE model by incorporating additional loss terms that enforce ontological constraints. For instance, if entity $e_1$ is an instance of class $C_1$ and entity $e_2$ is an instance of class $C_2$, and $C_1$ is a subclass of $C_2$, then their embeddings should reflect this hierarchical relationship. Similarly, if $C_1$ and $C_2$ are disjoint, then instances of these classes should not be aligned. This joint embedding ensures that entity representations are consistent with both the factual triples and the underlying schema.
        \item \textbf{Conditions for Success}: Requires the availability of explicit and reasonably complete ontologies for the KGs being aligned. It performs best when these ontologies are consistent and well-defined, providing strong semantic guidance.
        \item \textbf{Theoretical Limitations}: The effectiveness is directly tied to the quality and completeness of the ontologies. Sparse or inconsistent ontologies can introduce noise or misleading constraints, potentially hindering alignment. The integration of hard logical constraints into continuous embedding spaces can also be challenging.
        \item \textbf{Practical Limitations}: Many real-world KGs, especially those automatically extracted, may lack rich or explicit ontological schemas. The process of extracting or harmonizing ontologies can be complex and time-consuming.
    \end{enumerate}
\end{itemize}

\subsubsection*{Comparative Framework: KGE-based Entity Alignment Methods}
\begin{tabular}{|p{0.15\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.15\textwidth}|}
\hline
\textbf{Method Family} & \textbf{Core Innovation} & \textbf{Problem Solved} & \textbf{Conditions for Success} & \textbf{Limitations} \\
\hline
\textsf{Bootstrapping} [sun2018] & Iterative self-labeling with error reduction. & Scarcity of labeled training data. & High-quality initial seeds; effective error editing. & Risk of error accumulation; sensitive to initial seed quality. \\
\hline
\textsf{Semi-Supervised} [pei2019] & Semi-supervised learning with adversarial degree awareness. & Labeled data scarcity; entity degree bias. & Robust semi-supervised framework; stable adversarial training. & Adversarial training complexity; pseudo-label quality. \\
\hline
\textsf{Multi-view} [zhang2019] & Unifies diverse entity features (names, relations, attributes). & Limited feature utilization; incomplete semantic capture. & Complementary views; effective fusion strategy. & Increased complexity; feature engineering overhead; fusion heuristics. \\
\hline
\textsf{Ontology-guided} [xiang2021] & Joint embedding of KGs and their ontologies. & Neglect of meta-information (schema, hierarchy). & Available, high-quality, consistent ontologies. & Dependence on ontology quality; ontology acquisition overhead. \\
\hline
\end{tabular}

\subsubsection*{Synthesis and Critical Analysis}
The evolution of KGE for entity alignment reflects a continuous effort to move beyond purely structural matching towards more semantically rich and robust approaches. The initial challenge of limited labeled data, which implicitly affects any supervised KGE model, was directly tackled by \textsf{Bootstrapping Entity Alignment} [sun2018] and \textsf{SEA} [pei2019]. While \textsf{Bootstrapping} iteratively expands the training set, \textsf{SEA} further refines this by explicitly addressing the bias introduced by varying entity degrees through adversarial training. This highlights a crucial pattern in the field: the progression from basic self-training to more sophisticated mechanisms that account for inherent data characteristics. However, a common tension across these iterative methods is the trade-off between leveraging more unlabeled data and the risk of error propagation, which can lead to suboptimal or even incorrect alignments if not carefully managed by mechanisms like alignment editing [sun2018].

Furthermore, the field has recognized that structural information alone is often insufficient for robust EA, especially when KGs are sparse or structurally dissimilar. This led to the development of methods that integrate auxiliary information. \textsf{Multi-view Knowledge Graph Embedding} [zhang2019] exemplifies this by unifying entity names, relations, and attributes. This approach directly addresses the limitation of earlier KGEs that might only consider relational triples (as discussed in Section 7.1), by showing that a richer feature set leads to more discriminative embeddings. Similarly, \textsf{OntoEA} [xiang2021] extends this by incorporating ontological information, which provides a powerful, explicit form of meta-knowledge. This demonstrates a broader trend towards leveraging all available information, moving from implicit structural patterns to explicit semantic constraints. While \textsf{Multi-view} focuses on diverse *data types*, \textsf{OntoEA} focuses on *schema-level knowledge*, offering complementary improvements. A critical assumption often made by these methods is the existence of high-quality auxiliary data or ontologies, which may not always hold true for all KGs, especially those automatically constructed or from highly specialized domains.

Methodologically, the quality of evaluation in EA is paramount. As highlighted by experimental reviews like [fanourakis2022], a systematic quantitative assessment of strengths and weaknesses across different KG characteristics is crucial. Many EA papers evaluate performance using metrics like Hits@k and MRR on benchmark datasets such as DBP15K, which are valuable but may not fully capture real-world complexities like different entity granularities, varying levels of noise, or the presence of non-isomorphic structures. The generalizability of methods like \textsf{SEA} [pei2019] in mitigating degree bias, for instance, needs to be thoroughly tested across KGs with diverse degree distributions. Similarly, the reliance of \textsf{OntoEA} [xiang2021] on well-defined ontologies means its performance might degrade significantly for KGs lacking such rich schema, an unstated assumption that limits its universal applicability.

In conclusion, KGEs provide a powerful, data-driven approach to integrate heterogeneous knowledge sources, which is crucial for building comprehensive knowledge bases and enabling cross-KG reasoning. The field has progressed from basic adaptation of KGE models to specialized frameworks that address data scarcity through iterative learning [sun2018, pei2019], integrate diverse entity features [zhang2019], and leverage ontological information [xiang2021]. These advancements collectively enhance the ability to find semantic correspondences between disparate knowledge structures. However, challenges persist in ensuring robustness to noise, handling highly sparse KGs, and achieving truly unsupervised alignment. Future research, as indicated by recent surveys [zhu2024, yan2022], will likely focus on integrating even more diverse information (e.g., temporal, multimodal), developing more robust self-training mechanisms, and improving the generalizability of these methods across a wider spectrum of real-world KGs.
\subsection{Question Answering and Recommendation Systems}
\label{sec:7_3_question_answering__and__recommendation_systems}

Leveraging the capability of Knowledge Graph Embeddings (KGEs) to represent complex relational data in a continuous, low-dimensional space, the field has significantly advanced applications in Question Answering (QA) and Recommender Systems. As discussed in previous sections, KGEs transform symbolic knowledge into machine-understandable formats, addressing issues of sparsity and enabling semantic reasoning. This subsection explores how KGEs bridge the gap between natural language and structured knowledge, fostering more intelligent, personalized, and interpretable user interactions in these critical application contexts.

\subsubsection*{Knowledge Graph Embedding for Question Answering}
Question Answering over Knowledge Graphs (QA-KG) aims to answer natural language questions by querying structured facts within a KG. This task is challenging due to the inherent ambiguity and variability of natural language, as well as the complexity of mapping linguistic expressions to precise KG entities and relations. KGEs offer a powerful mechanism to overcome these challenges by embedding both questions and KG elements into a shared semantic space.

\begin{itemize}
    \item \textbf{KEQA: Bridging Natural Language and KG Embeddings}
    \begin{enumerate}
        \item \textbf{Context}: Traditional QA-KG systems often rely on complex semantic parsing or template matching, which struggle with predicate variability and entity ambiguity in natural language questions. The success of KGE in tasks like link prediction [rossi2020] motivated its application to QA.
        \item \textbf{Problem Solved}: \textsf{Knowledge Graph Embedding Based Question Answering} (\textsf{KEQA}) [huang2019] addresses the difficulty of identifying the correct head entity and predicate from a natural language question to retrieve a single-fact answer. It aims to make KG access more efficient and intuitive for end-users.
        \item \textbf{Core Innovation}: Instead of directly inferring the head entity and predicate, \textsf{KEQA} jointly recovers the question's head entity, predicate, and tail entity representations within the KG embedding spaces. It employs a carefully designed joint distance metric to find the closest fact in the KG as the answer.
        \item \textbf{Mechanism}: The natural language question is first processed to extract potential entities and relations. These are then mapped to their corresponding KGEs. The core idea is to find a (head, relation, tail) triple in the KG whose embeddings are maximally similar to the question's inferred (head, relation, tail) embedding, using a scoring function that measures this joint proximity.
        \item \textbf{Conditions for Success}: \textsf{KEQA} performs well for "simple questions" that can be answered by a single KG triple. Its effectiveness relies on the quality of the underlying KGE model (e.g., \textsf{TransE}, \textsf{TransH} [wang2014]) and robust entity linking to correctly identify entities mentioned in the question.
        \item \textbf{Theoretical Limitations}: The primary theoretical limitation is its restriction to simple questions. It struggles with multi-hop reasoning, complex logical operations (e.g., aggregations, comparisons), or questions requiring external knowledge beyond a single fact. The joint distance metric, while innovative, may not fully capture the nuances of complex natural language semantics.
        \item \textbf{Practical Limitations}: Scalability to very large KGs can be an issue if the search space for candidate triples is not efficiently pruned. It also requires a pre-trained KGE model, adding to the overall system complexity.
        \item \textbf{Comparison}: Compared to rule-based QA systems, \textsf{KEQA} is more robust to linguistic variations due to the semantic capture of embeddings. However, for complex questions, it is less capable than systems that incorporate explicit logical reasoning or advanced NLP parsing.
    \end{enumerate}

    \item \textbf{Hybrid Systems: Marie and BERT for Domain-Specific QA}
    \begin{enumerate}
        \item \textbf{Context}: While \textsf{KEQA} established the utility of KGE for simple QA, real-world applications, especially in specialized domains, demand handling complex queries, deep ontologies, and numerical filtering. This necessitates integrating KGE with advanced Natural Language Processing (NLP) models.
        \item \textbf{Problem Solved}: \textsf{Marie and BERT} [zhou2023] addresses fact-oriented information retrieval in complex, specialized domains like chemistry. It tackles challenges such as deep ontologies, implicit multi-hop relations, numerical filtering, and the need for dynamic calculations via semantic agents.
        \item \textbf{Core Innovation}: This system integrates hybrid KGEs (employing multiple embedding methods in parallel), a BERT-based entity-linking model for enhanced robustness, and specialized modules for implicit multi-hop relations and numerical filtering. It also incorporates semantic agents for dynamic calculations.
        \item \textbf{Mechanism}: The system queries multiple KGE spaces simultaneously, leveraging their diverse strengths. A BERT-based model significantly improves entity linking by capturing contextual semantics. An algorithm is implemented to derive implicit multi-hop relations, extending beyond direct connections. A score alignment model reranks answers from different KGEs, and a joint numerical embedding model handles numerical constraints.
        \item \textbf{Conditions for Success}: Requires a rich, well-structured domain-specific KG and the availability of pre-trained large language models (LLMs) like BERT. Effective performance hinges on the careful integration and tuning of its heterogeneous components.
        \item \textbf{Theoretical Limitations}: The complexity of integrating multiple KGEs and a BERT model can make the system difficult to interpret end-to-end, despite the individual components being understandable. Generalizability to vastly different domains might require significant re-training and re-engineering.
        \item \textbf{Practical Limitations}: High computational cost due to running multiple KGE models and a BERT-based linker. Development and maintenance are complex given the multi-component architecture.
        \item \textbf{Comparison}: \textsf{Marie and BERT} significantly surpasses \textsf{KEQA} in handling complex, domain-specific QA by combining the semantic power of KGE with the contextual understanding of BERT. It addresses multi-hop and numerical queries that \textsf{KEQA} cannot, but at a much higher computational and engineering overhead. This exemplifies the field's shift towards hybrid, multi-modal approaches (as discussed in Section 4.3) for complex tasks.
    \end{enumerate}
\end{itemize}

\subsubsection*{Knowledge Graph Embedding for Recommendation Systems}
Recommender systems (RecSys) aim to predict user preferences and suggest relevant items. Traditional RecSys often suffer from data sparsity, cold start problems, and a lack of explainability. KGEs enrich item and user representations by incorporating rich relational information from KGs, enabling more accurate, personalized, and interpretable recommendations.

\begin{itemize}
    \item \textbf{RKGE: Recurrent KGE for Path-based Recommendation}
    \begin{enumerate}
        \item \textbf{Context}: Many RecSys rely on collaborative filtering or content-based methods, which struggle with sparsity and often lack transparency. KGs contain rich relational information (e.g., "user buys item," "item is of category," "category has attribute") that can alleviate these issues. Existing KG-based methods often rely on hand-engineered meta-paths, which require domain expertise.
        \item \textbf{Problem Solved}: \textsf{Recurrent knowledge graph embedding for effective recommendation} (\textsf{RKGE}) [sun2018] aims to improve recommendation by automatically learning semantic representations of paths between entities in a KG, thereby characterizing user preferences without manual feature engineering. It also provides meaningful explanations.
        \item \textbf{Core Innovation}: \textsf{RKGE} employs a novel recurrent neural network (RNN) architecture to model the semantics of paths linking entity pairs (e.g., user-item paths). These path representations are then seamlessly fused into the recommendation process. A pooling operator is used to discriminate the saliency of different paths, offering a form of explanation.
        \item \textbf{Mechanism}: User-item interactions and item attributes are represented as paths in a KG. The RNN processes these paths, learning a vector representation for each path. These path embeddings are then aggregated to form a comprehensive representation for user-item pairs, which is used for prediction. The pooling mechanism highlights important paths, providing a rudimentary explanation.
        \item \textbf{Conditions for Success}: Requires a sufficiently rich KG where user-item interactions can be meaningfully represented as paths. The RNN needs to be capable of learning effective representations from these paths.
        \item \textbf{Theoretical Limitations}: The interpretability of RNN-learned path representations can be limited; while it identifies "saliency," the underlying semantic reason might not be immediately clear to a human. It may also struggle with very long or highly diverse paths.
        \item \textbf{Practical Limitations}: Training RNNs on potentially numerous and long paths can be computationally intensive. Scalability to extremely large KGs with vast numbers of paths can be a challenge.
        \item \textbf{Comparison}: \textsf{RKGE} offers a more automated approach than earlier meta-path-based methods, which require significant manual feature engineering. It provides a basic level of explainability, moving beyond black-box matrix factorization models.
    \end{enumerate}

    \item \textbf{CKGE: Contextualized and Explainable Recommendation}
    \begin{enumerate}
        \item \textbf{Context}: Building on the success of path-based KGE for recommendation, there's a growing demand for more nuanced, motivation-aware, and transparent recommendations, especially in high-stakes domains like talent training. The limitations of RNNs in capturing global dependencies motivate the use of Transformer architectures.
        \item \textbf{Problem Solved}: \textsf{Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation} (\textsf{CKGE}) [yang2023] aims to provide precise and explainable recommendations for talent training courses, considering different learning motivations.
        \item \textbf{Core Innovation}: \textsf{CKGE} integrates "contextualized neighbor semantics" and "high-order connections" as "motivation-aware information" by constructing meta-graphs for talent-course pairs. It then employs a novel KG-based Transformer with relational attention and structural encoding, alongside "local path mask prediction" to reveal path importance for explainability.
        \item \textbf{Mechanism}: For each talent-course pair, a meta-graph is constructed, incorporating neighbors and meta-paths as contextual information. This meta-graph is serialized and fed into a KG-based Transformer, which uses relational attention and structural encoding to model global dependencies. A local path mask prediction task explicitly identifies and quantifies the importance of different paths, leading to fine-grained, motivation-aware explanations.
        \item \textbf{Conditions for Success}: Requires rich contextual information to build effective meta-graphs. The Transformer architecture needs to be robustly trained to capture complex relational patterns and long-range dependencies.
        \item \textbf{Theoretical Limitations}: The construction of meta-graphs and the interpretation of "motivation-aware information" can be complex and potentially heuristic. While local path mask prediction offers explainability, ensuring that these explanations are truly intuitive and actionable for human users remains a challenge.
        \item \textbf{Practical Limitations}: High computational cost associated with Transformer models and meta-graph construction. The system's specificity to talent training might limit its direct generalizability to other recommendation domains without significant adaptation.
        \item \textbf{Comparison}: \textsf{CKGE} represents a significant advancement over \textsf{RKGE} [sun2018] by incorporating contextualization and leveraging the powerful Transformer architecture. It moves beyond simple path saliency to provide more nuanced, motivation-aware recommendations and more precise, explicit path-based explanations. This aligns with the broader trend of integrating advanced deep learning models for enhanced expressiveness and interpretability.
    \end{enumerate}
\end{itemize}

\subsubsection*{Comparative Framework: KGE in Question Answering and Recommendation Systems}
\begin{tabular}{|p{0.15\textwidth}|p{0.15\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.15\textwidth}|}
\hline
\textbf{Application/Method} & \textbf{Core Innovation} & \textbf{Problem Solved} & \textbf{Key Strength} & \textbf{Primary Limitation} \\
\hline
\textbf{QA: KEQA} [huang2019] & Joint embedding recovery for question elements. & Simple QA with predicate variability/entity ambiguity. & Robust to linguistic variations for simple questions. & Limited to simple, single-hop questions. \\
\hline
\textbf{QA: Marie and BERT} [zhou2023] & Hybrid KGEs + BERT + specialized modules. & Complex, domain-specific QA (multi-hop, numerical). & Handles deep ontologies and diverse query types in specialized domains. & High complexity, domain-specific, high computational cost. \\
\hline
\textbf{RecSys: RKGE} [sun2018] & Recurrent network for automatic path representation. & Recommendation without manual feature engineering; basic explainability. & Automates path feature learning, offers initial explanations. & Limited interpretability of RNN paths, scalability for very long paths. \\
\hline
\textbf{RecSys: CKGE} [yang2023] & Contextualized meta-graphs + KG-based Transformer + path mask prediction. & Explainable, motivation-aware recommendations. & Highly personalized, context-aware, and precise explanations. & High computational cost, complex meta-graph construction. \\
\hline
\end{tabular}

\subsubsection*{Synthesis and Critical Analysis}
The application of KGEs to Question Answering and Recommender Systems clearly illustrates the field's progression from foundational embedding techniques to sophisticated, hybrid, and application-specific solutions. A recurring pattern is the evolution from leveraging KGEs for basic semantic matching to integrating them within complex deep learning architectures to handle more nuanced problems.

For QA, the trajectory moves from \textsf{KEQA}'s [huang2019] foundational approach of mapping simple natural language questions to KG triples in an embedding space, to the highly specialized and integrated \textsf{Marie and BERT} system [zhou2023]. While \textsf{KEQA} demonstrated the initial power of KGEs to overcome predicate variability, its theoretical limitation to single-hop questions highlighted the need for deeper NLP integration. \textsf{Marie and BERT} directly addresses this by combining multiple KGEs with a powerful BERT-based entity linker and dedicated modules for multi-hop and numerical queries. This exemplifies the broader trend of augmenting KGEs with advanced NLP models to capture the full semantic complexity of natural language, as also seen in multi-modal KGEs that integrate textual descriptions [shen2022]. The trade-off is evident: increased accuracy and capability for complex queries come at the cost of significantly higher computational complexity and domain specificity. \textsf{Marie and BERT}'s reliance on a rich domain-specific KG and pre-trained LLMs is an unstated assumption that limits its direct generalizability to resource-poor or open-domain QA.

Similarly, in recommender systems, KGEs have evolved from enriching item representations to modeling complex user preferences and providing explicit explanations. \textsf{RKGE} [sun2018] pioneered the automatic learning of path semantics using recurrent networks, moving beyond manual feature engineering. This built upon the understanding that relational paths in KGs encode rich interaction patterns, a concept that has been explored in various KGE applications [dai2020]. However, the interpretability of RNN-derived path saliency had practical limitations. \textsf{CKGE} [yang2023] significantly advances this by introducing contextualized meta-graphs and a KG-based Transformer, offering more precise, motivation-aware recommendations and explainable paths. This reflects the field's broader shift towards Transformer architectures (as discussed in Section 3.3) for their superior ability to model long-range dependencies and contextual information. The tension here lies between achieving highly personalized and explainable recommendations and the increased computational burden and data requirements of these advanced models. The "explainability" claimed by both \textsf{RKGE} and \textsf{CKGE} needs careful methodological scrutiny; while they identify influential paths, the *why* behind their influence might still require human interpretation, an evaluation gap that is often overlooked.

In essence, KGEs serve as a crucial bridge, transforming the symbolic, sparse nature of KGs into a continuous, semantically rich representation that is amenable to modern machine learning. This enables more intelligent, personalized, and interpretable user interactions in diverse application contexts. The evolution of these applications underscores a convergent research direction: the integration of KGEs with advanced deep learning models (like BERT and Transformers) and the increasing emphasis on explainability to build trust and provide actionable insights. However, persistent challenges include managing the computational cost of these hybrid systems, ensuring the generalizability of domain-specific solutions, and developing truly intuitive and actionable explanations for human users. Future research will likely focus on more seamless and efficient integration of KGEs with large language models, robust methods for handling noisy and dynamic data in these applications, and standardized metrics for evaluating the quality of explanations.
\subsection{Domain-Specific Applications and Explainability}
\label{sec:7_4_domain-specific_applications__and__explainability}

Building upon the discussion of Knowledge Graph Embeddings (KGEs) in general applications like Question Answering and Recommender Systems (Section 7.3), this subsection shifts focus to the specialized and often high-stakes domains where KGEs are not merely applied but meticulously tailored, validated, and, crucially, made interpretable. In fields such as biological systems, patent analysis, and drug discovery, the demand for verifiable and transparent solutions necessitates moving beyond generic performance metrics to deliver actionable insights and build trust. This represents a critical evolution in KGE research, where the emphasis is on practical utility and explainability rather than solely on abstract embedding quality.

\begin{table}[htbp]
    \centering
    \caption{Comparative Framework: KGE in Specialized Domains with Emphasis on Explainability and Validation}
    \label{tab:domain_specific_kge}
    \begin{tabularx}{\textwidth}{|p{0.15\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.15\textwidth}|}
        \hline
        \textbf{Domain/Paper} & \textbf{Problem Addressed} & \textbf{KGE Tailoring/Innovation} & \textbf{Domain-Specific Evaluation/Validation} & \textbf{Explainability Focus} \\
        \hline
        \textbf{Biological Systems} [mohamed2020] & Scalability for complex biological systems, drug-target prediction, polypharmacy side effects. & Learning low-rank vector representations of biological entities/relations. & Predictive accuracy for drug-target interactions, polypharmacy side effects. & Implicit (preserving graph structure), but less explicit. \\
        \hline
        \textbf{Patent Metadata Analysis} [li2022] & Measuring knowledge proximity, explaining domain expansion profiles of inventors/assignees. & Training KGE on patent metadata (citations, inventors, assignees, classifications). & Predicting target entities, explaining inventor/assignee domain expansion profiles. & Explaining domain expansion profiles. \\
        \hline
        \textbf{Drug Repurposing (COVID-19)} [islam2023] & Urgent drug discovery for COVID-19, need for verifiable predictions. & Ensemble KGE, deep neural network for prediction. & Molecular docking (novel for KGE-based repurposing), retrieval of in-trial drugs. & Rules extracted from KG, explanatory paths (post-hoc). \\
        \hline
        \textbf{Specific Diseases (Multimodal)} [zhu2022] & Discovering new reliable knowledge for specific diseases, multimodal reasoning. & Multimodal reasoning (structural, category, description embeddings), reverse-hyperplane projection. & Manual proofreading of predicted pairs (drug-gene, gene-disease, disease-drug), universality of embeddings. & Implicit through multimodal integration, manual proofreading for human-understandable validation. \\
        \hline
    \end{tabularx}
\end{table}

\subsubsection*{KGE in Biological Systems and Drug Discovery}
The complexity of biological systems, characterized by intricate networks of genes, proteins, drugs, and diseases, presents a formidable challenge for traditional analytical methods. Knowledge graphs naturally represent these interconnected biological entities, but their sheer scale often limits the scalability of graph exploratory approaches [mohamed2020]. KGE models offer a solution by learning low-dimensional vector representations that preserve the graph's inherent structure, enabling efficient predictive and analytical tasks.

\begin{itemize}
    \item \textbf{General Biological Applications}
    \begin{enumerate}
        \item \textbf{Context}: Traditional graph exploration methods for biological KGs, while accurate, struggle with scalability due to time-consuming path exploration. The need for efficient, high-accuracy predictive models in drug discovery and understanding disease mechanisms motivated the adoption of KGEs.
        \item \textbf{Problem Solved}: \textsf{Biological applications of knowledge graph embedding models} [mohamed2020] addresses the scalability limitations of traditional graph processing in biological systems and demonstrates KGE's utility for tasks like predicting drug-target interactions and polypharmacy side effects.
        \item \textbf{Core Innovation}: The core innovation lies in applying existing KGE models (e.g., \textsf{TransE}, \textsf{DistMult}) to biological knowledge graphs, leveraging their ability to learn low-rank vector representations of biological entities (drugs, proteins, diseases) and relations. This transforms complex graph structures into a computationally tractable embedding space.
        \item \textbf{Mechanism}: Biological entities and their relationships (e.g., "drug A targets protein B," "protein B is associated with disease C") are encoded into a KG. KGE models then embed these entities and relations, allowing for similarity-based predictions (e.g., if drug A is similar to drug D, and drug D targets protein E, then drug A might also target protein E).
        \item \textbf{Conditions for Success}: Requires well-curated biological KGs. The success hinges on the KGE model's ability to capture nuanced biological relationships accurately, which can be challenging given the heterogeneity and complexity of biological data.
        \item \textbf{Theoretical Limitations}: While KGEs improve scalability, the interpretability of the learned embeddings in a purely biological sense can be limited. It's often unclear *why* certain embeddings are close or far apart, making it difficult to derive novel biological hypotheses directly from the embedding space without further analysis.
        \item \textbf{Practical Limitations}: The quality and completeness of biological KGs are paramount. Data sparsity in specific biological pathways or drug-disease interactions can lead to unreliable embeddings.
        \item \textbf{Comparison}: KGE models offer superior scalability compared to traditional path-exploratory methods [mohamed2020]. However, they often lack the direct, human-readable reasoning paths that rule-based systems (as discussed in Section 4.2) or symbolic reasoning approaches might provide, which is a significant drawback in high-stakes fields.
    \end{enumerate}

    \item \textbf{Molecular-evaluated and Explainable Drug Repurposing for COVID-19}
    \begin{enumerate}
        \item \textbf{Context}: The urgent need for effective treatments for diseases like COVID-19 highlights the value of drug repurposing. While KGEs can predict drug-disease associations, the high-stakes nature of medical applications demands not just predictions, but also verifiable evidence and clear explanations. This addresses a critical gap in generic KGE applications, which often rely on abstract metrics.
        \item \textbf{Problem Solved}: \textsf{Molecular-evaluated and explainable drug repurposing for COVID-19 using ensemble knowledge graph embedding} [islam2023] tackles the challenge of identifying potential drug candidates for COVID-19 while providing molecular-level validation and human-interpretable explanations.
        \item \textbf{Core Innovation}: This work proposes an ensemble KGE approach to create robust latent representations, which are then fed into a deep neural network for drug discovery. Crucially, it introduces *molecular docking* as a novel, domain-specific evaluation metric for KGE-based drug repurposing, moving beyond standard link prediction metrics. Furthermore, it provides *explanations* through rules extracted from the KG and instantiated by explanatory paths, directly addressing the need for interpretability in high-stakes medical contexts.
        \item \textbf{Mechanism}: A COVID-19 centric KG is constructed. Ensemble embeddings are learned to capture diverse relational patterns, enhancing the robustness of representations. A deep neural network leverages these embeddings to predict drug-COVID-19 associations. Top predictions are then subjected to molecular docking simulations to assess their binding affinity to SARS-CoV-2 targets, providing a physical, verifiable validation. Explanatory paths are generated by tracing relevant relations in the KG that support the prediction, offering a transparent rationale.
        \item \textbf{Conditions for Success}: Requires a comprehensive and high-quality COVID-19 KG. The effectiveness of molecular docking depends on accurate protein structures and docking algorithms. The quality of extracted rules for explanation is also vital.
        \item \textbf{Theoretical Limitations}: While ensemble embeddings improve robustness, the underlying deep neural network remains a black box. The post-hoc nature of rule extraction for explanation, while valuable, may not fully capture the complex non-linear reasoning of the neural network. The generalizability of specific molecular docking results to clinical efficacy is also a known challenge in drug discovery.
        \item \textbf{Practical Limitations}: Molecular docking is computationally intensive. Constructing and maintaining a high-quality, up-to-date domain-specific KG is a significant effort.
        \item \textbf{Comparison}: This approach significantly advances beyond general biological KGE applications [mohamed2020] by integrating a *verifiable, domain-specific evaluation metric* (molecular docking) and *explicit explainability mechanisms*. It directly addresses the need for trustworthiness in medical AI, a theme echoed in discussions on evaluation and reproducibility (Section 6.3). The emphasis on rules for explanation connects to the rule-based KGE methods discussed in Section 4.2, demonstrating their practical utility in providing interpretability.
    \end{enumerate}

    \item \textbf{Multimodal Reasoning for Specific Diseases}
    \begin{enumerate}
        \item \textbf{Context}: Biomedical KGs are rich but often incomplete. Leveraging multimodal information (e.g., textual descriptions, categories) alongside structural data can enhance knowledge discovery, especially for specific diseases. The challenge is effectively integrating these diverse modalities.
        \item \textbf{Problem Solved}: \textsf{Multimodal reasoning based on knowledge graph embedding for specific diseases} [zhu2022] aims to discover new and reliable knowledge within Specific Disease Knowledge Graphs (SDKGs) by integrating structural, category, and description embeddings.
        \item \textbf{Core Innovation}: This work constructs SDKGs for various diseases and implements multimodal reasoning using reverse-hyperplane projection. It combines structural embeddings (from graph triples), category embeddings (from entity types), and description embeddings (from textual information) to enrich representations. Reliability is verified through *manual proofreading* of predicted drug-gene, gene-disease, and disease-drug pairs.
        \item \textbf{Mechanism}: SDKGs are built by extracting triplets, standardizing entities, and linking relations. KGEs are learned for each modality. Reverse-hyperplane projection is used to integrate these multimodal embeddings, allowing for more comprehensive reasoning. The predictions are then manually validated by domain experts, providing a human-centric layer of trustworthiness.
        \item \textbf{Conditions for Success}: Requires high-quality multimodal data (textual descriptions, categories) alongside structural data. The effectiveness of multimodal fusion is critical. Manual proofreading, while robust, is resource-intensive and relies on expert availability.
        \item \textbf{Theoretical Limitations}: The reverse-hyperplane projection, while effective, might not fully capture all complex interactions between modalities. The "universality" of embeddings, while demonstrated for biomolecular interaction classification, might not extend to all downstream tasks without further fine-tuning.
        \item \textbf{Practical Limitations}: Constructing and maintaining multimodal SDKGs is complex. Manual proofreading, while a strong validation, is not scalable for large-scale, continuous knowledge discovery.
        \item \textbf{Comparison}: This approach highlights the power of multimodal KGEs (as discussed in Section 4.3) in specialized domains. Similar to [islam2023], it emphasizes *verifiable reliability* through manual proofreading, reinforcing the need for human oversight in high-stakes applications. It shows that combining diverse information sources can lead to more robust and reliable knowledge discovery.
    \end{enumerate}

\subsubsection*{KGE in Patent Metadata Analysis}
Beyond biomedical applications, KGEs are proving invaluable in intellectual property and innovation analysis, where understanding the relationships between patents, inventors, assignees, and technological domains is crucial for strategic decision-making.

\begin{enumerate}
    \item \textbf{Embedding Knowledge Graph of Patent Metadata}
    \begin{enumerate}
        \item \textbf{Context}: Understanding "knowledge proximity" between entities (e.g., patents, inventors, companies) in large patent databases is essential for innovation management. Traditional methods often rely on simple co-occurrence or citation analysis, which may not capture nuanced semantic associations.
        \item \textbf{Problem Solved}: \textsf{Embedding knowledge graph of patent metadata to measure knowledge proximity} [li2022] operationalizes knowledge proximity in the US Patent Database by building a knowledge graph (\textsf{PatNet}) from patent metadata and applying KGEs to measure associations.
        \item \textbf{Core Innovation}: The work constructs \textsf{PatNet} using patent metadata (citations, inventors, assignees, domain classifications) and trains various KGE models on it. Knowledge proximity is then defined as the cosine similarity between the learned embeddings of entities. The approach is validated by its ability to predict target entities and *explain domain expansion profiles* of inventors and assignees.
        \item \textbf{Mechanism}: Patent metadata is transformed into a KG where patents, inventors, assignees, and technology classifications are entities, and relationships like "inventor invents patent," "patent cites patent," "assignee owns patent" are relations. KGE models (e.g., \textsf{TransE}, \textsf{DistMult}) learn embeddings for these entities and relations. The proximity in the embedding space reflects knowledge proximity.
        \item \textbf{Conditions for Success}: Requires a comprehensive patent database with rich metadata. The quality of the constructed \textsf{PatNet} and the chosen KGE model are critical.
        \item \textbf{Theoretical Limitations}: While cosine similarity in embedding space is a common proxy for semantic proximity, its direct interpretation as "knowledge proximity" might be an oversimplification. The KGE models primarily capture structural patterns, and the richness of semantic proximity might require more sophisticated models or multimodal integration.
        \item \textbf{Practical Limitations}: Building and maintaining a large-scale patent KG is a significant data engineering task. The interpretation of "domain expansion profiles" from embeddings requires careful domain expertise.
        \item \textbf{Comparison}: This application demonstrates KGE's utility in a non-biomedical, high-value domain. Similar to the drug repurposing work, it moves beyond generic link prediction to provide *actionable insights* (knowledge proximity, domain expansion) that are directly relevant to industry problems. The focus on explaining domain expansion profiles aligns with the broader demand for explainability, albeit in a different context than medical applications.
    \end{enumerate}
\end{itemize}

\subsubsection*{Synthesis and Critical Analysis}
The application of KGEs in specialized domains marks a significant maturation of the field, shifting from theoretical model development to delivering verifiable and transparent solutions for complex, real-world challenges. A clear pattern emerges: the high-stakes nature of these fields (e.g., medicine, intellectual property) necessitates a departure from sole reliance on generic KGE performance metrics (like Hits@K or MRR), which can be misleading or insufficient for building trust [ali2020, rossi2020]. Instead, there is a strong demand for *domain-specific evaluation* and *explainability*.

The evolution from general biological applications [mohamed2020] to highly specific tasks like COVID-19 drug repurposing [islam2023] illustrates this trend. While [mohamed2020] highlights KGE's scalability advantage over traditional graph methods, it implicitly assumes that preserving graph structure is sufficient for biological insight. [islam2023] directly questions this assumption by introducing *molecular docking* as a novel, independent, and physically verifiable evaluation metric. This is a crucial innovation, as it grounds abstract embedding predictions in concrete scientific evidence, addressing the methodological quality concern of evaluating KGEs solely on internal metrics. The integration of rule extraction and explanatory paths in [islam2023] further exemplifies the growing demand for interpretable KGE models, moving beyond "what" is predicted to "why" it is predicted, which is paramount in fields where decisions have direct human impact. This directly connects to the rule-based KGE approaches discussed in Section 4.2, showcasing their practical utility in providing transparency.

Similarly, the multimodal reasoning for specific diseases [zhu2022] and patent metadata analysis [li2022] underscore the need for tailored KGEs. [zhu2022] leverages multimodal information (structural, category, description embeddings) to enrich representations, aligning with the broader trend of multi-modal KGEs (Section 4.3). Its reliance on *manual proofreading* by experts for validation is a strong, albeit resource-intensive, commitment to trustworthiness. This highlights an unstated assumption in many KGE papers: that standard benchmark evaluations are sufficient. In contrast, these domain-specific applications demonstrate that human expert validation is often indispensable for high-stakes scenarios. [li2022]'s focus on explaining "domain expansion profiles" in patents further emphasizes that explainability is not confined to medical applications but is a cross-domain requirement for actionable insights.

A critical tension exists between the desire for highly expressive and complex KGE models (often deep learning-based, as discussed in Section 3) and the imperative for interpretability and verifiable solutions in these high-stakes domains. While complex models might achieve higher predictive accuracy, their black-box nature can hinder adoption where trust and accountability are paramount. The approaches reviewed here attempt to bridge this gap through *post-hoc explanation mechanisms* (e.g., rule extraction, explanatory paths [islam2023]) or *human-in-the-loop validation* (e.g., manual proofreading [zhu2022]). However, the theoretical limitations of post-hoc explanations, which may not fully reflect the model's internal reasoning, remain an open challenge.

In conclusion, the application of KGEs in specialized domains represents a significant paradigm shift. It underscores the field's commitment to real-world impact by demanding models that are not only performant but also interpretable, verifiable, and tailored to specific industry problems. This necessitates a re-evaluation of standard KGE evaluation practices, pushing towards domain-specific metrics and robust explanation mechanisms to build trust and deliver transparent, actionable insights for complex, real-world challenges. This evolution aligns with the broader demand for trustworthy AI, a theme that will continue to shape future KGE research.


### Conclusion and Future Directions

\label{sec:conclusion__and__future_directions}

\section{Conclusion and Future Directions}
\label{sec:conclusion_and_future_directions}

Having journeyed through the foundational theories, advanced architectures, and diverse real-world applications of Knowledge Graph Embeddings (KGEs), this concluding section offers a comprehensive synthesis of the field's intellectual trajectory and charts its future course. The preceding discussions, particularly in Section \ref{sec:applications_and_real-world_impact_of_kge}, underscored the transformative power of KGEs in converting symbolic knowledge into actionable insights across various AI tasks, from link prediction and entity alignment to complex question answering and recommender systems. This progress, driven by innovations spanning geometric models to sophisticated deep learning architectures and the integration of auxiliary information, has significantly advanced our ability to manage and reason with structured knowledge [community_0, community_1, community_2, community_3].

Here, we first consolidate these key developments, highlighting the evolution from basic embedding paradigms to highly expressive, dynamic, and multi-modal solutions. However, despite these remarkable strides, the field of KGE is not without its persistent challenges. We will critically examine the open theoretical gaps and practical limitations that continue to demand rigorous investigation, including the delicate balance between model expressiveness and computational complexity, the imperative for enhanced interpretability, and the need for more robust evaluation methodologies [community_1, community_3, community_6]. Looking forward, this section will then explore emerging trends, such as the increasingly symbiotic relationship between KGEs and large language models, the development of adaptive embedding spaces, and the growing importance of federated and privacy-preserving KGEs. Crucially, we will also address the ethical considerations inherent in the responsible development and deployment of KGE technologies, including issues of bias and transparency [68f34ed64fdf07bb1325097c93576658e061231e, 85064a4b1b96863af4fccff9ad34ce484945ad7b]. This forward-looking perspective aims not only to summarize the past but also to inspire new research directions and guide the responsible advancement of KGE technologies for a more intelligent and equitable future.

\subsection{Summary of Key Developments}
\label{sec:8_1_summary_of_key_developments}

The field of knowledge graph embedding (KGE) has undergone a remarkable evolution, transforming the static, symbolic representation of knowledge into dynamic, continuous vector spaces amenable to machine learning. This progression has been driven by a continuous quest for enhanced expressiveness, efficiency, and robustness, culminating in sophisticated models capable of extracting actionable insights for diverse AI applications [yan2022, choudhary2021].

The initial wave of KGE research was dominated by \textit{foundational geometric and algebraic models}, primarily translation-based approaches. Pioneering models like TransE [bordes2013], TransH [wang2014], TransR [lin2015], and TransD [ji2015] established the paradigm of representing relations as transformations between entity embeddings. These early models addressed the challenge of efficiently capturing relational semantics by modeling relations as simple translations in Euclidean space. TransE, for instance, represented relations as vectors $h + r \approx t$. However, its simple additive nature struggled with complex mapping properties (e.g., one-to-many, many-to-one, one-to-one, many-to-many) because a single relation vector could not adequately distinguish between multiple tail entities for a given head, or vice-versa [wang2014]. To mitigate this, TransH [wang2014] innovated by projecting entities onto relation-specific hyperplanes, allowing for more flexible representations of entities under different relations. Similarly, TransD [ji2015] introduced dynamic mapping matrices to account for the diversity of both relations and entities, reducing parameters compared to its predecessors. While efficient and scalable, a fundamental limitation of these translational models lay in their inability to fully capture intricate logical patterns like symmetry, antisymmetry, inversion, and composition, or to adequately represent polysemous relations, primarily because their linear transformations lacked the necessary algebraic properties [xiao2015]. This limitation spurred the development of \textit{rotational and complex space embeddings}. In contrast to the additive nature of translational models, RotatE [sun2018] modeled relations as rotations in complex vector spaces, demonstrating superior capability in inferring these complex patterns due to the inherent multiplicative properties of complex numbers. Building on this, Rotate3D [gao2020] extended rotations to three-dimensional space, leveraging non-commutative properties for multi-hop reasoning. Further algebraic innovations included HousE [li2022] with Householder transformations and CompoundE [ge2022] and CompoundE3D [ge2023], which combined translation, rotation, and scaling operations, aiming for a more generalized and expressive framework. Other geometric approaches, such as TorusE [ebrahimi2020] and CyclE [ebrahimi2020], explored embedding entities on Lie groups or using different metric choices to enhance expressiveness and circumvent regularization problems. These models continuously pushed the boundary of expressiveness by exploring richer mathematical operations and embedding spaces, with recent theoretical advancements ensuring properties like closure under composition [zheng2024] and resolving theoretical deficiencies like the "Z-paradox" in existing models [liu2024]. The practical limitation of these geometric models often involves a trade-off between increased expressiveness and higher computational complexity or parameter count, making them more challenging to scale than their simpler predecessors.

A significant paradigm shift occurred with the adoption of \textit{deep learning architectures} for KGE, which addressed the problem of capturing highly complex, hierarchical, and non-linear relational patterns that simpler geometric models often missed. Convolutional Neural Networks (CNNs) were introduced to extract local features and model intricate interactions between entity and relation embeddings. Models like AcrE [ren2020] utilized atrous convolutions and residual learning for efficient feature interactions, specifically designed to overcome the "reduced feature resolution" and "information forgotten" issues prevalent in standard deep CNNs, while maintaining parameter efficiency. In contrast, ReInceptionE [xie2020] integrated an Inception network with attention for joint local-global structural information. More advanced CNN-based approaches like M-DCN [zhang2020], CNN-ECFA [hu2024], and SEConv [yang2025] further refined multi-scale feature aggregation and entity-specific common feature learning, demonstrating CNNs' power in capturing non-linear patterns. PConvKB [PConvKB] further enhanced CNN-based KGE by incorporating relation paths, both locally via attention and globally via a novel DIPF measure, distinguishing itself from models that only utilize triple information. Graph Neural Networks (GNNs) and attention mechanisms also became central, leveraging the graph's topology to learn context-dependent embeddings. Early work like Logic Attention-based Neighborhood Aggregation [wang2018] provided inductive capabilities by aggregating neighbor information. DisenKGAT [wu2021] introduced disentangled graph attention networks for more diverse and independent component representations, while GAATs [wang2020] incorporated graph-attenuated attention to better extract features from neighbor nodes and relation paths. However, a key limitation of many GNN-based KGC models is their "data dependence," where performance is sensitive to local graph structures, and their message functions primarily operate in Euclidean space, failing to capture richer intrinsic structural information [shang2024]. To address this, MGTCA [shang2024] proposed a Mixed Geometry Message Function (MGMF) integrating hyperbolic, hypersphere, and Euclidean spaces, alongside a Trainable Convolutional Attention Network (TCAN) that adaptively switches between GNN types (GCN, GAT, KGCAT) to overcome data dependence. The emergence of Transformer architectures further revolutionized KGE, with models like CoKE [wang2019], Knowformer [li2023], and TGformer [shi2025] adapting self-attention mechanisms to capture long-range dependencies and contextualized representations. While these deep learning models achieve state-of-the-art accuracy by automatically extracting features and flexibly modeling interactions, their core practical limitations include increased computational cost, higher parameter counts, and reduced interpretability compared to their geometric counterparts.

Beyond purely structural modeling, the field has increasingly focused on \textit{enriching KGE with auxiliary information, rules, and multi-modality}. This addresses the inherent incompleteness and semantic ambiguity of raw knowledge graphs. Approaches like TransET [wang2021] and TaKE [he2023] incorporated entity type information to provide semantic guidance, leading to more discriminative embeddings for KG completion. Similarly, AEKE [zhang2024] leveraged entity attributes to make KGE models robust against noisy or erroneous triples, a critical practical concern, while HINGE [chen2020] extended beyond triplets to hyper-relational facts. The integration of \textit{logical rules and constraints} also gained prominence, with models like RUGE [guo2017] and RulE [tang2022] iteratively guiding embeddings with soft rules to enforce consistency and enhance reasoning capabilities. These methods solve the problem of injecting prior knowledge and ensuring logical coherence, moving beyond purely data-driven learning. However, a critical assumption here is the availability and quality of such explicit rules, which can be challenging to extract or define for large, dynamic KGs. Furthermore, \textit{multi-modal and cross-domain KGE} emerged to tackle data sparsity and enrich semantic understanding. SSP [xiao2016] projected textual descriptions into semantic space, while more recent works like "Joint Language Semantic and Structure Embedding" [shen2022] fine-tuned pre-trained language models to jointly learn from text and structure. Cross-Domain Knowledge Graph Chiasmal Embedding [liu2023] and multimodal reasoning for specific diseases [zhu2022] exemplify the application of multi-modal KGE to complex, specialized domains. These approaches collectively aim for a more comprehensive and nuanced knowledge representation by leveraging diverse, complementary information sources. A common practical limitation across all these enrichment strategies is the availability and quality of such auxiliary or multi-modal data, and the inherent complexity of effectively fusing heterogeneous information without introducing new biases or noise.

The field has also made significant strides in addressing \textit{practical considerations} such as efficiency, robustness, and adaptability to dynamic environments. To counter the computational and storage demands of large KGs, techniques like knowledge distillation (DualDE [zhu2020]), embedding compression [sachan2020, wang2021], and parameter-efficient learning (EARL [chen2023]) have been developed. System-level optimizations like GE2 [zheng2024] further accelerate KGE training. For instance, DualDE reduces model size and inference time by distilling knowledge from a larger teacher model, whereas LightKG focuses on compressing the embedding vectors themselves. Robustness against noisy data and imbalanced distributions has been tackled through methods like confidence-aware negative sampling [shan2018], reinforcement learning-based noise filtering [zhang2021], and weighted training schemes [zhang2023]. The critical role of negative sampling in KGE training has been extensively studied, leading to sophisticated strategies [zhang2018, qian2021] and even non-sampling approaches [li2021] to improve accuracy and efficiency. The dynamic nature of real-world KGs spurred research into \textit{inductive and continual KGE}. Approaches like Logic Attention-based Neighborhood Aggregation [wang2018] and meta-learning strategies [chen2021, sun2024] enable models to embed unseen entities and adapt to new facts efficiently. These continual learning models, however, face the persistent challenge of catastrophic forgetting, where acquiring new knowledge can degrade performance on previously learned information; techniques like incremental LoRA [liu2024] aim to mitigate this by parameter-efficient adaptation. Finally, the growing concern for data privacy led to the emergence of \textit{Federated KGE (FKGE)}, with solutions addressing communication efficiency [zhang2024], personalization for diverse client data [zhang2024], and security vulnerabilities like poisoning attacks [zhou2024]. While FKGE offers significant privacy benefits, it introduces a trade-off with communication overhead and the complexity of aggregating disparate local models. A comprehensive empirical study by [ali2020] further highlighted that reported KGE performance is highly sensitive to hyperparameter tuning and training configurations, often revealing reproducibility failures. This critique underscores the need for robust training optimization and standardized evaluation practices, as architectural innovation alone does not guarantee reliable real-world performance.

In summary, the narrative arc of KGE research traces a path from simple, elegant geometric models to highly sophisticated deep learning architectures, continuously integrating richer semantic context, logical constraints, and multi-modal information. This evolution reflects a persistent effort to enhance expressiveness, efficiency, and robustness, while increasingly focusing on adaptability to dynamic, inductive, and distributed knowledge environments. The significant strides made in transforming symbolic knowledge into actionable insights have positioned KGE as a cornerstone for modern AI systems, enabling tasks from link prediction and question answering to personalized recommendation and scientific discovery.
\subsection{Open Challenges and Theoretical Gaps}
\label{sec:8_2_open_challenges__and__theoretical_gaps}

Despite the significant advancements in knowledge graph embedding (KGE) research, as highlighted in the preceding sections, several critical open challenges and theoretical gaps persist, representing fertile ground for future investigation. These challenges often stem from the inherent complexity of knowledge graphs, the trade-offs in model design, and the practical demands of real-world applications.

One of the most fundamental and recurring tensions in KGE research is \textbf{balancing model expressiveness with computational complexity and interpretability}. As discussed in Section 2 and 3, the field has progressed from simpler translational models like TransH [wang2014] to highly expressive rotational models such as RotatE [sun2018] and advanced deep learning architectures including GNNs [wang2020, di2023] and Transformers. While models like GoldE [li2024] and Fully Hyperbolic Rotation [liang2024] push the boundaries of expressiveness by capturing intricate logical patterns and hierarchical structures, they often introduce higher computational costs and increased parameter counts. For instance, deep learning models, while powerful in automatically extracting features and modeling non-linear interactions, are notoriously difficult to interpret. This "black-box" nature is a significant practical limitation, especially in high-stakes domains like medicine or finance, where understanding *why* a prediction is made is as crucial as the prediction itself. Although some efforts, such as SpherE [li2024] and Contextualized KGE for Explainable Recommendation (CKGE) [yang2023], attempt to imbue models with interpretability through specific geometric designs or path-based explanations, these are often post-hoc or limited in scope. The challenge lies in developing models that are both highly expressive and inherently interpretable, without sacrificing efficiency. The work on knowledge distillation like DualDE [zhu2020] offers a partial solution by creating smaller, faster student models from larger teachers, but the interpretability of the underlying teacher model often remains unaddressed.

A second major challenge revolves around \textbf{data quality, rule integration, and the 'true' negative distribution in training}. Real-world knowledge graphs are inherently incomplete, noisy, and constantly evolving. As explored in Section 4.2, integrating logical rules (e.g., RUGE [guo2017], RulE [tang2022]) can inject prior knowledge and enhance reasoning. However, the efficient extraction of high-quality, consistent rules from massive and often noisy KGs remains a significant practical hurdle. Furthermore, balancing strict adherence to rules with the flexibility to capture exceptions is a delicate act. Theoretically, the most profound gap in training KGE models lies in resolving issues related to the 'true' negative distribution. Most KGE models rely on negative sampling strategies to create contrastive learning signals, as KGs typically only store positive facts. While methods like NSCaching [zhang2018] and Modality-Aware Negative Sampling (MANS) [zhang2023] have improved sampling efficiency and quality, and non-sampling approaches like NS-KGE [li2021] attempt to circumvent the problem entirely, the fundamental issue persists: the true distribution of false facts is unknown [qian2021, madushanka2024]. Current negative sampling methods are heuristics, implicitly assuming certain distributions of false facts, which may not hold in real-world scenarios. This can lead to suboptimal performance or models that are brittle to noise. For instance, Confidence-Aware Negative Sampling [shan2018] attempts to mitigate noise, but its effectiveness is still bounded by the quality of confidence scores. The challenge is to develop more theoretically grounded and robust training paradigms that are less reliant on such uncertain heuristics, or to devise mechanisms that can reliably infer the 'true' negative distribution.

The \textbf{scalability for extremely large and dynamic knowledge graphs} presents another formidable obstacle. As KGs grow to web-scale, traditional KGE training and inference become computationally prohibitive. Section 6.1 highlighted efforts in efficiency and compression, such as parameter-efficient learning (EARL [chen2023]), embedding compression [sachan2020, wang2021], and optimized systems like GE2 [zheng2024]. However, these often involve trade-offs. For example, while CPa-WAC [modak2024] uses graph partitioning to improve scalability for GNN-based KGEs, partitioning can inherently lead to a loss of global graph structure, potentially impacting the quality of embeddings that rely on long-range dependencies. The challenge is to devise methods that can scale to billions of entities and relations without sacrificing the richness of the learned representations. Moreover, real-world KGs are rarely static; new entities, relations, and facts emerge constantly. This necessitates the development of \textbf{truly generalizable inductive and continual models}. While meta-learning approaches like MorsE [chen2021] and MetaHG [sun2024] and parameter-efficient adaptation techniques like incremental LoRA (FastKGE [liu2024]) have shown promise in handling unseen entities and efficiently updating models, they still grapple with the problem of catastrophic forgetting, where new knowledge acquisition degrades performance on previously learned information. A truly generalizable inductive model should be able to embed entirely novel entities or relations with minimal or no prior context, a capability that remains largely elusive. Furthermore, the complexities of Federated KGE (FKGE), as discussed in Section 5.3, introduce additional scalability and privacy-preserving challenges, such as communication efficiency [zhang2024] and personalization across diverse client data [zhang2024], while also guarding against adversarial attacks [zhou2024].

Finally, the field continues to grapple with the \textbf{need for more robust evaluation metrics and improved reproducibility}. As highlighted in Section 6.3, studies like [ali2020] and [rossi2020] have exposed significant reproducibility failures and biases in standard evaluation practices, such as the over-representation of certain entities in benchmarks. The high sensitivity of KGE performance to hyperparameter tuning, which varies across datasets [lloyd2022], further complicates fair comparisons and hinders scientific progress. The current metrics (e.g., MRR, Hits@K) may not fully capture the nuances of complex relational patterns, the quality of explanations, or the utility in specific downstream applications (e.g., set retrieval [li2024], molecular docking for drug repurposing [islam2023]). The challenge is twofold: first, to develop standardized, robust, and domain-aware evaluation protocols that accurately reflect model performance across diverse tasks and real-world scenarios; and second, to foster a culture of rigorous reproducibility by providing comprehensive code, configurations, and detailed experimental setups, as advocated by initiatives like LibKGE [broscheit2020].

In summary, future research must push the boundaries of KGE capabilities by addressing these multifaceted challenges. This includes developing hybrid architectures that intelligently balance expressiveness, efficiency, and interpretability; designing more theoretically sound training paradigms that overcome the limitations of heuristic negative sampling; creating truly scalable and adaptable models for dynamic, web-scale KGs; and establishing more rigorous and comprehensive evaluation frameworks to ensure reliable and trustworthy advancements in the field.
\subsection{Emerging Trends and Ethical Considerations}
\label{sec:8_3_emerging_trends__and__ethical_considerations}

Building upon the persistent open challenges and theoretical gaps identified in the preceding subsection, the field of knowledge graph embedding (KGE) is now witnessing a convergence of cutting-edge technological trends and a growing imperative to address crucial ethical considerations. These emerging directions are poised to redefine the landscape of KGE research, pushing towards more intelligent, robust, and socially responsible AI systems.

One of the most significant emerging trends is the \textbf{deeper integration with pre-trained language models (PLMs)} for richer semantic understanding. Traditional KGE models, as discussed in Sections 2 and 3, primarily derive embeddings from the structural patterns of knowledge graphs (KGs). While effective, they often struggle with entities or relations that are sparse in the graph but have rich textual descriptions. PLMs, on the other hand, excel at capturing nuanced semantic information from vast text corpora. The synergy between these two paradigms promises to create KGEs that are not only structurally informed but also semantically grounded. Early efforts in this direction, such as type-augmented frameworks like TaKE [he2023] or systems like Marie and BERT [zhou2023] for chemistry question answering, have demonstrated the benefits of integrating textual features. These approaches, however, often treat text as auxiliary information rather than deeply fusing it with structural learning. The next generation of KGE models will likely move towards joint learning frameworks where PLMs are fine-tuned with KG structures, or where KGE models directly leverage contextualized embeddings from PLMs to enhance entity and relation representations. This integration aims to solve the problem of data sparsity and enhance the zero-shot or few-shot learning capabilities of KGEs, allowing them to generalize to unseen entities with only textual descriptions. However, this trend introduces practical limitations, particularly in computational cost, as large PLMs are resource-intensive. Furthermore, it raises theoretical concerns about how to effectively align the continuous vector spaces of PLMs with the discrete, relational logic of KGs, and how to prevent the propagation of biases inherent in large text corpora into the KGEs.

Another promising direction involves the development of \textbf{more adaptive multi-curvature embeddings}. As highlighted in Section 2.3 and 5.1, KGs often exhibit heterogeneous structures: some parts are hierarchical (best modeled by hyperbolic spaces [pan2021, liang2024]), others are cyclic or grid-like (suited for Euclidean spaces), and some might benefit from spherical geometries. The assumption that a single geometric space can optimally represent all facets of a complex KG is increasingly being questioned. Recent work, such as the Mixed Geometry Message and Trainable Convolutional Attention Network (MGTCA) [shang2024], explicitly addresses this by integrating messages and scoring functions from hyperbolic, hypersphere, and Euclidean spaces. The core innovation here is the ability to adaptively model diverse local structures within a single framework. This approach claims to solve the problem of representational rigidity inherent in single-geometry models, offering higher fidelity for complex KGs. The conditions for its success typically involve KGs with clearly identifiable mixed structural patterns. However, practical limitations include increased model complexity and potential training instability due to the need to optimize parameters across multiple geometric spaces. Compared to fully hyperbolic models like [liang2024] which commit to a single non-Euclidean space, mixed-geometry models offer greater flexibility but demand more sophisticated optimization strategies. This trend signifies a shift towards more nuanced and geometrically expressive KGEs, moving beyond the "one-size-fits-all" assumption of embedding spaces.

The increasing demand for data privacy and distributed computing is driving advancements in \textbf{federated and privacy-preserving KGE}. As discussed in Section 5.3, traditional KGE models often require centralizing sensitive KG data, which is problematic for privacy-conscious applications. Federated KGE (FKGE) addresses this by enabling collaborative model training across distributed KGs without direct data sharing. Innovations in this area focus on overcoming critical challenges such as communication efficiency and data heterogeneity. For instance, Communication-Efficient Federated KGE [zhang2024] proposes Entity-Wise Top-K Sparsification to reduce the volume of transmitted parameters, achieving significant efficiency gains with negligible performance degradation. Similarly, Personalized Federated KGE [zhang2024] tackles semantic disparities among clients by learning personalized supplementary knowledge, moving beyond a universal global model. These methods aim to solve the dual problem of privacy preservation and collaborative learning. However, the FKGE paradigm introduces new practical and theoretical limitations. Communication overhead, even with sparsification, remains a challenge, and ensuring fair and effective aggregation across heterogeneous client data is complex. Moreover, FKGE systems are vulnerable to adversarial attacks, as demonstrated by the Poisoning Attack on Federated KGE [zhou2024], where malicious clients can inject poisoned data to force victim clients to predict false facts. This highlights a critical, often unstated, assumption in FKGE: that participating clients are trustworthy. The development of robust defense mechanisms against such attacks is an urgent, unresolved debate in this emerging field.

Beyond technological advancements, the growing deployment of KGE in real-world applications necessitates a rigorous examination of \textbf{ethical considerations}. A primary concern is the \textbf{potential for biases in learned representations}. KGE models, by learning from existing KGs, inevitably inherit and can amplify societal biases present in the training data. These biases, whether related to gender, race, socioeconomic status, or other sensitive attributes, can manifest as unfair or discriminatory outcomes in downstream applications. For example, a KGE-powered recommender system might perpetuate stereotypes, or a KGE used in a hiring context could inadvertently discriminate. As highlighted in Section 6.3, the quality and characteristics of datasets significantly impact KGE performance [lloyd2022], and biases in these datasets are implicitly encoded into the embeddings. The problem is particularly insidious because these biases are often implicit and difficult to detect or quantify within high-dimensional vector spaces, unlike explicit biases in rule-based systems. This challenges the assumption that KGEs are neutral representations of knowledge. Future research must focus on developing fairness-aware KGE models, bias detection techniques, and debiasing strategies that can mitigate these risks without unduly compromising model utility.

This leads directly to the imperative for the \textbf{responsible use of KGE in sensitive applications}. KGEs are increasingly deployed in high-stakes domains such as healthcare (e.g., drug repurposing for COVID-19 [islam2023]), finance, legal systems, and talent management [yang2023]. In these contexts, erroneous or biased predictions can have severe societal, economic, or even life-threatening consequences. The methodological quality of KGE models, including their robustness to noise and generalizability, becomes paramount. For instance, the molecular-evaluated and explainable drug repurposing approach [islam2023] explicitly integrates domain-specific validation (molecular docking) and explanation mechanisms, recognizing the critical need for reliability and transparency in medical applications. Similarly, the Contextualized KGE for Explainable Talent Training Course Recommendation [yang2023] emphasizes providing interpretable recommendations. The unstated assumption that KGE models, once validated on standard benchmarks, are fit for sensitive real-world deployment is fundamentally flawed. There is a pressing need for domain-specific ethical guidelines, robust auditing mechanisms, and perhaps even regulatory frameworks to ensure that KGE systems are deployed responsibly, with human oversight and accountability.

Finally, the discussion on responsible use underscores the \textbf{imperative for transparent and explainable AI systems}. Many advanced KGE models, particularly those leveraging deep learning architectures (Section 3), operate as "black boxes." While they achieve high predictive performance, they offer little insight into *why* a particular prediction or embedding was generated. This lack of transparency hinders trust, makes debugging difficult, and impedes accountability, especially when KGEs are used in critical decision-making processes. As discussed in the previous subsection, achieving inherent interpretability without sacrificing expressiveness remains an open challenge. While post-hoc explanation methods exist, they can sometimes be misleading or incomplete. Models like SpherE [li2024] aim for inherent interpretability through their geometric design, and path-based explanations in recommendation systems [sun2018, yang2023] offer valuable insights. However, these are often specific to certain model types or applications. The field must move towards developing KGE models that are inherently more transparent, allowing stakeholders to understand the underlying reasoning. This involves research into intrinsically interpretable architectures, robust and faithful explanation techniques, and user-centric interfaces that present explanations in an understandable and actionable manner.

In conclusion, the next generation of KGE research is characterized by a dual mandate: to push the boundaries of technological innovation through deeper integration with PLMs, adaptive geometries, and distributed learning, while simultaneously upholding societal responsibility by rigorously addressing biases, ensuring responsible deployment in sensitive applications, and prioritizing transparency and explainability. These intertwined trends and ethical considerations will shape the research agenda, ensuring that KGE not only advances AI capabilities but also serves humanity ethically and equitably.


