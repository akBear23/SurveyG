\subsection*{Background: Knowledge Graphs}
Knowledge Graphs (KGs) have emerged as a pivotal technology for organizing and representing real-world knowledge in a structured, machine-readable format. Fundamentally, a knowledge graph is a directed graph composed of entities (nodes) and relations (edges) that connect them \cite{ge2023, dai2020}. Each piece of information within a KG is typically represented as a triple (head entity, relation, tail entity), often denoted as $(h, r, t)$. For instance, the triple (Barack Obama, bornIn, Hawaii) explicitly states that "Barack Obama was born in Hawaii." This structured representation allows for unambiguous storage and retrieval of facts, forming a rich network of interconnected information \cite{cao2022}.

The conceptual roots of knowledge graphs can be traced back to early efforts in Artificial Intelligence to represent knowledge symbolically. Semantic networks, developed in the 1960s and 1970s, were among the first attempts to model knowledge as a graph of concepts and relationships. These early networks, while foundational, often suffered from ambiguity, a lack of formal semantics, and limited scalability, making complex reasoning difficult. The subsequent rise of expert systems in the 1980s, which relied on explicit rules and symbolic logic, further highlighted the challenges of managing large, intricate knowledge bases.

The advent of the Semantic Web in the early 2000s marked a significant step towards formalizing knowledge representation on a global scale. Technologies like Resource Description Framework (RDF) and Web Ontology Language (OWL) provided standardized frameworks for defining ontologies and expressing knowledge with formal semantics, aiming to make web content machine-understandable. This era saw the emergence of early large-scale KGs like DBpedia, which systematically extracted structured information from Wikipedia infoboxes, effectively bridging the gap between unstructured text and structured knowledge \cite{dai2020}. DBpedia's success demonstrated the potential of automatically constructing KGs from existing data sources, laying the groundwork for more ambitious projects.

Modern knowledge graphs, such as Freebase and Wikidata, represent the culmination of these historical developments, leveraging massive datasets and advanced computational techniques. Freebase, a collaborative knowledge base initially developed by Metaweb and later acquired by Google, played a crucial role in organizing a vast array of general-purpose facts, significantly contributing to Google's own proprietary Knowledge Graph that powers its search engine \cite{ge2023}. Wikidata, an open and collaborative knowledge base maintained by the Wikimedia Foundation, has become a central hub for structured data across various Wikimedia projects and beyond. Its multilingual nature and community-driven approach underscore its role in democratizing access to structured world knowledge and enabling a wide range of intelligent systems, from virtual assistants to sophisticated data analytics platforms. These modern KGs are characterized by their immense scale, dynamic nature, and their ability to integrate information from diverse sources, making them indispensable resources for contemporary AI applications.

Despite their power in organizing information, symbolic knowledge graphs inherently face several significant challenges that limit their utility, particularly in the context of modern data-driven AI.

1.  \textbf{Computational Inefficiency and Scalability}: Reasoning directly with symbolic representations, especially on large-scale KGs containing billions of facts, is computationally intensive. Tasks like inferring new facts through logical rules or answering complex queries often involve combinatorial search problems, leading to prohibitive computational costs and poor scalability \cite{ge2023, dai2020}. For instance, traditional rule-based inference engines struggle with the sheer volume of facts in KGs like Wikidata, where deriving multi-hop relations can quickly become intractable. This inefficiency makes real-time applications challenging and limits the depth of reasoning that can be performed.

2.  \textbf{Sparsity and Incompleteness}: Real-world knowledge graphs are notoriously incomplete. Even the largest KGs, such as Freebase or DBpedia, contain only a fraction of all possible facts about entities and relations. Symbolic methods struggle to generalize from observed facts to infer missing ones, often requiring explicit rules or complete data to make deductions. This "cold-start" problem, where new entities or relations lack sufficient explicit connections, is a major bottleneck \cite{dai2020}. For example, if a KG lacks the explicit triple (Albert Einstein, studiedAt, ETH Zurich), a purely symbolic system might fail to infer this, even if it has related facts. This limitation contrasts sharply with human ability to infer and generalize from partial information.

3.  \textbf{Difficulty in Capturing Nuanced Semantics and Similarity}: Symbolic representations are discrete and rigid. They treat each entity and relation as a distinct, atomic symbol, making it difficult to capture subtle semantic similarities or relationships between them. For instance, "car" and "automobile" are semantically very close, but a symbolic KG would treat them as entirely separate entities unless explicitly linked by an equivalence relation. Similarly, understanding that "fatherOf" is the inverse of "childOf" requires explicit logical rules, rather than being inherently captured by the representation itself. This rigidity hinders tasks that rely on semantic proximity, such as recommendation systems or natural language understanding, where nuanced meanings are paramount.

4.  \textbf{Incompatibility with Machine Learning}: Perhaps the most critical limitation in the era of deep learning is the inherent incompatibility of symbolic representations with numerical machine learning models. Modern AI algorithms, particularly neural networks, operate on continuous vector spaces. Directly feeding discrete symbols into these models is inefficient and often requires extensive feature engineering, which is labor-intensive and prone to error. This disconnect prevents KGs from being seamlessly integrated into powerful, end-to-end machine learning pipelines.

These collective challenges of symbolic KGs---computational inefficiency, pervasive incompleteness, difficulty in capturing nuanced semantics, and incompatibility with modern machine learning paradigms---collectively motivate the need for more advanced representation techniques. This is precisely where Knowledge Graph Embedding (KGE) emerges as a transformative solution. KGE aims to address these limitations by transforming entities and relations from their sparse, discrete symbolic forms into dense, continuous, low-dimensional vector representations, often referred to as embeddings, in a continuous vector space \cite{cao2022, ge2023}. By embedding knowledge into a continuous space, KGE models enable several critical advantages: capturing semantic similarity, enhancing computational efficiency, handling incompleteness through generalization, and facilitating seamless integration with machine learning models. This paradigm shift from explicit symbolic reasoning to implicit, distributed representations in vector spaces has unlocked the full potential of knowledge graphs, making them more accessible, actionable, and powerful for a diverse range of intelligent systems.