The previous subsection on Inductive and Continual KGE underscored the necessity for KGE models to adapt to dynamic and evolving knowledge. Extending this, the increasing demand for privacy-aware AI systems, coupled with the distributed nature of real-world knowledge, has propelled research into \textit{Federated and Privacy-Preserving Knowledge Graph Embedding} (FKGE). This paradigm allows multiple clients to collaboratively train a shared KGE model without directly sharing their sensitive local knowledge graphs, thereby addressing critical privacy concerns. However, this distributed setting introduces unique challenges related to communication efficiency, personalization for diverse client data, and security vulnerabilities, which are not typically encountered in centralized KGE training. Furthermore, specific privacy-preserving mechanisms like differential privacy and secure multi-party computation become crucial to bolster the inherent privacy of federated learning.

\begin{table}[htbp]
\centering
\caption{Comparative Framework for Federated and Privacy-Preserving KGE Approaches}
\label{tab:fkge_framework}
\begin{tabularx}{\textwidth}{|p{0.15\textwidth}|p{0.16\textwidth}|p{0.16\textwidth}|p{0.16\textwidth}|p{0.16\textwidth}|p{0.16\textwidth}|}
\hline
\textbf{Dimension} & \textbf{Communication Efficiency} (\cite{zhang2024_feds}) & \textbf{Personalization} (\cite{zhang2024_pfedeg}) & \textbf{Security (Poisoning Attack)} (\cite{zhou2024}) & \textbf{Privacy-Preserving Defense (DP)} (\cite{dpflames}) & \textbf{Cross-Domain Alignment} (\cite{fedcke}) \\
\hline
\textbf{Primary Problem} & High communication costs in FL due to large parameters and extensive communication rounds. & Semantic disparities and heterogeneity among clients; "one-size-fits-all" global model. & Vulnerability to malicious clients injecting poisoned data to manipulate predictions. & Quantifying and defending against privacy threats (e.g., inference attacks) in FKGE. & Fusing/embedding cross-domain KGs securely without sharing raw data, especially for entity/relation alignment. \\
\hline
\textbf{Core Innovation} & FedS: Bidirectional Entity-Wise Top-K Sparsification and Intermittent Synchronization Mechanism. & PFedEG: Client-wise relation graph to learn personalized supplementary knowledge from "neighboring" clients. & Systematizes poisoning attacks; KG component inference + dynamic poisoning scheme. & DP-Flames: Differentially private FKGE with private selection and adaptive budget allocation, exploiting entity-binding sparse gradients. & FedCKE: Encrypted entity/relation alignment using vertical FL, followed by parameter-secure aggregation in horizontal FL. \\
\hline
\textbf{Conditions for Success} & KGE models where changes in entity embeddings are sparse or can be effectively approximated by top-K changes. & Clients have discernible "affinity" or semantic overlap, allowing meaningful knowledge transfer. & Attacker can infer target relations and craft poisoned data; aggregation mechanism is susceptible. & Acceptable privacy-utility trade-off; sparse gradient property of KGE holds. & Existence of shared entities/relations across domains; secure encryption/alignment protocols. \\
\hline
\textbf{Theoretical Limitations} & Potential loss of precision for less significant embedding changes; risk of embedding inconsistency. & Defining "affinity" robustly; potential for personalized models to diverge too much from global consensus. & Assumes attacker has sufficient control over local training/updates; privacy mechanisms might hinder detection. & Noise addition inherently reduces utility; tight privacy accounting is complex; may not fully protect against all attack vectors. & Complexity of encrypted alignment; potential for information leakage during alignment or aggregation if not perfectly secure. \\
\hline
\textbf{Practical Limitations} & Requires careful tuning of K (top-K); overhead of sparsification/desparsification. & Increased complexity in managing client-wise graphs; computational cost of personalized aggregation. & No direct defense mechanism proposed; identifying malicious clients is hard in practice. & Tuning privacy budget (epsilon) is critical; computational overhead of DP mechanisms. & High computational cost for encryption/decryption; scalability for very large cross-domain KGs. \\
\hline
\textbf{Comparison to Alternatives} & Focuses on parameter size reduction per round, unlike general FL methods reducing rounds (e.g., multiple local epochs). & Moves beyond global model averaging, offering client-specific adaptations, unlike FedE or FedEC. & First systematic analysis of poisoning in FKGE, highlighting a novel attack vector compared to centralized KGE. & First holistic study of privacy threats and defense (DP) for FKGE, offering a better privacy-utility trade-off than naive DP. & Addresses cross-domain fusion with privacy, unlike general FKGE that assumes shared schema or simple aggregation. \\
\hline
\end{tabularx}
\end{table}

\subsubsection*{Communication-Efficient Federated KGE}
The distributed nature of Federated Learning inherently leads to significant communication overhead, especially when dealing with large models like KGEs that involve numerous entity and relation embeddings. This challenge is exacerbated by the need for multiple communication rounds between clients and a central server. While general Federated Learning (FL) research has explored techniques like increasing local training epochs to reduce communication rounds, these often overlook the substantial size of parameters transmitted in each round.

\cite{zhang2024_feds} (FedS) directly addresses this by proposing \textbf{Communication-Efficient Federated Knowledge Graph Embedding with Entity-Wise Top-K Sparsification}. The problem it claims to solve is the high communication cost in FKGE, which hinders its scalability and deployment. Its core innovation lies in a bidirectional communication-efficient strategy. During the upload phase, clients identify and transmit only the Top-K entity embeddings that have undergone the most significant changes, rather than sending the entire model. Similarly, during download, the server performs personalized embedding aggregation for each client and then transmits only the Top-K aggregated embeddings back. This mechanism is complemented by an Intermittent Synchronization Mechanism designed to mitigate the negative effects of embedding inconsistency among shared entities, a common issue arising from the heterogeneity of federated KGs.

FedS succeeds under conditions where the most significant updates to entity embeddings can be captured by a sparse subset, allowing for substantial data reduction without critical information loss. Quantitative evidence from experiments on three datasets demonstrates that FedS significantly enhances communication efficiency with negligible, or even no, performance degradation. For instance, it can reduce communication by a factor of 50-100x while maintaining competitive link prediction accuracy. However, a theoretical limitation is the potential for precision loss for less significant, yet cumulatively important, embedding changes that fall outside the Top-K. This occurs because the Top-K selection implicitly assumes that smaller changes are less critical, an assumption that might not hold if many small changes collectively contribute to a significant shift in representation. Practically, determining the optimal value of K (the number of top entities to transmit) is crucial and dataset-dependent, requiring careful tuning. An overly aggressive K could lead to slower convergence or suboptimal model quality. Compared to general FL communication reduction methods that primarily focus on reducing the number of communication rounds (e.g., by increasing local computation), FedS specifically targets the *size* of the transmitted parameters within each round, offering a complementary approach. This aligns with broader efficiency concerns in KGE, similar to how \cite{zhu2020}'s DualDE uses knowledge distillation or \cite{sachan2020}'s Knowledge Graph Embedding Compression (LightKG) reduce parameter size in centralized settings, but FedS adapts this principle to the distributed FL environment.

\subsubsection*{Personalized Federated KGE and Cross-Domain Alignment}
A fundamental tension in Federated Learning is between learning a robust global model and accommodating the inherent data heterogeneity across diverse clients. For FKGE, this translates to semantic disparities, where different clients might have distinct knowledge patterns or even conflicting facts. Traditional federated averaging, which aims for a universal global model, often overlooks these client-specific semantic nuances, potentially leading to a "one-size-fits-all" model that performs suboptimally for individual clients.

\cite{zhang2024_pfedeg} (PFedEG) addresses this by proposing \textbf{Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph}. The problem it claims to solve is the compromise in embedding quality due to semantic disparities among clients and the limitations of a universal global model. Its core innovation is to move beyond a global consensus by learning personalized supplementary knowledge for each client. This is achieved through a novel "client-wise relation graph," which captures the semantic relevance or "affinity" between clients. PFedEG learns personalized supplementary knowledge for each client by amalgamating entity embeddings from its "neighboring" clients (based on their affinity) rather than simply averaging all client contributions. Each client then conducts personalized embedding learning using its local triples and this personalized supplementary knowledge.

PFedEG succeeds when there are meaningful semantic relationships or overlaps between client KGs that can be captured by the client-wise relation graph, allowing for beneficial knowledge transfer. Extensive experiments on four benchmark datasets demonstrate its superiority over state-of-the-art models, indicating that personalized aggregation significantly improves embedding quality. A theoretical limitation lies in the robust definition and dynamic adaptation of "affinity" within the client-wise relation graph; if affinity is poorly defined or static, it might not accurately reflect evolving semantic relationships, potentially leading to suboptimal personalization or even negative transfer. This is because the effectiveness of knowledge transfer is highly sensitive to the semantic alignment between source and target clients. Practically, managing and updating the client-wise relation graph adds complexity to the FKGE setup, and the computational cost of personalized aggregation can be higher than simple federated averaging. This approach directly contrasts with the implicit assumption of a homogeneous global model in many FL setups, offering a more nuanced way to handle non-IID data distributions. This challenge is also recognized in general inductive KGE where meta-learning approaches like \cite{chen2021}'s MorsE aim to learn transferable knowledge across diverse tasks, but PFedEG specializes this for the federated context.

Extending the notion of personalization and heterogeneity to distinct knowledge domains, \cite{fedcke} proposes \textbf{Cross-domain Knowledge Graph Embedding in Federated Learning (FedCKE)}. This work tackles the challenge of securely interrelating, fusing, and embedding KGs from different domains without sharing raw data. Unlike PFedEG which focuses on semantic disparities within a broadly shared schema, FedCKE explicitly addresses cross-domain scenarios where entity/relation alignment is a primary hurdle. Its core innovation is an inter-domain encrypted entity/relation alignment method, leveraging encrypted sample alignment from vertical federated learning. This allows clients to identify shared entities/relations without revealing their full KG structures. Subsequently, a parameter-secure aggregation method from horizontal federated learning is used on the server to combine embeddings. FedCKE is particularly successful when there are common entities or relations that can serve as anchors for alignment across disparate domains. A theoretical limitation is the inherent complexity and potential computational overhead of cryptographic operations for alignment, which could limit scalability for very large KGs or a high number of clients. Practically, the reliance on encrypted alignment adds significant computational cost, making it more resource-intensive than simpler federated averaging methods. FedCKE thus offers a more robust solution for privacy-preserving *cross-domain* knowledge fusion, a problem distinct from but related to the semantic personalization addressed by PFedEG. Both methods highlight a pattern of adapting general FL principles (vertical FL for alignment, horizontal FL for aggregation) to the specific needs of KGE, demonstrating a convergence of techniques.

\subsubsection*{Security Vulnerabilities and Privacy-Preserving Defenses}
While Federated Learning is designed to preserve data privacy by keeping raw data local, it does not inherently guarantee security against malicious actors. A critical, yet often unexplored, vulnerability in FKGE is the threat of poisoning attacks, where a malicious client injects carefully crafted data to manipulate the global model's behavior or force specific false predictions.

\cite{zhou2024} systematically explores this by presenting \textbf{Poisoning Attack on Federated Knowledge Graph Embedding}. The problem it claims to solve is the lack of understanding and systematization of poisoning attacks in FKGE, which can lead to biased decisions in downstream applications. Its core innovation is a novel framework for poisoning attacks that forces victim clients to predict specific false facts. Unlike centralized KGEs where attackers might directly inject poisoned triples, FKGE's distributed nature makes this challenging. Instead, the attacker in \cite{zhou2024}'s framework first infers targeted relations in the victim's local KG via a "KG component inference attack." Then, to accurately mislead the victim's embeddings through the federated aggregation process, the attacker locally trains a shadow model using the poisoned data and employs an optimized dynamic poisoning scheme to adjust its model updates.

This attack framework demonstrates remarkable effectiveness, achieving success rates as high as 100\% on models like TransE with WN18RR, while keeping the original task's performance nearly unchanged. This success is contingent on the attacker's ability to accurately infer target relations and craft poisoned data that can subtly influence the global model through aggregation. A significant theoretical limitation is that the paper identifies a problem without offering a direct defense mechanism, though it lays crucial groundwork for future robust FKGE designs. Practically, implementing such an attack requires sophisticated knowledge of the victim's KG structure and the FKGE aggregation mechanism. The implicit assumption here is that the federated server or other clients cannot easily detect or filter these malicious updates, highlighting a critical gap in current FKGE security protocols. This work serves as a stark reminder that privacy-preserving distributed training, while beneficial, introduces new attack surfaces that require dedicated security research, much like the broader field of KGE needs robustness against noisy data, as explored by \cite{zhang2024_pfedeg}'s AEKE framework in centralized settings.

To counter such privacy threats and provide stronger guarantees, research has turned to mechanisms like Differential Privacy (DP). \cite{dpflames} conducts the first holistic study of privacy threats on FKGE from both attack and defense perspectives, proposing \textbf{DP-Flames}. The problem it claims to solve is quantifying and defending against inference attacks that reveal the existence of KG triples from victim clients. Its core innovation is a novel differentially private FKGE with private selection, which offers a better privacy-utility trade-off. DP-Flames exploits the entity-binding sparse gradient property of FKGE and incorporates state-of-the-art private selection techniques, coupled with an adaptive privacy budget allocation policy. This allows it to dynamically adjust defense magnitude during training. DP-Flames demonstrates its effectiveness by successfully mitigating inference attacks, reducing their success rate significantly (e.g., from 80-90\% to 10-20\% on average) with only a modest utility decrease. Unlike \cite{zhou2024} which focuses on attack vectors, DP-Flames directly provides a defense mechanism. A theoretical limitation of DP is the inherent trade-off between privacy and utility; adding noise to achieve privacy guarantees inevitably reduces the accuracy of the learned model. The challenge lies in finding the optimal balance, which is often complex to prove theoretically for complex models. Practically, tuning the privacy budget (epsilon) is critical and highly sensitive to the desired level of privacy and acceptable utility loss. DP-Flames represents a significant step towards building truly privacy-preserving FKGE systems, moving beyond the inherent privacy of FL to offer quantifiable privacy guarantees against inference attacks. This contrasts sharply with the attack-focused work of \cite{zhou2024}, highlighting the two sides of the security coin in FKGE.

\subsubsection*{Patterns, Tensions, and Future Directions}
The emerging field of Federated and Privacy-Preserving KGE is characterized by a critical tension between three key dimensions: \textit{privacy}, \textit{utility} (model performance and personalization), and \textit{efficiency/security}. Solutions like FedS \cite{zhang2024_feds} strive for communication efficiency, often by making trade-offs in the precision of transmitted information via Top-K sparsification, which could theoretically impact model utility if critical but small changes are discarded. PFedEG \cite{zhang2024_pfedeg} pushes for personalization to enhance utility by tailoring knowledge transfer, but this adds complexity and potentially increases communication overhead for personalized aggregation. Similarly, FedCKE \cite{fedcke} enables cross-domain knowledge fusion with privacy, but at the cost of increased computational complexity due to encryption.

A recurring pattern is the adaptation of techniques from general FL or other AI domains to the specific challenges of KGE. For instance, sparsification for communication efficiency in FedS draws inspiration from general FL compression, while personalization strategies in PFedEG echo meta-learning's goal of transferability for diverse tasks, as seen in \cite{sun2024}'s MetaHG for dynamic KGE. The use of vertical FL for alignment in FedCKE further exemplifies this cross-domain technique transfer. The poisoning attack demonstrated by \cite{zhou2024} reveals that the privacy guarantees of FL do not inherently translate to security, highlighting a fundamental vulnerability that could severely compromise model utility and trustworthiness. This systematically challenges the unstated assumption across much of the constructive FKGE research (i.e., not attack-focused) that clients are benign. In response, DP-Flames \cite{dpflames} introduces differential privacy as a robust defense mechanism, directly addressing the security gap exposed by poisoning attacks. This highlights a clear evolutionary trajectory: from initial focus on efficiency and basic privacy (FedS), to enhanced utility through personalization (PFedEG) and cross-domain integration (FedCKE), and finally to explicit security and quantifiable privacy guarantees (DP-Flames countering \cite{zhou2024}'s attacks).

This field faces several open challenges. The generalizability of current FKGE solutions is an area requiring further scrutiny. While methods show promise on benchmark datasets, their performance in highly heterogeneous, real-world distributed KGs with varying data quality and client capabilities remains to be fully evaluated. The trade-off between privacy guarantees (e.g., epsilon in DP) and model utility needs more sophisticated theoretical and empirical analysis, especially for complex KGE models. Furthermore, the field needs to move beyond simple link prediction metrics to assess the impact of FKGE on downstream applications, similar to the broader KGE field's shift towards application-specific evaluations discussed in Subsection 7.3.

In conclusion, Federated and Privacy-Preserving KGE represents a crucial frontier for leveraging decentralized knowledge while respecting privacy. The initial solutions address communication efficiency and personalization, but the identified security vulnerabilities and the need for stronger privacy guarantees underscore the need for a holistic approach that integrates robust defense mechanisms. Future research must focus on developing FKGE frameworks that can simultaneously optimize for privacy, utility, efficiency, and security, ensuring that collaborative KGE models are not only effective but also trustworthy and resilient in real-world, privacy-sensitive environments. This will likely involve more sophisticated privacy-preserving techniques beyond simple non-sharing of raw data, and adaptive personalization strategies that dynamically adjust to evolving client needs and data distributions.