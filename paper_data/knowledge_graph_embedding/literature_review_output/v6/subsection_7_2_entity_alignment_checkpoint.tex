\subsection{Entity Alignment}
Building upon the foundational capabilities of Knowledge Graph Embedding (KGE) for tasks like link prediction and knowledge graph completion, KGEs have emerged as a powerful paradigm for Entity Alignment (EA). Entity alignment is the critical task of identifying equivalent entities across different knowledge graphs (KGs), a process essential for integrating heterogeneous knowledge sources, enriching existing KGs, and enabling comprehensive cross-KG reasoning \cite{zhu2024, dai2020}. The core idea is to embed entities from multiple KGs into a shared, low-dimensional vector space, where semantically equivalent entities are expected to be close to each other. This transforms the symbolic matching problem into a geometric proximity search, offering a data-driven approach to overcome the challenges of schema heterogeneity and data incompleteness inherent in real-world KGs.

The application of KGE to entity alignment has evolved significantly, addressing key challenges such as the scarcity of labeled alignment data, the integration of diverse entity features, and the leveraging of meta-information. Early approaches often adapted general KGE models (e.g., \textsf{TransE}, \textsf{TransH} \cite{wang2014}, \textsf{TransD} \cite{ji2015}) by training them on a small set of pre-aligned entities and then using distance metrics (e.g., Euclidean or cosine similarity) in the embedding space to find new alignments. However, these basic adaptations often struggled with the inherent differences in structure and semantics between KGs, as well as the practical limitation of requiring substantial initial alignment seeds. This motivated the development of more specialized KGE-based EA methods, which can be broadly categorized into iterative/self-training, multi-feature integration, and schema-aware approaches.

\subsubsection*{Iterative and Semi-Supervised Alignment}
A significant challenge in entity alignment is the scarcity of labeled training data, as manual annotation of equivalent entities across large KGs is prohibitively expensive. This limitation is particularly acute when dealing with KGs from different domains or languages.
\begin{itemize}
    \item \textbf{Bootstrapping Entity Alignment}: To mitigate the reliance on extensive prior alignment, \textsf{Bootstrapping Entity Alignment with Knowledge Graph Embedding} \cite{sun2018} proposed an iterative approach.
    \begin{enumerate}
        \item \textbf{Problem Solved}: Addresses the lack of sufficient prior alignment as labeled training data for embedding-based EA.
        \item \textbf{Core Innovation}: It iteratively labels likely entity alignments as training data, progressively expanding the set of known alignments. Furthermore, it incorporates an "alignment editing method" to reduce error accumulation, a critical concern in self-training loops.
        \item \textbf{Mechanism}: The process typically starts with a small set of seed alignments. A KGE model is trained using these seeds, and then entity similarities in the learned embedding space are used to identify new, highly confident alignment candidates. These candidates are added to the training set, and the process repeats. The alignment editing method acts as a filter, removing potentially erroneous self-labeled alignments to maintain data quality.
        \item \textbf{Conditions for Success}: This method succeeds when the initial seed alignments are of high quality and the KGE model can effectively generalize from them. The alignment editing mechanism is crucial for its robustness, preventing the propagation of errors.
        \item \textbf{Theoretical Limitations}: The primary theoretical limitation is the potential for error accumulation. If the initial KGE model or the confidence threshold for new alignments is not robust, incorrect alignments can be introduced and reinforced, leading to a drift in the embedding space.
        \item \textbf{Practical Limitations}: The performance is highly sensitive to the quality of the initial seed alignments and the effectiveness of the error reduction strategy. It can also be computationally intensive due to multiple retraining iterations of the KGE model.
    \end{enumerate}
    \item \textbf{Semi-Supervised Entity Alignment}: Building on the need to leverage unlabeled data, \textsf{Semi-Supervised Entity Alignment via Knowledge Graph Embedding with Awareness of Degree Difference} (\textsf{SEA}) \cite{pei2019} further refined the approach.
    \begin{enumerate}
        \item \textbf{Problem Solved}: Addresses two key issues: the difficulty of acquiring labeled data (similar to bootstrapping) and the challenge posed by entity degree differences (high-frequency vs. low-frequency entities) which can affect KGE accuracy.
        \item \textbf{Core Innovation}: It employs a semi-supervised learning framework to leverage both labeled and abundant unlabeled entity information. Crucially, it improves the KGE with awareness of degree difference by performing adversarial training, aiming to make embeddings more robust to varying entity frequencies.
        \item \textbf{Mechanism}: Labeled entity pairs are used to initialize KGEs. For unlabeled entities, a self-training mechanism generates pseudo-labels based on embedding similarity. Adversarial training is introduced to encourage the KGE model to learn representations that are less sensitive to an entity's degree, thereby preventing high-degree entities from dominating the embedding space and improving alignment for low-degree entities.
        \item \textbf{Conditions for Success}: Requires a robust semi-supervised learning framework and effective adversarial training to balance the influence of entity degrees. It performs well when there's a significant amount of unlabeled data that can be reliably pseudo-labeled.
        \item \textbf{Theoretical Limitations}: Adversarial training can be notoriously difficult to stabilize and tune, potentially leading to training instabilities or suboptimal embeddings if not carefully managed. The effectiveness of mitigating degree bias might also vary across different KG structures.
        \item \textbf{Practical Limitations}: Increased complexity due to the adversarial component. The quality of pseudo-labels, especially in early iterations, can still impact overall performance.
    \end{enumerate}
\end{itemize}

\subsubsection*{Multi-view and Ontology-guided Alignment}
Beyond structural information, entities in KGs possess rich auxiliary features that, if leveraged effectively, can significantly enhance alignment accuracy.
\begin{itemize}
    \item \textbf{Multi-view Knowledge Graph Embedding}: While early KGEs for EA primarily focused on relational structures, \textsf{Multi-view Knowledge Graph Embedding for Entity Alignment} \cite{zhang2019} recognized the untapped potential of diverse entity features.
    \begin{enumerate}
        \item \textbf{Problem Solved}: Addresses the limitation of previous methods that primarily focused on relational structure, sometimes incorporating attributes, but often overlooking or not equally treating a vast array of other entity features (e.g., entity names, textual descriptions). This oversight impairs accuracy and robustness.
        \item \textbf{Core Innovation}: Proposes a novel framework that unifies multiple views of entities (entity names, relations, and attributes) to learn more comprehensive embeddings for entity alignment.
        \item \textbf{Mechanism}: The framework learns separate embeddings for each view (e.g., using character embeddings for names, structural KGE for relations, and attribute value embeddings). These view-specific embeddings are then combined using various strategies (e.g., concatenation, weighted sum, or attention mechanisms). Cross-KG inference methods are designed to further enhance alignment by propagating alignment signals between the KGs.
        \item \textbf{Conditions for Success}: This approach thrives when multiple views provide complementary and non-redundant information. The effectiveness hinges on robust methods for learning embeddings from each view and intelligent strategies for combining them.
        \item \textbf{Theoretical Limitations}: The optimal strategy for combining heterogeneous view embeddings is often heuristic and dataset-dependent, lacking a universal theoretical guarantee. The interplay between different views can be complex, and simple concatenation might not fully capture nuanced interactions.
        \item \textbf{Practical Limitations}: Increased computational cost due to learning and combining multiple embedding spaces. Requires careful feature engineering and selection for each view. The quality of each view's data (e.g., completeness of attributes, clarity of names) directly impacts performance.
    \end{enumerate}
    \item \textbf{Ontology-guided Entity Alignment}: Recognizing that KGs often come with explicit schema or ontological information, \textsf{OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding} \cite{xiang2021} proposed to integrate this meta-knowledge.
    \begin{enumerate}
        \item \textbf{Problem Solved}: Addresses the oversight of existing methods that ignore ontological schemas, which contain critical meta-information such as classes, their hierarchies, and disjointness relationships. This meta-information can significantly constrain and guide the alignment process.
        \item \textbf{Core Innovation}: Jointly embeds both KGs and their associated ontologies, explicitly utilizing class hierarchy (e.g., `subClassOf` relations) and class disjointness (e.g., `disjointWith` axioms) to avoid false mappings and guide the embedding learning.
        \item \textbf{Mechanism}: \textsf{OntoEA} extends a base KGE model by incorporating additional loss terms that enforce ontological constraints. For instance, if entity $e_1$ is an instance of class $C_1$ and entity $e_2$ is an instance of class $C_2$, and $C_1$ is a subclass of $C_2$, then their embeddings should reflect this hierarchical relationship. Similarly, if $C_1$ and $C_2$ are disjoint, then instances of these classes should not be aligned. This joint embedding ensures that entity representations are consistent with both the factual triples and the underlying schema.
        \item \textbf{Conditions for Success}: Requires the availability of explicit and reasonably complete ontologies for the KGs being aligned. It performs best when these ontologies are consistent and well-defined, providing strong semantic guidance.
        \item \textbf{Theoretical Limitations}: The effectiveness is directly tied to the quality and completeness of the ontologies. Sparse or inconsistent ontologies can introduce noise or misleading constraints, potentially hindering alignment. The integration of hard logical constraints into continuous embedding spaces can also be challenging.
        \item \textbf{Practical Limitations}: Many real-world KGs, especially those automatically extracted, may lack rich or explicit ontological schemas. The process of extracting or harmonizing ontologies can be complex and time-consuming.
    \end{enumerate}
\end{itemize}

\subsubsection*{Comparative Framework: KGE-based Entity Alignment Methods}
\begin{tabular}{|p{0.15\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.15\textwidth}|}
\hline
\textbf{Method Family} & \textbf{Core Innovation} & \textbf{Problem Solved} & \textbf{Conditions for Success} & \textbf{Limitations} \\
\hline
\textsf{Bootstrapping} \cite{sun2018} & Iterative self-labeling with error reduction. & Scarcity of labeled training data. & High-quality initial seeds; effective error editing. & Risk of error accumulation; sensitive to initial seed quality. \\
\hline
\textsf{Semi-Supervised} \cite{pei2019} & Semi-supervised learning with adversarial degree awareness. & Labeled data scarcity; entity degree bias. & Robust semi-supervised framework; stable adversarial training. & Adversarial training complexity; pseudo-label quality. \\
\hline
\textsf{Multi-view} \cite{zhang2019} & Unifies diverse entity features (names, relations, attributes). & Limited feature utilization; incomplete semantic capture. & Complementary views; effective fusion strategy. & Increased complexity; feature engineering overhead; fusion heuristics. \\
\hline
\textsf{Ontology-guided} \cite{xiang2021} & Joint embedding of KGs and their ontologies. & Neglect of meta-information (schema, hierarchy). & Available, high-quality, consistent ontologies. & Dependence on ontology quality; ontology acquisition overhead. \\
\hline
\end{tabular}

\subsubsection*{Synthesis and Critical Analysis}
The evolution of KGE for entity alignment reflects a continuous effort to move beyond purely structural matching towards more semantically rich and robust approaches. The initial challenge of limited labeled data, which implicitly affects any supervised KGE model, was directly tackled by \textsf{Bootstrapping Entity Alignment} \cite{sun2018} and \textsf{SEA} \cite{pei2019}. While \textsf{Bootstrapping} iteratively expands the training set, \textsf{SEA} further refines this by explicitly addressing the bias introduced by varying entity degrees through adversarial training. This highlights a crucial pattern in the field: the progression from basic self-training to more sophisticated mechanisms that account for inherent data characteristics. However, a common tension across these iterative methods is the trade-off between leveraging more unlabeled data and the risk of error propagation, which can lead to suboptimal or even incorrect alignments if not carefully managed by mechanisms like alignment editing \cite{sun2018}.

Furthermore, the field has recognized that structural information alone is often insufficient for robust EA, especially when KGs are sparse or structurally dissimilar. This led to the development of methods that integrate auxiliary information. \textsf{Multi-view Knowledge Graph Embedding} \cite{zhang2019} exemplifies this by unifying entity names, relations, and attributes. This approach directly addresses the limitation of earlier KGEs that might only consider relational triples (as discussed in Section 7.1), by showing that a richer feature set leads to more discriminative embeddings. Similarly, \textsf{OntoEA} \cite{xiang2021} extends this by incorporating ontological information, which provides a powerful, explicit form of meta-knowledge. This demonstrates a broader trend towards leveraging all available information, moving from implicit structural patterns to explicit semantic constraints. While \textsf{Multi-view} focuses on diverse *data types*, \textsf{OntoEA} focuses on *schema-level knowledge*, offering complementary improvements. A critical assumption often made by these methods is the existence of high-quality auxiliary data or ontologies, which may not always hold true for all KGs, especially those automatically constructed or from highly specialized domains.

Methodologically, the quality of evaluation in EA is paramount. As highlighted by experimental reviews like \cite{fanourakis2022}, a systematic quantitative assessment of strengths and weaknesses across different KG characteristics is crucial. Many EA papers evaluate performance using metrics like Hits@k and MRR on benchmark datasets such as DBP15K, which are valuable but may not fully capture real-world complexities like different entity granularities, varying levels of noise, or the presence of non-isomorphic structures. The generalizability of methods like \textsf{SEA} \cite{pei2019} in mitigating degree bias, for instance, needs to be thoroughly tested across KGs with diverse degree distributions. Similarly, the reliance of \textsf{OntoEA} \cite{xiang2021} on well-defined ontologies means its performance might degrade significantly for KGs lacking such rich schema, an unstated assumption that limits its universal applicability.

In conclusion, KGEs provide a powerful, data-driven approach to integrate heterogeneous knowledge sources, which is crucial for building comprehensive knowledge bases and enabling cross-KG reasoning. The field has progressed from basic adaptation of KGE models to specialized frameworks that address data scarcity through iterative learning \cite{sun2018, pei2019}, integrate diverse entity features \cite{zhang2019}, and leverage ontological information \cite{xiang2021}. These advancements collectively enhance the ability to find semantic correspondences between disparate knowledge structures. However, challenges persist in ensuring robustness to noise, handling highly sparse KGs, and achieving truly unsupervised alignment. Future research, as indicated by recent surveys \cite{zhu2024, yan2022}, will likely focus on integrating even more diverse information (e.g., temporal, multimodal), developing more robust self-training mechanisms, and improving the generalizability of these methods across a wider spectrum of real-world KGs.