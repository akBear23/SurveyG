\subsection{Transformer-based KGE Models}
Following the advancements in Graph Neural Networks (GNNs) and attention mechanisms, which excel at aggregating local neighborhood information \cite{wu2021, wang2020}, a new paradigm has emerged in Knowledge Graph Embedding (KGE) research: the adaptation of Transformer architectures. Originally designed for sequence modeling in natural language processing \cite{vaswani2017attention}, Transformers, with their powerful self-attention mechanisms, offer a distinct advantage by inherently capturing long-range dependencies and global contextual information, which can be challenging for traditional GNNs that rely on iterative local message passing \cite{li2018}. This subsection explores how these architectures are innovatively applied to knowledge graphs, treating them either as sequences or by integrating graph structures directly into the Transformer framework, thereby enhancing expressiveness for complex contextual and multi-structural features.

The initial adaptation of Transformers to KGE, exemplified by \textbf{CoKE: Contextualized Knowledge Graph Embedding} \cite{wang2019}, marked a significant shift from static entity and relation embeddings. CoKE's core innovation lies in recognizing the intrinsic contextual nature of entities and relations within a knowledge graph. It claims to solve the problem of traditional KGE methods that assign a single, static embedding to each entity or relation, thereby ignoring their dynamic properties across different graph contexts. CoKE formulates both edges and paths within the KG as sequences of entities and relations. A Transformer encoder then processes these sequences to obtain dynamic, flexible, and fully contextualized representations. This mechanism allows CoKE to capture contextual meanings that are adaptive to the input sequence, offering an absolute improvement of 19.7\% in Hits@10 on path query answering compared to previous state-of-the-art models \cite{wang2019}.

CoKE succeeds under conditions where contextual information, particularly along paths, is crucial for disambiguation and accurate prediction. Its theoretical limitation, however, stems from the inherent challenge of linearizing a graph structure into sequences. While effective for paths, this approach might implicitly lose some explicit graph-level structural information, such as cycles or broader neighborhood connectivity, that GNNs are specifically designed to capture. Practically, processing very long paths can lead to increased computational cost and memory usage, a common challenge for Transformer models. Compared to GNNs like DisenKGAT \cite{wu2021} which focus on disentangling local graph features, CoKE aims for broader contextualization by viewing KGs as sequences, offering a different perspective on capturing rich semantics.

A critical challenge in applying vanilla Transformers to KGs, particularly to triplets, is their inherent \textit{order invariance}. The self-attention mechanism treats input tokens as an unordered set, making it unable to distinguish between a valid triplet (head, relation, tail) and its shuffled, semantically incorrect variants (e.g., tail, relation, head). \textbf{Knowformer: Position-Aware Relational Transformer for Knowledge Graph Embedding} \cite{li2023} directly addresses this fundamental limitation. Its core innovation is a novel Transformer architecture that incorporates \textit{relational compositions} into entity representations, explicitly injecting semantics and capturing the role of an entity based on its position (subject or object) within a relation triple. This is achieved by defining relational compositions as operators on the relation and the object (or subject) and integrating them into the self-attention mechanism via a carefully designed residual block. Knowformer claims to solve the training inconsistency problem of order-invariant Transformers and formally proves its ability to distinguish entity roles and capture correct relational semantics. It achieves state-of-the-art performance on both link prediction and entity alignment across six benchmark datasets \cite{li2023}.

Knowformer succeeds under conditions where precise understanding of entity roles within a triplet is paramount. Its theoretical limitation is that while it effectively addresses the order invariance for triplets, it primarily operates at the triplet level, potentially not fully capturing the global graph structure or multi-hop dependencies as comprehensively as a dedicated graph-aware Transformer might. Practically, the design of relational compositions and their integration into self-attention adds complexity to the model architecture and training. Compared to CoKE \cite{wang2019}, which implicitly handles order by framing paths as sequences, Knowformer directly confronts and resolves the order-invariance issue for individual triplets, making it more robust for fundamental KGE tasks.

Pushing the boundaries further, \textbf{TGformer: A Graph Transformer Framework for Knowledge Graph Embedding} \cite{shi2025} represents a significant step towards a more comprehensive integration of Transformer capabilities with graph structures. TGformer's core innovation is being the \textit{first} framework to use a graph transformer to build knowledge embeddings that incorporate both \textit{triplet-level} and \textit{graph-level structural features} in both static and temporal knowledge graphs. It claims to solve the limitations of existing methods: triplet-based approaches that ignore the essential graph structure, and graph-based methods (like many GNNs discussed in the previous subsection) that overlook the contextual information of nodes. TGformer constructs a context-level subgraph for each predicted triplet and then employs a Knowledge Graph Transformer Network (KGTN) to fully explore multi-structural features. This allows the model to understand entities and relations in different contexts, boosting performance in link prediction. Experimental results on several public datasets show that TGformer achieves state-of-the-art performance \cite{shi2025}.

TGformer succeeds by leveraging the global receptive field of Transformers to integrate diverse structural information, making it particularly effective for KGs where both local triplet patterns and broader graph context are important. Its theoretical limitation lies in the increased complexity of integrating multiple levels of structural information and handling temporal dynamics, which can lead to higher computational demands and parameter counts. Practically, the construction of context-level subgraphs and the design of the KGTN require careful engineering and can be resource-intensive, especially for very large and dense KGs. TGformer represents a convergence, building on the contextualization ideas of CoKE \cite{wang2019} and the structural awareness of GNNs, while extending to temporal KGs, a domain often addressed by specialized models (see Section 5.1).

\begin{table}[htbp]
    \centering
    \caption{Comparative Framework of Transformer-based KGE Models}
    \label{tab:transformer_kge_comparison}
    \begin{tabularx}{\textwidth}{|l|X|X|X|X|X|}
        \hline
        \textbf{Model} & \textbf{KG Representation} & \textbf{Core Innovation} & \textbf{Problem Addressed} & \textbf{Key Advantages} & \textbf{Limitations} \\
        \hline
        \textbf{CoKE \cite{wang2019}} & Edges/Paths as sequences & Transformer encoder on linearized sequences & Static embeddings, ignoring context & Captures dynamic, contextualized meanings along paths & Potential loss of explicit graph structure, computational for long paths \\
        \hline
        \textbf{Knowformer \cite{li2023}} & Triplet-level (h,r,t) & Position-Aware Relational Transformer with relational compositions & Transformer's order invariance for triplets & Distinguishes entity roles, robust for triplet semantics & Primarily triplet-focused, complex relational composition design \\
        \hline
        \textbf{TGformer \cite{shi2025}} & Triplet-level + Graph-level (static & Graph Transformer framework for multi-structural features & Triplet-based ignore graph, graph-based ignore node context & Integrates global/local, static/temporal features, state-of-the-art & High complexity, computational cost, resource-intensive \\
        \hline
    \end{tabularx}
\end{table}

The intellectual trajectory of Transformer-based KGE models demonstrates a clear evolution from adapting sequence models to explicitly integrating graph structures. Early work like CoKE \cite{wang2019} assumed that KGs could be effectively represented as sequences (paths or edges), leveraging the Transformer's strength in contextualization. This approach, while innovative, implicitly makes the assumption that the linear ordering of elements in a path is sufficient to capture all relevant graph semantics, which may not hold for complex, non-linear graph structures. This tension between sequence-centric and graph-centric views is a recurring theme. Knowformer \cite{li2023} then addressed a fundamental limitation of applying vanilla Transformers directly to triplets, the order invariance, by introducing position-aware mechanisms. This marked a shift towards explicitly making Transformers "graph-aware" at a granular level. Finally, TGformer \cite{shi2025} represents a more comprehensive synthesis, moving beyond mere sequence adaptation or triplet-level fixes to a full graph transformer framework that explicitly models both local (triplet-level) and global (graph-level) structural features, and even extends to temporal dynamics. This progression exemplifies the field's shift from simply applying powerful neural architectures to carefully adapting and innovating them to respect the unique structural properties of knowledge graphs.

A critical pattern across these models is the trade-off between the simplicity of adaptation and the richness of graph structure capture. CoKE's sequence-based approach is relatively straightforward but might sacrifice explicit graph topology. Knowformer's focus on triplet-level order invariance is crucial but remains largely localized. TGformer, while offering the most comprehensive integration, introduces significant architectural and computational complexity. This reflects a broader tension in KGE research: how to leverage the immense power of general-purpose deep learning models while respecting the specific, often irregular, structure of knowledge graphs. Furthermore, while these Transformer-based models achieve state-of-the-art performance, their increased complexity often comes at the cost of interpretability, making it harder to understand *why* a particular prediction is made, a limitation shared with many deep learning approaches. This issue is particularly relevant for high-stakes applications discussed in Section 7.4. The reliance on large datasets for pre-training or fine-tuning is another practical limitation, as Transformers are notoriously data-hungry, which can be a challenge for smaller or domain-specific KGs.

In conclusion, Transformer-based KGE models have revolutionized the field by introducing powerful self-attention mechanisms to capture long-range dependencies and highly contextualized representations. From CoKE's sequence-based contextualization \cite{wang2019} to Knowformer's position-aware relational modeling \cite{li2023} and TGformer's comprehensive graph transformer framework \cite{shi2025}, these models have significantly pushed the state-of-the-art in KGE performance. They demonstrate an innovative application of architectures originally designed for sequence modeling to graph structures, effectively capturing global and local semantic relationships. The ongoing challenge lies in balancing their immense expressive power with computational efficiency, scalability, and interpretability, particularly as KGs continue to grow in size and complexity.