\section{Foundational KGE Models and Geometric Paradigms}
\label{sec:2_foundational_kge_models_and_geometric_paradigms}

This section marks a pivotal point in our review, transitioning from the foundational motivations for Knowledge Graph Embedding (KGE) to the initial, groundbreaking methodologies that transformed symbolic knowledge into continuous vector representations. Building upon the understanding that traditional Knowledge Graphs (KGs) struggle with sparsity, computational inefficiency, and capturing nuanced semantics, this section delves into the bedrock of KGE research: the foundational geometric and algebraic paradigms. These early and influential models laid the groundwork for representing entities and relations in continuous vector spaces, establishing the core principles for how relational patterns could be encoded and inferred.

The evolution began with simple yet powerful translational models, such as TransE and its crucial extensions like TransH \cite{wang2014} and TransD \cite{ji2015}. These models conceptualized relations as direct translations between head and tail entities in an embedding space, offering significant advancements in efficiency and the ability to infer missing links. However, their limitations in capturing complex relational patterns, such as symmetry, antisymmetry, and composition, quickly became apparent. This challenge spurred the development of more sophisticated geometric approaches.

Consequently, the field progressed to models that leveraged rotations and complex-valued spaces, exemplified by RotatE \cite{sun2018}. These innovations allowed for the elegant modeling of intricate logical patterns through algebraic transformations, significantly enhancing expressiveness beyond simple translations. Further explorations extended to multi-dimensional geometric approaches, including embeddings on Lie groups, different metric choices, and advanced transformations like Householder parameterization. This continuous refinement of mathematical foundations, exploring diverse embedding spaces and transformation mechanisms, was driven by the imperative to capture richer semantic interactions and improve the theoretical soundness and robustness of KGE models. Ultimately, this section traces the critical journey from basic vector operations to complex geometric transformations, forming the theoretical and practical basis for all subsequent advancements in KGE.