\subsection{Link Prediction and Knowledge Graph Completion}
The fundamental applications of Knowledge Graph Embedding (KGE) are link prediction (LP) and knowledge graph completion (KGC), which involve inferring missing facts within the graph structure. These tasks are paramount for enhancing the completeness and utility of knowledge graphs, making them more robust and informative for downstream AI systems \cite{dai2020, ge2023}. LP typically focuses on predicting a missing head entity ($h$), relation ($r$), or tail entity ($t$) in an incomplete triple $(h, r, t)$, while KGC broadly encompasses filling in these missing links to expand the knowledge base. The continuous effort in this domain aims to improve accuracy and handle complex relational patterns, forming the bedrock for many other applications.

\subsubsection*{Foundational Geometric Approaches}
Early KGE models primarily leveraged geometric transformations to represent entities and relations, offering a computationally efficient way to model relational patterns for LP/KGC.
\begin{itemize}
    \item \textbf{Translational Models}: Models like \textsf{TransE} (as a baseline for \cite{wang2014, ji2015}) represent relations as translations in the embedding space, where $h+r \approx t$. While simple and efficient, \textsf{TransE} struggles with complex relation patterns such as one-to-many, many-to-one, and many-to-many. \textsf{TransH} \cite{wang2014} addressed this by projecting entities onto a relation-specific hyperplane before translation, allowing a single entity to have different representations for different relations. This significantly improved handling of complex mappings with comparable efficiency to \textsf{TransE}. Building on this, \textsf{TransD} \cite{ji2015} further refined the approach by using dynamic mapping matrices constructed from entity and relation vectors, enabling more fine-grained, entity-specific projections and reducing parameters compared to prior projection-based methods. \textsf{TransA} \cite{jia2015, jia2017} introduced an adaptive margin for the loss function, allowing for more flexible and context-aware embeddings, which was a crucial step towards locally adaptive learning.
    \item \textbf{Rotational and Compound Models}: A significant advancement came with models like \textsf{RotatE} \cite{sun2018}, which defined relations as rotations in complex vector spaces. This innovation inherently allowed it to model and infer complex relation patterns like symmetry, antisymmetry, inversion, and composition more effectively than translational models. \textsf{Rotate3D} \cite{gao2020} extended this to three-dimensional space, leveraging the non-commutative property of 3D rotations for multi-hop reasoning. More broadly, \textsf{CompoundE} \cite{ge2022} and \textsf{CompoundE3D} \cite{ge2023} generalized these concepts by combining translation, rotation, and scaling operations, demonstrating that a richer set of cascaded geometric manipulations can lead to superior performance by capturing diverse underlying characteristics of a KG. \textsf{HousE} \cite{li2022} introduced Householder transformations (rotations and projections) to achieve superior capacity for modeling relation patterns and mapping properties simultaneously, generalizing existing rotation-based models to high-dimensional spaces. \textsf{MQuinE} \cite{liu2024} identified and resolved a "Z-paradox" deficiency in some popular KGE models, ensuring stronger expressiveness for various relation patterns including symmetric/asymmetric, inverse, and compositional relations.
\end{itemize}

\textbf{Comparative Framework: Geometric KGE Models for LP/KGC}
\begin{tabular}{|p{0.15\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.15\textwidth}|}
\hline
\textbf{Method Family} & \textbf{Core Innovation} & \textbf{Problem Solved (LP/KGC)} & \textbf{Conditions for Success} & \textbf{Limitations} \\
\hline
\textsf{Translational} (\textsf{TransH} \cite{wang2014}, \textsf{TransD} \cite{ji2015}) & Project entities onto relation-specific hyperplanes/dynamic matrices. & Better handling of 1-to-N, N-to-1 relations. & Sufficient training data for relation-specific projections. & Still struggles with complex logical patterns (composition, inversion) compared to rotational models. \\
\hline
\textsf{Rotational} (\textsf{RotatE} \cite{sun2018}, \textsf{Rotate3D} \cite{gao2020}) & Relations as rotations in complex/3D space. & Captures symmetry, antisymmetry, inversion, composition. & Effective for KGs with rich logical patterns. & May not fully exploit theoretical underpinnings of composition (addressed by \textsf{HolmE} \cite{zheng2024}). \\
\hline
\textsf{Compound} (\textsf{CompoundE} \cite{ge2022}, \textsf{HousE} \cite{li2022}) & Combination of translation, rotation, scaling; Householder transforms. & Generalized modeling of diverse relation patterns and mapping properties. & KGs with heterogeneous relation types and complex structures. & Increased computational complexity and parameter count. \\
\hline
\end{tabular}

The evolution from simple translational models to complex rotational and compound operations highlights a continuous effort to enhance the expressiveness of KGEs for LP/KGC. While \textsf{TransH} \cite{wang2014} and \textsf{TransD} \cite{ji2015} significantly improved upon basic translation by addressing mapping properties, they often fell short in capturing complex logical patterns like composition. This gap was notably addressed by \textsf{RotatE} \cite{sun2018}, which demonstrated superior performance in modeling such patterns through complex space rotations. However, even \textsf{RotatE} might not fully exploit the theoretical underpinnings of composition, a limitation that \textsf{HolmE} \cite{zheng2024} explicitly aims to overcome by ensuring its relation embedding space is closed under composition, offering stronger theoretical guarantees for handling under-represented patterns. This progression exemplifies the field's shift towards more mathematically robust and expressive models. A common practical limitation across these geometric models is the trade-off between increased expressiveness and computational cost, though many strive for competitive efficiency.

\subsubsection*{Deep Learning Architectures for LP/KGC}
The advent of deep learning brought a paradigm shift, enabling KGE models to learn more intricate and context-aware representations for LP/KGC.
\begin{itemize}
    \item \textbf{Convolutional Neural Networks (CNNs)}: CNNs have been adapted to extract local features and model interactions between entity and relation embeddings. \textsf{AcrE} \cite{ren2020} used atrous convolutions and residual learning to increase feature interactions and address gradient issues, achieving high parameter efficiency. \textsf{ReInceptionE} \cite{xie2020} further explored Inception networks and relation-aware attention to capture joint local-global structural information, enhancing interactions between head and relation embeddings. More recently, \textsf{CNN-ECFA} \cite{hu2024} proposed an entity-specific common feature aggregation strategy, demonstrating superior performance by leveraging CNNs for richer feature extraction. \textsf{SEConv} \cite{yang2025} (designed for healthcare prediction) also employs multi-layer CNNs for deeper structural features, highlighting their utility in specialized domains. These models excel at learning complex, non-linear feature interactions, often outperforming simpler geometric models in accuracy.
    \item \textbf{Graph Neural Networks (GNNs) and Attention Mechanisms}: GNNs, through message passing and aggregation, are naturally suited to capture structural information and neighborhood context. \textsf{DisenKGAT} \cite{wu2021} introduced a disentangled graph attention network to learn diverse component representations, enhancing independence and adaptability to various score functions. \textsf{GAATs} \cite{wang2020} incorporated an attenuated attention mechanism to assign different weights to relation paths and acquire information from neighborhoods, leading to more complete mining of triple features. For inductive KGE, \textsf{Logic Attention Based Neighborhood Aggregation} \cite{wang2018} used a Logic Attention Network (LAN) to aggregate information from an entity's neighbors, allowing for embeddings of unseen entities.
    \item \textbf{Transformer-based KGE Models}: Transformers leverage self-attention mechanisms to capture long-range dependencies and contextualized representations. \textsf{CoKE} \cite{wang2019} treated KGs as sequences of entities and relations, using a Transformer encoder to obtain dynamic, contextualized representations. \textsf{Knowformer} \cite{li2023} addressed the order-invariance issue of standard Transformers by incorporating relational compositions and position-aware mechanisms, proving effective for both LP and entity alignment. \textsf{TGformer} \cite{shi2025} proposed a general graph Transformer framework that integrates triplet-level and graph-level structural features, boosting the model's ability to understand entities and relations in different contexts.
\end{itemize}

\textbf{Comparative Framework: Deep Learning KGE Models for LP/KGC}
\begin{tabular}{|p{0.15\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.15\textwidth}|}
\hline
\textbf{Method Family} & \textbf{Core Innovation} & \textbf{Problem Solved (LP/KGC)} & \textbf{Conditions for Success} & \textbf{Limitations} \\
\hline
\textsf{CNN-based} (\textsf{AcrE} \cite{ren2020}, \textsf{ReInceptionE} \cite{xie2020}) & Multi-scale filters, residual learning, inception networks for feature extraction. & Captures intricate non-linear feature interactions for complex relation types (1-to-N, N-to-1). & Requires sufficient data to learn complex features; benefits from diverse relation types. & Increased computational complexity and parameter count compared to simpler geometric models. \\
\hline
\textsf{GNN-based} (\textsf{DisenKGAT} \cite{wu2021}, \textsf{GAATs} \cite{wang2020}) & Message passing, attention mechanisms, disentangled representations. & Leverages graph topology and neighborhood context for richer, context-dependent embeddings. & KGs with rich local structural information and diverse relational paths. & Can be computationally intensive for very large graphs; disentanglement adds complexity. \\
\hline
\textsf{Transformer-based} (\textsf{CoKE} \cite{wang2019}, \textsf{Knowformer} \cite{li2023}) & Self-attention, contextualized representations, position-aware mechanisms. & Captures long-range dependencies and contextual meanings of entities/relations. & Benefits from KGs that can be linearized or where global context is crucial. & High computational cost for self-attention; order-invariance issue needs explicit handling (\textsf{Knowformer}). \\
\hline
\end{tabular}

Deep learning architectures address the limitations of foundational geometric models by automatically extracting complex features and capturing intricate structural and contextual information. For instance, while \textsf{RotatE} \cite{sun2018} effectively models compositional relations geometrically, \textsf{CoKE} \cite{wang2019} offers a more flexible, data-driven approach by using Transformers to learn contextualized representations, which can capture more nuanced semantic dependencies beyond simple geometric transformations. This exemplifies the field's shift towards leveraging powerful neural networks for enhanced expressiveness, albeit often at the cost of increased computational resources and reduced interpretability. The tension between model expressiveness and computational efficiency is particularly pronounced here, with models like \textsf{AcrE} \cite{ren2020} attempting to balance this through parameter-efficient designs.

\subsubsection*{Leveraging Auxiliary Information and Rules for LP/KGC}
Beyond structural patterns, integrating auxiliary information and logical rules significantly enhances KGE for LP/KGC, especially in incomplete or noisy KGs.
\begin{itemize}
    \item \textbf{Auxiliary Information}: Models like \textsf{TransET} \cite{wang2021} and \textsf{TaKE} \cite{he2023} incorporate entity type features to learn more semantic representations, guiding the embedding process and improving KG completion. \textsf{TaKE} specifically provides a universal framework that can augment any traditional KGE model. \textsf{HINGE} \cite{rosso2020} moved "beyond triplets" by directly learning from hyper-relational facts (triplets with associated key-value pairs), capturing richer data semantics that traditional triplet-based models overlook. \textsf{GeoEntity-type constrained KGE} \cite{hu2024} applied this principle to a specific domain, using geoentity types as constraints to predict natural-language spatial relations more accurately. These methods leverage well-structured prior knowledge to produce more semantic and robust embeddings.
    \item \textbf{Rule-based and Constraint-driven KGE}: This family injects explicit logical rules or constraints to ensure semantic consistency and improve reasoning. \textsf{Semantically Smooth KGE} \cite{guo2015} enforced a "semantically smooth" embedding space where entities of the same category lie close, using manifold learning as regularization. \textsf{RUGE} \cite{guo2017} introduced an iterative guidance mechanism from soft rules, allowing KGE models to learn simultaneously from labeled triples, unlabeled triples, and automatically extracted soft rules. This was a crucial step beyond one-time rule injection. \textsf{Improving Knowledge Graph Embedding Using Simple Constraints} \cite{ding2018} showed that even basic constraints like non-negativity and approximate entailment can significantly improve interpretability and structure. More recently, \textsf{Knowledge Graph Embedding Preserving Soft Logical Regularity} \cite{guo2020} imposed soft rule constraints directly on relation representations, improving scalability. \textsf{RulE} \cite{tang2022} learned rule embeddings jointly with entity and relation embeddings, enabling soft logical inference and using rule embeddings to regularize and enrich the overall embedding space.
\end{itemize}
\textbf{Comparative Framework: Auxiliary Information & Rule-based KGE for LP/KGC}
\begin{tabular}{|p{0.15\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.15\textwidth}|}
\hline
\textbf{Method Family} & \textbf{Core Innovation} & \textbf{Problem Solved (LP/KGC)} & \textbf{Conditions for Success} & \textbf{Limitations} \\
\hline
\textsf{Type/Attribute Augmented} (\textsf{TransET} \cite{wang2021}, \textsf{TaKE} \cite{he2023}) & Integrates entity types/attributes into embedding process. & Enhances semantic understanding, especially for incomplete KGs; improves robustness to noise. & Requires availability and quality of auxiliary information. & Benefits diminish if auxiliary data is sparse or noisy. \\
\hline
\textsf{Rule-based/Constraint-driven} (\textsf{RUGE} \cite{guo2017}, \textsf{RulE} \cite{tang2022}) & Incorporates logical rules (soft/hard) as regularization or joint learning. & Enforces consistency, improves reasoning, enhances interpretability. & Availability of high-quality rules (manual or extracted). & Rule extraction can be complex; balancing rule adherence with flexibility for exceptions. \\
\hline
\end{tabular}

These approaches address a critical assumption in many purely structural KGE models: that all necessary information is contained within the triples. \textsf{TransET} \cite{wang2021} and \textsf{TaKE} \cite{he2023} demonstrate that leveraging entity types provides crucial semantic guidance, leading to more discriminative embeddings. This complements the structural learning of models like \textsf{TransD} \cite{ji2015} by adding an orthogonal dimension of knowledge. Similarly, rule-based methods like \textsf{RUGE} \cite{guo2017} and \textsf{RulE} \cite{tang2022} address the limitation of purely data-driven models by injecting logical consistency, which is vital for robust reasoning in LP/KGC. While \textsf{TransH} \cite{wang2014} improves mapping properties, it doesn't inherently enforce logical consistency; rule-based methods fill this gap. A practical limitation is the reliance on the availability and quality of auxiliary information or rules, which may not always be complete or accurate.

\subsubsection*{Addressing Dynamic and Complex Structures for LP/KGC}
Real-world KGs are dynamic and often exhibit complex, hierarchical structures that Euclidean spaces struggle to represent efficiently.
\begin{itemize}
    \item \textbf{Hyperbolic Embeddings}: Hyperbolic spaces, with their inherent negative curvature, are naturally suited for embedding hierarchical data with high fidelity and fewer dimensions. \textsf{Hyperbolic Hierarchy-Aware KGE} \cite{pan2021} explicitly targeted this by extending the Poincar√© Ball model to capture hierarchical structures more effectively. \textsf{Fully Hyperbolic Rotation} \cite{liang2024} pushed for a more native hyperbolic formulation, defining the entire model directly in hyperbolic space using the Lorentz model, emphasizing hyperbolic rotation for relations. This offers superior representation for hierarchical KGs compared to Euclidean counterparts. \textsf{Mixed Geometry Message and Trainable Convolutional Attention Network} \cite{shang2024} proposed a versatile approach by integrating messages and scoring functions from hyperbolic, hypersphere, and Euclidean spaces, allowing for adaptive modeling of diverse local structures, acknowledging that not all KG structures are purely hierarchical.
    \item \textbf{Temporal KGE (TKGE)}: While a dedicated section (5.1) covers TKGE, it's crucial to note its role in LP/KGC for dynamic KGs. \textsf{HyTE} \cite{dasgupta2018} explicitly incorporates time by associating each timestamp with a hyperplane, enabling temporally aware inference and prediction of temporal scopes for facts. This is essential for predicting links that are valid only for specific time periods.
\end{itemize}
The tension here lies between the theoretical advantages of non-Euclidean geometries for certain graph structures (e.g., hierarchies) and the increased computational complexity of operations within these spaces. While \textsf{Fully Hyperbolic Rotation} \cite{liang2024} claims competitive results with fewer parameters, the overall training and inference can still be more involved than in Euclidean space.

\subsubsection*{Efficiency and Robustness in LP/KGC}
Practical deployment of KGE for LP/KGC necessitates addressing efficiency and robustness concerns.
\begin{itemize}
    \item \textbf{Negative Sampling and Training Optimization}: Training KGE models relies heavily on negative sampling to discriminate positive from negative triples. \textsf{NSCaching} \cite{zhang2018} proposed an efficient method to track and sample "hard" negative triplets using a cache, distilling the benefits of complex GAN-based sampling into a simpler framework. \textsf{Efficient Non-Sampling KGE} \cite{li2021} took a radical approach by considering all negative instances and leveraging mathematical derivations to reduce computational complexity, aiming for more stable and accurate performance by removing sampling uncertainty. \textsf{Confidence-Aware Negative Sampling} \cite{shan2018} addressed noise in KGs by introducing negative triple confidence. \textsf{Modality-Aware Negative Sampling} \cite{zhang2023} extended this to multi-modal KGE, adapting sampling for diverse data modalities. These methods are crucial for optimizing the training process, which directly impacts the accuracy and efficiency of LP/KGC.
    \item \textbf{Compression and Parameter Efficiency}: For large-scale KGs, embedding size and parameter count are critical. \textsf{DualDE} \cite{zhu2020} used knowledge distillation to build low-dimensional student KGEs from high-dimensional teachers, reducing parameters by 7-15x and increasing inference speed by 2-6x with minor performance loss. \textsf{Knowledge Graph Embedding Compression} \cite{sachan2020} represented entities with discrete codes, achieving 50-1000x compression. \textsf{LightKG} \cite{wang2021} introduced a framework that stores only codebooks and indices, drastically reducing storage and boosting inference. \textsf{EARL} \cite{chen2023} achieved parameter efficiency by learning embeddings for only a small set of "reserved entities" and deriving others from context using entity-agnostic encoders.
\end{itemize}
The tension between achieving high accuracy in LP/KGC and maintaining computational efficiency is a recurring theme. While models like \textsf{RotatE} \cite{sun2018} achieve state-of-the-art accuracy, their parameter count can be substantial. Methods like \textsf{DualDE} \cite{zhu2020} and \textsf{EARL} \cite{chen2023} directly address this by reducing model size and computational cost, making KGE more deployable in resource-constrained environments. However, this often comes with a "minor loss in performance" \cite{sachan2020}, which might be unacceptable in high-stakes applications. The choice of negative sampling strategy, as reviewed by \cite{qian2021} and \cite{madushanka2024}, is also critical; an inefficient or poor sampling method can severely degrade LP/KGC performance, even for highly expressive models.

In conclusion, LP/KGC remains the central benchmark for KGE models, driving innovations from foundational geometric transformations to advanced deep learning architectures and sophisticated strategies for incorporating auxiliary information and managing dynamic data. The field continuously navigates trade-offs between model expressiveness, computational efficiency, and robustness, with recent efforts increasingly focusing on making these models more adaptable and reliable for real-world applications.