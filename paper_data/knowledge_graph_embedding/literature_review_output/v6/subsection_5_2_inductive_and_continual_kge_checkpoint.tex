\subsection{Inductive and Continual KGE}
The previous discussion on Temporal Knowledge Graph Embedding (TKGE) highlighted the necessity of adapting KGE models to evolving facts and time-dependent relationships. Building upon this, the challenge extends to handling the emergence of entirely new entities and efficiently updating models with novel information without full retraining. This subsection delves into \textit{Inductive} and \textit{Continual Knowledge Graph Embedding} (KGE) methods, which are crucial for adapting KGE models to the dynamic and ever-growing nature of real-world knowledge bases. Inductive KGE focuses on embedding unseen entities, while continual KGE aims to efficiently acquire new knowledge and retain previously learned information, mitigating catastrophic forgetting and ensuring scalability.

The research in this area can be broadly categorized into three evolving methodological families: (1) neighborhood aggregation-based approaches, which leverage local graph structures for inductive inference; (2) meta-learning strategies, designed to learn transferable knowledge for new entities or dynamic updates; and (3) parameter-efficient adaptation techniques, which enable incremental learning by selectively updating model parameters.

\begin{table}[htbp]
\centering
\caption{Comparative Framework for Inductive and Continual KGE Approaches}
\label{tab:inductive_continual_kge}
\begin{tabularx}{\textwidth}{|p{0.18\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.15\textwidth}|}
\hline
\textbf{Dimension} & \textbf{Neighborhood Aggregation} (\cite{wang2018}) & \textbf{Meta-Learning (Inductive)} (\cite{chen2021}) & \textbf{Meta-Learning (Dynamic/Continual)} (\cite{sun2024}) & \textbf{Parameter-Efficient Adaptation} (\cite{liu2024}) \\
\hline
\textbf{Primary Problem} & Inductive KGE (unseen entities) & Inductive KGE (general unseen entities, entity embeddings) & Dynamic KGE (emerging entities in evolving KGs) & Continual KGE (new knowledge, mitigate forgetting, efficiency) \\
\hline
\textbf{Core Innovation} & Logic Attention Network (LAN) for neighbor aggregation with rules- and network-based attention. & MorsE: Learns entity-independent meta-knowledge via meta-learning to *produce* entity embeddings. & MetaHG: Meta-learning with hybrid GNN (local + global) for emerging entities. & FastKGE: Incremental LoRA (IncLoRA) for efficient, isolated new knowledge acquisition and forgetting mitigation. \\
\hline
\textbf{Conditions for Success} & New entities have existing, informative neighbors. & Diverse source KGs for meta-training; generalizable meta-knowledge. & KGs with both local and global structural patterns; dynamic nature. & Large pre-trained KGE models; new knowledge can be localized. \\
\hline
\textbf{Theoretical Limitations} & Struggles with cold-start (isolated) entities or sparse neighborhoods. Limited by rule expressiveness. & Transferability depends on source-target KG similarity; struggles with highly novel entities. & Complexity of balancing local/global info; hybrid GNNs can be intricate. & Assumption that new knowledge can be "isolated" might not always hold; low-rank approximation limits expressiveness. \\
\hline
\textbf{Practical Limitations} & Performance sensitive to neighborhood quality/density; scalability for very large neighborhoods. & High computational cost of meta-learning; complex setup. & High computational cost of meta-learning + complex GNNs; tuning. & Requires careful design of layer isolation; small overhead from LoRA. \\
\hline
\textbf{Comparison to Alternatives} & Early inductive, simpler but less general than meta-learning. & More general inductive than aggregation, but computationally heavier. & Specialized meta-learning for dynamic KGs, integrates GNNs. & Addresses both efficiency and forgetting; adapts existing models rather than learning from scratch. \\
\hline
\end{tabularx}
\end{table}

\subsubsection*{Neighborhood Aggregation for Inductive KGE}
Early efforts to tackle the inductive KGE problem focused on leveraging the local neighborhood structure of new entities. Traditional KGE models, being transductive, require all entities to be present during training, making them impractical for real-world KGs where new entities constantly emerge. \cite{wang2018} addressed this by proposing \textbf{Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding}. The core problem it claims to solve is enabling KGE models to embed unseen entities by learning a generalized representation function. Its core innovation is the \textit{Logic Attention Network (LAN)}, which aggregates information from an entity's neighbors using both rules-based and network-based attention weights. This mechanism allows the model to dynamically compute an embedding for a new entity based on its existing connections, rather than requiring a pre-learned, static vector.

LAN succeeds under conditions where new entities have a sufficiently rich and informative neighborhood structure in the graph. The rules-based attention component can also leverage explicit logical patterns, enhancing interpretability. However, a significant theoretical limitation is its reliance on the presence of neighbors; truly cold-start entities (those with no or very few connections) would still pose a challenge, as there would be insufficient information to aggregate. Practically, the performance of LAN is highly sensitive to the quality and density of the neighborhood information. Furthermore, scaling neighborhood aggregation to extremely large graphs with vast neighborhoods can introduce computational overhead, as the aggregation process might become expensive. Compared to later meta-learning approaches, LAN offers a simpler, more direct inductive mechanism but is less generalizable to entities with entirely novel structural patterns.

\subsubsection*{Meta-Learning for Inductive and Dynamic KGE}
Building on the need for more generalized inductive capabilities, meta-learning emerged as a powerful paradigm. Instead of learning entity-specific embeddings, meta-learning aims to learn \textit{transferable meta-knowledge} that can be rapidly adapted to new tasks or entities.

\cite{chen2021} introduced \textbf{Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding} (MorsE), directly addressing the limitation of existing inductive KGE methods that could only solve inductive relation prediction, not produce embeddings for unseen entities. The core innovation of MorsE is to learn entity-independent modules that encapsulate meta-knowledge, which can then be used to generate embeddings for new entities. This is achieved through a meta-learning framework, where the model learns how to learn, rather than learning specific entity representations. MorsE claims to significantly outperform baselines for both in-KG and out-of-KG tasks in inductive settings. It succeeds when there is sufficient diversity in the source KGs used for meta-training, allowing the model to learn truly generalizable meta-knowledge. A theoretical limitation is that the quality of meta-knowledge transfer is inherently dependent on the similarity between the meta-training KGs and the target KGs; highly novel entities or graph structures might still be challenging. Practically, meta-learning frameworks are often computationally expensive and complex to set up and train, requiring careful design of meta-tasks and optimization strategies. MorsE represents a significant step beyond neighborhood aggregation by offering a more general mechanism for cold-start entity embedding, as it doesn't strictly rely on existing neighbors but on learned transferable knowledge.

Extending meta-learning to dynamic and evolving KGs, \cite{sun2024} proposed \textbf{MetaHG} for \textbf{Learning Dynamic Knowledge Graph Embedding in Evolving Service Ecosystems}. This work specifically tackles the challenge of efficiently updating incremental knowledge and representing emerging entities in dynamic service ecosystems, where conventional KGE methods often struggle with spatial deformation. MetaHG's core innovation is a meta-learning strategy that incorporates both local (via a GNN layer) and potential global (via a Hypergraph Neural Network, HGNN, layer) structural information from current snapshot KGs. It initializes entity embeddings using 'in' and 'out' relationship matrices and refines them through this hybrid GNN framework. This approach aims to enhance the representation quality of unseen entities by leveraging richer structural context. MetaHG succeeds particularly well in dynamic KGs that exhibit both localized and higher-order global structural patterns, such as those found in complex service ecosystems. A theoretical limitation is the increased complexity of balancing and optimizing the contributions from both local GNN and global HGNN layers, which might lead to intricate optimization landscapes. Practically, the combination of meta-learning with a hybrid GNN framework can result in high computational costs and requires extensive hyperparameter tuning. MetaHG specializes the meta-learning paradigm for dynamic updates and emerging entities, integrating advanced GNNs to capture richer structural context, a more nuanced problem than the general inductive entity embedding addressed by \cite{chen2021}.

\subsubsection*{Parameter-Efficient Adaptation for Continual KGE}
While meta-learning offers solutions for inductive capabilities and dynamic updates, the challenge of \textit{continual learning} in KGE involves not only acquiring new knowledge efficiently but also mitigating \textit{catastrophic forgetting} of previously learned information. This is a critical tension in dynamic KGE, as models must balance plasticity (learning new facts) with stability (retaining old facts).

\cite{liu2024} addressed this dual problem with \textbf{Fast and Continual Knowledge Graph Embedding via Incremental LoRA} (FastKGE). The problem it claims to solve is the inefficiency of fine-tuning KGE models for new knowledge acquisition and the persistent issue of catastrophic forgetting. Its core innovation is the \textit{Incremental LoRA (IncLoRA)} mechanism, inspired by parameter-efficient fine-tuning techniques from large language models. FastKGE isolates and allocates new knowledge to specific layers based on the fine-grained influence between old and new KGs. It then devises an efficient IncLoRA mechanism, which embeds these specific layers into low-rank adapters, significantly reducing the number of trainable parameters during fine-tuning. Furthermore, IncLoRA introduces adaptive rank allocation, making the LoRA aware of entity importance. FastKGE demonstrates significant reductions in training time (34-49\% on public datasets, 51-68\% on larger datasets) while maintaining competitive link prediction performance. It succeeds particularly well for large pre-trained KGE models where new knowledge can be effectively localized to specific parts of the network. A theoretical limitation is the underlying assumption that new knowledge can indeed be "isolated" to specific layers without significantly impacting the representations of old knowledge, which might not hold for highly intertwined or foundational new facts. The low-rank approximation, while efficient, could also theoretically limit the expressiveness for very complex new relational patterns. Practically, designing the influence mechanism for layer isolation requires careful consideration, and while LoRA is parameter-efficient, it still adds a small computational overhead. FastKGE directly addresses the efficiency concerns raised in Subsection 6.1, offering a parameter-efficient solution specifically for continual updates, a distinct approach from the more general meta-learning strategies.

\subsubsection*{Comparative Framework and Evolution}
The intellectual trajectory in Inductive and Continual KGE demonstrates a clear evolution from basic inductive capabilities to more sophisticated and efficient dynamic learning paradigms, as summarized in Table \ref{tab:inductive_continual_kge}. Early work, exemplified by \cite{wang2018}'s Logic Attention, established the feasibility of inductive learning through neighborhood aggregation. This approach, while effective for entities with existing connections, implicitly assumes the availability of such neighbors and struggles with truly cold-start scenarios.

The field then progressed to more generalized inductive capabilities through meta-learning. \cite{chen2021}'s MorsE represents a significant leap by learning transferable meta-knowledge to *produce* entity embeddings, thereby overcoming the reliance on direct neighborhood aggregation and enabling embedding for a broader range of unseen entities. This addresses a fundamental limitation of earlier methods by shifting from direct entity embedding to learning a meta-function. Building on this, \cite{sun2024}'s MetaHG further refines meta-learning for *dynamic* KGs, specializing it for emerging entities in evolving service ecosystems and integrating hybrid GNNs to capture richer structural context. This highlights a trend towards tailoring general learning paradigms to specific challenges in dynamic environments.

The most recent advancements, such as \cite{liu2024}'s FastKGE, tackle the critical practical challenge of \textit{efficiency} in continual learning, a concern that meta-learning approaches, despite their power, often face due to their computational complexity. FastKGE's IncLoRA mechanism directly addresses the dual problem of efficient new knowledge acquisition and catastrophic forgetting, a recurring tension in continual learning research. This approach leverages insights from parameter-efficient fine-tuning, demonstrating a convergence of techniques from different AI domains (e.g., large language models) to solve KGE challenges.

A critical tension across this subgroup is balancing the \textit{generality} of inductive capabilities (e.g., embedding any unseen entity) with \textit{computational efficiency} and the effective mitigation of \textit{catastrophic forgetting}. While meta-learning approaches like MorsE and MetaHG offer strong inductive power, they can be computationally intensive. FastKGE, on the other hand, prioritizes efficiency and forgetting mitigation for continual updates, potentially at the cost of the full inductive generality offered by meta-learning for entirely novel entities. This echoes the broader efficiency concerns discussed in Subsection 6.1, where models like \cite{zhu2020}'s DualDE and \cite{wang2021}'s LightKG aim to reduce parameter counts and inference times in static settings; FastKGE extends this focus to the dynamic update scenario.

Another unstated assumption in many inductive KGE models is the presence of sufficient training data for meta-learning or a rich enough neighborhood for aggregation. This assumption can break down in extremely sparse or rapidly changing real-world KGs. The evaluation of these models often requires specialized dynamic datasets, which can be less standardized than static link prediction benchmarks, making direct comparisons challenging and potentially limiting the generalizability of reported performance gains. The field is actively working towards more robust and universally applicable solutions for these complex, real-world scenarios.