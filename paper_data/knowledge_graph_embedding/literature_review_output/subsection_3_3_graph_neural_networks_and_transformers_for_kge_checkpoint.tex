\subsection{Graph Neural Networks and Transformers for KGE}

Traditional knowledge graph embedding (KGE) models, often relying on geometric transformations or bilinear scoring functions, have proven effective but are inherently limited in capturing the intricate multi-hop structural context and complex, non-linear feature interactions within knowledge graphs. These models often struggle with inductive learning for unseen entities and the diverse semantic patterns present in real-world KGs. To overcome these limitations, advanced neural architectures, particularly Graph Neural Networks (GNNs) and Transformers, have emerged as powerful tools for learning more expressive and context-rich KGEs.

Early advancements in leveraging neural architectures for KGE moved beyond simple scoring functions to learn complex features directly from the graph. For instance, \textcite{zhang2020} introduced the Multi-Scale Dynamic Convolutional Network (M-DCN), which employs multi-scale filters with relation-specific dynamic weights to capture diverse characteristics and effectively handle complex relation types (e.g., 1-to-N, N-to-1, N-to-N). Building on this, \textcite{xie2020} proposed ReInceptionE, a model that integrates an Inception network to enhance interactions between head and relation embeddings, alongside a relation-aware attention mechanism to incorporate both local neighborhood and global entity information. More recently, \textcite{yang2025} developed SEConv for healthcare prediction, which combines a resource-efficient self-attention mechanism with a multi-layer Convolutional Neural Network (CNN) to learn deeper and more informative structural features from triplets, demonstrating the continued relevance of CNNs for feature extraction in KGE.

Graph Neural Networks (GNNs) represent a significant paradigm shift, explicitly designed to process graph-structured data by iteratively aggregating information from an entity's neighborhood. This message-passing mechanism allows GNNs to capture multi-hop structural context and neighborhood information, enabling more expressive and often inductive embeddings. \textcite{wang2018} pioneered inductive KGE with the Logic Attention Network (LAN), which aggregates neighbor information using a double-view attention mechanism that is permutation invariant, redundancy-aware, and query-relation-aware, allowing effective embedding of emerging entities. Addressing the heterogeneity inherent in KGs, \textcite{li2021qr0} proposed a heterogeneous GNN framework that aggregates neighbor features under each relation-path and learns the importance of different paths through an attention mechanism, thereby capturing various semantic aspects.

Further deepening the understanding of GNNs for KGE, \textcite{li2021} investigated how KGE models extrapolate to unseen data from a "Semantic Evidence" perspective. They introduced SE-GNN, a multi-layer GNN that explicitly models relation-level, entity-level, and triple-level semantic evidence through distinct neighbor aggregation patterns and attention mechanisms, achieving state-of-the-art performance in knowledge graph completion. For inductive KGE, \textcite{chen2021} introduced MorsE, a meta-learning framework that learns "meta-knowledge" (transferable structural patterns) through an entity initializer and a GNN modulator, enabling the generation of embeddings for entirely new entities in unseen KGs. In dynamic environments, \textcite{sun2024} proposed MetaHG, a meta-learning strategy for KGE that uses a hybrid GNN (combining a standard GNN layer with a Hypergraph Neural Network layer) to incorporate both local and global structural information for continuously updating knowledge in evolving service ecosystems.

The expressiveness of GNNs has been further enhanced by integrating diverse geometric spaces and adaptive attention. \textcite{shang2024} introduced MGTCA, a model that generates rich neighbor messages by combining information from hyperbolic, hypersphere, and Euclidean spaces. MGTCA also features a trainable convolutional attention network that adaptively switches between different GNN types (GCN, GAT, and a novel KGCAT) to overcome data dependence and learn optimal attention for local structures. To address the scalability challenges of GNNs for large-scale KGs, \textcite{modak2024} presented CPa-WAC, a lightweight architecture that utilizes constellation partitioning to divide KGs into topological clusters and a weighted aggregation composition GCN for efficient embedding learning, demonstrating significant reductions in training time while maintaining accuracy.

Beyond GNNs, Transformer architectures, with their powerful self-attention mechanisms, have been adapted to process graph structures, offering a global perspective on entity and relation interactions. These models move beyond localized message passing to capture long-range dependencies across the entire graph. \textcite{yang2023} developed CKGE, a KG-based Transformer for explainable talent training course recommendations. This model constructs motivation-aware meta-graphs and employs a specialized Transformer with relational attention and structural encoding to capture global dependencies and provide transparent explanations. A significant step forward is the TGformer framework proposed by \textcite{shi2025}, which leverages a graph transformer to integrate both triplet-level and graph-level structural features, along with contextual information, across static and temporal knowledge graphs. TGformer's Knowledge Graph Transformer Network (KGTN) comprehensively explores multi-structural features, achieving state-of-the-art link prediction by discerning valuable inter-triplet relationships. Furthermore, the synergy between KGE and large language models (LLMs), which are often Transformer-based, is explored by \textcite{liu2024q3q}. Their joint model incorporates aviation assembly KG embeddings into LLMs for prefix-tuning, enabling online reconfiguration and strengthening specialized knowledge for fault diagnosis, showcasing how KGE can enhance the capabilities of large-scale Transformer models in domain-specific applications.

In conclusion, the adoption of GNNs and Transformers has profoundly advanced KGE by enabling models to learn complex, non-linear features directly from the graph structure, moving beyond the limitations of earlier geometric models. GNNs excel at capturing multi-hop contextual and neighborhood information, facilitating inductive learning and nuanced representations. Transformers, through their attention mechanisms, provide a powerful means to model global dependencies and integrate diverse structural and contextual cues. Despite these advancements, challenges remain in balancing the increased expressiveness of these complex models with computational efficiency for extremely large and dynamic KGs, ensuring interpretability, and effectively integrating multimodal information for a holistic understanding of knowledge. Future research will likely continue to refine these architectures, explore more efficient training paradigms, and develop hybrid models that combine the strengths of different neural approaches.