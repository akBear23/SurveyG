\documentclass[12pt,a4paper]{article}
    \usepackage[utf8]{inputenc}
    \usepackage[T1]{fontenc}
    \usepackage{amsmath,amsfonts,amssymb}
    \usepackage{graphicx}
    \usepackage[margin=2.5cm]{geometry}
    \usepackage{setspace}
    \usepackage{natbib}
    \usepackage{url}
    \usepackage{hyperref}
    \usepackage{booktabs}
    \usepackage{longtable}
    \usepackage{array}
    \usepackage{multirow}
    \usepackage{wrapfig}
    \usepackage{float}
    \usepackage{colortbl}
    \usepackage{pdflscape}
    \usepackage{tabu}
    \usepackage{threeparttable}
    \usepackage{threeparttablex}
    \usepackage[normalem]{ulem}
    \usepackage{makecell}
    \usepackage{xcolor}

    % Set line spacing
    \doublespacing

    % Configure hyperref
    \hypersetup{
        colorlinks=true,
        linkcolor=blue,
        filecolor=magenta,      
        urlcolor=cyan,
        citecolor=red,
    }

    % Title and author information
    \title{A Comprehensive Literature Review with Self-Reflection}
    \author{Literature Review}
    \date{\today}

    \begin{document}

    \maketitle

    % Abstract (optional)
    \begin{abstract}
    This literature review provides a comprehensive analysis of recent research in the field. The review synthesizes findings from 377 research papers, identifying key themes, methodological approaches, and future research directions.
    \end{abstract}

    \newpage
    \tableofcontents
    \newpage

    \label{sec:introduction}

\section{Introduction}
\label{sec:introduction}

\subsection{Background: Knowledge Graphs}
\label{sec:1\_1\_background:\_knowledge\_graphs}

The increasing volume and complexity of digital information necessitate robust mechanisms for organizing, representing, and reasoning with vast amounts of data. Knowledge Graphs (KGs) have emerged as a powerful paradigm to address this challenge, offering a structured and machine-readable representation of world knowledge \cite{semantic\_web\_foundations}. Fundamentally, a knowledge graph is a network of real-world entities (e.g., people, places, concepts) and their semantic relationships, typically expressed as subject-predicate-object triples. For instance, the triple (\textit{Barack Obama}, \textit{bornIn}, \textit{Hawaii}) explicitly states a factual relationship between two entities. This structured format distinguishes KGs from unstructured text or traditional relational databases by embedding rich semantic meaning directly into their graph structure, making complex information actionable for intelligent systems.

The historical development of knowledge graphs can be traced back to early artificial intelligence research on semantic networks in the 1960s, which aimed to represent human knowledge in a graph-like structure for machine understanding \cite{semantic\_networks}. This foundational work evolved into the vision of the Semantic Web in the early 2000s, advocating for a "web of data" where information is given well-defined meaning, enabling computers and people to work in cooperation \cite{semantic\_web}. Technologies like the Resource Description Framework (RDF) and Web Ontology Language (OWL) became cornerstones for building this structured web. Modern large-scale knowledge bases, often referred to as knowledge graphs, are a direct realization of this vision, moving beyond domain-specific ontologies to encompass vast amounts of general world knowledge, providing a backbone for many AI applications.

Several prominent examples illustrate the scale and impact of modern knowledge graphs. \textit{Freebase}, an early collaborative knowledge base acquired by Google, was instrumental in demonstrating the potential of large-scale, open-domain KGs to organize factual information \cite{freebase\_origin}. It provided a structured repository of millions of entities and their relationships, significantly influencing the development of Google's Knowledge Graph used in search. Similarly, \textit{DBpedia} automatically extracts structured information from Wikipedia infoboxes, creating a vast cross-domain knowledge graph that serves as a central hub for linking various datasets on the Semantic Web \cite{dbpedia\_creation}. More recently, \textit{Wikidata} has become a crucial, collaboratively edited, and multilingual knowledge base that provides structured data for Wikipedia, Wikimedia Commons, and other projects, acting as a central repository for facts that can be queried and reused globally \cite{wikidata\_overview}. These examples highlight how KGs serve as invaluable resources for aggregating and organizing diverse world knowledge, making it accessible and interoperable.

The structured nature of knowledge graphs makes complex information inherently machine-readable, facilitating a wide array of intelligent systems and artificial intelligence tasks. By providing explicit representations of facts and their interconnections, KGs enable more sophisticated forms of information processing than keyword-based approaches. For instance, in \textit{semantic search}, KGs allow systems to understand the intent behind a query and retrieve relevant entities and relationships, rather than just matching keywords. \textit{Question Answering (QA)} systems leverage KGs to directly find answers to natural language questions by traversing the graph to locate specific facts. Furthermore, KGs enhance \textit{recommendation systems} by providing rich contextual information about items and users, enabling the discovery of nuanced preferences and relationships that go beyond simple co-occurrence. They also support various forms of \textit{reasoning}, allowing AI systems to infer new facts from existing ones, thereby expanding the knowledge base. This foundational understanding of knowledge graphs, as rich, structured repositories of interconnected facts, is paramount for appreciating the subsequent need for advanced techniques, such as knowledge graph embedding, to effectively leverage these intricate structures for machine learning tasks.

\% Note: The provided "Papers to reference" and "Community summaries" were focused on Knowledge Graph Embedding (KGE) models, not the foundational "Background: Knowledge Graphs" itself. Therefore, generic placeholder citations (e.g., \cite{semantic\_web\_foundations}) have been used for the fundamental concepts and examples of Knowledge Graphs, as no specific papers on KG background were available in the provided list.
\subsection{Motivation for KG Embedding}
\label{sec:1\_2\_motivation\_for\_kg\_embedding}

The landscape of artificial intelligence and knowledge representation has long grappled with the challenge of effectively harnessing the vast, structured information encapsulated within symbolic knowledge graphs (KGs). While traditional symbolic KGs, built upon discrete entities and relations, offer explicit, human-interpretable representations of facts, they are inherently constrained by several fundamental limitations. These limitations have catalyzed a profound paradigm shift towards Knowledge Graph Embeddings (KGEs), which transform these symbolic structures into continuous vector space representations \cite{yan2022, rossi2020, dai2020, cao2022}. This subsection elucidates the core motivations behind this transformation, highlighting how KGEs address these limitations and, in doing so, bridge the historical divide between symbolic AI and modern statistical machine learning, albeit with inherent trade-offs.

A primary motivation for the advent of KGEs stems from the pervasive \textbf{sparsity, incompleteness, and computational intractability of symbolic KGs}. Real-world KGs are inherently incomplete; the absence of a triple (head, relation, tail) typically signifies an unknown fact rather than a false one, adhering to an "open-world assumption" \cite{yan2022}. This incompleteness renders traditional rule-based or logical inference computationally prohibitive for large-scale graphs. Exhaustive search over discrete symbols to discover missing facts or infer new knowledge quickly becomes intractable, making large-scale knowledge graph completion a formidable challenge. KGEs directly address this by learning dense, low-dimensional vector representations for entities and relations. In this continuous space, the plausibility of unobserved triples can be efficiently estimated through numerical computations, effectively recasting the problem of discovering missing facts from symbolic manipulation to statistical inference. Early embedding paradigms, while simple, demonstrated this potential for scalable inference, laying the groundwork for more efficient knowledge graph completion \cite{xiao2015}. The rigidity of point-wise models and the ill-posed nature of the algebraic systems they formed further underscored the need for more flexible and robust representations that could handle the inherent ambiguity and incompleteness of KGs \cite{xiao2015}.

Beyond merely overcoming sparsity, a crucial motivation for KGEs is the imperative for \textbf{enhanced expressiveness to capture complex relational patterns and semantic nuances}. Symbolic systems can explicitly define intricate properties such as symmetry, antisymmetry, inversion, and compositionality, or handle complex relation types like one-to-many and many-to-many relationships. However, translating these rich logical properties into a simple continuous vector space without losing fidelity proved challenging for early KGE models. Many initial embedding approaches, while effective for basic link prediction, often struggled to comprehensively model these diverse connectivity patterns and mapping properties simultaneously \cite{peng2020}. This limitation spurred the development of more sophisticated geometric and algebraic structures within the embedding space. For instance, the need to model symmetric and antisymmetric relations simultaneously motivated approaches leveraging complex-valued embeddings, while capturing compositional patterns drove the exploration of rotational models \cite{cao2022}. However, this pursuit of expressiveness in KGEs introduces a critical tension: while they excel at statistical pattern matching, they often sacrifice the direct interpretability and formal guarantees inherent in symbolic logic. Research has highlighted that many popular embedding methods struggle to model even simple ontological rules in a principled way, indicating a fundamental incompatibility between certain vector space representations and formal logical constraints \cite{gutirrezbasulto2018oi0, fatemi2018e6v}. This ongoing challenge underscores the motivation to develop KGE models that can better reconcile the statistical power of embeddings with the logical consistency of symbolic knowledge.

Another significant motivation for KGEs is their ability to \textbf{seamlessly integrate with statistical machine learning models and leverage richer contextual information}. Symbolic representations, being discrete, are not directly amenable to gradient-based neural networks and other statistical learning paradigms. KGEs provide a crucial bridge by transforming symbolic knowledge into a universal, dense vector format, making it directly consumable by modern machine learning algorithms \cite{yan2022}. Furthermore, real-world entities are rarely defined solely by their structural connections; they are often accompanied by textual descriptions, categorized by types, or associated with multi-modal data. Relying exclusively on structural triples can lead to impoverished representations, particularly for entities with limited connections (the "cold-start problem"). This motivated the development of methods that could fuse diverse information sources—such as textual descriptions, entity types, or other attributes—to create more robust and semantically rich embeddings. The inability of early models to inherently respect background taxonomic information, for example, highlighted the need for more sophisticated approaches that could incorporate such auxiliary knowledge to improve embedding quality and consistency \cite{fatemi2018e6v}.

In conclusion, the motivation for knowledge graph embedding is multifaceted and profound, addressing the fundamental limitations of symbolic KGs in terms of sparsity, computational cost, and their inherent difficulty in integrating with modern statistical machine learning paradigms. By providing dense, semantically rich representations, KGEs have enabled a wide array of scalable downstream applications and fostered a powerful, albeit complex, synergy between symbolic and neural AI. This ongoing pursuit of more expressive, context-aware, and application-ready embeddings continues to drive innovation in the field, pushing the boundaries of how knowledge can be represented, reasoned with, and leveraged.
\subsection{Scope and Organization of the Review}
\label{sec:1\_3\_scope\_\_and\_\_organization\_of\_the\_review}

This literature review provides a structured and comprehensive overview of Knowledge Graph Embedding (KGE), delineating its evolution from foundational concepts to advanced methodologies, diverse applications, and emerging research frontiers. The aim is to offer readers a clear roadmap through the intellectual landscape of KGE, highlighting key contributions and the progression of ideas that have shaped the field.

The review commences by establishing the foundational concepts of knowledge graphs and the inherent motivation for embedding entities and relations into continuous vector spaces. Early survey papers, such as \cite{yan2022}, provide a critical starting point, categorizing KGE models into translational distance, semantic matching, and neural network-based approaches, thereby laying the groundwork for understanding the field's initial intellectual trajectories. This initial phase underscores the necessity of KGE to overcome the limitations of symbolic representations, enhancing computational efficiency and enabling downstream machine learning tasks.

Following this foundation, the review delves into core KGE methodologies, tracing a pedagogical progression from simple geometric models to more complex algebraic and neural architectures. The seminal work of \cite{Bordes2013TransE} introduced TransE, a foundational translational model positing that relations act as translations between entity embeddings ($h+r \approx t$). While simple and effective, TransE struggled with complex relational patterns like one-to-many. Subsequent advancements, such as TransH \cite{Wang2014TransH}, addressed this by projecting entities onto relation-specific hyperplanes, while TransR and CTransR \cite{Lin2015TransR} further refined this by mapping entities to relation-specific spaces. \cite{Ji2015TransD} extended this with dynamic mapping matrices, enhancing flexibility. A significant evolution in this paradigm is RotatE \cite{Sun2019RotatE}, which models relations as rotations in complex space, elegantly capturing symmetry, anti-symmetry, and compositionality, demonstrating the expressive power achievable within the translational framework. Complementing these geometric approaches, semantic matching models offered alternative paradigms. RESCAL \cite{Nickel2016RESCAL} introduced a tensor factorization approach, representing relations as matrices, which allowed for richer semantic interactions but at a higher computational cost. ComplEx \cite{Trouillon2016ComplEx} advanced this by using complex-valued embeddings and a Hermitian dot product, efficiently modeling symmetric and antisymmetric relations.

The review then transitions to advanced architectural innovations, particularly those leveraging deep learning. ConvE \cite{Dettmers2018ConvE} marked a significant shift by introducing convolutional neural networks to learn rich, non-linear features from concatenated entity and relation embeddings, achieving state-of-the-art results and moving beyond predefined scoring functions. Further pushing the boundaries, KG-BERT \cite{Zhang2020KG-BERT} adapted pre-trained language models (BERT) to score the validity of triples, demonstrating the power of integrating external, text-based knowledge. Graph Neural Networks (GNNs) also emerged as a powerful paradigm for incorporating structural context, as exemplified by models like R-GCN (mentioned in community summaries). \cite{li2021qr0} further enhanced GNNs for KGE by proposing heterogeneous relation attention networks, specifically designed to address the intrinsic heterogeneity of KGs and aggregate diverse semantic information effectively.

A significant portion of the review is dedicated to practical considerations and diverse applications of KGE, with a particular focus on entity alignment (EA). EA is a critical task for integrating heterogeneous knowledge graphs, but it faces challenges such as the scarcity of labeled data and sensitivity to entity degree differences. BootEA \cite{sun2018} pioneered a bootstrapping approach with global optimization and alignment editing to mitigate error accumulation and improve EA under data scarcity. Building on this, SEA \cite{pei2019} further refined semi-supervised EA by incorporating adversarial training to account for entity degree differences, enhancing robustness. MultiKE \cite{zhang2019} innovated by unifying multiple entity "views" (name, relation, attribute) into a comprehensive embedding framework and introduced "soft alignment" for relations/attributes, reducing dependency on seed alignments. Most recently, OntoEA \cite{xiang2021} presented the first comprehensive framework to integrate ontological schema (class hierarchies, disjointness) into joint KG-ontology embedding, directly addressing "class conflict" errors previously overlooked in embedding-based EA. The utility of KGE extends beyond EA, as demonstrated by applications like explicit semantic ranking for academic search \cite{xiong2017zqu}. The field's rapid evolution is also captured by specialized surveys like \cite{zhu2024}, which offers an updated perspective on graph embedding-based EA, proposing a novel three-module framework and identifying critical gaps. Complementing these, empirical reviews such as \cite{fanourakis2022} provide meta-level analyses, offering statistically significant rankings and correlations between method performance and KG characteristics, guiding practitioners in method selection.

Finally, the review culminates in a discussion of emerging trends, future challenges, and ethical implications. The integration of KGE with Large Language Models (LLMs) represents a cutting-edge direction, as seen in \cite{liu2024q3q}, which proposes a knowledge-enhanced joint model for fault diagnosis in aviation assembly by incorporating KG embedding into LLMs for prefix-tuning and knowledge-based reasoning. This highlights a broader trend towards combining structured knowledge with generative AI capabilities. Future research directions, as identified by surveys like \cite{zhu2024} and \cite{yan2022}, include multimodal KGE, handling dynamic knowledge graphs, improving model explainability, and exploring more complex vector spaces, all of which aim to enhance the expressiveness, robustness, and applicability of KGE models in increasingly complex real-world scenarios.


\label{sec:foundational_kge_models:_geometric_and_semantic_matching}

\section{Foundational KGE Models: Geometric and Semantic Matching}
\label{sec:foundational\_kge\_models:\_geometric\_\_and\_\_semantic\_matching}

\subsection{Translational Models}
\label{sec:2\_1\_translational\_models}

Translational models constitute a foundational paradigm in knowledge graph embedding (KGE), conceptualizing relations as vector translations between entity embeddings within a continuous vector space. This elegant approach posits that for a valid triple $(\text{head, relation, tail})$, the embedding of the head entity ($\mathbf{h}$) plus the relation vector ($\mathbf{r}$) should approximate the embedding of the tail entity ($\mathbf{t}$), typically expressed as $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$. The plausibility of a triple is measured by a scoring function, often the squared L1 or L2-norm of the difference $||\mathbf{h} + \mathbf{r} - \mathbf{t}||$.

The pioneering model in this category is \textbf{TransE (Translating Embeddings)} \cite{bordes2013}, which established this core principle. TransE's appeal lay in its elegant simplicity and computational efficiency. However, its assumption of a single, fixed vector representation for each entity across all relations proved to be a significant limitation. This rigidity made it particularly challenging to model complex relation types such as one-to-many, many-to-one, and many-to-many. For instance, in a one-to-many relation like "person has\_child child", if a person has multiple children, TransE would force the embeddings of all children to be clustered around $\mathbf{h} + \mathbf{r}$, making them indistinguishable or leading to suboptimal representations for the children. These foundational concepts and their initial challenges are comprehensively reviewed in works like \cite{asmara2023}.

To address TransE's limitations, \cite{wang2014} introduced \textbf{TransH (Translating on Hyperplanes)}. This model innovatively allows entities to have distributed representations by projecting them onto relation-specific hyperplanes. For a given relation $\mathbf{r}$, TransH defines a normal vector $\mathbf{w}\_r$ for its hyperplane and a translation vector $\mathbf{d}\_r$ that lies within this hyperplane. Entity embeddings $\mathbf{h}$ and $\mathbf{t}$ are first projected onto this relation-specific hyperplane, yielding $\mathbf{h}\_{\perp} = \mathbf{h} - \mathbf{w}\_r^T \mathbf{h} \mathbf{w}\_r$ and $\mathbf{t}\_{\perp} = \mathbf{t} - \mathbf{w}\_r^T \mathbf{t} \mathbf{w}\_r$. The plausibility of a triple is then measured by the squared L2-norm of the difference: $f\_r(\mathbf{h},\mathbf{t}) = ||\mathbf{h}\_{\perp} + \mathbf{d}\_r - \mathbf{t}\_{\perp}||\_2^2$. This approach effectively handles complex relation types by allowing an entity's representation to adapt to the context of the relation it participates in. Furthermore, \cite{wang2014} incorporated an orthogonality constraint ($\mathbf{w}\_r^T \mathbf{d}\_r = 0$) to ensure the translation vector resides within the hyperplane and proposed an improved Bernoulli negative sampling strategy, which leverages relation mapping properties to generate more effective negative examples during training.

Building on the success of TransH, subsequent models further refined the concept of relation-specific representations. \cite{lin2015} proposed \textbf{TransR (Translating on Relations)} and \textbf{CTransR (Cluster-based Translating on Relations)}. TransR introduced the idea of mapping entities from a general entity space to a distinct relation-specific space using a projection matrix $\mathbf{M}\_r$ for each relation $r$. Thus, the projected entity embeddings become $\mathbf{h}\_r = \mathbf{h}\mathbf{M}\_r$ and $\mathbf{t}\_r = \mathbf{t}\mathbf{M}\_r$, and the scoring function becomes $||\mathbf{h}\_r + \mathbf{r} - \mathbf{t}\_r||$. This allowed for more flexible and expressive representations, as entities could have different "appearances" depending on the relation they are involved in. CTransR extended TransR by recognizing that complex relations might exhibit multiple semantics. Instead of a single relation vector, CTransR clusters head-tail entity pairs for complex relations and learns multiple relation vectors and projection matrices for each cluster, capturing diverse semantic patterns within a single relation type.

Further enhancing this flexibility, \cite{ji2015} introduced \textbf{TransD (Translating with Dynamic Mapping Matrices)}. Unlike TransR, which uses a fixed projection matrix for each relation, TransD employs dynamic mapping matrices that are adaptively generated based on both the entity and the relation. Specifically, for a head entity $h$ and relation $r$, the projection matrix $\mathbf{M}\_{rh}$ is constructed from $\mathbf{r}$ and $\mathbf{h}$ (and similarly $\mathbf{M}\_{rt}$ for the tail entity). This allows for even greater adaptability, as the projection is tailored to the specific entity-relation pair, offering enhanced expressiveness in capturing diverse relational semantics.

Beyond these core extensions, the translational paradigm continues to evolve. \cite{li2020ek4} proposed the Knowledge Graph Embedding with Relational Constraints (KRC) framework, which enhances translation-based models by explicitly encoding regularities between a relation and its arguments into the embedding space. This allows for a more nuanced understanding of relational semantics beyond simple geometric translations. More recently, \cite{li2024uio} introduced TransE-MTP (TransE with Multi-Translation Principles), a direct advancement of TransE designed to better handle complex relations (one-to-many, many-to-one, many-to-many) by defining multiple translation principles for different relation types. This demonstrates ongoing efforts to refine the core translational idea to address persistent challenges.

Despite these significant advancements, translational models, even with their sophisticated extensions, primarily rely on predefined geometric operations. This reliance can limit their capacity to capture highly complex, implicit, or non-linear semantic patterns. A fundamental limitation stems from their inability to inherently model certain relational properties:
\begin{itemize}
    \item \textbf{Symmetry/Antisymmetry}: If $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$ for a symmetric relation, it ideally implies $\mathbf{t} + \mathbf{r} \approx \mathbf{h}$. This only holds if $\mathbf{r} \approx \mathbf{0}$, which trivializes the relation, or if specific constraints are added. For antisymmetric relations, it's even more challenging.
    \item \textbf{Inverse Relations}: For relations like \texttt{parentOf} and \texttt{childOf}, translational models typically require learning two distinct relation vectors, $\mathbf{r}\_{\text{parentOf}}$ and $\mathbf{r}\_{\text{childOf}}$, without an inherent connection between them (e.g., $\mathbf{r}\_{\text{parentOf}} \approx -\mathbf{r}\_{\text{childOf}}$).
    \item \textbf{Compositional Relations}: Translational models struggle to capture compositional patterns (e.g., \texttt{fatherOf} + \texttt{fatherOf} = \texttt{grandfatherOf}). The sum of two relation vectors does not inherently represent the composite relation. This is related to the "Z-paradox," where certain logical inferences cannot be captured by simple vector addition.
\end{itemize}
While models like TransH, TransR, and TransD mitigate some of TransE's initial shortcomings by introducing relation-specific contexts, the core translational assumption still imposes constraints on their expressiveness. These inherent geometric limitations motivated the exploration of alternative paradigms, such as rotational models and tensor factorization, which offer different mathematical frameworks to capture richer and more diverse relational patterns.
\subsection{Rotational and Complex Space Models}
\label{sec:2\_2\_rotational\_\_and\_\_complex\_space\_models}

Moving beyond simple linear translations, a significant advancement in Knowledge Graph Embedding (KGE) models involves leveraging more sophisticated mathematical operations, particularly rotations in complex or higher-dimensional spaces, to capture richer relational semantics. These approaches demonstrate enhanced expressiveness by employing intricate geometric transformations.

An early and influential model in this paradigm is ComplEx \cite{Trouillon\_2016}. ComplEx extends the idea of semantic matching by employing complex-valued embeddings for both entities and relations. It defines a scoring function based on a Hermitian dot product, which naturally allows for the effective capture of both symmetric and antisymmetric relations without requiring explicit constraints or additional parameters. This algebraic approach provided a powerful alternative to earlier translational models, offering a more elegant and efficient solution for these specific relational patterns compared to more parameter-heavy tensor factorization methods like RESCAL. However, while adept at symmetry and antisymmetry, ComplEx did not inherently capture compositional patterns as elegantly as later models.

Addressing the limitations of previous models in simultaneously capturing diverse relational patterns, RotatE \cite{sun2018} emerged as a pivotal model. RotatE maps entities and relations into a complex vector space, interpreting each relation as an element-wise rotation from the head entity to the tail entity. Specifically, for a valid triple $(h, r, t)$, RotatE posits that $t \approx h \circ r$, where $\circ$ denotes the Hadamard (element-wise) product and the modulus of each component of the relation embedding $r$ is constrained to 1. This elegant formulation, inspired by Euler's identity, allows RotatE to simultaneously model symmetric, antisymmetric, and compositional patterns, as well as inversion, within a unified framework. For instance, a relation $r$ can be symmetric if its rotation is its own inverse, antisymmetric if its rotation is distinct from its inverse, and compositional if the rotation of $r\_1$ followed by $r\_2$ results in $r\_3$. This capability represented a significant leap, as prior state-of-the-art models often excelled at one or two patterns but struggled to capture all three concurrently. RotatE also introduced self-adversarial negative sampling, an efficient training technique that generates more informative negative samples based on the current model's scores, further improving its performance.

The development of RotatE, building upon the foundations laid by ComplEx and generalizing translational models, showcased the power of moving beyond simple linear transformations. By embedding entities and relations in complex spaces and defining relations as rotations, these models gained a substantial increase in expressiveness. This allowed for a more nuanced understanding and prediction of complex relational dynamics within knowledge graphs, marking a critical step towards more intricate geometric transformations for KGE. While highly effective, these models still rely on predefined geometric operations, which, despite their sophistication, might not fully capture highly complex or implicit semantic patterns compared to more advanced neural approaches that learn features directly from data.
\subsection{Tensor Factorization and Semantic Matching}
\label{sec:2\_3\_tensor\_factorization\_\_and\_\_semantic\_matching}

Knowledge Graph Embedding (KGE) models often frame the task of learning entity and relation representations as a tensor factorization problem or employ direct semantic matching functions to assess the plausibility of a triple $(h, r, t)$. This paradigm focuses on capturing intricate interactions between entities and relations through algebraic operations, moving beyond purely geometric translations.

A foundational model in this category is RESCAL \cite{nickel2016}, which represents relations as full matrices, allowing for a direct, bilinear interaction between the head entity vector $\mathbf{h}$ and the tail entity vector $\mathbf{t}$ through the relation matrix $\mathbf{M}\_r$. Specifically, the plausibility score for a triple $(h, r, t)$ is computed as $\mathbf{h}^\top \mathbf{M}\_r \mathbf{t}$. This approach captures complex, direct interactions and can model various relational patterns effectively, but it is notably parameter-heavy, leading to high computational costs and scalability challenges for large knowledge graphs. The general class of such models is often referred to as tensor decomposition models in comprehensive surveys \cite{rossi2020}.

Building upon the insights of bilinear models, ComplEx \cite{trouillon2016} introduced a significant advancement by employing complex-valued embeddings for entities and relations. It defines a scoring function using a Hermitian dot product, which naturally models both symmetric and antisymmetric relations without requiring explicit constraints. ComplEx offered a more elegant and parameter-efficient solution for capturing these specific relational patterns compared to the more parameter-heavy RESCAL, demonstrating the power of higher-dimensional, algebraic representations.

The success of these semantic matching approaches spurred further research into generalizing and optimizing scoring functions. AutoSF \cite{zhang2019} proposed an automated machine learning (AutoML) framework to search for optimal scoring functions tailored to specific knowledge graphs. By identifying a unified representation for popular bilinear models, AutoSF could discover novel, KG-dependent scoring functions that consistently outperformed human-designed baselines, highlighting the diversity of optimal semantic matching strategies.

Further extending the expressiveness of semantic matching, models began to incorporate more dynamic and compound operations. TransD \cite{ji2015}, while rooted in the translational paradigm, introduced dynamic mapping matrices for both entities and relations. This allowed for a more fine-grained, relation-aware projection of entities into relation-specific spaces, effectively enabling a more flexible semantic interaction than fixed bilinear forms, while maintaining parameter efficiency. Similarly, TransMS \cite{yang2019} enhanced semantic interaction by modeling multidirectional semantic transmission (e.g., from head to tail entity via the relation, and from entities to relations) using nonlinear transformations, capturing richer semantic flows with remarkable parameter efficiency.

More sophisticated algebraic and geometric transformations have also been explored. CompoundE \cite{ge2022} generalized geometric operations by combining translation, rotation, and scaling into a cascaded "compound operation." This powerful form of semantic matching allows for a richer modeling of complex relation types and provides a unifying framework for several prior distance-based models. HousE \cite{li2022} further advanced this by introducing Householder parameterization for high-dimensional rotations and invertible projections. This unified framework can simultaneously model diverse intrinsic relation patterns (like symmetry, antisymmetry, composition) and complex relation mapping properties (e.g., 1-to-N, N-to-N), offering superior expressiveness through advanced linear algebra. In non-Euclidean spaces, models like TorusE \cite{ebisu2017} embedded entities and relations on a Lie group (torus) to resolve regularization conflicts in translation-based models, providing a novel mathematical space for semantic matching. Similarly, Fully Hyperbolic Rotation (FHRE) \cite{liang2024} performs direct rotational semantic matching within Lorentz hyperbolic space, leveraging its properties for hierarchical data without the overhead of frequent space mappings. Ji et al. \cite{ji2024} employed quaternion embeddings, representing relations as rotations in a complex space, to model multihop fuzzy spatiotemporal knowledge graphs, effectively capturing noncommutative compositional patterns.

The advent of neural networks further expanded the capabilities of semantic matching models by allowing for highly flexible and non-linear scoring functions. M-DCN \cite{zhang2020} utilized multi-scale dynamic convolutional networks with relation-specific filters to extract richer feature interactions from concatenated entity and relation embeddings. ReInceptionE \cite{xie2020} combined an Inception network for deep interaction learning with a relation-aware attention mechanism to integrate both local neighborhood and global entity structural information into the query embedding, enhancing semantic matching with contextual awareness. SEConv \cite{yang2025} applied self-attention and multilayer CNNs to learn deeper structural features from triplets, particularly for medical knowledge graphs, demonstrating the power of neural architectures in specialized domains. Graph Neural Networks (GNNs) and Transformers have also been adapted for semantic matching. LAN \cite{wang2018} introduced a Logic Attention Network for inductive KGE, aggregating neighborhood information with attention. SE-GNN \cite{li2021} explicitly modeled different levels of "semantic evidence" (relation, entity, triple) using GNNs and attention mechanisms for improved extrapolation. MorsE \cite{chen2021} used a GNN modulator within a meta-learning framework to transfer meta-knowledge for inductive entity embedding. More recently, CPa-WAC \cite{modak2024} proposed a weighted aggregation composition GCN, and TGformer \cite{shi2025} leveraged a Graph Transformer framework to capture multi-structural and contextual features for robust entity and relation understanding. MGTCA \cite{shang2024} introduced mixed geometry messages and a trainable convolutional attention network for adaptive GNN switching, further enhancing the flexibility of neural semantic matching. For specific applications, CKGE \cite{yang2023} employed a KG-based Transformer with relational attention for explainable talent training course recommendations, while RKGE \cite{sun2018} used recurrent networks to learn path semantics for effective recommendation. SpherE \cite{li2024} introduced a novel semantic matching function by embedding entities as spheres and relations as rotations, defining plausibility through sphere overlap, which directly addresses the problem of set retrieval for many-to-many relations.

In conclusion, the evolution of tensor factorization and semantic matching models has progressed from simple bilinear interactions to highly expressive neural architectures and multi-geometric spaces. While early models like RESCAL established the foundation for direct algebraic interaction, later works introduced complex-valued embeddings, dynamic projections, compound geometric operations, and sophisticated neural networks to capture increasingly nuanced relational patterns and contextual information. A persistent challenge remains in balancing model expressiveness with interpretability and computational efficiency, as the optimal semantic matching strategy often depends heavily on the specific characteristics of the knowledge graph and the downstream application.


\label{sec:enhancing_kge_expressiveness:_context,_types,_and_advanced_architectures}

\section{Enhancing KGE Expressiveness: Context, Types, and Advanced Architectures}
\label{sec:enhancing\_kge\_expressiveness:\_context,\_types,\_\_and\_\_advanced\_architectures}

\subsection{Context-Augmented and Multi-view Embeddings}
\label{sec:3\_1\_context-augmented\_\_and\_\_multi-view\_embeddings}

Traditional Knowledge Graph Embedding (KGE) models, primarily relying on the structural information of (head, relation, tail) triples, often struggle with data sparsity and fail to capture the full semantic richness of entities and relations. Real-world knowledge graphs (KGs) are frequently accompanied by abundant auxiliary information, such as textual descriptions, entity attributes, and logical rules. This subsection investigates KGE approaches that enrich entity and relation representations by integrating these diverse data types, leading to "context-augmented" or "multi-view" embeddings. These methods aim to provide a more comprehensive understanding of entities, mitigate data sparsity, and enhance performance in tasks like link prediction and entity alignment by providing richer contextual cues \cite{choudhary2021, ge2023}.

One prominent direction involves leveraging \textbf{textual descriptions and linguistic context} associated with entities and relations. These methods bridge the gap between symbolic knowledge and natural language semantics. Early efforts, such as Semantic Space Projection (SSP) \cite{xiao2016}, augmented KGE by projecting the triple loss vector onto a semantic hyperplane derived from textual descriptions, thereby enforcing consistency between structural and textual embedding spaces. This approach highlights the potential of textual data to provide dense semantic cues that are often implicit in sparse triple structures. More recently, with the advent of powerful pre-trained language models (PLMs), methods like LASS \cite{shen2022} have emerged, which jointly embed language semantics from PLMs with structural information. LASS fine-tunes PLMs using a probabilistic structured loss, enhancing knowledge graph completion by integrating rich linguistic context directly into the embedding process. The integration of PLMs offers significant advantages in capturing nuanced semantics and handling out-of-vocabulary entities, but it also introduces challenges related to computational cost, the potential for noise from unstructured text, and effectively aligning the different semantic spaces \cite{ge2023}.

Beyond unstructured text, incorporating richer \textbf{structural context and logical rules} provides powerful inductive biases and enhances the logical consistency of embeddings. While basic KGE models only consider direct triples, context-augmented approaches look at the neighborhood or broader graph patterns. For instance, \cite{gao2018di0} introduced Triple Context-Based Knowledge Graph Embedding, which explicitly utilizes the context of each triple, composed of neighboring entities, their outgoing relations, and relation paths between target entities. This allows for a more informed inference process by considering local graph topology. Logical rules, often expressed as first-order logic, offer a declarative way to encode domain knowledge and enforce consistency. RUGE \cite{guo2017} proposed an iterative framework that guides KGE models with soft logical rules, enabling embeddings to learn from both labeled and rule-inferred unlabeled triples. This approach demonstrates how automatically extracted rules can improve embedding quality by providing additional supervision. Extending this, RulE \cite{tang2022} learns explicit rule embeddings and integrates them into a unified space with entities and relations, allowing for soft rule reasoning and mutual regularization. To improve scalability for rule integration, SLRE \cite{guo2020} directly regularizes relation representations based on soft logical regularities, making the complexity of rule learning independent of the entity set size. Furthermore, the inherent structure of relations can be leveraged, as seen in HRS \cite{zhang2018}, which utilizes a three-layer hierarchical relation structure (relation clusters, individual relations, sub-relations) to enrich knowledge representations, extending existing KGE models to better capture relational hierarchies. From a theoretical perspective, frameworks like Knowledge Sheaves \cite{gebhart2021gtp} offer a sophisticated way to incorporate prior constraints and schema-induced consistency, providing a generalized framework for reasoning about KGE models and adapting them for composite relations. While rule-based methods enhance logical consistency and explainability, they often suffer from the difficulty of extracting complete and accurate rule sets and can be brittle in the face of noisy data.

The concept of multi-view embeddings further extends to integrating \textbf{explicit entity attributes and truly multi-modal data}. This is crucial for creating comprehensive entity representations, especially in domains where entities have rich descriptive properties or non-textual information. Moving beyond simple triples, hyper-relational KGE models like HINGE \cite{rosso2020} directly learn from facts augmented with key-value pairs, simultaneously capturing the base triplet structure and its associated attributes. This allows for a richer representation of complex facts. For tasks like entity alignment, MultiKE \cite{zhang2019} developed a framework that unifies multiple views of entities—such as names, relations, and attributes—by learning view-specific embeddings and employing novel cross-KG inference and soft alignment methods. This approach is particularly effective in integrating heterogeneous information from different KGs. In specialized domains, multimodal reasoning has gained traction. For instance, \cite{zhu2022} constructed Specific Disease Knowledge Graphs (SDKGs) and performed multimodal reasoning by integrating structure, category, and description embeddings via a reverse-hyperplane projection method to discover new disease-specific knowledge. This demonstrates the power of combining diverse modalities for complex inference. In the context of explainable recommendations, CKGE \cite{yang2023} leverages contextualized neighbor semantics and high-order connections within meta-graphs to learn motivation-aware embeddings for talents and courses, providing transparent explanations by integrating rich user and item attributes. These multi-modal and attribute-rich approaches significantly enhance the representational power of KGEs, address cold-start problems, and facilitate cross-domain knowledge transfer, but they face challenges in data heterogeneity, effective fusion strategies, and managing noise across different modalities \cite{choudhary2021}.

In conclusion, the evolution of KGE from purely structural models to sophisticated context-augmented and multi-view approaches marks a critical advancement. By effectively integrating diverse auxiliary information—ranging from textual descriptions and linguistic context to richer structural patterns, logical rules, explicit attributes, and multi-modal data—these methods successfully mitigate data sparsity and significantly enhance the semantic richness and robustness of entity and relation representations. While substantial progress has been made, challenges remain in developing truly unified, adaptive frameworks that can seamlessly and efficiently combine an ever-growing variety of contextual cues, especially in dynamic and evolving knowledge graphs. Future research should focus on adaptive mechanisms for selecting and weighting different information views, ensuring interpretability in complex multi-modal models, and exploring more robust ways to handle noisy or incomplete auxiliary data.
\subsection{Type-Aware and Hierarchical Embeddings}
\label{sec:3\_2\_type-aware\_\_and\_\_hierarchical\_embeddings}

Knowledge graphs (KGs) often possess rich ontological schemas, including semantic type information and hierarchical structures, which are crucial for enhancing the logical consistency and accuracy of learned embeddings. This subsection explores Knowledge Graph Embedding (KGE) models that explicitly leverage entity types, class hierarchies, or logical rules to constrain and refine the embedding process, ensuring that learned representations respect these inherent semantic categories and structural relationships.

Early efforts to incorporate semantic information focused on ensuring a smooth embedding space. \cite{guo2015} introduced Semantically Smooth Embedding (SSE), which leverages entity categories to enforce a "semantically smooth" embedding space, ensuring entities within the same category are embedded closely. Building on this idea of context-dependent representations, \cite{yoon2016} proposed Logical Property Preserving (lpp) variants of TransE, TransR, and TransD. These models utilize role-specific projections to map entities to distinct representations based on their position (head or tail) in a triple, thereby preserving logical properties like transitivity and symmetricity, which are foundational to hierarchical relations.

A significant advancement in explicitly differentiating semantic categories came with \cite{lv2018}'s TransC (Translating Concepts). TransC innovatively models concepts as spheres and instances as vectors in the embedding space, capturing the inherent difference between a category and its members. This geometric distinction allows for the natural preservation of \texttt{instanceOf} and \texttt{subClassOf} transitivity by requiring instance vectors to lie within concept spheres, and sub-concept spheres within super-concept spheres. Extending the idea of structured relations, \cite{zhang2018} introduced a three-layer Hierarchical Relation Structure (HRS) to enrich KGE models. This approach categorizes relations into clusters, individual relations, and fine-grained sub-relations, allowing KGE models to learn more nuanced representations by explicitly leveraging these hierarchical dependencies within the relational schema.

Another powerful avenue for incorporating semantic constraints is through logical rules, which can implicitly define types and hierarchies. \cite{guo2017} proposed RUGE (RUle-Guided Embedding), an iterative framework that combines KGE with automatically extracted soft logical rules. By iteratively predicting soft labels for unlabeled triples and rectifying embeddings based on these rule-guided labels, RUGE enhances knowledge acquisition and representation. Following this, \cite{guo2020} developed Soft Logical Rule Embedding (SLRE), a scalable method for preserving soft logical regularities. SLRE represents relations as bilinear forms and maps entities to a non-negative bounded space, deriving a novel rule-based regularization that directly enforces constraints on relation representations, making its complexity independent of the entity set size. Further advancing rule-guided learning, \cite{tang2022} introduced RulE, which learns explicit rule embeddings and jointly represents entities, relations, and logical rules in a unified continuous space. RulE calculates confidence scores for rules and aggregates them with KGE scores for soft, context-dependent inference, mitigating the brittleness of traditional logic.

The concept of projecting entities onto relation-specific hyperplanes, initially explored by \cite{wang2014} in TransH to handle complex relation mapping properties, has been adapted to incorporate type information. \cite{he2023} presented TaKE (Type-augmented Knowledge graph Embedding), a model-agnostic framework that learns implicit type features without explicit supervision. TaKE employs a relation-specific hyperplane mechanism to project an entity's type representation onto different hyperplanes, capturing diverse type features relevant to specific relations. Similarly, \cite{wang2021} proposed TransET, which explicitly leverages entity types to learn more semantic features. TransET uses circle convolution based on the embeddings of both entities and their types to generate type-specific representations for head and tail entities, which are then used in a translation-based score function. In a domain-specific application, \cite{zhu2022} integrated \textit{category} embeddings (a form of type information) with structural and descriptive embeddings using a reverse-hyperplane projection for multimodal reasoning in Specific Disease Knowledge Graphs (SDKGs), demonstrating the utility of type-awareness in specialized hierarchical contexts.

More recently, hyperbolic geometry has emerged as a natural fit for modeling hierarchical structures due to its exponentially increasing volume with distance. \cite{pan2021} introduced a hyperbolic hierarchy-aware KGE model that leverages an extended Poincaré Ball and a polar coordinate system to effectively represent both hierarchical structures and logical patterns. Building on this, \cite{liang2024} proposed Fully Hyperbolic Rotation (FHRE), which defines KGE operations entirely within the Lorentz hyperbolic space. FHRE models relations as Lorentz rotations, eliminating the need for complex and potentially unstable logarithmic and exponential mappings between hyperbolic and tangent spaces during training, thereby offering a more stable and direct approach to embedding hierarchical data.

Despite these advancements, challenges remain in seamlessly integrating diverse forms of type and hierarchical information while maintaining scalability and interpretability. Future research could focus on developing more adaptive mechanisms for inferring and utilizing implicit type information in highly dynamic and sparse KGs, exploring novel geometric spaces that can simultaneously capture multiple facets of semantic and structural hierarchies, and designing methods for more transparently explaining how type and hierarchical constraints influence embedding outcomes.
\subsection{Graph Neural Networks and Transformers for KGE}
\label{sec:3\_3\_graph\_neural\_networks\_\_and\_\_transformers\_for\_kge}

Traditional knowledge graph embedding (KGE) models, often relying on geometric transformations or bilinear scoring functions, have proven effective but are inherently limited in capturing the intricate multi-hop structural context and complex, non-linear feature interactions within knowledge graphs. These models often struggle with inductive learning for unseen entities and the diverse semantic patterns present in real-world KGs. To overcome these limitations, advanced neural architectures, particularly Graph Neural Networks (GNNs) and Transformers, have emerged as powerful tools for learning more expressive and context-rich KGEs.

Early advancements in leveraging neural architectures for KGE moved beyond simple scoring functions to learn complex features directly from the graph. For instance, \textcite{zhang2020} introduced the Multi-Scale Dynamic Convolutional Network (M-DCN), which employs multi-scale filters with relation-specific dynamic weights to capture diverse characteristics and effectively handle complex relation types (e.g., 1-to-N, N-to-1, N-to-N). Building on this, \textcite{xie2020} proposed ReInceptionE, a model that integrates an Inception network to enhance interactions between head and relation embeddings, alongside a relation-aware attention mechanism to incorporate both local neighborhood and global entity information. More recently, \textcite{yang2025} developed SEConv for healthcare prediction, which combines a resource-efficient self-attention mechanism with a multi-layer Convolutional Neural Network (CNN) to learn deeper and more informative structural features from triplets, demonstrating the continued relevance of CNNs for feature extraction in KGE.

Graph Neural Networks (GNNs) represent a significant paradigm shift, explicitly designed to process graph-structured data by iteratively aggregating information from an entity's neighborhood. This message-passing mechanism allows GNNs to capture multi-hop structural context and neighborhood information, enabling more expressive and often inductive embeddings. \textcite{wang2018} pioneered inductive KGE with the Logic Attention Network (LAN), which aggregates neighbor information using a double-view attention mechanism that is permutation invariant, redundancy-aware, and query-relation-aware, allowing effective embedding of emerging entities. Addressing the heterogeneity inherent in KGs, \textcite{li2021qr0} proposed a heterogeneous GNN framework that aggregates neighbor features under each relation-path and learns the importance of different paths through an attention mechanism, thereby capturing various semantic aspects.

Further deepening the understanding of GNNs for KGE, \textcite{li2021} investigated how KGE models extrapolate to unseen data from a "Semantic Evidence" perspective. They introduced SE-GNN, a multi-layer GNN that explicitly models relation-level, entity-level, and triple-level semantic evidence through distinct neighbor aggregation patterns and attention mechanisms, achieving state-of-the-art performance in knowledge graph completion. For inductive KGE, \textcite{chen2021} introduced MorsE, a meta-learning framework that learns "meta-knowledge" (transferable structural patterns) through an entity initializer and a GNN modulator, enabling the generation of embeddings for entirely new entities in unseen KGs. In dynamic environments, \textcite{sun2024} proposed MetaHG, a meta-learning strategy for KGE that uses a hybrid GNN (combining a standard GNN layer with a Hypergraph Neural Network layer) to incorporate both local and global structural information for continuously updating knowledge in evolving service ecosystems.

The expressiveness of GNNs has been further enhanced by integrating diverse geometric spaces and adaptive attention. \textcite{shang2024} introduced MGTCA, a model that generates rich neighbor messages by combining information from hyperbolic, hypersphere, and Euclidean spaces. MGTCA also features a trainable convolutional attention network that adaptively switches between different GNN types (GCN, GAT, and a novel KGCAT) to overcome data dependence and learn optimal attention for local structures. To address the scalability challenges of GNNs for large-scale KGs, \textcite{modak2024} presented CPa-WAC, a lightweight architecture that utilizes constellation partitioning to divide KGs into topological clusters and a weighted aggregation composition GCN for efficient embedding learning, demonstrating significant reductions in training time while maintaining accuracy.

Beyond GNNs, Transformer architectures, with their powerful self-attention mechanisms, have been adapted to process graph structures, offering a global perspective on entity and relation interactions. These models move beyond localized message passing to capture long-range dependencies across the entire graph. \textcite{yang2023} developed CKGE, a KG-based Transformer for explainable talent training course recommendations. This model constructs motivation-aware meta-graphs and employs a specialized Transformer with relational attention and structural encoding to capture global dependencies and provide transparent explanations. A significant step forward is the TGformer framework proposed by \textcite{shi2025}, which leverages a graph transformer to integrate both triplet-level and graph-level structural features, along with contextual information, across static and temporal knowledge graphs. TGformer's Knowledge Graph Transformer Network (KGTN) comprehensively explores multi-structural features, achieving state-of-the-art link prediction by discerning valuable inter-triplet relationships. Furthermore, the synergy between KGE and large language models (LLMs), which are often Transformer-based, is explored by \textcite{liu2024q3q}. Their joint model incorporates aviation assembly KG embeddings into LLMs for prefix-tuning, enabling online reconfiguration and strengthening specialized knowledge for fault diagnosis, showcasing how KGE can enhance the capabilities of large-scale Transformer models in domain-specific applications.

In conclusion, the adoption of GNNs and Transformers has profoundly advanced KGE by enabling models to learn complex, non-linear features directly from the graph structure, moving beyond the limitations of earlier geometric models. GNNs excel at capturing multi-hop contextual and neighborhood information, facilitating inductive learning and nuanced representations. Transformers, through their attention mechanisms, provide a powerful means to model global dependencies and integrate diverse structural and contextual cues. Despite these advancements, challenges remain in balancing the increased expressiveness of these complex models with computational efficiency for extremely large and dynamic KGs, ensuring interpretability, and effectively integrating multimodal information for a holistic understanding of knowledge. Future research will likely continue to refine these architectures, explore more efficient training paradigms, and develop hybrid models that combine the strengths of different neural approaches.


\label{sec:dynamic_and_temporal_knowledge_graph_embeddings}

\section{Dynamic and Temporal Knowledge Graph Embeddings}
\label{sec:dynamic\_\_and\_\_temporal\_knowledge\_graph\_embeddings}

\subsection{Early Temporal Integration and Time Series Modeling}
\label{sec:4\_1\_early\_temporal\_integration\_\_and\_\_time\_series\_modeling}

The transition from static to dynamic Knowledge Graph Embeddings (KGEs) marks a pivotal advancement, acknowledging that real-world facts are inherently time-dependent and evolve. This foundational phase saw the emergence of distinct paradigms to explicitly integrate the temporal dimension, laying the groundwork for more sophisticated temporal reasoning. These early efforts primarily explored geometric, statistical, and algebraic approaches to capture the fluidity of knowledge.

One of the pioneering geometric approaches is HyTE \cite{dasgupta2018}, which introduced a novel mechanism for temporal integration. HyTE associates each timestamp with a unique hyperplane in the embedding space. This allows for a temporally-guided inference, where the plausibility of a triple $(h, r, t)$ at a specific time $T$ is determined by its proximity to the hyperplane corresponding to $T$. A key strength of HyTE lies in its ability to predict temporal scopes for facts with missing time annotations, addressing a critical challenge in incomplete knowledge graphs. However, HyTE's approach has inherent limitations. The assumption of a distinct hyperplane for every timestamp can become computationally expensive and parameter-heavy for continuous time or very long temporal spans. Moreover, its linear projections might struggle to capture complex, non-linear, or periodic temporal patterns. Building on HyTE, Hybrid-TE \cite{wang20198d2} sought to enhance its capabilities by combining it with TransD and projecting a triplet to \textit{all} time-specific hyperplanes on which it is temporally valid. This extension aimed to improve precision for multi-relational facts, demonstrating an early attempt to refine the geometric temporal modeling paradigm.

Addressing the deterministic nature of HyTE's temporal representations, ATiSE \cite{xu2019} introduced a more sophisticated statistical approach. ATiSE conceptualizes entity and relation evolution not as fixed points or hyperplanes, but as multi-dimensional additive time series. Crucially, it represents these evolving embeddings as Gaussian distributions, explicitly capturing temporal uncertainty through their covariance matrices. This novel connection between relational processes and time series analysis allows ATiSE to model the probabilistic nature of temporal facts. While offering a richer representation of uncertainty, ATiSE faces challenges related to the computational cost of learning and storing covariance matrices for all entities and relations, especially in high-dimensional spaces. Its reliance on explicit time series modeling assumptions might also limit its flexibility in capturing highly irregular or sparse temporal events.

Complementing these geometric and statistical models, tensor decomposition methods emerged as a powerful algebraic framework for temporal modeling. This paradigm inherently integrates time by conceptualizing knowledge graph facts as a fourth-order tensor $\mathcal{X} \in \mathbb{R}^{|\mathcal{E}| \times |\mathcal{R}| \times |\mathcal{E}| \times |\mathcal{T}|}$, where the dimensions correspond to head entities, relations, tail entities, and time. Early works in this area, such as TTransE \cite{leblay2018ttranse} and TA-TransE/TA-DistMult \cite{garcia2018learning}, extended static tensor factorization models to incorporate the temporal dimension. TTransE, for instance, augments the translational scoring function of TransE by adding a time embedding that influences the triple's plausibility. Similarly, TA-TransE and TA-DistMult learn time-aware relation embeddings or project static embeddings into time-specific spaces, effectively treating time as an additional context for relations. By applying various tensor decomposition techniques (e.g., CP decomposition, Tucker decomposition) to this four-dimensional structure, these methods directly embed the temporal dimension alongside other components, offering a robust way to learn temporal representations. More recent works like \cite{lin2020} continue to explore and refine tensor decomposition for temporal KGEs, often focusing on efficiency or specific decomposition strategies. However, a common limitation of tensor decomposition methods is their susceptibility to data sparsity, as real-world temporal KGs often have many missing entries in the 4D tensor. Furthermore, the computational complexity of decomposing high-order tensors can be substantial, and these methods often treat time as discrete indices, which may not adequately capture continuous temporal dynamics or fine-grained temporal interactions without further modifications.

These foundational models collectively demonstrated the feasibility and necessity of temporal integration in KGEs, employing diverse strategies from geometric interpretations and statistical time series analysis to multi-dimensional tensor representations. Each paradigm offered unique strengths in modeling specific aspects of temporal knowledge, from discrete temporal validity to probabilistic evolution. However, a common challenge across these early methods lies in their potential struggle with highly complex, non-linear temporal patterns, the scalability issues associated with explicit temporal indexing or high-order tensor operations, and the limited capacity to model continuous time or capture long-range temporal dependencies efficiently. These limitations subsequently motivated the development of more advanced models, particularly those leveraging more expressive geometric spaces and neural architectures, capable of handling intricate temporal dynamics and geometric complexities.
\subsection{Rotation-based and Complex Space Temporal Models}
\label{sec:4\_2\_rotation-based\_\_and\_\_complex\_space\_temporal\_models}

Building on the success of static rotational models like RotatE \cite{sun2019rotate} and addressing limitations of earlier temporal KGE approaches that often relied on additive time-series or hyperplane projections \cite{li2017hyte, ma2020atise}, a sophisticated class of temporal models emerged. These models leverage rotations in complex or higher-dimensional geometric spaces to capture the intricate dynamics of evolving knowledge graphs, offering enhanced expressiveness for diverse temporal patterns. This paradigm shift provides more robust mechanisms for modeling how entities and relations evolve over time, moving beyond simpler linear transformations to capture more nuanced relational semantics.

A pivotal contribution in this direction is TeRo (Time-aware Knowledge Graph Embedding via Temporal Rotation) by \textcite{xu2020}, which introduced the concept of modeling temporal evolution as element-wise rotations in a complex vector space. TeRo addressed the limitations of earlier temporal KGE models, which often struggled with complex relation patterns like asymmetry and reflexivity, and were not robust in handling diverse time annotations such as intervals. By deriving time-specific entity embeddings through rotations from their initial states, TeRo effectively captures dynamic changes while preserving underlying structural properties. Furthermore, it innovated by employing dual relation embeddings for time intervals, allowing it to adapt to various temporal annotations and providing the first investigation into the effect of time granularity on performance \cite{xu2020}. While TeRo significantly advanced temporal modeling, its reliance on a 2-dimensional complex space per embedding component might limit its capacity to capture extremely high-dimensional or topologically complex temporal patterns.

Building upon the foundation laid by TeRo, \textcite{sadeghian2021} ChronoR (Rotation Based Temporal Knowledge Graph Embedding) generalized the rotation mechanism to $k$-dimensions, aiming to improve temporal link prediction while addressing issues of large parameter counts and the limitations of Euclidean distance in high-dimensional spaces. ChronoR introduced an innovative inner product scoring function, which theoretically encompasses complex-domain models like ComplEx as a special case when $k=2$, thereby providing a unifying theoretical framework \cite{sadeghian2021}. This generalization offers greater flexibility in choosing the embedding dimensionality, potentially allowing for the capture of more intricate temporal dynamics than a fixed complex space. The model further enhanced robustness and generalizability through novel tensor nuclear norm-inspired regularization and a temporal smoothness objective that explicitly encourages similar transformations for chronologically closer timestamps \cite{sadeghian2021}. Compared to TeRo's implicit temporal evolution through rotation, ChronoR's explicit smoothness objective provides a more direct mechanism for modeling gradual changes. However, the increased dimensionality and regularization complexity in ChronoR can lead to higher computational costs and potentially more challenging optimization landscapes.

Further extending the expressiveness of rotation-based temporal modeling, researchers have explored hypercomplex numbers and non-Euclidean geometries. \textcite{zhang2022muu} introduced TimeLine-Traced Knowledge Graph Embedding (TLT-KGE), which leverages complex or, more notably, \textit{quaternion} vectors to embed entities and relations with timestamps. TLT-KGE uniquely models semantic information and temporal information as distinct axes within the quaternion space, allowing for a separation of these concerns while establishing a connection between them. Quaternions, as 4-dimensional hypercomplex numbers, offer a richer algebraic structure for representing rotations in 3D space, providing a more powerful mechanism than complex numbers for encoding complex, multi-faceted temporal dynamics. This approach addresses the challenge of distinguishing representations of the same entity or relation at different timestamps, which is often difficult for models that conflate semantic and temporal information.

Pushing the boundaries of geometric complexity even further, the 5EL model by \textcite{zhang2025ebv} proposes embedding temporal knowledge graphs into \textit{projective geometric space}, specifically leveraging Möbius Group transformations on the Riemann Sphere. This advanced geometric approach is designed to overcome the limitations of single underlying embedding spaces (like Euclidean or standard complex spaces) that struggle to model intricate temporal patterns such as hierarchical and ring structures. By operating in a non-Euclidean space, 5EL can inherently capture these complex topological relationships. Furthermore, 5EL integrates Large Language Models (LLMs) to extract crucial temporal node information, employing a parameter-efficient fine-tuning strategy to align LLMs with specific task requirements. This hybrid approach enhances the model's ability to recognize structural information of key nodes in historical chains and enriches the representation of central entities, demonstrating a trend towards combining sophisticated geometric modeling with advanced neural architectures for temporal KGE.

These rotation-based and complex/higher-dimensional space models represent a significant advancement by providing highly expressive mechanisms for capturing the intricate dynamics of temporal knowledge graphs. Their ability to model diverse relation patterns (asymmetry, reflexivity), handle various time intervals, generalize to higher dimensions, and even integrate more complex factors like hierarchical or cyclic temporal structures, marks a robust evolution in temporal KGE. However, the increased dimensionality and integration of multiple factors, especially with hypercomplex numbers or non-Euclidean geometries, can lead to significantly higher model complexity, posing challenges for interpretability, computational efficiency, and the practical deployment of these models, particularly as the number of dimensions or integrated features grows. The choice of the most suitable geometric space often remains an empirical decision, heavily dependent on the specific characteristics and temporal patterns present in the knowledge graph.
\subsection{Multi-Curvature and Spatiotemporal Embeddings}
\label{sec:4\_3\_multi-curvature\_\_and\_\_spatiotemporal\_embeddings}

The inherent complexity of dynamic knowledge graphs, characterized by diverse geometric structures, temporal evolution, spatial attributes, and inherent uncertainties, necessitates embedding models that transcend the limitations of single Euclidean spaces. This subsection explores cutting-edge approaches that leverage multi-curvature geometries, integrate spatiotemporal information, and incorporate fuzziness to capture the multifaceted nature of real-world knowledge.

Early efforts to introduce geometric awareness into temporal knowledge graph embeddings (TKGE) include \cite{dasgupta2018}, which proposed HyTE, a hyperplane-based model that associates each timestamp with a hyperplane to enable temporally-guided inference and predict temporal scopes. Building on the need to capture more nuanced temporal dynamics, \cite{xu2019} introduced ATiSE, which modeled entity and relation evolution as additive time series with Gaussian distributions, marking a significant step towards explicitly accounting for temporal uncertainty. Similarly, \cite{lin2020} offered a tensor decomposition method, representing facts as a fourth-order tensor to inherently capture time, providing a generalizable framework. More recently, \cite{li2023} TeAST introduced a novel Archimedean spiral timeline for relations, transforming the problem into 3rd-order tensor completion to avoid direct entity evolution and enhance interpretability.

A significant paradigm shift in temporal modeling emerged with rotation-based embeddings, which leverage complex spaces to elegantly capture temporal evolution. \cite{xu2020} TeRo pioneered this by introducing temporal rotation in complex space for entity evolution, effectively handling diverse relation patterns and time intervals. This approach offered superior expressiveness for various relation patterns and temporal dynamics. Extending this, \cite{sadeghian2021} ChronoR generalized rotation to k-dimensions, proposing an inner product scoring function that theoretically encompasses complex-domain models like ComplEx, and introduced advanced regularization for temporal smoothness. These rotation-based models laid the groundwork for integrating more complex dimensions.

Recognizing that real-world knowledge graphs exhibit a variety of geometric structures (e.g., hierarchical, ring-like, chain-like) that cannot be adequately captured by a single Euclidean space, recent research has moved towards multi-curvature adaptive embeddings. \cite{wang2024} MADE addresses this limitation by modeling temporal knowledge graphs in multiple curvature spaces, specifically Euclidean (zero curvature), hyperbolic (negative curvature), and hyperspherical (positive curvature) spaces. It employs a data-driven weighting mechanism to adaptively select and strengthen the most suitable geometry for different parts of the graph, alongside a quadruplet distributor and innovative temporal regularization for smoothness. Further refining this concept, \cite{wang2024} IME builds upon MADE by explicitly addressing the "spatial gap" and heterogeneity between different curvature spaces. IME integrates "space-shared" properties to capture commonalities across spaces and "space-specific" properties for unique features, complemented by an Adjustable Multi-curvature Pooling (AMP) mechanism that learns optimal pooling weights for adaptive information fusion and novel similarity, difference, and structure loss functions.

The integration of spatial and fuzzy information alongside temporal dynamics represents another critical advancement. \cite{ji2024} FSTRE (Fuzzy Spatiotemporal RDF Knowledge Graph Embedding) introduces a groundbreaking model that comprehensively integrates fuzziness, spatial, and temporal dimensions within a complex vector space. This model uniquely employs projection for spatial information and rotation for temporal embedding, leveraging anisotropic vectors to incorporate fine-grained fuzziness into spatiotemporal five-tuples. FSTRE addresses the insufficiency of traditional KGE models for uncertain and dynamic knowledge, demonstrating its capability to handle complex fuzzy spatiotemporal information. Extending this, \cite{ji2024} Multihop Fuzzy Spatiotemporal RDF Knowledge Graph Query further leverages quaternion-based embeddings to jointly represent spatiotemporal entities and enable multihop fuzzy spatiotemporal queries. Quaternions, with their non-commutative compositional patterns, allow for robust path construction and reasoning over uncertain and dynamic knowledge graphs, addressing the challenge of multihop querying in such complex environments. Complementary to these geometric approaches, \cite{xie2023} TARGAT offers a Graph Neural Network (GNN) based approach with a dynamic time-aware relational generator to explicitly capture multi-fact interactions across different timestamps, providing another avenue for modeling complex temporal dependencies.

In conclusion, the evolution of temporal KGE has progressed significantly from foundational temporal integration to sophisticated multi-curvature and spatiotemporal embeddings. While models like MADE and IME offer geometrically adaptive solutions for diverse TKG structures, FSTRE and its quaternion-based extension push the boundaries by comprehensively integrating spatial, fuzzy, and temporal dimensions, even enabling complex multihop queries. A shared challenge across these advanced models lies in managing the increased computational overhead associated with multi-space embeddings, complex GNN architectures, or quaternion operations, and enhancing the interpretability of how different curvature spaces or integrated dimensions contribute to the overall embedding quality. Future directions will likely focus on optimizing these complex models for scalability, developing more transparent mechanisms for adaptive geometry selection, and exploring their application in real-world scenarios demanding highly nuanced spatiotemporal and uncertain reasoning.


\label{sec:practical_considerations:_efficiency,_scalability,_and_robustness}

\section{Practical Considerations: Efficiency, Scalability, and Robustness}
\label{sec:practical\_considerations:\_efficiency,\_scalability,\_\_and\_\_robustness}

\subsection{Training Optimization and Negative Sampling}
\label{sec:5\_1\_training\_optimization\_\_and\_\_negative\_sampling}

Optimizing the training process and effectively generating negative examples are paramount for the stability, efficiency, and accuracy of knowledge graph embedding (KGE) models. The inherent incompleteness of knowledge graphs necessitates strategies to distinguish true facts from false ones, typically achieved through contrasting positive triples with synthetically generated negative triples. This section delves into the evolution of negative sampling techniques, adaptive loss functions, and other optimization strategies that enhance KGE model performance.

The critical role of negative sampling (NS) in KGE training is underscored by comprehensive reviews such as \cite{madushanka2024}, which systematically categorize and analyze various NS methods, highlighting their advantages, disadvantages, and open challenges. Early improvements over uniform random sampling focused on generating more informative negative examples. For instance, \cite{sun2018} introduced self-adversarial negative sampling within the RotatE model, which samples negative triples based on the current model's scores, making them more challenging and providing stronger gradient signals. Building on this, \cite{shan2018} proposed a confidence-aware negative sampling method specifically designed for noisy knowledge graphs, leveraging the concept of "negative triple confidence" to support robust training of confidence-aware KGE models like CKRL, mitigating issues like zero loss and false detection.

To address the efficiency and quality limitations of dynamic negative sampling, \cite{zhang2018} introduced NSCaching, a simple yet effective cache-based approach. NSCaching maintains and samples from a small, dynamically updated cache of "hard" negative triplets, effectively tackling the vanishing gradient problem without the complexity, instability, or computational overhead associated with Generative Adversarial Network (GAN)-based methods. Extending the concept of negative sampling to more complex data structures, \cite{zhang2023} developed Modality-Aware Negative Sampling (MANS) for multi-modal KGE. MANS innovates by performing modal-level negative sampling (e.g., only sampling negative visual embeddings), which explicitly facilitates modality alignment between heterogeneous embeddings, a crucial aspect often overlooked by traditional entity-level sampling. Furthermore, domain-specific negative sampling strategies have emerged, such as the type-constrained negative sampling proposed by \cite{he2023} within the TaKE framework, which leverages implicit type features to construct more effective negative samples without requiring explicit type supervision.

While negative sampling remains a cornerstone, the field has also explored non-sampling frameworks to overcome its inherent instability and suboptimal accuracy. \cite{li2021} presented the Efficient Non-Sampling Knowledge Graph Embedding (NS-KGE) framework, which mathematically re-derives and re-organizes the non-sampling square loss function. This innovation enables KGE models to be trained efficiently on all positive and negative instances, eliminating the need for sampling and leading to more stable and accurate embeddings without incurring prohibitive computational or space complexity.

Beyond negative sampling, other optimization techniques focus on refining loss functions and enhancing model robustness. The adaptive nature of training parameters is crucial; for example, \cite{jia2017} introduced TransA, an adaptive translation method that dynamically determines the margin of the loss function based on the local characteristics of different knowledge graphs. This adaptive margin improves embedding performance and facilitates incremental learning for evolving KGs. Similarly, \cite{xiao2015} (also named TransA) proposed an adaptive approach that replaces the oversimplified Euclidean distance with an adaptive Mahalanobis distance. This allows for flexible elliptical equipotential surfaces and suppresses noise by weighting different dimensions of the loss vector, thereby better modeling complex relation topologies.

Robustness to noisy data, a pervasive issue in real-world KGs, has also driven significant optimization efforts. \cite{zhang2021} developed a multi-task reinforcement learning (MTRL) framework for robust KGE, where RL agents are trained to make hard decisions to filter out noisy triples. This framework, combined with multi-task learning for similar relations, provides a powerful mechanism for data cleansing and robust representation learning. Furthermore, ensuring the reliability of KGE model predictions is vital for downstream applications. \cite{tabacof2019} addressed the problem of probability calibration in KGE models, demonstrating that KGE predictions are often uncalibrated. They proposed calibration techniques, including a novel heuristic for handling synthetic negatives, to ensure that predicted probabilities accurately reflect true confidence, thereby simplifying triple classification and enhancing trustworthiness.

In conclusion, the optimization of KGE training has evolved from basic negative sampling to sophisticated, context-aware, and even non-sampling methodologies. This progression reflects a continuous drive to enhance stability, efficiency, and accuracy, alongside a growing emphasis on robustness to noise and the reliability of predictions. Despite these advancements, challenges remain in designing NS methods that are universally optimal, integrating diverse contextual information seamlessly, and developing truly adaptive and self-correcting training paradigms, as highlighted by ongoing research in the field \cite{madushanka2024}.
\subsection{Scalability and Parallelization Techniques}
\label{sec:5\_2\_scalability\_\_and\_\_parallelization\_techniques}

Scaling knowledge graph embedding (KGE) models to handle massive, real-world knowledge graphs (KGs) presents a crucial challenge, demanding innovative techniques to manage computational and memory costs while accelerating training times. Early efforts focused on optimizing model architectures for inherent efficiency. For instance, \textcite{ji2015} introduced TransD, a model that uses a dual-vector representation for entities and relations to dynamically construct mapping matrices, significantly reducing parameter count and avoiding computationally intensive matrix-vector multiplications, thereby enhancing scalability. Similarly, \textcite{yang2019} proposed TransMS, which models multidirectional semantics with remarkable parameter efficiency, adding only a single scalar parameter per triplet, making it highly scalable for large KGs. \textcite{ebisu2017} addressed TransE's regularization conflict by embedding on a compact Lie group (torus), leading to a regularization-free and more efficient model with lower computational complexity. More recently, models like CompoundE \textcite{ge2022} and HousE \textcite{li2022} have achieved superior expressiveness while maintaining efficiency; CompoundE leverages compound geometric operations to achieve state-of-the-art performance on large KGs with fewer parameters, while HousE employs Householder parameterization for high-dimensional rotations and invertible projections, optimizing matrix-vector multiplications into efficient vector operations.

As KGE models grew in complexity, the need for more direct computational and memory optimizations became apparent. Knowledge distillation emerged as a powerful technique to reduce model footprint. DualDE \textcite{zhu2020} dually distills knowledge from a high-dimensional teacher KGE to a low-dimensional student, achieving 7x-15x parameter reduction and 2x-6x inference speedup with minimal accuracy loss by adaptively weighting soft labels and employing a two-stage distillation process. Complementing this, LightKG \textcite{wang2021} introduced a lightweight framework that uses codebooks and codewords for efficient storage and inference, significantly reducing memory footprint while maintaining high approximate search accuracy. This framework also incorporates a residual module for codebook diversity and a continuous function to approximate non-differentiable codeword selection. In the context of neural architectures, SEConv \textcite{yang2025} integrates a less resource-consuming self-attention mechanism with a multilayer convolutional neural network to learn deeper structural features, designed for efficient deployment on resource-limited consumer electronics.

For truly massive KGs, parallelization and distributed training techniques are indispensable. \textcite{kochsiek2021} conducted a comprehensive empirical comparison of various parallelization techniques for KGE models, demonstrating that even simple random partitioning with suitable sampling can be highly effective, and proposing a stratification variation that mitigates negative impacts on embedding quality. Building on this, CPa-WAC \textcite{modak2024} tackled the scalability of GNN-based KGE models, which are notoriously resource-intensive. It introduced Constellation Partitioning (CPa), a topology-preserving algorithm using Louvain/Leiden clustering with hierarchical merging to divide KGs into manageable subgraphs, and a novel Global Decoder framework to effectively combine cluster-specific embeddings for global inference. This approach achieved up to a five-fold reduction in training time without sacrificing prediction accuracy.

Beyond model and partitioning strategies, efficient system designs and data management are critical for minimizing communication overhead and maximizing throughput. GE2 \textcite{zheng2024} presented a general and efficient KGE learning system that offloads computationally intensive operations from CPU to GPU for enhanced parallelism and proposes the novel COVER algorithm for efficient multi-GPU data swap, achieving substantial training speedups (2x to 7.5x) by minimizing communication costs. In distributed and privacy-sensitive environments, Federated KGE (FKGE) introduces unique challenges. FedS \textcite{zhang2024} addressed communication efficiency in FKGE by proposing an Entity-Wise Top-K Sparsification strategy. This method dynamically identifies and transmits only the most changed entity embeddings, coupled with an intermittent synchronization mechanism to handle embedding inconsistency, significantly reducing communication overhead with negligible performance degradation.

In conclusion, the evolution of KGE scalability techniques has progressed from initial architectural optimizations and parameter reduction to sophisticated distributed training frameworks and highly efficient system designs. While significant strides have been made in managing computational and memory costs, particularly for GNN-based models and federated learning, challenges persist in balancing accuracy with efficiency in highly dynamic, heterogeneous, and extremely large-scale distributed environments. Future research will likely focus on more adaptive resource allocation, novel communication-efficient algorithms for diverse distributed settings, and integrating hardware-aware optimizations to further push the boundaries of KGE scalability.
\subsection{Robustness to Noise, Errors, and Data Imbalance}
\label{sec:5\_3\_robustness\_to\_noise,\_errors,\_\_and\_\_data\_imbalance}

Real-world knowledge graphs (KGs) are inherently imperfect, frequently containing noisy data, erroneous triples, and skewed (long-tail) distributions, which significantly challenge the reliability and performance of Knowledge Graph Embedding (KGE) models. This subsection explores advanced techniques designed to enhance the robustness of KGE models against these pervasive data imperfections.

Early efforts to address data quality focused on mitigating noise during training. \cite{shan2018} introduced the concept of \textit{negative triple confidence} and proposed a \textit{confidence-aware negative sampling method} to improve training stability and prevent false detections in confidence-aware KGE models operating on noisy KGs. Building on the need for reliable predictions, \cite{tabacof2019} revealed that KGE models often produce \textit{uncalibrated probabilities}, making their confidence scores unreliable. They proposed methods to \textit{calibrate output probabilities} using synthetic negatives and a novel weighting scheme, ensuring trustworthy confidence scores crucial for downstream decision-making. To actively combat noise during the learning process, \cite{zhang2021} introduced a \textit{multi-task reinforcement learning (MTRL)} framework. In this approach, RL agents make \textit{hard decisions} to select high-quality triples, guided by \textit{delayed rewards} from the KGE model and leveraging multi-task learning for semantically similar relations, moving beyond passive confidence scoring to active data filtering. Further addressing the issue of erroneous data, \cite{zhang2024} presented the \textit{Error-Aware Knowledge Graph Embedding (AEKE)} framework. This framework leverages \textit{entity attributes and hypergraphs} to compute \textit{joint confidence scores} for triples, adaptively \textit{down-weighting unreliable ones} during embedding learning, thereby providing a direct mechanism to mitigate the impact of errors using auxiliary information.

Beyond noise and errors, KGE models must contend with data imbalance. While not explicitly designed for imbalance, \cite{xiao2015} (TransA) introduced an \textit{adaptive Mahalanobis distance with a relation-specific weight matrix} to model diverse and complex relation topologies, implicitly adapting to varying data characteristics and suppressing noise from irrelevant dimensions. This laid groundwork for adaptive metric learning. Tackling a specific form of semantic imbalance, \cite{lv2018} (TransC) differentiated between \textit{concepts (represented as spheres) and instances (as vectors)}, modeling \texttt{isA} relations through spatial containment to capture hierarchical structures and transitivity, which are often skewed in real KGs. Directly addressing the pervasive \textit{long-tail data imbalance}, \cite{zhang2023} (WeightE) introduced a novel \textit{bilevel optimization framework} to adaptively assign \textit{higher weights to infrequent entities and relations} during training. This ensures better representation for under-represented knowledge, providing a direct and flexible mechanism to mitigate the impact of skewed distributions.

General improvements in training robustness and efficiency also contribute to handling imperfect data. To ensure more robust and efficient training, \cite{zhang2018} (NSCaching) proposed a \textit{cache-based negative sampling method} that directly stores and samples "hard" negative triplets, using \textit{importance sampling} for dynamic updates. This approach effectively addresses the vanishing gradient problem more efficiently than complex GAN-based methods. Further enhancing training stability and accuracy, \cite{li2021} (NS-KGE) introduced an \textit{efficient non-sampling framework} that mathematically re-derives the square loss to consider \textit{all negative instances} without prohibitive computational cost, thereby eliminating the uncertainty inherent in sampling. A comprehensive \textit{review of negative sampling methods} by \cite{madushanka2024} consolidates the diverse strategies, highlighting their impact on KGE robustness and efficiency, and underscoring the ongoing importance of this training component. Additionally, \cite{xiao2015} (TransG) addressed ambiguity, which can be seen as a form of inherent data imperfection, by utilizing a \textit{Bayesian non-parametric infinite mixture model} to discover \textit{multiple latent semantic components} for relations. This allows for adaptive selection of the most appropriate translation vector for each triple, reducing noise from unrelated semantics.

Despite significant progress, several challenges remain. The dynamic nature of noise and imbalance in evolving KGs requires continuous adaptation rather than one-time fixes. Generalizing these robustness techniques across diverse KGE architectures and seamlessly integrating multiple robustness mechanisms (e.g., error detection, imbalance handling, and calibrated outputs) within a single, end-to-end optimized framework remains an open research direction. Future work could also explore more sophisticated causal inference techniques to better understand and mitigate the root causes of data imperfections.


\label{sec:kge_for_downstream_applications}

\section{KGE for Downstream Applications}
\label{sec:kge\_for\_downstream\_applications}

\subsection{Link Prediction and Knowledge Graph Completion}
\label{sec:6\_1\_link\_prediction\_\_and\_\_knowledge\_graph\_completion}

Link prediction, often synonymous with knowledge graph completion, stands as the most fundamental and widely studied application of knowledge graph embeddings (KGEs). Its primary objective is to infer missing facts or relationships between entities within an incomplete knowledge graph (KG), thereby enriching and maintaining the integrity of these crucial large-scale knowledge bases \cite{rossi2020}. The task is typically framed as predicting the missing head entity ($?$, $r$, $t$), tail entity ($h$, $r$, $?$), or even the relation ($h$, $?$, $t$) for a given triple. KGE models address this by learning low-dimensional vector representations (embeddings) for entities and relations, which are then used to compute a plausibility score for any given triple $(h, r, t)$. A higher score indicates a greater likelihood that the triple represents a true fact.

The general mechanism involves training KGE models to assign high scores to observed (true) triples and low scores to unobserved (false) triples. During inference, for a query such as $(h, r, ?)$, the model computes scores for all possible tail entities $t'$ (i.e., $(h, r, t')$) and ranks them. The entities with the highest scores are predicted as the most plausible missing links. This process is critical for various reasons: KGs are inherently incomplete, manually curating them is resource-intensive, and automated completion is essential for their continuous growth and utility in downstream AI applications.

The effectiveness of KGE models for link prediction is rigorously evaluated using a set of standard metrics. Key among these are Hits@k and Mean Reciprocal Rank (MRR). Hits@k measures the proportion of correct entities ranked within the top $k$ positions (e.g., Hits@1, Hits@3, Hits@10). A higher Hits@k value indicates that the model frequently ranks the correct answer among its top predictions. Mean Reciprocal Rank (MRR) calculates the average of the reciprocal ranks of the correct entities across all queries; it assigns a higher score if the correct entity is ranked higher. These metrics are typically reported in two settings: "raw" and "filtered" \cite{rossi2020}. The "raw" setting considers all candidate entities, including other true triples that might exist in the KG. The "filtered" setting, which is more commonly used and considered more indicative of a model's true predictive power, removes all other known true triples from the candidate list before ranking. This ensures that a model is not penalized for predicting another valid fact that happens to be different from the one in the test set. The comprehensive survey by \cite{rossi2020} provides a critical comparative analysis of 16 state-of-the-art KGE models, emphasizing the importance of these evaluation practices and how structural features of the training data influence predictive performance. Similarly, \cite{ferrari2022r82} conducted an extensive study comparing 13 models across six datasets, highlighting the high dependence between model performance and graph types, and advocating for fine-grained evaluation considering training times, memory, and carbon footprint.

Different KGE architectures contribute to inferring new facts by capturing distinct types of relational patterns. Foundational models, whether based on geometric transformations (e.g., translational models like TransE, TransH, TransD discussed in Section 2.1) or semantic matching (e.g., RESCAL, ComplEx discussed in Section 2.2 and 2.3), provide a scoring function that is used to rank all possible entities as potential completions for a given query. More advanced architectures, such as Graph Neural Networks (GNNs) and Transformers (Section 3.3), enhance link prediction by effectively capturing multi-hop structural context and neighborhood information, leading to more expressive and often inductive embeddings. Furthermore, the integration of temporal information (Section 4) allows models like HyTE \cite{dasgupta2018} to predict missing links while respecting the temporal validity of facts, adding a crucial dimension to knowledge graph completion.

The development of efficient and reproducible tools has also been instrumental in advancing link prediction research. Libraries like LibKGE \cite{broscheit2020} and TorchKGE \cite{boschin2020ki4} provide researchers with modular frameworks for training, hyperparameter optimization, and evaluation of KGE models specifically for link prediction. These platforms facilitate systematic experimentation, ensuring reproducibility and enabling deeper analysis of model components, which is vital for identifying robust solutions and pushing the boundaries of the field.

Despite significant progress, challenges in link prediction persist. Accurately predicting rare or long-tail relations, handling the open-world assumption (where unobserved facts are not necessarily false), and performing complex multi-hop reasoning remain active research areas. The computational cost of training and evaluating models on massive KGs is also a practical concern, necessitating scalable and efficient training paradigms (Section 5.2). Future research continues to explore more robust, generalizable, and efficient methods for link prediction, aiming to further enhance the completeness and utility of knowledge graphs across diverse domains.
\subsection{Entity Alignment}
\label{sec:6\_2\_entity\_alignment}

Entity alignment (EA) is a critical task for integrating heterogeneous knowledge graphs (KGs), aiming to identify equivalent entities across different knowledge bases to enable interoperability and enrich semantic understanding. Knowledge Graph Embedding (KGE) techniques, which represent entities and relations as low-dimensional vectors, have proven instrumental in addressing this challenge by leveraging learned entity representations to match corresponding entities. The foundation for such methods lies in robust KGE models like TransH \cite{wang2014}, which improved upon earlier translational models by projecting entities onto relation-specific hyperplanes to better capture complex relation types, and TransD \cite{ji2015}, which further refined entity and relation diversity through dynamic mapping matrices. More advanced models like RotatE \cite{sun2018} introduced relational rotations in complex space, capable of simultaneously modeling diverse relational patterns such as symmetry, inversion, and composition, thereby generating richer entity embeddings crucial for discerning subtle equivalences in EA.

Early embedding-based entity alignment approaches faced significant hurdles, particularly the scarcity of labeled training data (prior alignments), which limited their precision. To mitigate this, \cite{sun2018} introduced BootEA, a pioneering bootstrapping approach that iteratively labels likely entity alignments to expand training data. BootEA further incorporated an alignment editing method and a global optimization strategy based on max-weighted matching to reduce error accumulation and ensure one-to-one alignment, marking a substantial advancement in semi-supervised EA. Building upon the need for robust semi-supervised learning, \cite{pei2019} proposed Semi-supervised Entity Alignment (SEA), which enhanced KGE by explicitly incorporating awareness of entity degree differences through adversarial training. This approach improved alignment accuracy and robustness, particularly for entities with varying frequencies, by addressing a limitation where high-frequency entities might dominate the embedding space.

While these methods effectively addressed data scarcity, they often relied primarily on relational structures, overlooking other rich entity features. Recognizing this limitation, \cite{zhang2019} introduced MultiKE, a novel framework that unifies multiple, complementary "views" of entities (name, relation, and attribute) to learn more comprehensive embeddings for alignment. MultiKE innovated by designing view-specific embedding models and, crucially, a "soft alignment" mechanism for relations and attributes that automatically identifies and updates alignment information during training, thereby reducing the heavy dependency on pre-existing seed alignments for these components. This multi-view approach significantly enhanced the accuracy and robustness of entity alignment by leveraging a broader spectrum of entity characteristics.

Despite advancements in integrating diverse features, a critical source of error, termed "class conflicts," persisted due to the neglect of ontological schema (classes, hierarchies, and logical constraints) in existing EA methods. To address this, \cite{xiang2021} presented OntoEA, the first comprehensive framework to integrate ontological knowledge into joint KG-ontology embedding. OntoEA introduced a novel Class Conflict Matrix (CCM) to model inter-class conflicts and employed non-linear ontology and membership embedding modules to ensure semantically consistent alignments, preventing false positives that arise from aligning entities belonging to incompatible classes. This marked a significant step towards ensuring the semantic integrity of aligned knowledge graphs.

As the field of KGE-based entity alignment matured, the need for systematic analysis and evaluation became paramount. General KGE surveys, such as that by \cite{yan2022}, provided foundational classifications of embedding models and their applications. More specifically for EA, \cite{fanourakis2022} conducted an experimental review, offering a meta-level analysis of state-of-the-art EA methods. This study provided statistically significant rankings of methods and identified correlations between their performance and various KG meta-features, offering empirical guidance for method selection. Most recently, \cite{zhu2024} provided a specialized and up-to-date survey on embedding-based EA, proposing a novel three-module framework (Information Aggregation, Alignment, and Post-Alignment) and identifying critical future directions, including multimodal EA and handling dynamic KGs.

In conclusion, the application of KGE to entity alignment has evolved from foundational embedding models to sophisticated, multi-faceted solutions. Initial efforts focused on overcoming data scarcity through semi-supervised techniques like bootstrapping \cite{sun2018} and addressing entity heterogeneity \cite{pei2019}. Subsequent research expanded the scope of information leveraged, incorporating diverse entity features through multi-view embeddings \cite{zhang2019} and integrating higher-level ontological semantics to ensure semantic consistency \cite{xiang2021}. While significant progress has been made in resolving data heterogeneity and enabling interoperability, challenges remain, particularly concerning scalability to extremely large and dynamic KGs, the integration of multimodal data, and the continuous need to balance rich information exploitation with computational efficiency. These areas represent promising avenues for future research in entity alignment.
\subsection{Question Answering and Recommendation Systems}
\label{sec:6\_3\_question\_answering\_\_and\_\_recommendation\_systems}

Knowledge Graph Embeddings (KGEs) serve as a foundational technology for advancing intelligent systems in natural language understanding (NLU) for question answering (QA) and personalized recommendation systems (RS). By learning rich, low-dimensional vector representations of entities and relations within knowledge graphs (KGs), KGEs bridge the gap between unstructured natural language and structured knowledge, and enable the discovery of intricate user-item interactions that often elude traditional, sparse data models.

In the realm of Question Answering over Knowledge Graphs (QA-KG), KGEs provide a critical mechanism for mapping complex natural language queries into relevant KG components, facilitating accurate and efficient answer retrieval. Early approaches, such as the Knowledge Embedding based Question Answering (KEQA) framework \cite{huang2019}, focused on "simple questions" answerable by a single head entity and predicate. KEQA innovated by jointly recovering the question's head entity, predicate, and tail entity representations within the KG embedding space, utilizing a carefully designed joint distance metric to retrieve the closest fact. While effective for single-hop queries, this highlighted the need for more sophisticated mechanisms to handle the inherent complexities of natural language and multi-hop reasoning. To address the challenge of integrating rich textual semantics with KG structures, the Joint Language Semantic and Structure Embedding (LASS) model \cite{shen2022} fine-tunes pre-trained language models (LMs) using a probabilistic structured loss. LASS effectively leverages the forward pass of LMs to capture semantics from textual descriptions while simultaneously reconstructing KG structures through backpropagation, thereby enabling joint learning of both types of information. This integration is vital for accurately understanding natural language questions and mapping them to relevant KG components, especially for multi-hop queries where intermediate entities and relations must be inferred. More recently, the synergy between KGEs and large language models (LLMs) has been explored to enhance QA capabilities. KGEs provide a structured, factual grounding for LLMs, mitigating issues like hallucination and enabling more precise, knowledge-based reasoning. Approaches often involve incorporating KGEs into LLMs through techniques like prefix-tuning or retrieval augmentation, where graph-structured data from KGs is used to reconfigure LLMs or retrieve relevant subgraphs, strengthening specialized knowledge within a domain and providing more accurate responses \cite{liu2024q3q}.

For Recommendation Systems (RS), KGEs offer a powerful paradigm to learn semantic representations of users, items, and their interactions, effectively addressing the limitations of traditional collaborative filtering, such as data sparsity and cold-start problems \cite{liu2019e1u}. By embedding KG entities and relations, KGEs can capture rich contextual information and infer implicit relationships, leading to more effective and often explainable personalized suggestions.

The application of KGEs in recommendation systems can be broadly categorized into several paradigms:

Firstly, \textbf{embedding-based collaborative filtering} methods directly leverage KGEs to enrich user and item representations. These approaches often treat the recommendation task as a link prediction problem within an extended knowledge graph that includes user-item interactions. For instance, KGECF \cite{zhang2020wou} proposes a neural network that models user-item interactions as an interaction knowledge graph, learning vector representations through the RotatE KGE model. This converts collaborative filtering into link prediction, demonstrating state-of-the-art performance across diverse datasets. Similarly, other factorization-based KGE models like HolE and DistMult have been applied to build semantic-based recommenders, showing effectiveness in extracting semantics for explainable recommendations and alleviating cold-start issues \cite{kartheek2021aj7}.

Secondly, \textbf{path-based methods} explicitly exploit the rich relational paths between entities in a KG to infer user preferences and item similarities. The Recurrent Knowledge Graph Embedding (RKGE) model \cite{sun2018} exemplifies this by automatically learning semantic representations for both entities and the paths connecting them within a KG. RKGE employs a novel recurrent network architecture to model the semantics of multiple paths linking the same entity pair, fusing these learned path semantics into the recommendation process. Furthermore, it incorporates a pooling operator to discriminate and leverage the saliency (importance) of different paths, offering not only effective personalized suggestions but also meaningful explanations for its recommendations. This move towards explainable recommendations, driven by interpretable path semantics, represents a significant step beyond opaque black-box models.

Thirdly, the current state-of-the-art in KG-based recommendation is largely dominated by \textbf{Graph Neural Network (GNN) based approaches}. GNNs, through their message passing and aggregation mechanisms, are particularly adept at capturing higher-order, implicit interactions and structural information within KGs, which path-based methods might struggle to fully leverage due to computational complexity or reliance on predefined meta-paths. Models like KGCN (Knowledge Graph Convolutional Network) and KGAT (Knowledge Graph Attention Network) have demonstrated superior performance by iteratively aggregating neighborhood information to learn more expressive entity and user embeddings. A recent example, IDGCN \cite{pham20243mh}, proposes a knowledge graph embedding with a Graph Convolution Network for context-aware recommendation systems. It considers all user and item-based relationships to detect intricate connections, outperforming baseline approaches in personalized recommendations by effectively modeling the relationships between entities.

Finally, the drive for \textbf{explainable recommendation systems} has seen significant advancements, moving beyond simple path interpretations. While RKGE offered initial steps towards explainability through path saliency \cite{sun2018}, more sophisticated models now integrate contextualized KGEs with advanced neural architectures. For instance, CKGE (Contextualized Knowledge Graph Embedding) \cite{yang2023} proposes an explainable training course recommender system that integrates motivation-aware information by constructing specific meta-graphs for talent-course pairs. It employs a novel KG-based Transformer with relational attention and structural encoding, and crucially, a local path mask prediction mechanism that quantifies and highlights the saliency of meta-paths, providing explicit, motivation-driven explanations for recommendations.

Despite these advancements, several challenges remain. For QA, accurately handling highly complex, multi-hop questions that require intricate reasoning over diverse relation types, and ensuring robust entity and relation linking in noisy or evolving KGs, continues to be an active research area. The dynamic nature of KGs also poses a challenge for maintaining up-to-date embeddings for both QA and RS, requiring models that can adapt to temporal changes efficiently. For recommendation systems, integrating real-time user feedback and dynamic item attributes into KGE models, as well as developing more sophisticated mechanisms for cold-start scenarios that do not rely on extensive side information, are crucial future directions. The ongoing exploration of synergistic approaches combining KGEs with large language models, particularly for complex reasoning and personalized content generation, holds significant promise for the next generation of intelligent QA and recommendation systems, demanding KGEs that are not only expressive but also scalable, interpretable, and robust.
\subsection{Domain-Specific Applications}
\label{sec:6\_4\_domain-specific\_applications}

Knowledge graph embeddings (KGEs) have proven instrumental in transcending general-purpose knowledge representation, demonstrating remarkable versatility and impact when tailored for specialized, complex domains. These applications leverage KGEs for intricate knowledge discovery, nuanced analytical tasks, and highly specialized question answering, often integrating multimodal data and domain-specific reasoning mechanisms.

In the biomedical domain, KGEs are pivotal for accelerating knowledge discovery and drug repurposing. \cite{zhu2022} addressed the lack of disease-specific focus by pioneering the construction of Specific Disease Knowledge Graphs (SDKGs) and introducing a novel multimodal reasoning approach. This method effectively integrates structural, categorical, and descriptive embeddings via reverse-hyperplane projection to uncover new, reliable knowledge, with manual proofreading and molecular-level validation confirming its efficacy for specific diseases. Building on the need for deeper insights and practical deployment, \cite{yang2025} developed SEConv, a semantic-enhanced KGE model for healthcare prediction. SEConv leverages a resource-efficient self-attention mechanism to generate expressive embeddings and employs multi-layer convolutional neural networks to learn complex, deeper structural features from medical KGs, demonstrating its applicability for deployment on resource-limited consumer electronics and enhancing medical decision-making.

Beyond life sciences, KGEs offer powerful tools for innovation analysis within vast, heterogeneous datasets like patent metadata. \cite{li2022} tackled the challenge of measuring complex, heterogeneous knowledge proximity by constructing 'PatNet', a large-scale knowledge graph from US patent metadata (1976-2020). By applying various KGE models, including TransE\\_l2, to PatNet, their work operationalized heterogeneous knowledge proximity (e.g., between inventors and technological domains) using cosine similarity of embeddings, providing a unified framework for analyzing innovation dynamics that surpassed limitations of prior measures restricted to homogeneous entity pairs. This approach demonstrated strong performance in explaining real-world domain expansion profiles.

For highly specialized scientific fields, KGEs underpin advanced question answering systems capable of handling deep domain ontologies and intricate mechanisms. \cite{zhou2023} presented "Marie and BERT," a sophisticated KGQA system specifically designed for chemistry. This system integrates hybrid multi-embedding spaces, parallel querying, a BERT-based bidirectional entity-linking model, and a joint numerical embedding model to efficiently answer questions requiring numerical filtering and understanding of complex chemical reaction mechanisms, thereby navigating deep chemical ontologies with high precision and robustness. The system's modular design allows for specialized handling of diverse chemical knowledge.

These diverse applications collectively underscore the transformative potential of KGEs in specialized domains. They highlight a critical evolution from generic link prediction to context-aware, multimodal, and domain-specific reasoning, often integrating advanced neural architectures and validation mechanisms. The progression from foundational multimodal reasoning to efficient deep structural feature learning, and from heterogeneous proximity measurement to highly specialized QA, showcases KGEs' adaptability. Future research will likely focus on further enhancing the explainability of these complex models, adapting to real-time knowledge evolution, and developing more robust multimodal integration strategies to unlock even deeper insights in these critical fields.


\label{sec:emerging_directions_and_future_challenges}

\section{Emerging Directions and Future Challenges}
\label{sec:emerging\_directions\_\_and\_\_future\_challenges}

\subsection{Inherent Limitations and Unresolved Tensions}
\label{sec:7\_1\_inherent\_limitations\_\_and\_\_unresolved\_tensions}

Despite significant advancements in knowledge graph embedding (KGE) paradigms, the field continues to grapple with inherent limitations and persistent unresolved tensions that hinder the realization of truly intelligent and robust knowledge systems. These challenges primarily revolve around the difficulty in handling open-world assumptions, the limitations in performing complex logical reasoning, the persistent gap between expressiveness and interpretability, and the trade-offs between model complexity, computational efficiency, and the ability to capture highly diverse and uncertain relational patterns.

A fundamental tension in KGE lies in its implicit reliance on a closed-world assumption, where unobserved facts are often treated as false during training, particularly through negative sampling strategies. While early translational models like TransH \cite{wang2014} and TransD \cite{ji2015} significantly improved the modeling of complex relation types (e.g., 1-N, N-1, N-N) by introducing relation-specific hyperplanes or dynamic mapping matrices, their core mechanisms for link prediction still operate within a binary truth framework. This contrasts sharply with real-world knowledge graphs (KGs), which are inherently incomplete and where an unobserved fact is merely unknown, not necessarily false. The challenge of performing complex, multi-hop logical reasoning, a hallmark of symbolic AI, also remains largely unresolved by current KGE methods. Even models designed to capture compositional patterns, such as RotatE (a special case of HolmE \cite{zheng2024}), which formalizes the property of being "closed under composition" and provides a unifying framework for several geometric models, primarily focus on learning statistical patterns of composition rather than enabling explicit, arbitrary logical deductions over multiple hops. Similarly, neural architectures like the Multi-Scale Dynamic Convolutional Network (M-DCN) \cite{zhang2020} or ReInceptionE \cite{xie2020}, which leverage advanced CNNs and attention mechanisms to extract richer, multi-hop contextual information, excel at statistical pattern matching for link prediction but struggle with the explicit, rule-based inference capabilities of symbolic systems. While efforts to model uncertainty and fuzziness, as seen in the quaternion embedding approach for fuzzy spatiotemporal RDF KGs \cite{ji2024}, move beyond binary truth values, their "multihop query" capabilities are more akin to pathfinding in an uncertain context rather than complex logical reasoning. Even the integration of hyper-relational facts by HINGE \cite{rosso2020}, which captures richer information beyond simple triplets, does not inherently equip models with robust logical inference capabilities.

Another persistent tension lies in the trade-off between the expressiveness of KGE models and their interpretability. Highly expressive geometric models, such as CompoundE \cite{ge2022}, which combines translation, rotation, and scaling operations, or HousE \cite{li2022}, which uses Householder parameterization for high-dimensional rotations and invertible projections to model all relation patterns, achieve superior performance but often at the cost of transparency. Their intricate mathematical operations make it challenging to directly interpret \textit{why} a specific prediction is made or to understand the semantic meaning of individual embedding dimensions. Similarly, advanced neural architectures like TGformer \cite{shi2025}, a Graph Transformer framework designed to capture multi-structural and contextual features, or SEConv \cite{yang2025}, which uses self-attention and multilayer CNNs for deeper structural features in healthcare KGs, often operate as "black boxes." While some models attempt to bridge this gap, their interpretability is often limited. SpherE \cite{li2024}, which embeds entities as spheres with an interpretable radius reflecting entity "universality" and is designed for set retrieval, offers a step towards more intuitive embeddings. Similarly, Contextualized Knowledge Graph Embedding (CKGE) \cite{yang2023} for explainable recommendations uses meta-graphs and local path mask prediction to provide post-hoc explanations, but these are often derived features rather than inherent properties of the embedding space itself.

The pursuit of increased expressiveness also frequently leads to greater model complexity and computational cost, creating a significant tension with efficiency and scalability. For large-scale KGs, this becomes a critical bottleneck. While models like TransMS \cite{yang2019} aim for parameter efficiency by modeling multidirectional semantics, many state-of-the-art neural and geometric models are computationally intensive. To address this, approaches like DualDE \cite{zhu2020} employ knowledge distillation to achieve faster and cheaper reasoning, and LightKG \cite{wang2021} utilizes codebooks for efficient storage and inference. However, these efficiency gains often come with inherent approximations or potential sacrifices in expressiveness. For large-scale Graph Neural Network (GNN)-based KGEs, CPa-WAC \cite{modak2024} introduces constellation partitioning to enable scalability, but managing partitioned embeddings for global inference remains a complex challenge. Furthermore, capturing highly diverse, uncertain, and dynamic relational patterns adds another layer of complexity. Hyperbolic embedding models, such as the Hyperbolic Hierarchy-Aware KGE \cite{pan2021} and Fully Hyperbolic Rotation (FHRE) \cite{liang2024}, are adept at modeling hierarchical structures due to their negative curvature. FHRE \cite{liang2024} specifically aims to operate entirely within hyperbolic space to reduce mapping overhead. However, specialized geometric spaces may not universally capture all types of relational patterns, leading to a trade-off between tailored expressiveness and general applicability. The challenge of adapting model architectures to specific KG properties, as highlighted by AutoSF \cite{zhang2019} which automatically searches for optimal scoring functions, underscores the difficulty in designing a single, universally effective model for diverse and evolving KGs. Moreover, handling dynamic and evolving KGs, addressed by MetaHG \cite{sun2024} (meta-learning for incremental updates) and MorsE \cite{chen2021} (meta-knowledge transfer for inductive KGE), introduces complexities in maintaining consistency and generalization over time.

In conclusion, current KGE paradigms face fundamental limitations in moving beyond closed-world assumptions, performing robust multi-hop logical reasoning, and reconciling expressiveness with interpretability. The inherent trade-offs between model complexity, computational efficiency, and the ability to capture highly diverse, uncertain, and dynamic relational patterns necessitate fundamental breakthroughs. Future research needs to explore hybrid symbolic-neural approaches that combine the strengths of both paradigms, develop inherently interpretable embedding spaces, and design adaptive, resource-efficient models that can truly operate under open-world assumptions while performing complex logical inference.
\subsection{Theoretical Foundations and Expressiveness Guarantees}
\label{sec:7\_2\_theoretical\_foundations\_\_and\_\_expressiveness\_guarantees}

The advancement of Knowledge Graph Embedding (KGE) models necessitates a profound theoretical understanding that transcends mere empirical performance. This pursuit focuses on elucidating their inherent representational capabilities and limitations, formalizing algebraic properties, and resolving fundamental expressiveness paradoxes to guide the design of mathematically sound and logically consistent architectures.

Early KGE models, while demonstrating impressive empirical performance, often lacked explicit theoretical guarantees regarding their ability to capture complex relational patterns. For instance, foundational translational models like TransE \cite{bordes2013} struggled with modeling symmetric relations or complex compositional patterns without specific architectural modifications or heuristic rules. Building upon these initial observations, models such as RotatE \cite{sun2019}, as discussed in Section 2.2, made significant strides by implicitly capturing diverse relational patterns through element-wise rotations in complex vector space. RotatE's inherent capacity to infer symmetry, antisymmetry, inversion, and composition demonstrated a higher degree of expressiveness. However, this capability was largely an emergent property of its design, highlighting a critical need for a more generalized and formal framework to define and guarantee such properties across the broader spectrum of KGE architectures.

A significant line of theoretical inquiry has focused on formalizing the logical expressiveness of KGE models, often by drawing connections to fragments of first-order logic (FOL). Researchers have analyzed which logical rules (e.g., symmetry, transitivity, inversion, composition) different KGE models can inherently represent \cite{wang2018, guo2018, chen2020}. This work revealed that many popular models, despite their empirical success, are fundamentally limited in their ability to capture even basic logical inferences due to their underlying geometric or algebraic structures. For example, simple vector addition in TransE, while elegant, inherently restricts its capacity to model certain complex logical relationships. These analyses laid the groundwork for understanding \textit{why} certain models succeed or fail at specific reasoning tasks, moving beyond black-box performance metrics and providing a critical lens for evaluating their theoretical soundness.

More recently, efforts have intensified to formalize algebraic properties that ensure robust modeling of complex relational patterns. A critical property in this regard is "closure under composition," which ensures that if relations $r\_1$ and $r\_2$ can be composed to form $r\_3$ (i.e., $h \xrightarrow{r\_1} m \xrightarrow{r\_2} t \implies h \xrightarrow{r\_3} t$), the model can consistently represent this composition. \textcite{zheng2024} introduced HolmE, a Riemannian KGE model specifically designed to achieve this property. By embedding entities and relations into a Riemannian manifold, HolmE offers a flexible geometric framework that inherently supports compositional reasoning, even for complex or long-tail relational patterns. Their work provides a unifying theoretical framework, demonstrating that prominent models like TransE and RotatE can be interpreted as special cases of HolmE under specific geometric assumptions. This formalization represents a significant step towards providing mathematical guarantees for relational inference, moving beyond ad-hoc pattern recognition to principled design.

Concurrently, researchers have identified and addressed fundamental expressiveness paradoxes that expose inherent limitations in KGE model capabilities. A notable example is the "Z-paradox," which describes a specific four-entity inference pattern where two distinct paths (e.g., $A \xrightarrow{r\_1} B \xrightarrow{r\_2} D$ and $A \xrightarrow{r\_3} C \xrightarrow{r\_4} D$) should logically imply the same relationship between $A$ and $D$, but many KGE models, due to their inherent structural limitations (e.g., reliance on simple vector addition or fixed transformations), are structurally incapable of representing this equivalence. This paradox highlights a critical gap between empirical performance and logical consistency. \textcite{liu2024} addressed this by proposing MQuinE, a novel KGE model designed to explicitly cure the "Z-paradox." MQuinE achieves this by employing a richer algebraic structure based on quaternions, which provides greater flexibility in modeling complex transformations and interactions between entities and relations, thereby enabling the model to capture the nuanced equivalences inherent in the Z-paradox. By identifying and providing a mathematically sound resolution to such fundamental limitations, MQuinE contributes to the development of models that are not only empirically strong but also logically consistent and theoretically robust.

These theoretical advancements collectively aim to provide unifying frameworks that explain the underlying principles, capabilities, and limitations of diverse KGE architectures. While significant progress has been made in formalizing properties like closure under composition and resolving expressiveness paradoxes, critical challenges remain. The ongoing quest involves extending these theoretical guarantees to more complex scenarios, such as the dynamic and temporal KGs discussed in Section 4, or multi-modal KGs (Section 7.3), where the interplay of time, context, and diverse data modalities introduces new layers of complexity. Furthermore, developing frameworks that can dynamically adapt their expressiveness based on the specific relational patterns present in a given knowledge graph, rather than relying on a fixed expressive capacity, represents a promising future research avenue. This continuous drive for deeper theoretical understanding is crucial for guiding the design of more robust, mathematically sound, and logically consistent KGE models capable of sophisticated reasoning.
\subsection{Multi-modal and Cross-Domain KGE}
\label{sec:7\_3\_multi-modal\_\_and\_\_cross-domain\_kge}

The full potential of Knowledge Graph Embeddings (KGE) is unlocked by moving beyond structured triples to integrate diverse data modalities and enable knowledge transfer across different domains. This cutting-edge direction addresses the challenges of combining heterogeneous information sources to create richer entity and relation representations, while also tackling issues like semantic disparity and cold-start problems in multi-domain applications.

Initial efforts to enrich KGE representations extended beyond simple triplets to incorporate more complex structured information. For instance, \textcite{rosso2020} introduced HINGE, a hyper-relational KGE model that directly learns from facts composed of base triplets and associated key-value pairs, thereby capturing richer structural information than traditional triplet-based methods. Building on this, \textcite{li2021qr0} addressed the inherent structural heterogeneity within KGs by proposing a novel heterogeneous Graph Neural Network (GNN) framework. This framework leverages an attention mechanism to aggregate multiple types of semantic information from entity neighbor features under distinct relation-paths, allowing for a more nuanced understanding of diverse relationships.

A more direct approach to integrating diverse modalities involves combining different "views" of entities. \textcite{zhang2019} (Multi-view KGE) proposed a framework that unifies name, relation, and attribute views into comprehensive entity embeddings for alignment tasks. Their method introduces "soft alignment" for relations and attributes, significantly reducing the reliance on costly seed alignments, which is particularly beneficial in cross-domain scenarios. Expanding the scope of data types, \textcite{ji2024} tackled fuzzy spatiotemporal RDF knowledge graphs by employing quaternion embeddings. This approach represents relations as rotations and incorporates uncertainty via a bias factor, demonstrating how complex, non-standard data modalities can be effectively integrated into KGE models for multihop query answering. Furthermore, in specialized domains, \textcite{yang2025} developed SEConv for healthcare prediction, a semantic-enhanced KGE model that combines a resource-efficient self-attention mechanism with a multilayer Convolutional Neural Network (CNN) to extract deeper structural features from medical KGs, even drawing inspiration from AI-generated content (AIGC) principles for richer representations.

A fundamental aspect of cross-domain KGE is entity alignment (EA), which aims to bridge semantic gaps and mitigate cold-start issues between disparate knowledge graphs. Early work by \textcite{sun2018} (Bootstrapping EA) addressed the challenge of limited prior alignment by introducing a bootstrapping approach with global optimization and an alignment editing method, significantly improving EA precision. Building upon this, \textcite{pei2019} further enhanced semi-supervised EA by incorporating adversarial training to account for entity degree differences, thereby increasing robustness in data-scarce cross-domain settings. Recognizing that purely structural or attribute-based alignment can be insufficient, \textcite{xiang2021} proposed OntoEA, an ontology-guided EA method. OntoEA jointly embeds KGs and their associated ontologies, leveraging class hierarchies and disjointness to prevent "class conflict" errors and achieve more semantically consistent alignments across domains.

Beyond direct entity alignment, inductive KGE models enable broader knowledge transfer to entirely new domains or KGs. \textcite{chen2021} introduced MorsE, a meta-learning framework that learns "meta-knowledge" (transferable structural patterns) through an entity initializer and GNN modulator. This allows MorsE to produce embeddings for entirely unseen entities in new KGs, effectively addressing cold-start problems in novel domains without requiring full retraining. This meta-learning paradigm is extended by \textcite{sun2024} for dynamic KGE in evolving service ecosystems. Their MetaHG model utilizes a hybrid GNN framework (combining a standard GNN layer with a Hypergraph Neural Network layer) to capture both local and global structural information, facilitating continuous knowledge updates and generating high-quality embeddings for emerging entities in dynamic, multi-domain environments.

Multi-domain recommendation systems represent a key application area for cross-domain KGE. \textcite{liu2023} directly addressed this challenge with a "cross-domain knowledge graph chiasmal embedding approach" for multi-domain item-item recommendation. This method efficiently models both homo-domain item associations and hetero-domain item interactions, effectively tackling the cross-domain cold-start problem. Further enhancing recommendation, \textcite{yang2023} developed CKGE, which uses a KG-based Transformer to integrate contextualized neighbor semantics and high-order connections as "motivation-aware information," providing explainable recommendations in talent training, a multi-domain application. Earlier work by \textcite{sun2018} (Recurrent KGE) also demonstrated the utility of KGE in recommendation by automatically learning path semantics and fusing them into the recommendation process, offering explainable results. Comprehensive surveys such as \textcite{fanourakis2022} and \textcite{zhu2024} provide valuable overviews of entity alignment methods, highlighting the evolution of techniques and identifying multimodal EA as a crucial future direction.

In conclusion, the research trajectory in multi-modal and cross-domain KGE demonstrates a significant shift from static, structured-only representations towards more dynamic, context-aware, and domain-agnostic knowledge representation. While substantial progress has been made in integrating richer structured data, heterogeneous graph information, and multi-view entity features, challenges remain. These include the truly seamless and scalable integration of vastly different modalities (e.g., video, sensor data) into a unified embedding space, robust alignment and knowledge transfer in highly sparse or rapidly evolving cross-domain scenarios, and the development of more interpretable multi-modal KGE models that can explain their predictions derived from complex, heterogeneous inputs. Future work will likely focus on developing more generalized frameworks that can adapt to arbitrary new modalities and domains with minimal supervision, further unlocking the potential of KGE in real-world applications.
\subsection{Explainability, Interpretability, and Trustworthiness}
\label{sec:7\_4\_explainability,\_interpretability,\_\_and\_\_trustworthiness}

The increasing deployment of Knowledge Graph Embedding (KGE) models in sensitive and high-stakes domains, such as healthcare, finance, and scientific discovery, necessitates a profound shift from merely achieving high predictive accuracy to ensuring transparency, understandability, and reliability. Despite their impressive performance, many KGE models often function as "black boxes," making it challenging to discern the rationale behind their inferences. This critical need has spurred research into three interconnected yet distinct concepts: interpretability, explainability, and trustworthiness.

\textbf{Interpretability} refers to the degree to which a human can comprehend the cause of a model's decision or the meaning encoded within its internal representations. In KGE, this involves understanding the semantic properties or relational patterns captured by the learned embedding dimensions. For instance, \cite{tran2019j42} explored the semantic structures within KGE spaces, proposing methods to analyze these structures and perform algebraic operations (e.g., similarity, analogy queries) to shed light on the relationships between entities. Their work aims to make the underlying knowledge more accessible for data exploration, providing insights into what the model has learned. Extending this, \cite{li2021} delved into \textit{how} KGE models extrapolate to unseen data, proposing a "Semantic Evidence" view. They identified three levels of evidence (relation-level, entity-level, triple-level) observable from the training set that strongly correlate with extrapolation ability. By understanding these data-centric factors, researchers can interpret the mechanisms of generalization, thereby making the model's behavior more predictable and understandable. However, a significant challenge remains in directly mapping dense, high-dimensional embedding vectors to genuinely human-understandable features without losing fidelity, often requiring complex dimensionality reduction or projection techniques whose own interpretability can be questioned.

\textbf{Explainability}, in contrast, focuses on providing post-hoc justifications for specific predictions or recommendations, addressing the question of \textit{why} a particular inference was made. A prevalent approach involves extracting symbolic explanations from the underlying knowledge graph. For example, \cite{islam2023} developed an ensemble KGE approach for drug repurposing that not only delivered robust predictions but also generated \textit{rule-based explanations}. These explanations, derived from specific meta-paths or logical rules within the knowledge graph, elucidate the predicted drug-disease association. Such explicit justifications are paramount in critical applications like medicine, enabling domain experts to validate the model's reasoning and fostering confidence. Beyond single KGE models, \cite{kurokawa2021f4f} proposed an explainable knowledge reasoning framework that combines \textit{multiple} KGE techniques with corresponding explainable AI methods, highlighting the complexity and multi-faceted nature of providing explanations, especially when integrating diverse knowledge sources. The primary challenge for explainability lies in ensuring that these explanations are both faithful to the model's internal, often subsymbolic, reasoning and genuinely comprehensible and actionable for human users, particularly for complex multi-hop inferences or those involving non-Euclidean embedding spaces.

\textbf{Trustworthiness} is a broader, overarching concept encompassing the reliability, robustness, validity, and confidence one can place in a KGE system's outputs. It integrates aspects of both interpretability and explainability, alongside rigorous validation, theoretical guarantees, and uncertainty quantification.

A foundational pillar of trustworthiness is \textbf{robust validation and reproducibility}. The KGE field has historically faced challenges regarding the consistency of reported results. \cite{ali2020} addressed this by developing PyKEEN, a unified, open-source framework designed to enable fair and consistent comparison of diverse KGE models. This initiative was vital for establishing reliable baselines and fostering trust in research findings through independent verification. Building on this, \cite{lloyd2022} critically investigated the impact of hyperparameters on KGE quality and, crucially, identified and rectified data leakage issues in benchmark datasets like UMLS. Their contribution of a leakage-robust variant (UMLS-43) is fundamental for ensuring that KGE performance claims are based on sound and unbiased evaluations. Despite these efforts, maintaining universal reproducibility across diverse hardware and software environments remains an ongoing challenge, requiring continuous community vigilance.

Trustworthiness is also significantly bolstered by \textbf{strong theoretical foundations and expressiveness guarantees}. \cite{zheng2024} (HolmE) introduced a Riemannian KGE model that is "closed under composition," offering stronger theoretical guarantees for modeling complex relational patterns and unifying several existing models. Such theoretical soundness contributes to a more predictable and fundamentally understandable KGE behavior, increasing trust in its inferences. A critical advancement in this area is the identification and resolution of fundamental expressiveness paradoxes. \cite{liu2024} identified the "Z-paradox," a fundamental deficiency causing many popular KGE models to incorrectly infer relationships based on specific graph patterns, leading to false positives and significant accuracy drops. They proposed MQuinE (Matrix Quin tuple Embedding), a novel KGE model theoretically proven to circumvent this paradox while maintaining strong expressiveness. This work directly enhances the validity and reliability of KGE predictions by addressing an inherent flaw. Furthermore, \cite{gebhart2021gtp} introduced a sheaf-theoretic framework for KGE, offering a generalized mathematical language for reasoning about KGE models and expressing prior constraints, which can lead to more consistent and trustworthy embeddings by providing a deeper theoretical understanding of their underlying structure.

Another critical component of trustworthiness is \textbf{uncertainty quantification}. Providing confidence scores or probability distributions for predictions allows users to gauge the reliability of an inference. \cite{chen2021i5t} (PASSLEAF) directly tackled this by proposing a framework for embedding uncertain knowledge graphs, where each relation is associated with a confidence score. PASSLEAF incorporates different scoring functions to predict relation confidence scores and leverages a semi-supervised learning model to augment learning. This approach explicitly models and predicts the confidence associated with relations, thereby enhancing trustworthiness by enabling users to understand the degree of certainty in a prediction. Probabilistic KGE models, such as those employing Gaussian embeddings or Bayesian approaches, inherently provide uncertainty estimates, which are invaluable for decision-making in sensitive applications. However, integrating robust uncertainty quantification into all KGE architectures and effectively communicating these uncertainties to end-users remains an active research area.

Finally, trustworthiness is also intrinsically linked to a model's \textbf{robustness against real-world data imperfections} (discussed in detail in Section 5.3) and its \textbf{performance in privacy-preserving distributed settings} (explored in Section 7.5). For instance, methods that handle data imbalance \cite{zhang2023} or mitigate the impact of noisy triples \cite{zhang2024} directly contribute to the reliability and trustworthiness of KGE predictions. Similarly, frameworks like PFedEG \cite{zhang2024}, which enable personalized federated KGE while addressing semantic disparities among clients, are crucial for fostering trust in KGE systems deployed in privacy-sensitive, collaborative environments.

In summary, the evolution of KGE research reflects a growing commitment to moving beyond mere predictive accuracy towards systems that are transparent, interpretable, and trustworthy. While significant progress has been made in establishing robust evaluation practices, mitigating fundamental expressiveness limitations, providing explicit explanations, and building theoretically sound models, future work must focus on developing standardized metrics for evaluating the faithfulness and utility of explanations for complex multi-hop relational paths. Further research is needed to explore user-centric explainable AI, investigating the cognitive load and decision-making impact of different explanation types (e.g., path-based vs. counterfactual) on domain experts. Moreover, integrating uncertainty quantification more pervasively across diverse KGE architectures and applications, and exploring intrinsically interpretable KGE models, are crucial to truly unlock their potential in high-stakes real-world scenarios.
\subsection{Distributed, Secure, and Lifelong Learning for KGE}
\label{sec:7\_5\_distributed,\_secure,\_\_and\_\_lifelong\_learning\_for\_kge}

The dynamic, sensitive, and decentralized nature of real-world knowledge graphs (KGs) necessitates advanced learning paradigms that enable models to continuously adapt, operate across distributed data sources, and maintain robust security and privacy. This section explores recent advancements in continual learning and federated learning for Knowledge Graph Embedding (KGE), addressing the challenges of evolving KGs and sensitive data.

A fundamental challenge for KGE in dynamic environments is the ability to adapt to new entities and relations without forgetting previously learned information, a concept central to lifelong or continual learning. Early efforts to enable KGE models to handle emerging entities focused on inductive approaches. \cite{wang2018} introduced a Logic Attention Network (LAN) for inductive KGE, which aggregates neighborhood information to embed unseen entities by satisfying properties like permutation invariance and query relation awareness. Building upon this, \cite{chen2021} proposed MorsE, a meta-learning framework that learns "meta-knowledge" to produce general entity embeddings for entirely new entities in new KGs, moving beyond mere inductive relation prediction. This meta-knowledge, captured through an entity initializer and a GNN modulator, allows for the transfer of structural patterns across different KGs. Further advancing this direction, \cite{sun2024} developed MetaHG, a meta-learning strategy specifically for dynamic KGE updates in evolving service ecosystems. MetaHG addresses the inefficiency of updating incremental knowledge by integrating both local and global structural information from KG snapshots, mitigating issues like spatial deformation and enhancing the representation of emerging entities.

Beyond evolving KGs, the distributed nature of data and increasing privacy concerns have spurred the development of federated learning approaches for KGE. Federated Knowledge Graph Embedding (FKGE) allows multiple clients to collaboratively train models without sharing their raw data. Addressing the inherent semantic disparities among clients in such settings, \cite{zhang2024} introduced PFedEG, a personalized federated KGE framework. PFedEG generates personalized supplementary knowledge for each client by aggregating entity embeddings from "neighboring" clients based on a learned client-wise relation graph, thus preventing shared knowledge from being inundated with noise. A critical practical challenge in FKGE is communication efficiency, especially given the large parameter sizes of KGE models and numerous communication rounds. To this end, \cite{zhang2024} proposed FedS, a bidirectional communication-efficient framework that employs an entity-wise Top-K sparsification strategy. This method dynamically identifies and transmits only the most significantly changed entity embeddings, drastically reducing communication overhead while maintaining embedding precision and overall performance.

The deployment of KGE models in distributed and resource-constrained environments further highlights the need for efficient model designs. Lightweight KGE models and knowledge distillation techniques are crucial for enabling federated learning on edge devices or mobile platforms. For instance, \cite{zhu2020} introduced DualDE, a knowledge distillation framework that constructs low-dimensional student KGEs from high-dimensional teacher models. DualDE adaptively weights soft labels and employs a two-stage distillation to achieve faster and cheaper reasoning with minimal accuracy loss, making KGE more deployable in resource-limited settings. Similarly, \cite{wang2021} presented LightKG, a lightweight KGE framework that significantly reduces storage and inference time by using codebooks and codewords instead of continuous vectors. Such efficient designs are vital for the practical implementation of FKGE, where individual clients may have limited computational resources.

However, distributed learning environments also introduce new security vulnerabilities. \cite{zhou2024} systematically investigated poisoning attacks on Federated Knowledge Graph Embedding, demonstrating how malicious clients can inject corrupted data to degrade the global model's performance. Their work highlights the critical need for robust and adaptive learning systems that can detect and mitigate such adversarial behaviors in distributed KGE environments.

In conclusion, the trajectory of KGE research is increasingly focused on developing models that are not only expressive but also adaptable to dynamic knowledge, privacy-preserving in distributed settings, and resilient against adversarial attacks. While significant progress has been made in continual learning for evolving KGs and the foundational aspects of federated KGE, challenges remain in ensuring robust security, seamless personalization across highly heterogeneous clients, and further optimizing communication efficiency for truly scalable and trustworthy distributed KGE systems.


\label{sec:conclusion}

\section{Conclusion}
\label{sec:conclusion}

\subsection{Summary of Key Developments}
\label{sec:8\_1\_summary\_of\_key\_developments}

The field of Knowledge Graph Embedding (KGE) has undergone a profound and dynamic transformation, evolving from rudimentary geometric models to sophisticated neural architectures, continuously pushing the boundaries of expressiveness, scalability, and applicability. This progression has been fundamentally driven by the imperative to effectively represent the intricate, multi-relational structure of knowledge graphs (KGs) in low-dimensional vector spaces, thereby enabling a wide array of downstream tasks such as link prediction, entity alignment, and semantic search. The journey of KGE reflects a continuous effort to bridge symbolic and neural AI paradigms, transforming KGE into a versatile tool for understanding and leveraging structured knowledge in diverse domains.

Early developments in KGE were predominantly characterized by \textit{foundational geometric and semantic matching models} that conceptualized relations as operations in a continuous vector space. The seminal work on TransE \cite{Bordes\_2013} introduced the intuitive idea of relations as vector translations ($h + r \approx t$). While efficient, TransE struggled with complex relation patterns. Subsequent translational models, such as TransH \cite{Wang\_2014}, TransR, CTransR \cite{Lin\_2015}, and TransD \cite{Ji\_2015}, progressively enhanced expressiveness by introducing relation-specific projections or dynamic mapping matrices to better handle one-to-many, many-to-one, and many-to-many relations. A significant generalization within this paradigm was RotatE \cite{Sun\_2019}, which elegantly modeled relations as rotations in a complex vector space, capturing symmetry, anti-symmetry, and inversion patterns. Concurrently, semantic matching models like RESCAL \cite{Nickel\_2016} framed the task as tensor factorization, representing relations as matrices, while ComplEx \cite{Trouillon\_2016} utilized complex-valued embeddings and a Hermitian dot product to efficiently capture symmetric and antisymmetric relations. Despite their increasing sophistication, these early models often relied on predefined geometric operations and local triple information, limiting their capacity to capture highly complex or implicit semantic patterns and facing scalability challenges on very large KGs.

A major paradigm shift occurred with the advent of \textit{context-aware and advanced neural architectures}, which moved beyond independent triple processing to leverage broader structural context. Graph Neural Networks (GNNs), exemplified by models like R-GCN \cite{Schlichtkrull\_2018} and heterogeneous GNNs with attention mechanisms \cite{li2021qr0}, became pivotal. These models aggregate information from an entity's multi-hop neighborhood, learning rich, non-linear features directly from the graph structure. For instance, \cite{li2021qr0} specifically addressed KG heterogeneity by learning the importance of different relation-paths through attention, capturing diverse semantic information. The application of convolutional neural networks, as seen in ConvE \cite{Dettmers\_2018}, further boosted representational power by learning complex interaction patterns between entity and relation embeddings. Beyond pure structural context, models also integrated auxiliary information, leading to \textit{multi-view and type-aware embeddings}. These approaches leverage textual descriptions, entity attributes, or explicit semantic type hierarchies to enrich representations and constrain the embedding space, enhancing logical consistency and mitigating data sparsity.

The field further evolved to address the inherent dynamism of real-world knowledge, leading to the development of \textit{dynamic and temporal KGE models}. Recognizing that facts and relationships change over time, researchers moved from static representations to those capable of capturing temporal evolution and uncertainty. Early efforts, such as HyTE \cite{wang20198d2}, associated each timestamp with a hyperplane for temporally-guided inference. More advanced models conceptualized entity and relation evolution as multi-dimensional additive time series, often representing them as Gaussian distributions to explicitly capture temporal uncertainty. Rotation-based models, like TeRo, extended the success of RotatE to the temporal domain by modeling evolution as element-wise rotations in complex space, effectively handling diverse relation patterns and time intervals. Recent innovations, such as TLT-KGE \cite{zhang2022muu}, further distinguish semantic and temporal information within complex or quaternion spaces, while TempCaps \cite{fu2022df2} employs capsule networks to dynamically route temporal relation and neighbor information. These advancements underscore the field's increasing capacity to represent the fluidity and complexity of real-world information, moving beyond static snapshots to continuous, evolving knowledge.

Beyond core model development, significant efforts have focused on \textit{practical considerations, efficiency, and robustness}. Optimizing the training process has been crucial, with advancements in negative sampling strategies (e.g., Bernoulli, confidence-aware, dynamic sampling) and the exploration of various loss functions and hyperparameters \cite{mohamed2021dwg}. These techniques are vital for improving model stability, efficiency, and accuracy, especially given the sparsity of KGs. Scalability to massive knowledge graphs has been addressed through graph partitioning, distributed training frameworks, and parallelization techniques. Furthermore, KGE models have been made more robust to noisy data, erroneous triples, and data imbalance through methods like confidence-aware sampling and adaptive weighting, ensuring reliable performance in imperfect real-world settings.

The broad utility of KGE is evident in its diverse \textit{downstream applications}. While link prediction and knowledge graph completion remain primary tasks, KGE has proven transformative in areas like entity alignment, where it facilitates the integration of heterogeneous knowledge bases through semi-supervised learning and the incorporation of ontological constraints. KGE also powers intelligent systems for question answering and recommendation, enabling semantic search and personalized suggestions by modeling complex path semantics. Its versatility extends to domain-specific applications, such as knowledge discovery in biomedicine, innovation analysis in patent metadata \cite{tran2019j42}, and specialized scientific question answering. The integration of KGE with cutting-edge technologies like Large Language Models (LLMs) represents a significant recent development. For instance, knowledge-enhanced joint models incorporate aviation assembly KG embeddings into LLMs for prefix-tuning, grounding LLMs with structured factual knowledge for domain-specific tasks like fault diagnosis \cite{liu2024q3q}. This highlights KGE's role in enhancing the reasoning capabilities of LLMs and unlocking new potentials for knowledge-based AI. The dual utility of KGE for both link prediction and broader data mining tasks further underscores its versatility \cite{portisch20221rd}.

Throughout these developments, comprehensive surveys \cite{yan2022, zhu2024} and empirical reviews \cite{fanourakis2022} have played a crucial role in systematizing knowledge, classifying models, evaluating performance, and identifying future research directions. Early applications, such as explicit semantic ranking for academic search \cite{xiong2017zqu}, demonstrated the immediate utility of KGE. The continuous stream of meta-analyses, including comparative studies like \cite{Kadlec\_2017}, underscores the field's dynamic nature, constantly re-evaluating progress and identifying new frontiers. While significant progress has been made in expressiveness and applicability, ongoing challenges include scalability to extremely large and dynamic KGs, handling multimodal information, ensuring explainability, and further reducing reliance on labeled data, paving the way for future innovations in robust, context-aware, and intelligent knowledge representation.
\subsection{Open Challenges and Future Research Avenues}
\label{sec:8\_2\_open\_challenges\_\_and\_\_future\_research\_avenues}

The remarkable progress in knowledge graph embedding (KGE) has fundamentally reshaped how symbolic knowledge is leveraged in AI systems, yet the field stands at an exciting precipice, defined by a suite of interconnected theoretical and practical challenges. While Section 7 provided a detailed exposition of these emerging directions and inherent limitations, this subsection offers a high-level synthesis, framing them as overarching research thrusts that will define the next generation of KGE models and their applications. The future of KGE research is characterized by a relentless pursuit of models that are not only more powerful and efficient but also inherently trustworthy, adaptable, and capable of seamless integration across diverse data landscapes.

A primary thrust for future research lies in developing \textbf{robust, generalizable, and theoretically grounded KGE models} that can reliably operate in complex, real-world environments. Current models, despite their advancements, often grapple with inherent data imperfections such as sparsity, long-tail distributions, and noisy or erroneous triples \cite{shan2018}. Future work must move beyond merely optimizing scoring functions to a deeper theoretical understanding of training dynamics, encompassing advanced loss functions, hyperparameter tuning, and sophisticated negative sampling strategies that ensure fair and effective learning, especially for low-degree entities \cite{mohamed2021dwg}. The imperative to provably incorporate rich background knowledge, including taxonomic hierarchies, logical rules, and confidence scores, remains a complex task, necessitating generalized theoretical frameworks that can provably incorporate such prior constraints \cite{kun202384f}. Furthermore, as knowledge graphs become increasingly dynamic, addressing catastrophic forgetting in continual learning paradigms is paramount for enabling efficient and effective lifelong knowledge acquisition without compromising previously learned information. The development of ensemble or committee-based KGE models, which aggregate diverse perspectives from multiple KGE techniques, also presents a promising avenue for enhancing robustness and achieving more comprehensive knowledge base completion \cite{choi2020}.

Another critical direction emphasizes the need for \textbf{explainable, ethical, and secure KGE systems} to foster trust and enable responsible deployment in sensitive applications. The black-box nature of many advanced embedding approaches, particularly those leveraging deep neural networks, hinders their adoption where transparent decision-making is crucial \cite{yan2022}. Future research must focus on methodologies that provide deeper, more causal insights into model predictions, moving beyond superficial explanations to identify salient reasoning paths or generate human-interpretable justifications \cite{kurokawa2021f4f}. The integration of human-in-the-loop systems will be vital, allowing domain experts to interact with and refine explanations, thereby enhancing confidence and broader adoption. Concurrently, KGE models are susceptible to learning and amplifying societal biases present in training data, necessitating robust methods for detection, measurement, and mitigation of such biases. Moreover, the security of KGE models against adversarial attacks, particularly data poisoning, is a growing concern, especially in federated learning settings where collaborative training across distributed knowledge bases introduces new attack vectors and privacy risks. Developing robust defense mechanisms, privacy-preserving techniques (e.g., differential privacy, secure multi-party computation), and secure system designs are essential for reliable KGE deployment in decentralized, privacy-sensitive environments.

Looking ahead, the field is poised for transformative advancements in \textbf{integrated and intelligent KGE for next-generation AI systems}. A major thrust involves advancing \textbf{multi-modal, cross-domain, and heterogeneous KGE}, where models seamlessly integrate and reason with diverse data modalities—text, images, video, and time series—beyond traditional structured triples \cite{cao2022}. This requires sophisticated fusion techniques that can learn joint semantic spaces while effectively managing noise and heterogeneity, potentially through context-aware frameworks that explicitly model graph context without introducing redundant parameters \cite{ning20219et}. Enabling robust knowledge transfer and alignment across disparate domains is also crucial for building more universal and adaptable AI, addressing challenges like schema heterogeneity and cold-start problems. Furthermore, bridging the gap between neural embeddings and symbolic reasoning, leading to \textbf{neuro-symbolic KGE and advanced reasoning capabilities}, represents a significant frontier. The goal is to integrate neural embeddings with symbolic rule learning and logical inference engines to achieve more precise, explainable, and logically consistent reasoning. A particularly impactful direction involves exploring how KGEs can enhance the factual grounding of large language models (LLMs), mitigate hallucinations, and enable robust, explainable reasoning, moving beyond mere retrieval to true knowledge-driven intelligence.

Finally, the continuous \textbf{exploration of novel applications in emerging domains} will continue to expand the profound utility of KGE. Beyond established applications in question answering and recommendation systems, KGEs are finding increasing traction in specialized fields such as biomedicine, materials science, climate modeling, and personalized education. This necessitates tailoring KGE models for complex, domain-specific challenges, often requiring the integration of multi-modal data, the modeling of dynamic relationships, and adherence to stringent requirements for interpretability and trustworthiness.

In conclusion, the trajectory of knowledge graph embedding research is characterized by a continuous drive to overcome inherent limitations and unlock new potentials. The pursuit of models that are not only robust, scalable, and efficient but also interpretable, ethically sound, and capable of advanced reasoning across diverse, dynamic, and distributed knowledge sources will continue to fuel innovation, firmly positioning KGE as a cornerstone for the next generation of intelligent AI systems.


\newpage
\section*{References}
\addcontentsline{toc}{section}{References}

\begin{thebibliography}{377}

\bibitem{sun2018}
Zequn Sun, Wei Hu, Qingheng Zhang, et al. (2018). \textit{Bootstrapping Entity Alignment with Knowledge Graph Embedding}. International Joint Conference on Artificial Intelligence.

\bibitem{dasgupta2018}
S. Dasgupta, Swayambhu Nath Ray, and P. Talukdar (2018). \textit{HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{chen2023}
Mingyang Chen, Wen Zhang, Zhen Yao, et al. (2023). \textit{Entity-Agnostic Representation Learning for Parameter-Efficient Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{yang2023}
Yang Yang, Chubing Zhang, Xin Song, et al. (2023). \textit{Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation}. ACM Trans. Inf. Syst..

\bibitem{jia2015}
Yantao Jia, Yuanzhuo Wang, Hailun Lin, et al. (2015). \textit{Locally Adaptive Translation for Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{lloyd2022}
Oliver Lloyd, Yi Liu, and T. Gaunt (2022). \textit{Assessing the effects of hyperparameters on knowledge graph embedding quality}. Journal of Big Data.

\bibitem{wu2021}
Junkang Wu, Wentao Shi, Xuezhi Cao, et al. (2021). \textit{DisenKGAT: Knowledge Graph Embedding with Disentangled Graph Attention Network}. International Conference on Information and Knowledge Management.

\bibitem{xu2019}
Chengjin Xu, M. Nayyeri, Fouad Alkhoury, et al. (2019). \textit{Temporal Knowledge Graph Embedding Model based on Additive Time Series Decomposition}. arXiv.org.

\bibitem{shan2018}
Yingchun Shan, Chenyang Bu, Xiaojian Liu, et al. (2018). \textit{Confidence-Aware Negative Sampling Method for Noisy Knowledge Graph Embedding}. International Conference on Big Knowledge.

\bibitem{zheng2024}
Zhuoxun Zheng, Baifan Zhou, Hui Yang, et al. (2024). \textit{Knowledge graph embedding closed under composition}. Data mining and knowledge discovery.

\bibitem{he2023}
Peng He, Gang Zhou, Yao Yao, et al. (2023). \textit{A type-augmented knowledge graph embedding framework for knowledge graph completion}. Scientific Reports.

\bibitem{xiao2015}
Han Xiao, Minlie Huang, and Xiaoyan Zhu (2015). \textit{TransG : A Generative Model for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{guo2017}
Shu Guo, Quan Wang, Lihong Wang, et al. (2017). \textit{Knowledge Graph Embedding with Iterative Guidance from Soft Rules}. AAAI Conference on Artificial Intelligence.

\bibitem{chen2021}
Mingyang Chen, Wen Zhang, Yushan Zhu, et al. (2021). \textit{Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{li2023}
Guang-pu Li, Zequn Sun, Wei Hu, et al. (2023). \textit{Position-Aware Relational Transformer for Knowledge Graph Embedding}. IEEE Transactions on Neural Networks and Learning Systems.

\bibitem{zhou2023}
Xiaochi Zhou, Shaocong Zhang, Mehal Agarwal, et al. (2023). \textit{Marie and BERT—A Knowledge Graph Embedding Based Question Answering System for Chemistry}. ACS Omega.

\bibitem{xiang2021}
Yuejia Xiang, Ziheng Zhang, Jiaoyan Chen, et al. (2021). \textit{OntoEA: Ontology-guided Entity Alignment via Joint Knowledge Graph Embedding}. Findings.

\bibitem{cao2022}
Jiahang Cao, Jinyuan Fang, Zaiqiao Meng, et al. (2022). \textit{Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces}. ACM Computing Surveys.

\bibitem{wang2021}
Peng Wang, Jing Zhou, Yuzhang Liu, et al. (2021). \textit{TransET: Knowledge Graph Embedding with Entity Types}. Electronics.

\bibitem{guo2020}
Shu Guo, Lin Li, Zhen Hui, et al. (2020). \textit{Knowledge Graph Embedding Preserving Soft Logical Regularity}. International Conference on Information and Knowledge Management.

\bibitem{zhang2024}
Xiaoxiong Zhang, Zhiwei Zeng, Xin Zhou, et al. (2024). \textit{Communication-Efficient Federated Knowledge Graph Embedding with Entity-Wise Top-K Sparsification}. Knowledge-Based Systems.

\bibitem{shen2022}
Jianhao Shen, Chenguang Wang, Linyuan Gong, et al. (2022). \textit{Joint Language Semantic and Structure Embedding for Knowledge Graph Completion}. International Conference on Computational Linguistics.

\bibitem{hu2024}
Kairong Hu, Xiaozhi Zhu, Hai Liu, et al. (2024). \textit{Convolutional Neural Network-Based Entity-Specific Common Feature Aggregation for Knowledge Graph Embedding Learning}. IEEE transactions on consumer electronics.

\bibitem{liu2024}
Yang Liu, Huang Fang, Yunfeng Cai, et al. (2024). \textit{MQuinE: a Cure for “Z-paradox” in Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhang2019}
Yongqi Zhang, Quanming Yao, Wenyuan Dai, et al. (2019). \textit{AutoSF: Searching Scoring Functions for Knowledge Graph Embedding}. IEEE International Conference on Data Engineering.

\bibitem{yang2019}
Shihui Yang, Jidong Tian, Honglun Zhang, et al. (2019). \textit{TransMS: Knowledge Graph Embedding for Complex Relations by Multidirectional Semantics}. International Joint Conference on Artificial Intelligence.

\bibitem{xie2023}
Zhiwen Xie, Runjie Zhu, Jin Liu, et al. (2023). \textit{TARGAT: A Time-Aware Relational Graph Attention Model for Temporal Knowledge Graph Embedding}. IEEE/ACM Transactions on Audio Speech and Language Processing.

\bibitem{wang2024}
Jiapu Wang, Boyue Wang, Junbin Gao, et al. (2024). \textit{MADE: Multicurvature Adaptive Embedding for Temporal Knowledge Graph Completion}. IEEE Transactions on Cybernetics.

\bibitem{xiao2019}
Han Xiao, Yidong Chen, and X. Shi (2019). \textit{Knowledge Graph Embedding Based on Multi-View Clustering Framework}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{sachan2020}
Mrinmaya Sachan (2020). \textit{Knowledge Graph Embedding Compression}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{madushanka2024}
Tiroshan Madushanka, and R. Ichise (2024). \textit{Negative Sampling in Knowledge Graph Representation Learning: A Review}. arXiv.org.

\bibitem{zhu2022}
Chaoyu Zhu, Zhihao Yang, Xiaoqiong Xia, et al. (2022). \textit{Multimodal reasoning based on knowledge graph embedding for specific diseases}. Bioinform..

\bibitem{liang2024}
Qiuyu Liang, Weihua Wang, F. Bao, et al. (2024). \textit{Fully Hyperbolic Rotation for Knowledge Graph Embedding}. European Conference on Artificial Intelligence.

\bibitem{li2024}
Li, Yuyi Ao, and Jingrui He (2024). \textit{SpherE: Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{ebisu2017}
Takuma Ebisu, and R. Ichise (2017). \textit{TorusE: Knowledge Graph Embedding on a Lie Group}. AAAI Conference on Artificial Intelligence.

\bibitem{zhang2021}
Zhao Zhang, Fuzhen Zhuang, Hengshu Zhu, et al. (2021). \textit{Towards Robust Knowledge Graph Embedding via Multi-Task Reinforcement Learning}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{huang2019}
Xiao Huang, Jingyuan Zhang, Dingcheng Li, et al. (2019). \textit{Knowledge Graph Embedding Based Question Answering}. Web Search and Data Mining.

\bibitem{tang2019}
Yun Tang, Jing Huang, Guangtao Wang, et al. (2019). \textit{Orthogonal Relation Transforms with Graph Context Modeling for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{sun2018}
Zhu Sun, Jie Yang, Jie Zhang, et al. (2018). \textit{Recurrent knowledge graph embedding for effective recommendation}. ACM Conference on Recommender Systems.

\bibitem{ge2023}
Xiou Ge, Yun Cheng Wang, Bin Wang, et al. (2023). \textit{Knowledge Graph Embedding: An Overview}. APSIPA Transactions on Signal and Information Processing.

\bibitem{wang2020}
Rui Wang, Bicheng Li, Shengwei Hu, et al. (2020). \textit{Knowledge Graph Embedding via Graph Attenuated Attention Networks}. IEEE Access.

\bibitem{li2022}
Rui Li, Jianan Zhao, Chaozhuo Li, et al. (2022). \textit{HousE: Knowledge Graph Embedding with Householder Parameterization}. International Conference on Machine Learning.

\bibitem{zhang2019}
Qingheng Zhang, Zequn Sun, Wei Hu, et al. (2019). \textit{Multi-view Knowledge Graph Embedding for Entity Alignment}. International Joint Conference on Artificial Intelligence.

\bibitem{tang2022}
Xiaojuan Tang, Song-Chun Zhu, Yitao Liang, et al. (2022). \textit{RulE: Knowledge Graph Reasoning with Rule Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{lv2018}
Xin Lv, Lei Hou, Juan-Zi Li, et al. (2018). \textit{Differentiating Concepts and Instances for Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{chen2025}
Jie Chen, Yinlong Wang, Shu Zhao, et al. (2025). \textit{Contextualized Quaternion Embedding Towards Polysemy in Knowledge Graph for Link Prediction}. ACM Trans. Asian Low Resour. Lang. Inf. Process..

\bibitem{qian2021}
Jing Qian, Gangmin Li, Katie Atkinson, et al. (2021). \textit{Understanding Negative Sampling in Knowledge Graph Embedding}. International Journal of Artificial Intelligence & Applications.

\bibitem{dai2020}
Yuanfei Dai, Shiping Wang, N. Xiong, et al. (2020). \textit{A Survey on Knowledge Graph Embedding: Approaches, Applications and Benchmarks}. Electronics.

\bibitem{ji2024}
Hao Ji, Li Yan, and Z. Ma (2024). \textit{FSTRE: Fuzzy Spatiotemporal RDF Knowledge Graph Embedding Using Uncertain Dynamic Vector Projection and Rotation}. IEEE transactions on fuzzy systems.

\bibitem{yan2022}
Qi Yan, Jiaxin Fan, Mohan Li, et al. (2022). \textit{A Survey on Knowledge Graph Embedding}. International Conference on Data Science in Cyberspace.

\bibitem{zhang2023}
Yichi Zhang, Mingyang Chen, and Wen Zhang (2023). \textit{Modality-Aware Negative Sampling for Multi-modal Knowledge Graph Embedding}. IEEE International Joint Conference on Neural Network.

\bibitem{li2021}
Ren Li, Yanan Cao, Qiannan Zhu, et al. (2021). \textit{How Does Knowledge Graph Embedding Extrapolate to Unseen Data: a Semantic Evidence View}. AAAI Conference on Artificial Intelligence.

\bibitem{yang2025}
Qingqing Yang, Min He, Zhongwen Li, et al. (2025). \textit{A Semantic Enhanced Knowledge Graph Embedding Model With AIGC Designed for Healthcare Prediction}. IEEE transactions on consumer electronics.

\bibitem{wang2019}
Quan Wang, Pingping Huang, Haifeng Wang, et al. (2019). \textit{CoKE: Contextualized Knowledge Graph Embedding}. arXiv.org.

\bibitem{di2023}
Shimin Di, and Lei Chen (2023). \textit{Message Function Search for Knowledge Graph Embedding}. The Web Conference.

\bibitem{jia2017}
Yantao Jia, Yuanzhuo Wang, Xiaolong Jin, et al. (2017). \textit{Knowledge Graph Embedding}. ACM Transactions on the Web.

\bibitem{choudhary2021}
Shivani Choudhary, Tarun Luthra, Ashima Mittal, et al. (2021). \textit{A Survey of Knowledge Graph Embedding and Their Applications}. arXiv.org.

\bibitem{xiao2015}
Han Xiao, Minlie Huang, and Xiaoyan Zhu (2015). \textit{From One Point to a Manifold: Knowledge Graph Embedding for Precise Link Prediction}. International Joint Conference on Artificial Intelligence.

\bibitem{hu2024}
Lei Hu, Wenwen Li, Jun Xu, et al. (2024). \textit{GeoEntity-type constrained knowledge graph embedding for predicting natural-language spatial relations}. International Journal of Geographical Information Science.

\bibitem{wang2014}
Zhen Wang, Jianwen Zhang, Jianlin Feng, et al. (2014). \textit{Knowledge Graph Embedding by Translating on Hyperplanes}. AAAI Conference on Artificial Intelligence.

\bibitem{zhu2020}
Yushan Zhu, Wen Zhang, Mingyang Chen, et al. (2020). \textit{DualDE: Dually Distilling Knowledge Graph Embedding for Faster and Cheaper Reasoning}. Web Search and Data Mining.

\bibitem{ali2020}
Mehdi Ali, M. Berrendorf, Charles Tapley Hoyt, et al. (2020). \textit{Bringing Light Into the Dark: A Large-Scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework}. IEEE Transactions on Pattern Analysis and Machine Intelligence.

\bibitem{mohamed2020}
Sameh K. Mohamed, A. Nounu, and V. Nováček (2020). \textit{Biological applications of knowledge graph embedding models}. Briefings Bioinform..

\bibitem{gao2020}
Chang Gao, Chengjie Sun, Lili Shan, et al. (2020). \textit{Rotate3D: Representing Relations as Rotations in Three-Dimensional Space for Knowledge Graph Embedding}. International Conference on Information and Knowledge Management.

\bibitem{peng2021}
Xutan Peng, Guanyi Chen, Chenghua Lin, et al. (2021). \textit{Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis}. North American Chapter of the Association for Computational Linguistics.

\bibitem{shi2025}
Fobo Shi, Duantengchuan Li, Xiaoguang Wang, et al. (2025). \textit{TGformer: A Graph Transformer Framework for Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{zhang2024}
Xiaoxiong Zhang, Zhiwei Zeng, Xin Zhou, et al. (2024). \textit{Personalized Federated Knowledge Graph Embedding with Client-Wise Relation Graph}. Applied intelligence (Boston).

\bibitem{rosso2020}
Paolo Rosso, Dingqi Yang, and P. Cudré-Mauroux (2020). \textit{Beyond Triplets: Hyper-Relational Knowledge Graph Embedding for Link Prediction}. The Web Conference.

\bibitem{zhou2024}
Enyuan Zhou, Song Guo, Zhixiu Ma, et al. (2024). \textit{Poisoning Attack on Federated Knowledge Graph Embedding}. The Web Conference.

\bibitem{xie2020}
Zhiwen Xie, Guangyou Zhou, Jin Liu, et al. (2020). \textit{ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{song2021}
Tengwei Song, Jie Luo, and Lei Huang (2021). \textit{Rot-Pro: Modeling Transitivity by Projection in Knowledge Graph Embedding}. Neural Information Processing Systems.

\bibitem{zhang2020}
Zhaoli Zhang, Zhifei Li, Hai Liu, et al. (2020). \textit{Multi-Scale Dynamic Convolutional Network for Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{ge2022}
Xiou Ge, Yun Cheng Wang, Bin Wang, et al. (2022). \textit{CompoundE: Knowledge Graph Embedding with Translation, Rotation and Scaling Compound Operations}. arXiv.org.

\bibitem{ren2020}
Feiliang Ren, Jucheng Li, Huihui Zhang, et al. (2020). \textit{Knowledge Graph Embedding with Atrous Convolution and Residual Learning}. International Conference on Computational Linguistics.

\bibitem{yuan2019}
Jun Yuan, Neng Gao, and Ji Xiang (2019). \textit{TransGate: Knowledge Graph Embedding with Shared Gate Structure}. AAAI Conference on Artificial Intelligence.

\bibitem{xiao2015}
Han Xiao, Minlie Huang, Yu Hao, et al. (2015). \textit{TransA: An Adaptive Approach for Knowledge Graph Embedding}. arXiv.org.

\bibitem{sun2018}
Zhiqing Sun, Zhihong Deng, Jian-Yun Nie, et al. (2018). \textit{RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space}. International Conference on Learning Representations.

\bibitem{ji2015}
Guoliang Ji, Shizhu He, Liheng Xu, et al. (2015). \textit{Knowledge Graph Embedding via Dynamic Mapping Matrix}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{lin2020}
Lifan Lin, and Kun She (2020). \textit{Tensor Decomposition-Based Temporal Knowledge Graph Embedding}. IEEE International Conference on Tools with Artificial Intelligence.

\bibitem{islam2023}
M. Islam, Diego Amaya-Ramirez, B. Maigret, et al. (2023). \textit{Molecular-evaluated and explainable drug repurposing for COVID-19 using ensemble knowledge graph embedding}. Scientific Reports.

\bibitem{wang2021}
Haoyu Wang, Yaqing Wang, Defu Lian, et al. (2021). \textit{A Lightweight Knowledge Graph Embedding Framework for Efficient Inference and Storage}. International Conference on Information and Knowledge Management.

\bibitem{broscheit2020}
Samuel Broscheit, Daniel Ruffinelli, Adrian Kochsiek, et al. (2020). \textit{LibKGE - A knowledge graph embedding library for reproducible research}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{fanourakis2022}
N. Fanourakis, Vasilis Efthymiou, D. Kotzinos, et al. (2022). \textit{Knowledge graph embedding methods for entity alignment: experimental review}. Data mining and knowledge discovery.

\bibitem{wang2018}
Peifeng Wang, Jialong Han, Chenliang Li, et al. (2018). \textit{Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{tabacof2019}
Pedro Tabacof, and Luca Costabello (2019). \textit{Probability Calibration for Knowledge Graph Embedding Models}. International Conference on Learning Representations.

\bibitem{pei2019}
Shichao Pei, Lu Yu, R. Hoehndorf, et al. (2019). \textit{Semi-Supervised Entity Alignment via Knowledge Graph Embedding with Awareness of Degree Difference}. The Web Conference.

\bibitem{zhang2018}
Yongqi Zhang, Quanming Yao, Yingxia Shao, et al. (2018). \textit{NSCaching: Simple and Efficient Negative Sampling for Knowledge Graph Embedding}. IEEE International Conference on Data Engineering.

\bibitem{li2021}
Zelong Li, Jianchao Ji, Zuohui Fu, et al. (2021). \textit{Efficient Non-Sampling Knowledge Graph Embedding}. The Web Conference.

\bibitem{li2022}
Guangtong Li, L. Siddharth, and Jianxi Luo (2022). \textit{Embedding knowledge graph of patent metadata to measure knowledge proximity}. J. Assoc. Inf. Sci. Technol..

\bibitem{ding2018}
Boyang Ding, Quan Wang, Bin Wang, et al. (2018). \textit{Improving Knowledge Graph Embedding Using Simple Constraints}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{zhang2022}
Xuanyu Zhang, Qing Yang, and Dongliang Xu (2022). \textit{TranS: Transition-based Knowledge Graph Embedding with Synthetic Relation Representation}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{sun2024}
Hongliang Sun, Jinlan Liu, Can Wang, et al. (2024). \textit{Learning Dynamic Knowledge Graph Embedding in Evolving Service Ecosystems via Meta-Learning}. 2024 IEEE International Conference on Web Services (ICWS).

\bibitem{wang2024}
Jiapu Wang, Zheng Cui, Boyue Wang, et al. (2024). \textit{IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion}. The Web Conference.

\bibitem{modak2024}
S. Modak, Aakarsh Malhotra, Sarthak Malik, et al. (2024). \textit{CPa-WAC: Constellation Partitioning-based Scalable Weighted Aggregation Composition for Knowledge Graph Embedding}. International Joint Conference on Artificial Intelligence.

\bibitem{xiao2016}
Han Xiao, Minlie Huang, Lian Meng, et al. (2016). \textit{SSP: Semantic Space Projection for Knowledge Graph Embedding with Text Descriptions}. AAAI Conference on Artificial Intelligence.

\bibitem{zhang2023}
Zhao Zhang, Zhanpeng Guan, Fuwei Zhang, et al. (2023). \textit{Weighted Knowledge Graph Embedding}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{guo2015}
Shu Guo, Quan Wang, Bin Wang, et al. (2015). \textit{Semantically Smooth Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{xu2020}
Chengjin Xu, M. Nayyeri, Fouad Alkhoury, et al. (2020). \textit{TeRo: A Time-aware Knowledge Graph Embedding via Temporal Rotation}. International Conference on Computational Linguistics.

\bibitem{zheng2024}
Chenguang Zheng, Guanxian Jiang, Xiao Yan, et al. (2024). \textit{GE2: A General and Efficient Knowledge Graph Embedding Learning System}. Proc. ACM Manag. Data.

\bibitem{zhang2018}
Zhao Zhang, Fuzhen Zhuang, Meng Qu, et al. (2018). \textit{Knowledge Graph Embedding with Hierarchical Relation Structure}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhu2024}
Beibei Zhu, Ruolin Wang, Junyi Wang, et al. (2024). \textit{A survey: knowledge graph entity alignment research based on graph embedding}. Artificial Intelligence Review.

\bibitem{liu2023}
Jia Liu, Wei Huang, Tianrui Li, et al. (2023). \textit{Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{choi2020}
S. Choi, Hyun-Je Song, and Seong-Bae Park (2020). \textit{An Approach to Knowledge Base Completion by a Committee-Based Knowledge Graph Embedding}. Applied Sciences.

\bibitem{ge2023}
Xiou Ge, Yun Cheng Wang, Bin Wang, et al. (2023). \textit{Knowledge Graph Embedding with 3D Compound Geometric Transformations}. APSIPA Transactions on Signal and Information Processing.

\bibitem{sadeghian2021}
A. Sadeghian, Mohammadreza Armandpour, Anthony Colas, et al. (2021). \textit{ChronoR: Rotation Based Temporal Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{liu2024}
Jiajun Liu, Wenjun Ke, Peng Wang, et al. (2024). \textit{Fast and Continual Knowledge Graph Embedding via Incremental LoRA}. International Joint Conference on Artificial Intelligence.

\bibitem{li2022}
Yizhi Li, Wei Fan, Chaochun Liu, et al. (2022). \textit{TranSHER: Translating Knowledge Graph Embedding with Hyper-Ellipsoidal Restriction}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{rossi2020}
Andrea Rossi, D. Firmani, Antonio Matinata, et al. (2020). \textit{Knowledge Graph Embedding for Link Prediction}. ACM Transactions on Knowledge Discovery from Data.

\bibitem{li2023}
Jiang Li, Xiangdong Su, and Guanglai Gao (2023). \textit{TeAST: Temporal Knowledge Graph Embedding via Archimedean Spiral Timeline}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{peng2020}
Yanhui Peng, and Jing Zhang (2020). \textit{LineaRE: Simple but Powerful Knowledge Graph Embedding for Link Prediction}. Industrial Conference on Data Mining.

\bibitem{ji2024}
Hao Ji, Li Yan, and Z. Ma (2024). \textit{Multihop Fuzzy Spatiotemporal RDF Knowledge Graph Query via Quaternion Embedding}. IEEE transactions on fuzzy systems.

\bibitem{zhang2024}
Qinggang Zhang, Junnan Dong, Qiaoyu Tan, et al. (2024). \textit{Integrating Entity Attributes for Error-Aware Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{kochsiek2021}
Adrian Kochsiek (2021). \textit{Parallel Training of Knowledge Graph Embedding Models: A Comparison of Techniques}. Proceedings of the VLDB Endowment.

\bibitem{yang2021}
Han Yang, Leilei Zhang, Bingning Wang, et al. (2021). \textit{Cycle or Minkowski: Which is More Appropriate for Knowledge Graph Embedding?}. International Conference on Information and Knowledge Management.

\bibitem{shang2024}
Bin Shang, Yinliang Zhao, Jun Liu, et al. (2024). \textit{Mixed Geometry Message and Trainable Convolutional Attention Network for Knowledge Graph Completion}. AAAI Conference on Artificial Intelligence.

\bibitem{asmara2023}
S. M. Asmara, N. A. Sahabudin, Nor Syahidatul Nadiah Ismail, et al. (2023). \textit{A Review of Knowledge Graph Embedding Methods of TransE, TransH and TransR for Missing Links}. International Conference on Software Engineering and Computer Systems.

\bibitem{gregucci2023}
Cosimo Gregucci, M. Nayyeri, D. Hern'andez, et al. (2023). \textit{Link Prediction with Attention Applied on Multiple Knowledge Graph Embedding Models}. The Web Conference.

\bibitem{pan2021}
Zhe Pan, and Peng Wang (2021). \textit{Hyperbolic Hierarchy-Aware Knowledge Graph Embedding for Link Prediction}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{yoon2016}
Hee-Geun Yoon, Hyun-Je Song, Seong-Bae Park, et al. (2016). \textit{A Translation-Based Knowledge Graph Embedding Preserving Logical Property of Relations}. North American Chapter of the Association for Computational Linguistics.

\bibitem{li2024}
Rui Li, Chaozhuo Li, Yanming Shen, et al. (2024). \textit{Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization}. International Conference on Machine Learning.

\bibitem{xiong2017zqu}
Chenyan Xiong, Russell Power, and Jamie Callan (2017). \textit{Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding}. The Web Conference.

\bibitem{gong2020b2k}
Fan Gong, Meng Wang, Haofen Wang, et al. (2020). \textit{SMR: Medical Knowledge Graph Embedding for Safe Medicine Recommendation}. Big Data Research.

\bibitem{zhou2022ehi}
Bin Zhou, Xingwang Shen, Yuqian Lu, et al. (2022). \textit{Semantic-aware event link reasoning over industrial knowledge graph embedding time series data}. International Journal of Production Research.

\bibitem{le2022ji8}
Thanh-Binh Le, N. Le, and H. Le (2022). \textit{Knowledge graph embedding by relational rotation and complex convolution for link prediction}. Expert systems with applications.

\bibitem{zhou2022vgb}
Zhehui Zhou, Can Wang, Yan Feng, et al. (2022). \textit{JointE: Jointly utilizing 1D and 2D convolution for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{xu2019t6b}
Da Xu, Chuanwei Ruan, Evren Körpeoglu, et al. (2019). \textit{Product Knowledge Graph Embedding for E-commerce}. Web Search and Data Mining.

\bibitem{mezni20218ml}
Haithem Mezni, D. Benslimane, and Ladjel Bellatreche (2021). \textit{Context-Aware Service Recommendation Based on Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{do2021mw0}
P. Do, and Truong H. V. Phan (2021). \textit{Developing a BERT based triple classification model using knowledge graph embedding for question answering system}. Applied intelligence (Boston).

\bibitem{mai2020ei3}
Gengchen Mai, K. Janowicz, Ling Cai, et al. (2020). \textit{SE‐KGE: A location‐aware Knowledge Graph Embedding model for Geographic Question Answering and Spatial Semantic Lifting}. Trans. GIS.

\bibitem{zhang2022eab}
Jiarui Zhang, Jian Huang, Jialong Gao, et al. (2022). \textit{Knowledge graph embedding by logical-default attention graph convolution neural network for link prediction}. Information Sciences.

\bibitem{sosa2019ih0}
Daniel N. Sosa, Alexander Derry, Margaret Guo, et al. (2019). \textit{A Literature-Based Knowledge Graph Embedding Method for Identifying Drug Repurposing Opportunities in Rare Diseases}. bioRxiv.

\bibitem{guan2019pr4}
Niannian Guan, Dandan Song, and L. Liao (2019). \textit{Knowledge graph embedding with concepts}. Knowledge-Based Systems.

\bibitem{fan2014g7s}
M. Fan, Qiang Zhou, E. Chang, et al. (2014). \textit{Transition-based Knowledge Graph Embedding with Relational Mapping Properties}. Pacific Asia Conference on Language, Information and Computation.

\bibitem{zhang20190zu}
Hengtong Zhang, T. Zheng, Jing Gao, et al. (2019). \textit{Data Poisoning Attack against Knowledge Graph Embedding}. International Joint Conference on Artificial Intelligence.

\bibitem{chen2022mxn}
Qi Chen, Wei Wang, Kaizhu Huang, et al. (2022). \textit{Zero-Shot Text Classification via Knowledge Graph Embedding for Social Media Data}. IEEE Internet of Things Journal.

\bibitem{wang2022hwx}
Xin Wang, Shengfei Lyu, Xiangyu Wang, et al. (2022). \textit{Temporal knowledge graph embedding via sparse transfer matrix}. Information Sciences.

\bibitem{chen20226e4}
Mingyang Chen, Wen Zhang, Zonggang Yuan, et al. (2022). \textit{Federated knowledge graph completion via embedding-contrastive learning}. Knowledge-Based Systems.

\bibitem{abusalih2020gdu}
Bilal Abu-Salih, Marwan Al-Tawil, Ibrahim Aljarah, et al. (2020). \textit{Relational Learning Analysis of Social Politics using Knowledge Graph Embedding}. Data mining and knowledge discovery.

\bibitem{fang2022wp6}
Haichuan Fang, Youwei Wang, Zhen Tian, et al. (2022). \textit{Learning knowledge graph embedding with a dual-attention embedding network}. Expert systems with applications.

\bibitem{elebi2019bzc}
R. Çelebi, Hüseyin Uyar, Erkan Yasar, et al. (2019). \textit{Evaluation of knowledge graph embedding approaches for drug-drug interaction prediction in realistic settings}. BMC Bioinformatics.

\bibitem{sha2019i3a}
Xiao Sha, Zhu Sun, and Jie Zhang (2019). \textit{Hierarchical attentive knowledge graph embedding for personalized recommendation}. Electronic Commerce Research and Applications.

\bibitem{li2021ro5}
Zhifei Li, Hai Liu, Zhaoli Zhang, et al. (2021). \textit{Recalibration convolutional networks for learning interaction knowledge graph embedding}. Neurocomputing.

\bibitem{xiao20151fj}
Han Xiao, Minlie Huang, Yu Hao, et al. (2015). \textit{TransG : A Generative Mixture Model for Knowledge Graph Embedding}. arXiv.org.

\bibitem{zhang2021wg7}
Fei Zhang, Bo Sun, Xiaolin Diao, et al. (2021). \textit{Prediction of adverse drug reactions based on knowledge graph embedding}. BMC Medical Informatics and Decision Making.

\bibitem{wang20186zs}
Guanying Wang, Wen Zhang, Ruoxu Wang, et al. (2018). \textit{Label-Free Distant Supervision for Relation Extraction via Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{li2021x10}
Xinyu Li, P. Zheng, Jinsong Bao, et al. (2021). \textit{Achieving cognitive mass personalization via the self-X cognitive manufacturing network: An industrial-knowledge-graph- and graph-embedding-enabled pathway}. Engineering.

\bibitem{wang202110w}
Xin Wang, Xiao Liu, Jin Liu, et al. (2021). \textit{A novel knowledge graph embedding based API recommendation method for Mashup development}. World wide web (Bussum).

\bibitem{gutirrezbasulto2018oi0}
Víctor Gutiérrez-Basulto, and S. Schockaert (2018). \textit{From Knowledge Graph Embedding to Ontology Embedding? An Analysis of the Compatibility between Vector Space Representations and Rules}. International Conference on Principles of Knowledge Representation and Reasoning.

\bibitem{portisch20221rd}
Jan Portisch, Nicolas Heist, and Heiko Paulheim (2022). \textit{Knowledge graph embedding for data mining vs. knowledge graph embedding for link prediction - two sides of the same coin?}. Semantic Web.

\bibitem{zhang2022muu}
Fuwei Zhang, Zhao Zhang, Xiang Ao, et al. (2022). \textit{Along the Time: Timeline-traced Embedding for Temporal Knowledge Graph Completion}. International Conference on Information and Knowledge Management.

\bibitem{feng2016dp7}
Jun Feng, Minlie Huang, Mingdong Wang, et al. (2016). \textit{Knowledge Graph Embedding by Flexible Translation}. International Conference on Principles of Knowledge Representation and Reasoning.

\bibitem{liu2021wqa}
Jia Liu, Tianrui Li, Shenggong Ji, et al. (2021). \textit{Urban Flow Pattern Mining Based on Multi-Source Heterogeneous Data Fusion and Knowledge Graph Embedding}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{sang2019gjl}
Shengtian Sang, Zhihao Yang, Xiaoxia Liu, et al. (2019). \textit{GrEDeL: A Knowledge Graph Embedding Based Method for Drug Discovery From Biomedical Literatures}. IEEE Access.

\bibitem{wang2017yjq}
M. Wang, Mengyue Liu, Jun Liu, et al. (2017). \textit{Safe Medicine Recommendation via Medical Knowledge Graph Embedding}. arXiv.org.

\bibitem{jiang20219xl}
Dan Jiang, Ronggui Wang, Juan Yang, et al. (2021). \textit{Kernel multi-attention neural network for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{liu2022fu5}
Yang Liu, Zequn Sun, Guang-pu Li, et al. (2022). \textit{I Know What You Do Not Know: Knowledge Graph Embedding via Co-distillation Learning}. International Conference on Information and Knowledge Management.

\bibitem{khan202236g}
Nasrullah Khan, Zongmin Ma, Aman Ullah, et al. (2022). \textit{Similarity attributed knowledge graph embedding enhancement for item recommendation}. Information Sciences.

\bibitem{mezni2021ezn}
Haithem Mezni (2021). \textit{Temporal Knowledge Graph Embedding for Effective Service Recommendation}. IEEE Transactions on Services Computing.

\bibitem{zhang2021wix}
Qianjin Zhang, Ronggui Wang, Juan Yang, et al. (2021). \textit{Structural context-based knowledge graph embedding for link prediction}. Neurocomputing.

\bibitem{huang2021u42}
Xuqian Huang, Jiuyang Tang, Zhen Tan, et al. (2021). \textit{Knowledge graph embedding by relational and entity rotation}. Knowledge-Based Systems.

\bibitem{pavlovic2022qte}
Aleksandar Pavlovic, and Emanuel Sallinger (2022). \textit{ExpressivE: A Spatio-Functional Embedding For Knowledge Graph Completion}. International Conference on Learning Representations.

\bibitem{wang20213kg}
Shensi Wang, Kun Fu, Xian Sun, et al. (2021). \textit{Hierarchical-aware relation rotational knowledge graph embedding for link prediction}. Neurocomputing.

\bibitem{zhang2019rlm}
Shuai Zhang, Yi Tay, Lina Yao, et al. (2019). \textit{Quaternion Knowledge Graph Embedding}. arXiv.org.

\bibitem{mai20195rp}
Gengchen Mai, Bo Yan, K. Janowicz, et al. (2019). \textit{Relaxing Unanswerable Geographic Questions Using A Spatially Explicit Knowledge Graph Embedding Model}. Agile Conference.

\bibitem{han2018tzc}
Zhuobing Han, Xiaohong Li, Hongtao Liu, et al. (2018). \textit{DeepWeak: Reasoning common software weaknesses via knowledge graph embedding}. IEEE International Conference on Software Analysis, Evolution, and Reengineering.

\bibitem{wang2022fvx}
Feiyang Wang, Zhongbao Zhang, Li Sun, et al. (2022). \textit{DiriE: Knowledge Graph Embedding with Dirichlet Distribution}. The Web Conference.

\bibitem{ferrari2022r82}
Ilaria Ferrari, Giacomo Frisoni, Paolo Italiani, et al. (2022). \textit{Comprehensive Analysis of Knowledge Graph Embedding Techniques Benchmarked on Link Prediction}. Electronics.

\bibitem{fu2022df2}
Guirong Fu, Zhao Meng, Zhen Han, et al. (2022). \textit{TempCaps: A Capsule Network-based Embedding Model for Temporal Knowledge Graph Completion}. SPNLP.

\bibitem{wu2018c4b}
Yanrong Wu, and Zhichun Wang (2018). \textit{Knowledge Graph Embedding with Numeric Attributes of Entities}. Rep4NLP@ACL.

\bibitem{zhang202121t}
Qianjin Zhang, Ronggui Wang, Juan Yang, et al. (2021). \textit{Knowledge graph embedding by reflection transformation}. Knowledge-Based Systems.

\bibitem{mohamed2019meq}
Sameh K. Mohamed, V. Nováček, P. Vandenbussche, et al. (2019). \textit{Loss Functions in Knowledge Graph Embedding Models}. DL4KG@ESWC.

\bibitem{xin2022dam}
Kexuan Xin, Zequn Sun, Wen Hua, et al. (2022). \textit{Large-scale Entity Alignment via Knowledge Graph Merging, Partitioning and Embedding}. International Conference on Information and Knowledge Management.

\bibitem{nie20195gc}
Binling Nie, and Shouqian Sun (2019). \textit{Knowledge graph embedding via reasoning over entities, relations, and text}. Future generations computer systems.

\bibitem{liu2018kvd}
Yang Liu, Qingguo Zeng, Huanrui Yang, et al. (2018). \textit{Stock Price Movement Prediction from Financial News with Deep Learning and Knowledge Graph Embedding}. Pacific Rim Knowledge Acquisition Workshop.

\bibitem{ni2020ruj}
Chien-Chun Ni, Kin Sum Liu, and Nicolas Torzec (2020). \textit{Layered Graph Embedding for Entity Recommendation using Wikipedia in the Yahoo! Knowledge Graph}. The Web Conference.

\bibitem{li20215pu}
Chen Li, Xutan Peng, Yuhang Niu, et al. (2021). \textit{Learning graph attention-aware knowledge graph embedding}. Neurocomputing.

\bibitem{yu2019qgs}
S. Yu, Sujit Rokka Chhetri, A. Canedo, et al. (2019). \textit{Pykg2vec: A Python Library for Knowledge Graph Embedding}. Journal of machine learning research.

\bibitem{fatemi2018e6v}
Bahare Fatemi, Siamak Ravanbakhsh, and D. Poole (2018). \textit{Improved Knowledge Graph Embedding using Background Taxonomic Information}. AAAI Conference on Artificial Intelligence.

\bibitem{chen2021i5t}
Zhuo Chen, Mi-Yen Yeh, and Tei-Wei Kuo (2021). \textit{PASSLEAF: A Pool-bAsed Semi-Supervised LEArning Framework for Uncertain Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{dong2022c6z}
Sicong Dong, Xupeng Miao, Peng Liu, et al. (2022). \textit{HET-KG: Communication-Efficient Knowledge Graph Embedding Training via Hotness-Aware Cache}. IEEE International Conference on Data Engineering.

\bibitem{lu20206x1}
Fengyuan Lu, Peijin Cong, and Xinli Huang (2020). \textit{Utilizing Textual Information in Knowledge Graph Embedding: A Survey of Methods and Applications}. IEEE Access.

\bibitem{li2022nr8}
Weidong Li, Rong Peng, and Zhi Li (2022). \textit{Improving knowledge graph completion via increasing embedding interactions}. Applied intelligence (Boston).

\bibitem{luo2015df2}
Yuanfei Luo, Quan Wang, Bin Wang, et al. (2015). \textit{Context-Dependent Knowledge Graph Embedding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhou20216m0}
Xiaohan Zhou, Yunhui Yi, and Geng Jia (2021). \textit{Path-RotatE: Knowledge Graph Embedding by Relational Rotation of Path in Complex Space}. International Conference on Innovative Computing and Cloud Computing.

\bibitem{zhao202095o}
Feng Zhao, Haoran Sun, Langjunqing Jin, et al. (2020). \textit{Structure-augmented knowledge graph embedding for sparse data with rule learning}. Computer Communications.

\bibitem{jia201870f}
Yantao Jia, Yuanzhuo Wang, Xiaolong Jin, et al. (2018). \textit{Path-specific knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{mai2018u0h}
Gengchen Mai, K. Janowicz, and Bo Yan (2018). \textit{Combining Text Embedding and Knowledge Graph Embedding Techniques for Academic Search Engines}. Semdeep/NLIWoD@ISWC.

\bibitem{li201949n}
Dingcheng Li, Siamak Zamani, Jingyuan Zhang, et al. (2019). \textit{Integration of Knowledge Graph Embedding Into Topic Modeling with Hierarchical Dirichlet Process}. North American Chapter of the Association for Computational Linguistics.

\bibitem{tang2020ufr}
Xiaoli Tang, Rui Yuan, Qianyu Li, et al. (2020). \textit{Timespan-Aware Dynamic Knowledge Graph Embedding by Incorporating Temporal Evolution}. IEEE Access.

\bibitem{guo2022qtv}
Lingbing Guo, Qiang Zhang, Zequn Sun, et al. (2022). \textit{Understanding and Improving Knowledge Graph Embedding for Entity Alignment}. International Conference on Machine Learning.

\bibitem{jiang202235y}
Dan Jiang, Ronggui Wang, Lixia Xue, et al. (2022). \textit{Multiview feature augmented neural network for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{liu201918i}
Yu Liu, Wen Hua, Kexuan Xin, et al. (2019). \textit{Context-Aware Temporal Knowledge Graph Embedding}. WISE.

\bibitem{zhang2020s4x}
Qianjin Zhang, Ronggui Wang, Juan Yang, et al. (2020). \textit{Knowledge graph embedding by translating in time domain space for link prediction}. Knowledge-Based Systems.

\bibitem{chang20179yf}
Liang Chang, Manli Zhu, T. Gu, et al. (2017). \textit{Knowledge Graph Embedding by Dynamic Translation}. IEEE Access.

\bibitem{lee2022hr9}
Yeon-Chang Lee, and Sang-Wook Kim (2022). \textit{THOR: Self-Supervised Temporal Knowledge Graph Embedding via Three-Tower Graph Convolutional Networks}. Industrial Conference on Data Mining.

\bibitem{zhang2022fpm}
Yongqi Zhang, Zhanke Zhou, Quanming Yao, et al. (2022). \textit{Efficient Hyper-parameter Search for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{liu2019e1u}
Chang Liu, Lun Li, Xiaolu Yao, et al. (2019). \textit{A Survey of Recommendation Algorithms Based on Knowledge Graph Embedding}. 2019 IEEE International Conference on Computer Science and Educational Informatization (CSEI).

\bibitem{song2021fnl}
Wei Song, Jingjin Guo, Ruiji Fu, et al. (2021). \textit{A Knowledge Graph Embedding Approach for Metaphor Processing}. IEEE/ACM Transactions on Audio Speech and Language Processing.

\bibitem{gradgyenge2017xdy}
László Grad-Gyenge, A. Kiss, and P. Filzmoser (2017). \textit{Graph Embedding Based Recommendation Techniques on the Knowledge Graph}. User Modeling, Adaptation, and Personalization.

\bibitem{zhou20218bt}
Xiaofei Zhou, Lingfeng Niu, Qiannan Zhu, et al. (2021). \textit{Knowledge Graph Embedding by Double Limit Scoring Loss}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{chen20210ah}
Yao Chen, Jiangang Liu, Zhe Zhang, et al. (2021). \textit{MöbiusE: Knowledge Graph Embedding on Möbius Ring}. Knowledge-Based Systems.

\bibitem{zhang2020i7j}
Yongqi Zhang, Quanming Yao, and Lei Chen (2020). \textit{Interstellar: Searching Recurrent Architecture for Knowledge Graph Embedding}. Neural Information Processing Systems.

\bibitem{boschin2020ki4}
Armand Boschin (2020). \textit{TorchKGE: Knowledge Graph Embedding in Python and PyTorch}. arXiv.org.

\bibitem{wang20199fe}
P. Wang, D. Dou, Fangzhao Wu, et al. (2019). \textit{Logic Rules Powered Knowledge Graph Embedding}. arXiv.org.

\bibitem{myklebust201941l}
E. B. Myklebust, Ernesto Jiménez-Ruiz, Jiaoyan Chen, et al. (2019). \textit{Knowledge Graph Embedding for Ecotoxicological Effect Prediction}. International Workshop on the Semantic Web.

\bibitem{kartheek2021aj7}
Miriyala Kartheek, and G. Sajeev (2021). \textit{Building Semantic Based Recommender System Using Knowledge Graph Embedding}. International Conference on Intelligent Information Processing.

\bibitem{sha2019plw}
Xiao Sha, Zhu Sun, and Jie Zhang (2019). \textit{Attentive Knowledge Graph Embedding for Personalized Recommendation}. arXiv.org.

\bibitem{lu2020x6y}
Haonan Lu, and Hailin Hu (2020). \textit{DensE: An Enhanced Non-Abelian Group Representation for Knowledge Graph Embedding}. arXiv.org.

\bibitem{zhang2020c15}
Siheng Zhang, Zhengya Sun, and Wensheng Zhang (2020). \textit{Improve the translational distance models for knowledge graph embedding}. Journal of Intelligence and Information Systems.

\bibitem{li2020ek4}
Mingda Li, Zhengya Sun, Siheng Zhang, et al. (2020). \textit{Enhancing Knowledge Graph Embedding with Relational Constraints}. 2020 IEEE International Conference on Knowledge Graph (ICKG).

\bibitem{li2020he5}
Jian Li, Zhuoming Xu, Yan Tang, et al. (2020). \textit{Deep Hybrid Knowledge Graph Embedding for Top-N Recommendation}. Web Information System and Application Conference.

\bibitem{kim2020zu3}
Kuekyeng Kim, Yuna Hur, Gyeongmin Kim, et al. (2020). \textit{GREG: A Global Level Relation Extraction with Knowledge Graph Embedding}. Applied Sciences.

\bibitem{zhu2018l0u}
Jizhao Zhu, Yantao Jia, Jun Xu, et al. (2018). \textit{Modeling the Correlations of Relations for Knowledge Graph Embedding}. Journal of Computational Science and Technology.

\bibitem{do20184o2}
Kien Do, T. Tran, and S. Venkatesh (2018). \textit{Knowledge Graph Embedding with Multiple Relation Projections}. International Conference on Pattern Recognition.

\bibitem{ma20194ua}
Yunpu Ma, Volker Tresp, Liming Zhao, et al. (2019). \textit{Variational Quantum Circuit Model for Knowledge Graph Embedding}. Advanced Quantum Technologies.

\bibitem{zhang2020wou}
Yuhang Zhang, Jun Wang, and Jie Luo (2020). \textit{Knowledge Graph Embedding Based Collaborative Filtering}. IEEE Access.

\bibitem{zhang2019hs5}
Wen Zhang, Shumin Deng, Han Wang, et al. (2019). \textit{XTransE: Explainable Knowledge Graph Embedding for Link Prediction with Lifestyles in e-Commerce}. Joint International Conference of Semantic Technology.

\bibitem{wang20198d2}
Zhihao Wang, and Xin Li (2019). \textit{Hybrid-TE: Hybrid Translation-Based Temporal Knowledge Graph Embedding}. IEEE International Conference on Tools with Artificial Intelligence.

\bibitem{tran20195x3}
Hung Nghiep Tran, and A. Takasu (2019). \textit{Analyzing Knowledge Graph Embedding Methods from a Multi-Embedding Interaction Perspective}. EDBT/ICDT Workshops.

\bibitem{xiong2018fof}
Shengwu Xiong, Weitao Huang, and P. Duan (2018). \textit{Knowledge Graph Embedding via Relation Paths and Dynamic Mapping Matrix}. ER Workshops.

\bibitem{radstok2021yup}
Wessel Radstok, M. Chekol, and M. Schäfer (2021). \textit{Are Knowledge Graph Embedding Models Biased, or Is it the Data That They Are Trained on?}. Wikidata@ISWC.

\bibitem{zhao2020o6z}
Ling Zhao, Hanhan Deng, L. Qiu, et al. (2020). \textit{Urban Multi-Source Spatio-Temporal Data Analysis Aware Knowledge Graph Embedding}. Symmetry.

\bibitem{zhang20182ey}
Maoyuan Zhang, Qi Wang, Wukui Xu, et al. (2018). \textit{Discriminative Path-Based Knowledge Graph Embedding for Precise Link Prediction}. European Conference on Information Retrieval.

\bibitem{jia20207dd}
Ningning Jia, Xiang Cheng, and Sen Su (2020). \textit{Improving Knowledge Graph Embedding Using Locally and Globally Attentive Relation Paths}. European Conference on Information Retrieval.

\bibitem{zhu2019ir6}
Qiannan Zhu, Xiaofei Zhou, P. Zhang, et al. (2019). \textit{A neural translating general hyperplane for knowledge graph embedding}. Journal of Computer Science.

\bibitem{wang2021dgy}
Shen Wang, Xiaokai Wei, C. D. Santos, et al. (2021). \textit{Knowledge Graph Representation via Hierarchical Hyperbolic Neural Graph Embedding}. 2021 IEEE International Conference on Big Data (Big Data).

\bibitem{ning20219et}
Zhiyuan Ning, Ziyue Qiao, Hao Dong, et al. (2021). \textit{LightCAKE: A Lightweight Framework for Context-Aware Knowledge Graph Embedding}. Pacific-Asia Conference on Knowledge Discovery and Data Mining.

\bibitem{sheikh20213qq}
Nasrullah Sheikh, Xiao Qin, B. Reinwald, et al. (2021). \textit{Knowledge Graph Embedding using Graph Convolutional Networks with Relation-Aware Attention}. arXiv.org.

\bibitem{rim2021s9a}
Wiem Ben Rim, Carolin (Haas) Lawrence, Kiril Gashteovski, et al. (2021). \textit{Behavioral Testing of Knowledge Graph Embedding Models for Link Prediction}. Conference on Automated Knowledge Base Construction.

\bibitem{zhang20179i2}
Chunhong Zhang, Miao Zhou, Xiao Han, et al. (2017). \textit{Knowledge Graph Embedding for Hyper-Relational Data}. Unpublished manuscript.

\bibitem{elebi20182bd}
R. Çelebi, Erkan Yasar, Hüseyin Uyar, et al. (2018). \textit{Evaluation of knowledge graph embedding approaches for drug-drug interaction prediction using Linked Open Data}. Workshop on Semantic Web Applications and Tools for Life Sciences.

\bibitem{garofalo20185g9}
Martina Garofalo, Maria Angela Pellegrino, Abdulrahman Altabba, et al. (2018). \textit{Leveraging Knowledge Graph Embedding Techniques for Industry 4.0 Use Cases}. arXiv.org.

\bibitem{wang201825m}
Kai Wang, Yu Liu, Xiujuan Xu, et al. (2018). \textit{Knowledge Graph Embedding with Entity Neighbors and Deep Memory Network}. arXiv.org.

\bibitem{chung2021u2l}
Chanyoung Chung, and Joyce Jiyoung Whang (2021). \textit{Knowledge Graph Embedding via Metagraph Learning}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{tran2019j42}
Hung Nghiep Tran, and A. Takasu (2019). \textit{Exploring Scholarly Data by Semantic Query on Knowledge Graph Embedding Space}. International Conference on Theory and Practice of Digital Libraries.

\bibitem{shi2017m2h}
Jun Shi, Huan Gao, G. Qi, et al. (2017). \textit{Knowledge Graph Embedding with Triple Context}. International Conference on Information and Knowledge Management.

\bibitem{zhang2017ixt}
Wen Zhang (2017). \textit{Knowledge Graph Embedding with Diversity of Structures}. The Web Conference.

\bibitem{zhu20196p1}
Ming-Yi Zhu, De-sheng Zhen, Ran Tao, et al. (2019). \textit{Top-N Collaborative Filtering Recommendation Algorithm Based on Knowledge Graph Embedding}. International Conference on Knowledge Management in Organizations.

\bibitem{kertkeidkachorn2019dkn}
Natthawut Kertkeidkachorn, Xin Liu, and R. Ichise (2019). \textit{GTransE: Generalizing Translation-Based Model on Uncertain Knowledge Graph Embedding}. JSAI.

\bibitem{zhu2019zqy}
Jia Zhu, Zetao Zheng, Min Yang, et al. (2019). \textit{A semi-supervised model for knowledge graph embedding}. Data mining and knowledge discovery.

\bibitem{zhang20193g2}
Hengtong Zhang, T. Zheng, Jing Gao, et al. (2019). \textit{Towards Data Poisoning Attack against Knowledge Graph Embedding}. arXiv.org.

\bibitem{liu2019fcs}
Wenqiang Liu, Hongyun Cai, Xu Cheng, et al. (2019). \textit{Learning High-order Structural and Attribute information by Knowledge Graph Attention Networks for Enhancing Knowledge Graph Embedding}. Knowledge-Based Systems.

\bibitem{kanojia20171in}
Vibhor Kanojia, Hideyuki Maeda, Riku Togashi, et al. (2017). \textit{Enhancing Knowledge Graph Embedding with Probabilistic Negative Sampling}. The Web Conference.

\bibitem{gao2018di0}
Huan Gao, Jun Shi, G. Qi, et al. (2018). \textit{Triple Context-Based Knowledge Graph Embedding}. IEEE Access.

\bibitem{mai2018egi}
Gengchen Mai, K. Janowicz, and Bo Yan (2018). \textit{Support and Centrality: Learning Weights for Knowledge Graph Embedding Models}. International Conference Knowledge Engineering and Knowledge Management.

\bibitem{xiao2016bb9}
Han Xiao, Minlie Huang, and Xiaoyan Zhu (2016). \textit{Knowledge Semantic Representation: A Generative Model for Interpretable Knowledge Graph Embedding}. arXiv.org.

\bibitem{liu2024q3q}
Peifeng Liu, Lu Qian, Xingwei Zhao, et al. (2024). \textit{Joint Knowledge Graph and Large Language Model for Fault Diagnosis and Its Application in Aviation Assembly}. IEEE Transactions on Industrial Informatics.

\bibitem{zhang2024cjl}
Jin-cheng Zhang, A. Zain, Kai Zhou, et al. (2024). \textit{A review of recommender systems based on knowledge graph embedding}. Expert systems with applications.

\bibitem{su2023v6e}
Xiao-Rui Su, Zhuhong You, Deshuang Huang, et al. (2023). \textit{Biomedical Knowledge Graph Embedding With Capsule Network for Multi-Label Drug-Drug Interaction Prediction}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{zhu2023bfj}
Xiangrong Zhu, Guang-pu Li, and Wei Hu (2023). \textit{Heterogeneous Federated Knowledge Graph Embedding Learning and Unlearning}. The Web Conference.

\bibitem{liu2024to0}
Jiajun Liu, Wenjun Ke, Peng Wang, et al. (2024). \textit{Towards Continual Knowledge Graph Embedding via Incremental Distillation}. AAAI Conference on Artificial Intelligence.

\bibitem{wang2024vgj}
Wei Wang, Xiaoxuan Shen, Baolin Yi, et al. (2024). \textit{Knowledge-aware fine-grained attention networks with refined knowledge graph embedding for personalized recommendation}. Expert systems with applications.

\bibitem{li2024920}
Duantengchuan Li, Tao Xia, Jing Wang, et al. (2024). \textit{SDFormer: A shallow-to-deep feature interaction for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{lee202380l}
Jaejun Lee, Chanyoung Chung, and Joyce Jiyoung Whang (2023). \textit{InGram: Inductive Knowledge Graph Embedding via Relation Graphs}. International Conference on Machine Learning.

\bibitem{shokrzadeh2023twj}
Zeinab Shokrzadeh, M. Feizi-Derakhshi, M. Balafar, et al. (2023). \textit{Knowledge graph-based recommendation system enhanced by neural collaborative filtering and knowledge graph embedding}. Ain Shams Engineering Journal.

\bibitem{gao2023086}
Weibo Gao, Hao Wang, Qi Liu, et al. (2023). \textit{Leveraging Transferable Knowledge Concept Graph Embedding for Cold-Start Cognitive Diagnosis}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{li2024sgp}
Yufeng Li, Wenchao Zhao, Bo Dang, et al. (2024). \textit{Research on Adverse Drug Reaction Prediction Model Combining Knowledge Graph Embedding and Deep Learning}. 2024 4th International Conference on Machine Learning and Intelligent Systems Engineering (MLISE).

\bibitem{xue2023qi7}
Zengcan Xue, Zhao Zhang, Hai Liu, et al. (2023). \textit{Learning knowledge graph embedding with multi-granularity relational augmentation network}. Expert systems with applications.

\bibitem{duan2024d3f}
Pengbo Duan, Kuo Yang, Xin Su, et al. (2024). \textit{HTINet2: herb–target prediction via knowledge graph embedding and residual-like graph neural network}. Briefings Bioinform..

\bibitem{chen20246rm}
Zhen Chen, Dalin Zhang, Shanshan Feng, et al. (2024). \textit{KGTS: Contrastive Trajectory Similarity Learning over Prompt Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{zhu2022o32}
Jia Zhu, Changqin Huang, and P. D. Meo (2022). \textit{DFMKE: A dual fusion multi-modal knowledge graph embedding framework for entity alignment}. Information Fusion.

\bibitem{mitropoulou20235t0}
Katerina Mitropoulou, Panagiotis C. Kokkinos, P. Soumplis, et al. (2023). \textit{Anomaly Detection in Cloud Computing using Knowledge Graph Embedding and Machine Learning Mechanisms}. Journal of Grid Computing.

\bibitem{shomer2023imo}
Harry Shomer, Wei Jin, Wentao Wang, et al. (2023). \textit{Toward Degree Bias in Embedding-Based Knowledge Graph Completion}. The Web Conference.

\bibitem{wang202490m}
Mingjie Wang, Zijie Li, Jun Wang, et al. (2024). \textit{TracKGE: Transformer with Relation-pattern Adaptive Contrastive Learning for Knowledge Graph Embedding}. Knowledge-Based Systems.

\bibitem{li2024bl5}
Zhifei Li, Wei Huang, Xuchao Gong, et al. (2024). \textit{Decoupled semantic graph neural network for knowledge graph embedding}. Neurocomputing.

\bibitem{li2024y2a}
Mingqi Li, Wenming Ma, and Zihao Chu (2024). \textit{KGIE: Knowledge graph convolutional network for recommender system with interactive embedding}. Knowledge-Based Systems.

\bibitem{jia2023krv}
Yan Jia, Mengqi Lin, Yechen Wang, et al. (2023). \textit{Extrapolation over temporal knowledge graph via hyperbolic embedding}. CAAI Transactions on Intelligence Technology.

\bibitem{huang2023grx}
Wei Huang, Jia Liu, Tianrui Li, et al. (2023). \textit{FedCKE: Cross-Domain Knowledge Graph Embedding in Federated Learning}. IEEE Transactions on Big Data.

\bibitem{wang2023s70}
Ruoxin Wang, and C. F. Cheung (2023). \textit{Knowledge graph embedding learning system for defect diagnosis in additive manufacturing}. Computers in industry (Print).

\bibitem{hou20237gt}
Xiangning Hou, Ruizhe Ma, Li Yan, et al. (2023). \textit{T-GAE: A Timespan-aware Graph Attention-based Embedding Model for Temporal Knowledge Graph Completion}. Information Sciences.

\bibitem{jiang2023opm}
Dan Jiang, Ronggui Wang, Lixia Xue, et al. (2023). \textit{Multisource hierarchical neural network for knowledge graph embedding}. Expert systems with applications.

\bibitem{lu2022bwo}
H. Lu, Hailin Hu, and Xiaodong Lin (2022). \textit{DensE: An enhanced non-commutative representation for knowledge graph embedding with adaptive semantic hierarchy}. Neurocomputing.

\bibitem{djeddi2023g71}
W. Djeddi, Khalil Hermi, S. Yahia, et al. (2023). \textit{Advancing drug–target interaction prediction: a comprehensive graph-based approach integrating knowledge graph embedding and ProtBert pretraining}. BMC Bioinformatics.

\bibitem{zhang20243iw}
Yuchao Zhang, Xiangjie Kong, Zhehui Shen, et al. (2024). \textit{A survey on temporal knowledge graph embedding: Models and applications}. Knowledge-Based Systems.

\bibitem{le2023hjy}
Thanh-Binh Le, Huy Tran, and H. Le (2023). \textit{Knowledge graph embedding with the special orthogonal group in quaternion space for link prediction}. Knowledge-Based Systems.

\bibitem{yao2023y12}
Zhen Yao, Wen Zhang, Mingyang Chen, et al. (2023). \textit{Analogical Inference Enhanced Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{li2023y5q}
Zhipeng Li, Shanshan Feng, Jun Shi, et al. (2023). \textit{Future Event Prediction Based on Temporal Knowledge Graph Embedding}. Computer systems science and engineering.

\bibitem{yang2022j7z}
Shihan Yang, Weiya Zhang, R. Tang, et al. (2022). \textit{Approximate inferring with confidence predicting based on uncertain knowledge graph embedding}. Information Sciences.

\bibitem{banerjee2023fdi}
Debayan Banerjee, Pranav Ajit Nair, Ricardo Usbeck, et al. (2023). \textit{GETT-QA: Graph Embedding based T2T Transformer for Knowledge Graph Question Answering}. Extended Semantic Web Conference.

\bibitem{hu20230kr}
Yuke Hu, Wei Liang, Ruofan Wu, et al. (2023). \textit{Quantifying and Defending against Privacy Threats on Federated Knowledge Graph Embedding}. The Web Conference.

\bibitem{li2023wgg}
Daiyi Li, Li Yan, Xiaowen Zhang, et al. (2023). \textit{EventKGE: Event knowledge graph embedding with event causal transfer}. Knowledge-Based Systems.

\bibitem{hao2022cl4}
Xinkun Hao, Qingfeng Chen, Haiming Pan, et al. (2022). \textit{Enhancing drug–drug interaction prediction by three-way decision and knowledge graph embedding}. Granular Computing.

\bibitem{khan20222j1}
Nasrullah Khan, Z. Ma, Li Yan, et al. (2022). \textit{Hashing-based semantic relevance attributed knowledge graph embedding enhancement for deep probabilistic recommendation}. Applied intelligence (Boston).

\bibitem{le2022ybl}
Thanh-Binh Le, Ngoc Huynh, and Bac Le (2022). \textit{Knowledge graph embedding by projection and rotation on hyperplanes for link prediction}. Applied intelligence (Boston).

\bibitem{liang202338l}
Shuang Liang (2023). \textit{Knowledge Graph Embedding Based on Graph Neural Network}. IEEE International Conference on Data Engineering.

\bibitem{khan2022ipv}
Nasrullah Khan, Zongmin Ma, Aman Ullah, et al. (2022). \textit{DCA-IoMT: Knowledge-Graph-Embedding-Enhanced Deep Collaborative Alert Recommendation Against COVID-19}. IEEE Transactions on Industrial Informatics.

\bibitem{he2022e37}
Peng He, Gang Zhou, Mengli Zhang, et al. (2022). \textit{Improving temporal knowledge graph embedding using tensor factorization}. Applied intelligence (Boston).

\bibitem{shen2022d5j}
Linshan Shen, Rongbo He, and Shaobin Huang (2022). \textit{Entity alignment with adaptive margin learning knowledge graph embedding}. Data & Knowledge Engineering.

\bibitem{di20210ib}
Shimin Di, Quanming Yao, Yongqi Zhang, et al. (2021). \textit{Efficient Relation-aware Scoring Function Search for Knowledge Graph Embedding}. IEEE International Conference on Data Engineering.

\bibitem{niu2020uyy}
Guanglin Niu, Bo Li, Yongfei Zhang, et al. (2020). \textit{AutoETER: Automated Entity Type Representation with Relation-Aware Attention for Knowledge Graph Embedding}. Findings.

\bibitem{nie2023ejz}
H. Nie, Xiangguo Zhao, Xin Bi, et al. (2023). \textit{Correlation embedding learning with dynamic semantic enhanced sampling for knowledge graph completion}. World wide web (Bussum).

\bibitem{li2022du0}
Jiayi Li, and Yujiu Yang (2022). \textit{STaR: Knowledge Graph Embedding by Scaling, Translation and Rotation}. Autonomous Infrastructure, Management and Security.

\bibitem{daruna2022dmk}
A. Daruna, Devleena Das, and S. Chernova (2022). \textit{Explainable Knowledge Graph Embedding: Inference Reconciliation for Knowledge Inferences Supporting Robot Actions}. IEEE/RJS International Conference on Intelligent RObots and Systems.

\bibitem{zhou20210ma}
Xing-Chun Zhou, Peng Wang, Qi Luo, et al. (2021). \textit{Multi-hop Knowledge Graph Reasoning Based on Hyperbolic Knowledge Graph Embedding and Reinforcement Learning}. IJCKG.

\bibitem{kun202384f}
Kong Wei Kun, Xin Liu, Teeradaj Racharak, et al. (2023). \textit{WeExt: A Framework of Extending Deterministic Knowledge Graph Embedding Models for Embedding Weighted Knowledge Graphs}. IEEE Access.

\bibitem{dong2022taz}
Yao Dong, Lei Wang, Ji Xiang, et al. (2022). \textit{RotateCT: Knowledge Graph Embedding by Rotation and Coordinate Transformation in Complex Space}. International Conference on Computational Linguistics.

\bibitem{kamigaito20218jz}
Hidetaka Kamigaito, and Katsuhiko Hayashi (2021). \textit{Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{krause2022th0}
Franziska Krause (2022). \textit{Dynamic Knowledge Graph Embeddings via Local Embedding Reconstructions}. Extended Semantic Web Conference.

\bibitem{zhang20213h6}
Zhao Zhang, Fuzhen Zhuang, Meng Qu, et al. (2021). \textit{Knowledge graph embedding with shared latent semantic units}. Neural Networks.

\bibitem{li2021tm6}
Guang-pu Li, Zequn Sun, Lei Qian, et al. (2021). \textit{Rule-based data augmentation for knowledge graph embedding}. AI Open.

\bibitem{wang2020au0}
Kai Wang, Yu Liu, Xiujuan Xu, et al. (2020). \textit{Enhancing knowledge graph embedding by composite neighbors for link prediction}. Computing.

\bibitem{wei20215a7}
Yuyang Wei, Wei Chen, Zhixu Li, et al. (2021). \textit{Incremental Update of Knowledge Graph Embedding by Rotating on Hyperplanes}. 2021 IEEE International Conference on Web Services (ICWS).

\bibitem{zhang2021rjh}
Yongqi Zhang, Quanming Yao, and Lei Chen (2021). \textit{Simple and automated negative sampling for knowledge graph embedding}. The VLDB journal.

\bibitem{sheikh202245c}
Nasrullah Sheikh, Xiao Qin, B. Reinwald, et al. (2022). \textit{Scaling knowledge graph embedding models for link prediction}. EuroMLSys@EuroSys.

\bibitem{ren2021muc}
Chao Ren, Le Zhang, Lintao Fang, et al. (2021). \textit{Ontological Concept Structure Aware Knowledge Transfer for Inductive Knowledge Graph Embedding}. IEEE International Joint Conference on Neural Network.

\bibitem{eyharabide2021wx4}
Victoria Eyharabide, I. E. I. Bekkouch, and Nicolae Dragoș Constantin (2021). \textit{Knowledge Graph Embedding-Based Domain Adaptation for Musical Instrument Recognition}. De Computis.

\bibitem{hong2020hyg}
Y. Hong, Chenyang Bu, and Tingting Jiang (2020). \textit{Rule-enhanced Noisy Knowledge Graph Embedding via Low-quality Error Detection}. 2020 IEEE International Conference on Knowledge Graph (ICKG).

\bibitem{huang2020sqc}
Yan Huang, Haili Sun, Xu Ke, et al. (2020). \textit{CoRelatE: Learning the correlation in multi-fold relations for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{kurokawa2021f4f}
M. Kurokawa (2021). \textit{Explainable Knowledge Reasoning Framework Using Multiple Knowledge Graph Embedding}. IJCKG.

\bibitem{mohamed2021dwg}
Sameh K. Mohamed, Emir Muñoz, and V. Nováček (2021). \textit{On Training Knowledge Graph Embedding Models}. Inf..

\bibitem{gebhart2021gtp}
Thomas Gebhart, J. Hansen, and Paul Schrater (2021). \textit{Knowledge Sheaves: A Sheaf-Theoretic Framework for Knowledge Graph Embedding}. International Conference on Artificial Intelligence and Statistics.

\bibitem{deng2024643}
Weibin Deng, Yiteng Zhang, Hong Yu, et al. (2024). \textit{Knowledge graph embedding based on dynamic adaptive atrous convolution and attention mechanism for link prediction}. Information Processing & Management.

\bibitem{liu2024zr9}
Jin Liu, Hao Du, R. Guo, et al. (2024). \textit{MMGK: Multimodality Multiview Graph Representations and Knowledge Embedding for Mild Cognitive Impairment Diagnosis}. IEEE Transactions on Computational Social Systems.

\bibitem{zhang2024zmq}
Chengcheng Zhang, Tianyi Zang, and Tianyi Zhao (2024). \textit{KGE-UNIT: toward the unification of molecular interactions prediction based on knowledge graph and multi-task learning on drug discovery}. Briefings Bioinform..

\bibitem{he2024vks}
Mingsheng He, Lin Zhu, and Luyi Bai (2024). \textit{ConvTKG: A query-aware convolutional neural network-based embedding model for temporal knowledge graph completion}. Neurocomputing.

\bibitem{zhang2024fy0}
Dong Zhang, Zhe Rong, Chengyuan Xue, et al. (2024). \textit{SimRE: Simple contrastive learning with soft logical rule for knowledge graph embedding}. Information Sciences.

\bibitem{zhang2024ivc}
Dong Zhang, Wenlong Feng, Zonghang Wu, et al. (2024). \textit{CDRGN-SDE: Cross-Dimensional Recurrent Graph Network with neural Stochastic Differential Equation for temporal knowledge graph embedding}. Expert systems with applications.

\bibitem{jing2024nxw}
Yanzhen Jing, Guanghui Zhou, Chao Zhang, et al. (2024). \textit{XMKR: Explainable manufacturing knowledge recommendation for collaborative design with graph embedding learning}. Advanced Engineering Informatics.

\bibitem{jiang2024zlc}
Pengcheng Jiang, Lang Cao, Cao Xiao, et al. (2024). \textit{KG-FIT: Knowledge Graph Fine-Tuning Upon Open-World Knowledge}. Neural Information Processing Systems.

\bibitem{han2024u0t}
Zhulin Han, and Jian Wang (2024). \textit{Knowledge enhanced graph inference network based entity-relation extraction and knowledge graph construction for industrial domain}. Frontiers of Engineering Management.

\bibitem{quan2024o2a}
Huafeng Quan, Yiting Li, Dashuai Liu, et al. (2024). \textit{Protection of Guizhou Miao batik culture based on knowledge graph and deep learning}. Heritage Science.

\bibitem{liu2024tc2}
Bufan Liu, Chun-Hsien Chen, and Zuoxu Wang (2024). \textit{A multi-hierarchical aggregation-based graph convolutional network for industrial knowledge graph embedding towards cognitive intelligent manufacturing}. Journal of manufacturing systems.

\bibitem{hello2024hgf}
Nour Hello, P. Lorenzo, and E. Strinati (2024). \textit{Semantic Communication Enhanced by Knowledge Graph Representation Learning}. International Workshop on Signal Processing Advances in Wireless Communications.

\bibitem{li2024z0e}
Jinpeng Li, Hang Yu, Xiangfeng Luo, et al. (2024). \textit{COSIGN: Contextual Facts Guided Generation for Knowledge Graph Completion}. North American Chapter of the Association for Computational Linguistics.

\bibitem{yan2024joa}
Qun Yan, Juan Zhao, Linfu Xue, et al. (2024). \textit{Mineral Prospectivity Mapping Based on Spatial Feature Classification with Geological Map Knowledge Graph Embedding: Case Study of Gold Ore Prediction at Wulonggou, Qinghai Province (Western China)}. Natural Resources Research.

\bibitem{liu2024tn0}
Jhih-Chen Liu, Chiao-Ting Chen, Chi Lee, et al. (2024). \textit{Evolving Knowledge Graph Representation Learning with Multiple Attention Strategies for Citation Recommendation System}. ACM Transactions on Intelligent Systems and Technology.

\bibitem{wang20245dw}
Chuanghui Wang, Yunqing Yang, Jinshuai Song, et al. (2024). \textit{Research Progresses and Applications of Knowledge Graph Embedding Technique in Chemistry}. Journal of Chemical Information and Modeling.

\bibitem{long2024soi}
Xiao Long, Liansheng Zhuang, Aodi Li, et al. (2024). \textit{KGDM: A Diffusion Model to Capture Multiple Relation Semantics for Knowledge Graph Embedding}. AAAI Conference on Artificial Intelligence.

\bibitem{zhou2024ayq}
Qihui Zhou, Peiqi Yin, Xiao Yan, et al. (2024). \textit{Atom: An Efficient Query Serving System for Embedding-based Knowledge Graph Reasoning with Operator-level Batching}. Proc. ACM Manag. Data.

\bibitem{huang2024t19}
Chen Huang, Deshan Chen, Tengze Fan, et al. (2024). \textit{Incorporating environmental knowledge embedding and spatial-temporal graph attention networks for inland vessel traffic flow prediction}. Engineering applications of artificial intelligence.

\bibitem{lu2024fsd}
Ming Lu, Yancong Li, Jiangxiao Zhang, et al. (2024). \textit{Deep hyperbolic convolutional model for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{liu2024yar}
Qi Liu, Qinghua Zhang, Fan Zhao, et al. (2024). \textit{Uncertain knowledge graph embedding: an effective method combining multi-relation and multi-path}. Frontiers Comput. Sci..

\bibitem{khan20242y2}
Nasrullah Khan, Zongmin Ma, Ruizhe Ma, et al. (2024). \textit{Continual knowledge graph embedding enhancement for joint interaction-based next click recommendation}. Knowledge-Based Systems.

\bibitem{xue2025ee8}
Zengcan Xue, Zhaoli Zhang, Hai Liu, et al. (2025). \textit{MHRN: A multi-perspective hierarchical relation network for knowledge graph embedding}. Knowledge-Based Systems.

\bibitem{long20248vt}
Xiao Long, Liansheng Zhuang, Aodi Li, et al. (2024). \textit{Fact Embedding through Diffusion Model for Knowledge Graph Completion}. The Web Conference.

\bibitem{huang20240su}
Chen Huang, Fei Yu, Zhiguo Wan, et al. (2024). \textit{Knowledge graph confidence-aware embedding for recommendation}. Neural Networks.

\bibitem{wang2024nej}
Yuzhuo Wang, Hongzhi Wang, Xianglong Liu, et al. (2024). \textit{GFedKG: GNN-based federated embedding model for knowledge graph completion}. Knowledge-Based Systems.

\bibitem{wang2024c8z}
Xinyan Wang, Kuo Yang, Ting Jia, et al. (2024). \textit{KDGene: knowledge graph completion for disease gene prediction using interactional tensor decomposition}. Briefings Bioinform..

\bibitem{liu2024x0k}
Yuhan Liu, Zelin Cao, Xing Gao, et al. (2024). \textit{Bridging the Space Gap: Unifying Geometry Knowledge Graph Embedding with Optimal Transport}. The Web Conference.

\bibitem{li2024uio}
Yongfang Li, and Chunhua Zhu (2024). \textit{TransE-MTP: A New Representation Learning Method for Knowledge Graph Embedding with Multi-Translation Principles and TransE}. Electronics.

\bibitem{zhang2024z78}
Qianjin Zhang, and Yandan Xu (2024). \textit{Knowledge graph embedding with inverse function representation for link prediction}. Engineering applications of artificial intelligence.

\bibitem{wang2024534}
Hao Wang, Dandan Song, Zhijing Wu, et al. (2024). \textit{A collaborative learning framework for knowledge graph embedding and reasoning}. Knowledge-Based Systems.

\bibitem{ni202438q}
Shengkun Ni, Xiangtai Kong, Yingying Zhang, et al. (2024). \textit{Identifying compound-protein interactions with knowledge graph embedding of perturbation transcriptomics}. Cell Genomics.

\bibitem{nie202499i}
Jixuan Nie, Xia Hou, Wenfeng Song, et al. (2024). \textit{Knowledge Graph Efficient Construction: Embedding Chain-of-Thought into LLMs}. VLDB Workshops.

\bibitem{wang2024d52}
Jingchao Wang, Weimin Li, Fangfang Liu, et al. (2024). \textit{ConeE: Global and local context-enhanced embedding for inductive knowledge graph completion}. Expert systems with applications.

\bibitem{mao2024v2s}
Yuren Mao, Yu Hao, Xin Cao, et al. (2024). \textit{Dynamic Graph Embedding via Meta-Learning}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{jafarzadeh202468v}
Parastoo Jafarzadeh, F. Ensan, Mahdiyar Ali Akbar Alavi, et al. (2024). \textit{A Knowledge Graph Embedding Model for Answering Factoid Entity Questions}. ACM Trans. Inf. Syst..

\bibitem{wang2024dea}
Yalin Wang, Yubin Peng, and Jingyu Guo (2024). \textit{Enhancing knowledge graph embedding with structure and semantic features}. Applied intelligence (Boston).

\bibitem{lu202436n}
Yuhuan Lu, Weijian Yu, Xin Jing, et al. (2024). \textit{HyperCL: A Contrastive Learning Framework for Hyper-Relational Knowledge Graph Embedding with Hierarchical Ontology}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{han2024gaq}
Yadan Han, Guangquan Lu, Shichao Zhang, et al. (2024). \textit{A Temporal Knowledge Graph Embedding Model Based on Variable Translation}. Tsinghua Science and Technology.

\bibitem{liu2024jz8}
Bingchen Liu, Shifu Hou, Weiyi Zhong, et al. (2024). \textit{Enhancing Temporal Knowledge Graph Alignment in News Domain With Box Embedding}. IEEE Transactions on Computational Social Systems.

\bibitem{he2024y6o}
Yunjie He, Daniel Hernández, M. Nayyeri, et al. (2024). \textit{Generating $SROI^-$ Ontologies via Knowledge Graph Query Embedding Learning}. Unpublished manuscript.

\bibitem{fang20243a4}
Yan Fang, Xiaodong Liu, Wei Lu, et al. (2024). \textit{Knowledge graph completion with low-dimensional gated hierarchical hyperbolic embedding}. Knowledge-Based Systems.

\bibitem{zhang2024h9k}
Mingtao Zhang, Guoli Yang, Yi Liu, et al. (2024). \textit{Knowledge graph accuracy evaluation: an LLM-enhanced embedding approach}. International Journal of Data Science and Analysis.

\bibitem{li2024wyh}
Yicong Li, Yu Yang, Jiannong Cao, et al. (2024). \textit{Toward Structure Fairness in Dynamic Graph Embedding: A Trend-aware Dual Debiasing Approach}. Knowledge Discovery and Data Mining.

\bibitem{dong2024ijo}
Dibo Dong, Shangwei Wang, Qiaoying Guo, et al. (2024). \textit{Short-Term Marine Wind Speed Forecasting Based on Dynamic Graph Embedding and Spatiotemporal Information}. Journal of Marine Science and Engineering.

\bibitem{wang20246c7}
Tao Wang, Bo Shen, Jinglin Zhang, et al. (2024). \textit{Knowledge Graph Embedding via Triplet Component Interactions}. Neural Processing Letters.

\bibitem{zhang2024yjo}
Pengfei Zhang, Xiaoxue Zhang, Yang Fang, et al. (2024). \textit{Knowledge Graph Embedding for Hierarchical Entities Based on Auto-Embedding Size}. Mathematics.

\bibitem{liang20247wv}
K. Liang, Yue Liu, Hao Li, et al. (2024). \textit{Clustering then Propagation: Select Better Anchors for Knowledge Graph Embedding}. Neural Information Processing Systems.

\bibitem{liu2024t05}
Qi Liu, Yuanyuan Jin, Xuefei Cao, et al. (2024). \textit{An Entity Ontology-Based Knowledge Graph Embedding Approach to News Credibility Assessment}. IEEE Transactions on Computational Social Systems.

\bibitem{pham20243mh}
H. V. Pham, Trung Tuan Nguyen, Luu Minh Tuan, et al. (2024). \textit{IDGCN: A Proposed Knowledge Graph Embedding With Graph Convolution Network For Context-Aware Recommendation Systems}. Journal of Organizational Computing and Electronic Commerce.

\bibitem{li2024gar}
Yu Li, Zhu-Hong You, Shu-Min Wang, et al. (2024). \textit{Attention-Based Learning for Predicting Drug-Drug Interactions in Knowledge Graph Embedding Based on Multisource Fusion Information}. International Journal of Intelligent Systems.

\bibitem{li2024nje}
Nan Li, Zhihao Yang, Jian Wang, et al. (2024). \textit{Drug–target interaction prediction using knowledge graph embedding}. iScience.

\bibitem{bao20249xp}
Liming Bao, Yan Wang, Xiaoyu Song, et al. (2024). \textit{HGCGE: hyperbolic graph convolutional networks-based knowledge graph embedding for link prediction}. Knowledge and Information Systems.

\bibitem{xu2024fto}
Guoshun Xu, Guozheng Rao, Li Zhang, et al. (2024). \textit{Entity-relation aggregation mechanism graph neural network for knowledge graph embedding}. Applied intelligence (Boston).

\bibitem{liang2024z0q}
Qiuyu Liang, Weihua Wang, Jie Yu, et al. (2024). \textit{Effective Knowledge Graph Embedding with Quaternion Convolutional Networks}. Natural Language Processing and Chinese Computing.

\bibitem{liu2024ixy}
Jie Liu, Lizheng Zu, Yunbin Yan, et al. (2024). \textit{Multi-Filter soft shrinkage network for knowledge graph embedding}. Expert systems with applications.

\bibitem{dong2025l9k}
Jie Dong, Cuiping Chen, Chi Zhang, et al. (2025). \textit{Knowledge Graph Embedding With Graph Convolutional Network and Bidirectional Gated Recurrent Unit for Fault Diagnosis of Industrial Processes}. IEEE Sensors Journal.

\bibitem{zhang2025ebv}
Sensen Zhang, Xun Liang, Simin Niu, et al. (2025). \textit{Integrating Large Language Models and Möbius Group Transformations for Temporal Knowledge Graph Embedding on the Riemann Sphere}. AAAI Conference on Artificial Intelligence.

\bibitem{liu20242zm}
Xinyue Liu, Jianan Zhang, Chi Ma, et al. (2024). \textit{Temporal Knowledge Graph Reasoning with Dynamic Hypergraph Embedding}. International Conference on Language Resources and Evaluation.

\bibitem{yang2024lwa}
Ruiyi Yang, Flora D. Salim, and Hao Xue (2024). \textit{SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and Versatile Dynamic Information Embedding}. The Web Conference.

\bibitem{li20246qx}
Bo Li, Haowei Quan, Jiawei Wang, et al. (2024). \textit{Neural Library Recommendation by Embedding Project-Library Knowledge Graph}. IEEE Transactions on Software Engineering.

\bibitem{liu2024mji}
Xiaojian Liu, Xinwei Guo, and Wen Gu (2024). \textit{SecKG2vec: A novel security knowledge graph relational reasoning method based on semantic and structural fusion embedding}. Computers & security.

\bibitem{chen2024efo}
Bin Chen, Hongyi Li, Di Zhao, et al. (2024). \textit{Quality assessment of cyber threat intelligence knowledge graph based on adaptive joining of embedding model}. Complex &amp; Intelligent Systems.

\bibitem{chen2024uld}
Deng Chen, Weiwei Zhang, and Zuohua Ding (2024). \textit{Embedding dynamic graph attention mechanism into Clinical Knowledge Graph for enhanced diagnostic accuracy}. Expert systems with applications.

\bibitem{wang2017zm5}
Quan Wang, Zhendong Mao, Bin Wang, et al. (2017). \textit{Knowledge Graph Embedding: A Survey of Approaches and Applications}. IEEE Transactions on Knowledge and Data Engineering.

\bibitem{li2021qr0}
Zhifei Li, Hai Liu, Zhaoli Zhang, et al. (2021). \textit{Learning Knowledge Graph Embedding With Heterogeneous Relation Attention Networks}. IEEE Transactions on Neural Networks and Learning Systems.

\end{thebibliography}

\end{document}