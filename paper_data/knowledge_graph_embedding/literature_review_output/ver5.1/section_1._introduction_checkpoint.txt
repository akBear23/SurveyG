\section{1. Introduction}
The rapid proliferation of digital information across diverse domains has ushered in an era where the ability to effectively organize, interpret, and leverage vast quantities of data is paramount for the advancement of Artificial Intelligence (AI) systems \cite{wang2017zm5, dai2020, choudhary2021, ge2023, yan2022}. At the forefront of this organizational paradigm are Knowledge Graphs (KGs), which have emerged as a foundational technology for structuring complex, interconnected information into a machine-readable format. KGs represent real-world entities and their relationships as a collection of triples (head entity, relation, tail entity), thereby providing a rich, semantic layer over raw data \cite{wang2017zm5}. This structured representation facilitates a deeper understanding of information, enabling AI systems to move beyond pattern recognition to more sophisticated reasoning, inference, and decision-making capabilities. From powering search engines and recommendation systems to facilitating drug discovery and scientific research, KGs are increasingly indispensable components of modern AI architectures \cite{xiong2017zqu, huang2019, mohamed2020, sosa2019ih0, gong2020b2k, li2021x10, zhou2023, islam2023, zhu2022, li2022, liu2018kvd, ni2020ruj, mezni20218ml, wang202110w, kartheek2021aj7, sha2019i3a, sha2019plw, liu2019e1u, gradgyenge2017xdy, myklebust201941l, elebi20182bd, elebi2019bzc, garofalo20185g9, zhou2022ehi, xu2019t6b, liu2021wqa, sang2019gjl, wang2017yjq, chen2022mxn, abusalih2020gdu, eyharabide2021wx4, mitropoulou20235t0, djeddi2023g71, duan2024d3f, li2024nje, li2024gar, hao2022cl4, su2023v6e, zhang2024zmq, liu2024q3q, jing2024nxw, han2024u0t, quan2024o2a, liu2024tc2, hello2024hgf, li2024z0e, yan2024joa, liu2024tn0, wang20245dw, huang2024t19, liu2024t05, pham20243mh, dong2025l9k, chen2024uld, chen2024efo, liu2024mji, li20246qx}.

However, the inherent symbolic nature of KGs, while offering precision and interpretability, presents significant challenges when confronted with the demands of modern AI. Traditional symbolic knowledge representation methods, relying on discrete symbols and explicit logical rules, often struggle with the scale, sparsity, and inherent fuzziness of real-world knowledge \cite{gutirrezbasulto2018oi0}. They are computationally expensive for large graphs, brittle in the face of noise or incompleteness, and fundamentally incompatible with the continuous vector spaces that underpin most contemporary machine learning (ML) models. This fundamental mismatch necessitates a transformative approach: the conversion of KGs into dense, continuous vector spaces, a process known as Knowledge Graph Embedding (KGE) \cite{wang2017zm5, dai2020}.

The core motivation behind KGE is to overcome the limitations of symbolic representation by learning low-dimensional, continuous vector representations (embeddings) for entities and relations within a KG. These embeddings aim to capture the latent semantic and structural properties of the graph, allowing for efficient computation, enhanced scalability, and seamless integration with a wide array of machine learning algorithms \cite{wang2017zm5, dai2020, choudhary2021, ge2023, yan2022}. By transforming discrete symbols into a continuous space, KGE models can perform tasks such as link prediction (inferring missing relationships), entity alignment (identifying equivalent entities across different KGs), and question answering with unprecedented efficiency and accuracy. For instance, models like \cite{sun2018} leverage KGE for bootstrapping entity alignment, demonstrating how embedding-based approaches can effectively overcome data scarcity challenges in integrating KGs. Similarly, the work by \cite{chen2023} highlights the critical need for parameter-efficient KGE models to ensure scalability and deployability in resource-constrained environments, directly addressing the computational burden often associated with large KGs.

This comprehensive literature review aims to synthesize and critically analyze the evolution of KGE research, providing a pedagogical progression from its foundational principles to its cutting-edge applications and future directions. We will explore how KGE models have evolved to capture increasingly complex aspects of knowledge, including temporal dynamics \cite{dasgupta2018, xu2019}, multi-modal information, and hierarchical structures. The review will not merely enumerate existing models but will delve into the underlying theoretical assumptions, comparative strengths and weaknesses, and the persistent challenges that continue to drive innovation in the field. By constructing explicit comparative frameworks and identifying recurring patterns, tensions, and unresolved debates, we seek to provide a holistic understanding of the intellectual trajectory of KGE research, highlighting both its profound successes and its enduring frontiers.

\subsection{1.1. Background: The Rise of Knowledge Graphs}
The landscape of modern AI is increasingly defined by its capacity to process and reason over vast, intricate datasets. In this context, Knowledge Graphs (KGs) have emerged as a pivotal technology, offering a structured and semantic framework for organizing information that transcends the limitations of unstructured text or tabular data \cite{wang2017zm5, dai2020, choudhary2021}. A KG fundamentally represents knowledge as a directed graph, where nodes denote real-world entities (e.g., "Albert Einstein," "E=mc²") and edges represent semantic relations between them (e.g., "discovered," "is_a") \cite{wang2017zm5}. Each piece of information is typically expressed as a triple (head entity, relation, tail entity), such as (Albert Einstein, discovered, E=mc²). This explicit, machine-readable structure allows AI systems to not only store facts but also to understand the relationships and context surrounding those facts, which is crucial for higher-level cognitive tasks.

The prevalence of KGs has surged due to their ability to provide a rich, interconnected web of facts that can power a multitude of AI applications. For instance, KGs are instrumental in enhancing search engine relevance by understanding user intent and providing direct answers rather than just links \cite{xiong2017zqu}. In recommendation systems, KGs enable more personalized and explainable recommendations by modeling complex user-item interactions and underlying reasons for preferences \cite{sun2018, yang2023, sha2019i3a, sha2019plw, liu2019e1u, gradgyenge2017xdy, kartheek2021aj7, wang2017yjq, mezni20218ml, wang202110w, shokrzadeh2023twj, khan202236g, khan20222j1, khan2022ipv, khan20242y2, liu2024tn0, wang2024vgj}. For example, \cite{yang2023} introduces Contextualized Knowledge Graph Embedding (CKGE) for explainable talent training course recommendation, demonstrating how integrating motivation-aware information and high-order connections within a KG can yield superior and interpretable results. This work highlights a critical tension in recommendation systems: the trade-off between predictive accuracy and explainability, which KGE-based approaches like CKGE aim to resolve by leveraging the structured nature of KGs.

Beyond these common applications, KGs are increasingly vital in specialized domains. In biomedicine, they facilitate drug discovery, repurposing, and adverse drug reaction prediction by mapping complex biological pathways and drug-target interactions \cite{mohamed2020, sosa2019ih0, gong2020b2k, islam2023, zhu2022, elebi20182bd, elebi2019bzc, myklebust201941l, sang2019gjl, zhang2021wg7, su2023v6e, djeddi2023g71, duan2024d3f, li2024nje, li2024gar, hao2022cl4, zhang2024zmq, ni202438q}. For instance, \cite{elebi2019bzc} evaluates KGE approaches for drug-drug interaction prediction, underscoring the practical utility of KGs in identifying critical medical insights. Similarly, KGs support entity alignment across heterogeneous data sources, a crucial task for integrating disparate knowledge bases \cite{sun2018, zhang2019, xin2022dam, fanourakis2022, guo2022qtv, zhu2024}. The work by \cite{sun2018}, "Bootstrapping Entity Alignment with Knowledge Graph Embedding," exemplifies this by proposing a novel bootstrapping approach that iteratively labels likely entity alignments to expand training data for alignment-oriented KG embeddings. This method addresses the significant challenge of limited prior alignment, which often plagues entity alignment tasks, by using a global optimal labeling strategy based on max-weighted matching and an alignment editing method to reduce error propagation. This directly contrasts with earlier methods like IPTransE \cite{zhu2017} which relied on local optimal distance measures and were highly sensitive to initial alignment precision, leading to error accumulation.

Despite their immense utility, KGs are not without their inherent limitations. A primary challenge is their incompleteness; real-world KGs are often sparse, with many potential facts missing \cite{wang2017zm5}. This sparsity is exacerbated by the dynamic nature of knowledge, where facts evolve over time, necessitating temporal awareness \cite{xu2019, dasgupta2018}. Furthermore, KGs can be noisy, containing erroneous or inconsistent information, especially when constructed automatically. The sheer scale of many KGs, comprising millions of entities and billions of facts, also poses significant computational and storage challenges. For example, the need for parameter-efficient models, as highlighted by \cite{chen2023}, becomes critical when dealing with KGs that can have hundreds of millions of parameters, making deployment on resource-constrained devices or in federated learning scenarios impractical. These limitations underscore that while KGs provide an unparalleled framework for knowledge organization, their full potential can only be realized through advanced representation techniques that can overcome these intrinsic hurdles. The evolution of KGE research is a direct response to these challenges, aiming to transform KGs into a more flexible, scalable, and computationally amenable format.

\subsection{1.2. The Imperative for Knowledge Graph Embedding}
The intrinsic value of Knowledge Graphs (KGs) in structuring complex information for AI systems is undeniable, yet their traditional symbolic representation presents a formidable bottleneck to their full utilization. Symbolic methods, which represent entities and relations as discrete tokens and rely on explicit logical rules for inference, are inherently limited in several critical ways. Firstly, they suffer from **sparsity and incompleteness** \cite{wang2017zm5, dai2020}. Real-world KGs are never exhaustive; many valid facts are simply unknown or unrecorded. Symbolic systems struggle to infer these missing links without explicit rules, leading to brittle and incomplete knowledge bases. Secondly, symbolic representations are **computationally expensive and lack scalability** for large-scale KGs. Performing complex logical inference over millions of entities and billions of relations can lead to combinatorial explosions, making real-time reasoning impractical \cite{wang2017zm5}. Thirdly, they are **sensitive to noise and lack robustness**. A single erroneous fact or rule can propagate errors throughout the system, leading to incorrect inferences. Finally, and perhaps most critically for modern AI, symbolic representations are **fundamentally incompatible with the continuous vector spaces** that underpin the vast majority of contemporary machine learning (ML) models, particularly deep learning architectures. This incompatibility creates a chasm between structured knowledge and powerful data-driven learning paradigms, hindering the seamless integration of knowledge into ML systems \cite{wang2017zm5}.

This growing chasm necessitated a paradigm shift, giving rise to the imperative for Knowledge Graph Embedding (KGE). KGE addresses these limitations by transforming discrete symbolic entities and relations into dense, low-dimensional, continuous vector representations (embeddings) in a latent semantic space \cite{wang2017zm5, dai2020, choudhary2021, ge2023, yan2022}. The core innovation lies in capturing the latent semantic and structural properties of the KG within these vector spaces, allowing for geometric interpretations of relationships. For instance, translational models like TransE \cite{jia2015} conceptualize relations as translation vectors between entity embeddings (i.e., $h + r \approx t$), while rotational models like RotatE \cite{sun2018} represent relations as rotations in complex vector spaces. This transformation offers several profound benefits that are critical for the advancement of AI.

Firstly, KGE enables **efficient computation and scalability**. Once entities and relations are embedded as vectors, operations like similarity calculations or relation predictions become simple vector arithmetic, which can be performed efficiently on GPUs. This drastically reduces the computational cost compared to symbolic inference, allowing KGE models to handle massive KGs with millions of entities and billions of triples \cite{wang2017zm5, dai2020}. The work by \cite{chen2023} on "Entity-Agnostic Representation Learning for Parameter-Efficient Knowledge Graph Embedding" directly tackles the scalability challenge by proposing a method that learns embeddings without the parameter count scaling linearly with the number of entities. This is a crucial innovation, as conventional KGE models can require colossal parameter counts (e.g., 123 million for RotatE on YAGO3-10), making them impractical for deployment on edge devices or in federated learning scenarios. By focusing on entity-agnostic encoding, \cite{chen2023} offers a path toward more sustainable and deployable KGE models, demonstrating that significant parameter reduction can be achieved without sacrificing performance. This highlights a critical, often unstated, assumption in KGE research: that entity embeddings must be unique and directly learned for each entity. \cite{chen2023} challenges this by showing that rich representations can be learned compositionally from local context.

Secondly, KGE significantly enhances **generalization and robustness**. By learning continuous representations, KGE models can infer missing links (link prediction) by identifying plausible relationships in the embedding space, even if they have never been explicitly observed in the KG \cite{rossi2020}. This addresses the inherent incompleteness of KGs. For example, methods like \cite{jia2015}'s TransA, which introduces "Locally Adaptive Translation," improve generalization by adaptively determining optimal margins for the loss function based on the specific structure and locality of the knowledge graph. This is a critical advancement over prior methods like TransE and TransH, which relied on global, fixed margins that often led to suboptimal performance across diverse KG subsets. TransA's innovation lies in its ability to theoretically justify and empirically demonstrate that different graph localities require different optimal loss functions, thereby making the embedding process more sensitive and robust to the nuances of KG structure. This directly addresses the implicit assumption that a single, globally tuned hyperparameter (like the margin) is sufficient for all parts of a heterogeneous KG.

Thirdly, KGE ensures **seamless integration with machine learning models**. The output of KGE models—dense vector embeddings—can be directly fed as features into various downstream ML tasks, including natural language processing, computer vision, and recommendation systems. This bridges the gap between structured knowledge and statistical learning, allowing ML models to leverage rich semantic information that was previously inaccessible or difficult to incorporate. This integration is exemplified in applications like explainable recommendation systems \cite{yang2023} or question answering systems \cite{huang2019, zhou2023}, where KGE provides the semantic backbone for intelligent agents.

Finally, KGE models have evolved to capture increasingly complex aspects of knowledge, such as **temporal dynamics**. Early KGE models treated KGs as static, ignoring the time-varying nature of facts. However, the recognition that facts have temporal validity (e.g., "Barack Obama is President of the USA" is only true for a specific period) led to the development of Temporal Knowledge Graph Embedding (TKGE) models \cite{xu2019, dasgupta2018, xu2020, liu201918i, tang2020ufr, zhang2020s4x, sadeghian2021, lee2022hr9, fu2022df2, zhang2022muu, xie2023, li2023y5q, li2023, ji2024, wang2024, wang2024ime, zhang20243iw, han2024gaq, liu2024jz8, zhang2025ebv, liu20242zm, yang2024lwa, he2024vks, zhang2024ivc}. For instance, \cite{dasgupta2018}'s HyTE (Hyperplane-based Temporally aware Knowledge Graph Embedding) explicitly incorporates time by associating each timestamp with a hyperplane, allowing for temporally guided inference and prediction of temporal scopes for missing annotations. This is a significant advancement over static KGEs, which would treat facts like `(Obama, presidentOf, USA, 2010)` and `(Obama, presidentOf, USA, 2020)` identically, leading to inaccurate reasoning. Building on this, \cite{xu2019} proposes ATiSE (Additive Time Series Embedding), which models entity and relation representations as multi-dimensional Gaussian distributions evolving via additive time series decomposition (trend, seasonal, random components). This novel approach explicitly captures *temporal uncertainty* in knowledge evolution, a critical limitation ignored by many prior TKGE models that assumed deterministic changes. The tension here lies between the simplicity and efficiency of static embeddings versus the expressive power and accuracy required for dynamic, real-world knowledge.

The field also grapples with the challenge of **disentangled representations**, where entities might have multiple facets, and relations might focus on distinct aspects. \cite{wu2021}'s DisenKGAT (Disentangled Knowledge Graph Attention Network) addresses this by learning disentangled entity representations, incorporating both micro-disentanglement (relation-aware aggregation) and macro-disentanglement (mutual information regularization) to ensure independence between learned components. This innovation directly confronts the limitation of single, static entity representations that fail to capture the complex, multi-faceted nature of entities and the context-dependency of relations, a problem that even advanced GNN-based KGC models often overlook. The implicit assumption that a single vector can adequately represent all aspects of an entity is systematically questioned and addressed by DisenKGAT.

In summary, the imperative for KGE stems from the inherent limitations of symbolic knowledge representation in the face of massive, dynamic, and often incomplete real-world KGs. KGE provides a powerful framework for transforming these symbolic structures into a computationally efficient, scalable, generalizable, and ML-compatible format, thereby unlocking the full potential of KGs in modern AI systems. The evolution of KGE models reflects a continuous effort to address these challenges, pushing the boundaries of what knowledge-driven AI can achieve.

\subsection{1.3. Scope and Structure of the Review}
Having established the foundational importance of Knowledge Graphs (KGs) in modern AI and the compelling imperative for Knowledge Graph Embedding (KGE) to overcome the limitations of symbolic representation, this literature review now delineates its scope and pedagogical structure. The primary objective is not merely to catalog existing KGE models but to provide a comprehensive, critical, and synthetic analysis of the field's intellectual evolution, identifying key paradigms, recurring challenges, and future trajectories. This review aims to serve as a structured guide for researchers and practitioners, offering insights into the underlying mechanisms, comparative performance, and theoretical underpinnings of various KGE approaches.

The pedagogical progression of this review is designed to build knowledge systematically, starting from the foundational principles and gradually advancing to more complex and specialized topics. We will begin by categorizing KGE models into major families, such as translational models, semantic matching models, and graph neural network-based approaches. This initial categorization will establish a comparative framework, highlighting how different model families (e.g., TransE \cite{jia2015}, DistMult, ComplEx, RotatE \cite{sun2018}) share the common goal of embedding entities and relations but diverge significantly in their scoring functions and geometric interpretations. For instance, while translational models focus on vector addition, rotational models leverage complex-space rotations, each offering distinct advantages in capturing specific relational patterns like symmetry, anti-symmetry, and compositionality \cite{sun2018}. A critical analysis will compare these foundational approaches on dimensions such as expressiveness, computational complexity, and their ability to model different relation types, drawing on empirical evaluations like those in \cite{lloyd2022} which assess the effects of hyperparameters on KGE quality across various datasets. The work by \cite{lloyd2022} is particularly insightful as it empirically demonstrates that hyperparameter sensitivities vary substantially between knowledge graphs, suggesting that optimal tuning strategies are dataset-specific. This finding implicitly questions the common assumption in many KGE papers that a single set of hyperparameters can generalize well across diverse KG structures.

Following this foundational overview, the review will delve into advanced KGE techniques that address specific challenges and extend the capabilities of initial models. This includes the integration of auxiliary information, such as entity types \cite{wang2021}, textual descriptions \cite{xiao2016}, and logical rules \cite{guo2017, guo2020, wang20199fe, tang2022}, to enrich embeddings and improve performance. We will critically examine how these extensions enhance model expressiveness and robustness, often by mitigating the sparsity problem inherent in KGs. For example, methods that incorporate textual information (e.g., \cite{xiao2016}) aim to leverage the rich semantics embedded in natural language descriptions to compensate for sparse relational data, thereby improving embedding quality, especially for long-tail entities. A key tension here is the trade-off between the complexity introduced by integrating diverse data sources and the resulting gains in performance and interpretability.

A significant portion of the review will be dedicated to the evolution of KGE models in handling dynamic and complex knowledge. This includes Temporal Knowledge Graph Embedding (TKGE) models, which explicitly account for the time-varying nature of facts \cite{dasgupta2018, xu2019, xu2020, liu201918i, tang2020ufr, zhang2020s4x, sadeghian2021, lee2022hr9, fu2022df2, zhang2022muu, xie2023, li2023y5q, li2023, ji2024, wang2024, wang2024ime, zhang20243iw, han2024gaq, liu2024jz8, zhang2025ebv, liu20242zm, yang2024lwa, he2024vks, zhang2024ivc}. We will analyze how models like HyTE \cite{dasgupta2018} and ATiSE \cite{xu2019} represent time, comparing their approaches to capturing temporal validity and uncertainty. HyTE's use of hyperplanes for timestamps offers a geometric interpretation of temporal facts, while ATiSE's Gaussian distributions and additive time series decomposition provide a novel way to model the evolution and inherent uncertainty of entity/relation representations over time. This comparison will highlight the shift from static to dynamic knowledge representation, a critical paradigm change driven by the need for more realistic and context-aware AI systems. Furthermore, the review will explore models designed for hyper-relational data \cite{rosso2020, zhang20179i2} and those that learn disentangled representations \cite{wu2021}, addressing the multi-faceted nature of entities and relations. DisenKGAT \cite{wu2021}, for example, challenges the assumption of single, static entity representations by learning independent components, thereby enhancing interpretability and robustness, especially for complex relation types.

The synthesis and critical analysis throughout the review will focus on identifying recurring trade-offs (e.g., expressiveness vs. efficiency, interpretability vs. performance), methodological trends (e.g., from simple linear models to deep neural architectures), and unresolved debates (e.g., the optimal geometry for embedding spaces, the best negative sampling strategies \cite{shan2018, qian2021, madushanka2024}). We will systematically question assumptions, such as the generalizability of evaluation metrics or the implicit assumption of uniform confidence in KG facts, as addressed by works like \cite{shan2018} which proposes confidence-aware negative sampling for noisy KGs. The review will also highlight evaluation gaps, such as the lack of consistent benchmarks for certain complex KGE tasks or the limited focus on real-world deployment challenges, including parameter efficiency \cite{chen2023} and federated learning scenarios \cite{chen20226e4, zhang2024, zhang2024cjl, hu20230kr, zhu2023bfj, zhou2024}.

Finally, the review will conclude by outlining cutting-edge applications of KGE across various domains and discussing promising future directions. These include the integration of KGE with Large Language Models (LLMs) \cite{liu2024q3q, nie202499i}, advancements in explainable KGE \cite{yang2023, daruna2022dmk, kurokawa2021f4f}, robust KGE against adversarial attacks \cite{zhang20190zu, zhang20193g2, zhou2024}, and the development of more adaptive and efficient learning systems \cite{chen2023, zheng2024}. By providing a structured narrative that connects foundational principles to emerging trends, this review aims to offer a profound understanding of how KGE has transformed the utility of knowledge graphs in AI and where the field is headed next.