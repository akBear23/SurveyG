\section{Enriching KGE with Semantic Context, Rules, and Multi-modality}

The foundational Knowledge Graph Embedding (KGE) models, as discussed in preceding sections, primarily focus on learning representations from the structural triples $(h, r, t)$ within a knowledge graph (KG). While translational, rotational, and even advanced deep learning architectures (Sections 2, 3, and 4) have significantly enhanced the capacity to capture intricate structural patterns and relational semantics, they often operate under the implicit assumption that the triple structure alone provides sufficient information. However, real-world KGs are far richer than mere collections of triples; they are often accompanied by diverse auxiliary information, governed by logical rules, and can be enriched by multi-modal data sources such as textual descriptions and visual features. This section delves into advanced KGE paradigms that move beyond purely structural learning, incorporating these additional layers of semantic and contextual information to create more discriminative, meaningful, and robust embeddings.

The motivation for enriching KGE with external information stems from several critical limitations of structural-only approaches. Firstly, **data sparsity** is a pervasive issue in KGs, where many entities or relations have limited connections, making it difficult for models to learn robust representations solely from observed triples \cite{dai2020, choudhary2021}. Auxiliary data can provide crucial supplementary signals. Secondly, structural embeddings often lack **semantic richness and interpretability**. While a vector might capture a relation, it rarely conveys the nuances of entity types, attributes, or temporal validity that are readily available in other forms. For instance, knowing that "Barack Obama" is a "Person" and "President" is an "Occupation" adds significant semantic depth beyond just the triple (Barack Obama, hasOccupation, President). Thirdly, purely data-driven KGEs can suffer from **logical inconsistency** and struggle to incorporate **prior domain knowledge**. Logical rules, whether explicit or implicit, are fundamental to human reasoning and can enforce consistency or guide the embedding process, leading to more reliable inferences. Finally, the world is inherently **multi-modal**, and entities in KGs often correspond to textual descriptions, images, or other sensory data. Fusing these diverse modalities can provide a holistic understanding, mitigate sparsity, and enhance the semantic grounding of embeddings, especially in specialized domains like chemistry \cite{zhou2023} or healthcare \cite{zhu2022}.

This section explores three primary avenues for enriching KGE:
\begin{enumerate}
    \item **Incorporating Auxiliary Information and Entity Types**: This paradigm leverages structured metadata associated with entities and relations, such as entity types, hierarchical classifications, numerical attributes, and temporal validity. By explicitly integrating these features, KGE models can learn more discriminative embeddings that respect the inherent characteristics and constraints of the knowledge elements.
    \item **Rule-based and Constraint-driven Embedding**: This approach injects logical rules and constraints, either hard or soft, directly into the embedding learning process. These rules, often derived from domain expertise or automatically mined, enforce consistency, guide reasoning, and inject valuable prior knowledge, thereby improving the logical soundness and predictive accuracy of the embeddings.
    \item **Multi-modal KGE: Integrating Textual and Other Modalities**: This cutting-edge area focuses on fusing information from heterogeneous data sources beyond the graph structure itself, such as textual descriptions, visual features, and even audio. By leveraging powerful pre-trained language models (PLMs) and advanced fusion techniques, multi-modal KGE aims to overcome data sparsity, enhance semantic understanding, and enable richer cross-modal reasoning.
\end{enumerate}
Collectively, these enrichment strategies represent a significant evolution in KGE research, moving towards models that are not only capable of capturing complex structural patterns but also deeply grounded in the broader semantic, logical, and contextual fabric of knowledge. This shift is crucial for developing KGE models that are more accurate, interpretable, and applicable to the nuanced demands of real-world AI systems.

\subsection{Incorporating Auxiliary Information and Entity Types}

Traditional Knowledge Graph Embedding (KGE) models, particularly those based on translational or rotational principles (as discussed in Sections 2 and 3), often treat entities and relations as atomic symbols, represented by vectors in a continuous space. This approach, while effective for capturing structural patterns, overlooks a wealth of explicit semantic information available in many Knowledge Graphs (KGs). Auxiliary information, such as entity types, hierarchical structures, numerical attributes, and temporal validity, provides crucial context that can significantly enrich entity and relation representations, leading to more discriminative and meaningful embeddings.

**Context and Problem Solved**: The primary problem addressed by incorporating auxiliary information is the limited expressiveness and discriminative power of embeddings learned solely from structural triples. When entities or relations are sparse or ambiguous, their structural context alone might be insufficient to learn robust representations. Auxiliary data offers additional, often explicit, semantic cues that can resolve ambiguities, enforce consistency, and guide the embedding process towards more semantically aligned representations. For instance, knowing that "Paris" is a "City" and "France" is a "Country" helps differentiate the relation "locatedIn" from "capitalOf," even if both appear in similar structural contexts. This also helps mitigate the data sparsity challenge, as entities with few direct links might still have rich auxiliary descriptions.

**Mechanism and Core Innovations**: Approaches to integrate auxiliary information vary, but generally involve either concatenating auxiliary features with structural embeddings, projecting them into a shared space, or using attention mechanisms to weigh their relevance.

1.  **Entity Types and Hierarchies**: Entity types (e.g., Person, Organization, Location) and their hierarchical relationships (e.g., City $\subset$ Location) provide strong semantic signals.
    *   \cite{he2023} proposed a **type-augmented KGE framework** for knowledge graph completion, explicitly integrating entity type information to enhance embeddings. Their core innovation lies in designing a mechanism that allows type embeddings to influence entity embeddings, making them more discriminative based on their semantic categories. Similarly, \cite{wang2021} introduced **TransET**, a KGE model that incorporates entity types, demonstrating improved performance by leveraging this categorical information.
    *   Beyond simple types, some works consider **hierarchical structures**. \cite{zhang2018} explored KGE with hierarchical relation structure, arguing that relations often exist within a hierarchy, and incorporating this can refine their representations. More recently, \cite{wang2021dgy} presented a **Hierarchical Hyperbolic Neural Graph Embedding** model, which leverages hyperbolic geometry to naturally represent hierarchical structures, allowing for more accurate embeddings of entities and relations within a type hierarchy. This is further supported by \cite{lu2022bwo} with DensE, which uses an enhanced non-commutative representation for KGE with adaptive semantic hierarchy, and \cite{fang20243a4} with a low-dimensional gated hierarchical hyperbolic embedding.
    *   The concept of **ontologies**, which formally define types and their relationships, is also leveraged. \cite{xiang2021} proposed OntoEA for ontology-guided entity alignment, using ontology information to guide the joint KGE process. \cite{gutirrezbasulto2018oi0} analyzed the compatibility between vector space representations and rules derived from ontologies, highlighting the challenge of integrating symbolic ontological knowledge. Recent work like \cite{he2024y6o} focuses on generating ontologies via KGE query embedding, showcasing a bidirectional relationship.
    *   **Differentiation of Concepts and Instances**: \cite{lv2018} introduced a method for differentiating concepts and instances for KGE, recognizing that these distinct types of entities require different embedding strategies. This aligns with \cite{guan2019pr4}, which proposes KGE with concepts, further emphasizing the importance of type-level information.
    *   **Automated Type Representation**: \cite{niu2020uyy} developed AutoETER, an automated entity type representation with relation-aware attention, which learns how to best represent and integrate entity type information without manual feature engineering. This represents an evolution towards more adaptive and less human-dependent integration of auxiliary data.

2.  **Attributes (Numeric and Categorical)**: Entities often possess descriptive attributes (e.g., population for a city, birth year for a person).
    *   \cite{wu2018c4b} specifically addressed **KGE with Numeric Attributes of Entities**, demonstrating that incorporating these features can significantly enhance entity representations. This is typically achieved by encoding numerical attributes into vectors and integrating them with the structural embeddings.
    *   \cite{zhang2024} proposed integrating entity attributes for error-aware KGE, suggesting that attributes can also help in identifying and mitigating errors in the KG. Similarly, \cite{khan202236g} and \cite{khan20222j1} explored attribute-enhanced KGE for recommendation systems, emphasizing the role of semantic relevance attributed to entities.
    *   \cite{liu2019fcs} highlighted the importance of learning **high-order structural and attribute information** for enhancing KGE, often using attention networks to weigh attribute relevance.

3.  **Temporal Information**: Facts in KGs are often time-sensitive. Ignoring time leads to static and potentially inaccurate representations.
    *   \cite{dasgupta2018} introduced **HyTE (Hyperplane-based Temporally aware KGE)**, a pioneering work that explicitly models time by associating each timestamp with a hyperplane. This allows for temporal validity to be geometrically represented, enabling temporally-guided inference and prediction of temporal scopes for facts.
    *   Building on this, \cite{xu2019} proposed **ATiSE (Additive Time Series Embedding)**, which models the evolution of entity and relation representations as multi-dimensional additive time series (trend, seasonal, random components). Crucially, ATiSE represents embeddings as *Gaussian distributions* to capture temporal uncertainty, a significant advancement over deterministic temporal models. This idea was further explored by \cite{xu2020} with TeRo, a time-aware KGE via temporal rotation, and \cite{sadeghian2021} with ChronoR, which uses rotations for temporal embeddings.
    *   More recent works continue to refine temporal KGE. \cite{wang2024} introduced MADE, a multicurvature adaptive embedding for temporal KGE, and \cite{wang2024} proposed IME, integrating multi-curvature shared and specific embedding for temporal KGE. These models leverage complex geometric spaces to better capture temporal dynamics. Other notable contributions include \cite{lin2020} using tensor decomposition, \cite{li2023} with TeAST (Archimedean Spiral Timeline), \cite{ji2024} with fuzzy spatiotemporal embeddings, \cite{hou20237gt} with a timespan-aware graph attention model, and \cite{zhang2024ivc} with a cross-dimensional recurrent graph network.
    *   The field is also exploring **dynamic KGE** that can adapt to evolving KGs, as seen in \cite{krause2022th0} and \cite{sun2024} (using meta-learning for evolving service ecosystems). \cite{liu201918i} proposed context-aware temporal KGE, and \cite{zhang2020s4x} explored translating in time domain space. \cite{lee2022hr9} introduced THOR, a self-supervised temporal KGE model.

4.  **Contextual/Path Information and Disentangled Representations**: Beyond direct auxiliary features, the broader structural context, such as paths between entities or disentangled aspects of entities, can be seen as enriched auxiliary information.
    *   \cite{yang2023} introduced CKGE, a Contextualized KGE for explainable recommendation, which constructs a *meta-graph* for each talent-course pair, integrating contextualized neighbor semantics and high-order connections. This meta-graph acts as rich auxiliary input for a specialized Transformer.
    *   Path-based methods, like \cite{jia201870f} with path-specific KGE and \cite{zhou20216m0} with Path-RotatE, explicitly leverage multi-hop paths as contextual information to refine embeddings. \cite{jia20207dd} further improved this by using locally and globally attentive relation paths.
    *   \cite{wu2021} introduced **DisenKGAT**, a significant innovation that learns *disentangled entity representations*. This addresses the problem that entities often have multiple facets, and relations focus on distinct aspects. DisenKGAT uses relation-aware aggregation for *micro-disentanglement* and mutual information regularization for *macro-disentanglement*, leading to adaptive, robust, and interpretable embeddings. This moves beyond a single, monolithic embedding per entity to a more nuanced, multi-component representation.
    *   Other approaches include \cite{tang2019} with orthogonal relation transforms and graph context modeling, \cite{luo2015df2} with context-dependent KGE, and \cite{ning20219et} with LightCAKE for lightweight context-aware KGE. \cite{li2024z0e} uses contextual facts to guide generation for KGE completion.

**Comparative Framework**:
\begin{table}[h!]
\centering
\caption{Comparative Framework for Incorporating Auxiliary Information}
\label{tab:auxiliary_comparison}
\begin{tabularx}{\textwidth}{|l|X|X|X|X|}
\hline
\textbf{Auxiliary Type} & \textbf{Method Family} & \textbf{Core Mechanism} & \textbf{Advantages} & \textbf{Limitations} \\
\hline
\textbf{Entity Types/Hierarchies} & Type-augmented KGEs (\cite{he2023, wang2021, lv2018, ren2021muc, gao2023086, liu2024t05}); Hierarchical KGEs (\cite{zhang2018, wang2021dgy, lu2022bwo, he2024y6o, lu202436n, zhang2024yjo}) & Concatenation, projection, attention, hyperbolic geometry & Enhance discriminability, enforce semantic consistency, improve reasoning over categories & Requires explicit type/hierarchy data, potential for type noise, complexity of hierarchical modeling \\
\hline
\textbf{Attributes (Numeric/Categorical)} & Attribute-aware KGEs (\cite{wu2018c4b, zhang2024, khan202236g, liu2019fcs}) & Feature encoding (e.g., MLP for numeric), concatenation, attention & Provide rich descriptive context, mitigate sparsity, improve entity understanding & Requires attribute data, heterogeneity of attribute types, feature engineering complexity, potential for noise \\
\hline
\textbf{Temporal Information} & Temporal KGEs (\cite{dasgupta2018, xu2019, xu2020, sadeghian2021, li2023, wang2024, zhang2024ivc, he2024vks, han2024gaq, liu2024jz8, dong2024ijo, yang2024lwa, chen2024uld, zhang2025ebv, liu20242zm, huang2024t19}) & Time-aware transformations (hyperplanes, rotations), time series decomposition, Gaussian distributions, dedicated temporal encoders & Capture dynamic validity, enable temporal reasoning, predict future facts & Requires explicit timestamps/intervals, increased model complexity, challenges in modeling uncertainty and long-term trends \\
\hline
\textbf{Contextual/Path Information} & Path-based KGEs (\cite{jia201870f, zhou20216m0, jia20207dd}); Context-aware KGEs (\cite{yang2023, luo2015df2, ning20219et, wang2024d52, li2024z0e, liu2024yar, wang2024dea, li2021qr0, liu2024mji, pham20243mh, long2024soi}) & Path encoders (RNNs, Transformers), meta-graph construction, attention over neighbors & Capture multi-hop dependencies, provide richer context for ambiguous entities/relations & Increased computational cost for path enumeration/encoding, risk of noise from irrelevant paths, scalability issues for long paths \\
\hline
\textbf{Disentangled Representations} & DisenKGAT (\cite{wu2021}) & Relation-aware aggregation for micro-disentanglement, MI regularization for macro-disentanglement & Capture multi-faceted nature of entities, improve interpretability, adaptive to relation context & Requires careful design of disentanglement mechanisms, choice of component number (hyperparameter), computational cost of MI regularization \\
\hline
\end{tabularx}
\end{table}

**Critical Analysis**:
The integration of auxiliary information into KGE models represents a significant step towards more semantically rich and robust representations. The core innovation across these methods is to move beyond the simplistic view of entities and relations as atomic symbols and instead leverage their rich metadata. For instance, while earlier models like TransE \cite{bordes2013} would embed "Paris" as a single vector, type-augmented models \cite{he2023, wang2021} would ensure this vector is influenced by its "City" type, making it more distinct from a "Person" or "River" named "Paris."

These approaches succeed under conditions where the auxiliary data is available, accurate, and relevant to the task. For example, temporal KGEs like HyTE \cite{dasgupta2018} and ATiSE \cite{xu2019} achieve superior performance on datasets with explicit timestamps, demonstrating their ability to capture dynamic knowledge. ATiSE's innovation of modeling temporal uncertainty with Gaussian distributions \cite{xu2019} addresses a key theoretical limitation of deterministic temporal models, leading to more robust predictions. Similarly, DisenKGAT \cite{wu2021} excels when entities exhibit polysemy or multiple distinct facets, as its disentangled representations can capture these nuances, offering improved interpretability compared to monolithic embeddings.

However, several theoretical and practical limitations persist. A major theoretical challenge is the **heterogeneity of auxiliary data**. Integrating diverse types of information (e.g., categorical types, numerical attributes, temporal intervals) into a unified embedding space is non-trivial. Different modalities of auxiliary data may require distinct encoding strategies and fusion mechanisms, increasing model complexity. For instance, while \cite{wu2018c4b} shows the benefit of numeric attributes, their integration method might not be optimal for categorical types. Another limitation is the **assumption of auxiliary data availability and quality**. Many real-world KGs lack complete or accurate auxiliary information, limiting the applicability of these methods. Furthermore, the increased model complexity due to auxiliary data integration can lead to higher computational costs and more challenging hyperparameter tuning, as highlighted by \cite{lloyd2022} for general KGEs. For example, the mutual information regularization in DisenKGAT \cite{wu2021}, while theoretically sound for disentanglement, adds computational overhead.

Compared to structural-only KGEs, these methods generally offer higher accuracy and better reasoning capabilities, especially for tasks requiring fine-grained semantic understanding or temporal awareness. However, this comes at the cost of increased data requirements and model complexity. The field is moving towards more adaptive and automated ways of integrating auxiliary information, as seen in AutoETER \cite{niu2020uyy}, to reduce the burden of manual feature engineering and make these powerful techniques more accessible. The tension lies in balancing the desire for rich, context-aware embeddings with the practical constraints of data availability, model complexity, and computational efficiency.

\subsection{Rule-based and Constraint-driven Embedding}

While incorporating auxiliary semantic information enriches the content of embeddings, it does not inherently guarantee logical consistency or allow for the direct injection of prior domain knowledge. Rule-based and constraint-driven embedding approaches address this by integrating logical rules, either hard or soft, directly into the Knowledge Graph Embedding (KGE) learning process. This paradigm aims to bridge the gap between symbolic reasoning and sub-symbolic representations, enforcing consistency, improving reasoning, and leveraging expert knowledge.

**Context and Problem Solved**: The primary problem that rule-based and constraint-driven KGE seeks to solve is the inherent lack of logical consistency and the inability to inject prior knowledge in purely data-driven KGE models. Traditional KGEs learn statistical patterns from observed triples, but they often fail to capture implicit logical relationships (e.g., transitivity: if A is a `parentOf` B and B is a `parentOf` C, then A is a `grandparentOf` C). Without explicit guidance, embeddings might represent logically contradictory facts or fail to generalize to unseen but logically inferable triples. This limits their utility in applications requiring robust and explainable reasoning.

**Mechanism and Core Innovations**: The integration of rules and constraints typically involves modifying the KGE training objective function.

1.  **Soft Rules and Regularization**: Most approaches integrate rules as soft constraints, meaning they are encouraged but not strictly enforced. This is usually done through regularization terms added to the loss function.
    *   \cite{guo2017} proposed guiding KGE with **iterative guidance from soft rules**. Their approach iteratively mines rules from the KG and then uses these rules to generate additional "pseudo-positive" or "pseudo-negative" triples, or as a regularization term, to guide the embedding learning. This iterative refinement helps the model learn representations that are more consistent with the rules.
    *   Building on this, \cite{guo2020} introduced a method for KGE **preserving soft logical regularity**. This work focuses on ensuring that the embeddings adhere to logical regularities (e.g., symmetry, transitivity, inversion) by incorporating specific regularization terms that penalize deviations from these properties in the embedding space. This allows for a more principled way to inject logical knowledge.
    *   \cite{ding2018} demonstrated the effectiveness of **improving KGE using simple constraints**, showing that even basic logical constraints can significantly enhance embedding quality. These constraints might include properties like "functional" or "inverse functional" relations.
    *   \cite{li2020ek4} further explored **enhancing KGE with relational constraints**, highlighting how explicit constraints on relations can improve the discriminative power and logical consistency of embeddings.
    *   More recently, \cite{zhang2024fy0} proposed SimRE, which uses **simple contrastive learning with soft logical rules** for KGE. This approach combines the power of contrastive learning (pushing positives closer, negatives further) with the guidance of soft logical rules, leading to more robust and logically consistent embeddings.

2.  **Hard Rules and Rule-Guided Learning**: Some methods attempt to enforce rules more strictly or use rules to directly influence the sampling or generation of training data.
    *   \cite{wang20199fe} presented **Logic Rules Powered KGE**, which integrates logical rules more directly into the embedding process, often by modifying the scoring function or the sampling strategy to ensure rule adherence.
    *   \cite{zhao202095o} proposed **structure-augmented KGE for sparse data with rule learning**, where rules are not just constraints but also help in augmenting sparse data, thereby improving embedding quality in data-scarce scenarios.
    *   \cite{li2021tm6} introduced **rule-based data augmentation for KGE**, a practical approach where logical rules are used to generate synthetic training triples, effectively expanding the training data in a logically consistent manner. This is particularly useful for improving generalization and handling sparse relations.
    *   \cite{hong2020hyg} addressed **rule-enhanced noisy KGE via low-quality error detection**, demonstrating how rules can be used not only to enforce consistency but also to identify and mitigate noise in the KG, leading to more reliable embeddings.
    *   \cite{tang2022} developed RulE, a KGE model that performs **reasoning with rule embedding**. This approach embeds rules themselves into the same space as entities and relations, allowing for direct computation of rule satisfaction and inference within the embedding space. This is a powerful way to bridge symbolic and sub-symbolic reasoning.
    *   The work by \cite{yoon2016} on a translation-based KGE preserving logical properties of relations is an early example of embedding models designed with explicit logical considerations. Similarly, \cite{zhang2022eab} uses a **logical-default attention graph convolution neural network** for link prediction, showing how logical principles can be integrated into deep learning architectures.

**Comparative Framework**:
\begin{table}[h!]
\centering
\caption{Comparative Framework for Rule-based and Constraint-driven KGE}
\label{tab:rules_comparison}
\begin{tabularx}{\textwidth}{|l|X|X|X|X|}
\hline
\textbf{Approach Family} & \textbf{Key Papers} & \textbf{Rule Integration Mechanism} & \textbf{Advantages} & \textbf{Limitations} \\
\hline
\textbf{Soft Rule Regularization} & \cite{guo2017, guo2020, ding2018, li2020ek4, zhang2024fy0} & Add regularization terms to loss function, iterative guidance & Flexible, allows for some rule violations (robust to noisy rules), improves logical consistency & Weaker enforcement, requires careful tuning of regularization weights, rules must be formulated for embedding space \\
\hline
\textbf{Rule-Guided Data Augmentation/Sampling} & \cite{zhao202095o, li2021tm6, hong2020hyg} & Generate synthetic triples, guide negative sampling, detect noisy triples & Addresses data sparsity, improves generalization, can enhance data quality & Relies on correctness of rules, potential for error propagation if rules are flawed, computational cost of rule application \\
\hline
\textbf{Rule Embedding/Direct Integration} & \cite{wang20199fe, tang2022, yoon2016, zhang2022eab} & Embed rules directly, modify scoring functions, integrate into GNNs & Stronger logical enforcement, enables direct rule-based reasoning in embedding space & Increased model complexity, challenges in representing complex rules, potential for over-constraining embeddings \\
\hline
\end{tabularx}
\end{table}

**Critical Analysis**:
Rule-based and constraint-driven KGE models represent a crucial step towards making KGE more robust, logically consistent, and aligned with human reasoning. The core innovation is to explicitly inject symbolic knowledge into the sub-symbolic embedding space, addressing the fundamental limitation of purely statistical models that might learn spurious correlations or overlook crucial logical dependencies. For example, while a standard KGE might learn that (Germany, hasCapital, Berlin) is true, it might not implicitly understand the inverse (Berlin, isCapitalOf, Germany) without explicit training or a rule-based mechanism. Models like \cite{guo2020} and \cite{tang2022} directly address this by incorporating logical regularities or embedding rules themselves.

These approaches succeed when high-quality, relevant logical rules are available, either from domain experts or through automated rule mining. They are particularly effective in domains where logical consistency is paramount (e.g., medical KGs \cite{gong2020b2k}) or where data is sparse, and rules can provide valuable inductive biases. The iterative guidance from soft rules \cite{guo2017} demonstrates how rules can progressively refine embeddings.

However, significant theoretical and practical limitations exist. A major theoretical challenge is the **"rule acquisition bottleneck"**: obtaining a comprehensive set of accurate logical rules for large-scale KGs is often difficult and labor-intensive. Automated rule mining can be noisy and error-prone, potentially introducing incorrect biases into the embeddings, as implicitly acknowledged by \cite{hong2020hyg}'s focus on error detection. Another challenge is the **integration of discrete symbolic logic into continuous vector spaces**. While regularization terms offer a soft way to bridge this, they can be difficult to tune, and the degree to which rules are truly "enforced" can vary. Over-constraining the embedding space with too many or too strict rules can limit the model's flexibility and ability to discover novel patterns, leading to underfitting. \cite{gutirrezbasulto2018oi0} explicitly discusses the compatibility challenges between vector space representations and rules.

Practically, rule-based methods often incur **increased computational costs**. Mining rules, applying them during training (especially for data augmentation \cite{li2021tm6}), and computing complex regularization terms can significantly slow down the learning process. The interpretability of how rules influence the final embeddings can also be challenging, as the interaction between the structural learning objective and the rule-based constraints is complex.

Compared to auxiliary information methods (Section 5.1), rule-based approaches focus on enforcing *relationships* and *consistency* rather than merely enriching entity/relation content. While auxiliary data provides semantic context, rules provide logical structure. The tension lies in finding the right balance between allowing the model to learn from data and imposing prior logical knowledge, ensuring that rules enhance rather than hinder the discovery of new, valid patterns. The field is moving towards more sophisticated ways of learning and integrating rules, potentially through differentiable rule learning, to overcome these limitations.

\subsection{Multi-modal KGE: Integrating Textual and Other Modalities}

The previous sections highlighted how structured auxiliary information and logical rules can enrich KGE. However, knowledge is not confined to structured triples or explicit metadata. Real-world entities and relations are often described, depicted, or otherwise represented across diverse modalities such as text, images, and even audio. Multi-modal KGE aims to fuse information from these heterogeneous sources with structural embeddings, providing a holistic understanding that can overcome data sparsity, enhance semantic understanding, and enable richer reasoning, especially in specialized domains.

**Context and Problem Solved**: The core problem addressed by multi-modal KGE is the inherent incompleteness and data sparsity of KGs, coupled with the limited semantic depth achievable from structural data alone. Many entities in KGs have few direct links, making it challenging for purely structural KGE models to learn robust representations. However, these entities might have rich textual descriptions (e.g., Wikipedia articles), associated images, or other sensory data. Multi-modal KGE leverages these complementary data sources to: (1) **Alleviate data sparsity** by providing alternative signals for entities with few structural connections; (2) **Enhance semantic understanding** by grounding embeddings in real-world perceptual data; and (3) **Enable cross-modal reasoning**, allowing inference that spans different types of information. This is particularly crucial for specialized domains where knowledge is often distributed across various formats, such as chemistry \cite{zhou2023} or healthcare \cite{zhu2022}.

**Mechanism and Core Innovations**: Multi-modal KGE involves encoding information from different modalities into a common embedding space and then fusing these representations with the structural KGE.

1.  **Textual Information Integration**: Textual descriptions are the most common auxiliary modality, often providing rich semantic context for entities and relations.
    *   \cite{xiao2016} proposed **SSP (Semantic Space Projection)**, an early work that integrates textual descriptions of entities into KGE by projecting text embeddings into the same semantic space as structural embeddings. This allows the text to inform and refine the entity representations.
    *   \cite{nie20195gc} explored KGE via **reasoning over entities, relations, and text**, demonstrating how textual information can be jointly learned with structural embeddings to improve reasoning capabilities.
    *   \cite{shen2022} introduced **Joint Language Semantic and Structure Embedding** for knowledge graph completion, emphasizing the importance of jointly learning from both modalities.
    *   **Leveraging Pre-trained Language Models (PLMs)**: The advent of powerful PLMs like BERT has revolutionized textual information integration.
        *   \cite{zhou2023} presented "Marie and BERT," a KGE-based Question Answering system for chemistry, which leverages BERT to encode textual questions and entity descriptions, then integrates these with KGE for robust QA. This highlights the power of PLMs in capturing nuanced semantic meanings from text.
        *   \cite{djeddi2023g71} advanced drug-target interaction prediction by integrating KGE and ProtBert pretraining, demonstrating how specialized PLMs can extract domain-specific textual features.
        *   \cite{nie202499i} explored embedding Chain-of-Thought into LLMs for KGE construction, suggesting a role for LLMs not just in encoding but also in reasoning and knowledge generation. Similarly, \cite{liu2024q3q} proposed a joint KG and Large Language Model for fault diagnosis, showcasing the synergy between structured knowledge and generative text models.
        *   \cite{zhang2024h9k} used an LLM-enhanced embedding approach for KG accuracy evaluation, further illustrating the broad utility of PLMs.
    *   Other applications include using financial news for stock price prediction \cite{liu2018kvd}, combining text embeddings for academic search engines \cite{mai2018u0h}, and zero-shot text classification via KGE \cite{chen2022mxn}. \cite{lu20206x1} provides a comprehensive survey on utilizing textual information in KGE.

2.  **Visual Information Integration**: Images associated with entities can provide rich perceptual features.
    *   \cite{zhu2022} explored **multimodal reasoning based on KGE for specific diseases**, where visual features (e.g., from medical images) are fused with KGE to enhance diagnostic accuracy.
    *   \cite{zhang2023} introduced **Modality-Aware Negative Sampling for Multi-modal KGE**, addressing the challenge of generating effective negative samples when dealing with heterogeneous modalities, a crucial aspect for robust training.
    *   \cite{zhu2022o32} proposed DFMKE, a **dual fusion multi-modal KGE framework for entity alignment**, which effectively combines visual and structural information to align entities across different KGs.
    *   \cite{liu2024zr9} presented MMGK, a **Multimodality Multiview Graph Representations and Knowledge Embedding** for Mild Cognitive Impairment diagnosis, showcasing the fusion of various visual and graph-based features.

3.  **Other Modalities and Fusion Strategies**: The concept extends to any data type that can be embedded.
    *   **Spatio-temporal data**: \cite{liu2021wqa} mined urban flow patterns by fusing multi-source heterogeneous data (e.g., traffic, weather) with KGE. Similarly, \cite{zhao2020o6z} focused on urban multi-source spatio-temporal data analysis.
    *   **Multi-source fusion**: \cite{li2024gar} used attention-based learning for predicting drug-drug interactions based on multisource fusion information, demonstrating the power of combining diverse data streams.
    *   **Fusion Architectures**: The key challenge is how to effectively combine these diverse modalities. Early fusion involves concatenating raw features, while late fusion combines predictions from separate modal encoders. Intermediate fusion, often using attention mechanisms or cross-modal transformers, is common, allowing modalities to interact at various levels. For example, \cite{zhang2025ebv} proposes integrating LLMs and MÃ¶bius Group Transformations for temporal KGE, a complex multi-modal, multi-geometric fusion.

**Comparative Framework**:
\begin{table}[h!]
\centering
\caption{Comparative Framework for Multi-modal KGE}
\label{tab:multimodal_comparison}
\begin{tabularx}{\textwidth}{|l|X|X|X|X|}
\hline
\textbf{Modality Type} & \textbf{Method Family} & \textbf{Core Mechanism} & \textbf{Advantages} & \textbf{Limitations} \\
\hline
\textbf{Textual Information} & SSP (\cite{xiao2016}); Joint Semantic-Structural (\cite{shen2022, nie20195gc}); PLM-enhanced (\cite{zhou2023, djeddi2023g71, nie202499i, liu2024q3q, zhang2024h9k, zhang2025ebv}) & Text encoders (CNN, RNN, Transformer), projection, concatenation, cross-modal attention & Overcomes sparsity, provides rich semantic context, enables text-based reasoning & Requires high-quality text, modality gap (aligning text/structural embeddings), computational cost of PLMs \\
\hline
\textbf{Visual Information} & Multimodal Reasoning (\cite{zhu2022}); Modality-Aware Sampling (\cite{zhang2023}); Dual Fusion (\cite{zhu2022o32}); Multiview Graph Reps (\cite{liu2024zr9}) & Image encoders (CNN), projection, attention, specialized fusion networks & Grounds embeddings in perceptual data, useful for visual entities, enhances entity alignment & Requires high-quality images, large datasets, significant computational resources, modality gap \\
\hline
\textbf{Other Modalities (Spatio-temporal, etc.)} & Multi-source Data Fusion (\cite{liu2021wqa, zhao2020o6z, li2024gar}) & Specialized encoders for each modality, concatenation, attention, graph neural networks & Comprehensive understanding, leverages diverse data streams for specific applications & High heterogeneity challenges, complex fusion architectures, data synchronization issues, scalability \\
\hline
\end{tabularx}
\end{table}

**Critical Analysis**:
Multi-modal KGE represents the most comprehensive approach to enriching knowledge representations by leveraging the full spectrum of available data. The core innovation is the ability to synthesize information from disparate sources, creating embeddings that are deeply grounded in both symbolic structure and perceptual reality. This directly addresses the fundamental problem of data sparsity in KGs, as entities with few structural links can still be richly described by their associated text or images. For instance, an obscure historical figure might have limited triples but a detailed biography, which PLM-enhanced KGEs \cite{zhou2023, nie202499i} can effectively leverage.

These approaches succeed when there is a meaningful correlation between different modalities and when high-quality multi-modal data is available. They are particularly effective in specialized domains like biomedicine \cite{zhu2022, djeddi2023g71} or e-commerce, where entities often have rich textual descriptions and visual representations. The use of pre-trained language models (PLMs) has been a game-changer, as they provide powerful, context-aware textual embeddings that significantly reduce the "modality gap" between text and structural representations.

However, multi-modal KGE faces substantial theoretical and practical limitations. The primary theoretical challenge is the **modality gap**: effectively aligning and fusing representations from fundamentally different data types (e.g., symbolic triples, continuous text, pixel arrays) into a coherent embedding space. While various fusion strategies exist, determining the optimal interaction between modalities remains an open research question. Simply concatenating embeddings might lead to high-dimensional, noisy representations, whereas complex attention mechanisms can be difficult to interpret. Another limitation is the **heterogeneity and potential noise of multi-modal data**. Textual descriptions can be ambiguous, images might be irrelevant, and aligning these to specific KG entities is non-trivial. \cite{zhang2023}'s work on modality-aware negative sampling highlights the challenges of handling noise and imbalance in multi-modal contexts.

Practically, multi-modal KGE models are often **computationally expensive** due to the need for multiple encoders (one for each modality), complex fusion networks, and potentially large pre-trained models. This leads to increased training time, memory footprint, and deployment challenges, especially for large-scale KGs. Data acquisition and preprocessing for multiple modalities can also be a significant bottleneck.

Compared to auxiliary information (Section 5.1) and rule-based methods (Section 5.2), multi-modal KGE offers the most holistic understanding but also introduces the highest complexity and data requirements. While auxiliary information focuses on structured metadata and rules on logical consistency, multi-modal KGE aims for a comprehensive semantic grounding across all available data forms. The tension lies in effectively integrating these diverse, often unstructured, data sources without overwhelming the model or introducing excessive noise, while also ensuring scalability and interpretability. The field is actively exploring more efficient fusion architectures, self-supervised pre-training across modalities, and adaptive mechanisms to make multi-modal KGE more robust and practical.