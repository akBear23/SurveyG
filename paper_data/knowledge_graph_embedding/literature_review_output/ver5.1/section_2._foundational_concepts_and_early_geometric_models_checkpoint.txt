\section{2. Foundational Concepts and Early Geometric Models}
The transition from symbolic knowledge representation to continuous vector spaces marks a pivotal paradigm shift in the field of Knowledge Graphs (KGs), driven by the imperative to overcome the inherent limitations of discrete symbols, as discussed in Section 1.2 \cite{wang2017zm5, dai2020}. This section lays the groundwork for understanding Knowledge Graph Embedding (KGE) by introducing its fundamental concepts, particularly the representation of entities and relations in low-dimensional vector spaces. It then delves into the pioneering 'translational distance models,' which established a foundational geometric paradigm for KGE. These early models, including TransE, TransH, and TransR, significantly advanced the field by offering efficient ways to capture basic relational patterns, thereby enabling scalable link prediction and other downstream tasks. We will critically analyze their core mechanisms, comparative strengths, and inherent limitations, underscoring the continuous drive for enhanced representational power in KGE.

The core idea behind KGE is to embed entities and relations into a continuous vector space, typically $\mathbb{R}^k$, where $k$ is the dimension of the embedding. Each entity $e \in \mathcal{E}$ (the set of entities) is represented by a vector $\mathbf{e} \in \mathbb{R}^k$, and each relation $r \in \mathcal{R}$ (the set of relations) is also represented by a vector $\mathbf{r} \in \mathbb{R}^k$ or a matrix/tensor. The goal is to learn these embeddings such that the structural and semantic properties of the KG are preserved. Specifically, for a given triple $(h, r, t)$ where $h$ is the head entity, $r$ is the relation, and $t$ is the tail entity, a scoring function $f_r(h, t)$ is defined. This function measures the plausibility of the triple, aiming for a low score for valid triples and a high score for invalid ones. The embeddings are learned by minimizing a margin-based ranking loss or a logistic loss over observed (positive) and corrupted (negative) triples \cite{wang2017zm5, dai2020}. Negative sampling, where valid triples are corrupted by replacing the head or tail entity, is a crucial technique in this process, though its effectiveness and biases have been subjects of ongoing research \cite{shan2018, qian2021, madushanka2024}. For instance, \cite{shan2018} highlights that most KGE models implicitly assume uniform confidence in KG facts, which is often violated in noisy real-world KGs. They propose a confidence-aware negative sampling method to address this, demonstrating a critical tension between idealized model assumptions and practical data characteristics.

The early geometric models, particularly the translational family, conceptualized relations as operations that transform entity embeddings in the vector space. This geometric intuition provided a simple yet powerful framework for modeling relational patterns. TransE \cite{jia2015} pioneered this approach by proposing that for a valid triple $(h, r, t)$, the embedding of the head entity $\mathbf{h}$ plus the embedding of the relation $\mathbf{r}$ should be approximately equal to the embedding of the tail entity $\mathbf{t}$ (i.e., $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$). This elegant formulation offered a computationally efficient way to capture basic relational patterns, making it a cornerstone for subsequent KGE research. However, its simplicity also introduced limitations, particularly in handling complex relation types such as one-to-many (1-N), many-to-one (N-1), and many-to-many (N-N) relations, as well as symmetric relations. These limitations spurred immediate extensions like TransH \cite{wang2014} and TransR \cite{lin2015}, which sought to enhance representational power by introducing more flexible geometric transformations, such as projecting entities onto relation-specific hyperplanes or spaces.

The continuous drive for enhanced representational power led to further innovations, moving beyond strict translational assumptions. TransD \cite{ji2015} introduced dynamic mapping matrices, allowing entities to have distinct projection vectors when acting as head or tail entities for different relations, thereby increasing expressiveness. ManifoldE \cite{xiao2015} took a different approach by expanding entity representations from single points to manifolds, offering a more flexible geometric form to capture richer semantic information and address the limitations of rigid point-based embeddings. This evolution from simple vector addition to more complex geometric transformations—hyperplanes, dynamic matrices, and manifolds—reflects a fundamental tension in KGE research: balancing model simplicity and computational efficiency with the need to capture the intricate, diverse, and often ambiguous semantics of real-world knowledge graphs. The parameter efficiency of these models also became a concern, as highlighted by \cite{chen2023}, where conventional KGEs can lead to colossal parameter counts, hindering deployment in resource-constrained environments. This section will critically examine these foundational models, their contributions, and the challenges they posed, setting the stage for more advanced KGE paradigms.

\subsection{Core Principles of Knowledge Graph Embedding}
The foundational principle of Knowledge Graph Embedding (KGE) is to represent entities and relations as continuous, low-dimensional vectors (embeddings) in a latent semantic space, typically $\mathbb{R}^k$ \cite{wang2017zm5, dai2020, choudhary2021, ge2023}. This transformation from discrete symbols to dense vectors is crucial for bridging the gap between symbolic knowledge representation and modern machine learning algorithms, which operate most effectively on continuous data. The primary objective is to learn these embeddings such that the inherent structure and semantics of the Knowledge Graph (KG) are preserved, enabling computational efficiency and generalization.

At its heart, KGE operates on the assumption that a valid triple $(h, r, t)$ (head entity, relation, tail entity) should exhibit a particular geometric or algebraic relationship between their respective embeddings $(\mathbf{h}, \mathbf{r}, \mathbf{t})$ in the vector space. This relationship is quantified by a **scoring function**, $f_r(h, t)$, which assigns a plausibility score to each triple. The design of this scoring function is a defining characteristic of different KGE models. For instance, in translational models, the score is often based on a distance metric, while in semantic matching models, it might involve dot products or bilinear forms. The lower the score for a positive triple and the higher for a negative (corrupted) triple, the better the embeddings are considered to be.

The learning process for these embeddings typically involves minimizing a **loss function**, which aims to maximize the scores of positive triples while minimizing the scores of negative triples. A common approach is the **margin-based ranking loss**, exemplified by:
$L = \sum_{(h,r,t) \in S} \sum_{(h',r',t') \in S'} \max(0, \gamma + f_r(h,t) - f_{r'}(h',t'))$
where $S$ is the set of positive triples, $S'$ is the set of negative triples, and $\gamma$ is a margin hyperparameter \cite{wang2017zm5}. The selection of an appropriate margin is critical, and as highlighted by \cite{jia2015} with their TransA model, a globally fixed margin can be suboptimal for heterogeneous KGs, leading to the need for adaptive margin determination. This demonstrates a tension between model simplicity and the need to capture local structural nuances.

A crucial component of training KGE models is **negative sampling** \cite{wang2017zm5, dai2020, madushanka2024}. Since KGs are typically sparse, explicitly enumerating all negative triples is infeasible. Instead, negative triples are generated by corrupting positive triples, usually by replacing either the head or tail entity with a randomly chosen entity from the KG. For example, for a positive triple $(h, r, t)$, negative samples could be $(h', r, t)$ or $(h, r, t')$. The quality of negative samples significantly impacts the learning process. Naive uniform negative sampling can be inefficient, as many sampled negative triples might be easily distinguishable from positive ones. This led to more sophisticated strategies, such as **self-adversarial negative sampling** \cite{sun2018}, which samples negative triples that are harder to distinguish, thereby forcing the model to learn finer distinctions. \cite{sun2018} introduces $\epsilon$-truncated uniform negative sampling, limiting the sampling scope to $s$-nearest neighbors to generate more challenging negatives, directly addressing the inefficiency of arbitrary sampling. This evolution in negative sampling techniques reflects a recurring pattern in KGE research: the refinement of training methodologies to improve learning efficiency and embedding quality.

The embeddings themselves are typically initialized randomly and then iteratively updated using optimization algorithms like Stochastic Gradient Descent (SGD) or Adam. The dimension $k$ of the embedding space is a crucial hyperparameter, balancing representational capacity with computational cost and the risk of overfitting. A higher dimension allows for more information to be encoded but requires more parameters and computational resources \cite{lloyd2022}. The work by \cite{lloyd2022} empirically demonstrates the substantial variability in hyperparameter sensitivities, including embedding dimension, across different KGs, suggesting that optimal tuning strategies are highly dataset-specific. This finding challenges the implicit assumption that a universal set of hyperparameters exists for all KGE tasks and datasets.

The learned embeddings offer several advantages:
\begin{enumerate}
    \item \textbf{Generalization and Link Prediction}: By capturing latent semantic relationships, KGE models can predict missing links (i.e., infer new facts) by identifying plausible connections in the embedding space \cite{rossi2020}. This addresses the inherent incompleteness of KGs.
    \item \textbf{Computational Efficiency}: Vector operations are highly optimized, allowing KGE models to handle large-scale KGs more efficiently than symbolic reasoning systems \cite{chen2023}. However, parameter efficiency remains a concern, as highlighted by \cite{chen2023}, where conventional KGE models can lead to colossal parameter counts (e.g., 123 million for RotatE on YAGO3-10). Their proposed Entity-Agnostic Representation Learning (EARL) directly addresses this by learning representations that do not scale linearly with the number of entities, challenging the implicit assumption that each entity requires a unique, directly learned embedding.
    \item \textbf{Integration with Machine Learning}: The continuous vector representations serve as rich features for various downstream ML tasks, such as question answering \cite{huang2019, zhou2023}, recommendation systems \cite{yang2023}, and entity alignment \cite{sun2018}. For example, \cite{yang2023}'s CKGE leverages contextualized KGE for explainable talent training course recommendation, demonstrating how embeddings can capture motivation-aware information for more nuanced and interpretable outputs.
\end{enumerate}
In essence, the core principles of KGE revolve around transforming discrete knowledge into a continuous, geometrically interpretable space, enabling efficient learning and inference. This foundational shift paved the way for the development of various model families, starting with the pioneering translational models.

\subsection{Translational Models: From TransE to TransH and TransR}
The translational distance models represent a seminal family of Knowledge Graph Embedding (KGE) approaches, establishing a powerful geometric intuition for modeling relations. This family, beginning with TransE, conceptualizes relations as "translations" between entity embeddings in a low-dimensional vector space. This section traces the evolution within this family, from the foundational TransE to its more expressive successors, TransH and TransR, highlighting their innovations, capabilities, and limitations.

\textbf{TransE (Translating Embeddings)} \cite{bordes2013}:
*   **Context and Problem Solved**: TransE was one of the first and most influential KGE models, addressing the problem of efficiently learning embeddings for large-scale KGs to perform link prediction. It aimed to capture the basic relational patterns (e.g., "capitalOf" as a translation from "France" to "Paris") in a simple and scalable manner. Prior methods often struggled with scalability or lacked a clear geometric interpretation of relations.
*   **Core Innovation**: Its core innovation lies in its elegant and intuitive scoring function: $f(h, r, t) = \|\mathbf{h} + \mathbf{r} - \mathbf{t}\|_{L_1/L_2}$. For a valid triple $(h, r, t)$, it posits that the embedding of the head entity $\mathbf{h}$ plus the embedding of the relation $\mathbf{r}$ should be close to the embedding of the tail entity $\mathbf{t}$. This makes relations act as translation vectors in the embedding space.
*   **Conditions for Success**: TransE performs well on datasets where relations primarily exhibit one-to-one (1-1) mapping properties. Its simplicity makes it computationally efficient and scalable for very large KGs.
*   **Theoretical Limitations**: The primary theoretical limitation of TransE is its inability to effectively model complex relation types, specifically one-to-many (1-N), many-to-one (N-1), and many-to-many (N-N) relations, as well as symmetric relations. For instance, if "Obama" has two children, "Malia" and "Sasha," and "hasChild" is the relation, TransE would try to make $\mathbf{Obama} + \mathbf{hasChild} \approx \mathbf{Malia}$ and $\mathbf{Obama} + \mathbf{hasChild} \approx \mathbf{Sasha}$. This forces $\mathbf{Malia}$ and $\mathbf{Sasha}$ to be very close in the embedding space, which is often semantically incorrect. Similarly, for symmetric relations (e.g., "friendOf"), $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$ implies $\mathbf{t} + \mathbf{r} \approx \mathbf{h}$, which means $\mathbf{r} \approx -\mathbf{r}$, forcing $\mathbf{r}$ to be a zero vector, losing its meaning.
*   **Practical Limitations**: While efficient, its representational power is limited, leading to suboptimal performance on KGs rich in complex relations. The choice of distance metric ($L_1$ or $L_2$) and the margin hyperparameter are crucial and often require extensive tuning \cite{lloyd2022}.

**TransH (Translating on Hyperplanes)** \cite{wang2014}:
*   **Context and Problem Solved**: TransH was proposed to address TransE's limitations with 1-N, N-1, and N-N relations. It recognized that a single vector for an entity might be insufficient when it participates in different relations with different "aspects."
*   **Core Innovation**: Instead of representing relations as direct translation vectors between entity embeddings, TransH models each relation $r$ as a **hyperplane** (defined by its normal vector $\mathbf{w}_r$) and a **translation vector** $\mathbf{d}_r$ on that hyperplane. Entities are first projected onto this relation-specific hyperplane before the translation operation. The scoring function becomes $f(h, r, t) = \|\mathbf{h}_{\perp} + \mathbf{d}_r - \mathbf{t}_{\perp}\|_{L_1/L_2}$, where $\mathbf{h}_{\perp} = \mathbf{h} - \mathbf{w}_r^\top \mathbf{h} \mathbf{w}_r$ and $\mathbf{t}_{\perp} = \mathbf{t} - \mathbf{w}_r^\top \mathbf{t} \mathbf{w}_r$. This allows an entity to have different "projected" representations for different relations, mitigating the issue of forcing multiple tail entities to be close.
*   **Conditions for Success**: TransH significantly improves performance on KGs with complex relations compared to TransE. It offers a more flexible geometric interpretation without a drastic increase in computational complexity.
*   **Theoretical Limitations**: While better than TransE, TransH still assumes that entities and relations lie in the same semantic space. The projection mechanism might not be expressive enough to capture highly diverse semantic contexts, especially for relations with very different characteristics. The normal vector $\mathbf{w}_r$ and translation vector $\mathbf{d}_r$ are still learned independently for each relation, which can lead to issues with sparse relations.
*   **Practical Limitations**: It introduces more parameters than TransE (a normal vector and a translation vector per relation), slightly increasing training complexity. The projection operation adds a small computational overhead.

**TransR (Translating in Relation Space)** \cite{lin2015}:
*   **Context and Problem Solved**: Building on TransH, TransR further enhances expressiveness by recognizing that entities and relations should not necessarily reside in the same semantic space. It addresses the problem that entities and relations are fundamentally different types of objects and should be embedded in distinct spaces.
*   **Core Innovation**: TransR proposes to embed entities in an entity space $\mathbb{R}^k$ and relations in a separate relation space $\mathbb{R}^d$. For each relation $r$, a **relation-specific projection matrix** $\mathbf{M}_r \in \mathbb{R}^{d \times k}$ is introduced. Head and tail entities are first projected from the entity space to the relation space: $\mathbf{h}_r = \mathbf{h}\mathbf{M}_r$ and $\mathbf{t}_r = \mathbf{t}\mathbf{M}_r$. The translation then occurs in this relation-specific space: $f(h, r, t) = \|\mathbf{h}_r + \mathbf{r} - \mathbf{t}_r\|_{L_1/L_2}$. This allows for more flexible and context-dependent entity representations for each relation.
*   **Conditions for Success**: TransR generally outperforms both TransE and TransH, especially on KGs with diverse and complex relations, as it provides a more nuanced way to model how entities interact under different relational contexts.
*   **Theoretical Limitations**: The introduction of a projection matrix for each relation significantly increases the number of parameters, potentially leading to overfitting for sparse relations. The projection operation itself is linear, which might still limit its ability to capture highly non-linear semantic transformations.
*   **Practical Limitations**: The increased parameter count (a $d \times k$ matrix per relation) and the matrix multiplication operations make TransR computationally more expensive and memory-intensive than TransE and TransH. This can be a significant bottleneck for very large KGs or KGs with a vast number of unique relations.

\textbf{Comparative Framework and Evolution}:
The translational models represent a clear evolutionary path driven by the need for greater expressiveness to handle complex relational patterns.
\begin{itemize}
    \item \textbf{TransE} (Papers like \cite{jia2015} build on this) is the simplest, assuming entities and relations exist in a single, unified space, and relations are direct translations. It excels in efficiency and 1-1 relations but struggles with 1-N/N-1/N-N and symmetric relations due to its rigid geometric assumption.
    \item \textbf{TransH} \cite{wang2014} introduces relation-specific hyperplanes, allowing entities to have different "roles" or "aspects" depending on the relation. This addresses the 1-N/N-1/N-N problem by projecting entities, thereby relaxing the strict point-to-point translation.
    \item \textbf{TransR} \cite{lin2015} takes this a step further by embedding entities and relations in *separate* spaces and using relation-specific projection matrices. This provides the most flexible entity representation within this family, as entities are transformed into a relation-specific semantic space before translation. This directly addresses the implicit assumption in TransE and TransH that entities and relations are semantically comparable in the same vector space.
\end{itemize}
This evolution showcases a recurring trade-off: increasing model complexity and parameter count for enhanced representational power. While TransE is highly efficient, its limitations spurred the development of more expressive, albeit more computationally demanding, models. The empirical evidence consistently shows TransR outperforming TransE and TransH on link prediction tasks across various benchmarks, demonstrating the value of its increased expressiveness \cite{asmara2023}. However, this comes at the cost of higher training time and memory footprint, a practical limitation that subsequent research often seeks to mitigate.

\subsection{Early Extensions: Dynamic Mappings and Manifold Embeddings}
The foundational translational models, while groundbreaking, revealed inherent limitations in capturing the full spectrum of relational semantics within Knowledge Graphs. This spurred immediate research into extensions that sought to enhance representational power by introducing more flexible geometric transformations. Two notable directions in this early phase were models incorporating dynamic mapping matrices and those expanding entity representations to manifolds.

\textbf{TransD (Translating with Dynamic Mapping Matrix)} \cite{ji2015}:
*   **Context and Problem Solved**: TransD emerged as a direct response to the limitations of TransR, particularly its large number of parameters (a full projection matrix for each relation) and its potential for overfitting on sparse relations. TransR's projection matrices $\mathbf{M}_r$ were relation-specific but entity-agnostic, meaning the same matrix was applied regardless of the specific head or tail entity. TransD aimed to make these projections more dynamic and entity-aware.
*   **Core Innovation**: TransD introduces **dynamic mapping matrices** that are constructed from entity and relation vectors. Instead of a fixed projection matrix $\mathbf{M}_r$ for each relation, TransD learns two projection vectors for each entity ($h, t$) and relation ($r$): $\mathbf{h}, \mathbf{t}, \mathbf{r} \in \mathbb{R}^k$ and $\mathbf{h}_p, \mathbf{t}_p, \mathbf{r}_p \in \mathbb{R}^k$. The projection matrix for a head entity $\mathbf{M}_{rh}$ is dynamically constructed using $\mathbf{r}_p$ and $\mathbf{h}_p$, and similarly for the tail entity $\mathbf{M}_{rt}$ using $\mathbf{r}_p$ and $\mathbf{t}_p$. Specifically, the projected entity embeddings are $\mathbf{h}_r = \mathbf{h} + \mathbf{h}_p \mathbf{r}_p^\top \mathbf{h}$ and $\mathbf{t}_r = \mathbf{t} + \mathbf{t}_p \mathbf{r}_p^\top \mathbf{t}$. The scoring function remains translational in the projected space: $f(h, r, t) = \|\mathbf{h}_r + \mathbf{r} - \mathbf{t}_r\|_{L_1/L_2}$. This allows the projection to be specific to both the relation *and* the entity involved, making it more expressive than TransR while using fewer parameters.
*   **Conditions for Success**: TransD excels in scenarios where entities exhibit diverse roles across different relations, requiring highly context-dependent projections. It offers a good balance between expressiveness and parameter efficiency compared to TransR, making it more robust to sparse relations.
*   **Theoretical Limitations**: While more flexible, the dynamic construction of projection matrices is still a linear transformation. It might not capture highly non-linear or complex interactions between entities and relations. The assumption that projection vectors can effectively capture the "type" or "aspect" of an entity for a given relation is implicit.
*   **Practical Limitations**: Although more parameter-efficient than TransR, it still introduces additional vectors per entity and relation. The matrix construction and multiplication operations add computational overhead compared to simpler models like TransE.

\textbf{ManifoldE (Knowledge Graph Embedding for Precise Link Prediction)} \cite{xiao2015}:
*   **Context and Problem Solved**: ManifoldE challenged the fundamental assumption of point-based entity representations common in TransE, TransH, and TransR. It recognized that representing entities as single points in a vector space might be too restrictive to capture their inherent ambiguity, fuzziness, or multi-faceted nature. The problem it aimed to solve was to allow for more flexible and robust entity representations that could better capture semantic nuances and improve link prediction precision.
*   **Core Innovation**: ManifoldE expands entity representations from discrete points to **manifolds** (specifically, spheres or hyperplanes) in the embedding space. For a triple $(h, r, t)$, the head entity $h$ is represented by a manifold $M_h$, and the tail entity $t$ by a manifold $M_t$. The relation $r$ is still a translation vector $\mathbf{r}$. The core idea is that for a valid triple, the manifold $M_h$ translated by $\mathbf{r}$ should "intersect" or be "close" to the manifold $M_t$. The scoring function is defined as the distance between the translated manifold $M_h + \mathbf{r}$ and $M_t$. For example, if entities are represented as spheres, the score could be the distance between the center of the translated head sphere and the center of the tail sphere, adjusted by their radii. This allows for a more "fuzzy" matching, where entities are not forced to align perfectly at a single point.
*   **Conditions for Success**: ManifoldE can be particularly effective in KGs where entities have rich, multi-dimensional semantics that are not adequately captured by single points. It offers increased robustness to noise and ambiguity in the data by allowing for a region of plausibility rather than a single exact location.
*   **Theoretical Limitations**: Representing entities as manifolds increases the complexity of the model and the number of parameters (e.g., center and radius for a sphere). The choice of manifold type (sphere, hyperplane) is a hyperparameter that needs careful consideration and might not be universally optimal. The geometric interpretation of "translation of a manifold" can be more complex than point translation.
*   **Practical Limitations**: The increased parameter count and the more complex distance calculations involving manifolds lead to higher computational costs and memory requirements compared to point-based models. This can make it less scalable for extremely large KGs.

\textbf{Comparative Framework and Evolution}:
The development of TransD and ManifoldE illustrates a crucial evolutionary step beyond the initial translational models, driven by the need to overcome their inherent geometric rigidity.
\begin{itemize}
    \item **TransD** \cite{ji2015} directly addresses the parameter efficiency and expressiveness trade-off observed in TransR. By making projection matrices dynamic and entity-aware, it offers a more nuanced way to handle context-dependent entity roles without the prohibitive parameter count of TransR. This is a refinement within the "projection" paradigm, making it more adaptive.
    \item **ManifoldE** \cite{xiao2015} represents a more fundamental departure, questioning the very nature of entity representation. By moving from points to manifolds, it introduces a richer, more flexible geometric object for entities, allowing for inherent fuzziness and multi-faceted semantics. This addresses the limitation that a single point might not adequately capture all aspects of an entity, especially in ambiguous or noisy data. This approach implicitly acknowledges the "uncertainty" in knowledge representation, a theme later explored more explicitly by models like ATiSE \cite{xu2019} which use Gaussian distributions to model temporal uncertainty.
\end{itemize}
Both TransD and ManifoldE exemplify the field's early recognition that simple, rigid geometric forms (points and fixed translations) were insufficient for the complexity of real-world KGs. They represent a shift towards more adaptive and flexible representations, laying the groundwork for later models that would explore even more complex geometries (e.g., hyperbolic spaces \cite{pan2021, liang2024}) or entirely different mathematical frameworks. This continuous drive for enhanced representational power, often at the cost of increased complexity, is a defining characteristic of KGE research.

\subsection{Limitations of Early Geometric Models and the Drive for Enhanced Expressiveness}
The early geometric models, particularly the translational family (TransE, TransH, TransR, TransD) and their immediate extensions like ManifoldE, were foundational in establishing the Knowledge Graph Embedding (KGE) paradigm. They demonstrated the feasibility and efficiency of representing symbolic knowledge in continuous vector spaces for tasks like link prediction. However, their inherent design choices and geometric assumptions also exposed significant limitations, which collectively fueled a continuous drive for enhanced expressiveness in subsequent KGE research.

One of the most pervasive limitations of these early models stems from their **geometric rigidity and simplified relational semantics**. TransE, for instance, assumes relations are simple translations, which inherently struggles with complex relational patterns such as 1-N, N-1, N-N, and symmetric/anti-symmetric relations \cite{wang2014, lin2015}. As discussed, if `(h, r, t1)` and `(h, r, t2)` are both true, TransE forces `t1` and `t2` to be very close, which is often semantically inaccurate. While TransH and TransR introduced projections onto hyperplanes or relation-specific spaces, and TransD offered dynamic mappings, they still relied on linear transformations and distance-based scoring functions. These linear transformations, despite their improvements, may not be sufficient to capture highly non-linear and intricate semantic interactions present in real-world KGs \cite{cao2022}. The assumption of a single, fixed geometric operation (translation, projection) per relation limits their ability to model the diverse and context-dependent nature of relational semantics.

Another critical limitation, often implicitly assumed away by these early models, is the **static nature of knowledge**. Most early KGE models treated KGs as static snapshots, ignoring the temporal validity of facts \cite{xu2019, dasgupta2018}. A fact like `(Barack Obama, PresidentOf, USA)` is only true for a specific time interval. Ignoring this temporal dimension leads to inaccurate reasoning and an inability to predict future events or understand historical changes. This fundamental oversight became a significant bottleneck, prompting the development of Temporal KGE (TKGE) models. For example, \cite{dasgupta2018}'s HyTE explicitly incorporates time by associating each timestamp with a hyperplane, allowing for temporally guided inference. Building on this, \cite{xu2019}'s ATiSE further addresses this by modeling entity and relation representations as multi-dimensional Gaussian distributions evolving via additive time series decomposition, explicitly capturing *temporal uncertainty*—a nuance that static models completely miss. This highlights a clear evolution: early models implicitly assumed static knowledge, leading to a recognized limitation, which then spurred the explicit modeling of time and even temporal uncertainty.

Furthermore, these models often struggled with **representational capacity versus parameter efficiency**. While TransR and TransD offered increased expressiveness, they did so by introducing more parameters (projection matrices or dynamic mapping vectors). For KGs with a vast number of relations, this can lead to a parameter explosion, making models computationally expensive and prone to overfitting, especially for sparse relations \cite{chen2023}. The work by \cite{chen2023} on "Entity-Agnostic Representation Learning" directly questions the implicit assumption that each entity requires a unique, directly learned embedding. They demonstrate that conventional KGEs can lead to colossal parameter counts (e.g., 123 million for RotatE on YAGO3-10), hindering deployment. Their solution, EARL, which learns entity representations from distinguishable information rather than direct lookup, highlights a critical practical limitation of these early models' parameter scaling. This tension between needing more parameters for expressiveness and the practical constraints of memory and computation is a recurring theme in KGE research.

The reliance on **point-based entity representations** was also a limitation. ManifoldE \cite{xiao2015} was an early attempt to address this by representing entities as manifolds (e.g., spheres), allowing for a more flexible and "fuzzy" representation that could better capture ambiguity or multi-faceted semantics. However, even ManifoldE's geometric forms were somewhat constrained. The strictness of representing entities as single points or simple geometric shapes often failed to capture the rich, multi-aspect nature of real-world entities, where different relations might highlight different facets of the same entity. This inability to capture disentangled representations was a significant gap, later addressed by models like DisenKGAT \cite{wu2021}, which explicitly learns independent components for entities, moving beyond the implicit assumption that a single vector can adequately represent all aspects of an entity.

Finally, the **handling of noise and uncertainty** in KGs was largely overlooked by these early geometric models. Most assumed that triples were uniformly confident and error-free. However, real-world KGs are often noisy due to automatic extraction processes \cite{shan2018}. This led to issues where models could overfit to erroneous facts. The development of confidence-aware negative sampling methods, as proposed by \cite{shan2018}, directly addresses this by acknowledging that not all negative samples are equally informative, especially in noisy KGs. This highlights a critical methodological gap in the training procedures of early models.

In summary, the limitations of early geometric models can be categorized along several dimensions:
\begin{enumerate}
    \item \textbf{Geometric Rigidity}: Inability to model complex relational patterns (1-N, N-1, N-N, symmetric) due to simple linear transformations.
    \item \textbf{Static Knowledge Assumption}: Failure to account for the temporal dynamics and evolution of facts.
    \item \textbf{Parameter Inefficiency}: Increased expressiveness often came with a prohibitive increase in parameters, leading to scalability and overfitting issues.
    \item \textbf{Limited Entity Representation}: Representing entities as single points or simple manifolds was insufficient for capturing multi-faceted or ambiguous semantics.
    \item \textbf{Ignorance of Noise and Uncertainty}: Implicit assumption of perfect, uniformly confident triples.
\end{enumerate}
These limitations collectively underscored the need for KGE models with enhanced expressiveness. The field's trajectory shifted towards exploring more complex geometric spaces (e.g., hyperbolic embeddings), non-linear transformations (e.g., neural networks), and the integration of auxiliary information, all aimed at developing more robust, flexible, and semantically rich representations for knowledge graphs. This continuous drive for enhanced representational power, often involving trade-offs between simplicity, efficiency, and accuracy, remains a central theme in KGE research.