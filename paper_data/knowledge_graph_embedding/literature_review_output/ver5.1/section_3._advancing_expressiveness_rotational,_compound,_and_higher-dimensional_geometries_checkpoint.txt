\section*{3. Advancing Expressiveness: Rotational, Compound, and Higher-Dimensional Geometries}

Building upon the foundational translational models discussed in Section 2, the field of Knowledge Graph Embedding (KGE) has progressively moved towards more sophisticated geometric and algebraic approaches to capture the intricate and diverse relational patterns inherent in real-world knowledge graphs (KGs). The initial simplicity of translation-based models, while efficient, proved insufficient for handling complex relational properties such as symmetry, antisymmetry, composition, and hierarchical structures \cite{cao2022, ge2023}. This section explores a significant paradigm shift, detailing the emergence of models that leverage rotations in complex and quaternion spaces, compound geometric operations, and embeddings in non-Euclidean geometries. These advancements collectively aim to overcome the geometric rigidity and limited expressiveness of earlier models, offering a more faithful and nuanced representation of knowledge.

The limitations of translational models, particularly their struggle with 1-N, N-1, N-N relations and symmetric properties (as highlighted in Section 2.2), spurred the development of rotational models. These models, exemplified by RotatE \cite{sun2018}, conceptualize relations not as simple translations but as rotations in complex vector spaces. This algebraic shift provides an elegant mechanism to inherently model symmetry (a relation is its own inverse), antisymmetry (a relation implies a distinct inverse), and composition (relations can be chained), which were significant challenges for TransE and its direct extensions. The move to complex numbers and quaternions introduces a richer mathematical framework, allowing for more nuanced transformations that can naturally encode these relational characteristics.

Further advancing expressiveness, researchers explored compound geometric operations. Models like CompoundE \cite{ge2022} and CompoundE3D \cite{ge2023} recognize that a single, monolithic transformation (be it translation or rotation) might be too simplistic for the multifaceted nature of real-world relations. Instead, they propose combining multiple elementary transformations—such as translation, rotation, and scaling—to form a more versatile and adaptive relational mapping. This approach acknowledges that different aspects of a relation might require different geometric operations, thereby enabling a more fine-grained modeling of complex interactions. This represents an evolution from the fixed operations of earlier models towards more learnable and composite transformations, directly addressing the geometric rigidity identified in Section 2.3.

A more profound shift in KGE research involves moving beyond the conventional Euclidean embedding space. The implicit assumption that Euclidean space is optimal for all KG structures began to be questioned, particularly for representing hierarchical and complex topological patterns \cite{pan2021, liang2024}. Non-Euclidean geometries, such as hyperbolic and hyperspherical spaces, offer intrinsic properties that are better suited for specific data structures. Hyperbolic spaces, with their negative curvature, are inherently adept at modeling hierarchical data and tree-like structures, where distances grow exponentially. Conversely, hyperspherical spaces, with positive curvature, can be beneficial for cyclic or periodic patterns. This exploration culminates in multi-curvature embeddings, such as MADE \cite{wang2024} and IME \cite{wang2024}, which attempt to adapt the local curvature of the embedding space to the specific characteristics of different parts of the knowledge graph. This adaptive geometry represents a significant leap, moving away from a one-size-fits-all embedding space to a more flexible, structure-aware representation.

The increasing complexity of these advanced geometric models, however, often comes with trade-offs. While they offer enhanced representational power, they can introduce greater computational costs, increased parameter counts, and potentially reduced interpretability. The parameter efficiency concerns highlighted by \cite{chen2023} for conventional KGEs become even more pertinent here, as complex transformations and higher-dimensional spaces can exacerbate the parameter explosion problem. This section will critically analyze these advanced approaches, comparing their theoretical underpinnings, practical implications, and the specific types of relational patterns they are designed to capture, ultimately illustrating the field's continuous quest for more expressive and robust knowledge graph embeddings.

### Relational Rotations in Complex and Quaternion Spaces

The limitations of translational models in capturing complex relational patterns, particularly symmetry, antisymmetry, and composition, motivated a significant shift towards representing relations as rotations in higher-dimensional algebraic spaces. This paradigm leverages the inherent properties of complex numbers and quaternions to naturally encode these relational characteristics, offering a more expressive alternative to simple vector addition.

\textbf{RotatE (Knowledge Graph Embedding by Relational Rotation in Complex Space)} \cite{sun2018}:
*   **Context and Problem Solved**: RotatE was introduced to address the fundamental inability of translational models (like TransE, TransH, TransR) to model symmetric, antisymmetric, and compositional relations effectively. As discussed in Section 2.2, TransE struggles with symmetric relations by forcing the relation vector to be zero, and with compositional relations like `(a, r1, b)` and `(b, r2, c)` implying `(a, r1+r2, c)`. RotatE sought to provide a unified and elegant solution for these properties.
*   **Core Innovation**: RotatE represents entities $\mathbf{h}, \mathbf{t}$ as vectors in a complex vector space $\mathbb{C}^k$ and relations $\mathbf{r}$ as diagonal matrices where each diagonal element is a complex number with modulus 1 (i.e., a rotation). For a triple $(h, r, t)$, the scoring function is defined as $f(h, r, t) = \|\mathbf{h} \circ \mathbf{r} - \mathbf{t}\|_{L_1/L_2}$, where $\circ$ denotes the Hadamard (element-wise) product. The relation $\mathbf{r}$ acts as an element-wise rotation from $\mathbf{h}$ to $\mathbf{t}$ in the complex plane.
    *   **Symmetry**: If $r$ is symmetric, then $\mathbf{r}$ should be close to a vector of ones (no rotation), meaning $\mathbf{h} \approx \mathbf{t}$.
    *   **Antisymmetry**: If $r$ is antisymmetric, then $\mathbf{r}$ should be far from a vector of ones, ensuring $\mathbf{h} \circ \mathbf{r} \ne \mathbf{t}$ implies $\mathbf{t} \circ \mathbf{r} \ne \mathbf{h}$.
    *   **Composition**: For `r1` and `r2` such that `r1 + r2` is a composite relation, RotatE naturally models this as $\mathbf{r_1} \circ \mathbf{r_2}$, meaning sequential rotations compose correctly. This directly addresses a major limitation of TransE's additive composition.
*   **Conditions for Success**: RotatE demonstrates strong performance on datasets rich in symmetric, antisymmetric, and compositional relations. Its elegant mathematical formulation provides a powerful inductive bias for these relational patterns.
*   **Theoretical Limitations**: While powerful for rotation-based patterns, RotatE's fixed rotation mechanism might still struggle with relations that are better modeled by translations or more complex non-linear transformations. The assumption that all relations can be adequately represented as element-wise rotations in complex space might be overly simplistic for extremely diverse relation types.
*   **Practical Limitations**: Complex number arithmetic can be slightly more computationally intensive than real vector operations. The parameter count is similar to TransE (two vectors per entity, one per relation), but the complex nature adds a factor of two in storage. \cite{lloyd2022} notes that RotatE, among other models, sometimes failed to complete trials on larger datasets, suggesting potential computational demands or implementation challenges, which can be a practical bottleneck for very large KGs.

**Quaternion Embeddings (QuatE, HousE, TorusE, SpherE)**:
Building on the idea of complex rotations, quaternion embeddings extend this concept to a 4-dimensional hypercomplex space, offering even richer rotational capabilities. Quaternions can represent 3D rotations more naturally and without gimbal lock issues, making them appealing for KGE.
*   **QuatE (Quaternion Knowledge Graph Embedding)** \cite{zhang2019rlm}:
    *   **Context and Problem Solved**: QuatE extends RotatE by using quaternions, which can capture more intricate relational patterns than complex numbers. It aims to model relations as rotations in 3D space, providing a more expressive representation for entities and relations.
    *   **Core Innovation**: Entities and relations are embedded as quaternion vectors. For a triple $(h, r, t)$, the scoring function is typically based on quaternion multiplication, e.g., $f(h, r, t) = \|\mathbf{h} \circ \mathbf{r} - \mathbf{t}\|_{L_1/L_2}$ or similar, where $\circ$ is quaternion multiplication. This allows for a more general form of rotation.
    *   **Conditions for Success**: QuatE often outperforms RotatE on benchmarks, demonstrating improved expressiveness, particularly for relations that benefit from richer rotational transformations.
    *   **Theoretical Limitations**: While more expressive, the interpretability of quaternion operations can be challenging. The increased dimensionality (4x real numbers per element) adds to parameter count and computational complexity.
*   **HousE (Knowledge Graph Embedding with Householder Parameterization)** \cite{li2022}:
    *   **Context and Problem Solved**: HousE addresses the challenge of designing expressive and parameter-efficient rotational models. It leverages Householder transformations, which are reflections, to model relations.
    *   **Core Innovation**: Relations are parameterized using Householder matrices, which are orthogonal and symmetric. This allows for modeling relations as reflections, which can be seen as a specific type of rotation. The scoring function involves applying this Householder transformation to the head entity embedding to approximate the tail entity.
    *   **Comparison**: HousE offers a novel way to achieve rotational expressiveness with a potentially more parameter-efficient representation than full quaternion embeddings, as Householder matrices can be defined by a single vector. This directly addresses the parameter efficiency concerns raised by \cite{chen2023}.
*   **TorusE (Knowledge Graph Embedding on a Lie Group)** \cite{ebisu2017}:
    *   **Context and Problem Solved**: TorusE explores embedding KGs on a Lie group, specifically a torus, which is a compact manifold. This aims to capture cyclic or periodic relational patterns that might not be well-represented in Euclidean or even complex spaces.
    *   **Core Innovation**: Embeds entities and relations as points on a torus, where relations are modeled as translations along the torus. The periodic nature of the torus naturally handles cyclic relations.
    *   **Comparison**: TorusE offers a unique geometric space that is distinct from complex or quaternion rotations but shares the goal of capturing specific algebraic properties. It provides an alternative to RotatE for certain types of cyclic relations, but its applicability might be more niche.
*   **SpherE (Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval)** \cite{li2024}:
    *   **Context and Problem Solved**: SpherE focuses on embedding knowledge graphs on a hypersphere, aiming for expressive and interpretable representations, particularly for set retrieval tasks.
    *   **Core Innovation**: Entities and relations are embedded on a unit hypersphere. Relations are modeled as rotations or transformations within this spherical space. The spherical geometry is particularly adept at capturing similarity based on angles, which can be useful for certain semantic relationships.
    *   **Comparison**: SpherE shares the rotational intuition with RotatE and QuatE but situates it within a spherical manifold. This offers a different inductive bias, potentially better for relations where angular distance is more meaningful than Euclidean distance.

**Comparative Framework and Evolution**:
The evolution from translational to rotational models, and further to quaternion and other manifold-based embeddings, represents a clear trajectory towards increased algebraic and geometric expressiveness.
\begin{itemize}
    \item **TransE** (Section 2.2) provides a simple additive model, failing for complex relations.
    \item **RotatE** \cite{sun2018} introduces complex numbers, enabling inherent modeling of symmetry, antisymmetry, and composition through element-wise rotations. This is a direct algebraic enhancement that resolves many of TransE's relational limitations. The work by \cite{song2021} (Rot-Pro) further explores transitivity modeling through projection in a similar vein.
    \item **QuatE** \cite{zhang2019rlm} extends RotatE to quaternions, offering a richer 4D rotational space, potentially capturing more complex transformations. This represents a further step in leveraging higher-dimensional algebra for expressiveness.
    \item **HousE** \cite{li2022} and **TorusE** \cite{ebisu2017} and **SpherE** \cite{li2024} explore alternative mathematical frameworks (Householder reflections, Lie groups, spherical geometry) to achieve similar goals of enhanced rotational or geometrically constrained expressiveness. HousE, in particular, highlights a tension between expressiveness and parameter efficiency, offering a potentially more compact way to represent transformations than full quaternion embeddings.
\end{itemize}
This family of models demonstrates a recurring pattern: leveraging advanced mathematical structures (complex numbers, quaternions, Lie groups) to imbue relations with richer algebraic properties. While these models significantly enhance the ability to capture specific relational patterns, they also introduce increased mathematical complexity and potentially higher computational demands. The choice among them often depends on the specific relational properties dominant in a given KG and the trade-off between expressiveness, efficiency, and interpretability.

### Compound Geometric Transformations

While rotational models significantly advanced the expressiveness of KGEs by introducing complex and quaternion algebra, they still largely relied on a single, albeit more sophisticated, geometric operation per relation. Recognizing that real-world relations often involve a confluence of different semantic transformations, a new class of models emerged that employs **compound geometric operations**, combining elements like translation, rotation, and scaling. This approach aims to provide a more flexible and adaptive framework for modeling intricate relational patterns, moving beyond the monolithic transformations of earlier models.

\textbf{CompoundE (Knowledge Graph Embedding with Translation, Rotation and Scaling Compound Operations)} \cite{ge2022}:
*   **Context and Problem Solved**: CompoundE addresses the limitation that many relations cannot be adequately captured by a single type of geometric transformation (e.g., pure translation or pure rotation). For instance, a "part-of" relation might involve both a translation (moving from a whole to a part) and a scaling (the part is smaller than the whole). Previous models lacked the flexibility to combine these fundamental operations.
*   **Core Innovation**: CompoundE represents each relation $r$ as a combination of three distinct geometric operations: a **translation vector** $\mathbf{t}_r$, a **rotation matrix** $\mathbf{R}_r$, and a **scaling vector** $\mathbf{s}_r$. For a triple $(h, r, t)$, the scoring function is based on applying these compound operations to the head entity embedding: $f(h, r, t) = \|\mathbf{s}_r \circ (\mathbf{R}_r \mathbf{h}) + \mathbf{t}_r - \mathbf{t}\|_{L_1/L_2}$. This allows relations to simultaneously translate, rotate, and scale entity embeddings, providing a much richer set of transformations. The rotation matrix $\mathbf{R}_r$ can be parameterized efficiently (e.g., using Euler angles or Rodrigues' rotation formula) to avoid excessive parameters.
*   **Conditions for Success**: CompoundE is particularly effective on KGs where relations exhibit diverse semantic properties that require a blend of transformations, such as hierarchical relations (scaling), part-whole relations (translation + scaling), or abstract conceptual shifts (rotation). It offers superior performance over models relying on single operations by adapting to the multifaceted nature of relations.
*   **Theoretical Limitations**: The interpretability of a relation as a combination of translation, rotation, and scaling can be complex. While more expressive, the model still operates within a Euclidean space, which may not be optimal for highly hierarchical or non-Euclidean structures. The parameterization of rotation matrices and the interaction between the three operations can be challenging to optimize.
*   **Practical Limitations**: The introduction of three distinct sets of parameters per relation (translation vector, rotation matrix parameters, scaling vector) significantly increases the total parameter count compared to simpler models like TransE or RotatE. This can lead to higher memory consumption and computational cost during training, especially for KGs with many relations. This exacerbates the parameter efficiency concerns raised by \cite{chen2023}, where even single-operation models like RotatE already face parameter explosion on large KGs.

\textbf{CompoundE3D (Knowledge Graph Embedding with 3D Compound Geometric Transformations)} \cite{ge2023}:
*   **Context and Problem Solved**: CompoundE3D is an extension of CompoundE, specifically designed to operate in a 3D embedding space, aiming to further enhance the expressiveness of compound operations by leveraging the geometric properties of 3D rotations and transformations more directly.
*   **Core Innovation**: Similar to CompoundE, it combines translation, rotation, and scaling, but explicitly within a 3D vector space. This might involve using quaternions for more stable and efficient 3D rotation representation, rather than less stable Euler angles or more parameter-heavy rotation matrices. The scoring function adapts to this 3D context.
*   **Comparison**: CompoundE3D builds directly on CompoundE's principle, focusing on optimizing the compound operations for a 3D space. This suggests an iterative refinement of the compound operation idea, seeking to improve stability and efficiency while maintaining expressiveness. Its performance is expected to be competitive with or superior to CompoundE, especially where 3D geometric intuition aligns well with relational semantics.

\textbf{STaR (Knowledge Graph Embedding by Scaling, Translation and Rotation)} \cite{li2022}:
*   **Context and Problem Solved**: STaR also proposes a compound operation model, similar to CompoundE, but potentially with different parameterizations or optimization strategies. It aims to provide a robust framework for combining the three fundamental geometric transformations.
*   **Core Innovation**: STaR models relations as a sequence of scaling, translation, and rotation operations. The specific order or parameterization might differ from CompoundE, but the core idea of composite transformations remains.
*   **Comparison**: STaR is another manifestation of the compound operation paradigm. The existence of multiple models (CompoundE, CompoundE3D, STaR) employing similar compound operations highlights a convergent research direction, recognizing the need for more flexible relational transformations. The differences likely lie in the specific mathematical parameterizations of these operations (e.g., how rotations are represented, how scaling is applied) and their optimization strategies.

**Comparative Framework and Evolution**:
The emergence of compound geometric transformation models marks a significant evolution from both translational and purely rotational models.
\begin{itemize}
    \item **Translational Models (e.g., TransE, TransH, TransR)** (Section 2.2) use a single, linear operation (translation, possibly with projection). They are simple but geometrically rigid.
    \item **Rotational Models (e.g., RotatE, QuatE)** (Section 3.1) introduce complex or quaternion algebra for rotations, addressing symmetry and composition, but still rely on a single type of transformation.
    \item **Compound Models (e.g., CompoundE \cite{ge2022}, CompoundE3D \cite{ge2023}, STaR \cite{li2022})** represent a paradigm shift by combining multiple fundamental geometric operations (translation, rotation, scaling) into a single relational transformation. This directly addresses the limitation that real-world relations are often multifaceted and cannot be adequately captured by a singular geometric action. This approach acknowledges that relations are not just "one thing" but a composite of several semantic shifts.
\end{itemize}
A critical tension exists between the enhanced expressiveness of these compound models and their increased complexity and parameter count. While they achieve higher accuracy by better modeling complex relations, they are inherently more demanding computationally. This trade-off is a recurring challenge in KGE research, where the pursuit of higher representational power often necessitates more intricate models. The success of these models suggests that the field is moving towards a more nuanced understanding of relational semantics, where relations are viewed as dynamic, multi-component transformations rather than static, singular operations.

### Exploring Non-Euclidean and Multi-Curvature Embedding Spaces

A profound advancement in KGE research involves questioning the fundamental assumption that Euclidean space is the optimal manifold for embedding knowledge graphs. The recognition that real-world KGs often exhibit inherent hierarchical structures, power-law distributions, and complex topological patterns has led to the exploration of non-Euclidean geometries, particularly hyperbolic and hyperspherical spaces, and subsequently, multi-curvature embeddings. This shift represents a paradigm change, moving from a universally flat space to spaces whose intrinsic curvature better aligns with the underlying structure of knowledge.

\textbf{Hyperbolic Embedding Spaces}:
*   **Context and Problem Solved**: Euclidean space struggles to efficiently embed hierarchical data, where the number of nodes grows exponentially with depth. Representing such structures in Euclidean space requires exponentially increasing embedding dimensions or leads to high distortion. Hyperbolic spaces, with their negative curvature, naturally expand exponentially, making them inherently well-suited for embedding tree-like and hierarchical structures with low distortion \cite{pan2021, liang2024}. This addresses a critical limitation of all previous Euclidean-based models (translational, rotational, compound) which implicitly assume a flat geometry.
*   **Core Innovation**: Entities and relations are embedded in a hyperbolic manifold (e.g., Poincaré disk or hyperboloid model). Relations are typically modeled as transformations (e.g., Mobius transformations for translations/rotations) within this non-Euclidean space. The scoring function uses hyperbolic distance, which naturally captures hierarchical relationships: entities far apart in the hierarchy are exponentially distant in hyperbolic space, while entities close in the hierarchy are close in hyperbolic space.
    *   **Hyperbolic Hierarchy-Aware KGE for Link Prediction** \cite{pan2021}: This work explicitly leverages hyperbolic geometry to capture hierarchical information within KGs for link prediction. It demonstrates how the intrinsic properties of hyperbolic space can better preserve the structural hierarchy of entities.
    *   **Fully Hyperbolic Rotation for KGE** \cite{liang2024}: This model combines the benefits of hyperbolic geometry with rotational transformations, allowing for expressive modeling of relations (like symmetry/antisymmetry) within a hierarchy-aware space. This represents a convergence of rotational models (Section 3.1) with non-Euclidean geometry.
*   **Conditions for Success**: Hyperbolic embeddings excel on KGs that possess strong hierarchical structures, such as ontologies, taxonomies, or biological networks. They achieve significantly lower distortion and higher accuracy in link prediction and entity classification tasks on such datasets compared to Euclidean counterparts.
*   **Theoretical Limitations**: While ideal for hierarchies, hyperbolic spaces might not be optimal for all types of relational patterns, particularly those that are not inherently tree-like or exhibit cyclic structures. The mathematical operations (e.g., addition, distance calculation) in hyperbolic space are more complex than in Euclidean space, requiring specialized algorithms.
*   **Practical Limitations**: Implementing and optimizing models in hyperbolic space is more challenging due to the non-linear nature of the geometry and the need for specialized Riemannian optimization techniques. This can lead to higher computational costs and slower training times.

\textbf{Hyperspherical Embedding Spaces}:
*   **Context and Problem Solved**: While hyperbolic spaces address hierarchies, some KGs or parts of KGs might exhibit cyclic, periodic, or clustered patterns that are better captured by positive curvature. Hyperspherical spaces (e.g., unit spheres) offer such a geometry.
*   **Core Innovation**: Entities and relations are embedded on the surface of a hypersphere. Relations are modeled as rotations or transformations on this sphere. The distance metric is typically geodesic distance (arc length).
    *   \textbf{SpherE} \cite{li2024} (also mentioned in 3.1) is a prime example, leveraging spherical geometry for expressive and interpretable embeddings, particularly for set retrieval. It highlights how the angular relationships on a sphere can capture certain semantic similarities.
*   **Conditions for Success**: Hyperspherical embeddings are beneficial for KGs where relations exhibit periodicity, or where entities naturally form clusters on a spherical manifold.
*   **Theoretical Limitations**: Similar to hyperbolic spaces, hyperspherical spaces are not universally optimal. They might struggle with deep hierarchies or relations that require significant expansion.
*   **Practical Limitations**: Optimization on spheres also requires specialized techniques, adding to computational complexity.

\textbf{Multi-Curvature Adaptive Embedding Spaces (MADE, IME)}:
*   **Context and Problem Solved**: The realization that different parts of a knowledge graph might inherently possess different structural properties (e.g., some parts are hierarchical, others are more grid-like, some are cyclic) led to the idea of adaptive or multi-curvature embedding spaces. The limitation of a single, global geometry (Euclidean, hyperbolic, or spherical) is that it cannot optimally represent the heterogeneous nature of real-world KGs.
*   **Core Innovation**: These models aim to adapt the local curvature of the embedding space to the specific characteristics of entities and relations.
    *   **MADE (Multicurvature Adaptive Embedding for Temporal Knowledge Graph Completion)** \cite{wang2024}: MADE introduces a mechanism to dynamically determine the optimal curvature for different entities and relations, or even different parts of the embedding space. This allows the model to use hyperbolic geometry for hierarchical components and Euclidean geometry for more grid-like structures, or spherical for cyclic ones, all within a unified framework. It applies this to temporal KGs, further increasing complexity.
    *   **IME (Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion)** \cite{wang2024}: IME builds on the multi-curvature concept by integrating both shared (global) and specific (local) embeddings across different curvatures. This allows for a more nuanced representation where some aspects of an entity might benefit from one curvature, while others from another. It also applies this to temporal KGs.
*   **Conditions for Success**: Multi-curvature models are particularly powerful for large, heterogeneous KGs that exhibit diverse structural patterns. They offer the potential for higher accuracy by reducing distortion across the entire graph.
*   **Theoretical Limitations**: The theoretical justification for dynamically assigning or learning optimal local curvatures is complex. The interaction between different curvatures and how transformations behave across these varying geometries is an active research area. The interpretability of embeddings in such mixed-geometry spaces can be significantly reduced.
*   **Practical Limitations**: These models are significantly more complex computationally. Learning and optimizing parameters for multiple curvatures, potentially dynamically, adds substantial overhead. This directly challenges the parameter efficiency goals, as highlighted by \cite{chen2023}, by introducing more intricate mathematical structures and optimization problems.

**Comparative Framework and Evolution**:
The journey from Euclidean to non-Euclidean and then to multi-curvature spaces represents the most advanced frontier in geometric KGE, driven by the need to match the embedding space's intrinsic properties to the KG's structural characteristics.
\begin{itemize}
    \item **Euclidean Models (Translational, Rotational, Compound)** (Section 2.2, 3.1, 3.2) implicitly assume a flat, uniform space. They are computationally simpler but suffer from distortion when embedding non-Euclidean structures like hierarchies.
    \item **Hyperbolic KGEs** \cite{pan2021, liang2024} explicitly address the challenge of embedding hierarchical data by leveraging negative curvature, offering superior performance for such structures. This is a direct response to the geometric limitations of Euclidean space for specific KG topologies.
    \item **Hyperspherical KGEs** \cite{li2024} provide an alternative for cyclic or clustered patterns, demonstrating that different non-Euclidean geometries serve different structural biases.
    \item **Multi-Curvature Models (MADE \cite{wang2024}, IME \cite{wang2024})** represent the cutting edge, attempting to overcome the "one-size-fits-all" limitation of a single global geometry. They aim to adapt the embedding space's curvature locally, offering the most flexible and structure-aware representations. This approach directly challenges the implicit assumption that a single, global geometric space is sufficient for all KGs, recognizing their inherent heterogeneity.
\end{itemize}
This evolution highlights a fundamental tension in KGE research: the trade-off between geometric simplicity and computational efficiency versus the fidelity of representation. While non-Euclidean and multi-curvature embeddings offer unparalleled expressiveness for complex KG structures, they introduce significant mathematical and computational challenges. The field is actively exploring how to balance these factors, with a clear trend towards more adaptive and structure-aware embedding spaces to achieve a more faithful representation of diverse knowledge graph characteristics. The unresolved debate lies in determining the optimal strategy for learning and integrating these diverse geometries efficiently and interpretably.