# A Comprehensive Literature Review with Self-Reflection

**Generated on:** 2025-10-06T00:46:50.523277
**Papers analyzed:** 377

## Papers Included:
1. d899e434a7f2eecf33a90053df84cf32842fbca9.pdf [sun2018]
2. 83d58bc46b7adb92d8750da52313f060b10f201d.pdf [dasgupta2018]
3. 10d949dee482aeea1cab8b42c326d0dbf0505de3.pdf [chen2023]
4. b1d807fc6b184d757ebdea67acd81132d8298ff6.pdf [yang2023]
5. abea782b5d0bdb4cd90ec42f672711613e71e43e.pdf [jia2015]
6. 658702b2fa647ae7eaf1255058105da9eefe6f52.pdf [lloyd2022]
7. 29eb99518d16ccf8ac306d92f4a6377ae109d9be.pdf [wu2021]
8. 58e1b93b18370433633152cb8825917edc2f16a6.pdf [xu2019]
9. d4220644ef94fa4c2e5138a619cfcd86508d2ea1.pdf [shan2018]
10. 15710515bae025372f298570267d234d4a3141cb.pdf [zheng2024]
11. 354fb91810c6d3756600c99ad84d2e6ef4136021.pdf [he2023]
12. 67cab3bafc8fa9e1ae3ff89791ad43c81441d271.pdf [xiao2015]
13. 405a7a7464cfe175333d6f04703ac272e00a85b4.pdf [guo2017]
14. 8b717c4dfb309638307fcc7d2c798b1c20927a3e.pdf [chen2021]
15. 29052ddd048acb1afa2c42613068b63bb7428a34.pdf [li2023]
16. 23efe9b99b5f0e79d7dbd4e3bfcf1c2d8b23c1ff.pdf [zhou2023]
17. af051c87cecca64c2de4ad9110608f7579766653.pdf [xiang2021]
18. 85064a4b1b96863af4fccff9ad34ce484945ad7b.pdf [cao2022]
19. 06315f8b2633a54b087c6094cdb281f01dd06482.pdf [wang2021]
20. a905a690ec350b1aeb5fcfd7f2ff0f5e1663b3a0.pdf [guo2020]
21. 3ac716ac5d47d4420010678fda766ebb5b882ba9.pdf [zhang2024]
22. 933cb8bf1cd50d6d5833a627683327b15db28836.pdf [shen2022]
23. bb3e135757bfb82c4de202c807c9e381caecb623.pdf [hu2024]
24. 398978c84ca8dab093d0b7fa73c6d380f5fa914c.pdf [liu2024]
25. b594b21557395c6a8fa8356249373f8e318c2df2.pdf [zhang2019]
26. 3e3a84bbceba79843ca1105939b2eb438c149e9e.pdf [yang2019]
27. b3f0cdc217a3d192d2671e44913542903c94105b.pdf [xie2023]
28. 52eb7f27cdfbf359096b8b5ef56b2c2826beb660.pdf [wang2024]
29. ecb80d1e5507e163be4a6757b00c8809a2de4863.pdf [xiao2019]
30. 33d469c6d9fc09b59522d91b7696b15dc60a9a93.pdf [sachan2020]
31. 4801db5c5cb24a9069f2d264252fa26986ceefa9.pdf [madushanka2024]
32. a166957ec488cd20e61360d630568b3b81af3397.pdf [zhu2022]
33. bcffbb40e7922d2a34e752f8faaa4fe99649e21a.pdf [liang2024]
34. 7029ecb5d5fc04f54e1e25e739db2e993fb147c8.pdf [li2024]
35. 990334cf76845e2da64d3baa10b0a671e433d4b6.pdf [ebisu2017]
36. 0367603c0197ab48eeba29aa6af391584a5077c0.pdf [zhang2021]
37. 7572aefcd241ec76341addcb2e2e417587cb2e4c.pdf [huang2019]
38. c2c6edc5750a438bddd1217481832d38df6336de.pdf [tang2019]
39. a6a735f8e218f772e5b9dac411fa4abea87fdb9c.pdf [sun2018]
40. f2b924e69735fb7fd6fd95c6a032954480862029.pdf [ge2023]
41. e39afdbd832bd8fd0fb4f4f7df3722dc5f5cab2a.pdf [wang2020]
42. 63836e669416668744c3676a831060e8de3f58a1.pdf [li2022]
43. 11e402c699bcb54d57da1a5fdbc57076d7255baf.pdf [zhang2019]
44. 191815e4109ee392b9120b61642c0e859fb662a1.pdf [tang2022]
45. d3c287ff061f295ddf8dc3cb02a6f39e301cae3b.pdf [lv2018]
46. c64433657869ecdaaa7988a029eabfe774d3ac47.pdf [chen2025]
47. 8fef3f8bb8bcd254898b5d24f3d78beab09e99d4.pdf [qian2021]
48. 68f34ed64fdf07bb1325097c93576658e061231e.pdf [dai2020]
49. efea0197c956e981e98c4d2532fa720c58954492.pdf [ji2024]
50. f470e11faa6200026cf39e248510070c078e509a.pdf [yan2022]
51. 5dc88d795cbcd01e6e99ba673e91e9024f0c3318.pdf [zhang2023]
52. 0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f.pdf [li2021]
53. 33f3f53c957c4a8832b1dcb095a4ac967bd89897.pdf [yang2025]
54. 2e925a02db26a60ee1cc022f3923e09f3fae7b39.pdf [wang2019]
55. 040fe47af8f4870bf681f34861c42b3ea46d76cf.pdf [di2023]
56. c762e198b0239313ee50476021b1939390c4ef9d.pdf [jia2017]
57. 1f20378d2820fdf1c1bb09ce22f739ab77b14e82.pdf [choudhary2021]
58. 991b64748dfeecf026a27030c16fe1743aa20167.pdf [xiao2015]
59. 6a2f26cece133b0aa52843be0f149a65e78374f7.pdf [hu2024]
60. 2a3f862199883ceff5e3c74126f0c80770653e05.pdf [wang2014]
61. 21f8ea62da6a4031d85a1ee701dbc3e6847fa6d3.pdf [zhu2020]
62. acc855d74431537b98de5185e065e4eacbab7b26.pdf [ali2020]
63. 2a25540e3ce0baba56ee71da7ca938f0264f790d.pdf [mohamed2020]
64. d42b7c6c8fecab40b445aa3d24eb6b0124f05fd4.pdf [gao2020]
65. d7ef14459674b75807cd9be549f1e12d53849ead.pdf [peng2021]
66. 3f170af3566f055e758fa3bdf2bfd3a0e8787e58.pdf [shi2025]
67. 5b5b3face4be1cf131d0cb9c40ae5adcd0c16408.pdf [zhang2024]
68. f4e39a4f8fd8f8453372b74fda17047b9860d870.pdf [rosso2020]
69. 6a86594566fc9fa2e92afb6f0229d63a45fe25e6.pdf [zhou2024]
70. 1620a20881b572b5ffc6f9cb3cf39f6090cee19f.pdf [xie2020]
71. 83a46afaeb520abcd9b0138507a253f6d4d8bff7.pdf [song2021]
72. f44ee7932aacd054101b00f37d4c26c27630c557.pdf [zhang2020]
73. 44ce738296c3148c6593324773706cdc228614d4.pdf [ge2022]
74. bcdb8914550df02bfe1f69348c9830d775f6590a.pdf [ren2020]
75. 77dc07c92c37586f94a6f5ac3de103b218931578.pdf [yuan2019]
76. d1a525c16a53b94200029df1037f2c9c7c244d7b.pdf [xiao2015]
77. 8f096071a09701012c9c279aee2a88143a295935.pdf [sun2018]
78. 18bd7cd489874ed9976b4f87a6a558f9533316e0.pdf [ji2015]
79. 0364e17da01358e2705524cd781ef8cc928256f5.pdf [lin2020]
80. fda63b289d4c0c332f88975994114fb61b514ced.pdf [islam2023]
81. 3f0d5aa7a637d2c0bb3d768c99cc203430b4481e.pdf [wang2021]
82. 2bd20cfec4ad3df0fd9cd87cef3eefe6f3847b83.pdf [broscheit2020]
83. 84aa127dc5ca3080385439cb10edc50b5d2c04e4.pdf [fanourakis2022]
84. 727183c5cff89a6f2c3b71167ae50c02ca2cacc4.pdf [wang2018]
85. 19a672bdf29367b7509586a4be27c6843af903b1.pdf [tabacof2019]
86. ecc04e9285f016090697a1a8f9e96ce01e94e742.pdf [pei2019]
87. beade097ff41c62a8d8d29065be0e1339be39f30.pdf [zhang2018]
88. bbb89d88ad5b8279709ff089d3c00cd2750cd26b.pdf [li2021]
89. d605a7628b2a7ff8ce04fc27111626e2d734cab4.pdf [li2022]
90. 322aa32b2a409d2e135dbb14736d9aeb497f1c52.pdf [ding2018]
91. b2d2ad9a458bdcb0523d22be659eb013ca2d3c67.pdf [zhang2022]
92. ce7291c5cd919a97ced6369ca697db9849848688.pdf [sun2024]
93. 780bc77fac1aaf460ba191daa218f3c111119092.pdf [wang2024]
94. 6205f75cb6db1503c94386441ca68c63c9cbd456.pdf [modak2024]
95. e379f7c85441df5d8ddc1565cabf4b4290c22f1f.pdf [xiao2016]
96. c180564160d0788a82df203f9e5f61380d9846aa.pdf [zhang2023]
97. 69418ff5d4eac106c72130e152b807004e2b979c.pdf [guo2015]
98. 552bfaca30af29647c083993fbe406867fc70d4c.pdf [xu2020]
99. 33a7b7abf006d22de24c1471e6f6c93842a497b6.pdf [zheng2024]
100. 86ac98157da100a529ca65fe6e1da064b0a651e8.pdf [zhang2018]
101. 52b167a90a10cde25309e40d7f6e6b5e14ec3261.pdf [zhu2024]
102. 145fa4ea1567a6b9d981fdea0e183140d99aeb97.pdf [liu2023]
103. e9a13a97b7266ac27dcd7117a99a4fcbadc5fd9c.pdf [choi2020]
104. 4085a5cf49c193fe3d3ff19ff2d696fe20a5a596.pdf [ge2023]
105. 4e52607397a96fb2104a99c570c9cec29c9ca519.pdf [sadeghian2021]
106. eae107f7eeed756dfc996c47bc3faf381d36fd94.pdf [liu2024]
107. 7e5f318bf5b9c986ca82d2d97e11f50d58ee6680.pdf [li2022]
108. 8c93f3cecf79bd9f8d021f589d095305e281dd2f.pdf [rossi2020]
109. cab5194d13c1ce89a96322adaac754b2cb630d87.pdf [li2023]
110. 95c3d25b40f963eb248136555bd9b9e35817cc09.pdf [peng2020]
111. 12cc4b65644a84a16ef7dfe7bdd70172cd38cffd.pdf [ji2024]
112. 40479fd70115e545d21c01853aad56e6922280ac.pdf [zhang2024]
113. 5515fd5d14ac7b19806294119560a8c74f7fa4b2.pdf [kochsiek2021]
114. e5c851867af5587466f7cd9c22f8b2c84f8c6b63.pdf [yang2021]
115. eb14b24b329a6cc80747644616e15491ef49596f.pdf [shang2024]
116. 9c510e24b5edc5720440b695d7bd0636b52f4f66.pdf [asmara2023]
117. d9802a67b326fe89bbd761c261937ee1e4d4d674.pdf [gregucci2023]
118. b307e96f59fde63567cd0beb30c9e36d968fad8e.pdf [pan2021]
119. e4e7bc893b6fb4ff8ebbff899be65d96d50ccd1d.pdf [yoon2016]
120. c075a84356b529464df2e06a02bf9b524a815152.pdf [li2024]
121. b30481dd5467a187b7e1a5a2dd326d97cafd95ac.pdf [xiong2017zqu]
122. 2930168f3be575781939a57f4bb92e6b29c33b08.pdf [gong2020b2k]
123. da60d33d007681743d939861ae24f4cdac15667e.pdf [zhou2022ehi]
124. bb65c0898647c57c87a72e80d97a53576e3034ca.pdf [le2022ji8]
125. c03965d00865074ae66d0324c7145bf59aec73e6.pdf [zhou2022vgb]
126. 4b0e3d0721ea9324e9950b3bb98d917da8acb222.pdf [xu2019t6b]
127. 8df10fa4eca07dbb5fe2fe2ecc1e546cb8a8c947.pdf [mezni20218ml]
128. d6cc2a58df29d3e3fe4c55902880908dde32ee60.pdf [do2021mw0]
129. a57af41c3845a6d15ffbe5bd278e971ca9b8124a.pdf [mai2020ei3]
130. 8f255a7df12c8ec1b2d7c73c473882eacd8059d2.pdf [zhang2022eab]
131. 23ae48cdb8b7985e5a32fc79b6aae0de3230fe4f.pdf [sosa2019ih0]
132. 87ccb0d6c3e9f6367cd753538f4e906838cea8c2.pdf [guan2019pr4]
133. 0dddf37145689e5f2899f8081d9971882e6ff1e9.pdf [fan2014g7s]
134. 4be29e1cd866ab31f83f03723e2f307cdc1faab0.pdf [zhang20190zu]
135. 2a81032e5bb4b29f6e1423b6083b9a04bb54b605.pdf [chen2022mxn]
136. c88055688c4cd1e4a97da8601e90adbc0acdbd1e.pdf [wang2022hwx]
137. d97ec8a07cea1a18edf0a20981aad7e3dfe351e6.pdf [chen20226e4]
138. 389935511c395526817cf4ae62dae8913845ebdf.pdf [abusalih2020gdu]
139. ba524aa0ae24971b56eef6e92491de07d097a233.pdf [fang2022wp6]
140. a264af122f9f2ea5df46c030beb8ec0c25d6e907.pdf [elebi2019bzc]
141. 90450fe686c0fa645a1954950adffc5b2401e4b7.pdf [sha2019i3a]
142. 2257eb642e9ecae24f455a58dc807ee2a843081f.pdf [li2021ro5]
143. d77de3a4ddfa62f8105c0591fd41e549edcfd95f.pdf [xiao20151fj]
144. 52457f574780c53c68ad645fcdc86e2492b5074a.pdf [zhang2021wg7]
145. ac79b551ca16f98c1c3a5592c22d8093a492c4f3.pdf [wang20186zs]
146. 0abee37fe165b86753b306ffcc59a77e89de0599.pdf [li2021x10]
147. 512177d6b1e643b49b1d5ab1ad389666750144a9.pdf [wang202110w]
148. 60347869db7d1940958ee465b3010b3a612bf791.pdf [gutirrezbasulto2018oi0]
149. 9f7731d72e2aa251d2994eb1729c22aa78d0f718.pdf [portisch20221rd]
150. c7d3a1e82d4d7f6f1b6cffae049e930d0d3f487a.pdf [zhang2022muu]
151. 4ac5f7ad786fbee89b04023383a4fbe095ccc779.pdf [feng2016dp7]
152. 9fc2fd3d53a04d082edc80bafa470a66acdebb14.pdf [liu2021wqa]
153. 747dff7b9cd0d6feb16c340b684b1923034e8777.pdf [sang2019gjl]
154. 3e76e90180fc8300ecdeb5b543015cc68e0fd249.pdf [wang2017yjq]
155. 547dfe2a9d6a1bb1023f2208fb31f3a0671bf9ca.pdf [jiang20219xl]
156. 39eb51ae87c168ad4339214de6b91e2e2fdcfaa1.pdf [liu2022fu5]
157. fee5ac3604ccdefee2b65275fed47503234099e2.pdf [khan202236g]
158. 154fac5040865b4d74cf5a2cad39381c134a8b7d.pdf [mezni2021ezn]
159. 543497b1e551ad6473ddb9aa46697db28bccd3f5.pdf [zhang2021wix]
160. 6cc55dec26f5c078c6872d612c1561b1646d459a.pdf [huang2021u42]
161. ee5ceab9fa5f3bad231469923a03ad16184b51b9.pdf [pavlovic2022qte]
162. 3705cfe0d7dab8881518cb932f2465ca432d3f24.pdf [wang20213kg]
163. 882d6fe22a093ff95a8106a215bca37603ada710.pdf [zhang2019rlm]
164. 92ef8ff6715733697ca915c65cb18b160a764da6.pdf [mai20195rp]
165. a0ca7d39296d8d31dbbf300f58e7e375fb879492.pdf [han2018tzc]
166. 9155e1340e9263cf042d144681acccfc0c9d194b.pdf [wang2022fvx]
167. b5167990eda7d48f1a70a1fcb900ed5d46c40985.pdf [ferrari2022r82]
168. 0a8faa6c0e6dc9f743e96f276239d02d8839aca2.pdf [fu2022df2]
169. 71245f9d9ba0317f78151698dc1ddba7583a3afd.pdf [wu2018c4b]
170. f0499c2123e17106039e8e772878aad073ccf916.pdf [zhang202121t]
171. 2bdb9985208a7c7805676029300e3ba648125bd1.pdf [mohamed2019meq]
172. 7ccb05062f9ea7179532fd3355cf984b0102cfc5.pdf [xin2022dam]
173. c8214cac9c841f7b295a78c5bf71b6ed37c40eec.pdf [nie20195gc]
174. dab87bce4ac8c6033f5836f575b57c4a665b4f49.pdf [liu2018kvd]
175. 7ae22798887ff4e19033a8028007e1780b53ba8c.pdf [ni2020ruj]
176. 01c1e7830031b25410ed70965d239ac439a6fb68.pdf [li20215pu]
177. 021cbcd59c0438ac8a50c511be7634b0c00a1b89.pdf [yu2019qgs]
178. f211a2123e28d60cd8cdc05449c3cb7da2610b0a.pdf [fatemi2018e6v]
179. 3646e2947827c0a9314443e5cbb15575fafaf4ba.pdf [chen2021i5t]
180. 67c03d7a477059dc20faa02e3b45ca7055433615.pdf [dong2022c6z]
181. 91d8e1339eddee3217a6897cebdeb526b4bb1f72.pdf [lu20206x1]
182. b1464e3f0c82e21e23dfd9bc28e423856754b3d6.pdf [li2022nr8]
183. 57a7804d4e4e57de9a5c096ce7ea3e50d2c86f0f.pdf [luo2015df2]
184. 678dacdf029becac1116f345520f8e4afff5a873.pdf [zhou20216m0]
185. 1a25c8afacb6d36d4d8635eb9e3f8b8cf2e2122c.pdf [zhao202095o]
186. 60ad3ce0492a004020ff55653a51d6bfc457f12d.pdf [jia201870f]
187. 434b32d34b5d21071fc78a081741757f263c14ae.pdf [mai2018u0h]
188. 4a96636d1fc92221f2232d2d74be6e303cd0642a.pdf [li201949n]
189. 9c17d3f1837ae9f10f57c0b07c8288137d84026b.pdf [tang2020ufr]
190. e740a9aa753fcc926857ef4b90c1f91dd086e08d.pdf [guo2022qtv]
191. 315b239040f73063076014fdfabcc621b2719d83.pdf [jiang202235y]
192. 96b1f6fb6e904a674aef5cd32efee3edfa1c8ee2.pdf [liu201918i]
193. 5d6b4c5e48ec0585facea96a746bcbf7225d424c.pdf [zhang2020s4x]
194. 441f124d48662d6bd4f8e3190633371aa1b034eb.pdf [chang20179yf]
195. 5f9ea28be0d3bb9a73d62512190a772b10e92db0.pdf [lee2022hr9]
196. 836d1d1c94f0fd0713c77b86ce136fffd059dbc0.pdf [zhang2022fpm]
197. 0639efde0d9351bf5466235a492dbe9175f9cd5f.pdf [liu2019e1u]
198. 00529345e4a604674477f8a1dc1333114883b8d9.pdf [song2021fnl]
199. f0d5351c76448e28626177ece5ce97715087a0f9.pdf [gradgyenge2017xdy]
200. 9866a21c0ada20b62b28b3722c975595be819e24.pdf [zhou20218bt]
201. 50e7017c7768b7b2f5215a35539db1490ddc37ab.pdf [chen20210ah]
202. 95a501bfe4b09323e6e178edd64dc24a6935c23f.pdf [zhang2020i7j]
203. 46b5198a535dfcaf1cc7d57d471ad9ec050e46cf.pdf [boschin2020ki4]
204. cda7a1bdce2bfa77c2d463b631ff84b69ce3c9ed.pdf [wang20199fe]
205. f76a6e8f059820667af53edbd42d33fc4bca85fd.pdf [myklebust201941l]
206. 40667a731593a44d4e2f9391f1d14f368321b751.pdf [kartheek2021aj7]
207. 6bf53a97f5a3f5b0375f4702cbec28d8e9ab61c0.pdf [sha2019plw]
208. 4ae2631fb5e99cb64ff7d6e7ed3a1e6b0bedd269.pdf [lu2020x6y]
209. d76b3bf29366b4f0902ea145a3f7c020a35f084f.pdf [zhang2020c15]
210. 151c9bb547306d66ba252be7c20e35f711e9f330.pdf [li2020ek4]
211. c0827be29366be4b8cfa0dfbef4ead3f7b08f562.pdf [li2020he5]
212. 2d38cdaf2e232b5d1cb1dce388aa0fe75babcf29.pdf [kim2020zu3]
213. d6508e8825a6a1281ad415de47a2f108d98df87d.pdf [zhu2018l0u]
214. 18101998fb57704b79eb4c4c37891144ede8f8b9.pdf [do20184o2]
215. 23830bb104b25103162ec9f9f463624d9a434194.pdf [ma20194ua]
216. 77e23cd2437c6afb16082793badbb02842442e13.pdf [zhang2020wou]
217. 92351a799555df8d49465c2d4959118030339cc0.pdf [zhang2019hs5]
218. 6de535eb1b0024887227f7987e6eb22478af2a95.pdf [wang20198d2]
219. be7b102315ce70a7e01eb87c1140dd6850148e8b.pdf [tran20195x3]
220. 5b6a24ea3ffdccb14ce0267a815845c62ef026c9.pdf [xiong2018fof]
221. 75f7e3459e53fa0775c941cb703f049797851ef0.pdf [radstok2021yup]
222. 3ea066e35fdd45162a7fa94e995abe0beeceb532.pdf [zhao2020o6z]
223. c7a630751e45e3a74691bd0fc0880b4bf87be101.pdf [zhang20182ey]
224. a2a7f85d2ba28750725c4956eb14d53f6a90f003.pdf [jia20207dd]
225. bb0613ea0d39e35901aa0018de40deaf35cbbd5d.pdf [zhu2019ir6]
226. 509fa029989e89a4b82dd01ab75734aed937d684.pdf [wang2021dgy]
227. 4f2cc26b689cdac36ceb2037338eac65e7e5a193.pdf [ning20219et]
228. 7bb4cd36de648ca44cc390fe886ee70a4b2ad1ac.pdf [sheikh20213qq]
229. 93db6077c12cc83ea165a0d8851efb69b0055f3a.pdf [rim2021s9a]
230. 2f700be8a387101411a84199adfe30636e331752.pdf [zhang20179i2]
231. 2dba03d338170bde4e965909230256086bafa9f8.pdf [elebi20182bd]
232. c2648a294ef2fc299e1dd959bc1f92973f9c9ebc.pdf [garofalo20185g9]
233. 62c50e300ee87b185401ce27323bbb3f5262fdff.pdf [wang201825m]
234. 66f19b09f644578f808e69f38d3e76f8b972f813.pdf [chung2021u2l]
235. 9b68475f787be0999e7d09456003e664d37c2155.pdf [tran2019j42]
236. f0ac0c2f82886700dc7e7a178d597d33deebfc88.pdf [shi2017m2h]
237. a5aeca7ef265b27ff6d9ea08873c9499632b6439.pdf [zhang2017ixt]
238. 8412cc4dd7c8d309d7a84573637d4daaad8d33b5.pdf [zhu20196p1]
239. 8be21591c29d68d99e89a71fc7755f09f5eed3a1.pdf [kertkeidkachorn2019dkn]
240. 6493e6d563282fcb65029162a71cd2cb8168765b.pdf [zhu2019zqy]
241. d5eabc89e2346411134569a603e63a143d1d6552.pdf [zhang20193g2]
242. 89cf9719b97e69f5bb7d715d5a16609676c14e86.pdf [liu2019fcs]
243. 1c1b5fd282d3a1fe03a671f7d13092d49cb31139.pdf [kanojia20171in]
244. 7f7137d3e1de7e0e801c27d5e8b963dfd6d94eb4.pdf [gao2018di0]
245. 49899fd94cd272914f7d1e81b0915058c25bb665.pdf [mai2018egi]
246. e64557514ab856d22ddbb34bc23ffb7085d5d6b0.pdf [xiao2016bb9]
247. 7eece37709dceba5086f48dc43ac1a69d0427486.pdf [liu2024q3q]
248. 83424a4fea2e75311632059914bf358bc045435f.pdf [zhang2024cjl]
249. 3f8b13ede9f4d3a770ec8b4771b6036b9f603bfa.pdf [su2023v6e]
250. ac0c9afa9c19f0700d903e00a92e83e41587add3.pdf [zhu2023bfj]
251. f42d060fb530a11daecd90695211c01a5c264f8d.pdf [liu2024to0]
252. 7aca91d068d20d3389b28b8277ebc3d488be459f.pdf [wang2024vgj]
253. fa07384402f5c9d5b789edf7667bbcc555f381e3.pdf [li2024920]
254. 48c2e0d87b84efca7f11462bbdac1be1177e2433.pdf [lee202380l]
255. 51c18009b2c566d7cddc934b2cf9a1bca813f58f.pdf [shokrzadeh2023twj]
256. 5cbf9bc26b3d0471cb37c3f4a931990b1260d82d.pdf [gao2023086]
257. 4383242be5bdfb30ffa84e58cc252acfb58d4878.pdf [li2024sgp]
258. f26d45a806d1f1319f37eb41b8aa87d768a1d656.pdf [xue2023qi7]
259. 7b569aecc97f5fe57ce19ca0670a6b1bc62c7f7c.pdf [duan2024d3f]
260. 8bd3e0c1b6a68a1068da83003335ac01f1af8dcf.pdf [chen20246rm]
261. e83b693a44ec32ddfb084d13138e8d7ebc85a7c3.pdf [zhu2022o32]
262. f284977aa917be0ff15b835b538294b827135d19.pdf [mitropoulou20235t0]
263. f3fa1ef467c996b30242124a298b5b9d031e9ed5.pdf [shomer2023imo]
264. 61ef322fba87ccfd36c004afc875542a290fe879.pdf [wang202490m]
265. 5bef4d28d12dd578ce8a971d88d2779ec01c7ec5.pdf [li2024bl5]
266. c441b2833db8bd96b4ad133679a68f79d464ef59.pdf [li2024y2a]
267. edfbe0b62b9f628858d05b64bd830cf9b0a1ab74.pdf [jia2023krv]
268. 88e700e9fd6c14f3aa4502176a60512ca4020e35.pdf [huang2023grx]
269. 942541df1b97a9d1e46059c7c2d11503adc51c4c.pdf [wang2023s70]
270. abc424e17642df01e0e056427250526bc624f762.pdf [hou20237gt]
271. 825d7339eadadd2baf962f7d3c8fe7dc0cdc9819.pdf [jiang2023opm]
272. b6839f89a59132f0e62011a218ec229a27ffff6b.pdf [lu2022bwo]
273. 59116a07dbdb3cdeebb20085fdfde8b899de8f6a.pdf [djeddi2023g71]
274. 3cab78074e79122fd28cd76f37fd8805e8e4fc31.pdf [zhang20243iw]
275. ed21098804490b98899bcb7195084983ce69ed6c.pdf [le2023hjy]
276. 354b651dbc3ba2af4c3785ccbecd3df0585d30b2.pdf [yao2023y12]
277. c620d157f5f999d698f0da86fb91d267ad8ded5c.pdf [li2023y5q]
278. dc949e502e35307753a1acbcdf937f0cd866e63b.pdf [yang2022j7z]
279. a64167fcaa7a487575c6479510e57795afc9974e.pdf [banerjee2023fdi]
280. f9a575349133b2d4bf512cfb7754fca6d13b0a81.pdf [hu20230kr]
281. 5f850f1f522f959e2d3dcad263d05b0fdbb187c3.pdf [li2023wgg]
282. 4c68ee32d3db73d4d05803c1b3f2f4b929a88b78.pdf [hao2022cl4]
283. 2ac47be80b02a3ff1b87c46cf2b8c27e739c2873.pdf [khan20222j1]
284. b5aedc0464d04aa3fed7e59a141d9be7ee18c217.pdf [le2022ybl]
285. 463c7e28be13eb02620ad7e29b562bf6e5014ba2.pdf [liang202338l]
286. 7009fd9eb533df6882644a1c8e1019dc034b9cc5.pdf [khan2022ipv]
287. e186e5000174ea70729c90d465e60279c5f88646.pdf [he2022e37]
288. 70dc4c1ec4cda0a7c88751fb9a6b0c648e48e11f.pdf [shen2022d5j]
289. 5a8c6890e524b708dc262d3f456c985e8a46d7d1.pdf [di20210ib]
290. 86631a005e1a88a66926ac0c364ed0101a02b7e7.pdf [niu2020uyy]
291. 92b9aeabaaac0f20f66c5a68fbb4fc268c5eaae5.pdf [nie2023ejz]
292. ce494973ceefe5ac011f7e9879843530395fa9db.pdf [li2022du0]
293. 25edfb99d3b8377a11433cf7be2bcd9f8bfbdb87.pdf [daruna2022dmk]
294. 709a128e752414c973613814ddc2509f2abe092f.pdf [zhou20210ma]
295. 18fd8982051fc1de652a9882c2c52db11bca646b.pdf [kun202384f]
296. a7f0b4776d3df11cf0d0e72785c3035cc744726c.pdf [dong2022taz]
297. e2783f8aa4c61443760a8754cd6d88165d50b213.pdf [kamigaito20218jz]
298. 77fedfa533871c6c4218285493f725d5df4e74e5.pdf [krause2022th0]
299. 695ef4cf57b4fd0c7ec17a6e10dffade51f38179.pdf [zhang20213h6]
300. 90d5e74b18d03f733c6086418bfe9b20bb6a0a69.pdf [li2021tm6]
301. c495b2780accfbb53a932181e3c9fd957d16895d.pdf [wang2020au0]
302. 85bfec413860c072529ab8399676ab4b072f2e34.pdf [wei20215a7]
303. a89f61021e5382912aaeb3f69a6d8a6265787af4.pdf [zhang2021rjh]
304. b3cbbc1f34a20c22853f3dd347fd635b2e414fd5.pdf [sheikh202245c]
305. df7265b4652b21bc690497b3967a708d811ddd23.pdf [ren2021muc]
306. f6182d5c14c6047d197f1af842862653a13238f2.pdf [eyharabide2021wx4]
307. 082856e9b36fac60b9b9400abffaff0e74552fe1.pdf [hong2020hyg]
308. b25744d3c5d93e49b1906991dc8b5426ea2cf51d.pdf [huang2020sqc]
309. 18bcad2521cbe8df9d84b1adff1dd57c72c68a9d.pdf [kurokawa2021f4f]
310. bdd6c1a6695e3d201b70f4a913ffc758b74216e7.pdf [mohamed2021dwg]
311. e93565f447a42b158df27ba75385f5e2fc30dde7.pdf [gebhart2021gtp]
312. cf436f34ca6aabe1971c3531d465ecaa3d480d68.pdf [deng2024643]
313. 76016197d7d4f2213a4ace29988c93285793e154.pdf [liu2024zr9]
314. 9730f484b84074c1d61c154211ea06cc6ff20940.pdf [zhang2024zmq]
315. 10c388fa25dd6f07707a414946e5b7a674e7155b.pdf [he2024vks]
316. 7e6a50b70223dc00c712a17537fb7e23f8fd5ad4.pdf [zhang2024fy0]
317. ae58ebc99f67eed0de7f4ba2ca6f7ceb9ab056fb.pdf [zhang2024ivc]
318. 6ad02ad36e7a2c7d72d1a0b15ffc61dae2be1d7a.pdf [jing2024nxw]
319. 75ba0b92bcf095e7cd1544425f1818fed195f83f.pdf [jiang2024zlc]
320. 905d27e361c50da406439bdac25807dd38258fd8.pdf [han2024u0t]
321. b2646d9ee88c3dd6822b039a38c9604932aaaf47.pdf [quan2024o2a]
322. c7666fbaa49da21c465dbfabcf5fdd768b8c7b9e.pdf [liu2024tc2]
323. f1b7682df472a88fbaac3e6049f638ecec6937e7.pdf [hello2024hgf]
324. d66622beef468f7b934a5bf601cb8a3fcefe78f3.pdf [li2024z0e]
325. 20486c2fb358730ee99ae39b5e0a88d7b39ca720.pdf [yan2024joa]
326. b49f6029d681ac286ab929238f5aef5f352767c8.pdf [liu2024tn0]
327. c5a19440511a741edd1581d41d37d3e9b7088186.pdf [wang20245dw]
328. 822ad7c33316202a2511d300c6b8a263b758ad1a.pdf [long2024soi]
329. ba61c59abb560ff47a8dd780c8ccffb0af5e14c2.pdf [zhou2024ayq]
330. b3c340aa22bcd183c41836ef7265d656f741911f.pdf [huang2024t19]
331. 7c82aa0ae4b4e027a2df8afe9bbeccf88368c62f.pdf [lu2024fsd]
332. 0d9a788260e3abff4794d79f72b2b5ab2fb5abe5.pdf [liu2024yar]
333. 6cba788eea4fdb3bd0d1db4ecdd8a70040b81e62.pdf [khan20242y2]
334. 6c195ec2d5a491ffca9ab893968c4d44a6d0ce7d.pdf [xue2025ee8]
335. 37b274eb6fa68dede9f4aaad6dec1e2ea56095ce.pdf [long20248vt]
336. 9be88067bd7351b36bb0c698f5559ced3918a1d5.pdf [huang20240su]
337. e0d17f8b2fffff6c5eaf3f13bc45126196ddd128.pdf [wang2024nej]
338. a4b6e13efa80bedf8e588ac69f91fdaecc8e5077.pdf [wang2024c8z]
339. ccb6674576de48f8cfd99374c3b737a94dc3cb98.pdf [liu2024x0k]
340. 75b5c716e2b20b92a2a0f49674b7411a469a5575.pdf [li2024uio]
341. 8ff387296878f23632a588076823b160673866ab.pdf [zhang2024z78]
342. 6a66b459955959c4b8a67bd298ed291506923b7a.pdf [wang2024534]
343. 6b69c8848a1cc50ed8775beb483c71cfc314c66b.pdf [ni202438q]
344. d57e01d80c7f0f86b5e3f096b193ab9210e9095f.pdf [nie202499i]
345. a9bfb9ab236553768782f2b90a69c5625f033186.pdf [wang2024d52]
346. 6903aea3553a449257388580028e0bddf119d021.pdf [mao2024v2s]
347. 767d56fe80f7681b97943a8bff39f0b580e4acd8.pdf [jafarzadeh202468v]
348. 9e7799ef313143aa9c0669a7d1918fcfd5d21359.pdf [wang2024dea]
349. 563b3d57927b688e59322dbbfc973e5f1b269584.pdf [lu202436n]
350. 984c18fa61b10b6d1c34affc98f27ca8344d4224.pdf [han2024gaq]
351. 4a0048f1942a68e7c39adac43588d1604af26fc7.pdf [liu2024jz8]
352. 49dfd47177fa3aeab8a6bea82a77ec8bdb93bf1e.pdf [he2024y6o]
353. 2a5c888b2df4fd8c49aef46ee065422b00b178c0.pdf [fang20243a4]
354. 48c07506022634f332b410fb59dca9f61f89b032.pdf [zhang2024h9k]
355. 575af1587dea578d48eb27f45f008203565d9170.pdf [li2024wyh]
356. 7bd50842503e23e6479447b98912ac482ef43adc.pdf [dong2024ijo]
357. 4f0e1d5c77d463b136b594c891c4686fde7a1b12.pdf [wang20246c7]
358. c3861a930a65e8d9ee7ab9f0a6ee71e0e59df7ed.pdf [zhang2024yjo]
359. 217a4712feae7d7590d813d23e88f5fbb4f2c37f.pdf [liang20247wv]
360. cf696a919b8476a4d74b8b726e919812a2f05779.pdf [liu2024t05]
361. 91d5aa3d43237ec60266563ec6e8079f86532cfa.pdf [pham20243mh]
362. 58480444670ff933fe644563f7e2948a79503442.pdf [li2024gar]
363. 9b836b4764d4f6947ac684fd4ba3e8c3597d95bd.pdf [li2024nje]
364. bd0e8d6db97111686d02b51134f87439f8f1acfa.pdf [bao20249xp]
365. bea79d59ab3d203d06c88ebf67ac47cb34adeaa7.pdf [xu2024fto]
366. 241904795d94dcb1946ad46c9184c59899783af1.pdf [liang2024z0q]
367. 55dab161c25d1dd04fbeecdeca085274bfe8463f.pdf [liu2024ixy]
368. 3ff6b617cd839c9d85cb7b58aa6ad56e95b6cf69.pdf [dong2025l9k]
369. 9560ca767022020ccf414a2a8514f25b89f78cb3.pdf [zhang2025ebv]
370. d5c8dcc8f5c87c269780c7011a355b9202858847.pdf [liu20242zm]
371. a77b3c5f532e61af63a9d95e671ce02d8065ee24.pdf [yang2024lwa]
372. 2d12d1cec23e1c26c65de52100db70d91ca90035.pdf [li20246qx]
373. 4b1d0cf2b99aec85cdedceaef88c3a074de79832.pdf [liu2024mji]
374. 0845cea58467d372eb296fa1f184ecabe02be18b.pdf [chen2024efo]
375. 6a9caace1919b0e7bb247f0ecb585068c1ec4ff8.pdf [chen2024uld]
376. 30321b036607a7936221235ea8ec7cf7c1627100.pdf [wang2017zm5]
377. e03b8e02ddda86eafb54cafc5c44d231992be95a.pdf [li2021qr0]

## Literature Review

### Introduction

\section{Introduction}
\label{sec:introduction}

The landscape of artificial intelligence has been profoundly shaped by advancements in knowledge representation, evolving from early symbolic systems to the sophisticated structures of modern knowledge graphs (KGs). These KGs, which organize world knowledge into networks of entities and relations, have become indispensable for various AI tasks, yet their inherent symbolic nature presents significant challenges. Issues such as data sparsity, computational inefficiency in large-scale reasoning, and difficulty in capturing nuanced semantic similarities often hinder their full potential [68f34ed64fdf07bb1325097c93576658e061231e]. This introductory section establishes the foundational context for understanding Knowledge Graph Embeddings (KGEs), a paradigm-shifting approach that addresses these limitations by transforming discrete symbolic knowledge into continuous, low-dimensional vector spaces.

KGE methods have emerged as a central pillar in modern knowledge graph research, enabling machines to understand, reason with, and leverage complex relational data more effectively. By converting entities and relations into dense embeddings, KGEs facilitate seamless integration with advanced machine learning models, thereby enhancing capabilities in diverse AI applications such as link prediction, entity alignment, question answering, and recommender systems. This section begins by providing a comprehensive background on knowledge graphs, tracing their evolution and highlighting their structural characteristics and inherent challenges. Subsequently, it delves into the core motivations behind the development of knowledge graph embedding techniques, explaining how they overcome the limitations of traditional symbolic representations. Finally, this section delineates the scope and organizational structure of the entire literature review, offering readers a roadmap through the intricate landscape of KGE research. By laying this essential groundwork, we aim to underscore the significance and trajectory of KGEs in transforming symbolic knowledge into actionable, machine-understandable formats, thereby advancing the broader field of artificial intelligence.

\subsection{Background: Knowledge Graphs}
\label{sec:1_1_background:_knowledge_graphs}


Knowledge Graphs (KGs) represent a fundamental paradigm for organizing and representing world knowledge in a structured, machine-readable format. At their core, KGs are directed graphs composed of entities (nodes) and relations (edges), forming a collection of factual triplets in the form of (head entity, relation, tail entity) [ge2023, dai2020]. For instance, the triplet (Barack Obama, bornIn, Hawaii) explicitly states a factual relationship between two entities. This structured representation allows for explicit semantic connections, enabling machines to understand and process information in a manner closer to human cognition.

The historical trajectory of knowledge representation has seen a significant evolution, from early semantic networks and expert systems in artificial intelligence to the more formalized ontologies and the vision of the Semantic Web. These foundational efforts aimed to capture human knowledge in a symbolic form, providing a basis for logical reasoning and inference. With the advent of the internet and the explosion of digital information, the need for large-scale, interconnected knowledge bases became paramount. This led to the development of modern, expansive KGs such as Freebase (now largely integrated into Wikidata), DBpedia, and Wikidata itself [wang2014, lv2018, zhang2018]. These prominent examples serve as crucial repositories, aggregating and organizing vast amounts of world knowledge from diverse sources like Wikipedia, enabling a wide array of intelligent systems, from search engines and question-answering systems to recommender platforms [huang2019, sun2018].

Despite their immense utility and structured nature, symbolic KGs inherently face several significant challenges that limit their scalability, efficiency, and ability to handle real-world complexities. Firstly, reasoning with symbolic KGs, particularly when involving complex logical rules or multi-hop inference, can be computationally inefficient and resource-intensive, often exhibiting exponential complexity [ge2023, dai2020]. This makes real-time inference on large-scale KGs a formidable task. Secondly, KGs are almost always incomplete; real-world knowledge is vast and constantly evolving, making it practically impossible to explicitly represent every single fact. Symbolic methods struggle profoundly with this incompleteness, as they typically require explicit rules or complete data to infer missing links, leading to brittle and often inaccurate predictions in sparse environments. The "data sparsity" problem is a recurring theme, where many entities and relations have limited connections, hindering comprehensive analysis [ge2023, dai2020].

Furthermore, symbolic representations treat entities and relations as discrete, atomic tokens, which inherently limits their capacity to capture nuanced semantic similarities or implicit relationships. For example, while "car" and "automobile" are semantically very close, a purely symbolic KG would treat them as distinct, unrelated entities unless explicitly linked by a relation. This lack of inherent semantic fluidity makes it difficult to generalize knowledge or discover novel patterns based on underlying similarities. These limitations—computational inefficiency, difficulty in managing growing data, challenges in handling incompleteness, and the inability to capture implicit semantic similarities—collectively underscore the necessity for more advanced representation techniques. As highlighted in the broader context of knowledge graph embedding research, these issues motivate the fundamental shift towards embedding entities and relations into continuous, low-dimensional vector spaces, thereby transforming complex symbolic problems into more efficient vector operations and laying the "bedrock for representing complex relational data in a machine-understandable format" [cao2022]. This transition to embedding techniques is crucial for unlocking the full potential of KGs in modern AI applications, providing a robust and scalable foundation for knowledge inference and fusion [ge2023, dai2020].
\subsection{Motivation for Knowledge Graph Embedding}
\label{sec:1_2_motivation_for_knowledge_graph_embedding}

Knowledge Graphs (KGs) serve as powerful repositories of structured world knowledge, representing entities and their relationships in a symbolic, triple-based format (e.g., (subject, predicate, object)). While invaluable for many AI applications, traditional symbolic KGs inherently suffer from several critical limitations that impede their scalability, flexibility, and integration with modern machine learning paradigms. These limitations form the core motivation for the development of Knowledge Graph Embedding (KGE) techniques.

Firstly, symbolic representations are inherently **sparse and discrete**, making it challenging to capture nuanced semantic similarities between entities and relations [dai2020, cao2022]. For instance, while a symbolic KG might state (``Paris'', ``locatedIn'', ``France'') and (``Berlin'', ``locatedIn'', ``Germany''), it struggles to infer that ``Paris'' and ``Berlin'' are both capital cities or that ``France'' and ``Germany'' are both European countries without explicit rules or additional facts. This sparsity also makes it difficult to generalize to unseen entities or relations, as there is no inherent notion of proximity or relatedness in the discrete symbolic space.

Secondly, reasoning over large-scale symbolic KGs is often **computationally inefficient**. As KGs grow exponentially in size, performing complex queries or multi-hop reasoning becomes computationally expensive, often requiring graph traversal algorithms that do not scale well [dai2020]. Managing and processing vast amounts of discrete data poses significant challenges, leading to bottlenecks in real-world applications.

Thirdly, KGs are almost always **incomplete**, a pervasive issue that limits their utility. Many real-world facts are missing, and traditional symbolic methods struggle to infer these missing links without explicit, hand-crafted rules. This incompleteness directly impacts the performance of downstream tasks that rely on comprehensive knowledge.

To overcome these limitations, Knowledge Graph Embedding (KGE) emerged as a transformative approach, converting sparse, symbolic entities and relations into continuous, low-dimensional vector representations (embeddings) in a latent space [dai2020, cao2022]. This fundamental shift from symbolic to vector-based representation offers several profound advantages:

\begin{itemize}
    \item \textbf{Scalability and Efficiency:} By representing entities and relations as dense vectors, KGE models transform complex symbolic problems into efficient vector operations, such as distance calculations or dot products. This significantly enhances computational efficiency and scalability, enabling KGs with millions of entities and relations to be processed and reasoned over more effectively. The "Efficiency, Compression, and System Optimization" subgroup highlights this, with works like [zhu2020] demonstrating how knowledge distillation can reduce embedding parameters by 7-15x and increase inference speed, and [wang2021] proposing lightweight frameworks for efficient storage and inference. More recently, system-level optimizations like [zheng2024] aim for general and efficient KGE learning systems, achieving significant speedups.
    
    \item \textbf{Capturing Nuanced Semantic Similarities:} In the continuous embedding space, semantically similar entities or relations are mapped to proximate vectors. This allows KGE models to inherently capture nuanced semantic relationships that are difficult to express symbolically. For instance, translational models like TransH [wang2014] and rotational models like RotatE [sun2018] learn to represent relations as transformations (translations on hyperplanes or rotations in complex space) that connect head and tail entities, thereby capturing patterns like symmetry, antisymmetry, inversion, and composition. This capability is central to the "Core KGE Model Architectures and Expressiveness" subgroup.
    
    \item \textbf{Handling Incompleteness and Enabling Link Prediction:} KGE models are particularly adept at addressing KG incompleteness through link prediction. By learning the underlying patterns of existing facts, they can infer the plausibility of unobserved triples. This is achieved by scoring candidate triples based on the learned embeddings, effectively transforming the problem of finding missing links into an efficient vector similarity search [rossi2020]. This capability is a cornerstone of KGE applications, as detailed in the "Link Prediction and Knowledge Graph Completion" section.
    
    \item \textbf{Seamless Integration with Modern Machine Learning Pipelines:} KGEs provide a powerful bridge between structured knowledge and modern machine learning (ML) techniques, especially deep learning. The continuous vector representations can be seamlessly integrated as features or pre-trained components into various ML models for tasks like natural language processing, computer vision, and recommendation systems. This allows KGs to enrich data-driven models with structured background knowledge, enhancing their performance and interpretability. The "KGE for Downstream Applications and Explainability" subgroup exemplifies this, with models like RKGE [sun2018] and CKGE [yang2023] leveraging embeddings for explainable recommendation, and systems like Marie and BERT [zhou2023] integrating KGEs for chemistry-specific question answering.
    
    \item \textbf{Facilitating Diverse AI Tasks:} Beyond link prediction, KGEs enable a wide array of AI tasks by converting complex symbolic problems into efficient vector operations. These include:
    \begin{itemize}
        \item \textbf{Entity Alignment:} Identifying equivalent entities across different KGs by comparing their embeddings [sun2018, zhang2019].
        \item \textbf{Question Answering (QA):} Matching natural language questions to relevant facts in the KG by embedding both questions and KG elements into a common space [huang2019, zhou2023].
        \item \textbf{Recommendation Systems:} Modeling user-item interactions and preferences by embedding items and users within a KG context [sun2018, yang2023].
    \end{itemize}
\end{itemize}

While the conversion to vector space offers immense benefits, it also introduces challenges. Early KGE models, though efficient, often struggled to capture all complex relational patterns, leading to a continuous evolution towards more expressive geometric and deep learning models, as evidenced by the "Core KGE Model Architectures" and "Geometric KGE" subgroups. Furthermore, while embeddings make KGs more actionable for AI, the inherent interpretability of symbolic logic can sometimes be lost in dense vector spaces, spurring research into explainable KGEs [yang2023]. This strategic shift from explicit symbolic to implicit dense vector representations, as highlighted in the overall perspective, represents a fundamental progression in knowledge representation, making KGs more accessible, scalable, and powerful for a wide spectrum of AI applications.
\subsection{Scope and Structure of the Review}
\label{sec:1_3_scope__and__structure_of_the_review}

This literature review offers a comprehensive exploration of Knowledge Graph Embedding (KGE) research, meticulously tracing its evolution from foundational theoretical models to cutting-edge architectural advancements, critical practical considerations, and diverse real-world applications. The scope is designed to provide a pedagogical progression, beginning with core concepts and gradually building towards more sophisticated and specialized developments, thereby ensuring a coherent narrative that captures the field's dynamic trajectory. We aim to synthesize the vast landscape of KGE, offering a structured roadmap for understanding its complexities and future directions.

The review commences with an \textbf{Introduction} (Section 1), which sets the stage by outlining the fundamental role of knowledge graphs in AI and elucidating the core motivations behind embedding them into continuous vector spaces. This initial section establishes why KGE has become indispensable for overcoming the limitations of symbolic representations, such as scalability and the inability to capture nuanced semantic similarities.

Following this, \textbf{Foundational KGE Models and Geometric Paradigms} (Section 2) delves into the bedrock of KGE research. This section examines early and influential models, primarily those based on geometric and algebraic principles. It discusses how relations are conceptualized as transformations within embedding spaces, detailing the progression from simple translational models like TransH [wang2014] and TransD [ji2015] to more complex rotational approaches such as RotatE [sun2018]. These foundational works, categorized in the thematic taxonomy as "Core Translational Models and Their Extensions" and "Advanced Geometric Models," are critically analyzed for their ability to capture diverse relational patterns, including symmetry, antisymmetry, and composition, while balancing model capacity and computational efficiency. The limitations of earlier models in handling complex relation patterns often necessitated the development of more expressive geometric operations.

The review then transitions to \textbf{Deep Learning Architectures for Knowledge Graph Embedding} (Section 3), reflecting a significant paradigm shift in the field. This section explores how Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and Transformer models have been adapted to learn more expressive and context-aware representations. This progression highlights the move from predefined geometric transformations to data-driven feature extraction, enabling the capture of intricate structural patterns and non-linear relationships that were challenging for simpler models.

Building upon these architectural advancements, \textbf{Enriching KGE: Auxiliary Information, Rules, and Multi-modality} (Section 4) investigates methods that transcend purely structural information. This section details the integration of auxiliary data (e.g., entity types, attributes), explicit logical rules, and multi-modal information (e.g., text, images) to enhance embedding quality. This critical area addresses the inherent incompleteness and sparsity of KGs, demonstrating how external knowledge can lead to more robust, semantically rich, and interpretable embeddings.

Recognizing the dynamic nature of real-world knowledge, \textbf{Dynamic, Inductive, and Distributed KGE} (Section 5) focuses on models capable of handling temporal changes, learning embeddings for unseen entities, and operating in privacy-preserving, distributed environments. Models like HyTE [dasgupta2018] are crucial here, explicitly incorporating time to enable temporally aware inference. This section underscores the field's evolution towards more adaptable and scalable solutions, moving beyond static and centralized assumptions to meet the demands of evolving knowledge bases.

\textbf{Practical Considerations: Efficiency, Robustness, and Evaluation} (Section 6) addresses the critical challenges in deploying and evaluating KGE models. It covers strategies for improving computational efficiency, enhancing robustness against noisy data, and optimizing training processes. This section also critically examines the importance of rigorous evaluation, benchmarking, and reproducibility, drawing insights from comprehensive comparative analyses of state-of-the-art methods [rossi2020]. The discussion here highlights how experimental setups and reporting practices can significantly affect generalizability, emphasizing the need for standardized benchmarks and transparent methodologies to ensure reliable scientific progress. Survey papers like [dai2020] and [cao2022] further underscore the importance of systematic classification and comparison of KGE techniques based on their underlying representation spaces and performance across various benchmarks.

The review culminates with \textbf{Applications and Real-World Impact of KGE} (Section 7), showcasing the diverse utility of KGE across various AI tasks. This includes core applications like link prediction and knowledge graph completion, as well as more complex tasks such as entity alignment [sun2018, zhang2019], question answering [huang2019], and recommender systems [sun2018]. This section demonstrates how KGE bridges the gap between structured knowledge and practical AI problems, providing tangible benefits in various domains. The inclusion of application-specific KGE frameworks from the taxonomy, such as KEQA [huang2019] and RKGE [sun2018], illustrates the versatility and real-world impact of these embedding techniques.

Finally, the \textbf{Conclusion and Future Directions} (Section 8) synthesizes the key developments, identifies persistent open challenges, theoretical gaps, and practical limitations, and outlines emerging trends and ethical considerations. This forward-looking perspective aims to inspire new research and guide the responsible advancement of KGE technologies, providing a comprehensive roadmap for navigating the complex and rapidly evolving landscape of knowledge graph embedding research.


### Foundational KGE Models and Geometric Paradigms

\label{sec:foundational_kge_models__and__geometric_paradigms}

\section{Foundational KGE Models and Geometric Paradigms}
\label{sec:foundational_kge_models_and_geometric_paradigms}

Building upon the motivation to overcome the limitations of sparse symbolic knowledge graphs, this section delves into the pioneering efforts that established the bedrock of knowledge graph embedding research. It explores the early and influential models that first translated entities and relations into continuous vector spaces, laying the theoretical and practical groundwork for subsequent advancements. The primary focus here is on geometric and algebraic paradigms, which conceptualize relations not merely as static links, but as dynamic transformations or interactions within these learned embedding spaces. This fundamental shift enabled the capture of implicit semantic similarities and relational patterns that were previously inaccessible.

The evolution began with simple yet powerful translational models, such as TransE and its extensions like TransH [wang2014] and TransD [ji2015]. These models represent relations as direct translations from head to tail entities, offering an efficient way to capture basic relational patterns. However, their inherent limitations in modeling complex properties like symmetry, antisymmetry, and composition spurred the development of more sophisticated approaches. This led to the emergence of rotational models, exemplified by RotatE [sun2018], which leverage rotations in complex or higher-dimensional spaces to capture richer and more diverse relational semantics. Further innovations explored other geometric and algebraic structures, including embeddings on Lie groups or using quaternions, to enhance expressiveness and address specific challenges. Collectively, these foundational models were instrumental in demonstrating how geometric operations could effectively capture intricate relational patterns, significantly improving the expressiveness and utility of KGEs and forming the essential basis for the field's rapid expansion into more advanced architectures.

\subsection{Core Translational Models and Extensions}
\label{sec:2_1_core_translational_models__and__extensions}

The advent of knowledge graph embedding (KGE) marked a significant paradigm shift from purely symbolic knowledge representation to continuous vector spaces, offering enhanced efficiency and expressiveness for tasks such as link prediction and knowledge graph completion. At the forefront of this transformation were the translational models, which posited that a relation could be represented as a translation operation in an embedding space, moving a head entity vector closer to a tail entity vector. This foundational idea was first popularized by TransE, a pioneering model for its simplicity and computational efficiency [wang2014]. TransE models a triple $(h, r, t)$ by enforcing the constraint $\mathbf{h} + \mathbf{r} \approx \mathbf{t}$, where $\mathbf{h}, \mathbf{r}, \mathbf{t}$ are the embeddings of the head entity, relation, and tail entity, respectively. While remarkably effective for its time, TransE exhibited limitations in handling complex relational patterns, particularly one-to-many, many-to-one, and reflexive relations, where a single relation vector could not adequately distinguish between multiple valid tail entities for a given head, or vice-versa.

To address these inherent limitations, subsequent models extended the translational paradigm by introducing more sophisticated mechanisms. TransH emerged as a notable improvement, proposing to model relations as translations on relation-specific hyperplanes rather than directly in the entity embedding space [wang2014]. Specifically, for a triple $(h, r, t)$, TransH projects the head and tail entity embeddings ($\mathbf{h}, \mathbf{t}$) onto a hyperplane defined by the relation $\mathbf{r}$'s normal vector $\mathbf{w}_r$, resulting in projected entities $\mathbf{h}_{\perp}$ and $\mathbf{t}_{\perp}$. The translational assumption then applies to these projected vectors: $\mathbf{h}_{\perp} + \mathbf{d}_r \approx \mathbf{t}_{\perp}$, where $\mathbf{d}_r$ is the relation-specific translation vector on the hyperplane. This mechanism allows TransH to better distinguish entities involved in one-to-many or many-to-one relations, as different entity pairs can be projected onto different points on the hyperplane while sharing the same relation vector. For instance, if a person has multiple children, TransH can project the person and each child onto the 'has\_child' hyperplane, allowing distinct representations for each child while maintaining the 'has\_child' relation. This approach offered a crucial trade-off, significantly improving expressiveness for complex relation types with almost the same model complexity as TransE, thereby maintaining scalability [wang2014]. The recent review by [asmara2023] further underscores TransH's importance in addressing these early challenges.

Building upon the concept of relation-specific transformations, TransD further refined the translational approach by introducing dynamic mapping matrices for entities and relations [ji2015]. Unlike TransH, which uses a single hyperplane per relation, TransD employs two vectors for each entity and relation: one representing its meaning and another for constructing a dynamic mapping matrix. This allows for more fine-grained, entity-specific projections, where the projection matrix for a relation is dynamically constructed based on both the entity and relation vectors. The core idea is that different entities might interact with a relation in different ways, and a static projection (as in TransH) might not capture this diversity. TransD's dynamic mapping matrices provide a more adaptive mechanism to project entities into relation-specific spaces, thereby accounting for the diversity of both relations and entities. A significant advantage of TransD over its predecessors like TransR/CTransR (which used static, larger projection matrices) is its reduced parameter count and avoidance of computationally intensive matrix-vector multiplication operations, making it more scalable for large knowledge graphs [ji2015]. This efficiency gain, while increasing expressiveness, was a critical step in making KGE models practical for real-world applications.

These core translational models and their extensions collectively established a fundamental paradigm for KGE. They demonstrated that representing symbolic knowledge in continuous vector spaces could not only be efficient but also expressive enough to capture intricate relational semantics. While TransH and TransD significantly improved upon TransE's ability to model one-to-many/many-to-one relations, they still operated within the limitations of a Euclidean embedding space and relatively simple geometric transformations. This inherent simplicity, while beneficial for efficiency, meant they struggled with more complex logical patterns such as symmetry, antisymmetry, inversion, and composition, which later models like RotatE would address more elegantly through rotational transformations in complex spaces [sun2018]. Nevertheless, the foundational work of TransE, TransH, and TransD laid the essential groundwork, proving the viability of the embedding approach and setting the stage for the diverse array of KGE models that continue to influence modern research, as highlighted in various surveys [dai2020, cao2022]. Their emphasis on balancing model capacity with computational efficiency remains a crucial design principle in the field.
\subsection{Rotational and Complex Space Embeddings}
\label{sec:2_2_rotational__and__complex_space_embeddings}

While foundational translational models like TransE and TransH [wang2014] offered a significant step forward in knowledge graph embedding (KGE) by modeling relations as vector translations, they often struggled with capturing the full spectrum of complex relational semantics, such as symmetry, antisymmetry, inversion, and composition [rossi2020]. This limitation spurred the development of models that leverage rotations in complex or higher-dimensional spaces, offering more nuanced and powerful transformations to represent these intricate logical patterns. This represents a key methodological shift within the "Core KGE Model Architectures and Expressiveness" and "Geometric and Algebraic KGE Models for Complex Relations" subgroups, moving beyond simpler linear operations to richer algebraic structures.

A seminal contribution in this direction is RotatE [sun2018], which defines each relation as a rotation from the head entity to the tail entity in a complex vector space. By representing entities as vectors and relations as Hadamard products with complex-valued relation vectors (which correspond to rotations), RotatE inherently captures symmetry (rotation by $\pi$), antisymmetry (rotation by non-$\pi$ angles), inversion (rotation by negative angle), and composition (sequential rotations). This elegant formulation proved highly effective for modeling complex patterns and significantly outperformed existing state-of-the-art models for link prediction on benchmark datasets [sun2018]. The success of RotatE highlighted the expressive power of complex space embeddings, demonstrating how algebraic structures could directly encode logical properties.

Building upon this rotational paradigm, researchers explored extensions to higher-dimensional Euclidean and non-Euclidean spaces. Rotate3D [gao2020] extends the concept of relations as rotations to a three-dimensional Euclidean space. A key motivation for Rotate3D was to capture non-commutative composition patterns, which are essential for multi-hop reasoning and are naturally supported by rotations in 3D space. While RotatE primarily operates in a 2D complex plane for each dimension, Rotate3D generalizes this to a full 3D rotation, allowing for a richer set of transformations. Similarly, Orthogonal Relation Transforms [tang2019] further generalize this idea by employing high-dimensional orthogonal transforms, which encompass rotations and reflections, to model relations. This approach aims to retain the benefits of rotational models (symmetry, inversion, composition) while enhancing modeling capacity for complex relations like N-to-1 and 1-to-N by integrating graph context.

The pursuit of even more expressive geometric transformations led to models like HousE [li2022], which introduces Householder parameterization. Householder transformations, a type of reflection, can be generalized to represent rotations and projections in high-dimensional spaces. HousE aims to simultaneously capture crucial relation patterns and mapping properties, theoretically generalizing existing rotation-based models while extending rotations to higher dimensions. This exemplifies the continuous effort to find more powerful mathematical tools to encode relational semantics.

Further enhancing the complexity of transformations, CompoundE [ge2022] and its 3D extension, CompoundE3D [ge2023], propose using compound geometric operations, including translation, rotation, and scaling. These models treat relations not as a single operation but as a cascade of multiple transformations, suggesting that a richer set of combined operations can lead to better modeling capacity. CompoundE, by framing itself within group theory, demonstrates that several existing KGE models are special cases of its generalized framework, highlighting a trend towards unifying diverse geometric approaches. While these compound operations offer increased expressiveness, they also introduce greater model complexity and potentially higher computational costs, a common trade-off in KGE research [cao2022].

A significant recent development is the use of quaternions, an extension of complex numbers to four dimensions, to represent relations. ConQuatE [chen2025] leverages quaternion rotations to address the challenge of polysemy in knowledge graphs, where entities can exhibit different semantic characteristics depending on the relation. By incorporating contextual cues from various connected relations through efficient vector transformations in quaternion space, ConQuatE aims to capture diverse relational contexts without requiring extra information beyond original triples. This approach offers a novel way to handle the nuanced semantic variations that simpler rotational models might overlook, particularly for link prediction and multihop reasoning.

The "Geometric and Algebraic KGE Models for Complex Relations" subgroup analysis highlights that while these models achieve state-of-the-art performance, a common limitation is the potential for increased computational cost and parameter count, which can affect scalability to extremely large KGs. Moreover, the empirical validation often relies heavily on standard link prediction metrics, which may not fully capture the nuances of all the complex patterns these models aim to capture, especially for tasks like set retrieval or complex logical reasoning. For instance, MQuinE [liu2024] identifies and addresses a theoretical deficiency, termed the "Z-paradox," in some popular KGE models, demonstrating that even advanced models can suffer from subtle expressiveness issues that degrade performance on challenging test samples. This underscores that merely introducing complex operations is insufficient; theoretical soundness and complete expressiveness are paramount.

The intellectual trajectory in this area, as noted in the "Core KGE Model Architectures and Expressiveness" subgroup, shows a clear progression from specific rotational models to more theoretically grounded and generalized orthogonal transformations. HolmE [zheng2024] introduces a KGE model whose relation embedding space is "closed under composition," a crucial property for inherently modeling under-represented (long-tail) composition patterns and extrapolating to unseen relations. This addresses a theoretical gap where prior KGEs often considered relations compositional only if well-represented in training data. Similarly, GoldE [li2024] proposes a universal orthogonal parameterization based on a generalized Householder reflection, aiming to unify dimensional extension and geometric unification with theoretical guarantees, thereby capturing both logical patterns and topological heterogeneity. SpherE [li2024] further extends rotational embeddings by representing entities as spheres instead of vectors, specifically targeting the challenging problem of set retrieval and many-to-many relations, while maintaining interpretability. These advancements demonstrate a continuous drive to enhance the fundamental expressiveness of KGE models, ensuring they can effectively handle the complexities and imperfections of real-world knowledge graphs.
\subsection{Other Geometric and Algebraic Innovations}
\label{sec:2_3_other_geometric__and__algebraic_innovations}

Beyond the foundational translational and rotational paradigms, Knowledge Graph Embedding (KGE) research has continuously sought to refine its mathematical underpinnings by exploring a broader spectrum of geometric spaces and algebraic transformations. This pursuit is driven by the need for more expressive, theoretically sound, and robust representations capable of capturing the intricate nuances of real-world knowledge graphs.

One significant direction involves embedding entities and relations within non-Euclidean spaces, particularly Lie groups, to circumvent inherent limitations of standard vector spaces. [ebisu2017] introduced \textbf{TorusE}, a pioneering model that embeds entities on a Lie group, specifically a torus. The primary motivation for TorusE was to address the regularization problems encountered by models like TransE, where forcing entity embeddings onto a sphere in Euclidean space could warp representations and adversely affect link prediction accuracy. By leveraging the compact nature of a torus, TorusE naturally avoids the need for explicit regularization, as the space itself is bounded. While innovative in its choice of embedding space, TorusE still adheres to a translation-like principle, defining relations as translations within the Lie group. However, its geometric complexity, while elegant, might not inherently capture all forms of complex relation patterns, such as compositionality, as effectively as models designed with specific algebraic operations for such patterns.

Another critical area of innovation lies in scrutinizing and redefining the metric used within the embedding space. [yang2021] presented \textbf{CyclE}, which critically examines the implications of the widely adopted Minkowski metric in KGE. The authors argue that the choice of metric significantly influences the expressiveness of the embedding space and propose a novel "Cycle metric" based on the oscillation property of periodic functions. Their quantitative analysis suggests that a smaller function period in the Cycle metric leads to superior expressive ability. CyclE, by combining this new metric with popular KGE models, demonstrated enhanced performance. This work highlights a fundamental aspect of geometric KGE: the distance function itself is a crucial design choice. However, focusing solely on the metric, while foundational, may not inherently provide the rich *transformations* required to model complex logical patterns like transitivity or hierarchy without further architectural or operational enhancements.

A more direct approach to enhancing modeling capacity involves introducing advanced algebraic transformations. [li2022] proposed \textbf{HousE}, a powerful KGE framework that leverages Householder parameterization. HousE employs two types of Householder transformations: Householder rotations to achieve superior capacity for modeling relation patterns and Householder projections to handle sophisticated relation mapping properties (e.g., 1-to-N, N-to-1). Theoretically, HousE is capable of simultaneously modeling crucial relation patterns and mapping properties, and it generalizes existing rotation-based models by extending rotations to high-dimensional spaces. Empirically, HousE achieved state-of-the-art performance on several benchmarks, indicating its enhanced expressiveness compared to simpler rotation-based models like [gao2020] Rotate3D or [tang2019] Orthogonal Relation Transforms. The strength of HousE lies in its mathematically robust and versatile transformations, offering a richer set of operations than basic rotations.

Building upon the idea of combining multiple operations, [ge2022] introduced \textbf{CompoundE}, which integrates translation, rotation, and scaling operations into a cascaded compound transformation. This model views relations as complex geometric manipulations, demonstrating that a synergy of these operations can lead to highly expressive embeddings. Further extending this concept, [ge2023] developed \textbf{CompoundE3D}, which leverages 3D compound geometric transformations, including translation, rotation, scaling, reflection, and shear. CompoundE3D offers multiple design variants, allowing for flexibility to match the rich underlying characteristics of diverse knowledge graphs. These compound operation models represent a significant evolutionary step, as they generalize many existing scoring-function-based KGE models as special cases, effectively encompassing the strengths of both translational and rotational approaches while adding further dimensions of transformation. This contrasts with models like [yang2019] TransMS, which focuses on multidirectional semantics within a translation framework, or [peng2020] LineaRE, which models relations as simple linear functions.

Other notable algebraic innovations include [song2021] \textbf{Rot-Pro}, which combines projection and relational rotation to specifically model transitivity, a common but challenging relation pattern. [zhang2022] \textbf{TranS} introduces synthetic relation representations within transition-based frameworks to better handle complex scenarios where the same entity pair might have different relations. More recently, [liu2024] proposed \textbf{MQuinE}, which directly addresses and cures a theoretical deficiency termed the "Z-paradox" in some popular KGE models, thereby ensuring stronger expressiveness and theoretical soundness. The use of advanced algebraic structures is further exemplified by [chen2025] \textbf{ConQuatE}, which leverages quaternion rotations to capture diverse relational contexts and address the polysemy issue, where entities exhibit different semantic characteristics depending on the relation.

Collectively, these innovations highlight a continuous intellectual trajectory in KGE research: moving from simpler, single-operation models to more complex, multi-operation, multi-dimensional, and non-Euclidean spaces. This evolution is driven by the relentless quest to capture increasingly complex and nuanced relational patterns, thereby enhancing model expressiveness and theoretical rigor. While these models achieve state-of-the-art performance on various benchmarks, they often introduce increased mathematical complexity and computational costs, which can impact scalability, especially for extremely large knowledge graphs. Furthermore, the theoretical elegance of these geometric and algebraic models, while appealing, sometimes comes at the cost of interpretability, making it challenging to fully understand *why* certain transformations are optimal for specific relation patterns. The choice of the "best" geometric space or transformation remains highly dependent on the specific characteristics of the knowledge graph and the types of relations it contains.


### Deep Learning Architectures for Knowledge Graph Embedding

\label{sec:deep_learning_architectures_for_knowledge_graph_embedding}

\section{Deep Learning Architectures for Knowledge Graph Embedding}
\label{sec:deep_learning_architectures_for_knowledge_graph_embedding}

Building upon the foundational geometric and algebraic paradigms discussed in Section \ref{sec:foundational_kge_models_and_geometric_paradigms}, which established the initial framework for representing knowledge in continuous vector spaces, this section explores a significant paradigm shift in Knowledge Graph Embedding (KGE) research. While earlier geometric models, such as translational and rotational approaches, provided valuable insights into capturing specific relational patterns, their reliance on predefined transformations often limited their capacity to model the highly intricate, hierarchical, and non-linear relationships pervasive in real-world knowledge graphs [rossi2020, cao2022]. The advent of deep learning has revolutionized this landscape, enabling the development of more expressive and context-aware KGE models by allowing them to automatically extract features and learn complex interactions directly from data.

This section delves into how advanced deep learning architectures have been adapted to overcome these limitations, pushing the boundaries of KGE performance. We will detail the application of Convolutional Neural Networks (CNNs), which are leveraged to capture local features and intricate structural patterns within triplets and their neighborhoods. Subsequently, we examine Graph Neural Networks (GNNs), including various attention mechanisms, which inherently excel at encoding rich structural information and neighborhood context through message passing, thereby learning more robust, context-dependent embeddings. Finally, we explore the emergence of Transformer models in KGE, demonstrating how their powerful self-attention mechanisms capture long-range dependencies and contextualized representations, enabling the modeling of both global and local semantic relationships. This architectural evolution marks a crucial advancement, facilitating the learning of nuanced, non-linear relationships and paving the way for significantly enhanced performance across a spectrum of downstream AI tasks [dai2020, cao2022].

\subsection{Convolutional Neural Networks (CNNs) for KGE}
\label{sec:3_1_convolutional_neural_networks_(cnns)_for_kge}

Convolutional Neural Networks (CNNs) have emerged as a powerful paradigm in Knowledge Graph Embedding (KGE), offering a distinct advantage over traditional geometric models by automatically extracting local features and modeling intricate, non-linear interactions between entity and relation embeddings. Unlike models that rely on predefined geometric transformations, CNNs learn complex patterns directly from the data, enabling them to capture nuanced relational semantics that are often challenging for simpler approaches [cao2022]. This shift represents a significant evolution in KGE, moving towards more expressive and data-driven architectures.

Early applications of CNNs in KGE, such as AcrE [ren2020] and M-DCN [zhang2020], demonstrated their capability to enhance link prediction. AcrE, for instance, introduced atrous convolutions and residual learning to effectively increase feature interactions while maintaining a simpler structure and higher parameter efficiency. This approach addressed the limitation of conventional models in capturing diverse relation patterns by allowing for a broader receptive field without increasing the number of parameters. M-DCN further advanced this by proposing a multi-scale dynamic convolutional network, utilizing dynamic filters to extract richer and more expressive feature embeddings. M-DCN was particularly designed to handle complex relation patterns like 1-to-N, N-to-1, and N-to-N, which often pose significant challenges for translation-based or simple semantic matching models [ge2023]. The dynamic nature of its filters, tailored to each relation, allowed for a more adaptive modeling of these complex interactions.

The integration of attention mechanisms further refined CNN-based KGE models. ReInceptionE [xie2020] exemplified this by combining an Inception network with a relation-aware attention mechanism. The Inception network was employed to increase interactions between head and relation embeddings, while the attention mechanism enriched these embeddings with joint local and global structural information. This allowed ReInceptionE to adaptively utilize neighborhood context, a capability that purely convolutional models might partially miss, thereby bridging the gap between local feature extraction and broader graph topology awareness. This approach highlights an evolutionary trend within the "Deep Learning Architectures for KGE" subgroup, where models increasingly seek to combine the strengths of different neural components to capture a more comprehensive view of the knowledge graph.

More recent works continue to refine CNN-based techniques for KGE. CNN-ECFA [hu2024] introduced a Convolutional Neural Network-based Entity-specific Common Feature Aggregation strategy, aiming to improve knowledge graph representation learning by leveraging common features that are specific to entities. This model demonstrates that by aggregating entity-specific features, CNNs can learn more effective representations, outperforming state-of-the-art feature projection strategies. Similarly, SEConv [yang2025] proposed a semantic-enhanced KGE model, incorporating a less resource-consuming self-attention mechanism alongside a multi-layer CNN. The multi-layer CNN in SEConv is specifically designed to learn deeper structural features from triplets, while self-attention generates more expressive embedding representations. This model, with its application focus on healthcare prediction, underscores the practical utility of CNNs in learning discriminative features for specialized domains.

A key strength of CNN-based KGE models lies in their ability to automatically discover intricate, non-linear feature interactions, which contrasts with the hand-crafted transformations of geometric models (e.g., TransD [ji2015] or TorusE [ebisu2017]). This automatic feature learning often leads to superior performance in link prediction tasks, achieving state-of-the-art results on various benchmarks. However, this expressiveness comes with trade-offs. CNN models typically involve higher computational complexity and a larger number of parameters compared to simpler geometric models, potentially impacting scalability for extremely large knowledge graphs. Furthermore, while they excel at capturing local patterns, their ability to model long-range dependencies or global graph structures might be less direct than Graph Neural Networks (GNNs) or Transformer-based models, which are inherently designed for such tasks. The development trajectory of CNNs in KGE shows a clear progression from basic convolutional operations to more sophisticated designs incorporating multi-scale processing, dynamic filters, and attention, continually pushing the boundaries of what can be learned from entity-relation interactions.
\subsection{Graph Neural Networks (GNNs) and Attention Mechanisms}
\label{sec:3_2_graph_neural_networks_(gnns)__and__attention_mechanisms}


The integration of Graph Neural Networks (GNNs) and attention mechanisms represents a significant advancement in Knowledge Graph Embedding (KGE), moving beyond simple triplet-based interactions to leverage the rich topological and relational context of knowledge graphs. GNNs, through their inherent message passing and aggregation mechanisms, are uniquely suited to capture structural information and neighborhood context, which is crucial for understanding complex relational patterns and inferring missing links. This paradigm shift enables KGE models to learn richer, context-dependent embeddings by explicitly modeling multi-hop relational paths and local graph structures.

Early explorations into inductive capabilities for KGE, a key advantage of GNNs, were demonstrated by models like Logic Attention-based Neighborhood Aggregation (LAN) [wang2018]. LAN addressed the challenges of unordered and unequal neighbors by introducing a novel aggregator that uses both rule- and network-based attention weights. This allowed for the inductive embedding of new entities by aggregating information from their existing neighbors, a crucial step towards handling the dynamic nature of real-world knowledge graphs where new entities frequently emerge. However, while LAN provided a foundational approach to inductive learning, its attention mechanism was relatively simple and might not fully capture the nuanced importance of different relational paths.

Building upon the strengths of GNNs, Graph Attenuated Attention Networks (GAATs) [wang2020] further refined the use of attention. GAATs incorporated an attenuated attention mechanism to assign varying weights to different relation paths within the knowledge graph, thereby acquiring more informative features from neighbor nodes. This approach recognized that not all paths or neighbors contribute equally to an entity's representation, and by attenuating less relevant information, GAATs could learn more discriminative embeddings. This marked an improvement over uniform aggregation strategies, allowing entities and relations to be learned within any neighborhood context, enriching the feature extraction process.

A more sophisticated approach to GNN-based KGE is seen in DisenKGAT [wu2021], which introduced a novel Disentangled Graph Attention Network. DisenKGAT leverages both micro-disentanglement and macro-disentanglement to learn diverse and independent component representations. Micro-disentanglement is achieved through a relation-aware aggregation mechanism that generates varied component representations, while macro-disentanglement uses mutual information as a regularization to enhance the independence of these components. This disentangled approach allows the model to generate adaptive representations based on the given scenario, thereby capturing more diverse and nuanced semantics behind complex relations. DisenKGAT's ability to produce adaptive and explainable representations showcases a significant strength, addressing the limitation of single, static representations in traditional KGE models.

The ongoing research in GNNs for KGE also includes efforts to optimize their design and understand their generalization capabilities. For instance, [di2023] proposed a Message Function Search for KGE, aiming to automatically discover suitable GNN message functions for various KG forms (e.g., n-ary, hyper-relational data). This highlights a meta-level approach to GNN design, seeking to overcome the limitations of fixed GNN architectures by adapting them to specific data characteristics. Similarly, [li2021] delved into understanding *how* KGE models, particularly GNN-based ones, extrapolate to unseen data, proposing "Semantic Evidences" and introducing SE-GNN to explicitly model and merge these evidences for improved inductive capabilities.

While GNNs and attention mechanisms significantly enhance KGE by capturing complex structural and contextual information, they are not without limitations. A primary concern is their computational complexity and scalability, especially for very large knowledge graphs, as message passing can become resource-intensive. Furthermore, deep GNNs can suffer from over-smoothing, where entity representations become indistinguishable after many layers of aggregation, diminishing their discriminative power. The effectiveness of these models also heavily relies on the quality and density of local neighborhood information; sparse neighborhoods can limit their ability to learn rich contextual embeddings. Despite these challenges, the continuous development of more efficient GNN architectures, such as those explored in message function search [di2023], and a deeper understanding of their generalization properties [li2021], indicates a strong future for GNNs and attention mechanisms in KGE, pushing towards more robust and context-aware knowledge representation.
\subsection{Transformer-based KGE Models}
\label{sec:3_3_transformer-based_kge_models}

The emergence of Transformer architectures, initially lauded for their unparalleled success in natural language processing, has profoundly influenced the landscape of knowledge graph embedding (KGE). These models, characterized by their self-attention mechanisms, offer a powerful paradigm for capturing long-range dependencies and generating highly contextualized representations, capabilities that were often limited in earlier KGE approaches. The adaptation of Transformers to knowledge graphs represents a significant methodological evolution within the "Deep Learning Architectures for KGE" subgroup, pushing the boundaries of expressiveness beyond traditional geometric or simpler neural network models.

Early adaptations of Transformers to KGE primarily treated knowledge graphs as sequences of entities and relations. A pioneering example is CoKE (Contextualized Knowledge Graph Embedding) [wang2019], which frames edges and paths within a KG as sequences. By feeding these sequences into a Transformer encoder, CoKE learns dynamic, flexible, and fully contextualized embeddings for entities and relations. This approach marked a critical shift from static, context-independent embeddings, allowing the model to capture varying properties of entities and relations based on their surrounding graph context. While CoKE demonstrated superior performance in link prediction and path query answering, its sequence-centric view inherently grappled with the graph's non-sequential nature, potentially overlooking intricate topological structures that are not easily linearized. This limitation highlights a theoretical gap: how to reconcile the order-invariance of self-attention with the directed, ordered nature of relational facts in KGs.

Subsequent research has focused on explicitly integrating graph structures into Transformer frameworks, moving beyond simple sequence linearization. Knowformer [li2023] directly addresses the challenge of order invariance inherent in the self-attention mechanism, which struggles to distinguish between a valid triplet (subject, relation, object) and its shuffled variants. Knowformer innovatively incorporates relational compositions into entity representations, explicitly injecting semantics and capturing the role of an entity based on its position (subject or object) within a relation triplet. This design choice allows the Transformer to correctly capture relational semantics by distinguishing entity roles, a crucial advancement for modeling complex relational patterns that simpler translation-based models like TransD [ji2015] or even rotational models like RotatE [sun2018] might struggle to fully express without explicit positional encoding. Knowformer's ability to integrate positional awareness and relational semantics directly into the self-attention mechanism significantly enhances its expressiveness, particularly for modeling complex contextual information.

The latest advancements, such as TGformer [shi2025], further refine the integration of Transformer architectures with graph structures, proposing a general graph Transformer framework for KGE. TGformer is notable for being the first to explicitly leverage a graph Transformer to build knowledge embeddings that incorporate both triplet-level and graph-level structural features. This comprehensive approach addresses a critical limitation of previous methods: triplet-based models often ignore the broader graph structure, while some graph-based methods (e.g., certain GNNs like DisenKGAT [wu2021]) might overlook the specific contextual information of individual nodes within a triplet. By constructing context-level subgraphs for each predicted triplet and employing a Knowledge Graph Transformer Network (KGTN), TGformer fully explores multi-structural features, boosting the model's understanding of entities and relations in diverse contexts. Furthermore, TGformer extends its capabilities to temporal knowledge graphs, a significant step towards handling the dynamic nature of real-world knowledge, aligning with the broader development direction of building "more powerful and comprehensive models that leverage advanced neural architectures to capture increasingly complex structural and contextual information, including temporal dynamics."

The primary strength of Transformer-based KGE models lies in their ability to capture global and local semantic relationships through self-attention, leading to highly contextualized representations. This contrasts with CNN-based KGE models like AcrE [ren2020] or ReInceptionE [xie2020], which excel at extracting local features and interactions but may require additional mechanisms to capture long-range dependencies effectively. While CNN-ECFA [hu2024] and SEConv [yang2025] demonstrate the continued refinement of CNNs for feature aggregation, Transformers offer a more inherent capability for global context. Compared to foundational models like TorusE [ebisu2017] or CyclE [yang2021] that focus on refining the embedding space's geometry, Transformers provide a data-driven approach to learn complex, non-linear transformations and interactions, often achieving state-of-the-art performance in link prediction.

However, Transformer-based models also present trade-offs. Their computational complexity and high parameter count can pose scalability challenges for extremely large knowledge graphs, a practical constraint that needs careful consideration. While models like Knowformer address the initial order-invariance issue, the fundamental assumption of treating graph elements as sequences, even with sophisticated positional encodings, can sometimes be an oversimplification of the rich, multi-relational graph topology. The experimental setups for these models typically involve standard benchmark datasets, and while results are often superior, the generalizability to highly sparse or domain-specific KGs with limited data might still be a concern, requiring extensive pre-training or specialized fine-tuning. Despite these challenges, the innovative application of Transformers to graph structures, particularly in integrating multi-structural features and handling temporal dynamics, signifies a robust and adaptive direction for KGE research, continually pushing the state-of-the-art in capturing the intricate semantics of knowledge graphs.


### Enriching KGE: Auxiliary Information, Rules, and Multi-modality

\label{sec:enriching_kge:_auxiliary_information,_rules,__and__multi-modality}

\section{Enriching KGE: Auxiliary Information, Rules, and Multi-modality}
\label{sec:enriching_kge_auxiliary_information_rules_and_multi_modality}

Building upon the advancements in deep learning architectures for Knowledge Graph Embedding (KGE) discussed in Section \ref{sec:deep_learning_architectures_for_knowledge_graph_embedding}, which primarily focused on leveraging structural patterns within the graph, this section explores a crucial paradigm shift: enriching KGE models by integrating diverse external knowledge sources and logical constraints. While deep learning models like GNNs and Transformers have significantly enhanced the capture of intricate structural and contextual relationships, purely structural information often proves insufficient in addressing challenges such as data sparsity, ambiguity, and the need for more robust reasoning capabilities in complex, real-world scenarios [general_kge_review_1].

This section delves into advanced KGE approaches that move beyond the confines of graph topology alone, aiming to provide a more comprehensive and nuanced representation of knowledge. We will first examine how auxiliary information, such as entity types and attributes, can be seamlessly incorporated to provide semantic guidance, thereby improving embedding quality and robustness, particularly for incomplete or noisy knowledge graphs. Subsequently, the discussion will shift to the integration of explicit logical rules and constraints, which inject prior knowledge into the embedding process, enhancing reasoning capabilities and model interpretability by ensuring learned representations adhere to logical patterns. Finally, we explore the burgeoning field of multi-modal KGE, detailing how information from diverse modalities, including textual descriptions and visual features, can be leveraged to overcome data sparsity and enrich semantic understanding, enabling more holistic knowledge representation [general_kge_review_2]. By exploring these complementary dimensions, this section highlights how KGE models can achieve superior performance, interpretability, and applicability in complex AI tasks.

\subsection{Incorporating Auxiliary Information (Types, Attributes)}
\label{sec:4_1_incorporating_auxiliary_information_(types,_attributes)}

The effectiveness of Knowledge Graph Embedding (KGE) models, while primarily driven by structural information, can be significantly enhanced by integrating auxiliary semantic information such as entity types and attributes. This approach moves beyond the simplistic triplet structure, grounding embeddings in a richer context to yield more semantic, discriminative, and robust representations, particularly crucial when dealing with incomplete or noisy knowledge graphs (KGs). The intellectual trajectory in this area reflects a growing recognition that external, well-structured knowledge can bridge gaps that purely structural models cannot, contributing to the development of more inherently capable KGE models.

Early efforts to incorporate auxiliary information often focused on entity types. [wang2021] proposed \textit{TransET}, a novel KGE model that leverages entity types to learn more semantic features. By utilizing circle convolution based on entity and entity type embeddings, TransET maps head and tail entities to type-specific representations, which are then used in a translation-based scoring function. This method demonstrated that even a relatively straightforward integration of type information could lead to improved performance in link prediction and triple classification tasks. Building on this, [he2023] introduced \textit{TaKE}, a more universal \textit{Type-augmented Knowledge graph Embedding framework}. TaKE distinguishes itself by automatically capturing type features without explicit supervision and learning relation-specific type representations, allowing for a nuanced understanding of how entity types interact with different relations. Furthermore, TaKE incorporates a type-constrained negative sampling strategy, which is critical for constructing more effective negative samples during training, a fundamental aspect for KGE robustness [sachan2020]. While TransET provided a specific model, TaKE offers a generalizable framework that can enhance various traditional KGE models, showcasing a methodological evolution towards broader applicability.

The utility of type information extends to domain-specific applications. [hu2024] presented \textit{SR-KGE}, a \textit{GeoEntity-type constrained knowledge graph embedding} framework designed for predicting natural-language spatial relations. This approach integrates geoentity types as a constraint, combining graph structures with semantic attributes to capture spatial and semantic relations more accurately. While TaKE provides a universal type integration, SR-KGE exemplifies how tailored auxiliary information, when applied to a specific domain, can yield superior results for specialized tasks. The strength of these type-augmented methods lies in their ability to provide semantic guidance, making embeddings more discriminative by enforcing type consistency and enriching the relational context. However, a common limitation is their reliance on the availability and quality of type information; if types are sparse, noisy, or inconsistently defined, the benefits may diminish, and the complexity of integrating diverse type hierarchies can be substantial.

Beyond explicit types, entity attributes offer a richer, instance-specific form of auxiliary information. [zhang2024] addressed the critical problem of erroneous triples in KGs by proposing \textit{AEKE}, a framework for \textit{Attributed Error-aware Knowledge Embedding}. AEKE leverages entity attributes to guide the KGE model in learning against the impact of erroneous triples. It designs triple-level hypergraphs to model both KG topological structures and attribute structures, jointly calculating confidence scores for each triple based on self-contradiction, structural consistency, and attribute homogeneity. These confidence scores then adaptively weigh contributions during multi-view graph learning and margin loss calculation, ensuring that potentially erroneous triples have minimal impact. AEKE represents a significant step towards enhancing KGE robustness, moving beyond merely completing KGs to making them more reliable. While type-based methods provide broad semantic categories, attribute-based approaches like AEKE offer fine-grained details that can be crucial for identifying and mitigating data quality issues.

The scope of auxiliary information also extends to hyper-relational facts, moving "beyond triplets" to capture richer contextual data. [rosso2020] introduced \textit{HINGE}, a \textit{hyper-relational Knowledge Graph Embedding model} that directly learns from hyper-relational facts, where each fact includes a base triplet (\textit{h, r, t}) and associated key-value pairs (\textit{k, v}). HINGE captures not only the primary structural information of the KG but also the correlation between each triplet and its associated key-value pairs. This is a crucial distinction from type or attribute integration, as HINGE directly models additional structured facts that are part of the knowledge base, rather than meta-information about entities. This allows for a more comprehensive understanding of complex data semantics, outperforming models that rely solely on triplets or transform hyper-relational facts into less structured n-ary representations.

In synthesis, the integration of auxiliary information, whether through entity types [wang2021, he2023, hu2024], attributes [zhang2024], or hyper-relational facts [rosso2020], represents a vital direction in KGE research. These approaches collectively address the limitations of purely structural KGEs by providing richer semantic context, making embeddings more discriminative and robust to noise and incompleteness. The evolution from specific type integration (TransET) to universal frameworks (TaKE) and domain-specific applications (SR-KGE) highlights a growing sophistication. Furthermore, the focus on error-aware learning through attributes (AEKE) and the direct modeling of hyper-relational facts (HINGE) underscore the field's commitment to developing KGE models that can handle the complexities and imperfections of real-world knowledge graphs. A key trade-off, however, is the increased reliance on the availability and quality of this auxiliary data, which may not always be consistent across diverse KGs. Nevertheless, these advancements are crucial for pushing KGE towards greater practical utility and theoretical soundness, enabling more intelligent and reliable AI applications.
\subsection{Rule-based and Constraint-driven KGE}
\label{sec:4_2_rule-based__and__constraint-driven_kge}

While purely data-driven knowledge graph embedding (KGE) models excel at capturing statistical patterns from observed triples, they often struggle with ensuring logical consistency, facilitating explicit reasoning, and providing interpretability aligned with human understanding. This subsection delves into approaches that integrate logical rules and explicit constraints directly into the KGE learning process, thereby injecting prior knowledge to address these limitations.

One foundational approach to injecting semantic consistency is exemplified by "Semantically Smooth Knowledge Graph Embedding" (SSE) [guo2015]. This method enforces a "semantically smooth" embedding space, where entities belonging to the same semantic category are encouraged to lie close to each other. By employing manifold learning techniques, such as Laplacian Eigenmaps and Locally Linear Embedding, as regularization terms, SSE guides the embedding process to discover intrinsic geometric structures that reflect categorical semantics. While effective in promoting semantic coherence, SSE primarily relies on entity categories as a form of soft constraint, which is less explicit than logical rules and might not directly enhance complex reasoning capabilities. Its strength lies in its generality, as the smoothness assumption can be applied to various embedding models and constructed from diverse information beyond just entity categories.

A significant advancement in this domain is the explicit incorporation of logical rules. Early rule-based methods often relied on hard rules, which are rigid and require extensive manual curation. However, real-world knowledge graphs are often noisy and incomplete, making hard rules brittle. "Knowledge Graph Embedding with Iterative Guidance from Soft Rules" (RUGE) [guo2017] introduced a novel paradigm to iteratively integrate soft rules (rules associated with confidence levels) into the embedding learning process. RUGE simultaneously learns from observed triples, unlabeled triples (whose labels are iteratively predicted), and automatically extracted soft rules. This iterative guidance allows the knowledge embodied in rules to be progressively transferred into the learned embeddings, leading to more robust representations. The key advantage of RUGE is its ability to leverage abundant, albeit uncertain, automatically extracted rules, moving beyond the limitations of manually curated hard rules. This iterative feedback loop between rule inference and embedding updates represents a crucial step towards deeply intertwining symbolic logic with subsymbolic representations.

Complementing complex rule integration, simpler structural constraints can also significantly enhance KGE models. "Improving Knowledge Graph Embedding Using Simple Constraints" [ding2018] demonstrated that even straightforward constraints can yield substantial improvements in interpretability and structure without adding significant computational overhead. Specifically, this work explored non-negativity constraints on entity representations, which help learn compact and interpretable features, and approximate entailment constraints on relation representations. These entailment constraints encode regularities of logical entailment between relations, structuring the embedding space to reflect hierarchical or inferential relationships. While these constraints are less expressive than full first-order logic rules, their simplicity makes them highly efficient and broadly applicable, offering a practical trade-off between model complexity and the benefits of injected prior knowledge.

More recent contributions have further refined the integration of soft rules. "Knowledge Graph Embedding Preserving Soft Logical Regularity" [guo2020] focused on imposing soft rule constraints directly on relation representations. By representing relations as bilinear forms and mapping entity representations into a non-negative and bounded space, the method derives a rule-based regularization that enforces relation representations to satisfy rule-introduced constraints. A notable strength of this approach is its improved scalability, as the complexity of rule learning becomes independent of the entity set size, making it more feasible for large-scale KGs. This direct regularization of relations ensures that logical patterns are preserved in the relational space, which is crucial for consistent reasoning.

Building on these foundations, RulE ("Rule Embedding") [tang2022] presents a principled framework that learns rule embeddings jointly with entity and relation embeddings within a unified vector space. Unlike previous methods that might treat rules as external regularization, RulE explicitly represents rules as vectors, allowing for soft logical inference directly within the embedding space. This deep integration enables rule embeddings to regularize and enrich entity/relation embeddings, leading to more coherent and reasoning-capable representations. RulE's ability to calculate a confidence score for each rule based on its consistency with observed triples further refines the "softness" of logical inference, alleviating the brittleness often associated with strict logic. This joint learning paradigm represents a sophisticated approach to intertwining symbolic knowledge with subsymbolic representations, pushing the boundaries of reasoning capabilities within KGEs.

Collectively, these rule-based and constraint-driven methods highlight a critical evolution in KGE research. They move beyond purely data-driven models (like those in the "Core KGE Model Architectures" subgroup) by leveraging explicit logical knowledge to enhance robustness, improve reasoning, and increase interpretability. The progression from general semantic smoothness [guo2015] to iterative soft rule guidance [guo2017], the application of simple yet effective structural constraints [ding2018], and finally to the joint embedding of rules themselves [tang2022] demonstrates a continuous effort to bridge the gap between symbolic logic and continuous vector spaces.

However, several challenges persist. A primary limitation across many rule-based approaches is the reliance on the availability and quality of logical rules. While methods like RUGE can leverage automatically extracted soft rules, the efficiency and accuracy of such rule extraction remain a practical hurdle. Furthermore, balancing the strict adherence to logical rules with the flexibility to capture exceptions or nuanced patterns not covered by rules is a delicate trade-off. Over-constraining the embedding space with rules might inadvertently reduce its expressiveness or ability to generalize to unseen, complex scenarios. The scalability of managing and applying very large and complex rule sets, particularly for higher-order logic, also poses a significant challenge, despite efforts like [guo2020] to optimize rule learning complexity. Future research needs to explore more robust and automated rule discovery mechanisms, develop adaptive frameworks that can dynamically weigh rule adherence against data-driven insights, and investigate more principled theoretical foundations for combining probabilistic logic with continuous embeddings.
\subsection{Multi-modal and Cross-domain KGE}
\label{sec:4_3_multi-modal__and__cross-domain_kge}

The limitations of relying solely on structural information in knowledge graphs, particularly data sparsity and the inability to capture rich semantic nuances, have driven significant research into multi-modal and cross-domain Knowledge Graph Embedding (KGE). These approaches integrate diverse information sources, such as textual descriptions, visual features, or data from multiple domains, to enrich representations and enable more comprehensive knowledge understanding. The core motivation is to leverage complementary information, thereby enhancing the expressiveness and robustness of KGE models.

Early efforts in multi-modal KGE primarily focused on integrating textual descriptions to augment structural embeddings. For instance, the Semantic Space Projection (SSP) model [xiao2016] proposed a method that jointly learns from symbolic triples and textual descriptions. SSP projects textual information into a semantic space, using text to discover semantic relevance and provide more precise embeddings. This was a crucial step in addressing the "weak-semantic" nature of purely geometric models, which often struggle to differentiate entities with similar structural positions but distinct semantic meanings. While SSP offered a foundational approach, its text integration was relatively simple, often relying on bag-of-words or basic word embeddings.

More recent advancements have leveraged the power of pre-trained language models (PLMs) for deeper semantic understanding. The "Joint Language Semantic and Structure Embedding for Knowledge Graph Completion" model [shen2022] exemplifies this by fine-tuning PLMs with a probabilistic structured loss. This method effectively captures semantics from natural language descriptions while simultaneously reconstructing structural information, demonstrating state-of-the-art performance, particularly in low-resource settings where semantic cues are invaluable. This approach highlights a significant evolutionary trend, moving from simple text projection to sophisticated joint learning frameworks that deeply intertwine language semantics with graph structure. A key strength of such models is their ability to infer relations and entity properties even when structural data is sparse, a common challenge in many real-world KGs. However, the computational cost of fine-tuning large PLMs and the availability of high-quality textual descriptions for all entities remain practical constraints.

Beyond textual data, multi-modal KGE has expanded to incorporate other modalities and domain-specific knowledge. For instance, in the biomedical domain, "Multimodal reasoning based on knowledge graph embedding for specific diseases" [zhu2022] constructs Specific Disease Knowledge Graphs (SDKGs) and implements multimodal reasoning using reverse-hyperplane projection. This model integrates structural, category, and description embeddings to discover new, reliable knowledge, showcasing how combining different modalities can lead to enhanced insights in specialized contexts. This demonstrates the power of multimodal integration in addressing the unique complexities and data characteristics of domain-specific KGs. However, the generalizability of such highly specialized models to other domains without significant re-engineering remains an open question. Furthermore, the challenge of designing effective negative sampling strategies becomes more pronounced in multi-modal settings, as highlighted by [zhang2023], which proposes Modality-Aware Negative Sampling (MANS) to align structural and visual embeddings, underscoring that training optimization must adapt to the complexity of heterogeneous data.

Cross-domain KGE extends this concept by enabling knowledge transfer and interaction across distinct knowledge graphs or domains. This is particularly vital for applications like recommender systems, where user preferences and item characteristics often span multiple categories or platforms. "Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation" [liu2023] addresses the critical problems of cross-domain cold start and multi-domain recommendations. This approach proposes a "binding rule" to efficiently interact items across multiple domains, allowing for both homo-domain and hetero-domain item embeddings. By modeling associations and interactions between items across diverse domains, this method significantly improves multi-domain item-item recommendations, outperforming traditional recommender systems that struggle with data sparsity in new domains. The strength lies in its ability to leverage shared entities or relations to bridge information gaps, enriching representations for items even in domains with limited data. A limitation, however, is the reliance on explicit links or shared entities between domains, which may not always be readily available or accurately reflect complex cross-domain relationships.

In summary, multi-modal and cross-domain KGE represent a crucial evolutionary trajectory in knowledge representation. They move beyond the limitations of purely structural models by integrating diverse, complementary information sources. While models like SSP [xiao2016] laid the groundwork for textual integration, the field has progressed to sophisticated joint learning frameworks leveraging pre-trained language models [shen2022] and domain-specific multimodal reasoning [zhu2022]. Concurrently, cross-domain approaches [liu2023] tackle challenges like data sparsity and cold start in complex applications such as recommendation. The collective contribution of these methods is the creation of richer, more semantically grounded, and robust embeddings, which are essential for comprehensive knowledge understanding and practical applicability in diverse AI tasks. However, challenges persist in effectively fusing heterogeneous information, managing increased computational complexity, and designing robust training strategies like modality-aware negative sampling [zhang2023].


### Dynamic, Inductive, and Distributed KGE

\section{Dynamic, Inductive, and Distributed KGE}
\label{sec:dynamic,_inductive,__and__distributed_kge}

\label{sec:dynamic_inductive_and_distributed_kge}
\section{Dynamic, Inductive, and Distributed KGE}

Building upon the enriched KGE models discussed in Section \ref{sec:enriching_kge_auxiliary_information_rules_and_multi_modality}, which focused on leveraging diverse auxiliary information and logical constraints to deepen semantic understanding, this section shifts attention to the critical operational challenges of Knowledge Graph Embeddings in real-world environments. Traditional KGE models often assume static, complete, and centrally managed knowledge graphs, a paradigm increasingly at odds with the dynamic, evolving, and distributed nature of modern knowledge bases. This section addresses these fundamental limitations by exploring advanced KGE methodologies designed for adaptability, scalability, and security in complex, real-world operational settings, moving beyond static and centralized assumptions to meet the demands of modern knowledge management systems.

We delve into three interconnected areas. First, \textit{Temporal Knowledge Graph Embedding (TKGE)} tackles the inherent dynamism of real-world facts, where entities and relations change over time. These methods move beyond static representations to capture the fluidity and evolution of knowledge, crucial for tasks requiring reasoning over time. Second, \textit{Inductive and Continual KGE} addresses the challenge of unseen entities and the need for continuous model updates. This area explores how KGE models can efficiently learn embeddings for new entities without full retraining and adapt to a constant influx of new facts, mitigating catastrophic forgetting. Finally, \textit{Federated and Privacy-Preserving KGE} investigates collaborative learning paradigms for distributed knowledge graphs. This crucial direction enables multiple parties to jointly train robust KGE models while safeguarding sensitive data, addressing the growing demand for privacy-aware AI systems. Together, these advancements are pivotal for transitioning KGE from theoretical constructs to robust, secure, and continuously operational components within complex, real-world knowledge management systems.

\subsection{Temporal Knowledge Graph Embedding (TKGE)}
\label{sec:5_1_temporal_knowledge_graph_embedding_(tkge)}

The inherent dynamism of real-world knowledge necessitates models capable of capturing the temporal evolution of facts within knowledge graphs (KGs). Traditional Knowledge Graph Embedding (KGE) models, primarily designed for static KGs, fall short in tasks requiring reasoning over time or understanding the fluidity of information. Temporal Knowledge Graph Embedding (TKGE) addresses this by explicitly integrating time into the embedding process, moving beyond static representations to model evolving entities and relations [dai2020].

Early approaches to TKGE focused on explicitly structuring and modeling time itself. [dasgupta2018] introduced \textit{HyTE}, a hyperplane-based method that associates each timestamp with a corresponding hyperplane in the entity-relation space. This allows for temporally guided KG inference and prediction of temporal scopes for facts, marking a significant step towards dynamic KGEs. While intuitive, HyTE's reliance on simple hyperplanes might struggle with highly complex, non-linear temporal dependencies. Another prominent method involves treating the entire fact set as a higher-order tensor, typically a fourth-order tensor (head, relation, tail, time), and applying tensor decomposition to learn dense, low-dimensional temporal embeddings [lin2020]. This provides a robust mathematical framework for integrating time as a distinct dimension. Complementing this, \textit{ATiSE} models temporal evolution using additive time series decomposition, mapping representations into multi-dimensional Gaussian distributions where covariance captures temporal uncertainty, offering a probabilistic view of temporal dynamics [xu2019]. More recently, \textit{TeAST} innovatively structures time by mapping relations onto an Archimedean spiral timeline, transforming the quadruple completion problem into a 3rd-order tensor completion task. This approach aims to ensure relations evolve orderly over time with a spiral regularizer, offering a degree of interpretability regarding temporal patterns [li2023]. A common limitation across these tensor and time series methods is the computational cost associated with higher-order operations or complex decompositions, especially for very dense temporal data.

A significant advancement in TKGE involves leveraging geometric transformations to model temporal dynamics. \textit{TeRo} defines the temporal evolution of entity embeddings as a rotation from an initial time to the current time in a complex vector space, representing relations for time intervals with dual complex embeddings [xu2020]. Building upon this, \textit{ChronoR} extends the concept by employing k-dimensional rotation transformations, parametrized by relation and time, to transform a head entity to fall near its tail entity. This effectively captures rich interactions between temporal and multi-relational characteristics [sadeghian2021]. While powerful, the interpretability of complex rotations in high-dimensional spaces can be challenging, and the computational complexity associated with learning these transformations can be substantial, particularly for very large KGs or highly granular temporal data.

More advanced methods, particularly emerging in recent years, address the complexities of dynamic, spatiotemporal, and even fuzzy knowledge by moving beyond a single Euclidean space. \textit{MADE} and \textit{IME}, both published in 2024, represent a cutting-edge shift towards modeling TKGs in multi-curvature spaces, including Euclidean, hyperbolic, and hyperspherical geometries [wang2024, wang2024a]. The rationale is that TKGs often contain interwoven complex geometric structures (e.g., hierarchical, ring, chain) that no single curvature space can optimally capture. MADE introduces an adaptive weighting mechanism to assign different weights to these spaces in a data-driven manner, along with a quadruplet distributor and temporal regularization for timestamp smoothness [wang2024]. IME, on the other hand, incorporates "space-shared" properties to learn commonalities across spaces and alleviate spatial gaps, and "space-specific" properties to capture characteristic features, complemented by an Adjustable Multi-curvature Pooling (AMP) approach [wang2024a]. While both achieve state-of-the-art results, MADE's adaptive weighting offers a more flexible approach to handling diverse geometric structures without requiring explicit design of shared/specific properties. The primary limitation for both is the increased complexity of optimizing embeddings across multiple, potentially disparate, geometric spaces, and the computational overhead.

Further extending these geometric transformations, recent works tackle fuzzy and spatiotemporal dimensions. \textit{FSTRE} uses projection and rotation in a complex vector space to embed spatial and temporal information, introducing fine-grained fuzziness through modal lengths of anisotropic vectors [ji2024]. This addresses the insufficiency of prior KGE models for uncertain and dynamic knowledge. Building on this, [ji2024a] leverages quaternion embeddings to jointly embed spatiotemporal entities, representing relations as rotations and exploiting the non-commutative compositional pattern of quaternions for multihop path reasoning and uncertainty modeling. This approach is particularly powerful for complex tasks like multihop querying on incomplete fuzzy spatiotemporal KGs, where previous methods overlooked uncertainty and spatiotemporal sensitivity during reasoning. These advanced models, while powerful, introduce additional complexity (fuzziness, spatiotemporal, quaternions) which can increase model intricacy and training demands.

Finally, \textit{TARGAT} offers an alternative paradigm by employing a time-aware relational graph attention model based on Graph Neural Networks (GNNs) [xie2023]. It addresses the limitation of previous GNN-based models that struggle to directly capture multi-fact interactions at different timestamps by dynamically generating time-aware relational message transformation matrices. This GNN-based approach provides a unified way to process the entire graph of multi-facts over time. However, GNNs can face scalability challenges with extremely large and dense TKGs due to the computational intensity of message passing.

In summary, TKGE research has evolved from explicit temporal integration using hyperplanes or tensors to sophisticated geometric transformations (rotations, multi-curvature spaces) and advanced algebraic structures (quaternions) to handle the multifaceted nature of dynamic, spatiotemporal, and fuzzy knowledge. The trade-off between model expressiveness and computational complexity remains a persistent challenge, with recent models pushing the boundaries of what can be captured, often at the cost of increased model intricacy and optimization demands.
\subsection{Inductive and Continual KGE}
\label{sec:5_2_inductive__and__continual_kge}

Real-world knowledge graphs (KGs) are inherently dynamic, with new entities, relations, and facts constantly emerging. Traditional Knowledge Graph Embedding (KGE) models are often transductive, meaning they can only generate embeddings for entities seen during training, necessitating expensive full retraining when new information arrives. This limitation has spurred significant research into inductive and continual KGE, aiming to adapt models to evolving KGs by handling unseen entities and efficiently updating knowledge without catastrophic forgetting [liu2024, sun2024]. These methods are crucial for maintaining the scalability and relevance of KGE models in dynamic environments.

Early efforts in inductive KGE focused on neighborhood aggregation techniques. [wang2018] introduced the Logic Attention Network (LAN), an aggregator that learns to embed new entities by combining the embeddings of their existing neighbors. LAN addresses the unordered and unequal nature of an entity's neighbors by employing both rules- and network-based attention weights. While innovative for its time, aggregation-based methods like LAN inherently rely on the presence of existing neighbors for new entities. This poses a limitation when truly novel, isolated entities with sparse connections emerge, as their representations might be less robust or even impossible to generate. The generalizability of such methods is also constrained by the quality and density of the local neighborhood information.

To overcome the limitations of direct entity embedding and enhance transferability, meta-learning has emerged as a powerful paradigm for inductive KGE. [chen2021] proposed MorsE, a model that does not learn explicit entity embeddings but instead learns transferable meta-knowledge. This meta-knowledge, modeled by entity-independent modules and learned through meta-learning, can then be used to produce embeddings for new entities in an inductive setting. This approach offers a more generalized inductive capability compared to simple aggregation, as it aims to capture underlying patterns that are independent of specific entities. Building on this, [sun2024] applied meta-learning to dynamic KGE in evolving service ecosystems with MetaHG. This model incorporates both local (via a GNN layer) and potential global (via a hypergraph neural network, HGNN, layer) structural information from current KG snapshots to enhance the representation of emerging entities. MetaHG's hybrid GNN framework and meta-learning strategy aim to mitigate issues like spatial deformation and improve the quality of embeddings for new entities. A critical comparison reveals that while meta-learning offers a more robust inductive framework, its complexity in training and the need for sufficient meta-training tasks can be significant. Furthermore, the effectiveness of meta-knowledge transfer can still be influenced by the similarity between the meta-training and target domains.

Beyond inductive learning, continual KGE addresses the challenge of efficiently acquiring new knowledge while simultaneously preserving previously learned information, a problem often plagued by catastrophic forgetting. [liu2024] introduced FastKGE, a framework incorporating an incremental low-rank adapter (IncLoRA) mechanism. FastKGE tackles both efficient new knowledge acquisition and catastrophic forgetting by isolating and allocating new knowledge to specific layers based on the fine-grained influence between old and new KGs. The IncLoRA mechanism then embeds these specific layers into low-rank adapters, significantly reducing the number of trainable parameters during fine-tuning. This approach also features adaptive rank allocation, making the LoRA aware of entity importance. Experimental results demonstrate that FastKGE can reduce training time by 34-49\% on public datasets while maintaining competitive performance, and even greater savings (51-68\%) on larger, newly constructed datasets, alongside performance improvements. This parameter-efficient adaptation is a crucial advancement for large-scale KGE models, balancing the acquisition of new knowledge with the retention of old, a key trade-off in continual learning. However, the efficacy of LoRA-based methods can depend on the intrinsic rank of the updates and the architecture of the base KGE model.

The overarching goal across these inductive and continual KGE methods is to balance the acquisition of new knowledge with the retention of previously learned information, mitigating catastrophic forgetting and ensuring scalability. While neighborhood aggregation provides a straightforward, albeit limited, inductive capability, meta-learning offers a more generalized approach by learning transferable knowledge. Parameter-efficient adaptation techniques like IncLoRA represent a practical solution for continual learning, particularly for large models, by enabling efficient updates without full retraining. A common theoretical gap preventing a complete solution to these problems lies in developing truly universal inductive mechanisms that are robust to completely novel, isolated entities and can perform continuous, long-term updates without any degradation in performance or significant increase in computational cost. The experimental setups for these models often require specialized dynamic datasets, which can be less standardized than static link prediction benchmarks, making direct comparisons challenging and potentially affecting generalizability. The field continues to seek methods that can seamlessly integrate new information while preserving the integrity and expressiveness of the entire knowledge graph, a critical step towards truly adaptive and intelligent AI systems.
\subsection{Federated and Privacy-Preserving KGE}
\label{sec:5_3_federated__and__privacy-preserving_kge}

The increasing concerns over data privacy and the proliferation of distributed knowledge sources have propelled Federated Learning (FL) as a crucial paradigm for Knowledge Graph Embedding (KGE). Federated KGE (FKGE) enables collaborative model training across multiple clients, each holding a local knowledge graph (KG), without centralizing sensitive data. This approach is vital for leveraging decentralized knowledge in privacy-sensitive domains, addressing the growing need for privacy-aware AI systems. However, FKGE introduces unique challenges, primarily related to communication efficiency, personalization for diverse client data, and security vulnerabilities.

A significant challenge in FKGE is the high communication cost stemming from the large size of KGE parameters and the extensive number of communication rounds required for convergence. Traditional FL methods often focus on reducing communication rounds by increasing local training epochs, but they frequently overlook the size of parameters transmitted in each round. To address this, [zhang2024] (Communication-Efficient FKGE) proposes FedS, a bidirectional communication-efficient framework based on Entity-Wise Top-K Sparsification. This method allows clients to dynamically identify and upload only the Top-K entity embeddings with the most significant changes to the server. Similarly, the server transmits only the Top-K aggregated embeddings to each client after performing personalized aggregation. This approach, coupled with an Intermittent Synchronization Mechanism, aims to mitigate the negative effects of embedding inconsistency caused by client heterogeneity. While FedS significantly enhances communication efficiency, a critical analysis reveals a potential trade-off: universal reduction in embedding precision, as noted by the authors, can impede convergence speed. The challenge lies in precisely identifying the "most significant" changes without losing crucial information, especially for less frequently updated entities or relations. This aligns with the broader "Efficiency, Compression, and System Optimization" subgroup's goal of reducing resource consumption while maintaining performance, but within the added constraint of distributed, privacy-preserving learning.

Beyond communication efficiency, the semantic disparities among clients pose a substantial hurdle for FKGE. Existing FKGE methods often rely on a global consensus model, typically using the arithmetic mean of entity embeddings as global supplementary knowledge [zhang2024]. This "one-size-fits-all" approach, however, neglects the inherent semantic heterogeneity across diverse client KGs, leading to a global model that might be inundated with noise when tailored to a specific client. To overcome this, [zhang2024] (Personalized Federated KGE) introduces PFedEG, a novel approach that employs a client-wise relation graph to learn personalized embeddings. PFedEG discerns the semantic relevance of embeddings from other clients, allowing each client to learn personalized supplementary knowledge by amalgamating entity embeddings from its "neighboring" clients based on their affinity on this graph. This personalized approach addresses the "Personalized Federated KGE" challenge, moving beyond a universal global model to improve embedding quality for individual clients. The strength of PFedEG lies in its ability to adapt to diverse data distributions, a critical aspect highlighted in the "Dynamic, Inductive, and Continual KGE" subgroup, which emphasizes adaptability to evolving and heterogeneous knowledge. However, the construction and maintenance of such a client-wise relation graph introduce additional computational complexity and potential privacy leakage risks if the "affinity" metrics are not carefully designed.

While FL is designed to preserve data privacy, it is not inherently immune to security vulnerabilities. [zhou2024] (Poisoning Attack on Federated KGE) systematically explores the risks of poisoning attacks in FKGE, highlighting a critical security challenge. This pioneering work develops a novel framework that forces victim clients to predict specific false facts, demonstrating that privacy-preserving distributed training does not automatically equate to security. Unlike centralized KGEs, where attackers might directly inject poisoned data, FKGE's local data maintenance necessitates indirect attack vectors. The proposed attack framework involves inferring targeted relations in the victim's local KG via a "KG component inference attack" and then using an optimized dynamic poisoning scheme to generate progressive poisoned updates through FKGE aggregation. The experimental results demonstrate remarkable success rates (e.g., 100\% on TransE with WN18RR) with minimal impact on the original task's performance, exposing a significant vulnerability. This research, while adversarial, is crucial for informing the design of robust FKGE systems, aligning with the "Robustness and Training Optimization" subgroup's focus on mitigating data imperfections and ensuring model integrity. The theoretical gap here is the lack of a direct defense mechanism proposed by the authors, which remains an open challenge for future research.

In synthesis, the emerging field of Federated and Privacy-Preserving KGE is rapidly addressing the practical demands of distributed and privacy-sensitive environments. The works by [zhang2024] (Communication-Efficient FKGE) and [zhang2024] (Personalized Federated KGE) represent constructive efforts to optimize FKGE by tackling communication bottlenecks and semantic heterogeneity, respectively. These solutions are critical for making FKGE scalable and effective in real-world deployments. However, the findings of [zhou2024] (Poisoning Attack on Federated KGE) serve as a stark reminder that privacy and security are distinct concerns, and the distributed nature of FL introduces new attack surfaces. The trade-offs are evident: aggressive communication sparsification might impact model convergence, personalization adds complexity and potential for noise, and robust security measures could introduce computational overhead. The rapid publication of these papers in 2024 underscores the contemporary and pressing nature of these challenges, reflecting the field's accelerated evolution towards practical, secure, and adaptable KGE solutions for decentralized knowledge.


### Practical Considerations: Efficiency, Robustness, and Evaluation

\label{sec:practical_considerations:_efficiency,_robustness,__and__evaluation}

\section{Practical Considerations: Efficiency, Robustness, and Evaluation}
\label{sec:practical_considerations_efficiency_robustness_and_evaluation}

Building upon the advancements in dynamic, inductive, and distributed knowledge graph embedding (KGE) models discussed in Section \ref{sec:dynamic_inductive_and_distributed_kge}, which focused on adapting KGEs to evolving and decentralized knowledge, this section shifts its focus to the critical operational challenges of deploying and evaluating these models in real-world scenarios. While previous sections explored theoretical foundations and architectural innovations, the true utility of KGEs hinges on their practical viability, encompassing computational efficiency, resilience to imperfect data, and the trustworthiness of their empirical validation. This section bridges the gap between theoretical progress and reliable real-world application, addressing the fundamental requirements for KGE models to move from research prototypes to robust, scalable, and trustworthy components of intelligent systems.

We delve into three interconnected areas vital for practical KGE deployment. First, we examine strategies for enhancing \textit{efficiency, compression, and scalability}, which are paramount for handling the immense size and complexity of modern knowledge graphs [community_1, community_6]. This involves techniques to reduce computational cost, minimize memory footprint, and optimize training and inference processes, making KGE models viable for resource-constrained environments and large-scale applications. Second, we explore methods for improving \textit{robustness and training optimization}, crucial for mitigating the impact of noisy, incomplete, or imbalanced data inherent in real-world KGs [community_2, community_3, 2a3f862199883ceff5e3c74126f0c80770653e05]. This includes advanced negative sampling techniques and noise filtering mechanisms that ensure models learn accurate and reliable representations. Finally, a significant portion is dedicated to the importance of rigorous \textit{evaluation, benchmarking, and reproducibility} [community_0, community_6]. This area underscores the necessity for standardized metrics, fair comparisons, and transparent research practices to ensure that KGE advancements are scientifically sound and lead to reliable, trustworthy applications. Together, these practical considerations are indispensable for translating the rich theoretical landscape of KGE into impactful and dependable real-world solutions.

\subsection{Efficiency, Compression, and Scalability}
\label{sec:6_1_efficiency,_compression,__and__scalability}

The practical deployment of Knowledge Graph Embedding (KGE) models for massive knowledge graphs (KGs) is often hampered by significant computational costs, extensive training times, and prohibitive memory footprints. Addressing these bottlenecks is crucial for transitioning KGEs from academic benchmarks to real-world applications, especially in resource-constrained environments. This area of research focuses on techniques spanning knowledge distillation, embedding compression, parameter-efficient learning, optimized system designs, and novel algorithms.

One prominent strategy for reducing the computational burden and memory footprint is \textit{knowledge distillation}. [zhu2020] introduced DualDE, a method that distills knowledge from a high-dimensional, high-performing teacher KGE model into a low-dimensional student model. This approach significantly reduces embedding parameters (by 7-15x) and increases inference speed (by 2-6x) while retaining competitive performance. DualDE's generality allows its application across various KGE architectures, making it a versatile tool for efficiency. However, a common trade-off in distillation is a minor, albeit often acceptable, loss in performance, which is inherent to compressing information.

Complementary to distillation are direct \textit{embedding compression} techniques. [sachan2020] proposed representing entities with discrete codes, achieving remarkable compression ratios (50-1000x) of the embedding layer with only a minor performance drop. Building on this, LightKG [wang2021] introduced a lightweight framework that stores only a few codebooks and indices, drastically reducing storage and boosting inference efficiency through quick look-ups. LightKG also incorporates a dynamic negative sampling strategy, which further enhances performance. While these methods offer substantial gains in storage and inference speed, they rely on approximating the original embeddings, which might introduce subtle inaccuracies compared to full-precision representations.

\textit{Parameter-efficient learning} offers another avenue for scalability. Entity-Agnostic Representation Learning (EARL) [chen2023] tackles the escalating parameter storage costs by learning embeddings only for a small set of "reserved entities." Embeddings for other entities are then derived from their context (e.g., connected relations, k-nearest reserved entities, multi-hop neighbors) using universal, entity-agnostic encoders. This approach results in a static and significantly lower parameter count, making it particularly beneficial for large and continuously growing KGs. A potential limitation of EARL is its reliance on contextual information; entities with sparse connections or those truly "unseen" without sufficient neighbors might have less expressive representations compared to directly learned embeddings.

Beyond model-specific optimizations, \textit{optimized system designs} are critical for large-scale KGE training. GE2 [zheng2024] proposes a general and efficient KGE learning system that addresses long CPU times and high CPU-GPU communication overhead, especially in multi-GPU setups. GE2 offloads operations to GPUs and introduces COVER, a novel algorithm for managing data swap between CPU and multiple GPUs, achieving speedups of 2x to 7.5x across various models and datasets. This system-level innovation provides a foundational improvement for KGE research, enabling faster experimentation and deployment of complex models. While GE2 focuses on training efficiency, it doesn't directly reduce the size of the *final* embedding model, which is where distillation and compression techniques become relevant.

Novel algorithmic approaches also contribute significantly to efficiency. "Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis" [peng2021] introduces a fundamentally different paradigm by proposing a closed-form solution using Orthogonal Procrustes Analysis (OPA). This enables full-batch learning and non-negative sampling, reducing training time and carbon footprint by orders of magnitude while yielding competitive performance. The closed-form nature bypasses iterative optimization, which is a major source of computational cost in traditional KGEs. However, the inherent mathematical constraints of OPA might limit its expressiveness compared to more flexible, iteratively optimized models for certain complex relational patterns.

For Graph Neural Network (GNN)-based KGEs, which are known for their computational intensity, \textit{graph partitioning strategies} are essential. CPa-WAC [modak2024] employs modularity maximization-based constellation partitioning to break down KGs into subgraphs. This allows for separate processing, reducing memory and training time for GNNs while aiming to retain prediction accuracy. CPa-WAC demonstrates up to a five-fold speedup, highlighting the effectiveness of distributed processing. Nevertheless, partitioning a graph can sometimes hinder the capture of global dependencies that span across different subgraphs, potentially impacting performance on tasks that require broader structural understanding.

Finally, a comprehensive understanding of \textit{parallelization techniques} is vital. [kochsiek2021] provided a critical meta-study, re-implementing and comparing various parallelization strategies for KGE training. Their work revealed that naive parallelization can degrade embedding quality and proposed effective mitigations, such as a variation of the stratification technique. This study underscores that simply distributing computation does not guarantee efficiency or quality, emphasizing the need for careful technique selection.

In summary, the pursuit of efficiency and scalability in KGE involves a multi-faceted approach. Techniques like DualDE [zhu2020], [sachan2020], and LightKG [wang2021] focus on compressing the model itself for storage and inference. EARL [chen2023] addresses parameter growth for evolving KGs. Meanwhile, GE2 [zheng2024], OPA-based learning [peng2021], CPa-WAC [modak2024], and parallelization studies [kochsiek2021] target the efficiency of the training process. These innovations collectively aim to overcome the practical bottlenecks of KGE, making them deployable in resource-constrained environments and capable of handling the ever-growing scale of real-world knowledge bases. The ongoing challenge lies in balancing the gains in efficiency and scalability with the preservation of model expressiveness and predictive accuracy.
\subsection{Robustness and Training Optimization}
\label{sec:6_2_robustness__and__training_optimization}

The efficacy of Knowledge Graph Embedding (KGE) models in real-world applications is profoundly dependent on their robustness against data imperfections and the optimization of their training processes. Knowledge graphs are inherently noisy, incomplete, and often suffer from imbalanced data distributions, necessitating sophisticated techniques to ensure that learned representations are accurate, reliable, and generalize well. This subsection delves into methods designed to enhance KGE model robustness and refine their training, particularly focusing on negative sampling strategies.

A critical aspect of model reliability is the trustworthiness of its predictions. [tabacof2019] highlight that many popular KGE models, despite achieving high accuracy, produce uncalibrated probability estimates, meaning their predicted scores do not directly correspond to true probabilities. They propose post-hoc calibration methods like Platt scaling and isotonic regression, which are particularly valuable when ground truth negatives are scarce, a common scenario in KGs. While these methods offer a general solution applicable to various KGE models, their effectiveness relies on the quality of the calibration data and may introduce additional computational overhead during inference. This emphasizes that model performance metrics alone are insufficient; the reliability of output probabilities is equally crucial for practical deployment.

To address the pervasive issue of noisy data within KGs, which often arises from automatic knowledge construction, [zhang2021] introduce a multi-task reinforcement learning framework. This innovative approach actively filters out noisy triples during training, allowing the KGE model to learn from a cleaner, more reliable subset of facts. By exploiting correlations among semantically similar relations through multi-task learning, their method aims to learn more robust representations. This proactive noise-filtering mechanism is a significant advancement over passive error handling, as it directly impacts the quality of the input data for embedding. However, the complexity introduced by a reinforcement learning agent within the training loop can increase computational cost and require careful hyperparameter tuning, posing scalability challenges for extremely large KGs.

Another common imperfection in KGs is data imbalance, where entities and relations follow a long-tail distribution, with a few occurring frequently and many appearing rarely. Traditional KGE methods often assign equal weights during training, leading to unreliable representations for infrequent (long-tail) entities and relations. To counteract this, [zhang2023] propose WeightE, a weighted KGE model that employs a bilevel optimization scheme to assign differential weights. WeightE dynamically endows lower weights to frequent elements and higher weights to infrequent ones, ensuring that long-tail entities and relations receive adequate training attention. This flexible weighting technique can be applied to various existing KGE models, offering a practical solution to a widespread problem. While effective, the bilevel optimization adds a layer of complexity to the training process, which might require more computational resources or careful convergence monitoring compared to standard training.

Beyond handling data imperfections, optimizing the training process itself, particularly through effective negative sampling, is paramount. KGE models typically rely on contrastive learning, requiring both positive (observed) and negative (false) triples. The quality of generated negative samples profoundly impacts model performance [qian2021, madushanka2024]. Early approaches to negative sampling often struggled in noisy environments. [shan2018] address this with a confidence-aware negative sampling method for noisy KGE, introducing the concept of negative triple confidence to improve training stability and prevent issues like zero loss or false detection. This method acknowledges that not all negative samples are equally informative, especially in the presence of noise. Building on the idea of identifying "hard" negatives, [zhang2018] propose NSCaching, an efficient caching strategy that tracks and samples challenging negative triplets. This approach, inspired by generative adversarial networks (GANs) but simpler, aims to distill the benefits of complex adversarial sampling into a more computationally efficient framework, demonstrating a trade-off between model complexity and training efficiency.

A more radical departure from traditional negative sampling is presented by [li2021] with their Efficient Non-Sampling Knowledge Graph Embedding (NS-KGE). This method proposes to avoid negative sampling entirely by considering all negative instances. While this theoretically removes the uncertainty and potential instability inherent in sampling, it dramatically increases computational complexity. NS-KGE addresses this by leveraging mathematical derivations to reduce the complexity of the non-sampling loss function, aiming for both better efficiency and accuracy. This non-sampling paradigm offers a compelling alternative, particularly for models whose loss functions can be efficiently reformulated, but its generalizability across all KGE architectures and its practical scalability to extremely dense KGs remain areas of active research.

The increasing complexity of KGE models and the integration of diverse data modalities further complicate negative sampling. [zhang2023] (Modality-Aware Negative Sampling) address this by proposing MANS, a method specifically designed for multi-modal KGE. MANS aligns structural and visual embeddings for entities, demonstrating that negative sampling strategies must adapt to the unique characteristics of multi-modal information to learn meaningful embeddings. This highlights a critical development direction where training optimization must evolve in tandem with model architectural advancements.

Overall, the advancements in robustness and training optimization reflect a maturing KGE field that is moving beyond purely theoretical model expressiveness towards practical utility. The systematic reviews of negative sampling [qian2021, madushanka2024] underscore its foundational importance, even as models like [chen2025] (ConQuatE) introduce sophisticated quaternion-based embeddings to handle polysemy. The continued focus on refining training mechanisms, such as negative sampling, alongside the development of advanced architectures, indicates a holistic approach where the effectiveness of complex KGE models is deeply intertwined with robust and efficient training methodologies. While significant progress has been made in handling noise, imbalance, and sampling, the challenge of efficiently and accurately identifying the "true" negative distribution in KGs, especially in dynamic and multi-modal settings, remains a theoretical gap and an active area of research.
\subsection{Evaluation, Benchmarking, and Reproducibility}
\label{sec:6_3_evaluation,_benchmarking,__and__reproducibility}


The rapid proliferation of Knowledge Graph Embedding (KGE) models has underscored the critical importance of rigorous evaluation, standardized benchmarking, and robust reproducibility practices. Without these, fair comparisons between models become challenging, scientific progress can be hindered by unreliable results, and the trustworthy deployment of KGE models in real-world applications is compromised. The field has increasingly recognized the need to move towards higher standards of empirical validation and transparency to ensure the reliability and generalizability of research findings.

A significant step towards addressing these challenges has been the development of unified frameworks and libraries. [broscheit2020] introduced \texttt{LibKGE}, an open-source PyTorch-based library designed to foster reproducible research. Its key strengths lie in its high configurability, decoupled components that allow for flexible mixing and matching, and comprehensive logging, making it an invaluable tool for conducting systematic experimental studies and analyzing the contributions of individual model components. While \texttt{LibKGE} provides the infrastructure for reproducible experimentation, large-scale comparative studies have simultaneously exposed the widespread issues plaguing KGE research.

[ali2020]'s seminal work, "Bringing Light Into the Dark," provided a stark revelation of reproducibility failures within the KGE community. By re-implementing and evaluating 21 models within a unified framework, \texttt{PyKEEN}, the authors found that many published results could not be reproduced with their reported hyperparameters, and some not at all. This highlights a significant methodological limitation: the heterogeneity in implementations, training procedures, and evaluation protocols across different research groups often leads to incomparable results and inflated performance claims. The study emphasized that model performance is not solely determined by architecture but by a complex interplay of architecture, training approach, loss function, and the explicit modeling of inverse relations. This suggests that many experimental setups in prior work lacked the necessary standardization to ensure generalizability.

Further compounding these issues, [rossi2020] conducted a comprehensive comparison of 18 state-of-the-art KGE methods for link prediction, critically examining the effect of design choices and exposing biases in standard evaluation practices. They highlighted that the common practice of aggregating accuracy over a large number of test facts, where some entities are vastly more represented than others, allows models to achieve good results by focusing on these high-frequency entities, thereby ignoring the majority of the knowledge graph. This inherent bias in benchmark datasets can lead to an overestimation of a model's true generalization capabilities, as its performance might be artificially boosted by exploiting statistical artifacts rather than genuinely learning complex relational patterns. This reveals a critical assumption made in many KGE evaluations—that benchmark datasets provide a uniformly representative test bed—which is often unrealistic.

Beyond evaluation biases, the impact of hyperparameter tuning on KGE quality has also been rigorously investigated. [lloyd2022] employed Sobol sensitivity analysis to quantify the importance of various hyperparameters, revealing substantial variability in their sensitivities across different knowledge graphs. This implies that optimal hyperparameter configurations are often dataset-specific, making universal recommendations difficult and further complicating reproducibility. A particularly concerning finding was the identification of data leakage in the widely used UMLS-43 benchmark due to inverse relations, which could lead to artificially inflated performance metrics. This directly challenges the integrity of a common experimental setup and underscores the need for meticulous data curation and validation.

Collectively, these studies reveal that the KGE field, while innovative in model development, has historically suffered from a lack of meta-scientific rigor. The methodological limitations include inconsistent implementations, biased evaluation metrics, and an underestimation of hyperparameter sensitivity. These issues directly impact the generalizability of findings, as models might be overfit to specific experimental conditions or benchmark quirks. The theoretical gap isn't necessarily in the KGE models themselves, but in the overarching framework for their empirical validation. Addressing these challenges requires a concerted community effort towards adopting unified frameworks, conducting transparent and reproducible experiments, and developing more robust, unbiased evaluation metrics that truly reflect a model's understanding of the knowledge graph. This move towards higher standards is crucial for fostering reliable scientific progress and ensuring the trustworthy deployment of KGE models in critical applications.


### Applications and Real-World Impact of KGE

\label{sec:applications__and__real-world_impact_of_kge}

\section{Applications and Real-World Impact of KGE}
\label{sec:applications_and_real_world_impact_of_kge}

Following the discussion on practical considerations for KGE models in Section \ref{sec:practical_considerations_efficiency_robustness_and_evaluation}, which focused on ensuring their efficiency, robustness, and rigorous evaluation, this section shifts its attention to the tangible outcomes and widespread utility of these advancements. Here, we move beyond theoretical developments to showcase the diverse and significant real-world impact of knowledge graph embedding across various artificial intelligence applications. KGE models are no longer confined to academic benchmarks; they are actively leveraged to address complex problems across numerous domains, demonstrating their transformative potential in modern AI systems.

The subsequent subsections will delve into how KGE underpins fundamental tasks such as \textit{link prediction and knowledge graph completion}, which are crucial for enhancing the completeness and inferential capabilities of knowledge bases [community_0, community_1]. We will then explore its vital role in \textit{entity alignment}, enabling the seamless integration of heterogeneous knowledge sources by identifying equivalent entities across disparate graphs [community_5]. Furthermore, this section highlights the application of KGE in enhancing user-facing AI systems, specifically in improving the intelligence and personalization of \textit{question answering} and \textit{recommender systems} [community_1, community_3]. Finally, we will examine various \textit{domain-specific applications}, from biological systems to patent analysis, emphasizing how KGE models are tailored to solve industry-specific challenges and often incorporate principles of \textit{explainability} to build trust and provide actionable insights [community_2, community_6]. This comprehensive overview illustrates the practical utility, tangible benefits, and broad applicability of knowledge graph embedding techniques, underscoring their indispensable contribution to the evolution of intelligent systems that can effectively understand, reason with, and leverage vast amounts of structured knowledge.

\subsection{Link Prediction and Knowledge Graph Completion}
\label{sec:7_1_link_prediction__and__knowledge_graph_completion}

Link prediction (LP) and knowledge graph completion (KGC) represent the fundamental applications of knowledge graph embedding (KGE), aiming to infer missing facts and enhance the completeness of knowledge graphs. These tasks are crucial for making KGs more robust and informative for downstream AI systems by automatically inferring unobserved facts within the graph structure. The evolution of KGE models for LP/KGC reflects a continuous effort to improve accuracy, handle complex relational patterns, and address practical challenges.

Early KGE models primarily leveraged geometric transformations to represent entities and relations. Translational models, such as TransH [wang2014] and TransD [ji2015], extended the foundational TransE by modeling relations as translations on hyperplanes or through dynamic mapping matrices, respectively. TransH notably improved the handling of one-to-many and many-to-one relations by allowing entity projections, while TransD further refined this by considering the diversity of both entities and relations. However, these models, while efficient, often struggled to capture more intricate logical patterns like symmetry, antisymmetry, inversion, and composition. This limitation spurred the development of rotational models, with RotatE [sun2018] defining relations as rotations in complex vector spaces. RotatE demonstrated superior expressiveness for these complex patterns, significantly outperforming its translational predecessors in link prediction tasks. Further geometric innovations include embedding entities on Lie groups (e.g., TorusE [ebisu2017]) to address regularization issues, exploring alternative metrics like the Cycle metric [yang2021] for enhanced expressiveness, and introducing powerful transformations such as Householder parameterization (HousE [li2022]) or compound operations (CompoundE [ge2022], CompoundE3D [ge2023]) to capture a broader spectrum of relational semantics. More recently, models like HolmE [zheng2024] have focused on ensuring closure under composition, a theoretical property vital for modeling under-represented compositional patterns, while MQuinE [liu2024] addresses specific theoretical deficiencies ("Z-paradox") in existing models to enhance expressiveness. While these geometric models offer mathematical elegance and interpretability, their ability to capture highly complex, non-linear interactions can be limited compared to deep learning approaches.

The advent of deep learning architectures marked a significant paradigm shift in KGE for LP/KGC. Convolutional Neural Networks (CNNs) have been widely adopted to extract local features and model interactions between entity and relation embeddings. Models like AcrE [ren2020] and ReInceptionE [xie2020] utilize various convolutional layers and attention mechanisms to capture complex relation patterns and aggregate entity-specific features, achieving state-of-the-art results. More recent CNN-based methods, such as CNN-ECFA [hu2024] and SEConv [yang2025], continue to refine feature aggregation and interaction. Graph Neural Networks (GNNs) and attention mechanisms further leverage the graph's topology, with models like DisenKGAT [wu2021] employing disentangled graph attention networks for diverse representations and GAATs [wang2020] incorporating graph attenuated attention to capture rich neighborhood information. Transformer-based architectures, including CoKE [wang2019], Knowformer [li2023], and TGformer [shi2025], treat KGs as sequences or integrate graph structures into Transformer frameworks, leveraging self-attention to capture long-range dependencies and contextualized representations. These deep learning models excel at learning intricate, non-linear, and context-dependent features, often surpassing purely geometric approaches in accuracy, but typically incur higher computational costs and can be less interpretable.

Beyond structural information, KGE models for LP/KGC have been enriched by incorporating auxiliary information, logical rules, and multi-modal data. Integrating entity types, as seen in TransET [wang2021] and TaKE [he2023], provides semantic guidance that significantly improves KG completion, especially in low-resource settings. Hyper-relational KGE models like HINGE [rosso2020] move beyond simple triplets to incorporate associated key-value pairs, capturing richer data semantics. Rule-based KGE approaches, such as RUGE [guo2017] and RulE [tang2022], iteratively guide embedding learning with soft logical rules, enhancing reasoning capabilities and ensuring semantic consistency, though rule extraction and balancing rule adherence with flexibility remain challenges. Furthermore, multi-modal KGEs, exemplified by SSP [xiao2016] which projects text descriptions into semantic space, and Joint Language Semantic and Structure Embedding [shen2022] which integrates pre-trained language models, enrich embeddings with external semantic context, crucial for overcoming data sparsity and improving understanding.

The practical effectiveness of KGE for LP/KGC also heavily relies on training optimizations and robustness. Negative sampling strategies, such as Confidence-Aware Negative Sampling [shan2018] and NSCaching [zhang2018], are critical for efficient and effective training, though the "true" negative distribution remains a theoretical gap. Some approaches, like Efficient Non-Sampling KGE [li2021], even attempt to avoid negative sampling entirely to achieve more stable performance, albeit with increased computational complexity. Robustness against noisy data is addressed by methods like those using multi-task reinforcement learning [zhang2021] to filter out erroneous triples or weighted training schemes (e.g., WeightE [zhang2023]) to handle data imbalance. The continuous development across these diverse methodologies underscores the central role of link prediction and knowledge graph completion in advancing the utility and reliability of knowledge graphs for a wide array of AI applications [dai2020, cao2022, ge2023, rossi2020].
\subsection{Entity Alignment}
\label{sec:7_2_entity_alignment}

Entity Alignment (EA) is a critical task in knowledge graph integration, aiming to identify equivalent entities across different, often heterogeneous, knowledge graphs (KGs). The proliferation of KGs from diverse sources necessitates robust methods for their integration, which is fundamental for building comprehensive knowledge bases and enabling sophisticated cross-KG reasoning [dai2020, choudhary2021]. Knowledge Graph Embeddings (KGEs) have emerged as a powerful, data-driven approach to tackle this challenge, transforming symbolic entities and relations into low-dimensional vector spaces where semantic correspondences can be identified through similarity measures [yan2022, cao2022]. This approach leverages the ability of KGEs to capture intricate structural and semantic patterns, making them highly suitable for finding equivalences between disparate knowledge structures.

A significant hurdle in embedding-based entity alignment is the scarcity of labeled training data, which can limit the accuracy and generalizability of models. To address this, bootstrapping methods have been developed. For instance, [sun2018] proposed an iterative bootstrapping approach that progressively labels likely entity alignments to augment the training data for learning alignment-oriented KG embeddings. This method strategically employs an alignment editing technique to mitigate the accumulation of errors during the iterative process, which is a common pitfall in self-training schemes. While effective in leveraging unlabeled data, the performance of such bootstrapping approaches can be sensitive to the quality of the initial seed alignments and the robustness of the error reduction mechanism, as false positives in early iterations can propagate and degrade overall accuracy.

Extending beyond purely bootstrapping, semi-supervised learning frameworks have been introduced to more effectively utilize both limited labeled data and abundant unlabeled information. [pei2019] presented a semi-supervised entity alignment method (SEA) that not only leverages unlabeled entities but also incorporates an awareness of entity degree differences. This is crucial because entities with vastly different degrees (i.e., number of connections) can lead to biased embeddings, making alignment challenging, particularly between high-frequency and low-frequency entities. By employing adversarial training, SEA aims to learn more robust embeddings that are less affected by these structural disparities. This approach addresses a practical limitation of many KGE models, where embedding quality can be disproportionately influenced by highly connected entities, thereby improving alignment accuracy across the entire spectrum of entities. However, the complexity of adversarial training can introduce challenges in model stability and hyperparameter tuning.

Further enhancing the robustness and accuracy of EA, multi-view frameworks integrate diverse types of entity information. [zhang2019] proposed a novel multi-view KGE framework that unifies entity names, relational structures, and attributes to learn more comprehensive embeddings for alignment. Traditional KGE methods often focus predominantly on relational structures, overlooking other rich features that can provide complementary semantic cues. By combining these multiple views with various strategies and designing cross-KG inference methods, this approach significantly improves alignment performance. The strength of multi-view learning lies in its ability to capture a broader spectrum of semantic information, making the embeddings more discriminative. Nevertheless, the challenge lies in effectively weighting and integrating potentially conflicting signals from different views, and the computational overhead increases with the number of views considered.

More recently, the integration of ontological information has provided another powerful dimension for entity alignment. [xiang2021] introduced OntoEA, an ontology-guided entity alignment method that jointly embeds both KGs and their associated ontologies. This approach explicitly utilizes critical meta-information, such as class hierarchies and class disjointness constraints, which are often ignored by purely structural or attribute-based methods. By enforcing these ontological constraints during the embedding process, OntoEA can prevent false mappings and guide the alignment towards semantically coherent equivalences. This addresses a theoretical gap where KGEs, while powerful, often lack an explicit mechanism to incorporate higher-level schema knowledge. The effectiveness of OntoEA, however, is contingent on the availability and quality of consistent ontological information across the KGs being aligned, which may not always be present in real-world scenarios.

The collective advancements in these KGE-based EA methods underscore a clear evolution in the field. Early approaches focused on leveraging structural similarity, while subsequent methods have progressively integrated more semantic context, auxiliary information, and robust learning paradigms to overcome limitations like data scarcity and feature incompleteness. Comprehensive surveys and experimental reviews, such as those by [zhu2024] and [fanourakis2022], further highlight the strengths and weaknesses of various KGE-based EA techniques. They emphasize the need for more robust noise filtering strategies, better utilization of additional information, and rigorous comparative analyses across diverse datasets to ensure generalizability. These meta-analyses confirm that KGEs provide a versatile and powerful foundation for integrating heterogeneous knowledge sources, enabling the construction of more comprehensive knowledge bases and facilitating complex cross-KG reasoning, which is crucial for advancing knowledge-driven AI applications.
\subsection{Question Answering and Recommendation Systems}
\label{sec:7_3_question_answering__and__recommendation_systems}

Knowledge Graph Embeddings (KGEs) have emerged as a pivotal technology for bridging the semantic gap between natural language and structured knowledge, enabling more intelligent and interpretable interactions in diverse applications such as Question Answering (QA) and Recommender Systems. These applications leverage KGEs to transform complex symbolic reasoning into efficient vector space operations, thereby enhancing performance and user experience [dai2020, cao2022].

In the realm of Question Answering over Knowledge Graphs (QA-KG), KGEs facilitate the understanding of natural language queries by mapping them to entities and relations within the underlying knowledge graph. Early frameworks, such as Knowledge Embedding based Question Answering (KEQA), demonstrated the utility of KGEs by jointly recovering head entity, predicate, and tail entity representations in the embedding space to answer simple natural language questions [huang2019]. KEQA's strength lies in its ability to address predicate variability and entity ambiguity by leveraging the semantic proximity captured by embeddings. However, its focus on "simple questions" highlights a limitation in handling more complex, multi-hop, or nuanced queries, which often require deeper integration with natural language processing (NLP) capabilities.

More advanced QA systems have evolved into hybrid architectures that seamlessly integrate KGEs with sophisticated NLP models. A prime example is the Marie and BERT system for chemistry, which showcases a comprehensive approach to domain-specific QA [zhou2023]. This system employs hybrid KGEs, leveraging multiple embedding spaces to capture diverse relational patterns, and integrates a BERT-based entity-linking model to enhance robustness and accuracy in identifying entities from natural language queries. Furthermore, Marie and BERT addresses the complexities of deep ontologies by deriving implicit multi-hop relations and incorporates mechanisms for numerical filtering, demonstrating a significant leap in handling intricate, fact-oriented information retrieval in specialized domains. While such hybrid systems offer superior performance in complex scenarios, their domain specificity and the inherent complexity of integrating heterogeneous components (KGEs, BERT, semantic agents) can limit generalizability and increase development overhead. The methodological challenge lies in effectively harmonizing the continuous vector representations from KGEs with the discrete, symbolic reasoning often required for precise QA.

For recommender systems, KGEs provide a powerful mechanism to model user preferences and item characteristics by representing them as entities in a knowledge graph, thereby enabling more personalized and transparent suggestions. Recurrent Knowledge Graph Embedding (RKGE) was an early and influential approach that utilized a recurrent network to automatically learn semantic representations of paths between entities [sun2018]. By fusing these path semantics into the recommendation process, RKGE not only improved recommendation accuracy but also offered meaningful explanations based on path saliency, a crucial step towards interpretable recommendations. The recurrent network architecture allowed RKGE to discriminate the saliency of different paths in characterizing user preferences, moving beyond traditional feature engineering that often requires extensive domain knowledge.

Building upon this foundation, recent research has pushed towards more contextualized and explainable approaches. Contextualized Knowledge Graph Embedding for Explainable Talent Training Course Recommendation (CKGE) represents a significant advancement in this direction [yang2023]. CKGE constructs "meta-graphs" for talent-course pairs, incorporating contextualized neighbor semantics and high-order connections as "motivation-aware information." It then employs a novel KG-based Transformer, equipped with relational attention and structural encoding, to model the global dependencies of KG structured data. A key innovation in CKGE is its "local path mask prediction," which effectively reveals the importance of different paths, thereby offering precise and explainable recommendations that can discriminate the saliencies of meta-paths in characterizing corresponding preferences [yang2023].

Comparing RKGE and CKGE, the evolution highlights a shift from recurrent networks for path modeling to more sophisticated Transformer architectures that capture richer contextual information. While RKGE provided explanations based on path saliency, CKGE offers a more granular and motivation-aware interpretability by integrating contextualization and high-order connections. This progression, as noted in the development directions, reflects an acceleration in research driven by the demand for more sophisticated, robust, and interpretable recommendation solutions. However, the increased complexity of models like CKGE, with their meta-graph construction and Transformer integration, introduces trade-offs in computational cost and the interpretability of the underlying embedding space. The theoretical gap remains in developing truly intuitive and actionable explanations that are universally understandable to human users, rather than merely technical artifacts of the model.

In summary, KGEs have profoundly impacted QA and recommender systems by providing a robust framework to bridge natural language and structured knowledge. From foundational KEQA to hybrid systems like Marie and BERT, and from recurrent RKGEs to contextualized CKGEs, these advancements demonstrate KGE's utility in enabling more intelligent, personalized, and interpretable user interactions. The ongoing challenge lies in balancing model expressiveness with computational efficiency and ensuring that the generated explanations are genuinely transparent and actionable across diverse application contexts.
\subsection{Domain-Specific Applications and Explainability}
\label{sec:7_4_domain-specific_applications__and__explainability}

The utility of Knowledge Graph Embedding (KGE) models extends significantly into specialized, high-stakes domains, where their ability to transform symbolic knowledge into actionable insights is paramount. This section highlights the application of KGE in fields such as biological systems, patent metadata analysis, and drug repurposing, emphasizing how these models are tailored, validated, and increasingly scrutinized for explainability to build trust and deliver verifiable solutions.

In biological and biomedical systems, KGE models offer powerful tools for discovery and analysis. [mohamed2020] provides a comprehensive review of KGE applications in this domain, showcasing their predictive and analytical capabilities for tasks like drug-target interactions and polypharmacy side effects. The authors argue that KGEs are a natural fit for representing complex biological knowledge, overcoming the scalability limitations of traditional graph exploratory approaches. Building on this, [zhu2022] demonstrates multimodal reasoning based on KGE for specific diseases. They construct Specific Disease Knowledge Graphs (SDKGs) and integrate structural, category, and description embeddings using reverse-hyperplane projection. This multimodal approach enhances the discovery of new, reliable knowledge, underscoring the value of tailoring KGEs to domain-specific knowledge structures and leveraging diverse data modalities to improve reasoning.

A particularly compelling example of KGE application in a critical domain is drug repurposing for diseases like COVID-19. [islam2023] proposes an innovative approach that utilizes ensemble KGEs to generate robust latent representations, which are then fed into a deep neural network for identifying potential drug candidates. Crucially, this work moves beyond generic KGE evaluation metrics by incorporating \textit{molecular docking} to validate predictions, a domain-specific and verifiable method for assessing drug-target interactions. This integration of molecular-level evaluation is a significant step towards delivering transparent solutions, as it provides concrete, scientific validation for the abstract KGE predictions. Furthermore, [islam2023] addresses the paramount need for explainability by providing explanations through rules extracted from the knowledge graph and instantiated by explanatory paths. This allows medical professionals and researchers to understand *why* a particular drug is predicted, fostering trust and enabling actionable insights in a field where decisions have direct human impact.

Beyond biomedicine, KGEs are also applied to analyze complex structured data like patent metadata. [li2022] operationalizes knowledge proximity within the US Patent Database by training KGE models on a "PatNet" knowledge graph constructed from patent citations, inventors, assignees, and domain classifications. By using cosine similarity between learned embeddings, they measure knowledge proximity between homogeneous (e.g., patent-patent) and heterogeneous (e.g., inventor-assignee) entities. This application demonstrates how KGEs can be tailored to specific industry problems, providing quantitative measures for abstract concepts like "knowledge proximity" and enabling the analysis of domain expansion profiles for inventors and assignees. While this work primarily focuses on predictive performance, the inherent interpretability of proximity measures in the embedding space can offer insights into innovation landscapes.

The growing demand for interpretable KGE models in these high-stakes fields is a significant evolutionary trend. The explicit focus on explainability in [islam2023] highlights a shift from merely achieving high performance on abstract metrics to delivering verifiable and transparent solutions. This aligns with broader research efforts in making KGEs more understandable. For instance, methods that integrate logical rules and constraints into the embedding process, such as [guo2017]'s iterative guidance from soft rules (RUGE) or [tang2022]'s RulE framework, which learns rule embeddings jointly with entity and relation embeddings, inherently contribute to explainability. By aligning embeddings with human-understandable logical patterns, these approaches can provide a basis for explaining model predictions. Similarly, [ding2018] showed that even simple constraints like non-negativity on entity representations can improve model interpretability by structuring the embedding space.

However, achieving robust explainability in complex KGE models, especially those leveraging deep learning architectures, remains a challenge. The trade-off often lies between the high expressiveness and predictive power of complex models and the inherent difficulty in extracting clear, human-understandable explanations from their latent spaces. While [islam2023] successfully combines ensemble KGEs with molecular docking and rule-based explanations, the generalizability of such multi-faceted explanation strategies across all domain-specific KGE applications requires further investigation. The theoretical gap in universally interpretable embedding spaces, particularly for highly non-linear models, prevents a straightforward solution to providing transparent insights for every prediction. Nevertheless, the explicit integration of domain-specific validation and explanation mechanisms, as exemplified by these works, marks a crucial step towards building trust and enabling the responsible deployment of KGE technologies in critical real-world scenarios.


### Conclusion and Future Directions

\section{Conclusion and Future Directions}
\label{sec:conclusion__and__future_directions}

\label{sec:conclusion__and__future_directions}
\section{Conclusion and Future Directions}
\label{sec:conclusion_and_future_directions}

Having explored the diverse applications and real-world impact of Knowledge Graph Embedding (KGE) models in Section \ref{sec:applications_and_real_world_impact_of_kge}, this concluding section offers a comprehensive synthesis of the field's intellectual trajectory and charts a course for its future. We reflect on the remarkable progression of KGE research, which has evolved from foundational geometric and algebraic models to sophisticated deep learning architectures and advanced application-driven solutions. This journey has seen continuous efforts to enhance model expressiveness, efficiency, and robustness, leading to KGEs becoming indispensable tools for tasks ranging from link prediction and entity alignment to complex question answering and recommendation systems [community_0, community_1, community_3].

Despite these significant advancements, the field of KGE still grapples with persistent open challenges and theoretical gaps. These include the intricate balance between model complexity and interpretability, the efficient integration of high-quality logical rules, and the development of truly scalable and generalizable inductive models capable of handling dynamic, ever-evolving knowledge graphs [community_1, community_6]. Addressing these limitations is crucial for unlocking the full potential of KGE technologies. Looking ahead, this section will outline several emerging trends that are poised to redefine the landscape of KGE, notably the increasing integration with large language models for richer semantic understanding, advancements in adaptive multi-curvature embeddings, and the critical development of federated and privacy-preserving KGE methods [85064a4b1b96863af4fccff9ad34ce484945ad7b, community_1]. Finally, we will delve into the paramount ethical considerations surrounding KGE, including potential biases in learned representations and the imperative for transparent and responsible deployment in sensitive applications. This forward-looking perspective aims to inspire new research directions and guide the responsible advancement of KGE technologies towards a more intelligent and ethically sound future.

\subsection{Summary of Key Developments}
\label{sec:8_1_summary_of_key_developments}


The field of Knowledge Graph Embedding (KGE) has undergone a remarkable evolution, transitioning from foundational geometric and algebraic models to highly sophisticated deep learning architectures, driven by a continuous pursuit of enhanced expressiveness, efficiency, and robustness. Early advancements were rooted in the geometric paradigm, where relations were conceptualized as transformations within continuous vector spaces. Pioneering models like TransE and its successors, such as TransH [wang2014] and TransD [ji2015], established the translation-based approach, modeling relations as vector translations from head to tail entities. These early efforts focused on refining the embedding space to better capture diverse relational patterns, with TransD introducing dynamic mapping matrices for finer-grained distinctions between entities and relations, thereby improving expressiveness while managing parameter complexity [ji2015]. Further geometric innovations explored non-Euclidean spaces, such as TorusE [ebisu2017] embedding on Lie groups to address regularization challenges, and CyclE [yang2021] investigating the impact of metric choices (e.g., Cycle vs. Minkowski) on expressiveness. These models underscored the importance of the underlying geometry in accurately representing complex knowledge.

A significant paradigm shift occurred with the integration of deep learning architectures, which enabled KGE models to automatically learn intricate features and structural patterns. Convolutional Neural Networks (CNNs) were adapted to KGE, with models like AcrE [ren2020] leveraging atrous convolutions and residual learning for efficient feature interactions, and ReInceptionE [xie2020] employing inception networks and attention for joint local-global structural information. More recent CNN-based approaches, such as CNN-ECFA [hu2024] and SEConv [yang2025], continue to refine feature aggregation for improved performance. Graph Neural Networks (GNNs) further enhanced KGE by capturing neighborhood context and structural information through message passing, exemplified by DisenKGAT [wu2021], which introduced disentangled graph attention networks for more diverse and independent component representations. The emergence of Transformer architectures, as seen in CoKE [wang2019], Knowformer [li2023], and TGformer [shi2025], marked another leap, enabling contextualized embeddings by treating KGs as sequences or integrating graph structures into self-attention mechanisms, thereby capturing long-range dependencies and multi-structural features.

Beyond core architectural advancements, the field has continuously sought to enrich KGE models with auxiliary information, logical rules, and multi-modal data to overcome inherent limitations. Approaches like TransET [wang2021] and TaKE [he2023] demonstrated the value of incorporating entity type information to provide semantic guidance and improve knowledge graph completion. Similarly, models like HINGE [rosso2020] moved "beyond triplets" to directly learn from hyper-relational facts, capturing richer data semantics. The integration of logical rules, as in RUGE [guo2017] and RulE [tang2022], allowed KGE models to inject prior knowledge and enforce semantic consistency, moving towards more robust and interpretable reasoning. Furthermore, multi-modal KGE, exemplified by SSP [xiao2016] integrating text descriptions and recent works leveraging pre-trained language models [shen2022], has addressed data sparsity and enhanced semantic understanding by fusing diverse information sources.

The practical deployment of KGE models has driven significant research into efficiency, robustness, and adaptability. Efforts to enhance efficiency include knowledge distillation (DualDE [zhu2020]), embedding compression [sachan2020], parameter-efficient learning (EARL [chen2023]), and optimized training systems like GE2 [zheng2024]. Robustness has been improved through techniques like confidence-aware negative sampling [shan2018], reinforcement learning-based noise filtering [zhang2021], and weighted training for imbalanced data [zhang2023]. The challenge of dynamic KGs has led to inductive KGE models leveraging neighborhood aggregation [wang2018] and meta-learning [chen2021, sun2024], as well as continual learning approaches like incremental LoRA for efficient updates [liu2024]. Moreover, the rise of federated learning has spurred research into privacy-preserving KGE, addressing communication efficiency [zhang2024] and personalization [zhang2024_personalized], while also acknowledging security vulnerabilities like poisoning attacks [zhou2024].

In summary, the KGE landscape has evolved from simple geometric models to complex deep learning architectures, continuously pushing the boundaries of expressiveness and efficiency. The field's progression is marked by a holistic approach: enhancing core models, enriching them with diverse contextual and logical information, and ensuring their practical utility through robust, scalable, and adaptable designs. These advancements underscore the significant strides made in transforming symbolic knowledge into actionable insights, making KGE a cornerstone for intelligent AI systems across various applications, from link prediction and question answering [huang2019, zhou2023] to recommendation systems [sun2018, yang2023].
\subsection{Open Challenges and Theoretical Gaps}
\label{sec:8_2_open_challenges__and__theoretical_gaps}

Despite significant advancements in Knowledge Graph Embedding (KGE) research, several critical open challenges and theoretical gaps persist, representing fertile ground for future investigation. These issues often stem from inherent trade-offs, the complexity of real-world knowledge graphs, and the limitations of current theoretical understandings.

One pervasive challenge lies in balancing model expressiveness with computational complexity. While models like RotatE [sun2018] and the composition-closed HolmE [zheng2024] have pushed the boundaries of capturing intricate relational patterns, their increased expressiveness often comes at the cost of higher computational demands for training and inference (Subgroup 1, Overall Perspective). Efforts in efficiency and compression, such as DualDE's distillation [zhu2020] and LightKG's codebook-based storage [wang2021], aim to mitigate this. However, as noted by [sachan2020], these often entail a "minor loss in performance." The theoretical gap here is to devise architectures that are *inherently* expressive yet computationally lean, perhaps through novel mathematical formulations like the Orthogonal Procrustes Analysis in [peng2021], which offers a closed-form solution for efficiency, rather than relying on post-hoc compression or distillation.

Ensuring the interpretability of complex deep learning KGE models is another significant hurdle. As KGE increasingly leverages Graph Neural Networks (GNNs) and Transformer architectures (Subgroup 6, Advanced Model Design), their black-box nature becomes a concern, especially in high-stakes applications. While some models like SpherE [li2024] claim interpretability through their geometric properties, this is often specific to the model's design and does not generalize to the complex reasoning paths learned by deep networks. Application-focused works, such as the explainable drug repurposing by [islam2023] and contextualized recommendation by [yang2023], demonstrate the *need* for explanations, but typically rely on post-hoc rule extraction or path saliency. A theoretical gap exists in developing intrinsically interpretable deep KGE models that can transparently reveal their reasoning processes without sacrificing predictive power.

The efficient extraction and integration of high-quality logical rules remain a bottleneck. While methods like RUGE [guo2017] and RulE [tang2022] have shown the value of incorporating soft rules to enhance reasoning and consistency (Subgroup 3, Rule-based & Constraint-driven KGE), the process of obtaining these rules and balancing their adherence with the flexibility to capture exceptions is challenging. Automatically extracted rules often carry "uncertainties" [guo2017], and the scalability of integrating complex rule sets can be problematic [guo2020]. The theoretical challenge lies in developing robust, automated rule induction systems that can generate high-fidelity rules from noisy KGs and seamlessly integrate them into embedding models without introducing significant computational overhead or compromising the data-driven learning of nuanced patterns.

Furthermore, resolving issues related to the 'true' negative distribution in training is fundamental. KGE models rely heavily on negative sampling for contrastive learning, yet the "true" negative distribution is inherently unknown (Subgroup 3, Negative Sampling & Training Optimization). While approaches like NSCaching [zhang2018] and confidence-aware sampling [shan2018] improve efficiency and robustness to noise, they are still heuristic approximations. The "Efficient Non-Sampling Knowledge Graph Embedding" [li2021] attempts to bypass sampling entirely, but requires complex mathematical derivations to manage computational complexity. As highlighted by comprehensive reviews [qian2021, madushanka2024], this remains a persistent challenge, particularly when extending to multi-modal KGE [zhang2023]. A theoretical breakthrough is needed to either accurately model the true negative distribution or develop training paradigms that are robust to its uncertainty without prohibitive computational costs.

The field also critically needs more robust and unbiased evaluation metrics. As revealed by the "Evaluation, Benchmarking, and Reproducibility" subgroup, standard metrics often suffer from biases, such as the over-representation of certain entities [rossi2020], and reported results can be sensitive to hyperparameter choices or even data leakage [lloyd2022]. The lack of standardized frameworks and reproducibility issues [ali2020, broscheit2020] further complicate fair comparisons. The theoretical gap is in developing evaluation protocols that are robust to dataset characteristics, sensitive to diverse relational patterns (e.g., long-tail, compositional), and truly reflect real-world application performance, moving beyond simple link prediction accuracy.

The challenges of scalability for extremely large and dynamic knowledge graphs are paramount. While progress has been made in efficiency [peng2021, zheng2024] and compression [sachan2020, wang2021], handling KGs with billions of triples and continuous, real-time updates remains difficult. Dynamic KGE methods like FastKGE [liu2024] and MetaHG [sun2024] focus on efficient updates and mitigating catastrophic forgetting, but the sheer volume and velocity of changes in real-world KGs can still overwhelm these systems. Federated KGE introduces additional complexities related to communication efficiency [zhang2024] and personalized aggregation for diverse client data [zhang2024_personalized]. The theoretical challenge is to design truly elastic and adaptive KGE architectures that can scale horizontally, handle continuous streams of new information, and maintain global consistency and performance in highly distributed and dynamic environments without prohibitive computational or communication costs.

Finally, the development of truly generalizable inductive models remains an open problem. While neighborhood aggregation [wang2018] and meta-learning approaches [chen2021, sun2024] have enabled inductive capabilities for new entities (Subgroup 4, Dynamic, Inductive, and Continual KGE), they often rely on existing neighbors or transferable meta-knowledge. Truly novel entities with entirely new semantic or structural patterns, or isolated entities with sparse connections, still pose a significant challenge. A deeper theoretical understanding of "semantic evidence" for extrapolation [li2021] is required to build models that can robustly infer representations for such unseen entities or entirely new relational types, pushing beyond mere interpolation to genuine generalization. These challenges collectively underscore the need for continued fundamental and applied research to unlock the full potential of KGE.
\subsection{Emerging Trends and Ethical Considerations}
\label{sec:8_3_emerging_trends__and__ethical_considerations}

The landscape of Knowledge Graph Embedding (KGE) is continuously evolving, driven by both technological advancements and a growing awareness of societal impact. This section delves into key emerging trends that are shaping the future of KGE research, alongside crucial ethical considerations that must guide its development. These discussions are informed by the field's methodological evolution, knowledge progression, and the increasing demand for robust, responsible AI systems.

One of the most significant emerging trends is the deeper integration of KGE with pre-trained language models (PLMs) for richer semantic understanding. Traditional KGE models, as discussed in the "Core KGE Model Architectures and Expressiveness" subgroup, primarily learn representations from the structural patterns of knowledge graphs [wang2014, sun2018, zheng2024]. While effective, these models often struggle with data sparsity and entities lacking sufficient structural connections, a limitation highlighted in the "Knowledge Progression" of KGE research. PLMs, on the other hand, excel at capturing rich contextual semantics from vast amounts of text. Hybrid approaches, such as those that leverage BERT-based models for entity linking and contextual understanding within KGE systems [zhou2023], represent a powerful synergy. This integration allows KGEs to infer meaning from textual descriptions, thereby enhancing embedding quality and addressing the cold-start problem for new entities. Surveys like [dai2020] and [cao2022] have begun to acknowledge the potential of incorporating textual information, but the current trend moves towards more sophisticated, joint learning frameworks that align and fuse representations from both modalities. A key challenge here lies in effectively bridging the gap between the discrete, symbolic nature of KGs and the continuous, contextualized space of PLMs, while managing the increased computational complexity.

Another prominent trend involves the development of more adaptive multi-curvature embeddings. As explored in the "Geometric KGE for Hierarchical and Complex Structures" subgroup, hyperbolic spaces have demonstrated superior capabilities for modeling hierarchical structures due to their negative curvature [pan2021, liang2024]. However, real-world knowledge graphs often exhibit a mixture of structural patterns—hierarchical, cyclic, and Euclidean-like. The emerging direction is to move beyond single-geometry embeddings towards models that can adaptively utilize different curvatures (e.g., hyperbolic, spherical, Euclidean) for different parts of a knowledge graph or for different types of relations. Approaches like [shang2024], which propose mixed geometry message functions and scoring functions, exemplify this trend. By integrating information from multiple geometric spaces, these models aim to capture diverse local structures with higher fidelity and fewer dimensions, offering a more expressive and compact representation than any single geometry could provide. The methodological challenge lies in designing robust mechanisms for dynamically selecting or combining appropriate geometries, ensuring stable training, and avoiding an explosion in model complexity. Furthermore, models like [li2024] which embed entities as spheres, extend rotational embeddings to better capture many-to-many relations and enable set retrieval, showcasing the continuous innovation in geometric KGE.

Advancements in federated and privacy-preserving KGE also constitute a critical emerging trend, directly addressing the practical and ethical concerns of data distribution and privacy. The "Federated KGE, Privacy, and Security" subgroup highlights the growing interest in collaboratively training KGE models across distributed knowledge graphs without centralizing sensitive data. This is crucial for applications where data privacy regulations (e.g., GDPR) are paramount. Recent works focus on improving communication efficiency, such as [zhang2024] which proposes entity-wise Top-K sparsification to reduce transmitted parameters. Furthermore, addressing data heterogeneity among clients is vital, leading to personalized federated KGE approaches that learn client-specific supplementary knowledge [zhang2024]. However, this distributed paradigm introduces new security vulnerabilities, as demonstrated by poisoning attacks that can manipulate model outcomes by injecting malicious data indirectly through aggregation [zhou2024]. The trade-off between privacy guarantees, communication efficiency, personalization, and robustness against adversarial attacks remains a central challenge, requiring sophisticated cryptographic techniques and robust aggregation mechanisms.

Beyond these technological advancements, crucial ethical considerations are increasingly guiding KGE research. Foremost among these is the issue of potential biases in learned representations. KGE models learn from existing knowledge graphs, which are often constructed from diverse sources reflecting historical, societal, or cultural biases. If the training data contains skewed representations (e.g., gender stereotypes, racial biases, under-representation of certain groups), the KGE model will inevitably learn and potentially amplify these biases. This can lead to discriminatory outcomes in downstream applications, such as biased recommendations or unfair decision-making in sensitive domains. While the "Evaluation, Benchmarking, and Reproducibility" subgroup has focused on hyperparameter effects [lloyd2022] and evaluation biases [rossi2020], there is a growing imperative to explicitly detect, measure, and mitigate these semantic biases within the embedding space.

The responsible use of KGE in sensitive applications is another paramount ethical concern. As KGE models are increasingly deployed in high-stakes domains like healthcare (e.g., drug repurposing for COVID-19 [islam2023]), finance, and legal systems, the consequences of erroneous or biased predictions can be severe. The "Domain-Specific Application and Explainability" subgroup underscores the need for rigorous, domain-specific validation and explainability in such contexts. For instance, in drug repurposing, molecular evaluation is integrated to verify predictions [islam2023]. This highlights a critical shift from solely optimizing for accuracy on standard benchmarks to ensuring real-world safety, fairness, and accountability. The assumptions made during model design and the generalizability of experimental setups must be critically scrutinized, especially when findings from one domain are applied to another.

Finally, the imperative for transparent and explainable AI systems is gaining traction. Complex KGE models, particularly those leveraging deep learning architectures like GNNs or Transformers, often operate as "black boxes," making it difficult to understand *why* a particular prediction or recommendation was made. The "KGE for Downstream Applications and Explainability" subgroup directly addresses this, with models like CKGE [yang2023] and RKGE [sun2018] aiming to provide explainable recommendations through path saliency or contextualized neighbor semantics. For sensitive applications, mere performance is insufficient; users and stakeholders need to trust and verify the model's reasoning. This necessitates the development of KGE models that can provide human-understandable explanations, whether through extracting logical rules, highlighting influential paths, or visualizing attention mechanisms. The theoretical gap often lies in balancing the high expressiveness of complex models with the inherent simplicity required for genuine interpretability, presenting a continuous trade-off that future research must navigate. These emerging trends and ethical considerations collectively define the next frontier for KGE research, demanding not only technological ingenuity but also a strong commitment to societal responsibility.


