\subsection*{Summary of Key Developments}

The field of Knowledge Graph Embedding (KGE) has undergone a remarkable evolution, transitioning from foundational geometric and algebraic models to highly sophisticated deep learning architectures, driven by a continuous pursuit of enhanced expressiveness, efficiency, and robustness. Early advancements were rooted in the geometric paradigm, where relations were conceptualized as transformations within continuous vector spaces. Pioneering models like TransE and its successors, such as TransH \cite{wang2014} and TransD \cite{ji2015}, established the translation-based approach, modeling relations as vector translations from head to tail entities. These early efforts focused on refining the embedding space to better capture diverse relational patterns, with TransD introducing dynamic mapping matrices for finer-grained distinctions between entities and relations, thereby improving expressiveness while managing parameter complexity \cite{ji2015}. Further geometric innovations explored non-Euclidean spaces, such as TorusE \cite{ebisu2017} embedding on Lie groups to address regularization challenges, and CyclE \cite{yang2021} investigating the impact of metric choices (e.g., Cycle vs. Minkowski) on expressiveness. These models underscored the importance of the underlying geometry in accurately representing complex knowledge.

A significant paradigm shift occurred with the integration of deep learning architectures, which enabled KGE models to automatically learn intricate features and structural patterns. Convolutional Neural Networks (CNNs) were adapted to KGE, with models like AcrE \cite{ren2020} leveraging atrous convolutions and residual learning for efficient feature interactions, and ReInceptionE \cite{xie2020} employing inception networks and attention for joint local-global structural information. More recent CNN-based approaches, such as CNN-ECFA \cite{hu2024} and SEConv \cite{yang2025}, continue to refine feature aggregation for improved performance. Graph Neural Networks (GNNs) further enhanced KGE by capturing neighborhood context and structural information through message passing, exemplified by DisenKGAT \cite{wu2021}, which introduced disentangled graph attention networks for more diverse and independent component representations. The emergence of Transformer architectures, as seen in CoKE \cite{wang2019}, Knowformer \cite{li2023}, and TGformer \cite{shi2025}, marked another leap, enabling contextualized embeddings by treating KGs as sequences or integrating graph structures into self-attention mechanisms, thereby capturing long-range dependencies and multi-structural features.

Beyond core architectural advancements, the field has continuously sought to enrich KGE models with auxiliary information, logical rules, and multi-modal data to overcome inherent limitations. Approaches like TransET \cite{wang2021} and TaKE \cite{he2023} demonstrated the value of incorporating entity type information to provide semantic guidance and improve knowledge graph completion. Similarly, models like HINGE \cite{rosso2020} moved "beyond triplets" to directly learn from hyper-relational facts, capturing richer data semantics. The integration of logical rules, as in RUGE \cite{guo2017} and RulE \cite{tang2022}, allowed KGE models to inject prior knowledge and enforce semantic consistency, moving towards more robust and interpretable reasoning. Furthermore, multi-modal KGE, exemplified by SSP \cite{xiao2016} integrating text descriptions and recent works leveraging pre-trained language models \cite{shen2022}, has addressed data sparsity and enhanced semantic understanding by fusing diverse information sources.

The practical deployment of KGE models has driven significant research into efficiency, robustness, and adaptability. Efforts to enhance efficiency include knowledge distillation (DualDE \cite{zhu2020}), embedding compression \cite{sachan2020}, parameter-efficient learning (EARL \cite{chen2023}), and optimized training systems like GE2 \cite{zheng2024}. Robustness has been improved through techniques like confidence-aware negative sampling \cite{shan2018}, reinforcement learning-based noise filtering \cite{zhang2021}, and weighted training for imbalanced data \cite{zhang2023}. The challenge of dynamic KGs has led to inductive KGE models leveraging neighborhood aggregation \cite{wang2018} and meta-learning \cite{chen2021, sun2024}, as well as continual learning approaches like incremental LoRA for efficient updates \cite{liu2024}. Moreover, the rise of federated learning has spurred research into privacy-preserving KGE, addressing communication efficiency \cite{zhang2024} and personalization \cite{zhang2024_personalized}, while also acknowledging security vulnerabilities like poisoning attacks \cite{zhou2024}.

In summary, the KGE landscape has evolved from simple geometric models to complex deep learning architectures, continuously pushing the boundaries of expressiveness and efficiency. The field's progression is marked by a holistic approach: enhancing core models, enriching them with diverse contextual and logical information, and ensuring their practical utility through robust, scalable, and adaptable designs. These advancements underscore the significant strides made in transforming symbolic knowledge into actionable insights, making KGE a cornerstone for intelligent AI systems across various applications, from link prediction and question answering \cite{huang2019, zhou2023} to recommendation systems \cite{sun2018, yang2023}.