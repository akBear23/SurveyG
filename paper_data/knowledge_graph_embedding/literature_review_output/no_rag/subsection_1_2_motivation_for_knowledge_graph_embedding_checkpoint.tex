\subsection{Motivation for Knowledge Graph Embedding}
Knowledge Graphs (KGs) serve as powerful repositories of structured world knowledge, representing entities and their relationships in a symbolic, triple-based format (e.g., (subject, predicate, object)). While invaluable for many AI applications, traditional symbolic KGs inherently suffer from several critical limitations that impede their scalability, flexibility, and integration with modern machine learning paradigms. These limitations form the core motivation for the development of Knowledge Graph Embedding (KGE) techniques.

Firstly, symbolic representations are inherently **sparse and discrete**, making it challenging to capture nuanced semantic similarities between entities and relations \cite{dai2020, cao2022}. For instance, while a symbolic KG might state (``Paris'', ``locatedIn'', ``France'') and (``Berlin'', ``locatedIn'', ``Germany''), it struggles to infer that ``Paris'' and ``Berlin'' are both capital cities or that ``France'' and ``Germany'' are both European countries without explicit rules or additional facts. This sparsity also makes it difficult to generalize to unseen entities or relations, as there is no inherent notion of proximity or relatedness in the discrete symbolic space.

Secondly, reasoning over large-scale symbolic KGs is often **computationally inefficient**. As KGs grow exponentially in size, performing complex queries or multi-hop reasoning becomes computationally expensive, often requiring graph traversal algorithms that do not scale well \cite{dai2020}. Managing and processing vast amounts of discrete data poses significant challenges, leading to bottlenecks in real-world applications.

Thirdly, KGs are almost always **incomplete**, a pervasive issue that limits their utility. Many real-world facts are missing, and traditional symbolic methods struggle to infer these missing links without explicit, hand-crafted rules. This incompleteness directly impacts the performance of downstream tasks that rely on comprehensive knowledge.

To overcome these limitations, Knowledge Graph Embedding (KGE) emerged as a transformative approach, converting sparse, symbolic entities and relations into continuous, low-dimensional vector representations (embeddings) in a latent space \cite{dai2020, cao2022}. This fundamental shift from symbolic to vector-based representation offers several profound advantages:

\begin{itemize}
    \item \textbf{Scalability and Efficiency:} By representing entities and relations as dense vectors, KGE models transform complex symbolic problems into efficient vector operations, such as distance calculations or dot products. This significantly enhances computational efficiency and scalability, enabling KGs with millions of entities and relations to be processed and reasoned over more effectively. The "Efficiency, Compression, and System Optimization" subgroup highlights this, with works like \cite{zhu2020} demonstrating how knowledge distillation can reduce embedding parameters by 7-15x and increase inference speed, and \cite{wang2021} proposing lightweight frameworks for efficient storage and inference. More recently, system-level optimizations like \cite{zheng2024} aim for general and efficient KGE learning systems, achieving significant speedups.
    
    \item \textbf{Capturing Nuanced Semantic Similarities:} In the continuous embedding space, semantically similar entities or relations are mapped to proximate vectors. This allows KGE models to inherently capture nuanced semantic relationships that are difficult to express symbolically. For instance, translational models like TransH \cite{wang2014} and rotational models like RotatE \cite{sun2018} learn to represent relations as transformations (translations on hyperplanes or rotations in complex space) that connect head and tail entities, thereby capturing patterns like symmetry, antisymmetry, inversion, and composition. This capability is central to the "Core KGE Model Architectures and Expressiveness" subgroup.
    
    \item \textbf{Handling Incompleteness and Enabling Link Prediction:} KGE models are particularly adept at addressing KG incompleteness through link prediction. By learning the underlying patterns of existing facts, they can infer the plausibility of unobserved triples. This is achieved by scoring candidate triples based on the learned embeddings, effectively transforming the problem of finding missing links into an efficient vector similarity search \cite{rossi2020}. This capability is a cornerstone of KGE applications, as detailed in the "Link Prediction and Knowledge Graph Completion" section.
    
    \item \textbf{Seamless Integration with Modern Machine Learning Pipelines:} KGEs provide a powerful bridge between structured knowledge and modern machine learning (ML) techniques, especially deep learning. The continuous vector representations can be seamlessly integrated as features or pre-trained components into various ML models for tasks like natural language processing, computer vision, and recommendation systems. This allows KGs to enrich data-driven models with structured background knowledge, enhancing their performance and interpretability. The "KGE for Downstream Applications and Explainability" subgroup exemplifies this, with models like RKGE \cite{sun2018} and CKGE \cite{yang2023} leveraging embeddings for explainable recommendation, and systems like Marie and BERT \cite{zhou2023} integrating KGEs for chemistry-specific question answering.
    
    \item \textbf{Facilitating Diverse AI Tasks:} Beyond link prediction, KGEs enable a wide array of AI tasks by converting complex symbolic problems into efficient vector operations. These include:
    \begin{itemize}
        \item \textbf{Entity Alignment:} Identifying equivalent entities across different KGs by comparing their embeddings \cite{sun2018, zhang2019}.
        \item \textbf{Question Answering (QA):} Matching natural language questions to relevant facts in the KG by embedding both questions and KG elements into a common space \cite{huang2019, zhou2023}.
        \item \textbf{Recommendation Systems:} Modeling user-item interactions and preferences by embedding items and users within a KG context \cite{sun2018, yang2023}.
    \end{itemize}
\end{itemize}

While the conversion to vector space offers immense benefits, it also introduces challenges. Early KGE models, though efficient, often struggled to capture all complex relational patterns, leading to a continuous evolution towards more expressive geometric and deep learning models, as evidenced by the "Core KGE Model Architectures" and "Geometric KGE" subgroups. Furthermore, while embeddings make KGs more actionable for AI, the inherent interpretability of symbolic logic can sometimes be lost in dense vector spaces, spurring research into explainable KGEs \cite{yang2023}. This strategic shift from explicit symbolic to implicit dense vector representations, as highlighted in the overall perspective, represents a fundamental progression in knowledge representation, making KGs more accessible, scalable, and powerful for a wide spectrum of AI applications.