\subsection{Inductive and Continual KGE}
Real-world knowledge graphs (KGs) are inherently dynamic, with new entities, relations, and facts constantly emerging. Traditional Knowledge Graph Embedding (KGE) models are often transductive, meaning they can only generate embeddings for entities seen during training, necessitating expensive full retraining when new information arrives. This limitation has spurred significant research into inductive and continual KGE, aiming to adapt models to evolving KGs by handling unseen entities and efficiently updating knowledge without catastrophic forgetting \cite{liu2024, sun2024}. These methods are crucial for maintaining the scalability and relevance of KGE models in dynamic environments.

Early efforts in inductive KGE focused on neighborhood aggregation techniques. \cite{wang2018} introduced the Logic Attention Network (LAN), an aggregator that learns to embed new entities by combining the embeddings of their existing neighbors. LAN addresses the unordered and unequal nature of an entity's neighbors by employing both rules- and network-based attention weights. While innovative for its time, aggregation-based methods like LAN inherently rely on the presence of existing neighbors for new entities. This poses a limitation when truly novel, isolated entities with sparse connections emerge, as their representations might be less robust or even impossible to generate. The generalizability of such methods is also constrained by the quality and density of the local neighborhood information.

To overcome the limitations of direct entity embedding and enhance transferability, meta-learning has emerged as a powerful paradigm for inductive KGE. \cite{chen2021} proposed MorsE, a model that does not learn explicit entity embeddings but instead learns transferable meta-knowledge. This meta-knowledge, modeled by entity-independent modules and learned through meta-learning, can then be used to produce embeddings for new entities in an inductive setting. This approach offers a more generalized inductive capability compared to simple aggregation, as it aims to capture underlying patterns that are independent of specific entities. Building on this, \cite{sun2024} applied meta-learning to dynamic KGE in evolving service ecosystems with MetaHG. This model incorporates both local (via a GNN layer) and potential global (via a hypergraph neural network, HGNN, layer) structural information from current KG snapshots to enhance the representation of emerging entities. MetaHG's hybrid GNN framework and meta-learning strategy aim to mitigate issues like spatial deformation and improve the quality of embeddings for new entities. A critical comparison reveals that while meta-learning offers a more robust inductive framework, its complexity in training and the need for sufficient meta-training tasks can be significant. Furthermore, the effectiveness of meta-knowledge transfer can still be influenced by the similarity between the meta-training and target domains.

Beyond inductive learning, continual KGE addresses the challenge of efficiently acquiring new knowledge while simultaneously preserving previously learned information, a problem often plagued by catastrophic forgetting. \cite{liu2024} introduced FastKGE, a framework incorporating an incremental low-rank adapter (IncLoRA) mechanism. FastKGE tackles both efficient new knowledge acquisition and catastrophic forgetting by isolating and allocating new knowledge to specific layers based on the fine-grained influence between old and new KGs. The IncLoRA mechanism then embeds these specific layers into low-rank adapters, significantly reducing the number of trainable parameters during fine-tuning. This approach also features adaptive rank allocation, making the LoRA aware of entity importance. Experimental results demonstrate that FastKGE can reduce training time by 34-49\% on public datasets while maintaining competitive performance, and even greater savings (51-68\%) on larger, newly constructed datasets, alongside performance improvements. This parameter-efficient adaptation is a crucial advancement for large-scale KGE models, balancing the acquisition of new knowledge with the retention of old, a key trade-off in continual learning. However, the efficacy of LoRA-based methods can depend on the intrinsic rank of the updates and the architecture of the base KGE model.

The overarching goal across these inductive and continual KGE methods is to balance the acquisition of new knowledge with the retention of previously learned information, mitigating catastrophic forgetting and ensuring scalability. While neighborhood aggregation provides a straightforward, albeit limited, inductive capability, meta-learning offers a more generalized approach by learning transferable knowledge. Parameter-efficient adaptation techniques like IncLoRA represent a practical solution for continual learning, particularly for large models, by enabling efficient updates without full retraining. A common theoretical gap preventing a complete solution to these problems lies in developing truly universal inductive mechanisms that are robust to completely novel, isolated entities and can perform continuous, long-term updates without any degradation in performance or significant increase in computational cost. The experimental setups for these models often require specialized dynamic datasets, which can be less standardized than static link prediction benchmarks, making direct comparisons challenging and potentially affecting generalizability. The field continues to seek methods that can seamlessly integrate new information while preserving the integrity and expressiveness of the entire knowledge graph, a critical step towards truly adaptive and intelligent AI systems.