\subsection{Multi-modal and Cross-domain KGE}
The limitations of relying solely on structural information in knowledge graphs, particularly data sparsity and the inability to capture rich semantic nuances, have driven significant research into multi-modal and cross-domain Knowledge Graph Embedding (KGE). These approaches integrate diverse information sources, such as textual descriptions, visual features, or data from multiple domains, to enrich representations and enable more comprehensive knowledge understanding. The core motivation is to leverage complementary information, thereby enhancing the expressiveness and robustness of KGE models.

Early efforts in multi-modal KGE primarily focused on integrating textual descriptions to augment structural embeddings. For instance, the Semantic Space Projection (SSP) model \cite{xiao2016} proposed a method that jointly learns from symbolic triples and textual descriptions. SSP projects textual information into a semantic space, using text to discover semantic relevance and provide more precise embeddings. This was a crucial step in addressing the "weak-semantic" nature of purely geometric models, which often struggle to differentiate entities with similar structural positions but distinct semantic meanings. While SSP offered a foundational approach, its text integration was relatively simple, often relying on bag-of-words or basic word embeddings.

More recent advancements have leveraged the power of pre-trained language models (PLMs) for deeper semantic understanding. The "Joint Language Semantic and Structure Embedding for Knowledge Graph Completion" model \cite{shen2022} exemplifies this by fine-tuning PLMs with a probabilistic structured loss. This method effectively captures semantics from natural language descriptions while simultaneously reconstructing structural information, demonstrating state-of-the-art performance, particularly in low-resource settings where semantic cues are invaluable. This approach highlights a significant evolutionary trend, moving from simple text projection to sophisticated joint learning frameworks that deeply intertwine language semantics with graph structure. A key strength of such models is their ability to infer relations and entity properties even when structural data is sparse, a common challenge in many real-world KGs. However, the computational cost of fine-tuning large PLMs and the availability of high-quality textual descriptions for all entities remain practical constraints.

Beyond textual data, multi-modal KGE has expanded to incorporate other modalities and domain-specific knowledge. For instance, in the biomedical domain, "Multimodal reasoning based on knowledge graph embedding for specific diseases" \cite{zhu2022} constructs Specific Disease Knowledge Graphs (SDKGs) and implements multimodal reasoning using reverse-hyperplane projection. This model integrates structural, category, and description embeddings to discover new, reliable knowledge, showcasing how combining different modalities can lead to enhanced insights in specialized contexts. This demonstrates the power of multimodal integration in addressing the unique complexities and data characteristics of domain-specific KGs. However, the generalizability of such highly specialized models to other domains without significant re-engineering remains an open question. Furthermore, the challenge of designing effective negative sampling strategies becomes more pronounced in multi-modal settings, as highlighted by \cite{zhang2023}, which proposes Modality-Aware Negative Sampling (MANS) to align structural and visual embeddings, underscoring that training optimization must adapt to the complexity of heterogeneous data.

Cross-domain KGE extends this concept by enabling knowledge transfer and interaction across distinct knowledge graphs or domains. This is particularly vital for applications like recommender systems, where user preferences and item characteristics often span multiple categories or platforms. "Cross-Domain Knowledge Graph Chiasmal Embedding for Multi-Domain Item-Item Recommendation" \cite{liu2023} addresses the critical problems of cross-domain cold start and multi-domain recommendations. This approach proposes a "binding rule" to efficiently interact items across multiple domains, allowing for both homo-domain and hetero-domain item embeddings. By modeling associations and interactions between items across diverse domains, this method significantly improves multi-domain item-item recommendations, outperforming traditional recommender systems that struggle with data sparsity in new domains. The strength lies in its ability to leverage shared entities or relations to bridge information gaps, enriching representations for items even in domains with limited data. A limitation, however, is the reliance on explicit links or shared entities between domains, which may not always be readily available or accurately reflect complex cross-domain relationships.

In summary, multi-modal and cross-domain KGE represent a crucial evolutionary trajectory in knowledge representation. They move beyond the limitations of purely structural models by integrating diverse, complementary information sources. While models like SSP \cite{xiao2016} laid the groundwork for textual integration, the field has progressed to sophisticated joint learning frameworks leveraging pre-trained language models \cite{shen2022} and domain-specific multimodal reasoning \cite{zhu2022}. Concurrently, cross-domain approaches \cite{liu2023} tackle challenges like data sparsity and cold start in complex applications such as recommendation. The collective contribution of these methods is the creation of richer, more semantically grounded, and robust embeddings, which are essential for comprehensive knowledge understanding and practical applicability in diverse AI tasks. However, challenges persist in effectively fusing heterogeneous information, managing increased computational complexity, and designing robust training strategies like modality-aware negative sampling \cite{zhang2023}.