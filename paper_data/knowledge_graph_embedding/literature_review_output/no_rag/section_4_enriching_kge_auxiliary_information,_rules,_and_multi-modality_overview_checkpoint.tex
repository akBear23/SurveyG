\section{Enriching KGE: Auxiliary Information, Rules, and Multi-modality}
\label{sec:enriching_kge_auxiliary_information_rules_and_multi_modality}

Building upon the advancements in deep learning architectures for Knowledge Graph Embedding (KGE) discussed in Section \ref{sec:deep_learning_architectures_for_knowledge_graph_embedding}, which primarily focused on leveraging structural patterns within the graph, this section explores a crucial paradigm shift: enriching KGE models by integrating diverse external knowledge sources and logical constraints. While deep learning models like GNNs and Transformers have significantly enhanced the capture of intricate structural and contextual relationships, purely structural information often proves insufficient in addressing challenges such as data sparsity, ambiguity, and the need for more robust reasoning capabilities in complex, real-world scenarios \cite{general_kge_review_1}.

This section delves into advanced KGE approaches that move beyond the confines of graph topology alone, aiming to provide a more comprehensive and nuanced representation of knowledge. We will first examine how auxiliary information, such as entity types and attributes, can be seamlessly incorporated to provide semantic guidance, thereby improving embedding quality and robustness, particularly for incomplete or noisy knowledge graphs. Subsequently, the discussion will shift to the integration of explicit logical rules and constraints, which inject prior knowledge into the embedding process, enhancing reasoning capabilities and model interpretability by ensuring learned representations adhere to logical patterns. Finally, we explore the burgeoning field of multi-modal KGE, detailing how information from diverse modalities, including textual descriptions and visual features, can be leveraged to overcome data sparsity and enrich semantic understanding, enabling more holistic knowledge representation \cite{general_kge_review_2}. By exploring these complementary dimensions, this section highlights how KGE models can achieve superior performance, interpretability, and applicability in complex AI tasks.